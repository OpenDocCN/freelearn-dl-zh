<html><head></head><body>
<div id="_idContainer326" class="calibre2">
<h1 class="chapter-number" id="_idParaDest-92"><a id="_idTextAnchor130" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1.1">5</span></h1>
<h1 id="_idParaDest-93" class="calibre4"><a id="_idTextAnchor131" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.2.1">Empowering Text Classification: Leveraging Traditional Machine Learning Techniques</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.3.1">In this chapter, we’ll delve into the fascinating world of text classification, a foundational task in </span><strong class="bold"><span class="kobospan" id="kobo.4.1">natural language processing</span></strong><span class="kobospan" id="kobo.5.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.6.1">NLP</span></strong><span class="kobospan" id="kobo.7.1">) and </span><strong class="bold"><span class="kobospan" id="kobo.8.1">machine learning</span></strong><span class="kobospan" id="kobo.9.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.10.1">ML</span></strong><span class="kobospan" id="kobo.11.1">) that deals with </span><a id="_idIndexMarker429" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.12.1">categorizing text documents into predefined classes. </span><span class="kobospan" id="kobo.12.2">As the </span><a id="_idIndexMarker430" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.13.1">volume of digital text data continues to grow exponentially, the ability to accurately and efficiently classify text has become increasingly important for a wide range of applications, such as sentiment analysis, spam detection, and document organization. </span><span class="kobospan" id="kobo.13.2">This chapter provides a comprehensive overview of the key concepts, methodologies, and techniques that are employed in text classification, catering to readers from diverse backgrounds and </span><span><span class="kobospan" id="kobo.14.1">skill levels.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.15.1">We’ll begin by exploring the various types of text classification tasks and their unique characteristics, offering insights into the challenges and opportunities each type presents. </span><span class="kobospan" id="kobo.15.2">Next, we’ll introduce </span><a id="_idIndexMarker431" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.16.1">the concept of </span><strong class="bold"><span class="kobospan" id="kobo.17.1">N-grams</span></strong><span class="kobospan" id="kobo.18.1"> and discuss how they can be utilized as features for text classification, capturing not only individual words </span><a id="_idIndexMarker432" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.19.1">but also the local context and word sequences within the text. </span><span class="kobospan" id="kobo.19.2">We’ll then examine the widely used </span><strong class="bold"><span class="kobospan" id="kobo.20.1">term frequency-inverse document frequency</span></strong><span class="kobospan" id="kobo.21.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.22.1">TF-IDF</span></strong><span class="kobospan" id="kobo.23.1">) method, which assigns weights to words based on their frequency in a document and across the entire corpus, showcasing its effectiveness in distinguishing relevant words for </span><span><span class="kobospan" id="kobo.24.1">classification tasks.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.25.1">Following that, we’ll delve </span><a id="_idIndexMarker433" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.26.1">into the powerful </span><strong class="bold"><span class="kobospan" id="kobo.27.1">Word2Vec</span></strong><span class="kobospan" id="kobo.28.1"> algorithm and its application in text classification. </span><span class="kobospan" id="kobo.28.2">We’ll discuss how </span><strong class="bold"><span class="kobospan" id="kobo.29.1">Word2Vec</span></strong><span class="kobospan" id="kobo.30.1"> creates dense vector representations of words that capture semantic meaning and relationships, and how these embeddings can be used as features to improve classification performance. </span><span class="kobospan" id="kobo.30.2">Furthermore, we’ll </span><a id="_idIndexMarker434" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.31.1">cover popular architectures such as </span><strong class="bold"><span class="kobospan" id="kobo.32.1">continuous bag-of-words</span></strong><span class="kobospan" id="kobo.33.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.34.1">CBOW</span></strong><span class="kobospan" id="kobo.35.1">) and Skip-Gram, providing a deeper understanding of their </span><span><span class="kobospan" id="kobo.36.1">inner workings.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.37.1">Lastly, we’ll explore the concept of topic modeling, a technique for discovering hidden thematic </span><a id="_idIndexMarker435" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.38.1">structures within a collection of documents. </span><span class="kobospan" id="kobo.38.2">We’ll examine popular algorithms such as </span><strong class="bold"><span class="kobospan" id="kobo.39.1">latent Dirichlet allocation</span></strong><span class="kobospan" id="kobo.40.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.41.1">LDA</span></strong><span class="kobospan" id="kobo.42.1">) and describe how topic modeling can be applied to text classification, enabling the discovery of semantic relationships between documents and improving </span><span><span class="kobospan" id="kobo.43.1">classification performance.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.44.1">Throughout this chapter, we aim to provide a thorough understanding of the underlying concepts and techniques that are employed in text classification, equipping you with the knowledge and skills needed to successfully tackle real-world text </span><span><span class="kobospan" id="kobo.45.1">classification problems.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.46.1">The following topics will be covered in </span><span><span class="kobospan" id="kobo.47.1">this chapter:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><span class="kobospan" id="kobo.48.1">Types of </span><span><span class="kobospan" id="kobo.49.1">text classification</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.50.1">Text classification based </span><span><span class="kobospan" id="kobo.51.1">on N-grams</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.52.1">Text classification based </span><span><span class="kobospan" id="kobo.53.1">on TF-IDF</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.54.1">Word2Vec and its application in </span><span><span class="kobospan" id="kobo.55.1">text classification</span></span></li>
<li class="calibre15"><span><span class="kobospan" id="kobo.56.1">Topic modeling</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.57.1">Reviewing our use case – ML system design for NLP classification in a </span><span><span class="kobospan" id="kobo.58.1">Jupyter no</span><a id="_idTextAnchor132" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.59.1">tebook</span></span></li>
</ul>
<h1 id="_idParaDest-94" class="calibre4"><a id="_idTextAnchor133" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.60.1">Technical requirements</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.61.1">To effectively read and understand this chapter, it is essential to have a solid foundation in various technical areas. </span><span class="kobospan" id="kobo.61.2">A strong grasp of fundamental concepts in NLP, ML, and linear algebra is crucial. </span><span class="kobospan" id="kobo.61.3">Familiarity with text preprocessing techniques, such as tokenization, stop word removal, and stemming or lemmatization, is necessary to comprehend the data </span><span><span class="kobospan" id="kobo.62.1">preparation stage.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.63.1">Additionally, understanding </span><a id="_idIndexMarker436" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.64.1">basic ML algorithms, such as logistic regression and </span><strong class="bold"><span class="kobospan" id="kobo.65.1">support vector machines</span></strong><span class="kobospan" id="kobo.66.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.67.1">SVMs</span></strong><span class="kobospan" id="kobo.68.1">), is crucial for implementing text classification models. </span><span class="kobospan" id="kobo.68.2">Finally, being comfortable with evaluation metrics such as accuracy, precision, recall, and F1 score, along with concepts such as overfitting, underfitting, and hyperparameter tuning, will enable a deeper appreciation of the challenges and best practices in </span><span><span class="kobospan" id="kobo.69.1">text classifi</span><a id="_idTextAnchor134" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.70.1">cation.</span></span></p>
<h1 id="_idParaDest-95" class="calibre4"><a id="_idTextAnchor135" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.71.1">Types of text classification</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.72.1">Text classification is an NLP task where ML algorithms assign predefined categories or labels to text </span><a id="_idIndexMarker437" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.73.1">based on its content. </span><span class="kobospan" id="kobo.73.2">It involves training a model on a labeled dataset to enable it to accurately predict the category of unseen or new text inputs. </span><span class="kobospan" id="kobo.73.3">Text classification methods can be categorized into three main types – </span><strong class="bold"><span class="kobospan" id="kobo.74.1">supervised learning</span></strong><span class="kobospan" id="kobo.75.1">, </span><strong class="bold"><span class="kobospan" id="kobo.76.1">unsupervised learning</span></strong><span class="kobospan" id="kobo.77.1">, and </span><span><strong class="bold"><span class="kobospan" id="kobo.78.1">semi-supervised learning</span></strong></span><span><span class="kobospan" id="kobo.79.1">:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.80.1">Supervised learning</span></strong><span class="kobospan" id="kobo.81.1">: This type of text classification involves training a model on labeled </span><a id="_idIndexMarker438" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.82.1">data, where each data point is associated with a target label or category. </span><span class="kobospan" id="kobo.82.2">The model then uses this labeled data to learn the patterns and relationships between the input text and the target labels. </span><span class="kobospan" id="kobo.82.3">Examples </span><a id="_idIndexMarker439" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.83.1">of supervised learning algorithms </span><a id="_idIndexMarker440" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.84.1">for text classification include naive bayes, SVMs, and neural networks such as </span><strong class="bold"><span class="kobospan" id="kobo.85.1">convolutional neural networks</span></strong><span class="kobospan" id="kobo.86.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.87.1">CNNs</span></strong><span class="kobospan" id="kobo.88.1">) and </span><strong class="bold"><span class="kobospan" id="kobo.89.1">recurrent neural </span></strong><span><strong class="bold"><span class="kobospan" id="kobo.90.1">networks</span></strong></span><span><span class="kobospan" id="kobo.91.1"> (</span></span><span><strong class="bold"><span class="kobospan" id="kobo.92.1">RNNs</span></strong></span><span><span class="kobospan" id="kobo.93.1">).</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.94.1">Unsupervised learning</span></strong><span class="kobospan" id="kobo.95.1">: This type of text classification involves clustering or grouping </span><a id="_idIndexMarker441" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.96.1">text documents into categories or topics without any prior knowledge of the categories or labels. </span><span class="kobospan" id="kobo.96.2">Unsupervised learning is useful when there is no labeled data available or when the number of categories or topics is not known. </span><span class="kobospan" id="kobo.96.3">Examples of unsupervised learning algorithms for text </span><a id="_idIndexMarker442" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.97.1">classification include K-means clustering, LDA, and </span><strong class="bold"><span class="kobospan" id="kobo.98.1">hierarchical Dirichlet </span></strong><span><strong class="bold"><span class="kobospan" id="kobo.99.1">process</span></strong></span><span><span class="kobospan" id="kobo.100.1"> (</span></span><span><strong class="bold"><span class="kobospan" id="kobo.101.1">HDP</span></strong></span><span><span class="kobospan" id="kobo.102.1">).</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.103.1">Semi-supervised learning</span></strong><span class="kobospan" id="kobo.104.1">: This type of text classification combines both supervised </span><a id="_idIndexMarker443" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.105.1">and unsupervised learning approaches. </span><span class="kobospan" id="kobo.105.2">It involves using a small amount of labeled data to train a model and then using the model to classify the remaining unlabeled data. </span><span class="kobospan" id="kobo.105.3">The model then uses the unlabeled data to improve its classification performance. </span><span class="kobospan" id="kobo.105.4">Semi-supervised learning is useful when labeled data is scarce or expensive to obtain. </span><span class="kobospan" id="kobo.105.5">Examples of semi-supervised learning algorithms for text classification include </span><strong class="bold"><span class="kobospan" id="kobo.106.1">self-training</span></strong><span class="kobospan" id="kobo.107.1">, </span><strong class="bold"><span class="kobospan" id="kobo.108.1">co-training</span></strong><span class="kobospan" id="kobo.109.1">, and </span><span><strong class="bold"><span class="kobospan" id="kobo.110.1">multi-view learning</span></strong></span><span><span class="kobospan" id="kobo.111.1">.</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.112.1">Each of these text classification types has its strengths and weaknesses and is suitable for different types of applications. </span><span class="kobospan" id="kobo.112.2">Understanding these types can help in choosing the appropriate </span><a id="_idIndexMarker444" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.113.1">approach for a given problem. </span><span class="kobospan" id="kobo.113.2">In the following subsections, we’ll explain each of these met</span><a id="_idTextAnchor136" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.114.1">hods </span><span><span class="kobospan" id="kobo.115.1">in detail.</span></span></p>
<h2 id="_idParaDest-96" class="calibre7"><a id="_idTextAnchor137" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.116.1">Supervised learning</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.117.1">Supervised </span><a id="_idIndexMarker445" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.118.1">learning is a type of ML where an algorithm learns from labeled data to predict the label of new, </span><span><span class="kobospan" id="kobo.119.1">unseen data.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.120.1">In the context </span><a id="_idIndexMarker446" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.121.1">of text classification, supervised learning involves training a model on a labeled dataset, where each document or text sample is labeled with the corresponding category or class. </span><span class="kobospan" id="kobo.121.2">The model then uses this training data to learn patterns and relationships between the text features and their </span><span><span class="kobospan" id="kobo.122.1">associated labels:</span></span></p>
<ol class="calibre16">
<li class="calibre15"><span class="kobospan" id="kobo.123.1">In a supervised text classification task, the first step is to obtain a labeled dataset, where each text sample is annotated with its corresponding category </span><span><span class="kobospan" id="kobo.124.1">or class.</span></span><p class="calibre6"><span class="kobospan" id="kobo.125.1">A labeled dataset is assumed to possess the highest level of reliability. </span><span class="kobospan" id="kobo.125.2">Often, it is derived by having subject matter experts manually review the text and assign the appropriate class to each item. </span><span class="kobospan" id="kobo.125.3">In other scenarios, there may be automated methods for deriving the labels. </span><span class="kobospan" id="kobo.125.4">For instance, in cybersecurity, you may collect historical data and then assign labels, which may collect the outcome that followed each item – that is, whether the action was legitimate or not. </span><span class="kobospan" id="kobo.125.5">Since such historical data exists in most domains, that too can serve as a reliable </span><span><span class="kobospan" id="kobo.126.1">labeled set.</span></span></p></li>
<li class="calibre15"><span class="kobospan" id="kobo.127.1">The next step is to preprocess the text data to prepare it for modeling. </span><span class="kobospan" id="kobo.127.2">This may include steps such as tokenization, stemming or lemmatization, removing stop words, and other text </span><span><span class="kobospan" id="kobo.128.1">preprocessing techniques.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.129.1">After preprocessing, the text data is transformed into numerical features, often using techniques such as bag-of-words or </span><span><span class="kobospan" id="kobo.130.1">TF-IDF encoding.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.131.1">Then, a supervised learning algorithm such as logistic regression, SVM, or a neural network is trained on the labeled dataset using these </span><span><span class="kobospan" id="kobo.132.1">numerical features.</span></span></li>
</ol>
<p class="calibre6"><span class="kobospan" id="kobo.133.1">Once the </span><a id="_idIndexMarker447" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.134.1">model has been trained, it can be used to predict the category or class of new, unseen text data based on the learned patterns and </span><a id="_idIndexMarker448" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.135.1">relationships between the text features and their </span><span><span class="kobospan" id="kobo.136.1">associated labels.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.137.1">Supervised learning algorithms are commonly used for text classification tasks. </span><span class="kobospan" id="kobo.137.2">Let’s look at some common supervised learning algorithms that are used for </span><span><span class="kobospan" id="kobo.138.1">t</span><a id="_idTextAnchor138" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.139.1">ext classification.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.140.1">Naive Bayes</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.141.1">Naive Bayes is a </span><a id="_idIndexMarker449" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.142.1">probabilistic algorithm that is commonly </span><a id="_idIndexMarker450" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.143.1">used for text classification. </span><span class="kobospan" id="kobo.143.2">It is based on Bayes’ theorem, which states that the probability of a hypothesis (in this case, a document belonging to a particular class), given some observed evidence (in this case, the words in the document), is proportional to the probability of the evidence given the hypothesis times the prior probability of the hypothesis. </span><span class="kobospan" id="kobo.143.3">Naive Bayes assumes that the features (words) are independent of each other given the class label, which is where the “naive” part of</span><a id="_idTextAnchor139" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.144.1"> the name </span><span><span class="kobospan" id="kobo.145.1">comes from.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.146.1">Logistic regression</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.147.1">Logistic </span><a id="_idIndexMarker451" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.148.1">regression is a statistical method that is used </span><a id="_idIndexMarker452" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.149.1">for binary classification problems (that is, problems where there are only two possible classes). </span><span class="kobospan" id="kobo.149.2">It models the probability of the document belonging to a particular class using a logistic function, which maps any real-valued input to a</span><a id="_idTextAnchor140" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.150.1"> value between 0 </span><span><span class="kobospan" id="kobo.151.1">and 1.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.152.1">SVM</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.153.1">SVM is a </span><a id="_idIndexMarker453" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.154.1">powerful classification algorithm that is used </span><a id="_idIndexMarker454" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.155.1">in a variety of applications, including text classification. </span><span class="kobospan" id="kobo.155.2">SVM works by finding the hyperplane that best separates the data into different classes. </span><span class="kobospan" id="kobo.155.3">In text classification, the features are typically the words in the document, and the hyperplane is used to divide the space of all possible documents into different regions corresponding to </span><span><span class="kobospan" id="kobo.156.1">different classes.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.157.1">All of these algorithms can be trained using labeled data, where the class labels are known for </span><a id="_idIndexMarker455" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.158.1">each document in the training set. </span><span class="kobospan" id="kobo.158.2">Once </span><a id="_idIndexMarker456" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.159.1">trained, the model can be used to predict the class label of new, unlabeled documents. </span><span class="kobospan" id="kobo.159.2">The performance of the model is typically evaluated using metrics such as accuracy, preci</span><a id="_idTextAnchor141" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.160.1">sion, recall, and </span><span><span class="kobospan" id="kobo.161.1">F1 score.</span></span></p>
<h2 id="_idParaDest-97" class="calibre7"><a id="_idTextAnchor142" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.162.1">Unsupervised learning</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.163.1">Unsupervised learning is a type of ML where the data is not labeled and the algorithm is left to </span><a id="_idIndexMarker457" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.164.1">find patterns and structures on its own. </span><span class="kobospan" id="kobo.164.2">In the context of text classification, unsupervised learning methods can be used when there is no labeled </span><a id="_idIndexMarker458" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.165.1">data available or when the goal is to discover hidden patterns in the </span><span><span class="kobospan" id="kobo.166.1">text data.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.167.1">One common </span><a id="_idIndexMarker459" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.168.1">unsupervised learning method for text classification is </span><strong class="bold"><span class="kobospan" id="kobo.169.1">clustering</span></strong><span class="kobospan" id="kobo.170.1">. </span><span class="kobospan" id="kobo.170.2">Clustering algorithms group similar documents together based on their content, without any prior knowledge of what each document is about. </span><span class="kobospan" id="kobo.170.3">Clustering can be used to identify topics in a collection of documents or to group similar documents together for </span><span><span class="kobospan" id="kobo.171.1">further analysis.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.172.1">Another popular </span><a id="_idIndexMarker460" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.173.1">unsupervised learning algorithm for text classification is </span><strong class="bold"><span class="kobospan" id="kobo.174.1">LDA</span></strong><span class="kobospan" id="kobo.175.1">. </span><span class="kobospan" id="kobo.175.2">LDA is a probabilistic generative model that assumes that each document in a corpus is a mixture of topics, and each topic is a probability distribution over words. </span><span class="kobospan" id="kobo.175.3">LDA can be used to discover the underlying topics in a collection of documents, even when the topics are not </span><span><span class="kobospan" id="kobo.176.1">explicitly labeled.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.177.1">Finally, word embeddings are a popular unsupervised learning technique used for text classification. </span><span class="kobospan" id="kobo.177.2">Word embeddings are dense vector representations of words that capture their semantic meaning based on the context in which they appear. </span><span class="kobospan" id="kobo.177.3">They can be used to identify similar words and to find relationships between words, which can be useful for tasks such as text similarity and recommendation systems. </span><span class="kobospan" id="kobo.177.4">Common word embedding models include Word2Vec </span><span><span class="kobospan" id="kobo.178.1">and GloVe.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.179.1">Word2Vec is a popular algorithm that’s used to generate word embeddings, which are vector representations of words in a high-dimensional space. </span><span class="kobospan" id="kobo.179.2">The algorithm was developed by a team of researchers at Google, led by Tomas Mikolov, in 2013. </span><span class="kobospan" id="kobo.179.3">The main idea behind Word2Vec is that words that appear in similar contexts tend to have </span><span><span class="kobospan" id="kobo.180.1">similar meanings.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.181.1">The algorithm takes in a large corpus of text as input and generates a vector representation for each word in the vocabulary. </span><span class="kobospan" id="kobo.181.2">The vectors are typically high-dimensional (for example, 100 or 300 dimensions) and can be used to perform various NLP tasks, such as sentiment analysis, text classification, and </span><span><span class="kobospan" id="kobo.182.1">machine translation.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.183.1">Two main </span><a id="_idIndexMarker461" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.184.1">architectures are used in Word2Vec: </span><strong class="bold"><span class="kobospan" id="kobo.185.1">CBOW</span></strong><span class="kobospan" id="kobo.186.1"> and </span><strong class="bold"><span class="kobospan" id="kobo.187.1">skip-gram</span></strong><span class="kobospan" id="kobo.188.1">. </span><span class="kobospan" id="kobo.188.2">In the CBOW architecture, the algorithm tries to predict the </span><a id="_idIndexMarker462" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.189.1">target word given a window of context words. </span><span class="kobospan" id="kobo.189.2">In the skip-gram architecture, the algorithm tries to predict the context words given a target word. </span><span class="kobospan" id="kobo.189.3">The training objective is to maximize the likelihood of the target word or context words given </span><span><span class="kobospan" id="kobo.190.1">the input.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.191.1">Word2Vec has been widely adopted in the NLP community and has shown state-of-the-art performance on various benchmarks. </span><span class="kobospan" id="kobo.191.2">It has also been used in many real-world applications, such as recommender syste</span><a id="_idTextAnchor143" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.192.1">ms, search engines, </span><span><span class="kobospan" id="kobo.193.1">and chatbots.</span></span></p>
<h2 id="_idParaDest-98" class="calibre7"><a id="_idTextAnchor144" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.194.1">Semi-supervised learning</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.195.1">Semi-supervised learning is an ML paradigm that sits between supervised and unsupervised </span><a id="_idIndexMarker463" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.196.1">learning. </span><span class="kobospan" id="kobo.196.2">It utilizes a combination of labeled </span><a id="_idIndexMarker464" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.197.1">and unlabeled data for training, which is especially useful when  the underlying models require labeled data which is expensive or time-consuming. </span><span class="kobospan" id="kobo.197.2">This approach allows the model to leverage the information in the unlabeled data to improve its performance on the </span><span><span class="kobospan" id="kobo.198.1">classification task.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.199.1">In the context of text classification, semi-supervised learning can be beneficial when we have a limited number of labeled documents but a large corpus of unlabeled documents. </span><span class="kobospan" id="kobo.199.2">The goal is to improve the performance of the classifier by leveraging the information contained in the </span><span><span class="kobospan" id="kobo.200.1">unlabeled data.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.201.1">There are several common semi-supervised learning algorithms, including label propagation and co-training. </span><span class="kobospan" id="kobo.201.2">We’ll discuss</span><a id="_idTextAnchor145" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.202.1"> each of these in more </span><span><span class="kobospan" id="kobo.203.1">detail next.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.204.1">Label propagation</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.205.1">Label propagation is a graph-based semi-supervised learning algorithm. </span><span class="kobospan" id="kobo.205.2">It builds a graph using both </span><a id="_idIndexMarker465" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.206.1">labeled and unlabeled data points, with each data point represented as a node and edges representing the similarity </span><a id="_idIndexMarker466" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.207.1">between nodes. </span><span class="kobospan" id="kobo.207.2">The algorithm works by propagating the labels from the labeled nodes to the unlabeled nodes based on </span><span><span class="kobospan" id="kobo.208.1">their similarity.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.209.1">The key idea is that similar data points should have similar labels. </span><span class="kobospan" id="kobo.209.2">The algorithm begins by assigning initial label probabilities to the unlabeled nodes, typically based on their similarity to labeled nodes. </span><span class="kobospan" id="kobo.209.3">Then, an iterative process propagates these probabilities throughout the graph until convergence. </span><span class="kobospan" id="kobo.209.4">The final label probabilities are used t</span><a id="_idTextAnchor146" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.210.1">o classify the unlabeled </span><span><span class="kobospan" id="kobo.211.1">data points.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.212.1">Co-training</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.213.1">Co-training is another semi-supervised learning technique that trains multiple classifiers on different </span><a id="_idIndexMarker467" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.214.1">views of the data. </span><span class="kobospan" id="kobo.214.2">A view is a subset of features that are sufficient for the learning task and are conditionally independent given the class label. </span><span class="kobospan" id="kobo.214.3">The basic </span><a id="_idIndexMarker468" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.215.1">idea is to use one classifier’s predictions to label some of the unlabeled data, and then use that newly labeled data to train the other classifier. </span><span class="kobospan" id="kobo.215.2">This process is performed iteratively, with each classifier improving the other until a stopping criterion </span><span><span class="kobospan" id="kobo.216.1">is met.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.217.1">To apply semi-supervised learning in a specific domain, let’s consider a medical domain where we want to classify scientific articles into different categories such as </span><strong class="bold"><span class="kobospan" id="kobo.218.1">cardiology</span></strong><span class="kobospan" id="kobo.219.1">, </span><strong class="bold"><span class="kobospan" id="kobo.220.1">neurology</span></strong><span class="kobospan" id="kobo.221.1">, and </span><strong class="bold"><span class="kobospan" id="kobo.222.1">oncology</span></strong><span class="kobospan" id="kobo.223.1">. </span><span class="kobospan" id="kobo.223.2">Suppose we have a small set of labeled articles and a large set of </span><span><span class="kobospan" id="kobo.224.1">unlabeled articles.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.225.1">A possible approach could be to use label propagation by creating a graph of articles where the nodes represent the articles and the edges represent the similarity between the articles. </span><span class="kobospan" id="kobo.225.2">The similarity could be based on various factors, such as the words used, the topics covered, or the citation networks between the articles. </span><span class="kobospan" id="kobo.225.3">After propagating the labels, we can classify the unlabeled articles based on the final </span><span><span class="kobospan" id="kobo.226.1">label probabilities.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.227.1">Alternatively, we could use co-training by splitting the features into two views, such as the abstract and the full text of the articles. </span><span class="kobospan" id="kobo.227.2">We would train two classifiers, one for each view, and iteratively update the classifiers using the predictions made by the other classifier on the </span><span><span class="kobospan" id="kobo.228.1">unlabeled data.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.229.1">In both cases, the goal is to leverage the information in the unlabeled data to improve the performance of the classifier in the </span><span><span class="kobospan" id="kobo.230.1">specific domain.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.231.1">In this chapter, we’ll elaborate on supervised t</span><a id="_idTextAnchor147" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.232.1">ext classification and </span><span><span class="kobospan" id="kobo.233.1">topic modeling.</span></span></p>
<h2 id="_idParaDest-99" class="calibre7"><a id="_idTextAnchor148" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.234.1">Sentence classification using one-hot encoding vector representation</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.235.1">One-hot </span><a id="_idIndexMarker469" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.236.1">encoded vector representation </span><a id="_idIndexMarker470" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.237.1">is a method of representing categorical data, such as words, as binary vectors. </span><span class="kobospan" id="kobo.237.2">In the context of text classification, one-hot encoding can be used to represent text data as numerical input features for a classification model. </span><span class="kobospan" id="kobo.237.3">Here’s a detailed explanation of text class</span><a id="_idTextAnchor149" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.238.1">ification using one</span><a id="_idTextAnchor150" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.239.1">-hot </span><span><span class="kobospan" id="kobo.240.1">encoding vectors.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.241.1">Text preprocessing</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.242.1">The </span><a id="_idIndexMarker471" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.243.1">first step is to preprocess the text data, as explained in the previous chapter. </span><span class="kobospan" id="kobo.243.2">The main goal of preprocessing is to transform raw text into a more structured and consistent format that can be </span><a id="_idIndexMarker472" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.244.1">easily understood and processed by ML algorithms. </span><span class="kobospan" id="kobo.244.2">Here are several reasons why text preprocessing is essential for one-hot encoded </span><span><span class="kobospan" id="kobo.245.1">vector classification:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.246.1">Noise reduction</span></strong><span class="kobospan" id="kobo.247.1">: Raw text data often contains noise, such as typos, spelling errors, special characters, and formatting inconsistencies. </span><span class="kobospan" id="kobo.247.2">Preprocessing helps to clean the text, reducing noise that may negatively impact the performance of the </span><span><span class="kobospan" id="kobo.248.1">classification model.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.249.1">Dimensionality reduction</span></strong><span class="kobospan" id="kobo.250.1">: One-hot encoded vector representation has a high dimensionality as each unique word in the dataset corresponds to a separate feature. </span><span class="kobospan" id="kobo.250.2">Preprocessing techniques, such as stop word removal, stemming, or lemmatization, can help reduce the size of the vocabulary, leading to a lower-dimensional feature space. </span><span class="kobospan" id="kobo.250.3">This can improve the efficiency of the classification algorithm and reduce the risk </span><span><span class="kobospan" id="kobo.251.1">of overfitting.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.252.1">Consistent representation</span></strong><span class="kobospan" id="kobo.253.1">: Converting all text to lowercase and applying stemming or lemmatization ensures that words with the same meaning or root form are consistently represented in the one-hot encoding vectors. </span><span class="kobospan" id="kobo.253.2">This can help the classification model learn more meaningful patterns from the data as it will not treat different forms of the same word as </span><span><span class="kobospan" id="kobo.254.1">separate features.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.255.1">Handling irrelevant information</span></strong><span class="kobospan" id="kobo.256.1">: Preprocessing can help remove irrelevant information, such as URLs, email addresses, or numbers, that may not contribute </span><a id="_idIndexMarker473" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.257.1">to the classification task. </span><span class="kobospan" id="kobo.257.2">Removing such information can improve the model’s ability to focus on the meaningful words and patterns in </span><span><span class="kobospan" id="kobo.258.1">the text.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.259.1">Improving model performance</span></strong><span class="kobospan" id="kobo.260.1">: Preprocessed text data can lead to better performance of the classification model as the model will learn from a cleaner and more structured dataset. </span><span class="kobospan" id="kobo.260.2">This can result in improved accuracy and generalization to new, unseen </span><span><span class="kobospan" id="kobo.261.1">text data.</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.262.1">Once </span><a id="_idIndexMarker474" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.263.1">we preprocess the text, we can start extracting the words in the tex</span><a id="_idTextAnchor151" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.264.1">t. </span><span class="kobospan" id="kobo.264.2">We call this task </span><span><span class="kobospan" id="kobo.265.1">vocabulary construction.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.266.1">Vocabulary construction</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.267.1">Construct a vocabulary containing all unique words in the preprocessed text. </span><span class="kobospan" id="kobo.267.2">Assign a unique </span><a id="_idIndexMarker475" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.268.1">index to each word in </span><span><span class="kobospan" id="kobo.269.1">the vocabulary.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.270.1">Vocabulary construction is an essential step in preparing text data for one-hot encoded vector </span><a id="_idIndexMarker476" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.271.1">classification. </span><span class="kobospan" id="kobo.271.2">The vocabulary is a set of all unique words (tokens) in the preprocessed text data. </span><span class="kobospan" id="kobo.271.3">It serves as a basis for creating one-hot-encoded feature vectors for each document. </span><span class="kobospan" id="kobo.271.4">Here’s a detailed explanation of the vocabulary construction process for one-hot encoded </span><span><span class="kobospan" id="kobo.272.1">vector classification:</span></span></p>
<ol class="calibre16">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.273.1">Create a set of unique words</span></strong><span class="kobospan" id="kobo.274.1">: After preprocessing the text data, gather all the words from all documents and create a set of unique words. </span><span class="kobospan" id="kobo.274.2">This set will represent the vocabulary. </span><span class="kobospan" id="kobo.274.3">The order of the words in the vocabulary does not matter, but it’s crucial to keep track of the indices assigned to each word as they will be used to create one-hot encoded </span><span><span class="kobospan" id="kobo.275.1">vectors later.</span></span><p class="calibre6"><span class="kobospan" id="kobo.276.1">For example, consider that the following preprocessed dataset consists of </span><span><span class="kobospan" id="kobo.277.1">two documents:</span></span></p><ul class="calibre17"><li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.278.1">Document 1</span></strong><span class="kobospan" id="kobo.279.1">: “apple </span><span><span class="kobospan" id="kobo.280.1">banana orange”</span></span></li><li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.281.1">Document 2</span></strong><span class="kobospan" id="kobo.282.1">: “banana </span><span><span class="kobospan" id="kobo.283.1">grape apple”</span></span></li></ul><p class="calibre6"><span class="kobospan" id="kobo.284.1">The vocabulary for this dataset would be {“apple”, “banana”, “</span><span><span class="kobospan" id="kobo.285.1">orange”, “grape”}.</span></span></p></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.286.1">Assign indices to the words</span></strong><span class="kobospan" id="kobo.287.1">: Once you have the set of unique words, assign a unique </span><a id="_idIndexMarker477" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.288.1">index to each word in the vocabulary. </span><span class="kobospan" id="kobo.288.2">These </span><a id="_idIndexMarker478" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.289.1">indices will be used to create one-hot-encoded vectors for </span><span><span class="kobospan" id="kobo.290.1">each document.</span></span><p class="calibre6"><span class="kobospan" id="kobo.291.1">Using the preceding example, you might assign the </span><span><span class="kobospan" id="kobo.292.1">following indic</span><a id="_idTextAnchor152" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.293.1">es:</span></span></p><ul class="calibre17"><li class="calibre15"><span class="kobospan" id="kobo.294.1">“</span><span><span class="kobospan" id="kobo.295.1">apple”: 0</span></span></li><li class="calibre15"><span class="kobospan" id="kobo.296.1">“</span><span><span class="kobospan" id="kobo.297.1">banana”: 1</span></span></li><li class="calibre15"><span class="kobospan" id="kobo.298.1">“</span><span><span class="kobospan" id="kobo.299.1">orange”: 2</span></span></li><li class="calibre15"><span class="kobospan" id="kobo.300.1">“</span><span><span class="kobospan" id="kobo.301.1">grape”: 3</span></span></li></ul></li>
</ol>
<h3 class="calibre8"><span class="kobospan" id="kobo.302.1">One-hot encoding</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.303.1">With </span><a id="_idIndexMarker479" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.304.1">the constructed vocabulary and assigned indices, you can now create one-hot encoded vectors for </span><a id="_idIndexMarker480" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.305.1">each document in the dataset. </span><span class="kobospan" id="kobo.305.2">One simple approach to creating </span><a id="_idIndexMarker481" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.306.1">a one-hot encoded vector is to use </span><strong class="bold"><span class="kobospan" id="kobo.307.1">bag-of-words</span></strong><span class="kobospan" id="kobo.308.1">. </span><span class="kobospan" id="kobo.308.2">For each word in a document, find its corresponding index in the vocabulary and set the value at that index to 1 in the one-hot-encoded vector. </span><span class="kobospan" id="kobo.308.3">If a word appears multiple times in the document, its corresponding value in the one-hot-encoded vector remains 1. </span><span class="kobospan" id="kobo.308.4">All other values in the vector will </span><span><span class="kobospan" id="kobo.309.1">be 0.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.310.1">For example, using the vocabulary and indices mentioned previously, the one-hot encoded vectors for the documents would be </span><span><span class="kobospan" id="kobo.311.1">as follows:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.312.1">Document 1</span></strong><span class="kobospan" id="kobo.313.1">: [1, 1, 1, 0] (apple, banana, and orange </span><span><span class="kobospan" id="kobo.314.1">are present)</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.315.1">Document 2</span></strong><span class="kobospan" id="kobo.316.1">: [1, 1, 0, 1] (apple, banana, and grape </span><span><span class="kobospan" id="kobo.317.1">are present)</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.318.1">Once we have the corresponding values for each document, we can create a feature matrix with one-hot-encoded vectors as rows, where each row represents a document and each column represents a word from the vocabulary. </span><span class="kobospan" id="kobo.318.2">This matrix will be used as input </span><a id="_idIndexMarker482" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.319.1">for the text classification </span><a id="_idIndexMarker483" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.320.1">model. </span><span class="kobospan" id="kobo.320.2">For example, in the previous example, the feature vectors for two documents are </span><span><span class="kobospan" id="kobo.321.1">as follows:</span></span></p>
<table class="no-table-style" id="table001-3">
<colgroup class="calibre10">
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
</colgroup>
<thead class="calibre12">
<tr class="no-table-style1">
<td class="no-table-style2"/>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.322.1">Apple</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.323.1">Banana</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.324.1">Orange</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.325.1">Grape</span></span></p>
</td>
</tr>
</thead>
<tbody class="calibre13">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.326.1">Document 1</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span class="kobospan" id="kobo.327.1">1</span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span class="kobospan" id="kobo.328.1">1</span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span class="kobospan" id="kobo.329.1">1</span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span class="kobospan" id="kobo.330.1">0</span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.331.1">Document 2</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span class="kobospan" id="kobo.332.1">1</span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span class="kobospan" id="kobo.333.1">1</span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span class="kobospan" id="kobo.334.1">0</span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span class="kobospan" id="kobo.335.1">1</span></p>
</td>
</tr>
</tbody>
</table>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.336.1">Table 5.1 – Sample one-hot-encoded vector for two documents</span></p>
<p class="calibre6"><span class="kobospan" id="kobo.337.1">Please note that with text preprocessing, it helps to have a smaller vocabulary and it gives us better model performance. </span><span class="kobospan" id="kobo.337.2">Besides that, if needed, we can perform feature selection methods (as explained previously in this book) on the extracted feature vectors to improve our </span><span><span class="kobospan" id="kobo.338.1">model performance.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.339.1">While creating a one-hot encoded vector from words is useful, sometimes, we need to consider the existence of two words beside each other. </span><span class="kobospan" id="kobo.339.2">For example, “very good” and “not good” can have different meanings. </span><span class="kobospan" id="kobo.339.3">To achieve this goal, we can </span><span><span class="kobospan" id="kobo.340.1">use N-grams.</span></span></p>
<h3 class="calibre8"><a id="_idTextAnchor153" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.341.1">N-grams</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.342.1">N-grams </span><a id="_idIndexMarker484" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.343.1">are a generalization of the bag-of-words model that takes into account the order of words by considering sequences of </span><em class="italic"><span class="kobospan" id="kobo.344.1">n</span></em><span class="kobospan" id="kobo.345.1"> consecutive words. </span><span class="kobospan" id="kobo.345.2">An N-gram </span><a id="_idIndexMarker485" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.346.1">is a contiguous sequence of </span><em class="italic"><span class="kobospan" id="kobo.347.1">n</span></em><span class="kobospan" id="kobo.348.1"> items (typically words) from a given text. </span><span class="kobospan" id="kobo.348.2">For example, in the sentence “The cat is on the mat,” the 2-grams (bigrams) would be “The cat,” “cat is,” “is on,” “on the,” and “</span><span><span class="kobospan" id="kobo.349.1">the mat.”</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.350.1">Using N-grams can help capture local context and word relationships, which may improve the performance of the classifier. </span><span class="kobospan" id="kobo.350.2">However, it also increases the dimensionality of the feature space, which can be </span><span><span class="kobospan" id="kobo.351.1">computationally expensive</span><a id="_idTextAnchor154" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.352.1">.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.353.1">Model training</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.354.1">Train an ML model, such as logistic regression, SVM, or neural networks, on the feature matrix </span><a id="_idIndexMarker486" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.355.1">to learn the relationship between the one-hot encoded text features and the target labels. </span><span class="kobospan" id="kobo.355.2">The model will </span><a id="_idIndexMarker487" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.356.1">learn to predict the class label based on the presence or absence of specific words in the document. </span><span class="kobospan" id="kobo.356.2">Once we’ve decided on the training process, we need to perform the </span><span><span class="kobospan" id="kobo.357.1">following tasks:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.358.1">Model evaluation</span></strong><span class="kobospan" id="kobo.359.1">: Evaluate the performance of the model using appropriate evaluation </span><a id="_idIndexMarker488" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.360.1">metrics, such as accuracy, precision, recall, F1 score, or confusion matrix, and use techniques such as cross-validation to get a reliable estimate of the model’s performance on </span><span><span class="kobospan" id="kobo.361.1">unseen data.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.362.1">Model application</span></strong><span class="kobospan" id="kobo.363.1">: Apply the trained model to new, unseen text data. </span><span class="kobospan" id="kobo.363.2">Preprocess </span><a id="_idIndexMarker489" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.364.1">and one-hot encode the new text data using the same vocabulary and use the model to predict the </span><span><span class="kobospan" id="kobo.365.1">class labels.</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.366.1">One potential limitation of using one-hot encoded vectors for text classification is that they do not capture word order, context, or semantic relationships between words. </span><span class="kobospan" id="kobo.366.2">This can lead to suboptimal performance, especially in more complex classification tasks. </span><span class="kobospan" id="kobo.366.3">More advanced techniques, such as word embeddings (for example, Word2Vec or GloVe) or deep learning models (for example, CNNs or RNNs), can provide better representations for text data in </span><span><span class="kobospan" id="kobo.367.1">these cases.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.368.1">In summary, text classification using one-hot-encoded vectors involves preprocessing text data, constructing a vocabulary, representing text data as one-hot encoded feature vectors, training an ML model on the feature vectors, and evaluating and applying the model to new text data. </span><span class="kobospan" id="kobo.368.2">The one-hot encoded vector representation is a simple but sometimes limited approach to text classification, and more advanced techniques may be necessary for </span><span><span class="kobospan" id="kobo.369.1">complex tasks.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.370.1">So far, we’ve learned about classifying documents using N-grams. </span><span class="kobospan" id="kobo.370.2">However, this approach has a drawback. </span><span class="kobospan" id="kobo.370.3">There are a considerable number of words that occur in the documents frequently and do not add value to our models. </span><span class="kobospan" id="kobo.370.4">To improve the models, text classification using TF-IDF has </span><span><span class="kobospan" id="kobo.371.1">been prop</span><a id="_idTextAnchor155" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.372.1">osed.</span></span></p>
<h1 id="_idParaDest-100" class="calibre4"><a id="_idTextAnchor156" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.373.1">Text classification using TF-IDF</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.374.1">One-hot encoded vector is a good approach to perform classification. </span><span class="kobospan" id="kobo.374.2">However, one of its weaknesses </span><a id="_idIndexMarker490" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.375.1">is that it does not consider the importance of different words based on different documents. </span><span class="kobospan" id="kobo.375.2">To solve this issue, using </span><strong class="bold"><span class="kobospan" id="kobo.376.1">TF-IDF</span></strong><span class="kobospan" id="kobo.377.1"> can </span><span><span class="kobospan" id="kobo.378.1">be helpful.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.379.1">TF-IDF </span><a id="_idIndexMarker491" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.380.1">is a numerical statistic that is used to measure the importance of a word in a document within a document collection. </span><span class="kobospan" id="kobo.380.2">It helps reflect the relevance of words in a document, considering not </span><a id="_idIndexMarker492" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.381.1">only their frequency within the document but also their rarity across the entire document collection. </span><span class="kobospan" id="kobo.381.2">The TF-IDF value of a word increases proportionally to its frequency in a document but is offset by the frequency of the word in the entire </span><span><span class="kobospan" id="kobo.382.1">document collection.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.383.1">Here’s a detailed explanation of the mathematical equations involved in </span><span><span class="kobospan" id="kobo.384.1">calculating TF-IDF:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.385.1">Term frequency (TF)</span></strong><span class="kobospan" id="kobo.386.1">: The TF of a word, </span><em class="italic"><span class="kobospan" id="kobo.387.1">t</span></em><span class="kobospan" id="kobo.388.1">, in a document, </span><em class="italic"><span class="kobospan" id="kobo.389.1">d</span></em><span class="kobospan" id="kobo.390.1">, represents the number of times the word occurs in the document, normalized by the total number of words in the document. </span><span class="kobospan" id="kobo.390.2">The TF can be calculated using the </span><span><span class="kobospan" id="kobo.391.1">following equation:</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.392.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo&gt;′&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;′&lt;/mo&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;′&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo&gt;′&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;′&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo&gt;′&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/259.png" class="calibre258"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.393.1">The TF measures the importance of a word within a </span><span><span class="kobospan" id="kobo.394.1">specific document.</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.395.1">Inverse document frequency (IDF)</span></strong><span class="kobospan" id="kobo.396.1">: The IDF of a word, </span><em class="italic"><span class="kobospan" id="kobo.397.1">t</span></em><span class="kobospan" id="kobo.398.1">, reflects the rarity of the word across the entire document collection. </span><span class="kobospan" id="kobo.398.2">IDF can be calculated using the </span><span><span class="kobospan" id="kobo.399.1">following equation:</span></span><p class="calibre6"><span><span class="kobospan" id="kobo.400.1">I</span></span><span><span class="kobospan" id="kobo.401.1">D</span></span><span><span class="kobospan" id="kobo.402.1">F</span></span><span><span class="kobospan" id="kobo.403.1">(</span></span><span><span class="kobospan" id="kobo.404.1">t</span></span><span><span class="kobospan" id="kobo.405.1">)</span></span><span> </span><span><span class="kobospan" id="kobo.406.1">=</span></span><span> </span><span><span class="kobospan" id="kobo.407.1">l</span></span><span><span class="kobospan" id="kobo.408.1">o</span></span><span><span class="kobospan" id="kobo.409.1">g</span></span><span> </span><span><span class="kobospan" id="kobo.410.1">(</span></span><span><span class="kobospan" id="kobo.411.1">(</span></span><span><span class="kobospan" id="kobo.412.1">T</span></span><span><span class="kobospan" id="kobo.413.1">o</span></span><span><span class="kobospan" id="kobo.414.1">t</span></span><span><span class="kobospan" id="kobo.415.1">a</span></span><span><span class="kobospan" id="kobo.416.1">l</span></span><span> </span><span><span class="kobospan" id="kobo.417.1">n</span></span><span><span class="kobospan" id="kobo.418.1">u</span></span><span><span class="kobospan" id="kobo.419.1">m</span></span><span><span class="kobospan" id="kobo.420.1">b</span></span><span><span class="kobospan" id="kobo.421.1">e</span></span><span><span class="kobospan" id="kobo.422.1">r</span></span><span> </span><span><span class="kobospan" id="kobo.423.1">o</span></span><span><span class="kobospan" id="kobo.424.1">f</span></span><span> </span><span><span class="kobospan" id="kobo.425.1">d</span></span><span><span class="kobospan" id="kobo.426.1">o</span></span><span><span class="kobospan" id="kobo.427.1">c</span></span><span><span class="kobospan" id="kobo.428.1">u</span></span><span><span class="kobospan" id="kobo.429.1">m</span></span><span><span class="kobospan" id="kobo.430.1">e</span></span><span><span class="kobospan" id="kobo.431.1">n</span></span><span><span class="kobospan" id="kobo.432.1">t</span></span><span><span class="kobospan" id="kobo.433.1">s</span></span><span> </span><span><span class="kobospan" id="kobo.434.1">i</span></span><span><span class="kobospan" id="kobo.435.1">n</span></span><span> </span><span><span class="kobospan" id="kobo.436.1">t</span></span><span><span class="kobospan" id="kobo.437.1">h</span></span><span><span class="kobospan" id="kobo.438.1">e</span></span><span> </span><span><span class="kobospan" id="kobo.439.1">c</span></span><span><span class="kobospan" id="kobo.440.1">o</span></span><span><span class="kobospan" id="kobo.441.1">l</span></span><span><span class="kobospan" id="kobo.442.1">l</span></span><span><span class="kobospan" id="kobo.443.1">e</span></span><span><span class="kobospan" id="kobo.444.1">c</span></span><span><span class="kobospan" id="kobo.445.1">t</span></span><span><span class="kobospan" id="kobo.446.1">i</span></span><span><span class="kobospan" id="kobo.447.1">o</span></span><span><span class="kobospan" id="kobo.448.1">n</span></span><span><span class="kobospan" id="kobo.449.1">)</span></span><span> </span><span><span class="kobospan" id="kobo.450.1">/</span></span><span> </span><span><span class="kobospan" id="kobo.451.1">(</span></span><span><span class="kobospan" id="kobo.452.1">N</span></span><span><span class="kobospan" id="kobo.453.1">u</span></span><span><span class="kobospan" id="kobo.454.1">m</span></span><span><span class="kobospan" id="kobo.455.1">b</span></span><span><span class="kobospan" id="kobo.456.1">e</span></span><span><span class="kobospan" id="kobo.457.1">r</span></span><span> </span><span><span class="kobospan" id="kobo.458.1">o</span></span><span><span class="kobospan" id="kobo.459.1">f</span></span><span> </span><span><span class="kobospan" id="kobo.460.1">d</span></span><span><span class="kobospan" id="kobo.461.1">o</span></span><span><span class="kobospan" id="kobo.462.1">c</span></span><span><span class="kobospan" id="kobo.463.1">u</span></span><span><span class="kobospan" id="kobo.464.1">m</span></span><span><span class="kobospan" id="kobo.465.1">e</span></span><span><span class="kobospan" id="kobo.466.1">n</span></span><span><span class="kobospan" id="kobo.467.1">t</span></span><span><span class="kobospan" id="kobo.468.1">s</span></span><span> </span><span><span class="kobospan" id="kobo.469.1">c</span></span><span><span class="kobospan" id="kobo.470.1">o</span></span><span><span class="kobospan" id="kobo.471.1">n</span></span><span><span class="kobospan" id="kobo.472.1">t</span></span><span><span class="kobospan" id="kobo.473.1">a</span></span><span><span class="kobospan" id="kobo.474.1">i</span></span><span><span class="kobospan" id="kobo.475.1">n</span></span><span><span class="kobospan" id="kobo.476.1">i</span></span><span><span class="kobospan" id="kobo.477.1">n</span></span><span><span class="kobospan" id="kobo.478.1">g</span></span><span> </span><span><span><span class="kobospan" id="kobo.479.1">w</span></span></span><span><span><span class="kobospan" id="kobo.480.1">o</span></span></span><span><span><span class="kobospan" id="kobo.481.1">r</span></span></span><span><span><span class="kobospan" id="kobo.482.1">d</span></span></span><span><span> </span></span><span><span><span class="kobospan" id="kobo.483.1">′</span></span></span><span><span><span class="kobospan" id="kobo.484.1">t</span></span></span><span><span><span class="kobospan" id="kobo.485.1">′</span></span></span><span><span><span class="kobospan" id="kobo.486.1">)</span></span></span><span><span><span class="kobospan" id="kobo.487.1">)</span></span></span></p><p class="calibre6"><span class="kobospan" id="kobo.488.1">The logarithm is used to dampen the effect of the IDF component. </span><span class="kobospan" id="kobo.488.2">If a word appears in many documents, its IDF value will be closer to 0, and if it appears in fewer documents, its IDF value will </span><span><span class="kobospan" id="kobo.489.1">be higher.</span></span></p></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.490.1">TF-IDF computation</span></strong><span class="kobospan" id="kobo.491.1">: The TF-IDF value of a word, </span><em class="italic"><span class="kobospan" id="kobo.492.1">t</span></em><span class="kobospan" id="kobo.493.1">, in a document, </span><em class="italic"><span class="kobospan" id="kobo.494.1">d</span></em><span class="kobospan" id="kobo.495.1">, can be calculated by multiplying the TF of the word in the document with the IDF of the word across the </span><span><span class="kobospan" id="kobo.496.1">document collection:</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.497.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;I&lt;/mi&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;*&lt;/mi&gt;&lt;mi&gt;I&lt;/mi&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/260.png" class="calibre259"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.498.1">The resulting TF-IDF value represents the importance of a word in a document, taking into account both its frequency within the document and its rarity across the entire document collection. </span><span class="kobospan" id="kobo.498.2">High TF-IDF values indicate words that are more significant in a particular document, whereas low TF-IDF values indicate words that are either common across all documents or rare within the </span><span><span class="kobospan" id="kobo.499.1">specific document.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.500.1">Let’s consider a simple example </span><a id="_idIndexMarker493" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.501.1">of classifying movie </span><a id="_idIndexMarker494" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.502.1">reviews into two categories: positive and negative. </span><span class="kobospan" id="kobo.502.2">We have a small dataset with three movie reviews and their respective labels, </span><span><span class="kobospan" id="kobo.503.1">as follows:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.504.1">Document 1 (positive)</span></strong><span class="kobospan" id="kobo.505.1">: “I loved the movie. </span><span class="kobospan" id="kobo.505.2">The acting was great and the story </span><span><span class="kobospan" id="kobo.506.1">was captivating.”</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.507.1">Document 2 (negative)</span></strong><span class="kobospan" id="kobo.508.1">: “The movie was boring. </span><span class="kobospan" id="kobo.508.2">I did not like the story, and the acting </span><span><span class="kobospan" id="kobo.509.1">was terrible.”</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.510.1">Document 3 (positive)</span></strong><span class="kobospan" id="kobo.511.1">: “An amazing movie with a wonderful story and </span><span><span class="kobospan" id="kobo.512.1">brilliant acting.”</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.513.1">Now, we will use TF-IDF to classify a new, unseen </span><span><span class="kobospan" id="kobo.514.1">movie review:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.515.1">Document 4 (unknown)</span></strong><span class="kobospan" id="kobo.516.1">: “The story was interesting, and the acting </span><span><span class="kobospan" id="kobo.517.1">was good.”</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.518.1">Here are the steps that we need to perform to have the classifier predict the class of </span><span><span class="kobospan" id="kobo.519.1">our document:</span></span></p>
<ol class="calibre16">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.520.1">Step 1 – preprocess the text data</span></strong><span class="kobospan" id="kobo.521.1">: Tokenize, lowercase, remove stop words, and apply stemming or lemmatization to the words in </span><span><span class="kobospan" id="kobo.522.1">all documents:</span></span><ul class="calibre17"><li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.523.1">Document 1</span></strong><span class="kobospan" id="kobo.524.1">: “love movi act great </span><span><span class="kobospan" id="kobo.525.1">stori captiv”</span></span></li><li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.526.1">Document 2</span></strong><span class="kobospan" id="kobo.527.1">: “movi bore not like stori </span><span><span class="kobospan" id="kobo.528.1">act terribl”</span></span></li><li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.529.1">Document 3</span></strong><span class="kobospan" id="kobo.530.1">: “amaz movi wonder stori </span><span><span class="kobospan" id="kobo.531.1">brilliant act”</span></span></li><li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.532.1">Document 4</span></strong><span class="kobospan" id="kobo.533.1">: “stori interest </span><span><span class="kobospan" id="kobo.534.1">act good”</span></span></li></ul></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.535.1">Step 2 – create the vocabulary</span></strong><span class="kobospan" id="kobo.536.1">: Combine all unique words from the </span><span><span class="kobospan" id="kobo.537.1">preprocessed documents:</span></span><p class="calibre6"><span class="kobospan" id="kobo.538.1">Vocabulary: {“love”, “movi”, “act”, “great”, “stori”, “captiv”, “bore”, “not”, “like”, “terribl”, “amaz”, “wonder”, “brilliant”, “</span><span><span class="kobospan" id="kobo.539.1">interest”, “good”}</span></span></p></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.540.1">Step 3 – calculate the TF and IDF values</span></strong><span class="kobospan" id="kobo.541.1">: Compute the TF and IDF for each word </span><a id="_idIndexMarker495" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.542.1">in </span><span><span class="kobospan" id="kobo.543.1">each document.</span></span><p class="calibre6"><span class="kobospan" id="kobo.544.1">For </span><a id="_idIndexMarker496" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.545.1">example, for the word “stori” in Document 4, we have </span><span><span class="kobospan" id="kobo.546.1">the following:</span></span></p></li>
</ol>
<p class="calibre6"><span class="kobospan" id="kobo.547.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&quot;&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&quot;&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mn&gt;4&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;mn&gt;4&lt;/mn&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;0.25&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/261.png" class="calibre260"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.548.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;I&lt;/mi&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&quot;&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&quot;&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;4&lt;/mn&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;≈&lt;/mo&gt;&lt;mn&gt;0.287&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/262.png" class="calibre261"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.549.1">4.</span><strong class="bold"><span class="kobospan" id="kobo.550.1">	Step 4 – compute the TF-IDF values</span></strong><span class="kobospan" id="kobo.551.1">: Calculate the TF-IDF values for each word in </span><span><span class="kobospan" id="kobo.552.1">each document.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.553.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;I&lt;/mi&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&quot;&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&quot;&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mn&gt;4&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;0.25&lt;/mn&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;*&lt;/mi&gt;&lt;mn&gt;0.287&lt;/mn&gt;&lt;mo&gt;≈&lt;/mo&gt;&lt;mn&gt;0.0717&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/263.png" class="calibre262"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.554.1">Repeat this process for all words in all documents and create a feature matrix with the </span><span><span class="kobospan" id="kobo.555.1">TF-IDF values.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.556.1">5.</span><strong class="bold"><span class="kobospan" id="kobo.557.1">	Step 5 – train a classifier</span></strong><span class="kobospan" id="kobo.558.1">: Split the dataset into a training set (documents 1 to 3) and a test set (document 4). </span><span class="kobospan" id="kobo.558.2">Train a classifier, such as logistic regression or SVM, using the training set’s TF-IDF feature matrix and their corresponding labels (positive </span><span><span class="kobospan" id="kobo.559.1">or negative).</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.560.1">6.</span><strong class="bold"><span class="kobospan" id="kobo.561.1">	Step 6 – predict the class label</span></strong><span class="kobospan" id="kobo.562.1">: Preprocess and compute the TF-IDF values for the new movie review (document 4) using the same vocabulary. </span><span class="kobospan" id="kobo.562.2">Use the trained classifier to predict the class label for document 4 based on its TF-IDF </span><span><span class="kobospan" id="kobo.563.1">feature vector.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.564.1">For example, if the classifier predicts a positive label for document 4, the classification result would be </span><span><span class="kobospan" id="kobo.565.1">as follows:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.566.1">Document 4 (</span></strong><span><strong class="bold"><span class="kobospan" id="kobo.567.1">Predicted)</span></strong></span><span><span class="kobospan" id="kobo.568.1">: “Positive”</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.569.1">By following these steps, you can use the TF-IDF representation to classify text documents </span><a id="_idIndexMarker497" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.570.1">based on the importance of words in the </span><a id="_idIndexMarker498" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.571.1">documents relative to the entire </span><span><span class="kobospan" id="kobo.572.1">document collection.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.573.1">In summary, the TF-IDF value is calculated using the mathematical equations for TF and IDF. </span><span class="kobospan" id="kobo.573.2">It serves as a measure of the importance of a word in a document relative to the entire document collection, considering both the frequency of the word within the document and its rarity across </span><a id="_idTextAnchor157" class="calibre5 pcalibre1 pcalibre"/><span><span class="kobospan" id="kobo.574.1">all documents.</span></span></p>
<h1 id="_idParaDest-101" class="calibre4"><a id="_idTextAnchor158" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.575.1">Text classification using Word2Vec</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.576.1">One of the </span><a id="_idIndexMarker499" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.577.1">methods to perform text classification </span><a id="_idIndexMarker500" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.578.1">is to convert the words into embedding vectors so that you can use those vectors for classification. </span><span class="kobospan" id="kobo.578.2">Word2Vec is a well-known method to pe</span><a id="_idTextAnchor159" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.579.1">rform </span><span><span class="kobospan" id="kobo.580.1">this task.</span></span></p>
<h2 id="_idParaDest-102" class="calibre7"><a id="_idTextAnchor160" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.581.1">Word2Vec</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.582.1">Word2Vec </span><a id="_idIndexMarker501" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.583.1">is a group of neural network-based models that are used to create word embeddings, which are dense vector representations of words in a continuous vector space. </span><span class="kobospan" id="kobo.583.2">These embeddings capture the semantic meaning and relationships between words based on the context in which they appear in the text. </span><span class="kobospan" id="kobo.583.3">Word2Vec has two main architectures. </span><span class="kobospan" id="kobo.583.4">As mentioned previously, the two main architectures that were designed to learn word embeddings are </span><strong class="bold"><span class="kobospan" id="kobo.584.1">CBOW</span></strong><span class="kobospan" id="kobo.585.1"> and </span><strong class="bold"><span class="kobospan" id="kobo.586.1">skip-gram</span></strong><span class="kobospan" id="kobo.587.1">. </span><span class="kobospan" id="kobo.587.2">Both architectures are designed to learn word embeddings by predicting words based on their </span><span><span class="kobospan" id="kobo.588.1">surrounding context:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.589.1">CBOW</span></strong><span class="kobospan" id="kobo.590.1">: The CBOW </span><a id="_idIndexMarker502" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.591.1">architecture aims to predict the target word given its surrounding context words. </span><span class="kobospan" id="kobo.591.2">It takes the average of the context word embeddings as input and predicts the target word. </span><span class="kobospan" id="kobo.591.3">CBOW is faster to train and works well with smaller datasets but may be less accurate for </span><span><span class="kobospan" id="kobo.592.1">infrequent words.</span></span><p class="calibre6"><span class="kobospan" id="kobo.593.1">In the CBOW model, the objective is to maximize the average log probability of observing the target word given the </span><span><span class="kobospan" id="kobo.594.1">context words:</span></span></p></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.595.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;C&lt;/mi&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/mfrac&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/munder&gt;&lt;mrow&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/264.png" class="calibre263"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.596.1">Here, T is the total number of words in the text, and P(target | context) is the probability of observing the target word given the context words, which is calculated using the </span><span><span class="kobospan" id="kobo.597.1">softmax function:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.598.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;msup&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;v&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msubsup&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;v&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;msup&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;v&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msubsup&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;v&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/265.png" class="calibre264"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.599.1">Here, </span><span class="kobospan" id="kobo.600.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="image/266.png" class="calibre265"/></span><span class="kobospan" id="kobo.601.1"> is the output vector (word embedding) of the target word, </span><span class="kobospan" id="kobo.602.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/267.png" class="calibre266"/></span><span class="kobospan" id="kobo.603.1"> is the average input vector (context word embedding) of the context words, and the sum in the denominator runs over all words in </span><span><span class="kobospan" id="kobo.604.1">the vocabulary.</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.605.1">Skip-gram</span></strong><span class="kobospan" id="kobo.606.1">: The skip-gram </span><a id="_idIndexMarker503" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.607.1">architecture aims to predict the surrounding context words given the target word. </span><span class="kobospan" id="kobo.607.2">It takes the target word embedding as input and predicts the context words. </span><span class="kobospan" id="kobo.607.3">Skip-gram works well with larger datasets and can capture the meaning of infrequent words more accurately, but it may be slower to train compared </span><span><span class="kobospan" id="kobo.608.1">to CBOW.</span></span><p class="calibre6"><span class="kobospan" id="kobo.609.1">In the skip-gram model, the objective is to maximize the average log probability of observing the context words given the </span><span><span class="kobospan" id="kobo.610.1">target word:</span></span></p></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.611.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;S&lt;/mml:mi&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;G&lt;/mml:mi&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:munder&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;l&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;o&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;g&lt;/mml:mi&gt;&lt;mml:mo&gt;⁡&lt;/mml:mo&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" src="image/268.png" class="calibre267"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.612.1">Here, T is the total </span><a id="_idIndexMarker504" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.613.1">number of words in the text, and P(context | target) is the probability of observing the context words given the target word, which is calculated using the </span><span><span class="kobospan" id="kobo.614.1">softmax function:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.615.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;msup&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;v&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msubsup&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;v&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;msup&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;v&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msubsup&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;v&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/269.png" class="calibre268"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.616.1">Here, </span><span class="kobospan" id="kobo.617.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="image/270.png" class="calibre269"/></span><span class="kobospan" id="kobo.618.1">is the output vector (context word embedding) of the context word, </span><span class="kobospan" id="kobo.619.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/271.png" class="calibre270"/></span><span class="kobospan" id="kobo.620.1">is the input vector (word embedding) of the target word, and the sum in the denominator runs over all words in </span><span><span class="kobospan" id="kobo.621.1">the vocabulary.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.622.1">The training process for both CBOW and skip-gram involves iterating through the text and updating </span><a id="_idIndexMarker505" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.623.1">the input and output weight matrices using </span><strong class="bold"><span class="kobospan" id="kobo.624.1">stochastic gradient descent</span></strong><span class="kobospan" id="kobo.625.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.626.1">SGD</span></strong><span class="kobospan" id="kobo.627.1">) and backpropagation to minimize the difference between the predicted words and the actual words. </span><span class="kobospan" id="kobo.627.2">The learned input weight matrix contains the word embeddings for each wo</span><a id="_idTextAnchor161" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.628.1">rd in </span><span><span class="kobospan" id="kobo.629.1">the vocabulary.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.630.1">Text classification using Word2Vec</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.631.1">Text classification </span><a id="_idIndexMarker506" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.632.1">using Word2Vec involves creating </span><a id="_idIndexMarker507" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.633.1">word embeddings using the Word2Vec algorithm and then training an ML model to classify text based on these embeddings. </span><span class="kobospan" id="kobo.633.2">The following steps outline the process in detail, including the </span><span><span class="kobospan" id="kobo.634.1">mathematical aspects:</span></span></p>
<ol class="calibre16">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.635.1">Text preprocessing</span></strong><span class="kobospan" id="kobo.636.1">: Clean and preprocess the text data by tokenizing, lowercasing, removing stop words, and stemming or lemmatizing </span><span><span class="kobospan" id="kobo.637.1">the words.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.638.1">Train the Word2Vec model</span></strong><span class="kobospan" id="kobo.639.1">: Train a Word2Vec model (either CBOW or Skip-Gram) on the preprocessed text data to create word embeddings. </span><span class="kobospan" id="kobo.639.2">The Word2Vec algorithm learns to predict a target word based on its context (CBOW) or predict the context words based on a target word (skip-gram). </span><span class="kobospan" id="kobo.639.3">The training </span><a id="_idIndexMarker508" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.640.1">objective is to maximize the average </span><a id="_idIndexMarker509" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.641.1">log probability of observing the context words given the </span><span><span class="kobospan" id="kobo.642.1">target word:</span></span></li>
</ol>
<p class="calibre6"><span class="kobospan" id="kobo.643.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/mfrac&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/munder&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;l&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;o&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;g&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/272.png" class="calibre271"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.644.1">Here, </span><em class="italic"><span class="kobospan" id="kobo.645.1">T</span></em><span class="kobospan" id="kobo.646.1"> is the total number of words in the text, and </span><em class="italic"><span class="kobospan" id="kobo.647.1">P(context | target)</span></em><span class="kobospan" id="kobo.648.1"> is the probability of observing the context words given the target word, which is calculated using the </span><span><span class="kobospan" id="kobo.649.1">softmax function:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.650.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;msup&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;v&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msubsup&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;v&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;msup&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;v&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msubsup&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;v&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/273.png" class="calibre272"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.651.1">Here, </span><span class="kobospan" id="kobo.652.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="image/274.png" class="calibre273"/></span><span class="kobospan" id="kobo.653.1">is the output vector (context word embedding) of the context word, </span><span class="kobospan" id="kobo.654.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/275.png" class="calibre274"/></span><span class="kobospan" id="kobo.655.1"> is the input vector (word embedding) of the target word, and the sum in the denominator runs over all words in </span><span><span class="kobospan" id="kobo.656.1">the vocabulary.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.657.1">3.</span><strong class="bold"><span class="kobospan" id="kobo.658.1">	Create document embeddings</span></strong><span class="kobospan" id="kobo.659.1">: For each document in the dataset, calculate the document embedding by averaging the word embeddings of the words in </span><span><span class="kobospan" id="kobo.660.1">the document:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.661.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/mfrac&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/munder&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;W&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;o&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;r&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;d&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;E&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;m&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;b&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;e&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;d&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;d&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;i&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;n&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;g&lt;/mi&gt;&lt;/mrow&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/276.png" class="calibre275"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.662.1">Here, N is the number of words in the document, and the sum runs over all words in the document. </span><span class="kobospan" id="kobo.662.2">Please note that based on our experience, this approach for text classification using Word2Vec is only useful when the document’s length is short. </span><span class="kobospan" id="kobo.662.3">If you have longer documents or there are opposite words in </span><a id="_idIndexMarker510" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.663.1">the document, this approach won’t perform well. </span><span class="kobospan" id="kobo.663.2">An alternative solution is to use Word2Vec and CNN together to </span><a id="_idIndexMarker511" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.664.1">fetch the word embeddings and then feed those embeddings as input of </span><span><span class="kobospan" id="kobo.665.1">the CNN.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.666.1">4.</span><strong class="bold"><span class="kobospan" id="kobo.667.1">	Model training</span></strong><span class="kobospan" id="kobo.668.1">: Use the document embeddings as features to train an ML model, such as logistic regression, SVM, or a neural network, for text classification. </span><span class="kobospan" id="kobo.668.2">The model learns to predict the class label based on the </span><span><span class="kobospan" id="kobo.669.1">document embeddings.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.670.1">5.</span><strong class="bold"><span class="kobospan" id="kobo.671.1">	Model evaluation</span></strong><span class="kobospan" id="kobo.672.1">: Evaluate the performance of the model using appropriate evaluation metrics, such as accuracy, precision, recall, F1 score, or confusion matrix, and use techniques such as cross-validation to get a reliable estimate of the model’s performance on </span><span><span class="kobospan" id="kobo.673.1">unseen data.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.674.1">6.</span><strong class="bold"><span class="kobospan" id="kobo.675.1">	Model application</span></strong><span class="kobospan" id="kobo.676.1">: Apply the trained model to new, unseen text data. </span><span class="kobospan" id="kobo.676.2">Preprocess and compute the document embeddings for the new text data using the same Word2Vec model and vocabulary, and use the model to predict the </span><span><span class="kobospan" id="kobo.677.1">class labels.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.678.1">In summary, text classification using Word2Vec involves creating word embeddings with the Word2Vec algorithm, averaging these embeddings to create document embeddings, and training an ML model to classify text based on these document embeddings. </span><span class="kobospan" id="kobo.678.2">The Word2Vec algorithm learns word embeddings by maximizing the average log probability of observing context words given a target word, capturing the semantic relationships be</span><a id="_idTextAnchor162" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.679.1">tween words in </span><span><span class="kobospan" id="kobo.680.1">the process.</span></span></p>
<h2 id="_idParaDest-103" class="calibre7"><a id="_idTextAnchor163" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.681.1">Model evaluation</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.682.1">Evaluating the performance of text classification models is crucial to ensure that they meet </span><a id="_idIndexMarker512" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.683.1">the desired level of accuracy and generalizability. </span><span class="kobospan" id="kobo.683.2">Several metrics and techniques are commonly used to evaluate text </span><a id="_idIndexMarker513" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.684.1">classification models, including accuracy, precision, recall, F1 score, and confusion matrix. </span><span class="kobospan" id="kobo.684.2">Let’s discuss </span><a id="_idTextAnchor164" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.685.1">each of these in </span><span><span class="kobospan" id="kobo.686.1">more detail:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.687.1">Accuracy</span></strong><span class="kobospan" id="kobo.688.1">: Accuracy </span><a id="_idIndexMarker514" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.689.1">is the most straightforward metric for classification tasks. </span><span class="kobospan" id="kobo.689.2">It measures the number of correctly classified records out of all classified records. </span><span class="kobospan" id="kobo.689.3">It is defined </span><span><span class="kobospan" id="kobo.690.1">as follows:</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.691.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/277.png" class="calibre276"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.692.1">While accuracy is easy to understand, it may not be the best metric for imbalanced datasets, where the majority class ca</span><a id="_idTextAnchor165" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.693.1">n dominate the </span><span><span class="kobospan" id="kobo.694.1">metric’s value.</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.695.1">Precision</span></strong><span class="kobospan" id="kobo.696.1">: Precision </span><a id="_idIndexMarker515" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.697.1">gauges the ratio of correctly identified positive instances to the total instances predicted as positive by the model. </span><span class="kobospan" id="kobo.697.2">It </span><a id="_idIndexMarker516" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.698.1">is also referred to as </span><strong class="bold"><span class="kobospan" id="kobo.699.1">positive predictive value</span></strong><span class="kobospan" id="kobo.700.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.701.1">PPV</span></strong><span class="kobospan" id="kobo.702.1">). </span><span class="kobospan" id="kobo.702.2">Precision proves valuable in scenarios where the expense associated with false positives is significant. </span><span class="kobospan" id="kobo.702.3">Precision is defined </span><span><span class="kobospan" id="kobo.703.1">as follows:</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.704.1">[[</span><span><span class="kobospan" id="kobo.705.1">OMM</span><a id="_idTextAnchor166" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.706.1">L-EQ-21D]]</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.707.1">Recall</span></strong><span class="kobospan" id="kobo.708.1">: Recall, also </span><a id="_idIndexMarker517" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.709.1">recognized as sensitivity or the </span><strong class="bold"><span class="kobospan" id="kobo.710.1">true positive rate</span></strong><span class="kobospan" id="kobo.711.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.712.1">TPR</span></strong><span class="kobospan" id="kobo.713.1">), assesses the ratio of correctly identified positive instances </span><a id="_idIndexMarker518" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.714.1">among the total actual positive instances. </span><span class="kobospan" id="kobo.714.2">Recall is useful when the cost of false negatives is high. </span><span class="kobospan" id="kobo.714.3">Mathematically, it is defined </span><span><span class="kobospan" id="kobo.715.1">as follows:</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.716.1">[[</span><span><span class="kobospan" id="kobo.717.1">O</span><a id="_idTextAnchor167" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.718.1">MML-EQ-22D]]</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.719.1">F1 score</span></strong><span class="kobospan" id="kobo.720.1">: The F1 score, derived as the harmonic mean of precision and recall, integrates </span><a id="_idIndexMarker519" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.721.1">both metrics into a unified value. </span><span class="kobospan" id="kobo.721.2">It is an important metric in the context of imbalanced datasets as it considers both false positives and false negatives. </span><span class="kobospan" id="kobo.721.3">Spanning from 0 to 1, with 1 representing the optimal outcome, the F1 score is mathematically expressed </span><span><span class="kobospan" id="kobo.722.1">as follows:</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.723.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/278.png" class="calibre277"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.724.1">When dealing with multi-class classification, we have F1 micro and F1 macro. </span><span class="kobospan" id="kobo.724.2">F1 micro and F1 macro are two ways to compute the F1 score for multi-class or multi-label classification problems. </span><span class="kobospan" id="kobo.724.3">They aggregate precision and recall differently, leading to different interpretations of the classifier’s performance.</span><a id="_idTextAnchor168" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.725.1"> Let’s discuss each in </span><span><span class="kobospan" id="kobo.726.1">more detail:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.727.1">F1 macro</span></strong><span class="kobospan" id="kobo.728.1">: F1 macro computes the F1 score for each class independently and then takes </span><a id="_idIndexMarker520" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.729.1">the average of those values. </span><span class="kobospan" id="kobo.729.2">This approach treats each class as equally important and does not consider the class imbalance. </span><span class="kobospan" id="kobo.729.3">Mathematically, F1 macro is defined </span><span><span class="kobospan" id="kobo.730.1">as follows:</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.731.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mfrac&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/munder&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/279.png" class="calibre278"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.732.1">Here, </span><em class="italic"><span class="kobospan" id="kobo.733.1">n</span></em><span class="kobospan" id="kobo.734.1"> is the number of classes, and </span><span class="kobospan" id="kobo.735.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/280.png" class="calibre279"/></span><span class="kobospan" id="kobo.736.1"> is the F1 score for the </span><span><span class="kobospan" id="kobo.737.1">i-th class.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.738.1">F1 macro is particularly useful when you want to evaluate the performance of a classifier across all classes without giving more weight to the majority class. </span><span class="kobospan" id="kobo.738.2">However, it may </span><a id="_idIndexMarker521" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.739.1">not be suitable when the class distribution is highly imbalanced as it can provide an overly optimistic</span><a id="_idTextAnchor169" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.740.1"> estimate of the </span><span><span class="kobospan" id="kobo.741.1">model’s performance.</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.742.1">F1 micro</span></strong><span class="kobospan" id="kobo.743.1">: F1 micro, on the other hand, aggregates the contributions of all classes to </span><a id="_idIndexMarker522" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.744.1">compute the F1 score. </span><span class="kobospan" id="kobo.744.2">It does this by calculating the global precision and recall values across all classes and then computing the F1 score based on these global values. </span><span class="kobospan" id="kobo.744.3">F1 micro takes class imbalance into account as it considers the number of instances in each class. </span><span class="kobospan" id="kobo.744.4">Mathematically, F1 micro is defined </span><span><span class="kobospan" id="kobo.745.1">as follows:</span></span><p class="calibre6"><span class="kobospan" id="kobo.746.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/281.png" class="calibre280"/></span></p><p class="calibre6"><span class="kobospan" id="kobo.747.1">Here, global precision and global recall are calculated </span><span><span class="kobospan" id="kobo.748.1">as follows:</span></span></p></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.749.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/282.png" class="calibre281"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.750.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/283.png" class="calibre282"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.751.1">F1 micro is useful when you want to evaluate the overall performance of a classifier considering the class distribution, especially when dealing with </span><span><span class="kobospan" id="kobo.752.1">imbalanced datasets.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.753.1">In summary, F1 macro and F1 micro are two ways to compute the F1 score for multi-class or multi-label classification problems. </span><span class="kobospan" id="kobo.753.2">F1 macro treats each class as equally important, regardless of the class distribution, while F1 micro takes class imbalance into account by considering the number of instances in each class. </span><span class="kobospan" id="kobo.753.3">The choice between F1 macro and F1 micro depends on the specific problem and whether class imbalan</span><a id="_idTextAnchor170" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.754.1">ce is an important factor </span><span><span class="kobospan" id="kobo.755.1">to consider.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.756.1">Confusion matrix</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.757.1">A confusion matrix </span><a id="_idIndexMarker523" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.758.1">serves as a tabular representation, showcasing the count of true positive, true negative, false positive, and false negative predictions made by a classification model. </span><span class="kobospan" id="kobo.758.2">This matrix offers a nuanced perspective on the model’s efficacy, enabling a thorough comprehension of both its strengths </span><span><span class="kobospan" id="kobo.759.1">and weaknesses.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.760.1">For a binary classification problem, the confusion matrix is arranged </span><span><span class="kobospan" id="kobo.761.1">as follows:</span></span></p>
<table class="no-table-style" id="table002-1">
<colgroup class="calibre10">
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
</colgroup>
<thead class="calibre12">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><span><strong class="bold"><span class="kobospan" id="kobo.762.1">Actual/Predicted</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold"><span class="kobospan" id="kobo.763.1">(</span></strong><span><strong class="bold"><span class="kobospan" id="kobo.764.1">Predicted) Positive</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold"><span class="kobospan" id="kobo.765.1">(</span></strong><span><strong class="bold"><span class="kobospan" id="kobo.766.1">Predicted) Negative</span></strong></span></p>
</td>
</tr>
</thead>
<tbody class="calibre13">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><span class="kobospan" id="kobo.767.1">(</span><span><span class="kobospan" id="kobo.768.1">Actual) Positive</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.769.1">True Positive</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.770.1">False Negative</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><span class="kobospan" id="kobo.771.1">(</span><span><span class="kobospan" id="kobo.772.1">Actual) Negative</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.773.1">False Positive</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.774.1">True Negative</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.775.1">Table 5.2 – Confusion matrix – general view</span></p>
<p class="calibre6"><span class="kobospan" id="kobo.776.1">For multi-class classification problems, the confusion matrix is extended to include the true and predicted counts for each class. </span><span class="kobospan" id="kobo.776.2">The diagonal elements represent the correctly classified instances, while the off-diagonal elements </span><span><span class="kobospan" id="kobo.777.1">represent misclassifications.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.778.1">In summary, evaluating text classification models involves using various metrics and techniques, such as accuracy, precision, recall, F1 score, and the confusion matrix. </span><span class="kobospan" id="kobo.778.2">Selecting the appropriate evaluation metrics depends on the specific problem, dataset characteristics, and the trade-offs between false positives and false negatives. </span><span class="kobospan" id="kobo.778.3">Evaluating a model using multiple metrics can provide a more comprehensive understanding of its performance and help guide </span><span><span class="kobospan" id="kobo.779.1">further improvements.</span></span></p>
<h2 id="_idParaDest-104" class="calibre7"><a id="_idTextAnchor171" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.780.1">Overfitting and underfitting</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.781.1">Overfitting and underfitting are two common issues that arise during the training of ML models, including text classification models. </span><span class="kobospan" id="kobo.781.2">They both relate to how well a model generalizes to new, unseen data. </span><span class="kobospan" id="kobo.781.3">This section will explain overfitting and underfitting, when they happen, and how to </span><span><span class="kobospan" id="kobo.782.1">prevent them.</span></span></p>
<h3 class="calibre8"><a id="_idTextAnchor172" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.783.1">Overfitting</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.784.1">Overfitting arises when a model excessively tailors itself to the intricacies of the training data. </span><span class="kobospan" id="kobo.784.2">In this case, the model captures noise and random fluctuations rather than discerning the fundamental patterns. </span><span class="kobospan" id="kobo.784.3">Consequently, although the model may exhibit high performance on the training data, its effectiveness diminishes when applied to unseen data, such as a validation or </span><span><span class="kobospan" id="kobo.785.1">test set.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.786.1">To avoid overfitting in text classification, consider the </span><span><span class="kobospan" id="kobo.787.1">following strategies:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><a id="_idTextAnchor173" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.788.1">Regularization</span></strong><span class="kobospan" id="kobo.789.1">: Introduce regularization techniques, such as </span><span class="kobospan" id="kobo.790.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;script&quot;&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/284.png" class="calibre283"/></span><span class="kobospan" id="kobo.791.1"> or </span><span class="kobospan" id="kobo.792.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;script&quot;&gt;l&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/285.png" class="calibre284"/></span><span class="kobospan" id="kobo.793.1">L2 regularization, which add a penalty to the loss function, discouraging overly </span><span><span class="kobospan" id="kobo.794.1">complex models.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.795.1">Early stopping</span></strong><span class="kobospan" id="kobo.796.1">: In this approach, we monitor the performance of the model on the validation set, and stop the training process as soon as the performance on the validation set starts getting worse, even though the model performance on the training set is getting better. </span><span class="kobospan" id="kobo.796.2">It helps us to </span><span><span class="kobospan" id="kobo.797.1">prevent overfitting.</span></span></li>
<li class="calibre15"><strong class="bold"><a id="_idTextAnchor174" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.798.1">Feature selection</span></strong><span class="kobospan" id="kobo.799.1">: Reduce the number of features used for classification by selecting the most informative features or using dimensionality reduction techniques such as PCA </span><span><span class="kobospan" id="kobo.800.1">or LSA.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.801.1">Ensemble methods</span></strong><span class="kobospan" id="kobo.802.1">: Combine multiple models, such as bagging or boosting, to reduce overfitting by averaging </span><span><span class="kobospan" id="kobo.803.1">their predictions.</span></span></li>
<li class="calibre15"><strong class="bold"><a id="_idTextAnchor175" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.804.1">Cross-validation</span></strong><span class="kobospan" id="kobo.805.1">: Use k-fold cross-validation to get a more reliable estimate of model performance on unseen data and fine-tune model </span><span><span class="kobospan" id="kobo.806.1">hyperparameters accordingly.</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.807.1">Next, we’ll </span><span><span class="kobospan" id="kobo.808.1">cover underfitting.</span></span></p>
<h3 class="calibre8"><a id="_idTextAnchor176" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.809.1">Underfitting</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.810.1">Underfitting happens when a model is too simple and fails to capture the underlying patterns in the data. </span><span class="kobospan" id="kobo.810.2">Consequently, the model performance is low on both training and test data. </span><span class="kobospan" id="kobo.810.3">The model is too simple to represent the complexity of the data and can’t </span><span><span class="kobospan" id="kobo.811.1">generalize well.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.812.1">To avoid underfitting in text classification, consider the </span><span><span class="kobospan" id="kobo.813.1">following strategies:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><a id="_idTextAnchor177" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.814.1">Increase model complexity</span></strong><span class="kobospan" id="kobo.815.1">: Use a more complex model, such as a deeper neural network, to capture more intricate patterns in </span><span><span class="kobospan" id="kobo.816.1">the data.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.817.1">Feature engineering</span></strong><span class="kobospan" id="kobo.818.1">: Create new, informative features that help the model better understand the underlying patterns in the text data, such as adding N-grams or using </span><span><span class="kobospan" id="kobo.819.1">word embeddings.</span></span></li>
<li class="calibre15"><strong class="bold"><a id="_idTextAnchor178" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.820.1">Hyperparameter tuning</span></strong><span class="kobospan" id="kobo.821.1">: Optimize model hyperparameters, such as the learning rate, number of layers, or number of hidden units, to improve the model’s ability to learn from the data. </span><span class="kobospan" id="kobo.821.2">We’ll explain hyperparameter tuning and the different methods to perform this task in the </span><span><span class="kobospan" id="kobo.822.1">next section.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.823.1">Increase training data</span></strong><span class="kobospan" id="kobo.824.1">: If possible, collect more labeled data for training, as more examples can help the model learn the underlying </span><span><span class="kobospan" id="kobo.825.1">patterns better.</span></span></li>
<li class="calibre15"><strong class="bold"><a id="_idTextAnchor179" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.826.1">Reduce regularization</span></strong><span class="kobospan" id="kobo.827.1">: If the model is heavily regularized, consider reducing the regularization strength, allowing the model to become more complex and better fit </span><span><span class="kobospan" id="kobo.828.1">the data.</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.829.1">In summary, overfitting and underfitting are two common issues in text classification that affect a model’s ability to generalize to new data. </span><span class="kobospan" id="kobo.829.2">Avoiding these issues involves balancing model complexity, using appropriate features, tuning hyperparameters, employing regularization, and monitoring model performance on a validation set. </span><span class="kobospan" id="kobo.829.3">By addressing overfitting and underfitting, you can improve the performance and generalizability of your text </span><span><span class="kobospan" id="kobo.830.1">classification models.</span></span></p>
<h2 id="_idParaDest-105" class="calibre7"><a id="_idTextAnchor180" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.831.1">Hyperparameter tuning</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.832.1">An important step in building an effective classification model is hyperparameter tuning. </span><span class="kobospan" id="kobo.832.2">Hyperparameters are the model parameters that are defined before training; they will not change during training. </span><span class="kobospan" id="kobo.832.3">These parameters determine the model architecture and behavior. </span><span class="kobospan" id="kobo.832.4">Some of the hyperparameters that can be used are the learning rate and the number of iterations. </span><span class="kobospan" id="kobo.832.5">They can significantly impact the model’s performance </span><span><span class="kobospan" id="kobo.833.1">and generalizability.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.834.1">The process of hyperparameter tuning in text classification involves the </span><span><span class="kobospan" id="kobo.835.1">following steps:</span></span></p>
<ol class="calibre16">
<li class="calibre15"><strong class="bold"><a id="_idTextAnchor181" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.836.1">Define the hyperparameters and their search space</span></strong><span class="kobospan" id="kobo.837.1">: Identify the hyperparameters you want to optimize and specify the range of possible values for each of them. </span><span class="kobospan" id="kobo.837.2">Common hyperparameters in text classification include the learning rate, number of layers, number of hidden units, dropout rate, regularization strength, and feature extraction parameters such as N-grams or </span><span><span class="kobospan" id="kobo.838.1">vocabulary size.</span></span></li>
<li class="calibre15"><strong class="bold"><a id="_idTextAnchor182" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.839.1">Choose a search strategy</span></strong><span class="kobospan" id="kobo.840.1">: Select a method to explore the hyperparameter search space, such as grid search, random search, or Bayesian optimization. </span><span class="kobospan" id="kobo.840.2">Grid search systematically evaluates all combinations of hyperparameter values, while random search samples random combinations within the search space. </span><span class="kobospan" id="kobo.840.3">Bayesian optimization uses a probabilistic model to guide the search, balancing exploration and exploitation based on the </span><span><span class="kobospan" id="kobo.841.1">model’s predictions.</span></span></li>
<li class="calibre15"><strong class="bold"><a id="_idTextAnchor183" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.842.1">Choose an evaluation metric and method</span></strong><span class="kobospan" id="kobo.843.1">: Select a performance metric that best represents the goals of your text classification task, such as accuracy, precision, recall, F1 score, or area under the ROC curve. </span><span class="kobospan" id="kobo.843.2">Also, choose an evaluation method, such as k-fold cross-validation, to get a reliable estimate of the model’s performance on </span><span><span class="kobospan" id="kobo.844.1">unseen data.</span></span></li>
<li class="calibre15"><strong class="bold"><a id="_idTextAnchor184" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.845.1">Perform the search</span></strong><span class="kobospan" id="kobo.846.1">: For each combination of hyperparameter values, train a model on the training data, and evaluate its performance using the chosen metric and evaluation method. </span><span class="kobospan" id="kobo.846.2">Keep track of the best-performing </span><span><span class="kobospan" id="kobo.847.1">hyperparameter combination.</span></span></li>
<li class="calibre15"><strong class="bold"><a id="_idTextAnchor185" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.848.1">Select the best hyperparameters</span></strong><span class="kobospan" id="kobo.849.1">: After the search is complete, select the hyperparameter combination that yields the best performance on the evaluation metric. </span><span class="kobospan" id="kobo.849.2">Retrain the model using these hyperparameters on the entire </span><span><span class="kobospan" id="kobo.850.1">training set.</span></span></li>
<li class="calibre15"><strong class="bold"><a id="_idTextAnchor186" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.851.1">Evaluate on the test set</span></strong><span class="kobospan" id="kobo.852.1">: Assess the performance of the final model with the optimized hyperparameters on a held-out test set to get an unbiased estimate of </span><span><span class="kobospan" id="kobo.853.1">its generalizability.</span></span></li>
</ol>
<p class="calibre6"><span class="kobospan" id="kobo.854.1">Hyperparameter tuning affects the performance of the model by finding the optimal combination of parameters that results in the best model performance on the chosen evaluation metric. </span><span class="kobospan" id="kobo.854.2">Tuning hyperparameters can help address issues such as overfitting and underfitting, balance model complexity, and improve the model’s ability to generalize to </span><span><span class="kobospan" id="kobo.855.1">new data.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.856.1">Hyperparameter tuning is a crucial process in text classification that involves searching for the optimal combination of model parameters to maximize performance on a chosen evaluation metric. </span><span class="kobospan" id="kobo.856.2">By carefully tuning hyperparameters, you can improve the performance and generalizability of your text </span><span><span class="kobospan" id="kobo.857.1">classification models.</span></span></p>
<h2 id="_idParaDest-106" class="calibre7"><a id="_idTextAnchor187" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.858.1">Additional topics in applied text classification</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.859.1">In the real world, applying text classification involves various practical considerations and challenges that arise from the nature of real-world data and problem requirements. </span><span class="kobospan" id="kobo.859.2">Some common issues include dealing with imbalanced datasets, handling noisy data, and choosing appropriate </span><span><span class="kobospan" id="kobo.860.1">evaluation metrics.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.861.1">Let’s discuss each of these in </span><span><span class="kobospan" id="kobo.862.1">more detail.</span></span></p>
<h3 class="calibre8"><a id="_idTextAnchor188" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.863.1">Dealing with imbalanced datasets</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.864.1">Text classification tasks often encounter imbalanced datasets, wherein certain classes boast a notably higher number of instances compared to others. </span><span class="kobospan" id="kobo.864.2">This imbalance can result in models that are skewed, excelling in predicting the majority class while faltering in accurately classifying the minority class. </span><span class="kobospan" id="kobo.864.3">To handle imbalanced datasets, consider the </span><span><span class="kobospan" id="kobo.865.1">following strategies:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><a id="_idTextAnchor189" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.866.1">Resampling</span></strong><span class="kobospan" id="kobo.867.1">: You can oversample the minority class, undersample the majority class, or use a combination of both to balance the </span><span><span class="kobospan" id="kobo.868.1">class distribution.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.869.1">Weighted loss function</span></strong><span class="kobospan" id="kobo.870.1">: Assign higher weights to the minority class in the loss function, making the model more sensitive to misclassifications in the </span><span><span class="kobospan" id="kobo.871.1">minority class.</span></span></li>
<li class="calibre15"><strong class="bold"><a id="_idTextAnchor190" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.872.1">Ensemble methods</span></strong><span class="kobospan" id="kobo.873.1">: Use ensemble techniques such as bagging or boosting with a focus on the minority class. </span><span class="kobospan" id="kobo.873.2">For example, you can use random under-sampling with bagging or cost-sensitive </span><span><span class="kobospan" id="kobo.874.1">boosting algorithms.</span></span></li>
<li class="calibre15"><strong class="bold"><a id="_idTextAnchor191" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.875.1">Evaluation metrics</span></strong><span class="kobospan" id="kobo.876.1">: Choose evaluation metrics that are less sensitive to class imbalance, such as precision, recall, F1 score, or area under the ROC curve, instead </span><span><span class="kobospan" id="kobo.877.1">of accuracy.</span></span></li>
<li class="calibre15"><strong class="bold"><a id="_idTextAnchor192" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.878.1">Handling noisy data</span></strong><span class="kobospan" id="kobo.879.1">: Real-world text data is often noisy, containing misspellings, grammatical errors, or irrelevant information. </span><span class="kobospan" id="kobo.879.2">Noisy data can negatively impact the performance of text </span><span><span class="kobospan" id="kobo.880.1">classification models.</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.881.1">To handle noisy data, consider the </span><span><span class="kobospan" id="kobo.882.1">following strategies:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><a id="_idTextAnchor193" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.883.1">Preprocessing</span></strong><span class="kobospan" id="kobo.884.1">: Clean the text data by correcting misspellings, removing special characters, expanding contractions, and converting text </span><span><span class="kobospan" id="kobo.885.1">into lowercase</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.886.1">Stopword removal</span></strong><span class="kobospan" id="kobo.887.1">: Remove common words that do not carry much meaning, such as “the,” “is,” “and,” and </span><span><span class="kobospan" id="kobo.888.1">so on</span></span></li>
<li class="calibre15"><strong class="bold"><a id="_idTextAnchor194" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.889.1">Stemming or lemmatization</span></strong><span class="kobospan" id="kobo.890.1">: Reduce words to their root form to minimize the impact of </span><span><span class="kobospan" id="kobo.891.1">morphological variations</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.892.1">Feature selection</span></strong><span class="kobospan" id="kobo.893.1">: Use techniques such as chi-square or mutual information to select the most informative features, reducing the impact of noisy or </span><span><span class="kobospan" id="kobo.894.1">irrelevant features</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.895.1">Whether we’re working on imbalanced data or not, we always need to evaluate our model, and choosing the right metric to evaluate our model is important. </span><span class="kobospan" id="kobo.895.2">Next, we’ll explain how to select the best metric to evaluate </span><span><span class="kobospan" id="kobo.896.1">our model.</span></span></p>
<h3 class="calibre8"><a id="_idTextAnchor195" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.897.1">Choosing appropriate evaluation metrics</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.898.1">Selecting the right evaluation metrics is crucial for measuring the performance of your text classification model and guiding </span><span><span class="kobospan" id="kobo.899.1">model improvements.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.900.1">Consider the following when choosing </span><span><span class="kobospan" id="kobo.901.1">evaluation metrics:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.902.1">Problem requirements</span></strong><span class="kobospan" id="kobo.903.1">: Choose metrics that align with the specific goals of your text classification task, such as minimizing false positives or </span><span><span class="kobospan" id="kobo.904.1">false negatives</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.905.1">Class imbalance</span></strong><span class="kobospan" id="kobo.906.1">: For imbalanced datasets, use metrics that account for class imbalance, such as precision, recall, F1 score, or area under the ROC curve, instead </span><span><span class="kobospan" id="kobo.907.1">of accuracy</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.908.1">Multi-class or multi-label problems</span></strong><span class="kobospan" id="kobo.909.1">: For multi-class or multi-label classification tasks, use metrics such as micro- and macro-averaged F1 scores, which aggregate precision and recall differently based on the </span><span><span class="kobospan" id="kobo.910.1">problem’s requirements</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.911.1">In summary, practical considerations in text classification include dealing with imbalanced datasets, handling noisy data, and choosing appropriate evaluation metrics. </span><span class="kobospan" id="kobo.911.2">Addressing these issues can help improve the performance and generalizability of your text classification models and ensure that they meet the specific requirements of </span><span><span class="kobospan" id="kobo.912.1">your problem.</span></span></p>
<h1 id="_idParaDest-107" class="calibre4"><a id="_idTextAnchor196" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.913.1">Topic modeling – a particular use case of unsupervised text classification</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.914.1">Topic modeling is an unsupervised ML technique that’s used to discover abstract topics or themes within a large collection of documents. </span><span class="kobospan" id="kobo.914.2">It assumes that each document can be represented as a mixture of topics, and each topic is represented as a distribution over words. </span><span class="kobospan" id="kobo.914.3">The goal of topic modeling is to find the underlying topics and their word distributions, as well as the topic proportions for </span><span><span class="kobospan" id="kobo.915.1">each document.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.916.1">There are several topic modeling algorithms, but one of the most popular and widely used is LDA. </span><span class="kobospan" id="kobo.916.2">We will discuss LDA in detail, including its </span><span><span class="kobospan" id="kobo.917.1">mathematical formulation.</span></span></p>
</div>


<div id="_idContainer326" class="calibre2">
<h2 id="_idParaDest-108" class="calibre7"><a id="_idTextAnchor197" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.918.1">LDA</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.919.1">LDA is a generative probabilistic model that assumes the following generative process for </span><span><span class="kobospan" id="kobo.920.1">each document:</span></span></p>
<ol class="calibre16">
<li class="calibre15"><span class="kobospan" id="kobo.921.1">Choose the number of words in </span><span><span class="kobospan" id="kobo.922.1">the document.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.923.1">Choose a topic distribution (</span><em class="italic"><span class="kobospan" id="kobo.924.1">θ</span></em><span class="kobospan" id="kobo.925.1">) for the document from a Dirichlet distribution with </span><span><span class="kobospan" id="kobo.926.1">parameter α.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.927.1">For each word in the document, do </span><span><span class="kobospan" id="kobo.928.1">the following:</span></span><ol class="calibre285"><li class="upper-roman"><span class="kobospan" id="kobo.929.1">Choose a topic (</span><em class="italic"><span class="kobospan" id="kobo.930.1">z</span></em><span class="kobospan" id="kobo.931.1">) from the topic </span><span><span class="kobospan" id="kobo.932.1">distribution (</span></span><span><em class="italic"><span class="kobospan" id="kobo.933.1">θ</span></em></span><span><span class="kobospan" id="kobo.934.1">).</span></span></li><li class="upper-roman"><span class="kobospan" id="kobo.935.1">Choose a word (</span><em class="italic"><span class="kobospan" id="kobo.936.1">w</span></em><span class="kobospan" id="kobo.937.1">) from the word distribution of the chosen topic (</span><em class="italic"><span class="kobospan" id="kobo.938.1">φ</span></em><span class="kobospan" id="kobo.939.1">), which is a distribution over words for that topic, drawn from a Dirichlet distribution with </span><span><span class="kobospan" id="kobo.940.1">parameter </span></span><span><em class="italic"><span class="kobospan" id="kobo.941.1">β</span></em></span><span><span class="kobospan" id="kobo.942.1">.</span></span></li></ol></li>
</ol>
<p class="calibre6"><span class="kobospan" id="kobo.943.1">The generative process is a theoretical model used by LDA to reverse-engineer the original documents from </span><span><span class="kobospan" id="kobo.944.1">presumed topics.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.945.1">LDA aims to find the topic-word distributions (</span><em class="italic"><span class="kobospan" id="kobo.946.1">φ</span></em><span class="kobospan" id="kobo.947.1">) and document-topic distributions (</span><em class="italic"><span class="kobospan" id="kobo.948.1">θ</span></em><span class="kobospan" id="kobo.949.1">) that best explain the </span><span><span class="kobospan" id="kobo.950.1">observed documents.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.951.1">Mathematically, LDA can be described using the </span><span><span class="kobospan" id="kobo.952.1">following notation:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><em class="italic"><span class="kobospan" id="kobo.953.1">M</span></em><span class="kobospan" id="kobo.954.1">: Number </span><span><span class="kobospan" id="kobo.955.1">of documents</span></span></li>
<li class="calibre15"><em class="italic"><span class="kobospan" id="kobo.956.1">N</span></em><span class="kobospan" id="kobo.957.1">: Number of words in </span><span><span class="kobospan" id="kobo.958.1">a document</span></span></li>
<li class="calibre15"><em class="italic"><span class="kobospan" id="kobo.959.1">K</span></em><span class="kobospan" id="kobo.960.1">: Number </span><span><span class="kobospan" id="kobo.961.1">of topics</span></span></li>
<li class="calibre15"><em class="italic"><span class="kobospan" id="kobo.962.1">α</span></em><span class="kobospan" id="kobo.963.1">: Dirichlet before document-topic distribution, it affects the sparsity of topics </span><span><span class="kobospan" id="kobo.964.1">within documents</span></span></li>
<li class="calibre15"><em class="italic"><span class="kobospan" id="kobo.965.1">β</span></em><span class="kobospan" id="kobo.966.1">: Dirichlet before topic-word distribution, it affects the sparsity of words </span><span><span class="kobospan" id="kobo.967.1">within topics</span></span></li>
<li class="calibre15"><em class="italic"><span class="kobospan" id="kobo.968.1">θ</span></em><span class="kobospan" id="kobo.969.1">: Document-topic distributions (M × </span><span><span class="kobospan" id="kobo.970.1">K matrix)</span></span></li>
<li class="calibre15"><em class="italic"><span class="kobospan" id="kobo.971.1">φ</span></em><span class="kobospan" id="kobo.972.1">: Topic-word distributions (K × V matrix, where V is the </span><span><span class="kobospan" id="kobo.973.1">vocabulary size)</span></span></li>
<li class="calibre15"><em class="italic"><span class="kobospan" id="kobo.974.1">z</span></em><span class="kobospan" id="kobo.975.1">: Topic assignments for each word in each document (M × </span><span><span class="kobospan" id="kobo.976.1">N matrix)</span></span></li>
<li class="calibre15"><em class="italic"><span class="kobospan" id="kobo.977.1">w</span></em><span class="kobospan" id="kobo.978.1">: Observed words in the documents (M × </span><span><span class="kobospan" id="kobo.979.1">N matrix)</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.980.1">The joint probability of the topic assignments (</span><em class="italic"><span class="kobospan" id="kobo.981.1">z</span></em><span class="kobospan" id="kobo.982.1">) and words (</span><em class="italic"><span class="kobospan" id="kobo.983.1">w</span></em><span class="kobospan" id="kobo.984.1">) in the documents, given the topic-word distributions (</span><em class="italic"><span class="kobospan" id="kobo.985.1">φ</span></em><span class="kobospan" id="kobo.986.1">) and document-topic distributions (</span><em class="italic"><span class="kobospan" id="kobo.987.1">θ</span></em><span class="kobospan" id="kobo.988.1">), can be written </span><span><span class="kobospan" id="kobo.989.1">as follows:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.990.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;φ&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;∏&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;/munderover&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;∏&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;/munderover&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi&gt;φ&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/286.png" class="calibre286"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.991.1">The objective of LDA is to maximize the likelihood of the observed words given the Dirichlet priors α </span><span><span class="kobospan" id="kobo.992.1">and β:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.993.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;α&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∫&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∫&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;φ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;α&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;φ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;φ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" src="image/287.png" class="calibre287"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.994.1">However, computing the likelihood directly is intractable due to the integration over the latent variables θ and φ. </span><span class="kobospan" id="kobo.994.2">Therefore, LDA uses approximate inference algorithms, such as Gibbs sampling or variational inference, to estimate the posterior distributions </span><em class="italic"><span class="kobospan" id="kobo.995.1">P</span></em><span class="kobospan" id="kobo.996.1">(θ | w, α, β) and </span><em class="italic"><span class="kobospan" id="kobo.997.1">P</span></em><span class="kobospan" id="kobo.998.1">(φ | w, </span><span><span class="kobospan" id="kobo.999.1">α, β).</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1000.1">Once the posterior distributions have been estimated, we can obtain the document-topic distributions (θ) and topic-word distributions (φ), which can be used to analyze the discovered topics and their word distributions, as well as the topic proportions for </span><span><span class="kobospan" id="kobo.1001.1">each document.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1002.1">Let’s consider a simple example of </span><span><span class="kobospan" id="kobo.1003.1">topic modeling.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1004.1">Suppose we have a collection of </span><span><span class="kobospan" id="kobo.1005.1">three documents:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.1006.1">Document 1</span></strong><span class="kobospan" id="kobo.1007.1">: “I love playing football with </span><span><span class="kobospan" id="kobo.1008.1">my friends.”</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.1009.1">Document 2</span></strong><span class="kobospan" id="kobo.1010.1">: “The football match was intense </span><span><span class="kobospan" id="kobo.1011.1">and exciting.”</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.1012.1">Document 3</span></strong><span class="kobospan" id="kobo.1013.1">: “My new laptop has an amazing battery life </span><span><span class="kobospan" id="kobo.1014.1">and performance.”</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.1015.1">We want to discover two topics (K = 2) in this document collection. </span><span class="kobospan" id="kobo.1015.2">Here are the steps that we need </span><span><span class="kobospan" id="kobo.1016.1">to perform:</span></span></p>
<ol class="calibre16">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.1017.1">Preprocessing</span></strong><span class="kobospan" id="kobo.1018.1">: First, we need to preprocess the text data, which typically involves tokenization, stopword removal, and stemming/lemmatization (which was explained previously in this chapter). </span><span class="kobospan" id="kobo.1018.2">In this example, we will skip these steps for simplicity and assume our documents are </span><span><span class="kobospan" id="kobo.1019.1">already preprocessed.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.1020.1">Initialization</span></strong><span class="kobospan" id="kobo.1021.1">: Choose the initial values for the Dirichlet priors, α and β. </span><span class="kobospan" id="kobo.1021.2">For example, we can set α = [1, 1] and β = [0.1, 0.1, ..., 0.1] (assuming a V-dimensional vector with 0.1 for each word in </span><span><span class="kobospan" id="kobo.1022.1">the vocabulary).</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.1023.1">Random topic assignments</span></strong><span class="kobospan" id="kobo.1024.1">: Randomly assign a topic (1 or 2) to each word in </span><span><span class="kobospan" id="kobo.1025.1">each document.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.1026.1">Iterative inference (for example, Gibbs sampling or variational inference)</span></strong><span class="kobospan" id="kobo.1027.1">: Iteratively update the topic assignments and the topic-word and document-topic distributions (φ and θ) until convergence or a fixed number of iterations. </span><span class="kobospan" id="kobo.1027.2">This process refines the assignments and distributions, ultimately revealing the underlying </span><span><span class="kobospan" id="kobo.1028.1">topic structure.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.1029.1">Interpretation</span></strong><span class="kobospan" id="kobo.1030.1">: After the algorithm converges or reaches the maximum number of iterations, we can interpret the discovered topics by looking at the most probable words for each topic and the most probable topics for </span><span><span class="kobospan" id="kobo.1031.1">each document.</span></span><p class="calibre6"><span class="kobospan" id="kobo.1032.1">For our example, LDA might discover the </span><span><span class="kobospan" id="kobo.1033.1">following topics:</span></span></p><ul class="calibre17"><li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.1034.1">Topic 1</span></strong><span class="kobospan" id="kobo.1035.1">: {“football”, “playing”, “friends”, “match”, “</span><span><span class="kobospan" id="kobo.1036.1">intense”, “exciting”}</span></span></li><li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.1037.1">Topic 2</span></strong><span class="kobospan" id="kobo.1038.1">: {“laptop”, “battery”, “</span><span><span class="kobospan" id="kobo.1039.1">life”, “performance”}</span></span></li></ul><p class="calibre6"><span class="kobospan" id="kobo.1040.1">With these topics, the document-topic distribution (θ) might look </span><span><span class="kobospan" id="kobo.1041.1">like this:</span></span></p><ul class="calibre17"><li class="calibre15"><span class="kobospan" id="kobo.1042.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/288.png" class="calibre288"/></span><span class="kobospan" id="kobo.1043.1"> = [0.9, 0.1] (Document 1 is 90% about Topic 1 and 10% about </span><span><span class="kobospan" id="kobo.1044.1">Topic 2)</span></span></li><li class="calibre15"><span class="kobospan" id="kobo.1045.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/289.png" class="calibre288"/></span><span class="kobospan" id="kobo.1046.1"> = [0.8, 0.2] (Document 2 is 80% about Topic 1 and 20% about </span><span><span class="kobospan" id="kobo.1047.1">Topic 2)</span></span></li><li class="calibre15"><span class="kobospan" id="kobo.1048.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/290.png" class="calibre289"/></span><span class="kobospan" id="kobo.1049.1"> = [0.1, 0.9] (Document 3 is 10% about Topic 1 and 90% about </span><span><span class="kobospan" id="kobo.1050.1">Topic 2)</span></span></li></ul></li>
</ol>
<p class="calibre6"><span class="kobospan" id="kobo.1051.1">In this example, topic 1 seems to be related to football and sports, while topic 2 seems to be related to technology and gadgets. </span><span class="kobospan" id="kobo.1051.2">The topic distributions for each document show that documents 1 and 2 are mostly about football, while document 3 is </span><span><span class="kobospan" id="kobo.1052.1">about technology.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1053.1">Please note that this is a simplified example, and real-world data would require more sophisticated preprocessing and a larger number of iterations </span><span><span class="kobospan" id="kobo.1054.1">for convergence.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1055.1">We are now ready to discuss the paradigm for putting together a complete project in a work or </span><span><span class="kobospan" id="kobo.1056.1">research setting.</span></span></p>
<h2 id="_idParaDest-109" class="calibre7"><a id="_idTextAnchor198" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1057.1">Real-world ML system design for NLP text classification</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.1058.1">This section is dedicated to the practical implementation of the various methods we discussed. </span><span class="kobospan" id="kobo.1058.2">It will revolve around Python code, which serves as a </span><span><span class="kobospan" id="kobo.1059.1">complete pipeline.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1060.1">To provide a comprehensive learning experience, we will discuss the entire journey of a typical ML project. </span><span><em class="italic"><span class="kobospan" id="kobo.1061.1">Figure 5</span></em></span><em class="italic"><span class="kobospan" id="kobo.1062.1">.1</span></em><span class="kobospan" id="kobo.1063.1"> depicts the different phases of the </span><span><span class="kobospan" id="kobo.1064.1">ML project:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer318">
<span class="kobospan" id="kobo.1065.1"><img alt="Figure 5.1 – The paradigm of a typical ML project" src="image/B18949_05_1.jpg" class="calibre3"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.1066.1">Figure 5.1 – The paradigm of a typical ML project</span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1067.1">Let’s break the problem down in a similar fashion to a typical project in </span><span><span class="kobospan" id="kobo.1068.1">the industry.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.1069.1">The business objective</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.1070.1">An ML project, whether in a business or research setting, stems from an original objective, which is often qualitative rather </span><span><span class="kobospan" id="kobo.1071.1">than technical.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1072.1">Here’s </span><span><span class="kobospan" id="kobo.1073.1">an example:</span></span></p>
<ol class="calibre16">
<li class="calibre15"><span class="kobospan" id="kobo.1074.1">“We need to know which of our patients is at a </span><span><span class="kobospan" id="kobo.1075.1">higher risk.”</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1076.1">“We would like to maximize the engagement of </span><span><span class="kobospan" id="kobo.1077.1">our ad.”</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1078.1">“We need the autonomous car to be alerted when a person is stepping in front </span><span><span class="kobospan" id="kobo.1079.1">of it.”</span></span></li>
</ol>
<p class="calibre6"><span class="kobospan" id="kobo.1080.1">Next comes the </span><span><span class="kobospan" id="kobo.1081.1">technical objective.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.1082.1">The technical objective</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.1083.1">The original objective needs to be translated into a technical objective, </span><span><span class="kobospan" id="kobo.1084.1">like so:</span></span></p>
<ol class="calibre16">
<li class="calibre15"><span class="kobospan" id="kobo.1085.1">“We will process every patient’s medical record and build a risk estimator based on the history of </span><span><span class="kobospan" id="kobo.1086.1">realized risk.”</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1087.1">“We will collect data about all the ads from the last year and will build a regressor to estimate the level of engagement based on the </span><span><span class="kobospan" id="kobo.1088.1">ad’s features.”</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1089.1">“We will collect a set of images taken by the car’s front camera and present those to our online users who are visiting our site, telling them it’s for security reasons and that they need to click on the parts that show a human to prove they are not robots. </span><span class="kobospan" id="kobo.1089.2">However, in practice, we’ll collect their free labels for training, develop a computer vision classifier for humans, and won’t give them </span><span><span class="kobospan" id="kobo.1090.1">any credit.”</span></span></li>
</ol>
<p class="calibre6"><span class="kobospan" id="kobo.1091.1">While the original business or research objective is somewhat of an open-ended question, the technical objective reflects an actionable plan. </span><span class="kobospan" id="kobo.1091.2">Note, however, that any given technical objective represents just one among several potential solutions aligned with the original business or research aim. </span><span class="kobospan" id="kobo.1091.3">It is the responsibility of the technical authority, such as the CTO, ML manager, or senior developer, to understand the original objective and translate it into a technical objective. </span><span class="kobospan" id="kobo.1091.4">Moreover, it may be that the technical objective would be refined or even replaced down the line. </span><span class="kobospan" id="kobo.1091.5">The next step after forming a technical objective is to form a plan </span><span><span class="kobospan" id="kobo.1092.1">for it.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.1093.1">Tentative high-level system design</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.1094.1">To realize the technical objective, we need to derive a plan to decide which data would be used to feed into the ML system, and what the expected output of the ML system is. </span><span class="kobospan" id="kobo.1094.2">In the first steps of a project, there may be several candidate sources of potential data that are believed to be indicative of the </span><span><span class="kobospan" id="kobo.1095.1">desired output.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1096.1">Following the set of three examples mentioned previously, here are some examples of </span><span><span class="kobospan" id="kobo.1097.1">data descriptions:</span></span></p>
<ol class="calibre16">
<li class="calibre15"><span class="kobospan" id="kobo.1098.1">The input data would be columns A, B, and C of the </span><strong class="source-inline1"><span class="kobospan" id="kobo.1099.1">patient_records</span></strong><span class="kobospan" id="kobo.1100.1"> SQL table and the risk would be assessed as </span><em class="italic"><span class="kobospan" id="kobo.1101.1">1/N</span></em><span class="kobospan" id="kobo.1102.1">, where </span><em class="italic"><span class="kobospan" id="kobo.1103.1">N</span></em><span class="kobospan" id="kobo.1104.1"> is the number of days that passed from a given moment until the patient showed up in the </span><span><span class="kobospan" id="kobo.1105.1">emergency room.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1106.1">The input data would be the geometric and color descriptions of the ads, and the level of engagement would be the number of clicks per day that the </span><span><span class="kobospan" id="kobo.1107.1">ad received.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1108.1">The input data is the images of the car’s front camera to be fed to a computer vision neural network classifier, and the output data would be whether the image captures a person </span><span><span class="kobospan" id="kobo.1109.1">or not.</span></span></li>
</ol>
<h3 class="calibre8"><span class="kobospan" id="kobo.1110.1">Choosing a metric</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.1111.1">When defining a potential solution approach, extra attention should be dedicated to identifying the best metric to focus on, also known as the objective function or error function. </span><span class="kobospan" id="kobo.1111.2">This is the metric by which the success of the solution will be evaluated. </span><span class="kobospan" id="kobo.1111.3">It is important to relate the metric to the original business or </span><span><span class="kobospan" id="kobo.1112.1">research objective.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1113.1">As per the previous examples, we could have </span><span><span class="kobospan" id="kobo.1114.1">the following:</span></span></p>
<ol class="calibre16">
<li class="calibre15"><span class="kobospan" id="kobo.1115.1">Minimize the 70</span><span class="superscript"><span class="kobospan1" id="kobo.1116.1">th</span></span><span class="kobospan" id="kobo.1117.1"> percentile </span><span><span class="kobospan" id="kobo.1118.1">confidence interval.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1119.1">Minimize the mean </span><span><span class="kobospan" id="kobo.1120.1">absolute error.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1121.1">Maximize precision while constraining on a fixed recall. </span><span class="kobospan" id="kobo.1121.2">This fixed recall will ideally be dictated by business leaders or the legal team, in the form of “the system must capture at least 99.9% of the cases where a person steps in front of </span><span><span class="kobospan" id="kobo.1122.1">a car.”</span></span></li>
</ol>
<p class="calibre6"><span class="kobospan" id="kobo.1123.1">Now that we have a tentative plan, we can explore the data and evaluate the feasibility of </span><span><span class="kobospan" id="kobo.1124.1">the design.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.1125.1">Exploration</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.1126.1">Exploration is divided into two parts – exploring the data and exploring the feasibility of the design. </span><span class="kobospan" id="kobo.1126.2">Let’s take a </span><span><span class="kobospan" id="kobo.1127.1">closer look.</span></span></p>
<h4 class="calibre135"><span class="kobospan" id="kobo.1128.1">Data exploration</span></h4>
<p class="calibre6"><span class="kobospan" id="kobo.1129.1">Data is not always perfect for our objective. </span><span class="kobospan" id="kobo.1129.2">We discussed some of the data shortcomings in previous chapters. </span><span class="kobospan" id="kobo.1129.3">In particular, free text is often notorious for having many abnormal phenomena, such as encodings, special characters, typos, and so on. </span><span class="kobospan" id="kobo.1129.4">When exploring our data, we want to uncover all these phenomena and make sure that the data can be brought to a form that serves </span><span><span class="kobospan" id="kobo.1130.1">the objective.</span></span></p>
<h4 class="calibre135"><span class="kobospan" id="kobo.1131.1">Feasibility study</span></h4>
<p class="calibre6"><span class="kobospan" id="kobo.1132.1">Here, we want to prospectively identify proxies for whether the planned design is expected to succeed. </span><span class="kobospan" id="kobo.1132.2">While with some problems there are known proxies for expected success, in most problems in the business and especially research setting, it takes much experience and ingenuity to suggest preliminary proxies </span><span><span class="kobospan" id="kobo.1133.1">for success.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1134.1">An example of a very simple case is a simple regression problem with a single input variable and a single output variable. </span><span class="kobospan" id="kobo.1134.2">Let’s say the independent variable is the number of active viewers that your streaming service currently has, and the dependent variable is the risk that the company’s servers have for maxing out their capacity. </span><span class="kobospan" id="kobo.1134.3">The tentative design plan would be to build a regressor that estimates the risk at any given moment. </span><span class="kobospan" id="kobo.1134.4">A strong proxy for the feasibility of developing a successful regressor could be calculating the linear correlation between the historical data points. </span><span class="kobospan" id="kobo.1134.5">Calculating linear correlation based on sample data is easy and quick and if its result is close to 1 (or -1 in cases different than our business problem), then it means that a linear regressor is guaranteed to succeed, thus, making it a great proxy. </span><span class="kobospan" id="kobo.1134.6">However, note that if the linear correlation is close to 0, it doesn’t necessarily mean that a regressor would fail, only that a linear regression would fail. </span><span class="kobospan" id="kobo.1134.7">In such a case, a different proxy should be </span><span><span class="kobospan" id="kobo.1135.1">deferred to.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1136.1">In the </span><em class="italic"><span class="kobospan" id="kobo.1137.1">Reviewing our use case – ML system design for NLP classification in a Jupyter Notebook</span></em><span class="kobospan" id="kobo.1138.1"> section, we’ll review our code solution. </span><span class="kobospan" id="kobo.1138.2">We’ll also present a method to assess the feasibility of a text classifier. </span><span class="kobospan" id="kobo.1138.3">The method aims to mimic a relationship between the input text to the output class. </span><span class="kobospan" id="kobo.1138.4">But since we want to have that method suit a variable that is text and not numeric, we’ll go back to the origin and calculate a measure for the statistical dependency between the input text and the output class. </span><span class="kobospan" id="kobo.1138.5">Statistical dependency is the most basic measure for a relationship between variables and thus doesn’t require either of them to </span><span><span class="kobospan" id="kobo.1139.1">be numeric.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1140.1">Assuming the </span><strong class="bold"><span class="kobospan" id="kobo.1141.1">feasibility study</span></strong><span class="kobospan" id="kobo.1142.1"> is successful, we can move on to implementing the </span><span><span class="kobospan" id="kobo.1143.1">ML solution.</span></span></p>
<h2 id="_idParaDest-110" class="calibre7"><a id="_idTextAnchor199" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1144.1">Implementing an ML solution</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.1145.1">This part is where the expertise of the ML developer comes into play. </span><span class="kobospan" id="kobo.1145.2">There are different steps for it and the developer chooses which ones are relevant based on the problem – whether it’s data cleaning, text segmentation, feature design, model comparison, or </span><span><span class="kobospan" id="kobo.1146.1">metric choice.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1147.1">We will elaborate on this as we review the specific use case </span><span><span class="kobospan" id="kobo.1148.1">we’ve solved.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.1149.1">Evaluating the results</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.1150.1">We evaluate the solution given the metric that was chosen. </span><span class="kobospan" id="kobo.1150.2">This part requires some experience as ML developers tend to get better at this over time. </span><span class="kobospan" id="kobo.1150.3">The main pitfall in this task is the ability to set up an objective assessment of the result. </span><span class="kobospan" id="kobo.1150.4">That objective assessment is done by applying the finished model to data it had never “seen” before. </span><span class="kobospan" id="kobo.1150.5">But often folks who are only starting to apply ML find themselves improving their design after seeing what the results of that held-out set are. </span><span class="kobospan" id="kobo.1150.6">This leads to a feedback loop where the design is practically fitted to the no-longer-held-out set. </span><span class="kobospan" id="kobo.1150.7">While this may indeed improve the model and the design, it takes away from the ability to provide an objective forecast of how the model would perform when implemented in the real world. </span><span class="kobospan" id="kobo.1150.8">In the real world, it would see data that is truly held out and that it wasn’t </span><span><span class="kobospan" id="kobo.1151.1">fitted to.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.1152.1">Done and delivered</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.1153.1">Typically, when the design is done, the implementation is complete, and the results have been found satisfactory, the work is presented for business implementation, or in the research setting, for publication. </span><span class="kobospan" id="kobo.1153.2">In the business setting, implementation can take on </span><span><span class="kobospan" id="kobo.1154.1">different forms.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1155.1">One of the simplest forms is where the output is used to provide business insights. </span><span class="kobospan" id="kobo.1155.2">Its purpose is to be presented. </span><span class="kobospan" id="kobo.1155.3">For instance, when looking to evaluate how much a marketing campaign was contributing to the growth in sales, the ML team may calculate an estimation for that measure of contribution and present it </span><span><span class="kobospan" id="kobo.1156.1">to leadership.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1157.1">Another form of implementation is within a dashboard in real time. </span><span class="kobospan" id="kobo.1157.2">For instance, the model calculates the predicted risk of patients coming to the emergency room, and it does so on a daily cadence. </span><span class="kobospan" id="kobo.1157.3">The results are aggregated and a graph is presented on the hospital dashboard to show the expected number of people who would come to the emergency room for every day of the next </span><span><span class="kobospan" id="kobo.1158.1">30 days.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1159.1">A more advanced and common form is when the output of the data is directed so that it can be fed into downstream tasks. </span><span class="kobospan" id="kobo.1159.2">The model would then be implemented in production to become a microservice within a larger production pipeline. </span><span class="kobospan" id="kobo.1159.3">An example of that is when a classifier evaluates every post on your company’s Facebook page. </span><span class="kobospan" id="kobo.1159.4">When it identifies offensive language, it outputs a detection that then passes down the pipeline to another system that removes that post and perhaps blocks </span><span><span class="kobospan" id="kobo.1160.1">that user.</span></span></p>
<h4 class="calibre135"><span class="kobospan" id="kobo.1161.1">Code design</span></h4>
<p class="calibre6"><span class="kobospan" id="kobo.1162.1">The code’s design should suit the purpose of the code once the work is complete. </span><span class="kobospan" id="kobo.1162.2">As per the different forms of implementation mentioned previously, some implementations dictate a specific code structure. </span><span class="kobospan" id="kobo.1162.3">For instance, when the completed code is handed off to production within a larger, already existing pipeline, it is the production engineer who would dictate the constraints to the ML team. </span><span class="kobospan" id="kobo.1162.4">These constraints may be around computation and timing resources, but they would also be around code design. </span><span class="kobospan" id="kobo.1162.5">Often, basic code files, such as </span><strong class="source-inline"><span class="kobospan" id="kobo.1163.1">.py</span></strong><span class="kobospan" id="kobo.1164.1"> files, </span><span><span class="kobospan" id="kobo.1165.1">are necessary.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1166.1">As with cases where the code is used for presentations, such as in the example of presenting how contributive the marketing campaign was, Jupyter Notebooks may be the </span><span><span class="kobospan" id="kobo.1167.1">better choice.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1168.1">Jupyter Notebooks can be very informative and instructional. </span><span class="kobospan" id="kobo.1168.2">For that reason, many ML developers start their projects with Jupyter Notebooks for the </span><span><span class="kobospan" id="kobo.1169.1">exploration phase.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1170.1">Next, we will review our design in a Jupyter Notebook. </span><span class="kobospan" id="kobo.1170.2">This will allow us to encapsulate the entire process in a single coherent file that is meant to be presented to </span><span><span class="kobospan" id="kobo.1171.1">the reader.</span></span></p>
<h1 id="_idParaDest-111" class="calibre4"><a id="_idTextAnchor200" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1172.1">Reviewing our use case – ML system design for NLP classification in a Jupyter Notebook</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.1173.1">In this section, we will walk through a hands-on example. </span><span class="kobospan" id="kobo.1173.2">We will follow the steps we presented previously for articulating the problem, designing the solution, and evaluating the results. </span><span class="kobospan" id="kobo.1173.3">This section portrays the process that an ML developer goes through when working on a typical project in the industry. </span><span class="kobospan" id="kobo.1173.4">Refer to the notebook at </span><a href="https://colab.research.google.com/drive/1ZG4xN665le7X_HPcs52XSFbcd1OVaI9R?usp=sharing" class="calibre5 pcalibre1 pcalibre"><span class="kobospan" id="kobo.1174.1">https://colab.research.google.com/drive/1ZG4xN665le7X_HPcs52XSFbcd1OVaI9R?usp=sharing</span></a><span class="kobospan" id="kobo.1175.1"> for </span><span><span class="kobospan" id="kobo.1176.1">more information.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.1177.1">The business objective</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.1178.1">In this scenario, we are working for a financial news agency. </span><span class="kobospan" id="kobo.1178.2">Our objective is to publish news about companies and products in </span><span><span class="kobospan" id="kobo.1179.1">real time.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.1180.1">The technical objective</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.1181.1">The CTO derives several technical objectives from the business objective. </span><span class="kobospan" id="kobo.1181.2">One objective is for the ML team: given a stream of financial tweets in real time, detect those tweets that discuss information about companies </span><span><span class="kobospan" id="kobo.1182.1">or products.</span></span></p>
<h2 id="_idParaDest-112" class="calibre7"><a id="_idTextAnchor201" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1183.1">The pipeline</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.1184.1">Let’s review the different parts of the pipeline, as shown in </span><span><em class="italic"><span class="kobospan" id="kobo.1185.1">Figure 5</span></em></span><span><em class="italic"><span class="kobospan" id="kobo.1186.1">.2</span></em></span><span><span class="kobospan" id="kobo.1187.1">:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer319">
<span class="kobospan" id="kobo.1188.1"><img alt="Figure 5.2 – The structure of a typical ML pipeline" src="image/B18949_05_2.jpg" class="calibre3"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.1189.1">Figure 5.2 – The structure of a typical ML pipeline</span></p>
<p class="callout-heading"><span class="kobospan" id="kobo.1190.1">Note</span></p>
<p class="callout"><span class="kobospan" id="kobo.1191.1">The phases of the pipeline in </span><span><em class="italic"><span class="kobospan" id="kobo.1192.1">Figure 5</span></em></span><em class="italic"><span class="kobospan" id="kobo.1193.1">.2</span></em><span class="kobospan" id="kobo.1194.1"> are explored in the </span><span><span class="kobospan" id="kobo.1195.1">following subsections</span></span></p>
<h2 id="_idParaDest-113" class="calibre7"><a id="_idTextAnchor202" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1196.1">Code settings</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.1197.1">In this part of the code, we set the key parameters. </span><span class="kobospan" id="kobo.1197.2">We choose to have them as a part of the code as this is instructional code made for presentation. </span><span class="kobospan" id="kobo.1197.3">In cases where the code is expected to go to production, it may be better to host the parameters in a separate </span><strong class="source-inline"><span class="kobospan" id="kobo.1198.1">.yaml</span></strong><span class="kobospan" id="kobo.1199.1"> file. </span><span class="kobospan" id="kobo.1199.2">That would also suit heavy iterations during the development phase as it will allow you to iterate over different code parameters without having to change the code, which is </span><span><span class="kobospan" id="kobo.1200.1">often desirable.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1201.1">As for the choice of these values, it should be stressed that some of these values should be optimized to suit the optimization of the solution. </span><span class="kobospan" id="kobo.1201.2">We have chosen fixed measures here to simplify the process. </span><span class="kobospan" id="kobo.1201.3">For instance, the number of features to be used for classification is a fixed quantity here, but it should also be optimized to fit the </span><span><span class="kobospan" id="kobo.1202.1">training set.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.1203.1">Gathering the data</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.1204.1">This part loads the dataset. </span><span class="kobospan" id="kobo.1204.2">In our case, the loading function is simple. </span><span class="kobospan" id="kobo.1204.3">In other business cases, this part could be quite large as it may include a collection of SQL queries that are called. </span><span class="kobospan" id="kobo.1204.4">In such a case, it may be ideal to write a dedicated function in a separate </span><strong class="source-inline"><span class="kobospan" id="kobo.1205.1">.py</span></strong><span class="kobospan" id="kobo.1206.1"> file and source it via the </span><span><span class="kobospan" id="kobo.1207.1">imports section.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.1208.1">Processing the data</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.1209.1">Here, we format the data in a way that suits our work. </span><span class="kobospan" id="kobo.1209.2">We also observe some of it for the first time. </span><span class="kobospan" id="kobo.1209.3">This allows us to get a feel of its nature </span><span><span class="kobospan" id="kobo.1210.1">and quality.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1211.1">One key action we take here is to define the classes we </span><span><span class="kobospan" id="kobo.1212.1">care about.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.1213.1">Preprocessing</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.1214.1">As we discussed in </span><a href="B18949_04.xhtml#_idTextAnchor113" class="calibre5 pcalibre1 pcalibre"><span><em class="italic"><span class="kobospan" id="kobo.1215.1">Chapter 4</span></em></span></a><span class="kobospan" id="kobo.1216.1">, preprocessing is a key part of the pipeline. </span><span class="kobospan" id="kobo.1216.2">For instance, we notice that many of the tweets have a URL, which we choose </span><span><span class="kobospan" id="kobo.1217.1">to remove.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.1218.1">Preliminary data exploration</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.1219.1">At this point, we have observed the quality of the text and the distribution of the classes. </span><span class="kobospan" id="kobo.1219.2">This is where we explore any other characteristics of the data that may imply either its quality or its ability to indicate the </span><span><span class="kobospan" id="kobo.1220.1">desired class.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.1221.1">Feature engineering</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.1222.1">Next, we start processing the text. </span><span class="kobospan" id="kobo.1222.2">We seek to represent the text of each observation as a set of numerical features. </span><span class="kobospan" id="kobo.1222.3">The main reason for this is that traditional ML models are designed to accept numbers as input, not text. </span><span class="kobospan" id="kobo.1222.4">For instance, a common linear regression or logistic regression model is applied to numbers, not words, categories, or image pixels. </span><span class="kobospan" id="kobo.1222.5">Thus, we need to suggest a numeric representation for the text. </span><span class="kobospan" id="kobo.1222.6">This design constraint is lifted when working with </span><strong class="bold"><span class="kobospan" id="kobo.1223.1">language models</span></strong><span class="kobospan" id="kobo.1224.1"> such as </span><strong class="bold"><span class="kobospan" id="kobo.1225.1">BERT</span></strong><span class="kobospan" id="kobo.1226.1"> and </span><strong class="bold"><span class="kobospan" id="kobo.1227.1">GPT</span></strong><span class="kobospan" id="kobo.1228.1">. </span><span class="kobospan" id="kobo.1228.2">We will see this in the </span><span><span class="kobospan" id="kobo.1229.1">coming chapters.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1230.1">We partition the text into N-grams, where </span><em class="italic"><span class="kobospan" id="kobo.1231.1">N</span></em><span class="kobospan" id="kobo.1232.1"> is a parameter of the code. </span><em class="italic"><span class="kobospan" id="kobo.1233.1">N</span></em><span class="kobospan" id="kobo.1234.1"> is fixed in this code but should be optimized to best fit the </span><span><span class="kobospan" id="kobo.1235.1">training set.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1236.1">Once the text has been partitioned into N-grams, they are modeled as numeric values. </span><span class="kobospan" id="kobo.1236.2">When a binary (that is, </span><strong class="bold"><span class="kobospan" id="kobo.1237.1">one-hot encoding</span></strong><span class="kobospan" id="kobo.1238.1">) method is chosen, the numerical feature that represents some N-gram gets a “1” when the observed text includes that N-gram, and “0” otherwise. </span><span class="kobospan" id="kobo.1238.2">See </span><span><em class="italic"><span class="kobospan" id="kobo.1239.1">Figure 5</span></em></span><em class="italic"><span class="kobospan" id="kobo.1240.1">.3</span></em><span class="kobospan" id="kobo.1241.1"> for an example. </span><span class="kobospan" id="kobo.1241.2">If a BOW approach is chosen, then the value of the feature is the number of times the N-gram appears in the observed text. </span><span class="kobospan" id="kobo.1241.3">Another common feature engineering method that isn’t implemented here </span><span><span class="kobospan" id="kobo.1242.1">is </span></span><span><strong class="bold"><span class="kobospan" id="kobo.1243.1">TF-IDF</span></strong></span><span><span class="kobospan" id="kobo.1244.1">.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1245.1">Here’s what we get by using </span><span><span class="kobospan" id="kobo.1246.1">unigrams only:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1247.1">Input sentence: “</span><span><span class="kobospan" id="kobo.1248.1">filing submitted.”</span></span></p>
<table class="no-table-style" id="table003">
<colgroup class="calibre10">
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
</colgroup>
<thead class="calibre12">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><span><strong class="bold"><span class="kobospan" id="kobo.1249.1">N-gram</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold"><span class="kobospan" id="kobo.1250.1">“</span></strong><span><strong class="bold"><span class="kobospan" id="kobo.1251.1">report”</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold"><span class="kobospan" id="kobo.1252.1">“</span></strong><span><strong class="bold"><span class="kobospan" id="kobo.1253.1">filing”</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold"><span class="kobospan" id="kobo.1254.1">“</span></strong><span><strong class="bold"><span class="kobospan" id="kobo.1255.1">submitted”</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold"><span class="kobospan" id="kobo.1256.1">“</span></strong><span><strong class="bold"><span class="kobospan" id="kobo.1257.1">product”</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold"><span class="kobospan" id="kobo.1258.1">“</span></strong><span><strong class="bold"><span class="kobospan" id="kobo.1259.1">quarterly”</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold"><span class="kobospan" id="kobo.1260.1">The rest of </span></strong><span><strong class="bold"><span class="kobospan" id="kobo.1261.1">the unigrams</span></strong></span></p>
</td>
</tr>
</thead>
<tbody class="calibre13">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><span class="kobospan" id="kobo.1262.1">Feature </span><span><span class="kobospan" id="kobo.1263.1">value</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span class="kobospan" id="kobo.1264.1">0</span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span class="kobospan" id="kobo.1265.1">1</span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span class="kobospan" id="kobo.1266.1">1</span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span class="kobospan" id="kobo.1267.1">0</span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span class="kobospan" id="kobo.1268.1">0</span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span class="kobospan" id="kobo.1269.1">(</span><span><span class="kobospan" id="kobo.1270.1">0’s)</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.1271.1">Figure 5.3 – Transforming an input text sentence into a numerical representation by partitioning to unigrams via one-hot encoding</span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1272.1">The following figure shows what we get by using both unigrams </span><span><span class="kobospan" id="kobo.1273.1">and bigrams:</span></span></p>
<table class="no-table-style" id="table004">
<colgroup class="calibre10">
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
</colgroup>
<thead class="calibre12">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><span><strong class="bold"><span class="kobospan" id="kobo.1274.1">N-gram</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold"><span class="kobospan" id="kobo.1275.1">“</span></strong><span><strong class="bold"><span class="kobospan" id="kobo.1276.1">report”</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold"><span class="kobospan" id="kobo.1277.1">“</span></strong><span><strong class="bold"><span class="kobospan" id="kobo.1278.1">filing”</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold"><span class="kobospan" id="kobo.1279.1">“</span></strong><span><strong class="bold"><span class="kobospan" id="kobo.1280.1">filing submitted”</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold"><span class="kobospan" id="kobo.1281.1">“</span></strong><span><strong class="bold"><span class="kobospan" id="kobo.1282.1">report news”</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold"><span class="kobospan" id="kobo.1283.1">“</span></strong><span><strong class="bold"><span class="kobospan" id="kobo.1284.1">submitted”</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold"><span class="kobospan" id="kobo.1285.1">The rest of </span></strong><span><strong class="bold"><span class="kobospan" id="kobo.1286.1">the N-grams</span></strong></span></p>
</td>
</tr>
</thead>
<tbody class="calibre13">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.1287.1">Feature value</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span class="kobospan" id="kobo.1288.1">0</span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span class="kobospan" id="kobo.1289.1">1</span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span class="kobospan" id="kobo.1290.1">1</span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span class="kobospan" id="kobo.1291.1">0</span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span class="kobospan" id="kobo.1292.1">1</span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span class="kobospan" id="kobo.1293.1">(</span><span><span class="kobospan" id="kobo.1294.1">0’s)</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.1295.1">Figure 5.4 – Transforming an input text sentence into a numerical representation by partitioning to unigrams and bigrams via one-hot encoding</span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1296.1">Note </span><a id="_idIndexMarker524" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1297.1">that at this point in the code, the dataset hasn’t been partitioned into train and test sets, and the held-out set has not been excluded yet. </span><span class="kobospan" id="kobo.1297.2">This is because the binary and BOW feature engineering methods don’t depend on data outside of the underlying observation. </span><span class="kobospan" id="kobo.1297.3">With TF-IDF, this is different. </span><span class="kobospan" id="kobo.1297.4">Every feature value is calculated using the entire dataset for the </span><span><span class="kobospan" id="kobo.1298.1">document frequency.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.1299.1">Exploring the new numerical features</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.1300.1">Now </span><a id="_idIndexMarker525" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1301.1">that our text has been represented as a feature, we can explore it numerically. </span><span class="kobospan" id="kobo.1301.2">We can look at its frequencies and statistics and get a sense of how </span><span><span class="kobospan" id="kobo.1302.1">it’s distributed.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.1303.1">Splitting into train/test sets</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.1304.1">This is the part where we must pause and carve out a held-out set, also known as a test set, and </span><a id="_idIndexMarker526" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1305.1">sometimes as the validation set. </span><span class="kobospan" id="kobo.1305.2">Since these terms are used differently in different sources, it is important to explain that what we refer to as a test set is a held-out set. </span><span class="kobospan" id="kobo.1305.3">A held-out set is a data subset that we dedicate to evaluating our solution’s performance. </span><span class="kobospan" id="kobo.1305.4">It is held out to simulate the results that we expect to get when the system is implemented in the real world and will encounter new </span><span><span class="kobospan" id="kobo.1306.1">data samples.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1307.1">How do we know when to carve out the </span><span><span class="kobospan" id="kobo.1308.1">held-out set?</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1309.1">If we carve it out “too early,” such as right after loading the data, then we are guaranteed to keep it held out, but we may miss discrepancies in the data as it won’t take part in the preliminary exploration. </span><span class="kobospan" id="kobo.1309.2">If we carve it out “too late,” our design decisions might become biased because of it. </span><span class="kobospan" id="kobo.1309.3">For example, if we choose one ML model over another based on results that include the would-be held-out set, then our design becomes tailored to that set, preventing us from offering an objective evaluation of </span><span><span class="kobospan" id="kobo.1310.1">the model.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1311.1">Then, we </span><a id="_idIndexMarker527" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1312.1">need to carry out the test set right before the first action that will feed into design decisions. </span><span class="kobospan" id="kobo.1312.2">In the next section, we’ll perform statistical analysis, which we can then feed into feature selection. </span><span class="kobospan" id="kobo.1312.3">Since that selection should be agnostic to the held-out set, we’ll exclude that set from this </span><span><span class="kobospan" id="kobo.1313.1">part onwards.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.1314.1">Preliminary statistical analysis and feasibility study</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.1315.1">This is the second part of the exploration phase we spoke about a few pages ago. </span><span class="kobospan" id="kobo.1315.2">The first </span><a id="_idIndexMarker528" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1316.1">part was data exploration, and we implemented that in the previous parts of the code. </span><span class="kobospan" id="kobo.1316.2">Now that we have the text represented as numerical features, we can perform the </span><span><span class="kobospan" id="kobo.1317.1">feasibility study.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1318.1">We seek to measure the statistical dependence between the text inputs and the class values. </span><span class="kobospan" id="kobo.1318.2">Again, the motivation is to mimic the proxy that linear correlation provides with a </span><span><span class="kobospan" id="kobo.1319.1">regression problem.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1320.1">We know that for two random variables, </span><em class="italic"><span class="kobospan" id="kobo.1321.1">X</span></em><span class="kobospan" id="kobo.1322.1"> and </span><em class="italic"><span class="kobospan" id="kobo.1323.1">Y</span></em><span class="kobospan" id="kobo.1324.1">, if they are statistically independent, then we get </span><span><span class="kobospan" id="kobo.1325.1">the following:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1326.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;Y&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;Y&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/291.png" class="calibre290"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1327.1">Alternatively, we get </span><span><span class="kobospan" id="kobo.1328.1">the following:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1329.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;Y&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;Y&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/292.png" class="calibre291"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1330.1">This happens for every </span><em class="italic"><span class="kobospan" id="kobo.1331.1">x, y</span></em><span class="kobospan" id="kobo.1332.1"> value that yields a </span><span><span class="kobospan" id="kobo.1333.1">non-zero probability.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1334.1">Conversely, we could use </span><span><span class="kobospan" id="kobo.1335.1">Bayes’s rules:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1336.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi&gt;Y&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;Y&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;Y&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/293.png" class="calibre292"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1337.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi&gt;Y&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/294.png" class="calibre293"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1338.1">Now, let’s think about any two random variables that aren’t necessarily statistically independent. </span><span class="kobospan" id="kobo.1338.2">We would like to evaluate whether there is a statistical relationship between </span><span><span class="kobospan" id="kobo.1339.1">the two.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1340.1">Let one random variable be any of our numerical features, and the other random variable be the </span><a id="_idIndexMarker529" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1341.1">output class taking on values 0 or 1. </span><span class="kobospan" id="kobo.1341.2">Let’s assume the feature engineering method is binary, so the feature also takes on values of 0 </span><span><span class="kobospan" id="kobo.1342.1">or 1.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1343.1">Looking at the last equation, the expression on the left-hand side presents a very powerful measure of the ability of the relationship between </span><em class="italic"><span class="kobospan" id="kobo.1344.1">X</span></em> <span><span class="kobospan" id="kobo.1345.1">and </span></span><span><em class="italic"><span class="kobospan" id="kobo.1346.1">Y</span></em></span><span><span class="kobospan" id="kobo.1347.1">:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1348.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mo&gt;{&lt;/mo&gt;&lt;mn&gt;0,1&lt;/mn&gt;&lt;mo&gt;}&lt;/mo&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/295.png" class="calibre294"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1349.1">It is powerful because if the feature is completely nonindicative of the class value, then in statistical terms, we say the two are statistically independent, and thus this measure would be equal </span><span><span class="kobospan" id="kobo.1350.1">to 1.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1351.1">Conversely, the </span><a id="_idIndexMarker530" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1352.1">bigger the difference between this measure and 1, the stronger the relationship is between this feature and this class. </span><span class="kobospan" id="kobo.1352.2">When performing a </span><strong class="bold"><span class="kobospan" id="kobo.1353.1">feasibility study</span></strong><span class="kobospan" id="kobo.1354.1"> of our design, we want to see that there are features in the data that have a statistical relationship with the </span><span><span class="kobospan" id="kobo.1355.1">output class.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1356.1">For that reason, we calculate the value of this expression for every pair of every feature and </span><span><span class="kobospan" id="kobo.1357.1">every class.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1358.1">We present the most indicative terms for class “0,” which is the class of tweets that don’t indicate a company or product information, and we also present the terms that are most indicative of class “1,” meaning, when a tweet is discussing information about a company or </span><span><span class="kobospan" id="kobo.1359.1">a product.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1360.1">This proves to us that there are indeed text terms that are indicative of the class value. </span><span class="kobospan" id="kobo.1360.2">This is a definite and clear success of the feasibility study. </span><span class="kobospan" id="kobo.1360.3">We are good to go and we are expecting productive outcomes when implementing a </span><span><span class="kobospan" id="kobo.1361.1">classification model.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1362.1">As a side note, keep in mind that as with most evaluations, what we’ve just mentioned is just one sufficient condition for the potential of the text to predict the class. </span><span class="kobospan" id="kobo.1362.2">If it had failed, it would not necessarily indicate that there is no feasibility. </span><span class="kobospan" id="kobo.1362.3">Just like when the linear </span><a id="_idIndexMarker531" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1363.1">correlation between </span><em class="italic"><span class="kobospan" id="kobo.1364.1">X</span></em><span class="kobospan" id="kobo.1365.1"> and</span><em class="italic"><span class="kobospan" id="kobo.1366.1"> Y</span></em><span class="kobospan" id="kobo.1367.1"> is near 0, this doesn’t mean that </span><em class="italic"><span class="kobospan" id="kobo.1368.1">X</span></em><span class="kobospan" id="kobo.1369.1"> can’t infer </span><em class="italic"><span class="kobospan" id="kobo.1370.1">Y</span></em><span class="kobospan" id="kobo.1371.1">. </span><span class="kobospan" id="kobo.1371.2">It just means that </span><em class="italic"><span class="kobospan" id="kobo.1372.1">X</span></em><span class="kobospan" id="kobo.1373.1"> cannot infer </span><em class="italic"><span class="kobospan" id="kobo.1374.1">Y</span></em><span class="kobospan" id="kobo.1375.1"> via a linear model. </span><span class="kobospan" id="kobo.1375.2">The linearity is an assumption that’s made to make things simple if </span><span><span class="kobospan" id="kobo.1376.1">it prevails.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1377.1">In the method that we’ve suggested, we make two key assumptions. </span><span class="kobospan" id="kobo.1377.2">First, we assume a very particular manner for feature design, being a certain </span><em class="italic"><span class="kobospan" id="kobo.1378.1">N</span></em><span class="kobospan" id="kobo.1379.1"> for the N-gram partition, and a certain quantitative method for the value – binary. </span><span class="kobospan" id="kobo.1379.2">The second is that we perform the most simple evaluation of statistical dependency, a univariate statistical dependency. </span><span class="kobospan" id="kobo.1379.3">But it could be that only a higher order, such as univariate, would have statistical dependence on the </span><span><span class="kobospan" id="kobo.1380.1">outcome class.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1381.1">With a </span><strong class="bold"><span class="kobospan" id="kobo.1382.1">feasibility study</span></strong><span class="kobospan" id="kobo.1383.1"> of text classification, it’s ideal if the method is as simple as possible while covering as much of the “signal” it is hoping to uncover. </span><span class="kobospan" id="kobo.1383.2">The approach we designed in this example was derived after years of experience with different sets and various problem settings. </span><span class="kobospan" id="kobo.1383.3">We find that it hits the target </span><span><span class="kobospan" id="kobo.1384.1">very well.</span></span></p>
<h2 id="_idParaDest-114" class="calibre7"><a id="_idTextAnchor203" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1385.1">Feature selection</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.1386.1">With the </span><strong class="bold"><span class="kobospan" id="kobo.1387.1">feasibility study</span></strong><span class="kobospan" id="kobo.1388.1">, we often kill two birds with one stone. </span><span class="kobospan" id="kobo.1388.2">As a </span><strong class="bold"><span class="kobospan" id="kobo.1389.1">feasibility study</span></strong><span class="kobospan" id="kobo.1390.1"> is successful, it not only helps us by confirming our plan, but it often hints toward the next </span><a id="_idIndexMarker532" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1391.1">steps that we should take. </span><span class="kobospan" id="kobo.1391.2">As we saw, some features are indicative of the class, and we learned which are the most significant. </span><span class="kobospan" id="kobo.1391.3">This allows us to reduce the feature space that the classification model will need to partition. </span><span class="kobospan" id="kobo.1391.4">We do that by keeping the most indicative features for each of the two classes. </span><span class="kobospan" id="kobo.1391.5">The number of features that we choose to keep would ideally be derived by computation constraints (for example, too many features would take too long to compute a model around), model capabilities (for example, too many features can’t be handled well by the model due to co-linearity), and optimization of the train results. </span><span class="kobospan" id="kobo.1391.6">In our code, we fixed this number to make things quick </span><span><span class="kobospan" id="kobo.1392.1">and simple.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1393.1">It should be stressed that in many ML models, feature selection is an inherited part of the </span><a id="_idIndexMarker533" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1394.1">model design. </span><span class="kobospan" id="kobo.1394.2">For instance, with the </span><strong class="bold"><span class="kobospan" id="kobo.1395.1">least absolute shrinkage and selection operator</span></strong><span class="kobospan" id="kobo.1396.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.1397.1">LASSO</span></strong><span class="kobospan" id="kobo.1398.1">), the hyperparameter scaler of the </span><span class="kobospan" id="kobo.1399.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;script&quot;&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/296.png" class="calibre295"/></span><span class="kobospan" id="kobo.1400.1"> norm component has an impact on which features get a zero coefficient, and thus get “thrown out.” </span><span class="kobospan" id="kobo.1400.2">It is possible and sometimes recommended to skip this part of the feature selection process, leave all features in, and let the model perform feature selection. </span><span class="kobospan" id="kobo.1400.3">It is advised to do so when all the models that are being evaluated and compared possess </span><span><span class="kobospan" id="kobo.1401.1">that characteristic.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1402.1">Remember </span><a id="_idIndexMarker534" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1403.1">that at this point, we are only observing the train set. </span><span class="kobospan" id="kobo.1403.2">Now that we have decided which features to keep, we need to apply that selection to the test set </span><span><span class="kobospan" id="kobo.1404.1">as well.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1405.1">With that, our data has been prepared for </span><span><span class="kobospan" id="kobo.1406.1">ML modeling.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.1407.1">Iterating over ML models</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.1408.1">To choose which model suits this problem best, we must train several models and see which one of them </span><span><span class="kobospan" id="kobo.1409.1">does best.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1410.1">We </span><a id="_idIndexMarker535" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1411.1">should stress that we could do many things to try and identify the best model choice for a given </span><a id="_idIndexMarker536" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1412.1">problem. </span><span class="kobospan" id="kobo.1412.2">In our case, we only chose to evaluate a handful of models. </span><span class="kobospan" id="kobo.1412.3">Moreover, to make things simple and quick, we chose to not optimize the hyperparameters of each model in a comprehensive cross-validation approach. </span><span class="kobospan" id="kobo.1412.4">We simply fit each model to the training set with the default settings that its function comes with. </span><span class="kobospan" id="kobo.1412.5">Once we’ve identified the model we’d like to use, we optimize its hyperparameters for the train set </span><span><span class="kobospan" id="kobo.1413.1">via cross-validation.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1414.1">By doing this, we identify the best model for </span><span><span class="kobospan" id="kobo.1415.1">the problem.</span></span></p>
<h2 id="_idParaDest-115" class="calibre7"><a id="_idTextAnchor204" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1416.1">Generating the chosen model</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.1417.1">Here, we </span><a id="_idIndexMarker537" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1418.1">optimize the hyperparameters of the chosen model and fit it to our </span><span><span class="kobospan" id="kobo.1419.1">train set.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.1420.1">Generating the train results – design choices</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.1421.1">At this </span><a id="_idIndexMarker538" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1422.1">stage, we observe the results of the model for the first time. </span><span class="kobospan" id="kobo.1422.2">This result can be used to feed insight back into the design choice and the parameters chosen, such as the feature engineering method, the number of features left in the feature selection, and even the </span><span><span class="kobospan" id="kobo.1423.1">preprocessing scheme.</span></span></p>
<p class="callout-heading"><span class="kobospan" id="kobo.1424.1">Important note</span></p>
<p class="callout"><span class="kobospan" id="kobo.1425.1">Note that when feeding back insights from the results of the train set to the design of the solution, you are risking overfitting the train set. </span><span class="kobospan" id="kobo.1425.2">You’ll know whether you are by the gap between the results on the train set and the results on the </span><span><span class="kobospan" id="kobo.1426.1">test set.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1427.1">While </span><a id="_idIndexMarker539" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1428.1">a gap is expected between these results in favor of the train results, a large gap should be treated as an alarm that the design isn’t optimal. </span><span class="kobospan" id="kobo.1428.2">In such cases, the design should be redone with systematic code-based parameters to ensure fair choices are made. </span><span class="kobospan" id="kobo.1428.3">It is possible to even carve out another semi-held-out set from the train set, often referred to as the </span><span><span class="kobospan" id="kobo.1429.1">validation set.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.1430.1">Generating the test results – presenting performance</span></h3>
<p class="calibre6"><span><span class="kobospan" id="kobo.1431.1">That’s it!</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1432.1">Now </span><a id="_idIndexMarker540" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1433.1">that the design has been optimized and we are confident that it suits our objective, we can apply it to our held-out set and observe the test results. </span><span class="kobospan" id="kobo.1433.2">These results are our most objective forecast of how well the system would do in the </span><span><span class="kobospan" id="kobo.1434.1">real world.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1435.1">As mentioned previously, we should avoid letting these results impact our</span><a id="_idTextAnchor205" class="calibre5 pcalibre1 pcalibre"/><a id="_idTextAnchor206" class="calibre5 pcalibre1 pcalibre"/><a id="_idTextAnchor207" class="calibre5 pcalibre1 pcalibre"/> <span><span class="kobospan" id="kobo.1436.1">design choices.</span></span></p>
<h1 id="_idParaDest-116" class="calibre4"><a id="_idTextAnchor208" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1437.1">Summary</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.1438.1">In this chapter, we embarked on a comprehensive exploration of text classification, an indispensable aspect of NLP and ML. </span><span class="kobospan" id="kobo.1438.2">We delved into various types of text classification tasks, each presenting unique challenges and opportunities. </span><span class="kobospan" id="kobo.1438.3">This foundational understanding sets the stage for effectively tackling a broad range of applications, from sentiment analysis to </span><span><span class="kobospan" id="kobo.1439.1">spam detection.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1440.1">We walked through the role of N-grams in capturing local context and word sequences within text, thereby enhancing the feature set used for classification tasks. </span><span class="kobospan" id="kobo.1440.2">We also illuminated the power of the TF-IDF method, the role of Word2Vec in text classification, and popular architectures such as CBOW and skip-gram, giving you a deep understanding of </span><span><span class="kobospan" id="kobo.1441.1">their mechanics.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1442.1">Then, we introduced topic modeling and examined how popular algorithms such as LDA can be applied to </span><span><span class="kobospan" id="kobo.1443.1">text classification.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1444.1">Lastly, we introduced a professional paradigm for leading an NLP-ML project in a business or research setting. </span><span class="kobospan" id="kobo.1444.2">We discussed the objectives and the project design aspect, and then dove into the system design. </span><span class="kobospan" id="kobo.1444.3">We implemented a real-world example in code and experimented </span><span><span class="kobospan" id="kobo.1445.1">with this.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1446.1">In essence, this chapter has aimed to equip you with a holistic understanding of text classification and topic modeling by touching on the key concepts, methodologies, and techniques in the field. </span><span class="kobospan" id="kobo.1446.2">The knowledge and skills imparted will enable you to effectively approach and solve real-world text </span><span><span class="kobospan" id="kobo.1447.1">classification problems.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1448.1">In the next chapter, we will introduce advanced methods for text classification. </span><span class="kobospan" id="kobo.1448.2">We will review deep learning methods such as language models, discuss their theory and design, and present a hands-on system design </span><span><span class="kobospan" id="kobo.1449.1">in code.</span></span></p>
</div>
</body></html>