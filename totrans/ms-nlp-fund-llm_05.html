<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-92"><a id="_idTextAnchor130" class="calibre5 pcalibre1 pcalibre"/>5</h1>
<h1 id="_idParaDest-93" class="calibre4"><a id="_idTextAnchor131" class="calibre5 pcalibre1 pcalibre"/>Empowering Text Classification: Leveraging Traditional Machine Learning Techniques</h1>
<p class="calibre6">In this chapter, we’ll delve into the fascinating world of text classification, a foundational task in <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>) and <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) that deals with <a id="_idIndexMarker429" class="calibre5 pcalibre1 pcalibre"/>categorizing text documents into predefined classes. As the <a id="_idIndexMarker430" class="calibre5 pcalibre1 pcalibre"/>volume of digital text data continues to grow exponentially, the ability to accurately and efficiently classify text has become increasingly important for a wide range of applications, such as sentiment analysis, spam detection, and document organization. This chapter provides a comprehensive overview of the key concepts, methodologies, and techniques that are employed in text classification, catering to readers from diverse backgrounds and skill levels.</p>
<p class="calibre6">We’ll begin by exploring the various types of text classification tasks and their unique characteristics, offering insights into the challenges and opportunities each type presents. Next, we’ll introduce <a id="_idIndexMarker431" class="calibre5 pcalibre1 pcalibre"/>the concept of <strong class="bold">N-grams</strong> and discuss how they can be utilized as features for text classification, capturing not only individual words <a id="_idIndexMarker432" class="calibre5 pcalibre1 pcalibre"/>but also the local context and word sequences within the text. We’ll then examine the widely used <strong class="bold">term frequency-inverse document frequency</strong> (<strong class="bold">TF-IDF</strong>) method, which assigns weights to words based on their frequency in a document and across the entire corpus, showcasing its effectiveness in distinguishing relevant words for classification tasks.</p>
<p class="calibre6">Following that, we’ll delve <a id="_idIndexMarker433" class="calibre5 pcalibre1 pcalibre"/>into the powerful <strong class="bold">Word2Vec</strong> algorithm and its application in text classification. We’ll discuss how <strong class="bold">Word2Vec</strong> creates dense vector representations of words that capture semantic meaning and relationships, and how these embeddings can be used as features to improve classification performance. Furthermore, we’ll <a id="_idIndexMarker434" class="calibre5 pcalibre1 pcalibre"/>cover popular architectures such as <strong class="bold">continuous bag-of-words</strong> (<strong class="bold">CBOW</strong>) and Skip-Gram, providing a deeper understanding of their inner workings.</p>
<p class="calibre6">Lastly, we’ll explore the concept of topic modeling, a technique for discovering hidden thematic <a id="_idIndexMarker435" class="calibre5 pcalibre1 pcalibre"/>structures within a collection of documents. We’ll examine popular algorithms such as <strong class="bold">latent Dirichlet allocation</strong> (<strong class="bold">LDA</strong>) and describe how topic modeling can be applied to text classification, enabling the discovery of semantic relationships between documents and improving classification performance.</p>
<p class="calibre6">Throughout this chapter, we aim to provide a thorough understanding of the underlying concepts and techniques that are employed in text classification, equipping you with the knowledge and skills needed to successfully tackle real-world text classification problems.</p>
<p class="calibre6">The following topics will be covered in this chapter:</p>
<ul class="calibre14">
<li class="calibre15">Types of text classification</li>
<li class="calibre15">Text classification based on N-grams</li>
<li class="calibre15">Text classification based on TF-IDF</li>
<li class="calibre15">Word2Vec and its application in text classification</li>
<li class="calibre15">Topic modeling</li>
<li class="calibre15">Reviewing our use case – ML system design for NLP classification in a Jupyter no<a id="_idTextAnchor132" class="calibre5 pcalibre1 pcalibre"/>tebook</li>
</ul>
<h1 id="_idParaDest-94" class="calibre4"><a id="_idTextAnchor133" class="calibre5 pcalibre1 pcalibre"/>Technical requirements</h1>
<p class="calibre6">To effectively read and understand this chapter, it is essential to have a solid foundation in various technical areas. A strong grasp of fundamental concepts in NLP, ML, and linear algebra is crucial. Familiarity with text preprocessing techniques, such as tokenization, stop word removal, and stemming or lemmatization, is necessary to comprehend the data preparation stage.</p>
<p class="calibre6">Additionally, understanding <a id="_idIndexMarker436" class="calibre5 pcalibre1 pcalibre"/>basic ML algorithms, such as logistic regression and <strong class="bold">support vector machines</strong> (<strong class="bold">SVMs</strong>), is crucial for implementing text classification models. Finally, being comfortable with evaluation metrics such as accuracy, precision, recall, and F1 score, along with concepts such as overfitting, underfitting, and hyperparameter tuning, will enable a deeper appreciation of the challenges and best practices in text classifi<a id="_idTextAnchor134" class="calibre5 pcalibre1 pcalibre"/>cation.</p>
<h1 id="_idParaDest-95" class="calibre4"><a id="_idTextAnchor135" class="calibre5 pcalibre1 pcalibre"/>Types of text classification</h1>
<p class="calibre6">Text classification is an NLP task where ML algorithms assign predefined categories or labels to text <a id="_idIndexMarker437" class="calibre5 pcalibre1 pcalibre"/>based on its content. It involves training a model on a labeled dataset to enable it to accurately predict the category of unseen or new text inputs. Text classification methods can be categorized into three main types – <strong class="bold">supervised learning</strong>, <strong class="bold">unsupervised learning</strong>, and <strong class="bold">semi-supervised learning</strong>:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Supervised learning</strong>: This type of text classification involves training a model on labeled <a id="_idIndexMarker438" class="calibre5 pcalibre1 pcalibre"/>data, where each data point is associated with a target label or category. The model then uses this labeled data to learn the patterns and relationships between the input text and the target labels. Examples <a id="_idIndexMarker439" class="calibre5 pcalibre1 pcalibre"/>of supervised learning algorithms <a id="_idIndexMarker440" class="calibre5 pcalibre1 pcalibre"/>for text classification include naive bayes, SVMs, and neural networks such as <strong class="bold">convolutional neural networks</strong> (<strong class="bold">CNNs</strong>) and <strong class="bold">recurrent neural </strong><strong class="bold">networks</strong> (<strong class="bold">RNNs</strong>).</li>
<li class="calibre15"><strong class="bold">Unsupervised learning</strong>: This type of text classification involves clustering or grouping <a id="_idIndexMarker441" class="calibre5 pcalibre1 pcalibre"/>text documents into categories or topics without any prior knowledge of the categories or labels. Unsupervised learning is useful when there is no labeled data available or when the number of categories or topics is not known. Examples of unsupervised learning algorithms for text <a id="_idIndexMarker442" class="calibre5 pcalibre1 pcalibre"/>classification include K-means clustering, LDA, and <strong class="bold">hierarchical Dirichlet </strong><strong class="bold">process</strong> (<strong class="bold">HDP</strong>).</li>
<li class="calibre15"><strong class="bold">Semi-supervised learning</strong>: This type of text classification combines both supervised <a id="_idIndexMarker443" class="calibre5 pcalibre1 pcalibre"/>and unsupervised learning approaches. It involves using a small amount of labeled data to train a model and then using the model to classify the remaining unlabeled data. The model then uses the unlabeled data to improve its classification performance. Semi-supervised learning is useful when labeled data is scarce or expensive to obtain. Examples of semi-supervised learning algorithms for text classification include <strong class="bold">self-training</strong>, <strong class="bold">co-training</strong>, and <strong class="bold">multi-view learning</strong>.</li>
</ul>
<p class="calibre6">Each of these text classification types has its strengths and weaknesses and is suitable for different types of applications. Understanding these types can help in choosing the appropriate <a id="_idIndexMarker444" class="calibre5 pcalibre1 pcalibre"/>approach for a given problem. In the following subsections, we’ll explain each of these met<a id="_idTextAnchor136" class="calibre5 pcalibre1 pcalibre"/>hods in detail.</p>
<h2 id="_idParaDest-96" class="calibre7"><a id="_idTextAnchor137" class="calibre5 pcalibre1 pcalibre"/>Supervised learning</h2>
<p class="calibre6">Supervised <a id="_idIndexMarker445" class="calibre5 pcalibre1 pcalibre"/>learning is a type of ML where an algorithm learns from labeled data to predict the label of new, unseen data.</p>
<p class="calibre6">In the context <a id="_idIndexMarker446" class="calibre5 pcalibre1 pcalibre"/>of text classification, supervised learning involves training a model on a labeled dataset, where each document or text sample is labeled with the corresponding category or class. The model then uses this training data to learn patterns and relationships between the text features and their associated labels:</p>
<ol class="calibre16">
<li class="calibre15">In a supervised text classification task, the first step is to obtain a labeled dataset, where each text sample is annotated with its corresponding category or class.<p class="calibre6">A labeled dataset is assumed to possess the highest level of reliability. Often, it is derived by having subject matter experts manually review the text and assign the appropriate class to each item. In other scenarios, there may be automated methods for deriving the labels. For instance, in cybersecurity, you may collect historical data and then assign labels, which may collect the outcome that followed each item – that is, whether the action was legitimate or not. Since such historical data exists in most domains, that too can serve as a reliable labeled set.</p></li>
<li class="calibre15">The next step is to preprocess the text data to prepare it for modeling. This may include steps such as tokenization, stemming or lemmatization, removing stop words, and other text preprocessing techniques.</li>
<li class="calibre15">After preprocessing, the text data is transformed into numerical features, often using techniques such as bag-of-words or TF-IDF encoding.</li>
<li class="calibre15">Then, a supervised learning algorithm such as logistic regression, SVM, or a neural network is trained on the labeled dataset using these numerical features.</li>
</ol>
<p class="calibre6">Once the <a id="_idIndexMarker447" class="calibre5 pcalibre1 pcalibre"/>model has been trained, it can be used to predict the category or class of new, unseen text data based on the learned patterns and <a id="_idIndexMarker448" class="calibre5 pcalibre1 pcalibre"/>relationships between the text features and their associated labels.</p>
<p class="calibre6">Supervised learning algorithms are commonly used for text classification tasks. Let’s look at some common supervised learning algorithms that are used for t<a id="_idTextAnchor138" class="calibre5 pcalibre1 pcalibre"/>ext classification.</p>
<h3 class="calibre8">Naive Bayes</h3>
<p class="calibre6">Naive Bayes is a <a id="_idIndexMarker449" class="calibre5 pcalibre1 pcalibre"/>probabilistic algorithm that is commonly <a id="_idIndexMarker450" class="calibre5 pcalibre1 pcalibre"/>used for text classification. It is based on Bayes’ theorem, which states that the probability of a hypothesis (in this case, a document belonging to a particular class), given some observed evidence (in this case, the words in the document), is proportional to the probability of the evidence given the hypothesis times the prior probability of the hypothesis. Naive Bayes assumes that the features (words) are independent of each other given the class label, which is where the “naive” part of<a id="_idTextAnchor139" class="calibre5 pcalibre1 pcalibre"/> the name comes from.</p>
<h3 class="calibre8">Logistic regression</h3>
<p class="calibre6">Logistic <a id="_idIndexMarker451" class="calibre5 pcalibre1 pcalibre"/>regression is a statistical method that is used <a id="_idIndexMarker452" class="calibre5 pcalibre1 pcalibre"/>for binary classification problems (that is, problems where there are only two possible classes). It models the probability of the document belonging to a particular class using a logistic function, which maps any real-valued input to a<a id="_idTextAnchor140" class="calibre5 pcalibre1 pcalibre"/> value between 0 and 1.</p>
<h3 class="calibre8">SVM</h3>
<p class="calibre6">SVM is a <a id="_idIndexMarker453" class="calibre5 pcalibre1 pcalibre"/>powerful classification algorithm that is used <a id="_idIndexMarker454" class="calibre5 pcalibre1 pcalibre"/>in a variety of applications, including text classification. SVM works by finding the hyperplane that best separates the data into different classes. In text classification, the features are typically the words in the document, and the hyperplane is used to divide the space of all possible documents into different regions corresponding to different classes.</p>
<p class="calibre6">All of these algorithms can be trained using labeled data, where the class labels are known for <a id="_idIndexMarker455" class="calibre5 pcalibre1 pcalibre"/>each document in the training set. Once <a id="_idIndexMarker456" class="calibre5 pcalibre1 pcalibre"/>trained, the model can be used to predict the class label of new, unlabeled documents. The performance of the model is typically evaluated using metrics such as accuracy, preci<a id="_idTextAnchor141" class="calibre5 pcalibre1 pcalibre"/>sion, recall, and F1 score.</p>
<h2 id="_idParaDest-97" class="calibre7"><a id="_idTextAnchor142" class="calibre5 pcalibre1 pcalibre"/>Unsupervised learning</h2>
<p class="calibre6">Unsupervised learning is a type of ML where the data is not labeled and the algorithm is left to <a id="_idIndexMarker457" class="calibre5 pcalibre1 pcalibre"/>find patterns and structures on its own. In the context of text classification, unsupervised learning methods can be used when there is no labeled <a id="_idIndexMarker458" class="calibre5 pcalibre1 pcalibre"/>data available or when the goal is to discover hidden patterns in the text data.</p>
<p class="calibre6">One common <a id="_idIndexMarker459" class="calibre5 pcalibre1 pcalibre"/>unsupervised learning method for text classification is <strong class="bold">clustering</strong>. Clustering algorithms group similar documents together based on their content, without any prior knowledge of what each document is about. Clustering can be used to identify topics in a collection of documents or to group similar documents together for further analysis.</p>
<p class="calibre6">Another popular <a id="_idIndexMarker460" class="calibre5 pcalibre1 pcalibre"/>unsupervised learning algorithm for text classification is <strong class="bold">LDA</strong>. LDA is a probabilistic generative model that assumes that each document in a corpus is a mixture of topics, and each topic is a probability distribution over words. LDA can be used to discover the underlying topics in a collection of documents, even when the topics are not explicitly labeled.</p>
<p class="calibre6">Finally, word embeddings are a popular unsupervised learning technique used for text classification. Word embeddings are dense vector representations of words that capture their semantic meaning based on the context in which they appear. They can be used to identify similar words and to find relationships between words, which can be useful for tasks such as text similarity and recommendation systems. Common word embedding models include Word2Vec and GloVe.</p>
<p class="calibre6">Word2Vec is a popular algorithm that’s used to generate word embeddings, which are vector representations of words in a high-dimensional space. The algorithm was developed by a team of researchers at Google, led by Tomas Mikolov, in 2013. The main idea behind Word2Vec is that words that appear in similar contexts tend to have similar meanings.</p>
<p class="calibre6">The algorithm takes in a large corpus of text as input and generates a vector representation for each word in the vocabulary. The vectors are typically high-dimensional (for example, 100 or 300 dimensions) and can be used to perform various NLP tasks, such as sentiment analysis, text classification, and machine translation.</p>
<p class="calibre6">Two main <a id="_idIndexMarker461" class="calibre5 pcalibre1 pcalibre"/>architectures are used in Word2Vec: <strong class="bold">CBOW</strong> and <strong class="bold">skip-gram</strong>. In the CBOW architecture, the algorithm tries to predict the <a id="_idIndexMarker462" class="calibre5 pcalibre1 pcalibre"/>target word given a window of context words. In the skip-gram architecture, the algorithm tries to predict the context words given a target word. The training objective is to maximize the likelihood of the target word or context words given the input.</p>
<p class="calibre6">Word2Vec has been widely adopted in the NLP community and has shown state-of-the-art performance on various benchmarks. It has also been used in many real-world applications, such as recommender syste<a id="_idTextAnchor143" class="calibre5 pcalibre1 pcalibre"/>ms, search engines, and chatbots.</p>
<h2 id="_idParaDest-98" class="calibre7"><a id="_idTextAnchor144" class="calibre5 pcalibre1 pcalibre"/>Semi-supervised learning</h2>
<p class="calibre6">Semi-supervised learning is an ML paradigm that sits between supervised and unsupervised <a id="_idIndexMarker463" class="calibre5 pcalibre1 pcalibre"/>learning. It utilizes a combination of labeled <a id="_idIndexMarker464" class="calibre5 pcalibre1 pcalibre"/>and unlabeled data for training, which is especially useful when  the underlying models require labeled data which is expensive or time-consuming. This approach allows the model to leverage the information in the unlabeled data to improve its performance on the classification task.</p>
<p class="calibre6">In the context of text classification, semi-supervised learning can be beneficial when we have a limited number of labeled documents but a large corpus of unlabeled documents. The goal is to improve the performance of the classifier by leveraging the information contained in the unlabeled data.</p>
<p class="calibre6">There are several common semi-supervised learning algorithms, including label propagation and co-training. We’ll discuss<a id="_idTextAnchor145" class="calibre5 pcalibre1 pcalibre"/> each of these in more detail next.</p>
<h3 class="calibre8">Label propagation</h3>
<p class="calibre6">Label propagation is a graph-based semi-supervised learning algorithm. It builds a graph using both <a id="_idIndexMarker465" class="calibre5 pcalibre1 pcalibre"/>labeled and unlabeled data points, with each data point represented as a node and edges representing the similarity <a id="_idIndexMarker466" class="calibre5 pcalibre1 pcalibre"/>between nodes. The algorithm works by propagating the labels from the labeled nodes to the unlabeled nodes based on their similarity.</p>
<p class="calibre6">The key idea is that similar data points should have similar labels. The algorithm begins by assigning initial label probabilities to the unlabeled nodes, typically based on their similarity to labeled nodes. Then, an iterative process propagates these probabilities throughout the graph until convergence. The final label probabilities are used t<a id="_idTextAnchor146" class="calibre5 pcalibre1 pcalibre"/>o classify the unlabeled data points.</p>
<h3 class="calibre8">Co-training</h3>
<p class="calibre6">Co-training is another semi-supervised learning technique that trains multiple classifiers on different <a id="_idIndexMarker467" class="calibre5 pcalibre1 pcalibre"/>views of the data. A view is a subset of features that are sufficient for the learning task and are conditionally independent given the class label. The basic <a id="_idIndexMarker468" class="calibre5 pcalibre1 pcalibre"/>idea is to use one classifier’s predictions to label some of the unlabeled data, and then use that newly labeled data to train the other classifier. This process is performed iteratively, with each classifier improving the other until a stopping criterion is met.</p>
<p class="calibre6">To apply semi-supervised learning in a specific domain, let’s consider a medical domain where we want to classify scientific articles into different categories such as <strong class="bold">cardiology</strong>, <strong class="bold">neurology</strong>, and <strong class="bold">oncology</strong>. Suppose we have a small set of labeled articles and a large set of unlabeled articles.</p>
<p class="calibre6">A possible approach could be to use label propagation by creating a graph of articles where the nodes represent the articles and the edges represent the similarity between the articles. The similarity could be based on various factors, such as the words used, the topics covered, or the citation networks between the articles. After propagating the labels, we can classify the unlabeled articles based on the final label probabilities.</p>
<p class="calibre6">Alternatively, we could use co-training by splitting the features into two views, such as the abstract and the full text of the articles. We would train two classifiers, one for each view, and iteratively update the classifiers using the predictions made by the other classifier on the unlabeled data.</p>
<p class="calibre6">In both cases, the goal is to leverage the information in the unlabeled data to improve the performance of the classifier in the specific domain.</p>
<p class="calibre6">In this chapter, we’ll elaborate on supervised t<a id="_idTextAnchor147" class="calibre5 pcalibre1 pcalibre"/>ext classification and topic modeling.</p>
<h2 id="_idParaDest-99" class="calibre7"><a id="_idTextAnchor148" class="calibre5 pcalibre1 pcalibre"/>Sentence classification using one-hot encoding vector representation</h2>
<p class="calibre6">One-hot <a id="_idIndexMarker469" class="calibre5 pcalibre1 pcalibre"/>encoded vector representation <a id="_idIndexMarker470" class="calibre5 pcalibre1 pcalibre"/>is a method of representing categorical data, such as words, as binary vectors. In the context of text classification, one-hot encoding can be used to represent text data as numerical input features for a classification model. Here’s a detailed explanation of text class<a id="_idTextAnchor149" class="calibre5 pcalibre1 pcalibre"/>ification using one<a id="_idTextAnchor150" class="calibre5 pcalibre1 pcalibre"/>-hot encoding vectors.</p>
<h3 class="calibre8">Text preprocessing</h3>
<p class="calibre6">The <a id="_idIndexMarker471" class="calibre5 pcalibre1 pcalibre"/>first step is to preprocess the text data, as explained in the previous chapter. The main goal of preprocessing is to transform raw text into a more structured and consistent format that can be <a id="_idIndexMarker472" class="calibre5 pcalibre1 pcalibre"/>easily understood and processed by ML algorithms. Here are several reasons why text preprocessing is essential for one-hot encoded vector classification:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Noise reduction</strong>: Raw text data often contains noise, such as typos, spelling errors, special characters, and formatting inconsistencies. Preprocessing helps to clean the text, reducing noise that may negatively impact the performance of the classification model.</li>
<li class="calibre15"><strong class="bold">Dimensionality reduction</strong>: One-hot encoded vector representation has a high dimensionality as each unique word in the dataset corresponds to a separate feature. Preprocessing techniques, such as stop word removal, stemming, or lemmatization, can help reduce the size of the vocabulary, leading to a lower-dimensional feature space. This can improve the efficiency of the classification algorithm and reduce the risk of overfitting.</li>
<li class="calibre15"><strong class="bold">Consistent representation</strong>: Converting all text to lowercase and applying stemming or lemmatization ensures that words with the same meaning or root form are consistently represented in the one-hot encoding vectors. This can help the classification model learn more meaningful patterns from the data as it will not treat different forms of the same word as separate features.</li>
<li class="calibre15"><strong class="bold">Handling irrelevant information</strong>: Preprocessing can help remove irrelevant information, such as URLs, email addresses, or numbers, that may not contribute <a id="_idIndexMarker473" class="calibre5 pcalibre1 pcalibre"/>to the classification task. Removing such information can improve the model’s ability to focus on the meaningful words and patterns in the text.</li>
<li class="calibre15"><strong class="bold">Improving model performance</strong>: Preprocessed text data can lead to better performance of the classification model as the model will learn from a cleaner and more structured dataset. This can result in improved accuracy and generalization to new, unseen text data.</li>
</ul>
<p class="calibre6">Once <a id="_idIndexMarker474" class="calibre5 pcalibre1 pcalibre"/>we preprocess the text, we can start extracting the words in the tex<a id="_idTextAnchor151" class="calibre5 pcalibre1 pcalibre"/>t. We call this task vocabulary construction.</p>
<h3 class="calibre8">Vocabulary construction</h3>
<p class="calibre6">Construct a vocabulary containing all unique words in the preprocessed text. Assign a unique <a id="_idIndexMarker475" class="calibre5 pcalibre1 pcalibre"/>index to each word in the vocabulary.</p>
<p class="calibre6">Vocabulary construction is an essential step in preparing text data for one-hot encoded vector <a id="_idIndexMarker476" class="calibre5 pcalibre1 pcalibre"/>classification. The vocabulary is a set of all unique words (tokens) in the preprocessed text data. It serves as a basis for creating one-hot-encoded feature vectors for each document. Here’s a detailed explanation of the vocabulary construction process for one-hot encoded vector classification:</p>
<ol class="calibre16">
<li class="calibre15"><strong class="bold">Create a set of unique words</strong>: After preprocessing the text data, gather all the words from all documents and create a set of unique words. This set will represent the vocabulary. The order of the words in the vocabulary does not matter, but it’s crucial to keep track of the indices assigned to each word as they will be used to create one-hot encoded vectors later.<p class="calibre6">For example, consider that the following preprocessed dataset consists of two documents:</p><ul class="calibre17"><li class="calibre15"><strong class="bold">Document 1</strong>: “apple banana orange”</li><li class="calibre15"><strong class="bold">Document 2</strong>: “banana grape apple”</li></ul><p class="calibre6">The vocabulary for this dataset would be {“apple”, “banana”, “orange”, “grape”}.</p></li>
<li class="calibre15"><strong class="bold">Assign indices to the words</strong>: Once you have the set of unique words, assign a unique <a id="_idIndexMarker477" class="calibre5 pcalibre1 pcalibre"/>index to each word in the vocabulary. These <a id="_idIndexMarker478" class="calibre5 pcalibre1 pcalibre"/>indices will be used to create one-hot-encoded vectors for each document.<p class="calibre6">Using the preceding example, you might assign the following indic<a id="_idTextAnchor152" class="calibre5 pcalibre1 pcalibre"/>es:</p><ul class="calibre17"><li class="calibre15">“apple”: 0</li><li class="calibre15">“banana”: 1</li><li class="calibre15">“orange”: 2</li><li class="calibre15">“grape”: 3</li></ul></li>
</ol>
<h3 class="calibre8">One-hot encoding</h3>
<p class="calibre6">With <a id="_idIndexMarker479" class="calibre5 pcalibre1 pcalibre"/>the constructed vocabulary and assigned indices, you can now create one-hot encoded vectors for <a id="_idIndexMarker480" class="calibre5 pcalibre1 pcalibre"/>each document in the dataset. One simple approach to creating <a id="_idIndexMarker481" class="calibre5 pcalibre1 pcalibre"/>a one-hot encoded vector is to use <strong class="bold">bag-of-words</strong>. For each word in a document, find its corresponding index in the vocabulary and set the value at that index to 1 in the one-hot-encoded vector. If a word appears multiple times in the document, its corresponding value in the one-hot-encoded vector remains 1. All other values in the vector will be 0.</p>
<p class="calibre6">For example, using the vocabulary and indices mentioned previously, the one-hot encoded vectors for the documents would be as follows:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Document 1</strong>: [1, 1, 1, 0] (apple, banana, and orange are present)</li>
<li class="calibre15"><strong class="bold">Document 2</strong>: [1, 1, 0, 1] (apple, banana, and grape are present)</li>
</ul>
<p class="calibre6">Once we have the corresponding values for each document, we can create a feature matrix with one-hot-encoded vectors as rows, where each row represents a document and each column represents a word from the vocabulary. This matrix will be used as input <a id="_idIndexMarker482" class="calibre5 pcalibre1 pcalibre"/>for the text classification <a id="_idIndexMarker483" class="calibre5 pcalibre1 pcalibre"/>model. For example, in the previous example, the feature vectors for two documents are as follows:</p>
<table class="no-table-style" id="table001-3">
<colgroup class="calibre10">
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
</colgroup>
<thead class="calibre12">
<tr class="no-table-style1">
<td class="no-table-style2"/>
<td class="no-table-style2">
<p class="calibre6">Apple</p>
</td>
<td class="no-table-style2">
<p class="calibre6">Banana</p>
</td>
<td class="no-table-style2">
<p class="calibre6">Orange</p>
</td>
<td class="no-table-style2">
<p class="calibre6">Grape</p>
</td>
</tr>
</thead>
<tbody class="calibre13">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6">Document 1</p>
</td>
<td class="no-table-style2">
<p class="calibre6">1</p>
</td>
<td class="no-table-style2">
<p class="calibre6">1</p>
</td>
<td class="no-table-style2">
<p class="calibre6">1</p>
</td>
<td class="no-table-style2">
<p class="calibre6">0</p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6">Document 2</p>
</td>
<td class="no-table-style2">
<p class="calibre6">1</p>
</td>
<td class="no-table-style2">
<p class="calibre6">1</p>
</td>
<td class="no-table-style2">
<p class="calibre6">0</p>
</td>
<td class="no-table-style2">
<p class="calibre6">1</p>
</td>
</tr>
</tbody>
</table>
<p class="img---caption" lang="en-US" xml:lang="en-US">Table 5.1 – Sample one-hot-encoded vector for two documents</p>
<p class="calibre6">Please note that with text preprocessing, it helps to have a smaller vocabulary and it gives us better model performance. Besides that, if needed, we can perform feature selection methods (as explained previously in this book) on the extracted feature vectors to improve our model performance.</p>
<p class="calibre6">While creating a one-hot encoded vector from words is useful, sometimes, we need to consider the existence of two words beside each other. For example, “very good” and “not good” can have different meanings. To achieve this goal, we can use N-grams.</p>
<h3 class="calibre8"><a id="_idTextAnchor153" class="calibre5 pcalibre1 pcalibre"/>N-grams</h3>
<p class="calibre6">N-grams <a id="_idIndexMarker484" class="calibre5 pcalibre1 pcalibre"/>are a generalization of the bag-of-words model that takes into account the order of words by considering sequences of <em class="italic">n</em> consecutive words. An N-gram <a id="_idIndexMarker485" class="calibre5 pcalibre1 pcalibre"/>is a contiguous sequence of <em class="italic">n</em> items (typically words) from a given text. For example, in the sentence “The cat is on the mat,” the 2-grams (bigrams) would be “The cat,” “cat is,” “is on,” “on the,” and “the mat.”</p>
<p class="calibre6">Using N-grams can help capture local context and word relationships, which may improve the performance of the classifier. However, it also increases the dimensionality of the feature space, which can be computationally expensive<a id="_idTextAnchor154" class="calibre5 pcalibre1 pcalibre"/>.</p>
<h3 class="calibre8">Model training</h3>
<p class="calibre6">Train an ML model, such as logistic regression, SVM, or neural networks, on the feature matrix <a id="_idIndexMarker486" class="calibre5 pcalibre1 pcalibre"/>to learn the relationship between the one-hot encoded text features and the target labels. The model will <a id="_idIndexMarker487" class="calibre5 pcalibre1 pcalibre"/>learn to predict the class label based on the presence or absence of specific words in the document. Once we’ve decided on the training process, we need to perform the following tasks:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Model evaluation</strong>: Evaluate the performance of the model using appropriate evaluation <a id="_idIndexMarker488" class="calibre5 pcalibre1 pcalibre"/>metrics, such as accuracy, precision, recall, F1 score, or confusion matrix, and use techniques such as cross-validation to get a reliable estimate of the model’s performance on unseen data.</li>
<li class="calibre15"><strong class="bold">Model application</strong>: Apply the trained model to new, unseen text data. Preprocess <a id="_idIndexMarker489" class="calibre5 pcalibre1 pcalibre"/>and one-hot encode the new text data using the same vocabulary and use the model to predict the class labels.</li>
</ul>
<p class="calibre6">One potential limitation of using one-hot encoded vectors for text classification is that they do not capture word order, context, or semantic relationships between words. This can lead to suboptimal performance, especially in more complex classification tasks. More advanced techniques, such as word embeddings (for example, Word2Vec or GloVe) or deep learning models (for example, CNNs or RNNs), can provide better representations for text data in these cases.</p>
<p class="calibre6">In summary, text classification using one-hot-encoded vectors involves preprocessing text data, constructing a vocabulary, representing text data as one-hot encoded feature vectors, training an ML model on the feature vectors, and evaluating and applying the model to new text data. The one-hot encoded vector representation is a simple but sometimes limited approach to text classification, and more advanced techniques may be necessary for complex tasks.</p>
<p class="calibre6">So far, we’ve learned about classifying documents using N-grams. However, this approach has a drawback. There are a considerable number of words that occur in the documents frequently and do not add value to our models. To improve the models, text classification using TF-IDF has been prop<a id="_idTextAnchor155" class="calibre5 pcalibre1 pcalibre"/>osed.</p>
<h1 id="_idParaDest-100" class="calibre4"><a id="_idTextAnchor156" class="calibre5 pcalibre1 pcalibre"/>Text classification using TF-IDF</h1>
<p class="calibre6">One-hot encoded vector is a good approach to perform classification. However, one of its weaknesses <a id="_idIndexMarker490" class="calibre5 pcalibre1 pcalibre"/>is that it does not consider the importance of different words based on different documents. To solve this issue, using <strong class="bold">TF-IDF</strong> can be helpful.</p>
<p class="calibre6">TF-IDF <a id="_idIndexMarker491" class="calibre5 pcalibre1 pcalibre"/>is a numerical statistic that is used to measure the importance of a word in a document within a document collection. It helps reflect the relevance of words in a document, considering not <a id="_idIndexMarker492" class="calibre5 pcalibre1 pcalibre"/>only their frequency within the document but also their rarity across the entire document collection. The TF-IDF value of a word increases proportionally to its frequency in a document but is offset by the frequency of the word in the entire document collection.</p>
<p class="calibre6">Here’s a detailed explanation of the mathematical equations involved in calculating TF-IDF:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Term frequency (TF)</strong>: The TF of a word, <em class="italic">t</em>, in a document, <em class="italic">d</em>, represents the number of times the word occurs in the document, normalized by the total number of words in the document. The TF can be calculated using the following equation:</li>
</ul>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo&gt;′&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;′&lt;/mo&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;′&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo&gt;′&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;′&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo&gt;′&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/259.png" class="calibre258"/></p>
<p class="calibre6">The TF measures the importance of a word within a specific document.</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Inverse document frequency (IDF)</strong>: The IDF of a word, <em class="italic">t</em>, reflects the rarity of the word across the entire document collection. IDF can be calculated using the following equation:<p class="calibre6">IDF(t) = log ((Total number of documents in the collection) / (Number of documents containing word ′t′))</p><p class="calibre6">The logarithm is used to dampen the effect of the IDF component. If a word appears in many documents, its IDF value will be closer to 0, and if it appears in fewer documents, its IDF value will be higher.</p></li>
<li class="calibre15"><strong class="bold">TF-IDF computation</strong>: The TF-IDF value of a word, <em class="italic">t</em>, in a document, <em class="italic">d</em>, can be calculated by multiplying the TF of the word in the document with the IDF of the word across the document collection:</li>
</ul>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;I&lt;/mi&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;*&lt;/mi&gt;&lt;mi&gt;I&lt;/mi&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/260.png" class="calibre259"/></p>
<p class="calibre6">The resulting TF-IDF value represents the importance of a word in a document, taking into account both its frequency within the document and its rarity across the entire document collection. High TF-IDF values indicate words that are more significant in a particular document, whereas low TF-IDF values indicate words that are either common across all documents or rare within the specific document.</p>
<p class="calibre6">Let’s consider a simple example <a id="_idIndexMarker493" class="calibre5 pcalibre1 pcalibre"/>of classifying movie <a id="_idIndexMarker494" class="calibre5 pcalibre1 pcalibre"/>reviews into two categories: positive and negative. We have a small dataset with three movie reviews and their respective labels, as follows:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Document 1 (positive)</strong>: “I loved the movie. The acting was great and the story was captivating.”</li>
<li class="calibre15"><strong class="bold">Document 2 (negative)</strong>: “The movie was boring. I did not like the story, and the acting was terrible.”</li>
<li class="calibre15"><strong class="bold">Document 3 (positive)</strong>: “An amazing movie with a wonderful story and brilliant acting.”</li>
</ul>
<p class="calibre6">Now, we will use TF-IDF to classify a new, unseen movie review:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Document 4 (unknown)</strong>: “The story was interesting, and the acting was good.”</li>
</ul>
<p class="calibre6">Here are the steps that we need to perform to have the classifier predict the class of our document:</p>
<ol class="calibre16">
<li class="calibre15"><strong class="bold">Step 1 – preprocess the text data</strong>: Tokenize, lowercase, remove stop words, and apply stemming or lemmatization to the words in all documents:<ul class="calibre17"><li class="calibre15"><strong class="bold">Document 1</strong>: “love movi act great stori captiv”</li><li class="calibre15"><strong class="bold">Document 2</strong>: “movi bore not like stori act terribl”</li><li class="calibre15"><strong class="bold">Document 3</strong>: “amaz movi wonder stori brilliant act”</li><li class="calibre15"><strong class="bold">Document 4</strong>: “stori interest act good”</li></ul></li>
<li class="calibre15"><strong class="bold">Step 2 – create the vocabulary</strong>: Combine all unique words from the preprocessed documents:<p class="calibre6">Vocabulary: {“love”, “movi”, “act”, “great”, “stori”, “captiv”, “bore”, “not”, “like”, “terribl”, “amaz”, “wonder”, “brilliant”, “interest”, “good”}</p></li>
<li class="calibre15"><strong class="bold">Step 3 – calculate the TF and IDF values</strong>: Compute the TF and IDF for each word <a id="_idIndexMarker495" class="calibre5 pcalibre1 pcalibre"/>in each document.<p class="calibre6">For <a id="_idIndexMarker496" class="calibre5 pcalibre1 pcalibre"/>example, for the word “stori” in Document 4, we have the following:</p></li>
</ol>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&quot;&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&quot;&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mn&gt;4&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;mn&gt;4&lt;/mn&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;0.25&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/261.png" class="calibre260"/></p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;I&lt;/mi&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&quot;&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&quot;&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;4&lt;/mn&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;≈&lt;/mo&gt;&lt;mn&gt;0.287&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/262.png" class="calibre261"/></p>
<p class="calibre6">4.<strong class="bold">	Step 4 – compute the TF-IDF values</strong>: Calculate the TF-IDF values for each word in each document.</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;I&lt;/mi&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&quot;&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&quot;&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mn&gt;4&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;0.25&lt;/mn&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;*&lt;/mi&gt;&lt;mn&gt;0.287&lt;/mn&gt;&lt;mo&gt;≈&lt;/mo&gt;&lt;mn&gt;0.0717&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/263.png" class="calibre262"/></p>
<p class="calibre6">Repeat this process for all words in all documents and create a feature matrix with the TF-IDF values.</p>
<p class="calibre6">5.<strong class="bold">	Step 5 – train a classifier</strong>: Split the dataset into a training set (documents 1 to 3) and a test set (document 4). Train a classifier, such as logistic regression or SVM, using the training set’s TF-IDF feature matrix and their corresponding labels (positive or negative).</p>
<p class="calibre6">6.<strong class="bold">	Step 6 – predict the class label</strong>: Preprocess and compute the TF-IDF values for the new movie review (document 4) using the same vocabulary. Use the trained classifier to predict the class label for document 4 based on its TF-IDF feature vector.</p>
<p class="calibre6">For example, if the classifier predicts a positive label for document 4, the classification result would be as follows:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Document 4 (</strong><strong class="bold">Predicted)</strong>: “Positive”</li>
</ul>
<p class="calibre6">By following these steps, you can use the TF-IDF representation to classify text documents <a id="_idIndexMarker497" class="calibre5 pcalibre1 pcalibre"/>based on the importance of words in the <a id="_idIndexMarker498" class="calibre5 pcalibre1 pcalibre"/>documents relative to the entire document collection.</p>
<p class="calibre6">In summary, the TF-IDF value is calculated using the mathematical equations for TF and IDF. It serves as a measure of the importance of a word in a document relative to the entire document collection, considering both the frequency of the word within the document and its rarity across <a id="_idTextAnchor157" class="calibre5 pcalibre1 pcalibre"/>all documents.</p>
<h1 id="_idParaDest-101" class="calibre4"><a id="_idTextAnchor158" class="calibre5 pcalibre1 pcalibre"/>Text classification using Word2Vec</h1>
<p class="calibre6">One of the <a id="_idIndexMarker499" class="calibre5 pcalibre1 pcalibre"/>methods to perform text classification <a id="_idIndexMarker500" class="calibre5 pcalibre1 pcalibre"/>is to convert the words into embedding vectors so that you can use those vectors for classification. Word2Vec is a well-known method to pe<a id="_idTextAnchor159" class="calibre5 pcalibre1 pcalibre"/>rform this task.</p>
<h2 id="_idParaDest-102" class="calibre7"><a id="_idTextAnchor160" class="calibre5 pcalibre1 pcalibre"/>Word2Vec</h2>
<p class="calibre6">Word2Vec <a id="_idIndexMarker501" class="calibre5 pcalibre1 pcalibre"/>is a group of neural network-based models that are used to create word embeddings, which are dense vector representations of words in a continuous vector space. These embeddings capture the semantic meaning and relationships between words based on the context in which they appear in the text. Word2Vec has two main architectures. As mentioned previously, the two main architectures that were designed to learn word embeddings are <strong class="bold">CBOW</strong> and <strong class="bold">skip-gram</strong>. Both architectures are designed to learn word embeddings by predicting words based on their surrounding context:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">CBOW</strong>: The CBOW <a id="_idIndexMarker502" class="calibre5 pcalibre1 pcalibre"/>architecture aims to predict the target word given its surrounding context words. It takes the average of the context word embeddings as input and predicts the target word. CBOW is faster to train and works well with smaller datasets but may be less accurate for infrequent words.<p class="calibre6">In the CBOW model, the objective is to maximize the average log probability of observing the target word given the context words:</p></li>
</ul>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;C&lt;/mi&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/mfrac&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/munder&gt;&lt;mrow&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/264.png" class="calibre263"/></p>
<p class="calibre6">Here, T is the total number of words in the text, and P(target | context) is the probability of observing the target word given the context words, which is calculated using the softmax function:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;msup&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;v&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msubsup&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;v&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;msup&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;v&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msubsup&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;v&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/265.png" class="calibre264"/></p>
<p class="calibre6">Here, <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="img/266.png" class="calibre265"/> is the output vector (word embedding) of the target word, <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/267.png" class="calibre266"/> is the average input vector (context word embedding) of the context words, and the sum in the denominator runs over all words in the vocabulary.</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Skip-gram</strong>: The skip-gram <a id="_idIndexMarker503" class="calibre5 pcalibre1 pcalibre"/>architecture aims to predict the surrounding context words given the target word. It takes the target word embedding as input and predicts the context words. Skip-gram works well with larger datasets and can capture the meaning of infrequent words more accurately, but it may be slower to train compared to CBOW.<p class="calibre6">In the skip-gram model, the objective is to maximize the average log probability of observing the context words given the target word:</p></li>
</ul>
<p class="calibre6"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;S&lt;/mml:mi&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;G&lt;/mml:mi&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:munder&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;l&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;o&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;g&lt;/mml:mi&gt;&lt;mml:mo&gt;⁡&lt;/mml:mo&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" src="img/268.png" class="calibre267"/></p>
<p class="calibre6">Here, T is the total <a id="_idIndexMarker504" class="calibre5 pcalibre1 pcalibre"/>number of words in the text, and P(context | target) is the probability of observing the context words given the target word, which is calculated using the softmax function:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;msup&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;v&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msubsup&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;v&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;msup&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;v&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msubsup&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;v&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/269.png" class="calibre268"/></p>
<p class="calibre6">Here, <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="img/270.png" class="calibre269"/>is the output vector (context word embedding) of the context word, <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/271.png" class="calibre270"/>is the input vector (word embedding) of the target word, and the sum in the denominator runs over all words in the vocabulary.</p>
<p class="calibre6">The training process for both CBOW and skip-gram involves iterating through the text and updating <a id="_idIndexMarker505" class="calibre5 pcalibre1 pcalibre"/>the input and output weight matrices using <strong class="bold">stochastic gradient descent</strong> (<strong class="bold">SGD</strong>) and backpropagation to minimize the difference between the predicted words and the actual words. The learned input weight matrix contains the word embeddings for each wo<a id="_idTextAnchor161" class="calibre5 pcalibre1 pcalibre"/>rd in the vocabulary.</p>
<h3 class="calibre8">Text classification using Word2Vec</h3>
<p class="calibre6">Text classification <a id="_idIndexMarker506" class="calibre5 pcalibre1 pcalibre"/>using Word2Vec involves creating <a id="_idIndexMarker507" class="calibre5 pcalibre1 pcalibre"/>word embeddings using the Word2Vec algorithm and then training an ML model to classify text based on these embeddings. The following steps outline the process in detail, including the mathematical aspects:</p>
<ol class="calibre16">
<li class="calibre15"><strong class="bold">Text preprocessing</strong>: Clean and preprocess the text data by tokenizing, lowercasing, removing stop words, and stemming or lemmatizing the words.</li>
<li class="calibre15"><strong class="bold">Train the Word2Vec model</strong>: Train a Word2Vec model (either CBOW or Skip-Gram) on the preprocessed text data to create word embeddings. The Word2Vec algorithm learns to predict a target word based on its context (CBOW) or predict the context words based on a target word (skip-gram). The training <a id="_idIndexMarker508" class="calibre5 pcalibre1 pcalibre"/>objective is to maximize the average <a id="_idIndexMarker509" class="calibre5 pcalibre1 pcalibre"/>log probability of observing the context words given the target word:</li>
</ol>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/mfrac&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/munder&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;l&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;o&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;g&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/272.png" class="calibre271"/></p>
<p class="calibre6">Here, <em class="italic">T</em> is the total number of words in the text, and <em class="italic">P(context | target)</em> is the probability of observing the context words given the target word, which is calculated using the softmax function:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;msup&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;v&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msubsup&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;v&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;msup&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;v&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msubsup&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;v&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/273.png" class="calibre272"/></p>
<p class="calibre6">Here, <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="img/274.png" class="calibre273"/>is the output vector (context word embedding) of the context word, <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/275.png" class="calibre274"/> is the input vector (word embedding) of the target word, and the sum in the denominator runs over all words in the vocabulary.</p>
<p class="calibre6">3.<strong class="bold">	Create document embeddings</strong>: For each document in the dataset, calculate the document embedding by averaging the word embeddings of the words in the document:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/mfrac&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/munder&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;W&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;o&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;r&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;d&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;E&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;m&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;b&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;e&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;d&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;d&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;i&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;n&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;g&lt;/mi&gt;&lt;/mrow&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/276.png" class="calibre275"/></p>
<p class="calibre6">Here, N is the number of words in the document, and the sum runs over all words in the document. Please note that based on our experience, this approach for text classification using Word2Vec is only useful when the document’s length is short. If you have longer documents or there are opposite words in <a id="_idIndexMarker510" class="calibre5 pcalibre1 pcalibre"/>the document, this approach won’t perform well. An alternative solution is to use Word2Vec and CNN together to <a id="_idIndexMarker511" class="calibre5 pcalibre1 pcalibre"/>fetch the word embeddings and then feed those embeddings as input of the CNN.</p>
<p class="calibre6">4.<strong class="bold">	Model training</strong>: Use the document embeddings as features to train an ML model, such as logistic regression, SVM, or a neural network, for text classification. The model learns to predict the class label based on the document embeddings.</p>
<p class="calibre6">5.<strong class="bold">	Model evaluation</strong>: Evaluate the performance of the model using appropriate evaluation metrics, such as accuracy, precision, recall, F1 score, or confusion matrix, and use techniques such as cross-validation to get a reliable estimate of the model’s performance on unseen data.</p>
<p class="calibre6">6.<strong class="bold">	Model application</strong>: Apply the trained model to new, unseen text data. Preprocess and compute the document embeddings for the new text data using the same Word2Vec model and vocabulary, and use the model to predict the class labels.</p>
<p class="calibre6">In summary, text classification using Word2Vec involves creating word embeddings with the Word2Vec algorithm, averaging these embeddings to create document embeddings, and training an ML model to classify text based on these document embeddings. The Word2Vec algorithm learns word embeddings by maximizing the average log probability of observing context words given a target word, capturing the semantic relationships be<a id="_idTextAnchor162" class="calibre5 pcalibre1 pcalibre"/>tween words in the process.</p>
<h2 id="_idParaDest-103" class="calibre7"><a id="_idTextAnchor163" class="calibre5 pcalibre1 pcalibre"/>Model evaluation</h2>
<p class="calibre6">Evaluating the performance of text classification models is crucial to ensure that they meet <a id="_idIndexMarker512" class="calibre5 pcalibre1 pcalibre"/>the desired level of accuracy and generalizability. Several metrics and techniques are commonly used to evaluate text <a id="_idIndexMarker513" class="calibre5 pcalibre1 pcalibre"/>classification models, including accuracy, precision, recall, F1 score, and confusion matrix. Let’s discuss <a id="_idTextAnchor164" class="calibre5 pcalibre1 pcalibre"/>each of these in more detail:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Accuracy</strong>: Accuracy <a id="_idIndexMarker514" class="calibre5 pcalibre1 pcalibre"/>is the most straightforward metric for classification tasks. It measures the number of correctly classified records out of all classified records. It is defined as follows:</li>
</ul>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/277.png" class="calibre276"/></p>
<p class="calibre6">While accuracy is easy to understand, it may not be the best metric for imbalanced datasets, where the majority class ca<a id="_idTextAnchor165" class="calibre5 pcalibre1 pcalibre"/>n dominate the metric’s value.</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Precision</strong>: Precision <a id="_idIndexMarker515" class="calibre5 pcalibre1 pcalibre"/>gauges the ratio of correctly identified positive instances to the total instances predicted as positive by the model. It <a id="_idIndexMarker516" class="calibre5 pcalibre1 pcalibre"/>is also referred to as <strong class="bold">positive predictive value</strong> (<strong class="bold">PPV</strong>). Precision proves valuable in scenarios where the expense associated with false positives is significant. Precision is defined as follows:</li>
</ul>
<p class="calibre6">[[OMM<a id="_idTextAnchor166" class="calibre5 pcalibre1 pcalibre"/>L-EQ-21D]]</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Recall</strong>: Recall, also <a id="_idIndexMarker517" class="calibre5 pcalibre1 pcalibre"/>recognized as sensitivity or the <strong class="bold">true positive rate</strong> (<strong class="bold">TPR</strong>), assesses the ratio of correctly identified positive instances <a id="_idIndexMarker518" class="calibre5 pcalibre1 pcalibre"/>among the total actual positive instances. Recall is useful when the cost of false negatives is high. Mathematically, it is defined as follows:</li>
</ul>
<p class="calibre6">[[O<a id="_idTextAnchor167" class="calibre5 pcalibre1 pcalibre"/>MML-EQ-22D]]</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">F1 score</strong>: The F1 score, derived as the harmonic mean of precision and recall, integrates <a id="_idIndexMarker519" class="calibre5 pcalibre1 pcalibre"/>both metrics into a unified value. It is an important metric in the context of imbalanced datasets as it considers both false positives and false negatives. Spanning from 0 to 1, with 1 representing the optimal outcome, the F1 score is mathematically expressed as follows:</li>
</ul>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/278.png" class="calibre277"/></p>
<p class="calibre6">When dealing with multi-class classification, we have F1 micro and F1 macro. F1 micro and F1 macro are two ways to compute the F1 score for multi-class or multi-label classification problems. They aggregate precision and recall differently, leading to different interpretations of the classifier’s performance.<a id="_idTextAnchor168" class="calibre5 pcalibre1 pcalibre"/> Let’s discuss each in more detail:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">F1 macro</strong>: F1 macro computes the F1 score for each class independently and then takes <a id="_idIndexMarker520" class="calibre5 pcalibre1 pcalibre"/>the average of those values. This approach treats each class as equally important and does not consider the class imbalance. Mathematically, F1 macro is defined as follows:</li>
</ul>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mfrac&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/munder&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/279.png" class="calibre278"/></p>
<p class="calibre6">Here, <em class="italic">n</em> is the number of classes, and <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/280.png" class="calibre279"/> is the F1 score for the i-th class.</p>
<p class="calibre6">F1 macro is particularly useful when you want to evaluate the performance of a classifier across all classes without giving more weight to the majority class. However, it may <a id="_idIndexMarker521" class="calibre5 pcalibre1 pcalibre"/>not be suitable when the class distribution is highly imbalanced as it can provide an overly optimistic<a id="_idTextAnchor169" class="calibre5 pcalibre1 pcalibre"/> estimate of the model’s performance.</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">F1 micro</strong>: F1 micro, on the other hand, aggregates the contributions of all classes to <a id="_idIndexMarker522" class="calibre5 pcalibre1 pcalibre"/>compute the F1 score. It does this by calculating the global precision and recall values across all classes and then computing the F1 score based on these global values. F1 micro takes class imbalance into account as it considers the number of instances in each class. Mathematically, F1 micro is defined as follows:<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/281.png" class="calibre280"/></p><p class="calibre6">Here, global precision and global recall are calculated as follows:</p></li>
</ul>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/282.png" class="calibre281"/></p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/283.png" class="calibre282"/></p>
<p class="calibre6">F1 micro is useful when you want to evaluate the overall performance of a classifier considering the class distribution, especially when dealing with imbalanced datasets.</p>
<p class="calibre6">In summary, F1 macro and F1 micro are two ways to compute the F1 score for multi-class or multi-label classification problems. F1 macro treats each class as equally important, regardless of the class distribution, while F1 micro takes class imbalance into account by considering the number of instances in each class. The choice between F1 macro and F1 micro depends on the specific problem and whether class imbalan<a id="_idTextAnchor170" class="calibre5 pcalibre1 pcalibre"/>ce is an important factor to consider.</p>
<h3 class="calibre8">Confusion matrix</h3>
<p class="calibre6">A confusion matrix <a id="_idIndexMarker523" class="calibre5 pcalibre1 pcalibre"/>serves as a tabular representation, showcasing the count of true positive, true negative, false positive, and false negative predictions made by a classification model. This matrix offers a nuanced perspective on the model’s efficacy, enabling a thorough comprehension of both its strengths and weaknesses.</p>
<p class="calibre6">For a binary classification problem, the confusion matrix is arranged as follows:</p>
<table class="no-table-style" id="table002-1">
<colgroup class="calibre10">
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
</colgroup>
<thead class="calibre12">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><strong class="bold">Actual/Predicted</strong></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold">(</strong><strong class="bold">Predicted) Positive</strong></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold">(</strong><strong class="bold">Predicted) Negative</strong></p>
</td>
</tr>
</thead>
<tbody class="calibre13">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6">(Actual) Positive</p>
</td>
<td class="no-table-style2">
<p class="calibre6">True Positive</p>
</td>
<td class="no-table-style2">
<p class="calibre6">False Negative</p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6">(Actual) Negative</p>
</td>
<td class="no-table-style2">
<p class="calibre6">False Positive</p>
</td>
<td class="no-table-style2">
<p class="calibre6">True Negative</p>
</td>
</tr>
</tbody>
</table>
<p class="img---caption" lang="en-US" xml:lang="en-US">Table 5.2 – Confusion matrix – general view</p>
<p class="calibre6">For multi-class classification problems, the confusion matrix is extended to include the true and predicted counts for each class. The diagonal elements represent the correctly classified instances, while the off-diagonal elements represent misclassifications.</p>
<p class="calibre6">In summary, evaluating text classification models involves using various metrics and techniques, such as accuracy, precision, recall, F1 score, and the confusion matrix. Selecting the appropriate evaluation metrics depends on the specific problem, dataset characteristics, and the trade-offs between false positives and false negatives. Evaluating a model using multiple metrics can provide a more comprehensive understanding of its performance and help guide further improvements.</p>
<h2 id="_idParaDest-104" class="calibre7"><a id="_idTextAnchor171" class="calibre5 pcalibre1 pcalibre"/>Overfitting and underfitting</h2>
<p class="calibre6">Overfitting and underfitting are two common issues that arise during the training of ML models, including text classification models. They both relate to how well a model generalizes to new, unseen data. This section will explain overfitting and underfitting, when they happen, and how to prevent them.</p>
<h3 class="calibre8"><a id="_idTextAnchor172" class="calibre5 pcalibre1 pcalibre"/>Overfitting</h3>
<p class="calibre6">Overfitting arises when a model excessively tailors itself to the intricacies of the training data. In this case, the model captures noise and random fluctuations rather than discerning the fundamental patterns. Consequently, although the model may exhibit high performance on the training data, its effectiveness diminishes when applied to unseen data, such as a validation or test set.</p>
<p class="calibre6">To avoid overfitting in text classification, consider the following strategies:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><a id="_idTextAnchor173" class="calibre5 pcalibre1 pcalibre"/>Regularization</strong>: Introduce regularization techniques, such as <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;script&quot;&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/284.png" class="calibre283"/> or <img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;script&quot;&gt;l&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/285.png" class="calibre284"/>L2 regularization, which add a penalty to the loss function, discouraging overly complex models.</li>
<li class="calibre15"><strong class="bold">Early stopping</strong>: In this approach, we monitor the performance of the model on the validation set, and stop the training process as soon as the performance on the validation set starts getting worse, even though the model performance on the training set is getting better. It helps us to prevent overfitting.</li>
<li class="calibre15"><strong class="bold"><a id="_idTextAnchor174" class="calibre5 pcalibre1 pcalibre"/>Feature selection</strong>: Reduce the number of features used for classification by selecting the most informative features or using dimensionality reduction techniques such as PCA or LSA.</li>
<li class="calibre15"><strong class="bold">Ensemble methods</strong>: Combine multiple models, such as bagging or boosting, to reduce overfitting by averaging their predictions.</li>
<li class="calibre15"><strong class="bold"><a id="_idTextAnchor175" class="calibre5 pcalibre1 pcalibre"/>Cross-validation</strong>: Use k-fold cross-validation to get a more reliable estimate of model performance on unseen data and fine-tune model hyperparameters accordingly.</li>
</ul>
<p class="calibre6">Next, we’ll cover underfitting.</p>
<h3 class="calibre8"><a id="_idTextAnchor176" class="calibre5 pcalibre1 pcalibre"/>Underfitting</h3>
<p class="calibre6">Underfitting happens when a model is too simple and fails to capture the underlying patterns in the data. Consequently, the model performance is low on both training and test data. The model is too simple to represent the complexity of the data and can’t generalize well.</p>
<p class="calibre6">To avoid underfitting in text classification, consider the following strategies:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><a id="_idTextAnchor177" class="calibre5 pcalibre1 pcalibre"/>Increase model complexity</strong>: Use a more complex model, such as a deeper neural network, to capture more intricate patterns in the data.</li>
<li class="calibre15"><strong class="bold">Feature engineering</strong>: Create new, informative features that help the model better understand the underlying patterns in the text data, such as adding N-grams or using word embeddings.</li>
<li class="calibre15"><strong class="bold"><a id="_idTextAnchor178" class="calibre5 pcalibre1 pcalibre"/>Hyperparameter tuning</strong>: Optimize model hyperparameters, such as the learning rate, number of layers, or number of hidden units, to improve the model’s ability to learn from the data. We’ll explain hyperparameter tuning and the different methods to perform this task in the next section.</li>
<li class="calibre15"><strong class="bold">Increase training data</strong>: If possible, collect more labeled data for training, as more examples can help the model learn the underlying patterns better.</li>
<li class="calibre15"><strong class="bold"><a id="_idTextAnchor179" class="calibre5 pcalibre1 pcalibre"/>Reduce regularization</strong>: If the model is heavily regularized, consider reducing the regularization strength, allowing the model to become more complex and better fit the data.</li>
</ul>
<p class="calibre6">In summary, overfitting and underfitting are two common issues in text classification that affect a model’s ability to generalize to new data. Avoiding these issues involves balancing model complexity, using appropriate features, tuning hyperparameters, employing regularization, and monitoring model performance on a validation set. By addressing overfitting and underfitting, you can improve the performance and generalizability of your text classification models.</p>
<h2 id="_idParaDest-105" class="calibre7"><a id="_idTextAnchor180" class="calibre5 pcalibre1 pcalibre"/>Hyperparameter tuning</h2>
<p class="calibre6">An important step in building an effective classification model is hyperparameter tuning. Hyperparameters are the model parameters that are defined before training; they will not change during training. These parameters determine the model architecture and behavior. Some of the hyperparameters that can be used are the learning rate and the number of iterations. They can significantly impact the model’s performance and generalizability.</p>
<p class="calibre6">The process of hyperparameter tuning in text classification involves the following steps:</p>
<ol class="calibre16">
<li class="calibre15"><strong class="bold"><a id="_idTextAnchor181" class="calibre5 pcalibre1 pcalibre"/>Define the hyperparameters and their search space</strong>: Identify the hyperparameters you want to optimize and specify the range of possible values for each of them. Common hyperparameters in text classification include the learning rate, number of layers, number of hidden units, dropout rate, regularization strength, and feature extraction parameters such as N-grams or vocabulary size.</li>
<li class="calibre15"><strong class="bold"><a id="_idTextAnchor182" class="calibre5 pcalibre1 pcalibre"/>Choose a search strategy</strong>: Select a method to explore the hyperparameter search space, such as grid search, random search, or Bayesian optimization. Grid search systematically evaluates all combinations of hyperparameter values, while random search samples random combinations within the search space. Bayesian optimization uses a probabilistic model to guide the search, balancing exploration and exploitation based on the model’s predictions.</li>
<li class="calibre15"><strong class="bold"><a id="_idTextAnchor183" class="calibre5 pcalibre1 pcalibre"/>Choose an evaluation metric and method</strong>: Select a performance metric that best represents the goals of your text classification task, such as accuracy, precision, recall, F1 score, or area under the ROC curve. Also, choose an evaluation method, such as k-fold cross-validation, to get a reliable estimate of the model’s performance on unseen data.</li>
<li class="calibre15"><strong class="bold"><a id="_idTextAnchor184" class="calibre5 pcalibre1 pcalibre"/>Perform the search</strong>: For each combination of hyperparameter values, train a model on the training data, and evaluate its performance using the chosen metric and evaluation method. Keep track of the best-performing hyperparameter combination.</li>
<li class="calibre15"><strong class="bold"><a id="_idTextAnchor185" class="calibre5 pcalibre1 pcalibre"/>Select the best hyperparameters</strong>: After the search is complete, select the hyperparameter combination that yields the best performance on the evaluation metric. Retrain the model using these hyperparameters on the entire training set.</li>
<li class="calibre15"><strong class="bold"><a id="_idTextAnchor186" class="calibre5 pcalibre1 pcalibre"/>Evaluate on the test set</strong>: Assess the performance of the final model with the optimized hyperparameters on a held-out test set to get an unbiased estimate of its generalizability.</li>
</ol>
<p class="calibre6">Hyperparameter tuning affects the performance of the model by finding the optimal combination of parameters that results in the best model performance on the chosen evaluation metric. Tuning hyperparameters can help address issues such as overfitting and underfitting, balance model complexity, and improve the model’s ability to generalize to new data.</p>
<p class="calibre6">Hyperparameter tuning is a crucial process in text classification that involves searching for the optimal combination of model parameters to maximize performance on a chosen evaluation metric. By carefully tuning hyperparameters, you can improve the performance and generalizability of your text classification models.</p>
<h2 id="_idParaDest-106" class="calibre7"><a id="_idTextAnchor187" class="calibre5 pcalibre1 pcalibre"/>Additional topics in applied text classification</h2>
<p class="calibre6">In the real world, applying text classification involves various practical considerations and challenges that arise from the nature of real-world data and problem requirements. Some common issues include dealing with imbalanced datasets, handling noisy data, and choosing appropriate evaluation metrics.</p>
<p class="calibre6">Let’s discuss each of these in more detail.</p>
<h3 class="calibre8"><a id="_idTextAnchor188" class="calibre5 pcalibre1 pcalibre"/>Dealing with imbalanced datasets</h3>
<p class="calibre6">Text classification tasks often encounter imbalanced datasets, wherein certain classes boast a notably higher number of instances compared to others. This imbalance can result in models that are skewed, excelling in predicting the majority class while faltering in accurately classifying the minority class. To handle imbalanced datasets, consider the following strategies:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><a id="_idTextAnchor189" class="calibre5 pcalibre1 pcalibre"/>Resampling</strong>: You can oversample the minority class, undersample the majority class, or use a combination of both to balance the class distribution.</li>
<li class="calibre15"><strong class="bold">Weighted loss function</strong>: Assign higher weights to the minority class in the loss function, making the model more sensitive to misclassifications in the minority class.</li>
<li class="calibre15"><strong class="bold"><a id="_idTextAnchor190" class="calibre5 pcalibre1 pcalibre"/>Ensemble methods</strong>: Use ensemble techniques such as bagging or boosting with a focus on the minority class. For example, you can use random under-sampling with bagging or cost-sensitive boosting algorithms.</li>
<li class="calibre15"><strong class="bold"><a id="_idTextAnchor191" class="calibre5 pcalibre1 pcalibre"/>Evaluation metrics</strong>: Choose evaluation metrics that are less sensitive to class imbalance, such as precision, recall, F1 score, or area under the ROC curve, instead of accuracy.</li>
<li class="calibre15"><strong class="bold"><a id="_idTextAnchor192" class="calibre5 pcalibre1 pcalibre"/>Handling noisy data</strong>: Real-world text data is often noisy, containing misspellings, grammatical errors, or irrelevant information. Noisy data can negatively impact the performance of text classification models.</li>
</ul>
<p class="calibre6">To handle noisy data, consider the following strategies:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><a id="_idTextAnchor193" class="calibre5 pcalibre1 pcalibre"/>Preprocessing</strong>: Clean the text data by correcting misspellings, removing special characters, expanding contractions, and converting text into lowercase</li>
<li class="calibre15"><strong class="bold">Stopword removal</strong>: Remove common words that do not carry much meaning, such as “the,” “is,” “and,” and so on</li>
<li class="calibre15"><strong class="bold"><a id="_idTextAnchor194" class="calibre5 pcalibre1 pcalibre"/>Stemming or lemmatization</strong>: Reduce words to their root form to minimize the impact of morphological variations</li>
<li class="calibre15"><strong class="bold">Feature selection</strong>: Use techniques such as chi-square or mutual information to select the most informative features, reducing the impact of noisy or irrelevant features</li>
</ul>
<p class="calibre6">Whether we’re working on imbalanced data or not, we always need to evaluate our model, and choosing the right metric to evaluate our model is important. Next, we’ll explain how to select the best metric to evaluate our model.</p>
<h3 class="calibre8"><a id="_idTextAnchor195" class="calibre5 pcalibre1 pcalibre"/>Choosing appropriate evaluation metrics</h3>
<p class="calibre6">Selecting the right evaluation metrics is crucial for measuring the performance of your text classification model and guiding model improvements.</p>
<p class="calibre6">Consider the following when choosing evaluation metrics:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Problem requirements</strong>: Choose metrics that align with the specific goals of your text classification task, such as minimizing false positives or false negatives</li>
<li class="calibre15"><strong class="bold">Class imbalance</strong>: For imbalanced datasets, use metrics that account for class imbalance, such as precision, recall, F1 score, or area under the ROC curve, instead of accuracy</li>
<li class="calibre15"><strong class="bold">Multi-class or multi-label problems</strong>: For multi-class or multi-label classification tasks, use metrics such as micro- and macro-averaged F1 scores, which aggregate precision and recall differently based on the problem’s requirements</li>
</ul>
<p class="calibre6">In summary, practical considerations in text classification include dealing with imbalanced datasets, handling noisy data, and choosing appropriate evaluation metrics. Addressing these issues can help improve the performance and generalizability of your text classification models and ensure that they meet the specific requirements of your problem.</p>
<h1 id="_idParaDest-107" class="calibre4"><a id="_idTextAnchor196" class="calibre5 pcalibre1 pcalibre"/>Topic modeling – a particular use case of unsupervised text classification</h1>
<p class="calibre6">Topic modeling is an unsupervised ML technique that’s used to discover abstract topics or themes within a large collection of documents. It assumes that each document can be represented as a mixture of topics, and each topic is represented as a distribution over words. The goal of topic modeling is to find the underlying topics and their word distributions, as well as the topic proportions for each document.</p>
<p class="calibre6">There are several topic modeling algorithms, but one of the most popular and widely used is LDA. We will discuss LDA in detail, including its mathematical formulation.</p>
</div>


<div><h2 id="_idParaDest-108" class="calibre7"><a id="_idTextAnchor197" class="calibre5 pcalibre1 pcalibre"/>LDA</h2>
<p class="calibre6">LDA is a generative probabilistic model that assumes the following generative process for each document:</p>
<ol class="calibre16">
<li class="calibre15">Choose the number of words in the document.</li>
<li class="calibre15">Choose a topic distribution (<em class="italic">θ</em>) for the document from a Dirichlet distribution with parameter α.</li>
<li class="calibre15">For each word in the document, do the following:<ol class="calibre285"><li class="upper-roman">Choose a topic (<em class="italic">z</em>) from the topic distribution (<em class="italic">θ</em>).</li><li class="upper-roman">Choose a word (<em class="italic">w</em>) from the word distribution of the chosen topic (<em class="italic">φ</em>), which is a distribution over words for that topic, drawn from a Dirichlet distribution with parameter <em class="italic">β</em>.</li></ol></li>
</ol>
<p class="calibre6">The generative process is a theoretical model used by LDA to reverse-engineer the original documents from presumed topics.</p>
<p class="calibre6">LDA aims to find the topic-word distributions (<em class="italic">φ</em>) and document-topic distributions (<em class="italic">θ</em>) that best explain the observed documents.</p>
<p class="calibre6">Mathematically, LDA can be described using the following notation:</p>
<ul class="calibre14">
<li class="calibre15"><em class="italic">M</em>: Number of documents</li>
<li class="calibre15"><em class="italic">N</em>: Number of words in a document</li>
<li class="calibre15"><em class="italic">K</em>: Number of topics</li>
<li class="calibre15"><em class="italic">α</em>: Dirichlet before document-topic distribution, it affects the sparsity of topics within documents</li>
<li class="calibre15"><em class="italic">β</em>: Dirichlet before topic-word distribution, it affects the sparsity of words within topics</li>
<li class="calibre15"><em class="italic">θ</em>: Document-topic distributions (M × K matrix)</li>
<li class="calibre15"><em class="italic">φ</em>: Topic-word distributions (K × V matrix, where V is the vocabulary size)</li>
<li class="calibre15"><em class="italic">z</em>: Topic assignments for each word in each document (M × N matrix)</li>
<li class="calibre15"><em class="italic">w</em>: Observed words in the documents (M × N matrix)</li>
</ul>
<p class="calibre6">The joint probability of the topic assignments (<em class="italic">z</em>) and words (<em class="italic">w</em>) in the documents, given the topic-word distributions (<em class="italic">φ</em>) and document-topic distributions (<em class="italic">θ</em>), can be written as follows:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;φ&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;∏&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;/munderover&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;∏&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;/munderover&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi&gt;φ&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/286.png" class="calibre286"/></p>
<p class="calibre6">The objective of LDA is to maximize the likelihood of the observed words given the Dirichlet priors α and β:</p>
<p class="calibre6"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;α&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∫&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∫&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;φ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;α&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;φ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;φ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" src="img/287.png" class="calibre287"/></p>
<p class="calibre6">However, computing the likelihood directly is intractable due to the integration over the latent variables θ and φ. Therefore, LDA uses approximate inference algorithms, such as Gibbs sampling or variational inference, to estimate the posterior distributions <em class="italic">P</em>(θ | w, α, β) and <em class="italic">P</em>(φ | w, α, β).</p>
<p class="calibre6">Once the posterior distributions have been estimated, we can obtain the document-topic distributions (θ) and topic-word distributions (φ), which can be used to analyze the discovered topics and their word distributions, as well as the topic proportions for each document.</p>
<p class="calibre6">Let’s consider a simple example of topic modeling.</p>
<p class="calibre6">Suppose we have a collection of three documents:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Document 1</strong>: “I love playing football with my friends.”</li>
<li class="calibre15"><strong class="bold">Document 2</strong>: “The football match was intense and exciting.”</li>
<li class="calibre15"><strong class="bold">Document 3</strong>: “My new laptop has an amazing battery life and performance.”</li>
</ul>
<p class="calibre6">We want to discover two topics (K = 2) in this document collection. Here are the steps that we need to perform:</p>
<ol class="calibre16">
<li class="calibre15"><strong class="bold">Preprocessing</strong>: First, we need to preprocess the text data, which typically involves tokenization, stopword removal, and stemming/lemmatization (which was explained previously in this chapter). In this example, we will skip these steps for simplicity and assume our documents are already preprocessed.</li>
<li class="calibre15"><strong class="bold">Initialization</strong>: Choose the initial values for the Dirichlet priors, α and β. For example, we can set α = [1, 1] and β = [0.1, 0.1, ..., 0.1] (assuming a V-dimensional vector with 0.1 for each word in the vocabulary).</li>
<li class="calibre15"><strong class="bold">Random topic assignments</strong>: Randomly assign a topic (1 or 2) to each word in each document.</li>
<li class="calibre15"><strong class="bold">Iterative inference (for example, Gibbs sampling or variational inference)</strong>: Iteratively update the topic assignments and the topic-word and document-topic distributions (φ and θ) until convergence or a fixed number of iterations. This process refines the assignments and distributions, ultimately revealing the underlying topic structure.</li>
<li class="calibre15"><strong class="bold">Interpretation</strong>: After the algorithm converges or reaches the maximum number of iterations, we can interpret the discovered topics by looking at the most probable words for each topic and the most probable topics for each document.<p class="calibre6">For our example, LDA might discover the following topics:</p><ul class="calibre17"><li class="calibre15"><strong class="bold">Topic 1</strong>: {“football”, “playing”, “friends”, “match”, “intense”, “exciting”}</li><li class="calibre15"><strong class="bold">Topic 2</strong>: {“laptop”, “battery”, “life”, “performance”}</li></ul><p class="calibre6">With these topics, the document-topic distribution (θ) might look like this:</p><ul class="calibre17"><li class="calibre15"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/288.png" class="calibre288"/> = [0.9, 0.1] (Document 1 is 90% about Topic 1 and 10% about Topic 2)</li><li class="calibre15"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/289.png" class="calibre288"/> = [0.8, 0.2] (Document 2 is 80% about Topic 1 and 20% about Topic 2)</li><li class="calibre15"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/290.png" class="calibre289"/> = [0.1, 0.9] (Document 3 is 10% about Topic 1 and 90% about Topic 2)</li></ul></li>
</ol>
<p class="calibre6">In this example, topic 1 seems to be related to football and sports, while topic 2 seems to be related to technology and gadgets. The topic distributions for each document show that documents 1 and 2 are mostly about football, while document 3 is about technology.</p>
<p class="calibre6">Please note that this is a simplified example, and real-world data would require more sophisticated preprocessing and a larger number of iterations for convergence.</p>
<p class="calibre6">We are now ready to discuss the paradigm for putting together a complete project in a work or research setting.</p>
<h2 id="_idParaDest-109" class="calibre7"><a id="_idTextAnchor198" class="calibre5 pcalibre1 pcalibre"/>Real-world ML system design for NLP text classification</h2>
<p class="calibre6">This section is dedicated to the practical implementation of the various methods we discussed. It will revolve around Python code, which serves as a complete pipeline.</p>
<p class="calibre6">To provide a comprehensive learning experience, we will discuss the entire journey of a typical ML project. <em class="italic">Figure 5</em><em class="italic">.1</em> depicts the different phases of the ML project:</p>
<div><div><img alt="Figure 5.1 – The paradigm of a typical ML project" src="img/B18949_05_1.jpg" class="calibre3"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 5.1 – The paradigm of a typical ML project</p>
<p class="calibre6">Let’s break the problem down in a similar fashion to a typical project in the industry.</p>
<h3 class="calibre8">The business objective</h3>
<p class="calibre6">An ML project, whether in a business or research setting, stems from an original objective, which is often qualitative rather than technical.</p>
<p class="calibre6">Here’s an example:</p>
<ol class="calibre16">
<li class="calibre15">“We need to know which of our patients is at a higher risk.”</li>
<li class="calibre15">“We would like to maximize the engagement of our ad.”</li>
<li class="calibre15">“We need the autonomous car to be alerted when a person is stepping in front of it.”</li>
</ol>
<p class="calibre6">Next comes the technical objective.</p>
<h3 class="calibre8">The technical objective</h3>
<p class="calibre6">The original objective needs to be translated into a technical objective, like so:</p>
<ol class="calibre16">
<li class="calibre15">“We will process every patient’s medical record and build a risk estimator based on the history of realized risk.”</li>
<li class="calibre15">“We will collect data about all the ads from the last year and will build a regressor to estimate the level of engagement based on the ad’s features.”</li>
<li class="calibre15">“We will collect a set of images taken by the car’s front camera and present those to our online users who are visiting our site, telling them it’s for security reasons and that they need to click on the parts that show a human to prove they are not robots. However, in practice, we’ll collect their free labels for training, develop a computer vision classifier for humans, and won’t give them any credit.”</li>
</ol>
<p class="calibre6">While the original business or research objective is somewhat of an open-ended question, the technical objective reflects an actionable plan. Note, however, that any given technical objective represents just one among several potential solutions aligned with the original business or research aim. It is the responsibility of the technical authority, such as the CTO, ML manager, or senior developer, to understand the original objective and translate it into a technical objective. Moreover, it may be that the technical objective would be refined or even replaced down the line. The next step after forming a technical objective is to form a plan for it.</p>
<h3 class="calibre8">Tentative high-level system design</h3>
<p class="calibre6">To realize the technical objective, we need to derive a plan to decide which data would be used to feed into the ML system, and what the expected output of the ML system is. In the first steps of a project, there may be several candidate sources of potential data that are believed to be indicative of the desired output.</p>
<p class="calibre6">Following the set of three examples mentioned previously, here are some examples of data descriptions:</p>
<ol class="calibre16">
<li class="calibre15">The input data would be columns A, B, and C of the <strong class="source-inline1">patient_records</strong> SQL table and the risk would be assessed as <em class="italic">1/N</em>, where <em class="italic">N</em> is the number of days that passed from a given moment until the patient showed up in the emergency room.</li>
<li class="calibre15">The input data would be the geometric and color descriptions of the ads, and the level of engagement would be the number of clicks per day that the ad received.</li>
<li class="calibre15">The input data is the images of the car’s front camera to be fed to a computer vision neural network classifier, and the output data would be whether the image captures a person or not.</li>
</ol>
<h3 class="calibre8">Choosing a metric</h3>
<p class="calibre6">When defining a potential solution approach, extra attention should be dedicated to identifying the best metric to focus on, also known as the objective function or error function. This is the metric by which the success of the solution will be evaluated. It is important to relate the metric to the original business or research objective.</p>
<p class="calibre6">As per the previous examples, we could have the following:</p>
<ol class="calibre16">
<li class="calibre15">Minimize the 70th percentile confidence interval.</li>
<li class="calibre15">Minimize the mean absolute error.</li>
<li class="calibre15">Maximize precision while constraining on a fixed recall. This fixed recall will ideally be dictated by business leaders or the legal team, in the form of “the system must capture at least 99.9% of the cases where a person steps in front of a car.”</li>
</ol>
<p class="calibre6">Now that we have a tentative plan, we can explore the data and evaluate the feasibility of the design.</p>
<h3 class="calibre8">Exploration</h3>
<p class="calibre6">Exploration is divided into two parts – exploring the data and exploring the feasibility of the design. Let’s take a closer look.</p>
<h4 class="calibre135">Data exploration</h4>
<p class="calibre6">Data is not always perfect for our objective. We discussed some of the data shortcomings in previous chapters. In particular, free text is often notorious for having many abnormal phenomena, such as encodings, special characters, typos, and so on. When exploring our data, we want to uncover all these phenomena and make sure that the data can be brought to a form that serves the objective.</p>
<h4 class="calibre135">Feasibility study</h4>
<p class="calibre6">Here, we want to prospectively identify proxies for whether the planned design is expected to succeed. While with some problems there are known proxies for expected success, in most problems in the business and especially research setting, it takes much experience and ingenuity to suggest preliminary proxies for success.</p>
<p class="calibre6">An example of a very simple case is a simple regression problem with a single input variable and a single output variable. Let’s say the independent variable is the number of active viewers that your streaming service currently has, and the dependent variable is the risk that the company’s servers have for maxing out their capacity. The tentative design plan would be to build a regressor that estimates the risk at any given moment. A strong proxy for the feasibility of developing a successful regressor could be calculating the linear correlation between the historical data points. Calculating linear correlation based on sample data is easy and quick and if its result is close to 1 (or -1 in cases different than our business problem), then it means that a linear regressor is guaranteed to succeed, thus, making it a great proxy. However, note that if the linear correlation is close to 0, it doesn’t necessarily mean that a regressor would fail, only that a linear regression would fail. In such a case, a different proxy should be deferred to.</p>
<p class="calibre6">In the <em class="italic">Reviewing our use case – ML system design for NLP classification in a Jupyter Notebook</em> section, we’ll review our code solution. We’ll also present a method to assess the feasibility of a text classifier. The method aims to mimic a relationship between the input text to the output class. But since we want to have that method suit a variable that is text and not numeric, we’ll go back to the origin and calculate a measure for the statistical dependency between the input text and the output class. Statistical dependency is the most basic measure for a relationship between variables and thus doesn’t require either of them to be numeric.</p>
<p class="calibre6">Assuming the <strong class="bold">feasibility study</strong> is successful, we can move on to implementing the ML solution.</p>
<h2 id="_idParaDest-110" class="calibre7"><a id="_idTextAnchor199" class="calibre5 pcalibre1 pcalibre"/>Implementing an ML solution</h2>
<p class="calibre6">This part is where the expertise of the ML developer comes into play. There are different steps for it and the developer chooses which ones are relevant based on the problem – whether it’s data cleaning, text segmentation, feature design, model comparison, or metric choice.</p>
<p class="calibre6">We will elaborate on this as we review the specific use case we’ve solved.</p>
<h3 class="calibre8">Evaluating the results</h3>
<p class="calibre6">We evaluate the solution given the metric that was chosen. This part requires some experience as ML developers tend to get better at this over time. The main pitfall in this task is the ability to set up an objective assessment of the result. That objective assessment is done by applying the finished model to data it had never “seen” before. But often folks who are only starting to apply ML find themselves improving their design after seeing what the results of that held-out set are. This leads to a feedback loop where the design is practically fitted to the no-longer-held-out set. While this may indeed improve the model and the design, it takes away from the ability to provide an objective forecast of how the model would perform when implemented in the real world. In the real world, it would see data that is truly held out and that it wasn’t fitted to.</p>
<h3 class="calibre8">Done and delivered</h3>
<p class="calibre6">Typically, when the design is done, the implementation is complete, and the results have been found satisfactory, the work is presented for business implementation, or in the research setting, for publication. In the business setting, implementation can take on different forms.</p>
<p class="calibre6">One of the simplest forms is where the output is used to provide business insights. Its purpose is to be presented. For instance, when looking to evaluate how much a marketing campaign was contributing to the growth in sales, the ML team may calculate an estimation for that measure of contribution and present it to leadership.</p>
<p class="calibre6">Another form of implementation is within a dashboard in real time. For instance, the model calculates the predicted risk of patients coming to the emergency room, and it does so on a daily cadence. The results are aggregated and a graph is presented on the hospital dashboard to show the expected number of people who would come to the emergency room for every day of the next 30 days.</p>
<p class="calibre6">A more advanced and common form is when the output of the data is directed so that it can be fed into downstream tasks. The model would then be implemented in production to become a microservice within a larger production pipeline. An example of that is when a classifier evaluates every post on your company’s Facebook page. When it identifies offensive language, it outputs a detection that then passes down the pipeline to another system that removes that post and perhaps blocks that user.</p>
<h4 class="calibre135">Code design</h4>
<p class="calibre6">The code’s design should suit the purpose of the code once the work is complete. As per the different forms of implementation mentioned previously, some implementations dictate a specific code structure. For instance, when the completed code is handed off to production within a larger, already existing pipeline, it is the production engineer who would dictate the constraints to the ML team. These constraints may be around computation and timing resources, but they would also be around code design. Often, basic code files, such as <code>.py</code> files, are necessary.</p>
<p class="calibre6">As with cases where the code is used for presentations, such as in the example of presenting how contributive the marketing campaign was, Jupyter Notebooks may be the better choice.</p>
<p class="calibre6">Jupyter Notebooks can be very informative and instructional. For that reason, many ML developers start their projects with Jupyter Notebooks for the exploration phase.</p>
<p class="calibre6">Next, we will review our design in a Jupyter Notebook. This will allow us to encapsulate the entire process in a single coherent file that is meant to be presented to the reader.</p>
<h1 id="_idParaDest-111" class="calibre4"><a id="_idTextAnchor200" class="calibre5 pcalibre1 pcalibre"/>Reviewing our use case – ML system design for NLP classification in a Jupyter Notebook</h1>
<p class="calibre6">In this section, we will walk through a hands-on example. We will follow the steps we presented previously for articulating the problem, designing the solution, and evaluating the results. This section portrays the process that an ML developer goes through when working on a typical project in the industry. Refer to the notebook at <a href="https://colab.research.google.com/drive/1ZG4xN665le7X_HPcs52XSFbcd1OVaI9R?usp=sharing" class="calibre5 pcalibre1 pcalibre">https://colab.research.google.com/drive/1ZG4xN665le7X_HPcs52XSFbcd1OVaI9R?usp=sharing</a> for more information.</p>
<h3 class="calibre8">The business objective</h3>
<p class="calibre6">In this scenario, we are working for a financial news agency. Our objective is to publish news about companies and products in real time.</p>
<h3 class="calibre8">The technical objective</h3>
<p class="calibre6">The CTO derives several technical objectives from the business objective. One objective is for the ML team: given a stream of financial tweets in real time, detect those tweets that discuss information about companies or products.</p>
<h2 id="_idParaDest-112" class="calibre7"><a id="_idTextAnchor201" class="calibre5 pcalibre1 pcalibre"/>The pipeline</h2>
<p class="calibre6">Let’s review the different parts of the pipeline, as shown in <em class="italic">Figure 5</em><em class="italic">.2</em>:</p>
<div><div><img alt="Figure 5.2 – The structure of a typical ML pipeline" src="img/B18949_05_2.jpg" class="calibre3"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 5.2 – The structure of a typical ML pipeline</p>
<p class="callout-heading">Note</p>
<p class="callout">The phases of the pipeline in <em class="italic">Figure 5</em><em class="italic">.2</em> are explored in the following subsections</p>
<h2 id="_idParaDest-113" class="calibre7"><a id="_idTextAnchor202" class="calibre5 pcalibre1 pcalibre"/>Code settings</h2>
<p class="calibre6">In this part of the code, we set the key parameters. We choose to have them as a part of the code as this is instructional code made for presentation. In cases where the code is expected to go to production, it may be better to host the parameters in a separate <code>.yaml</code> file. That would also suit heavy iterations during the development phase as it will allow you to iterate over different code parameters without having to change the code, which is often desirable.</p>
<p class="calibre6">As for the choice of these values, it should be stressed that some of these values should be optimized to suit the optimization of the solution. We have chosen fixed measures here to simplify the process. For instance, the number of features to be used for classification is a fixed quantity here, but it should also be optimized to fit the training set.</p>
<h3 class="calibre8">Gathering the data</h3>
<p class="calibre6">This part loads the dataset. In our case, the loading function is simple. In other business cases, this part could be quite large as it may include a collection of SQL queries that are called. In such a case, it may be ideal to write a dedicated function in a separate <code>.py</code> file and source it via the imports section.</p>
<h3 class="calibre8">Processing the data</h3>
<p class="calibre6">Here, we format the data in a way that suits our work. We also observe some of it for the first time. This allows us to get a feel of its nature and quality.</p>
<p class="calibre6">One key action we take here is to define the classes we care about.</p>
<h3 class="calibre8">Preprocessing</h3>
<p class="calibre6">As we discussed in <a href="B18949_04.xhtml#_idTextAnchor113" class="calibre5 pcalibre1 pcalibre"><em class="italic">Chapter 4</em></a>, preprocessing is a key part of the pipeline. For instance, we notice that many of the tweets have a URL, which we choose to remove.</p>
<h3 class="calibre8">Preliminary data exploration</h3>
<p class="calibre6">At this point, we have observed the quality of the text and the distribution of the classes. This is where we explore any other characteristics of the data that may imply either its quality or its ability to indicate the desired class.</p>
<h3 class="calibre8">Feature engineering</h3>
<p class="calibre6">Next, we start processing the text. We seek to represent the text of each observation as a set of numerical features. The main reason for this is that traditional ML models are designed to accept numbers as input, not text. For instance, a common linear regression or logistic regression model is applied to numbers, not words, categories, or image pixels. Thus, we need to suggest a numeric representation for the text. This design constraint is lifted when working with <strong class="bold">language models</strong> such as <strong class="bold">BERT</strong> and <strong class="bold">GPT</strong>. We will see this in the coming chapters.</p>
<p class="calibre6">We partition the text into N-grams, where <em class="italic">N</em> is a parameter of the code. <em class="italic">N</em> is fixed in this code but should be optimized to best fit the training set.</p>
<p class="calibre6">Once the text has been partitioned into N-grams, they are modeled as numeric values. When a binary (that is, <strong class="bold">one-hot encoding</strong>) method is chosen, the numerical feature that represents some N-gram gets a “1” when the observed text includes that N-gram, and “0” otherwise. See <em class="italic">Figure 5</em><em class="italic">.3</em> for an example. If a BOW approach is chosen, then the value of the feature is the number of times the N-gram appears in the observed text. Another common feature engineering method that isn’t implemented here is <strong class="bold">TF-IDF</strong>.</p>
<p class="calibre6">Here’s what we get by using unigrams only:</p>
<p class="calibre6">Input sentence: “filing submitted.”</p>
<table class="no-table-style" id="table003">
<colgroup class="calibre10">
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
</colgroup>
<thead class="calibre12">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><strong class="bold">N-gram</strong></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold">“</strong><strong class="bold">report”</strong></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold">“</strong><strong class="bold">filing”</strong></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold">“</strong><strong class="bold">submitted”</strong></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold">“</strong><strong class="bold">product”</strong></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold">“</strong><strong class="bold">quarterly”</strong></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold">The rest of </strong><strong class="bold">the unigrams</strong></p>
</td>
</tr>
</thead>
<tbody class="calibre13">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6">Feature value</p>
</td>
<td class="no-table-style2">
<p class="calibre6">0</p>
</td>
<td class="no-table-style2">
<p class="calibre6">1</p>
</td>
<td class="no-table-style2">
<p class="calibre6">1</p>
</td>
<td class="no-table-style2">
<p class="calibre6">0</p>
</td>
<td class="no-table-style2">
<p class="calibre6">0</p>
</td>
<td class="no-table-style2">
<p class="calibre6">(0’s)</p>
</td>
</tr>
</tbody>
</table>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 5.3 – Transforming an input text sentence into a numerical representation by partitioning to unigrams via one-hot encoding</p>
<p class="calibre6">The following figure shows what we get by using both unigrams and bigrams:</p>
<table class="no-table-style" id="table004">
<colgroup class="calibre10">
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
</colgroup>
<thead class="calibre12">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><strong class="bold">N-gram</strong></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold">“</strong><strong class="bold">report”</strong></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold">“</strong><strong class="bold">filing”</strong></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold">“</strong><strong class="bold">filing submitted”</strong></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold">“</strong><strong class="bold">report news”</strong></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold">“</strong><strong class="bold">submitted”</strong></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold">The rest of </strong><strong class="bold">the N-grams</strong></p>
</td>
</tr>
</thead>
<tbody class="calibre13">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6">Feature value</p>
</td>
<td class="no-table-style2">
<p class="calibre6">0</p>
</td>
<td class="no-table-style2">
<p class="calibre6">1</p>
</td>
<td class="no-table-style2">
<p class="calibre6">1</p>
</td>
<td class="no-table-style2">
<p class="calibre6">0</p>
</td>
<td class="no-table-style2">
<p class="calibre6">1</p>
</td>
<td class="no-table-style2">
<p class="calibre6">(0’s)</p>
</td>
</tr>
</tbody>
</table>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 5.4 – Transforming an input text sentence into a numerical representation by partitioning to unigrams and bigrams via one-hot encoding</p>
<p class="calibre6">Note <a id="_idIndexMarker524" class="calibre5 pcalibre1 pcalibre"/>that at this point in the code, the dataset hasn’t been partitioned into train and test sets, and the held-out set has not been excluded yet. This is because the binary and BOW feature engineering methods don’t depend on data outside of the underlying observation. With TF-IDF, this is different. Every feature value is calculated using the entire dataset for the document frequency.</p>
<h3 class="calibre8">Exploring the new numerical features</h3>
<p class="calibre6">Now <a id="_idIndexMarker525" class="calibre5 pcalibre1 pcalibre"/>that our text has been represented as a feature, we can explore it numerically. We can look at its frequencies and statistics and get a sense of how it’s distributed.</p>
<h3 class="calibre8">Splitting into train/test sets</h3>
<p class="calibre6">This is the part where we must pause and carve out a held-out set, also known as a test set, and <a id="_idIndexMarker526" class="calibre5 pcalibre1 pcalibre"/>sometimes as the validation set. Since these terms are used differently in different sources, it is important to explain that what we refer to as a test set is a held-out set. A held-out set is a data subset that we dedicate to evaluating our solution’s performance. It is held out to simulate the results that we expect to get when the system is implemented in the real world and will encounter new data samples.</p>
<p class="calibre6">How do we know when to carve out the held-out set?</p>
<p class="calibre6">If we carve it out “too early,” such as right after loading the data, then we are guaranteed to keep it held out, but we may miss discrepancies in the data as it won’t take part in the preliminary exploration. If we carve it out “too late,” our design decisions might become biased because of it. For example, if we choose one ML model over another based on results that include the would-be held-out set, then our design becomes tailored to that set, preventing us from offering an objective evaluation of the model.</p>
<p class="calibre6">Then, we <a id="_idIndexMarker527" class="calibre5 pcalibre1 pcalibre"/>need to carry out the test set right before the first action that will feed into design decisions. In the next section, we’ll perform statistical analysis, which we can then feed into feature selection. Since that selection should be agnostic to the held-out set, we’ll exclude that set from this part onwards.</p>
<h3 class="calibre8">Preliminary statistical analysis and feasibility study</h3>
<p class="calibre6">This is the second part of the exploration phase we spoke about a few pages ago. The first <a id="_idIndexMarker528" class="calibre5 pcalibre1 pcalibre"/>part was data exploration, and we implemented that in the previous parts of the code. Now that we have the text represented as numerical features, we can perform the feasibility study.</p>
<p class="calibre6">We seek to measure the statistical dependence between the text inputs and the class values. Again, the motivation is to mimic the proxy that linear correlation provides with a regression problem.</p>
<p class="calibre6">We know that for two random variables, <em class="italic">X</em> and <em class="italic">Y</em>, if they are statistically independent, then we get the following:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;Y&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;Y&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/291.png" class="calibre290"/></p>
<p class="calibre6">Alternatively, we get the following:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;Y&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;Y&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/292.png" class="calibre291"/></p>
<p class="calibre6">This happens for every <em class="italic">x, y</em> value that yields a non-zero probability.</p>
<p class="calibre6">Conversely, we could use Bayes’s rules:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi&gt;Y&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;Y&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;Y&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/293.png" class="calibre292"/></p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi&gt;Y&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/294.png" class="calibre293"/></p>
<p class="calibre6">Now, let’s think about any two random variables that aren’t necessarily statistically independent. We would like to evaluate whether there is a statistical relationship between the two.</p>
<p class="calibre6">Let one random variable be any of our numerical features, and the other random variable be the <a id="_idIndexMarker529" class="calibre5 pcalibre1 pcalibre"/>output class taking on values 0 or 1. Let’s assume the feature engineering method is binary, so the feature also takes on values of 0 or 1.</p>
<p class="calibre6">Looking at the last equation, the expression on the left-hand side presents a very powerful measure of the ability of the relationship between <em class="italic">X</em> and <em class="italic">Y</em>:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mo&gt;{&lt;/mo&gt;&lt;mn&gt;0,1&lt;/mn&gt;&lt;mo&gt;}&lt;/mo&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/295.png" class="calibre294"/></p>
<p class="calibre6">It is powerful because if the feature is completely nonindicative of the class value, then in statistical terms, we say the two are statistically independent, and thus this measure would be equal to 1.</p>
<p class="calibre6">Conversely, the <a id="_idIndexMarker530" class="calibre5 pcalibre1 pcalibre"/>bigger the difference between this measure and 1, the stronger the relationship is between this feature and this class. When performing a <strong class="bold">feasibility study</strong> of our design, we want to see that there are features in the data that have a statistical relationship with the output class.</p>
<p class="calibre6">For that reason, we calculate the value of this expression for every pair of every feature and every class.</p>
<p class="calibre6">We present the most indicative terms for class “0,” which is the class of tweets that don’t indicate a company or product information, and we also present the terms that are most indicative of class “1,” meaning, when a tweet is discussing information about a company or a product.</p>
<p class="calibre6">This proves to us that there are indeed text terms that are indicative of the class value. This is a definite and clear success of the feasibility study. We are good to go and we are expecting productive outcomes when implementing a classification model.</p>
<p class="calibre6">As a side note, keep in mind that as with most evaluations, what we’ve just mentioned is just one sufficient condition for the potential of the text to predict the class. If it had failed, it would not necessarily indicate that there is no feasibility. Just like when the linear <a id="_idIndexMarker531" class="calibre5 pcalibre1 pcalibre"/>correlation between <em class="italic">X</em> and<em class="italic"> Y</em> is near 0, this doesn’t mean that <em class="italic">X</em> can’t infer <em class="italic">Y</em>. It just means that <em class="italic">X</em> cannot infer <em class="italic">Y</em> via a linear model. The linearity is an assumption that’s made to make things simple if it prevails.</p>
<p class="calibre6">In the method that we’ve suggested, we make two key assumptions. First, we assume a very particular manner for feature design, being a certain <em class="italic">N</em> for the N-gram partition, and a certain quantitative method for the value – binary. The second is that we perform the most simple evaluation of statistical dependency, a univariate statistical dependency. But it could be that only a higher order, such as univariate, would have statistical dependence on the outcome class.</p>
<p class="calibre6">With a <strong class="bold">feasibility study</strong> of text classification, it’s ideal if the method is as simple as possible while covering as much of the “signal” it is hoping to uncover. The approach we designed in this example was derived after years of experience with different sets and various problem settings. We find that it hits the target very well.</p>
<h2 id="_idParaDest-114" class="calibre7"><a id="_idTextAnchor203" class="calibre5 pcalibre1 pcalibre"/>Feature selection</h2>
<p class="calibre6">With the <strong class="bold">feasibility study</strong>, we often kill two birds with one stone. As a <strong class="bold">feasibility study</strong> is successful, it not only helps us by confirming our plan, but it often hints toward the next <a id="_idIndexMarker532" class="calibre5 pcalibre1 pcalibre"/>steps that we should take. As we saw, some features are indicative of the class, and we learned which are the most significant. This allows us to reduce the feature space that the classification model will need to partition. We do that by keeping the most indicative features for each of the two classes. The number of features that we choose to keep would ideally be derived by computation constraints (for example, too many features would take too long to compute a model around), model capabilities (for example, too many features can’t be handled well by the model due to co-linearity), and optimization of the train results. In our code, we fixed this number to make things quick and simple.</p>
<p class="calibre6">It should be stressed that in many ML models, feature selection is an inherited part of the <a id="_idIndexMarker533" class="calibre5 pcalibre1 pcalibre"/>model design. For instance, with the <strong class="bold">least absolute shrinkage and selection operator</strong> (<strong class="bold">LASSO</strong>), the hyperparameter scaler of the <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;script&quot;&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/296.png" class="calibre295"/> norm component has an impact on which features get a zero coefficient, and thus get “thrown out.” It is possible and sometimes recommended to skip this part of the feature selection process, leave all features in, and let the model perform feature selection. It is advised to do so when all the models that are being evaluated and compared possess that characteristic.</p>
<p class="calibre6">Remember <a id="_idIndexMarker534" class="calibre5 pcalibre1 pcalibre"/>that at this point, we are only observing the train set. Now that we have decided which features to keep, we need to apply that selection to the test set as well.</p>
<p class="calibre6">With that, our data has been prepared for ML modeling.</p>
<h3 class="calibre8">Iterating over ML models</h3>
<p class="calibre6">To choose which model suits this problem best, we must train several models and see which one of them does best.</p>
<p class="calibre6">We <a id="_idIndexMarker535" class="calibre5 pcalibre1 pcalibre"/>should stress that we could do many things to try and identify the best model choice for a given <a id="_idIndexMarker536" class="calibre5 pcalibre1 pcalibre"/>problem. In our case, we only chose to evaluate a handful of models. Moreover, to make things simple and quick, we chose to not optimize the hyperparameters of each model in a comprehensive cross-validation approach. We simply fit each model to the training set with the default settings that its function comes with. Once we’ve identified the model we’d like to use, we optimize its hyperparameters for the train set via cross-validation.</p>
<p class="calibre6">By doing this, we identify the best model for the problem.</p>
<h2 id="_idParaDest-115" class="calibre7"><a id="_idTextAnchor204" class="calibre5 pcalibre1 pcalibre"/>Generating the chosen model</h2>
<p class="calibre6">Here, we <a id="_idIndexMarker537" class="calibre5 pcalibre1 pcalibre"/>optimize the hyperparameters of the chosen model and fit it to our train set.</p>
<h3 class="calibre8">Generating the train results – design choices</h3>
<p class="calibre6">At this <a id="_idIndexMarker538" class="calibre5 pcalibre1 pcalibre"/>stage, we observe the results of the model for the first time. This result can be used to feed insight back into the design choice and the parameters chosen, such as the feature engineering method, the number of features left in the feature selection, and even the preprocessing scheme.</p>
<p class="callout-heading">Important note</p>
<p class="callout">Note that when feeding back insights from the results of the train set to the design of the solution, you are risking overfitting the train set. You’ll know whether you are by the gap between the results on the train set and the results on the test set.</p>
<p class="calibre6">While <a id="_idIndexMarker539" class="calibre5 pcalibre1 pcalibre"/>a gap is expected between these results in favor of the train results, a large gap should be treated as an alarm that the design isn’t optimal. In such cases, the design should be redone with systematic code-based parameters to ensure fair choices are made. It is possible to even carve out another semi-held-out set from the train set, often referred to as the validation set.</p>
<h3 class="calibre8">Generating the test results – presenting performance</h3>
<p class="calibre6">That’s it!</p>
<p class="calibre6">Now <a id="_idIndexMarker540" class="calibre5 pcalibre1 pcalibre"/>that the design has been optimized and we are confident that it suits our objective, we can apply it to our held-out set and observe the test results. These results are our most objective forecast of how well the system would do in the real world.</p>
<p class="calibre6">As mentioned previously, we should avoid letting these results impact our<a id="_idTextAnchor205" class="calibre5 pcalibre1 pcalibre"/><a id="_idTextAnchor206" class="calibre5 pcalibre1 pcalibre"/><a id="_idTextAnchor207" class="calibre5 pcalibre1 pcalibre"/> design choices.</p>
<h1 id="_idParaDest-116" class="calibre4"><a id="_idTextAnchor208" class="calibre5 pcalibre1 pcalibre"/>Summary</h1>
<p class="calibre6">In this chapter, we embarked on a comprehensive exploration of text classification, an indispensable aspect of NLP and ML. We delved into various types of text classification tasks, each presenting unique challenges and opportunities. This foundational understanding sets the stage for effectively tackling a broad range of applications, from sentiment analysis to spam detection.</p>
<p class="calibre6">We walked through the role of N-grams in capturing local context and word sequences within text, thereby enhancing the feature set used for classification tasks. We also illuminated the power of the TF-IDF method, the role of Word2Vec in text classification, and popular architectures such as CBOW and skip-gram, giving you a deep understanding of their mechanics.</p>
<p class="calibre6">Then, we introduced topic modeling and examined how popular algorithms such as LDA can be applied to text classification.</p>
<p class="calibre6">Lastly, we introduced a professional paradigm for leading an NLP-ML project in a business or research setting. We discussed the objectives and the project design aspect, and then dove into the system design. We implemented a real-world example in code and experimented with this.</p>
<p class="calibre6">In essence, this chapter has aimed to equip you with a holistic understanding of text classification and topic modeling by touching on the key concepts, methodologies, and techniques in the field. The knowledge and skills imparted will enable you to effectively approach and solve real-world text classification problems.</p>
<p class="calibre6">In the next chapter, we will introduce advanced methods for text classification. We will review deep learning methods such as language models, discuss their theory and design, and present a hands-on system design in code.</p>
</div>
</body></html>