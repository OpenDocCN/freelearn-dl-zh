<html><head></head><body>
<div id="_idContainer009" class="calibre2">
<h1 class="chapter-number" id="_idParaDest-16"><a id="_idTextAnchor015" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1.1">1</span></h1>
<h1 id="_idParaDest-17" class="calibre4"><a id="_idTextAnchor016" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.2.1">Navigating the NLP Landscape: A Comprehensive Introduction</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.3.1">This book is aimed at helping professionals apply </span><strong class="bold"><span class="kobospan" id="kobo.4.1">natural language processing</span></strong><span class="kobospan" id="kobo.5.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.6.1">NLP</span></strong><span class="kobospan" id="kobo.7.1">) techniques to their work, whether they are working on NLP projects or using NLP in other areas, such as data science. </span><span class="kobospan" id="kobo.7.2">The purpose of the book is to introduce you to the field of NLP and its underlying techniques, including </span><strong class="bold"><span class="kobospan" id="kobo.8.1">machine learning</span></strong><span class="kobospan" id="kobo.9.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.10.1">ML</span></strong><span class="kobospan" id="kobo.11.1">) and </span><strong class="bold"><span class="kobospan" id="kobo.12.1">deep learning</span></strong><span class="kobospan" id="kobo.13.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.14.1">DL</span></strong><span class="kobospan" id="kobo.15.1">). </span><span class="kobospan" id="kobo.15.2">Throughout the book, we highlight the importance of mathematical foundations, such as linear algebra, statistics and probability, and optimization theory, which are necessary to understand the algorithms used in NLP. </span><span class="kobospan" id="kobo.15.3">The content is accompanied by code examples in Python to allow you to pre-practice, experiment, and generate some of the development presented in </span><span><span class="kobospan" id="kobo.16.1">the book.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.17.1">The book discusses the challenges faced in NLP, such as understanding the context and meaning of words, the relationships between them, and the need for labeled data. </span><span class="kobospan" id="kobo.17.2">The book also mentions the recent advancements in NLP, including pre-trained language models, such as BERT and GPT, and the availability of large amounts of text data, which has led to improved performance on </span><span><span class="kobospan" id="kobo.18.1">NLP tasks.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.19.1">The book will engage you by discussing the impact of language models on the field of NLP, including improved accuracy and effectiveness in NLP tasks, the development of more advanced NLP systems, and accessibility to a broader range </span><span><span class="kobospan" id="kobo.20.1">of people.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.21.1">We will be covering the following headings in </span><span><span class="kobospan" id="kobo.22.1">the chapter:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><span class="kobospan" id="kobo.23.1">What is natural </span><span><span class="kobospan" id="kobo.24.1">language processing?</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.25.1">Initial strategies in the machine processing of </span><span><span class="kobospan" id="kobo.26.1">natural language</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.27.1">A winning synergy – the coming together of NLP </span><span><span class="kobospan" id="kobo.28.1">and ML</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.29.1">Introduction to math and statistics </span><span><span class="kobospan" id="kobo.30.1">in NLP</span></span></li>
</ul>
<div class="calibre9"/><h1 id="_idParaDest-18" class="calibre4"><a id="_idTextAnchor017" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.31.1">Who this book is for</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.32.1">The target audience of the book is professionals who work with text as part of their projects. </span><span class="kobospan" id="kobo.32.2">This may include NLP practitioners, who may be beginners, as well as those who do not typically work </span><span><span class="kobospan" id="kobo.33.1">with text.</span></span></p>
<h1 id="_idParaDest-19" class="calibre4"><a id="_idTextAnchor018" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.34.1">What is natural language processing?</span></h1>
<p class="calibre6"><strong class="bold"><span class="kobospan" id="kobo.35.1">NLP</span></strong><span class="kobospan" id="kobo.36.1"> is a field of </span><strong class="bold"><span class="kobospan" id="kobo.37.1">artificial intelligence</span></strong><span class="kobospan" id="kobo.38.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.39.1">AI</span></strong><span class="kobospan" id="kobo.40.1">) focused on the interaction between computers and human</span><a id="_idIndexMarker000" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.41.1"> languages. </span><span class="kobospan" id="kobo.41.2">It involves using computational</span><a id="_idIndexMarker001" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.42.1"> techniques to understand, interpret, and generate human language, making it possible for computers to understand and respond to human input naturally </span><span><span class="kobospan" id="kobo.43.1">and meaningfully.</span></span></p>
<h2 id="_idParaDest-20" class="calibre7"><a id="_idTextAnchor019" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.44.1">The history and evolution of natural language processing</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.45.1">The history of NLP is a fascinating</span><a id="_idIndexMarker002" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.46.1"> journey through time, tracing back to the 1950s, with significant contributions from pioneers such as Alan Turing. </span><span class="kobospan" id="kobo.46.2">Turing’s seminal paper, </span><em class="italic"><span class="kobospan" id="kobo.47.1">Computing Machinery and Intelligence</span></em><span class="kobospan" id="kobo.48.1">, introduced the Turing test, laying the groundwork for future explorations in AI and NLP. </span><span class="kobospan" id="kobo.48.2">This period marked the inception of symbolic NLP, characterized by the use of rule-based systems, such as the notable Georgetown experiment in 1954, which ambitiously aimed to solve machine translation by generating a translation of Russian content into English (see </span><a href="https://en.wikipedia.org/wiki/Georgetown%E2%80%93IBM_experiment" class="calibre5 pcalibre1 pcalibre"><span class="kobospan" id="kobo.49.1">https://en.wikipedia.org/wiki/Georgetown%E2%80%93IBM_experiment</span></a><span class="kobospan" id="kobo.50.1">). </span><span class="kobospan" id="kobo.50.2">Despite early optimism, progress was slow, revealing the complexities of language understanding </span><span><span class="kobospan" id="kobo.51.1">and generation.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.52.1">The 1960s and 1970s saw the development of early NLP systems, which demonstrated the potential for machines to engage in human-like interactions using limited vocabularies and knowledge bases. </span><span class="kobospan" id="kobo.52.2">This era also witnessed the creation of conceptual ontologies, crucial for structuring real-world information in a computer-understandable format. </span><span class="kobospan" id="kobo.52.3">However, the limitations of rule-based methods led to a paradigm shift in the late 1980s towards statistical NLP, fueled by advances in ML and increased computational power. </span><span class="kobospan" id="kobo.52.4">This shift enabled more effective learning from large corpora, significantly advancing machine translation and other NLP tasks. </span><span class="kobospan" id="kobo.52.5">This paradigm shift not only represented a technological and methodological advancement but also underscored a conceptual evolution in the approach to linguistics within NLP. </span><span class="kobospan" id="kobo.52.6">In moving away from the rigidity of predefined grammar rules, this transition embraced corpus linguistics, a method that allows machines to “perceive” and understand languages through extensive exposure to large bodies of text. </span><span class="kobospan" id="kobo.52.7">This approach reflects a more empirical and data-driven understanding of language, where patterns and </span><a id="_idIndexMarker003" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.53.1">meanings are derived from actual language use rather than theoretical constructs, enabling more nuanced and flexible language </span><span><span class="kobospan" id="kobo.54.1">processing capabilities.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.55.1">Entering the 21st century, the emergence of the web provided vast amounts of data, catalyzing research in unsupervised and semi-supervised learning algorithms. </span><span class="kobospan" id="kobo.55.2">The breakthrough came with the advent of neural NLP in the 2010s, where DL techniques began to dominate, offering unprecedented accuracy in language modeling and parsing. </span><span class="kobospan" id="kobo.55.3">This era has been marked by the development of sophisticated models such as Word2Vec and the proliferation of deep neural networks, driving NLP towards more natural and effective human-computer interaction. </span><span class="kobospan" id="kobo.55.4">As we continue to build on these advancements, NLP stands at the forefront of AI research, with its history reflecting a relentless pursuit of understanding and replicating the nuances of </span><span><span class="kobospan" id="kobo.56.1">human language.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.57.1">In recent years, NLP has also been applied to a wide range of industries, such as healthcare, finance, and social media, where it has been used to automate decision-making and enhance communication between humans and machines. </span><span class="kobospan" id="kobo.57.2">For example, NLP has been used to extract information from medical documents, analyze customer feedback, translate documents between languages, and search through enormous amounts </span><span><span class="kobospan" id="kobo.58.1">of posts.</span></span></p>
<h1 id="_idParaDest-21" class="calibre4"><a id="_idTextAnchor020" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.59.1">Initial strategies in the machine processing of natural language</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.60.1">Traditional methods in NLP consist of text preprocessing, which is synonymous with text preparation, which</span><a id="_idIndexMarker004" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.61.1"> is then followed by applying </span><strong class="bold"><span class="kobospan" id="kobo.62.1">ML</span></strong><span class="kobospan" id="kobo.63.1"> methods. </span><span class="kobospan" id="kobo.63.2">Preprocessing text is an essential step in NLP and ML applications. </span><span class="kobospan" id="kobo.63.3">It involves cleaning and transforming the original text data into a form that can be easily understood and analyzed by ML algorithms. </span><span class="kobospan" id="kobo.63.4">The goal of preprocessing is to remove noise and inconsistencies and standardize the data, making it more suitable for advanced NLP and </span><span><span class="kobospan" id="kobo.64.1">ML methods.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.65.1">One of the key benefits of preprocessing is that it can significantly improve the performance of ML algorithms. </span><span class="kobospan" id="kobo.65.2">For example, removing stop words, which are common words that do not carry much meaning, such as “the” and “is,” can help reduce the dimensionality of the data, making it easier for the algorithm to </span><span><span class="kobospan" id="kobo.66.1">identify patterns.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.67.1">Take the following sentence as </span><span><span class="kobospan" id="kobo.68.1">an example:</span></span></p>
<p class="calibre6"><strong class="bold"><span class="kobospan" id="kobo.69.1">I am going to the store to buy some milk </span></strong><span><strong class="bold"><span class="kobospan" id="kobo.70.1">and bread</span></strong></span><span><span class="kobospan" id="kobo.71.1">.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.72.1">After removing the stop words, we have </span><span><span class="kobospan" id="kobo.73.1">the following:</span></span></p>
<p class="calibre6"><strong class="bold"><span class="kobospan" id="kobo.74.1">going store buy </span></strong><span><strong class="bold"><span class="kobospan" id="kobo.75.1">milk bread</span></strong></span><span><span class="kobospan" id="kobo.76.1">.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.77.1">In the example sentence, the stop words “</span><strong class="bold"><span class="kobospan" id="kobo.78.1">I</span></strong><span class="kobospan" id="kobo.79.1">,” “</span><strong class="bold"><span class="kobospan" id="kobo.80.1">am</span></strong><span class="kobospan" id="kobo.81.1">,” “</span><strong class="bold"><span class="kobospan" id="kobo.82.1">to</span></strong><span class="kobospan" id="kobo.83.1">,” “</span><strong class="bold"><span class="kobospan" id="kobo.84.1">the</span></strong><span class="kobospan" id="kobo.85.1">,” “</span><strong class="bold"><span class="kobospan" id="kobo.86.1">some</span></strong><span class="kobospan" id="kobo.87.1">,” and “</span><strong class="bold"><span class="kobospan" id="kobo.88.1">and</span></strong><span class="kobospan" id="kobo.89.1">” do not add any additional meaning to the sentence and can be removed without changing the overall meaning </span><a id="_idIndexMarker005" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.90.1">of the sentence. </span><span class="kobospan" id="kobo.90.2">It should be emphasized that the removal of stop words needs to be tailored to the specific objective, as the omission of a particular word might be trivial in one context but detrimental </span><span><span class="kobospan" id="kobo.91.1">in another.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.92.1">Additionally, </span><strong class="bold"><span class="kobospan" id="kobo.93.1">stemming</span></strong><span class="kobospan" id="kobo.94.1"> and </span><strong class="bold"><span class="kobospan" id="kobo.95.1">lemmatization</span></strong><span class="kobospan" id="kobo.96.1">, which reduce </span><a id="_idIndexMarker006" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.97.1">words to their base forms, can help reduce the </span><a id="_idIndexMarker007" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.98.1">number of unique words in the data, making it easier for the algorithm to identify relationships between them, which will be explained completely in </span><span><span class="kobospan" id="kobo.99.1">this book.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.100.1">Take the following sentence as </span><span><span class="kobospan" id="kobo.101.1">an example:</span></span></p>
<p class="calibre6"><strong class="bold"><span class="kobospan" id="kobo.102.1">The boys ran, jumped, and </span></strong><span><strong class="bold"><span class="kobospan" id="kobo.103.1">swam quickly</span></strong></span><span><span class="kobospan" id="kobo.104.1">.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.105.1">After applying stemming, which reduces each word to its root or stem form, disregarding word tense or derivational affixes, we </span><span><span class="kobospan" id="kobo.106.1">might get:</span></span></p>
<p class="calibre6"><strong class="bold"><span class="kobospan" id="kobo.107.1">The boy ran, jump, and </span></strong><span><strong class="bold"><span class="kobospan" id="kobo.108.1">swam quick</span></strong></span><span><span class="kobospan" id="kobo.109.1">.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.110.1">Stemming simplifies the text to its base forms. </span><span class="kobospan" id="kobo.110.2">In this example, “</span><strong class="bold"><span class="kobospan" id="kobo.111.1">ran</span></strong><span class="kobospan" id="kobo.112.1">,” “</span><strong class="bold"><span class="kobospan" id="kobo.113.1">jumped</span></strong><span class="kobospan" id="kobo.114.1">,” and “</span><strong class="bold"><span class="kobospan" id="kobo.115.1">swam</span></strong><span class="kobospan" id="kobo.116.1">” are reduced to “</span><strong class="bold"><span class="kobospan" id="kobo.117.1">ran</span></strong><span class="kobospan" id="kobo.118.1">,” “</span><strong class="bold"><span class="kobospan" id="kobo.119.1">jump</span></strong><span class="kobospan" id="kobo.120.1">,” and “</span><strong class="bold"><span class="kobospan" id="kobo.121.1">swam</span></strong><span class="kobospan" id="kobo.122.1">,” respectively. </span><span class="kobospan" id="kobo.122.2">Note that “ran” and “swam” do not change, as stemming often results in words that are close to their root form but not exactly the dictionary base form. </span><span class="kobospan" id="kobo.122.3">This process helps reduce the complexity of the text data, making it easier for machine learning algorithms to match and analyze patterns without getting bogged down by variations of the </span><span><span class="kobospan" id="kobo.123.1">same word.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.124.1">Take the following sentence as </span><span><span class="kobospan" id="kobo.125.1">an example:</span></span></p>
<p class="calibre6"><strong class="bold"><span class="kobospan" id="kobo.126.1">The boys ran, jumped, and </span></strong><span><strong class="bold"><span class="kobospan" id="kobo.127.1">swam quickly</span></strong></span><span><span class="kobospan" id="kobo.128.1">.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.129.1">After applying lemmatization, which considers the morphological analysis of the words, aiming to return the base or dictionary form of a word, known as the lemma, </span><span><span class="kobospan" id="kobo.130.1">we get:</span></span></p>
<p class="calibre6"><strong class="bold"><span class="kobospan" id="kobo.131.1">The boy run, jump, and </span></strong><span><strong class="bold"><span class="kobospan" id="kobo.132.1">swim quickly</span></strong></span><span><span class="kobospan" id="kobo.133.1">.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.134.1">Lemmatization accurately converts “</span><strong class="bold"><span class="kobospan" id="kobo.135.1">ran</span></strong><span class="kobospan" id="kobo.136.1">,” “</span><strong class="bold"><span class="kobospan" id="kobo.137.1">jumped</span></strong><span class="kobospan" id="kobo.138.1">,” and “</span><strong class="bold"><span class="kobospan" id="kobo.139.1">swam</span></strong><span class="kobospan" id="kobo.140.1">” to “</span><strong class="bold"><span class="kobospan" id="kobo.141.1">run</span></strong><span class="kobospan" id="kobo.142.1">,” “</span><strong class="bold"><span class="kobospan" id="kobo.143.1">jump</span></strong><span class="kobospan" id="kobo.144.1">,” and “</span><strong class="bold"><span class="kobospan" id="kobo.145.1">swim</span></strong><span class="kobospan" id="kobo.146.1">.” </span><span class="kobospan" id="kobo.146.2">This process takes into account the part of speech of each word, ensuring that the reduction to the base form is both grammatically and contextually appropriate. </span><span class="kobospan" id="kobo.146.3">Unlike stemming, lemmatization provides a more precise reduction to the base form, ensuring </span><a id="_idIndexMarker008" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.147.1">that the processed text remains meaningful and contextually accurate. </span><span class="kobospan" id="kobo.147.2">This enhances the performance of NLP models by enabling them to understand and process language more effectively, reducing the dataset’s complexity while maintaining the integrity of the </span><span><span class="kobospan" id="kobo.148.1">original text.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.149.1">Two other important aspects of preprocessing </span><a id="_idIndexMarker009" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.150.1">are data normalization and data cleaning. </span><strong class="bold"><span class="kobospan" id="kobo.151.1">Data normalization</span></strong><span class="kobospan" id="kobo.152.1"> includes converting all text to lowercase, removing punctuation, and standardizing the format of the data. </span><span class="kobospan" id="kobo.152.2">This helps to ensure that the algorithm does not treat different variations of the same word as separate entities, which can lead to </span><span><span class="kobospan" id="kobo.153.1">inaccurate results.</span></span></p>
<p class="calibre6"><strong class="bold"><span class="kobospan" id="kobo.154.1">Data cleaning</span></strong><span class="kobospan" id="kobo.155.1"> includes removing</span><a id="_idIndexMarker010" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.156.1"> duplicate or irrelevant data and correcting errors or inconsistencies in the data. </span><span class="kobospan" id="kobo.156.2">This is particularly important in large datasets, where manual cleaning is time-consuming and error-prone. </span><span class="kobospan" id="kobo.156.3">Automated preprocessing tools can help to quickly identify and remove errors, making the data more reliable </span><span><span class="kobospan" id="kobo.157.1">for analysis.</span></span></p>
<p class="calibre6"><span><em class="italic"><span class="kobospan" id="kobo.158.1">Figure 1</span></em></span><em class="italic"><span class="kobospan" id="kobo.159.1">.1</span></em><span class="kobospan" id="kobo.160.1"> portrays a comprehensive preprocessing pipeline. </span><span class="kobospan" id="kobo.160.2">We will cover this code example in </span><a href="B18949_04.xhtml#_idTextAnchor113" class="calibre5 pcalibre1 pcalibre"><span><em class="italic"><span class="kobospan" id="kobo.161.1">Chapter 4</span></em></span></a><span><span class="kobospan" id="kobo.162.1">:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer006">
<span class="kobospan" id="kobo.163.1"><img alt="Figure 1.1 – Comprehensive preprocessing pipeline" src="image/B18949_01_1.jpg" class="calibre3"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.164.1">Figure 1.1 – Comprehensive preprocessing pipeline</span></p>
<p class="calibre6"><span class="kobospan" id="kobo.165.1">In conclusion, preprocessing text is a vital step in NLP and ML applications; it improves the performance of ML algorithms by removing noise and inconsistencies and standardizing the data. </span><span class="kobospan" id="kobo.165.2">Additionally, it plays a crucial role in data preparation for NLP tasks and in </span><a id="_idIndexMarker011" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.166.1">data cleaning. </span><span class="kobospan" id="kobo.166.2">By investing time and resources in preprocessing, one can ensure that the data is of high quality and is ready for advanced NLP and ML methods, resulting in more accurate and </span><span><span class="kobospan" id="kobo.167.1">reliable results.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.168.1">As our text data is prepared for further processing, the next step typically involves fitting an ML model </span><span><span class="kobospan" id="kobo.169.1">to it.</span></span></p>
<h1 id="_idParaDest-22" class="calibre4"><a id="_idTextAnchor021" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.170.1">A winning synergy – the coming together of NLP and ML</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.171.1">ML is a subfield of AI that involves training</span><a id="_idIndexMarker012" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.172.1"> algorithms to learn from data, allowing them to make predictions or decisions without those being</span><a id="_idIndexMarker013" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.173.1"> explicitly programmed. </span><span class="kobospan" id="kobo.173.2">ML is driving advancements in so many different fields, such as computer vision, voice recognition, and, of </span><span><span class="kobospan" id="kobo.174.1">course, NLP.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.175.1">Diving a little more into the specific techniques of ML, a particular technique used in NLP is </span><strong class="bold"><span class="kobospan" id="kobo.176.1">statistical language modeling</span></strong><span class="kobospan" id="kobo.177.1">, which </span><a id="_idIndexMarker014" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.178.1">involves training algorithms on large text corpora to predict the likelihood of a given sequence of words. </span><span class="kobospan" id="kobo.178.2">This is used in a wide range of applications, such as speech recognition, machine translation, and </span><span><span class="kobospan" id="kobo.179.1">text generation.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.180.1">Another essential technique is </span><strong class="bold"><span class="kobospan" id="kobo.181.1">DL</span></strong><span class="kobospan" id="kobo.182.1">, which is a subfield of ML that involves training artificial neural </span><a id="_idIndexMarker015" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.183.1">networks on large amounts of data. </span><span class="kobospan" id="kobo.183.2">DL models, such as </span><strong class="bold"><span class="kobospan" id="kobo.184.1">convolutional neural networks</span></strong><span class="kobospan" id="kobo.185.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.186.1">CNNs</span></strong><span class="kobospan" id="kobo.187.1">) and </span><strong class="bold"><span class="kobospan" id="kobo.188.1">recurrent neural networks</span></strong><span class="kobospan" id="kobo.189.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.190.1">RNNs</span></strong><span class="kobospan" id="kobo.191.1">), have been </span><a id="_idIndexMarker016" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.192.1">shown to be adequate for NLP tasks such as language understanding, text summarization, and </span><span><span class="kobospan" id="kobo.193.1">sentiment analysis.</span></span></p>
<p class="calibre6"><span><em class="italic"><span class="kobospan" id="kobo.194.1">Figure 1</span></em></span><em class="italic"><span class="kobospan" id="kobo.195.1">.2</span></em><span class="kobospan" id="kobo.196.1"> portrays the relationship between AI, ML, DL, </span><span><span class="kobospan" id="kobo.197.1">and NLP:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer007">
<span class="kobospan" id="kobo.198.1"><img alt="Figure 1.2 – The relationship between the different disciplines" src="image/B18949_01_2.jpg" class="calibre3"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.199.1">Figure 1.2 – The relationship between the different disciplines</span></p>
<h1 id="_idParaDest-23" class="calibre4"><a id="_idTextAnchor022" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.200.1">Introduction to math and statistics in NLP</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.201.1">The solid base for NLP and ML is the </span><a id="_idIndexMarker017" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.202.1">mathematical foundations from which the algorithms stem. </span><span class="kobospan" id="kobo.202.2">In particular, the key foundations are linear algebra, statistics and probability, and optimization theory. </span><a href="B18949_02_split_000.xhtml#_idTextAnchor026" class="calibre5 pcalibre1 pcalibre"><span><em class="italic"><span class="kobospan" id="kobo.203.1">Chapter 2</span></em></span></a><span class="kobospan" id="kobo.204.1"> will survey the key topics you will need to understand these topics. </span><span class="kobospan" id="kobo.204.2">Throughout the book, we will present proofs and justifications for the various methods </span><span><span class="kobospan" id="kobo.205.1">and hypotheses.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.206.1">One of the challenges in NLP is </span><a id="_idIndexMarker018" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.207.1">dealing with the vast amount of data that is generated in human language. </span><span class="kobospan" id="kobo.207.2">This includes understanding the context, as well as the meaning of the words and relationships between them. </span><span class="kobospan" id="kobo.207.3">To deal with this challenge, researchers have developed various techniques, such as embeddings and attention mechanisms, which represent the meaning of words in a numerical format and help identify the most critical parts of the </span><span><span class="kobospan" id="kobo.208.1">text, respectively.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.209.1">Another challenge in NLP is the need for labeled data, as manually annotating large text corpora is expensive and time-consuming. </span><span class="kobospan" id="kobo.209.2">To address this problem, researchers have developed unsupervised and weakly supervised methods that can learn from unlabeled data, such as clustering, topic modeling, and </span><span><span class="kobospan" id="kobo.210.1">self-supervised learning.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.211.1">Overall, NLP is a rapidly evolving field that has the potential to transform the way we interact with computers and</span><a id="_idIndexMarker019" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.212.1"> information. </span><span class="kobospan" id="kobo.212.2">It is used in various applications, from chatbots and language </span><a id="_idIndexMarker020" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.213.1">translation to text summarization and sentiment analysis. </span><span class="kobospan" id="kobo.213.2">The use of ML techniques, such as statistical language modeling and DL, has been crucial in developing these systems. </span><span class="kobospan" id="kobo.213.3">Ongoing research addresses the remaining challenges, such as understanding context and dealing with the lack of </span><span><span class="kobospan" id="kobo.214.1">labeled data.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.215.1">One of the most significant</span><a id="_idIndexMarker021" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.216.1"> advances in NLP has been the development of pre-trained language models, such as </span><strong class="bold"><span class="kobospan" id="kobo.217.1">bidirectional encoder representations from transformers</span></strong><span class="kobospan" id="kobo.218.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.219.1">BERTs</span></strong><span class="kobospan" id="kobo.220.1">) and </span><strong class="bold"><span class="kobospan" id="kobo.221.1">generative pre-trained transformers</span></strong><span class="kobospan" id="kobo.222.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.223.1">GPTs</span></strong><span class="kobospan" id="kobo.224.1">). </span><span class="kobospan" id="kobo.224.2">These models have been trained on massive amounts </span><a id="_idIndexMarker022" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.225.1">of text data and can be fine-tuned for specific tasks, such as sentiment analysis or </span><span><span class="kobospan" id="kobo.226.1">language translation.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.227.1">Transformers, the technology behind the BERT and GPT models, revolutionized NLP by enabling machines to understand the context of words in sentences more effectively. </span><span class="kobospan" id="kobo.227.2">Unlike previous methods that processed text linearly, transformers can handle words in parallel, capturing nuances in language through attention mechanisms. </span><span class="kobospan" id="kobo.227.3">This allows them to discern the importance of each word relative to others, greatly enhancing the model’s ability to grasp complex language patterns and nuances and setting a new standard for accuracy and fluency in NLP applications. </span><span class="kobospan" id="kobo.227.4">This has enhanced the creation of NLP applications and has led to improved performance on a wide range of </span><span><span class="kobospan" id="kobo.228.1">NLP tasks.</span></span></p>
<p class="calibre6"><span><em class="italic"><span class="kobospan" id="kobo.229.1">Figure 1</span></em></span><em class="italic"><span class="kobospan" id="kobo.230.1">.3</span></em><span class="kobospan" id="kobo.231.1"> details the functional design of the </span><span><span class="kobospan" id="kobo.232.1">Transformer component.</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer008">
<span class="kobospan" id="kobo.233.1"><img alt="Figure 1.3 – Transformer in model architecture" src="image/B18949_01_3.jpg" class="calibre3"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.234.1">Figure 1.3 – Transformer in model</span><a href="https://arxiv.org/abs/1706.03762" class="calibre5 pcalibre1 pcalibre"><span class="kobospan" id="kobo.235.1"> architecture</span></a></p>
<p class="calibre6"><span class="kobospan" id="kobo.236.1">Another important development in NLP has been the increase in the availability of large amounts of annotated text data, which has allowed for the training of more accurate models. </span><span class="kobospan" id="kobo.236.2">Additionally, the development of unsupervised and semi-supervised learning </span><a id="_idIndexMarker023" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.237.1">techniques has allowed for the training of models on smaller amounts of labeled data, making it possible to apply NLP in a wider range </span><span><span class="kobospan" id="kobo.238.1">of scenarios.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.239.1">Language models have had a significant impact on the field of NLP. </span><span class="kobospan" id="kobo.239.2">One of the key ways that language models have changed the field is by improving the accuracy and effectiveness of natural language processing tasks. </span><span class="kobospan" id="kobo.239.3">For example, many language models have been trained on large amounts of text data, allowing them to better understand the nuances and complexities of human language. </span><span class="kobospan" id="kobo.239.4">This has led to improved performance in tasks such as language translation, text summarization, and </span><span><span class="kobospan" id="kobo.240.1">sentiment analysis.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.241.1">Another way that language models have changed the field of NLP is by enabling the development of more advanced, sophisticated NLP</span><a id="_idIndexMarker024" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.242.1"> systems. </span><span class="kobospan" id="kobo.242.2">For example, some language models, such as GPT, can generate human-like text, which has opened up new possibilities for natural language generation and dialogue systems. </span><span class="kobospan" id="kobo.242.3">Other language models, such as BERT, have improved the performance of tasks such as question answering, sentiment analysis, and named </span><span><span class="kobospan" id="kobo.243.1">entity recognition.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.244.1">Language models have also changed the field by making it more accessible to a broader range of people. </span><span class="kobospan" id="kobo.244.2">With the advent of pre-trained language models, developers can now easily fine-tune these models to specific tasks without the need for large amounts of labeled data or the expertise to train models from scratch. </span><span class="kobospan" id="kobo.244.3">This has made it easier for developers to build NLP applications and has led to an explosion of new NLP-based products </span><span><span class="kobospan" id="kobo.245.1">and services.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.246.1">Overall, language models have played a key role in advancing the field of NLP by improving the performance of existing NLP tasks, enabling the development of more advanced NLP systems, and making NLP more accessible to a broader range </span><span><span class="kobospan" id="kobo.247.1">of people.</span></span></p>
<h2 id="_idParaDest-24" class="calibre7"><a id="_idTextAnchor023" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.248.1">Understanding language models – ChatGPT example</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.249.1">ChatGPT, a variant of the GPT model, has become popular because of its ability to generate human-like text, which can be used for a broad range of natural language generation tasks, such as chatbot systems, text </span><a id="_idIndexMarker025" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.250.1">summarization, and </span><span><span class="kobospan" id="kobo.251.1">dialogue systems.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.252.1">The main reason for its popularity is its high-quality outputs and its ability to generate text that is hard to distinguish from text written by humans. </span><span class="kobospan" id="kobo.252.2">This makes it well-suited for applications that require</span><a id="_idIndexMarker026" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.253.1"> natural-sounding text, such as chatbot systems, virtual assistants, and </span><span><span class="kobospan" id="kobo.254.1">text summarization.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.255.1">Additionally, ChatGPT is pre-trained on a large amount of text data, allowing it to understand human language nuances and complexities. </span><span class="kobospan" id="kobo.255.2">This makes it well-suited for applications that require a deep understanding of language, such as question answering and </span><span><span class="kobospan" id="kobo.256.1">sentiment analysis.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.257.1">Moreover, ChatGPT can be fine-tuned for specific use cases by providing it with a small amount of task-specific data, which makes it versatile and adaptable to a wide range of applications. </span><span class="kobospan" id="kobo.257.2">It is widely used in industry, research, and personal projects, ranging from customer service chatbots, virtual assistants, automated content creation, text summarization, dialogue systems, question answering, and </span><span><span class="kobospan" id="kobo.258.1">sentiment analysis.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.259.1">Overall, ChatGPT’s ability to generate high-quality, human-like text and its ability to be fine-tuned for specific tasks makes it a popular choice for a wide range of natural language </span><span><span class="kobospan" id="kobo.260.1">generation applications.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.261.1">Let’s move on to summarize the </span><span><span class="kobospan" id="kobo.262.1">chapter now.</span></span></p>
<h1 id="_idParaDest-25" class="calibre4"><a id="_idTextAnchor024" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.263.1">Summary</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.264.1">In this chapter, we introduced you to the field of NLP, which is a subfield of AI. </span><span class="kobospan" id="kobo.264.2">The chapter highlights the importance of mathematical foundations, such as linear algebra, statistics and probability, and optimization theory, which are necessary to understand the algorithms used in NLP. </span><span class="kobospan" id="kobo.264.3">It also covers the challenges faced in NLP, such as understanding the context and meaning of words, the relationships between them, and the need for labeled data. </span><span class="kobospan" id="kobo.264.4">We discussed the recent advancements in NLP, including pre-trained language models, such as BERT and GPT, and the availability of large amounts of text data, which has led to improved performance in NLP tasks. </span><span class="kobospan" id="kobo.264.5">We touched on the importance of text preprocessing as you gains knowledge of the importance of data cleaning, data normalization, stemming, and lemmatization in text preprocessing. </span><span class="kobospan" id="kobo.264.6">We then talked about how the coming together of NLP and ML is driving advancements in the field and is becoming an increasingly important tool for automating tasks and improving </span><span><span class="kobospan" id="kobo.265.1">human-computer interaction.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.266.1">After learning from this chapter, you will be able to understand the importance of NLP, ML, and DL techniques. </span><span class="kobospan" id="kobo.266.2">you will be able to understand the recent advancements in NLP, including pre-trained language models. </span><span class="kobospan" id="kobo.266.3">you will also have gained knowledge of the importance of text preprocessing and how it plays a crucial role in data preparation for NLP tasks and in </span><span><span class="kobospan" id="kobo.267.1">data cleaning.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.268.1">In the next chapter, we will cover the mathematical foundations of ML. </span><span class="kobospan" id="kobo.268.2">These foundations will serve us throughout </span><span><span class="kobospan" id="kobo.269.1">the book.</span></span></p>
<h1 id="_idParaDest-26" class="calibre4"><a id="_idTextAnchor025" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.270.1">Questions and answers</span></h1>
<ol class="calibre16">
<li class="calibre15"><span class="kobospan" id="kobo.271.1">What is natural language </span><span><span class="kobospan" id="kobo.272.1">processing (NLP)?</span></span><ul class="calibre17"><li class="calibre15"><span class="kobospan" id="kobo.273.1">Q: What defines NLP in the field of </span><span><span class="kobospan" id="kobo.274.1">artificial intelligence?</span></span></li><li class="calibre15"><span class="kobospan" id="kobo.275.1">A: NLP is a subfield of AI focused on enabling computers to understand, interpret, and generate human language in a way that is both natural and meaningful to </span><span><span class="kobospan" id="kobo.276.1">human users.</span></span></li></ul></li>
<li class="calibre15"><span class="kobospan" id="kobo.277.1">Initial strategies in machine processing of </span><span><span class="kobospan" id="kobo.278.1">natural language.</span></span><ul class="calibre17"><li class="calibre15"><span class="kobospan" id="kobo.279.1">Q: What is the importance of preprocessing </span><span><span class="kobospan" id="kobo.280.1">in NLP?</span></span></li><li class="calibre15"><span class="kobospan" id="kobo.281.1">A: Preprocessing, including tasks such as removing stop words and applying stemming or lemmatization, is crucial for cleaning and preparing text data, thereby improving the performance of machine learning algorithms on </span><span><span class="kobospan" id="kobo.282.1">NLP tasks.</span></span></li></ul></li>
<li class="calibre15"><span class="kobospan" id="kobo.283.1">The synergy of NLP and machine </span><span><span class="kobospan" id="kobo.284.1">learning (ML).</span></span><ul class="calibre17"><li class="calibre15"><span class="kobospan" id="kobo.285.1">Q: How does machine learning contribute to advancements </span><span><span class="kobospan" id="kobo.286.1">in NLP?</span></span></li><li class="calibre15"><span class="kobospan" id="kobo.287.1">A: ML, especially techniques such as statistical language modeling and deep learning, drives NLP forward by enabling algorithms to learn from data, predict word sequences, and perform tasks such as language understanding and sentiment analysis </span><span><span class="kobospan" id="kobo.288.1">more effectively.</span></span></li></ul></li>
<li class="calibre15"><span class="kobospan" id="kobo.289.1">Introduction to math and statistics </span><span><span class="kobospan" id="kobo.290.1">in NLP</span></span><ul class="calibre17"><li class="calibre15"><span class="kobospan" id="kobo.291.1">Q: Why are mathematical foundations important </span><span><span class="kobospan" id="kobo.292.1">in NLP?</span></span></li><li class="calibre15"><span class="kobospan" id="kobo.293.1">A: Mathematical foundations such as linear algebra, statistics, and probability are essential for understanding and developing the algorithms that underpin NLP techniques, from basic preprocessing to complex </span><span><span class="kobospan" id="kobo.294.1">model training.</span></span></li></ul></li>
<li class="calibre15"><span class="kobospan" id="kobo.295.1">Advancements in NLP – the role of pre-trained </span><span><span class="kobospan" id="kobo.296.1">language models</span></span><ul class="calibre17"><li class="calibre15"><span class="kobospan" id="kobo.297.1">Q: How have pre-trained models such as BERT and GPT </span><span><span class="kobospan" id="kobo.298.1">influenced NLP?</span></span></li><li class="calibre15"><span class="kobospan" id="kobo.299.1">A: Pre-trained models, trained on vast amounts of text data, can be fine-tuned for specific tasks such as sentiment analysis or language translation, significantly simplifying the development of NLP applications and enhancing </span><span><span class="kobospan" id="kobo.300.1">task performance.</span></span></li></ul></li>
<li class="calibre15"><span class="kobospan" id="kobo.301.1">Understanding transformers in </span><span><span class="kobospan" id="kobo.302.1">language models</span></span><ul class="calibre17"><li class="calibre15"><span class="kobospan" id="kobo.303.1">Q: Why are transformers considered a breakthrough </span><span><span class="kobospan" id="kobo.304.1">in NLP?</span></span></li><li class="calibre15"><span class="kobospan" id="kobo.305.1">A: Transformers process words in parallel and use attention mechanisms to understand word context within sentences, significantly improving a model’s ability to handle the complexities of </span><span><span class="kobospan" id="kobo.306.1">human language.</span></span></li></ul></li>
</ol>
</div>
</body></html>