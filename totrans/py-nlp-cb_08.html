<html><head></head><body>
		<div><h1 id="_idParaDest-201" class="chapter-number"><a id="_idTextAnchor205" class="calibre6 pcalibre pcalibre1"/>8</h1>
			<h1 id="_idParaDest-202" class="calibre7"><a id="_idTextAnchor206" class="calibre6 pcalibre pcalibre1"/>Transformers and  Their Applications</h1>
			<p class="calibre3">In this chapter, we will learn about transformers and how to apply them to perform various NLP tasks. Typical tasks in the NLP domain involve loading and processing data so that it can be used downstream seamlessly. Once the data is read, another task is that of transforming the data into a form that the various models can use. Once the data is transformed into the requisite format, we use it to perform the actual tasks, such as classification, text generation, and language translation.</p>
			<p class="calibre3">Here is a list of the recipes in this chapter:</p>
			<ul class="calibre15">
				<li class="calibre14">Loading a dataset</li>
				<li class="calibre14">Tokenizing the text in your dataset</li>
				<li class="calibre14">Using the tokenized text to perform classification with Transformer models</li>
				<li class="calibre14">Using different Transformer models based on different requirements</li>
				<li class="calibre14">Generating text by taking a cue from an initial starting sentence</li>
				<li class="calibre14">Translating text between different languages using pre-trained Transformer models</li>
			</ul>
			<h1 id="_idParaDest-203" class="calibre7"><a id="_idTextAnchor207" class="calibre6 pcalibre pcalibre1"/><a id="_idTextAnchor208" class="calibre6 pcalibre pcalibre1"/>Technical requirements</h1>
			<p class="calibre3">The code for this chapter is in the <code>Chapter08</code> folder in the GitHub repository of the book (<a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter08" class="calibre6 pcalibre pcalibre1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter08</a>).</p>
			<p class="calibre3">As in previous chapters, the packages required for the chapter are part of the <code>poetry</code> environment. Alternatively, you can install all the packages using the <code>requirements.txt</code> file.</p>
			<h1 id="_idParaDest-204" class="calibre7"><a id="_idTextAnchor209" class="calibre6 pcalibre pcalibre1"/>Loading a dataset</h1>
			<p class="calibre3">In this recipe, we will learn <a id="_idIndexMarker440" class="calibre6 pcalibre pcalibre1"/>how to load a public dataset and work with it. We will use the <code>RottenTomatoes</code> dataset for this recipe as an example. This dataset contains ratings and reviews for movies. Please refer to the link at <a href="https://www.kaggle.com/datasets/stefanoleone992/rotten-tomatoes-movies-and-critic-reviews-dataset" class="calibre6 pcalibre pcalibre1">https://www.kaggle.com/datasets/stefanoleone992/rotten-tomatoes-movies-and-critic-reviews-dataset</a> for more information about the dataset</p>
			<h2 id="_idParaDest-205" class="calibre5"><a id="_idTextAnchor210" class="calibre6 pcalibre pcalibre1"/>Getting ready</h2>
			<p class="calibre3">As part of this chapter, we will use the libraries from the <code>HuggingFace</code> site (<a href="http://huggingface.co" class="calibre6 pcalibre pcalibre1">huggingface.co</a>). For this recipe, we will use the dataset package. You can use the <code>8.1_Transformers_dataset.ipynb</code> notebook from the code site if you need to work from an existing notebook.</p>
			<h2 id="_idParaDest-206" class="calibre5"><a id="_idTextAnchor211" class="calibre6 pcalibre pcalibre1"/>How to do it...</h2>
			<p class="calibre3">In this recipe, you will load the <code>RottenTomatoes</code> dataset from the <code>HuggingFace</code> site using the dataset package. This package will download the dataset for you if it does not exist. For any subsequent runs, it will use the downloaded dataset from the cache if it was downloaded previously.</p>
			<p class="calibre3">The recipe does the following things:</p>
			<ul class="calibre15">
				<li class="calibre14"> Reads in the <strong class="source-inline1">RottenTomatoes</strong> dataset</li>
				<li class="calibre14"> Describes the features of the dataset</li>
				<li class="calibre14"> Loads the data from the training split of the dataset</li>
				<li class="calibre14"> Samples a few sentences from the dataset and prints them</li>
			</ul>
			<p class="calibre3">The steps for the recipe are as follows:</p>
			<ol class="calibre13">
				<li class="calibre14">Do the necessary imports to import the necessary types and functions from the datasets package:<pre class="source-code">
from datasets import load_dataset, get_dataset_split_names</pre></li>				<li class="calibre14">Load <strong class="source-inline1">"rotten tomatoes"</strong> via the <strong class="source-inline1">load_dataset</strong> function and print the internal<a id="_idIndexMarker441" class="calibre6 pcalibre pcalibre1"/> dataset splits. This dataset contains the train, validation, and test splits:<pre class="source-code">
dataset = load_dataset("rotten_tomatoes")
print(get_dataset_split_names("rotten_tomatoes"))</pre><p class="calibre3">The output of the preceding command would be as follows:</p><pre class="source-code">['train', 'validation', 'test']</pre></li>				<li class="calibre14">Load the dataset and print the attributes of the train split. The <strong class="source-inline1">training_data.description</strong> describes the dataset details and the <strong class="source-inline1">training_data.features</strong> describes the features of the dataset. In the output, we can see that the <strong class="source-inline1">training_data</strong> split contains the features <strong class="source-inline1">text</strong>, which is of the type string, and <strong class="source-inline1">label</strong>, which is of type categorical, with the values <strong class="source-inline1">neg</strong> and <strong class="source-inline1">pos</strong>:<pre class="source-code">
training_data = dataset['train']
print(training_data.description)
print(training_data.features)</pre><p class="calibre3">The output of the command is as follows:</p><pre class="source-code">Movie Review Dataset.
This is a dataset of containing 5,331 positive and 5,331 negative processed  sentences from Rotten Tomatoes movie reviews. This data was first used in Bo Pang and Lillian Lee, ``Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.'', Proceedings of the ACL, 2005.
{'text': Value(dtype='string', id=None), 
    'label':ClassLabel(names=['neg', 'pos'], id=None)}</pre></li>				<li class="calibre14">Now that we have loaded the dataset, we will print the first five sentences from it. This is just to confirm that we are indeed able to read from the dataset:<pre class="source-code">
sentences = training_data['text'][:5]
[print(sentence) for sentence in sentences]</pre><p class="calibre3">The <a id="_idIndexMarker442" class="calibre6 pcalibre pcalibre1"/>output of the command is as follows:</p><pre class="source-code">the rock is destined to be the 21st century's new " conan " and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .
the gorgeously elaborate continuation of " the lord of the rings " trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .
effective but too-tepid biopic
if you sometimes like to go to the movies to have fun , wasabi is a good place to start .
emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one<a id="_idTextAnchor212" class="pcalibre pcalibre1 calibre20"/> .</pre></li>			</ol>
			<h1 id="_idParaDest-207" class="calibre7"><a id="_idTextAnchor213" class="calibre6 pcalibre pcalibre1"/>Tokenizing the text in your dataset</h1>
			<p class="calibre3">The<a id="_idIndexMarker443" class="calibre6 pcalibre pcalibre1"/> components <a id="_idIndexMarker444" class="calibre6 pcalibre pcalibre1"/>contained within the transformer do not have any intrinsic knowledge of the words that it processes. Instead, the tokenizer only uses the token identifiers for the words that it processes. In this recipe, we will learn how to transform the text in your dataset into a representation that can be used by the models for downstream tasks.</p>
			<h2 id="_idParaDest-208" class="calibre5"><a id="_idTextAnchor214" class="calibre6 pcalibre pcalibre1"/>Getting ready</h2>
			<p class="calibre3">As <a id="_idIndexMarker445" class="calibre6 pcalibre pcalibre1"/>part of this recipe, we will use the <code>AutoTokenizer</code> module <a id="_idIndexMarker446" class="calibre6 pcalibre pcalibre1"/>from the transformers package. You can use the <code>8.2_Basic_Tokenization.ipynb</code> notebook from the code site if you need to work from an existing notebook.</p>
			<h2 id="_idParaDest-209" class="calibre5"><a id="_idTextAnchor215" class="calibre6 pcalibre pcalibre1"/>How to do it...</h2>
			<p class="calibre3">In this recipe, you will continue from the previous example of using the <code>RottenTomatoes</code> dataset and sampling a few sentences from it. We will then encode the sampled sentences into tokens and their respective representations.</p>
			<p class="calibre3">The recipe does the following things:</p>
			<ul class="calibre15">
				<li class="calibre14"> Loads a few sentences into memory</li>
				<li class="calibre14"> Instantiates a tokenizer and tokenizes the sentences</li>
				<li class="calibre14"> Converts the token IDs generated from the previous step back into the tokens</li>
			</ul>
			<p class="calibre3">The steps for the recipe are as follows:</p>
			<ol class="calibre13">
				<li class="calibre14">Do the necessary imports to import the necessary <strong class="source-inline1">AutoTokenizer</strong> module from the <strong class="source-inline1">transformers</strong> library:<pre class="source-code">
from transformers import AutoTokenizer</pre></li>				<li class="calibre14">We initialize a sentence array consisting of three sentences that we will use for this example. These sentences are of different lengths and have a good combination of the same and different words. This will allow us to understand how the tokenized representation varies for each of them:<pre class="source-code">
sentences = [
    "The first sentence, which is the longest one in the list.",
    "The second sentence is not that long.",
    "A very short sentence."]</pre></li>				<li class="calibre14">Instantiate a tokenizer of the <strong class="source-inline1">bert-base-cased</strong> type. This tokenizer is case-sensitive. This means that the words star and STAR will have different tokenized representations:<pre class="source-code">
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")</pre></li>				<li class="calibre14">In this <a id="_idIndexMarker447" class="calibre6 pcalibre pcalibre1"/>step, we tokenize all the sentences in the <strong class="source-inline1">sentences</strong> array. We <a id="_idIndexMarker448" class="calibre6 pcalibre pcalibre1"/>call the tokenizer constructor and pass it the <strong class="source-inline1">sentences</strong> array as an argument followed by printing the <strong class="source-inline1">tokenized_output</strong> instance returned by the constructor function. This object is a dictionary of three items:<ul class="calibre19"><li class="calibre14"><strong class="source-inline1">input_ids</strong>: These are the numerical token identifiers that are assigned to each token.</li><li class="calibre14"><strong class="source-inline1">token_type_ids</strong>: These IDs define the type of tokens that are contained in the sentences.</li><li class="calibre14"><strong class="source-inline1">attention_mask</strong>: These define the attention values for each token in the input. This mask determines what tokens are paid attention to when downstream tasks are performed. These values are floats and can vary from 0 (no attention) to 1 (full attention).<pre class="source-code">
tokenized_input = tokenizer(sentences)
print(tokenized_input)
{'input_ids': [[101, 1109, 1148, 5650, 117, 1134, 1110, 1103, 6119, 1141, 1107, 1103, 2190, 119, 102],
[101, 1109, 1248, 5650, 1110, 1136, 1115, 1263, 119, 102],[101, 138, 1304, 1603, 5650, 119, 102]],
'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0]],
'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1]]}</pre></li></ul></li>				<li class="calibre14">In this step, we <a id="_idIndexMarker449" class="calibre6 pcalibre pcalibre1"/>take the input IDs of the first sentence and <a id="_idIndexMarker450" class="calibre6 pcalibre pcalibre1"/>convert them back into tokens:<pre class="source-code">
tokens = tokenizer.convert_ids_to_tokens(
    tokenized_input["input_ids"][0])
print(tokens)
['[CLS]', 'The', 'first', 'sentence', ',', 'which', 'is', 'the', 'longest', 'one', 'in', 'the', 'list', '.', '[SEP]']
[101, 1109, 1148, 5650, 117, 1134, 1110, 1103, 6119, 1141, 1107, 1103, 2190, 119, 102]</pre><p class="calibre3">Converting them into tokens returns the following output:</pre><pre class="source-code">['[CLS]', 'The', 'first', 'sentence', ',', 'which', 'is', 'the', 'longest', 'one', 'in', 'the', 'list', '.', '[SEP]']</pre><p class="calibre3">In addition to the original tokens, the <a id="_idIndexMarker451" class="calibre6 pcalibre pcalibre1"/>tokenizer adds the <code>[CLS]</code> and <code>[SEP]</code>. These tokens were added for the training tasks that were performed to train BERT.</p><p class="calibre3">Now that we have learned about the internal representation of the text used by the transformer internally, let us learn how we can classify a piece of text into different categories.</p></li>			</ol>
			<h1 id="_idParaDest-210" class="calibre7"><a id="_idTextAnchor216" class="calibre6 pcalibre pcalibre1"/>Classifying text</h1>
			<p class="calibre3">In this recipe, we will use<a id="_idIndexMarker453" class="calibre6 pcalibre pcalibre1"/> the <code>RottenTomatoes</code> dataset and classify the review texts for sentiment. We will classify the test split of the dataset and evaluate the results of the classifier against the true labels in the test split of the dataset.</p>
			<h2 id="_idParaDest-211" class="calibre5"><a id="_idTextAnchor217" class="calibre6 pcalibre pcalibre1"/>Getting ready</h2>
			<p class="calibre3">As part of this recipe, we will use the pipeline module from the transformers package. You can use the <code>8.3_Classification_And_Evaluation.ipynb</code> notebook from the code site if you need to work from an existing notebook.</p>
			<h2 id="_idParaDest-212" class="calibre5"><a id="_idTextAnchor218" class="calibre6 pcalibre pcalibre1"/>How to do it...</h2>
			<p class="calibre3">In this recipe, you will continue from the previous example using the <code>RottenTomatoes</code> dataset and sample a few sentences from it. We will then classify a small subset of five sentences for sentiment classification and demonstrate the results on this smaller subset. We will then perform inference on the whole test split of the dataset and evaluate the results of the classification.</p>
			<p class="calibre3">The recipe does the following things:</p>
			<ul class="calibre15">
				<li class="calibre14">Loads the <strong class="source-inline1">RottenTomatoes</strong> dataset and prints the first five sentences from it</li>
				<li class="calibre14">Instantiates a pipeline with a pre-trained Roberta model that has been trained on the same dataset for sentiment analysis</li>
				<li class="calibre14">Performs inference (or sentiment prediction) on the whole test split of the dataset using the pipeline</li>
				<li class="calibre14">Evaluates the results of the inference</li>
			</ul>
			<p class="calibre3">The steps for the<a id="_idIndexMarker454" class="calibre6 pcalibre pcalibre1"/> recipe are as follows:</p>
			<ol class="calibre13">
				<li class="calibre14">Do the necessary imports to import the required packages and modules:<pre class="source-code">
from datasets import load_dataset
from evaluate import evaluator, combine
from transformers import pipeline
import torch</pre></li>				<li class="calibre14">In this step, we <a id="_idIndexMarker455" class="calibre6 pcalibre pcalibre1"/>probe for the presence of a <strong class="bold">Compute Unified Device Architecture</strong> (<strong class="bold">CUDA</strong>) compatible <a id="_idIndexMarker456" class="calibre6 pcalibre pcalibre1"/>device (or <strong class="bold">Graphics Processing Unit</strong> (<strong class="bold">GPU</strong>)) present in the system. If such a device is present, our model will be loaded on it. This accelerates the training and inference performance of a model if it is supported. However, if such a device is not present, the <strong class="bold">Central Processing Unit</strong> (<strong class="bold">CPU</strong>) will<a id="_idIndexMarker457" class="calibre6 pcalibre pcalibre1"/> be used. We also load the <strong class="source-inline1">RottenTomatoes</strong> dataset and select the first five sentences from it. This is to ensure we are indeed able to read the data present in the dataset:<pre class="source-code">
device = torch.device(
    "cuda" if torch.cuda.is_available() else "cpu")
sentences = load_dataset(
    "rotten_tomatoes", split="test").select(range(5))
[print(sentence) for sentence in sentences['text']]
lovingly photographed in the manner of a golden book sprung to life , stuart little 2 manages sweetness largely without stickiness .
consistently clever and suspenseful .
it's like a " big chill " reunion of the baader-meinhof gang , only these guys are more harmless pranksters than political activists .
the story gives ample opportunity for large-scale action and suspense , which director shekhar kapur supplies with tremendous skill .
red dragon " never cuts corners .</pre></li>				<li class="calibre14">Initialize <a id="_idIndexMarker458" class="calibre6 pcalibre pcalibre1"/>the pipeline for sentiment analysis via a pipeline. A pipeline is an abstraction that allows us to easily use models or inference tasks without having to write the code to piece them together. We load the <strong class="source-inline1">roberta-base-rotten-tomatoes</strong> model from <strong class="source-inline1">textattack</strong>, which has been trained on this dataset. In the following segment, we use the pipeline for the sentiment analysis task and set a specific model to be used for this task:<pre class="source-code">
roberta_pipe = pipeline("sentiment-analysis",
    model="textattack/roberta-base-rotten-tomatoes")</pre></li>				<li class="calibre14">In this step, we generate the predictions for the small subset of sentences that we selected in step 2. Using the pipeline object to generate predictions is as easy as just passing it a series of sentences. If you are running this example on a machine without a compatible CUDA device, this step might take a little time:<pre class="source-code">
predictions = roberta_pipe(sentences['text'])</pre></li>				<li class="calibre14">In this step, we iterate through our sentences and check the predictions for our sentences. We print the actual and generated predictions along with the sentence text for the five sentences. The actual labels are read from the dataset, whereas the predictions are generated via the pipeline object:<pre class="source-code">
for idx, _sentence in enumerate(sentences['text']):
    print(
        f"actual: {sentences['label'][idx]}\n"
        f"predicted: {'1' if predictions[idx]['label'] 
            == 'LABEL_1' else '0'}\n"
        f"sentence: {_sentence}\n\n"
    )
actual:1
predicted:1
sentence:lovingly photographed in the manner of a golden book sprung to life , stuart little 2 manages sweetness largely without stickiness .
actual:1
predicted:1
sentence:consistently clever and suspenseful .
actual:1
predicted:0
sentence:it's like a " big chill " reunion of the baader-meinhof gang , only these guys are more harmless pranksters than political activists .
actual:1
predicted:1
sentence:the story gives ample opportunity for large-scale action and suspense , which director shekhar kapur supplies with tremendous skill .
actual:1
predicted:1
sentence:red dragon " never cuts corners .</pre></li>				<li class="calibre14">Now <a id="_idIndexMarker459" class="calibre6 pcalibre pcalibre1"/>that we have validated the pipeline and its results, let’s generate the inference for the whole test set and generate the evaluation measures of this particular model. Load the complete test split for the <strong class="source-inline1">RottenTomatoes</strong> dataset:<pre class="source-code">
sentences = load_dataset("rotten_tomatoes", split="test")</pre></li>				<li class="calibre14">In this step, we initialize an evaluator object that can be used to perform the inference along with evaluating the results of the classification. It can also be used to present an easy-to-read summary of the evaluation results:<pre class="source-code">
task_evaluator = evaluator("sentiment-analysis")</pre></li>				<li class="calibre14">In this step, we call the <strong class="source-inline1">compute</strong> method on the <strong class="source-inline1">evaluator</strong> instance. This triggers the inference and the evaluation using the same pipeline instance that we initialized in step 4. It returns the evaluation metrics of <strong class="source-inline1">accuracy</strong>, <strong class="source-inline1">precision</strong>, <strong class="source-inline1">recall</strong>, and <strong class="source-inline1">f1</strong>, along with some performance metrics related to inference:<pre class="source-code">
eval_results = task_evaluator.compute(
    model_or_pipeline=roberta_pipe,
    data=sentences,
    metric=combine(["accuracy", "precision", "recall", "f1"]),
    label_mapping={"LABEL_0": 0, "LABEL_1": 1}
)</pre></li>				<li class="calibre14">In this step, we <a id="_idIndexMarker460" class="calibre6 pcalibre pcalibre1"/>print the results of the evaluation. Of note are the <strong class="source-inline1">precision</strong>, <strong class="source-inline1">recall</strong>, and <strong class="source-inline1">f1</strong> values. An <strong class="source-inline1">f1</strong> of <strong class="source-inline1">0.88</strong>, observed in this case, is an indicator of the very good efficacy of the classifier, though it could always be improved further:<pre class="source-code">
print(eval_results)
{'accuracy': 0.88,
'precision': 0.92,
'recall': 0.84,
'f1': 0.88,
'total_time_in_seconds': 27.23,
'samples_per_second': 39.146,
'latency_in_seconds': 0.025}</pre></li>			</ol>
			<p class="calibre3">In this recipe, we used a pre-trained classifier to classify the data on a dataset. The dataset and model were both for sentiment analysis. There are cases where we can use classifiers that are trained on a different class of data but can still be used as is. This saves us from having to train a classifier of our own and repurpose a model that already exists. We will learn about this use case in the next recipe.</p>
			<h1 id="_idParaDest-213" class="calibre7"><a id="_idTextAnchor219" class="calibre6 pcalibre pcalibre1"/>Using a zero-shot classifier</h1>
			<p class="calibre3">In this recipe, we will classify a sentence using a zero-shot classifier. There are instances where we do not have the luxury of training a classifier from scratch or using a model that has been trained as per the labels of our<a id="_idIndexMarker461" class="calibre6 pcalibre pcalibre1"/> data. <strong class="bold">Zero-shot classification</strong> can be used in such scenarios for any team to get up and running quickly. The zero in the terminology means that the classifier has not seen any data (zero samples precisely) from the target dataset that will be used for inference.</p>
			<h2 id="_idParaDest-214" class="calibre5"><a id="_idTextAnchor220" class="calibre6 pcalibre pcalibre1"/>Getting ready</h2>
			<p class="calibre3">As part of this recipe, we will use the pipeline module from the transformers package. You can use the <code>8.4_Zero_shot_classification.ipynb</code> notebook from the code site if you need to work from an existing notebook.</p>
			<h2 id="_idParaDest-215" class="calibre5"><a id="_idTextAnchor221" class="calibre6 pcalibre pcalibre1"/>How to do it...</h2>
			<p class="calibre3">In this <a id="_idIndexMarker462" class="calibre6 pcalibre pcalibre1"/>recipe, we will use a couple of sentences and classify them. We will use our own set of labels for these sentences. We will use the <code>facebook/bart-large-mnli </code>model for this recipe. This model is suitable for the task of zero-shot classification.</p>
			<p class="calibre3">The recipe does the following things:</p>
			<ul class="calibre15">
				<li class="calibre14"> Initializes a pipeline based on a zero-shot classification model</li>
				<li class="calibre14"> Uses the pipeline to classify a sentence into a custom set of user-defined labels</li>
				<li class="calibre14">Prints the results of the classification with the classes and their associated probabilities</li>
			</ul>
			<p class="calibre3">The steps for the recipe are as follows:</p>
			<ol class="calibre13">
				<li class="calibre14">Do the necessary imports and identify the compute device, as described in the previous classification recipe:<pre class="source-code">
from transformers import pipeline
import torch
device = torch.device(
    "cuda" if torch.cuda.is_available() else "cpu")</pre></li>				<li class="calibre14">In this step, we initialize a pipeline instance with the <strong class="source-inline1">facebook/bart-large-mnli</strong> model. We have chosen this particular model for our example, but other models can also be used – available on the <strong class="source-inline1">HuggingFace</strong> site:<pre class="source-code">
pipeline_instance = pipeline(
    model="facebook/bart-large-mnli")</pre></li>				<li class="calibre14">Use the<a id="_idIndexMarker463" class="calibre6 pcalibre pcalibre1"/> pipeline instance to classify a sentence into a given set of candidate labels. The labels provided in the example are completely novel and have been defined by us. The model was not trained on examples with these labels. The classification output is stored in the <strong class="source-inline1">result</strong> variable, which is a dictionary. This dictionary has the <strong class="source-inline1">'sequence'</strong>, <strong class="source-inline1">'labels'</strong>, and <strong class="source-inline1">'scores'</strong> keys. The <strong class="source-inline1">'sequence'</strong> element stores the original sentence passed to the classifier. The <strong class="source-inline1">'labels'</strong> element stores the labels for the classes, but the ordering is different than what we passed in the arguments. The <strong class="source-inline1">'scores'</strong> element stores the probabilities of the classes and corresponds to the same ordering in the <strong class="source-inline1">'labels'</strong> element. The last argument in this call is <strong class="source-inline1">device</strong>. If there is a CUDA-compatible device present in the system, it will be used:<pre class="source-code">
result = pipeline_instance(
    "I am so hooked to video games as I cannot get any work done!",
    candidate_labels=["technology", "gaming", "hobby", "art", "computer"], device=device)</pre></li>				<li class="calibre14">We print the sequence, followed by printing each label and its associated probability. Note that the order of the labels has changed from the initial input that we specified in the previous step. The function call reorders the labels based on the descen<a id="_idTextAnchor222" class="calibre6 pcalibre pcalibre1"/>ding order of the label probability:<pre class="source-code">
print(result['sequence'])
for i, label in enumerate(result['labels']):
    print(f"{label}:  {result['scores'][i]:.2f}")
I am so hooked to video games as I cannot get any work done!
gaming:  0.85
hobby:  0.08
technology:  0.07
computer:  0.00
art:  0.00</pre></li>				<li class="calibre14">We run a <a id="_idIndexMarker464" class="calibre6 pcalibre pcalibre1"/>zero-shot classification on a different sentence and print the results for it. This time, we emit a result that picks the class with the highest probability and prints the result:<pre class="source-code">
result = pipeline_instance(
    "A early morning exercise regimen can drive many diseases away!",
    candidate_labels=["health", "medical", "weather", "geography", "politics"], )
print(result['sequence'])
for i, label in enumerate(result['labels']):
    print(f"{label}:  {result['scores'][i]:.2f}")
print(
    f"The most probable class for the sentence is ** 
    {result['labels'][0]} ** "
    f"with a probability of {result['scores'][0]:.2f}"
)
A early morning exercise regimen can drive many diseases away!
health:  0.91
medical:  0.07
weather:  0.01
geography:  0.01
politics:  0.00
The most probable class for the sentence is ** health ** with a probability of 0.91</pre></li>			</ol>
			<p class="calibre3">So far, we have<a id="_idIndexMarker465" class="calibre6 pcalibre pcalibre1"/> used the transformer and some pre-trained models to generate token IDs and classifications. These recipes have used the encoder part of the transformer. The encoder generates a representation of the text, which is then used by a classifier head in front of it to generate classification labels. However, the transformer has another component, called the decoder. A decoder uses a given representation of text and generates subsequent text. In the next recipe, we will learn more about the decoder.</p>
			<h1 id="_idParaDest-216" class="calibre7"><a id="_idTextAnchor223" class="calibre6 pcalibre pcalibre1"/>Generating text</h1>
			<p class="calibre3">In this recipe, we will use<a id="_idIndexMarker466" class="calibre6 pcalibre pcalibre1"/> a <strong class="bold">generative transformer model</strong> to generate text from a given seed sentence. One such model to generate text is the GPT-2 model, which <a id="_idIndexMarker467" class="calibre6 pcalibre pcalibre1"/>is an improved version of the original <strong class="bold">General Purpose Transformer</strong> (<strong class="bold">GPT</strong>) model.</p>
			<h2 id="_idParaDest-217" class="calibre5"><a id="_idTextAnchor224" class="calibre6 pcalibre pcalibre1"/>Getting ready</h2>
			<p class="calibre3">As part of this recipe, we will use the pipeline module from the transformers package. You can use the <code>8.5_Transformer_text_generation.ipynb</code> notebook from the code site if you need to work from an existing notebook.</p>
			<h2 id="_idParaDest-218" class="calibre5"><a id="_idTextAnchor225" class="calibre6 pcalibre pcalibre1"/>How to do it...</h2>
			<p class="calibre3">In this recipe, we <a id="_idIndexMarker468" class="calibre6 pcalibre pcalibre1"/>will start with an initial seed sentence and use the GPT-2 model to generate text based on the given seed sentence. We will also tinker with certain parameters to improve the quality of the generated text.</p>
			<p class="calibre3">The recipe does the following things:</p>
			<ul class="calibre15">
				<li class="calibre14">It initializes a starting sentence from which a continuing sentence will be generated</li>
				<li class="calibre14">It initializes a GPT-2 model as part of a pipeline and uses it to generate five sentences as part of the parameters passed to the generator method</li>
				<li class="calibre14">It prints the results of the generation</li>
			</ul>
			<p class="calibre3">The steps for <a id="_idIndexMarker469" class="calibre6 pcalibre pcalibre1"/>the recipe are as follows:</p>
			<ol class="calibre13">
				<li class="calibre14">Do the necessary imports and identify the compute device, as described in the previous classification recipe:<pre class="source-code">
from transformers import pipeline
import torch
device = torch.device(
    "cuda" if torch.cuda.is_available() else "cpu")</pre></li>				<li class="calibre14">Initialize a seed input sentence based on which the subsequent text will be generated. Our goal here is to use the GPT-2 decoder to hypothetically generate the text that follows it based on the generation parameters:<pre class="source-code">
text = "The cat had no business entering the neighbors garage, but"</pre></li>				<li class="calibre14">In this step, we initialize a text-generation pipeline with the <strong class="source-inline1">'gpt-2'</strong> model. This model is based on <a id="_idIndexMarker470" class="calibre6 pcalibre pcalibre1"/>a <strong class="bold">large language model</strong> (<strong class="bold">LLM</strong>) that was trained using a large text corpus. The last argument in this call is <strong class="source-inline1">device</strong>. If there is a CUDA-compatible device present in the system, it will be used:<pre class="source-code">
generator = pipeline(
    'text-generation', model='gpt2', device=device)</pre></li>				<li class="calibre14">Generate the continuing sequence for the seed sentence and store the results. The parameters of note used in the call other than the seed text are as follows:<ul class="calibre19"><li class="calibre14"><strong class="source-inline1">max_length</strong>: The maximum length of the generated sentence, including the length of the seed sentence.</li><li class="calibre14"><strong class="source-inline1">num_return_sequences</strong>: The number of generated sequences to return.</li><li class="calibre14"><strong class="source-inline1">num_beams</strong>: This <a id="_idIndexMarker471" class="calibre6 pcalibre pcalibre1"/>parameter controls the quality of the generated sequence. A higher number generally results in improved quality of the generated sequence but also slows down the generation. We encourage you to try out different values for this parameter based on the quality requirements of the generated sequence.<pre class="source-code">
generated_sentences = generator(
    text,do_sample=True, max_length=30,
    num_return_sequences=5, num_beams=5,
    pad_token_id=50256)</pre></li></ul></li>				<li class="calibre14">Print the <a id="_idIndexMarker472" class="calibre6 pcalibre pcalibre1"/>generated sentences:<pre class="source-code">
[print(generated_sentence['generated_text']) 
    for generated_sentence in generated_sentences]
The cat had no business entering the neighbors garage, but  he was able to get inside.  The cat had been in the neighbor's
The cat had no business entering the neighbors garage, but  the owner of the house called 911.  He said he found the cat in
The cat had no business entering the neighbors garage, but  he was able to get his hands on one of the keys.  It was
The cat had no business entering the neighbors garage, but  he didn't seem to mind at all.  He had no idea what he
The cat had no business entering the neighbors garage, but  the cat had no business entering the neighbors garage, but the cat had no business entering</pre></li>			</ol>
			<h2 id="_idParaDest-219" class="calibre5"><a id="_idTextAnchor226" class="calibre6 pcalibre pcalibre1"/>There’s more…</h2>
			<p class="calibre3">As we can see in the preceding example, the generated output was rudimentary, repetitive, grammatically incorrect, or perhaps incoherent. There are different techniques<a id="_idIndexMarker473" class="calibre6 pcalibre pcalibre1"/> that we can use to improve the g<a id="_idTextAnchor227" class="calibre6 pcalibre pcalibre1"/>enerated output.</p>
			<p class="calibre3">We will use the <code>no_repeat_ngram_size</code> parameter this time to generate the text. We will set the value of this parameter to <code>2</code>. This instructs the generator to not repeat bi-grams.</p>
			<p class="calibre3">We will change the line in <em class="italic">step 4</em> to the following:</p>
			<pre class="source-code">
generated_sentences = generator(text, do_sample=True,
    max_length=30, num_return_sequences=5, num_beams=5,
    <strong class="bold1">no_repeat_ngram_size=2</strong>,  pad_token_id=50256)</pre>			<p class="calibre3">As we can see in the following output, the sentences have reduced repetition, but some of them are still incoherent:</p>
			<pre class="source-code">
The cat had no business entering the neighbors garage, but  it was too late to stop it.
"I don't know if it was
The cat had no business entering the neighbors garage, but  she was able to find her way to the porch, where she and her friend were
The cat had no business entering the neighbors garage, but  he did get in the way.
The next day, the neighbor called the police
The cat had no business entering the neighbors garage, but  he managed to get his hands on one of the keys, which he used to unlock
The cat had no business entering the neighbors garage, but  the neighbors thought they were in the right place.
"What's going on</pre>			<p class="calibre3">To improve the coherency, we can use another technique to include the next word from a set of words that have the highest likelihood of being the next word. We use the <code>top_k</code> parameter and set its value to <code>50</code>. This instructs the generator to sample the next word from the top 50 words, arranged according to their probabilities.</p>
			<p class="calibre3">We change the<a id="_idIndexMarker474" class="calibre6 pcalibre pcalibre1"/> line in <em class="italic">step 4</em> to the following:</p>
			<pre class="source-code">
generated_sentences = generator(text, do_sample=True,
    max_length=30, num_return_sequences=5, num_beams=5,
    no_repeat_ngram_size=2, <strong class="bold1">top_k=50</strong>, pad_token_id=50256)
The cat had no business entering the neighbors garage, but  it did get into a neighbor's garage. The neighbor went to check on the cat
The cat had no business entering the neighbors garage, but  she was there to take care of it.
The next morning, the cat was
The cat had no business entering the neighbors garage, but  it didn't want to leave. The neighbor told the cat to get out of the
The cat had no business entering the neighbors garage, but  the neighbors were too afraid to call 911.The neighbor told the police that he
The cat had no business entering the neighbors garage, but  it was there that he found his way to the kitchen, where it was discovered that</pre>			<p class="calibre3">We can also combine the <code>top_k</code> parameter with the <code>top_p</code> parameter. This instructs the generator to select the next word from the set of words that have a probability higher than this defined value. Adding this parameter with a value of <code>0.8</code> yields the following output:</p>
			<pre class="source-code">
generated_sentences = generator(text, do_sample=True,
    max_length=30, num_return_sequences=5, num_beams=5,
    no_repeat_ngram_size=2, <strong class="bold1">top_k=50</strong>, <strong class="bold1">top_p=0.8</strong>,
    pad_token_id=50256)
The cat had no business entering the neighbors garage, but  the owner of the house told the police that he did not know what was going on
The cat had no business entering the neighbors garage, but  he did, and the cat was able to get out of the garage.The
The cat had no business entering the neighbors garage, but  he was able to get in through the back door. The cat was not injured,
The cat had no business entering the neighbors garage, but  the neighbor told the police that the cat was a stray, and the neighbor said that
The cat had no business entering the neighbors garage, but  the owner of the house said he didn't know what to do with the cat.</pre>			<p class="calibre3">As we can see, the<a id="_idIndexMarker475" class="calibre6 pcalibre pcalibre1"/> addition of additional parameters to the generator continues to improve the generated output.</p>
			<p class="calibre3">As a final example, let us generate a longer output sequence by changing the line in <em class="italic">step 4</em> to the following:</p>
			<pre class="source-code">
generated_sentences = generator(text, do_sample=True,
    <strong class="bold1">max_length=500, num_return_sequences=1</strong>, num_beams=5,
    no_repeat_ngram_size=2, top_k=50, top_p=0.85,
    pad_token_id=50256)
The cat had no business entering the neighbors garage, but  she was there to help.
"I was like, 'Oh my God, she's here,'" she said. "I'm like 'What are you doing here?' "
The neighbor, who asked not to be identified, said she didn't know what to make of the cat's behavior. She said it seemed like it was trying to get into her home, and that she was afraid for her life. The neighbor said that when she went to check on her cat, it ran into the neighbor's garage and hit her in the face, knocking her to the ground.</pre>			<p class="calibre3">As we can see, the generated output, however fictitious, is more coherent and readable. We encourage you to experiment with different mixes of parameters and their respective values to improve the generated output based on their use cases.</p>
			<p class="calibre3">Please note that the output returned by the model might differ a bit from what this example has shown. This happens because the internal language model is probabilistic in nature. The next word is sampled from a distribution that contains words that have a probability larger<a id="_idIndexMarker476" class="calibre6 pcalibre pcalibre1"/> than what we defined in our parameters for generation.</p>
			<p class="calibre3">In this recipe, we used the decoder module of the transformer to generate text, given a seed sentence. There are use cases where an encoder and decoder are used together to generate text. We will learn about this in the next recipe.</p>
			<h1 id="_idParaDest-220" class="calibre7"><a id="_idTextAnchor228" class="calibre6 pcalibre pcalibre1"/>Language translation</h1>
			<p class="calibre3">In this recipe, we will use transformers for language translation. We will use the <strong class="bold">Google Text-To-Text Transfer Transformer</strong> (<strong class="bold">T5</strong>) model. This <a id="_idIndexMarker477" class="calibre6 pcalibre pcalibre1"/>model is an end-to-end model that uses both the encoder and decoder components of the transformer model.</p>
			<h2 id="_idParaDest-221" class="calibre5"><a id="_idTextAnchor229" class="calibre6 pcalibre pcalibre1"/>Getting ready</h2>
			<p class="calibre3">As part of this recipe, we will use the pipeline module from the transformers package. You can use the <code>8.6_Language_Translation_with_transformers.ipynb</code> notebook from the code site if you need to work from an existing notebook.</p>
			<h2 id="_idParaDest-222" class="calibre5"><a id="_idTextAnchor230" class="calibre6 pcalibre pcalibre1"/>How to do it...</h2>
			<p class="calibre3">In this recipe, you will initialize a seed sentence in English and translate it to French. The T5 model <a id="_idIndexMarker478" class="calibre6 pcalibre pcalibre1"/>expects the input format to encode the information about the language translation task along with the seed sentence. In this case, the encoder uses the input in the source language and generates a representation of the text. The decoder uses this representation and generates text for the target language. The <a id="_idIndexMarker479" class="calibre6 pcalibre pcalibre1"/>T5 model is trained specifically for this task, in addition to many others. If you are running on a machine that does not have a CUDA-compatible device, it might take some time for the recipe steps to be executed.</p>
			<p class="calibre3">The recipe does the following things:</p>
			<ul class="calibre15">
				<li class="calibre14"> It initializes the <strong class="source-inline1">Google t5-base</strong> model and tokenizer</li>
				<li class="calibre14"> It initializes a seed sentence in English that will be translated into French</li>
				<li class="calibre14"> It tokenizes the seed sentence along with the task specification to translate the seed sentence into French</li>
				<li class="calibre14">It generates the translated tokens, decodes them into the target language (French), and prints them</li>
			</ul>
			<p class="calibre3">The steps for the recipe are as follows:</p>
			<ol class="calibre13">
				<li class="calibre14">Do the necessary imports and identify the compute device, as described in the previous classification recipe:<pre class="source-code">
from transformers import (
    T5Tokenizer, T5ForConditionalGeneration)
import torch
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")</pre></li>				<li class="calibre14">Initialize a tokenizer and model instance with the <strong class="source-inline1">t5-base</strong> model from Google. We use the <strong class="source-inline1">model_max_length</strong> parameter of <strong class="source-inline1">200</strong>. Feel free to experiment with higher values if your seed sentence is longer than 200 words. We also load the model onto the device that was identified for computation in step 1:<pre class="source-code">
tokenizer = T5Tokenizer.from_pretrained(
    't5-base', model_max_length=200)
model = T5ForConditionalGeneration.from_pretrained(
    't5-base', return_dict=True)
model = model.to(device)</pre></li>				<li class="calibre14">Initialize a seed sequence that you want to translate:<pre class="source-code">
language_sequence = ("It's such a beautiful morning today!")</pre></li>				<li class="calibre14">Tokenize the input sequence. The tokenizer specifies the source and the target language as part of its input encoding. This is done by appending the “<strong class="source-inline1">translate English to French:</strong>” text to the input seed sequence. We load these token IDs into the device that is used for computation. It is a requirement for<a id="_idIndexMarker480" class="calibre6 pcalibre pcalibre1"/> both the model and the token IDs to be on the same device:<pre class="source-code">
input_ids = tokenizer(
    "translate English to French: " + language_sequence,
    return_tensors="pt",
    truncation=True).input_ids.to(device)</pre></li>				<li class="calibre14">Translate the source language token IDs to the target language token IDs via the model. The model uses the encoder-decoder architecture to convert the input token IDs to the output token IDs:<pre class="source-code">
language_ids = model.generate(input_ids, max_new_tokens=200)</pre></li>				<li class="calibre14">Decode the text from the token IDs to the target language tokens. We use the tokenizer to convert the output token IDs to the target language tokens:<pre class="source-code">
language_translation = tokenizer.decode(
    language_ids[0], skip_special_tokens=True)</pre></li>				<li class="calibre14">Print the<a id="_idIndexMarker481" class="calibre6 pcalibre pcalibre1"/> translated output:<pre class="source-code">
print(language_translation)
C'est un matin si beau!</pre></li>			</ol>
			<p class="calibre3">In conclusion, this chapter introduced the concept of transformers, along with some of its basic applications. The next chapter will focus on how we can use the different NLP techniques to understand text better.</p>
		</div>
	</body></html>