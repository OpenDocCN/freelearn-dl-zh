<html><head></head><body>
		<div id="_idContainer039" class="calibre2">
			<h1 id="_idParaDest-201" class="chapter-number"><a id="_idTextAnchor205" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.1.1">8</span></h1>
			<h1 id="_idParaDest-202" class="calibre7"><a id="_idTextAnchor206" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.2.1">Transformers and  Their Applications</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.3.1">In this chapter, we will learn about transformers and how to apply them to perform various NLP tasks. </span><span class="kobospan" id="kobo.3.2">Typical tasks in the NLP domain involve loading and processing data so that it can be used downstream seamlessly. </span><span class="kobospan" id="kobo.3.3">Once the data is read, another task is that of transforming the data into a form that the various models can use. </span><span class="kobospan" id="kobo.3.4">Once the data is transformed into the requisite format, we use it to perform the actual tasks, such as classification, text generation, and </span><span><span class="kobospan" id="kobo.4.1">language translation.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.5.1">Here is a list of the recipes in </span><span><span class="kobospan" id="kobo.6.1">this chapter:</span></span></p>
			<ul class="calibre15">
				<li class="calibre14"><span class="kobospan" id="kobo.7.1">Loading </span><span><span class="kobospan" id="kobo.8.1">a dataset</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.9.1">Tokenizing the text in </span><span><span class="kobospan" id="kobo.10.1">your dataset</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.11.1">Using the tokenized text to perform classification with </span><span><span class="kobospan" id="kobo.12.1">Transformer models</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.13.1">Using different Transformer models based on </span><span><span class="kobospan" id="kobo.14.1">different requirements</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.15.1">Generating text by taking a cue from an initial </span><span><span class="kobospan" id="kobo.16.1">starting sentence</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.17.1">Translating text between different languages using pre-trained </span><span><span class="kobospan" id="kobo.18.1">Transformer models</span></span></li>
			</ul>
			<h1 id="_idParaDest-203" class="calibre7"><a id="_idTextAnchor207" class="calibre6 pcalibre pcalibre1"/><a id="_idTextAnchor208" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.19.1">Technical requirements</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.20.1">The code for this chapter is in the </span><strong class="source-inline"><span class="kobospan" id="kobo.21.1">Chapter08</span></strong><span class="kobospan" id="kobo.22.1"> folder in the GitHub repository of the </span><span><span class="kobospan" id="kobo.23.1">book (</span></span><a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter08" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.24.1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter08</span></span></a><span><span class="kobospan" id="kobo.25.1">).</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.26.1">As in previous chapters, the packages required for the chapter are part of the </span><strong class="source-inline"><span class="kobospan" id="kobo.27.1">poetry</span></strong><span class="kobospan" id="kobo.28.1"> environment. </span><span class="kobospan" id="kobo.28.2">Alternatively, you can install all the packages using the </span><span><strong class="source-inline"><span class="kobospan" id="kobo.29.1">requirements.txt</span></strong></span><span><span class="kobospan" id="kobo.30.1"> file.</span></span></p>
			<h1 id="_idParaDest-204" class="calibre7"><a id="_idTextAnchor209" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.31.1">Loading a dataset</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.32.1">In this recipe, we will learn </span><a id="_idIndexMarker440" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.33.1">how to load a public dataset and work with it. </span><span class="kobospan" id="kobo.33.2">We will use the </span><strong class="source-inline"><span class="kobospan" id="kobo.34.1">RottenTomatoes</span></strong><span class="kobospan" id="kobo.35.1"> dataset for this recipe as an example. </span><span class="kobospan" id="kobo.35.2">This dataset contains ratings and reviews for movies. </span><span class="kobospan" id="kobo.35.3">Please refer to the link at </span><a href="https://www.kaggle.com/datasets/stefanoleone992/rotten-tomatoes-movies-and-critic-reviews-dataset" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.36.1">https://www.kaggle.com/datasets/stefanoleone992/rotten-tomatoes-movies-and-critic-reviews-dataset</span></a><span class="kobospan" id="kobo.37.1"> for more information about </span><span><span class="kobospan" id="kobo.38.1">the dataset</span></span></p>
			<h2 id="_idParaDest-205" class="calibre5"><a id="_idTextAnchor210" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.39.1">Getting ready</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.40.1">As part of this chapter, we will use the libraries from the </span><strong class="source-inline"><span class="kobospan" id="kobo.41.1">HuggingFace</span></strong><span class="kobospan" id="kobo.42.1"> site (</span><a href="http://huggingface.co" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.43.1">huggingface.co</span></a><span class="kobospan" id="kobo.44.1">). </span><span class="kobospan" id="kobo.44.2">For this recipe, we will use the dataset package. </span><span class="kobospan" id="kobo.44.3">You can use the </span><strong class="source-inline"><span class="kobospan" id="kobo.45.1">8.1_Transformers_dataset.ipynb</span></strong><span class="kobospan" id="kobo.46.1"> notebook from the code site if you need to work from an </span><span><span class="kobospan" id="kobo.47.1">existing notebook.</span></span></p>
			<h2 id="_idParaDest-206" class="calibre5"><a id="_idTextAnchor211" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.48.1">How to do it...</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.49.1">In this recipe, you will load the </span><strong class="source-inline"><span class="kobospan" id="kobo.50.1">RottenTomatoes</span></strong><span class="kobospan" id="kobo.51.1"> dataset from the </span><strong class="source-inline"><span class="kobospan" id="kobo.52.1">HuggingFace</span></strong><span class="kobospan" id="kobo.53.1"> site using the dataset package. </span><span class="kobospan" id="kobo.53.2">This package will download the dataset for you if it does not exist. </span><span class="kobospan" id="kobo.53.3">For any subsequent runs, it will use the downloaded dataset from the cache if it was </span><span><span class="kobospan" id="kobo.54.1">downloaded previously.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.55.1">The recipe does the </span><span><span class="kobospan" id="kobo.56.1">following things:</span></span></p>
			<ul class="calibre15">
				<li class="calibre14"><span class="kobospan" id="kobo.57.1"> Reads in the </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.58.1">RottenTomatoes</span></strong></span><span><span class="kobospan" id="kobo.59.1"> dataset</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.60.1"> Describes the features of </span><span><span class="kobospan" id="kobo.61.1">the dataset</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.62.1"> Loads the data from the training split of </span><span><span class="kobospan" id="kobo.63.1">the dataset</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.64.1"> Samples a few sentences from the dataset and </span><span><span class="kobospan" id="kobo.65.1">prints them</span></span></li>
			</ul>
			<p class="calibre3"><span class="kobospan" id="kobo.66.1">The steps for the recipe are </span><span><span class="kobospan" id="kobo.67.1">as follows:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.68.1">Do the necessary imports to import the necessary types and functions from the </span><span><span class="kobospan" id="kobo.69.1">datasets package:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.70.1">
from datasets import load_dataset, get_dataset_split_names</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.71.1">Load </span><strong class="source-inline1"><span class="kobospan" id="kobo.72.1">"rotten tomatoes"</span></strong><span class="kobospan" id="kobo.73.1"> via the </span><strong class="source-inline1"><span class="kobospan" id="kobo.74.1">load_dataset</span></strong><span class="kobospan" id="kobo.75.1"> function and print the internal</span><a id="_idIndexMarker441" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.76.1"> dataset splits. </span><span class="kobospan" id="kobo.76.2">This dataset contains the train, validation, and </span><span><span class="kobospan" id="kobo.77.1">test splits:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.78.1">
dataset = load_dataset("rotten_tomatoes")
print(get_dataset_split_names("rotten_tomatoes"))</span></pre><p class="calibre3"><span class="kobospan" id="kobo.79.1">The output of the preceding command would be </span><span><span class="kobospan" id="kobo.80.1">as follows:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.81.1">['train', 'validation', 'test']</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.82.1">Load the dataset and print the attributes of the train split. </span><span class="kobospan" id="kobo.82.2">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.83.1">training_data.description</span></strong><span class="kobospan" id="kobo.84.1"> describes the dataset details and the </span><strong class="source-inline1"><span class="kobospan" id="kobo.85.1">training_data.features</span></strong><span class="kobospan" id="kobo.86.1"> describes the features of the dataset. </span><span class="kobospan" id="kobo.86.2">In the output, we can see that the </span><strong class="source-inline1"><span class="kobospan" id="kobo.87.1">training_data</span></strong><span class="kobospan" id="kobo.88.1"> split contains the features </span><strong class="source-inline1"><span class="kobospan" id="kobo.89.1">text</span></strong><span class="kobospan" id="kobo.90.1">, which is of the type string, and </span><strong class="source-inline1"><span class="kobospan" id="kobo.91.1">label</span></strong><span class="kobospan" id="kobo.92.1">, which is of type categorical, with the values </span><strong class="source-inline1"><span class="kobospan" id="kobo.93.1">neg</span></strong> <span><span class="kobospan" id="kobo.94.1">and </span></span><span><strong class="source-inline1"><span class="kobospan" id="kobo.95.1">pos</span></strong></span><span><span class="kobospan" id="kobo.96.1">:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.97.1">
training_data = dataset['train']
print(training_data.description)
print(training_data.features)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.98.1">The output of the command is </span><span><span class="kobospan" id="kobo.99.1">as follows:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.100.1">Movie Review Dataset.
</span><span class="kobospan1" id="kobo.100.2">This is a dataset of containing 5,331 positive and 5,331 negative processed  sentences from Rotten Tomatoes movie reviews. </span><span class="kobospan1" id="kobo.100.3">This data was first used in Bo Pang and Lillian Lee, ``Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.'', Proceedings of the ACL, 2005.
</span><span class="kobospan1" id="kobo.100.4">{'text': Value(dtype='string', id=None), 
    'label':ClassLabel(names=['neg', 'pos'], id=None)}</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.101.1">Now that we have loaded the dataset, we will print the first five sentences from it. </span><span class="kobospan" id="kobo.101.2">This is just to confirm that we are indeed able to read from </span><span><span class="kobospan" id="kobo.102.1">the dataset:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.103.1">
sentences = training_data['text'][:5]
[print(sentence) for sentence in sentences]</span></pre><p class="calibre3"><span class="kobospan" id="kobo.104.1">The </span><a id="_idIndexMarker442" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.105.1">output of the command is </span><span><span class="kobospan" id="kobo.106.1">as follows:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.107.1">the rock is destined to be the 21st century's new " conan " and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .
</span><span class="kobospan1" id="kobo.107.2">the gorgeously elaborate continuation of " the lord of the rings " trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . </span><span class="kobospan1" id="kobo.107.3">r . </span><span class="kobospan1" id="kobo.107.4">r . </span><span class="kobospan1" id="kobo.107.5">tolkien's middle-earth .
</span><span class="kobospan1" id="kobo.107.6">effective but too-tepid biopic
if you sometimes like to go to the movies to have fun , wasabi is a good place to start .
</span><span class="kobospan1" id="kobo.107.7">emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one</span><a id="_idTextAnchor212" class="pcalibre pcalibre1 calibre20"/><span class="kobospan1" id="kobo.108.1"> .</span></pre></li>			</ol>
			<h1 id="_idParaDest-207" class="calibre7"><a id="_idTextAnchor213" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.109.1">Tokenizing the text in your dataset</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.110.1">The</span><a id="_idIndexMarker443" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.111.1"> components </span><a id="_idIndexMarker444" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.112.1">contained within the transformer do not have any intrinsic knowledge of the words that it processes. </span><span class="kobospan" id="kobo.112.2">Instead, the tokenizer only uses the token identifiers for the words that it processes. </span><span class="kobospan" id="kobo.112.3">In this recipe, we will learn how to transform the text in your dataset into a representation that can be used by the models for </span><span><span class="kobospan" id="kobo.113.1">downstream tasks.</span></span></p>
			<h2 id="_idParaDest-208" class="calibre5"><a id="_idTextAnchor214" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.114.1">Getting ready</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.115.1">As </span><a id="_idIndexMarker445" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.116.1">part of this recipe, we will use the </span><strong class="source-inline"><span class="kobospan" id="kobo.117.1">AutoTokenizer</span></strong><span class="kobospan" id="kobo.118.1"> module </span><a id="_idIndexMarker446" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.119.1">from the transformers package. </span><span class="kobospan" id="kobo.119.2">You can use the </span><strong class="source-inline"><span class="kobospan" id="kobo.120.1">8.2_Basic_Tokenization.ipynb</span></strong><span class="kobospan" id="kobo.121.1"> notebook from the code site if you need to work from an </span><span><span class="kobospan" id="kobo.122.1">existing notebook.</span></span></p>
			<h2 id="_idParaDest-209" class="calibre5"><a id="_idTextAnchor215" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.123.1">How to do it...</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.124.1">In this recipe, you will continue from the previous example of using the </span><strong class="source-inline"><span class="kobospan" id="kobo.125.1">RottenTomatoes</span></strong><span class="kobospan" id="kobo.126.1"> dataset and sampling a few sentences from it. </span><span class="kobospan" id="kobo.126.2">We will then encode the sampled sentences into tokens and their </span><span><span class="kobospan" id="kobo.127.1">respective representations.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.128.1">The recipe does the </span><span><span class="kobospan" id="kobo.129.1">following things:</span></span></p>
			<ul class="calibre15">
				<li class="calibre14"><span class="kobospan" id="kobo.130.1"> Loads a few sentences </span><span><span class="kobospan" id="kobo.131.1">into memory</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.132.1"> Instantiates a tokenizer and tokenizes </span><span><span class="kobospan" id="kobo.133.1">the sentences</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.134.1"> Converts the token IDs generated from the previous step back into </span><span><span class="kobospan" id="kobo.135.1">the tokens</span></span></li>
			</ul>
			<p class="calibre3"><span class="kobospan" id="kobo.136.1">The steps for the recipe are </span><span><span class="kobospan" id="kobo.137.1">as follows:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.138.1">Do the necessary imports to import the necessary </span><strong class="source-inline1"><span class="kobospan" id="kobo.139.1">AutoTokenizer</span></strong><span class="kobospan" id="kobo.140.1"> module from the </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.141.1">transformers</span></strong></span><span><span class="kobospan" id="kobo.142.1"> library:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.143.1">
from transformers import AutoTokenizer</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.144.1">We initialize a sentence array consisting of three sentences that we will use for this example. </span><span class="kobospan" id="kobo.144.2">These sentences are of different lengths and have a good combination of the same and different words. </span><span class="kobospan" id="kobo.144.3">This will allow us to understand how the tokenized representation varies for each </span><span><span class="kobospan" id="kobo.145.1">of them:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.146.1">
sentences = [
    "The first sentence, which is the longest one in the list.",
    "The second sentence is not that long.",
    "A very short sentence."]</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.147.1">Instantiate a tokenizer of the </span><strong class="source-inline1"><span class="kobospan" id="kobo.148.1">bert-base-cased</span></strong><span class="kobospan" id="kobo.149.1"> type. </span><span class="kobospan" id="kobo.149.2">This tokenizer is case-sensitive. </span><span class="kobospan" id="kobo.149.3">This means that the words star and STAR will have different </span><span><span class="kobospan" id="kobo.150.1">tokenized representations:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.151.1">
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.152.1">In this </span><a id="_idIndexMarker447" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.153.1">step, we tokenize all the sentences in the </span><strong class="source-inline1"><span class="kobospan" id="kobo.154.1">sentences</span></strong><span class="kobospan" id="kobo.155.1"> array. </span><span class="kobospan" id="kobo.155.2">We </span><a id="_idIndexMarker448" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.156.1">call the tokenizer constructor and pass it the </span><strong class="source-inline1"><span class="kobospan" id="kobo.157.1">sentences</span></strong><span class="kobospan" id="kobo.158.1"> array as an argument followed by printing the </span><strong class="source-inline1"><span class="kobospan" id="kobo.159.1">tokenized_output</span></strong><span class="kobospan" id="kobo.160.1"> instance returned by the constructor function. </span><span class="kobospan" id="kobo.160.2">This object is a dictionary of </span><span><span class="kobospan" id="kobo.161.1">three items:</span></span><ul class="calibre19"><li class="calibre14"><strong class="source-inline1"><span class="kobospan" id="kobo.162.1">input_ids</span></strong><span class="kobospan" id="kobo.163.1">: These are the numerical token identifiers that are assigned to </span><span><span class="kobospan" id="kobo.164.1">each token.</span></span></li><li class="calibre14"><strong class="source-inline1"><span class="kobospan" id="kobo.165.1">token_type_ids</span></strong><span class="kobospan" id="kobo.166.1">: These IDs define the type of tokens that are contained in </span><span><span class="kobospan" id="kobo.167.1">the sentences.</span></span></li><li class="calibre14"><strong class="source-inline1"><span class="kobospan" id="kobo.168.1">attention_mask</span></strong><span class="kobospan" id="kobo.169.1">: These define the attention values for each token in the input. </span><span class="kobospan" id="kobo.169.2">This mask determines what tokens are paid attention to when downstream tasks are performed. </span><span class="kobospan" id="kobo.169.3">These values are floats and can vary from 0 (no attention) to 1 (</span><span><span class="kobospan" id="kobo.170.1">full attention).</span></span><pre class="source-code"><span class="kobospan1" id="kobo.171.1">
tokenized_input = tokenizer(sentences)
print(tokenized_input)
{'input_ids': [[101, 1109, 1148, 5650, 117, 1134, 1110, 1103, 6119, 1141, 1107, 1103, 2190, 119, 102],
[101, 1109, 1248, 5650, 1110, 1136, 1115, 1263, 119, 102],[101, 138, 1304, 1603, 5650, 119, 102]],
'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0]],
'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1]]}</span></pre></li></ul></li>				<li class="calibre14"><span class="kobospan" id="kobo.172.1">In this step, we </span><a id="_idIndexMarker449" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.173.1">take the input IDs of the first sentence and </span><a id="_idIndexMarker450" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.174.1">convert them back </span><span><span class="kobospan" id="kobo.175.1">into tokens:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.176.1">
tokens = tokenizer.convert_ids_to_tokens(
    tokenized_input["input_ids"][0])
print(tokens)
['[CLS]', 'The', 'first', 'sentence', ',', 'which', 'is', 'the', 'longest', 'one', 'in', 'the', 'list', '.', '[SEP]']</span></pre><p class="calibre3"><span class="kobospan" id="kobo.177.1">The token IDs for the first sentence are in the </span><span><span class="kobospan" id="kobo.178.1">following form:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.179.1">[101, 1109, 1148, 5650, 117, 1134, 1110, 1103, 6119, 1141, 1107, 1103, 2190, 119, 102]</span></pre><p class="calibre3"><span class="kobospan" id="kobo.180.1">Converting them into tokens returns the </span><span><span class="kobospan" id="kobo.181.1">following output:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.182.1">['[CLS]', 'The', 'first', 'sentence', ',', 'which', 'is', 'the', 'longest', 'one', 'in', 'the', 'list', '.', '[SEP]']</span></pre><p class="calibre3"><span class="kobospan" id="kobo.183.1">In addition to the original tokens, the </span><a id="_idIndexMarker451" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.184.1">tokenizer adds the </span><strong class="bold"><span class="kobospan" id="kobo.185.1">classification</span></strong><span class="kobospan" id="kobo.186.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.187.1">CLS</span></strong><span class="kobospan" id="kobo.188.1">) and the </span><strong class="bold"><span class="kobospan" id="kobo.189.1">separator</span></strong><span class="kobospan" id="kobo.190.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.191.1">SEP</span></strong><span class="kobospan" id="kobo.192.1">) tokens</span><a id="_idIndexMarker452" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.193.1"> to the beginning and the end of the sentence, respectively, denoted as </span><strong class="source-inline"><span class="kobospan" id="kobo.194.1">[CLS]</span></strong><span class="kobospan" id="kobo.195.1"> and </span><strong class="source-inline"><span class="kobospan" id="kobo.196.1">[SEP]</span></strong><span class="kobospan" id="kobo.197.1">. </span><span class="kobospan" id="kobo.197.2">These tokens were added for the training tasks that were performed to </span><span><span class="kobospan" id="kobo.198.1">train BERT.</span></span></p><p class="calibre3"><span class="kobospan" id="kobo.199.1">Now that we have learned about the internal representation of the text used by the transformer internally, let us learn how we can classify a piece of text into </span><span><span class="kobospan" id="kobo.200.1">different categories.</span></span></p></li>			</ol>
			<h1 id="_idParaDest-210" class="calibre7"><a id="_idTextAnchor216" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.201.1">Classifying text</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.202.1">In this recipe, we will use</span><a id="_idIndexMarker453" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.203.1"> the </span><strong class="source-inline"><span class="kobospan" id="kobo.204.1">RottenTomatoes</span></strong><span class="kobospan" id="kobo.205.1"> dataset and classify the review texts for sentiment. </span><span class="kobospan" id="kobo.205.2">We will classify the test split of the dataset and evaluate the results of the classifier against the true labels in the test split of </span><span><span class="kobospan" id="kobo.206.1">the dataset.</span></span></p>
			<h2 id="_idParaDest-211" class="calibre5"><a id="_idTextAnchor217" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.207.1">Getting ready</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.208.1">As part of this recipe, we will use the pipeline module from the transformers package. </span><span class="kobospan" id="kobo.208.2">You can use the </span><strong class="source-inline"><span class="kobospan" id="kobo.209.1">8.3_Classification_And_Evaluation.ipynb</span></strong><span class="kobospan" id="kobo.210.1"> notebook from the code site if you need to work from an </span><span><span class="kobospan" id="kobo.211.1">existing notebook.</span></span></p>
			<h2 id="_idParaDest-212" class="calibre5"><a id="_idTextAnchor218" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.212.1">How to do it...</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.213.1">In this recipe, you will continue from the previous example using the </span><strong class="source-inline"><span class="kobospan" id="kobo.214.1">RottenTomatoes</span></strong><span class="kobospan" id="kobo.215.1"> dataset and sample a few sentences from it. </span><span class="kobospan" id="kobo.215.2">We will then classify a small subset of five sentences for sentiment classification and demonstrate the results on this smaller subset. </span><span class="kobospan" id="kobo.215.3">We will then perform inference on the whole test split of the dataset and evaluate the results of </span><span><span class="kobospan" id="kobo.216.1">the classification.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.217.1">The recipe does the </span><span><span class="kobospan" id="kobo.218.1">following things:</span></span></p>
			<ul class="calibre15">
				<li class="calibre14"><span class="kobospan" id="kobo.219.1">Loads the </span><strong class="source-inline1"><span class="kobospan" id="kobo.220.1">RottenTomatoes</span></strong><span class="kobospan" id="kobo.221.1"> dataset and prints the first five sentences </span><span><span class="kobospan" id="kobo.222.1">from it</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.223.1">Instantiates a pipeline with a pre-trained Roberta model that has been trained on the same dataset for </span><span><span class="kobospan" id="kobo.224.1">sentiment analysis</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.225.1">Performs inference (or sentiment prediction) on the whole test split of the dataset using </span><span><span class="kobospan" id="kobo.226.1">the pipeline</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.227.1">Evaluates the results of </span><span><span class="kobospan" id="kobo.228.1">the inference</span></span></li>
			</ul>
			<p class="calibre3"><span class="kobospan" id="kobo.229.1">The steps for the</span><a id="_idIndexMarker454" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.230.1"> recipe are </span><span><span class="kobospan" id="kobo.231.1">as follows:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.232.1">Do the necessary imports to import the required packages </span><span><span class="kobospan" id="kobo.233.1">and modules:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.234.1">
from datasets import load_dataset
from evaluate import evaluator, combine
from transformers import pipeline
import torch</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.235.1">In this step, we </span><a id="_idIndexMarker455" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.236.1">probe for the presence of a </span><strong class="bold"><span class="kobospan" id="kobo.237.1">Compute Unified Device Architecture</span></strong><span class="kobospan" id="kobo.238.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.239.1">CUDA</span></strong><span class="kobospan" id="kobo.240.1">) compatible </span><a id="_idIndexMarker456" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.241.1">device (or </span><strong class="bold"><span class="kobospan" id="kobo.242.1">Graphics Processing Unit</span></strong><span class="kobospan" id="kobo.243.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.244.1">GPU</span></strong><span class="kobospan" id="kobo.245.1">)) present in the system. </span><span class="kobospan" id="kobo.245.2">If such a device is present, our model will be loaded on it. </span><span class="kobospan" id="kobo.245.3">This accelerates the training and inference performance of a model if it is supported. </span><span class="kobospan" id="kobo.245.4">However, if such a device is not present, the </span><strong class="bold"><span class="kobospan" id="kobo.246.1">Central Processing Unit</span></strong><span class="kobospan" id="kobo.247.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.248.1">CPU</span></strong><span class="kobospan" id="kobo.249.1">) will</span><a id="_idIndexMarker457" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.250.1"> be used. </span><span class="kobospan" id="kobo.250.2">We also load the </span><strong class="source-inline1"><span class="kobospan" id="kobo.251.1">RottenTomatoes</span></strong><span class="kobospan" id="kobo.252.1"> dataset and select the first five sentences from it. </span><span class="kobospan" id="kobo.252.2">This is to ensure we are indeed able to read the data present in </span><span><span class="kobospan" id="kobo.253.1">the dataset:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.254.1">
device = torch.device(
    "cuda" if torch.cuda.is_available() else "cpu")
sentences = load_dataset(
    "rotten_tomatoes", split="test").select(range(5))
[print(sentence) for sentence in sentences['text']]
lovingly photographed in the manner of a golden book sprung to life , stuart little 2 manages sweetness largely without stickiness .
</span><span class="kobospan1" id="kobo.254.2">consistently clever and suspenseful .
</span><span class="kobospan1" id="kobo.254.3">it's like a " big chill " reunion of the baader-meinhof gang , only these guys are more harmless pranksters than political activists .
</span><span class="kobospan1" id="kobo.254.4">the story gives ample opportunity for large-scale action and suspense , which director shekhar kapur supplies with tremendous skill .
</span><span class="kobospan1" id="kobo.254.5">red dragon " never cuts corners .</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.255.1">Initialize </span><a id="_idIndexMarker458" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.256.1">the pipeline for sentiment analysis via a pipeline. </span><span class="kobospan" id="kobo.256.2">A pipeline is an abstraction that allows us to easily use models or inference tasks without having to write the code to piece them together. </span><span class="kobospan" id="kobo.256.3">We load the </span><strong class="source-inline1"><span class="kobospan" id="kobo.257.1">roberta-base-rotten-tomatoes</span></strong><span class="kobospan" id="kobo.258.1"> model from </span><strong class="source-inline1"><span class="kobospan" id="kobo.259.1">textattack</span></strong><span class="kobospan" id="kobo.260.1">, which has been trained on this dataset. </span><span class="kobospan" id="kobo.260.2">In the following segment, we use the pipeline for the sentiment analysis task and set a specific model to be used for </span><span><span class="kobospan" id="kobo.261.1">this task:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.262.1">
roberta_pipe = pipeline("sentiment-analysis",
    model="textattack/roberta-base-rotten-tomatoes")</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.263.1">In this step, we generate the predictions for the small subset of sentences that we selected in step 2. </span><span class="kobospan" id="kobo.263.2">Using the pipeline object to generate predictions is as easy as just passing it a series of sentences. </span><span class="kobospan" id="kobo.263.3">If you are running this example on a machine without a compatible CUDA device, this step might take a </span><span><span class="kobospan" id="kobo.264.1">little time:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.265.1">
predictions = roberta_pipe(sentences['text'])</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.266.1">In this step, we iterate through our sentences and check the predictions for our sentences. </span><span class="kobospan" id="kobo.266.2">We print the actual and generated predictions along with the sentence text for the five sentences. </span><span class="kobospan" id="kobo.266.3">The actual labels are read from the dataset, whereas the predictions are generated via the </span><span><span class="kobospan" id="kobo.267.1">pipeline object:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.268.1">
for idx, _sentence in enumerate(sentences['text']):
    print(
        f"actual: {sentences['label'][idx]}\n"
        f"predicted: {'1' if predictions[idx]['label'] 
            == 'LABEL_1' else '0'}\n"
        f"sentence: {_sentence}\n\n"
    )
actual:1
predicted:1
sentence:lovingly photographed in the manner of a golden book sprung to life , stuart little 2 manages sweetness largely without stickiness .
</span><span class="kobospan1" id="kobo.268.2">actual:1
predicted:1
sentence:consistently clever and suspenseful .
</span><span class="kobospan1" id="kobo.268.3">actual:1
predicted:0
sentence:it's like a " big chill " reunion of the baader-meinhof gang , only these guys are more harmless pranksters than political activists .
</span><span class="kobospan1" id="kobo.268.4">actual:1
predicted:1
sentence:the story gives ample opportunity for large-scale action and suspense , which director shekhar kapur supplies with tremendous skill .
</span><span class="kobospan1" id="kobo.268.5">actual:1
predicted:1
sentence:red dragon " never cuts corners .</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.269.1">Now </span><a id="_idIndexMarker459" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.270.1">that we have validated the pipeline and its results, let’s generate the inference for the whole test set and generate the evaluation measures of this particular model. </span><span class="kobospan" id="kobo.270.2">Load the complete test split for the </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.271.1">RottenTomatoes</span></strong></span><span><span class="kobospan" id="kobo.272.1"> dataset:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.273.1">
sentences = load_dataset("rotten_tomatoes", split="test")</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.274.1">In this step, we initialize an evaluator object that can be used to perform the inference along with evaluating the results of the classification. </span><span class="kobospan" id="kobo.274.2">It can also be used to present an easy-to-read summary of the </span><span><span class="kobospan" id="kobo.275.1">evaluation results:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.276.1">
task_evaluator = evaluator("sentiment-analysis")</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.277.1">In this step, we call the </span><strong class="source-inline1"><span class="kobospan" id="kobo.278.1">compute</span></strong><span class="kobospan" id="kobo.279.1"> method on the </span><strong class="source-inline1"><span class="kobospan" id="kobo.280.1">evaluator</span></strong><span class="kobospan" id="kobo.281.1"> instance. </span><span class="kobospan" id="kobo.281.2">This triggers the inference and the evaluation using the same pipeline instance that we initialized in step 4. </span><span class="kobospan" id="kobo.281.3">It returns the evaluation metrics of </span><strong class="source-inline1"><span class="kobospan" id="kobo.282.1">accuracy</span></strong><span class="kobospan" id="kobo.283.1">, </span><strong class="source-inline1"><span class="kobospan" id="kobo.284.1">precision</span></strong><span class="kobospan" id="kobo.285.1">, </span><strong class="source-inline1"><span class="kobospan" id="kobo.286.1">recall</span></strong><span class="kobospan" id="kobo.287.1">, and </span><strong class="source-inline1"><span class="kobospan" id="kobo.288.1">f1</span></strong><span class="kobospan" id="kobo.289.1">, along with some performance metrics related </span><span><span class="kobospan" id="kobo.290.1">to inference:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.291.1">
eval_results = task_evaluator.compute(
    model_or_pipeline=roberta_pipe,
    data=sentences,
    metric=combine(["accuracy", "precision", "recall", "f1"]),
    label_mapping={"LABEL_0": 0, "LABEL_1": 1}
)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.292.1">In this step, we </span><a id="_idIndexMarker460" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.293.1">print the results of the evaluation. </span><span class="kobospan" id="kobo.293.2">Of note are the </span><strong class="source-inline1"><span class="kobospan" id="kobo.294.1">precision</span></strong><span class="kobospan" id="kobo.295.1">, </span><strong class="source-inline1"><span class="kobospan" id="kobo.296.1">recall</span></strong><span class="kobospan" id="kobo.297.1">, and </span><strong class="source-inline1"><span class="kobospan" id="kobo.298.1">f1</span></strong><span class="kobospan" id="kobo.299.1"> values. </span><span class="kobospan" id="kobo.299.2">An </span><strong class="source-inline1"><span class="kobospan" id="kobo.300.1">f1</span></strong><span class="kobospan" id="kobo.301.1"> of </span><strong class="source-inline1"><span class="kobospan" id="kobo.302.1">0.88</span></strong><span class="kobospan" id="kobo.303.1">, observed in this case, is an indicator of the very good efficacy of the classifier, though it could always be </span><span><span class="kobospan" id="kobo.304.1">improved further:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.305.1">
print(eval_results)
{'accuracy': 0.88,
'precision': 0.92,
'recall': 0.84,
'f1': 0.88,
'total_time_in_seconds': 27.23,
'samples_per_second': 39.146,
'latency_in_seconds': 0.025}</span></pre></li>			</ol>
			<p class="calibre3"><span class="kobospan" id="kobo.306.1">In this recipe, we used a pre-trained classifier to classify the data on a dataset. </span><span class="kobospan" id="kobo.306.2">The dataset and model were both for sentiment analysis. </span><span class="kobospan" id="kobo.306.3">There are cases where we can use classifiers that are trained on a different class of data but can still be used as is. </span><span class="kobospan" id="kobo.306.4">This saves us from having to train a classifier of our own and repurpose a model that already exists. </span><span class="kobospan" id="kobo.306.5">We will learn about this use case in the </span><span><span class="kobospan" id="kobo.307.1">next recipe.</span></span></p>
			<h1 id="_idParaDest-213" class="calibre7"><a id="_idTextAnchor219" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.308.1">Using a zero-shot classifier</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.309.1">In this recipe, we will classify a sentence using a zero-shot classifier. </span><span class="kobospan" id="kobo.309.2">There are instances where we do not have the luxury of training a classifier from scratch or using a model that has been trained as per the labels of our</span><a id="_idIndexMarker461" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.310.1"> data. </span><strong class="bold"><span class="kobospan" id="kobo.311.1">Zero-shot classification</span></strong><span class="kobospan" id="kobo.312.1"> can be used in such scenarios for any team to get up and running quickly. </span><span class="kobospan" id="kobo.312.2">The zero in the terminology means that the classifier has not seen any data (zero samples precisely) from the target dataset that will be used </span><span><span class="kobospan" id="kobo.313.1">for inference.</span></span></p>
			<h2 id="_idParaDest-214" class="calibre5"><a id="_idTextAnchor220" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.314.1">Getting ready</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.315.1">As part of this recipe, we will use the pipeline module from the transformers package. </span><span class="kobospan" id="kobo.315.2">You can use the </span><strong class="source-inline"><span class="kobospan" id="kobo.316.1">8.4_Zero_shot_classification.ipynb</span></strong><span class="kobospan" id="kobo.317.1"> notebook from the code site if you need to work from an </span><span><span class="kobospan" id="kobo.318.1">existing notebook.</span></span></p>
			<h2 id="_idParaDest-215" class="calibre5"><a id="_idTextAnchor221" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.319.1">How to do it...</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.320.1">In this </span><a id="_idIndexMarker462" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.321.1">recipe, we will use a couple of sentences and classify them. </span><span class="kobospan" id="kobo.321.2">We will use our own set of labels for these sentences. </span><span class="kobospan" id="kobo.321.3">We will use the </span><strong class="source-inline"><span class="kobospan" id="kobo.322.1">facebook/bart-large-mnli </span></strong><span class="kobospan" id="kobo.323.1">model for this recipe. </span><span class="kobospan" id="kobo.323.2">This model is suitable for the task of </span><span><span class="kobospan" id="kobo.324.1">zero-shot classification.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.325.1">The recipe does the </span><span><span class="kobospan" id="kobo.326.1">following things:</span></span></p>
			<ul class="calibre15">
				<li class="calibre14"><span class="kobospan" id="kobo.327.1"> Initializes a pipeline based on a zero-shot </span><span><span class="kobospan" id="kobo.328.1">classification model</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.329.1"> Uses the pipeline to classify a sentence into a custom set of </span><span><span class="kobospan" id="kobo.330.1">user-defined labels</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.331.1">Prints the results of the classification with the classes and their </span><span><span class="kobospan" id="kobo.332.1">associated probabilities</span></span></li>
			</ul>
			<p class="calibre3"><span class="kobospan" id="kobo.333.1">The steps for the recipe are </span><span><span class="kobospan" id="kobo.334.1">as follows:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.335.1">Do the necessary imports and identify the compute device, as described in the previous </span><span><span class="kobospan" id="kobo.336.1">classification recipe:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.337.1">
from transformers import pipeline
import torch
device = torch.device(
    "cuda" if torch.cuda.is_available() else "cpu")</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.338.1">In this step, we initialize a pipeline instance with the </span><strong class="source-inline1"><span class="kobospan" id="kobo.339.1">facebook/bart-large-mnli</span></strong><span class="kobospan" id="kobo.340.1"> model. </span><span class="kobospan" id="kobo.340.2">We have chosen this particular model for our example, but other models can also be used – available on the </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.341.1">HuggingFace</span></strong></span><span><span class="kobospan" id="kobo.342.1"> site:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.343.1">
pipeline_instance = pipeline(
    model="facebook/bart-large-mnli")</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.344.1">Use the</span><a id="_idIndexMarker463" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.345.1"> pipeline instance to classify a sentence into a given set of candidate labels. </span><span class="kobospan" id="kobo.345.2">The labels provided in the example are completely novel and have been defined by us. </span><span class="kobospan" id="kobo.345.3">The model was not trained on examples with these labels. </span><span class="kobospan" id="kobo.345.4">The classification output is stored in the </span><strong class="source-inline1"><span class="kobospan" id="kobo.346.1">result</span></strong><span class="kobospan" id="kobo.347.1"> variable, which is a dictionary. </span><span class="kobospan" id="kobo.347.2">This dictionary has the </span><strong class="source-inline1"><span class="kobospan" id="kobo.348.1">'sequence'</span></strong><span class="kobospan" id="kobo.349.1">, </span><strong class="source-inline1"><span class="kobospan" id="kobo.350.1">'labels'</span></strong><span class="kobospan" id="kobo.351.1">, and </span><strong class="source-inline1"><span class="kobospan" id="kobo.352.1">'scores'</span></strong><span class="kobospan" id="kobo.353.1"> keys. </span><span class="kobospan" id="kobo.353.2">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.354.1">'sequence'</span></strong><span class="kobospan" id="kobo.355.1"> element stores the original sentence passed to the classifier. </span><span class="kobospan" id="kobo.355.2">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.356.1">'labels'</span></strong><span class="kobospan" id="kobo.357.1"> element stores the labels for the classes, but the ordering is different than what we passed in the arguments. </span><span class="kobospan" id="kobo.357.2">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.358.1">'scores'</span></strong><span class="kobospan" id="kobo.359.1"> element stores the probabilities of the classes and corresponds to the same ordering in the </span><strong class="source-inline1"><span class="kobospan" id="kobo.360.1">'labels'</span></strong><span class="kobospan" id="kobo.361.1"> element. </span><span class="kobospan" id="kobo.361.2">The last argument in this call is </span><strong class="source-inline1"><span class="kobospan" id="kobo.362.1">device</span></strong><span class="kobospan" id="kobo.363.1">. </span><span class="kobospan" id="kobo.363.2">If there is a CUDA-compatible device present in the system, it will </span><span><span class="kobospan" id="kobo.364.1">be used:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.365.1">
result = pipeline_instance(
    "I am so hooked to video games as I cannot get any work done!",
    candidate_labels=["technology", "gaming", "hobby", "art", "computer"], device=device)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.366.1">We print the sequence, followed by printing each label and its associated probability. </span><span class="kobospan" id="kobo.366.2">Note that the order of the labels has changed from the initial input that we specified in the previous step. </span><span class="kobospan" id="kobo.366.3">The function call reorders the labels based on the descen</span><a id="_idTextAnchor222" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.367.1">ding order of the </span><span><span class="kobospan" id="kobo.368.1">label probability:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.369.1">
print(result['sequence'])
for i, label in enumerate(result['labels']):
    print(f"{label}:  {result['scores'][i]:.2f}")
I am so hooked to video games as I cannot get any work done!
</span><span class="kobospan1" id="kobo.369.2">gaming:  0.85
hobby:  0.08
technology:  0.07
computer:  0.00
art:  0.00</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.370.1">We run a </span><a id="_idIndexMarker464" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.371.1">zero-shot classification on a different sentence and print the results for it. </span><span class="kobospan" id="kobo.371.2">This time, we emit a result that picks the class with the highest probability and prints </span><span><span class="kobospan" id="kobo.372.1">the result:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.373.1">
result = pipeline_instance(
    "A early morning exercise regimen can drive many diseases away!",
    candidate_labels=["health", "medical", "weather", "geography", "politics"], )
print(result['sequence'])
for i, label in enumerate(result['labels']):
    print(f"{label}:  {result['scores'][i]:.2f}")
print(
    f"The most probable class for the sentence is ** 
    {result['labels'][0]} ** "
    f"with a probability of {result['scores'][0]:.2f}"
)
A early morning exercise regimen can drive many diseases away!
</span><span class="kobospan1" id="kobo.373.2">health:  0.91
medical:  0.07
weather:  0.01
geography:  0.01
politics:  0.00
The most probable class for the sentence is ** health ** with a probability of 0.91</span></pre></li>			</ol>
			<p class="calibre3"><span class="kobospan" id="kobo.374.1">So far, we have</span><a id="_idIndexMarker465" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.375.1"> used the transformer and some pre-trained models to generate token IDs and classifications. </span><span class="kobospan" id="kobo.375.2">These recipes have used the encoder part of the transformer. </span><span class="kobospan" id="kobo.375.3">The encoder generates a representation of the text, which is then used by a classifier head in front of it to generate classification labels. </span><span class="kobospan" id="kobo.375.4">However, the transformer has another component, called the decoder. </span><span class="kobospan" id="kobo.375.5">A decoder uses a given representation of text and generates subsequent text. </span><span class="kobospan" id="kobo.375.6">In the next recipe, we will learn more about </span><span><span class="kobospan" id="kobo.376.1">the decoder.</span></span></p>
			<h1 id="_idParaDest-216" class="calibre7"><a id="_idTextAnchor223" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.377.1">Generating text</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.378.1">In this recipe, we will use</span><a id="_idIndexMarker466" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.379.1"> a </span><strong class="bold"><span class="kobospan" id="kobo.380.1">generative transformer model</span></strong><span class="kobospan" id="kobo.381.1"> to generate text from a given seed sentence. </span><span class="kobospan" id="kobo.381.2">One such model to generate text is the GPT-2 model, which </span><a id="_idIndexMarker467" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.382.1">is an improved version of the original </span><strong class="bold"><span class="kobospan" id="kobo.383.1">General Purpose Transformer</span></strong><span class="kobospan" id="kobo.384.1"> (</span><span><strong class="bold"><span class="kobospan" id="kobo.385.1">GPT</span></strong></span><span><span class="kobospan" id="kobo.386.1">) model.</span></span></p>
			<h2 id="_idParaDest-217" class="calibre5"><a id="_idTextAnchor224" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.387.1">Getting ready</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.388.1">As part of this recipe, we will use the pipeline module from the transformers package. </span><span class="kobospan" id="kobo.388.2">You can use the </span><strong class="source-inline"><span class="kobospan" id="kobo.389.1">8.5_Transformer_text_generation.ipynb</span></strong><span class="kobospan" id="kobo.390.1"> notebook from the code site if you need to work from an </span><span><span class="kobospan" id="kobo.391.1">existing notebook.</span></span></p>
			<h2 id="_idParaDest-218" class="calibre5"><a id="_idTextAnchor225" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.392.1">How to do it...</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.393.1">In this recipe, we </span><a id="_idIndexMarker468" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.394.1">will start with an initial seed sentence and use the GPT-2 model to generate text based on the given seed sentence. </span><span class="kobospan" id="kobo.394.2">We will also tinker with certain parameters to improve the quality of the </span><span><span class="kobospan" id="kobo.395.1">generated text.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.396.1">The recipe does the </span><span><span class="kobospan" id="kobo.397.1">following things:</span></span></p>
			<ul class="calibre15">
				<li class="calibre14"><span class="kobospan" id="kobo.398.1">It initializes a starting sentence from which a continuing sentence will </span><span><span class="kobospan" id="kobo.399.1">be generated</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.400.1">It initializes a GPT-2 model as part of a pipeline and uses it to generate five sentences as part of the parameters passed to the </span><span><span class="kobospan" id="kobo.401.1">generator method</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.402.1">It prints the results of </span><span><span class="kobospan" id="kobo.403.1">the generation</span></span></li>
			</ul>
			<p class="calibre3"><span class="kobospan" id="kobo.404.1">The steps for </span><a id="_idIndexMarker469" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.405.1">the recipe are </span><span><span class="kobospan" id="kobo.406.1">as follows:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.407.1">Do the necessary imports and identify the compute device, as described in the previous </span><span><span class="kobospan" id="kobo.408.1">classification recipe:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.409.1">
from transformers import pipeline
import torch
device = torch.device(
    "cuda" if torch.cuda.is_available() else "cpu")</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.410.1">Initialize a seed input sentence based on which the subsequent text will be generated. </span><span class="kobospan" id="kobo.410.2">Our goal here is to use the GPT-2 decoder to hypothetically generate the text that follows it based on the </span><span><span class="kobospan" id="kobo.411.1">generation parameters:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.412.1">
text = "The cat had no business entering the neighbors garage, but"</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.413.1">In this step, we initialize a text-generation pipeline with the </span><strong class="source-inline1"><span class="kobospan" id="kobo.414.1">'gpt-2'</span></strong><span class="kobospan" id="kobo.415.1"> model. </span><span class="kobospan" id="kobo.415.2">This model is based on </span><a id="_idIndexMarker470" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.416.1">a </span><strong class="bold"><span class="kobospan" id="kobo.417.1">large language model</span></strong><span class="kobospan" id="kobo.418.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.419.1">LLM</span></strong><span class="kobospan" id="kobo.420.1">) that was trained using a large text corpus. </span><span class="kobospan" id="kobo.420.2">The last argument in this call is </span><strong class="source-inline1"><span class="kobospan" id="kobo.421.1">device</span></strong><span class="kobospan" id="kobo.422.1">. </span><span class="kobospan" id="kobo.422.2">If there is a CUDA-compatible device present in the system, it will </span><span><span class="kobospan" id="kobo.423.1">be used:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.424.1">
generator = pipeline(
    'text-generation', model='gpt2', device=device)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.425.1">Generate the continuing sequence for the seed sentence and store the results. </span><span class="kobospan" id="kobo.425.2">The parameters of note used in the call other than the seed text are </span><span><span class="kobospan" id="kobo.426.1">as follows:</span></span><ul class="calibre19"><li class="calibre14"><strong class="source-inline1"><span class="kobospan" id="kobo.427.1">max_length</span></strong><span class="kobospan" id="kobo.428.1">: The maximum length of the generated sentence, including the length of the </span><span><span class="kobospan" id="kobo.429.1">seed sentence.</span></span></li><li class="calibre14"><strong class="source-inline1"><span class="kobospan" id="kobo.430.1">num_return_sequences</span></strong><span class="kobospan" id="kobo.431.1">: The number of generated sequences </span><span><span class="kobospan" id="kobo.432.1">to return.</span></span></li><li class="calibre14"><strong class="source-inline1"><span class="kobospan" id="kobo.433.1">num_beams</span></strong><span class="kobospan" id="kobo.434.1">: This </span><a id="_idIndexMarker471" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.435.1">parameter controls the quality of the generated sequence. </span><span class="kobospan" id="kobo.435.2">A higher number generally results in improved quality of the generated sequence but also slows down the generation. </span><span class="kobospan" id="kobo.435.3">We encourage you to try out different values for this parameter based on the quality requirements of the </span><span><span class="kobospan" id="kobo.436.1">generated sequence.</span></span><pre class="source-code"><span class="kobospan1" id="kobo.437.1">
generated_sentences = generator(
    text,do_sample=True, max_length=30,
    num_return_sequences=5, num_beams=5,
    pad_token_id=50256)</span></pre></li></ul></li>				<li class="calibre14"><span class="kobospan" id="kobo.438.1">Print the </span><a id="_idIndexMarker472" class="calibre6 pcalibre pcalibre1"/><span><span class="kobospan" id="kobo.439.1">generated sentences:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.440.1">
[print(generated_sentence['generated_text']) 
    for generated_sentence in generated_sentences]
The cat had no business entering the neighbors garage, but  he was able to get inside.  The cat had been in the neighbor's
The cat had no business entering the neighbors garage, but  the owner of the house called 911.  He said he found the cat in
The cat had no business entering the neighbors garage, but  he was able to get his hands on one of the keys.  It was
The cat had no business entering the neighbors garage, but  he didn't seem to mind at all.  He had no idea what he
The cat had no business entering the neighbors garage, but  the cat had no business entering the neighbors garage, but the cat had no business entering</span></pre></li>			</ol>
			<h2 id="_idParaDest-219" class="calibre5"><a id="_idTextAnchor226" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.441.1">There’s more…</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.442.1">As we can see in the preceding example, the generated output was rudimentary, repetitive, grammatically incorrect, or perhaps incoherent. </span><span class="kobospan" id="kobo.442.2">There are different techniques</span><a id="_idIndexMarker473" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.443.1"> that we can use to improve the </span><span><span class="kobospan" id="kobo.444.1">g</span><a id="_idTextAnchor227" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.445.1">enerated output.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.446.1">We will use the </span><strong class="source-inline"><span class="kobospan" id="kobo.447.1">no_repeat_ngram_size</span></strong><span class="kobospan" id="kobo.448.1"> parameter this time to generate the text. </span><span class="kobospan" id="kobo.448.2">We will set the value of this parameter to </span><strong class="source-inline"><span class="kobospan" id="kobo.449.1">2</span></strong><span class="kobospan" id="kobo.450.1">. </span><span class="kobospan" id="kobo.450.2">This instructs the generator to not </span><span><span class="kobospan" id="kobo.451.1">repeat bi-grams.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.452.1">We will change the line in </span><em class="italic"><span class="kobospan" id="kobo.453.1">step 4</span></em><span class="kobospan" id="kobo.454.1"> to </span><span><span class="kobospan" id="kobo.455.1">the following:</span></span></p>
			<pre class="source-code"><span class="kobospan1" id="kobo.456.1">
generated_sentences = generator(text, do_sample=True,
    max_length=30, num_return_sequences=5, num_beams=5,
    </span><strong class="bold1"><span class="kobospan1" id="kobo.457.1">no_repeat_ngram_size=2</span></strong><span class="kobospan1" id="kobo.458.1">,  pad_token_id=50256)</span></pre>			<p class="calibre3"><span class="kobospan" id="kobo.459.1">As we can see in the following output, the sentences have reduced repetition, but some of them are </span><span><span class="kobospan" id="kobo.460.1">still incoherent:</span></span></p>
			<pre class="source-code"><span class="kobospan1" id="kobo.461.1">
The cat had no business entering the neighbors garage, but  it was too late to stop it.
</span><span class="kobospan1" id="kobo.461.2">"I don't know if it was
The cat had no business entering the neighbors garage, but  she was able to find her way to the porch, where she and her friend were
The cat had no business entering the neighbors garage, but  he did get in the way.
</span><span class="kobospan1" id="kobo.461.3">The next day, the neighbor called the police
The cat had no business entering the neighbors garage, but  he managed to get his hands on one of the keys, which he used to unlock
The cat had no business entering the neighbors garage, but  the neighbors thought they were in the right place.
</span><span class="kobospan1" id="kobo.461.4">"What's going on</span></pre>			<p class="calibre3"><span class="kobospan" id="kobo.462.1">To improve the coherency, we can use another technique to include the next word from a set of words that have the highest likelihood of being the next word. </span><span class="kobospan" id="kobo.462.2">We use the </span><strong class="source-inline"><span class="kobospan" id="kobo.463.1">top_k</span></strong><span class="kobospan" id="kobo.464.1"> parameter and set its value to </span><strong class="source-inline"><span class="kobospan" id="kobo.465.1">50</span></strong><span class="kobospan" id="kobo.466.1">. </span><span class="kobospan" id="kobo.466.2">This instructs the generator to sample the next word from the top 50 words, arranged according to </span><span><span class="kobospan" id="kobo.467.1">their probabilities.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.468.1">We change the</span><a id="_idIndexMarker474" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.469.1"> line in </span><em class="italic"><span class="kobospan" id="kobo.470.1">step 4</span></em><span class="kobospan" id="kobo.471.1"> to </span><span><span class="kobospan" id="kobo.472.1">the following:</span></span></p>
			<pre class="source-code"><span class="kobospan1" id="kobo.473.1">
generated_sentences = generator(text, do_sample=True,
    max_length=30, num_return_sequences=5, num_beams=5,
    no_repeat_ngram_size=2, </span><strong class="bold1"><span class="kobospan1" id="kobo.474.1">top_k=50</span></strong><span class="kobospan1" id="kobo.475.1">, pad_token_id=50256)
The cat had no business entering the neighbors garage, but  it did get into a neighbor's garage. </span><span class="kobospan1" id="kobo.475.2">The neighbor went to check on the cat
The cat had no business entering the neighbors garage, but  she was there to take care of it.
</span><span class="kobospan1" id="kobo.475.3">The next morning, the cat was
The cat had no business entering the neighbors garage, but  it didn't want to leave. </span><span class="kobospan1" id="kobo.475.4">The neighbor told the cat to get out of the
The cat had no business entering the neighbors garage, but  the neighbors were too afraid to call 911.The neighbor told the police that he
The cat had no business entering the neighbors garage, but  it was there that he found his way to the kitchen, where it was discovered that</span></pre>			<p class="calibre3"><span class="kobospan" id="kobo.476.1">We can also combine the </span><strong class="source-inline"><span class="kobospan" id="kobo.477.1">top_k</span></strong><span class="kobospan" id="kobo.478.1"> parameter with the </span><strong class="source-inline"><span class="kobospan" id="kobo.479.1">top_p</span></strong><span class="kobospan" id="kobo.480.1"> parameter. </span><span class="kobospan" id="kobo.480.2">This instructs the generator to select the next word from the set of words that have a probability higher than this defined value. </span><span class="kobospan" id="kobo.480.3">Adding this parameter with a value of </span><strong class="source-inline"><span class="kobospan" id="kobo.481.1">0.8</span></strong><span class="kobospan" id="kobo.482.1"> yields the </span><span><span class="kobospan" id="kobo.483.1">following output:</span></span></p>
			<pre class="source-code"><span class="kobospan1" id="kobo.484.1">
generated_sentences = generator(text, do_sample=True,
    max_length=30, num_return_sequences=5, num_beams=5,
    no_repeat_ngram_size=2, </span><strong class="bold1"><span class="kobospan1" id="kobo.485.1">top_k=50</span></strong><span class="kobospan1" id="kobo.486.1">, </span><strong class="bold1"><span class="kobospan1" id="kobo.487.1">top_p=0.8</span></strong><span class="kobospan1" id="kobo.488.1">,
    pad_token_id=50256)
The cat had no business entering the neighbors garage, but  the owner of the house told the police that he did not know what was going on
The cat had no business entering the neighbors garage, but  he did, and the cat was able to get out of the garage.The
The cat had no business entering the neighbors garage, but  he was able to get in through the back door. </span><span class="kobospan1" id="kobo.488.2">The cat was not injured,
The cat had no business entering the neighbors garage, but  the neighbor told the police that the cat was a stray, and the neighbor said that
The cat had no business entering the neighbors garage, but  the owner of the house said he didn't know what to do with the cat.</span></pre>			<p class="calibre3"><span class="kobospan" id="kobo.489.1">As we can see, the</span><a id="_idIndexMarker475" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.490.1"> addition of additional parameters to the generator continues to improve the </span><span><span class="kobospan" id="kobo.491.1">generated output.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.492.1">As a final example, let us generate a longer output sequence by changing the line in </span><em class="italic"><span class="kobospan" id="kobo.493.1">step 4</span></em><span class="kobospan" id="kobo.494.1"> to </span><span><span class="kobospan" id="kobo.495.1">the following:</span></span></p>
			<pre class="source-code"><span class="kobospan1" id="kobo.496.1">
generated_sentences = generator(text, do_sample=True,
    </span><strong class="bold1"><span class="kobospan1" id="kobo.497.1">max_length=500, num_return_sequences=1</span></strong><span class="kobospan1" id="kobo.498.1">, num_beams=5,
    no_repeat_ngram_size=2, top_k=50, top_p=0.85,
    pad_token_id=50256)
The cat had no business entering the neighbors garage, but  she was there to help.
</span><span class="kobospan1" id="kobo.498.2">"I was like, 'Oh my God, she's here,'" she said. </span><span class="kobospan1" id="kobo.498.3">"I'm like 'What are you doing here?' </span><span class="kobospan1" id="kobo.498.4">"
The neighbor, who asked not to be identified, said she didn't know what to make of the cat's behavior. </span><span class="kobospan1" id="kobo.498.5">She said it seemed like it was trying to get into her home, and that she was afraid for her life. </span><span class="kobospan1" id="kobo.498.6">The neighbor said that when she went to check on her cat, it ran into the neighbor's garage and hit her in the face, knocking her to the ground.</span></pre>			<p class="calibre3"><span class="kobospan" id="kobo.499.1">As we can see, the generated output, however fictitious, is more coherent and readable. </span><span class="kobospan" id="kobo.499.2">We encourage you to experiment with different mixes of parameters and their respective values to improve the generated output based on their </span><span><span class="kobospan" id="kobo.500.1">use cases.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.501.1">Please note that the output returned by the model might differ a bit from what this example has shown. </span><span class="kobospan" id="kobo.501.2">This happens because the internal language model is probabilistic in nature. </span><span class="kobospan" id="kobo.501.3">The next word is sampled from a distribution that contains words that have a probability larger</span><a id="_idIndexMarker476" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.502.1"> than what we defined in our parameters </span><span><span class="kobospan" id="kobo.503.1">for generation.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.504.1">In this recipe, we used the decoder module of the transformer to generate text, given a seed sentence. </span><span class="kobospan" id="kobo.504.2">There are use cases where an encoder and decoder are used together to generate text. </span><span class="kobospan" id="kobo.504.3">We will learn about this in the </span><span><span class="kobospan" id="kobo.505.1">next recipe.</span></span></p>
			<h1 id="_idParaDest-220" class="calibre7"><a id="_idTextAnchor228" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.506.1">Language translation</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.507.1">In this recipe, we will use transformers for language translation. </span><span class="kobospan" id="kobo.507.2">We will use the </span><strong class="bold"><span class="kobospan" id="kobo.508.1">Google Text-To-Text Transfer Transformer</span></strong><span class="kobospan" id="kobo.509.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.510.1">T5</span></strong><span class="kobospan" id="kobo.511.1">) model. </span><span class="kobospan" id="kobo.511.2">This </span><a id="_idIndexMarker477" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.512.1">model is an end-to-end model that uses both the encoder and decoder components of the </span><span><span class="kobospan" id="kobo.513.1">transformer model.</span></span></p>
			<h2 id="_idParaDest-221" class="calibre5"><a id="_idTextAnchor229" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.514.1">Getting ready</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.515.1">As part of this recipe, we will use the pipeline module from the transformers package. </span><span class="kobospan" id="kobo.515.2">You can use the </span><strong class="source-inline"><span class="kobospan" id="kobo.516.1">8.6_Language_Translation_with_transformers.ipynb</span></strong><span class="kobospan" id="kobo.517.1"> notebook from the code site if you need to work from an </span><span><span class="kobospan" id="kobo.518.1">existing notebook.</span></span></p>
			<h2 id="_idParaDest-222" class="calibre5"><a id="_idTextAnchor230" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.519.1">How to do it...</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.520.1">In this recipe, you will initialize a seed sentence in English and translate it to French. </span><span class="kobospan" id="kobo.520.2">The T5 model </span><a id="_idIndexMarker478" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.521.1">expects the input format to encode the information about the language translation task along with the seed sentence. </span><span class="kobospan" id="kobo.521.2">In this case, the encoder uses the input in the source language and generates a representation of the text. </span><span class="kobospan" id="kobo.521.3">The decoder uses this representation and generates text for the target language. </span><span class="kobospan" id="kobo.521.4">The </span><a id="_idIndexMarker479" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.522.1">T5 model is trained specifically for this task, in addition to many others. </span><span class="kobospan" id="kobo.522.2">If you are running on a machine that does not have a CUDA-compatible device, it might take some time for the recipe steps to </span><span><span class="kobospan" id="kobo.523.1">be executed.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.524.1">The recipe does the </span><span><span class="kobospan" id="kobo.525.1">following things:</span></span></p>
			<ul class="calibre15">
				<li class="calibre14"><span class="kobospan" id="kobo.526.1"> It initializes the </span><strong class="source-inline1"><span class="kobospan" id="kobo.527.1">Google t5-base</span></strong><span class="kobospan" id="kobo.528.1"> model </span><span><span class="kobospan" id="kobo.529.1">and tokenizer</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.530.1"> It initializes a seed sentence in English that will be translated </span><span><span class="kobospan" id="kobo.531.1">into French</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.532.1"> It tokenizes the seed sentence along with the task specification to translate the seed sentence </span><span><span class="kobospan" id="kobo.533.1">into French</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.534.1">It generates the translated tokens, decodes them into the target language (French), and </span><span><span class="kobospan" id="kobo.535.1">prints them</span></span></li>
			</ul>
			<p class="calibre3"><span class="kobospan" id="kobo.536.1">The steps for the recipe are </span><span><span class="kobospan" id="kobo.537.1">as follows:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.538.1">Do the necessary imports and identify the compute device, as described in the previous </span><span><span class="kobospan" id="kobo.539.1">classification recipe:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.540.1">
from transformers import (
    T5Tokenizer, T5ForConditionalGeneration)
import torch
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.541.1">Initialize a tokenizer and model instance with the </span><strong class="source-inline1"><span class="kobospan" id="kobo.542.1">t5-base</span></strong><span class="kobospan" id="kobo.543.1"> model from Google. </span><span class="kobospan" id="kobo.543.2">We use the </span><strong class="source-inline1"><span class="kobospan" id="kobo.544.1">model_max_length</span></strong><span class="kobospan" id="kobo.545.1"> parameter of </span><strong class="source-inline1"><span class="kobospan" id="kobo.546.1">200</span></strong><span class="kobospan" id="kobo.547.1">. </span><span class="kobospan" id="kobo.547.2">Feel free to experiment with higher values if your seed sentence is longer than 200 words. </span><span class="kobospan" id="kobo.547.3">We also load the model onto the device that was identified for computation in </span><span><span class="kobospan" id="kobo.548.1">step 1:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.549.1">
tokenizer = T5Tokenizer.from_pretrained(
    't5-base', model_max_length=200)
model = T5ForConditionalGeneration.from_pretrained(
    't5-base', return_dict=True)
model = model.to(device)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.550.1">Initialize a seed sequence that you want </span><span><span class="kobospan" id="kobo.551.1">to translate:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.552.1">
language_sequence = ("It's such a beautiful morning today!")</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.553.1">Tokenize the input sequence. </span><span class="kobospan" id="kobo.553.2">The tokenizer specifies the source and the target language as part of its input encoding. </span><span class="kobospan" id="kobo.553.3">This is done by appending the “</span><strong class="source-inline1"><span class="kobospan" id="kobo.554.1">translate English to French:</span></strong><span class="kobospan" id="kobo.555.1">” text to the input seed sequence. </span><span class="kobospan" id="kobo.555.2">We load these token IDs into the device that is used for computation. </span><span class="kobospan" id="kobo.555.3">It is a requirement for</span><a id="_idIndexMarker480" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.556.1"> both the model and the token IDs to be on the </span><span><span class="kobospan" id="kobo.557.1">same device:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.558.1">
input_ids = tokenizer(
    "translate English to French: " + language_sequence,
    return_tensors="pt",
    truncation=True).input_ids.to(device)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.559.1">Translate the source language token IDs to the target language token IDs via the model. </span><span class="kobospan" id="kobo.559.2">The model uses the encoder-decoder architecture to convert the input token IDs to the output </span><span><span class="kobospan" id="kobo.560.1">token IDs:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.561.1">
language_ids = model.generate(input_ids, max_new_tokens=200)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.562.1">Decode the text from the token IDs to the target language tokens. </span><span class="kobospan" id="kobo.562.2">We use the tokenizer to convert the output token IDs to the target </span><span><span class="kobospan" id="kobo.563.1">language tokens:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.564.1">
language_translation = tokenizer.decode(
    language_ids[0], skip_special_tokens=True)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.565.1">Print the</span><a id="_idIndexMarker481" class="calibre6 pcalibre pcalibre1"/> <span><span class="kobospan" id="kobo.566.1">translated output:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.567.1">
print(language_translation)
C'est un matin si beau!</span></pre></li>			</ol>
			<p class="calibre3"><span class="kobospan" id="kobo.568.1">In conclusion, this chapter introduced the concept of transformers, along with some of its basic applications. </span><span class="kobospan" id="kobo.568.2">The next chapter will focus on how we can use the different NLP techniques to understand </span><span><span class="kobospan" id="kobo.569.1">text better.</span></span></p>
		</div>
	</body></html>