<html><head></head><body>
		<div><h1 id="_idParaDest-122" class="chapter-number"><a id="_idTextAnchor123"/>7</h1>
			<h1 id="_idParaDest-123"><a id="_idTextAnchor124"/>Swapping the Face Back into the Video</h1>
			<p>In this chapter, we’ll complete the deepfake process by converting the videos to swap faces using the models trained in the last chapter.</p>
			<p>Conversion is the last step of deepfaking, and it is the part that actually puts the new face onto the existing video. This requires you to already have a video that you have fully processed through the extraction process in <a href="B17535_05.xhtml#_idTextAnchor090"><em class="italic">Chapter 5</em></a>, <em class="italic">Extracting Faces</em>, and uses a trained model from <a href="B17535_06.xhtml#_idTextAnchor107"><em class="italic">Chapter 6</em></a>, <em class="italic">Training a </em><em class="italic">Deepfake Model</em>.</p>
			<p>We will cover the following topics in this chapter:</p>
			<ul>
				<li>Preparing to convert video</li>
				<li>Getting hands-on with the convert code</li>
				<li>Creating the video from images</li>
			</ul>
			<h1 id="_idParaDest-124"><a id="_idTextAnchor125"/>Technical requirements</h1>
			<p>For this chapter, you’ll need a <code>conda</code> environment setup. If you set this up in earlier chapters, the same <code>conda</code> environment will work fine. To get into the <code>conda</code> environment, you can run the following command:</p>
			<pre class="console">
conda activate deepfakes</pre>
			<p>If you have not created a <code>conda</code> environment to run the code, it’s recommended that you go to the Git repository and follow the instructions there. You can find the full repository at <a href="https://github.com/PacktPublishing/Exploring-Deepfakes">https://github.com/PacktPublishing/Exploring-Deepfakes</a>.</p>
			<h1 id="_idParaDest-125"><a id="_idTextAnchor126"/>Preparing to convert video</h1>
			<p>Conversion is not just a “one-and-done” script. It requires you to have turned a video into a series <a id="_idIndexMarker325"/>of frames and run <code>C5-face_detection.py</code> on those frames. This gets the data that you need for the conversion process in the right form. The conversion process will require the full extraction of every frame, as well as the <code>face_alignments.json</code> file that is generated by the extract process:</p>
			<div><div><img src="img/B17535_07_001.jpg" alt="Figure 7.1 – Example of a folder that has been extracted already. Note the face_images folder created by the extract process"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.1 – Example of a folder that has been extracted already. Note the face_images folder created by the extract process</p>
			<p>If you haven’t done the extract process on the video you want to convert, then you should go back to <a href="B17535_05.xhtml#_idTextAnchor090"><em class="italic">Chapter 5</em></a>, <em class="italic">Extracting Faces</em>, and extract the video.</p>
			<p>We need to do this because this is how the model knows which faces to convert. AI can detect all faces in a <a id="_idIndexMarker326"/>frame but won’t know which ones should be swapped, meaning that all faces will be swapped. By running the extract process and cleaning out the faces we <em class="italic">don’t</em> want to swap from the folder of extracted faces, we can control which faces get swapped.</p>
			<p>In addition, you would probably want to include the frames you’re going to convert in your training data, in what we call “fit training,” which makes sure your model has some experience with the exact frames you’re converting. To do this, go back to <a href="B17535_06.xhtml#_idTextAnchor107"><em class="italic">Chapter 6</em></a>, <em class="italic">Training a Deepfake Model</em>, and point the “<em class="italic">A</em>” side of your model to the directory containing <a id="_idIndexMarker327"/>the frames you’re going to use to convert.</p>
			<p class="callout-heading">Author’s note</p>
			<p class="callout">If you’re interested in an “on the fly” conversion process that swaps all faces, you can check the exercises page at the end of this chapter, where we raise that exact question. In fact, every chapter in this section has a list of exercises for you to get your hands dirty and get the experience of writing your own code for deepfakes.</p>
			<p>Next, let’s look at the convert code.</p>
			<h1 id="_idParaDest-126"><a id="_idTextAnchor127"/>Getting hands-on with the convert code</h1>
			<p>Like the rest of the chapters in this section, we’ll be going through the code line by line to talk about how it works and what it’s doing.</p>
			<h2 id="_idParaDest-127"><a id="_idTextAnchor128"/>Initialization</h2>
			<p>Here we will initialize and <a id="_idIndexMarker328"/>prepare the code to run the convert process:</p>
			<ol>
				<li>Like all Python code, we’ll start with the imports:<pre class="source-code">
import os
from argparse import ArgumentParser
import json_tricks
import torch
import cv2
import numpy as np
from tqdm import tqdm
import face_alignment
from face_alignment.detection.sfd import FaceDetector
from face_alignment import FaceAlignment, LandmarksType
from lib.bisenet import BiSeNet
from lib.models import OriginalEncoder, OriginalDecoder</pre></li>
			</ol>
			<p>These libraries are all ones we’ve already seen in previous chapters. This is because <a id="_idIndexMarker329"/>the conversion process is not really doing anything too different from what we’ve done before. We’ll see that as we go through the code to covert the face back into the original images.</p>
			<ol>
				<li value="2">Next, we’ll check whether we’re running from the command line:<pre class="source-code">
If __name__ == "__main__":
  """ Process images, replacing the face with another as trained
      Example CLI:
      ------------
      python C7-convert.py "C:/media_files/"
  """</pre></li>
			</ol>
			<p>This code doesn’t actually do anything but is a common way to set up a Python file to run when called. It allows you to import the script into other scripts without running those commands.</p>
			<ol>
				<li value="3">Next, we’ll parse the arguments for the script:<pre class="source-code">
  parser = ArgumentParser()
  parser.add_argument("path",
    help="folder of images to convert")
  parser.add_argument("--model-path",
    default="model/",
    help="folder which has the trained model")
  parser.add_argument("--cpu",
    action="store_true",
    help="Force CPU usage")
  parser.add_argument("--swap",
    action="store_true",
    help="Convert to the first face instead of the second")
  parser.add_argument("--json-path",
   default="$path/face_images/face_alignments.json",
   help="path to the json data from the extract")
  parser.add_argument("--export-path",
    default="$path/convert/",
    help="folder to put images (swaps $path with input path)")</pre></li>
			</ol>
			<p>This code uses <a id="_idIndexMarker330"/>the standard <code>ArgumentParser</code> library from Python to parse command-line arguments. This lets us set defaults to some options and change them if we want:</p>
			<pre class="source-code">
  Opt = parser.parse_args()
  opt.export_path = opt.export_path.replace("$path", opt.path)
  opt.json_path = opt.json_path.replace("$path", opt.path)
  main(opt)</pre>
			<p>Here we process the arguments, add the path to applicable variables, and pass these arguments to the <code>main()</code> function, which will actually process the conversion.</p>
			<p>We adjust the path variables to add in the default path. This lets us have JSON and export folders within subfolders of the data folder. Otherwise, each would have to be specified <a id="_idIndexMarker331"/>separately and could be in very different places. You are still able to specify a specific folder if you want, but the defaults help keep things organized.</p>
			<ol>
				<li value="4">We now move back up to the start of the main function:<pre class="source-code">
def main(opt):</pre></li>
			</ol>
			<p>Again, this is just the main function that does the work. It doesn’t actually do anything except organize our code and allow us to keep things to a normal “pythonic” operation.</p>
			<ol>
				<li value="5">Our next step is to make sure that the folders we’re going to write into exist:<pre class="source-code">
if not os.path.exists(opt.export_path):
        os.mkdir(opt.export_path)</pre></li>
			</ol>
			<p>This section checks the export path and ensures that it already exists, creating it if it doesn’t.</p>
			<h2 id="_idParaDest-128"><a id="_idTextAnchor129"/>Loading the AI</h2>
			<p>The next step is to <a id="_idIndexMarker332"/>load the AI and put it onto the device that it needs to be on:</p>
			<ol>
				<li>First, we check whether <code>cuda</code> is available and whether the CPU override was given:<pre class="source-code">
device = "cuda" if torch.cuda.is_available() and not opt.cpu else "cpu"</pre></li>
			</ol>
			<p>This code checks whether <code>cuda</code> is enabled in PyTorch, and if it is and the user hasn’t disabled it with a command line switch, and enables <code>cuda</code> for the rest of the code accordingly.</p>
			<ol>
				<li value="2">Next, we build the AI models with the following code:<pre class="source-code">
encoder = OriginalEncoder()
decoder = OriginalDecoder()</pre></li>
			</ol>
			<p>This code establishes the encoder and decoder. Unlike when we were training, we only need <a id="_idIndexMarker333"/>one decoder. This is because training requires both faces to be able to learn successfully, but once trained, we only need the decoder for the face we’re swapping in.</p>
			<ol>
				<li value="3">Loading model weights comes next:<pre class="source-code">
encoder.load_state_dict(torch.load(
  os.path.join( opt.model_path, "encoder.pth")).state_dict())
if not opt.swap:
  decoder.load_state_dict(torch.load(
   os.path.join(opt.model_path, "decoderb.pth")).state_dict())
else:
  decoder.load_state_dict(torch.load(
   os.path.join(opt.model_path, "decodera.pth")).state_dict())</pre></li>
			</ol>
			<p>This code loads the weights from the trained model. We first load the encoder weights. These are always the same, so they get pulled in from the <code>encoder.pth</code> file, which holds the trained weights for the encoder.</p>
			<p>For the decoder, by default, we want to load the “b” weights, which are stored in the <code>decoderb.pth</code> file, but you may want the “a” face to be swapped into the “b” image, so we included a command line switch that will load the “a” weights from the <code>decodera.pth</code> file instead. The weights work identically and correlate to <a id="_idIndexMarker334"/>the faces that were used to train originally. The exact order doesn’t matter since we included the <code>swap</code> flag, but only one direction could be default, so the “b” face onto the “a” image is the assumption unless overridden here.</p>
			<p>No matter which decoder is loaded, we first load the weights and then assign them to the state dictionary of the model. PyTorch handles all the specifics of getting the dictionary loaded as matrices and into a form ready to handle tensors.</p>
			<ol>
				<li value="4">Next, we move the models to the GPU:<pre class="source-code">
If device == "cuda":
  encoder = encoder.cuda()
  decoder = decoder.cuda()</pre></li>
			</ol>
			<p>If the device is set to <code>cuda</code>, we load the models onto the GPU. To do this, we tell PyTorch to use <code>cuda</code> on the models, which will handle the nitty-gritty of moving the models from the CPU to the GPU.</p>
			<h2 id="_idParaDest-129"><a id="_idTextAnchor130"/>Preparing data</h2>
			<p>Next, we need to <a id="_idIndexMarker335"/>get the data loaded and into a format that PyTorch expects:</p>
			<ol>
				<li>First, we load the alignment data from a file:<pre class="source-code">
with open(os.path.join(json_path), "r", encoding="utf-8") as alignment_file:
  alignment_data = json_tricks.loads( alignment_file.read(), encoding="utf-8")
  alignment_keys = list(alignment_data.keys())</pre></li>
			</ol>
			<p>This code loads the alignment data from the JSON file saved by the extract process. This file includes all the information that we need to be able to pull the face from the original image, convert it, and paste it back into the image. This uses the information from the extraction instead of doing it on the fly because that information is already generated when we created the training data, and re-using that data saves a lot of time as well as enables clean up and manual editing of the data.</p>
			<p>You can specify the JSON file to load with the data for the image data that you’re <a id="_idIndexMarker336"/>converting, but, if left blank, the default locations will be looked at which, unless you changed it during extraction, should find the file.</p>
			<p>We use <code>json_tricks</code> again because of its very powerful handling of NumPy arrays, which automatically loads the arrays back into the correct datatype and matrix shape.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">While the inclusion or the description of tools that edit these alignments are outside the scope of this book, the Faceswap project does include advanced alignment modification tools, including an advanced “manual” tool that allows click-and-drag editing of landmarks and faces.</p>
			<ol>
				<li value="2">The next step is to get a list of images to convert:<pre class="source-code">
list_of_images_in_dir = [file for file in os.listdir(opt.path)
 if os.path.isfile(os.path.join(opt.path, file))
 and file in alignment_keys]</pre></li>
			</ol>
			<p>This code loads all the images from the folder and then filters the images by throwing out any that don’t exist in the alignment data we loaded from the JSON data file. This makes sure that we have all the information to convert the image since <a id="_idIndexMarker337"/>even if a new image were added to the folder, we would need to extract information to be able to convert the file anyway.</p>
			<h2 id="_idParaDest-130"><a id="_idTextAnchor131"/>The conversion loop</h2>
			<p>Here, we begin the <a id="_idIndexMarker338"/>loop that will go through each individual image one at a time to convert them:</p>
			<ol>
				<li>We’re now entering the loop and loading images.<pre class="source-code">
for file in tqdm(list_of_images_in_dir):
  filename, extension = os.path.splitext(file)
  image_bgr = cv2.imread(os.path.join(opt.path, file))
  image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)
  width, height, channels = image_bgr.shape
  output_image = image_rgb</pre></li>
			</ol>
			<p>This code loads the image and prepares it for use. First, it gets the filename and extension into variables so we can use them again later. It then loads the file in <strong class="bold">blue, green, red</strong> (<strong class="bold">BGR</strong>) color order and converts it into a <strong class="bold">red, green, blue </strong>(<strong class="bold">RGB</strong>)-ordered image as expected by our AI. Then, it saves the width, height, and color channels into variables so we can use them again later. Finally, it creates a working copy of the output image so that we can swap any faces in that image.</p>
			<ol>
				<li value="2">Next, we start another loop, this time for faces:<pre class="source-code">
for idx, face in enumerate(alignment_data[file]['faces']):
  aligned_face = cv2.warpAffine(image_rgb, face["warp_matrix"][:2], (256, 256))
  aligned_face_tensor = torch.tensor(aligned_face/255,
    dtype=torch.float32).permute(2, 0, 1)
  aligned_face_tensor_small = torch.nn.functional.interpolate(
    aligned_face_tensor.unsqueeze(0), size=(64,64),
    mode='bilinear', align_corners=False)
if device == "cuda":
  aligned_face_tensor_small = aligned_face_tensor_small.cuda()</pre></li>
			</ol>
			<p>This loop will process each face that is found in the alignment file and swap the faces that are found in it.</p>
			<p>The first thing it does is pull the face from the frame using the pre-computed warp matrix <a id="_idIndexMarker339"/>that was saved in the alignment file. This matrix allows us to align the face and generate a 256x256 image of it.</p>
			<p>Next, we convert that face image into a tensor and move the channels into the order that PyTorch expects them. The first part of the tensor conversion is to convert from an integer range of 0–255 to a standard range of 0–1. We do this by dividing by 255. Then we use <code>permute</code> to reorder the matrix because PyTorch wants the channels to be first, while OpenCV has them last.</p>
			<p>Next, we create a smaller 64x64 copy of the tensor, which is what we’ll actually feed into the model. Since we’re doing this one image at a time, we’re effectively working with a batch size of 1, but we need to use <code>unsqueeze</code> on the tensor to create the batch channel of the tensor. This just adds a new dimension of size 1, which contains the image we want to convert.</p>
			<p>Finally, if we are using <code>cuda</code>, we move the smaller aligned face tensor onto the GPU so that we can put it through the model there.</p>
			<ol>
				<li value="3">Then, we send the image through the AI:<pre class="source-code">
with torch.no_grad():
  output_face_tensor = decoder( encoder(
    aligned_face_tensor_small ))</pre></li>
			</ol>
			<p>This code does the actual AI swapping, and it’s rather astonishing how small it is.</p>
			<p>We start this section by telling PyTorch that we want to run the AI in this section without keeping track of gradients by using <code>torch.no_grad()</code>. We can save a lot of VRAM and run the conversion faster. This isn’t strictly necessary here, but it is a good habit to get into.</p>
			<p>Next, we put the tensor containing the 64x64 face through the encoder and then the <a id="_idIndexMarker340"/>decoder to get a swapped face. The encoder’s output is fed straight into the decoder because we don’t need to do anything with the latent encoding.</p>
			<ol>
				<li value="4">Here, we apply the mask to the output:<pre class="source-code">
output_face_tensor = torch.nn.functional.interpolate(
  output_face_tensor, size=(256,256))
mask_img = cv2.imread(os.path.join(extract_path,
  f"face_mask_{filename}_{idx}.png"), 0)
mask_tensor = torch.where(torch.tensor(mask_img) &gt;
  200, 1, 0)
output_face_tensor = (output_face_tensor.cpu() *
  mask_tensor) + ( aligned_face_tensor.cpu() * (1 –
  mask_tensor))</pre></li>
			</ol>
			<p>We want to apply the mask so that we don’t swap a big square box of noise around the face. To do this, we will load the mask image and use it to cut out just the face from the swap.</p>
			<p>First, we resize the swapped face up to a 256x256 image. This is done because the mask is a 256x256 image, and applying it at a higher resolution helps to get the best detail on the edge instead of downscaling the mask to 64x64.</p>
			<p>Next, we load the mask image. To do this, we use the aligned face filename to generate the mask image filename. We then load that as a grayscale image using OpenCV’s image reader:</p>
			<div><div><img src="img/B17535_07_002.jpg" alt="Figure 7.2 – An example of a mask image"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.2 – An example of a mask image</p>
			<p>That image is then converted into a tensor using a cutoff point where if a pixel of the grayscale <a id="_idIndexMarker341"/>mask image’s value is higher than 200 (in a range of 0-255), then treat it as a <code>1</code>; otherwise, treat it as a <code>0</code>. This gives us a clean binary mask where the <code>1</code> value is a face to swap and <code>0</code> is unimportant background. We can then use that mask to paste just the swapped face back into the original image.</p>
			<p>Finally, we apply the mask to the image. This is done by multiplying the output face by the mask and multiplying the original face by the inverse of the mask. Effectively, this combines the face from the swap result with the rest of the image being pulled from the pre-swapped aligned image.</p>
			<ol>
				<li value="5">Next, we’ll put the face back in the original image:<pre class="source-code">
output_face = (output_face_tensor[0].permute(1,2,0).numpy() *
  255).astype(np.uint8)
output_image = cv2.warpAffine(output_face,
  face["warp_matrix"][:2], (height, width), output_image,
  borderMode = cv2.BORDER_TRANSPARENT,
  flags = cv2.WARP_INVERSE_MAP)</pre></li>
			</ol>
			<p>This code section completes the face loop. To do this, we apply the face back to the output image.</p>
			<p>First, we convert the face tensor back into a NumPy array that OpenCV can work with. To do this, we grab the first instance in the tensor; this effectively removes the batch size <a id="_idIndexMarker342"/>dimension. Then, we’ll use <code>permute</code> to move the channels back to the end of the matrix. We then have to multiply by 255 to get into the 0–255 range of an integer. Finally, we convert the variable into an integer, making it usable in OpenCV as a proper image.</p>
			<p>We then use OpenCV’s <code>cv2.warpAffine</code> with a couple of flags to copy the face back into the original image in its original orientation. The first flag we use is <code>cv2.BORDER_TRANSPARENT</code>, which makes it so that only the area of the smaller aligned face gets changed; the rest of the image is left as it was. Without that flag, the image would only include the replaced face square; the rest of the image would be black. The other flag we use is <code>cv2.WARP_INVERSE_MAP</code>, which tells <code>cv2.warpAffine</code> that we’re copying the image back into the original image instead of copying part of the original image out.</p>
			<p>With those two flags, the aligned image of the face gets put back into the correct place of the original full-sized image. We do this with a copy of the original image so we can copy multiple faces onto the image if multiple faces were found.</p>
			<ol>
				<li value="6">Finally, we output the new image with the faces swapped:<pre class="source-code">
output_image = cv2.cvtColor(output_image, cv2.COLOR_RGB2BGR)
  cv2.imwrite(os.path.join(opt.export_path,
    f"{filename}.png"), output_image)</pre></li>
			</ol>
			<p>The last step of the image loop is to write the images in memory to separate image files. To <a id="_idIndexMarker343"/>do this, we first convert the images back into the BGR color order that OpenCV expects. Then we write the file out to the extract path using the same original filename with a PNG file type.</p>
			<div><div><img src="img/B17535_07_003.jpg" alt="Figure 7.3 – Example of originals (top) and swaps (bottom) of Bryan (top left) and Matt (top right), the authors"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.3 – Example of originals (top) and swaps (bottom) of Bryan (top left) and Matt (top right), the authors</p>
			<p>Now that we’ve run <a id="_idIndexMarker344"/>the conversion on the frames, we need to turn the images back into a video. Let’s do that now.</p>
			<h1 id="_idParaDest-131"><a id="_idTextAnchor132"/>Creating the video from images</h1>
			<p>The conversion <a id="_idIndexMarker345"/>code included produces swapped images, but if we <a id="_idIndexMarker346"/>want to create a video, we’ll need to combine the output into a video file. There are multiple options here, depending on what you want to include:</p>
			<ul>
				<li>The following is for including just the images:<pre class="source-code">
<strong class="bold">ffmpeg -i {path_to_convert}\%05d.png Output.mp4</strong></pre></li>
			</ul>
			<p>This command line will convert all the frames into a video with some default options. The <code>Output.mp4</code> file will include the frames but won’t include any audio and will be at a default frame rate of 25 frames per second. This will be close enough to accurate for videos that came from film sources, such as Blu-rays or DVDs. If the video looks too fast or too slow, then your frame rate is incorrect, and you should look at the next option instead to match the correct frame rate.</p>
			<ul>
				<li>Including the images at a specific frame rate:<pre class="source-code">
<strong class="bold">ffmpeg -framerate {framerate} -i {path_to_convert}\%05d.png</strong>
<strong class="bold">  Output.mp4</strong></pre></li>
			</ul>
			<p>This command line will include the frames at a specific frame rate. The frame rate is something you’ll need to find yourself from your original video. One way to do it using <code>ffmpeg</code> is to run the following code:</p>
			<pre class="source-code">
<strong class="bold">ffmpeg -i {OriginalVideo.mp4}</strong></pre>
			<p>This will output a lot of information, most of which will not be useful to us. What we need to do is look for a line containing the “stream” information for the video. It will look something like this:</p>
			<pre class="source-code">
Stream #0:0(eng): Video: hevc (Main) (hvc1 / 0x31637668), yuvj420p(pc, bt470bg/bt470bg/smpte170m), 1920x1080, 13661 kb/s, SAR 1:1 DAR 16:9, 59.32 fps, 59.94 tbr, 90k tbn, 90k tbc (default)</pre>
			<p>The important information here is where it says <code>59.32 fps</code>. In this case, we’d want to put the <code>59.32</code> into the framerate of the <code>ffmpeg</code> command.</p>
			<p>This option still won’t include any audio.</p>
			<ul>
				<li>Including audio with the video:<pre class="source-code">
<strong class="bold">ffmpeg -i {path_to_convert}\%05d.png -i {OriginalVideo.mp4}</strong>
<strong class="bold">  -map 0:v:0 -map 1:a:0 Output.mp4</strong></pre></li>
			</ul>
			<p>This command <a id="_idIndexMarker347"/>will convert the video while also copying over <a id="_idIndexMarker348"/>audio from the original video file. It’s important to use the exact same file for the audio to line up. If the audio doesn’t line up correctly, you may want to double-check the frame rate and the number of frames.</p>
			<h1 id="_idParaDest-132"><a id="_idTextAnchor133"/>Summary</h1>
			<p>In this chapter, we ran the convert process on a folder full of images, replacing the faces using a trained model. We also turned the images back into a video, including changes to account for frame rate and copying audio.</p>
			<p>We started by going over how to prepare a video for conversion. The convert process requires data created by the extract process from <a href="B17535_05.xhtml#_idTextAnchor090"><em class="italic">Chapter 5</em></a>, <em class="italic">Extracting Faces</em>, and a trained AI model from <a href="B17535_06.xhtml#_idTextAnchor107"><em class="italic">Chapter 6</em></a>, <em class="italic">Training a Deepfake Model</em>. With all the parts from the previous chapters, we were ready to convert.</p>
			<p>We then walked through the code for the conversion process. This involved looking at the initialization, where we covered getting the Python script ready to operate. We then loaded the AI models and got them set up to work on a GPU if we have one. Next, we got the data ready for us to convert the faces in each frame. Finally, we ran two nested loops, which processed every face in every frame, swapping them to the other face. This part gave us a folder filled with swapped faces.</p>
			<p>After that, we looked at some commands that took the folder of swapped face images and returned them into a video form, this involved taking every frame into the video, ensuring that the frame rate was correct, and the audio was copied if desired.</p>
			<p>In the next section, we’ll start looking into the potential future of deepfakes, with the next chapter looking at applying the techniques we’ve learned about deepfakes to solve other problems.</p>
			<h1 id="_idParaDest-133"><a id="_idTextAnchor134"/>Exercises</h1>
			<ol>
				<li>We use the mask to cut out the swapped face from the rest of the image but then copy it over to the aligned face. This means that the areas of the aligned image that aren’t the face also get a lower resolution. One way to fix this would be to apply the mask to the original image instead of the aligned image. To do this, you’ll need to call <code>cv2.warpAffine</code> separately for the mask and the aligned image, then use the mask to get just the face copied over. You may want to view the documentation for OpenCV’s <code>warpAffine</code> at <a href="https://docs.opencv.org/3.4/d4/d61/tutorial_warp_affine.html">https://docs.opencv.org/3.4/d4/d61/tutorial_warp_affine.html</a>.</li>
			</ol>
			<p>Be sure to account for the fact that OpenCV’s documentation is based on the C++ implementation, and things can be a bit different in the Python library. The tutorial pages have a <strong class="bold">Python</strong> button that lets you switch the tutorial to using the Python libraries.</p>
			<ol>
				<li value="2">We rely on pre-extracted faces in order to convert. This is because a lot of the data is already processed in the extract process and is already available, allowing you to filter images/faces that you don’t want to be converted. But if you’re running a lot of videos or planning on running conversion on live video, it might make sense to allow conversion to run on the fly. To do this, you can combine the extract process with convert and run the extraction steps as needed before you convert. You can look at the code in <code>C5-extract.py</code> and add the appropriate parts to the convert process to enable it to work directly on the images.</li>
				<li>We operated the convert process entirely on images, but it’s actually possible for Python to work directly with video files. To do this, try installing and using a library such as PyAV from <a href="https://github.com/PyAV-Org/PyAV">https://github.com/PyAV-Org/PyAV</a> to read and write directly to video files instead of images. Remember that you may need to account for audio data and frame rate in the output.</li>
				<li>One problem with the techniques used in this chapter is that the swapped-in face can look pretty obvious at the edges. This is because of a lack of color matching and edge blending. Both these techniques can improve the swap’s edges. There are a lot of color-matching techniques available; one option is histogram matching (<a href="https://docs.opencv.org/3.4/d4/d1b/tutorial_histogram_equalization.html">https://docs.opencv.org/3.4/d4/d1b/tutorial_histogram_equalization.html</a>). You’ll need to match the RGB channels separately. Edge blending is usually done by blurring the mask; you can accomplish this by smoothing the mask image with OpenCV (<a href="https://docs.opencv.org/4.x/d4/d13/tutorial_py_filtering.html">https://docs.opencv.org/4.x/d4/d13/tutorial_py_filtering.html</a>). This can dull the sharp edges of the swap.</li>
				<li>The results from our AI here are limited to just 64x64 pixels. There are newer models that go higher but are still limited heavily by available GPU memory and can take a lot longer to train. To get around this, you could run the output through an AI upscaler, such as ESRGAN (<a href="https://github.com/xinntao/Real-ESRGAN">https://github.com/xinntao/Real-ESRGAN</a>), or a face-specific restoration tool, such as GFP-GAN (<a href="https://github.com/TencentARC/GFPGAN">https://github.com/TencentARC/GFPGAN</a>). See if you can run the model’s output through these before returning the face to the original image to get a higher-quality result.</li>
			</ol>
		</div>
		<div><div></div>
		</div>
	<div><p>EBSCOhost - printed on 11/27/2023 6:20 AM via . All use subject to <a target="_blank" href="https://www.ebsco.com/terms-of-use">https://www.ebsco.com/terms-of-use</a></p></div>


		<div><h1 id="_idParaDest-134"><a id="_idTextAnchor135"/>Part 3: Where to Now?</h1>
			<p>Like all inventions, the development of deepfakes is just the beginning. Now that you know how deepfakes work, where can you take that knowledge and what can you do with it? You might be surprised at how flexible the techniques can be.</p>
			<p>In this part, we’ll examine some hypothetical projects and how techniques in deepfakes could be used to make them easier, as well as solving complicated issues that might otherwise stump the average developer (you’re not one of those – after all, you bought this book!). Then, we’ll ask the ultimate question: what will the future bring? We’ll try to answer it by looking at where generative AI might go in the near future, including looking at the limitations and challenges that these AI technologies must overcome.</p>
			<p>This part comprises the following chapters:</p>
			<ul>
				<li><a href="B17535_08.xhtml#_idTextAnchor136"><em class="italic">Chapter 8</em></a>, <em class="italic">Applying the Lessons of Deepfakes</em></li>
				<li><a href="B17535_09.xhtml#_idTextAnchor152"><em class="italic">Chapter 9</em></a>, <em class="italic">The Future of Generative AI</em></li>
			</ul>
		</div>
		<div><div></div>
		</div>
	<div><p>EBSCOhost - printed on 11/27/2023 6:20 AM via . All use subject to <a target="_blank" href="https://www.ebsco.com/terms-of-use">https://www.ebsco.com/terms-of-use</a></p></div>
</body></html>