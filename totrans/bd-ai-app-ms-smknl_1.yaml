- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1'
- en: Introducing Microsoft Semantic Kernel
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍微软语义内核
- en: The **generative artificial intelligence** (**GenAI**) space is evolving quickly,
    with dozens of new products and services being launched weekly; it is becoming
    hard for developers to keep up with the ever-changing features and **application
    programming interfaces** (**APIs**) for each of the services. In this book, you
    will learn about **Microsoft Semantic Kernel**, an API that will make it a lot
    easier for you to use GenAI as a developer, making your code shorter, simpler,
    and more maintainable. Microsoft Semantic Kernel will allow you, as a developer,
    to use a single interface to connect with several different GenAI providers. Microsoft
    used Semantic Kernel to develop its copilots, such as Microsoft 365 Copilot.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**生成式人工智能**（**GenAI**）领域正在快速发展，每周都有数十种新产品和服务推出；对于开发者来说，跟上每个服务不断变化的特性和**应用程序编程接口**（**API**）变得越来越困难。在这本书中，你将了解**微软语义内核**，这是一个API，它将使你作为开发者使用GenAI变得更加容易，使你的代码更短、更简单、更易于维护。微软语义内核将允许你作为开发者使用一个单一接口连接到多个不同的GenAI提供商。微软使用语义内核开发了其协同飞行员，例如微软365协同飞行员。'
- en: Billions of people already use GenAI as consumers, and you are probably one
    of them. We will start this chapter by showing some examples of what you can do
    with GenAI as a consumer. Then, you will learn how you can start using GenAI as
    a developer to add AI services to your own applications.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 数亿人已经作为消费者使用GenAI，你可能就是其中之一。我们将通过展示一些你作为消费者可以使用GenAI的例子来开始本章。然后，你将学习如何作为开发者开始使用GenAI，将AI服务添加到你的应用程序中。
- en: In this chapter, you will learn the differences between using GenAI as a user
    and as a developer and how to create and run a simple end-to-end request with
    Microsoft Semantic Kernel. This will help you see how powerful and simple Semantic
    Kernel is and will serve as a framework for all further chapters. It will enable
    you to begin integrating AI into your own apps right away.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习使用GenAI作为用户和作为开发者的区别，以及如何使用微软语义内核创建和运行一个简单的端到端请求。这将帮助你看到语义内核是多么强大和简单，并将作为所有后续章节的框架。它将使你能够立即开始将AI集成到自己的应用程序中。
- en: 'In this chapter, we’ll be covering the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍以下主题：
- en: Understanding the basic use of a generative AI application like ChatGPT
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解像ChatGPT这样的生成式AI应用的基本用法
- en: Installing Microsoft Semantic Kernel
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装微软语义内核
- en: Configuring Semantic Kernel to interact with AI services
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置语义内核以与AI服务交互
- en: Running a simple task using Semantic Kernel
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用语义内核运行简单任务
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'To complete this chapter, you will need to have a recent, supported version
    of your preferred Python or C# development environment:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成本章，你需要拥有你首选的Python或C#开发环境的最新、受支持的版本：
- en: For Python, the minimum supported version is Python 3.10, and the recommended
    version is Python 3.11
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于Python，最低支持的版本是Python 3.10，推荐版本是Python 3.11
- en: For C#, the minimum supported version is .NET 8
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于C#，最低支持的版本是.NET 8
- en: Important note
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The examples are presented in C# and Python, and you can choose to only read
    the examples of your preferred language. Occasionally, a feature is available
    in only one of the languages. In such cases, we provide an alternative in the
    other language for how to achieve the same objectives.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 示例以C#和Python展示，你可以选择只阅读你偏好的语言的示例。偶尔，某个功能只在一个语言中可用。在这种情况下，我们提供另一种语言中的替代方案，以实现相同的目标。
- en: In this chapter, we will call OpenAI services. Given the amount that companies
    spend on training these large language models (LLMs), it’s no surprise that using
    these services is not free. You will need an **OpenAI API** key, obtained either
    directly through **OpenAI** or **Microsoft**, via the **Azure** **OpenAI** service.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将调用OpenAI服务。鉴于公司在训练这些大型语言模型（LLMs）上花费的金额，使用这些服务不是免费的也就不足为奇了。你需要一个**OpenAI
    API**密钥，可以通过**OpenAI**或**微软**直接获得，或者通过**Azure** **OpenAI**服务。
- en: 'Important: Using the OpenAI services is not free'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 重要：使用OpenAI服务不是免费的
- en: The examples that we will run throughout this book will call the OpenAI API.
    These calls require a paid subscription, and each call will incur a cost. The
    costs are usually small per request (for example, GPT-4 costs up to $0.12 per
    1,000 tokens), but they can add up. In addition, note that different models have
    different prices, with GPT-3.5 being 30 times less expensive per token than GPT-4.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本书将运行的示例将调用 OpenAI API。这些调用需要付费订阅，并且每次调用都会产生费用。通常，每个请求的费用很小（例如，GPT-4 每千个标记的费用高达
    0.12 美元），但它们可能会累积。此外，请注意，不同模型的价格不同，GPT-3.5 的每标记价格比 GPT-4 低 30 倍。
- en: 'OpenAI pricing information can be found here: [https://openai.com/pricing](https://openai.com/pricing)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 的定价信息可在此处查看：[https://openai.com/pricing](https://openai.com/pricing)
- en: 'Azure OpenAI pricing information can be found here: [https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/](https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Azure OpenAI 的定价信息可在此处查看：[https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/](https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/)
- en: If you use .NET, the code for this chapter is at [https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/dotnet/ch1](https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/dotnet/ch1).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用 .NET，本章的代码位于[https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/dotnet/ch1](https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/dotnet/ch1)。
- en: If you use Python, the code for this chapter is at [https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/python/ch1](https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/python/ch1).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用 Python，本章的代码位于[https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/python/ch1](https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/python/ch1)。
- en: 'You can install the required packages by going to the GitHub repository and
    using the following: `pip install -``r requirements.txt`.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过访问 GitHub 仓库并使用以下命令安装所需的软件包：`pip install -r requirements.txt`。
- en: Obtaining an OpenAI API key
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取 OpenAI API 密钥
- en: Go to the OpenAI Platform website ([https://platform.openai.com](https://platform.openai.com)).
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 访问 OpenAI 平台网站([https://platform.openai.com](https://platform.openai.com))。
- en: Sign up for a new account or sign in with an existing account. You can use your
    email or an existing Microsoft, Google, or Apple account.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注册新账户或使用现有账户登录。您可以使用电子邮件或现有的 Microsoft、Google 或 Apple 账户。
- en: On the left sidebar menu, select **API keys**.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在左侧侧边栏菜单中选择 **API 密钥**。
- en: On the **Project API keys** screen, click the button labeled **+ Create new
    secret key** (optionally, give it a name).
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 **项目 API 密钥** 界面中，点击标有 **+ 创建新的密钥** 的按钮（可选，为其命名）。
- en: Important
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 重要
- en: You have to copy and save the key immediately. It will disappear as soon as
    you click **Done**. If you didn’t copy the key or if you lost it, you need to
    generate a new one. There’s no cost to generate a new key. Remember to delete
    old keys.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 您必须立即复制并保存密钥。一旦点击 **完成**，它就会消失。如果您没有复制密钥或丢失了密钥，您需要生成一个新的。生成新密钥没有费用。请记住删除旧密钥。
- en: Obtaining an Azure OpenAI API key
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取 Azure OpenAI API 密钥
- en: Currently, you need to submit an application to obtain access to the Azure OpenAI
    Service. To apply for access, you need to complete a form at [https://aka.ms/oai/access](https://aka.ms/oai/access).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，您需要提交申请以获取访问 Azure OpenAI 服务的权限。要申请访问权限，您需要在[https://aka.ms/oai/access](https://aka.ms/oai/access)填写表格。
- en: The instructions to obtain an Azure OpenAI API key are available at [https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/create-resource](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/create-resource).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 获取 Azure OpenAI API 密钥的说明可在[https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/create-resource](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/create-resource)找到。
- en: Generative AI and how to use it
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成式 AI 及其使用方法
- en: Generative AI refers to a subset of artificial intelligence programs that are
    capable of creating content that is similar to what humans can produce. These
    systems use training from very large datasets to learn their patterns, styles,
    and structures. Then, they can generate entirely new content, such as synthesized
    images, music, and text.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式 AI 指的是一类人工智能程序，这些程序能够创建与人类生产的内容相似的内容。这些系统使用来自非常大数据集的训练来学习它们的模式、风格和结构。然后，它们可以生成全新的内容，例如合成的图像、音乐和文本。
- en: Using GenAI as consumer or end-user is very easy, and as a technical person,
    you probably have already done it. There are many consumer-facing AI products.
    The most famous is OpenAI’s **ChatGPT**, but there are many others that have hundreds
    of millions of users every day, such as Microsoft Copilot, Google Gemini (formerly
    Bard), and Midjourney. As of October 2023, Meta, the parent company of Facebook,
    WhatsApp, and Instagram, is making GenAI services available to all its users,
    increasing the number of GenAI daily users to billions.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 GenAI 作为消费者或最终用户非常简单，作为一个技术人员，你可能已经这样做了。有许多面向消费者的 AI 产品。最著名的是 OpenAI 的 **ChatGPT**，但还有许多其他产品，每天有数亿用户，例如
    Microsoft Copilot、Google Gemini（以前称为 Bard）和 Midjourney。截至 2023 年 10 月，Facebook、WhatsApp
    和 Instagram 的母公司 Meta 正在将其 GenAI 服务提供给所有用户，使 GenAI 的日用户数量增加到数十亿。
- en: While the concept of GenAI has existed for a while, it gained a lot of users
    with the release of OpenAI’s ChatGPT in November of 2022\. The initial version
    of ChatGPT was based on a model called **generative pre-trained transformer**
    (**GPT**) version 3.5\. That version was a lot better than earlier versions in
    the task of mimicking human-like writing. In addition, OpenAI made it easy to
    use by adding a chatbot-like interface and making it available to the general
    public. This interface is called ChatGPT. With ChatGPT, users can easily initiate
    tasks in their own words. At its launch, ChatGPT was the product with the fastest
    adoption rate in history.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 GenAI 的概念已经存在了一段时间，但它在 2022 年 11 月 OpenAI 发布 ChatGPT 后获得了大量用户。ChatGPT 的初始版本是基于名为
    **生成式预训练转换器**（**GPT**）的 3.5 版本。这个版本在模仿人类写作的任务上比早期版本要好得多。此外，OpenAI 通过添加类似聊天机器人的界面并使其对公众开放，使其易于使用。这个界面被称为
    ChatGPT。有了 ChatGPT，用户可以轻松地用自己的话启动任务。在发布时，ChatGPT 是历史上采用速度最快的产品。
- en: The GenAI concept was further popularized with the release of Midjourney, an
    application that allows for the generation of high-quality images from prompts
    submitted through Discord, a popular chat application, and Microsoft Copilot,
    a free web application that can generate text by using GPT-4 (the newest version
    of OpenAI’s GPT) and images by using an OpenAI model called DALL-E 3.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: GenAI 概念随着 Midjourney 的发布而进一步普及，Midjourney 是一个应用程序，允许用户通过 Discord（一个流行的聊天应用）提交提示来生成高质量的图像，以及
    Microsoft Copilot（一个免费的网络应用），它可以通过使用 OpenAI 的 GPT-4（OpenAI 的 GPT 最新版本）生成文本，并通过使用名为
    DALL-E 3 的 OpenAI 模型生成图像。
- en: In the upcoming subsections, we will discuss text and image generation using
    GenAI applications and explain the differences between generating them using applications
    such as ChatGPT and with an API as a developer.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的小节中，我们将讨论使用 GenAI 应用程序生成文本和图像，并解释使用 ChatGPT 等应用程序与作为开发人员使用 API 生成它们之间的区别。
- en: Text generation models
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本生成模型
- en: The initial use cases for GenAI were to generate text based on a simple instruction
    called a **prompt**.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: GenAI 的初始用例是根据一个简单的指令生成文本，这个指令被称为 **prompt**。
- en: The technology used behind most text-based GenAI products is called a **transformer**,
    and it was introduced in the paper *Attention is All you Need* [1] in 2017\. The
    transformer immensely improved the quality of the text being generated, and in
    just a few years, the text looked very similar to human-generated text. The transformer
    greatly improved the ability of AI to guess masked words in a phrase after being
    trained on a large number of documents (a **corpus**). Models trained on very
    large corpuses are called **large language** **models** (**LLMs**).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数基于文本的 GenAI 产品背后的技术被称为 **transformer**，它在 2017 年的论文 *Attention is All you
    Need* [1] 中被引入。transformer 极大地提高了生成文本的质量，仅在几年内，文本看起来就非常类似于人类生成的文本。transformer
    在训练了大量的文档（一个 **语料库**）后，极大地提高了 AI 在短语中猜测被掩盖的单词的能力。在非常大的语料库上训练的模型被称为 **大型语言模型**（**LLMs**）。
- en: If LLMs are given a phrase such as “*I went to the fast-food restaurant to <X>*,”
    they can generate good options for *X*, such as “*eat*.” Applying the transformer
    repeatedly can generate coherent phrases and even stories. The next iteration
    could be “*I went to the fast-food restaurant to eat <X>*,” returning “*a*,” and
    then “*I went to the fast-food restaurant to eat a <X>*,” could return “*burger*,”
    forming the full phrase “*I went to the fast-food restaurant to eat* *a burger*.”
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果给 LLMs 一个像“*我去快餐店去 <X>*”这样的短语，它们可以为 *X* 生成好的选项，例如“*吃*”。重复应用 transformer 可以生成连贯的短语甚至故事。下一个迭代可能是“*我去快餐店去吃
    <X>*”，返回“*a*”，然后“*我去快餐店去吃 a <X>*”，可能会返回“*burger*”，形成完整的短语“*我去快餐店去吃* *一个汉堡*”。
- en: The performance of an LLM model depends on the number of parameters, which is
    roughly proportional to how many comparisons a model can make at once, the context
    window, the maximum size of the text that can be handled at once, and the data
    used to train the model, which is usually kept secret by the companies that create
    LLMs.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 模型的性能取决于参数数量，这大致与模型一次可以进行的比较数量、上下文窗口、一次可以处理的文本最大尺寸以及用于训练模型的训练数据成正比，这些数据通常由创建
    LLM 的公司保密。
- en: 'The GPT is a model created by OpenAI that uses transformers and is good at
    generating text. There are many versions of GPT:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: GPT 是由 OpenAI 创建的一个使用 Transformer 的模型，擅长生成文本。GPT 有许多版本：
- en: GPT-1, launched in February 2018, had 120 million parameters and a **context
    window** of 512 tokens.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 2018 年 2 月发布的 GPT-1 拥有 1.2 亿个参数和一个 512 个标记的上下文窗口。
- en: GPT-2 was launched in February 2019, with the number of parameters increasing
    to 1.5 billion and the context window increasing to 1,024 tokens. Up to this point,
    while they sometimes produced interesting results, these models were mostly used
    by academics.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 2019 年 2 月发布的 GPT-2，参数数量增加到 15 亿，上下文窗口增加到 1024 个标记。到目前为止，尽管它们有时会产生有趣的结果，但这些模型主要被学者们使用。
- en: 'This changed with GPT-3, launched in June 2020, which had several sizes: small,
    medium, large, and extra-large. Extra-large had 175 billion parameters and a 2,048
    token context window. The generated text was, in most cases, hard to distinguish
    from human-generated text. OpenAI followed it with GPT-3.5, released in November
    2022, with still 175 billion parameters and a context window of 4,096 tokens (now
    expanded to 16,384 tokens), and launched a user interface named ChatGPT.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况在 2020 年 6 月发布的 GPT-3 中发生了变化，它有几个尺寸：小型、中型、大型和超大型。超大型拥有 1750 亿个参数和 2048 个标记的上下文窗口。生成的文本在大多数情况下难以与人类生成的文本区分开来。OpenAI
    随后发布了 GPT-3.5，于 2022 年 11 月发布，参数数量仍为 1750 亿，上下文窗口为 4096 个标记（现在已扩展到 16384 个标记），并推出了名为
    ChatGPT 的用户界面。
- en: ChatGPT is a web and mobile application that uses the GPT models in the background
    and allows users to submit prompts to the GPT models and get responses online.
    It was launched together with GPT-3.5, and at the time, it was the consumer product
    with the fastest adoption rate, reaching 100 million users in less than two months.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT 是一个使用后台 GPT 模型的网页和移动应用程序，允许用户向 GPT 模型提交提示并在线获取响应。它与 GPT-3.5 同时发布，当时是采用率最快的消费产品，不到两个月就达到了一亿用户。
- en: In February 2023, Microsoft released Bing Chat, which also uses OpenAI’s GPT
    models in the back end, further popularizing the usage of transformer models and
    AI. Recently, Microsoft renamed it to Microsoft Copilot.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 2023 年 2 月，微软发布了 Bing Chat，该产品也使用 OpenAI 的 GPT 模型作为后端，进一步普及了 Transformer 模型和
    AI 的使用。最近，微软将其更名为 Microsoft Copilot。
- en: Just a month later, in March 2023, OpenAI released the GPT-4 model, which was
    quickly incorporated into the backend of consumer products such as ChatGPT and
    Bing.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅一个月后，在 2023 年 3 月，OpenAI 发布了 GPT-4 模型，该模型很快就被集成到 ChatGPT 和 Bing 等消费产品的后端。
- en: Not all details about the GPT-4 model have been released to the public. It’s
    known that it has a context window of up to 32,768 tokens; however, its number
    of parameters has not been made public, but it has been estimated at 1.8 trillion.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 GPT-4 模型的所有细节尚未向公众发布。已知其上下文窗口可达 32,768 个标记；然而，其参数数量尚未公开，但据估计为 1.8 万亿。
- en: The GPT-4 model is notably good at human-like tasks related to text generation.
    A benchmark test shown in the GPT-4 technical report academic paper [2] shows
    the performance of GPT-3.5 and GPT-4 in taking exams. GPT-4 could pass many high-school
    and college level exams. You can read the paper at [https://doi.org/10.48550/arXiv.2303.08774](https://doi.org/10.48550/arXiv.2303.08774).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4 模型在涉及文本生成的类似人类任务方面表现突出。GPT-4 技术报告学术论文 [2] 中展示的基准测试显示了 GPT-3.5 和 GPT-4
    在考试中的表现。GPT-4 可以通过许多高中和大学水平的考试。您可以在 [https://doi.org/10.48550/arXiv.2303.08774](https://doi.org/10.48550/arXiv.2303.08774)
    阅读这篇论文。
- en: Understanding the difference between applications and models
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解应用和模型之间的区别
- en: Most people, including you, have likely used a GenAI application, such as ChatGPT,
    Microsoft Copilot, Bing Image Creator, Bard (now named Gemini), or Midjourney.
    These applications use GenAI models in their backend, but they also add a user
    interface and configurations that restrict and control the output of the models.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 包括你在内的大多数人可能都使用过GenAI应用程序，例如ChatGPT、Microsoft Copilot、Bing Image Creator、Bard（现在更名为Gemini）或Midjourney。这些应用程序在其后端使用GenAI模型，但它们也添加了用户界面和配置，这些配置限制了并控制了模型的输出。
- en: When you are developing your own application, you will need to do these things
    by yourself. You may not yet realize how much is carried out behind the scenes
    by applications such as Bing and ChatGPT.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在开发自己的应用程序时，你需要自己完成这些事情。你可能还没有意识到像Bing和ChatGPT这样的应用程序在幕后执行了多少工作。
- en: 'When you submit a prompt to an application, the application may add several
    additional instructions to the prompt you submitted. The most typical things added
    are instructions to restrict some types of output, for example: “*your reply should
    contain no curse words*.” For example, when you submit the prompt “*Tell me a
    joke*” to an application like ChatGPT, it may modify your prompt to “*Tell me
    a joke. Your reply should contain no curse words*” and pass that modified prompt
    to the model.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 当你向应用程序提交提示时，应用程序可能会在你提交的提示中添加几个额外的指令。最典型的是添加限制某些类型输出的指令，例如：“*你的回复不应包含脏话*。”例如，当你向ChatGPT这样的应用程序提交“*讲一个笑话*”的提示时，它可能会修改你的提示为“*讲一个笑话。你的回复不应包含脏话*”，并将这个修改后的提示传递给模型。
- en: The application may also add a summary of the questions that you have already
    submitted and the answers that were already given. For example, if you ask, “*How
    warm is Rio de Janeiro, Brazil, in the summer?*,” the answer may be, “*Rio de
    Janeiro is typically between 90 and 100 degrees Fahrenheit (30-40 degrees Celsius)
    in the summer*.” If you then ask the question, “*How long is the flight from New
    York to there?*,” an application such as ChatGPT will not submit “*How long is
    the flight from New York to there?”* directly to the model because the answer
    would be something like “*I don’t know what you mean* *by ‘there’*.”
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序还可能添加你已经提交的问题和已经给出的答案的摘要。例如，如果你问，“*巴西里约热内卢在夏天有多热？*”，答案可能是，“*里约热内卢在夏天通常在90到100华氏度（30-40摄氏度）之间*。”如果你接着问，“*从纽约到那里的航班有多长？*”，像ChatGPT这样的应用程序不会直接将“*从纽约到那里的航班有多长？*”提交给模型，因为答案可能类似于“*我不明白你说的‘那里’是什么意思*。”
- en: 'A straightforward way to address this problem is to save everything that the
    user entered and all the answers that were provided and re-submit them with every
    new prompt. For example, when the user submits “*How long is the flight from New
    York to there?*” after asking about the temperature, the application prepends
    the earlier questions and answers to the prompt, and what is actually submitted
    to the model is: “*How warm is Rio de Janeiro, Brazil, in the summer? Rio de Janeiro
    is typically between 90 and 100 degrees Fahrenheit (30-40 degrees Celsius) in
    the summer. How long is the flight from New York to there?*” Now, the model knows
    that “*there*” means “*Rio de Janeiro*,” and the answer will be something like
    “*Approximately* *10 hours*.”'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的直接方法是将用户输入的每一项内容以及提供的所有答案保存下来，并在每次新的提示中重新提交它们。例如，当用户在询问温度之后提交“*从纽约到那里的航班有多长？*”时，应用程序会在提示前添加之前的问题和答案，实际提交给模型的内容是：“*巴西里约热内卢在夏天有多热？里约热内卢在夏天通常在90到100华氏度（30-40摄氏度）之间。从纽约到那里的航班有多长？*”现在，模型知道“*那里*”指的是“*里约热内卢*”，答案可能类似于“*大约*
    *10小时*。”
- en: 'The consequence of appending all earlier prompts and responses to each new
    prompt is that it consumes a lot of space in the context window. Therefore, some
    techniques have been developed to compress the information that is added to the
    user prompt. The simplest technique is to keep only the earlier user questions,
    but not the answers given by the application. In that case, for example, the modified
    prompt would be something like “*Earlier I said: ‘How warm is Rio de Janeiro,
    Brazil, in the summer?, now answer only: “How long is the flight from New York
    to there?”*’. Note that the prompt needs to tell the model to respond only to
    the last question submitted by the user.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有之前的提示和响应附加到每个新提示的后果是，它会在上下文窗口中消耗大量空间。因此，已经开发了一些技术来压缩添加到用户提示中的信息。最简单的技术是只保留早期的用户问题，但不包括应用程序给出的答案。在这种情况下，例如，修改后的提示可能类似于“*之前我说过：‘巴西里约热内卢夏天有多热？’，现在只需回答：‘从纽约到那里的航班有多长？’*”。请注意，提示需要告诉模型只对用户提交的最后一个问题进行响应。
- en: Knowing that applications modify your prompts will be relevant for you if you
    test your prompts using consumer applications because the output you get from
    them can be substantially different from what you get when you use the model directly
    through an API, such as Microsoft Semantic Kernel. There’s usually no way to know
    how the applications are modifying your prompts, as the providers usually don’t
    reveal all their techniques.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用消费者应用程序测试您的提示，了解应用程序会修改您的提示将是有意义的，因为您从它们那里获得的输出可能与您直接通过API（如Microsoft Semantic
    Kernel）使用模型时获得的输出大不相同。通常无法知道应用程序是如何修改您的提示的，因为提供商通常不会透露所有他们的技术。
- en: Furthermore, a significant part of what you will do as an application developer
    will be to create prompt modifications that match your own application. Therefore,
    when your user submits their prompt, you will add your own prompt modifications
    to ensure they get an appropriate result. The techniques to modify user prompts
    are called **prompt engineering**, which we will explore briefly in the next chapter.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您作为应用程序开发者将要做的大部分工作将是创建与您自己的应用程序相匹配的提示修改。因此，当您的用户提交他们的提示时，您将添加自己的提示修改以确保他们获得适当的结果。修改用户提示的技术被称为**提示工程**，我们将在下一章中简要探讨。
- en: Generating text using consumer applications
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用消费者应用程序生成文本
- en: 'Let’s explore the two most popular text generation applications: ChatGPT and
    Microsoft Copilot. If you are not familiar with the power of GenAI, trying them
    out will give you a sense of what can be done with them and how powerful they
    are. We will also briefly talk about their configuration parameters and their
    architecture, which can help you decide the appropriate architecture for your
    own applications later.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索两个最受欢迎的文本生成应用程序：ChatGPT和Microsoft Copilot。如果您不熟悉GenAI的力量，尝试它们将让您了解它们可以做什么以及它们的强大之处。我们还将简要讨论它们的配置参数和架构，这可以帮助您决定自己应用程序的适当架构。
- en: OpenAI ChatGPT
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: OpenAI ChatGPT
- en: If you have never used a GenAI app, ChatGPT is likely the best place to start.
    ChatGPT’s default backend model is GPT 3.5, a fast and very powerful model. ChatGPT
    is free to use when OpenAI has available capacity on their servers. You can also
    buy a subscription to ChatGPT Plus for $20 per month, and that will give you the
    ability to use their most powerful model (currently GPT-4) and ensure you will
    always have capacity.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您从未使用过GenAI应用程序，ChatGPT可能是开始的最佳选择。ChatGPT的默认后端模型是GPT 3.5，这是一个快速且非常强大的模型。当OpenAI的服务器上有可用容量时，ChatGPT是免费的。您还可以每月支付20美元购买ChatGPT
    Plus订阅，这将使您能够使用他们最强大的模型（目前是GPT-4），并确保您始终有容量。
- en: To use ChatGPT, go to [https://chat.openai.com](https://chat.openai.com).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用ChatGPT，请访问[https://chat.openai.com](https://chat.openai.com)。
- en: 'The ChatGPT interface is very simple. It allows you to choose the backend model
    on top, some suggestions of prompts in the middle, and a text box to enter prompts
    in the bottom. It also includes a notice that the output produced may include
    incorrect information:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT的界面非常简单。它允许你在顶部选择后端模型，中间提供一些提示建议，底部有一个文本框用于输入提示。它还包括一个通知，指出生成的输出可能包含错误信息：
- en: '![Figure 1.1 – Submitting requests to ChatGPT using the web interface](img/B21826_01_1.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图1.1 – 使用Web界面向ChatGPT提交请求](img/B21826_01_1.jpg)'
- en: Figure 1.1 – Submitting requests to ChatGPT using the web interface
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1 – 使用Web界面向ChatGPT提交请求
- en: 'I will submit the following prompt in the **Send a** **Message** textbox:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我将在**发送消息**文本框中提交以下提示：
- en: '`How long is the flight between New York City and Rio` `de Janeiro?`'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '`纽约市和里约热内卢之间的航班有多长？`'
- en: 'ChatGPT, using the GPT-3.5 model, provides the following answer:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 使用GPT-3.5模型的ChatGPT提供了以下答案：
- en: '[PRE0]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Microsoft Copilot
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 微软Copilot
- en: Another free alternative is Microsoft Copilot, formerly Bing Chat. It is available
    from the [www.bing.com](http://www.bing.com) page, but it can be accessed directly
    from [https://www.bing.com/chat](https://www.bing.com/chat).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个免费的选择是微软Copilot，之前称为Bing Chat。它可以从[www.bing.com](http://www.bing.com)页面访问，但也可以直接从[https://www.bing.com/chat](https://www.bing.com/chat)访问。
- en: The user interface for Microsoft Copilot is like the interface of ChatGPT. It
    has some suggestions for prompts in the middle of the screen and a text box, where
    the user can enter a prompt at the bottom. The Microsoft Copilot UI also shows
    a couple of options that will be relevant from when we use models programmatically.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 微软Copilot的用户界面类似于ChatGPT的界面。屏幕中间有一些提示建议，以及一个文本框，用户可以在底部输入提示。微软Copilot的UI还显示了一些在使用模型编程时相关的选项。
- en: The first is the conversation style. Copilot offers the option of being More
    Creative, More Balanced, or More Precise. This is related to the temperature parameter
    that will be passed to the underlying model. We will talk about the temperature
    parameter in [*Chapter 3*](B21826_03.xhtml#_idTextAnchor071), but in short, the
    temperature parameter determines how common the words chosen by the LLM are.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 第一是对话风格。Copilot提供了更创意、更平衡或更精确的选项。这与将传递给底层模型的温度参数相关。我们将在[*第3章*](B21826_03.xhtml#_idTextAnchor071)中讨论温度参数，但简而言之，温度参数决定了LLM选择的单词有多常见。
- en: Parameters in Microsoft Copilot
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 微软Copilot的参数
- en: While Microsoft Copilot does not reveal the exact configuration values, `0`
    and `0.2`), resulting in very safe guesses for the next word. For `0.4` and `0.6`),
    resulting *mostly* in safe guesses, but with the occasional guess being rare.
    `0.8`. Most guesses will still be safe, but more guesses will be rare words. Since
    LLMs guess words of a phrase in sequence, previous guesses influence subsequent
    guesses. When generating a phrase, each rare word makes the whole phrase more
    unusual.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然微软Copilot没有透露确切的配置值（例如`0`和`0.2`），这导致对下一个词的猜测非常安全。对于`0.4`和`0.6`），结果*主要*是安全的猜测，但偶尔会有一些罕见的猜测。`0.8`。大多数猜测仍然会是安全的，但会有更多罕见的单词。由于LLM按顺序猜测短语中的单词，先前的猜测会影响后续的猜测。在生成短语时，每个罕见的单词都会使整个短语更加不寻常。
- en: Another interesting component in the UI is that the bottom right of the text
    box shows how many characters have been entered, giving you an idea of how much
    you will consume of the underlying model’s context window. Note that you cannot
    know for sure how much you will consume because the Copilot application will modify
    your prompt.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: UI中另一个有趣的组件是文本框的右下角显示了已经输入了多少个字符，这让你对将消耗多少底层模型的上下文窗口有一个大致的了解。请注意，你无法确切知道你会消耗多少，因为Copilot应用程序会修改你的提示。
- en: '![Figure 1.2 – Microsoft Copilot user interface](img/B21826_01_2.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图1.2 – 微软Copilot用户界面](img/B21826_01_2.jpg)'
- en: Figure 1.2 – Microsoft Copilot user interface
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2 – 微软Copilot用户界面
- en: 'On August 11, 2023, Mikhail Parakhin, Bing’s former CEO, posted on X/Twitter
    that Microsoft Copilot outperforms GPT-4 because it uses **retrieval augmented**
    **inference** ([https://x.com/MParakhin/status/1689824478602424320?s=20](https://x.com/MParakhin/status/1689824478602424320?s=20)):'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 2023年8月11日，必应前CEO米哈伊尔·帕拉欣在X/Twitter上发帖称，微软Copilot的表现优于GPT-4，因为它使用了**检索增强**的**推理**([https://x.com/MParakhin/status/1689824478602424320?s=20](https://x.com/MParakhin/status/1689824478602424320?s=20))：
- en: '![Figure 1.3 – Post by Bing’s former CEO about Microsoft Copilot using RAG](img/B21826_01_3.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图1.3 – 必应前CEO关于微软Copilot使用RAG的帖子](img/B21826_01_3.jpg)'
- en: Figure 1.3 – Post by Bing’s former CEO about Microsoft Copilot using RAG
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3 – 必应前CEO关于微软Copilot使用RAG的帖子
- en: We will talk more about retrieval augmented inference in *Chapters 6* and *7*,
    but for our current purposes, this means that Microsoft Copilot does not directly
    submit your prompt to the model. Bing has not publicly released the details of
    their architecture yet, but it is likely that Bing modifies your prompt (it shows
    the modified prompt in the UI, under **Searching for**), makes a regular Bing
    query using the modified prompt, gathers the results of that Bing query, concatenates
    them, and submits the concatenated results as a large prompt to a GPT model, asking
    it to combine the results to output a coherent answer.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第6章和第7章中更详细地讨论检索增强推理，但就我们当前的目的而言，这意味着微软Co-Pilot不会直接将您的提示提交给模型。必应尚未公开其架构的细节，但很可能是必应修改了您的提示（在UI中的**搜索中**显示了修改后的提示），使用修改后的提示进行常规的必应查询，收集该查询的结果，将它们连接起来，并将连接的结果作为大提示提交给GPT模型，要求它将结果组合起来输出一个连贯的答案。
- en: 'Using retrieval augmentation allows Bing to add citations and advertisements
    more easily. In the figure below, note that my prompt `How long is the flight
    between New York City and Rio de Janeiro?` was modified by Copilot to `Searching
    for flight duration New York City Rio` `de Janeiro`:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 使用检索增强允许必应更容易地添加引用和广告。在下图中，请注意我的提示`How long is the flight between New York City
    and Rio de Janeiro?`被Co-Pilot修改为`Searching for flight duration New York City Rio
    de Janeiro`：
- en: '![Figure 1.4 – An example of using Microsoft Copilot](img/B21826_01_4.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图1.4 – 使用微软Co-Pilot的示例](img/B21826_01_4.jpg)'
- en: Figure 1.4 – An example of using Microsoft Copilot
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.4 – 使用微软Co-Pilot的示例
- en: As you can see, you can use consumer applications such as ChatGPT and Microsoft
    Copilot to familiarize yourself with how LLMs work for GenAI and to do some initial
    testing of your prompts, but be aware that the prompt you submit may be heavily
    modified by the application, and the response that you get from the underlying
    model can be very different from the responses that you will get when you actually
    create your own application.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，您可以使用ChatGPT和Microsoft Copilot等消费级应用程序来熟悉LLMs在GenAI中的应用，并对您的提示进行一些初步测试，但请注意，您提交的提示可能会被应用程序大量修改，并且从底层模型获得的响应可能与您实际创建自己的应用程序时获得的响应非常不同。
- en: Generating images
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成图像
- en: Besides generating text, AI can also be used to generate images from text prompts.
    The details of the process used to generate images from prompts are outside the
    scope of this book, but we will provide a quick summary. The main models in the
    image generation space are Midjourney, which is available through the Midjourney
    Bot in Discord; the open-source Stable Diffusion, which is also used by OpenAI’s
    DALL-E 2; and the DALL-E 3, released in October 2023 and available through the
    Bing Chat (now Microsoft Copilot) and ChatGPT applications.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 除了生成文本，AI还可以根据文本提示生成图像。用于从提示生成图像的过程细节超出了本书的范围，但我们将提供一个简要概述。图像生成领域的主要模型包括Midjourney，它可通过Discord中的Midjourney机器人访问；开源的Stable
    Diffusion，它也被OpenAI的DALL-E 2使用；以及2023年10月发布的DALL-E 3，可通过Bing Chat（现在称为Microsoft
    Copilot）和ChatGPT应用程序访问。
- en: At the time of writing, Microsoft Semantic Kernel only supports DALL-E; therefore,
    this is the example we are going to explore. DALL-E 3 is available for free through
    the Microsoft Copilot application, with some limitations.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，微软语义内核仅支持DALL-E；因此，这是我们将要探讨的示例。DALL-E 3可通过Microsoft Copilot应用程序免费使用，但有一些限制。
- en: 'If you are using the Microsoft Copilot application from the earlier example,
    make sure to reset your chat history by clicking on the **New Topic** button to
    the left of the text box. To generate images, make sure your conversation style
    is set to **More Creative**, as image generation only works in Creative mode:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在使用前面示例中的Microsoft Copilot应用程序，请确保通过点击文本框左侧的**新主题**按钮重置您的聊天历史。要生成图像，请确保您的对话风格设置为**更富有创意**，因为图像生成仅在创意模式下工作：
- en: '![Figure 1.5 – Choosing the conversation style](img/B21826_01_5.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图1.5 – 选择对话风格](img/B21826_01_5.jpg)'
- en: Figure 1.5 – Choosing the conversation style
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.5 – 选择对话风格
- en: 'I will use the following prompt:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我将使用以下提示：
- en: '`create a photorealistic image of a salt-and-pepper standard schnauzer on a
    street corner holding a sign "Will do tricks` `for cheese."`'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '`create a photorealistic image of a salt-and-pepper standard schnauzer on a
    street corner holding a sign "Will do tricks for cheese."`'
- en: 'Microsoft Copilot will call DALL-E 3 and generate four images based on my specification:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 微软Co-Pilot将调用DALL-E 3并根据我的要求生成四幅图像：
- en: '![Figure 1.6 – Images generated by Microsoft Copilot](img/B21826_01_6.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图1.6 – 由Microsoft Copilot生成的图像](img/B21826_01_6.jpg)'
- en: Figure 1.6 – Images generated by Microsoft Copilot
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.6 – 由Microsoft Copilot生成的图像
- en: One way in which DALL-E 3 is better than other image-generating models is that
    it can correctly add text to images. Earlier versions of DALL-E and most other
    models cannot spell words properly.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: DALL-E 3比其他图像生成模型更好的一个方面是它可以正确地将文本添加到图像中。DALL-E的早期版本和大多数其他模型都不能正确拼写单词。
- en: 'The images are presented in a grid with a total resolution of 1024 x 1024 pixels
    (512 x 512 per image). If you select one image, that specific image will be upscaled
    to the 1024 x 1024 pixel resolution. In my case, I will select the image in the
    bottom left corner. You can see the final result in the next figure:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图片以网格形式展示，总分辨率为1024 x 1024像素（每张图片为512 x 512像素）。如果您选择一张图片，该图片将被放大到1024 x 1024像素的分辨率。在我的情况下，我将选择左下角的图片。您可以在下一张图中看到最终结果：
- en: '![Figure 1.7 – High-resolution image generated by Microsoft Copilot](img/B21826_01_7.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图1.7 – 由Microsoft Copilot生成的高分辨率图像](img/B21826_01_7.jpg)'
- en: Figure 1.7 – High-resolution image generated by Microsoft Copilot
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.7 – 由Microsoft Copilot生成的高分辨率图像
- en: As you can see, GenAI can also be used to generate images and now you have an
    idea of how powerful it can be. We will explore generating images with Microsoft
    Semantic Kernel in [*Chapter 4*](B21826_04.xhtml#_idTextAnchor086). There is a
    lot to explore before we get there, and we will start with a quick, comprehensive
    tour of Microsoft Semantic Kernel.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，生成式AI也可以用来生成图像，现在您已经了解到它有多么强大。我们将在[*第4章*](B21826_04.xhtml#_idTextAnchor086)中探讨如何使用Microsoft语义内核生成图像。在达到那里之前，还有很多东西可以探索，我们将从对Microsoft语义内核的快速全面浏览开始。
- en: Microsoft Semantic Kernel
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Microsoft语义内核
- en: Microsoft Semantic Kernel ([https://github.com/microsoft/semantic-kernel](https://github.com/microsoft/semantic-kernel))
    is a thin, open source **software development toolkit** (**SDK**) that makes it
    easier for applications developed in C# and Python to interact with AI services
    such as the ones made available through OpenAI, Azure OpenAI, and Hugging Face.
    Semantic Kernel can receive requests from your application and route them to different
    AI services. Furthermore, if you extend the functionality of Semantic Kernel by
    adding your own functions, which we will explore in [*Chapter 3*](B21826_03.xhtml#_idTextAnchor071),
    Semantic Kernel can automatically discover which functions need to be used, and
    in which order, to fulfill a request. The request can come directly from the user
    and be passed through directly from your application, or your application can
    modify and enrich the user request before sending it to Semantic Kernel.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Microsoft语义内核([https://github.com/microsoft/semantic-kernel](https://github.com/microsoft/semantic-kernel))是一个轻量级的开源**软件开发工具包**（SDK），它使得使用C#和Python开发的应用程序与AI服务（如通过OpenAI、Azure
    OpenAI和Hugging Face提供的AI服务）交互变得更加容易。语义内核可以接收来自您应用程序的请求并将它们路由到不同的AI服务。此外，如果您通过添加自己的函数扩展了语义内核的功能，我们将在[*第3章*](B21826_03.xhtml#_idTextAnchor071)中探讨这一点，语义内核可以自动发现哪些函数需要使用以及使用顺序，以满足请求。请求可以直接来自用户并通过您的应用程序直接传递，或者您的应用程序可以在将其发送到语义内核之前修改和丰富用户请求。
- en: It was originally designed to power different versions of Microsoft Copilot,
    such as Microsoft 365 Copilot and the Bing Copilot, and then be released to the
    developer community as an open source package. Developers can use Semantic Kernel
    to create plugins that execute complex actions using AI services and combine these
    plugins using just a few lines of code.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 它最初是为了为不同版本的Microsoft Copilot提供动力，例如Microsoft 365 Copilot和Bing Copilot，然后作为开源软件包发布给开发者社区。开发者可以使用语义内核创建插件，这些插件可以使用AI服务执行复杂操作，并且只需几行代码就可以组合这些插件。
- en: 'In addition, Semantic Kernel can automatically orchestrate different plugins
    by using a **planner**. With the planner, a user can ask your application to achieve
    a complex goal. For example, if you have a function that identifies which animal
    is in a picture and another function that tells knock-knock jokes, your user can
    say, “*Tell me a knock-knock joke about the animal in the picture in this URL*,”
    and the planner will automatically understand that it needs to call the identification
    function first and the “tell joke” function after it. Semantic Kernel will automatically
    search and combine your plugins to achieve that goal and create a plan. Then,
    Semantic Kernel will execute that plan and provide a response to the user:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Semantic Kernel 可以通过使用 **规划器** 自动编排不同的插件。使用规划器，用户可以让您的应用程序实现一个复杂的目标。例如，如果您有一个识别图片中是哪种动物的功能，以及另一个讲打趣笑话的功能，您的用户可以说，“*告诉我一个关于这个
    URL 中图片中动物的打趣笑话*”，规划器将自动理解它需要首先调用识别功能，然后是“讲笑话”功能。Semantic Kernel 将自动搜索和组合您的插件以实现该目标并创建一个计划。然后，Semantic
    Kernel 将执行该计划并向用户提供响应：
- en: '![Figure 1.8 – Structure of Microsoft Semantic Kernel](img/B21826_01_8.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.8 – Microsoft Semantic Kernel 的结构](img/B21826_01_8.jpg)'
- en: Figure 1.8 – Structure of Microsoft Semantic Kernel
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.8 – Microsoft Semantic Kernel 的结构
- en: In the upcoming sections, we will do a quick end-to-end tour of Semantic Kernel,
    going through most of the steps in *Figure 1**.8*. We will send a request, create
    a plan, call the API, and call a native function and a semantic function. These,
    combined, will generate an answer to the user. First, we will do this manually,
    step-by-step, and then we will do everything in one go using the planner. You
    will see how powerful Semantic Kernel can be with just a little code.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将快速浏览 Semantic Kernel 的端到端流程，包括 *图 1**.8* 中的大多数步骤。我们将发送请求，创建计划，调用
    API，调用本地函数和语义函数。这些组合起来将为用户提供答案。首先，我们将手动逐步完成这个过程，然后我们将使用规划器一次性完成所有操作。您将看到 Semantic
    Kernel 只需少量代码就能多么强大。
- en: Before we start experimenting with Microsoft Semantic Kernel, we need to install
    it.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始使用 Microsoft Semantic Kernel 进行实验之前，我们需要安装它。
- en: Installing the Microsoft Semantic Kernel package
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装 Microsoft Semantic Kernel 包
- en: To use Microsoft Semantic Kernel, you must install it in your environment. Please
    note that Microsoft Semantic Kernel is still in active development, and there
    may be differences between versions.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 Microsoft Semantic Kernel，您必须在您的环境中安装它。请注意，Microsoft Semantic Kernel 仍在积极开发中，不同版本之间可能存在差异。
- en: Installing Microsoft Semantic Kernel in Python
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 Python 中安装 Microsoft Semantic Kernel
- en: 'To install Microsoft Semantic Kernel in Python, start in a new directory and
    follow these steps:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 Python 中安装 Microsoft Semantic Kernel，请在一个新目录中开始，并按照以下步骤操作：
- en: 'Create a new virtual environment with `venv`:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `venv` 创建一个新的虚拟环境：
- en: '[PRE1]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Activate the new environment you just created. This will ensure that Microsoft
    Semantic Kernel will be installed only for this directory:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 激活您刚刚创建的新环境。这将确保 Microsoft Semantic Kernel 只为此目录安装：
- en: 'In PowerShell, use the following:'
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 PowerShell 中，使用以下命令：
- en: '[PRE2]'
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: pip install semantic-kernel
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: pip install semantic-kernel
- en: '[PRE3]'
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Installing Microsoft Semantic Kernel in C#
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 C# 中安装 Microsoft Semantic Kernel
- en: 'To install Microsoft Semantic Kernel in C#, follow these steps:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 C# 中安装 Microsoft Semantic Kernel，请按照以下步骤操作：
- en: 'Create a new project targeting .NET 8:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个以 .NET 8 为目标的新项目：
- en: '[PRE4]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Change into the application directory:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 切换到应用程序目录：
- en: '[PRE5]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: dotnet add package Microsoft.SemanticKernel --prerelease
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: dotnet add package Microsoft.SemanticKernel --prerelease
- en: '[PRE6]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Run the program with the following simple instructions just to make sure that
    the installation succeeded:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下简单指令运行程序以确保安装成功：
- en: 'Instantiating the kernel in Python:'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Python 中实例化内核：
- en: '[PRE7]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Instantiating the kernel in C#:'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 C# 中实例化内核：
- en: '[PRE8]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We have now installed Semantic Kernel in your environment, and now we’re ready
    to connect it to AI services and start using them.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将 Semantic Kernel 安装到您的环境中，现在我们准备将其连接到 AI 服务并开始使用它们。
- en: Using Semantic Kernel to connect to AI services
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Semantic Kernel 连接到 AI 服务
- en: To complete this section, you must have an API key. The process to obtain an
    API key was described at the beginning of this chapter.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成本节，您必须有一个 API 密钥。获取 API 密钥的过程在本章开头已描述。
- en: In the upcoming subsections, we are only going to connect to the OpenAI text
    models GPT-3.5 and GPT-4\. If you have access to the OpenAI models through Azure,
    you will need to make minor modifications to your code.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的子章节中，我们只将连接到 OpenAI 的文本模型 GPT-3.5 和 GPT-4。如果您通过 Azure 访问 OpenAI 模型，您需要对您的代码进行一些小的修改。
- en: 'Although it would be simpler to connect to a single model, we are already going
    to show a simple but powerful Microsoft Semantic Kernel feature: we’re going to
    connect to two different models and run a simple prompt using the simpler but
    less expensive model, GPT-3.5, and a more complex prompt on the more advanced
    but also more expensive model, GPT-4.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然连接到单个模型会更简单，但我们已经展示了简单但强大的 Microsoft Semantic Kernel 功能：我们将连接到两个不同的模型，并使用更简单但成本更低的模型
    GPT-3.5 运行一个简单的提示，并在更先进但成本更高的模型 GPT-4 上运行一个更复杂的提示。
- en: This process of sending simpler requests to simpler models and more complex
    requests to more complex models is something that you will frequently do when
    creating your own applications. This approach is called **LLM cascade**, and it
    was popularized in the FrugalGPT [3] paper. It can result in substantial cost
    savings.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 向更简单的模型发送更简单的请求，向更复杂的模型发送更复杂的请求，这是您在创建自己的应用程序时经常会做的事情。这种方法被称为 **LLM 级联**，它在
    FrugalGPT [3] 论文中被普及。它可以带来实质性的成本节约。
- en: 'Important: Order matters'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 重要：顺序很重要
- en: The order in which you load your services matters. Both for Python (*step 3*
    in the *Connecting to OpenAI Services using Python* section) and for C# (*step
    4* in the *Connecting to OpenAI Services using C#* section), we are going to first
    load the GPT-3.5 model into the kernel, followed by the GPT-4 model. This will
    make GPT-3.5 the default model. Later, we will specify which model will be used
    for which command; if we don’t, GPT-3.5 will be used. If you load GPT-4 first,
    you will incur more costs.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 加载您的服务的顺序很重要。对于 Python（*在 *使用 Python 连接到 OpenAI 服务* 部分的 *步骤 3* 中）和 C#（*在 *使用
    C# 连接到 OpenAI 服务* 部分的 *步骤 4* 中），我们首先将 GPT-3.5 模型加载到内核中，然后加载 GPT-4 模型。这将使 GPT-3.5
    成为默认模型。稍后，我们将指定哪个模型将用于哪个命令；如果没有指定，则使用 GPT-3.5。如果您首先加载 GPT-4，您将承担更多费用。
- en: We assume that you are using the OpenAI service instead of the Azure OpenAI
    service. You will need your OpenAI key and the organization ID, which can be found
    under **Settings** in the left menu of [https://platform.openai.com/account/api-keys](https://platform.openai.com/account/api-keys).
    All examples work with Azure OpenAI; you just need to use the Azure connection
    information instead of the OpenAI connection information.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设您正在使用 OpenAI 服务而不是 Azure OpenAI 服务。您将需要您的 OpenAI 密钥和组织 ID，这些可以在 [https://platform.openai.com/account/api-keys](https://platform.openai.com/account/api-keys)
    的左侧菜单下的 **设置** 中找到。所有示例都适用于 Azure OpenAI；您只需使用 Azure 连接信息而不是 OpenAI 连接信息。
- en: Connecting to OpenAI Services using Python
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Python 连接到 OpenAI 服务
- en: This section assumes that you are using the OpenAI service. Before connecting
    to the OpenAI service, create a .env file in the ch1 directory containing your
    OpenAI key and your OpenAI organization ID. The organization ID can be found under
    Settings in the left menu of [https://platform.openai.com/account/api-keys](https://platform.openai.com/account/api-keys).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 本节假设您正在使用 OpenAI 服务。在连接到 OpenAI 服务之前，在 ch1 目录下创建一个 .env 文件，其中包含您的 OpenAI 密钥和您的
    OpenAI 组织 ID。组织 ID 可以在 [https://platform.openai.com/account/api-keys](https://platform.openai.com/account/api-keys)
    的左侧菜单下的 **设置** 中找到。
- en: 'Your .env file should look like this, with the appropriate values replacing
    x in the following example:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 您的 .env 文件应如下所示，以下示例中的 x 应由适当的值替换：
- en: '[PRE9]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'To connect to the OpenAI service using Python, perform the following steps:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 Python 连接到 OpenAI 服务，请执行以下步骤：
- en: 'Load an empty kernel:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载一个空内核：
- en: '[PRE10]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Load your API key and Organization ID into variables using the `openai_settings_from_dot_env`
    method from the `semantic_kernel_utils.settings` package:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `semantic_kernel_utils.settings` 包中的 `openai_settings_from_dot_env` 方法将您的
    API 密钥和组织 ID 加载到变量中：
- en: '[PRE11]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Use the `OpenAIChatCompletion` method to create connections to chat services:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `OpenAIChatCompletion` 方法创建到聊天服务的连接：
- en: '[PRE12]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'If you are using OpenAI through Azure, instead of using `OpenAIChatCompletion`,
    you need to use `AzureOpenAIChatCompletion`, as shown here:'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果您通过 Azure 使用 OpenAI，而不是使用 `OpenAIChatCompletion`，则需要使用 `AzureOpenAIChatCompletion`，如下所示：
- en: '[PRE13]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Your Semantic Kernel is now ready to make calls, which we will do in the *Running
    a simple* *prompt* section.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 您的 Semantic Kernel 现在已准备好进行调用，我们将在 *运行简单提示* 部分中这样做。
- en: Connecting to OpenAI services using C#
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 C# 连接到 OpenAI 服务
- en: Before connecting to the OpenAI service, create a `config.json` file in the
    `ch1/config` directory, containing your OpenAI key and your OpenAI organization
    ID.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在连接到 OpenAI 服务之前，在 `ch1/config` 目录下创建一个 `config.json` 文件，其中包含您的 OpenAI 密钥和您的
    OpenAI 组织 ID。
- en: 'To avoid keeping a key in your code, we will load your keys from a configuration
    file. Your `config/settings.json` file should look like the following example,
    with the appropriate values in the `apiKey` and `orgId` fields (`orgId` is optional.
    If you don’t have `orgId`, delete the field. An empty string will not work):'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免在代码中保留密钥，我们将从配置文件中加载您的密钥。您的`config/settings.json`文件应类似于以下示例，其中`apiKey`和`orgId`字段包含适当的值（`orgId`是可选的。如果您没有`orgId`，请删除该字段。空字符串不起作用）：
- en: '[PRE14]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'To connect to the OpenAI service in C#, perform the following steps:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 要在C#中连接到OpenAI服务，请执行以下步骤：
- en: 'Since we’re going to reuse the API Key and Organization ID a lot, create a
    class to load them in `Settings.cs`:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们将多次重用API密钥和组织ID，我们在`Settings.cs`中创建一个类来加载它们：
- en: '[PRE15]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The preceding code is boilerplate code to read a JSON file and load its attributes
    into C# variables. We’re looking for two attributes: `apiKey` and `orgID`.'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码是读取JSON文件并将其属性加载到C#变量中的样板代码。我们正在寻找两个属性：`apiKey`和`orgID`。
- en: 'Load the settings from `config/settings.json`. We’re going to create a class
    that will make this easier, as we are going to be doing this a lot. The class
    is very simple. It first checks whether the configuration file exists, and if
    it does, the class uses the JSON deserializer to load its contents into the `apiKey`
    and `orgId` variables:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`config/settings.json`加载设置。我们将创建一个类来简化这个过程，因为我们将会经常这样做。该类非常简单。它首先检查配置文件是否存在，如果存在，则类使用JSON反序列化器将内容加载到`apiKey`和`orgId`变量中：
- en: '[PRE16]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Next, use the `OpenAIChatCompletion` method to create connections to chat services.
    Note that we’re using `serviceID` to have a shortcut name for the model.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用`OpenAIChatCompletion`方法创建到聊天服务的连接。请注意，我们使用`serviceID`来为模型提供一个快捷名称。
- en: 'After you load the components, build the kernel with the `Build` method:'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在您加载组件后，使用`Build`方法构建内核：
- en: '[PRE17]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'If you are using OpenAI through Azure, instead of using `AddOpenAIChatCompletion`,
    you need to use `AddAzureOpenAIChatCompletion`, as shown here:'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果您通过Azure使用OpenAI，而不是使用`AddOpenAIChatCompletion`，则需要使用`AddAzureOpenAIChatCompletion`，如下所示：
- en: '[PRE18]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Your Semantic Kernel is now ready to make calls, which we will do in the next
    section.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 您的Semantic Kernel现在已准备好进行调用，我们将在下一节中这样做。
- en: Running a simple prompt
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行一个简单的提示
- en: This section assumes you completed the prior sections and builds upon the same
    code. By now, you should have instantiated Semantic Kernel and loaded both the
    GPT-3.5 and the GPT-4 services into it in that order. When you submit a prompt,
    it will default to the first service, and will run the prompt on GPT-3.5.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 本节假设您已完成前面的部分，并在此基础上构建相同的代码。到目前为止，您应该已经实例化了Semantic Kernel，并按顺序将其中的GPT-3.5和GPT-4服务加载进去。当您提交提示时，它将默认使用第一个服务，并在GPT-3.5上运行提示。
- en: When we send the prompt to the service, we will also send a parameter called
    `0.0` to `1.0`, and it controls how random the responses are. We’re going to explain
    the temperature parameter in more detail in later chapters. A temperature parameter
    of `0.8` generates a more creative response, while a temperature parameter of
    `0.2` generates a more precise response.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将提示发送到服务时，我们还会发送一个名为`0.0`到`1.0`的参数，它控制响应的随机性。我们将在后面的章节中更详细地解释温度参数。温度参数为`0.8`会产生更具创造性的响应，而温度参数为`0.2`会产生更精确的响应。
- en: To send the prompt to the service, we will use a method called `create_semantic_function`.
    For now, don’t worry about what a semantic function is. We’re going to explain
    it in the *Using generative AI to solve simple* *problems* section.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 要将提示发送到服务，我们将使用名为`create_semantic_function`的方法。现在，您不必担心什么是语义函数。我们将在*使用生成式AI解决简单*
    *问题*部分中解释它。
- en: Running a simple prompt in Python
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Python中运行一个简单的提示
- en: 'To run a prompt in Python, follow these steps:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Python中运行提示，请按照以下步骤操作：
- en: 'Load the prompt in a string variable:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在一个字符串变量中加载提示：
- en: '[PRE19]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Create a function by using the `add_function` method of the kernel. The `function_name`
    and `plugin_name` parameters are required but are not used, so you can give your
    function and plugin whatever name you want:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过使用内核的`add_function`方法创建一个函数。`function_name`和`plugin_name`参数是必需的，但它们不会被使用，因此您可以给函数和插件取任何您想要的名称：
- en: '[PRE20]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Call the function. Note that all invocation methods are asynchronous, so you
    need to use `await` to wait for their return:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用函数。请注意，所有调用方法都是异步的，因此您需要使用`await`来等待它们的返回：
- en: '[PRE21]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Print the response:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印响应：
- en: '[PRE22]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The response is nondeterministic. Here’s a possible response:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 响应是随机的。以下是一个可能的响应示例：
- en: '[PRE23]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Running a simple prompt in C#
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在C#中运行一个简单的提示
- en: 'To run a prompt in C#, follow these steps:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 要在C#中运行提示，请按照以下步骤操作：
- en: 'Load the prompt in a string variable:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在一个字符串变量中加载提示：
- en: '[PRE24]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Call the prompt by using the `Kernel.InvokePromptAsync` function:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`Kernel.InvokePromptAsync`函数调用提示：
- en: '[PRE25]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Print the response:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印响应：
- en: '[PRE26]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The response is nondeterministic. Here’s a possible response:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 响应是非确定性的。以下是一个可能的响应：
- en: '[PRE27]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We have now connected to an AI service, submitted a prompt to it, and obtained
    a response. We’re now ready to start creating our own functions.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已连接到AI服务，向其提交了一个提示，并获得了响应。我们现在可以开始创建我们自己的函数。
- en: Using generative AI to solve simple problems
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用生成式AI解决简单问题
- en: 'Microsoft Semantic Kernel distinguishes between two types of functions that
    can be loaded into it: **semantic functions** and **native functions**.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: Microsoft Semantic Kernel区分可以加载到其中的两种类型的函数：**语义函数**和**原生函数**。
- en: Semantic functions are functions that connect to AI services, usually LLMs,
    to perform a task. The service is not part of your codebase. Native functions
    are regular functions written in the language of your application.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 语义函数是与AI服务（通常是LLMs）连接以执行任务的函数。该服务不是你代码库的一部分。原生函数是用你的应用程序语言编写的常规函数。
- en: The reason to differentiate a native function from any other regular function
    in your code is that the native function will have additional attributes that
    will tell the kernel what it does. When you load a native function into the kernel,
    you can use it in chains that combine native and semantic functions. In addition,
    Semantic Kernel planner can use the function when creating a plan to achieve a
    user goal.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 将原生函数与你的代码中的任何其他常规函数区分开来的原因是，原生函数将具有额外的属性，这些属性将告诉内核它做什么。当你将原生函数加载到内核中时，你可以在结合原生和语义函数的链中使用它。此外，Semantic
    Kernel规划器可以在创建计划以实现用户目标时使用该函数。
- en: Creating semantic functions
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建语义函数
- en: We have already created a semantic function (`knock`) in the previous section.
    Now, we’re going to add a parameter to it. The default parameter for all semantic
    functions is called `{{$input}}`.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在上一节中创建了一个语义函数（`knock`）。现在，我们将向其中添加一个参数。所有语义函数的默认参数称为`{{$input}}`。
- en: Modified semantic function in Python
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 修改后的Python语义函数
- en: 'We’re going to make minor modifications to our previous code to allow the semantic
    function to receive a parameter. Again, the following code assumes that you have
    already instantiated a kernel and connected to at least one service:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将对之前的代码进行一些小的修改，以允许语义函数接收一个参数。同样，以下代码假设你已经实例化了一个内核并连接到了至少一个服务：
- en: '[PRE28]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The only differences from the code before are that now we have a variable, `{{$input}}`,
    and we’re calling the function using a parameter, the string `"Boo"`. To add the
    variable, we need to import the `KernelArguments` class from the `semantic_kernel_functions.kernel_arguments`
    package and create an instance of the object with the value we want.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的代码相比，唯一的区别是现在我们有一个变量`{{$input}}`，并且我们使用参数字符串`"Boo"`来调用函数。为了添加变量，我们需要从`semantic_kernel_functions.kernel_arguments`包中导入`KernelArguments`类，并创建一个具有我们想要值的对象实例。
- en: 'The answer is nondeterministic. Here’s a possible answer:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 响应是非确定性的。以下是一个可能的响应：
- en: '[PRE29]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Modified semantic function in C#
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 修改后的C#语义函数
- en: 'To create a function in C#, we are going to use the CreateFunctionFromPrompt
    kernel method, and to add a parameter, we will use the KernelArguments object:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 要在C#中创建一个函数，我们将使用`CreateFunctionFromPrompt`内核方法，并使用`KernelArguments`对象来添加参数：
- en: '[PRE30]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Here, too, the only differences from the code before are that now we have a
    variable, `{{$input}}`, and we’re calling the function using a parameter, the
    string `"Boo"`.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，与之前的代码相比，唯一的区别是现在我们有一个变量`{{$input}}`，并且我们使用参数字符串`"Boo"`来调用函数。
- en: 'The answer is nondeterministic. Here’s a possible answer:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 响应是非确定性的。以下是一个可能的响应：
- en: '[PRE31]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Creating native functions
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建原生函数
- en: Native functions are created in the same language your application is using.
    For example, if you are writing code in Python, a native function can be written
    in Python.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 原生函数是在与你的应用程序相同的语言中创建的。例如，如果你正在用Python编写代码，原生函数可以用Python编写。
- en: Although you can call a native function directly without loading it into the
    kernel, loading makes it available to the planner, which we will see in the last
    section of this chapter.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然你可以直接调用原生函数而不将其加载到内核中，但加载可以使它对规划器可用，这一点我们将在本章的最后部分看到。
- en: We’re going to explore native functions in greater detail in [*Chapter 3*](B21826_03.xhtml#_idTextAnchor071),
    but for now, let’s create and load a simple native function in the Kernel.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在 [*第3章*](B21826_03.xhtml#_idTextAnchor071) 中更详细地探讨原生函数，但就目前而言，让我们在内核中创建和加载一个简单的原生函数。
- en: The native function we’re going to create chooses a theme for a joke. For now,
    the themes are `Boo`, `Dishes`, `Art`, `Needle`, `Tank`, and `Police`, and the
    function simply returns one of these themes at random.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要创建的原生函数为笑话选择一个主题。目前，主题有 `Boo`、`Dishes`、`Art`、`Needle`、`Tank` 和 `Police`，函数简单地随机返回这些主题中的一个。
- en: Creating a native function in Python
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 Python 中创建原生函数
- en: In Python, the native functions need to be inside a class. The class used to
    be called a **skill**, and in some places, this name is still used. The name has
    recently changed to **plugin**. A plugin (formerly called skill) is just a collection
    of functions. You cannot mix native and semantic functions in the same skill.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中，原生函数需要位于一个类内部。这个类曾经被称为 **技能**，在某些地方，这个名字仍然在使用。这个名字最近已改为 **插件**。插件（以前称为技能）只是一个函数集合。你无法在同一个技能中混合原生和语义函数。
- en: We’re going to name our class `ShowManager`.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把我们的类命名为 `ShowManager`。
- en: To create a native function, you will use the `@kernel_function` decorator.
    The decorator must contain fields for `description` and `name`. To add a decorator,
    you must import `kernel_function` from the `semantic_kernel.functions.kernel_function_decorator`
    package.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建原生函数，你将使用 `@kernel_function` 装饰器。装饰器必须包含 `description` 和 `name` 字段。要添加装饰器，你必须从
    `semantic_kernel.functions.kernel_function_decorator` 包中导入 `kernel_function`。
- en: 'The function body comes immediately after the decorator. In our case, we are
    simply going to have the list of themes and use the `random.choice` function to
    return one random element from the list:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 函数体紧随装饰器之后。在我们的例子中，我们只是将有一个主题列表，并使用 `random.choice` 函数从列表中返回一个随机元素：
- en: '[PRE32]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Then, to load the plugin and all its functions in the kernel, we use the `add_plugin`
    method of the kernel. When you are adding a plugin, you need to give it a name:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，为了将插件及其所有功能加载到内核中，我们使用内核的 `add_plugin` 方法。当你添加插件时，你需要给它一个名字：
- en: '[PRE33]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'To call the native function from a plugin, simply put the name of the function
    within brackets, as shown here:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 要从插件中调用原生函数，只需在括号内放置函数名，如下所示：
- en: '[PRE34]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The function is not deterministic, but a possible result might be:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 函数不是确定的，但可能的结果可能是：
- en: '[PRE35]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Creating a native function in C#
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 C# 中创建原生函数
- en: In C#, native functions need to be inside a class. The class used to be called
    a skill, and this name is still used in some places; for example, in the SDK,
    we will need to import `Microsoft.SemanticKernel.SkillDefinition`. Skills have
    recently been renamed to plugins. A plugin is just a collection of functions.
    You cannot mix native and semantic functions in the same skill.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在 C# 中，原生函数需要位于一个类内部。这个类曾经被称为技能，这个名字在某些地方仍然在使用；例如，在 SDK 中，我们需要导入 `Microsoft.SemanticKernel.SkillDefinition`。技能最近已被重命名为插件。插件只是一个函数集合。你无法在同一个技能中混合原生和语义函数。
- en: We’re going to name our class `ShowManager`.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把我们的类命名为 `ShowManager`。
- en: 'To create a native function, you will use the `[KernelFunction]` decorator.
    The decorator must contain `Description`. The function body comes immediately
    after the decorator. In our case, we are simply going to have a list of themes
    and use the `Random().Next` method to return one random element from the list.
    We will call our class `ShowManager` and our function `RandomTheme`:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建原生函数，你将使用 `[KernelFunction]` 装饰器。装饰器必须包含 `Description`。函数体紧随装饰器之后。在我们的例子中，我们只是将有一个主题列表，并使用
    `Random().Next` 方法从列表中返回一个随机元素。我们将我们的类命名为 `ShowManager`，我们的函数命名为 `RandomTheme`：
- en: '[PRE36]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Then, to load the plugin and all its functions into the kernel, we use the
    `ImportPluginFromObject` method:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，为了将插件及其所有功能加载到内核中，我们使用 `ImportPluginFromObject` 方法：
- en: '[PRE37]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'To call the native function from a plugin, simply put the function name within
    brackets. You can pass parameters by using the `KernelArguments` class, as shown
    here:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 要从插件中调用原生函数，只需在括号内放置函数名。你可以通过使用 `KernelArguments` 类来传递参数，如下所示：
- en: '[PRE38]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The function is not deterministic, but a possible result might be the following:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 函数不是确定的，但可能的结果可能是以下内容：
- en: '[PRE39]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Now that you can run simple prompts from your code, let’s learn to separate
    the prompt configuration from the code that calls it by using plugins.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以从你的代码中运行简单的提示，让我们学习如何通过使用插件将提示配置与调用它的代码分开。
- en: Plugins
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 插件
- en: One of the greatest strengths of Microsoft Semantic Kernel is that you can create
    semantic plugins that are language agnostic. Semantic plugins are collections
    of semantic functions that can be imported into the kernel. Creating semantic
    plugins allows you to separate your code from the AI function, which makes your
    application easier to maintain. It also allows other people to work on the prompts,
    making it easier to implement prompt engineering, which will be explored in [*Chapter
    2*](B21826_02.xhtml#_idTextAnchor045).
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 微软语义内核的一个最大优势是您可以创建与语言无关的语义插件。语义插件是语义函数的集合，可以导入到内核中。创建语义插件允许您将代码与AI函数分离，这使得您的应用程序更容易维护。它还允许其他人处理提示，使得实现提示工程更容易，这将在[*第二章*](B21826_02.xhtml#_idTextAnchor045)中探讨。
- en: 'Each function is defined by a directory containing two text files: `config.json`,
    which contains the configuration for the semantic function, and `skprompt.txt`,
    which contains its prompt.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 每个函数都由一个包含两个文本文件的目录定义：`config.json`，其中包含语义函数的配置，以及`skprompt.txt`，其中包含其提示。
- en: The configuration of the semantic function includes the preferred engine to
    use, the temperature parameter, and a description of what the semantic function
    does and its inputs.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 语义函数的配置包括要使用的首选引擎、温度参数以及语义函数做什么以及其输入的描述。
- en: The text file contains the prompt that will be sent to the AI service to generate
    the response.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 文本文件包含将发送到AI服务以生成响应的提示。
- en: 'In this section, we are going to define a plugin that contains two semantic
    functions. The first semantic function is a familiar function: the knock-knock
    joke generator. The second function is a function that receives a joke as an input
    and tries to explain why it’s funny. Since this is a more complicated task, we’re
    going to use GPT-4 for this.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将定义一个包含两个语义函数的插件。第一个语义函数是一个熟悉的函数：knock-knock笑话生成器。第二个函数是接收一个笑话作为输入并尝试解释为什么它好笑的函数。由于这是一个更复杂的任务，我们将使用GPT-4来完成。
- en: '[PRE40]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: We will now see how to create the `config.json` and `skprompt.txt` files and
    how to load the plugin into our program.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将看到如何创建`config.json`和`skprompt.txt`文件以及如何将插件加载到我们的程序中。
- en: The config.json file for the knock-knock joke function
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: knock-knock笑话功能的config.json文件
- en: 'The following configuration file shows a possible configuration for the semantic
    function that generates knock-knock jokes:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 以下配置文件显示了为生成knock-knock笑话的语义函数的可能配置：
- en: '[PRE41]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The `default_services` property is an array of the preferred engines to use
    (in order). Since knock-knock jokes are simple, we’re going to use GPT-3.5 for
    it. All the parameters in the preceding file are required. In future chapters,
    we will explain each parameter in detail, but for now, you should just copy them.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '`default_services`属性是一个首选引擎的数组（按顺序）。由于knock-knock笑话很简单，我们将使用GPT-3.5。前一个文件中的所有参数都是必需的。在未来的章节中，我们将详细解释每个参数，但现在，您只需复制它们即可。'
- en: The `description` field is important because it can be used later by the planner,
    which will be explained in the last section of this chapter.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '`description`字段很重要，因为它可以在稍后的规划器中使用，这将在本章的最后部分进行解释。'
- en: The skprompt.txt file for the knock-knock joke function
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: knock-knock笑话功能的skprompt.txt文件
- en: 'Since we want to explain the joke later, we need our application to return
    the whole joke, not only the punchline. This will enable us to save the whole
    joke and pass it as a parameter to the explain-the-joke function. To do so, we
    need to modify the prompt. You can see the final prompt here:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们稍后要解释这个笑话，我们需要我们的应用程序返回整个笑话，而不仅仅是结尾。这将使我们能够保存整个笑话并将其作为参数传递给解释笑话函数。为此，我们需要修改提示。您可以在以下位置看到最终的提示：
- en: '[PRE42]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The config.json file for the semantic function that explains jokes
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解释笑话的语义功能的config.json文件
- en: You should now create a file for the function that explains jokes. Since this
    is a more complicated task, we should set `default_services` to use GPT-4.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在应该为解释笑话的函数创建一个文件。由于这是一个更复杂的任务，我们应该将`default_services`设置为使用GPT-4。
- en: 'This file is almost exactly the same as the `config.json` file used for the
    knock-knock joke function. We have made only three changes:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 此文件几乎与用于knock-knock笑话功能的`config.json`文件完全相同。我们只做了三项更改：
- en: The description
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述
- en: The description of the `input` variable
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input`变量的描述'
- en: The `default_services` field
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`default_services`字段'
- en: 'This can be seen in the following:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以在以下内容中看到：
- en: '[PRE43]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The skprompt.txt file for the explain joke function
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解释笑话功能的skprompt.txt文件
- en: 'The prompt for the function that explains jokes is very simple:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 解释笑话的函数提示非常简单：
- en: '[PRE44]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Loading the plugin from a directory into the kernel
  id: totrans-280
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将插件从目录加载到内核中
- en: Now that the semantic functions are defined in text files, you can load them
    into the kernel by simply pointing to the directory where they are. This can also
    help you to separate the prompt engineering function from the development function.
    Prompt engineers can work with the text files without ever having to touch the
    code of your application.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 现在语义函数已定义在文本文件中，您可以通过指向它们所在的目录来将它们加载到内核中。这也可以帮助您将提示工程函数与开发函数分开。提示工程师可以在不接触您应用程序代码的情况下与文本文件一起工作。
- en: Loading the plugin using Python
  id: totrans-282
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Python 加载插件
- en: 'You can load all the functions inside a plugin directory using the `add_plugin`
    method from the kernel object. Just set the first parameter to `None` and set
    the `parent_directory` parameter to the directory where the plugin is:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用内核对象的 `add_plugin` 方法加载插件目录中的所有函数。只需将第一个参数设置为 `None`，并将 `parent_directory`
    参数设置为插件所在的目录：
- en: '[PRE45]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'You can call the functions in the same way as you would call a function from
    a native plugin by putting the function name within brackets:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过将函数名放在括号内的方式，像调用原生插件中的函数一样调用这些函数：
- en: '[PRE46]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The result of the preceding call is nondeterministic. Here’s a sample result:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个调用的结果是不可预测的。以下是一个示例结果：
- en: '[PRE47]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We can pass the results of the preceding call to the `explain_joke` function:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将前一个调用的结果传递给 `explain_joke` 函数：
- en: '[PRE48]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Remember that this function is configured to use GPT-4\. The results of this
    function are nondeterministic. Here’s a sample result:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，此函数配置为使用 GPT-4。此函数的结果是不可预测的。以下是一个示例结果：
- en: '[PRE49]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Loading the plugin using C#
  id: totrans-293
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 C# 加载插件
- en: 'You can load all the functions inside a plugin directory. First, we obtain
    the path to the directory (your path may be different):'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以加载插件目录中的所有函数。首先，我们获取目录的路径（您的路径可能不同）：
- en: '[PRE50]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Then, we use `ImportPluginFromPromptDirectory` to load the functions into a
    variable. The result is a collection of functions. You can access them by referencing
    them inside brackets:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用 `ImportPluginFromPromptDirectory` 将函数加载到一个变量中。结果是函数的集合。您可以通过在括号内引用它们来访问它们：
- en: '[PRE51]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The last step is to call the function. To call it, we use the `InvokeAsync`
    method of the kernel object. We will, again, pass a parameter using the `KernelArguments`
    class:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是调用函数。要调用它，我们使用内核对象的 `InvokeAsync` 方法。我们再次将参数通过 `KernelArguments` 类传递：
- en: '[PRE52]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The result of the preceding call is nondeterministic. Here’s a sample result:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个调用的结果是不可预测的。以下是一个示例结果：
- en: '[PRE53]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'To get an explanation, we can pass the results of the preceding call to the
    `explain_joke` function:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取解释，我们可以将前一个调用的结果传递给 `explain_joke` 函数：
- en: '[PRE54]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Here’s a sample result:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例结果：
- en: '[PRE55]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Now that you have seen how to create and call one function of a plugin, we are
    going to learn how to use a planner to call multiple functions from different
    plugins.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经看到了如何创建和调用插件的一个函数，我们将学习如何使用规划器从不同的插件中调用多个函数。
- en: Using a planner to run a multistep task
  id: totrans-307
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用规划器运行多步骤任务
- en: Instead of calling functions yourself, you can let Microsoft Semantic Kernel
    choose the functions for you. This can make your code a lot simpler and can give
    your users the ability to combine your code in ways that you haven’t considered.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 您不必自己调用函数，可以让 Microsoft Semantic Kernel 为您选择函数。这可以使您的代码更加简单，并让您的用户能够以您未曾考虑过的方式组合您的代码。
- en: Right now, this will not seem very useful because we only have a few functions
    and plugins. However, in a large application, such as Microsoft Office, you may
    have hundreds or even thousands of plugins, and your users may want to combine
    them in ways that you can’t yet imagine. For example, you may be creating a copilot
    that helps a user be more efficient when learning about a subject, so you write
    a function that downloads the latest news about that subject from the web. You
    may also have independently created a function that explains a piece of text to
    the user so that the user can paste content to learn more about it. The user may
    decide to combine them both with “*download the news and write an article explaining
    them to me*,” something that you never thought about and didn’t add to your code.
    Semantic Kernel will understand that it can call the two functions you wrote in
    sequence to complete that task.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，这看起来可能不太有用，因为我们只有少数几个函数和插件。然而，在一个大型应用程序中，例如Microsoft Office，您可能有数百甚至数千个插件，并且您的用户可能希望以您目前无法想象的方式将它们结合起来。例如，您可能正在创建一个辅助程序，帮助用户在了解某个主题时更加高效，因此您编写了一个从网络上下载该主题最新新闻的函数。您也可能已经独立创建了一个向用户解释文本的函数，以便用户可以粘贴内容以了解更多信息。用户可能会决定将它们两者结合起来，例如“*下载新闻并为我撰写解释它们的文章*”，这是您从未想过并且没有添加到代码中的事情。语义内核将理解它可以按顺序调用您编写的两个函数来完成该任务。
- en: When you let users request their own tasks, they will use natural language,
    and you can let Semantic Kernel inspect all the functions that are loaded into
    it and use a planner to decide the best way of handling the user request.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 当您允许用户请求他们自己的任务时，他们会使用自然语言，您可以让语义内核检查所有加载到其中的函数，并使用规划器来决定处理用户请求的最佳方式。
- en: 'For now, we are only going to show a quick example of using a planner, but
    we will explore the topic in more depth in [*Chapter 5*](B21826_05.xhtml#_idTextAnchor106).
    Planners are still under active development, and there might be changes over time.
    Currently, Semantic Kernel is expected to have two planners: the **Function Calling
    Stepwise planner**, available for Python and C#, and the **Handlebars planner**,
    available only for C# at the time of writing.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们只展示使用规划器的快速示例，但我们将更深入地探讨这个主题，见[*第五章*](B21826_05.xhtml#_idTextAnchor106)。规划器仍在积极开发中，随着时间的推移可能会有所变化。目前，预计语义内核将有两个规划器：适用于Python和C#的**函数调用逐步规划器**，以及仅适用于C#的**Handlebars规划器**（在撰写本文时）。
- en: Although the following example is very simple and both planners behave in the
    same way, we will show how to use the Stepwise planner (Function Calling Stepwise
    Planner) with Python and the Handlebars planner with C#.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管以下示例非常简单，并且两个规划器表现相同，但我们将展示如何使用Python的逐步规划器（函数调用逐步规划器）和C#的Handlebars规划器。
- en: Calling the Function Calling Stepwise planner with Python
  id: totrans-313
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Python调用函数调用逐步规划器
- en: To use the Stepwise planner, we first create an object of the `FunctionCallingStepwisePlanner`
    class and make a request to it. In our case, we’re going to ask it to choose a
    random theme, create a knock-knock joke, and explain it.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用逐步规划器，我们首先创建一个`FunctionCallingStepwisePlanner`类的对象并向其发出请求。在我们的例子中，我们将要求它选择一个随机主题，创建一个敲门笑话，并解释它。
- en: 'We’re going to modify our earlier program, delete the function calls, and add
    a call to the planner instead:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将修改我们之前的程序，删除函数调用，并添加对规划器的调用：
- en: '[PRE56]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'There are a couple of details to note. The first one is that I used the class
    `FunctionCallingStepwisePlannerOptions` to pass a `max_tokens` parameter to the
    planner. Behind the scenes, the planner will create a prompt and send it to the
    AI service. The default `max_tokens` for most AI services tends to be small. At
    the time of writing, it was `250`, which may cause an error if the prompt generated
    by the planner is too large. The second detail to note is that I printed `result.final_answer`
    instead of `result`. The `result` variable contains the whole plan: the definition
    of the functions, the chat with the OpenAI model explaining how to proceed, etc.
    It’s interesting to print the `result` variable to see how the planner works internally,
    but to see the outcome of the planner execution, all you need to do is print `result.final_answer`.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个细节需要注意。第一个细节是，我使用了`FunctionCallingStepwisePlannerOptions`类来向规划器传递`max_tokens`参数。在幕后，规划器将创建一个提示并将其发送到AI服务。大多数AI服务的默认`max_tokens`通常较小。在撰写本文时，它是`250`，如果规划器生成的提示太大，可能会导致错误。第二个需要注意的细节是，我打印了`result.final_answer`而不是`result`。`result`变量包含整个计划：函数的定义、与OpenAI模型的对话，解释如何进行等。打印`result`变量很有趣，可以了解规划器是如何内部工作的，但要查看规划器执行的结果，您只需要打印`result.final_answer`。
- en: 'Here is a sample response, first telling the joke and then explaining it:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个示例响应，首先讲述笑话然后解释它：
- en: '[PRE57]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: As you can see, the planner generated the joke and the explanation, as expected,
    without us needing to tell Semantic Kernel in which order to call the functions.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，规划器生成了笑话和解释，正如预期的那样，我们无需告诉语义内核调用函数的顺序。
- en: Calling the Handlebars planner in C#
  id: totrans-321
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在C#中调用Handlebars规划器
- en: At the time of writing, the Handlebars planner is in version 1.0.1-preview,
    and it’s still experimental in C#, although it’s likely that a release version
    will be made available soon.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，Handlebars规划器处于1.0.1-preview版本，尽管它在C#中仍然是实验性的，但很可能很快就会提供一个发布版本。
- en: 'To use the Handlebars planner, you first need to install it, which you can
    do by using the following command (you should use the latest version available
    to you):'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用Handlebars规划器，您首先需要安装它，您可以通过以下命令完成（您应该使用可用的最新版本）：
- en: '[PRE58]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'To use the Handlebars planner, you need to use the following `pragma` warning
    in your code. The Handlebars planner code is still experimental, and if you don’t
    add the `#pragma` directive, your code will fail, with a warning that it contains
    experimental code. You also need to import the `Microsoft.SemanticKernel.Planning.Handlebars`
    package:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用Handlebars规划器，您需要在代码中使用以下`pragma`警告。Handlebars规划器代码仍然是实验性的，如果您不添加`#pragma`指令，您的代码将失败，并显示包含实验性代码的警告。您还需要导入`Microsoft.SemanticKernel.Planning.Handlebars`包：
- en: '[PRE59]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'We proceed as usual, instantiating our kernel and adding native and semantic
    functions to it:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 我们像往常一样进行，实例化我们的内核并向其中添加原生和语义函数：
- en: '[PRE60]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The big difference happens now – instead of telling which functions to call
    and how, we simply ask the planner to do what we want:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 现在出现了重大差异——我们不再告诉调用哪些函数以及如何调用，我们只是简单地要求规划器做我们想要的事情：
- en: '[PRE61]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'We can execute the plan by calling `InvokeAsync` from the `plan` object:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过从`plan`对象调用`InvokeAsync`来执行计划：
- en: '[PRE62]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'The result is nondeterministic. Here is a sample result, first telling the
    joke and then explaining it:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是非确定性的。以下是一个示例结果，首先讲述笑话然后解释它：
- en: '[PRE63]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: As you can see, the planner generated the joke and the explanation, as expected,
    without us needing to tell Semantic Kernel in which order to call the functions.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，规划器生成了笑话和解释，正如预期的那样，我们无需告诉语义内核调用函数的顺序。
- en: Summary
  id: totrans-336
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned about Generative AI and the main components of
    Microsoft Semantic Kernel. You learned how to create a prompt and submit it to
    a service and how to embed that prompt into a semantic function. You also learned
    how to execute multistep requests by using a planner.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您了解了生成式AI和Microsoft语义内核的主要组件。您学习了如何创建提示并将其提交给服务，以及如何将提示嵌入到语义函数中。您还学习了如何通过使用规划器来执行多步请求。
- en: In the next chapter, we are going to learn how to make our prompts better through
    a topic called **prompt engineering**. This will help you create prompts that
    get your users the correct result faster and use fewer tokens, therefore reducing
    costs.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将通过一个名为**提示工程**的主题来学习如何使我们的提示更好。这将帮助您创建能够更快地为用户提供正确结果并使用更少标记的提示，从而降低成本。
- en: References
  id: totrans-339
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: '[1] A. Vaswani et al., “Attention Is All You Need,” Jun. 2017.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] A. Vaswani等人，“Attention Is All You Need”，2017年6月。'
- en: '[2] OpenAI, “GPT-4 Technical Report.” arXiv, Mar. 27, 2023\. doi: 10.48550/arXiv.2303.08774.'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] OpenAI, “GPT-4 技术报告.” arXiv, 2023年3月27日\. doi: 10.48550/arXiv.2303.08774.'
- en: '[3] L. Chen, M. Zaharia, and J. Zou, “FrugalGPT: How to Use Large Language
    Models While Reducing Cost and Improving Performance.” arXiv, May 09, 2023\. doi:
    10.48550/arXiv.2305.05176.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] L. Chen, M. Zaharia, 和 J. Zou, “FrugalGPT: 如何在降低成本和提高性能的同时使用大型语言模型.” arXiv,
    2023年5月9日\. doi: 10.48550/arXiv.2305.05176.'
