- en: '1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introducing Microsoft Semantic Kernel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **generative artificial intelligence** (**GenAI**) space is evolving quickly,
    with dozens of new products and services being launched weekly; it is becoming
    hard for developers to keep up with the ever-changing features and **application
    programming interfaces** (**APIs**) for each of the services. In this book, you
    will learn about **Microsoft Semantic Kernel**, an API that will make it a lot
    easier for you to use GenAI as a developer, making your code shorter, simpler,
    and more maintainable. Microsoft Semantic Kernel will allow you, as a developer,
    to use a single interface to connect with several different GenAI providers. Microsoft
    used Semantic Kernel to develop its copilots, such as Microsoft 365 Copilot.
  prefs: []
  type: TYPE_NORMAL
- en: Billions of people already use GenAI as consumers, and you are probably one
    of them. We will start this chapter by showing some examples of what you can do
    with GenAI as a consumer. Then, you will learn how you can start using GenAI as
    a developer to add AI services to your own applications.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will learn the differences between using GenAI as a user
    and as a developer and how to create and run a simple end-to-end request with
    Microsoft Semantic Kernel. This will help you see how powerful and simple Semantic
    Kernel is and will serve as a framework for all further chapters. It will enable
    you to begin integrating AI into your own apps right away.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll be covering the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the basic use of a generative AI application like ChatGPT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing Microsoft Semantic Kernel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring Semantic Kernel to interact with AI services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running a simple task using Semantic Kernel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To complete this chapter, you will need to have a recent, supported version
    of your preferred Python or C# development environment:'
  prefs: []
  type: TYPE_NORMAL
- en: For Python, the minimum supported version is Python 3.10, and the recommended
    version is Python 3.11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For C#, the minimum supported version is .NET 8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The examples are presented in C# and Python, and you can choose to only read
    the examples of your preferred language. Occasionally, a feature is available
    in only one of the languages. In such cases, we provide an alternative in the
    other language for how to achieve the same objectives.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will call OpenAI services. Given the amount that companies
    spend on training these large language models (LLMs), it’s no surprise that using
    these services is not free. You will need an **OpenAI API** key, obtained either
    directly through **OpenAI** or **Microsoft**, via the **Azure** **OpenAI** service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Important: Using the OpenAI services is not free'
  prefs: []
  type: TYPE_NORMAL
- en: The examples that we will run throughout this book will call the OpenAI API.
    These calls require a paid subscription, and each call will incur a cost. The
    costs are usually small per request (for example, GPT-4 costs up to $0.12 per
    1,000 tokens), but they can add up. In addition, note that different models have
    different prices, with GPT-3.5 being 30 times less expensive per token than GPT-4.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenAI pricing information can be found here: [https://openai.com/pricing](https://openai.com/pricing)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Azure OpenAI pricing information can be found here: [https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/](https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/)'
  prefs: []
  type: TYPE_NORMAL
- en: If you use .NET, the code for this chapter is at [https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/dotnet/ch1](https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/dotnet/ch1).
  prefs: []
  type: TYPE_NORMAL
- en: If you use Python, the code for this chapter is at [https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/python/ch1](https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/python/ch1).
  prefs: []
  type: TYPE_NORMAL
- en: 'You can install the required packages by going to the GitHub repository and
    using the following: `pip install -``r requirements.txt`.'
  prefs: []
  type: TYPE_NORMAL
- en: Obtaining an OpenAI API key
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Go to the OpenAI Platform website ([https://platform.openai.com](https://platform.openai.com)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sign up for a new account or sign in with an existing account. You can use your
    email or an existing Microsoft, Google, or Apple account.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the left sidebar menu, select **API keys**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the **Project API keys** screen, click the button labeled **+ Create new
    secret key** (optionally, give it a name).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Important
  prefs: []
  type: TYPE_NORMAL
- en: You have to copy and save the key immediately. It will disappear as soon as
    you click **Done**. If you didn’t copy the key or if you lost it, you need to
    generate a new one. There’s no cost to generate a new key. Remember to delete
    old keys.
  prefs: []
  type: TYPE_NORMAL
- en: Obtaining an Azure OpenAI API key
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Currently, you need to submit an application to obtain access to the Azure OpenAI
    Service. To apply for access, you need to complete a form at [https://aka.ms/oai/access](https://aka.ms/oai/access).
  prefs: []
  type: TYPE_NORMAL
- en: The instructions to obtain an Azure OpenAI API key are available at [https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/create-resource](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/create-resource).
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI and how to use it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generative AI refers to a subset of artificial intelligence programs that are
    capable of creating content that is similar to what humans can produce. These
    systems use training from very large datasets to learn their patterns, styles,
    and structures. Then, they can generate entirely new content, such as synthesized
    images, music, and text.
  prefs: []
  type: TYPE_NORMAL
- en: Using GenAI as consumer or end-user is very easy, and as a technical person,
    you probably have already done it. There are many consumer-facing AI products.
    The most famous is OpenAI’s **ChatGPT**, but there are many others that have hundreds
    of millions of users every day, such as Microsoft Copilot, Google Gemini (formerly
    Bard), and Midjourney. As of October 2023, Meta, the parent company of Facebook,
    WhatsApp, and Instagram, is making GenAI services available to all its users,
    increasing the number of GenAI daily users to billions.
  prefs: []
  type: TYPE_NORMAL
- en: While the concept of GenAI has existed for a while, it gained a lot of users
    with the release of OpenAI’s ChatGPT in November of 2022\. The initial version
    of ChatGPT was based on a model called **generative pre-trained transformer**
    (**GPT**) version 3.5\. That version was a lot better than earlier versions in
    the task of mimicking human-like writing. In addition, OpenAI made it easy to
    use by adding a chatbot-like interface and making it available to the general
    public. This interface is called ChatGPT. With ChatGPT, users can easily initiate
    tasks in their own words. At its launch, ChatGPT was the product with the fastest
    adoption rate in history.
  prefs: []
  type: TYPE_NORMAL
- en: The GenAI concept was further popularized with the release of Midjourney, an
    application that allows for the generation of high-quality images from prompts
    submitted through Discord, a popular chat application, and Microsoft Copilot,
    a free web application that can generate text by using GPT-4 (the newest version
    of OpenAI’s GPT) and images by using an OpenAI model called DALL-E 3.
  prefs: []
  type: TYPE_NORMAL
- en: In the upcoming subsections, we will discuss text and image generation using
    GenAI applications and explain the differences between generating them using applications
    such as ChatGPT and with an API as a developer.
  prefs: []
  type: TYPE_NORMAL
- en: Text generation models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The initial use cases for GenAI were to generate text based on a simple instruction
    called a **prompt**.
  prefs: []
  type: TYPE_NORMAL
- en: The technology used behind most text-based GenAI products is called a **transformer**,
    and it was introduced in the paper *Attention is All you Need* [1] in 2017\. The
    transformer immensely improved the quality of the text being generated, and in
    just a few years, the text looked very similar to human-generated text. The transformer
    greatly improved the ability of AI to guess masked words in a phrase after being
    trained on a large number of documents (a **corpus**). Models trained on very
    large corpuses are called **large language** **models** (**LLMs**).
  prefs: []
  type: TYPE_NORMAL
- en: If LLMs are given a phrase such as “*I went to the fast-food restaurant to <X>*,”
    they can generate good options for *X*, such as “*eat*.” Applying the transformer
    repeatedly can generate coherent phrases and even stories. The next iteration
    could be “*I went to the fast-food restaurant to eat <X>*,” returning “*a*,” and
    then “*I went to the fast-food restaurant to eat a <X>*,” could return “*burger*,”
    forming the full phrase “*I went to the fast-food restaurant to eat* *a burger*.”
  prefs: []
  type: TYPE_NORMAL
- en: The performance of an LLM model depends on the number of parameters, which is
    roughly proportional to how many comparisons a model can make at once, the context
    window, the maximum size of the text that can be handled at once, and the data
    used to train the model, which is usually kept secret by the companies that create
    LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The GPT is a model created by OpenAI that uses transformers and is good at
    generating text. There are many versions of GPT:'
  prefs: []
  type: TYPE_NORMAL
- en: GPT-1, launched in February 2018, had 120 million parameters and a **context
    window** of 512 tokens.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-2 was launched in February 2019, with the number of parameters increasing
    to 1.5 billion and the context window increasing to 1,024 tokens. Up to this point,
    while they sometimes produced interesting results, these models were mostly used
    by academics.
  prefs: []
  type: TYPE_NORMAL
- en: 'This changed with GPT-3, launched in June 2020, which had several sizes: small,
    medium, large, and extra-large. Extra-large had 175 billion parameters and a 2,048
    token context window. The generated text was, in most cases, hard to distinguish
    from human-generated text. OpenAI followed it with GPT-3.5, released in November
    2022, with still 175 billion parameters and a context window of 4,096 tokens (now
    expanded to 16,384 tokens), and launched a user interface named ChatGPT.'
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT is a web and mobile application that uses the GPT models in the background
    and allows users to submit prompts to the GPT models and get responses online.
    It was launched together with GPT-3.5, and at the time, it was the consumer product
    with the fastest adoption rate, reaching 100 million users in less than two months.
  prefs: []
  type: TYPE_NORMAL
- en: In February 2023, Microsoft released Bing Chat, which also uses OpenAI’s GPT
    models in the back end, further popularizing the usage of transformer models and
    AI. Recently, Microsoft renamed it to Microsoft Copilot.
  prefs: []
  type: TYPE_NORMAL
- en: Just a month later, in March 2023, OpenAI released the GPT-4 model, which was
    quickly incorporated into the backend of consumer products such as ChatGPT and
    Bing.
  prefs: []
  type: TYPE_NORMAL
- en: Not all details about the GPT-4 model have been released to the public. It’s
    known that it has a context window of up to 32,768 tokens; however, its number
    of parameters has not been made public, but it has been estimated at 1.8 trillion.
  prefs: []
  type: TYPE_NORMAL
- en: The GPT-4 model is notably good at human-like tasks related to text generation.
    A benchmark test shown in the GPT-4 technical report academic paper [2] shows
    the performance of GPT-3.5 and GPT-4 in taking exams. GPT-4 could pass many high-school
    and college level exams. You can read the paper at [https://doi.org/10.48550/arXiv.2303.08774](https://doi.org/10.48550/arXiv.2303.08774).
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the difference between applications and models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most people, including you, have likely used a GenAI application, such as ChatGPT,
    Microsoft Copilot, Bing Image Creator, Bard (now named Gemini), or Midjourney.
    These applications use GenAI models in their backend, but they also add a user
    interface and configurations that restrict and control the output of the models.
  prefs: []
  type: TYPE_NORMAL
- en: When you are developing your own application, you will need to do these things
    by yourself. You may not yet realize how much is carried out behind the scenes
    by applications such as Bing and ChatGPT.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you submit a prompt to an application, the application may add several
    additional instructions to the prompt you submitted. The most typical things added
    are instructions to restrict some types of output, for example: “*your reply should
    contain no curse words*.” For example, when you submit the prompt “*Tell me a
    joke*” to an application like ChatGPT, it may modify your prompt to “*Tell me
    a joke. Your reply should contain no curse words*” and pass that modified prompt
    to the model.'
  prefs: []
  type: TYPE_NORMAL
- en: The application may also add a summary of the questions that you have already
    submitted and the answers that were already given. For example, if you ask, “*How
    warm is Rio de Janeiro, Brazil, in the summer?*,” the answer may be, “*Rio de
    Janeiro is typically between 90 and 100 degrees Fahrenheit (30-40 degrees Celsius)
    in the summer*.” If you then ask the question, “*How long is the flight from New
    York to there?*,” an application such as ChatGPT will not submit “*How long is
    the flight from New York to there?”* directly to the model because the answer
    would be something like “*I don’t know what you mean* *by ‘there’*.”
  prefs: []
  type: TYPE_NORMAL
- en: 'A straightforward way to address this problem is to save everything that the
    user entered and all the answers that were provided and re-submit them with every
    new prompt. For example, when the user submits “*How long is the flight from New
    York to there?*” after asking about the temperature, the application prepends
    the earlier questions and answers to the prompt, and what is actually submitted
    to the model is: “*How warm is Rio de Janeiro, Brazil, in the summer? Rio de Janeiro
    is typically between 90 and 100 degrees Fahrenheit (30-40 degrees Celsius) in
    the summer. How long is the flight from New York to there?*” Now, the model knows
    that “*there*” means “*Rio de Janeiro*,” and the answer will be something like
    “*Approximately* *10 hours*.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'The consequence of appending all earlier prompts and responses to each new
    prompt is that it consumes a lot of space in the context window. Therefore, some
    techniques have been developed to compress the information that is added to the
    user prompt. The simplest technique is to keep only the earlier user questions,
    but not the answers given by the application. In that case, for example, the modified
    prompt would be something like “*Earlier I said: ‘How warm is Rio de Janeiro,
    Brazil, in the summer?, now answer only: “How long is the flight from New York
    to there?”*’. Note that the prompt needs to tell the model to respond only to
    the last question submitted by the user.'
  prefs: []
  type: TYPE_NORMAL
- en: Knowing that applications modify your prompts will be relevant for you if you
    test your prompts using consumer applications because the output you get from
    them can be substantially different from what you get when you use the model directly
    through an API, such as Microsoft Semantic Kernel. There’s usually no way to know
    how the applications are modifying your prompts, as the providers usually don’t
    reveal all their techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, a significant part of what you will do as an application developer
    will be to create prompt modifications that match your own application. Therefore,
    when your user submits their prompt, you will add your own prompt modifications
    to ensure they get an appropriate result. The techniques to modify user prompts
    are called **prompt engineering**, which we will explore briefly in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Generating text using consumer applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s explore the two most popular text generation applications: ChatGPT and
    Microsoft Copilot. If you are not familiar with the power of GenAI, trying them
    out will give you a sense of what can be done with them and how powerful they
    are. We will also briefly talk about their configuration parameters and their
    architecture, which can help you decide the appropriate architecture for your
    own applications later.'
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI ChatGPT
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you have never used a GenAI app, ChatGPT is likely the best place to start.
    ChatGPT’s default backend model is GPT 3.5, a fast and very powerful model. ChatGPT
    is free to use when OpenAI has available capacity on their servers. You can also
    buy a subscription to ChatGPT Plus for $20 per month, and that will give you the
    ability to use their most powerful model (currently GPT-4) and ensure you will
    always have capacity.
  prefs: []
  type: TYPE_NORMAL
- en: To use ChatGPT, go to [https://chat.openai.com](https://chat.openai.com).
  prefs: []
  type: TYPE_NORMAL
- en: 'The ChatGPT interface is very simple. It allows you to choose the backend model
    on top, some suggestions of prompts in the middle, and a text box to enter prompts
    in the bottom. It also includes a notice that the output produced may include
    incorrect information:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.1 – Submitting requests to ChatGPT using the web interface](img/B21826_01_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.1 – Submitting requests to ChatGPT using the web interface
  prefs: []
  type: TYPE_NORMAL
- en: 'I will submit the following prompt in the **Send a** **Message** textbox:'
  prefs: []
  type: TYPE_NORMAL
- en: '`How long is the flight between New York City and Rio` `de Janeiro?`'
  prefs: []
  type: TYPE_NORMAL
- en: 'ChatGPT, using the GPT-3.5 model, provides the following answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Microsoft Copilot
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another free alternative is Microsoft Copilot, formerly Bing Chat. It is available
    from the [www.bing.com](http://www.bing.com) page, but it can be accessed directly
    from [https://www.bing.com/chat](https://www.bing.com/chat).
  prefs: []
  type: TYPE_NORMAL
- en: The user interface for Microsoft Copilot is like the interface of ChatGPT. It
    has some suggestions for prompts in the middle of the screen and a text box, where
    the user can enter a prompt at the bottom. The Microsoft Copilot UI also shows
    a couple of options that will be relevant from when we use models programmatically.
  prefs: []
  type: TYPE_NORMAL
- en: The first is the conversation style. Copilot offers the option of being More
    Creative, More Balanced, or More Precise. This is related to the temperature parameter
    that will be passed to the underlying model. We will talk about the temperature
    parameter in [*Chapter 3*](B21826_03.xhtml#_idTextAnchor071), but in short, the
    temperature parameter determines how common the words chosen by the LLM are.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters in Microsoft Copilot
  prefs: []
  type: TYPE_NORMAL
- en: While Microsoft Copilot does not reveal the exact configuration values, `0`
    and `0.2`), resulting in very safe guesses for the next word. For `0.4` and `0.6`),
    resulting *mostly* in safe guesses, but with the occasional guess being rare.
    `0.8`. Most guesses will still be safe, but more guesses will be rare words. Since
    LLMs guess words of a phrase in sequence, previous guesses influence subsequent
    guesses. When generating a phrase, each rare word makes the whole phrase more
    unusual.
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting component in the UI is that the bottom right of the text
    box shows how many characters have been entered, giving you an idea of how much
    you will consume of the underlying model’s context window. Note that you cannot
    know for sure how much you will consume because the Copilot application will modify
    your prompt.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.2 – Microsoft Copilot user interface](img/B21826_01_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.2 – Microsoft Copilot user interface
  prefs: []
  type: TYPE_NORMAL
- en: 'On August 11, 2023, Mikhail Parakhin, Bing’s former CEO, posted on X/Twitter
    that Microsoft Copilot outperforms GPT-4 because it uses **retrieval augmented**
    **inference** ([https://x.com/MParakhin/status/1689824478602424320?s=20](https://x.com/MParakhin/status/1689824478602424320?s=20)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.3 – Post by Bing’s former CEO about Microsoft Copilot using RAG](img/B21826_01_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.3 – Post by Bing’s former CEO about Microsoft Copilot using RAG
  prefs: []
  type: TYPE_NORMAL
- en: We will talk more about retrieval augmented inference in *Chapters 6* and *7*,
    but for our current purposes, this means that Microsoft Copilot does not directly
    submit your prompt to the model. Bing has not publicly released the details of
    their architecture yet, but it is likely that Bing modifies your prompt (it shows
    the modified prompt in the UI, under **Searching for**), makes a regular Bing
    query using the modified prompt, gathers the results of that Bing query, concatenates
    them, and submits the concatenated results as a large prompt to a GPT model, asking
    it to combine the results to output a coherent answer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using retrieval augmentation allows Bing to add citations and advertisements
    more easily. In the figure below, note that my prompt `How long is the flight
    between New York City and Rio de Janeiro?` was modified by Copilot to `Searching
    for flight duration New York City Rio` `de Janeiro`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.4 – An example of using Microsoft Copilot](img/B21826_01_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.4 – An example of using Microsoft Copilot
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, you can use consumer applications such as ChatGPT and Microsoft
    Copilot to familiarize yourself with how LLMs work for GenAI and to do some initial
    testing of your prompts, but be aware that the prompt you submit may be heavily
    modified by the application, and the response that you get from the underlying
    model can be very different from the responses that you will get when you actually
    create your own application.
  prefs: []
  type: TYPE_NORMAL
- en: Generating images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Besides generating text, AI can also be used to generate images from text prompts.
    The details of the process used to generate images from prompts are outside the
    scope of this book, but we will provide a quick summary. The main models in the
    image generation space are Midjourney, which is available through the Midjourney
    Bot in Discord; the open-source Stable Diffusion, which is also used by OpenAI’s
    DALL-E 2; and the DALL-E 3, released in October 2023 and available through the
    Bing Chat (now Microsoft Copilot) and ChatGPT applications.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, Microsoft Semantic Kernel only supports DALL-E; therefore,
    this is the example we are going to explore. DALL-E 3 is available for free through
    the Microsoft Copilot application, with some limitations.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are using the Microsoft Copilot application from the earlier example,
    make sure to reset your chat history by clicking on the **New Topic** button to
    the left of the text box. To generate images, make sure your conversation style
    is set to **More Creative**, as image generation only works in Creative mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.5 – Choosing the conversation style](img/B21826_01_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.5 – Choosing the conversation style
  prefs: []
  type: TYPE_NORMAL
- en: 'I will use the following prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '`create a photorealistic image of a salt-and-pepper standard schnauzer on a
    street corner holding a sign "Will do tricks` `for cheese."`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Microsoft Copilot will call DALL-E 3 and generate four images based on my specification:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.6 – Images generated by Microsoft Copilot](img/B21826_01_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.6 – Images generated by Microsoft Copilot
  prefs: []
  type: TYPE_NORMAL
- en: One way in which DALL-E 3 is better than other image-generating models is that
    it can correctly add text to images. Earlier versions of DALL-E and most other
    models cannot spell words properly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The images are presented in a grid with a total resolution of 1024 x 1024 pixels
    (512 x 512 per image). If you select one image, that specific image will be upscaled
    to the 1024 x 1024 pixel resolution. In my case, I will select the image in the
    bottom left corner. You can see the final result in the next figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.7 – High-resolution image generated by Microsoft Copilot](img/B21826_01_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.7 – High-resolution image generated by Microsoft Copilot
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, GenAI can also be used to generate images and now you have an
    idea of how powerful it can be. We will explore generating images with Microsoft
    Semantic Kernel in [*Chapter 4*](B21826_04.xhtml#_idTextAnchor086). There is a
    lot to explore before we get there, and we will start with a quick, comprehensive
    tour of Microsoft Semantic Kernel.
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft Semantic Kernel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Microsoft Semantic Kernel ([https://github.com/microsoft/semantic-kernel](https://github.com/microsoft/semantic-kernel))
    is a thin, open source **software development toolkit** (**SDK**) that makes it
    easier for applications developed in C# and Python to interact with AI services
    such as the ones made available through OpenAI, Azure OpenAI, and Hugging Face.
    Semantic Kernel can receive requests from your application and route them to different
    AI services. Furthermore, if you extend the functionality of Semantic Kernel by
    adding your own functions, which we will explore in [*Chapter 3*](B21826_03.xhtml#_idTextAnchor071),
    Semantic Kernel can automatically discover which functions need to be used, and
    in which order, to fulfill a request. The request can come directly from the user
    and be passed through directly from your application, or your application can
    modify and enrich the user request before sending it to Semantic Kernel.
  prefs: []
  type: TYPE_NORMAL
- en: It was originally designed to power different versions of Microsoft Copilot,
    such as Microsoft 365 Copilot and the Bing Copilot, and then be released to the
    developer community as an open source package. Developers can use Semantic Kernel
    to create plugins that execute complex actions using AI services and combine these
    plugins using just a few lines of code.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, Semantic Kernel can automatically orchestrate different plugins
    by using a **planner**. With the planner, a user can ask your application to achieve
    a complex goal. For example, if you have a function that identifies which animal
    is in a picture and another function that tells knock-knock jokes, your user can
    say, “*Tell me a knock-knock joke about the animal in the picture in this URL*,”
    and the planner will automatically understand that it needs to call the identification
    function first and the “tell joke” function after it. Semantic Kernel will automatically
    search and combine your plugins to achieve that goal and create a plan. Then,
    Semantic Kernel will execute that plan and provide a response to the user:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.8 – Structure of Microsoft Semantic Kernel](img/B21826_01_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.8 – Structure of Microsoft Semantic Kernel
  prefs: []
  type: TYPE_NORMAL
- en: In the upcoming sections, we will do a quick end-to-end tour of Semantic Kernel,
    going through most of the steps in *Figure 1**.8*. We will send a request, create
    a plan, call the API, and call a native function and a semantic function. These,
    combined, will generate an answer to the user. First, we will do this manually,
    step-by-step, and then we will do everything in one go using the planner. You
    will see how powerful Semantic Kernel can be with just a little code.
  prefs: []
  type: TYPE_NORMAL
- en: Before we start experimenting with Microsoft Semantic Kernel, we need to install
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the Microsoft Semantic Kernel package
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To use Microsoft Semantic Kernel, you must install it in your environment. Please
    note that Microsoft Semantic Kernel is still in active development, and there
    may be differences between versions.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Microsoft Semantic Kernel in Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To install Microsoft Semantic Kernel in Python, start in a new directory and
    follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new virtual environment with `venv`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Activate the new environment you just created. This will ensure that Microsoft
    Semantic Kernel will be installed only for this directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In PowerShell, use the following:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: pip install semantic-kernel
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: Installing Microsoft Semantic Kernel in C#
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To install Microsoft Semantic Kernel in C#, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new project targeting .NET 8:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Change into the application directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: dotnet add package Microsoft.SemanticKernel --prerelease
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the program with the following simple instructions just to make sure that
    the installation succeeded:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instantiating the kernel in Python:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiating the kernel in C#:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We have now installed Semantic Kernel in your environment, and now we’re ready
    to connect it to AI services and start using them.
  prefs: []
  type: TYPE_NORMAL
- en: Using Semantic Kernel to connect to AI services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To complete this section, you must have an API key. The process to obtain an
    API key was described at the beginning of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In the upcoming subsections, we are only going to connect to the OpenAI text
    models GPT-3.5 and GPT-4\. If you have access to the OpenAI models through Azure,
    you will need to make minor modifications to your code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although it would be simpler to connect to a single model, we are already going
    to show a simple but powerful Microsoft Semantic Kernel feature: we’re going to
    connect to two different models and run a simple prompt using the simpler but
    less expensive model, GPT-3.5, and a more complex prompt on the more advanced
    but also more expensive model, GPT-4.'
  prefs: []
  type: TYPE_NORMAL
- en: This process of sending simpler requests to simpler models and more complex
    requests to more complex models is something that you will frequently do when
    creating your own applications. This approach is called **LLM cascade**, and it
    was popularized in the FrugalGPT [3] paper. It can result in substantial cost
    savings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Important: Order matters'
  prefs: []
  type: TYPE_NORMAL
- en: The order in which you load your services matters. Both for Python (*step 3*
    in the *Connecting to OpenAI Services using Python* section) and for C# (*step
    4* in the *Connecting to OpenAI Services using C#* section), we are going to first
    load the GPT-3.5 model into the kernel, followed by the GPT-4 model. This will
    make GPT-3.5 the default model. Later, we will specify which model will be used
    for which command; if we don’t, GPT-3.5 will be used. If you load GPT-4 first,
    you will incur more costs.
  prefs: []
  type: TYPE_NORMAL
- en: We assume that you are using the OpenAI service instead of the Azure OpenAI
    service. You will need your OpenAI key and the organization ID, which can be found
    under **Settings** in the left menu of [https://platform.openai.com/account/api-keys](https://platform.openai.com/account/api-keys).
    All examples work with Azure OpenAI; you just need to use the Azure connection
    information instead of the OpenAI connection information.
  prefs: []
  type: TYPE_NORMAL
- en: Connecting to OpenAI Services using Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section assumes that you are using the OpenAI service. Before connecting
    to the OpenAI service, create a .env file in the ch1 directory containing your
    OpenAI key and your OpenAI organization ID. The organization ID can be found under
    Settings in the left menu of [https://platform.openai.com/account/api-keys](https://platform.openai.com/account/api-keys).
  prefs: []
  type: TYPE_NORMAL
- en: 'Your .env file should look like this, with the appropriate values replacing
    x in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'To connect to the OpenAI service using Python, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load an empty kernel:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load your API key and Organization ID into variables using the `openai_settings_from_dot_env`
    method from the `semantic_kernel_utils.settings` package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `OpenAIChatCompletion` method to create connections to chat services:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If you are using OpenAI through Azure, instead of using `OpenAIChatCompletion`,
    you need to use `AzureOpenAIChatCompletion`, as shown here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Your Semantic Kernel is now ready to make calls, which we will do in the *Running
    a simple* *prompt* section.
  prefs: []
  type: TYPE_NORMAL
- en: Connecting to OpenAI services using C#
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before connecting to the OpenAI service, create a `config.json` file in the
    `ch1/config` directory, containing your OpenAI key and your OpenAI organization
    ID.
  prefs: []
  type: TYPE_NORMAL
- en: 'To avoid keeping a key in your code, we will load your keys from a configuration
    file. Your `config/settings.json` file should look like the following example,
    with the appropriate values in the `apiKey` and `orgId` fields (`orgId` is optional.
    If you don’t have `orgId`, delete the field. An empty string will not work):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'To connect to the OpenAI service in C#, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we’re going to reuse the API Key and Organization ID a lot, create a
    class to load them in `Settings.cs`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code is boilerplate code to read a JSON file and load its attributes
    into C# variables. We’re looking for two attributes: `apiKey` and `orgID`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Load the settings from `config/settings.json`. We’re going to create a class
    that will make this easier, as we are going to be doing this a lot. The class
    is very simple. It first checks whether the configuration file exists, and if
    it does, the class uses the JSON deserializer to load its contents into the `apiKey`
    and `orgId` variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Next, use the `OpenAIChatCompletion` method to create connections to chat services.
    Note that we’re using `serviceID` to have a shortcut name for the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After you load the components, build the kernel with the `Build` method:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If you are using OpenAI through Azure, instead of using `AddOpenAIChatCompletion`,
    you need to use `AddAzureOpenAIChatCompletion`, as shown here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Your Semantic Kernel is now ready to make calls, which we will do in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Running a simple prompt
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section assumes you completed the prior sections and builds upon the same
    code. By now, you should have instantiated Semantic Kernel and loaded both the
    GPT-3.5 and the GPT-4 services into it in that order. When you submit a prompt,
    it will default to the first service, and will run the prompt on GPT-3.5.
  prefs: []
  type: TYPE_NORMAL
- en: When we send the prompt to the service, we will also send a parameter called
    `0.0` to `1.0`, and it controls how random the responses are. We’re going to explain
    the temperature parameter in more detail in later chapters. A temperature parameter
    of `0.8` generates a more creative response, while a temperature parameter of
    `0.2` generates a more precise response.
  prefs: []
  type: TYPE_NORMAL
- en: To send the prompt to the service, we will use a method called `create_semantic_function`.
    For now, don’t worry about what a semantic function is. We’re going to explain
    it in the *Using generative AI to solve simple* *problems* section.
  prefs: []
  type: TYPE_NORMAL
- en: Running a simple prompt in Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To run a prompt in Python, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the prompt in a string variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a function by using the `add_function` method of the kernel. The `function_name`
    and `plugin_name` parameters are required but are not used, so you can give your
    function and plugin whatever name you want:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Call the function. Note that all invocation methods are asynchronous, so you
    need to use `await` to wait for their return:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the response:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The response is nondeterministic. Here’s a possible response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Running a simple prompt in C#
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To run a prompt in C#, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the prompt in a string variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Call the prompt by using the `Kernel.InvokePromptAsync` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the response:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The response is nondeterministic. Here’s a possible response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: We have now connected to an AI service, submitted a prompt to it, and obtained
    a response. We’re now ready to start creating our own functions.
  prefs: []
  type: TYPE_NORMAL
- en: Using generative AI to solve simple problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Microsoft Semantic Kernel distinguishes between two types of functions that
    can be loaded into it: **semantic functions** and **native functions**.'
  prefs: []
  type: TYPE_NORMAL
- en: Semantic functions are functions that connect to AI services, usually LLMs,
    to perform a task. The service is not part of your codebase. Native functions
    are regular functions written in the language of your application.
  prefs: []
  type: TYPE_NORMAL
- en: The reason to differentiate a native function from any other regular function
    in your code is that the native function will have additional attributes that
    will tell the kernel what it does. When you load a native function into the kernel,
    you can use it in chains that combine native and semantic functions. In addition,
    Semantic Kernel planner can use the function when creating a plan to achieve a
    user goal.
  prefs: []
  type: TYPE_NORMAL
- en: Creating semantic functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have already created a semantic function (`knock`) in the previous section.
    Now, we’re going to add a parameter to it. The default parameter for all semantic
    functions is called `{{$input}}`.
  prefs: []
  type: TYPE_NORMAL
- en: Modified semantic function in Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We’re going to make minor modifications to our previous code to allow the semantic
    function to receive a parameter. Again, the following code assumes that you have
    already instantiated a kernel and connected to at least one service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The only differences from the code before are that now we have a variable, `{{$input}}`,
    and we’re calling the function using a parameter, the string `"Boo"`. To add the
    variable, we need to import the `KernelArguments` class from the `semantic_kernel_functions.kernel_arguments`
    package and create an instance of the object with the value we want.
  prefs: []
  type: TYPE_NORMAL
- en: 'The answer is nondeterministic. Here’s a possible answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Modified semantic function in C#
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To create a function in C#, we are going to use the CreateFunctionFromPrompt
    kernel method, and to add a parameter, we will use the KernelArguments object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Here, too, the only differences from the code before are that now we have a
    variable, `{{$input}}`, and we’re calling the function using a parameter, the
    string `"Boo"`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The answer is nondeterministic. Here’s a possible answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Creating native functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Native functions are created in the same language your application is using.
    For example, if you are writing code in Python, a native function can be written
    in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Although you can call a native function directly without loading it into the
    kernel, loading makes it available to the planner, which we will see in the last
    section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: We’re going to explore native functions in greater detail in [*Chapter 3*](B21826_03.xhtml#_idTextAnchor071),
    but for now, let’s create and load a simple native function in the Kernel.
  prefs: []
  type: TYPE_NORMAL
- en: The native function we’re going to create chooses a theme for a joke. For now,
    the themes are `Boo`, `Dishes`, `Art`, `Needle`, `Tank`, and `Police`, and the
    function simply returns one of these themes at random.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a native function in Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In Python, the native functions need to be inside a class. The class used to
    be called a **skill**, and in some places, this name is still used. The name has
    recently changed to **plugin**. A plugin (formerly called skill) is just a collection
    of functions. You cannot mix native and semantic functions in the same skill.
  prefs: []
  type: TYPE_NORMAL
- en: We’re going to name our class `ShowManager`.
  prefs: []
  type: TYPE_NORMAL
- en: To create a native function, you will use the `@kernel_function` decorator.
    The decorator must contain fields for `description` and `name`. To add a decorator,
    you must import `kernel_function` from the `semantic_kernel.functions.kernel_function_decorator`
    package.
  prefs: []
  type: TYPE_NORMAL
- en: 'The function body comes immediately after the decorator. In our case, we are
    simply going to have the list of themes and use the `random.choice` function to
    return one random element from the list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, to load the plugin and all its functions in the kernel, we use the `add_plugin`
    method of the kernel. When you are adding a plugin, you need to give it a name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'To call the native function from a plugin, simply put the name of the function
    within brackets, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The function is not deterministic, but a possible result might be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Creating a native function in C#
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In C#, native functions need to be inside a class. The class used to be called
    a skill, and this name is still used in some places; for example, in the SDK,
    we will need to import `Microsoft.SemanticKernel.SkillDefinition`. Skills have
    recently been renamed to plugins. A plugin is just a collection of functions.
    You cannot mix native and semantic functions in the same skill.
  prefs: []
  type: TYPE_NORMAL
- en: We’re going to name our class `ShowManager`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a native function, you will use the `[KernelFunction]` decorator.
    The decorator must contain `Description`. The function body comes immediately
    after the decorator. In our case, we are simply going to have a list of themes
    and use the `Random().Next` method to return one random element from the list.
    We will call our class `ShowManager` and our function `RandomTheme`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, to load the plugin and all its functions into the kernel, we use the
    `ImportPluginFromObject` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'To call the native function from a plugin, simply put the function name within
    brackets. You can pass parameters by using the `KernelArguments` class, as shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The function is not deterministic, but a possible result might be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Now that you can run simple prompts from your code, let’s learn to separate
    the prompt configuration from the code that calls it by using plugins.
  prefs: []
  type: TYPE_NORMAL
- en: Plugins
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the greatest strengths of Microsoft Semantic Kernel is that you can create
    semantic plugins that are language agnostic. Semantic plugins are collections
    of semantic functions that can be imported into the kernel. Creating semantic
    plugins allows you to separate your code from the AI function, which makes your
    application easier to maintain. It also allows other people to work on the prompts,
    making it easier to implement prompt engineering, which will be explored in [*Chapter
    2*](B21826_02.xhtml#_idTextAnchor045).
  prefs: []
  type: TYPE_NORMAL
- en: 'Each function is defined by a directory containing two text files: `config.json`,
    which contains the configuration for the semantic function, and `skprompt.txt`,
    which contains its prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: The configuration of the semantic function includes the preferred engine to
    use, the temperature parameter, and a description of what the semantic function
    does and its inputs.
  prefs: []
  type: TYPE_NORMAL
- en: The text file contains the prompt that will be sent to the AI service to generate
    the response.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we are going to define a plugin that contains two semantic
    functions. The first semantic function is a familiar function: the knock-knock
    joke generator. The second function is a function that receives a joke as an input
    and tries to explain why it’s funny. Since this is a more complicated task, we’re
    going to use GPT-4 for this.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: We will now see how to create the `config.json` and `skprompt.txt` files and
    how to load the plugin into our program.
  prefs: []
  type: TYPE_NORMAL
- en: The config.json file for the knock-knock joke function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following configuration file shows a possible configuration for the semantic
    function that generates knock-knock jokes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: The `default_services` property is an array of the preferred engines to use
    (in order). Since knock-knock jokes are simple, we’re going to use GPT-3.5 for
    it. All the parameters in the preceding file are required. In future chapters,
    we will explain each parameter in detail, but for now, you should just copy them.
  prefs: []
  type: TYPE_NORMAL
- en: The `description` field is important because it can be used later by the planner,
    which will be explained in the last section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The skprompt.txt file for the knock-knock joke function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since we want to explain the joke later, we need our application to return
    the whole joke, not only the punchline. This will enable us to save the whole
    joke and pass it as a parameter to the explain-the-joke function. To do so, we
    need to modify the prompt. You can see the final prompt here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The config.json file for the semantic function that explains jokes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You should now create a file for the function that explains jokes. Since this
    is a more complicated task, we should set `default_services` to use GPT-4.
  prefs: []
  type: TYPE_NORMAL
- en: 'This file is almost exactly the same as the `config.json` file used for the
    knock-knock joke function. We have made only three changes:'
  prefs: []
  type: TYPE_NORMAL
- en: The description
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The description of the `input` variable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `default_services` field
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This can be seen in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The skprompt.txt file for the explain joke function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The prompt for the function that explains jokes is very simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Loading the plugin from a directory into the kernel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that the semantic functions are defined in text files, you can load them
    into the kernel by simply pointing to the directory where they are. This can also
    help you to separate the prompt engineering function from the development function.
    Prompt engineers can work with the text files without ever having to touch the
    code of your application.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the plugin using Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can load all the functions inside a plugin directory using the `add_plugin`
    method from the kernel object. Just set the first parameter to `None` and set
    the `parent_directory` parameter to the directory where the plugin is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'You can call the functions in the same way as you would call a function from
    a native plugin by putting the function name within brackets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of the preceding call is nondeterministic. Here’s a sample result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'We can pass the results of the preceding call to the `explain_joke` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Remember that this function is configured to use GPT-4\. The results of this
    function are nondeterministic. Here’s a sample result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Loading the plugin using C#
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can load all the functions inside a plugin directory. First, we obtain
    the path to the directory (your path may be different):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we use `ImportPluginFromPromptDirectory` to load the functions into a
    variable. The result is a collection of functions. You can access them by referencing
    them inside brackets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'The last step is to call the function. To call it, we use the `InvokeAsync`
    method of the kernel object. We will, again, pass a parameter using the `KernelArguments`
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of the preceding call is nondeterministic. Here’s a sample result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'To get an explanation, we can pass the results of the preceding call to the
    `explain_joke` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s a sample result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Now that you have seen how to create and call one function of a plugin, we are
    going to learn how to use a planner to call multiple functions from different
    plugins.
  prefs: []
  type: TYPE_NORMAL
- en: Using a planner to run a multistep task
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Instead of calling functions yourself, you can let Microsoft Semantic Kernel
    choose the functions for you. This can make your code a lot simpler and can give
    your users the ability to combine your code in ways that you haven’t considered.
  prefs: []
  type: TYPE_NORMAL
- en: Right now, this will not seem very useful because we only have a few functions
    and plugins. However, in a large application, such as Microsoft Office, you may
    have hundreds or even thousands of plugins, and your users may want to combine
    them in ways that you can’t yet imagine. For example, you may be creating a copilot
    that helps a user be more efficient when learning about a subject, so you write
    a function that downloads the latest news about that subject from the web. You
    may also have independently created a function that explains a piece of text to
    the user so that the user can paste content to learn more about it. The user may
    decide to combine them both with “*download the news and write an article explaining
    them to me*,” something that you never thought about and didn’t add to your code.
    Semantic Kernel will understand that it can call the two functions you wrote in
    sequence to complete that task.
  prefs: []
  type: TYPE_NORMAL
- en: When you let users request their own tasks, they will use natural language,
    and you can let Semantic Kernel inspect all the functions that are loaded into
    it and use a planner to decide the best way of handling the user request.
  prefs: []
  type: TYPE_NORMAL
- en: 'For now, we are only going to show a quick example of using a planner, but
    we will explore the topic in more depth in [*Chapter 5*](B21826_05.xhtml#_idTextAnchor106).
    Planners are still under active development, and there might be changes over time.
    Currently, Semantic Kernel is expected to have two planners: the **Function Calling
    Stepwise planner**, available for Python and C#, and the **Handlebars planner**,
    available only for C# at the time of writing.'
  prefs: []
  type: TYPE_NORMAL
- en: Although the following example is very simple and both planners behave in the
    same way, we will show how to use the Stepwise planner (Function Calling Stepwise
    Planner) with Python and the Handlebars planner with C#.
  prefs: []
  type: TYPE_NORMAL
- en: Calling the Function Calling Stepwise planner with Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To use the Stepwise planner, we first create an object of the `FunctionCallingStepwisePlanner`
    class and make a request to it. In our case, we’re going to ask it to choose a
    random theme, create a knock-knock joke, and explain it.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’re going to modify our earlier program, delete the function calls, and add
    a call to the planner instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'There are a couple of details to note. The first one is that I used the class
    `FunctionCallingStepwisePlannerOptions` to pass a `max_tokens` parameter to the
    planner. Behind the scenes, the planner will create a prompt and send it to the
    AI service. The default `max_tokens` for most AI services tends to be small. At
    the time of writing, it was `250`, which may cause an error if the prompt generated
    by the planner is too large. The second detail to note is that I printed `result.final_answer`
    instead of `result`. The `result` variable contains the whole plan: the definition
    of the functions, the chat with the OpenAI model explaining how to proceed, etc.
    It’s interesting to print the `result` variable to see how the planner works internally,
    but to see the outcome of the planner execution, all you need to do is print `result.final_answer`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a sample response, first telling the joke and then explaining it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the planner generated the joke and the explanation, as expected,
    without us needing to tell Semantic Kernel in which order to call the functions.
  prefs: []
  type: TYPE_NORMAL
- en: Calling the Handlebars planner in C#
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At the time of writing, the Handlebars planner is in version 1.0.1-preview,
    and it’s still experimental in C#, although it’s likely that a release version
    will be made available soon.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use the Handlebars planner, you first need to install it, which you can
    do by using the following command (you should use the latest version available
    to you):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'To use the Handlebars planner, you need to use the following `pragma` warning
    in your code. The Handlebars planner code is still experimental, and if you don’t
    add the `#pragma` directive, your code will fail, with a warning that it contains
    experimental code. You also need to import the `Microsoft.SemanticKernel.Planning.Handlebars`
    package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'We proceed as usual, instantiating our kernel and adding native and semantic
    functions to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'The big difference happens now – instead of telling which functions to call
    and how, we simply ask the planner to do what we want:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'We can execute the plan by calling `InvokeAsync` from the `plan` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is nondeterministic. Here is a sample result, first telling the
    joke and then explaining it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the planner generated the joke and the explanation, as expected,
    without us needing to tell Semantic Kernel in which order to call the functions.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about Generative AI and the main components of
    Microsoft Semantic Kernel. You learned how to create a prompt and submit it to
    a service and how to embed that prompt into a semantic function. You also learned
    how to execute multistep requests by using a planner.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to learn how to make our prompts better through
    a topic called **prompt engineering**. This will help you create prompts that
    get your users the correct result faster and use fewer tokens, therefore reducing
    costs.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] A. Vaswani et al., “Attention Is All You Need,” Jun. 2017.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] OpenAI, “GPT-4 Technical Report.” arXiv, Mar. 27, 2023\. doi: 10.48550/arXiv.2303.08774.'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] L. Chen, M. Zaharia, and J. Zou, “FrugalGPT: How to Use Large Language
    Models While Reducing Cost and Improving Performance.” arXiv, May 09, 2023\. doi:
    10.48550/arXiv.2305.05176.'
  prefs: []
  type: TYPE_NORMAL
