<html><head></head><body>
		<div id="_idContainer057">
			<h1 id="_idParaDest-230" class="chapter-number"><a id="_idTextAnchor229"/><st c="0">11</st></h1>
			<h1 id="_idParaDest-231"><a id="_idTextAnchor230"/><st c="3">Using LangChain to Get More from RAG</st></h1>
			<p><st c="40">We have mentioned </st><strong class="bold"><st c="59">LangChain</st></strong><st c="68"> several</st><a id="_idIndexMarker724"/><st c="76"> times already, and we have shown you a lot of LangChain code, including code that implements the LangChain-specific language: </st><strong class="bold"><st c="203">LangChain Expression Language</st></strong><st c="232"> (</st><strong class="bold"><st c="234">LCEL</st></strong><st c="238">). </st><st c="242">Now that you are familiar with</st><a id="_idIndexMarker725"/><st c="272"> different ways to implement </st><strong class="bold"><st c="301">retrieval-augmented generation</st></strong><st c="331"> (</st><strong class="bold"><st c="333">RAG</st></strong><st c="336">) with LangChain, we thought now would be a good time to dive more into the various capabilities of LangChain that you can use to make your RAG </st><span class="No-Break"><st c="481">pipeline better.</st></span></p>
			<p><st c="497">In this chapter, we explore lesser-known but highly important components in LangChain that can enhance a RAG application. </st><st c="620">We will cover </st><span class="No-Break"><st c="634">the following:</st></span></p>
			<ul>
				<li><st c="648">Document loaders for loading and processing documents from </st><span class="No-Break"><st c="708">different sources</st></span></li>
				<li><st c="725">Text splitters for dividing documents into chunks suitable </st><span class="No-Break"><st c="785">for retrieval</st></span></li>
				<li><st c="798">Output parsers for structuring the responses from the </st><span class="No-Break"><st c="853">language model</st></span></li>
			</ul>
			<p><st c="867">We will use different code labs to step through examples of each type of component, starting with </st><span class="No-Break"><st c="966">document loaders.</st></span></p>
			<h1 id="_idParaDest-232"><a id="_idTextAnchor231"/><st c="983">Technical requirements</st></h1>
			<p><st c="1006">The code for this chapter is placed in the following GitHub </st><span class="No-Break"><st c="1067">repository: </st></span><a href="https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_11 "><span class="No-Break"><st c="1079">https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_11</st></span></a></p>
			<p><st c="1176">Individual file names for each code lab are mentioned in the </st><span class="No-Break"><st c="1238">respective sections.</st></span></p>
			<h1 id="_idParaDest-233"><a id="_idTextAnchor232"/><st c="1258">Code lab 11.1 – Document loaders</st></h1>
			<p><st c="1291">The file you need to </st><a id="_idIndexMarker726"/><st c="1313">access from the GitHub repository is </st><span class="No-Break"><st c="1350">titled </st></span><span class="No-Break"><strong class="source-inline"><st c="1357">CHAPTER11-1_DOCUMENT_LOADERS.ipynb</st></strong></span><span class="No-Break"><st c="1391">.</st></span></p>
			<p><st c="1392">Document loaders play a key role in accessing, extracting, and pulling in the data that makes our RAG application function. </st><st c="1517">Document loaders are used to load and process documents from various sources such as text files, PDFs, web pages, or databases. </st><st c="1645">They convert the documents into a format suitable for indexing </st><span class="No-Break"><st c="1708">and retrieval.</st></span></p>
			<p><st c="1722">Let’s install some</st><a id="_idIndexMarker727"/><st c="1741"> new packages to support our document loading, which, as you might have guessed, involves some different file </st><span class="No-Break"><st c="1851">format-related packages:</st></span></p>
			<pre class="source-code"><st c="1875">
%pip install bs4
%pip install python-docx
%pip install docx2txt
%pip install jq</st></pre>
			<p><st c="1955">The first one may look familiar, </st><strong class="source-inline"><st c="1989">bs4</st></strong><st c="1992"> (for Beautiful Soup 4), as we used it in </st><a href="B22475_02.xhtml#_idTextAnchor035"><span class="No-Break"><em class="italic"><st c="2034">Chapter 2</st></em></span></a><st c="2043"> for parsing HTML. </st><st c="2062">We also have a couple of Microsoft Word-related packages, such as </st><strong class="source-inline"><st c="2128">python_docx</st></strong><st c="2139">, which helps with creating and updating Microsoft Word (</st><strong class="source-inline"><st c="2196">.docx</st></strong><st c="2201">) files, and </st><strong class="source-inline"><st c="2215">docx2txt</st></strong><st c="2223">, which extracts text and images from </st><strong class="source-inline"><st c="2261">.docx</st></strong><st c="2266"> files. </st><st c="2274">The </st><strong class="source-inline"><st c="2278">jq</st></strong><st c="2280"> package is a lightweight </st><span class="No-Break"><st c="2306">JSON processor.</st></span></p>
			<p><st c="2321">Next, we are going to take an extra step you likely will not have to take in a </st><em class="italic"><st c="2401">real</st></em><st c="2405"> situation, which is turning our PDF document into a bunch of other formats, so we can test out the extraction of those formats. </st><st c="2534">We are going to add a whole new </st><strong class="source-inline"><st c="2566">document loaders</st></strong><st c="2582"> section to our code right after the </st><span class="No-Break"><st c="2619">OpenAI setup.</st></span></p>
			<p><st c="2632">In this section, we will provide code to generate the files, and then the different document loaders and their related packages to extract data from those types of files. </st><st c="2804">Right now, we have a PDF version of our document. </st><st c="2854">We will need an HTML/web version, a Microsoft Word version, and a JSON version of </st><span class="No-Break"><st c="2936">our document.</st></span></p>
			<p><st c="2949">We are going to start with a new cell under the OpenAI setup cell, where we will import the new packages we need for </st><span class="No-Break"><st c="3067">these conversions:</st></span></p>
			<pre class="source-code"><st c="3085">
from bs4 import BeautifulSoup
import docx
import json</st></pre>
			<p><st c="3139">As we mentioned, the </st><strong class="source-inline"><st c="3161">BeautifulSoup</st></strong><st c="3174"> package helps us parse HTML-based web pages. </st><st c="3220">We also import </st><strong class="source-inline"><st c="3235">docx</st></strong><st c="3239">, which represents the Microsoft Docx word processing format. </st><st c="3301">Lastly, we import </st><strong class="source-inline"><st c="3319">json</st></strong><st c="3323"> to interpret and manage </st><strong class="source-inline"><st c="3348">json</st></strong> <span class="No-Break"><st c="3352">formatted code.</st></span></p>
			<p><st c="3368">Next, we want to establish the filenames we will save each of </st><span class="No-Break"><st c="3431">our formats:</st></span></p>
			<pre class="source-code"><st c="3443">
pdf_path = "google-2023-environmental-report.pdf"
html_path = "google-2023-environmental-report.html"
word_path = "google-2023-environmental-report.docx"
json_path = "google-2023-environmental-report.json"</st></pre>
			<p><st c="3649">Here, we are </st><a id="_idIndexMarker728"/><st c="3663">defining the paths for each of the files that we use in this code, and later when we use the loaders to load each document. </st><st c="3787">These are going to be the final files that we generate from the original PDF document we have </st><span class="No-Break"><st c="3881">been using.</st></span></p>
			<p><st c="3892">And then this key part of our new code will extract the text from the PDF and use it to generate all of these new types </st><span class="No-Break"><st c="4013">of documents:</st></span></p>
			<pre class="source-code"><st c="4026">
with open(pdf_path, "rb") as pdf_file:
    pdf_reader = PdfReader(pdf_file)
    pdf_text = "".join(
        page.extract_text() for page in pdf_reader.pages)
    soup = BeautifulSoup("&lt;html&gt;&lt;body&gt;&lt;/body&gt;&lt;/html&gt;",
                         "html.parser")
    soup.body.append(pdf_text)
    with open(html_path, "w",
              encoding="utf-8") as html_file:
                  html_file.write(str(soup))
                  doc = docx.Document()
                  doc.add_paragraph(pdf_text)
                  doc.save(word_path)
    with open(json_path, "w") as json_file:
        json.dump({"text": pdf_text}, json_file)</st></pre>
			<p><st c="4497">We generate an HTML, Word, and JSON version of our document in a very basic sense. </st><st c="4581">If you were generating these documents to actually use in a pipeline, we recommend applying more formatting and extraction, but for the purposes of this demonstration, this will provide us with the </st><span class="No-Break"><st c="4779">necessary data.</st></span></p>
			<p><st c="4794">Next, we are going</st><a id="_idIndexMarker729"/><st c="4813"> to add our document loaders under the indexing stage of our code. </st><st c="4880">We have worked with the first two document loaders already, which we will show in this code lab, but updated so that they can be used interchangeably. </st><st c="5031">For each document loader, we show what package imports are specific to that loader alongside the loader code. </st><st c="5141">In the early chapters, we used a web loader that loaded directly from a website, so if that is a use case you have, refer to that document loader. </st><st c="5288">In the meantime, we are sharing a slightly different type of document loader here that is focused on using local HTML files, such as the one we just generated. </st><st c="5448">Here is the code for this </st><span class="No-Break"><st c="5474">HTML loader:</st></span></p>
			<pre class="source-code"><st c="5486">
from langchain_community.document_loaders import BSHTMLLoader
loader = BSHTMLLoader(html_path)
docs = loader.load()</st></pre>
			<p><st c="5602">Here, we use the HTML file we defined earlier to load the code from an HTML document. </st><st c="5689">The final variable, </st><strong class="source-inline"><st c="5709">docs</st></strong><st c="5713">, can be used interchangeably with any other </st><em class="italic"><st c="5758">docs</st></em><st c="5762"> we define in the following document loaders. </st><st c="5808">The way this code works, you can only use one loader at a time, and it will replace the docs with its version of the docs (including a metadata source tag of what document it came from). </st><st c="5995">If you run this cell and then skip down to run the splitter cell, you can run the remaining code in the lab and see similar results from what is the same data coming from different source file types. </st><st c="6195">We did have to make a slight update later in the code, which we will note in </st><span class="No-Break"><st c="6272">a moment.</st></span></p>
			<p><st c="6281">There are some alternative HTML loaders listed on the LangChain website that you can </st><span class="No-Break"><st c="6367">see here:</st></span></p>
			<p><a href="https://python.langchain.com/v0.2/docs/how_to/document_loader_html/ "><span class="No-Break"><st c="6376">https://python.langchain.com/v0.2/docs/how_to/document_loader_html/</st></span></a></p>
			<p><st c="6444">The next file type we will talk about is the other type we have been working with already, </st><span class="No-Break"><st c="6536">the PDF:</st></span></p>
			<pre class="source-code"><st c="6544">
from PyPDF2 import PdfReader
docs = []
with open(pdf_path, "rb") as pdf_file:
    pdf_reader = PdfReader(pdf_file)
    pdf_text = "".join(page.extract_text() for page in
               pdf_reader.pages)
    docs = [Document(page_content=page) for page in
            pdf_text.split("\n\n")]</st></pre>
			<p><st c="6796">Here, we have a </st><a id="_idIndexMarker730"/><st c="6813">slightly more streamlined version of the code we have used previously to extract the data from the PDF. </st><st c="6917">Using this new approach shows you an alternative way to access this data, but either will work for you in your code, ultimately loading up the docs with the data pulled from the PDF using </st><strong class="source-inline"><st c="7105">PdfReader</st></strong> <span class="No-Break"><st c="7114">from </st></span><span class="No-Break"><strong class="source-inline"><st c="7120">PyPDF2</st></strong></span><span class="No-Break"><st c="7126">.</st></span></p>
			<p><st c="7127">It should be noted that there are numerous and very capable ways to load PDF documents into LangChain, which is supported by many integrations with popular tools for PDF extraction. </st><st c="7310">Here are a few: </st><strong class="source-inline"><st c="7326">PyPDF2</st></strong><st c="7332"> (what we use here), </st><strong class="source-inline"><st c="7353">PyPDF</st></strong><st c="7358">, </st><strong class="source-inline"><st c="7360">PyMuPDF</st></strong><st c="7367">, </st><strong class="source-inline"><st c="7369">MathPix</st></strong><st c="7376">, </st><strong class="source-inline"><st c="7378">Unstructured</st></strong><st c="7390">, </st><strong class="source-inline"><st c="7392">AzureAIDocumentIntelligenceLoader</st></strong><st c="7425">, </st><span class="No-Break"><st c="7427">and </st></span><span class="No-Break"><strong class="source-inline"><st c="7431">UpstageLayoutAnalysisLoader</st></strong></span><span class="No-Break"><st c="7458">.</st></span></p>
			<p><st c="7459">We recommend you look at the latest list of PDF document loaders. </st><st c="7526">LangChain provides a helpful set of tutorials for many of </st><span class="No-Break"><st c="7584">them here:</st></span></p>
			<p><a href="https://python.langchain.com/v0.2/docs/how_to/document_loader_pdf/ "><span class="No-Break"><st c="7594">https://python.langchain.com/v0.2/docs/how_to/document_loader_pdf/</st></span></a></p>
			<p><st c="7661">Next, we will load the data from a Microsoft </st><span class="No-Break"><st c="7707">Word document:</st></span></p>
			<pre class="source-code"><st c="7721">
from langchain_community.document_loaders import Docx2txtLoader
loader = Docx2txtLoader(word_path)
docs = loader.load()</st></pre>
			<p><st c="7841">This code uses the </st><strong class="source-inline"><st c="7861">Docx2txtLoader</st></strong><st c="7875"> document loader from LangChain to turn the Word document we previously generated into text and load it up into our </st><strong class="source-inline"><st c="7991">docs</st></strong><st c="7995"> variable that can be later used by the splitter. </st><st c="8045">Again, stepping through the rest of the code will work with this data, just </st><a id="_idIndexMarker731"/><st c="8121">as it did with the HTML or PDF documents. </st><st c="8163">There are many options for loading Word documents as well, which you can find listed </st><span class="No-Break"><st c="8248">here: </st></span><a href="https://python.langchain.com/v0.2/docs/integrations/document_loaders/microsoft_word/ "><span class="No-Break"><st c="8254">https://python.langchain.com/v0.2/docs/integrations/document_loaders/microsoft_word/</st></span></a></p>
			<p><st c="8338">Lastly, we see a similar approach with the </st><span class="No-Break"><st c="8382">JSON loader:</st></span></p>
			<pre class="source-code"><st c="8394">
from langchain_community.document_loaders import JSONLoader
loader = JSONLoader(
    file_path=json_path,
    jq_schema='.text',
)
docs = loader.load()</st></pre>
			<p><st c="8538">Here, we use a JSON loader to load data that was stored in a JSON object format, but the results are the same: a </st><strong class="source-inline"><st c="8652">docs</st></strong><st c="8656"> variable that can be passed to the splitter and converted into the format we use throughout the remaining code. </st><st c="8769">Other options for JSON loaders can be </st><span class="No-Break"><st c="8807">found here:</st></span></p>
			<p><a href="https://python.langchain.com/v0.2/docs/how_to/document_loader_json/ "><span class="No-Break"><st c="8818">https://python.langchain.com/v0.2/docs/how_to/document_loader_json/</st></span></a></p>
			<p><st c="8886">Note that some document loaders add additional metadata to the </st><strong class="source-inline"><st c="8950">metadata</st></strong><st c="8958"> dictionary within the </st><strong class="source-inline"><st c="8981">Document</st></strong><st c="8989"> objects that are generated during this process. </st><st c="9038">This is causing some issues with our code when we add our own metadata. </st><st c="9110">To fix this, we update these lines when we index and create the </st><span class="No-Break"><st c="9174">vector store:</st></span></p>
			<pre class="source-code"><st c="9187">
dense_documents = [Document(page_content=doc.page_content,
    metadata={"id": str(i), "search_source": "dense"}) for
        i, doc in enumerate(splits)]
sparse_documents = [Document(page_content=doc.page_content,
    metadata={"id": str(i), "search_source": "sparse"}) for
        i, doc in enumerate(splits)]</st></pre>
			<p><st c="9475">We also update the code in the final output to test the response, changing the second line in this code to handle the changed </st><span class="No-Break"><strong class="source-inline"><st c="9602">metadata</st></strong></span><span class="No-Break"><st c="9610"> tag:</st></span></p>
			<pre class="source-code"><st c="9615">
for i, doc in enumerate(retrieved_docs, start=1):
    print(f"Document {i}: Document ID: {doc.metadata['id']}
    source: {doc.metadata['source']}")
    print(f"Content:\n{doc.page_content}\n")</st></pre>
			<p><st c="9797">Run each loader</st><a id="_idIndexMarker732"/><st c="9813"> and then run the remaining code to see each document in action! </st><st c="9878">There are numerous more integrations with third parties, allowing you to access just about any data source you can imagine and format that data in a way that you can better utilize LangChain’s other components. </st><st c="10089">Take a look at more examples here on the LangChain </st><span class="No-Break"><st c="10140">website: </st></span><a href="https://python.langchain.com/docs/modules/data_connection/document_loaders/ "><span class="No-Break"><st c="10149">https://python.langchain.com/docs/modules/data_connection/document_loaders/</st></span></a></p>
			<p><st c="10224">Document loaders play a supporting and very important role in your RAG application. </st><st c="10309">But for RAG-specific applications that typically utilize </st><em class="italic"><st c="10366">chunks</st></em><st c="10372"> of your data, document loaders are not nearly as useful until you pass them through a text splitter. </st><st c="10474">Next, we will review text splitters and how each one can be used to improve your </st><span class="No-Break"><st c="10555">RAG application.</st></span></p>
			<h1 id="_idParaDest-234"><a id="_idTextAnchor233"/><st c="10571">Code lab 11.2 – Text splitters</st></h1>
			<p><st c="10602">The file you need to access from the GitHub repository is </st><span class="No-Break"><st c="10661">titled </st></span><span class="No-Break"><strong class="source-inline"><st c="10668">CHAPTER11-2_TEXT_SPLITTERS.ipynb</st></strong></span><span class="No-Break"><st c="10700">.</st></span></p>
			<p><st c="10701">Text splitters </st><a id="_idIndexMarker733"/><st c="10717">split a document into chunks that can be used for retrieval. </st><st c="10778">Larger documents pose a threat to many parts of our RAG application and the splitter is our first line of defense. </st><st c="10893">If you were able to vectorize a very large document, the larger the document, the more context representation you will lose in the vector embedding. </st><st c="11042">But this assumes you can even vectorize a very large document, which you often can’t! </st><st c="11128">Most embedding models have relatively small limits on the size of documents we can pass to it compared to the large documents many of us work with. </st><st c="11276">For example, the context length for the OpenAI model we are using to generate our embeddings is 8,191 tokens. </st><st c="11386">If we try to pass a document larger than that to the model, it will generate an error. </st><st c="11473">These are the main reasons splitters exist, but these are not the only complexities introduced with this step in </st><span class="No-Break"><st c="11586">the process.</st></span></p>
			<p><st c="11598">The key element </st><a id="_idIndexMarker734"/><st c="11615">of text splitters for us to consider is how they split the text. </st><st c="11680">Let’s say you have 100 paragraphs that you want to split up. </st><st c="11741">In some cases, there may be two or three that are semantically meant to be together, such as the paragraphs in this one section. </st><st c="11870">In some cases, you may have a section title, a URL, or some other type of text. </st><st c="11950">Ideally, you want to keep the semantically related pieces of text together, but this can be much more complex than it first seems! </st><st c="12081">For a real-world example of this, go to this website and </st><a id="_idIndexMarker735"/><st c="12138">copy in a large set of </st><span class="No-Break"><st c="12161">text: </st></span><a href="https://chunkviz.up.railway.app/"><span class="No-Break"><st c="12167">https://chunkviz.up.railway.app/</st></span></a><span class="No-Break"><st c="12199">.</st></span></p>
			<p><st c="12200">ChunkViz is a </st><a id="_idIndexMarker736"/><st c="12215">utility created by Greg Kamradt that helps you visualize how your text splitter is working. </st><st c="12307">Change the parameters for the splitters to use what we are using: a chunk size of </st><strong class="source-inline"><st c="12389">1000</st></strong><st c="12393"> and a chunk overlap of </st><strong class="source-inline"><st c="12417">200</st></strong><st c="12420">. Try the character splitter compared to the recursive character text splitter. </st><st c="12500">Note that with the example they provide shown in </st><span class="No-Break"><em class="italic"><st c="12549">Figure 11</st></em></span><em class="italic"><st c="12558">.1</st></em><st c="12560">, the recursive character splitter captures all of the paragraphs separately at around a </st><strong class="source-inline"><st c="12649">434</st></strong> <span class="No-Break"><st c="12652">chunk size:</st></span></p>
			<div>
				<div id="_idContainer055" class="IMG---Figure">
					<img src="image/B22475_11_01.jpg" alt="Figure 11.1 – Recursive Character Text Splitter captures whole paragraphs at 434 characters"/><st c="12664"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="13572">Figure 11.1 – Recursive Character Text Splitter captures whole paragraphs at 434 characters</st></p>
			<p><st c="13663">As you increase</st><a id="_idIndexMarker737"/><st c="13679"> the chunk size, it stays on the paragraph splits well but eventually gets more and more paragraphs per chunk. </st><st c="13790">Note, though, that this is going to be different for different text. </st><st c="13859">If you have text with very long paragraphs, you will need a larger chunk setting to capture </st><span class="No-Break"><st c="13951">whole paragraphs.</st></span></p>
			<p><st c="13968">Meanwhile, if you try the character splitter, it will cut off in the middle of a sentence on </st><span class="No-Break"><st c="14062">any setting:</st></span></p>
			<div>
				<div id="_idContainer056" class="IMG---Figure">
					<img src="image/B22475_11_02.jpg" alt="Figure 11.2 – Character splitter captures partial paragraphs at 434 characters"/><st c="14074"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="14652">Figure 11.2 – Character splitter captures partial paragraphs at 434 characters</st></p>
			<p><st c="14730">This split of a </st><a id="_idIndexMarker738"/><st c="14747">sentence could have a significant impact on the ability of your chunks to capture all of the important semantic meanings of the text within them. </st><st c="14893">You can offset this by changing the chunk overlap, but you still have partial paragraphs, which will equate to noise to your LLM, distracting it away from providing the </st><span class="No-Break"><st c="15062">optimal response.</st></span></p>
			<p><st c="15079">Let’s step through actual coding examples of each to understand some of the </st><span class="No-Break"><st c="15156">options available.</st></span></p>
			<h2 id="_idParaDest-235"><a id="_idTextAnchor234"/><st c="15174">Character text splitter</st></h2>
			<p><st c="15198">This is the </st><a id="_idIndexMarker739"/><st c="15211">simplest approach to splitting your document. </st><st c="15257">A</st><a id="_idIndexMarker740"/><st c="15258"> text splitter enables you to divide your text into arbitrary N-character-sized chunks. </st><st c="15346">You can improve this slightly by adding a separator parameter, such as </st><strong class="source-inline"><st c="15417">\n</st></strong><st c="15419">. But this is a great place to start to understand how chunking works, and then we can move on to more approaches that work better but have added complexity </st><span class="No-Break"><st c="15576">to them.</st></span></p>
			<p><st c="15584">Here is </st><a id="_idIndexMarker741"/><st c="15593">code that uses the </st><strong class="source-inline"><st c="15612">CharacterTextSplitter</st></strong><st c="15633"> object with our documents that can be used interchangeably</st><a id="_idIndexMarker742"/><st c="15692"> with the other </st><span class="No-Break"><st c="15708">splitter outputs:</st></span></p>
			<pre class="source-code"><st c="15725">
from langchain_text_splitters import CharacterTextSplitter
text_splitter = CharacterTextSplitter(
    separator="\n",
    chunk_size=1000,
    chunk_overlap=200,
    is_separator_regex=False,
)
splits = text_splitter.split_documents(docs)</st></pre>
			<p><st c="15948">The output from the first split (</st><strong class="source-inline"><st c="15982">split[0]</st></strong><st c="15991">) looks </st><span class="No-Break"><st c="16000">like this:</st></span></p>
			<pre class="source-code"><st c="16010">
Document(page_content=</st><strong class="bold"><st c="16033">'Environmental \nReport\n2023What's \ninside\nAbout this report\nGoogle's 2023 Environmental Report provides an overview of our environmental \nsustainability strategy and targets and our annual progress towards them.\u20091  \nThis report features data, performance highlights, and progress against our targets from our 2022 fiscal year (January 1 to December 31, 2022). </st><st c="16405">It also mentions some notable achievements from the first half of 2023. </st><st c="16477">After two years of condensed reporting, we're sharing a deeper dive into our approach in one place.\nADDITIONAL RESOURCES\n• 2023 Environmental Report: Executive Summary\n• Sustainability.google\n• Sustainability reports\n• Sustainability blog\n• Our commitments\n• Alphabet environmental, social, and governance (ESG)\n• About GoogleIntroduction  3\nExecutive letters  4\nHighlights  6\nOur sustainability strategy 7\nTargets and progress summary 8\nEmerging opportunities 9\nEmpowering individuals  12\nOur ambition 13\nOur appr\noach 13\nHelp in\ng people make  14</st></strong><st c="17039">')</st></pre>
			<p><st c="17042">There are a lot of </st><strong class="source-inline"><st c="17062">\n</st></strong><st c="17064"> (also called newline) markup characters, and some </st><strong class="source-inline"><st c="17115">\u</st></strong><st c="17117"> as well. </st><st c="17127">We see that it counts out around 1,000 characters, finds the </st><strong class="source-inline"><st c="17188">\n</st></strong><st c="17190"> character nearest to that, and that </st><a id="_idIndexMarker743"/><st c="17227">becomes the first chunk. </st><st c="17252">It is right in the middle of a sentence, which could </st><span class="No-Break"><st c="17305">be problematic!</st></span></p>
			<p><st c="17320">The next </st><a id="_idIndexMarker744"/><st c="17330">chunk looks </st><span class="No-Break"><st c="17342">like this:</st></span></p>
			<pre class="source-code"><st c="17352">
Document(page_content=</st><strong class="bold"><st c="17375">'Highlights  6\nOur sustainability strategy 7\nTargets and progress summary 8\nEmerging opportunities 9\nEmpowering individuals  12\nOur ambition 13\nOur appr\noach 13\nHelp in\ng people make  14 \nmore sustainable choices  \nReducing home energy use 14\nProviding sustainable  \ntrans\nportation options  17 \nShari\nng other actionable information 19\nThe journey ahead  19\nWorking together 20\nOur ambition 21\nOur approach 21\nSupporting partners</st></strong><strong class="bold"><st c="17820">  22\nInvesting in breakthrough innovation 28\nCreating ecosystems for collaboration  29\nThe journey ahead  30Operating sustainably 31\nOur ambiti\non 32\nOur oper a\ntions  32\nNet-\nzero c\narbon  33\nWater stewardship 49\nCircular econom\ny 55\nNature and biodiversity 67\nSpotlight: Building a more sustainable  \ncam\npus in Mountain View73 \nGovernance and engagement  75\nAbout Google\n 76\nSustainab i\nlity governance  76\nRisk management  77\nStakeholder engagement  78\nPublic policy and advocacy</st></strong><strong class="bold"><st c="18318">  79\nPartnerships  83\nAwards and recognition 84\nAppendix  85</st></strong><st c="18378">')</st></pre>
			<p><st c="18381">As you can see here, it backtracked a little, which is due to the chunk overlap we set of 200 characters. </st><st c="18488">It then goes forward another 1,000 characters from there and breaks on another </st><strong class="source-inline"><st c="18567">\</st></strong><span class="No-Break"><strong class="source-inline"><st c="18568">n</st></strong></span><span class="No-Break"><st c="18569"> character.</st></span></p>
			<p><st c="18580">Let’s step through the parameters </st><span class="No-Break"><st c="18615">for this:</st></span></p>
			<ul>
				<li><strong class="bold"><st c="18624">Separators</st></strong><st c="18635"> – Depending </st><a id="_idIndexMarker745"/><st c="18648">on the separator you use, you may get a wide variety of results. </st><st c="18713">For this, we use </st><strong class="source-inline"><st c="18730">\n</st></strong><st c="18732">, and it works for this document. </st><st c="18766">But if you use </st><strong class="source-inline"><st c="18781">\n\n</st></strong><st c="18785"> (the double newline character) for your separator on this particular document, where there are no double newline characters, it never splits! </st><strong class="source-inline"><st c="18928">\n\n</st></strong><st c="18932"> is actually the default, so make sure you keep an eye on this and use a separator that will work with </st><span class="No-Break"><st c="19035">your content!</st></span></li>
				<li><strong class="bold"><st c="19048">Chunk size</st></strong><st c="19059"> – This </st><a id="_idIndexMarker746"/><st c="19067">defines the arbitrary number of characters you are aiming for with your chunk size. </st><st c="19151">This may still vary, such as at the end of the text, but for the most part, the chunks will be consistently </st><span class="No-Break"><st c="19259">this size.</st></span></li>
				<li><strong class="bold"><st c="19269">Chunk overlap</st></strong><st c="19283"> – This is</st><a id="_idIndexMarker747"/><st c="19293"> the amount of characters you would like overlapping in your sequential chunks. </st><st c="19373">This is a simple way to make sure you are capturing all context within your chunks. </st><st c="19457">For example, if you had no chunk overlap and cut a sentence in half, the majority of that context would likely not be captured in either chunk very well. </st><st c="19611">But with overlap, you can get better coverage of this context on </st><span class="No-Break"><st c="19676">the edges.</st></span></li>
				<li><strong class="bold"><st c="19686">Is separator regex</st></strong><st c="19705"> – This is</st><a id="_idIndexMarker748"/><st c="19715"> yet another parameter that indicates whether the separator used is in </st><span class="No-Break"><st c="19786">Regex format.</st></span></li>
			</ul>
			<p><st c="19799">In this case, we</st><a id="_idIndexMarker749"/><st c="19816"> are setting the chunk size to </st><strong class="source-inline"><st c="19847">1000</st></strong><st c="19851"> and the chunk overlap to </st><strong class="source-inline"><st c="19877">200</st></strong><st c="19880">. What we are saying here with this code is that we want it to use chunks that are smaller than 1,000 characters but with a 200-character overlap. </st><st c="20027">This overlapping technique is similar to the sliding window technique you see in </st><strong class="bold"><st c="20108">convolutional neural networks</st></strong><st c="20137"> (</st><strong class="bold"><st c="20139">CNNs</st></strong><st c="20143">) when</st><a id="_idIndexMarker750"/><st c="20150"> you are </st><em class="italic"><st c="20159">sliding</st></em><st c="20166"> the window over smaller parts of the image with overlap so that you capture the context between the different windows. </st><st c="20286">In this case, it is the context within the chunks that we are trying </st><span class="No-Break"><st c="20355">to capture.</st></span></p>
			<p><st c="20366">Here are some other things </st><span class="No-Break"><st c="20394">to note:</st></span></p>
			<ul>
				<li><strong class="bold"><st c="20402">Document objects</st></strong><st c="20419"> – We are using the LangChain </st><strong class="source-inline"><st c="20449">Document</st></strong><st c="20457"> object to store our text, so we use the </st><strong class="source-inline"><st c="20498">create_documents</st></strong><st c="20514"> function that allows it to work in the next step when these documents are vectorized. </st><st c="20601">If you want to obtain the string content directly, you can use the </st><span class="No-Break"><strong class="source-inline"><st c="20668">split_text</st></strong></span><span class="No-Break"><st c="20678"> function.</st></span></li>
				<li><strong class="bold"><st c="20688">create_documents expects a list</st></strong><st c="20720"> – </st><strong class="source-inline"><st c="20723">create_documents</st></strong><st c="20739"> expects a list of texts, so if you just have a string, you’ll need to wrap it in </st><strong class="source-inline"><st c="20821">[]</st></strong><st c="20823">. In our case, we have already set </st><strong class="source-inline"><st c="20858">docs</st></strong><st c="20862"> as a list, so this requirement </st><span class="No-Break"><st c="20894">is satisfied.</st></span></li>
				<li><strong class="bold"><st c="20907">Splitting versus chunking</st></strong><st c="20933"> – These terms can be </st><span class="No-Break"><st c="20955">used interchangeably.</st></span></li>
			</ul>
			<p><st c="20976">You can find more information about this specific text splitter on the LangChain </st><span class="No-Break"><st c="21058">website: </st></span><a href="https://python.langchain.com/v0.2/docs/how_to/character_text_splitter/ "><span class="No-Break"><st c="21067">https://python.langchain.com/v0.2/docs/how_to/character_text_splitter/</st></span></a></p>
			<p><st c="21137">The API documentation can be found </st><span class="No-Break"><st c="21173">here: </st></span><a href="https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.CharacterTextSplitter.html "><span class="No-Break"><st c="21179">https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.CharacterTextSplitter.html</st></span></a></p>
			<p><st c="21293">We can do better than this though; let’s take a look at a more sophisticated approach called </st><strong class="bold"><st c="21387">recursive character </st></strong><span class="No-Break"><strong class="bold"><st c="21407">text splitting</st></strong></span><span class="No-Break"><st c="21421">.</st></span></p>
			<h2 id="_idParaDest-236"><a id="_idTextAnchor235"/><st c="21422">Recursive character text splitter</st></h2>
			<p><st c="21456">We have</st><a id="_idIndexMarker751"/><st c="21464"> seen this one before! </st><st c="21487">We have used this </st><a id="_idIndexMarker752"/><st c="21505">splitter the most in our code labs so far because it is what LangChain recommends using when splitting generic text. </st><st c="21622">That is what we </st><span class="No-Break"><st c="21638">are doing!</st></span></p>
			<p><st c="21648">As the name states, this</st><a id="_idIndexMarker753"/><st c="21673"> splitter recursively splits text, with the intention of keeping related pieces of text next to each other. </st><st c="21781">You can pass a list of characters as a parameter and it will try to split those characters in order until the chunks are small enough. </st><st c="21916">The default list is </st><strong class="source-inline"><st c="21936">["\n\n", "\n", " ", ""]</st></strong><st c="21959">, which works well, but we are going to add </st><strong class="source-inline"><st c="22003">". </st><st c="22006">"</st></strong><st c="22007"> to this list as well. </st><st c="22030">This has the effect of trying to keep together all paragraphs, sentences defined by both </st><strong class="source-inline"><st c="22119">"\n"</st></strong><st c="22123"> and </st><strong class="source-inline"><st c="22128">". </st><st c="22131">"</st></strong><st c="22132">, and words as long </st><span class="No-Break"><st c="22152">as possible.</st></span></p>
			<p><st c="22164">Here is </st><span class="No-Break"><st c="22173">our code:</st></span></p>
			<pre class="source-code"><st c="22182">
recursive_splitter = RecursiveCharacterTextSplitter(
    separators=["\n\n", "\n", ". </st><st c="22265">", " ", ""],
    chunk_size=1000,
    chunk_overlap=200
)
splits = character_splitter.split_documents(docs)</st></pre>
			<p><st c="22364">Under the hood with this splitter, the chunks are split based on the </st><strong class="source-inline"><st c="22434">"\n\n"</st></strong><st c="22440"> separator, representing paragraph splits. </st><st c="22483">But it doesn’t stop there; it will look at the chunk size, and if that is larger than the 1,000 we set, then it will split by the next separator (</st><strong class="source-inline"><st c="22629">"\n"</st></strong><st c="22634">), and </st><span class="No-Break"><st c="22642">so on.</st></span></p>
			<p><st c="22648">Let’s talk </st><a id="_idIndexMarker754"/><st c="22660">about the recursive aspect of this that splits the text into chunks using a recursive algorithm. </st><st c="22757">The algorithm will only be applied if the text provided is longer than the chunk size, but it follows </st><span class="No-Break"><st c="22859">these steps:</st></span></p>
			<ol>
				<li><st c="22871">It finds the last space or newline character within the range </st><strong class="source-inline"><st c="22934">[chunk_size - chunk_overlap, chunk_size]</st></strong><st c="22974">. This ensures that chunks are split at word boundaries or </st><span class="No-Break"><st c="23033">line breaks.</st></span></li>
				<li><st c="23045">If a suitable split point is found, it splits the text into two parts: the chunk before the split point and the remaining text after the </st><span class="No-Break"><st c="23183">split point.</st></span></li>
				<li><st c="23195">It recursively applies the same splitting process to the remaining text until all chunks are within the </st><span class="No-Break"><strong class="source-inline"><st c="23300">chunk_size</st></strong></span><span class="No-Break"><st c="23310"> limit.</st></span></li>
			</ol>
			<p><st c="23317">Similar to the </st><a id="_idIndexMarker755"/><st c="23333">character splitter approach, the recursive splitter is driven largely by the chunk size you set, but then it combines this with the recursive approach outlined previously to provide a straightforward and logical way to properly capture context within </st><span class="No-Break"><st c="23584">your chunks.</st></span></p>
			<p><strong class="source-inline"><st c="23596">RecursiveCharacterTextSplitter</st></strong><st c="23627"> is particularly useful when dealing with large text documents that need to be processed by language models with input size limitations. </st><st c="23764">By splitting the text into smaller chunks, you can feed the chunks to the language model individually and then combine the results </st><span class="No-Break"><st c="23895">if needed.</st></span></p>
			<p><st c="23905">Clearly, recursive splitters are a step up from the character splitter, but they are still not splitting our content based on the semantics as much as just general separators such as paragraph and sentence breaks. </st><st c="24120">But this will not handle cases where two paragraphs are semantically part of one ongoing thought that should really be captured together in their vector representations. </st><st c="24290">Let’s see whether we can do better with the </st><span class="No-Break"><strong class="bold"><st c="24334">semantic chunker</st></strong></span><span class="No-Break"><st c="24350">.</st></span></p>
			<h2 id="_idParaDest-237"><a id="_idTextAnchor236"/><st c="24351">Semantic chunker</st></h2>
			<p><st c="24368">This is</st><a id="_idIndexMarker756"/><st c="24376"> another one you may recognize, as we used it in the first code lab! </st><strong class="source-inline"><st c="24445">SemanticChunker</st></strong><st c="24460"> is an interesting one, currently listed as experimental, but described on the LangChain website as follows: “</st><em class="italic"><st c="24570">First splits on sentences. </st><st c="24598">Then (it) combines ones next to each other if they are semantically similar enough</st></em><st c="24680">.” In other words, the goal here is to avoid having to define this arbitrary chunk size number </st><a id="_idIndexMarker757"/><st c="24775">that was a key parameter that drives how the character and recursive splitters divide the text and focus the splits more on the semantics of the text you are splitting. </st><st c="24944">Find out more about this </st><em class="italic"><st c="24969">chunker</st></em><st c="24976"> on the LangChain </st><span class="No-Break"><st c="24994">website: </st></span><a href="https://python.langchain.com/docs/modules/data_connection/document_transformers/semantic-chunker "><span class="No-Break"><st c="25003">https://python.langchain.com/docs/modules/data_connection/document_transformers/semantic-chunker</st></span></a></p>
			<p><st c="25099">Under the hood, </st><strong class="source-inline"><st c="25116">SemanticChunker</st></strong><st c="25131"> splits your text into sentences, groups those sentences into groups of three sentences, and then merges them when they are similar in the </st><span class="No-Break"><st c="25270">embedding space.</st></span></p>
			<p><st c="25286">When would this not work as well? </st><st c="25321">When the semantics of your document is difficult to discern. </st><st c="25382">For example, if you have a lot of code, addresses, names, internal reference IDs, and other text that has little semantic meaning, especially to an embedding model, this will likely reduce the ability of </st><strong class="source-inline"><st c="25586">SemanticChunker</st></strong><st c="25601"> to properly split your text. </st><st c="25631">But in general, </st><strong class="source-inline"><st c="25647">SemanticChunker</st></strong><st c="25662"> has a lot of promise. </st><st c="25685">Here is an example of the code to </st><span class="No-Break"><st c="25719">use it:</st></span></p>
			<pre class="source-code"><st c="25726">
from langchain_experimental.text_splitter import SemanticChunker
embedding_function = OpenAIEmbeddings()
semantic_splitter = SemanticChunker(embedding_function,
    number_of_chunks=200)
splits = semantic_splitter.split_documents(docs)</st></pre>
			<p><st c="25958">Here, we import the </st><strong class="source-inline"><st c="25979">SemanticChunker</st></strong><st c="25994"> class from the </st><strong class="source-inline"><st c="26010">langchain_experimental.text_splitter</st></strong><st c="26046"> module. </st><st c="26055">We use the same embedding model we used to vectorize our documents and pass them to the </st><strong class="source-inline"><st c="26143">SemanticChunker</st></strong><st c="26158"> class. </st><st c="26166">Note that this costs a little money, as it uses the same OpenAI API key we used to generate our embeddings. </st><strong class="source-inline"><st c="26274">SemanticChunker</st></strong><st c="26289"> uses these embeddings to determine how to split the documents based on semantic similarity. </st><st c="26382">We also set the </st><strong class="source-inline"><st c="26398">number_of_chunks</st></strong><st c="26414"> variable to </st><strong class="source-inline"><st c="26427">200</st></strong><st c="26430">, which indicates the desired number of chunks to split the documents into. </st><st c="26506">This determines the granularity of the splitting process. </st><st c="26564">A higher value of </st><strong class="source-inline"><st c="26582">number_of_chunks</st></strong><st c="26598"> will result in more fine-grained splits, while a lower value will produce fewer and </st><span class="No-Break"><st c="26683">larger chunks.</st></span></p>
			<p><st c="26697">This</st><a id="_idIndexMarker758"/><st c="26702"> code lab </st><a id="_idIndexMarker759"/><st c="26712">is set up so that you can use each type of splitter at a time. </st><st c="26775">Run through each splitter and then the rest of the code to see how each one impacts your results. </st><st c="26873">Also try changing parameter settings, such as </st><strong class="source-inline"><st c="26919">chunk_size</st></strong><st c="26929">, </st><strong class="source-inline"><st c="26931">chunk_overlap</st></strong><st c="26944"> and </st><strong class="source-inline"><st c="26949">number_of_chunks</st></strong><st c="26965">, depending on what splitter you are using. </st><st c="27009">Exploring all of these options will help give you a better sense of how they can be used for </st><span class="No-Break"><st c="27102">your projects.</st></span></p>
			<p><st c="27116">For a last supporting component, we will discuss output parsers, responsible for shaping the final output from our </st><span class="No-Break"><st c="27232">RAG application.</st></span></p>
			<h1 id="_idParaDest-238"><a id="_idTextAnchor237"/><st c="27248">Code lab 11.3 – Output parsers</st></h1>
			<p><st c="27279">The file you need to access from the GitHub repository is </st><span class="No-Break"><st c="27338">titled </st></span><span class="No-Break"><strong class="source-inline"><st c="27345">CHAPTER11-3_OUTPUT_PARSERS.ipynb</st></strong></span><span class="No-Break"><st c="27377">.</st></span></p>
			<p><st c="27378">The end</st><a id="_idIndexMarker760"/><st c="27386"> result of any RAG application is going to be text, along with potentially some formatting, metadata, and some other related data. </st><st c="27517">This output typically comes from the LLM itself. </st><st c="27566">But there are times when you want to get a more structured format than just text. </st><st c="27648">Output parsers are classes that help to structure the responses of the LLM wherever you use it in your RAG application. </st><st c="27768">The output that this provides will then be provided to the next step in the chain, or in the case of all of our code labs, as the final output from </st><span class="No-Break"><st c="27916">the model.</st></span></p>
			<p><st c="27926">We will cover two different output parsers at the same time, and use them at different times in our RAG pipeline. </st><st c="28041">We start with the parser we know, the string </st><span class="No-Break"><st c="28086">output parser.</st></span></p>
			<p><st c="28100">Under the </st><strong class="source-inline"><st c="28111">relevance_prompt</st></strong><st c="28127"> function, add this code to a </st><span class="No-Break"><st c="28157">new cell:</st></span></p>
			<pre class="source-code"><st c="28166">
from langchain_core.output_parsers import StrOutputParser
str_output_parser = StrOutputParser()</st></pre>
			<p><st c="28262">Note that we were already using this in the LangChain chain code that appears later, but we are going to assign this parser to a variable called </st><strong class="source-inline"><st c="28408">str_output_parser</st></strong><st c="28425">. Let’s talk about this type of parser in </st><span class="No-Break"><st c="28467">more depth.</st></span></p>
			<h2 id="_idParaDest-239"><a id="_idTextAnchor238"/><st c="28478">String output parser</st></h2>
			<p><st c="28499">This is a </st><a id="_idIndexMarker761"/><st c="28510">basic output parser. </st><st c="28531">In very simple approaches, as in our previous code labs, you can use the </st><strong class="source-inline"><st c="28604">StrOutputParser</st></strong><st c="28619"> class outright as </st><a id="_idIndexMarker762"/><st c="28638">the instance for your output parser. </st><st c="28675">Or you can do what we just did and assign it to a variable, particularly if you expect to see it in multiple areas of the code, which we will. </st><st c="28818">But we have seen this many times already. </st><st c="28860">It takes the output from the LLM in both places it is used and outputs the string response from the LLM to the next link in the chain. </st><st c="28995">The documentation for this parser can be found </st><span class="No-Break"><st c="29042">here: </st></span><a href="https://api.python.langchain.com/en/latest/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html#langchain_core.output_parsers.string.StrOutputParser "><span class="No-Break"><st c="29048">https://api.python.langchain.com/en/latest/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html#langchain_core.output_parsers.string.StrOutputParser</st></span></a></p>
			<p><st c="29216">Let’s look at a new type of parser, the JSON </st><span class="No-Break"><st c="29262">output parser.</st></span></p>
			<h2 id="_idParaDest-240"><a id="_idTextAnchor239"/><st c="29276">JSON output parser</st></h2>
			<p><st c="29295">As you</st><a id="_idIndexMarker763"/><st c="29302"> can imagine, this output parser takes input from an </st><a id="_idIndexMarker764"/><st c="29355">LLM and outputs it as JSON. </st><st c="29383">It is important to note that you may not need this parser, as many newer model providers support built-in ways to return structured output such as JSON and XML. </st><st c="29544">This approach is for those that </st><span class="No-Break"><st c="29576">do not.</st></span></p>
			<p><st c="29583">We start with some new imports, coming from a library we have already installed from </st><span class="No-Break"><st c="29669">LangChain (</st></span><span class="No-Break"><strong class="source-inline"><st c="29680">langchain_core</st></strong></span><span class="No-Break"><st c="29695">):</st></span></p>
			<pre class="source-code"><st c="29698">
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_core.outputs import Generation
import json</st></pre>
			<p><st c="29871">These lines import the necessary classes and modules from the </st><strong class="source-inline"><st c="29934">langchain_core </st></strong><st c="29949">library and the </st><strong class="source-inline"><st c="29965">json</st></strong><st c="29969"> module. </st><strong class="source-inline"><st c="29978">JsonOutputParser</st></strong><st c="29994"> is used to parse the JSON output. </st><strong class="source-inline"><st c="30029">BaseModel</st></strong><st c="30038"> and </st><strong class="source-inline"><st c="30043">Field</st></strong><st c="30048"> are used to define the structure of the JSON output model. </st><strong class="source-inline"><st c="30108">Generation</st></strong><st c="30118"> is used to represent the generated output. </st><st c="30162">And not surprisingly, we import a package for </st><strong class="source-inline"><st c="30208">json</st></strong><st c="30212">, so that we can better manage our </st><span class="No-Break"><st c="30247">JSON inputs/outputs.</st></span></p>
			<p><st c="30267">Next, we will create a Pydantic model called </st><strong class="source-inline"><st c="30313">FinalOutputModel</st></strong><st c="30329"> that represents the structure of the </st><span class="No-Break"><st c="30367">JSON output:</st></span></p>
			<pre class="source-code"><st c="30379">
class FinalOutputModel(BaseModel):
    relevance_score: float = Field(description="The
        relevance score of the retrieved context to the
        question")
    answer: str = Field(description="The final answer to
        the question")</st></pre>
			<p><st c="30589">It has two </st><a id="_idIndexMarker765"/><st c="30601">fields: </st><strong class="source-inline"><st c="30609">relevance_score</st></strong><st c="30624"> (float) and </st><strong class="source-inline"><st c="30637">answer</st></strong><st c="30643"> (string), along with their descriptions. </st><st c="30685">In a </st><em class="italic"><st c="30690">real-world</st></em><st c="30700"> application, this model is likely to</st><a id="_idIndexMarker766"/><st c="30737"> get substantially more complex, but this gives you a general concept of how it can </st><span class="No-Break"><st c="30821">be defined.</st></span></p>
			<p><st c="30832">Next, we will create an instance of the </st><span class="No-Break"><strong class="source-inline"><st c="30873">JsonOutputParser</st></strong></span><span class="No-Break"><st c="30889"> parser:</st></span></p>
			<pre class="source-code"><st c="30897">
json_parser = JsonOutputParser(
    pydantic_model=FinalOutputModel)</st></pre>
			<p><st c="30962">This line assigns </st><strong class="source-inline"><st c="30981">JsonOutputParser</st></strong><st c="30997"> with the </st><strong class="source-inline"><st c="31007">FinalOutputModel</st></strong><st c="31023"> class as a parameter to </st><strong class="source-inline"><st c="31048">json_parser</st></strong><st c="31059"> for use later in our code when we want to use </st><span class="No-Break"><st c="31106">this parser.</st></span></p>
			<p><st c="31118">Next, we are going add a new function right in between our two other helper functions, and then we will update </st><strong class="source-inline"><st c="31230">conditional_answer</st></strong><st c="31248"> to use that new function. </st><st c="31275">This code goes under the existing </st><strong class="source-inline"><st c="31309">extract_score</st></strong><st c="31322"> function, which remains </st><span class="No-Break"><st c="31347">the same:</st></span></p>
			<pre class="source-code"><st c="31356">
def format_json_output(x):
    # print(x)
    json_output = {"relevance_score":extract_score(
        x['relevance_score']),"answer": x['answer'],
    }
    return json_parser.parse_result(
        [Generation(text=json.dumps(json_output))])</st></pre>
			<p><st c="31566">This </st><strong class="source-inline"><st c="31572">format_json_output</st></strong><st c="31590"> function takes a dictionary, </st><strong class="source-inline"><st c="31620">x</st></strong><st c="31621">, as input and formats it into a JSON output. </st><st c="31667">It creates a </st><strong class="source-inline"><st c="31680">json_output</st></strong><st c="31691"> dictionary with two keys: </st><strong class="source-inline"><st c="31718">"relevance_score"</st></strong><st c="31735"> (obtained by calling </st><strong class="source-inline"><st c="31757">extract_score</st></strong><st c="31770"> on the </st><strong class="source-inline"><st c="31778">'relevance_score</st></strong><st c="31794">’ value from </st><strong class="source-inline"><st c="31808">x</st></strong><st c="31809">) and </st><strong class="source-inline"><st c="31815">"answer"</st></strong><st c="31823"> (directly taken from </st><strong class="source-inline"><st c="31845">x</st></strong><st c="31846">). </st><st c="31849">It then uses </st><strong class="source-inline"><st c="31862">json.dumps</st></strong><st c="31872"> to convert the </st><strong class="source-inline"><st c="31888">json_output</st></strong><st c="31899"> dictionary to a JSON string and creates a </st><strong class="source-inline"><st c="31942">Generation</st></strong><st c="31952"> object with the JSON string as its text. </st><st c="31994">Finally, it uses </st><strong class="source-inline"><st c="32011">json_parser</st></strong><st c="32022"> to parse the </st><strong class="source-inline"><st c="32036">Generation</st></strong><st c="32046"> object and returns the </st><span class="No-Break"><st c="32070">parsed result.</st></span></p>
			<p><st c="32084">We will</st><a id="_idIndexMarker767"/><st c="32092"> need to reference this function in the function we were</st><a id="_idIndexMarker768"/><st c="32148"> previously using, </st><strong class="source-inline"><st c="32167">conditional_answer</st></strong><st c="32185">. Update </st><strong class="source-inline"><st c="32194">conditional_answer</st></strong> <span class="No-Break"><st c="32212">like this:</st></span></p>
			<pre class="source-code"><st c="32223">
def conditional_answer(x):
    relevance_score = extract_score(x['relevance_score'])
    if relevance_score &lt; 4:
        return "I don't know."
    </st><st c="32352">else:
        return format_json_output(x)</st></pre>
			<p><st c="32386">Here, we update the </st><strong class="source-inline"><st c="32407">conditional_answer</st></strong><st c="32425"> function to apply that </st><strong class="source-inline"><st c="32449">format_json_output</st></strong><st c="32467"> function if it determines the answer is relevant and before it provides the </st><span class="No-Break"><st c="32544">returned output.</st></span></p>
			<p><st c="32560">Next, we are going to take the two chains we had before in our code and combine them into one larger chain handling the entire pipeline. </st><st c="32698">In the past, it was helpful to show this separately to give more focus to certain areas, but now we have a chance to clean up and show how these chains can be grouped together to handle our entire </st><span class="No-Break"><st c="32895">logic flow:</st></span></p>
			<pre class="source-code"><st c="32906">
rag_chain = (
    RunnableParallel({"context": ensemble_retriever,
        "question": RunnablePassthrough()})
    | RunnablePassthrough.assign(context=(lambda x:
        format_docs(x["context"])))
    | RunnableParallel({"relevance_score": (
          RunnablePassthrough()
          | (lambda x: relevance_prompt_template.format(
                 question=x["question"],
                 retrieved_context=x["context"]
                 )
             )
             | llm
             | str_output_parser
        ),
        "answer": (
            RunnablePassthrough()
            | prompt
            | llm
            | str_output_parser
            ),
        }
    )
    | RunnablePassthrough().assign(
         final_result=conditional_answer)
)</st></pre>
			<p><st c="33422">If you</st><a id="_idIndexMarker769"/><st c="33429"> look back at previous code labs, this was represented</st><a id="_idIndexMarker770"/><st c="33483"> by two chains. </st><st c="33499">Note that this is using </st><strong class="source-inline"><st c="33523">str_output_parser</st></strong><st c="33540"> in the same way it was before. </st><st c="33572">You do not see the JSON parser here because it is applied in the </st><strong class="source-inline"><st c="33637">format_json_output</st></strong><st c="33655"> function, which is called from the </st><strong class="source-inline"><st c="33691">conditional_answer</st></strong><st c="33709"> function, which you see in the last line. </st><st c="33752">This simplification of these chains works for this example, focused on parsing our output into JSON, but we should note that we do lose the context that we have used in previous code labs. </st><st c="33941">This is really just an example of an alternative approach to setting up </st><span class="No-Break"><st c="34013">our chain(s).</st></span></p>
			<p><st c="34026">Lastly, because our final output is in JSON format that we did need to add the context to, we need to update our </st><em class="italic"><st c="34140">test </st></em><span class="No-Break"><em class="italic"><st c="34145">run</st></em></span><span class="No-Break"><st c="34148"> code:</st></span></p>
			<pre class="source-code"><st c="34154">
result = rag_chain.invoke(user_query)
print(f"Original Question: {user_query}\n")
print(f"Relevance Score: {result['relevance_score']}\n")
print(f"Final Answer:\n{result[
    'final_result']['answer']}\n\n")
print(f"Final JSON Output:\n{result}\n\n")</st></pre>
			<p><st c="34401">When </st><a id="_idIndexMarker771"/><st c="34407">we print this out, we see a similar result as in the </st><a id="_idIndexMarker772"/><st c="34460">past, but we show how the JSON formatted final </st><span class="No-Break"><st c="34507">output looks:</st></span></p>
			<pre class="source-code"><st c="34520">
Original Question: What are Google's environmental initiatives?
</st><st c="34585">Relevance Score: 5
Final Answer:
Google's environmental initiatives include empowering individuals to take action, working together with partners and customers, operating sustainably… [TRUNCATED]
Final JSON Output:
{
'relevance_score': '5',
'answer': "Google's environmental initiatives include empowering individuals to take action, working together with partners and customers, operating sustainably, achieving net-zero carbon emissions, water stewardship, engaging in a circular economy, and supporting sustainable consumption of public goods. </st><st c="35132">They also engage with suppliers to reduce energy consumption and greenhouse gas emissions, report environmental data, and assess environmental criteria. </st><st c="35285">Google is involved in various sustainability initiatives, such as the iMasons Climate Accord, ReFED, and projects with The Nature Conservancy. </st><st c="35428">They also invest in breakthrough innovation and support sustainability-focused accelerators. </st><st c="35521">Additionally, Google focuses on renewable energy, data analytics tools for sustainability, and AI for sustainability to drive more intelligent supply chains.",
'final_result': {
     'relevance_score': 5.0,
     'answer': "Google's environmental initiatives include empowering individuals to take action, working together with partners and customers, operating sustainably, achieving net-zero carbon emissions, water stewardship, engaging in a circular economy, and supporting sustainable consumption of public goods. </st><st c="36029">They also engage with suppliers to reduce energy consumption and greenhouse gas emissions, report environmental data, and assess environmental criteria. </st><st c="36182">Google is involved in various sustainability initiatives, such as the iMasons Climate Accord, ReFED, and projects with The Nature Conservancy. </st><st c="36325">They also invest in breakthrough innovation and support sustainability-focused accelerators. </st><st c="36418">Additionally, Google focuses on renewable energy, data analytics tools for sustainability, and AI for sustainability to drive more intelligent supply chains."
</st><st c="36577">}
}</st></pre>
			<p><st c="36580">This is a </st><a id="_idIndexMarker773"/><st c="36591">simple example of a JSON output, but you can build off this </st><a id="_idIndexMarker774"/><st c="36651">and shape the JSON to anything you need using the </st><strong class="source-inline"><st c="36701">FinalOutputModel</st></strong><st c="36717"> class we defined and passed into our </st><span class="No-Break"><st c="36755">output parser.</st></span></p>
			<p><st c="36769">You can find more information about the JSON parser </st><span class="No-Break"><st c="36822">here: </st></span><a href="https://python.langchain.com/v0.2/docs/how_to/output_parser_json/ "><span class="No-Break"><st c="36828">https://python.langchain.com/v0.2/docs/how_to/output_parser_json/</st></span></a></p>
			<p><st c="36893">It is important to note that it is difficult to rely on LLMs to output in a certain format. </st><st c="36986">A more robust system would incorporate the parser deeper into the system, where it will likely be able to better utilize the JSON output, but it will also entail more checks to make sure the </st><a id="_idIndexMarker775"/><st c="37177">formatting</st><a id="_idIndexMarker776"/><st c="37187"> is as required for the next step to work off properly formatted JSON. </st><st c="37258">In our code here, we implemented a very lightweight layer for JSON formatting to show how the output parser could fit into our RAG application in a very </st><span class="No-Break"><st c="37411">simple way.</st></span></p>
			<h1 id="_idParaDest-241"><a id="_idTextAnchor240"/><st c="37422">Summary</st></h1>
			<p><st c="37430">In this chapter, we learned about various components in LangChain that can enhance a RAG application. </st><em class="italic"><st c="37533">Code lab 11.1</st></em><st c="37546"> focused on document loaders, which are used to load and process documents from various sources such as text files, PDFs, web pages, or databases. </st><st c="37693">The chapter covered examples of loading documents from HTML, PDF, Microsoft Word, and JSON formats using different LangChain document loaders, noting that some document loaders add metadata which may require adjustments in </st><span class="No-Break"><st c="37916">the code.</st></span></p>
			<p><em class="italic"><st c="37925">Code lab 11.2</st></em><st c="37939"> discussed text splitters, which divide documents into chunks suitable for retrieval, addressing issues with large documents and context representation in vector embeddings. </st><st c="38113">The chapter covered </st><strong class="source-inline"><st c="38133">CharacterTextSplitter</st></strong><st c="38154">, which splits text into arbitrary N-character-sized chunks, and </st><strong class="source-inline"><st c="38219">RecursiveCharacterTextSplitter</st></strong><st c="38249">, which recursively splits text while trying to keep related pieces together. </st><strong class="source-inline"><st c="38327">SemanticChunker</st></strong><st c="38342"> was introduced as an experimental splitter that combines semantically similar sentences to create more </st><span class="No-Break"><st c="38446">meaningful chunks.</st></span></p>
			<p><st c="38464">Lastly, </st><em class="italic"><st c="38473">Code lab 11.3</st></em><st c="38486"> focused on output parsers, which structure the responses from the language model in a RAG application. </st><st c="38590">The chapter covered the string output parser, which outputs the LLM’s response as a string, and the JSON output parser, which formats the output as JSON using a defined structure. </st><st c="38770">An example was provided to show how the JSON output parser can be integrated into the </st><span class="No-Break"><st c="38856">RAG application.</st></span></p>
			<p><st c="38872">In the next chapter, we will cover a relatively advanced but very powerful topic, LangGraph and </st><span class="No-Break"><st c="38969">AI agents.</st></span></p>
		</div>
	<div id="charCountTotal" value="38979"/>

		<div id="_idContainer058" class="Content">
			<h1 id="_idParaDest-242" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor241"/><st c="0">Part 3 – Implementing Advanced RAG</st></h1>
			<p><st c="35">In this part, you will learn advanced techniques for enhancing your RAG applications, including integrating AI agents with LangGraph for more sophisticated control flows, leveraging prompt engineering strategies to optimize retrieval and generation, and exploring cutting-edge approaches such as query expansion, query decomposition, and multi-modal RAG. </st><st c="391">You’ll gain hands-on experience in implementing these techniques through code labs and discover a wealth of additional methods covering indexing, retrieval, generation, and the entire </st><span class="No-Break"><st c="575">RAG pipeline.</st></span></p>
			<p><st c="588">This part contains the </st><span class="No-Break"><st c="612">following chapters:</st></span></p>
			<ul>
				<li><a href="B22475_12.xhtml#_idTextAnchor242"><em class="italic"><st c="631">Chapter 12</st></em></a><st c="642">, </st><em class="italic"><st c="644">Combining RAG with the Power of AI Agents and LangGraph</st></em></li>
				<li><a href="B22475_13.xhtml#_idTextAnchor256"><em class="italic"><st c="699">Chapter 13</st></em></a><st c="710">, </st><em class="italic"><st c="712">Using Prompt Engineering to Improve RAG Efforts</st></em></li>
				<li><a href="B22475_14.xhtml#_idTextAnchor283"><em class="italic"><st c="759">Chapter 14</st></em></a><st c="770">, </st><em class="italic"><st c="772">Advanced RAG-Related Techniques for Improving Results</st></em></li>
			</ul>
		</div>
		<div>
			<div id="_idContainer059">
			</div>
		</div>
		<div>
			<div id="_idContainer060">
			</div>
		</div>
		<div>
			<div id="_idContainer061">
			</div>
		</div>
		<div>
			<div id="_idContainer062">
			</div>
		</div>
		<div>
			<div id="_idContainer063">
			</div>
		</div>
		<div>
			<div id="_idContainer064" class="Basic-Graphics-Frame">
			</div>
		</div>
		<div>
			<div id="_idContainer065" class="Basic-Graphics-Frame">
			</div>
		</div>
		<div>
			<div id="_idContainer066">
			</div>
		</div>
		<div>
			<div id="_idContainer067">
			</div>
		</div>
		<div>
			<div id="_idContainer068" class="Basic-Graphics-Frame">
			</div>
		</div>
	<div id="charCountTotal" value="825"/></body></html>