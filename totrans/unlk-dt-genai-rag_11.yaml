- en: <st c="0">11</st>
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 11（<st c="0">11</st>）
- en: <st c="3">Using LangChain to Get More from RAG</st>
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用LangChain从RAG中获得更多（<st c="3">Using LangChain to Get More from RAG</st>）
- en: <st c="40">We have mentioned</st> **<st c="59">LangChain</st>** <st c="68">several</st>
    <st c="76">times already, and we have shown you a lot of LangChain code, including
    code that implements the LangChain-specific language:</st> **<st c="203">LangChain
    Expression Language</st>** <st c="232">(</st>**<st c="234">LCEL</st>**<st c="238">).</st>
    <st c="242">Now that you are familiar with</st> <st c="272">different ways to
    implement</st> **<st c="301">retrieval-augmented generation</st>** <st c="331">(</st>**<st
    c="333">RAG</st>**<st c="336">) with LangChain, we thought now would be a good
    time to dive more into the various capabilities of LangChain that you can use
    to make your RAG</st> <st c="481">pipeline better.</st>
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经提到过**LangChain**（<st c="40">We have mentioned</st> **<st c="59">LangChain</st>**
    <st c="68">several</st> <st c="76">times already, and we have shown you a lot
    of LangChain code, including code that implements the LangChain-specific language:</st>
    **<st c="203">LangChain Expression Language</st>** <st c="232">(</st>**<st c="234">LCEL</st>**<st
    c="238">).</st> <st c="242">Now that you are familiar with</st> <st c="272">different
    ways to implement</st> **<st c="301">retrieval-augmented generation</st>** <st
    c="331">(</st>**<st c="333">RAG</st>**<st c="336">) with LangChain, we thought
    now would be a good time to dive more into the various capabilities of LangChain
    that you can use to make your RAG</st> <st c="481">pipeline better.</st>）
- en: <st c="497">In this chapter, we explore lesser-known but highly important components
    in LangChain that can enhance a RAG application.</st> <st c="620">We will cover</st>
    <st c="634">the following:</st>
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了LangChain中一些不太为人所知但非常重要的组件，这些组件可以增强RAG应用（<st c="497">In this chapter,
    we explore lesser-known but highly important components in LangChain that can
    enhance a RAG application.</st> <st c="620">We will cover</st> <st c="634">the
    following:</st>）
- en: <st c="648">Document loaders for loading and processing documents from</st>
    <st c="708">different sources</st>
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于从不同来源加载和处理文档的文档加载器（<st c="648">Document loaders for loading and processing
    documents from</st> <st c="708">different sources</st>）
- en: <st c="725">Text splitters for dividing documents into chunks suitable</st>
    <st c="785">for retrieval</st>
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于将文档分割成适合检索的块的文字分割器（<st c="725">Text splitters for dividing documents into
    chunks suitable</st> <st c="785">for retrieval</st>）
- en: <st c="798">Output parsers for structuring the responses from the</st> <st c="853">language
    model</st>
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于结构化语言模型响应的输出解析器（<st c="798">Output parsers for structuring the responses from
    the</st> <st c="853">language model</st>）
- en: <st c="867">We will use different code labs to step through examples of each
    type of component, starting with</st> <st c="966">document loaders.</st>
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用不同的代码实验室来逐步演示每种类型组件的示例，首先是文档加载器（<st c="867">We will use different code
    labs to step through examples of each type of component, starting with</st> <st
    c="966">document loaders.</st>）
- en: <st c="983">Technical requirements</st>
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求（<st c="983">Technical requirements</st>）
- en: <st c="1006">The code for this chapter is placed in the following GitHub</st>
    <st c="1067">repository:</st> [<st c="1079">https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_11</st>](https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_11
    )
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码放置在以下GitHub仓库中：[https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_11](https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_11)（<st
    c="1006">The code for this chapter is placed in the following GitHub</st> <st
    c="1067">repository:</st> [<st c="1079">https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_11</st>](https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_11
    )）
- en: <st c="1176">Individual file names for each code lab are mentioned in the</st>
    <st c="1238">respective sections.</st>
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 每个代码实验室的单独文件名在各自的章节中提到（<st c="1176">Individual file names for each code lab
    are mentioned in the</st> <st c="1238">respective sections.</st>）
- en: <st c="1258">Code lab 11.1 – Document loaders</st>
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码实验室11.1 – 文档加载器（<st c="1258">Code lab 11.1 – Document loaders</st>）
- en: <st c="1291">The file you need to</st> <st c="1313">access from the GitHub repository
    is</st> <st c="1350">titled</st> `<st c="1357">CHAPTER11-1_DOCUMENT_LOADERS.ipynb</st>`<st
    c="1391">.</st>
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要从GitHub仓库访问的文件名为`<st c="1357">CHAPTER11-1_DOCUMENT_LOADERS.ipynb</st>`（<st
    c="1291">The file you need to</st> <st c="1313">access from the GitHub repository
    is</st> <st c="1350">titled</st> `<st c="1357">CHAPTER11-1_DOCUMENT_LOADERS.ipynb</st>`<st
    c="1391">.</st>）
- en: <st c="1392">Document loaders play a key role in accessing, extracting, and
    pulling in the data that makes our RAG application function.</st> <st c="1517">Document
    loaders are used to load and process documents from various sources such as text
    files, PDFs, web pages, or databases.</st> <st c="1645">They convert the documents
    into a format suitable for indexing</st> <st c="1708">and retrieval.</st>
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 文档加载器在访问、提取和拉取使我们的RAG应用功能的数据中起着关键作用（<st c="1392">Document loaders play a key
    role in accessing, extracting, and pulling in the data that makes our RAG application
    function.</st> <st c="1517">Document loaders are used to load and process documents
    from various sources such as text files, PDFs, web pages, or databases.</st> <st
    c="1645">They convert the documents into a format suitable for indexing</st> <st
    c="1708">and retrieval.</st>）
- en: <st c="1722">Let’s install some</st> <st c="1741">new packages to support our
    document loading, which, as you might have guessed, involves some different file</st>
    <st c="1851">format-related packages:</st>
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们安装一些新的包来支持我们的文档加载，正如你可能已经猜到的，这涉及到一些不同的文件格式相关的包（<st c="1722">Let’s install
    some</st> <st c="1741">new packages to support our document loading, which, as
    you might have guessed, involves some different file</st> <st c="1851">format-related
    packages:</st>）
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: <st c="1955">The first one may look familiar,</st> `<st c="1989">bs4</st>` <st
    c="1992">(for Beautiful Soup 4), as we used it in</st> [*<st c="2034">Chapter
    2</st>*](B22475_02.xhtml#_idTextAnchor035) <st c="2043">for parsing HTML.</st>
    <st c="2062">We also have a couple of Microsoft Word-related packages, such as</st>
    `<st c="2128">python_docx</st>`<st c="2139">, which helps with creating and updating
    Microsoft Word (</st>`<st c="2196">.docx</st>`<st c="2201">) files, and</st> `<st
    c="2215">docx2txt</st>`<st c="2223">, which extracts text and images from</st>
    `<st c="2261">.docx</st>` <st c="2266">files.</st> <st c="2274">The</st> `<st
    c="2278">jq</st>` <st c="2280">package is a lightweight</st> <st c="2306">JSON
    processor.</st>
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="1955">第一个可能看起来很熟悉，</st> `<st c="1989">bs4</st>` <st c="1992">(代表Beautiful
    Soup 4)，因为我们曾在</st> [*<st c="2034">第二章</st>*](B22475_02.xhtml#_idTextAnchor035)
    <st c="2043">用它来解析HTML。</st> <st c="2062">我们还有一些与Microsoft Word相关的包，例如</st> `<st
    c="2128">python_docx</st>`<st c="2139">，它有助于创建和更新Microsoft Word (</st>`<st c="2196">.docx</st>`<st
    c="2201">) 文件，以及</st> `<st c="2215">docx2txt</st>`<st c="2223">，它从</st> `<st c="2261">.docx</st>`
    <st c="2266">文件中提取文本和图像。</st> <st c="2274">`<st c="2278">jq</st>` <st c="2280">包是一个轻量级的</st>
    <st c="2306">JSON处理器。</st>
- en: <st c="2321">Next, we are going to take an extra step you likely will not have
    to take in a</st> *<st c="2401">real</st>* <st c="2405">situation, which is turning
    our PDF document into a bunch of other formats, so we can test out the extraction
    of those formats.</st> <st c="2534">We are going to add a whole new</st> `<st
    c="2566">document loaders</st>` <st c="2582">section to our code right after the</st>
    <st c="2619">OpenAI setup.</st>
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="2321">接下来，我们将采取一个额外的步骤，你可能不需要在</st> *<st c="2401">实际</st> * <st c="2405">情况下采取，即将我们的PDF文档转换为其他多种格式，以便我们可以测试这些格式的提取。</st>
    <st c="2534">我们将在OpenAI设置之后立即在我们的代码中添加一个全新的</st> `<st c="2566">文档加载器</st>` <st
    c="2582">部分。</st>
- en: <st c="2632">In this section, we will provide code to generate the files, and
    then the different document loaders and their related packages to extract data
    from those types of files.</st> <st c="2804">Right now, we have a PDF version
    of our document.</st> <st c="2854">We will need an HTML/web version, a Microsoft
    Word version, and a JSON version of</st> <st c="2936">our document.</st>
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="2632">在本节中，我们将提供生成文件的代码，然后是不同文档加载器和它们相关的包，用于从这些类型的文件中提取数据。</st> <st c="2804">目前，我们有一个文档的PDF版本。</st>
    <st c="2854">我们需要文档的HTML/web版本、Microsoft Word版本和JSON版本。</st>
- en: <st c="2949">We are going to start with a new cell under the OpenAI setup cell,
    where we will import the new packages we need for</st> <st c="3067">these conversions:</st>
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="2949">我们将在OpenAI设置单元格下创建一个新的单元格，我们将导入进行这些转换所需的</st> <st c="3067">新包：</st>
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: <st c="3139">As we mentioned, the</st> `<st c="3161">BeautifulSoup</st>` <st
    c="3174">package helps us parse HTML-based web pages.</st> <st c="3220">We also
    import</st> `<st c="3235">docx</st>`<st c="3239">, which represents the Microsoft
    Docx word processing format.</st> <st c="3301">Lastly, we import</st> `<st c="3319">json</st>`
    <st c="3323">to interpret and manage</st> `<st c="3348">json</st>` <st c="3352">formatted
    code.</st>
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="3139">正如我们提到的，</st> `<st c="3161">BeautifulSoup</st>` <st c="3174">包帮助我们解析基于HTML的网页。</st>
    <st c="3220">我们还导入</st> `<st c="3235">docx</st>`<st c="3239">，它代表Microsoft Docx文档格式。</st>
    <st c="3301">最后，我们导入</st> `<st c="3319">json</st>` <st c="3323">来解释和管理</st> `<st
    c="3348">json</st>` <st c="3352">格式的代码。</st>
- en: <st c="3368">Next, we want to establish the filenames we will save each of</st>
    <st c="3431">our formats:</st>
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="3368">接下来，我们想要确定我们将保存每种格式的</st> <st c="3431">文件名：</st>
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: <st c="3649">Here, we are</st> <st c="3663">defining the paths for each of the
    files that we use in this code, and later when we use the loaders to load each
    document.</st> <st c="3787">These are going to be the final files that we generate
    from the original PDF document we have</st> <st c="3881">been using.</st>
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="3649">在这里，我们</st> <st c="3663">定义了我们在代码中使用的每个文件的路径，稍后当我们使用加载器加载每个文档时。</st>
    <st c="3787">这些将是我们将从我们一直在使用的原始PDF文档中生成的最终文件。</st>
- en: <st c="3892">And then this key part of our new code will extract the text from
    the PDF and use it to generate all of these new types</st> <st c="4013">of documents:</st>
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="3892">然后，我们新代码的这个关键部分将提取PDF中的文本，并使用它来生成所有这些新类型的</st> <st c="4013">文档：</st>
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: <st c="4497">We generate an HTML, Word, and JSON version of our document in
    a very basic sense.</st> <st c="4581">If you were generating these documents to
    actually use in a pipeline, we recommend applying more formatting and extraction,
    but for the purposes of this demonstration, this will provide us with the</st>
    <st c="4779">necessary data.</st>
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="4497">我们在非常基本的意义上生成了文档的HTML、Word和JSON版本。</st> <st c="4581">如果您生成这些文档是为了在实际的管道中使用，我们建议应用更多的格式化和提取，但为了演示的目的，这将为我们提供</st>
    <st c="4779">必要的数据。</st>
- en: <st c="4794">Next, we are going</st> <st c="4813">to add our document loaders
    under the indexing stage of our code.</st> <st c="4880">We have worked with the
    first two document loaders already, which we will show in this code lab, but updated
    so that they can be used interchangeably.</st> <st c="5031">For each document
    loader, we show what package imports are specific to that loader alongside the
    loader code.</st> <st c="5141">In the early chapters, we used a web loader that
    loaded directly from a website, so if that is a use case you have, refer to that
    document loader.</st> <st c="5288">In the meantime, we are sharing a slightly
    different type of document loader here that is focused on using local HTML files,
    such as the one we just generated.</st> <st c="5448">Here is the code for this</st>
    <st c="5474">HTML loader:</st>
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="4794">接下来，我们将在代码的索引阶段添加我们的文档加载器。</st> <st c="4813">我们已经使用过前两个文档加载器，我们将在本代码实验室中展示它们，但进行了更新，以便它们可以互换使用。</st>
    <st c="4880">对于每个文档加载器，我们展示了与加载器代码相关的特定于该加载器的包导入。</st> <st c="5031">在早期章节中，我们使用了一个直接从网站加载的Web加载器，因此如果您有这种情况，请参考该文档加载器。</st>
    <st c="5288">同时，我们在这里分享了一种略微不同的文档加载器，它专注于使用本地HTML文件，例如我们刚刚生成的文件。</st> <st c="5448">以下是此</st>
    <st c="5474">HTML加载器的代码：</st>
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: <st c="5602">Here, we use the HTML file we defined earlier to load the code
    from an HTML document.</st> <st c="5689">The final variable,</st> `<st c="5709">docs</st>`<st
    c="5713">, can be used interchangeably with any other</st> *<st c="5758">docs</st>*
    <st c="5762">we define in the following document loaders.</st> <st c="5808">The
    way this code works, you can only use one loader at a time, and it will replace
    the docs with its version of the docs (including a metadata source tag of what
    document it came from).</st> <st c="5995">If you run this cell and then skip down
    to run the splitter cell, you can run the remaining code in the lab and see similar
    results from what is the same data coming from different source file types.</st>
    <st c="6195">We did have to make a slight update later in the code, which we will
    note in</st> <st c="6272">a moment.</st>
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="5602">在这里，我们使用之前定义的HTML文件来加载HTML文档中的代码。</st> <st c="5689">最终的变量，</st>
    `<st c="5709">docs</st>`<st c="5713">，可以与在以下文档加载器中定义的任何其他</st> *<st c="5758">docs</st>*
    <st c="5762">互换使用。</st> <st c="5808">这种代码的工作方式是，您一次只能使用一个加载器，并且它会用其版本的文档替换docs（包括一个元数据源标签，表明文档来自何处）。</st>
    <st c="5995">如果您运行此单元格，然后跳到运行拆分单元格，您可以在实验室中运行剩余的代码，并看到来自不同源文件类型相同数据的类似结果。</st>
    <st c="6195">我们后来在代码中不得不进行一些小的更新，我们将在</st> <st c="6272">稍后说明。</st>
- en: <st c="6281">There are some alternative HTML loaders listed on the LangChain
    website that you can</st> <st c="6367">see here:</st>
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="6281">LangChain网站上列出了一些备选的HTML加载器，您可以在以下链接中查看：</st> <st c="6367">[此处](https://python.langchain.com/v0.2/docs/how_to/document_loader_html/)</st>
- en: '[<st c="6376">https://python.langchain.com/v0.2/docs/how_to/document_loader_html/</st>](https://python.langchain.com/v0.2/docs/how_to/document_loader_html/
    )'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[<st c="6376">https://python.langchain.com/v0.2/docs/how_to/document_loader_html/</st>](https://python.langchain.com/v0.2/docs/how_to/document_loader_html/
    )'
- en: <st c="6444">The next file type we will talk about is the other type we have
    been working with already,</st> <st c="6536">the PDF:</st>
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="6444">接下来，我们将讨论的另一种文件类型是我们之前已经使用过的类型，</st> <st c="6536">即PDF：</st>
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: <st c="6796">Here, we have a</st> <st c="6813">slightly more streamlined version
    of the code we have used previously to extract the data from the PDF.</st> <st
    c="6917">Using this new approach shows you an alternative way to access this data,
    but either will work for you in your code, ultimately loading up the docs with
    the data pulled from the PDF using</st> `<st c="7105">PdfReader</st>` <st c="7114">from</st>
    `<st c="7120">PyPDF2</st>`<st c="7126">.</st>
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="6796">在这里，我们有我们之前用于从PDF中提取数据的代码的一个稍微精简的版本。</st> <st c="6813">使用这种新方法向您展示了访问这些数据的另一种方式，但无论哪种方式都可以在您的代码中工作，最终使用</st>
    `<st c="7105">PdfReader</st>` <st c="7114">从</st> `<st c="7120">PyPDF2</st>`<st
    c="7126">。</st>
- en: <st c="7127">It should be noted that there are numerous and very capable ways
    to load PDF documents into LangChain, which is supported by many integrations
    with popular tools for PDF extraction.</st> <st c="7310">Here are a few:</st>
    `<st c="7326">PyPDF2</st>` <st c="7332">(what we use here),</st> `<st c="7353">PyPDF</st>`<st
    c="7358">,</st> `<st c="7360">PyMuPDF</st>`<st c="7367">,</st> `<st c="7369">MathPix</st>`<st
    c="7376">,</st> `<st c="7378">Unstructured</st>`<st c="7390">,</st> `<st c="7392">AzureAIDocumentIntelligenceLoader</st>`<st
    c="7425">,</st> <st c="7427">and</st> `<st c="7431">UpstageLayoutAnalysisLoader</st>`<st
    c="7458">.</st>
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="7127">应该注意的是，有众多非常强大的方法可以将 PDF 文档加载到 LangChain 中，这得益于与许多流行 PDF 提取工具的集成。</st>
    <st c="7310">以下是一些方法：</st> `<st c="7326">PyPDF2</st>` <st c="7332">(我们在这里使用)，</st>
    `<st c="7353">PyPDF</st>`<st c="7358">,</st> `<st c="7360">PyMuPDF</st>`<st c="7367">,</st>
    `<st c="7369">MathPix</st>`<st c="7376">,</st> `<st c="7378">Unstructured</st>`<st
    c="7390">,</st> `<st c="7392">AzureAIDocumentIntelligenceLoader</st>`<st c="7425">,</st>
    <st c="7427">和</st> `<st c="7431">UpstageLayoutAnalysisLoader</st>`<st c="7458">。</st>
- en: <st c="7459">We recommend you look at the latest list of PDF document loaders.</st>
    <st c="7526">LangChain provides a helpful set of tutorials for many of</st> <st
    c="7584">them here:</st>
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="7459">我们建议您查看最新的 PDF 文档加载器列表。</st> <st c="7526">LangChain 在这里提供了一系列有用的教程，涵盖了其中许多：</st>
- en: '[<st c="7594">https://python.langchain.com/v0.2/docs/how_to/document_loader_pdf/</st>](https://python.langchain.com/v0.2/docs/how_to/document_loader_pdf/
    )'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[<st c="7594">https://python.langchain.com/v0.2/docs/how_to/document_loader_pdf/</st>](https://python.langchain.com/v0.2/docs/how_to/document_loader_pdf/
    )'
- en: <st c="7661">Next, we will load the data from a Microsoft</st> <st c="7707">Word
    document:</st>
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="7661">接下来，我们将从 Microsoft</st> <st c="7707">Word 文档中加载数据：</st>
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: <st c="7841">This code uses the</st> `<st c="7861">Docx2txtLoader</st>` <st
    c="7875">document loader from LangChain to turn the Word document we previously
    generated into text and load it up into our</st> `<st c="7991">docs</st>` <st
    c="7995">variable that can be later used by the splitter.</st> <st c="8045">Again,
    stepping through the rest of the code will work with this data, just</st> <st
    c="8121">as it did with the HTML or PDF documents.</st> <st c="8163">There are
    many options for loading Word documents as well, which you can find listed</st>
    <st c="8248">here:</st> [<st c="8254">https://python.langchain.com/v0.2/docs/integrations/document_loaders/microsoft_word/</st>](https://python.langchain.com/v0.2/docs/integrations/document_loaders/microsoft_word/
    )
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="7841">此代码使用了</st> `<st c="7861">Docx2txtLoader</st>` <st c="7875">文档加载器来自
    LangChain，将我们之前生成的 Word 文档转换为文本并加载到我们的</st> `<st c="7991">docs</st>` <st c="7995">变量中，该变量可以稍后由分割器使用。</st>
    <st c="8045">同样，遍历其余代码将使用这些数据，就像它处理 HTML 或 PDF 文档一样。</st> <st c="8121">此外，还有许多加载
    Word 文档的选项，您可以在以下位置找到列表：</st> [<st c="8254">https://python.langchain.com/v0.2/docs/integrations/document_loaders/microsoft_word/</st>](https://python.langchain.com/v0.2/docs/integrations/document_loaders/microsoft_word/
    )
- en: <st c="8338">Lastly, we see a similar approach with the</st> <st c="8382">JSON
    loader:</st>
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="8338">最后，我们看到与 JSON 加载器类似的方法：</st>
- en: '[PRE7]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '<st c="8538">Here, we use a JSON loader to load data that was stored in a JSON
    object format, but the results are the same: a</st> `<st c="8652">docs</st>` <st
    c="8656">variable that can be passed to the splitter and converted into the format
    we use throughout the remaining code.</st> <st c="8769">Other options for JSON
    loaders can be</st> <st c="8807">found here:</st>'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="8538">在这里，我们使用 JSON 加载器加载存储在 JSON 对象格式中的数据，但结果是一样的：一个</st> `<st c="8652">docs</st>`
    <st c="8656">变量，可以传递给分割器并转换为我们在剩余代码中使用的格式。</st> <st c="8769">JSON 加载器的其他选项可以在以下位置找到：</st>
- en: '[<st c="8818">https://python.langchain.com/v0.2/docs/how_to/document_loader_json/</st>](https://python.langchain.com/v0.2/docs/how_to/document_loader_json/
    )'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[<st c="8818">https://python.langchain.com/v0.2/docs/how_to/document_loader_json/</st>](https://python.langchain.com/v0.2/docs/how_to/document_loader_json/
    )'
- en: <st c="8886">Note that some document loaders add additional metadata to the</st>
    `<st c="8950">metadata</st>` <st c="8958">dictionary within the</st> `<st c="8981">Document</st>`
    <st c="8989">objects that are generated during this process.</st> <st c="9038">This
    is causing some issues with our code when we add our own metadata.</st> <st c="9110">To
    fix this, we update these lines when we index and create the</st> <st c="9174">vector
    store:</st>
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="8886">请注意，一些文档加载器会在生成文档对象的过程中向</st> `<st c="8950">metadata</st>` <st
    c="8958">字典中添加额外的元数据。</st> <st c="8981">当我们添加自己的元数据时，这会导致我们的代码出现一些问题。</st> <st
    c="9038">为了解决这个问题，我们在索引和创建向量存储时更新这些行：</st>
- en: '[PRE8]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: <st c="9475">We also update the code in the final output to test the response,
    changing the second line in this code to handle the changed</st> `<st c="9602">metadata</st>`
    <st c="9610">tag:</st>
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="9475">我们还更新了最终输出中的代码以测试响应，将此代码的第二行更改为处理已更改的</st> `<st c="9602">元数据</st>`
    <st c="9610">标签：</st>
- en: '[PRE9]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: <st c="9797">Run each loader</st> <st c="9813">and then run the remaining code
    to see each document in action!</st> <st c="9878">There are numerous more integrations
    with third parties, allowing you to access just about any data source you can
    imagine and format that data in a way that you can better utilize LangChain’s
    other components.</st> <st c="10089">Take a look at more examples here on the
    LangChain</st> <st c="10140">website:</st> [<st c="10149">https://python.langchain.com/docs/modules/data_connection/document_loaders/</st>](https://python.langchain.com/docs/modules/data_connection/document_loaders/
    )
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="9797">运行每个加载器</st> <st c="9813">然后运行剩余的代码，以查看每个文档的实际效果！</st> <st c="9878">还有许多与第三方集成的例子，允许您访问几乎任何可以想象的数据源，并以一种更好地利用LangChain其他组件的方式格式化数据。</st>
    <st c="10089">在此LangChain网站上查看更多示例：</st> <st c="10140">网站：</st> [<st c="10149">https://python.langchain.com/docs/modules/data_connection/document_loaders/</st>](https://python.langchain.com/docs/modules/data_connection/document_loaders/
    )
- en: <st c="10224">Document loaders play a supporting and very important role in
    your RAG application.</st> <st c="10309">But for RAG-specific applications that
    typically utilize</st> *<st c="10366">chunks</st>* <st c="10372">of your data,
    document loaders are not nearly as useful until you pass them through a text splitter.</st>
    <st c="10474">Next, we will review text splitters and how each one can be used
    to improve your</st> <st c="10555">RAG application.</st>
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="10224">文档加载器在您的RAG应用中扮演着支持和非常重要的角色。</st> <st c="10309">但对于通常利用</st> *<st
    c="10366">数据块</st>* <st c="10372">的RAG特定应用来说，直到您通过文本分割器处理它们之前，文档加载器几乎没有什么用处。</st>
    <st c="10474">接下来，我们将回顾文本分割器以及如何使用每个分割器来改进您的</st> <st c="10555">RAG应用。</st>
- en: <st c="10571">Code lab 11.2 – Text splitters</st>
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="10571">代码实验室11.2 – 文本分割器</st>
- en: <st c="10602">The file you need to access from the GitHub repository is</st>
    <st c="10661">titled</st> `<st c="10668">CHAPTER11-2_TEXT_SPLITTERS.ipynb</st>`<st
    c="10700">.</st>
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="10602">您需要从GitHub仓库访问的文件名为</st> <st c="10661">标题为</st> `<st c="10668">CHAPTER11-2_TEXT_SPLITTERS.ipynb</st>`<st
    c="10700">。</st>
- en: <st c="10701">Text splitters</st> <st c="10717">split a document into chunks
    that can be used for retrieval.</st> <st c="10778">Larger documents pose a threat
    to many parts of our RAG application and the splitter is our first line of defense.</st>
    <st c="10893">If you were able to vectorize a very large document, the larger
    the document, the more context representation you will lose in the vector embedding.</st>
    <st c="11042">But this assumes you can even vectorize a very large document, which
    you often can’t!</st> <st c="11128">Most embedding models have relatively small
    limits on the size of documents we can pass to it compared to the large documents
    many of us work with.</st> <st c="11276">For example, the context length for the
    OpenAI model we are using to generate our embeddings is 8,191 tokens.</st> <st
    c="11386">If we try to pass a document larger than that to the model, it will
    generate an error.</st> <st c="11473">These are the main reasons splitters exist,
    but these are not the only complexities introduced with this step in</st> <st
    c="11586">the process.</st>
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="10701">文本分割器</st> <st c="10717">将文档分割成可用于检索的块。</st> <st c="10778">较大的文档对我们的RAG应用中的许多部分构成了威胁，分割器是我们的第一道防线。</st>
    <st c="10893">如果您能够将一个非常大的文档向量化，那么文档越大，在向量嵌入中丢失的上下文表示就越多。</st> <st c="11042">但这是假设您甚至能够将一个非常大的文档向量化，而这通常是不可能的！</st>
    <st c="11128">与许多我们处理的大文档相比，大多数嵌入模型对我们可以传递给它的文档大小有相对较小的限制。</st> <st c="11276">例如，我们用于生成嵌入的OpenAI模型的上下文长度为8,191个标记。</st>
    <st c="11386">如果我们尝试向模型传递比这更大的文档，它将生成一个错误。</st> <st c="11473">这些都是分割器存在的主要原因，但这些并不是在处理步骤中引入的唯一复杂性。</st>
- en: <st c="11598">The key element</st> <st c="11615">of text splitters for us to
    consider is how they split the text.</st> <st c="11680">Let’s say you have 100
    paragraphs that you want to split up.</st> <st c="11741">In some cases, there
    may be two or three that are semantically meant to be together, such as the paragraphs
    in this one section.</st> <st c="11870">In some cases, you may have a section
    title, a URL, or some other type of text.</st> <st c="11950">Ideally, you want
    to keep the semantically related pieces of text together, but this can be much
    more complex than it first seems!</st> <st c="12081">For a real-world example
    of this, go to this website and</st> <st c="12138">copy in a large set of</st>
    <st c="12161">text:</st> [<st c="12167">https://chunkviz.up.railway.app/</st>](https://chunkviz.up.railway.app/)<st
    c="12199">.</st>
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="11598">我们需要考虑的文本分割器的关键元素是它们如何分割文本。</st> <st c="11615">假设你想要分割100个段落。</st>
    <st c="11680">在某些情况下，可能有两三个段落在语义上应该放在一起，比如这个部分中的段落。</st> <st c="11741">在某些情况下，你可能有一个章节标题、一个URL或某些其他类型的文本。</st>
    <st c="11870">理想情况下，你希望将语义相关的文本片段放在一起，但这可能比最初看起来要复杂得多！</st> <st c="11950">为了一个现实世界的例子，请访问这个网站并</st>
    <st c="12081">复制一大块</st> <st c="12138">文本：</st> [<st c="12167">https://chunkviz.up.railway.app/</st>](https://chunkviz.up.railway.app/)<st
    c="12199">。</st>
- en: '<st c="12200">ChunkViz is a</st> <st c="12215">utility created by Greg Kamradt
    that helps you visualize how your text splitter is working.</st> <st c="12307">Change
    the parameters for the splitters to use what we are using: a chunk size of</st>
    `<st c="12389">1000</st>` <st c="12393">and a chunk overlap of</st> `<st c="12417">200</st>`<st
    c="12420">. Try the character splitter compared to the recursive character text
    splitter.</st> <st c="12500">Note that with the example they provide shown in</st>
    *<st c="12549">Figure 11</st>**<st c="12558">.1</st>*<st c="12560">, the recursive
    character splitter captures all of the paragraphs separately at around a</st>
    `<st c="12649">434</st>` <st c="12652">chunk size:</st>'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="12200">ChunkViz是由Greg Kamradt创建的一个</st> <st c="12215">实用工具，帮助你可视化你的文本分割器是如何工作的。</st>
    <st c="12307">更改分割器的参数以使用我们正在使用的参数：块大小为</st> `<st c="12389">1000</st>` <st c="12393">和块重叠为</st>
    `<st c="12417">200</st>`<st c="12420">。尝试与递归字符文本分割器相比的字符分割器。</st> <st c="12500">请注意，他们提供的示例显示在</st>
    *<st c="12549">图11</st>**<st c="12558">.1</st>*<st c="12560">中，递归字符分割器在约</st>
    `<st c="12649">434</st>` <st c="12652">块大小处分别捕获了所有段落：</st>
- en: '![Figure 11.1 – Recursive Character Text Splitter captures whole paragraphs
    at 434 characters](img/B22475_11_01.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图11.1 – 递归字符文本分割器在434个字符处捕获整个段落](img/B22475_11_01.jpg)'
- en: <st c="13572">Figure 11.1 – Recursive Character Text Splitter captures whole
    paragraphs at 434 characters</st>
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="13572">图11.1 – 递归字符文本分割器在434个字符处捕获整个段落</st>
- en: <st c="13663">As you increase</st> <st c="13679">the chunk size, it stays on
    the paragraph splits well but eventually gets more and more paragraphs per chunk.</st>
    <st c="13790">Note, though, that this is going to be different for different text.</st>
    <st c="13859">If you have text with very long paragraphs, you will need a larger
    chunk setting to capture</st> <st c="13951">whole paragraphs.</st>
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="13663">随着块大小的增加，它很好地保持在段落分割上，但最终每个块中会有越来越多的段落。</st> <st c="13790">然而，请注意，这会因不同的文本而异。</st>
    <st c="13859">如果你有非常长的段落文本，你需要更大的块设置来捕获</st> <st c="13951">整个段落。</st>
- en: <st c="13968">Meanwhile, if you try the character splitter, it will cut off
    in the middle of a sentence on</st> <st c="14062">any setting:</st>
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="13968">同时，如果你尝试字符分割器，它将在任何设置下在句子的中间切断：</st>
- en: '![Figure 11.2 – Character splitter captures partial paragraphs at 434 characters](img/B22475_11_02.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图11.2 – 字符分割器在434个字符处捕获部分段落](img/B22475_11_02.jpg)'
- en: <st c="14652">Figure 11.2 – Character splitter captures partial paragraphs at
    434 characters</st>
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="14652">图11.2 – 字符分割器在434个字符处捕获部分段落</st>
- en: <st c="14730">This split of a</st> <st c="14747">sentence could have a significant
    impact on the ability of your chunks to capture all of the important semantic
    meanings of the text within them.</st> <st c="14893">You can offset this by changing
    the chunk overlap, but you still have partial paragraphs, which will equate to
    noise to your LLM, distracting it away from providing the</st> <st c="15062">optimal
    response.</st>
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="14730">这个句子的分割可能会对你的块捕获其中所有重要语义意义的能力产生重大影响。</st> <st c="14893">你可以通过改变块重叠来抵消这一点，但你仍然会有部分段落，这将对你的LLM来说等同于噪音，分散了它提供</st>
    <st c="15062">最佳响应的能力。</st>
- en: <st c="15079">Let’s step through actual coding examples of each to understand
    some of the</st> <st c="15156">options available.</st>
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="15079">让我们逐步分析每个实际的编码示例，以了解一些可用的</st> <st c="15156">选项。</st>
- en: <st c="15174">Character text splitter</st>
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="15174">字符文本分割器</st>
- en: <st c="15198">This is the</st> <st c="15211">simplest approach to splitting
    your document.</st> <st c="15257">A</st> <st c="15258">text splitter enables you
    to divide your text into arbitrary N-character-sized chunks.</st> <st c="15346">You
    can improve this slightly by adding a separator parameter, such as</st> `<st c="15417">\n</st>`<st
    c="15419">. But this is a great place to start to understand how chunking works,
    and then we can move on to more approaches that work better but have added complexity</st>
    <st c="15576">to them.</st>
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="15198">这是分割你的文档的最简单方法。</st> <st c="15211">文本分割器允许你将文本分割成任意N字符大小的块。</st>
    <st c="15257">你可以通过添加一个分隔符参数稍微改进这一点，例如</st> `<st c="15417">\n</st>`<st c="15419">。但这是一个了解块分割工作原理的好起点，然后我们可以转向更有效但增加了复杂性的方法。</st>
- en: <st c="15584">Here is</st> <st c="15593">code that uses the</st> `<st c="15612">CharacterTextSplitter</st>`
    <st c="15633">object with our documents that can be used interchangeably</st>
    <st c="15692">with the other</st> <st c="15708">splitter outputs:</st>
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="15584">这里有一个</st> <st c="15593">使用</st> `<st c="15612">CharacterTextSplitter</st>`
    <st c="15633">对象与我们的文档的代码，这些文档可以与其他</st> <st c="15692">分割器输出</st> <st c="15633">互换使用：</st>
- en: '[PRE10]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: <st c="15948">The output from the first split (</st>`<st c="15982">split[0]</st>`<st
    c="15991">) looks</st> <st c="16000">like this:</st>
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="15948">第一次分割的输出（</st>`<st c="15982">split[0]</st>`<st c="15991">）看起来</st>
    <st c="16000">像这样：</st>
- en: '[PRE11]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: <st c="17042">There are a lot of</st> `<st c="17062">\n</st>` <st c="17064">(also
    called newline) markup characters, and some</st> `<st c="17115">\u</st>` <st c="17117">as
    well.</st> <st c="17127">We see that it counts out around 1,000 characters, finds
    the</st> `<st c="17188">\n</st>` <st c="17190">character nearest to that, and
    that</st> <st c="17227">becomes the first chunk.</st> <st c="17252">It is right
    in the middle of a sentence, which could</st> <st c="17305">be problematic!</st>
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="17042">有很多</st> `<st c="17062">\n</st>` <st c="17064">(也称为换行符) 标记字符，还有一些</st>
    `<st c="17115">\u</st>` <st c="17117">也是如此。</st> <st c="17127">我们看到它大约计数了1,000个字符，找到</st>
    `<st c="17188">\n</st>` <st c="17190">最近的字符，然后</st> <st c="17227">它成为第一个块。</st>
    <st c="17252">它位于句子的中间，这可能会</st> <st c="17305">有问题！</st>
- en: <st c="17320">The next</st> <st c="17330">chunk looks</st> <st c="17342">like
    this:</st>
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="17320">下一个</st> <st c="17330">块看起来</st> <st c="17342">像这样：</st>
- en: '[PRE12]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: <st c="18381">As you can see here, it backtracked a little, which is due to
    the chunk overlap we set of 200 characters.</st> <st c="18488">It then goes forward
    another 1,000 characters from there and breaks on another</st> `<st c="18567">\</st>``<st
    c="18568">n</st>` <st c="18569">character.</st>
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="18381">正如你所见，它稍微回溯了一点，这是由于我们设置的200个字符的块重叠。</st> <st c="18488">然后它从那里再向前移动1,000个字符，并在另一个</st>
    `<st c="18567">\</st>``<st c="18568">n</st>` <st c="18569">字符处断开。</st>
- en: <st c="18580">Let’s step through the parameters</st> <st c="18615">for this:</st>
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="18580">让我们逐步分析这个</st> <st c="18615">参数：</st>
- en: '`<st c="18730">\n</st>`<st c="18732">, and it works for this document.</st>
    <st c="18766">But if you use</st> `<st c="18781">\n\n</st>` <st c="18785">(the
    double newline character) for your separator on this particular document, where
    there are no double newline characters, it never splits!</st> `<st c="18928">\n\n</st>`
    <st c="18932">is actually the default, so make sure you keep an eye on this and
    use a separator that will work with</st> <st c="19035">your content!</st>'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="18730">\n</st>`<st c="18732">，并且它适用于这份文档。</st> <st c="18766">但是如果你在这个特定文档中使用</st>
    `<st c="18781">\n\n</st>` <st c="18785">(双换行符字符) 作为分隔符，而在这个文档中没有双换行符，它永远不会分割！</st>
    `<st c="18928">\n\n</st>` <st c="18932">实际上是默认值，所以请确保你注意这一点，并使用一个与</st> <st c="19035">你的内容兼容的分隔符！</st>'
- en: '**<st c="19048">Chunk size</st>** <st c="19059">– This</st> <st c="19067">defines
    the arbitrary number of characters you are aiming for with your chunk size.</st>
    <st c="19151">This may still vary, such as at the end of the text, but for the
    most part, the chunks will be consistently</st> <st c="19259">this size.</st>'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="19048">块大小</st>** <st c="19059">– 这</st> <st c="19067">定义了你希望块大小达到的任意字符数。</st>
    <st c="19151">这可能会有些变化，例如在文本的末尾，但大部分块将保持这个大小。</st>'
- en: '**<st c="19269">Chunk overlap</st>** <st c="19283">– This is</st> <st c="19293">the
    amount of characters you would like overlapping in your sequential chunks.</st>
    <st c="19373">This is a simple way to make sure you are capturing all context
    within your chunks.</st> <st c="19457">For example, if you had no chunk overlap
    and cut a sentence in half, the majority of that context would likely not be captured
    in either chunk very well.</st> <st c="19611">But with overlap, you can get better
    coverage of this context on</st> <st c="19676">the edges.</st>'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="19269">块重叠</st>** <st c="19283">– 这是</st> <st c="19293">你希望在顺序块中重叠的字符数量。</st>
    <st c="19373">这是一种确保你捕捉到块内所有上下文的简单方法。</st> <st c="19457">例如，如果你没有块重叠并且将句子切半，那么大部分上下文可能都不会很好地被两个块捕捉到。</st>
    <st c="19611">但是，有了重叠，你可以在边缘获得更好的上下文覆盖。</st>'
- en: '**<st c="19686">Is separator regex</st>** <st c="19705">– This is</st> <st
    c="19715">yet another parameter that indicates whether the separator used is in</st>
    <st c="19786">Regex format.</st>'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="19686">分隔符正则表达式</st>** <st c="19705">– 这是</st> <st c="19715">另一个参数，表示所使用的分隔符是否为</st>
    <st c="19786">正则表达式格式。</st>'
- en: <st c="19799">In this case, we</st> <st c="19816">are setting the chunk size
    to</st> `<st c="19847">1000</st>` <st c="19851">and the chunk overlap to</st>
    `<st c="19877">200</st>`<st c="19880">. What we are saying here with this code
    is that we want it to use chunks that are smaller than 1,000 characters but with
    a 200-character overlap.</st> <st c="20027">This overlapping technique is similar
    to the sliding window technique you see in</st> **<st c="20108">convolutional
    neural networks</st>** <st c="20137">(</st>**<st c="20139">CNNs</st>**<st c="20143">)
    when</st> <st c="20150">you are</st> *<st c="20159">sliding</st>* <st c="20166">the
    window over smaller parts of the image with overlap so that you capture the context
    between the different windows.</st> <st c="20286">In this case, it is the context
    within the chunks that we are trying</st> <st c="20355">to capture.</st>
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="19799">在这种情况下，我们将块大小设置为</st> `<st c="19847">1000</st>` <st c="19851">并将块重叠设置为</st>
    `<st c="19877">200</st>`<st c="19880">. 这段代码所表达的意思是我们希望使用小于1,000个字符的块，但具有200个字符的重叠。</st>
    <st c="20027">这种重叠技术类似于你在</st> **<st c="20108">卷积神经网络</st>** <st c="20137">(</st>**<st
    c="20139">CNNs</st>**<st c="20143">) 中看到的滑动窗口技术，当你</st> <st c="20150">*<st c="20159">滑动</st>*
    <st c="20166">窗口覆盖图像的较小部分并重叠时，以便捕捉不同窗口之间的上下文。</st> <st c="20286">在这种情况下，我们试图捕捉的是块内的上下文。</st>
- en: <st c="20366">Here are some other things</st> <st c="20394">to note:</st>
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="20366">以下是一些其他需要注意的事项：</st>
- en: '`<st c="20449">Document</st>` <st c="20457">object to store our text, so we
    use the</st> `<st c="20498">create_documents</st>` <st c="20514">function that
    allows it to work in the next step when these documents are vectorized.</st> <st
    c="20601">If you want to obtain the string content directly, you can use the</st>
    `<st c="20668">split_text</st>` <st c="20678">function.</st>'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="20449">文档</st>` <st c="20457">对象来存储我们的文本，因此我们使用</st> `<st c="20498">create_documents</st>`
    <st c="20514">函数，以便在文档向量化时在下一步中使用它。</st> <st c="20601">如果你想直接获取字符串内容，可以使用</st>
    `<st c="20668">split_text</st>` <st c="20678">函数。</st>'
- en: '`<st c="20723">create_documents</st>` <st c="20739">expects a list of texts,
    so if you just have a string, you’ll need to wrap it in</st> `<st c="20821">[]</st>`<st
    c="20823">. In our case, we have already set</st> `<st c="20858">docs</st>` <st
    c="20862">as a list, so this requirement</st> <st c="20894">is satisfied.</st>'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="20723">create_documents</st>` <st c="20739">期望一个文本列表，所以如果你只有一个字符串，你需要将其包裹在</st>
    `<st c="20821">[]</st>`<st c="20823">. 在我们的例子中，我们已经将</st> `<st c="20858">docs</st>`
    <st c="20862">设置为一个列表，所以这个要求</st> <st c="20894">已经满足。</st>'
- en: '**<st c="20907">Splitting versus chunking</st>** <st c="20933">– These terms
    can be</st> <st c="20955">used interchangeably.</st>'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="20907">分割与分块</st>** <st c="20933">– 这些术语可以</st> <st c="20955">互换使用。</st>'
- en: <st c="20976">You can find more information about this specific text splitter
    on the LangChain</st> <st c="21058">website:</st> [<st c="21067">https://python.langchain.com/v0.2/docs/how_to/character_text_splitter/</st>](https://python.langchain.com/v0.2/docs/how_to/character_text_splitter/
    )
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="20976">你可以在LangChain</st> <st c="21058">网站上找到有关此特定文本分割器的更多信息：</st> [<st
    c="21067">https://python.langchain.com/v0.2/docs/how_to/character_text_splitter/</st>](https://python.langchain.com/v0.2/docs/how_to/character_text_splitter/
    )
- en: <st c="21137">The API documentation can be found</st> <st c="21173">here:</st>
    [<st c="21179">https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.CharacterTextSplitter.html</st>](https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.CharacterTextSplitter.html
    )
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="21137">API 文档可以在</st> <st c="21173">这里找到：</st> [<st c="21179">https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.CharacterTextSplitter.html</st>](https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.CharacterTextSplitter.html
    )
- en: <st c="21293">We can do better than this though; let’s take a look at a more
    sophisticated approach called</st> **<st c="21387">recursive character</st>**
    **<st c="21407">text splitting</st>**<st c="21421">.</st>
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="21293">我们可以做得更好；让我们看看一种更复杂的方法，称为</st> **<st c="21387">递归字符</st>** **<st
    c="21407">文本拆分</st>**<st c="21421">。</st>
- en: <st c="21422">Recursive character text splitter</st>
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="21422">递归字符文本拆分器</st>
- en: <st c="21456">We have</st> <st c="21464">seen this one before!</st> <st c="21487">We
    have used this</st> <st c="21505">splitter the most in our code labs so far because
    it is what LangChain recommends using when splitting generic text.</st> <st c="21622">That
    is what we</st> <st c="21638">are doing!</st>
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="21456">我们已经</st> <st c="21464">见过这个了！</st> <st c="21487">到目前为止，在我们的代码实验室中，我们最常使用这个</st>
    <st c="21505">拆分器，因为它正是LangChain推荐用于拆分通用文本的。</st> <st c="21622">这正是我们</st> <st
    c="21638">所做的事情！</st>
- en: <st c="21648">As the name states, this</st> <st c="21673">splitter recursively
    splits text, with the intention of keeping related pieces of text next to each
    other.</st> <st c="21781">You can pass a list of characters as a parameter and
    it will try to split those characters in order until the chunks are small enough.</st>
    <st c="21916">The default list is</st> `<st c="21936">["\n\n", "\n", " ", ""]</st>`<st
    c="21959">, which works well, but we are going to add</st> `<st c="22003">".</st>
    <st c="22006">"</st>` <st c="22007">to this list as well.</st> <st c="22030">This
    has the effect of trying to keep together all paragraphs, sentences defined by
    both</st> `<st c="22119">"\n"</st>` <st c="22123">and</st> `<st c="22128">".</st>
    <st c="22131">"</st>`<st c="22132">, and words as long</st> <st c="22152">as possible.</st>
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="21648">正如其名所示，这个</st> <st c="21673">拆分器会递归地拆分文本，目的是将相关的文本片段放在一起。</st>
    <st c="21781">你可以传递一个字符列表作为参数，并且它会尝试按顺序拆分这些字符，直到块的大小足够小。</st> <st c="21916">默认列表是</st>
    `<st c="21936">["\n\n", "\n", " ", ""]</st>`<st c="21959">，这效果很好，但我们打算也将</st>
    `<st c="22003">"。</st> <st c="22006">"</st>` <st c="22007">添加到这个列表中。</st> <st
    c="22030">这会使得尝试将所有段落、由</st> `<st c="22119">"\n"</st>` <st c="22123">和</st> `<st
    c="22128">"。</st> <st c="22131">"</st>`<st c="22132">定义的句子，以及尽可能长的单词</st> <st
    c="22152">放在一起。</st>
- en: <st c="22164">Here is</st> <st c="22173">our code:</st>
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="22164">以下是</st> <st c="22173">我们的代码：</st>
- en: '[PRE13]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: <st c="22364">Under the hood with this splitter, the chunks are split based
    on the</st> `<st c="22434">"\n\n"</st>` <st c="22440">separator, representing
    paragraph splits.</st> <st c="22483">But it doesn’t stop there; it will look at
    the chunk size, and if that is larger than the 1,000 we set, then it will split
    by the next separator (</st>`<st c="22629">"\n"</st>`<st c="22634">), and</st>
    <st c="22642">so on.</st>
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="22364">在这个拆分器内部，块是基于</st> `<st c="22434">"\n\n"</st>` <st c="22440">分隔符拆分的，代表段落拆分。</st>
    <st c="22483">但它不会停止在这里；它还会查看块的大小，如果它大于我们设置的1,000，那么它将使用下一个分隔符（</st>`<st c="22629">"\n"</st>`<st
    c="22634">），以此类推。</st>
- en: <st c="22648">Let’s talk</st> <st c="22660">about the recursive aspect of this
    that splits the text into chunks using a recursive algorithm.</st> <st c="22757">The
    algorithm will only be applied if the text provided is longer than the chunk size,
    but it follows</st> <st c="22859">these steps:</st>
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="22648">让我们谈谈</st> <st c="22660">这个递归方面，它使用递归算法将文本拆分成块。</st> <st c="22757">只有当提供的文本长度超过块大小时，算法才会被应用，但它遵循</st>
    <st c="22859">以下步骤：</st>
- en: <st c="22871">It finds the last space or newline character within the range</st>
    `<st c="22934">[chunk_size - chunk_overlap, chunk_size]</st>`<st c="22974">. This
    ensures that chunks are split at word boundaries or</st> <st c="23033">line breaks.</st>
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <st c="22871">它会在范围</st> `<st c="22934">[chunk_size - chunk_overlap, chunk_size]</st>`<st
    c="22974">内找到最后一个空格或换行符。这确保了块会在单词边界或</st> <st c="23033">行中断开。</st>
- en: '<st c="23045">If a suitable split point is found, it splits the text into two
    parts: the chunk before the split point and the remaining text after the</st>
    <st c="23183">split point.</st>'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <st c="23045">如果找到一个合适的拆分点，它会将文本拆分为两部分：拆分点之前的块和拆分点之后的剩余文本。</st>
- en: <st c="23195">It recursively applies the same splitting process to the remaining
    text until all chunks are within the</st> `<st c="23300">chunk_size</st>` <st
    c="23310">limit.</st>
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <st c="23195">它递归地应用相同的分割过程到剩余的文本中，直到所有块都在</st> `<st c="23300">chunk_size</st>`
    <st c="23310">限制内。</st>
- en: <st c="23317">Similar to the</st> <st c="23333">character splitter approach,
    the recursive splitter is driven largely by the chunk size you set, but then it
    combines this with the recursive approach outlined previously to provide a straightforward
    and logical way to properly capture context within</st> <st c="23584">your chunks.</st>
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="23317">与字符分割方法类似，递归分割器主要是由你设置的块大小驱动的，但它将此与之前概述的递归方法结合起来，提供了一种简单且逻辑的方法来正确地捕获你块中的上下文。</st>
    <st c="23584">你的块。</st>
- en: '`<st c="23596">RecursiveCharacterTextSplitter</st>` <st c="23627">is particularly
    useful when dealing with large text documents that need to be processed by language
    models with input size limitations.</st> <st c="23764">By splitting the text into
    smaller chunks, you can feed the chunks to the language model individually and
    then combine the results</st> <st c="23895">if needed.</st>'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '`<st c="23596">RecursiveCharacterTextSplitter</st>` <st c="23627">在处理需要通过具有输入大小限制的语言模型处理的较大文本文档时特别有用。</st>
    <st c="23764">通过将文本分割成更小的块，您可以单独将块喂给语言模型，然后在需要时将结果</st> <st c="23895">组合起来。</st>'
- en: <st c="23905">Clearly, recursive splitters are a step up from the character
    splitter, but they are still not splitting our content based on the semantics
    as much as just general separators such as paragraph and sentence breaks.</st>
    <st c="24120">But this will not handle cases where two paragraphs are semantically
    part of one ongoing thought that should really be captured together in their vector
    representations.</st> <st c="24290">Let’s see whether we can do better with the</st>
    **<st c="24334">semantic chunker</st>**<st c="24350">.</st>
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="23905">显然，递归分割器比字符分割器更进了一步，但它们仍然不是基于语义来分割我们的内容，而是基于一般的分隔符，如段落和句子断句。</st>
    <st c="24120">但这不会处理两个段落在语义上属于一个持续思考的情况，这些段落实际上应该在它们的向量表示中一起捕获。</st> <st c="24290">让我们看看我们是否可以用</st>
    **<st c="24334">语义块分割器</st>**<st c="24350">做得更好。</st>
- en: <st c="24351">Semantic chunker</st>
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="24351">语义块分割器</st>
- en: '<st c="24368">This is</st> <st c="24376">another one you may recognize, as
    we used it in the first code lab!</st> `<st c="24445">SemanticChunker</st>` <st
    c="24460">is an interesting one, currently listed as experimental, but described
    on the LangChain website as follows: “</st>*<st c="24570">First splits on sentences.</st>
    <st c="24598">Then (it) combines ones next to each other if they are semantically
    similar enough</st>*<st c="24680">.” In other words, the goal here is to avoid
    having to define this arbitrary chunk size number</st> <st c="24775">that was
    a key parameter that drives how the character and recursive splitters divide the
    text and focus the splits more on the semantics of the text you are splitting.</st>
    <st c="24944">Find out more about this</st> *<st c="24969">chunker</st>* <st c="24976">on
    the LangChain</st> <st c="24994">website:</st> [<st c="25003">https://python.langchain.com/docs/modules/data_connection/document_transformers/semantic-chunker</st>](https://python.langchain.com/docs/modules/data_connection/document_transformers/semantic-chunker
    )'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="24368">这是</st> <st c="24376">另一个你可能认识到的，因为我们已经在第一个代码实验室中使用过它了！</st> `<st
    c="24445">SemanticChunker</st>` <st c="24460">是一个有趣的工具，目前列为实验性，但在LangChain网站上描述如下：“</st>*<st
    c="24570">首先在句子上分割。</st> <st c="24598">然后（它）如果它们在语义上足够相似，就合并相邻的句子</st>*<st c="24680">。”换句话说，这里的目的是避免定义这个任意的块大小数字</st>
    <st c="24775">，这是一个关键参数，它驱动着字符和递归分割器如何分割文本，并使分割更关注于你正在分割的文本的语义。</st> <st c="24944">在LangChain</st>
    <st c="24969">网站上了解更多关于这个</st> *<st c="24976">分割器</st> * <st c="24994">的信息：</st>
    [<st c="25003">https://python.langchain.com/docs/modules/data_connection/document_transformers/semantic-chunker</st>](https://python.langchain.com/docs/modules/data_connection/document_transformers/semantic-chunker
    )
- en: <st c="25099">Under the hood,</st> `<st c="25116">SemanticChunker</st>` <st
    c="25131">splits your text into sentences, groups those sentences into groups
    of three sentences, and then merges them when they are similar in the</st> <st
    c="25270">embedding space.</st>
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="25099">在底层，</st> `<st c="25116">SemanticChunker</st>` <st c="25131">将你的文本分割成句子，将这些句子分成三句一组，然后在它们在</st>
    <st c="25270">嵌入空间中相似时将它们合并。</st>
- en: <st c="25286">When would this not work as well?</st> <st c="25321">When the
    semantics of your document is difficult to discern.</st> <st c="25382">For example,
    if you have a lot of code, addresses, names, internal reference IDs, and other
    text that has little semantic meaning, especially to an embedding model, this
    will likely reduce the ability of</st> `<st c="25586">SemanticChunker</st>` <st
    c="25601">to properly split your text.</st> <st c="25631">But in general,</st>
    `<st c="25647">SemanticChunker</st>` <st c="25662">has a lot of promise.</st>
    <st c="25685">Here is an example of the code to</st> <st c="25719">use it:</st>
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="25286">这种情况下不会像预期那样有效吗？</st> <st c="25321">当你的文档语义难以辨别时。</st> <st c="25382">例如，如果你有很多代码、地址、名称、内部参考ID和其他对嵌入模型来说语义意义很小的文本，这可能会降低</st>
    `<st c="25586">SemanticChunker</st>` <st c="25601">正确分割你的文本的能力。</st> <st c="25631">但总的来说，</st>
    `<st c="25647">SemanticChunker</st>` <st c="25662">有很大的潜力。</st> <st c="25685">以下是一个使用它的代码示例：</st>
    <st c="25719">：</st>
- en: '[PRE14]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: <st c="25958">Here, we import the</st> `<st c="25979">SemanticChunker</st>`
    <st c="25994">class from the</st> `<st c="26010">langchain_experimental.text_splitter</st>`
    <st c="26046">module.</st> <st c="26055">We use the same embedding model we used
    to vectorize our documents and pass them to the</st> `<st c="26143">SemanticChunker</st>`
    <st c="26158">class.</st> <st c="26166">Note that this costs a little money, as
    it uses the same OpenAI API key we used to generate our embeddings.</st> `<st
    c="26274">SemanticChunker</st>` <st c="26289">uses these embeddings to determine
    how to split the documents based on semantic similarity.</st> <st c="26382">We
    also set the</st> `<st c="26398">number_of_chunks</st>` <st c="26414">variable
    to</st> `<st c="26427">200</st>`<st c="26430">, which indicates the desired number
    of chunks to split the documents into.</st> <st c="26506">This determines the
    granularity of the splitting process.</st> <st c="26564">A higher value of</st>
    `<st c="26582">number_of_chunks</st>` <st c="26598">will result in more fine-grained
    splits, while a lower value will produce fewer and</st> <st c="26683">larger chunks.</st>
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="25958">在这里，我们从</st> `<st c="25979">SemanticChunker</st>` <st c="25994">类从</st>
    `<st c="26010">langchain_experimental.text_splitter</st>` <st c="26046">模块。</st>
    <st c="26055">我们使用与我们将文档向量化并传递给</st> `<st c="26143">SemanticChunker</st>` <st
    c="26158">类相同的嵌入模型。</st> <st c="26166">请注意，这会花费一点钱，因为它使用了我们用于生成嵌入的相同的OpenAI API密钥。</st>
    `<st c="26274">SemanticChunker</st>` <st c="26289">使用这些嵌入来确定如何根据语义相似度分割文档。</st>
    <st c="26382">我们还设置了</st> `<st c="26398">number_of_chunks</st>` <st c="26414">变量为</st>
    `<st c="26427">200</st>`<st c="26430">，这表示希望将文档分割成多少个块。</st> <st c="26506">这决定了分割过程的粒度。</st>
    <st c="26564">`<st c="26582">number_of_chunks</st>` <st c="26598">`的值越高，分割将越细粒度，而较低的值将产生更少且</st>
    <st c="26683">更大的块。</st>
- en: <st c="26697">This</st> <st c="26702">code lab</st> <st c="26712">is set up
    so that you can use each type of splitter at a time.</st> <st c="26775">Run through
    each splitter and then the rest of the code to see how each one impacts your results.</st>
    <st c="26873">Also try changing parameter settings, such as</st> `<st c="26919">chunk_size</st>`<st
    c="26929">,</st> `<st c="26931">chunk_overlap</st>` <st c="26944">and</st> `<st
    c="26949">number_of_chunks</st>`<st c="26965">, depending on what splitter you
    are using.</st> <st c="27009">Exploring all of these options will help give you
    a better sense of how they can be used for</st> <st c="27102">your projects.</st>
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="26697">这个</st> <st c="26702">代码实验室</st> <st c="26712">被设置为一次使用每种类型的分割器。</st>
    <st c="26775">运行每个分割器，然后运行剩余的代码，看看每个分割器如何影响你的结果。</st> <st c="26873">还可以尝试更改参数设置，例如</st>
    `<st c="26919">chunk_size</st>`<st c="26929">，</st> `<st c="26931">chunk_overlap</st>`
    <st c="26944">和</st> `<st c="26949">number_of_chunks</st>`<st c="26965">，具体取决于你使用的分割器。</st>
    <st c="27009">探索所有这些选项将帮助你更好地了解它们如何用于</st> <st c="27102">你的项目。</st>
- en: <st c="27116">For a last supporting component, we will discuss output parsers,
    responsible for shaping the final output from our</st> <st c="27232">RAG application.</st>
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="27116">作为最后一个支持组件，我们将讨论输出解析器，它们负责从我们的</st> <st c="27232">RAG应用</st> <st
    c="27232">中塑造最终输出。</st>
- en: <st c="27248">Code lab 11.3 – Output parsers</st>
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="27248">代码实验室 11.3 – 输出解析器</st>
- en: <st c="27279">The file you need to access from the GitHub repository is</st>
    <st c="27338">titled</st> `<st c="27345">CHAPTER11-3_OUTPUT_PARSERS.ipynb</st>`<st
    c="27377">.</st>
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="27279">你需要从GitHub仓库访问的文件</st> <st c="27338">被命名为</st> `<st c="27345">CHAPTER11-3_OUTPUT_PARSERS.ipynb</st>`<st
    c="27377">。</st>
- en: <st c="27378">The end</st> <st c="27386">result of any RAG application is going
    to be text, along with potentially some formatting, metadata, and some other related
    data.</st> <st c="27517">This output typically comes from the LLM itself.</st>
    <st c="27566">But there are times when you want to get a more structured format
    than just text.</st> <st c="27648">Output parsers are classes that help to structure
    the responses of the LLM wherever you use it in your RAG application.</st> <st
    c="27768">The output that this provides will then be provided to the next step
    in the chain, or in the case of all of our code labs, as the final output from</st>
    <st c="27916">the model.</st>
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="27378">任何RAG应用程序的最终结果都将是文本，可能还有一些格式、元数据和一些其他相关数据。</st> <st c="27517">这种输出通常来自LLM本身。</st>
    <st c="27566">但有时你希望得到比文本更结构化的格式。</st> <st c="27648">输出解析器是帮助在RAG应用程序中LLM的任何使用位置结构化响应的类。</st>
    <st c="27768">此提供的输出将随后提供给链中的下一个步骤，或者在我们的所有代码实验室中，作为模型的最终输出。</st>
- en: <st c="27926">We will cover two different output parsers at the same time, and
    use them at different times in our RAG pipeline.</st> <st c="28041">We start with
    the parser we know, the string</st> <st c="28086">output parser.</st>
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="27926">我们将同时介绍两种不同的输出解析器，并在我们的RAG管道的不同时间使用它们。</st> <st c="28041">我们首先介绍我们熟悉的解析器，即字符串</st>
    <st c="28086">输出解析器。</st>
- en: <st c="28100">Under the</st> `<st c="28111">relevance_prompt</st>` <st c="28127">function,
    add this code to a</st> <st c="28157">new cell:</st>
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="28100">在</st> `<st c="28111">relevance_prompt</st>` <st c="28127">函数下，将此代码添加到一个</st>
    <st c="28157">新单元格中：</st>
- en: '[PRE15]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: <st c="28262">Note that we were already using this in the LangChain chain code
    that appears later, but we are going to assign this parser to a variable called</st>
    `<st c="28408">str_output_parser</st>`<st c="28425">. Let’s talk about this type
    of parser in</st> <st c="28467">more depth.</st>
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="28262">请注意，我们已经在稍后出现的LangChain链代码中使用了这个功能，但我们将把这个解析器分配给一个名为</st> `<st
    c="28408">str_output_parser</st>`<st c="28425">的变量。让我们更深入地讨论这种解析器。</st>
- en: <st c="28478">String output parser</st>
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="28478">字符串输出解析器</st>
- en: <st c="28499">This is a</st> <st c="28510">basic output parser.</st> <st c="28531">In
    very simple approaches, as in our previous code labs, you can use the</st> `<st
    c="28604">StrOutputParser</st>` <st c="28619">class outright as</st> <st c="28638">the
    instance for your output parser.</st> <st c="28675">Or you can do what we just
    did and assign it to a variable, particularly if you expect to see it in multiple
    areas of the code, which we will.</st> <st c="28818">But we have seen this many
    times already.</st> <st c="28860">It takes the output from the LLM in both places
    it is used and outputs the string response from the LLM to the next link in the
    chain.</st> <st c="28995">The documentation for this parser can be found</st>
    <st c="29042">here:</st> [<st c="29048">https://api.python.langchain.com/en/latest/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html#langchain_core.output_parsers.string.StrOutputParser</st>](https://api.python.langchain.com/en/latest/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html#langchain_core.output_parsers.string.StrOutputParser
    )
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="28499">这是一个</st> <st c="28510">基本的输出解析器。</st> <st c="28531">在非常简单的方法中，就像我们之前的代码实验室一样，你可以直接使用</st>
    `<st c="28604">StrOutputParser</st>` <st c="28619">类作为</st> <st c="28638">你的输出解析器的实例。</st>
    <st c="28675">或者，你可以像我们刚才做的那样，将其分配给一个变量，特别是如果你预期在代码的多个区域看到它，我们将会这样做。</st> <st
    c="28818">但我们已经多次看到这种情况了。</st> <st c="28860">它从LLM在两个地方的使用中获取输出，并将LLM的字符串响应输出到链中的下一个链接。</st>
    <st c="28995">有关此解析器的文档可以在</st> <st c="29042">这里找到：</st> [<st c="29048">https://api.python.langchain.com/en/latest/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html#langchain_core.output_parsers.string.StrOutputParser</st>](https://api.python.langchain.com/en/latest/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html#langchain_core.output_parsers.string.StrOutputParser
    )
- en: <st c="29216">Let’s look at a new type of parser, the JSON</st> <st c="29262">output
    parser.</st>
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="29216">让我们看看一种新的解析器类型，JSON</st> <st c="29262">输出解析器。</st>
- en: <st c="29276">JSON output parser</st>
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="29276">JSON输出解析器</st>
- en: <st c="29295">As you</st> <st c="29302">can imagine, this output parser takes
    input from an</st> <st c="29355">LLM and outputs it as JSON.</st> <st c="29383">It
    is important to note that you may not need this parser, as many newer model providers
    support built-in ways to return structured output such as JSON and XML.</st> <st
    c="29544">This approach is for those that</st> <st c="29576">do not.</st>
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="29295">正如你所想，这个输出解析器从</st> <st c="29302">LLM</st> <st c="29355">那里获取输入，并以
    JSON 格式输出。</st> <st c="29383">需要注意的是，你可能不需要这个解析器，因为许多新的模型提供商支持内置的返回结构化输出（如 JSON
    和 XML）的方式。</st> <st c="29544">这种方法是为那些</st> <st c="29576">不支持此功能的人准备的。</st>
- en: <st c="29583">We start with some new imports, coming from a library we have
    already installed from</st> <st c="29669">LangChain (</st>`<st c="29680">langchain_core</st>`<st
    c="29695">):</st>
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="29583">我们首先添加一些新的导入，这些导入来自我们之前已安装的 LangChain 库（</st>`<st c="29669">langchain_core</st>`<st
    c="29695">）：</st>
- en: '[PRE16]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: <st c="29871">These lines import the necessary classes and modules from the</st>
    `<st c="29934">langchain_core</st>` <st c="29949">library and the</st> `<st c="29965">json</st>`
    <st c="29969">module.</st> `<st c="29978">JsonOutputParser</st>` <st c="29994">is
    used to parse the JSON output.</st> `<st c="30029">BaseModel</st>` <st c="30038">and</st>
    `<st c="30043">Field</st>` <st c="30048">are used to define the structure of the
    JSON output model.</st> `<st c="30108">Generation</st>` <st c="30118">is used
    to represent the generated output.</st> <st c="30162">And not surprisingly, we
    import a package for</st> `<st c="30208">json</st>`<st c="30212">, so that we
    can better manage our</st> <st c="30247">JSON inputs/outputs.</st>
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="29871">这些行从</st> `<st c="29934">langchain_core</st>` <st c="29949">库和</st>
    `<st c="29965">json</st>` <st c="29969">模块中导入必要的类和模块。</st> `<st c="29978">JsonOutputParser</st>`
    <st c="29994">用于解析 JSON 输出。</st> `<st c="30029">BaseModel</st>` <st c="30038">和</st>
    `<st c="30043">Field</st>` <st c="30048">用于定义 JSON 输出模型的结构。</st> `<st c="30108">Generation</st>`
    <st c="30118">用于表示生成的输出。</st> <st c="30162">不出所料，我们导入了一个用于</st> `<st c="30208">json</st>`<st
    c="30212">的包，以便更好地管理我们的</st> <st c="30247">JSON 输入/输出。</st>
- en: <st c="30267">Next, we will create a Pydantic model called</st> `<st c="30313">FinalOutputModel</st>`
    <st c="30329">that represents the structure of the</st> <st c="30367">JSON output:</st>
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="30267">接下来，我们将创建一个名为</st> `<st c="30313">FinalOutputModel</st>` <st c="30329">的
    Pydantic 模型，它表示 JSON 输出的结构：</st>
- en: '[PRE17]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: <st c="30589">It has two</st> <st c="30601">fields:</st> `<st c="30609">relevance_score</st>`
    <st c="30624">(float) and</st> `<st c="30637">answer</st>` <st c="30643">(string),
    along with their descriptions.</st> <st c="30685">In a</st> *<st c="30690">real-world</st>*
    <st c="30700">application, this model is likely to</st> <st c="30737">get substantially
    more complex, but this gives you a general concept of how it can</st> <st c="30821">be
    defined.</st>
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="30589">它有两个</st> <st c="30601">字段：</st> `<st c="30609">relevance_score</st>`
    <st c="30624">(浮点数) 和</st> `<st c="30637">answer</st>` <st c="30643">(字符串)，以及它们的描述。</st>
    <st c="30685">在实际应用中，这个模型可能会</st> *<st c="30690">变得更加复杂</st>*，但这也为你提供了一个如何定义它的基本概念。</st>
- en: <st c="30832">Next, we will create an instance of the</st> `<st c="30873">JsonOutputParser</st>`
    <st c="30889">parser:</st>
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="30832">接下来，我们将创建一个</st> `<st c="30873">JsonOutputParser</st>` <st c="30889">解析器的实例：</st>
- en: '[PRE18]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: <st c="30962">This line assigns</st> `<st c="30981">JsonOutputParser</st>` <st
    c="30997">with the</st> `<st c="31007">FinalOutputModel</st>` <st c="31023">class
    as a parameter to</st> `<st c="31048">json_parser</st>` <st c="31059">for use
    later in our code when we want to use</st> <st c="31106">this parser.</st>
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="30962">这一行将</st> `<st c="30981">JsonOutputParser</st>` <st c="30997">与</st>
    `<st c="31007">FinalOutputModel</st>` <st c="31023">类作为参数分配给</st> `<st c="31048">json_parser</st>`
    <st c="31059">，以便在代码中稍后使用此解析器时使用。</st>
- en: <st c="31118">Next, we are going add a new function right in between our two
    other helper functions, and then we will update</st> `<st c="31230">conditional_answer</st>`
    <st c="31248">to use that new function.</st> <st c="31275">This code goes under
    the existing</st> `<st c="31309">extract_score</st>` <st c="31322">function, which
    remains</st> <st c="31347">the same:</st>
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="31118">接下来，我们将在两个其他辅助函数之间添加一个新函数，然后我们将更新</st> `<st c="31230">conditional_answer</st>`
    <st c="31248">以使用该新函数。</st> <st c="31275">此代码位于现有的</st> `<st c="31309">extract_score</st>`
    <st c="31322">函数之下，该函数保持不变：</st>
- en: '[PRE19]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: <st c="31566">This</st> `<st c="31572">format_json_output</st>` <st c="31590">function
    takes a dictionary,</st> `<st c="31620">x</st>`<st c="31621">, as input and formats
    it into a JSON output.</st> <st c="31667">It creates a</st> `<st c="31680">json_output</st>`
    <st c="31691">dictionary with two keys:</st> `<st c="31718">"relevance_score"</st>`
    <st c="31735">(obtained by calling</st> `<st c="31757">extract_score</st>` <st
    c="31770">on the</st> `<st c="31778">'relevance_score</st>`<st c="31794">’ value
    from</st> `<st c="31808">x</st>`<st c="31809">) and</st> `<st c="31815">"answer"</st>`
    <st c="31823">(directly taken from</st> `<st c="31845">x</st>`<st c="31846">).</st>
    <st c="31849">It then uses</st> `<st c="31862">json.dumps</st>` <st c="31872">to
    convert the</st> `<st c="31888">json_output</st>` <st c="31899">dictionary to
    a JSON string and creates a</st> `<st c="31942">Generation</st>` <st c="31952">object
    with the JSON string as its text.</st> <st c="31994">Finally, it uses</st> `<st
    c="32011">json_parser</st>` <st c="32022">to parse the</st> `<st c="32036">Generation</st>`
    <st c="32046">object and returns the</st> <st c="32070">parsed result.</st>
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="31566">这个</st> `<st c="31572">format_json_output</st>` <st c="31590">函数接受一个字典</st>
    `<st c="31620">x</st>`<st c="31621">，并将其格式化为JSON输出。</st> <st c="31667">它创建一个</st>
    `<st c="31680">json_output</st>` <st c="31691">字典，包含两个键：</st> `<st c="31718">"relevance_score"</st>`
    <st c="31735">(通过调用</st> `<st c="31757">extract_score</st>` <st c="31770">从</st>
    `<st c="31778">'relevance_score</st>`<st c="31794">’值中获取</st> `<st c="31808">x</st>`<st
    c="31809">) 和</st> `<st c="31815">"answer"</st>` <st c="31823">(直接从</st> `<st
    c="31845">x</st>`<st c="31846">中获取)。</st> <st c="31849">然后，它使用</st> `<st c="31862">json.dumps</st>`
    <st c="31872">将</st> `<st c="31888">json_output</st>` <st c="31899">字典转换为JSON字符串，并创建一个包含该JSON字符串的</st>
    `<st c="31942">Generation</st>` <st c="31952">对象。</st> <st c="31994">最后，它使用</st>
    `<st c="32011">json_parser</st>` <st c="32022">解析</st> `<st c="32036">Generation</st>`
    <st c="32046">对象，并返回解析后的结果。</st>
- en: <st c="32084">We will</st> <st c="32092">need to reference this function in
    the function we were</st> <st c="32148">previously using,</st> `<st c="32167">conditional_answer</st>`<st
    c="32185">. Update</st> `<st c="32194">conditional_answer</st>` <st c="32212">like
    this:</st>
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="32084">我们</st> <st c="32092">需要在之前使用的函数中引用这个函数，即</st> `<st c="32148">conditional_answer</st>`<st
    c="32185">。按照以下方式更新</st> `<st c="32194">conditional_answer</st>` <st c="32212">：</st>
- en: '[PRE20]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: <st c="32386">Here, we update the</st> `<st c="32407">conditional_answer</st>`
    <st c="32425">function to apply that</st> `<st c="32449">format_json_output</st>`
    <st c="32467">function if it determines the answer is relevant and before it provides
    the</st> <st c="32544">returned output.</st>
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="32386">在这里，我们更新了</st> `<st c="32407">conditional_answer</st>` <st c="32425">函数，以便在它确定答案相关且在提供</st>
    `<st c="32449">format_json_output</st>` <st c="32467">函数之前，应用该</st> `<st c="32449">format_json_output</st>`
    <st c="32467">函数。</st>
- en: <st c="32560">Next, we are going to take the two chains we had before in our
    code and combine them into one larger chain handling the entire pipeline.</st>
    <st c="32698">In the past, it was helpful to show this separately to give more
    focus to certain areas, but now we have a chance to clean up and show how these
    chains can be grouped together to handle our entire</st> <st c="32895">logic flow:</st>
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="32560">接下来，我们将把之前在代码中使用的两个链合并成一个更大的链，以处理整个</st> `<st c="32698">管道。</st>`
    <st c="32698">在过去，将这部分单独展示有助于更专注于某些区域，但现在我们有了一个机会来清理并展示这些链如何组合在一起来处理我们的整个</st>
    `<st c="32895">逻辑流程：</st>`
- en: '[PRE21]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: <st c="33422">If you</st> <st c="33429">look back at previous code labs, this
    was represented</st> <st c="33483">by two chains.</st> <st c="33499">Note that
    this is using</st> `<st c="33523">str_output_parser</st>` <st c="33540">in the
    same way it was before.</st> <st c="33572">You do not see the JSON parser here
    because it is applied in the</st> `<st c="33637">format_json_output</st>` <st
    c="33655">function, which is called from the</st> `<st c="33691">conditional_answer</st>`
    <st c="33709">function, which you see in the last line.</st> <st c="33752">This
    simplification of these chains works for this example, focused on parsing our
    output into JSON, but we should note that we do lose the context that we have
    used in previous code labs.</st> <st c="33941">This is really just an example
    of an alternative approach to setting up</st> <st c="34013">our chain(s).</st>
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="33422">如果你</st> <st c="33429">回顾之前的代码实验室，这被表示为两个链。</st> <st c="33483">请注意，这里使用的是</st>
    `<st c="33523">str_output_parser</st>` <st c="33540">，与之前的方式相同。</st> <st c="33572">你在这里看不到JSON解析器，因为它是在</st>
    `<st c="33637">format_json_output</st>` <st c="33655">函数中应用的，该函数是从</st> `<st c="33691">conditional_answer</st>`
    <st c="33709">函数中调用的，你可以在最后一行看到这个函数。</st> <st c="33752">这种简化这些链的方法适用于这个示例，它专注于将我们的输出解析为JSON，但我们应注意的是，我们确实失去了之前代码实验室中使用的上下文。</st>
    <st c="33941">这实际上只是设置我们的链（s）的另一种方法的示例。</st>
- en: <st c="34026">Lastly, because our final output is in JSON format that we did
    need to add the context to, we need to update our</st> *<st c="34140">test</st>*
    *<st c="34145">run</st>* <st c="34148">code:</st>
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="34026">最后，由于我们的最终输出是JSON格式，我们需要添加上下文，因此我们需要更新我们的</st> *<st c="34140">测试</st>*
    *<st c="34145">运行</st> <st c="34148">代码：</st>
- en: '[PRE22]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: <st c="34401">When</st> <st c="34407">we print this out, we see a similar result
    as in the</st> <st c="34460">past, but we show how the JSON formatted final</st>
    <st c="34507">output looks:</st>
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="34401">当我们</st> <st c="34407">打印出这个结果时，我们看到的结果与之前类似，但我们展示了如何以JSON格式展示最终的</st>
    <st c="34507">输出：</st>
- en: '[PRE23]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: <st c="36580">This is a</st> <st c="36591">simple example of a JSON output,
    but you can build off this</st> <st c="36651">and shape the JSON to anything you
    need using the</st> `<st c="36701">FinalOutputModel</st>` <st c="36717">class
    we defined and passed into our</st> <st c="36755">output parser.</st>
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="36580">这是一个</st> <st c="36591">简单的JSON输出示例，但你可以在其基础上</st> <st c="36651">构建并使用</st>
    `<st c="36701">FinalOutputModel</st>` <st c="36717">类来调整JSON格式，以满足你的需求。</st> <st
    c="36755">我们定义了这个类并将其传递给我们的输出解析器。</st>
- en: <st c="36769">You can find more information about the JSON parser</st> <st c="36822">here:</st>
    [<st c="36828">https://python.langchain.com/v0.2/docs/how_to/output_parser_json/</st>](https://python.langchain.com/v0.2/docs/how_to/output_parser_json/
    )
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="36769">你可以在以下链接中找到有关JSON解析器的更多信息：</st> <st c="36822">[这里](https://python.langchain.com/v0.2/docs/how_to/output_parser_json/)</st>
    [<st c="36828">https://python.langchain.com/v0.2/docs/how_to/output_parser_json/</st>](https://python.langchain.com/v0.2/docs/how_to/output_parser_json/
    )
- en: <st c="36893">It is important to note that it is difficult to rely on LLMs to
    output in a certain format.</st> <st c="36986">A more robust system would incorporate
    the parser deeper into the system, where it will likely be able to better utilize
    the JSON output, but it will also entail more checks to make sure the</st> <st
    c="37177">formatting</st> <st c="37187">is as required for the next step to work
    off properly formatted JSON.</st> <st c="37258">In our code here, we implemented
    a very lightweight layer for JSON formatting to show how the output parser could
    fit into our RAG application in a very</st> <st c="37411">simple way.</st>
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="36893">需要注意的是，很难依赖LLMs以特定格式输出。</st> <st c="36986">一个更健壮的系统会将解析器更深入地集成到系统中，这样它可能能够更好地利用JSON输出，但这也意味着需要更多的检查来确保格式符合下一步操作对正确格式JSON的要求。</st>
    <st c="37177">格式</st> <st c="37187">必须符合要求，以便正确地处理JSON。</st> <st c="37258">在我们的代码中，我们实现了一个非常轻量级的JSON格式化层，以展示输出解析器如何以非常简单的方式集成到我们的RAG应用中。</st>
    <st c="37411">。</st>
- en: <st c="37422">Summary</st>
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="37422">摘要</st>
- en: <st c="37430">In this chapter, we learned about various components in LangChain
    that can enhance a RAG application.</st> *<st c="37533">Code lab 11.1</st>* <st
    c="37546">focused on document loaders, which are used to load and process documents
    from various sources such as text files, PDFs, web pages, or databases.</st> <st
    c="37693">The chapter covered examples of loading documents from HTML, PDF, Microsoft
    Word, and JSON formats using different LangChain document loaders, noting that
    some document loaders add metadata which may require adjustments in</st> <st c="37916">the
    code.</st>
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="37430">在本章中，我们了解了LangChain中可以增强RAG应用的各个组件。</st> *<st c="37533">代码实验室11.1</st>*
    <st c="37546">专注于文档加载器，这些加载器用于从各种来源（如文本文件、PDF、网页或数据库）加载和处理文档。</st> <st c="37693">本章涵盖了使用不同的LangChain文档加载器从HTML、PDF、Microsoft
    Word和JSON格式加载文档的示例，并指出某些文档加载器会添加元数据，这可能在代码中需要进行调整。</st>
- en: '*<st c="37925">Code lab 11.2</st>* <st c="37939">discussed text splitters,
    which divide documents into chunks suitable for retrieval, addressing issues with
    large documents and context representation in vector embeddings.</st> <st c="38113">The
    chapter covered</st> `<st c="38133">CharacterTextSplitter</st>`<st c="38154">,
    which splits text into arbitrary N-character-sized chunks, and</st> `<st c="38219">RecursiveCharacterTextSplitter</st>`<st
    c="38249">, which recursively splits text while trying to keep related pieces
    together.</st> `<st c="38327">SemanticChunker</st>` <st c="38342">was introduced
    as an experimental splitter that combines semantically similar sentences to create
    more</st> <st c="38446">meaningful chunks.</st>'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '*<st c="37925">代码实验室11.2</st>* <st c="37939">讨论了文本分割器，这些分割器将文档分割成适合检索的块，解决了大型文档和向量嵌入中的上下文表示问题。</st>
    <st c="38113">本章涵盖了</st> `<st c="38133">CharacterTextSplitter</st>`<st c="38154">，它将文本分割成任意N字符大小的块，以及</st>
    `<st c="38219">RecursiveCharacterTextSplitter</st>`<st c="38249">，它递归分割文本同时尝试将相关部分保持在一起。</st>
    `<st c="38327">SemanticChunker</st>` <st c="38342">被介绍为一个实验性的分割器，它将语义相似的句子组合成更具</st>
    <st c="38446">意义的块。</st>'
- en: <st c="38464">Lastly,</st> *<st c="38473">Code lab 11.3</st>* <st c="38486">focused
    on output parsers, which structure the responses from the language model in a
    RAG application.</st> <st c="38590">The chapter covered the string output parser,
    which outputs the LLM’s response as a string, and the JSON output parser, which
    formats the output as JSON using a defined structure.</st> <st c="38770">An example
    was provided to show how the JSON output parser can be integrated into the</st>
    <st c="38856">RAG application.</st>
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="38464">最后，</st> *<st c="38473">代码实验室11.3</st>* <st c="38486">专注于输出解析器，它将语言模型在RAG应用中的响应结构化。</st>
    <st c="38590">本章涵盖了字符串输出解析器，它将LLM的响应输出为字符串，以及JSON输出解析器，它使用定义的结构将输出格式化为JSON。</st>
    <st c="38770">提供了一个示例，展示了如何将JSON输出解析器集成到</st> <st c="38856">RAG应用中。</st>
- en: <st c="38872">In the next chapter, we will cover a relatively advanced but very
    powerful topic, LangGraph and</st> <st c="38969">AI agents.</st>
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="38872">在下一章中，我们将介绍一个相对高级但非常强大的主题，LangGraph和</st> <st c="38969">AI代理。</st>
- en: <st c="0">Part 3 – Implementing Advanced RAG</st>
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="0">第三部分 – 实施高级RAG</st>
- en: <st c="35">In this part, you will learn advanced techniques for enhancing your
    RAG applications, including integrating AI agents with LangGraph for more sophisticated
    control flows, leveraging prompt engineering strategies to optimize retrieval
    and generation, and exploring cutting-edge approaches such as query expansion,
    query decomposition, and multi-modal RAG.</st> <st c="391">You’ll gain hands-on
    experience in implementing these techniques through code labs and discover a wealth
    of additional methods covering indexing, retrieval, generation, and the entire</st>
    <st c="575">RAG pipeline.</st>
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="35">在本部分，您将学习增强您的RAG应用的高级技术，包括将AI代理与LangGraph集成以实现更复杂的控制流，利用提示工程策略优化检索和生成，以及探索查询扩展、查询分解和多模态RAG等前沿方法。</st>
    <st c="391">您将通过代码实验室获得这些技术的实践经验，并发现涵盖索引、检索、生成以及整个</st> <st c="575">RAG流程</st>的丰富方法。
- en: <st c="588">This part contains the</st> <st c="612">following chapters:</st>
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="588">本部分包含以下章节：</st>
- en: '[*<st c="631">Chapter 12</st>*](B22475_12.xhtml#_idTextAnchor242)<st c="642">,</st>
    *<st c="644">Combining RAG with the Power of AI Agents and LangGraph</st>*'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*<st c="631">第12章</st>*](B22475_12.xhtml#_idTextAnchor242)<st c="642">，*<st
    c="644">结合RAG与AI代理和LangGraph的力量</st>*'
- en: '[*<st c="699">Chapter 13</st>*](B22475_13.xhtml#_idTextAnchor256)<st c="710">,</st>
    *<st c="712">Using Prompt Engineering to Improve RAG Efforts</st>*'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*<st c="699">第13章</st>*](B22475_13.xhtml#_idTextAnchor256)<st c="710">,</st>
    *<st c="712">使用提示工程来提高RAG工作</st>*'
- en: '[*<st c="759">Chapter 14</st>*](B22475_14.xhtml#_idTextAnchor283)<st c="770">,</st>
    *<st c="772">Advanced RAG-Related Techniques for Improving Results</st>*'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*<st c="759">第14章</st>*](B22475_14.xhtml#_idTextAnchor283)<st c="770">,</st>
    *<st c="772">用于改进结果的先进RAG相关技术</st>*'
