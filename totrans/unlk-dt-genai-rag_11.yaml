- en: <st c="0">11</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="3">Using LangChain to Get More from RAG</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="40">We have mentioned</st> **<st c="59">LangChain</st>** <st c="68">several</st>
    <st c="76">times already, and we have shown you a lot of LangChain code, including
    code that implements the LangChain-specific language:</st> **<st c="203">LangChain
    Expression Language</st>** <st c="232">(</st>**<st c="234">LCEL</st>**<st c="238">).</st>
    <st c="242">Now that you are familiar with</st> <st c="272">different ways to
    implement</st> **<st c="301">retrieval-augmented generation</st>** <st c="331">(</st>**<st
    c="333">RAG</st>**<st c="336">) with LangChain, we thought now would be a good
    time to dive more into the various capabilities of LangChain that you can use
    to make your RAG</st> <st c="481">pipeline better.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="497">In this chapter, we explore lesser-known but highly important components
    in LangChain that can enhance a RAG application.</st> <st c="620">We will cover</st>
    <st c="634">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="648">Document loaders for loading and processing documents from</st>
    <st c="708">different sources</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="725">Text splitters for dividing documents into chunks suitable</st>
    <st c="785">for retrieval</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="798">Output parsers for structuring the responses from the</st> <st c="853">language
    model</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="867">We will use different code labs to step through examples of each
    type of component, starting with</st> <st c="966">document loaders.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="983">Technical requirements</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="1006">The code for this chapter is placed in the following GitHub</st>
    <st c="1067">repository:</st> [<st c="1079">https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_11</st>](https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_11
    )
  prefs: []
  type: TYPE_NORMAL
- en: <st c="1176">Individual file names for each code lab are mentioned in the</st>
    <st c="1238">respective sections.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="1258">Code lab 11.1 – Document loaders</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="1291">The file you need to</st> <st c="1313">access from the GitHub repository
    is</st> <st c="1350">titled</st> `<st c="1357">CHAPTER11-1_DOCUMENT_LOADERS.ipynb</st>`<st
    c="1391">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="1392">Document loaders play a key role in accessing, extracting, and
    pulling in the data that makes our RAG application function.</st> <st c="1517">Document
    loaders are used to load and process documents from various sources such as text
    files, PDFs, web pages, or databases.</st> <st c="1645">They convert the documents
    into a format suitable for indexing</st> <st c="1708">and retrieval.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="1722">Let’s install some</st> <st c="1741">new packages to support our
    document loading, which, as you might have guessed, involves some different file</st>
    <st c="1851">format-related packages:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: <st c="1955">The first one may look familiar,</st> `<st c="1989">bs4</st>` <st
    c="1992">(for Beautiful Soup 4), as we used it in</st> [*<st c="2034">Chapter
    2</st>*](B22475_02.xhtml#_idTextAnchor035) <st c="2043">for parsing HTML.</st>
    <st c="2062">We also have a couple of Microsoft Word-related packages, such as</st>
    `<st c="2128">python_docx</st>`<st c="2139">, which helps with creating and updating
    Microsoft Word (</st>`<st c="2196">.docx</st>`<st c="2201">) files, and</st> `<st
    c="2215">docx2txt</st>`<st c="2223">, which extracts text and images from</st>
    `<st c="2261">.docx</st>` <st c="2266">files.</st> <st c="2274">The</st> `<st
    c="2278">jq</st>` <st c="2280">package is a lightweight</st> <st c="2306">JSON
    processor.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2321">Next, we are going to take an extra step you likely will not have
    to take in a</st> *<st c="2401">real</st>* <st c="2405">situation, which is turning
    our PDF document into a bunch of other formats, so we can test out the extraction
    of those formats.</st> <st c="2534">We are going to add a whole new</st> `<st
    c="2566">document loaders</st>` <st c="2582">section to our code right after the</st>
    <st c="2619">OpenAI setup.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2632">In this section, we will provide code to generate the files, and
    then the different document loaders and their related packages to extract data
    from those types of files.</st> <st c="2804">Right now, we have a PDF version
    of our document.</st> <st c="2854">We will need an HTML/web version, a Microsoft
    Word version, and a JSON version of</st> <st c="2936">our document.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2949">We are going to start with a new cell under the OpenAI setup cell,
    where we will import the new packages we need for</st> <st c="3067">these conversions:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: <st c="3139">As we mentioned, the</st> `<st c="3161">BeautifulSoup</st>` <st
    c="3174">package helps us parse HTML-based web pages.</st> <st c="3220">We also
    import</st> `<st c="3235">docx</st>`<st c="3239">, which represents the Microsoft
    Docx word processing format.</st> <st c="3301">Lastly, we import</st> `<st c="3319">json</st>`
    <st c="3323">to interpret and manage</st> `<st c="3348">json</st>` <st c="3352">formatted
    code.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3368">Next, we want to establish the filenames we will save each of</st>
    <st c="3431">our formats:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: <st c="3649">Here, we are</st> <st c="3663">defining the paths for each of the
    files that we use in this code, and later when we use the loaders to load each
    document.</st> <st c="3787">These are going to be the final files that we generate
    from the original PDF document we have</st> <st c="3881">been using.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3892">And then this key part of our new code will extract the text from
    the PDF and use it to generate all of these new types</st> <st c="4013">of documents:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: <st c="4497">We generate an HTML, Word, and JSON version of our document in
    a very basic sense.</st> <st c="4581">If you were generating these documents to
    actually use in a pipeline, we recommend applying more formatting and extraction,
    but for the purposes of this demonstration, this will provide us with the</st>
    <st c="4779">necessary data.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="4794">Next, we are going</st> <st c="4813">to add our document loaders
    under the indexing stage of our code.</st> <st c="4880">We have worked with the
    first two document loaders already, which we will show in this code lab, but updated
    so that they can be used interchangeably.</st> <st c="5031">For each document
    loader, we show what package imports are specific to that loader alongside the
    loader code.</st> <st c="5141">In the early chapters, we used a web loader that
    loaded directly from a website, so if that is a use case you have, refer to that
    document loader.</st> <st c="5288">In the meantime, we are sharing a slightly
    different type of document loader here that is focused on using local HTML files,
    such as the one we just generated.</st> <st c="5448">Here is the code for this</st>
    <st c="5474">HTML loader:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: <st c="5602">Here, we use the HTML file we defined earlier to load the code
    from an HTML document.</st> <st c="5689">The final variable,</st> `<st c="5709">docs</st>`<st
    c="5713">, can be used interchangeably with any other</st> *<st c="5758">docs</st>*
    <st c="5762">we define in the following document loaders.</st> <st c="5808">The
    way this code works, you can only use one loader at a time, and it will replace
    the docs with its version of the docs (including a metadata source tag of what
    document it came from).</st> <st c="5995">If you run this cell and then skip down
    to run the splitter cell, you can run the remaining code in the lab and see similar
    results from what is the same data coming from different source file types.</st>
    <st c="6195">We did have to make a slight update later in the code, which we will
    note in</st> <st c="6272">a moment.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="6281">There are some alternative HTML loaders listed on the LangChain
    website that you can</st> <st c="6367">see here:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[<st c="6376">https://python.langchain.com/v0.2/docs/how_to/document_loader_html/</st>](https://python.langchain.com/v0.2/docs/how_to/document_loader_html/
    )'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="6444">The next file type we will talk about is the other type we have
    been working with already,</st> <st c="6536">the PDF:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: <st c="6796">Here, we have a</st> <st c="6813">slightly more streamlined version
    of the code we have used previously to extract the data from the PDF.</st> <st
    c="6917">Using this new approach shows you an alternative way to access this data,
    but either will work for you in your code, ultimately loading up the docs with
    the data pulled from the PDF using</st> `<st c="7105">PdfReader</st>` <st c="7114">from</st>
    `<st c="7120">PyPDF2</st>`<st c="7126">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="7127">It should be noted that there are numerous and very capable ways
    to load PDF documents into LangChain, which is supported by many integrations
    with popular tools for PDF extraction.</st> <st c="7310">Here are a few:</st>
    `<st c="7326">PyPDF2</st>` <st c="7332">(what we use here),</st> `<st c="7353">PyPDF</st>`<st
    c="7358">,</st> `<st c="7360">PyMuPDF</st>`<st c="7367">,</st> `<st c="7369">MathPix</st>`<st
    c="7376">,</st> `<st c="7378">Unstructured</st>`<st c="7390">,</st> `<st c="7392">AzureAIDocumentIntelligenceLoader</st>`<st
    c="7425">,</st> <st c="7427">and</st> `<st c="7431">UpstageLayoutAnalysisLoader</st>`<st
    c="7458">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="7459">We recommend you look at the latest list of PDF document loaders.</st>
    <st c="7526">LangChain provides a helpful set of tutorials for many of</st> <st
    c="7584">them here:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[<st c="7594">https://python.langchain.com/v0.2/docs/how_to/document_loader_pdf/</st>](https://python.langchain.com/v0.2/docs/how_to/document_loader_pdf/
    )'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="7661">Next, we will load the data from a Microsoft</st> <st c="7707">Word
    document:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: <st c="7841">This code uses the</st> `<st c="7861">Docx2txtLoader</st>` <st
    c="7875">document loader from LangChain to turn the Word document we previously
    generated into text and load it up into our</st> `<st c="7991">docs</st>` <st
    c="7995">variable that can be later used by the splitter.</st> <st c="8045">Again,
    stepping through the rest of the code will work with this data, just</st> <st
    c="8121">as it did with the HTML or PDF documents.</st> <st c="8163">There are
    many options for loading Word documents as well, which you can find listed</st>
    <st c="8248">here:</st> [<st c="8254">https://python.langchain.com/v0.2/docs/integrations/document_loaders/microsoft_word/</st>](https://python.langchain.com/v0.2/docs/integrations/document_loaders/microsoft_word/
    )
  prefs: []
  type: TYPE_NORMAL
- en: <st c="8338">Lastly, we see a similar approach with the</st> <st c="8382">JSON
    loader:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '<st c="8538">Here, we use a JSON loader to load data that was stored in a JSON
    object format, but the results are the same: a</st> `<st c="8652">docs</st>` <st
    c="8656">variable that can be passed to the splitter and converted into the format
    we use throughout the remaining code.</st> <st c="8769">Other options for JSON
    loaders can be</st> <st c="8807">found here:</st>'
  prefs: []
  type: TYPE_NORMAL
- en: '[<st c="8818">https://python.langchain.com/v0.2/docs/how_to/document_loader_json/</st>](https://python.langchain.com/v0.2/docs/how_to/document_loader_json/
    )'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="8886">Note that some document loaders add additional metadata to the</st>
    `<st c="8950">metadata</st>` <st c="8958">dictionary within the</st> `<st c="8981">Document</st>`
    <st c="8989">objects that are generated during this process.</st> <st c="9038">This
    is causing some issues with our code when we add our own metadata.</st> <st c="9110">To
    fix this, we update these lines when we index and create the</st> <st c="9174">vector
    store:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: <st c="9475">We also update the code in the final output to test the response,
    changing the second line in this code to handle the changed</st> `<st c="9602">metadata</st>`
    <st c="9610">tag:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: <st c="9797">Run each loader</st> <st c="9813">and then run the remaining code
    to see each document in action!</st> <st c="9878">There are numerous more integrations
    with third parties, allowing you to access just about any data source you can
    imagine and format that data in a way that you can better utilize LangChain’s
    other components.</st> <st c="10089">Take a look at more examples here on the
    LangChain</st> <st c="10140">website:</st> [<st c="10149">https://python.langchain.com/docs/modules/data_connection/document_loaders/</st>](https://python.langchain.com/docs/modules/data_connection/document_loaders/
    )
  prefs: []
  type: TYPE_NORMAL
- en: <st c="10224">Document loaders play a supporting and very important role in
    your RAG application.</st> <st c="10309">But for RAG-specific applications that
    typically utilize</st> *<st c="10366">chunks</st>* <st c="10372">of your data,
    document loaders are not nearly as useful until you pass them through a text splitter.</st>
    <st c="10474">Next, we will review text splitters and how each one can be used
    to improve your</st> <st c="10555">RAG application.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="10571">Code lab 11.2 – Text splitters</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="10602">The file you need to access from the GitHub repository is</st>
    <st c="10661">titled</st> `<st c="10668">CHAPTER11-2_TEXT_SPLITTERS.ipynb</st>`<st
    c="10700">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="10701">Text splitters</st> <st c="10717">split a document into chunks
    that can be used for retrieval.</st> <st c="10778">Larger documents pose a threat
    to many parts of our RAG application and the splitter is our first line of defense.</st>
    <st c="10893">If you were able to vectorize a very large document, the larger
    the document, the more context representation you will lose in the vector embedding.</st>
    <st c="11042">But this assumes you can even vectorize a very large document, which
    you often can’t!</st> <st c="11128">Most embedding models have relatively small
    limits on the size of documents we can pass to it compared to the large documents
    many of us work with.</st> <st c="11276">For example, the context length for the
    OpenAI model we are using to generate our embeddings is 8,191 tokens.</st> <st
    c="11386">If we try to pass a document larger than that to the model, it will
    generate an error.</st> <st c="11473">These are the main reasons splitters exist,
    but these are not the only complexities introduced with this step in</st> <st
    c="11586">the process.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="11598">The key element</st> <st c="11615">of text splitters for us to
    consider is how they split the text.</st> <st c="11680">Let’s say you have 100
    paragraphs that you want to split up.</st> <st c="11741">In some cases, there
    may be two or three that are semantically meant to be together, such as the paragraphs
    in this one section.</st> <st c="11870">In some cases, you may have a section
    title, a URL, or some other type of text.</st> <st c="11950">Ideally, you want
    to keep the semantically related pieces of text together, but this can be much
    more complex than it first seems!</st> <st c="12081">For a real-world example
    of this, go to this website and</st> <st c="12138">copy in a large set of</st>
    <st c="12161">text:</st> [<st c="12167">https://chunkviz.up.railway.app/</st>](https://chunkviz.up.railway.app/)<st
    c="12199">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '<st c="12200">ChunkViz is a</st> <st c="12215">utility created by Greg Kamradt
    that helps you visualize how your text splitter is working.</st> <st c="12307">Change
    the parameters for the splitters to use what we are using: a chunk size of</st>
    `<st c="12389">1000</st>` <st c="12393">and a chunk overlap of</st> `<st c="12417">200</st>`<st
    c="12420">. Try the character splitter compared to the recursive character text
    splitter.</st> <st c="12500">Note that with the example they provide shown in</st>
    *<st c="12549">Figure 11</st>**<st c="12558">.1</st>*<st c="12560">, the recursive
    character splitter captures all of the paragraphs separately at around a</st>
    `<st c="12649">434</st>` <st c="12652">chunk size:</st>'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.1 – Recursive Character Text Splitter captures whole paragraphs
    at 434 characters](img/B22475_11_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="13572">Figure 11.1 – Recursive Character Text Splitter captures whole
    paragraphs at 434 characters</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="13663">As you increase</st> <st c="13679">the chunk size, it stays on
    the paragraph splits well but eventually gets more and more paragraphs per chunk.</st>
    <st c="13790">Note, though, that this is going to be different for different text.</st>
    <st c="13859">If you have text with very long paragraphs, you will need a larger
    chunk setting to capture</st> <st c="13951">whole paragraphs.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="13968">Meanwhile, if you try the character splitter, it will cut off
    in the middle of a sentence on</st> <st c="14062">any setting:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.2 – Character splitter captures partial paragraphs at 434 characters](img/B22475_11_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="14652">Figure 11.2 – Character splitter captures partial paragraphs at
    434 characters</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="14730">This split of a</st> <st c="14747">sentence could have a significant
    impact on the ability of your chunks to capture all of the important semantic
    meanings of the text within them.</st> <st c="14893">You can offset this by changing
    the chunk overlap, but you still have partial paragraphs, which will equate to
    noise to your LLM, distracting it away from providing the</st> <st c="15062">optimal
    response.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="15079">Let’s step through actual coding examples of each to understand
    some of the</st> <st c="15156">options available.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="15174">Character text splitter</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="15198">This is the</st> <st c="15211">simplest approach to splitting
    your document.</st> <st c="15257">A</st> <st c="15258">text splitter enables you
    to divide your text into arbitrary N-character-sized chunks.</st> <st c="15346">You
    can improve this slightly by adding a separator parameter, such as</st> `<st c="15417">\n</st>`<st
    c="15419">. But this is a great place to start to understand how chunking works,
    and then we can move on to more approaches that work better but have added complexity</st>
    <st c="15576">to them.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="15584">Here is</st> <st c="15593">code that uses the</st> `<st c="15612">CharacterTextSplitter</st>`
    <st c="15633">object with our documents that can be used interchangeably</st>
    <st c="15692">with the other</st> <st c="15708">splitter outputs:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: <st c="15948">The output from the first split (</st>`<st c="15982">split[0]</st>`<st
    c="15991">) looks</st> <st c="16000">like this:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: <st c="17042">There are a lot of</st> `<st c="17062">\n</st>` <st c="17064">(also
    called newline) markup characters, and some</st> `<st c="17115">\u</st>` <st c="17117">as
    well.</st> <st c="17127">We see that it counts out around 1,000 characters, finds
    the</st> `<st c="17188">\n</st>` <st c="17190">character nearest to that, and
    that</st> <st c="17227">becomes the first chunk.</st> <st c="17252">It is right
    in the middle of a sentence, which could</st> <st c="17305">be problematic!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="17320">The next</st> <st c="17330">chunk looks</st> <st c="17342">like
    this:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: <st c="18381">As you can see here, it backtracked a little, which is due to
    the chunk overlap we set of 200 characters.</st> <st c="18488">It then goes forward
    another 1,000 characters from there and breaks on another</st> `<st c="18567">\</st>``<st
    c="18568">n</st>` <st c="18569">character.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="18580">Let’s step through the parameters</st> <st c="18615">for this:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="18730">\n</st>`<st c="18732">, and it works for this document.</st>
    <st c="18766">But if you use</st> `<st c="18781">\n\n</st>` <st c="18785">(the
    double newline character) for your separator on this particular document, where
    there are no double newline characters, it never splits!</st> `<st c="18928">\n\n</st>`
    <st c="18932">is actually the default, so make sure you keep an eye on this and
    use a separator that will work with</st> <st c="19035">your content!</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="19048">Chunk size</st>** <st c="19059">– This</st> <st c="19067">defines
    the arbitrary number of characters you are aiming for with your chunk size.</st>
    <st c="19151">This may still vary, such as at the end of the text, but for the
    most part, the chunks will be consistently</st> <st c="19259">this size.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="19269">Chunk overlap</st>** <st c="19283">– This is</st> <st c="19293">the
    amount of characters you would like overlapping in your sequential chunks.</st>
    <st c="19373">This is a simple way to make sure you are capturing all context
    within your chunks.</st> <st c="19457">For example, if you had no chunk overlap
    and cut a sentence in half, the majority of that context would likely not be captured
    in either chunk very well.</st> <st c="19611">But with overlap, you can get better
    coverage of this context on</st> <st c="19676">the edges.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="19686">Is separator regex</st>** <st c="19705">– This is</st> <st
    c="19715">yet another parameter that indicates whether the separator used is in</st>
    <st c="19786">Regex format.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="19799">In this case, we</st> <st c="19816">are setting the chunk size
    to</st> `<st c="19847">1000</st>` <st c="19851">and the chunk overlap to</st>
    `<st c="19877">200</st>`<st c="19880">. What we are saying here with this code
    is that we want it to use chunks that are smaller than 1,000 characters but with
    a 200-character overlap.</st> <st c="20027">This overlapping technique is similar
    to the sliding window technique you see in</st> **<st c="20108">convolutional
    neural networks</st>** <st c="20137">(</st>**<st c="20139">CNNs</st>**<st c="20143">)
    when</st> <st c="20150">you are</st> *<st c="20159">sliding</st>* <st c="20166">the
    window over smaller parts of the image with overlap so that you capture the context
    between the different windows.</st> <st c="20286">In this case, it is the context
    within the chunks that we are trying</st> <st c="20355">to capture.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="20366">Here are some other things</st> <st c="20394">to note:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="20449">Document</st>` <st c="20457">object to store our text, so we
    use the</st> `<st c="20498">create_documents</st>` <st c="20514">function that
    allows it to work in the next step when these documents are vectorized.</st> <st
    c="20601">If you want to obtain the string content directly, you can use the</st>
    `<st c="20668">split_text</st>` <st c="20678">function.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="20723">create_documents</st>` <st c="20739">expects a list of texts,
    so if you just have a string, you’ll need to wrap it in</st> `<st c="20821">[]</st>`<st
    c="20823">. In our case, we have already set</st> `<st c="20858">docs</st>` <st
    c="20862">as a list, so this requirement</st> <st c="20894">is satisfied.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="20907">Splitting versus chunking</st>** <st c="20933">– These terms
    can be</st> <st c="20955">used interchangeably.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="20976">You can find more information about this specific text splitter
    on the LangChain</st> <st c="21058">website:</st> [<st c="21067">https://python.langchain.com/v0.2/docs/how_to/character_text_splitter/</st>](https://python.langchain.com/v0.2/docs/how_to/character_text_splitter/
    )
  prefs: []
  type: TYPE_NORMAL
- en: <st c="21137">The API documentation can be found</st> <st c="21173">here:</st>
    [<st c="21179">https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.CharacterTextSplitter.html</st>](https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.CharacterTextSplitter.html
    )
  prefs: []
  type: TYPE_NORMAL
- en: <st c="21293">We can do better than this though; let’s take a look at a more
    sophisticated approach called</st> **<st c="21387">recursive character</st>**
    **<st c="21407">text splitting</st>**<st c="21421">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="21422">Recursive character text splitter</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="21456">We have</st> <st c="21464">seen this one before!</st> <st c="21487">We
    have used this</st> <st c="21505">splitter the most in our code labs so far because
    it is what LangChain recommends using when splitting generic text.</st> <st c="21622">That
    is what we</st> <st c="21638">are doing!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="21648">As the name states, this</st> <st c="21673">splitter recursively
    splits text, with the intention of keeping related pieces of text next to each
    other.</st> <st c="21781">You can pass a list of characters as a parameter and
    it will try to split those characters in order until the chunks are small enough.</st>
    <st c="21916">The default list is</st> `<st c="21936">["\n\n", "\n", " ", ""]</st>`<st
    c="21959">, which works well, but we are going to add</st> `<st c="22003">".</st>
    <st c="22006">"</st>` <st c="22007">to this list as well.</st> <st c="22030">This
    has the effect of trying to keep together all paragraphs, sentences defined by
    both</st> `<st c="22119">"\n"</st>` <st c="22123">and</st> `<st c="22128">".</st>
    <st c="22131">"</st>`<st c="22132">, and words as long</st> <st c="22152">as possible.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="22164">Here is</st> <st c="22173">our code:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: <st c="22364">Under the hood with this splitter, the chunks are split based
    on the</st> `<st c="22434">"\n\n"</st>` <st c="22440">separator, representing
    paragraph splits.</st> <st c="22483">But it doesn’t stop there; it will look at
    the chunk size, and if that is larger than the 1,000 we set, then it will split
    by the next separator (</st>`<st c="22629">"\n"</st>`<st c="22634">), and</st>
    <st c="22642">so on.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="22648">Let’s talk</st> <st c="22660">about the recursive aspect of this
    that splits the text into chunks using a recursive algorithm.</st> <st c="22757">The
    algorithm will only be applied if the text provided is longer than the chunk size,
    but it follows</st> <st c="22859">these steps:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="22871">It finds the last space or newline character within the range</st>
    `<st c="22934">[chunk_size - chunk_overlap, chunk_size]</st>`<st c="22974">. This
    ensures that chunks are split at word boundaries or</st> <st c="23033">line breaks.</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '<st c="23045">If a suitable split point is found, it splits the text into two
    parts: the chunk before the split point and the remaining text after the</st>
    <st c="23183">split point.</st>'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="23195">It recursively applies the same splitting process to the remaining
    text until all chunks are within the</st> `<st c="23300">chunk_size</st>` <st
    c="23310">limit.</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="23317">Similar to the</st> <st c="23333">character splitter approach,
    the recursive splitter is driven largely by the chunk size you set, but then it
    combines this with the recursive approach outlined previously to provide a straightforward
    and logical way to properly capture context within</st> <st c="23584">your chunks.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="23596">RecursiveCharacterTextSplitter</st>` <st c="23627">is particularly
    useful when dealing with large text documents that need to be processed by language
    models with input size limitations.</st> <st c="23764">By splitting the text into
    smaller chunks, you can feed the chunks to the language model individually and
    then combine the results</st> <st c="23895">if needed.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="23905">Clearly, recursive splitters are a step up from the character
    splitter, but they are still not splitting our content based on the semantics
    as much as just general separators such as paragraph and sentence breaks.</st>
    <st c="24120">But this will not handle cases where two paragraphs are semantically
    part of one ongoing thought that should really be captured together in their vector
    representations.</st> <st c="24290">Let’s see whether we can do better with the</st>
    **<st c="24334">semantic chunker</st>**<st c="24350">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="24351">Semantic chunker</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '<st c="24368">This is</st> <st c="24376">another one you may recognize, as
    we used it in the first code lab!</st> `<st c="24445">SemanticChunker</st>` <st
    c="24460">is an interesting one, currently listed as experimental, but described
    on the LangChain website as follows: “</st>*<st c="24570">First splits on sentences.</st>
    <st c="24598">Then (it) combines ones next to each other if they are semantically
    similar enough</st>*<st c="24680">.” In other words, the goal here is to avoid
    having to define this arbitrary chunk size number</st> <st c="24775">that was
    a key parameter that drives how the character and recursive splitters divide the
    text and focus the splits more on the semantics of the text you are splitting.</st>
    <st c="24944">Find out more about this</st> *<st c="24969">chunker</st>* <st c="24976">on
    the LangChain</st> <st c="24994">website:</st> [<st c="25003">https://python.langchain.com/docs/modules/data_connection/document_transformers/semantic-chunker</st>](https://python.langchain.com/docs/modules/data_connection/document_transformers/semantic-chunker
    )'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="25099">Under the hood,</st> `<st c="25116">SemanticChunker</st>` <st
    c="25131">splits your text into sentences, groups those sentences into groups
    of three sentences, and then merges them when they are similar in the</st> <st
    c="25270">embedding space.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="25286">When would this not work as well?</st> <st c="25321">When the
    semantics of your document is difficult to discern.</st> <st c="25382">For example,
    if you have a lot of code, addresses, names, internal reference IDs, and other
    text that has little semantic meaning, especially to an embedding model, this
    will likely reduce the ability of</st> `<st c="25586">SemanticChunker</st>` <st
    c="25601">to properly split your text.</st> <st c="25631">But in general,</st>
    `<st c="25647">SemanticChunker</st>` <st c="25662">has a lot of promise.</st>
    <st c="25685">Here is an example of the code to</st> <st c="25719">use it:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: <st c="25958">Here, we import the</st> `<st c="25979">SemanticChunker</st>`
    <st c="25994">class from the</st> `<st c="26010">langchain_experimental.text_splitter</st>`
    <st c="26046">module.</st> <st c="26055">We use the same embedding model we used
    to vectorize our documents and pass them to the</st> `<st c="26143">SemanticChunker</st>`
    <st c="26158">class.</st> <st c="26166">Note that this costs a little money, as
    it uses the same OpenAI API key we used to generate our embeddings.</st> `<st
    c="26274">SemanticChunker</st>` <st c="26289">uses these embeddings to determine
    how to split the documents based on semantic similarity.</st> <st c="26382">We
    also set the</st> `<st c="26398">number_of_chunks</st>` <st c="26414">variable
    to</st> `<st c="26427">200</st>`<st c="26430">, which indicates the desired number
    of chunks to split the documents into.</st> <st c="26506">This determines the
    granularity of the splitting process.</st> <st c="26564">A higher value of</st>
    `<st c="26582">number_of_chunks</st>` <st c="26598">will result in more fine-grained
    splits, while a lower value will produce fewer and</st> <st c="26683">larger chunks.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="26697">This</st> <st c="26702">code lab</st> <st c="26712">is set up
    so that you can use each type of splitter at a time.</st> <st c="26775">Run through
    each splitter and then the rest of the code to see how each one impacts your results.</st>
    <st c="26873">Also try changing parameter settings, such as</st> `<st c="26919">chunk_size</st>`<st
    c="26929">,</st> `<st c="26931">chunk_overlap</st>` <st c="26944">and</st> `<st
    c="26949">number_of_chunks</st>`<st c="26965">, depending on what splitter you
    are using.</st> <st c="27009">Exploring all of these options will help give you
    a better sense of how they can be used for</st> <st c="27102">your projects.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27116">For a last supporting component, we will discuss output parsers,
    responsible for shaping the final output from our</st> <st c="27232">RAG application.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27248">Code lab 11.3 – Output parsers</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="27279">The file you need to access from the GitHub repository is</st>
    <st c="27338">titled</st> `<st c="27345">CHAPTER11-3_OUTPUT_PARSERS.ipynb</st>`<st
    c="27377">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27378">The end</st> <st c="27386">result of any RAG application is going
    to be text, along with potentially some formatting, metadata, and some other related
    data.</st> <st c="27517">This output typically comes from the LLM itself.</st>
    <st c="27566">But there are times when you want to get a more structured format
    than just text.</st> <st c="27648">Output parsers are classes that help to structure
    the responses of the LLM wherever you use it in your RAG application.</st> <st
    c="27768">The output that this provides will then be provided to the next step
    in the chain, or in the case of all of our code labs, as the final output from</st>
    <st c="27916">the model.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27926">We will cover two different output parsers at the same time, and
    use them at different times in our RAG pipeline.</st> <st c="28041">We start with
    the parser we know, the string</st> <st c="28086">output parser.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="28100">Under the</st> `<st c="28111">relevance_prompt</st>` <st c="28127">function,
    add this code to a</st> <st c="28157">new cell:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: <st c="28262">Note that we were already using this in the LangChain chain code
    that appears later, but we are going to assign this parser to a variable called</st>
    `<st c="28408">str_output_parser</st>`<st c="28425">. Let’s talk about this type
    of parser in</st> <st c="28467">more depth.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="28478">String output parser</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="28499">This is a</st> <st c="28510">basic output parser.</st> <st c="28531">In
    very simple approaches, as in our previous code labs, you can use the</st> `<st
    c="28604">StrOutputParser</st>` <st c="28619">class outright as</st> <st c="28638">the
    instance for your output parser.</st> <st c="28675">Or you can do what we just
    did and assign it to a variable, particularly if you expect to see it in multiple
    areas of the code, which we will.</st> <st c="28818">But we have seen this many
    times already.</st> <st c="28860">It takes the output from the LLM in both places
    it is used and outputs the string response from the LLM to the next link in the
    chain.</st> <st c="28995">The documentation for this parser can be found</st>
    <st c="29042">here:</st> [<st c="29048">https://api.python.langchain.com/en/latest/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html#langchain_core.output_parsers.string.StrOutputParser</st>](https://api.python.langchain.com/en/latest/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html#langchain_core.output_parsers.string.StrOutputParser
    )
  prefs: []
  type: TYPE_NORMAL
- en: <st c="29216">Let’s look at a new type of parser, the JSON</st> <st c="29262">output
    parser.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="29276">JSON output parser</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="29295">As you</st> <st c="29302">can imagine, this output parser takes
    input from an</st> <st c="29355">LLM and outputs it as JSON.</st> <st c="29383">It
    is important to note that you may not need this parser, as many newer model providers
    support built-in ways to return structured output such as JSON and XML.</st> <st
    c="29544">This approach is for those that</st> <st c="29576">do not.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="29583">We start with some new imports, coming from a library we have
    already installed from</st> <st c="29669">LangChain (</st>`<st c="29680">langchain_core</st>`<st
    c="29695">):</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: <st c="29871">These lines import the necessary classes and modules from the</st>
    `<st c="29934">langchain_core</st>` <st c="29949">library and the</st> `<st c="29965">json</st>`
    <st c="29969">module.</st> `<st c="29978">JsonOutputParser</st>` <st c="29994">is
    used to parse the JSON output.</st> `<st c="30029">BaseModel</st>` <st c="30038">and</st>
    `<st c="30043">Field</st>` <st c="30048">are used to define the structure of the
    JSON output model.</st> `<st c="30108">Generation</st>` <st c="30118">is used
    to represent the generated output.</st> <st c="30162">And not surprisingly, we
    import a package for</st> `<st c="30208">json</st>`<st c="30212">, so that we
    can better manage our</st> <st c="30247">JSON inputs/outputs.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="30267">Next, we will create a Pydantic model called</st> `<st c="30313">FinalOutputModel</st>`
    <st c="30329">that represents the structure of the</st> <st c="30367">JSON output:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: <st c="30589">It has two</st> <st c="30601">fields:</st> `<st c="30609">relevance_score</st>`
    <st c="30624">(float) and</st> `<st c="30637">answer</st>` <st c="30643">(string),
    along with their descriptions.</st> <st c="30685">In a</st> *<st c="30690">real-world</st>*
    <st c="30700">application, this model is likely to</st> <st c="30737">get substantially
    more complex, but this gives you a general concept of how it can</st> <st c="30821">be
    defined.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="30832">Next, we will create an instance of the</st> `<st c="30873">JsonOutputParser</st>`
    <st c="30889">parser:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: <st c="30962">This line assigns</st> `<st c="30981">JsonOutputParser</st>` <st
    c="30997">with the</st> `<st c="31007">FinalOutputModel</st>` <st c="31023">class
    as a parameter to</st> `<st c="31048">json_parser</st>` <st c="31059">for use
    later in our code when we want to use</st> <st c="31106">this parser.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="31118">Next, we are going add a new function right in between our two
    other helper functions, and then we will update</st> `<st c="31230">conditional_answer</st>`
    <st c="31248">to use that new function.</st> <st c="31275">This code goes under
    the existing</st> `<st c="31309">extract_score</st>` <st c="31322">function, which
    remains</st> <st c="31347">the same:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: <st c="31566">This</st> `<st c="31572">format_json_output</st>` <st c="31590">function
    takes a dictionary,</st> `<st c="31620">x</st>`<st c="31621">, as input and formats
    it into a JSON output.</st> <st c="31667">It creates a</st> `<st c="31680">json_output</st>`
    <st c="31691">dictionary with two keys:</st> `<st c="31718">"relevance_score"</st>`
    <st c="31735">(obtained by calling</st> `<st c="31757">extract_score</st>` <st
    c="31770">on the</st> `<st c="31778">'relevance_score</st>`<st c="31794">’ value
    from</st> `<st c="31808">x</st>`<st c="31809">) and</st> `<st c="31815">"answer"</st>`
    <st c="31823">(directly taken from</st> `<st c="31845">x</st>`<st c="31846">).</st>
    <st c="31849">It then uses</st> `<st c="31862">json.dumps</st>` <st c="31872">to
    convert the</st> `<st c="31888">json_output</st>` <st c="31899">dictionary to
    a JSON string and creates a</st> `<st c="31942">Generation</st>` <st c="31952">object
    with the JSON string as its text.</st> <st c="31994">Finally, it uses</st> `<st
    c="32011">json_parser</st>` <st c="32022">to parse the</st> `<st c="32036">Generation</st>`
    <st c="32046">object and returns the</st> <st c="32070">parsed result.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="32084">We will</st> <st c="32092">need to reference this function in
    the function we were</st> <st c="32148">previously using,</st> `<st c="32167">conditional_answer</st>`<st
    c="32185">. Update</st> `<st c="32194">conditional_answer</st>` <st c="32212">like
    this:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: <st c="32386">Here, we update the</st> `<st c="32407">conditional_answer</st>`
    <st c="32425">function to apply that</st> `<st c="32449">format_json_output</st>`
    <st c="32467">function if it determines the answer is relevant and before it provides
    the</st> <st c="32544">returned output.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="32560">Next, we are going to take the two chains we had before in our
    code and combine them into one larger chain handling the entire pipeline.</st>
    <st c="32698">In the past, it was helpful to show this separately to give more
    focus to certain areas, but now we have a chance to clean up and show how these
    chains can be grouped together to handle our entire</st> <st c="32895">logic flow:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: <st c="33422">If you</st> <st c="33429">look back at previous code labs, this
    was represented</st> <st c="33483">by two chains.</st> <st c="33499">Note that
    this is using</st> `<st c="33523">str_output_parser</st>` <st c="33540">in the
    same way it was before.</st> <st c="33572">You do not see the JSON parser here
    because it is applied in the</st> `<st c="33637">format_json_output</st>` <st
    c="33655">function, which is called from the</st> `<st c="33691">conditional_answer</st>`
    <st c="33709">function, which you see in the last line.</st> <st c="33752">This
    simplification of these chains works for this example, focused on parsing our
    output into JSON, but we should note that we do lose the context that we have
    used in previous code labs.</st> <st c="33941">This is really just an example
    of an alternative approach to setting up</st> <st c="34013">our chain(s).</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="34026">Lastly, because our final output is in JSON format that we did
    need to add the context to, we need to update our</st> *<st c="34140">test</st>*
    *<st c="34145">run</st>* <st c="34148">code:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: <st c="34401">When</st> <st c="34407">we print this out, we see a similar result
    as in the</st> <st c="34460">past, but we show how the JSON formatted final</st>
    <st c="34507">output looks:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: <st c="36580">This is a</st> <st c="36591">simple example of a JSON output,
    but you can build off this</st> <st c="36651">and shape the JSON to anything you
    need using the</st> `<st c="36701">FinalOutputModel</st>` <st c="36717">class
    we defined and passed into our</st> <st c="36755">output parser.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="36769">You can find more information about the JSON parser</st> <st c="36822">here:</st>
    [<st c="36828">https://python.langchain.com/v0.2/docs/how_to/output_parser_json/</st>](https://python.langchain.com/v0.2/docs/how_to/output_parser_json/
    )
  prefs: []
  type: TYPE_NORMAL
- en: <st c="36893">It is important to note that it is difficult to rely on LLMs to
    output in a certain format.</st> <st c="36986">A more robust system would incorporate
    the parser deeper into the system, where it will likely be able to better utilize
    the JSON output, but it will also entail more checks to make sure the</st> <st
    c="37177">formatting</st> <st c="37187">is as required for the next step to work
    off properly formatted JSON.</st> <st c="37258">In our code here, we implemented
    a very lightweight layer for JSON formatting to show how the output parser could
    fit into our RAG application in a very</st> <st c="37411">simple way.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="37422">Summary</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="37430">In this chapter, we learned about various components in LangChain
    that can enhance a RAG application.</st> *<st c="37533">Code lab 11.1</st>* <st
    c="37546">focused on document loaders, which are used to load and process documents
    from various sources such as text files, PDFs, web pages, or databases.</st> <st
    c="37693">The chapter covered examples of loading documents from HTML, PDF, Microsoft
    Word, and JSON formats using different LangChain document loaders, noting that
    some document loaders add metadata which may require adjustments in</st> <st c="37916">the
    code.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '*<st c="37925">Code lab 11.2</st>* <st c="37939">discussed text splitters,
    which divide documents into chunks suitable for retrieval, addressing issues with
    large documents and context representation in vector embeddings.</st> <st c="38113">The
    chapter covered</st> `<st c="38133">CharacterTextSplitter</st>`<st c="38154">,
    which splits text into arbitrary N-character-sized chunks, and</st> `<st c="38219">RecursiveCharacterTextSplitter</st>`<st
    c="38249">, which recursively splits text while trying to keep related pieces
    together.</st> `<st c="38327">SemanticChunker</st>` <st c="38342">was introduced
    as an experimental splitter that combines semantically similar sentences to create
    more</st> <st c="38446">meaningful chunks.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="38464">Lastly,</st> *<st c="38473">Code lab 11.3</st>* <st c="38486">focused
    on output parsers, which structure the responses from the language model in a
    RAG application.</st> <st c="38590">The chapter covered the string output parser,
    which outputs the LLM’s response as a string, and the JSON output parser, which
    formats the output as JSON using a defined structure.</st> <st c="38770">An example
    was provided to show how the JSON output parser can be integrated into the</st>
    <st c="38856">RAG application.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="38872">In the next chapter, we will cover a relatively advanced but very
    powerful topic, LangGraph and</st> <st c="38969">AI agents.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="0">Part 3 – Implementing Advanced RAG</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="35">In this part, you will learn advanced techniques for enhancing your
    RAG applications, including integrating AI agents with LangGraph for more sophisticated
    control flows, leveraging prompt engineering strategies to optimize retrieval
    and generation, and exploring cutting-edge approaches such as query expansion,
    query decomposition, and multi-modal RAG.</st> <st c="391">You’ll gain hands-on
    experience in implementing these techniques through code labs and discover a wealth
    of additional methods covering indexing, retrieval, generation, and the entire</st>
    <st c="575">RAG pipeline.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="588">This part contains the</st> <st c="612">following chapters:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[*<st c="631">Chapter 12</st>*](B22475_12.xhtml#_idTextAnchor242)<st c="642">,</st>
    *<st c="644">Combining RAG with the Power of AI Agents and LangGraph</st>*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*<st c="699">Chapter 13</st>*](B22475_13.xhtml#_idTextAnchor256)<st c="710">,</st>
    *<st c="712">Using Prompt Engineering to Improve RAG Efforts</st>*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*<st c="759">Chapter 14</st>*](B22475_14.xhtml#_idTextAnchor283)<st c="770">,</st>
    *<st c="772">Advanced RAG-Related Techniques for Improving Results</st>*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
