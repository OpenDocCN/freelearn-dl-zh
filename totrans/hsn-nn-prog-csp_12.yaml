- en: Replacing Back Propagation with PSO
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the latest examples of success with neural networks is the field of study
    known as **Swarm Intelligence**. Even though this field of study has been around
    for many years, advances in computer hardware combined with our understanding
    of studying animals has helped us to take this fascinating field out of the laboratory
    and into many different directions and real-world scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to cover the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Basic theory on Particle Swarm Optimization, or PSO for short
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The open-source machine-learning framework Encog
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replacing the conventional back propagation with PSO
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will require Microsoft Visual Studio and also might want refer [https://github.com/encog](https://github.com/encog).
  prefs: []
  type: TYPE_NORMAL
- en: Check out the following video to see Code in Action: [http://bit.ly/2QPd6Qo](http://bit.ly/2QPd6Qo).
  prefs: []
  type: TYPE_NORMAL
- en: Basic theory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OK, pop quiz time. What do a flock of birds, a school of fish, and a swarm of
    bees all have in common? Swarm intelligence—knowing how to cooperatively live
    and work near each other while optimally achieving the same objective. It's not
    about the intelligence of the individual, but rather the achievements of the group.
    No one individual has a clear path or directive, no one is at the top shouting
    orders, yet the optimal goal is always accomplished. Swarms of bees find new nests
    by doing waggle dances. Birds fly in great harmony, each taking turns being the
    leader. Fish swim collectively in beautiful architectures we call schools. But
    if we as humans always need someone at the top giving orders, and we still collectively
    don't always agree, how is it that millions of these little creatures have been
    doing it for years and we can't? Oops, going off on a tangent there, sorry!
  prefs: []
  type: TYPE_NORMAL
- en: Let's start off with a few quick definitions that will be used throughout to
    ensure we are all on the same page going forward.
  prefs: []
  type: TYPE_NORMAL
- en: Swarm intelligence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Swarm intelligence is the collective behavior of self-organizing systems, decentralized
    in nature. The swarm itself exhibits social cognitive behavior and achieves a
    goal that individual contributors would not achieve by themselves. The collective
    achieves the goals rather than the efforts of any individual contributor. This
    leads us to PSO itself.
  prefs: []
  type: TYPE_NORMAL
- en: Particle Swarm Optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Particle Swarm Optimization** is a method (a population-based algorithm)
    that solves a problem by optimizing it iteratively while trying to improve a potential
    solution regarding its optimal quality. Every individual particle in the PSO algorithm
    learns from itself and another particle with a good fitness value. Each particle,
    which represents a solution, flies through the search space with a velocity that
    is dynamically adjusted according to its own and its companion''s historical behaviors.
    The particles tend to fly toward better search areas over the course of the search
    process.'
  prefs: []
  type: TYPE_NORMAL
- en: Types of Particle Swarm Optimizations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following is a list of just some of the variants of Particle Swarm Optimization:'
  prefs: []
  type: TYPE_NORMAL
- en: Traditional Particle Swarm Optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Canonical Particle Swarm Optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fully informed Particle Swarm Optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s now talk a little bit about the theory behind swarm intelligence, and
    then we''ll move into two of the more specialized types of study in that field:
    Particle Swarm Optimization and ant swarm optimization, which are both direct
    drop-in replacements for back propagation!'
  prefs: []
  type: TYPE_NORMAL
- en: However fascinating and intriguing you find this, please remember that nothing
    is perfect and there's no single shiny bullet that works for everything. This
    is a fascinating theory of study and entire books have been written on the subject.
    However, we always need to keep in mind the **No Free Lunch Theorem for Optimization**.
  prefs: []
  type: TYPE_NORMAL
- en: The No Free Lunch Theorem for Optimization states that no one can propose any
    one specific algorithm for solving all optimization problems. The success of an
    algorithm in solving one specific set of problems does not guarantee that it solves
    all optimization problems. More concretely, all optimization techniques perform
    equally well on average if you consider every optimization problem despite the
    performance on a subset of problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a very well written paper titled *A time performance comparison of Particle
    Swarm Optimization in mobile devices*, written by Luis Antonio Beltrán Prieto,
    Zuzana Komínkova-Oplatková, Rubén Torres Frías, and Juan Luis Escoto Hernández,
    Particle Swarm Optimization is described like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '"PSO is an optimization technique developed by Kennedy and Eberhart inspired
    by the collective behavior of animal groups, such as swarms of insects, to build
    a swarm of particles, i.e., a set of candidate solutions which flow through the
    parameter space generating trajectories driven by the best individuals. The initial
    population (swarm) consists of random solutions (particles) for the problem and
    is considered as a population of homogeneous agents which interact locally with
    other individuals without any central control. As a result, collective behavior
    is generated, thus evolution relies on cooperation and competition among individuals
    through the different epochs (generations). Each particle defines trajectories
    in the parameter space according to a motion function which is affected by velocity,
    inertia, cognitive coefficient and social coefficient. The objective is to find
    the global best solutions by stochastic weighting of the aforementioned elements.
    The process is iterative until a stopping criterion is met."'
  prefs: []
  type: TYPE_NORMAL
- en: More intuitive analogies for Particle Swarm Optimization are birds and how they
    behave collaboratively, or a swarm of bees and how they determine which flowers
    to visit or which humans to attack! If you've ever watched a flock of birds flying
    or inadvertently knocked down a bee's nest then you know exactly what I am referring
    to.
  prefs: []
  type: TYPE_NORMAL
- en: Now, instead of dealing in just theory, let's take a hypothetical journey, a
    Treasure Hunt. I am intentionally going to make this as verbose as possible to
    ensure the analogy fits the problem space. It goes something like this.
  prefs: []
  type: TYPE_NORMAL
- en: You and several of your friends are in a mountainous region trying to find a
    hidden treasure worth a lot of money. We are not quite sure where the treasure
    is located, but we do know that it is in the deepest valley in the region. This
    equates to the minimum elevation in terms of height above sea level.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us also state that all our friends can communicate with one another using
    their cell phones (let''s assume we have cell service here!). Let''s also assume
    for now that our cell phones have GPS apps on them that tell us the elevation
    we are currently at. We will search each day for the treasure until we find it,
    and at the end of each day we will have either found the treasure and are rich,
    or we need to update our information and try again the next day. So, here''s what
    each person has:'
  prefs: []
  type: TYPE_NORMAL
- en: A cell phone with a GPS app to determine elevation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pen and paper to track our information at the end of each day. On this we will
    write the best position we have found (individually), which is our personal best,
    or **PBEST**. We will also write on this paper the best position that the entire
    team has found thus far, being our global best value or **GBEST**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following are the rules that we need to follow in our search:'
  prefs: []
  type: TYPE_NORMAL
- en: Each person will start in a random location and in a random direction. We determine
    our elevation right away and write it on our paper. It would be best for us if
    each person was spread out as much as possible so that we can be efficient and
    cover more ground, but this is not necessary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our journey will take *T* number of days, to which at this point we are now
    aware of what that value is or will be.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Every morning we will plan our day.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Communications can only take place at the end of each day.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each morning, everyone compares the elevations they are at and updates **GBEST**
    on their paper.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GBEST** is the only piece of information each person can share (location
    and elevation).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each person will update **PBEST** on their paper if they find a better position.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PBEST** information is not shared; no one cares about anything but GBEST.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Take notes of this one; to move each day, each person takes (for instance) *x* steps
    in the direction of the last day, *y* steps in the direction towards **PBEST**,
    and *z* steps in the direction of **GBEST**. Confused?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Steps are random as we need some form of randomness in the search to make a
    stochastic search pattern for everyone as a collective group (that is, a flock
    or swarm of people).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With these few rules behind us, we can start our journey to the treasure. The
    team as a collective will keep locating different regions while watching the GBEST
    location found thus far. There is no guarantee of course that we will find the
    treasure, or that we will find it in the minimal number of days, but generally
    our search should be effective. Remember, no individual knows the exact location
    of the treasure, but cooperates with the swarm to develop collective intelligence
    to help find the treasure faster. For sure, it's better than a completely random
    search!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try and plot out our steps in pseudo-pseudo-code:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize a population of random solutions. For x number of decision variables,
    we have an x-space in which our solution exists as particles. Each particle has
    n variables and stores the best fitness for itself and the team.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each iteration (either a number or a fitness value), calculate the fitness
    and store the best fitness variable (**PBEST**) and communicate this to the swarm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Identify **GBEST** by comparing all the information we have received from the
    collective swarm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Identify what will take us in the direction of **GBEST** considering our **PBEST**
    and **GBEST**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Move in a specific time step in the direction of our velocity vector.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Over time, each team member (our particles in the swarm) will identify better
    **GBEST** variables and navigate towards them, thus also improving their **PBEST**
    at the same time.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'With Particle Swarm Optimization we have three basic components that we should
    briefly discuss. They are, in no particular order:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Position**: Similar to the location in the preceding analogy, referring to
    the parameter values. This refers to where a particle (bird, bee, fish, and so
    on) is in an x-dimensional search space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Velocity**: Similar to the movement direction in the preceding analogy, it
    is used for storing velocity, which will update each particle''s position.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fitness**: Similar to the elevation in the preceding analogy, this shows
    how fit the particle is.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Velocity is the main part of our Particle Swarm Optimization. It considers
    the current position of the particle, the best position found by the swarm (**GBEST**)
    (all particles), and the best position of the current particle (**PBEST**). Mathematically,
    it breaks down like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c772b9c1-f58f-405b-a68a-af3d00dcf41c.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/dfda3de2-bfd5-4853-8746-103f27d6c048.png)'
  prefs: []
  type: TYPE_IMG
- en: There are also three hyperparameters that we should mention as you will be hearing
    about them a lot.
  prefs: []
  type: TYPE_NORMAL
- en: '**Inertia Weight** (**W**): The inertia weight controls the impact of the previous
    historical velocities on the current velocity. It regulates the trade-off between
    the global and local exploration abilities. If the inertia is high, particles
    are constrained in changing their direction and thus turn around much slower.
    This implies a larger exploration area and less possibility of convergence towards
    the optimum.'
  prefs: []
  type: TYPE_NORMAL
- en: If inertia is small, then only a small amount of momentum is present from the
    previous time-step; this allows for much faster changes in direction. The problem
    here is that it could take quite a bit longer to converge.
  prefs: []
  type: TYPE_NORMAL
- en: By decreasing the inertia weight, it is easier to obtain a better global search
    ability and make the particles enter the optimal value area earlier. This means
    it will then be easier to have a better search ability and optimum value.
  prefs: []
  type: TYPE_NORMAL
- en: '**C1**: **Cognitive intelligence**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***C2***: **Social intelligence**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It should be noted that C1 and C2 are positive constants that control how far
    an individual particle can move within a single iteration. Lower values will allow
    particles to stray further from the targeted regions before being reined in. Higher
    values will result in shorter, more abrupt movements toward, or past, the targeted
    region. By default, we will set both values to 2.0.
  prefs: []
  type: TYPE_NORMAL
- en: You should experiment with the cognitive intelligence and social intelligence
    values, as sometimes different values lead to improved performance.
  prefs: []
  type: TYPE_NORMAL
- en: Original Particle Swarm Optimization strategy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As the particles (bees, birds, fish, termites) are moving along in the pre-designated
    search space to determine the best position, during each iteration of the cycle
    (where a *cycle* may be referred to as *max iterations*), each particle updates
    its velocity and position. Once the new velocity has been determined, it is used
    to compute the new particle position for the next time step.
  prefs: []
  type: TYPE_NORMAL
- en: Particle Swarm Optimization search strategy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For every particle over time, we will track the inertia (current velocity),
    the personal best (referred to as PBEST) and the global best (referred to as **GBEST**).
    As we mentioned, as we move through time to our global minimum, we will be tracking
    our personal best location, as well as the global best location of the swarm.
    This information will be communicated to the rest of the group so that the swarms'
    best location information can be communicated back to the group after each iteration
    is completed. We need to be either following the swarm or leading it in order
    to achieve our goals.
  prefs: []
  type: TYPE_NORMAL
- en: Particle Swarm Optimization search strategy pseudo-code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following is pseudo-code for the logic we will be using to find our global
    minimum (the location of the treasure):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Parameter effects on optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many varying theories for what each variable in Particle Swarm Optimization
    should look like. There are the theoretically acceptable values, and then there
    are the values determined over time with testing. The following are some of the
    recommendations I am making for your consideration.
  prefs: []
  type: TYPE_NORMAL
- en: The original (canonical) version of the Particle Swarm Optimization algorithm
    used values of 1, 2, and 2 respectively for Inertia, C1, and C2\. These values
    do seem to work quite well. I have also found through testing, as have others,
    that values of 0.5, 1.5, and 1.5 respectively work even better, providing the
    best convergence rate depending upon the function and strategy used. Other values
    lead to slower or complete non-convergence. You, the reader, should perform your
    own testing based upon the strategy and function you prefer and determine which
    values you find suitable for your purpose.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that, depending upon the strategy and function you select, your
    values should be different to provide proper convergence. For instance, using
    a minimization strategy and a Step function, I have seen optimal convergence happen
    using a global value of 0.729 for inertia. The cognitive intelligence (C1) is
    usually the same as the social intelligence (C2) with a pre-determined value of
    2\. I should point out however that, as you will see when we get to the chapter
    on building and using the visual workbench, the default value I use for C1 and
    C2 is 1.49445.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that any of the values shown here are not pulled out
    of thin air. They come from a tremendous amount of optimization testing. In addition,
    they closely align with those tested by Clerc and Kennedy (2002) for implementation
    of constriction coefficients. Please feel free to use your own values and always
    keep in mind the No Free Lunch theorem.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example of how swarm optimization is affected by weight,
    social, and cognitive parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![](img/8bebba5d-9b6d-4822-92af-efba3568a005.png) | ![](img/13400d88-1349-4592-9c6e-ca3224db16a7.png)
    | ![](img/ff8e8709-92ba-416b-a9d6-cbe78668ffff.png) |'
  prefs: []
  type: TYPE_TB
- en: '| Iteration=31,w=0,c1=c2=2  | Iteration=31,w=0.59,c1=c2=2 | Iteration=31,w=1,c1=c2=2
    |'
  prefs: []
  type: TYPE_TB
- en: Replacing back propagation with Particle Swarm Optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'And now we come to the moment of truth. How does any of this apply to my code?
    In order to answer this question, we are going to use the open source Encog machine
    learning framework for our next demonstration. You can download our sample project
    following the instructions for the web location of the files for the book. Please
    make sure you have it loaded and open in Visual Studio before proceeding:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0ad5422d-0e11-4afb-b539-ff935d1bec5b.png)'
  prefs: []
  type: TYPE_IMG
- en: We are going to create a sample application that will demonstrate replacing
    back propagation with Particle Swarm Optimization. If all goes well, from the
    outside looking in you will not notice a difference.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will be able to run this sample out of the box and follow along. We will
    be using the XOR problem solver, but instead of using back propagation it will
    be using the Particle Swarm Optimization we''ve been discussing. Let''s dig a
    little deeper into the code. The following is the data that we will be using to
    implement this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Pretty straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s look at the sample application itself. The following is how the
    XORPSO implementation is done:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run this sample application here is what it looks like. You will notice
    that it appears exactly like the normal XOR sample from the outside looking in:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/584424e8-4d92-497d-a14d-de57a029955e.png)'
  prefs: []
  type: TYPE_IMG
- en: You will notice that, when training is completed, we are very close to our ideal
    scores.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's talk about the internals. Let's look at some of the internal variables
    used to make this work. The following is where you will see why we spent time
    early on with our basic theory. It should all be familiar to you now.
  prefs: []
  type: TYPE_NORMAL
- en: 'Declare the variable `m_populationSize`. A typical range is 20 - 40 for many
    problems. More difficult problems may need a much higher value. It must be low
    enough to keep the training process computationally efficient:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This determines the size of the search space. The positional components of
    particle will be bounded to [-maxPos, maxPos]. A well chosen range can improve
    the performance. `-1` is a special value that represents boundless search space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This maximum change one particle can take during one iteration imposes a limit
    on the maximum absolute value of the velocity components of a particle, and affects
    the granularity of the search. If too high, particles can fly past the optimum
    solution. If too low, particles can get stuck in local minima. It is usually set
    to a fraction of the dynamic range of the search space (10% was shown to be good
    for high dimensional problems). -1 is a special value that represents boundless
    velocities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'For c1, cognitive learning rate >= 0 (the tendency to return to the personal
    best position):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'For c2, social learning rate >= 0 (tendency to move towards the swarm best
    position):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Inertia weight, w, controls global (higher-value) versus local exploration
    of the search space. It is analogous to temperature in simulated annealing and
    must be chosen carefully or gradually decreased over time. The value is usually
    between 0 and 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'All these variables should be familiar to you. Next, the heart of what we are
    doing involves the `UpdateParticle` function, shown as follows. This function
    is responsible for updating the velocity, position, and personal best position
    of a particle:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a new particle with random values (except the first particle, which
    has the same value as the network passed to the algorithm):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Randomize the velocity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Velocity clamping:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'New position ![](img/f955e8d5-b0d4-4511-b36a-a2f15a21981e.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Pin the particle against the boundary of the search space (only for components
    exceeding `maxPosition`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Each particle will need to have its velocity updated, as you can see in the
    preceding code. This function will use the inertia weight, cognitive, and social
    terms to compute the velocity of the particle. This function encompasses the standard
    Particle Swarm Optimization formula as we described in the pseudo-code earlier
    in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Standard PSO formula for inertia weight:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Standard PSO formula for cognitive term:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Standard PSO formula for social term:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: And this is how we substituted Particle Swarm Optimization for the standard
    backward propagation. Simple, right?
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we learned some basic theory behind Particle Swarm Optimization.
    We learned how this algorithm applies to, and has been influenced by flocks of
    birds, swarms of bees, schools of fish, and more. We also saw how we could replace
    the standard back propagation formula with Particle Swarm Optimization.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to learn how to delve into function optimization
    and show you how you can find optimal parameters, a process that will save you
    countless hours of testing!
  prefs: []
  type: TYPE_NORMAL
