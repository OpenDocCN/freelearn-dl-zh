<html><head></head><body>
		<div><h1 id="_idParaDest-176" class="chapter-number"><a id="_idTextAnchor289"/>15</h1>
			<h1 id="_idParaDest-177"><a id="_idTextAnchor290"/>Generating Image Descriptions Using BLIP-2 and LLaVA</h1>
			<p>Imagine you have an image in hand and need to upscale it or generate new images based on it, but you don’t have the prompt or description associated with it. You may say, <em class="italic">“Fine, I can write up a new prompt for it.” </em>For one image, that is acceptable, what if there are thousands or even millions of images without descriptions? It is impossible to write them all up manually.</p>
			<p>Fortunately, we can use artificial intelligence (AI) to help us generate descriptions. There are many pretrained models that can achieve this goal, and the number is always increasing. In this chapter, I am going to introduce two AI solutions to generate the caption, description, or prompt for an image, all fully automated:</p>
			<ul>
				<li>BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models [1]</li>
				<li>LLaVA: Large Language and Vision Assistant [3]</li>
			</ul>
			<p>BLIP-2 [1] is fast and requires relatively low hardware, while LLaVA [3] (with its <code>llava-v1.5-13b</code> model) is the newest and most powerful model at the time of writing.</p>
			<p>By the end of this chapter, you will be able to do the following:</p>
			<ul>
				<li>Generally understand how BLIP-2 and LLaVA work</li>
				<li>Write up Python code to use BLIP-2 and LLaVA to generate descriptions from images</li>
			</ul>
			<h1 id="_idParaDest-178"><a id="_idTextAnchor291"/>Technical requirements</h1>
			<p>Before diving into the BLIP-2 and LLaVA, let’s use Stable Diffusion to generate an image for testing.</p>
			<p>First, load up a <code>deliberate-v2</code> model without sending it to CUDA:</p>
			<pre class="source-code">
import torch
from diffusers import StableDiffusionPipeline
text2img_pipe = StableDiffusionPipeline.from_pretrained(
    "stablediffusionapi/deliberate-v2",
    torch_dtype = torch.float16
)</pre>
			<p>Next, in the following code, we first send the model to CUDA and generate an image, then we offload the model to CPU RAM, and clear the model out from CUDA:</p>
			<pre class="source-code">
text2img_pipe.to("cuda:0")
prompt ="high resolution, a photograph of an astronaut riding a horse"
input_image = text2img_pipe(
    prompt = prompt,
    generator = torch.Generator("cuda:0").manual_seed(100),
    height = 512,
    width = 768
).images[0]
text2img_pipe.to("cpu")
torch.cuda.empty_cache()
input_image</pre>
			<p>The preceding code will give us an image similar to that shown in the following figure, which will be used in the following sections:</p>
			<div><div><img src="img/B21263_15_01.jpg" alt="Figure 15.1: An image of an astronaut riding a horse, generated by SD v1.5"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 15.1: An image of an astronaut riding a horse, generated by SD v1.5</p>
			<p>Now, let’s get started.</p>
			<h1 id="_idParaDest-179">B<a id="_idTextAnchor292"/>LIP-2 – Bootstrapping Language-Image Pre-training</h1>
			<p>In the <em class="italic">BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</em> paper [4], Junnan Li et al. proposed a solution to bridge the gap between natural language and vision modalities. Notably, the BLIP model has demonstrated exceptional capabilities in generating high-quality image descriptions, surpassing existing benchmarks at the time of its publication.</p>
			<p>The reason behind its excellent quality is that Junnan Li et al. used an innovative technique to build two models from their first pretrained model:</p>
			<ul>
				<li>Filter model</li>
				<li>Captioner model</li>
			</ul>
			<p>The filter model can filter out low-quality text-image pairs, thus improving the training data quality, while its caption generation model can generate surprisingly good, short descriptions for the image. With the help of these two models, the authors of the paper not only <a id="_idIndexMarker451"/>improved the training data quality but also enlarged its size automatically. Then, they used the boosted train data to train the BLIP model again and the result was impressively good. But this was the story of 2022.</p>
			<p>In June 2023, the same team from Salesforce brought out the new BLIP-2.<a id="_idTextAnchor293"/></p>
			<h2 id="_idParaDest-180"><a id="_idTextAnchor294"/>How BLIP-2 works</h2>
			<p>BLIP was<a id="_idIndexMarker452"/> good at the time, but the language part of its model was still relatively weak. <strong class="bold">Large language models</strong> (<strong class="bold">LLMs</strong>) such <a id="_idIndexMarker453"/>as OpenAI’s GPT and Meta’s LLaMA are powerful, but also extremely expensive to train. So the BLIP team framed the challenge by asking themselves: can we take off-the-shelf, pretrained frozen image encoders and frozen LLMs and use them for vision language pretraining while still preserving their learned representations?</p>
			<p>The answer is yes. BLIP-2 solved this by introducing a Query Transformer that helps generate the visual representations corresponding to a text caption, which then is fed to a frozen LLM to decode text descriptions.</p>
			<p>The Query Transformer, often referred to as the Q-Former [2], is a crucial component of the BLIP-2 model. It serves as a bridge connecting the frozen image encoder and the frozen LLM. The primary function of the Q-Former is to map a set of “query tokens” to query embeddings. These query embeddings help in extracting visual features from the image encoder that are most pertinent to the given text instruction.</p>
			<p>During the training process of the BLIP-2 model, the weights of the image encoder and the LLM remain frozen. Meanwhile, the Q-Former undergoes training, allowing it to adapt and optimize its performance based on the specific task requirements. By employing a set of learnable query vectors, the Q-Former effectively distills valuable information from the image encoder, making it possible for the LLM to generate accurate and contextually appropriate responses grounded in visual content.</p>
			<p>A similar concept is also employed in LLaVA, which we will discuss later. The core idea of BLIP is to reuse the effective vision and language components, and only train a middle model to <a id="_idIndexMarker454"/>bridge them together.</p>
			<p>Next, let’s start using BLIP<a id="_idTextAnchor295"/>-2.</p>
			<h2 id="_idParaDest-181"><a id="_idTextAnchor296"/>Using BLIP-2 to generate descriptions</h2>
			<p>Using BLIP-2 is<a id="_idIndexMarker455"/> easy and clean with the help of the Hugging Face transformers [5] package. If you don’t have the package installed, simply run the following command to install or update it to the newest version:</p>
			<pre class="source-code">
pip install -U transformer</pre>
			<p>Then load up the BLIP-2 model data with the following code:</p>
			<pre class="source-code">
from transformers import AutoProcessor, Blip2ForConditionalGeneration
import torch
processor = AutoProcessor.from_pretrained("Salesforce/blip2-opt-2.7b")
# by default `from_pretrained` loads the weights in float32
# we load in float16 instead to save memory
device = "cuda" if torch.cuda.is_available() else "cpu"
model = Blip2ForConditionalGeneration.from_pretrained(
    "Salesforce/blip2-opt-2.7b",
    torch_dtype=torch.float16
).to(device)</pre>
			<p>Your first run will automatically download the model weights data from the Hugging Face model repository. It may take some time, so please be patient. Once the downloading is finished, run the following code to ask BLIP-2 about the image we provide:</p>
			<pre class="source-code">
prompt = "describe the content of the image:"
inputs = processor(
    input_image,
    text=prompt,
    return_tensors="pt"
).to(device, torch.float16)
generated_ids = model.generate(**inputs, max_new_tokens=768)
generated_text = processor.batch_decode(
    generated_ids,
    skip_special_tokens=True
)[0].strip()
print(generated_text)</pre>
			<p>The code returns the description <code>astronaut on horseback in space</code>, which is good and accurate. What if we ask how many planets are in the background? Let’s change the prompt to <code>how many planets in the background:</code>. And it returns <code>the universe is bigger than you think</code>. Not good enough this time.</p>
			<p>So, BLIP-2 is<a id="_idIndexMarker456"/> good at generating short descriptions of an entire image quickly. However, to generate more detailed descriptions or even interact with images, we can leverage the power of L<a id="_idTextAnchor297"/>LaVA.</p>
			<h1 id="_idParaDest-182"><a id="_idTextAnchor298"/>LLaVA – Large Language and Vision Assistant</h1>
			<p>As<a id="_idIndexMarker457"/> suggested by its name <strong class="bold">LLaVA</strong> [3], this model is very close to LLaMA, not only in name but also in terms of their internals. LLaVA uses LLaMA as its language part. making it possible to swap out the language model if needed This is definitely a killer feature for many scenarios. One of the key features of Stable Diffusion is its openness for model swapping and fine-tuning. Similar to Stable Diffusion, LLaVA is designed to leverage open-sourced LLM models.</p>
			<p>Next, let’s take a look at how LLaVA <a id="_idTextAnchor299"/>works.</p>
			<h2 id="_idParaDest-183"><a id="_idTextAnchor300"/>How LLaVA works</h2>
			<p>The LLaVA <a id="_idIndexMarker458"/>authors, Haotian Liu et al. [3], present a beautiful, accurate diagram showing how the model leverages pretrained CLIP and LLaMA models in its architecture, as shown in the following figure:</p>
			<div><div><img src="img/B21263_15_02.jpg" alt="Figure 15.2: Architecture of LLaVA"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 15.2: Architecture of LLaVA</p>
			<p>Let’s read the diagram from the bottom up. During the inference, we provide an image denoted as X v, and a language instruction denoted as X q. The vision encoder is the CLIP vision encoder ViT-L/14 [6]. The same CLIP is used by Stable Diffusion v1.5 as its text encoder.</p>
			<p>The CLIP model encodes the image to Z v, and the projection W is the model data provided by LLaVA. The projection model projects the encoded image embedding Z v to H v as follows:</p>
			<p>H v = W ⋅ Z v</p>
			<p>On the other side, the language instruction is encoded into CLIP’s 512-dimensional embedding chunks. Both the image and language embeddings share the same dimensionality.</p>
			<p>In this manner, the language model f ϕ is aware of both the image and the language! This method bears some similarity to Stable Diffusion’s textual inversion technique, being lightweight yet powerful.</p>
			<p>Next, let’s write some code to instruct LLaVA to interact with an <a id="_idTextAnchor301"/>image.</p>
			<h2 id="_idParaDest-184"><a id="_idTextAnchor302"/>Installing LLaVA</h2>
			<p>For an optimal <a id="_idIndexMarker459"/>experience, it is strongly advised to use LLaVA on a Linux machine. Using it on Windows may result in unexpected missing components. It is also suggested to establish a Python virtual environment for using LLaVA. Detailed steps and commands for setting up a Python virtual environment were provided in <a href="B21263_02.xhtml#_idTextAnchor037"><em class="italic">Chapter 2</em></a>.</p>
			<p>Clone the LLaVA repository to your local folder:</p>
			<pre class="console">
git clone https://github.com/haotian-liu/LLaVA.git
cd LLaVA</pre>
			<p>Then install LLaVA by simply running the following command:</p>
			<pre class="console">
pip install -U .</pre>
			<p>Next, download the model file from the Hugging Face model repository:</p>
			<pre class="console">
# Make sure you have git-lfs installed (https://git-lfs.com)
git lfs install
git clone https://huggingface.co/liuhaotian/llava-v1.5-7b</pre>
			<p>Please note that the model file is large and the download will take some time. At the time of writing this chapter, you can also download the 13B model by simply changing the <code>7</code> to <code>13</code> in the URL in the preceding code snippet to use the 13B LLaVA model.</p>
			<p>That’s it for the setup. Now, let’s proceed to writing the Pytho<a id="_idTextAnchor303"/>n code.</p>
			<h2 id="_idParaDest-185"><a id="_idTextAnchor304"/>Using LLaVA to generate image descriptions</h2>
			<p>Since we just <a id="_idIndexMarker460"/>installed LLaVA, we can reference the following related modules:</p>
			<pre class="source-code">
from llava.constants import (
    IMAGE_TOKEN_INDEX,
    DEFAULT_IMAGE_TOKEN,
    DEFAULT_IM_START_TOKEN,
    DEFAULT_IM_END_TOKEN
)
from llava.conversation import (
    conv_templates, SeparatorStyle
)
from llava.model.builder import load_pretrained_model
from llava.mm_utils import (
    process_images,
    tokenizer_image_token,
    get_model_name_from_path,
    KeywordsStoppingCriteria
)</pre>
			<p>Load the <code>tokenizer</code>, <code>image_processor</code>, and <code>model</code> components. <code>tokenizer</code> will convert text to token IDs, <code>image_processor</code> will convert images to tensors, and <code>model</code> is the pipeline <a id="_idIndexMarker461"/>that we will use to generate the output:</p>
			<pre class="source-code">
# load up tokenizer, model, image_processor
model_path = "/path/to/llava-v1.5-7b"
model_name = get_model_name_from_path(model_path)
conv_mode = "llava_v1"
tokenizer, model, image_processor, _ = load_pretrained_model(
    model_path = model_path,
    model_base = None,
    model_name = model_name,
    load_4bit = True,
    device = "cuda",
    device_map = {'':torch.cuda.current_device()}
)</pre>
			<p>The following is a breakdown of the preceding code:</p>
			<ul>
				<li><code>model_path</code>: This path points to the folder storing the pretrained models.</li>
				<li><code>model_base</code>: This is set to <code>None</code>, meaning no specific parent architecture has been specified.</li>
				<li><code>model_name</code>: The name of the pretrained model (<code>llava-v1.5</code>).</li>
				<li><code>load_4bit</code>: If set to <code>True</code>, this enables 4-bit quantization during inferencing. This reduces memory usage and boosts speed, but might negatively impact the quality of results slightly.</li>
				<li><code>device</code>: This specifies CUDA as the device where computations should occur.</li>
				<li><code>device_map</code>: This is used to map GPU devices to different parts of the model if you want to distribute the workload across multiple GPUs. Since only one device is mapped here, it implies a single GPU execution.</li>
			</ul>
			<p>Now, let’s create the<a id="_idIndexMarker462"/> image descriptions:</p>
			<ol>
				<li>Create a <code>conv</code> object to hold the conversation history:<pre class="source-code">
# start a new conversation</pre><pre class="source-code">
user_input = """Analyze the image in a comprehensive and detailed manner"""</pre><pre class="source-code">
conv = conv_templates[conv_mode].copy()</pre></li>
				<li>Convert the image to a tensor:<pre class="source-code">
# process image to tensor</pre><pre class="source-code">
image_tensor = process_images(</pre><pre class="source-code">
    [input_image],</pre><pre class="source-code">
    image_processor,</pre><pre class="source-code">
    {"image_aspect_ratio":"pad"}</pre><pre class="source-code">
).to(model.device, dtype=torch.float16)</pre></li>
				<li>Append an image placeholder to the conversation:<pre class="source-code">
if model.config.mm_use_im_start_end:</pre><pre class="source-code">
    inp = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + \</pre><pre class="source-code">
        DEFAULT_IM_END_TOKEN + '\n' + user_input</pre><pre class="source-code">
else:</pre><pre class="source-code">
    inp = DEFAULT_IMAGE_TOKEN + '\n' + user_input</pre><pre class="source-code">
conv.append_message(conv.roles[0], inp)</pre></li>
				<li>Get the <a id="_idIndexMarker463"/>prompt and convert it to tokens for inference:<pre class="source-code">
# get the prompt for inference</pre><pre class="source-code">
conv.append_message(conv.roles[1], None)</pre><pre class="source-code">
prompt = conv.get_prompt()</pre><pre class="source-code">
# convert prompt to token ids</pre><pre class="source-code">
input_ids = tokenizer_image_token(</pre><pre class="source-code">
    prompt,</pre><pre class="source-code">
    tokenizer,</pre><pre class="source-code">
    IMAGE_TOKEN_INDEX,</pre><pre class="source-code">
    return_tensors='pt'</pre><pre class="source-code">
).unsqueeze(0).cuda()</pre></li>
				<li>Prepare the stopping criteria:<pre class="source-code">
stop_str = conv.sep if conv.sep_style != \</pre><pre class="source-code">
    SeparatorStyle.TWO else conv.sep2</pre><pre class="source-code">
keywords = [stop_str]</pre><pre class="source-code">
stopping_criteria = KeywordsStoppingCriteria(keywords,</pre><pre class="source-code">
    tokenizer, input_ids)</pre></li>
				<li>Finally, get the<a id="_idIndexMarker464"/> output from LLaVA:<pre class="source-code">
# output the data</pre><pre class="source-code">
with torch.inference_mode():</pre><pre class="source-code">
    output_ids = model.generate(</pre><pre class="source-code">
        input_ids,</pre><pre class="source-code">
        images =image_tensor,</pre><pre class="source-code">
        do_sample = True,</pre><pre class="source-code">
        temperature = 0.2,</pre><pre class="source-code">
        max_new_tokens = 1024,</pre><pre class="source-code">
        streamer = None,</pre><pre class="source-code">
        use_cache = True,</pre><pre class="source-code">
        stopping_criteria = [stopping_criteria]</pre><pre class="source-code">
    )</pre><pre class="source-code">
outputs = tokenizer.decode(output_ids[0,</pre><pre class="source-code">
    input_ids.shape[1]:]).strip()</pre><pre class="source-code">
# make sure the conv object holds all the output</pre><pre class="source-code">
conv.messages[-1][-1] = outputs</pre><pre class="source-code">
print(outputs)</pre><p class="list-inset">As we can see in the following output, LLaVA can generate amazing descriptions:</p><pre class="source-code">
<strong class="bold">The image features a man dressed in a white space suit, riding a horse in a desert-like environment. The man appears to be a space traveler, possibly on a mission or exploring the area. The horse is galloping, and the man is skillfully riding it.</strong></pre><pre class="source-code">
<strong class="bold">In the background, there are two moons visible, adding to the sense of a space-themed setting. The combination of the man in a space suit, the horse, and the moons creates a captivating and imaginative scene.&lt;/s&gt;</strong></pre></li>
			</ol>
			<p>I have attempted to condense the code as much as possible, but it remains lengthy. It requires careful copying or transcription into your code editor. I recommend copy-pasting the code provided in the repository that accompanies this book. You can execute the aforementioned code in a single cell to observe how effectively LLaVA generates descriptions fr<a id="_idTextAnchor305"/>om an image.</p>
			<h1 id="_idParaDest-186"><a id="_idTextAnchor306"/>Summary</h1>
			<p>In this chapter, our primary focus was on two AI solutions designed to generate image descriptions. The first is BLIP-2, an effective and efficient solution for generating concise captions for images. The second is the LLaVA solution, which is capable of generating more detailed and accurate descriptive information from an image.</p>
			<p>With the assistance of LLaVA, we can even interact with an image to extract further information from it.</p>
			<p>The integration of vision and language capabilities also lays the groundwork for the development of even more powerful multimodal models, the potential of which we can only begin to imagine.</p>
			<p>In the next chapter, let’s get started using Stable D<a id="_idTextAnchor307"/>iffusion XL.</p>
			<h1 id="_idParaDest-187"><a id="_idTextAnchor308"/>References</h1>
			<ol>
				<li>Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, <em class="italic">BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language </em><em class="italic">Models</em>: <a href="https://arxiv.org/abs/2301.12597">https://arxiv.org/abs/2301.12597</a></li>
				<li>BLIP-2 Hugging Face documentation: <a href="https://huggingface.co/docs/transformers/main/model_doc/blip-2">https://huggingface.co/docs/transformers/main/model_doc/blip-2</a></li>
				<li>Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, <em class="italic">LLaVA: Large Language and Vision </em><em class="italic">Assistant</em>: <a href="https://llava-vl.github.io/">https://llava-vl.github.io/</a></li>
				<li>Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi, <em class="italic">BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and </em><em class="italic">Generation</em>: <a href="https://arxiv.org/abs/2201.12086">https://arxiv.org/abs/2201.12086</a></li>
				<li>Hugging Face Transformers GitHub repository: <a href="https://github.com/huggingface/transformers">https://github.com/huggingface/transformers</a></li>
				<li>Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, <em class="italic">Learning transferable visual models from natural language </em><em class="italic">supervision</em>: <a href="https://arxiv.org/abs/2103.00020">https://arxiv.org/abs/2103.00020</a></li>
				<li>LLaVA GitHub repository: <a href="https://github.com/haotian-liu/LLaVA">https://github.com/haotian-liu/LLaVA</a></li>
			</ol>
		</div>
	</body></html>