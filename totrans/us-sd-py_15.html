<html><head></head><body>
		<div id="_idContainer100">
			<h1 id="_idParaDest-176" class="chapter-number"><a id="_idTextAnchor289"/>15</h1>
			<h1 id="_idParaDest-177"><a id="_idTextAnchor290"/>Generating Image Descriptions Using BLIP-2 and LLaVA</h1>
			<p>Imagine you have an image in hand and need to upscale it or generate new images based on it, but you don’t have the prompt or description associated with it. You may say, <em class="italic">“Fine, I can write up a new prompt for it.” </em>For one image, that is acceptable, what if there are thousands or even millions of images without descriptions? It is impossible to write them all <span class="No-Break">up manually.</span></p>
			<p>Fortunately, we can use artificial intelligence (AI) to help us generate descriptions. There are many pretrained models that can achieve this goal, and the number is always increasing. In this chapter, I am going to introduce two AI solutions to generate the caption, description, or prompt for an image, all <span class="No-Break">fully automated:</span></p>
			<ul>
				<li>BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language <span class="No-Break">Models [1]</span></li>
				<li>LLaVA: Large Language and Vision <span class="No-Break">Assistant [3]</span></li>
			</ul>
			<p>BLIP-2 [1] is fast and requires relatively low hardware, while LLaVA [3] (with its <strong class="source-inline">llava-v1.5-13b</strong> model) is the newest and most powerful model at the time <span class="No-Break">of writing.</span></p>
			<p>By the end of this chapter, you will be able to do <span class="No-Break">the following:</span></p>
			<ul>
				<li>Generally understand how BLIP-2 and <span class="No-Break">LLaVA work</span></li>
				<li>Write up Python code to use BLIP-2 and LLaVA to generate descriptions <span class="No-Break">from images</span></li>
			</ul>
			<h1 id="_idParaDest-178"><a id="_idTextAnchor291"/>Technical requirements</h1>
			<p>Before diving into the BLIP-2 and LLaVA, let’s use Stable Diffusion to generate an image <span class="No-Break">for testing.</span></p>
			<p>First, load up a <strong class="source-inline">deliberate-v2</strong> model without sending it <span class="No-Break">to CUDA:</span></p>
			<pre class="source-code">
import torch
from diffusers import StableDiffusionPipeline
text2img_pipe = StableDiffusionPipeline.from_pretrained(
    "stablediffusionapi/deliberate-v2",
    torch_dtype = torch.float16
)</pre>
			<p>Next, in the following code, we first send the model to CUDA and generate an image, then we offload the model to CPU RAM, and clear the model out <span class="No-Break">from CUDA:</span></p>
			<pre class="source-code">
text2img_pipe.to("cuda:0")
prompt ="high resolution, a photograph of an astronaut riding a horse"
input_image = text2img_pipe(
    prompt = prompt,
    generator = torch.Generator("cuda:0").manual_seed(100),
    height = 512,
    width = 768
).images[0]
text2img_pipe.to("cpu")
torch.cuda.empty_cache()
input_image</pre>
			<p>The preceding code will give us an image similar to that shown in the following figure, which will be used in the <span class="No-Break">following sections:</span></p>
			<div>
				<div id="_idContainer098" class="IMG---Figure">
					<img src="image/B21263_15_01.jpg" alt="Figure 15.1: An image of an astronaut riding a horse, generated by SD v1.5"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 15.1: An image of an astronaut riding a horse, generated by SD v1.5</p>
			<p>Now, let’s <span class="No-Break">get started.</span></p>
			<h1 id="_idParaDest-179">B<a id="_idTextAnchor292"/>LIP-2 – Bootstrapping Language-Image Pre-training</h1>
			<p>In the <em class="italic">BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</em> paper [4], Junnan Li et al. proposed a solution to bridge the gap between natural language and vision modalities. Notably, the BLIP model has demonstrated exceptional capabilities in generating high-quality image descriptions, surpassing existing benchmarks at the time of <span class="No-Break">its publication.</span></p>
			<p>The reason behind its excellent quality is that Junnan Li et al. used an innovative technique to build two models from their first <span class="No-Break">pretrained model:</span></p>
			<ul>
				<li><span class="No-Break">Filter model</span></li>
				<li><span class="No-Break">Captioner model</span></li>
			</ul>
			<p>The filter model can filter out low-quality text-image pairs, thus improving the training data quality, while its caption generation model can generate surprisingly good, short descriptions for the image. With the help of these two models, the authors of the paper not only <a id="_idIndexMarker451"/>improved the training data quality but also enlarged its size automatically. Then, they used the boosted train data to train the BLIP model again and the result was impressively good. But this was the story <span class="No-Break">of 2022.</span></p>
			<p>In June 2023, the same team from Salesforce brought out the <span class="No-Break">new BLIP-2.</span><a id="_idTextAnchor293"/></p>
			<h2 id="_idParaDest-180"><a id="_idTextAnchor294"/>How BLIP-2 works</h2>
			<p>BLIP was<a id="_idIndexMarker452"/> good at the time, but the language part of its model was still relatively weak. <strong class="bold">Large language models</strong> (<strong class="bold">LLMs</strong>) such <a id="_idIndexMarker453"/>as OpenAI’s GPT and Meta’s LLaMA are powerful, but also extremely expensive to train. So the BLIP team framed the challenge by asking themselves: can we take off-the-shelf, pretrained frozen image encoders and frozen LLMs and use them for vision language pretraining while still preserving their <span class="No-Break">learned representations?</span></p>
			<p>The answer is yes. BLIP-2 solved this by introducing a Query Transformer that helps generate the visual representations corresponding to a text caption, which then is fed to a frozen LLM to decode <span class="No-Break">text descriptions.</span></p>
			<p>The Query Transformer, often referred to as the Q-Former [2], is a crucial component of the BLIP-2 model. It serves as a bridge connecting the frozen image encoder and the frozen LLM. The primary function of the Q-Former is to map a set of “query tokens” to query embeddings. These query embeddings help in extracting visual features from the image encoder that are most pertinent to the given <span class="No-Break">text instruction.</span></p>
			<p>During the training process of the BLIP-2 model, the weights of the image encoder and the LLM remain frozen. Meanwhile, the Q-Former undergoes training, allowing it to adapt and optimize its performance based on the specific task requirements. By employing a set of learnable query vectors, the Q-Former effectively distills valuable information from the image encoder, making it possible for the LLM to generate accurate and contextually appropriate responses grounded in <span class="No-Break">visual content.</span></p>
			<p>A similar concept is also employed in LLaVA, which we will discuss later. The core idea of BLIP is to reuse the effective vision and language components, and only train a middle model to <a id="_idIndexMarker454"/>bridge <span class="No-Break">them together.</span></p>
			<p>Next, let’s start <span class="No-Break">using BLIP<a id="_idTextAnchor295"/>-2.</span></p>
			<h2 id="_idParaDest-181"><a id="_idTextAnchor296"/>Using BLIP-2 to generate descriptions</h2>
			<p>Using BLIP-2 is<a id="_idIndexMarker455"/> easy and clean with the help of the Hugging Face transformers [5] package. If you don’t have the package installed, simply run the following command to install or update it to the <span class="No-Break">newest version:</span></p>
			<pre class="source-code">
pip install -U transformer</pre>
			<p>Then load up the BLIP-2 model data with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
from transformers import AutoProcessor, Blip2ForConditionalGeneration
import torch
processor = AutoProcessor.from_pretrained("Salesforce/blip2-opt-2.7b")
# by default `from_pretrained` loads the weights in float32
# we load in float16 instead to save memory
device = "cuda" if torch.cuda.is_available() else "cpu"
model = Blip2ForConditionalGeneration.from_pretrained(
    "Salesforce/blip2-opt-2.7b",
    torch_dtype=torch.float16
).to(device)</pre>
			<p>Your first run will automatically download the model weights data from the Hugging Face model repository. It may take some time, so please be patient. Once the downloading is finished, run the following code to ask BLIP-2 about the image <span class="No-Break">we provide:</span></p>
			<pre class="source-code">
prompt = "describe the content of the image:"
inputs = processor(
    input_image,
    text=prompt,
    return_tensors="pt"
).to(device, torch.float16)
generated_ids = model.generate(**inputs, max_new_tokens=768)
generated_text = processor.batch_decode(
    generated_ids,
    skip_special_tokens=True
)[0].strip()
print(generated_text)</pre>
			<p>The code returns the description <strong class="source-inline">astronaut on horseback in space</strong>, which is good and accurate. What if we ask how many planets are in the background? Let’s change the prompt to <strong class="source-inline">how many planets in the background:</strong>. And it returns <strong class="source-inline">the universe is bigger than you think</strong>. Not good enough <span class="No-Break">this time.</span></p>
			<p>So, BLIP-2 is<a id="_idIndexMarker456"/> good at generating short descriptions of an entire image quickly. However, to generate more detailed descriptions or even interact with images, we can leverage the power <span class="No-Break">of L<a id="_idTextAnchor297"/>LaVA.</span></p>
			<h1 id="_idParaDest-182"><a id="_idTextAnchor298"/>LLaVA – Large Language and Vision Assistant</h1>
			<p>As<a id="_idIndexMarker457"/> suggested by its name <strong class="bold">LLaVA</strong> [3], this model is very close to LLaMA, not only in name but also in terms of their internals. LLaVA uses LLaMA as its language part. making it possible to swap out the language model if needed This is definitely a killer feature for many scenarios. One of the key features of Stable Diffusion is its openness for model swapping and fine-tuning. Similar to Stable Diffusion, LLaVA is designed to leverage open-sourced <span class="No-Break">LLM models.</span></p>
			<p>Next, let’s take a look at how <span class="No-Break">LLaVA <a id="_idTextAnchor299"/>works.</span></p>
			<h2 id="_idParaDest-183"><a id="_idTextAnchor300"/>How LLaVA works</h2>
			<p>The LLaVA <a id="_idIndexMarker458"/>authors, Haotian Liu et al. [3], present a beautiful, accurate diagram showing how the model leverages pretrained CLIP and LLaMA models in its architecture, as shown in the <span class="No-Break">following figure:</span></p>
			<div>
				<div id="_idContainer099" class="IMG---Figure">
					<img src="image/B21263_15_02.jpg" alt="Figure 15.2: Architecture of LLaVA"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 15.2: Architecture of LLaVA</p>
			<p>Let’s read the diagram from the bottom up. During the inference, we provide an image denoted as <span class="_-----MathTools-_Math_Variable">X</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">v</span>, and a language instruction denoted as <span class="_-----MathTools-_Math_Variable">X</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">q</span>. The vision encoder is the CLIP vision encoder ViT-L/14 [6]. The same CLIP is used by Stable Diffusion v1.5 as its <span class="No-Break">text encoder.</span></p>
			<p>The CLIP model encodes the image to <span class="_-----MathTools-_Math_Variable">Z</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">v</span>, and the projection <span class="_-----MathTools-_Math_Variable">W</span> is the model data provided by LLaVA. The projection model projects the encoded image embedding <span class="_-----MathTools-_Math_Variable">Z</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">v</span> to <span class="_-----MathTools-_Math_Variable">H</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">v</span> <span class="No-Break">as follows:</span></p>
			<p><span class="_-----MathTools-_Math_Variable">H</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">v</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">W</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended">⋅</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">Z</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">v</span></span></p>
			<p>On the other side, the language instruction is encoded into CLIP’s 512-dimensional embedding chunks. Both the image and language embeddings share the <span class="No-Break">same dimensionality.</span></p>
			<p>In this manner, the language model <span class="_-----MathTools-_Math_Variable">f</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">ϕ</span> is aware of both the image and the language! This method bears some similarity to Stable Diffusion’s textual inversion technique, being lightweight <span class="No-Break">yet powerful.</span></p>
			<p>Next, let’s write some code to instruct LLaVA to interact with <span class="No-Break">an <a id="_idTextAnchor301"/>image.</span></p>
			<h2 id="_idParaDest-184"><a id="_idTextAnchor302"/>Installing LLaVA</h2>
			<p>For an optimal <a id="_idIndexMarker459"/>experience, it is strongly advised to use LLaVA on a Linux machine. Using it on Windows may result in unexpected missing components. It is also suggested to establish a Python virtual environment for using LLaVA. Detailed steps and commands for setting up a Python virtual environment were provided in <a href="B21263_02.xhtml#_idTextAnchor037"><span class="No-Break"><em class="italic">Chapter 2</em></span></a><span class="No-Break">.</span></p>
			<p>Clone the LLaVA repository to your <span class="No-Break">local folder:</span></p>
			<pre class="console">
git clone https://github.com/haotian-liu/LLaVA.git
cd LLaVA</pre>
			<p>Then install LLaVA by simply running the <span class="No-Break">following command:</span></p>
			<pre class="console">
pip install -U .</pre>
			<p>Next, download the model file from the Hugging Face <span class="No-Break">model repository:</span></p>
			<pre class="console">
# Make sure you have git-lfs installed (https://git-lfs.com)
git lfs install
git clone https://huggingface.co/liuhaotian/llava-v1.5-7b</pre>
			<p>Please note that the model file is large and the download will take some time. At the time of writing this chapter, you can also download the 13B model by simply changing the <strong class="source-inline">7</strong> to <strong class="source-inline">13</strong> in the URL in the preceding code snippet to use the 13B <span class="No-Break">LLaVA model.</span></p>
			<p>That’s it for the setup. Now, let’s proceed to writing the <span class="No-Break">Pytho<a id="_idTextAnchor303"/>n code.</span></p>
			<h2 id="_idParaDest-185"><a id="_idTextAnchor304"/>Using LLaVA to generate image descriptions</h2>
			<p>Since we just <a id="_idIndexMarker460"/>installed LLaVA, we can reference the following <span class="No-Break">related modules:</span></p>
			<pre class="source-code">
from llava.constants import (
    IMAGE_TOKEN_INDEX,
    DEFAULT_IMAGE_TOKEN,
    DEFAULT_IM_START_TOKEN,
    DEFAULT_IM_END_TOKEN
)
from llava.conversation import (
    conv_templates, SeparatorStyle
)
from llava.model.builder import load_pretrained_model
from llava.mm_utils import (
    process_images,
    tokenizer_image_token,
    get_model_name_from_path,
    KeywordsStoppingCriteria
)</pre>
			<p>Load the <strong class="source-inline">tokenizer</strong>, <strong class="source-inline">image_processor</strong>, and <strong class="source-inline">model</strong> components. <strong class="source-inline">tokenizer</strong> will convert text to token IDs, <strong class="source-inline">image_processor</strong> will convert images to tensors, and <strong class="source-inline">model</strong> is the pipeline <a id="_idIndexMarker461"/>that we will use to generate <span class="No-Break">the output:</span></p>
			<pre class="source-code">
# load up tokenizer, model, image_processor
model_path = "/path/to/llava-v1.5-7b"
model_name = get_model_name_from_path(model_path)
conv_mode = "llava_v1"
tokenizer, model, image_processor, _ = load_pretrained_model(
    model_path = model_path,
    model_base = None,
    model_name = model_name,
    load_4bit = True,
    device = "cuda",
    device_map = {'':torch.cuda.current_device()}
)</pre>
			<p>The following is a breakdown of the <span class="No-Break">preceding code:</span></p>
			<ul>
				<li><strong class="source-inline">model_path</strong>: This path points to the folder storing the <span class="No-Break">pretrained models.</span></li>
				<li><strong class="source-inline">model_base</strong>: This is set to <strong class="source-inline">None</strong>, meaning no specific parent architecture has <span class="No-Break">been specified.</span></li>
				<li><strong class="source-inline">model_name</strong>: The name of the pretrained <span class="No-Break">model (</span><span class="No-Break"><strong class="source-inline">llava-v1.5</strong></span><span class="No-Break">).</span></li>
				<li><strong class="source-inline">load_4bit</strong>: If set to <strong class="source-inline">True</strong>, this enables 4-bit quantization during inferencing. This reduces memory usage and boosts speed, but might negatively impact the quality of <span class="No-Break">results slightly.</span></li>
				<li><strong class="source-inline">device</strong>: This specifies CUDA as the device where computations <span class="No-Break">should occur.</span></li>
				<li><strong class="source-inline">device_map</strong>: This is used to map GPU devices to different parts of the model if you want to distribute the workload across multiple GPUs. Since only one device is mapped here, it implies a single <span class="No-Break">GPU execution.</span></li>
			</ul>
			<p>Now, let’s create the<a id="_idIndexMarker462"/> <span class="No-Break">image descriptions:</span></p>
			<ol>
				<li>Create a <strong class="source-inline">conv</strong> object to hold the <span class="No-Break">conversation history:</span><pre class="source-code">
# start a new conversation</pre><pre class="source-code">
user_input = """Analyze the image in a comprehensive and detailed manner"""</pre><pre class="source-code">
conv = conv_templates[conv_mode].copy()</pre></li>
				<li>Convert the image to <span class="No-Break">a tensor:</span><pre class="source-code">
# process image to tensor</pre><pre class="source-code">
image_tensor = process_images(</pre><pre class="source-code">
    [input_image],</pre><pre class="source-code">
    image_processor,</pre><pre class="source-code">
    {"image_aspect_ratio":"pad"}</pre><pre class="source-code">
).to(model.device, dtype=torch.float16)</pre></li>
				<li>Append an image placeholder to <span class="No-Break">the conversation:</span><pre class="source-code">
if model.config.mm_use_im_start_end:</pre><pre class="source-code">
    inp = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + \</pre><pre class="source-code">
        DEFAULT_IM_END_TOKEN + '\n' + user_input</pre><pre class="source-code">
else:</pre><pre class="source-code">
    inp = DEFAULT_IMAGE_TOKEN + '\n' + user_input</pre><pre class="source-code">
conv.append_message(conv.roles[0], inp)</pre></li>
				<li>Get the <a id="_idIndexMarker463"/>prompt and convert it to tokens <span class="No-Break">for inference:</span><pre class="source-code">
# get the prompt for inference</pre><pre class="source-code">
conv.append_message(conv.roles[1], None)</pre><pre class="source-code">
prompt = conv.get_prompt()</pre><pre class="source-code">
# convert prompt to token ids</pre><pre class="source-code">
input_ids = tokenizer_image_token(</pre><pre class="source-code">
    prompt,</pre><pre class="source-code">
    tokenizer,</pre><pre class="source-code">
    IMAGE_TOKEN_INDEX,</pre><pre class="source-code">
    return_tensors='pt'</pre><pre class="source-code">
).unsqueeze(0).cuda()</pre></li>
				<li>Prepare the <span class="No-Break">stopping criteria:</span><pre class="source-code">
stop_str = conv.sep if conv.sep_style != \</pre><pre class="source-code">
    SeparatorStyle.TWO else conv.sep2</pre><pre class="source-code">
keywords = [stop_str]</pre><pre class="source-code">
stopping_criteria = KeywordsStoppingCriteria(keywords,</pre><pre class="source-code">
    tokenizer, input_ids)</pre></li>
				<li>Finally, get the<a id="_idIndexMarker464"/> output <span class="No-Break">from LLaVA:</span><pre class="source-code">
# output the data</pre><pre class="source-code">
with torch.inference_mode():</pre><pre class="source-code">
    output_ids = model.generate(</pre><pre class="source-code">
        input_ids,</pre><pre class="source-code">
        images =image_tensor,</pre><pre class="source-code">
        do_sample = True,</pre><pre class="source-code">
        temperature = 0.2,</pre><pre class="source-code">
        max_new_tokens = 1024,</pre><pre class="source-code">
        streamer = None,</pre><pre class="source-code">
        use_cache = True,</pre><pre class="source-code">
        stopping_criteria = [stopping_criteria]</pre><pre class="source-code">
    )</pre><pre class="source-code">
outputs = tokenizer.decode(output_ids[0,</pre><pre class="source-code">
    input_ids.shape[1]:]).strip()</pre><pre class="source-code">
# make sure the conv object holds all the output</pre><pre class="source-code">
conv.messages[-1][-1] = outputs</pre><pre class="source-code">
print(outputs)</pre><p class="list-inset">As we can see in the following output, LLaVA can generate <span class="No-Break">amazing descriptions:</span></p><pre class="source-code">
<strong class="bold">The image features a man dressed in a white space suit, riding a horse in a desert-like environment. The man appears to be a space traveler, possibly on a mission or exploring the area. The horse is galloping, and the man is skillfully riding it.</strong></pre><pre class="source-code">
<strong class="bold">In the background, there are two moons visible, adding to the sense of a space-themed setting. The combination of the man in a space suit, the horse, and the moons creates a captivating and imaginative scene.&lt;/s&gt;</strong></pre></li>
			</ol>
			<p>I have attempted to condense the code as much as possible, but it remains lengthy. It requires careful copying or transcription into your code editor. I recommend copy-pasting the code provided in the repository that accompanies this book. You can execute the aforementioned code in a single cell to observe how effectively LLaVA generates descriptions fr<a id="_idTextAnchor305"/>om <span class="No-Break">an image.</span></p>
			<h1 id="_idParaDest-186"><a id="_idTextAnchor306"/>Summary</h1>
			<p>In this chapter, our primary focus was on two AI solutions designed to generate image descriptions. The first is BLIP-2, an effective and efficient solution for generating concise captions for images. The second is the LLaVA solution, which is capable of generating more detailed and accurate descriptive information from <span class="No-Break">an image.</span></p>
			<p>With the assistance of LLaVA, we can even interact with an image to extract further information <span class="No-Break">from it.</span></p>
			<p>The integration of vision and language capabilities also lays the groundwork for the development of even more powerful multimodal models, the potential of which we can only begin <span class="No-Break">to imagine.</span></p>
			<p>In the next chapter, let’s get started using Stable <span class="No-Break">D<a id="_idTextAnchor307"/>iffusion XL.</span></p>
			<h1 id="_idParaDest-187"><a id="_idTextAnchor308"/>References</h1>
			<ol>
				<li>Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, <em class="italic">BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language </em><span class="No-Break"><em class="italic">Models</em></span><span class="No-Break">: </span><a href="https://arxiv.org/abs/2301.12597"><span class="No-Break">https://arxiv.org/abs/2301.12597</span></a></li>
				<li>BLIP-2 Hugging Face <span class="No-Break">documentation: </span><a href="https://huggingface.co/docs/transformers/main/model_doc/blip-2"><span class="No-Break">https://huggingface.co/docs/transformers/main/model_doc/blip-2</span></a></li>
				<li>Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, <em class="italic">LLaVA: Large Language and Vision </em><span class="No-Break"><em class="italic">Assistant</em></span><span class="No-Break">: </span><a href="https://llava-vl.github.io/"><span class="No-Break">https://llava-vl.github.io/</span></a></li>
				<li>Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi, <em class="italic">BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and </em><span class="No-Break"><em class="italic">Generation</em></span><span class="No-Break">: </span><a href="https://arxiv.org/abs/2201.12086"><span class="No-Break">https://arxiv.org/abs/2201.12086</span></a></li>
				<li>Hugging Face Transformers GitHub <span class="No-Break">repository: </span><a href="https://github.com/huggingface/transformers"><span class="No-Break">https://github.com/huggingface/transformers</span></a></li>
				<li>Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, <em class="italic">Learning transferable visual models from natural language </em><span class="No-Break"><em class="italic">supervision</em></span><span class="No-Break">: </span><a href="https://arxiv.org/abs/2103.00020"><span class="No-Break">https://arxiv.org/abs/2103.00020</span></a></li>
				<li>LLaVA GitHub <span class="No-Break">repository: </span><a href="https://github.com/haotian-liu/LLaVA"><span class="No-Break">https://github.com/haotian-liu/LLaVA</span></a></li>
			</ol>
		</div>
	</body></html>