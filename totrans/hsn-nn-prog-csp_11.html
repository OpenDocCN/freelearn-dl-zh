<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Training Autoencoders Using RNNSharp</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will be discussing <strong>autoencoders</strong> and their usage. We will talk about what an autoencoder is, the different types of autoencoder, and present different samples to help you better understand how to use this technology in your applications. By the end of this chapter, you will know how to design your own autoencoder, load and save it from disk, and train and test it.<br/></p>
<p>The following topics are covered in this chapter:</p>
<ul>
<li>What is an autoencoder?</li>
<li>Different types of autoencoder</li>
<li>Creating your own autoencoder</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>You will require Microsoft Visual Studio.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What is an autoencoder?</h1>
                </header>
            
            <article>
                
<p>An autoencoder is an unsupervised learning algorithm that applies back propagation and sets target values equal to the inputs. An autoencoder learns to compress data from the input layer into shorter code, and then it uncompresses that code into something that closely matches the original data. This is better known as <strong>dimensionality reduction</strong>.</p>
<p>The following is a depiction of an autoencoder. The original images are encoded, and then decoded to reconstruct the original:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-995 image-border" src="assets/b65f99cf-4805-402c-9e69-518888711abb.png" style="width:37.42em;height:21.75em;"/></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Different types of autoencoder</h1>
                </header>
            
            <article>
                
<p>The following are different types of autoencoder:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-964 image-border" src="assets/4d624ffb-80b9-4c7a-a52b-138d50241b5f.png" style="width:38.50em;height:21.58em;"/></p>
<p>Let's briefly discuss autoencoders and the variants we have just seen. Please note that there are other variants out there; these are just probably the most common that I thought you should at least be familiar with.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Standard autoencoder</h1>
                </header>
            
            <article>
                
<p>An autoencoder learns to compress data from the input layer into smaller code, and then uncompress that code into something that (hopefully) matches the original data. The basic idea behind a standard autoencoder is to encode information automatically, hence the name. The entire network always resembles an hourglass, in terms of its shape, with fewer hidden layers than input and output layers. Everything up to the middle layer is called the encoding part, everything after the middle layer is called the decoding part, and the middle layer itself is called, as you have probably guessed, the code. You can train autoencoders by feeding input data and setting the error status as the difference between the input and what came out. Autoencoders can be built so that encoding weights are the same as decoding weights.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Variational autoencoders</h1>
                </header>
            
            <article>
                
<p><strong>Variational autoencoders</strong> have the same architecture as autoencoders but are taught something else: an approximated probability distribution of the input samples. This is because they are a bit more closely related to Boltzmann and Restricted Boltzmann Machines. They do however rely on Bayesian mathematics as well as a re-parametrization trick to achieve this different representation. The basics come down to this: taking influence into account. If one thing happens in one place, and something else happens somewhere else, they are not necessarily related. If they are not related, then error propagation should consider that. This is a useful approach, because neural networks are large graphs (in a way), so it helps to be able to rule out the influence some nodes have on other nodes as you dive into deeper layers.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">De-noising autoencoders</h1>
                </header>
            
            <article>
                
<p><strong>De-noising autoencoders</strong> reconstruct the input from a corrupted version of themselves. This does two things. First, it tries to encode the input while preserving as much information as possible. Second, it tries to undo the effect of the corruption process. The input is reconstructed after a percentage of the data has been randomly removed, which forces the network to learn robust features that tend to generalize better. De-noising autoencoders are autoencoders where we feed the input data with noise (such as making an image grainier). We compute the error the same way, so the output of the network is compared to the original input without noise. This encourages the network to learn not details but broader features, which are usually more accurate, as they are not affected by constantly changing noise.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sparse autoencoders</h1>
                </header>
            
            <article>
                
<p><strong>Sparse autoencoders</strong> are, in a way, the opposite of autoencoders. Instead of teaching a network to represent information in less space or fewer nodes, we try to encode information in more space. Instead of the network converging in the middle and then expanding back to the input size, we blow up the middle. These types of network can be used to extract many small features from a dataset. If you were to train a sparse autoencoder the same way as an autoencoder, you would in almost all cases end up with a pretty useless identity network (as in, what comes in is what comes out, without any transformation or decomposition). To prevent this, we feed back the input, plus what is known as a <strong>sparsity driver</strong>. This can take the form of a threshold filter, where only a certain error is passed back and trained. The other error will be irrelevant for that pass and will be set to zero. In a way, this resembles spiking neural networks, where not all neurons fire all the time.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating your own autoencoder</h1>
                </header>
            
            <article>
                
<p>Now that you are an expert on autoencoders, let's move on to less theory and more practice. Let's take a bit of a different route on this one. Instead of using an open-source package and showing you how to use it, let's write our own autoencoder framework that you can enhance to make your own. We'll discuss and implement the basic pieces needed, and then write some sample code showing how to use it. We will make this chapter unique in that we won't finish the usage sample; we'll do just enough to get you started along your own path to autoencoder creation. With that in mind, let's begin.</p>
<p>Let's start off by thinking about what an autoencoder is and what things we would want to include. First off, we're going to need to keep track of the number of layers that we have. These layers will be Restricted Boltzmann Machines for sure. Just so you know, we'll also refer to <strong>Restricted Boltzmann Machines</strong> as <strong>RBMs</strong> from time to time, where brevity is required.</p>
<p>So, we know we'll need to track the number of layers that our autoencoder has. We're also going to need to monitor the weights we'll need to use: learning rate, recognition weights, and generative weights. Training data is important, of course, as are errors. I think for now that should be it. Let's block out a class to do just this.</p>
<p>Let's start with an <kbd>interface</kbd>, which we will use to calculate errors. We will only need one method, which will calculate the error for us. The RBM will be responsible for doing this, but we'll get to that later:</p>
<pre>public interface IErrorObserver<br/>{<br/>void CalculateError(double PError);<br/>}</pre>
<p>Before we define our RBM class, we'll need to look at the layers that it will use. To best represent this, we'll create an <kbd>abstract</kbd> class. We'll need to track the state of the layer, the bias used, the amount of bias change, the activity itself, and how many neurons it will have. Rather than distinguish between mirror and canonical neurons, we'll simply represent all neuron types as one single object. We also will need to have multiple types of RBM layer. Gaussian and binary are two that come to mind, so the following will be the base class for those layers:</p>
<pre>public abstract class RestrictedBoltzmannMachineLayer<br/>{<br/>protected double[] state;<br/>protected double[] bias;<br/>protected double[] biasChange;<br/>protected double[] activity;<br/>protected int numNeurons = 0;<br/>}</pre>
<p>We must keep in mind that our RBM will need to track its weights. Since weights are applied through layers with a thing called a <strong>synapse</strong>, we'll create a class to represent all we want to do with weights. Since we'll need to track the weights, their changes, and the pre- and post-size, let's just create a class that encapsulates all of that:</p>
<pre>public class RestrictedBoltzmannMachineWeightSet<br/>{<br/>private int preSize;<br/>private int postSize;<br/>private double[][] weights;<br/>private double[][] weightChanges;<br/>}</pre>
<p>Next, as our learning rate encompasses features such as weights, biases, and momentum, we will be best served if we create a separate class to represent all of this:</p>
<pre>public struct RestrictedBoltzmannMachineLearningRate<br/>{<br/>internal double weights;<br/>internal double biases;<br/>internal double momentumWeights;<br/>internal double momentumBiases;<br/>}</pre>
<p>Finally, let's create a class that encompasses our training data:</p>
<pre>public struct TrainingData<br/>{<br/>public double[] posVisible;<br/>public double[] posHidden;<br/>public double[] negVisible;<br/>public double[] negHidden;<br/>}</pre>
<p>With all of this now defined, let's go ahead and work on our <kbd>RestrictedBoltzmannMachine</kbd> class. For this class, we'll need to keep track of how many visible and hidden layers we have, the weights and learning rate we will use, and our training data:</p>
<pre>public class RestrictedBoltzmannMachine<br/>{<br/>private RestrictedBoltzmannMachineLayer visibleLayers;<br/>private RestrictedBoltzmannMachineLayer hiddenLayers;<br/>private RestrictedBoltzmannMachineWeightSet weights;<br/>private RestrictedBoltzmannMachineLearningRate learnrate;<br/>private TrainingData trainingdata;<br/>private int numVisibleLayers;<br/>private int numHiddenLayers;<br/>}</pre>
<p>And, finally, with everything else in place, let's create our <kbd>Autoencoder</kbd> class:</p>
<pre>public class Autoencoder<br/>{<br/>private int numlayers;<br/>private bool pretraining = true;<br/>private RestrictedBoltzmannMachineLayer[] layers;<br/>private AutoencoderLearningRate learnrate;<br/>private AutoencoderWeights recognitionweights;<br/>private AutoencoderWeights generativeweights;<br/>private TrainingData[] trainingdata;<br/>private List&lt;IErrorObserver&gt; errorobservers;<br/>}</pre>
<p>Even though we know there will be a lot more required for some of these classes, this is the basic framework that we need to get started framing in the rest of the code. To do that, we should think about some things.</p>
<p>Since weights are a prominent aspect of our autoencoder, we are going to have to use and initialize weights quite often. But how should we initialize our weights, and with what values? We will provide two distinct choices. We will either initialize all weights to zero, or use a Gaussian. We will also have to initialize the biases as well. Let's go ahead and create an interface from which to do this so it will make it easier later to select the type of initialization we want (zero or Gaussian):</p>
<pre>public interface IWeightInitializer<br/>{<br/>double InitializeWeight();<br/>double InitializeBias();}</pre>
<p>We mentioned earlier that we needed to have multiple types of RBM layer to use. Gaussian and binary were two that came to mind. We have already created the interface for that, so let's go ahead and put our base classes into the form, as we will need them shortly. To do this, we will need to expand our RBM layer class and add two abstract methods so that they can be cloned, and so that we can set the state of the layer:</p>
<pre>public abstract void SetState(int PWhich, double PInput);<br/>public abstract object Clone();</pre>
<p>Our <kbd>RestrictedBoltzmannMachineLayer</kbd> class now looks like this:</p>
<pre>public abstract class RestrictedBoltzmannMachineLayer<br/>{<br/>protected double[] state;<br/>protected double[] bias;<br/>protected double[] biasChange;<br/>protected double[] activity;<br/>protected int numNeurons = 0;<br/>public abstract void SetState(int PWhich, double PInput);<br/>public abstract object Clone();<br/>}</pre>
<p>With our very basic autoencoder in place, we should now turn our attention to how we will build our autoencoder. Let's try to keep things as modular as possible and, with that in mind, let's create an <kbd>AutoEncoderBuilder</kbd> class that we can have encapsulate things such as weight initialization, adding layers, and so forth. It will look something like the following:</p>
<pre>public class AutoencoderBuilder<br/>{<br/>private List&lt;RestrictedBoltzmannMachineLayer&gt; layers = new List&lt;RestrictedBoltzmannMachineLayer&gt;();<br/>private AutoencoderLearningRate learnrate = new AutoencoderLearningRate();<br/>private IWeightInitializer weightinitializer = new GaussianWeightInitializer();<br/>}</pre>
<p>Now that we have this class blocked in, let's begin to add some meat to it in the form of functions. We know that when we build an autoencoder we are going to need to add layers. We can do that with this function. We will pass it the layer, and then update our internal learning-rate layer:</p>
<pre>private void AddLayer(RestrictedBoltzmannMachineLayer PLayer)<br/>{<br/>learnrate.preLearningRateBiases.Add(0.001);<br/>learnrate.preMomentumBiases.Add(0.5);<br/>learnrate.fineLearningRateBiases.Add(0.001);<br/>if (layers.Count &gt;= 1)<br/>{<br/>learnrate.preLearningRateWeights.Add(0.001);<br/>learnrate.preMomentumWeights.Add(0.5);<br/>learnrate.fineLearningRateWeights.Add(0.001);<br/>}<br/>layers.Add(PLayer);<br/>}</pre>
<p>Once we have this base function, we can then add some higher-level functions, which will make it easier for us to add layers to our autoencoder:</p>
<pre>public void AddBinaryLayer (int size)<br/>{<br/>AddLayer (new RestrictedBoltzmannMachineBinaryLayer(size));<br/>}<br/>public void AddGaussianLayer (int size)<br/>{<br/>AddLayer (new RestrictedBoltzmannMachineGaussianLayer(size));<br/>}</pre>
<p>Finally, let's add a <kbd>Build()</kbd> method to our autoencoder builder to make it easy to build:</p>
<pre>public Autoencoder Build()<br/>{<br/>return new Autoencoder(layers, learnrate, weightinitializer);<br/>}</pre>
<p>Now let's turn our attention to our autoencoder itself. We are going to need a function to help us initialize our biases:</p>
<pre>private void InitializeBiases(IWeightInitializer PWInitializer)<br/>{<br/>for (int <strong>i</strong> = 0; <strong>i</strong> &lt; numlayers; <strong>i</strong>++)<br/>{<br/>for (int <strong>j</strong> = 0; <strong>j</strong> &lt; layers[<strong>i</strong>].Count; <strong>j</strong>++)<br/>{<br/>layers[<strong>i</strong>].SetBias(<strong>j</strong>, PWInitializer.InitializeBias());<br/>}<br/>}<br/>}</pre>
<p>Next, we are going to need to initialize our training data. This will basically involve creating all the arrays that we need and setting their initial values to zero as follows:</p>
<pre>private void InitializeTrainingData()<br/>{<br/>trainingdata = new TrainingData[numlayers - 1];<br/>for (int<strong>i</strong> = 0; <strong>i</strong> &lt; numlayers - 1; <strong>i</strong>++)<br/>{<br/>trainingdata[<strong>i</strong>].posVisible = new double[layers[<strong>i</strong>].Count];<br/>Utility.SetArrayToZero(trainingdata[<strong>i</strong>].posVisible);<br/>trainingdata[<strong>i</strong>].posHidden = new double[layers[<strong>i</strong> + 1].Count];<br/>Utility.SetArrayToZero(trainingdata[<strong>i</strong>].posHidden);<br/>trainingdata[<strong>i</strong>].negVisible = new double[layers[<strong>i</strong>].Count];<br/>Utility.SetArrayToZero(trainingdata[<strong>i</strong>].negVisible);<br/>trainingdata[<strong>i</strong>].negHidden = new double[layers[<strong>i</strong> + 1].Count];<br/>Utility.SetArrayToZero(trainingdata[<strong>i</strong>].negHidden);<br/>}<br/>}</pre>
<p>With that behind us, we're off to a good start. Let's start to use the software and see what we're missing. Let's create our <kbd>builder</kbd> object, add some binary and Gaussian layers, and see how it looks:</p>
<pre>AutoencoderBuilder builder = new AutoencoderBuilder();<br/>builder.AddBinaryLayer(4);<br/>builder.AddBinaryLayer(3);</pre>
<pre>builder.AddGaussianLayer(3);<br/>builder.AddGaussianLayer(1);</pre>
<p>Not bad, right? So, what's next? Well, we've got our autoencoder created and have added layers. We now lack functions to allow us to fine tune and train learning rates and momentum. Let's see how they would look if we were to add them here as follows:</p>
<pre>builder.SetFineTuningLearningRateBiases(0, 1.0);<br/>builder.SetFineTuningLearningRateWeights(0, 1.0);<br/>builder.SetPreTrainingLearningRateBiases(0, 1.0);<br/>builder.SetPreTrainingLearningRateWeights(0, 1.0);<br/>builder.SetPreTrainingMomentumBiases(0, 0.1);<br/>builder.SetPreTrainingMomentumWeights(0, .05);</pre>
<p>That looks about right. At this point, we should add these functions into our <kbd>autoencoderbuilder</kbd> object so we can use them. Let's see how that would look. Remember that with our builder object we automatically created our learning rate object, so now we just have to use it to populate things such as our weights and biases, along with the momentum weights and biases:</p>
<pre>public void SetPreTrainingLearningRateWeights(int PWhich, double PLR)<br/>{<br/>learnrate.preLearningRateWeights[PWhich] = PLR;<br/>}<br/>public void SetPreTrainingLearningRateBiases(int PWhich, double PLR)<br/>{<br/>learnrate.preLearningRateBiases[PWhich] = PLR;<br/>}<br/>public void SetPreTrainingMomentumWeights(int PWhich, double PMom)<br/>{<br/>learnrate.preMomentumWeights[PWhich] = PMom;<br/>}<br/>public void SetPreTrainingMomentumBiases(int PWhich, double PMom)<br/>{<br/>learnrate.preMomentumBiases[PWhich] = PMom;<br/>}<br/>public void SetFineTuningLearningRateWeights(int PWhich, double PLR)<br/>{<br/>learnrate.fineLearningRateWeights[PWhich] = PLR;<br/>}<br/>public void SetFineTuningLearningRateBiases(int PWhich, double PLR)<br/>{<br/>learnrate.fineLearningRateBiases[PWhich] = PLR;<br/>}</pre>
<p>Well, let's now stop and take a look at what our sample program is turning out to look like:</p>
<pre>AutoencoderBuilder builder = new AutoencoderBuilder();<br/>builder.AddBinaryLayer(4);<br/>builder.AddBinaryLayer(3);<br/>builder.AddGaussianLayer(3);<br/>builder.AddGaussianLayer(1);<br/>builder.SetFineTuningLearningRateBiases(0, 1.0);<br/>builder.SetFineTuningLearningRateWeights(0, 1.0);<br/>builder.SetPreTrainingLearningRateBiases(0, 1.0);<br/>builder.SetPreTrainingLearningRateWeights(0, 1.0);<br/>builder.SetPreTrainingMomentumBiases(0, 0.1);<br/>builder.SetPreTrainingMomentumWeights(0, .05);</pre>
<p>Not bad. All we should need to do now is to call our <kbd>Build()</kbd> method on our <kbd>builder</kbd> and we should have the first version of our framework:</p>
<pre>Autoencoder encoder = builder.Build();</pre>
<p>With all this now complete,and looking back at the preceding code, I think at some point we are going to need to be able to gain access to our individual layers; what do you think? Just in case, we'd better provide a function to do that. Let's see how that would look:</p>
<pre>RestrictedBoltzmannMachineLayer layer = encoder.GetLayer(0);<br/>RestrictedBoltzmannMachineLayer layerHidden = encoder.GetLayer(1);</pre>
<p>Since our internal layers are <kbd>RestrictedBoltzmannMachine</kbd> layers, that is the type that we should be returning, as you can see from the previous code. The <kbd>GetLayer()</kbd> function needs to reside inside our autoencoder object, though, not the builder. So, let's go ahead and add it now. We'll need to be good developers and make sure that we have a bounds check to ensure that we are passing a valid layer index before we try to use it. We'll store all those neat little utility functions in a class of their own, and we might as well call it <kbd>Utility</kbd>, since the name makes sense. I won't go into how we can code that function, as I am fairly confident that every reader already knows how to do bounds checks, so you can either make up your own or look at the accompanying source code to see how it's done in this instance:</p>
<pre>public RestrictedBoltzmannMachineLayer GetLayer(int PWhichLayer)<br/>{<br/>Utility.WithinBounds("Layer index out of bounds!", PWhichLayer, numlayers);<br/>return layers[PWhichLayer];<br/>}</pre>
<p>OK, so we can now create our autoencoders, set weights and biases, and gain access to individual layers. I think the next thing we need to start thinking about is training and testing. We'll need to take each separately, of course, so why don't we start with training?</p>
<p>We will need to be able to train our RBM, so why don't we create an object dedicated to doing this. We'll call it, no surprise here, <kbd>RestrictedBoltzmannMachineTrainer</kbd>. Again, we are going to need to deal with our <kbd>LearningRate</kbd>, object, and weight sets, so let's make sure we add them as variables right away:</p>
<pre>public static class RestrictedBoltzmannMachineTrainer<br/>{<br/>private static RestrictedBoltzmannMachineLearningRate learnrate;<br/>private static RestrictedBoltzmannMachineWeightSet weightset;<br/>}</pre>
<p>Now, what functions do you think we will need for our trainer? Obviously, a <kbd>Train()</kbd> method is required; otherwise, we named our object incorrectly. We'll also need to train our weights and layer biases:</p>
<pre>private static void TrainWeight(int PWhichVis, int PWhichHid, double PTrainAmount);<br/>private static void TrainBias(RestrictedBoltzmannMachineLayer PLayer, int PWhich, double PPosPhase, double PNegPhase);</pre>
<p>Last, but not least, we should probably have a <kbd>helper</kbd> function that lets us know the training amount, which for us will involve taking the positive visible amount times the positive hidden amount and subtracting that from the negative visible amount times the negative hidden amount:</p>
<pre>private static double CalculateTrainAmount(double PPosVis, double PPosHid, double PNegVis, double PNegHid)<br/>{<br/>return ((PPosVis * PPosHid) - (PNegVis * PNegHid));<br/>}</pre>
<p>OK, let's see where our program stands:</p>
<pre>AutoencoderBuilder builder = new AutoencoderBuilder();<br/>builder.AddBinaryLayer(4);<br/>builder.AddBinaryLayer(3);<br/>builder.AddGaussianLayer(3);<br/>builder.AddGaussianLayer(1);<br/>builder.SetFineTuningLearningRateBiases(0, 1.0);<br/>builder.SetFineTuningLearningRateWeights(0, 1.0);<br/>builder.SetPreTrainingLearningRateBiases(0, 1.0);<br/>builder.SetPreTrainingLearningRateWeights(0, 1.0);<br/>builder.SetPreTrainingMomentumBiases(0, 0.1);<br/>builder.SetPreTrainingMomentumWeights(0, .05);<br/>Autoencoder encoder = builder.Build();<br/>RestrictedBoltzmannMachineLayer layer = encoder.GetLayer(0);<br/>RestrictedBoltzmannMachineLayer layerHidden = encoder.GetLayer(1);</pre>
<p>Nice. Can you see how it's all starting to come together? Now it's time to consider how we are going to add data to our network. Before we do any kind of training on the network, we will need to load data. How will we do this? Let's consider the notion of pre-training. This is the act of loading data into the network manually before we train it. What would this function look like in the context of our program? How about something such as this?</p>
<pre>encoder.PreTrain(0, new double[] {0.1, .05, .03, 0.8});</pre>
<p>We would just need to tell our autoencoder which layer we want to populate with data, and then supply the data. That should work for us. If we did this, then the following is how our program would evolve:</p>
<pre>AutoencoderBuilder builder = new AutoencoderBuilder();<br/>builder.AddBinaryLayer(4);<br/>builder.AddBinaryLayer(3);<br/>builder.AddGaussianLayer(3);<br/>builder.AddGaussianLayer(1);<br/>builder.SetFineTuningLearningRateBiases(0, 1.0);<br/>builder.SetFineTuningLearningRateWeights(0, 1.0);<br/>builder.SetPreTrainingLearningRateBiases(0, 1.0);<br/>builder.SetPreTrainingLearningRateWeights(0, 1.0);<br/>builder.SetPreTrainingMomentumBiases(0, 0.1);<br/>builder.SetPreTrainingMomentumWeights(0, .05);<br/>Autoencoder encoder = builder.Build();<br/>RestrictedBoltzmannMachineLayer layer = encoder.GetLayer(0);<br/>RestrictedBoltzmannMachineLayer layerHidden = encoder.GetLayer(1);<br/>encoder.PreTrain(0, new double[] {0.1, .05, .03, 0.8});<br/>encoder.PreTrain(1, new double[] { 0.1, .05, .03, 0.9 });<br/>encoder.PreTrain(2, new double[] { 0.1, .05, .03, 0.1 });<br/>encoder.PreTrainingComplete();</pre>
<p>What do you think so far? With this code, we would be able to populate three layers with data. I threw in an extra function, <kbd>PreTrainingComplete</kbd>, as a nice way to let our program know that we have finished pre-training. Now, let's figure out how those functions come together.</p>
<p>For pretraining, we will do this in batches. We can have from 1 to <em>n</em> number of batches. In many cases, the number of batches will be just 1. Once we determine the number of batches we want to use, we will iterate through each batch of data.</p>
<p>For each batch of data, we will process the data and determine whether our neurons were activated. We then set the layer state based upon that. We will move both forward and backward through the network, setting our states. Using the following diagram, we will move forward through layers like this <em>Y -&gt; V -&gt; W -&gt; (Z)</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1251 image-border" src="assets/0f4bef37-b92f-4a94-ab1b-1bbb8035a003.png" style="width:22.67em;height:20.75em;"/></p>
<p>Once activations are set, we must perform the actual pre-training. We do this in the pre-synaptic layer, starting at layer 0. When we pre-train, we call our trainer object's <kbd>Train</kbd> method, which we created earlier and then pass the layer(s) and the training data, our recognition weights, and learning rate. To do this, we will need to create our actual function, which we will call <kbd>PerformPreTraining()</kbd>. The following is what this code would look like:</p>
<pre>private void PerformPreTraining(int PPreSynapticLayer)<br/>{<br/>RestrictedBoltzmannMachineLearningRate sentlearnrate = new RestrictedBoltzmannMachineLearningRate(learnrate.preLearningRateWeights[PPreSynapticLayer],learnrate.preLearningRateBiases[PPreSynapticLayer],learnrate.preMomentumWeights[PPreSynapticLayer],learnrate.preMomentumBiases[PPreSynapticLayer]);RestrictedBoltzmannMachineTrainer.Train(layers[PPreSynapticLayer], layers[PPreSynapticLayer + 1],trainingdata[PPreSynapticLayer], sentlearnrate, recognitionweights.GetWeightSet(PPreSynapticLayer));<br/>}</pre>
<p>Once pre-training is complete, we now will need to calculate the error rate based upon the positive and negative visible data properties. That will complete our <kbd>pretraining</kbd> function, and our sample program will now look as follows:</p>
<pre>AutoencoderBuilder builder = new AutoencoderBuilder();<br/>builder.AddBinaryLayer(4);<br/>builder.AddBinaryLayer(3);<br/>builder.AddGaussianLayer(3);<br/>builder.AddGaussianLayer(1);<br/>builder.SetFineTuningLearningRateBiases(0, 1.0);<br/>builder.SetFineTuningLearningRateWeights(0, 1.0);<br/>builder.SetPreTrainingLearningRateBiases(0, 1.0);<br/>builder.SetPreTrainingLearningRateWeights(0, 1.0);<br/>builder.SetPreTrainingMomentumBiases(0, 0.1);<br/>builder.SetPreTrainingMomentumWeights(0, .05);<br/>Autoencoder encoder = builder.Build();<br/>RestrictedBoltzmannMachineLayer layer = encoder.GetLayer(0);<br/>RestrictedBoltzmannMachineLayer layerHidden = encoder.GetLayer(1);<br/>encoder.PreTrain(0, new double[] {0.1, .05, .03, 0.8});<br/>encoder.PreTrain(1, new double[] { 0.1, .05, .03, 0.9 });<br/>encoder.PreTrain(2, new double[] { 0.1, .05, .03, 0.1 });<br/>encoder.PreTrainingComplete();</pre>
<p>With all this code behind us, all we need to do now is to save the autoencoder, and we should be all set. We will do this by creating a <kbd>Save()</kbd> function in the autoencoder, and call it as follows:</p>
<pre>encoder.Save("testencoder.txt");</pre>
<p>To implement this function, let's look at what we need to do. First, we need a filename to use for the autoencoder name. Once we open a <strong>.NET TextWriter</strong> object, we then save the learning rates, the recognition weights, and generative weights. Next, we iterate through all the layers, write out the layer type, and then save the data. If you decide to implement more types of RBM layers than we created, make sure that you in turn update the <kbd>Save()</kbd> and <kbd>Load()</kbd> methods so that your new layer data is saved and re-loaded correctly.</p>
<p>Let's look at our <kbd>Save</kbd> function:</p>
<pre>public void Save(string PFilename)<br/>{<br/>TextWriter file = new StreamWriter(PFilename);<br/>learnrate.Save(file);<br/>recognitionweights.Save(file);<br/>generativeweights.Save(file);<br/>file.WriteLine(numlayers);<br/>for (int<strong>i</strong> = 0; <strong>i</strong> &lt; numlayers; <strong>i</strong>++)<br/>{<br/>if(layers[<strong>i</strong>].GetType() == typeof(RestrictedBoltzmannMachineGaussianLayer))<br/>{<br/>file.WriteLine("RestrictedBoltzmannMachineGaussianLayer");<br/>}<br/>else if (layers[<strong>i</strong>].GetType() == typeof(RestrictedBoltzmannMachineBinaryLayer))<br/>{<br/>file.WriteLine("RestrictedBoltzmannMachineBinaryLayer");<br/>}<br/>layers[<strong>i</strong>].Save(file);<br/>}<br/>file.WriteLine(pretraining);<br/>file.Close();<br/>}</pre>
<p>With our autoencoder saved to disk, we now should really deal with the ability to reload that data into memory and create an autoencoder from it. So, we'll now need a <kbd>Load()</kbd> function. We'll need to basically follow the steps we did to write our autoencoder to disk but, this time, we'll read them in, instead of writing them out. Our weights, learning rate, and layers will have also a <kbd>Load()</kbd> function, just like each of the preceding items had a <kbd>Save()</kbd> function.</p>
<p>Our <kbd>Load()</kbd> function will be a bit different in its declaration. Since we are loading in a saved autoencoder, we have to assume that, at the time this call is made, an autoencoder object has not yet been created. Therefore, we will make this function <kbd>static()</kbd> on the autoencoder object itself, as it will return a newly created autoencoder for us. Here's how our function will look:</p>
<pre>public static Autoencoder Load(string PFilename)<br/>{<br/>TextReader file = new StreamReader(PFilename);<br/>Autoencoder retval = new Autoencoder();<br/>retval.learnrate = new AutoencoderLearningRate();<br/>retval.learnrate.Load(file);<br/>retval.recognitionweights = new AutoencoderWeights();<br/>retval.recognitionweights.Load(file);<br/>retval.generativeweights = new AutoencoderWeights();<br/>retval.generativeweights.Load(file);<br/>retval.numlayers = int.Parse(file.ReadLine());<br/>retval.layers = new RestrictedBoltzmannMachineLayer[retval.numlayers];<br/>for (int<strong>i</strong> = 0; <strong>i</strong> &lt; retval.numlayers; <strong>i</strong>++)<br/>{<br/>string type = file.ReadLine();<br/>if (type == "RestrictedBoltzmannMachineGaussianLayer")<br/>{<br/>retval.layers[<strong>i</strong>] = new RestrictedBoltzmannMachineGaussianLayer();<br/>}<br/>else if (type == "RestrictedBoltzmannMachineBinaryLayer")<br/>{<br/>retval.layers[<strong>i</strong>] = new RestrictedBoltzmannMachineBinaryLayer();<br/>}<br/>retval.layers[<strong>i</strong>].Load(file);<br/>}<br/>retval.pretraining = bool.Parse(file.ReadLine());<br/>retval.InitializeTrainingData();<br/>retval.errorobservers = new List&lt;IErrorObserver&gt;();<br/>file.Close();<br/>return retval;<br/>}</pre>
<p>With that done, let's see how we would call our <kbd>Load()</kbd> function. It should be like the following:</p>
<pre>Autoencoder newAutoencoder = Autoencoder.Load("testencoder.txt");</pre>
<p>So, let's stop here and take a look at all we've accomplished. Let's see what our program can do, as follows:</p>
<pre>AutoencoderBuilder builder = new AutoencoderBuilder();<br/>builder.AddBinaryLayer(4);<br/>builder.AddBinaryLayer(3);<br/>builder.AddGaussianLayer(3);<br/>builder.AddGaussianLayer(1);<br/>builder.SetFineTuningLearningRateBiases(0, 1.0);<br/>builder.SetFineTuningLearningRateWeights(0, 1.0);<br/>builder.SetPreTrainingLearningRateBiases(0, 1.0);<br/>builder.SetPreTrainingLearningRateWeights(0, 1.0);<br/>builder.SetPreTrainingMomentumBiases(0, 0.1);<br/>builder.SetPreTrainingMomentumWeights(0, .05);<br/>Autoencoder encoder = builder.Build();<br/>RestrictedBoltzmannMachineLayer layer = encoder.GetLayer(0);<br/>RestrictedBoltzmannMachineLayer layerHidden = encoder.GetLayer(1);<br/>encoder.PreTrain(0, new double[] {0.1, .05, .03, 0.8});<br/>encoder.PreTrain(1, new double[] { 0.1, .05, .03, 0.9 });<br/>encoder.PreTrain(2, new double[] { 0.1, .05, .03, 0.1 });<br/>encoder.PreTrainingComplete();<br/>encoder.Save("testencoder.txt");<br/>Autoencoder newAutoencoder = Autoencoder.Load("testencoder.txt");</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>Well, folks, I think it's time to wrap this chapter up and move on. You should commend yourself, as you've written a complete autoencoder from start to (almost) finish. In the accompanying source code, I have added even more functions to make this more complete, and for you to have a better starting point from which to make this a powerful framework for you to use. As you are enhancing this, think about the things you need your autoencoder to do, block in those functions, and then complete them as we have done. Rather than learn to use an open-source framework, you've built your own—congratulations!</p>
<p>I have taken the liberty of developing a bit more of our autoencoder framework with the supplied source code. You can feel free to use it, discard it, or modify it to suit your needs. It's useful, but, as I mentioned, please feel free to embellish this and make it your own, even if it's just for educational purposes.</p>
<p>So, let's briefly recap what we have learned in this chapter: we learned about autoencoders and different variants, and we wrote our own autoencoder and created some powerful functionality. In the next chapter, we are going to move on to perhaps my most intense passion, and I hope it will soon be yours, <strong>swarm intelligence</strong>. There's going to be some theory, of course, but, once we're discussed that, I think you're going to be impressed with what particle swarm optimization algorithms can do!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<ul>
<li>Vincent P, La Rochelle H, Bengio Y, and Manzagol P A (2008), <em>Extracting and Composing Robust Features with Denoising Autoencoders</em>, proceedings of the 25th international conference on machine learning (ICML, 2008), pages 1,096 - 1,103, ACM.</li>
<li>Vincent, Pascal, et al, <em>Extracting and Composing Robust Features with De-noising Autoencoders.,</em> proceedings of the 25th international conference on machine learning. ACM, 2008.</li>
<li>Kingma, Diederik P and Max Welling<em>, Auto-encoding variational bayes,</em> arXiv pre-print arXiv:1312.6114 (2013).</li>
<li>Marc'Aurelio Ranzato, Christopher Poultney, Sumit Chopra, and Yann LeCun, <em>Efficient Learning of Sparse Representations with an Energy-Based Model,</em> proceedings of NIPS, 2007.</li>
<li>Bourlard, Hervé, and Yves Kamp, <em>Auto Association by Multilayer Perceptrons and Singular Value Decomposition, Biological Cybernetics 59.4–5</em> (1988): 291-294.</li>
</ul>


            </article>

            
        </section>
    </body></html>