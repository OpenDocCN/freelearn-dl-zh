["```py\nfrom transformers import GPT2Config, GPT2LMHeadModel\ndef create_llm(\n    num_layers, hidden_size, num_heads, ff_dim, vocab_size\n):\n    config = GPT2Config(\n        n_layer=num_layers,\n        n_embd=hidden_size,\n        n_head=num_heads,\n        n_inner=ff_dim,\n        vocab_size=vocab_size\n    )\n    model = GPT2LMHeadModel(config)\n    return model\n# Example usage\nmodel = create_llm(num_layers=12, hidden_size=768,\n    num_heads=12, ff_dim=3072, vocab_size=50257)\nprint(f\"Model parameters: {model.num_parameters():,}\")\n```", "```py\n    import numpy as np\n    from transformers import Trainer, TrainingArguments\n    from datasets import load_dataset\n    ```", "```py\n    dataset = load_dataset(\n        \"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n    def tokenize_function(examples):\n        return tokenizer(\n            examples[\"text\"], truncation=True, max_length=512)\n    tokenized_dataset = dataset.map(tokenize_function,\n        batched=True, remove_columns=dataset.column_names)\n    ```", "```py\n    manual_hyperparameters = [\n        {\"num_layers\": 6, \"hidden_size\": 512, \"num_heads\": 8, \"ff_dim\": 2048},\n        {\"num_layers\": 12, \"hidden_size\": 768, \"num_heads\": 12, \"ff_dim\": 3072},\n        {\"num_layers\": 24, \"hidden_size\": 1024, \"num_heads\": 16, \"ff_dim\": 4096}\n    ]\n    ```", "```py\n    for hp in manual_hyperparameters:\n        model = create_llm(hp, vocab_size=50257)\n        training_args = TrainingArguments(\n            output_dir=(\n                f\"./results_{hp['num_layers']}_\"\n                f\"{hp['hidden_size']}\"\n            ),\n            num_train_epochs=3,\n            per_device_train_batch_size=8,\n            logging_dir=(\n                f\"./logs_{hp['num_layers']}_\"\n                f\"{hp['hidden_size']}\"\n            ),\n        )\n        trainer = Trainer(\n            model=model,\n            args=training_args,\n            train_dataset=tokenized_dataset,\n        )\n        trainer.train()\n    ```", "```py\n        eval_results = trainer.evaluate()\n        print(f\"Hyperparameters: {hp}\")\n        print(f\"Evaluation results: {eval_results}\")\n    ```", "```py\n    import random\n    def random_hp_search(num_trials=10):\n        best_eval_loss = float('inf')\n        best_hp = None\n        for _ in range(num_trials):\n            hp = {\n                \"num_layers\": random.choice([6, 12, 24]),\n                \"hidden_size\": random.choice([512, 768, 1024]),\n                \"num_heads\": random.choice([8, 12, 16]),\n                \"ff_dim\": random.choice([2048, 3072, 4096])\n            }\n    ```", "```py\n            model = create_llm(hp, vocab_size=50257)\n            training_args = TrainingArguments(\n                output_dir=f\"./results_random_{_}\",\n                num_train_epochs=3,\n                per_device_train_batch_size=8,\n                logging_dir=f\"./logs_random_{_}\",\n            )\n            trainer = Trainer(\n                model=model,\n                args=training_args,\n                train_dataset=tokenized_dataset,\n            )\n            trainer.train()\n    ```", "```py\n            eval_results = trainer.evaluate()\n            eval_loss = eval_results['eval_loss']\n            if eval_loss < best_eval_loss:\n                best_eval_loss = eval_loss\n                best_hp = hp\n            print(\n                f\"Trial {_ + 1}: \"\n                f\"Hyperparameters: {hp}, \"\n                f\"Eval Loss: {eval_loss}\"\n            )\n        print(\n            f\"Best Hyperparameters: {best_hp}, \"\n            f\"Best Eval Loss: {best_eval_loss}\"\n        )\n    random_hp_search()\n    ```", "```py\n    import itertools\n    def grid_search():\n        hp_grid = {\n            \"num_layers\": [6, 12, 24],\n            \"hidden_size\": [512, 768, 1024],\n            \"num_heads\": [8, 12, 16],\n            \"ff_dim\": [2048, 3072, 4096]\n        }\n        best_eval_loss = float('inf')\n        best_hp = None\n    ```", "```py\n    for hp in itertools.product(*hp_grid.values()):\n            hp_dict = dict(zip(hp_grid.keys(),hp))\n            model = create_llm(\n                hp_dict[\"num_layers\"],\n                hp_dict[\"hidden_size\"],\n                hp_dict[\"num_heads\"],\n                hp_dict[\"ff_dim\"],\n                vocab_size=50257\n            )\n            training_args = TrainingArguments(\n                output_dir=(\n                    f\"./results_grid_{hp_dict['num_layers']}_\"\n                    f\"{hp_dict['hidden_size']}\"\n                ),\n                num_train_epochs=3,\n                per_device_train_batch_size=8,\n                logging_dir=(\n                    f\"./logs_grid_{hp_dict['num_layers']}_\"\n                    f\"{hp_dict['hidden_size']}\"\n                ),\n            )\n            trainer = Trainer(\n                model=model,\n                args=training_args,\n                train_dataset=tokenized_dataset,\n            )\n            trainer.train()\n    ```", "```py\n            eval_results = trainer.evaluate()\n            eval_loss = eval_results['eval_loss']\n            if eval_loss < best_eval_loss:\n                best_eval_loss = eval_loss\n                best_hp = hp_dict\n            print(\n                f\"Hyperparameters: {hp_dict}, \"\n                f\"Eval Loss: {eval_loss}\"\n            )\n        print(\n            f\"Best Hyperparameters: {best_hp}, \"\n            f\"Best Eval Loss: {best_eval_loss}\"\n        )\n    grid_search()\n    ```", "```py\n    import random\n    def advanced_random_search(num_trials=20):\n        best_eval_loss = float('inf')\n        best_hp = None\n        for _ in range(num_trials):\n            hp = {\n                \"num_layers\": random.choice([6, 12, 24]),\n                \"hidden_size\": random.choice([512, 768, 1024]),\n                \"num_heads\": random.choice([8, 12, 16]),\n                \"ff_dim\": random.choice([2048, 3072, 4096]),\n                \"learning_rate\": 10random.uniform(-5, -3),\n                \"batch_size\": random.choice([8, 16, 32]),\n                \"num_epochs\": random.randint(2, 5),\n                \"warmup_steps\": random.randint(100, 1000),\n                \"weight_decay\": random.uniform(0, 0.2)\n            }\n    ```", "```py\n            model = create_llm(\n                num_layers=hp['num_layers'],\n                    hidden_size=hp['hidden_size'],\n                num_heads=hp['num_heads'], ff_dim=hp['ff_dim'],\n                    vocab_size=50257)\n            training_args = TrainingArguments(\n                output_dir=f\"./results_advanced_random_{_}\",\n                num_train_epochs=hp['num_epochs'],\n                per_device_train_batch_size=hp['batch_size'],\n                learning_rate=hp['learning_rate'],\n                warmup_steps=hp['warmup_steps'],\n                weight_decay=hp['weight_decay'],\n                logging_dir=f\"./logs_advanced_random_{_}\",\n            )\n            trainer = Trainer(\n                model=model,\n                args=training_args,\n                train_dataset=tokenized_dataset,\n            )\n            trainer.train()\n    ```", "```py\n            eval_results = trainer.evaluate()\n            eval_loss = eval_results['eval_loss']\n            if eval_loss < best_eval_loss:\n                best_eval_loss = eval_loss\n                best_hp = hp\n            print(\n                f\"Trial {_ + 1}: Hyperparameters: {hp}, \"\n                f\"Eval Loss: {eval_loss}\"\n            )\n\n        print(\n            f\"Best Hyperparameters: {best_hp}, \"\n            f\"Best Eval Loss: {best_eval_loss}\"\n        )\n    ```", "```py\n    import optuna\n    from transformers import Trainer, TrainingArguments\n    import torch\n    def objective(trial):\n        # Define the hyperparameters to optimize\n        hp = {\n            \"num_layers\": trial.suggest_int(\"num_layers\", 6, 24),\n            \"hidden_size\": trial.suggest_categorical(\n                \"hidden_size\", [512, 768, 1024]\n            ,\n            \"num_heads\": trial.suggest_categorical(\n                \"num_heads\", [8, 12, 16]\n            ),\n            \"ff_dim\": trial.suggest_categorical(\n                \"ff_dim\", [2048, 3072, 4096]\n            ),\n            \"learning_rate\": trial.suggest_loguniform(\n                \"learning_rate\", 1e-5, 1e-3\n            ),\n            \"batch_size\": trial.suggest_categorical(\n                \"batch_size\", [8, 16, 32]\n            ),\n            \"num_epochs\": trial.suggest_int(\"num_epochs\", 2, 5),\n            \"warmup_steps\": trial.suggest_int(\n            \"warmup_steps\", 100, 1000),\n            \"weight_decay\": trial.suggest_uniform(        \"weight_decay\", 0, 0.2)\n        }\n        model = create_llm(\n            num_layers=hp['num_layers'],\n            hidden_size=hp['hidden_size'],\n            num_heads=hp['num_heads'], ff_dim=hp['ff_dim'],\n            vocab_size=50257\n        )\n    ```", "```py\n        training_args = TrainingArguments(\n            output_dir=f\"./results_bayesian_{trial.number}\",\n            num_train_epochs=hp['num_epochs'],\n            per_device_train_batch_size=hp['batch_size'],\n            learning_rate=hp['learning_rate'],\n            warmup_steps=hp['warmup_steps'],\n            weight_decay=hp['weight_decay'],\n            logging_dir=f\"./logs_bayesian_{trial.number}\",\n        )\n        trainer = Trainer(\n            model=model,\n            args=training_args,\n            train_dataset=tokenized_dataset,\n        )\n        trainer.train()\n        eval_results = trainer.evaluate()\n        return eval_results['eval_loss']\n    ```", "```py\n    study = optuna.create_study(direction=\"minimize\")\n    study.optimize(objective, n_trials=20)\n    print(\"Best trial:\")\n    trial = study.best_trial\n    print(f\"Value: {trial.value}\")\n    print(\"Params: \")\n    for key, value in trial.params.items():\n        print(f\"    {key}: {value}\")\n    ```", "```py\n    import random\n    import copy\n    class SimplePBT:\n        def __init__(self, population_size=4, num_generations=5):\n            self.population_size = population_size\n            self.num_generations = num_generations\n            self.population = []\n    ```", "```py\n    def initialize_population(self):\n        for _ in range(self.population_size):\n            hp = {\n                \"num_layers\": random.choice([6, 12, 24]),\n                \"hidden_size\": random.choice([512, 768, 1024]),\n                \"num_heads\": random.choice([8, 12, 16]),\n                \"ff_dim\": random.choice([2048, 3072, 4096]),\n                \"learning_rate\": 10random.uniform(-5, -3),\n                \"batch_size\": random.choice([8, 16, 32]),\n                \"weight_decay\": random.uniform(0, 0.2)\n            }\n            self.population.append({\"hp\": hp, \"score\": None})\n    ```", "```py\n    def train_and_evaluate(self, hp):\n        model = create_llm(num_layers=hp['num_layers'],\n            hidden_size=hp['hidden_size'],\n            num_heads=hp['num_heads'],\n            ff_dim=hp['ff_dim'], vocab_size=50257)\n        training_args = TrainingArguments(\n            output_dir=f\"./results_pbt_{random.randint(0, 1000)}\",\n            num_train_epochs=3,\n            per_device_train_batch_size=hp['batch_size'],\n            learning_rate=hp['learning_rate'],\n            weight_decay=hp['weight_decay'],\n            logging_dir=f\"./logs_pbt_{random.randint(0, 1000)}\",\n        )\n        trainer = Trainer(\n            model=model,\n            args=training_args,\n            train_dataset=tokenized_dataset,\n        )\n        trainer.train()\n        eval_results = trainer.evaluate()\n        return eval_results['eval_loss']\n    ```", "```py\n    def exploit_and_explore(self):\n        # Sort population by score\n        self.population.sort(key=lambda x: x['score'])\n        # Replace bottom half with mutated versions of top half\n        for i in range(self.population_size // 2):\n            self.population[i + self.population_size // 2]['hp'] =\\\n                self.mutate(\n                    copy.deepcopy(self.population[i]['hp'])\n                )\n    ```", "```py\n    def mutate(self, hp):\n        # Randomly mutate one hyperparameter\n        param_to_mutate = random.choice(list(hp.keys()))\n        if param_to_mutate in [\n            'num_layers', 'hidden_size', 'num_heads', 'ff_dim',\n            'batch_size'\n        ]:\n            hp[param_to_mutate] = random.choice(\n                [6, 12, 24] \n                if param_to_mutate == \"num_layers\" else\n                [512, 768, 1024] \n                if param_to_mutate == \"hidden_size\" else\n                [8, 12, 16] \n                if param_to_mutate == \"num_heads\" else\n                [2048, 3072, 4096] \n                if param_to_mutate == \"ff_dim\" else\n                [8, 16, 32]\n            )\n        elif param_to_mutate == 'learning_rate':\n            hp[param_to_mutate] *= random.uniform(0.8, 1.2)\n        elif param_to_mutate == 'weight_decay':\n            hp[param_to_mutate] = min(\n                max(hp[param_to_mutate]\n                    + random.uniform(-0.05, 0.05), 0), 0.2\n                )\n        return hp\n    ```", "```py\n    def run(self):\n        self.initialize_population()\n        for generation in range(self.num_generations):\n            print(f\"Generation {generation + 1}\")\n            for i, individual in enumerate(self.population):\n                individual['score'] = \\\n                    self.train_and_evaluate(individual['hp'])\n                print(\n                    f\"Individual {i + 1}:\n                    Score = {individual['score']}\"\n                )\n            self.exploit_and_explore()\n        best_individual = min(self.population,\n            key=lambda x: x['score'])\n        print(\"\\nBest Hyperparameters:\")\n        print(best_individual['hp'])\n        print(f\"Best Score: {best_individual['score']}\")\n    ```", "```py\n    # Run PBT\n    pbt = SimplePBT()\n    pbt.run()\n    ```", "```py\n    import optuna\n    def objective(trial):\n        hp = {\n            \"num_layers\": trial.suggest_int(\"num_layers\", 6, 24),\n            \"hidden_size\": trial.suggest_categorical(\n                \"hidden_size\", [512, 768, 1024]),\n            \"num_heads\": trial.suggest_categorical(\n                \"num_heads\", [8, 12, 16]),\n            \"ff_dim\": trial.suggest_categorical(\n                \"ff_dim\", [2048, 3072, 4096]),\n            \"learning_rate\": trial.suggest_loguniform(\n                \"learning_rate\", 1e-5, 1e-3),\n            \"batch_size\": trial.suggest_categorical(\n                \"batch_size\", [8, 16, 32]),\n            \"weight_decay\": trial.suggest_uniform(\n                \"weight_decay\", 0, 0.2)\n        }\n        model = create_llm(\n            num_layers=hp['num_layers'],\n            hidden_size=hp['hidden_size'],\n            num_heads=hp['num_heads'],\n            ff_dim=hp['ff_dim'],\n            vocab_size=50257\n        )\n    ```", "```py\n        training_args = TrainingArguments(\n            output_dir=f\"./results_multi_objective_{trial.number}\",\n            num_train_epochs=3,\n            per_device_train_batch_size=hp['batch_size'],\n            learning_rate=hp['learning_rate'],\n            weight_decay=hp['weight_decay'],\n            logging_dir=f\"./logs_multi_objective_{trial.number}\",\n        )\n           trainer = Trainer(\n            model=model,\n            args=training_args,\n            train_dataset=tokenized_dataset,\n        )\n        trainer.train()\n    ```", "```py\n        eval_results = trainer.evaluate()\n        eval_loss = eval_results['eval_loss']\n        # Calculate model size in MB\n        model_size = sum(p.numel() for p in model.parameters())\n            * 4 / 1024 / 1024  # assuming float32\n        # Simulate inference time (this would be more accurate if actually measured)\n        inference_time = 0.001 * hp['num_layers']\n            * (hp['hidden_size'] / 512)  2\n        return eval_loss, model_size, inference_time\n    ```", "```py\n    study = optuna.create_study(\n        directions=[\"minimize\", \"minimize\", \"minimize\"])\n    study.optimize(objective, n_trials=50)\n    print(\"Pareto front:\")\n    for trial in study.best_trials:\n        print(f\"Trial {trial.number}\")\n        print(f\"  Value: Loss={trial.values[0]:.4f},\n            Size={trial.values[1]:.2f}MB,\n            Inference Time={trial.values[2]:.4f}s\")\n        print(\"  Params:\")\n        for key, value in trial.params.items():\n            print(f\"    {key}: {value}\")\n    ```", "```py\n    import optuna\n    def objective(trial):\n        hp = {\n            \"num_layers\": trial.suggest_int(\"num_layers\", 6, 24),\n            \"hidden_size\": trial.suggest_categorical(\n                \"hidden_size\", [512, 768, 1024]),\n            \"num_heads\": trial.suggest_categorical(\n                \"num_heads\", [8, 12, 16]),\n            \"ff_dim\": trial.suggest_categorical(\n                \"ff_dim\", [2048, 3072, 4096]),\n            \"learning_rate\": trial.suggest_loguniform(\n                \"learning_rate\", 1e-5, 1e-3),\n            \"batch_size\": trial.suggest_categorical(\n                \"batch_size\", [8, 16, 32]),\n            \"weight_decay\": trial.suggest_uniform(\n                \"weight_decay\", 0, 0.2)\n        }\n        model = create_llm(\n            num_layers=hp['num_layers'],\n            hidden_size=hp['hidden_size'],\n            num_heads=hp['num_heads'], ff_dim=hp['ff_dim'],\n            vocab_size=50257)\n    ```", "```py\n        for steps in [100, 500, 2000]:\n            training_args = TrainingArguments(\n                output_dir= \\\n                    f\"./results_multi_fidelity_{trial.number}_\n                    {steps}\",\n                max_steps=steps,\n                per_device_train_batch_size=hp['batch_size'],\n                learning_rate=hp['learning_rate'],\n                weight_decay=hp['weight_decay'],\n                logging_dir=\\\n                    f\"./logs_multi_fidelity_{trial.number}_{steps}\",\n            )\n            trainer = Trainer(\n                model=model,\n                args=training_args,\n                train_dataset=tokenized_dataset,\n            )\n            trainer.train()\n    ```", "```py\n            eval_results = trainer.evaluate()\n            eval_loss =\n            eval_results = trainer.evaluate()\n            eval_loss = eval_results['eval_loss']\n            trial.report(eval_loss, step=steps)\n    ```", "```py\n            if trial.should_prune():\n                raise optuna.TrialPruned()\n        return eval_loss\n    ```", "```py\n    study = optuna.create_study(\n        pruner=optuna.pruners.MedianPruner())\n    study.optimize(objective, n_trials=30)\n    print(\"Best trial:\")\n    trial = study.best_trial\n    print(f\"Value: {trial.value}\")\n    print(\"Params: \")\n    for key, value in trial.params.items():\n        print(f\"    {key}: {value}\")\n    ```", "```py\n    import optuna\n    def objective(trial):\n        # ... (same as before) ...\n    # Create a study object with MySQL storage for distributed optimization\n    storage = optuna.storages.RDBStorage(\n        \"mysql://user:password@host/database\",\n        engine_kwargs={\"pool_size\": 20, \"max_overflow\": 0}\n    )\n    study = optuna.create_study(\n        storage=storage, pruner=optuna.pruners.MedianPruner())\n    # This can be run on multiple machines\n    study.optimize(objective, n_trials=10)\n    ```", "```py\n    from transformers import AutoModelForCausalLM, AutoTokenizer\n    def create_pretrained_llm(model_name, num_layers=None):\n        model = AutoModelForCausalLM.from_pretrained(model_name)\n        if num_layers is not None:\n            # Adjust the number of layers (this is a simplified approach)\n            model.transformer.h = model.transformer.h[:num_layers]\n        return model\n    def objective(trial):\n        hp = {\n            \"model_name\": trial.suggest_categorical(\n                \"model_name\",\n                [\"gpt2\", \"gpt2-medium\", \"gpt2-large\"]),\n            \"num_layers\": trial.suggest_int(\"num_layers\", 6, 24),\n            \"learning_rate\": trial.suggest_loguniform(\n                \"learning_rate\", 1e-5, 1e-3),\n            \"batch_size\": trial.suggest_categorical(\n                \"batch_size\", [8, 16, 32]),\n            \"weight_decay\": trial.suggest_uniform(\n                \"weight_decay\", 0, 0.2)\n        }\n        model = create_pretrained_llm(\n            hp['model_name'], hp['num_layers'])\n        # ... (rest of the objective function) ...\n    study = optuna.create_study(\n            pruner=optuna.pruners.MedianPruner())\n    study.optimize(objective, n_trials=30)\n    ```", "```py\n    import optuna\n    sampler = optuna.samplers.GPSampler()\n    study = optuna.create_study(sampler=sampler)\n    study.optimize(objective, n_trials=50)\n    ```", "```py\n    from optuna.pruners import SuccessiveHalvingPruner\n    pruner = SuccessiveHalvingPruner(\n        min_resource=100, reduction_factor=3,\n            min_early_stopping_rate=0)\n    study = optuna.create_study(pruner=pruner)\n    study.optimize(objective, n_trials=100)\n    ```"]