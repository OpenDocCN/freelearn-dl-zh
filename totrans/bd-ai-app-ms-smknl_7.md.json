["```py\nfrom azure.core.credentials import AzureKeyCredential\nfrom azure.search.documents.indexes import SearchIndexClient\n```", "```py\nfrom azure.search.documents.indexes.models import (\n    SearchIndex,\n    SearchField,\n    SearchFieldDataType,\n    SimpleField,\n    SearchableField,\n    VectorSearch,\n    HnswAlgorithmConfiguration,\n    HnswParameters,\n    VectorSearchAlgorithmKind,\n    VectorSearchProfile,\n    VectorSearchAlgorithmMetric,\n)\n```", "```py\ndef main() -> None:\n    index_name = os.getenv(\"ARXIV_SEARCH_INDEX_NAME\")\n    service_name = os.getenv(\"ARXIV_SEARCH_SERVICE_NAME\")\n    service_endpoint = f\"https://{service_name}.search.windows.net/\"\n    admin_key = os.getenv(\"ARXIV_SEARCH_ADMIN_KEY\")\n    credential = AzureKeyCredential(admin_key)\n    # Create a search index\n    index_client = SearchIndexClient(\n        endpoint=service_endpoint, credential=credential)\n    index_client.delete_index(index_name)\n```", "```py\nfields = [\n    SimpleField(name=\"Id\", type=SearchFieldDataType.String, key=True, sortable=True, filterable=True, facetable=True),\n    SearchableField(name=\"AdditionalMetadata\", type=SearchFieldDataType.String),\n    SearchableField(name=\"Text\", type=SearchFieldDataType.String),\n    SearchableField(name=\"Description\", type=SearchFieldDataType.String),\n    SearchableField(name=\"ExternalSourceName\", type=SearchFieldDataType.String),\n    SimpleField(name=\"IsReference\", type=SearchFieldDataType.Boolean),\n    SearchField(name=\"Embedding\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n                searchable=True, vector_search_dimensions=1536, vector_search_profile_name=\"myHnswProfile\"),\n]\n```", "```py\n    # Configure the vector search configuration\n    vector_search = VectorSearch(\n        algorithms=[\n            HnswAlgorithmConfiguration(\n                name=\"myHnsw\",\n                kind=VectorSearchAlgorithmKind.HNSW,\n                parameters=HnswParameters(\n                    m=10,\n                    ef_construction=400,\n                    ef_search=500,\n                    metric=VectorSearchAlgorithmMetric.COSINE\n                )\n            )\n        ],\n        profiles=[\n            VectorSearchProfile(\n                name=\"myHnswProfile\",\n                algorithm_configuration_name=\"myHnsw\",\n            )\n        ]\n    )\n```", "```py\n    # Create the search index with the semantic settings\n    index = SearchIndex(name=index_name, fields=fields,\n                        vector_search=vector_search)\n    result = index_client.create_or_update_index(index)\n    print(f' {result.name} created')\nif __name__ == '__main__':\n    load_dotenv()\n    main()\n```", "```py\nusing Azure.Search.Documents.Indexes;\nusing Azure.Search.Documents.Indexes.Models;\npublic class SearchModel\n{\n    [SimpleField(IsKey = true, IsSortable = true, IsFilterable = true, IsFacetable = true)]\n    public string Id { get; set; }\n    [SearchableField]\n    public string AdditionalMetadata { get; set; }\n    [SearchableField]\n    public string Text { get; set; }\n    [SearchableField]\n    public string Description { get; set; }\n    [SearchableField]\n    public string ExternalSourceName { get; set; }\n    [SimpleField(IsFilterable = true)]\n    public bool IsReference { get; set; }\n}\n```", "```py\nusing Azure;\nusing Azure.Search.Documents;\nusing Azure.Search.Documents.Indexes;\nusing Azure.Search.Documents.Indexes.Models;\nvar (apiKey, orgId, searchServiceName, searchServiceAdminKey, searchIndexName) = Settings.LoadFromFile();\nstring indexName = searchIndexName;\nAzureKeyCredential credential = new AzureKeyCredential(searchServiceAdminKey);\nSearchIndexClient indexClient = new SearchIndexClient(new Uri(searchServiceName), credential);\nindexClient.DeleteIndex(indexName);\nvar fields = new FieldBuilder().Build(typeof(SearchModel));\nSearchIndex index = new SearchIndex(indexName)\n{\n    Fields = fields,\n    // Add vector search configuration if needed\n};\nvar result = indexClient.CreateOrUpdateIndex(index);\n```", "```py\nfrom azure.core.credentials import AzureKeyCredential\nfrom azure.search.documents import SearchClient\n```", "```py\nimport asyncio\nimport semantic_kernel as sk\nimport semantic_kernel.connectors.ai.open_ai as sk_oai\n```", "```py\nfrom tenacity import retry, wait_random_exponential, stop_after_attempt\nimport pandas as pd\nimport os\nfrom dotenv import load_dotenv\n```", "```py\n@retry(wait=wait_random_exponential(min=1, max=5), stop=stop_after_attempt(3))\nasync def generate_embeddings(kernel: sk.Kernel, text):\n    e = await kernel.get_service(\"emb\").generate_embeddings(text)\n    return e[0]\n```", "```py\nasync def main():\n    kernel = sk.Kernel()\n    api_key = os.getenv(\"OPENAI_API_KEY\")\n    embedding_gen = sk_oai.OpenAITextEmbedding(service_id=\"emb\", ai_model_id=\"text-embedding-3-large\", api_key=api_key)\n    kernel.add_service(embedding_gen)\n    index_name = os.getenv(\"ARXIV_SEARCH_INDEX_NAME\")\n    service_name = os.getenv(\"ARXIV_SEARCH_SERVICE_NAME\")\n    service_endpoint = f\"https://{service_name}.search.windows.net/\"\n    admin_key = os.getenv(\"ARXIV_SEARCH_ADMIN_KEY\")\n    credential = AzureKeyCredential(admin_key)\n    # Create a search index\n    index_client = SearchClient(index_name=index_name,\n        endpoint=service_endpoint, credential=credential)\n```", "```py\n    df = pd.read_json('ai_arxiv_202101.json', lines=True)\n    count = 0\n    documents = []\n    for key, item in df.iterrows():\n        id = str(item[\"Id\"])\n        id = id.replace(\".\", \"_\")\n```", "```py\n        embeddings = await generate_embeddings(kernel, item[\"abstract\"])\n        # convert embeddings to a list of floats\n        embeddings = [float(x) for x in embeddings]\n        document = {\n            \"@search.action\": \"upload\",\n            \"Id\": id,\n            \"Text\": item[\"title\"],\n            \"Description\": item[\"abstract\"],\n            \"Embedding\": embeddings\n        }\n        documents.append(document)\n```", "```py\n    N = 100\n    for i in range(0, len(documents), N):\n        result = index_client.upload_documents(documents[i:i+N])\n        print(f\"Uploaded {len(documents[i:i+N])} records\")\n    print(f\"Final tally: inserted or updated {len(documents)} records\")\n```", "```py\nif __name__ == \"__main__\":\n    load_dotenv()\n    asyncio.run(main())\n```", "```py\nFinal tally: inserted or updated 35,808 records.\n```", "```py\ndotnet add package Microsoft.SemanticKernel.Connectors.AzureAISearch --prerelease\n```", "```py\ndotnet add package Microsoft.SemanticKernel.Connectors.OpenAI\n```", "```py\nusing Microsoft.SemanticKernel.Connectors.AzureAISearch;\nusing Microsoft.SemanticKernel.Memory;\nusing Microsoft.SemanticKernel;\nusing Microsoft.SemanticKernel.Connectors.OpenAI;\nusing System.Text.Json;\n```", "```py\n#pragma warning disable SKEXP0020\n#pragma warning disable SKEXP0010\n#pragma warning disable SKEXP0001\nISemanticTextMemory memoryWithCustomDb;\n```", "```py\nvar (apiKey, orgId, searchServiceName, searchServiceAdminKey, searchIndexName) = Settings.LoadFromFile();\n```", "```py\nmemoryWithCustomDb = new MemoryBuilder()\n                .WithOpenAITextEmbeddingGeneration(\"text-embedding-3-small\", apiKey)\n                    .WithMemoryStore(new AzureAISearchMemoryStore(searchServiceName, searchServiceAdminKey))\n                        .Build();\n```", "```py\nstring data = File.ReadAllText(\"ai_arxiv.json\");\nint i = 0;\nforeach (string line in data.Split('\\n'))\n{\n    i++;\n    var paper = JsonSerializer.Deserialize<Dictionary<string, object>>(line);\n    if (paper == null)\n    {\n        continue;\n    }\n    string title = paper[\"title\"]?.ToString() ?? \"No title available\";\n    string id = paper[\"id\"]?.ToString() ?? \"No ID available\";\n    string abstractText = paper[\"abstract\"]?.ToString() ?? \"No abstract available\";\n    id = id.Replace(\".\", \"_\");\n```", "```py\n    await memoryWithCustomDb.SaveInformationAsync(collection: searchIndexName,\n        text: abstractText,\n        id: id,\n        description: title);\n    if (i % 100 == 0)\n    {\n        Console.WriteLine($\"Processed {i} documents at {DateTime.Now}\");\n    }\n}\n```", "```py\nimport asyncio\nimport logging\nimport semantic_kernel as sk\nimport semantic_kernel.connectors.ai.open_ai as sk_oai\nfrom azure.core.credentials import AzureKeyCredential\nfrom azure.search.documents import SearchClient\nfrom azure.search.documents.models import VectorizedQuery\nfrom dotenv import load_dotenv\nfrom tenacity import retry, wait_random_exponential, stop_after_attempt\nimport pandas as pd\nimport os\n@retry(wait=wait_random_exponential(min=1, max=5), stop=stop_after_attempt(3))\nasync def generate_embeddings(kernel: sk.Kernel, text):\n    e = await kernel.get_service(\"emb\").generate_embeddings(text)\n    # convert e[0] to a vector of floats\n    result = [float(x) for x in e[0]]\n    return result\n```", "```py\ndef create_kernel() -> sk.Kernel:\n    kernel = sk.Kernel()\n    api_key = os.getenv(\"OPENAI_API_KEY\")\n    embedding_gen = sk_oai.OpenAITextEmbedding(service_id=\"emb\", ai_model_id=\"text-embedding-3-small\", api_key=api_key)\n    kernel.add_service(embedding_gen)\n    return kernel\nasync def main():\n    kernel = create_kernel()\n    ais_index_name = os.getenv(\"ARXIV_SEARCH_INDEX_NAME\")\n    ais_service_name = os.getenv(\"ARXIV_SEARCH_SERVICE_NAME\")\n    ais_service_endpoint = f\"https://{ais_service_name}.search.windows.net/\"\n    ais_admin_key = os.getenv(\"ARXIV_SEARCH_ADMIN_KEY\")\n    credential = AzureKeyCredential(ais_admin_key)\n    search_client = SearchClient(ais_service_endpoint, ais_index_name, credential=credential)\n```", "```py\n    query_string = \"<your query here>\"\n    emb = await generate_embeddings(kernel, query_string)\n    vector_query = VectorizedQuery(vector=emb, k_nearest_neighbors=5, exhaustive=True, fields=\"Embedding\")\n    results = search_client.search(\n        search_text=None,\n        vector_queries= [vector_query],\n        select=[\"Id\", \"Text\", \"Description\"]\n    )\n```", "```py\n    pd_results = []\n    for result in results:\n        d = {\n            \"id\": result['Id'],\n            \"title\": result['Description'],\n            \"abstract\": result['Text'],\n            \"score\": f\"{result['@search.score']:.2f}\"\n        }\n        pd_results.append(d)\n```", "```py\n    pd_results = pd.DataFrame(pd_results)\n    # print the title of each result\n    for index, row in pd_results.iterrows():\n        print(row[\"title\"])\nif __name__ == \"__main__\":\n    load_dotenv()\n    asyncio.run(main())\n```", "```py\nquery_string = \"models with long context windows lose information in the middle\"\n```", "```py\nIAsyncEnumerable<MemoryQueryResult> memories = memoryWithCustomDb.SearchAsync(searchIndexName, query_string, limit: 5, minRelevanceScore: 0.0);\nint i = 0;\nawait foreach (MemoryQueryResult item in memories)\n{\n    i++;\n    Console.WriteLine($\"{i}. {item.Metadata.Description}\");\n}\n```", "```py\n1\\. Lost in the Middle: How Language Models Use Long Contexts\n2\\. Parallel Context Windows for Large Language Models\n3\\. Revisiting Parallel Context Windows: A Frustratingly Simple Alternative and Chain-of-Thought Deterioration\n4\\. \"Paraphrasing The Original Text\" Makes High Accuracy Long-Context QA\n5\\. Emotion Detection in Unfix-length-Context Conversation\n```", "```py\nYou are a professor of computer science writing a report about artificial intelligence for a popular newspaper.\nKeep the language simple and friendly.\nBelow, I'm going to give you a list of 5 research papers about artificial intelligence. Each paper has a number and an abstract.\nSummarize the combined findings of the paper. When using the abstracts, refer to them by using their number inside [] brackets.\nYour summary should be about 250 words.\nAbstracts:\n{{$input}}\n```", "```py\n{\n    \"schema\": 1,\n    \"type\": \"completion\",\n    \"description\": \"Summarize abstracts of academic papers\",\n    \"execution_settings\": {\n       \"default\": {\n         \"max_tokens\": 4000,\n         \"temperature\": 0.5\n       }\n     },\n    \"input_variables\": [\n       {\n         \"name\": \"input\",\n         \"description\": \"A numbered list of abstracts to summarize.\",\n         \"required\": true\n       }\n    ]\n}\n```", "```py\ndef create_kernel() -> sk.Kernel:\n    kernel = sk.Kernel()\n    api_key = os.getenv(\"OPENAI_API_KEY\")\n    embedding_gen = sk_oai.OpenAITextEmbedding(service_id=\"emb\", ai_model_id=\"text-embedding-3-small\", api_key=api_key)\n    gpt_gen = sk_oai.OpenAIChatCompletion(service_id=\"gpt-4-turbo\", ai_model_id=\"gpt-4-turbo-preview\", api_key=api_key)\n    kernel.add_service(gpt_gen)\n    kernel.add_service(embedding_gen)\n    return kernel\n```", "```py\nasync def summarize_documents(kernel: sk.Kernel, df: pd.DataFrame) -> str:\n    doc_list = \"\"\n    i = 0\n    doc_list += \"Here are the top 5 documents that are most similar to your query:\\n\\n\"\n    for key, row in df.iterrows():\n        i = i + 1\n        id = row[\"Id\"].replace(\"_\", \".\")\n        doc_list += f\"{i}. \"\n        doc_list += f\"{row['Description']} - \"\n        doc_list += f\"https://arxiv.org/abs/{id}\\n\"\n```", "```py\n    a = 0\n    abstracts = \"\"\n    for key, row in df.iterrows():\n        a = a + 1\n        abstracts += f\"\\n\\n{a}. {row['Text']}\\n\"\n```", "```py\n    f = kernel.import_plugin_from_prompt_directory(\".\", \"prompts\")\n    summary = await kernel.invoke(f[\"summarize_abstracts\"], input=abstracts)\n```", "```py\n    response = f\"{doc_list}\\n\\n{summary}\"\n    return response\n```", "```py\nIAsyncEnumerable<MemoryQueryResult> memories = memoryWithCustomDb.SearchAsync(searchIndexName, query_string, limit: 5, minRelevanceScore: 0.0);\n```", "```py\nstring explanation = \"Here are the top 5 documents that are most like your query:\\n\";\nint j = 0;\nawait foreach (MemoryQueryResult item in memories)\n{\n    j++;\n    string id = item.Metadata.Id;\n    id.Replace('_', '.');\n    explanation += $\"{j}. {item.Metadata.Description}\\n\";\n    explanation += $\"https://arxiv.org/abs/{id}\\n\";\n}\nexplanation += \"\\n\";\n```", "```py\nstring input = \"\";\nint i = 0;\nawait foreach (MemoryQueryResult item in memories)\n{\n    i++;\n    input += $\"{i}. {item.Metadata.Text}\";\n}\n```", "```py\nKernel kernel = Kernel.CreateBuilder()\n                        .AddOpenAIChatCompletion(\"gpt-4-turbo\", apiKey, orgId, serviceId: \"gpt-4-turbo\")\n                        .Build();\nvar rag = kernel.ImportPluginFromPromptDirectory(\"prompts\", \"SummarizeAbstract\");\nexplanation += await kernel.InvokeAsync(rag[\"summarize_abstracts\"], new KernelArguments() {[\"input\"] = input});\nConsole.WriteLine(explanation);\n```", "```py\nHere are the top 5 documents that are most like your query:\n1\\. Lost in the Middle: How Language Models Use Long Contexts - https://arxiv.org/abs/2307.03172\n2\\. Parallel Context Windows for Large Language Models - https://arxiv.org/abs/2212.10947\n3\\. Revisiting Parallel Context Windows: A Frustratingly Simple Alternative and Chain-of-Thought Deterioration - https://arxiv.org/abs/2305.15262\n4\\. \"Paraphrasing the Original Text\" Makes High Accuracy Long-Context QA - https://arxiv.org/abs/2312.11193\n5\\. Emotion Detection in Unfix-length-Context Conversation - https://arxiv.org/abs/2302.06029\nIn the rapidly evolving field of artificial intelligence, particularly in the realm of language models, recent research has been shedding light on both the capabilities and limitations of these advanced systems. A critical challenge identified is the handling of long text sequences by language models, which is essential for tasks such as multi-document question answering and key-value retrieval [1]. Despite the advancements, it's observed that the performance of these models often diminishes when they need to process relevant information located in the middle of long contexts [1]. This indicates a need for better strategies to enable models to effectively utilize long input contexts.\nTo address these limitations, a novel method named Parallel Context Windows (PCW) has been introduced, which allows off-the-shelf Large Language Models (LLMs) to process long texts by dividing them into smaller chunks. This method has shown substantial improvements in handling diverse tasks requiring long text sequences without the need for further training [2]. However, further analysis reveals that PCW may not consistently enhance the models' understanding of long contexts in more complex reasoning tasks, suggesting that the method's design might not guarantee significant improvements in practical applications [3].\nAnother approach to enhancing long-context capabilities involves focusing on the quality of training data. It has been found that \"effective\" data, which can be achieved through techniques such as original text paraphrasing, is crucial for training models to handle long texts, leading to state-of-the-art performance in multi-document retrieval and question answering tasks [4].\nAdditionally, research into variable-length context windows for predicting emotions in conversations introduces new modules to better capture conversational dynamics. This approach significantly outperforms existing models by more accurately determining the relevant context for emotion prediction [5].\nCollectively, these studies highlight the importance of innovative methods and quality training data in overcoming the challenges of processing long texts. They also underscore the need for continued exploration to enhance the practical applicability of language models in real-world scenarios.\n```"]