["```py\n# Pairs\npairs = [('a', 'b'), ('b', 'e'), ('e', 'm'), ('m', 'p'), ('a', 'z'), ('b', 'q')]\nfriends = {('a', 'b'), ('b', 'e'), ('e', 'm'), ('m', 'p')} \n```", "```py\n# Build the tree\ntree, root = build_tree_from_pairs(pairs)\n# Check relationships\ncheck_relationships(pairs, friends) \n```", "```py\nPair ('a', 'b'): friend\nPair ('b', 'e'): friend\nPair ('e', 'm'): friend\nPair ('m', 'p'): friend\nPair ('a', 'z'): not friend\nPair ('b', 'q'): not friend \n```", "```py\n# Draw the tree\nlayout_choice = 'spring'  # Define your layout choice here\ndraw_tree(tree, layout_choice=layout_choice, root=root, friends=friends) \n```", "```py\ntry:\n  import wikipediaapi\nexcept:\n  !pip install Wikipedia-API==0.6.0\n  import wikipediaapi \n```", "```py\ndef nb_tokens(text):\n    # More sophisticated tokenization which includes punctuation\n    tokens = word_tokenize(text)\n    return len(tokens) \n```", "```py\n# Create an instance of the Wikipedia API with a detailed user agent\nwiki = wikipediaapi.Wikipedia(\n    language='en',\n    user_agent='Knowledge/1.0 ([USER AGENT EMAIL)'\n) \n```", "```py\ntopic=\"Marketing\"     # topic\nfilename=\"Marketing\"  # filename for saving the outputs\nmaxl=100 \n```", "```py\nimport textwrap # to wrap the text and display it in paragraphs\npage=wiki.page(topic)\nif page.exists()==True:\n  print(\"Page - Exists: %s\" % page.exists())\n  summary=page.summary\n  # number of tokens)\n  nbt=nb_tokens(summary)\n  print(\"Number of tokens: \",nbt)\n  # Use textwrap to wrap the summary text to a specified width, e.g., 70 characters\n  wrapped_text = textwrap.fill(summary, width=60)\n  # Print the wrapped summary text\n  print(wrapped_text)\nelse:\n  print(\"Page does not exist\") \n```", "```py\nPage - Exists: True\nNumber of tokens:  229\nMarketing is the act of satisfying and retaining customers.\nIt is one of the primary components of business management\nand commerce. Marketing is typically conducted by the seller, typically a retailer or manufacturer… \n```", "```py\nprint(page.fullurl) \n```", "```py\nhttps://en.wikipedia.org/wiki/Marketing \n```", "```py\n# prompt: read the program up to this cell. Then retrieve all the links for this page: print the link and a summary of each link.\n# Get all the links on the page\nlinks = page.links\n# Print the link and a summary of each link\nurls = []\ncounter=0\nfor link in links:\n  try:\n    counter+=1\n    print(f\"Link {counter}: {link}\")\n    summary = wiki.page(link).summary\n    print(f\"Link: {link}\")\n    print(wiki.page(link).fullurl)\n    urls.append(wiki.page(link).fullurl)\n    print(f\"Summary: {summary}\")\n    if counter>=maxl:\n      break\n  except page.exists()==False:\n    # Ignore pages that don't exist\n    pass\nprint(counter)\nprint(urls) \n```", "```py\nLink 1: 24-hour news cycle\nLink: 24-hour news cycle\nhttps://en.wikipedia.org/wiki/24-hour_news_cycle\nSummary: The 24-hour news cycle (or 24/7 news cycle) is 24-hour investigation and reporting of news, concomitant with fast-paced lifestyles… \n```", "```py\nfrom datetime import datetime\n# Get all the links on the page\nlinks = page.links\n# Prepare a file to store the outputs\nfname = filename+\"_citations.txt\"\nwith open(fname, \"w\") as file:\n    # Write the citation header\n    file.write(f\"Citation. In Wikipedia, The Free Encyclopedia. Pages retrieved from the following Wikipedia contributors on {datetime.now()}\\n\")\n    file.write(\"Root page: \" + page.fullurl + \"\\n\")\n    counter = 0\n    urls = []… \n```", "```py\nCitation. In Wikipedia, The Free Encyclopedia. Pages retrieved from the following Wikipedia contributors on {datetime.now()}\\n\") \n```", "```py\nurls \n```", "```py\n['https://en.wikipedia.org/wiki/Marketing',\n 'https://en.wikipedia.org/wiki/24-hour_news_cycle',\n 'https://en.wikipedia.org/wiki/Account-based_marketing',\n… \n```", "```py\n# Write URLs to a file\nufname = filename+\"_urls.txt\"\nwith open(ufname, 'w') as file:\n    for url in urls:\n        file.write(url + '\\n')\nprint(\"URLs have been written to urls.txt\") \n```", "```py\n#File name for file management\ngraph_name=\"Marketing\"\n# Path for vector store and dataset\ndb=\"hub://denis76/marketing01\"\nvector_store_path = db\ndataset_path = db\n#if True upserts data; if False, passes upserting and goes to connection\npop_vs=True\n# if pop_vs==True, overwrite=True will overwrite dataset, False will append it:\now=True \n```", "```py\n# Define your variables\nif pop_vs==True:\n  directory = \"Chapter07/citations\"\n  file_name = graph_name+\"_urls.txt\"\n  download(directory,file_name) \n```", "```py\nID:\n['a61734be-fe23-421e-9a8b-db6593c48e08']\nMetadata:\nfile_path: /content/data/24-hour_news_cycle.txt\nfile_name: 24-hour_news_cycle.txt\nfile_type: text/plain\nfile_size: 2763\ncreation_date: 2024-07-05\nlast_modified_date: 2024-07-05\n…\nText:\n['24hour investigation and reporting of news concomitant with fastpaced lifestyles This article is about the fastpaced cycle of news media in technologically advanced societies.\nEmbedding:\n[-0.00040736704249866307, 0.009565318934619427, 0.015906672924757004, -0.009085721336305141, …] \n```", "```py\nfrom llama_index.core import KnowledgeGraphIndex\nimport time\n# Start the timer\nstart_time = time.time() \n```", "```py\n#graph index with embeddings\ngraph_index = KnowledgeGraphIndex.from_documents(\n    documents,\n    max_triplets_per_chunk=2,\n    include_embeddings=True,\n) \n```", "```py\n# Stop the timer\nend_time = time.time()\n# Calculate and print the execution time\nelapsed_time = end_time - start_time\nprint(f\"Index creation time: {elapsed_time:.4f} seconds\")\nprint(type(graph_index)) \n```", "```py\nIndex creation time: 371.9844 seconds \n```", "```py\nprint(type(graph_index)) \n```", "```py\n<class 'llama_index.core.indices.knowledge_graph.base.KnowledgeGraphIndex'> \n```", "```py\n#similarity_top_k\nk=3\n#temperature\ntemp=0.1\n#num_output\nmt=1024\ngraph_query_engine = graph_index.as_query_engine(similarity_top_k=k, temperature=temp, num_output=mt) \n```", "```py\ngraph_query_engine = graph_index.as_query_engine(similarity_top_k=k, temperature=temp, num_output=mt) \n```", "```py\n## create graph\nfrom pyvis.network import Network\ng = graph_index.get_networkx_graph()\nnet = Network(notebook=True, cdn_resources=\"in_line\", directed=True)\nnet.from_nx(g)\n# Set node and edge properties: colors and sizes\nfor node in net.nodes:\n    node['color'] = 'lightgray'\n    node['size'] = 10\nfor edge in net.edges:\n    edge['color'] = 'black'\n    edge['width'] = 1 \n```", "```py\nfgraph=\"Knowledge_graph_\"+ graph_name + \".html\"\nnet.write_html(fgraph)\nprint(fgraph) \n```", "```py\nfrom IPython.display import HTML\n# Load the HTML content from a file and display it\nwith open(fgraph, 'r') as file:\n    html_content = file.read()\n# Display the HTML in the notebook\ndisplay(HTML(html_content)) \n```", "```py\nQuery execution time: 2.4789 seconds\nThe primary goal of marketing for the consumer market is to effectively target consumers, understand their behavior, preferences, and needs, and ultimately influence their purchasing decisions. \n```", "```py\nfrom google.colab import userdata\nuserdata.get('HF_TOKEN') \n```", "```py\n!pip install sentence-transformers==3.0.1 \n```", "```py\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\ndef calculate_cosine_similarity_with_embeddings(text1, text2):\n    embeddings1 = model.encode(text1)\n    embeddings2 = model.encode(text2)\n    similarity = cosine_similarity([embeddings1], [embeddings2])\n    return similarity[0][0] \n```", "```py\nimport time\nimport textwrap\nimport sys\nimport io \n```", "```py\nPsychologists, cultural anthropologists, and market researchers are often associated with marketing\ntheory. \n```", "```py\nBest Rank: 2\nBest Score: 0.5217772722244263\n[…In 1380 the German textile manufacturer **Johann Fugger****Daniel Defoe**  travelled from Augsburg to Graben in order to gather information on the international textile industry… During this period   a\nLondon merchant published information on trade and economic resources of England and Scotland…] \n```", "```py\n# create an empty array score human feedback scores:\nrscores =[]\n# create an empty score for similarity function scores\nscores=[] \n```", "```py\nQuery execution time: 1.9648 seconds\nPsychologists, cultural anthropologists, and other experts in behavioral sciences are often\nassociated with marketing theory. \n```", "```py\ntext1=str(response)\ntext2=user_query\nsimilarity_score3=calculate_cosine_similarity_with_embeddings(text1, text2)\nprint(f\"Cosine Similarity Score with sentence transformer: {similarity_score3:.3f}\")\nscores.append(similarity_score3)\nhuman_feedback=0.75\nrscores.append(human_feedback) \n```", "```py\nprint(len(scores), scores) \n```", "```py\n10 [0.808918, 0.720165, 0.7599532, 0.8513956, 0.5457667, 0.6963912, 0.9036964, 0.44829217, 0.59976315, 0.47448665] \n```", "```py\nprint(len(rscores), rscores) \n```", "```py\n10 [0.75, 0.5, 0.8, 0.9, 0.65, 0.8, 0.9, 0.2, 0.2, 0.9] \n```", "```py\nmean_score = np.mean(scores)\nmedian_score = np.median(scores)\nstd_deviation = np.std(scores)\nvariance = np.var(scores)\nmin_score = np.min(scores)\nmax_score = np.max(scores)\nrange_score = max_score - min_score\npercentile_25 = np.percentile(scores, 25)\npercentile_75 = np.percentile(scores, 75)\niqr = percentile_75 - percentile_25 \n```"]