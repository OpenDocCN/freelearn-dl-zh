<html><head></head><body>
        

                            
                    <h1 class="header-title">Monte Carlo Methods</h1>
                
            
            
                
<p>For this chapter, we will jump back to the trial-and-error thread of <strong>reinforcement learning</strong> (<strong>RL</strong>) and look at Monte Carlo methods. This is a class of methods that works by episodically playing through an environment instead of planning. We will see how this improves our RL search for the best policy and we now start to think of our algorithm as an actual agent—one that explores the game environment rather than preplans a policy, which, in turn, allows us to understand the benefits of using a model for planning or not. From there, we will look at the Monte Carlo method and how to implement it in code. Then, we will revisit a larger version of the FrozenLake environment with our new Monte Carlo agent algorithm.</p>
<p>In this chapter, we will continue looking at how RL has evolved and, in particular, focus on the trial and error thread with the Monte Carlo method. Here is a summary of the main topics we will cover in this chapter:</p>
<ul>
<li>Understanding model-based and model-free learning</li>
<li>Introducing the Monte Carlo method</li>
<li>Adding RL</li>
<li>Playing the FrozenLake game</li>
<li>Using prediction and control</li>
</ul>
<p>We again explore more foundations of RL, variational inference, and the trial and error method. This knowledge will be essential for anyone who is serious about finishing this book, so please don't skip this chapter.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Understanding model-based and model-free learning</h1>
                
            
            
                
<p>If you recall from our very first chapter, <a href="5553d896-c079-4404-a41b-c25293c745bb.xhtml">Chapter 1</a>, <em>Understanding Rewards-Based Learning</em>, we explored the primary elements of RL. We learned that RL comprises of a policy, a value function, a reward function, and, optionally, a model. We use the word <em>model</em> in this context to refer to a detailed plan of the environment. Going back to the last chapter again, where we used the FrozenLake environment, we had a perfect model of that environment:</p>
<div><img class="aligncenter size-full wp-image-481 image-border" src="img/2963ac28-ffbe-4c12-b905-03f65c54ff7c.png" style="width:23.08em;height:22.75em;"/><br/>
<br/>
Model of the FrozenLake environment</div>
<p>Of course, looking at problems with a fully described model in a finite MDP is all well and good for learning. However, when it comes to the real world, having a full and completely understood model of any environment would likely be highly improbable, if not impossible. This is because there are far too many states to account for or model in any real-world problem. As it turns out, this can also be the case for many other models as well. Later in this book, we will look at environments with more states than the number of atoms in the known universe. We could never possibly model such environments. Hence, the planning methods we learned in <a href="8237fd36-1edf-4da0-b271-9a50c5b8deb3.xhtml">Chapter 2</a>, <em>Dynamic Programming and the Bellman Equation</em>, won't work. Instead, we need a method that can explore an environment and learn from it. This is where Monte Carlo comes in and is something we will cover in the next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Introducing the Monte Carlo method</h1>
                
            
            
                
<p>The Monte Carlo method was so named because of its similarity to gambling or chance. Hence, the method was named after the famous gambling destination at the time. While the method is extremely powerful, it has been used to describe the atom, quantum mechanics, and the quantity of <sub><img class="fm-editor-equation" src="img/28309dc6-92e5-49ba-aea1-e9ad32bfce3c.png" style="width:0.83em;height:0.83em;"/></sub> itself. It is only until fairly recently, within the last 20 years, that it has seen widespread acceptance in everything from engineering to financial analysis. The method itself has now become foundational to many aspects of machine learning and is worth further study for anyone in the AI field.</p>
<p>In the next section, we will see how the Monte Carlo method can be used to solve for <sub><img class="fm-editor-equation" src="img/7b9758bc-0cbb-47ab-8289-66d8157600fb.png" style="width:0.83em;height:0.83em;"/></sub>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Solving for </h1>
                
            
            
                
<p>The standard introduction to Monte Carlo methods is to show how it can be used to solve for <img class="fm-editor-equation" src="img/cd567828-140e-4cb5-95fd-7a08af97d028.png" style="width:0.83em;height:0.83em;"/>. Recall from geometry, <sub><img class="fm-editor-equation" src="img/a35704d0-4695-4796-bd6a-a8693703de86.png" style="width:0.83em;height:0.83em;"/></sub> represents half the circumference of a circle and 2π represents a full circle. To find this relationship and value, let's consider a unit circle with a radius of 1 unit. That unit could be feet, meters, parsecs, or whatever—it's not important. Then, if we place that circle within a square box with dimensions of 1 unit, we can see the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-482 image-border" src="img/f97cc8b3-a951-4ddf-85b6-b8bae03b57ad.png" style="width:17.83em;height:20.08em;"/></p>
<p>A unit circle inside a unit square</p>
<p>Given the preceding, we know that we have a square that encompasses dimensions of 2 units by 2 units or 100% of the area with an area of 4 square units. Going back to geometry again, we know that the area of a circle is given by <sub><img class="fm-editor-equation" src="img/7a492136-0712-4f73-b12b-8c05460618aa.png" style="width:2.08em;height:1.42em;"/></sub>. Knowing that the circle is within the square and knowing the full area, we can then apply the Monte Carlo method to solve for the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/8ae5a92d-fd87-4bdb-ba2f-5260483a1247.png" style="width:6.58em;height:1.33em;"/></p>
<p>The Monte Carlo method works by randomly sampling an area and then determining what percentage of that sample is correct or incorrect. Going back to our example, we can think of this as randomly dropping darts onto the square and then counting how many land within the circle. By counting the number of darts that land within the circle, we can then backcalculate a number for π using the following equation:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/fde16558-a28b-439a-9af4-146fb64597ac.png" style="width:8.58em;height:1.42em;"/></p>
<p>In the preceding equation, we have the following:</p>
<ul>
<li><em>ins</em>: The total number of darts or samples that fell within the circle</li>
<li><em>total</em>: The total number of darts dropped</li>
</ul>
<p>The important part about the preceding equation is to realize that all we are doing here is taking a percentage (<em>ins</em>/<em>total</em>) of how many darts fell within the circle to determine a value for π. This may still be a little unclear, so let's look at a couple of examples in the next sections.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Implementing Monte Carlo</h1>
                
            
            
                
<p>In many cases, even understanding simple concepts that are abstract can be difficult without real-world examples. Therefore, open up the <kbd>Chapter_3_1.py</kbd> code sample.</p>
<p>We should mention before starting that <img class="fm-editor-equation" src="img/bc1fcbba-b85e-406a-bc3c-71556f0e3a90.png" style="width:0.83em;height:0.83em;"/>, in this case, refers to the actual value we estimate at 3.14.</p>
<p>Follow the exercise:</p>
<ol>
<li>The following is the entire code listing for reference:</li>
</ol>
<pre style="padding-left: 60px">from random import *<br/>from math import sqrt<br/><br/>ins = 0<br/>n = 100000<br/><br/>for i in range(0, n):<br/>   x = (random()-.5) * 2<br/>    y = (random()-.5) * 2<br/>    if sqrt(x*x+y*y)&lt;=1:<br/>        ins+=1<br/><br/>pi = 4 * ins / n<br/>print(pi)</pre>
<p style="padding-left: 60px">This code solves for <img class="fm-editor-equation" src="img/812e1640-0feb-46c5-a7eb-d34729e14040.png" style="width:0.83em;height:0.83em;"/> using the Monte Carlo method, which is quite impressive when you consider how simple the code is. Let's go over each section of the code.</p>
<ol start="2">
<li>We start with the <kbd>import</kbd> statements, and here we just import <kbd>random</kbd> and the <kbd>math</kbd> function, <kbd>sqrt</kbd>.</li>
<li>From there, we define a couple of variables, <kbd>ins</kbd> and <kbd>n</kbd>. The <kbd>ins</kbd> variable holds the number of times a dart or sample is inside the circle. The <kbd>n</kbd> variable represents how many iterations or darts to drop.</li>
<li>Next, we randomly drop darts with the following code:</li>
</ol>
<pre style="padding-left: 60px">for i in range(0, n):<br/>  x = (random()-.5) * 2<br/>  y = (random()-.5) * 2<br/>  if sqrt(x*x+y*y)&lt;=1:<br/>   ins+=1</pre>
<ol start="5">
<li>All this code does is randomly sample values in the range of <kbd>-1</kbd> to <kbd>1</kbd> for <kbd>x</kbd> and <kbd>y</kbd> and then determine whether they are within a circle radius of <kbd>1</kbd>, which is given by the calculation within the square root function.</li>
<li>Finally, the last couple of lines do the calculation and output the result.</li>
<li>Run the example as you normally would and observe the output.</li>
</ol>
<p>What you will likely find is the guess may be a bit off. That all depends on the number of samples. You see, the confidence of the Monte Carlo method and therefore the quality of the answer goes up with the more samples you do. Hence, to improve the last example, you will have to increase the value of the variable, <kbd>n</kbd>.</p>
<p>In the next section, we look at this example again but this time look at what those dart samples may actually look like in the next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Plotting the guesses</h1>
                
            
            
                
<p>If you are still having problems grasping this concept, visualizing this example may be more helpful. Run the exercise in the next section if you want to visualize what this sampling looks like:</p>
<ol>
<li>Before starting this exercise, we will install the <kbd>matplotlib</kbd> library. Install the library with <kbd>pip</kbd> using the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>pip install matplotlib</strong></pre>
<ol start="2">
<li>After the install, open up the <kbd>Chapter_3_2.py</kbd> code example shown here:</li>
</ol>
<pre style="padding-left: 60px">import matplotlib.pyplot as plt<br/>from random import random<br/><br/>ins = 0<br/>n = 1000<br/><br/><strong>x_ins = []</strong><br/><strong>y_ins = []</strong><br/><strong>x_outs = []</strong><br/><strong>y_outs = []</strong><br/><br/>for _ in range(n):<br/>    x = (random()-.5) * 2<br/>    y = (random()-.5) * 2 <br/>    if (x**2+y**2) &lt;= 1:<br/>        ins += 1<br/>        <strong>x_ins.append(x)</strong><br/><strong>        y_ins.append(y)</strong><br/>    else:<br/>        <strong>x_outs.append(x)</strong><br/><strong>        y_outs.append(y)</strong><br/><br/>pi = 4 * ins/n<br/>print(pi)<br/><br/><strong>fig, ax = plt.subplots()</strong><br/><strong>ax.set_aspect('equal')</strong><br/><strong>ax.scatter(x_ins, y_ins, color='g', marker='s')</strong><br/><strong>ax.scatter(x_outs, y_outs, color='r', marker='s')</strong><br/><strong>plt.show()</strong></pre>
<ol start="3">
<li>The code is quite similar to the last exercise and should be fairly self-explanatory. We will just focus on the important sections of code highlighted in the preceding.</li>
<li>The big difference in this example is we remember where the darts are dropped and identify whether they fell inside or outside of the circle. After that, we plot the results. We plot a point for each and color them green for inside the circle and red for outside.</li>
<li>Run the sample and observe the output, as shown here:</li>
</ol>
<div><img class="aligncenter size-full wp-image-856 image-border" src="img/112792e6-4d06-43b4-b15c-7806abcf1979.png" style="width:33.83em;height:30.67em;"/><br/>
<br/>
Example output from Chapter_3_2.py</div>
<p>The output looks like a circle, as much as we would expect it to. However, there is a problem with the output value of π. Notice how the estimated value of π is now quite low. This is because the value of <em>n—</em>the number of darts or samples—is only 1,000. That means, for the Monte Carlo method to be a good estimator, we also need to realize it needs a sufficiently large number of guesses.</p>
<p>In the next section, we look to see how we can apply this method to an expanded version of the FrozenLake problem with RL.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Adding RL</h1>
                
            
            
                
<p>Now that we understand the Monte Carlo method, we need to understand how to apply it to RL. Recall that our expectation now is that our environment is relatively unknown, that is, we do not have a model. Instead, we now need to develop an algorithm by which to explore the environment by trial and error. Then, we can take all of those various trials and, by using Monte Carlo, average them out and determine a best or better policy. We can then use that improved policy to continue exploring the environment for further improvements. Essentially, our algorithm becomes an explorer rather than a planner and this is why we now refer to it as an agent.</p>
<p>Using the term <strong>agent</strong> reminds us that our algorithm is now an explorer and learner. Hence, our agents not only explore but also learn from that exploration and improve on it. Now, this is real artificial intelligence.</p>
<p>Aside from the exploration part, which we already visited earlier in <a href="5553d896-c079-4404-a41b-c25293c745bb.xhtml">Chapter 1</a>, <em>Understanding Rewards-Based Learning</em>, the agent still needs to evaluate a value function and improve on a policy. Hence, much of what we covered in <a href="8237fd36-1edf-4da0-b271-9a50c5b8deb3.xhtml">Chapter 2</a>, <em>Dynamic Programming and the Bellman Equation</em>, will be applicable. However, this time, instead of planning, our agent will explore the environment and then, after each episode, re-evaluate the value function and update the policy. An episode is defined as one complete set of moves from the start to termination. We call this type of learning episodic since it refers to the agent only learning and improving after an episode. This, of course, has its limitations and we will see how continuous control is done in <a href="bb05e528-e21b-4753-9e4c-372b8ed11e96.xhtml">Chapter 4</a>, <em>Temporal Difference Learning</em>. In the next section, we jump in and look at the code and how this all works.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Monte Carlo control</h1>
                
            
            
                
<p>There are two ways to implement what is called <strong>Monte Carlo control</strong> on an agent. The difference between the two is how they calculate the average return or sampled mean. In what is called <strong>First-Visit Monte Carlo</strong>, the agent only samples the mean the first time a state is visited. The other method, <strong>Every-Visit Monte Carlo</strong>, samples the average return every time a state is visited. The latter method is what we will explore in the code example for this chapter.  </p>
<p>The original source code for this example was from Ankit Choudhary's blog (<a href="https://www.analyticsvidhya.com/blog/2018/11/reinforcement-learning-introduction-monte-carlo-learning-openai-gym/">https://www.analyticsvidhya.com/blog/2018/11/reinforcement-learning-introduction-monte-carlo-learning-openai-gym/</a>).</p>
<p>The code has been heavily modified from the original. Ankit goes far more heavily into the mathematics of this method and the original is recommended for those readers interested in exploring more math.</p>
<p>Open up <kbd>Chapter_3_3.py</kbd> and follow the exercise:</p>
<ol>
<li>Open the code and review the imports. The code for this example is too large to place inline. Instead, the code has been broken into sections.</li>
<li>Scroll to the bottom of the sample and review the following lines:</li>
</ol>
<pre style="padding-left: 60px">env = gym.make('FrozenLake8x8-v0')<br/>policy = monte_carlo_e_soft(env,episodes=50000)print(test_policy(policy, env))</pre>
<ol start="3">
<li>In the first line, we construct the environment. Then, we create <kbd>policy</kbd> using a function called <kbd>monte_carlo_e_soft</kbd>. We complete this step by printing out the results from the <kbd>test_policy</kbd> function.</li>
<li>Scroll up to the <kbd>monte_carlo_e_soft</kbd> function. We will get to the name later but, for now, the top lines are shown:</li>
</ol>
<pre style="padding-left: 60px">if not policy:<br/>  policy = create_random_policy(env)<br/>Q = create_state_action_dictionary(env, policy)<br/>returns = {}</pre>
<ol start="5">
<li>These lines create a policy if there is none. This shows how the random policy is created:</li>
</ol>
<pre style="padding-left: 60px">def create_random_policy(env):<br/>  policy = {}<br/>  for key in range(0, env.observation_space.n):<br/>    p = {}<br/>    for action in range(0, env.action_space.n):<br/>      p[action] = 1 / env.action_space.n<br/>      policy[key] = p<br/>  return policy</pre>
<ol start="6">
<li>After that, we create a dictionary to store state and action values, as shown here:</li>
</ol>
<pre style="padding-left: 60px">def create_state_action_dictionary(env, policy):<br/>  Q = {}<br/>  for key in policy.keys():<br/>    Q[key] = {a: 0.0 for a in range(0, env.action_space.n)}<br/>  return Q</pre>
<ol start="7">
<li>Then, we start with a <kbd>for</kbd> loop that iterates through the number of episodes, like so:</li>
</ol>
<pre style="padding-left: 60px">for e in range(episodes): <br/>  G = 0 <br/>  episode = play_game(env=env, policy=policy, <strong>display=False</strong>)<br/>  evaluate_policy_check(env, e, policy, test_policy_freq)</pre>
<ol start="8">
<li>Change <kbd>display=False</kbd> as highlighted in the preceding to <kbd>display=True</kbd>, as shown here:</li>
</ol>
<pre style="padding-left: 60px">episode = play_game(env=env, policy=policy, <strong>display=True</strong>)</pre>
<ol start="9">
<li>Now, before we get too far ahead, it may be helpful to see how the agent is playing a game. Run the code example and watch the output. Don't run the code until completion—just for a few seconds or up to a minute is fine. Make sure to undo your code changes before saving:</li>
</ol>
<div><img class="aligncenter size-full wp-image-857 image-border" src="img/55657369-f70f-447b-abe6-ceb363afb5cd.png" style="width:22.92em;height:32.08em;"/><br/>
<br/>
Example output of agent playing the game</div>
<p>This screenshot shows an example of the agent exploring the expanded 8 x 8 FrozenLake environment. In the next section, we look at how the agent plays the game.</p>
<p>Again, make sure you undo your code and change <kbd>display=True</kbd> to <kbd>display=False</kbd> before proceeding.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Playing the FrozenLake game</h1>
                
            
            
                
<p>The agent code now plays or explores the environment and it is helpful if we understand how this code runs. Open up <kbd>Chapter_3_3.py</kbd> again and follow the exercise:</p>
<ol>
<li>All we need to focus on for this section is how the agent plays the game. Scroll down to the <kbd>play_game</kbd> function, as shown in the following:</li>
</ol>
<pre style="padding-left: 60px">def play_game(env, policy, display=True):<br/>  env.reset()<br/>  episode = []<br/>  finished = False<br/>  while not finished:<br/>    s = env.env.s<br/>    if display:<br/>      clear_output(True)<br/>      env.render()<br/>      sleep(1)<br/>    timestep = []<br/>    timestep.append(s)<br/>    n = random.uniform(0, sum(policy[s].values()))<br/>    top_range = 0<br/>    action = 0<br/>    for prob in policy[s].items():<br/>      top_range += prob[1]            <br/>      if n &lt; top_range:<br/>        action = prob[0]<br/>        break <br/>    state, reward, finished, info = env.step(action)<br/>            <br/>    timestep.append(action)<br/>    timestep.append(reward)<br/>    episode.append(timestep)<br/> if display:<br/>   clear_output(True)<br/>   env.render()<br/>   sleep(1)<br/> return episode</pre>
<ol start="2">
<li>We can see the function takes the <kbd>env</kbd> and <kbd>policy</kbd> environment as inputs. Then, inside, it resets the environments with <kbd>reset</kbd> and then initializes the variables. The start of the <kbd>while</kbd> loop is where the agent begins playing the game:</li>
</ol>
<pre style="padding-left: 60px">while not finished:</pre>
<ol start="3">
<li>For this environment, we are letting the agent play infinitely. That is, we are not limiting the number of steps the agent may take. However, for this environment, that is not a problem since it is quite likely the agent will fall into a hole. But, that is not always the case and we often need to limit the number of steps and agent takes in an environment. In many cases, that limit is set at <kbd>100</kbd>, for example.</li>
<li>Inside the <kbd>while</kbd> loop, we update the agent's state, <kbd>s</kbd>, then display the environment is <kbd>display=True</kbd>. After that, we set up a <kbd>timestep</kbd> list to hold that <kbd>state</kbd>, <kbd>action</kbd>, and <kbd>value</kbd>. Then, we append the state, <kbd>s</kbd>:</li>
</ol>
<pre style="padding-left: 60px">s = env.env.s<br/>if display:<br/> clear_output(True)<br/> env.render()<br/> sleep(1)<br/>timestep = []<br/>timestep.append(s)</pre>
<ol start="5">
<li>Next, we look at the code that does the random sampling of the action based on the <kbd>policy</kbd> values, as shown:</li>
</ol>
<pre style="padding-left: 60px">n = random.uniform(0, sum(policy[s].values())) top_range = 0<br/>action = 0<br/>for prob in policy[s].items():<br/>  top_range += prob[1] <br/>  if n &lt; top_range:<br/>    action = prob[0]<br/>    break </pre>
<ol start="6">
<li>This is essentially where the agent performs a uniform sampling of the policy with <kbd>random.uniform</kbd>, which is the Monte Carlo method. Uniform means that sampling is uniform across values and not skewed if it were from a normal or Gaussian method. After that, an action is selected in the <kbd>for</kbd> loop based on a randomly selected item in the policy. Keep in mind that, at the start, all actions may have an equal likelihood of <kbd>0.25</kbd> but later, as the agent learns policy items, it will learn to distribute accordingly as well.</li>
</ol>
<p>Monte Carlo methods use a variety of sampling distributions to determine randomness. So far, we have extensively used uniform distributions, but in most real-world environments, a normal or Gaussian sampling method is used.</p>
<ol start="7">
<li> Then, after a random action is chosen, the agent takes a step and records it. It already recorded <kbd>state</kbd> and it now appends <kbd>action</kbd> and <kbd>reward</kbd>. Then, it appends the <kbd>timestep</kbd> list to the <kbd>episode</kbd> list, as shown:</li>
</ol>
<pre style="padding-left: 60px">state, reward, finished, info = env.step(action) timestep.append(action)<br/>timestep.append(reward)<br/>episode.append(timestep)</pre>
<ol start="8">
<li>Finally, when the agent has <kbd>finished</kbd>, by finding the goal or dropping in a hole, it returns the list of steps in <kbd>episode</kbd>.</li>
</ol>
<p>Now, with our understanding of how the agent plays the game, we can move on evaluating the game and optimizing it for prediction and control.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Using prediction and control</h1>
                
            
            
                
<p>When we previously had a model, our algorithm could learn to plan and improve a policy offline. Now, with no model, our algorithm needs to become an agent and learn to explore and, while doing that, also learn and improve. This allows our agent to now learn effectively by trial and error. Let's jump back into the <kbd>Chapter_3_3.py</kbd> code example and follow the exercise:</p>
<ol>
<li>We will start right from where we left off and review the last couple of lines including the <kbd>play_game</kbd> function:</li>
</ol>
<pre style="padding-left: 60px">episode = play_game(env=env, policy=policy, display=False)<br/>evaluate_policy_check(env, e, policy, test_policy_freq)</pre>
<ol start="2">
<li>Inside <kbd>evaluate_policy_check</kbd>, we test to see whether the <kbd>test_policy_freq</kbd> number has been reached. If it has, we output the current progress of the agent. In reality, what we are evaluating is how well the current policy will run an agent. The <kbd>evaluate_policy_check</kbd> function calls <kbd>test_policy</kbd> to evaluate the current policy. The <kbd>test_policy</kbd> function is shown here:</li>
</ol>
<pre style="padding-left: 60px">def test_policy(policy, env):<br/>  wins = 0<br/>  r = 100<br/>  for i in range(r):<br/>    w = play_game(env, policy, display=False)[-1][-1]<br/>    if w == 1:<br/>      wins += 1<br/>  return wins / r</pre>
<ol start="3">
<li><kbd>test_policy</kbd> evaluates the current policy by running the <kbd>play_game</kbd> function and setting a new agent loose for several games set by <kbd>r = 100</kbd>. This provides a <kbd>wins</kbd> percentage, which is output to show the agent's progress.</li>
<li>Back to the main function, we step into a <kbd>for</kbd> loop that loops through the last episode of gameplay in reverse order, as shown here:</li>
</ol>
<pre style="padding-left: 60px">for i in reversed(range(0, len(episode))):<br/>  s_t, a_t, r_t = episode[i] <br/>  state_action = (s_t, a_t)<br/>  G += r_t</pre>
<ol start="5">
<li>Looping through the episode in reverse order allows us to use the last reward and apply it backward. Hence, if the agent received a negative reward, all actions would be affected negatively. The same is also true for a positive reward. We keep track of the total reward with the <kbd>G</kbd> variable.</li>
<li>Inside the last loop, we then check whether the state was already evaluated for this episode; if not, we find the list of returns and average them. From the averages, we can then determine the best action, <kbd>A_star</kbd>. This is shown in the code block:</li>
</ol>
<pre style="padding-left: 60px">if not state_action in [(x[0], x[1]) for x in episode[0:i]]:<br/>  if returns.get(state_action):<br/>    returns[state_action].append(G)<br/>  else:<br/>    returns[state_action] = [G] <br/>  <br/>  Q[s_t][a_t] = sum(returns[state_action]) / len(returns[state_action]) <br/>  Q_list = list(map(lambda x: x[1], Q[s_t].items())) <br/>  indices = [i for i, x in enumerate(Q_list) if x == max(Q_list)]<br/>  max_Q = random.choice(indices) <br/>A_star = max_Q</pre>
<ol start="7">
<li>A lot is going on in this block of code, so work through it slowly if you need to. The key takeaway is that all we are doing here is averaging returns or a state and then determining the most likely best action, according to Monte Carlo, within that state.</li>
</ol>
<ol start="8">
<li>Before we jump to the last section of code, run the example as you normally would. This should yield a similar output to the following:</li>
</ol>
<div><img class="aligncenter size-full wp-image-485 image-border" src="img/0a04c1ab-7c86-4a9d-9627-4b24ad950da8.png" style="width:42.58em;height:35.42em;"/><br/>
<br/>
Example output from Chapter_3_3.py</div>
<p>Notice how we can now visualize the agent's progress as it randomly explores. The percentage of wins you may see could be entirely different and in some cases, they may be much higher or lower. This is because the agent is randomly exploring. To evaluate an agent entirely, you would likely need to run the agent for more than 50,000 episodes. However, continually averaging a mean after a new sample is added over 50,000 iterations would be far too computationally expensive. Instead, we use another method called an incremental mean, which we will explore in the next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Incremental means</h1>
                
            
            
                
<p>An incremental or running mean allows us to keep an average for a list of numbers without having to remember the list. This, of course, has huge benefits when we need to keep a mean over 50,000, 1 million, or more episodes. Instead of updating the mean from a full list, for every episode, we hold one value that we incrementally update using the following equation:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/1b649433-ce9c-4696-b18d-907d8c43b717.png" style="width:14.42em;height:1.25em;"/></p>
<p>In the preceding equation, we have the following:</p>
<ul>
<li><sub><img class="fm-editor-equation" src="img/b53a38e6-94e1-43ce-97bb-f35bb4774602.png" style="width:2.42em;height:1.08em;"/></sub> = The current state value for the policy</li>
<li><sub><img class="fm-editor-equation" src="img/d5bc9aee-bd81-489b-8c1c-9a946f2bafb5.png" style="width:1.00em;height:0.83em;"/></sub> = Represents a discount rate</li>
<li><sub><img style="color: #333333;width:1.17em;height:1.00em;" class="fm-editor-equation" src="img/a74c2e17-9f50-4ee2-8ef8-a4cb4ea6b7c6.png"/></sub> = The current total return</li>
</ul>
<p>By applying this equation, we now have a method to update the policy and, coincidentally, we use a similar method in the full Q equation. However, we are not there yet and, instead, we update the value using the following algorithm:</p>
<div><img class="aligncenter size-full wp-image-486 image-border" src="img/dcfa8499-fad1-4c12-986d-7243dc20928d.png" style="width:27.92em;height:22.33em;"/><br/>
<br/>
The Monte Carlo ε-soft policy algorithm</div>
<p>The algorithm shows how the e-soft or epsilon soft version of the Monte Carlo algorithm works. Recall this is the second method we can use to define an agent with Monte Carlo. While the preceding algorithm may be especially scary, the part we are interested in is the last one, shown in this equation:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/3c6d6aad-729c-42c5-a6ad-d40d85fd9e15.png" style="width:20.50em;height:3.42em;"/></p>
<p>This becomes a more effective method for policy updates and is what is shown in the example. Open up <kbd>Chapter_3_3.py</kbd> and follow the exercise<em>:</em></p>
<ol>
<li>Scroll down to the following section of code:</li>
</ol>
<pre style="padding-left: 60px">for a in policy[s_t].items(): <br/>  if a[0] == A_star:<br/>    policy[s_t][a[0]] = 1 - epsilon + (epsilon / abs(sum(policy[s_t].values())))<br/>  else:<br/>    policy[s_t][a[0]] = (epsilong / abs(sum(policy[s_t].values())))</pre>
<ol start="2">
<li>It is in this last block of code that we incrementally update the policy to the best value, as shown here:</li>
</ol>
<pre style="padding-left: 60px">policy[s_t][a[0]] = 1 - alpha + (alpha / abs(sum(policy[s_t].values())))</pre>
<ol start="3">
<li>Or we assign it some base value, as shown in the following:</li>
</ol>
<pre style="padding-left: 60px">policy[s_t][a[0]] = (alpha / abs(sum(policy[s_t].values())))</pre>
<ol start="4">
<li>From here, we can run the example again and enjoy the output.</li>
</ol>
<p>Now that you understand the basics of the Monte Carlo method, you can move on to more sample exercises in the next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Exercises</h1>
                
            
            
                
<p>As always, the exercises in this section are here to improve your knowledge and understanding of the material. Please attempt to complete 1-3 of these exercises on your own:</p>
<ol>
<li>What other constants like π could we use Monte Carlo methods to calculate? Think of an experiment to calculate another constant we use.</li>
<li>Open the <kbd>Chapter_3_1.py</kbd> sample code and change the value of <kbd>n</kbd>, that is, the number of darts dropped. How does that affect the calculated value for π? Use higher or lower values for <kbd>n</kbd>.</li>
<li>When we calculated π, we assumed a uniform distribution of darts. However, in the real world, the darts would likely be distributed in a normal or Gaussian manner. How would this affect the Monte Carlo experiment?</li>
<li>Refer to sample <kbd>Chapter_3_2.py</kbd> and change the value of <kbd>n</kbd>. How does that affect plot generation? Are you able to fix it?</li>
<li>Open <kbd>Chapter_3_3.py</kbd> and change the number of test episodes to run in the <kbd>test_policy</kbd> function to a higher or lower value.</li>
<li>Open <kbd>Chapter_3_3.py</kbd> and increase the number of episodes that are used to train the agent. How does the agent's performance increase, if at all?</li>
<li>Open <kbd>Chapter_3_3.py</kbd> and change the value of alpha that is used to update the incremental mean of averages. How does that affect the agent's ability to learn?</li>
<li>Add the ability to visualize each policy test in a graph. See whether you can transfer the way we created the plots in example <kbd>Chapter_3_2.py</kbd>.</li>
<li>Since the code is fairly generic, test this code on another Gym environment. Start with the standard 4 x 4 FrozenLake environment and see how well it performs.</li>
<li>Think of ways in which the Monte Carlo method given in this example could be improved upon.</li>
</ol>
<p>These exercises do not take much additional time and they can make the world of difference to your understanding of the materials in this book. Please use them.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>In this chapter, we extended our exploration of RL and looked again at trial-and-error methods. In particular, we focused on how the Monte Carlo method could be used as a way of learning from experimenting. We first looked at an example experiment of the Monte Carlo method for calculating π. From there, we looked at how to visualize the output of this experiment with matplotlib. Then, we looked at a code example that showed how to use the Monte Carlo method to solve a version of the FrozenLake problem. Exploring the code example in detail, we uncovered how the agent played the game and, through that exploration, learned to improve a policy. Finally, we finished this chapter by understanding how the agent improves this policy using an incremental sample mean.</p>
<p>The Monte Carlo method is powerful but, as we learned, it requires episodic gameplay while, in the real world, a working agent needs to continuously learn as it controls. This form of learning is called temporal difference learning and is something we will explore in the next chapter.</p>


            

            
        
    </body></html>