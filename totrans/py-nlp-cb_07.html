<html><head></head><body>
		<div><h1 id="_idParaDest-173" class="chapter-number"><a id="_idTextAnchor177" class="calibre6 pcalibre pcalibre1"/>7</h1>
			<h1 id="_idParaDest-174" class="calibre7"><a id="_idTextAnchor178" class="calibre6 pcalibre pcalibre1"/>Visualizing Text Data</h1>
			<p class="calibre3">This chapter is dedicated to creating visualizations for the different aspects of NLP work, much of which we have done in previous chapters. Visualizations are important when working with NLP tasks, as they help us to easier see the big picture of the work accomplished.</p>
			<p class="calibre3">We will create different types of visualizations, including visualizations of grammar details, parts of speech, and topic models. After working through this chapter, you will be well equipped to create compelling images to show and explain the outputs of various NLP tasks.</p>
			<p class="calibre3">These are the recipes you will find in this chapter:</p>
			<ul class="calibre15">
				<li class="calibre14">Visualizing the dependency parse</li>
				<li class="calibre14">Visualizing parts of speech</li>
				<li class="calibre14">Visualizing NER</li>
				<li class="calibre14">Creating a confusion matrix plot</li>
				<li class="calibre14">Constructing word clouds</li>
				<li class="calibre14">Visualizing topics from Gensim</li>
				<li class="calibre14">Visualizing topics from BERTopic</li>
			</ul>
			<h1 id="_idParaDest-175" class="calibre7"><a id="_idTextAnchor179" class="calibre6 pcalibre pcalibre1"/>Technical requirements</h1>
			<p class="calibre3">We will use the following packages in this chapter: <code>spacy</code>, <code>matplotlib</code>, <code>wordcloud</code>, and <code>pyldavis</code>. They are part of the <code>poetry</code> environment and the <code>requirements.txt</code> file.</p>
			<p class="calibre3">We will be using two datasets in this chapter. The first is the BBC news dataset, located at <a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/data/bbc_train.json" class="calibre6 pcalibre pcalibre1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/data/bbc_train.json</a> and <a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/data/bbc_test.json" class="calibre6 pcalibre pcalibre1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/data/bbc_test.json</a>.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">This dataset is used in this book with permission from the researchers. The original paper associated with this dataset is as follows:</p>
			<p class="callout">Derek Greene and Pádraig Cunningham. “Practical Solutions to the Problem of Diagonal Dominance in Kernel Document Clustering,” in Proc. 23rd International Conference on Machine Learning (ICML’06), 2006.</p>
			<p class="callout">All rights, including copyright, in the text content of the original articles are owned by the BBC.</p>
			<p class="calibre3">The second is the Sherlock Holmes text, located at <a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/data/sherlock_holmes.txt" class="calibre6 pcalibre pcalibre1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/data/sherlock_holmes.txt</a>.</p>
			<h1 id="_idParaDest-176" class="calibre7"><a id="_idTextAnchor180" class="calibre6 pcalibre pcalibre1"/>Visualizing the dependency parse</h1>
			<p class="calibre3">In this <a id="_idIndexMarker394" class="calibre6 pcalibre pcalibre1"/>recipe, we will learn how to use the <code>displaCy</code> library and visualize the dependency parse. It shows us the grammatical relations between words in a piece of text, usually a sentence.</p>
			<p class="calibre3">Details about how to create a dependency parse can be found in <a href="B18411_02.xhtml#_idTextAnchor042" class="calibre6 pcalibre pcalibre1"><em class="italic">Chapter 2</em></a>, in the <em class="italic">Getting the dependency parse</em> recipe. We will create two visualizations, one for a short text and another for a long multi-sentence text.</p>
			<p class="calibre3">After working through this recipe, you will be able to create visualizations of grammatical structures with different options for formatting.</p>
			<h2 id="_idParaDest-177" class="calibre5"><a id="_idTextAnchor181" class="calibre6 pcalibre pcalibre1"/>Getting ready</h2>
			<p class="calibre3">The <code>displaCy</code> library is part of the <code>spacy</code> package. You need at least version 2.0.12 of the <code>spacy</code> package for <code>displaCy</code> to work. The version in the <code>poetry</code> environment and <code>requirements.txt</code> is 3.6.1.</p>
			<p class="calibre3">The notebook is located at <a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter07/7.1_dependency_parse.ipynb" class="calibre6 pcalibre pcalibre1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter07/7.1_dependency_parse.ipynb</a>.</p>
			<h2 id="_idParaDest-178" class="calibre5"><a id="_idTextAnchor182" class="calibre6 pcalibre pcalibre1"/>How to do it...</h2>
			<p class="calibre3">To visualize <a id="_idIndexMarker395" class="calibre6 pcalibre pcalibre1"/>the dependency parse, we will use the functionality of the <code>displaCy</code> package to first show one sentence, and then two sentences together:</p>
			<ol class="calibre13">
				<li class="calibre14">Import the necessary packages:<pre class="source-code">
import spacy
from spacy import displacy</pre></li>				<li class="calibre14">Run the language utilities file:<pre class="source-code">
%run -i "../util/lang_utils.ipynb"</pre></li>				<li class="calibre14">Define the input text and process it using the small model:<pre class="source-code">
input_text = "I shot an elephant in my pajamas."
doc = small_model(input_text)</pre></li>				<li class="calibre14">We will now define different visualization options. The <code>render</code> command, we provide these options as an argument. We set the <code>jupyter</code> parameter to <code>True</code> for the visualization to work correctly in the notebook. You can omit the argument for non-Jupyter visualizations. We set the <code>style</code> parameter to <code>'dep'</code>, as we would like to have a <a id="_idIndexMarker396" class="calibre6 pcalibre pcalibre1"/>dependency parse output. The output is a visual representation of the dependency parse:</p><pre class="source-code">
options = {"add_lemma": True,
        "compact": True,
        "color": "green",
        "collapse_punct": True,
        "arrow_spacing": 20,
        "bg": "#FFFFE6",
        "font": "Times",
        "distance": 120}
displacy.render(doc, style='dep', options=options, jupyter=True)</pre></li>			</ol>
			<p class="calibre3">The output is shown in <em class="italic">Figure 7</em><em class="italic">.1</em>.</p>
			<div><div><img src="img/B18411_07_01.jpg" alt="" role="presentation" class="calibre4"/>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 7.1 – Dependency parse visualization</p>
			<ol class="calibre13">
				<li class="calibre14">In this<a id="_idIndexMarker397" class="calibre6 pcalibre pcalibre1"/> step, we save the visualization to a file. We first import the <strong class="source-inline1">Path</strong> object from the <strong class="source-inline1">pathlib</strong> package. We then initialize a string with the path where we want to save the file and create a <strong class="source-inline1">Path</strong> object. We use the same <strong class="source-inline1">render</strong> command, this time saving the output in a variable and setting the <strong class="source-inline1">jupyter</strong> parameter to <strong class="source-inline1">False</strong>. We then use the <strong class="source-inline1">output_path</strong> object and write the output to the corresponding file:<pre class="source-code">
from pathlib import Path
path = "../data/dep_parse_viz.svg"
output_path = Path(path)
svg = displacy.render(doc, style="dep", jupyter=False)
output_path.open("w", encoding="utf-8").write(svg)</pre><p class="calibre3">This will create the dependency parse and save it at <code>../data/dep_parse_viz.svg</code>.</p></li>				<li class="calibre14">Now, let’s define a longer text and process it using the small model. This way, we will be able to see how <strong class="source-inline1">displaCy</strong> deals with longer texts:<pre class="source-code">
input_text_list = "I shot an elephant in my pajamas. I hate it 
    when elephants wear my pajamas."
doc = small_model(input_text_list)</pre></li>				<li class="calibre14">Here, we visualize the new text. This time, we have to input a list of sentences from the <a id="_idIndexMarker398" class="calibre6 pcalibre pcalibre1"/>processed <strong class="source-inline1">spacy</strong> object to indicate that there is more than one sentence:<pre class="source-code">
displacy.render(list(doc.sents), style='dep', options=options, 
    jupyter=True)</pre><p class="calibre3">The output should look like in <em class="italic">Figure 7</em><em class="italic">.2</em>. We see that the output for the second sentence starts on a new line.</p></li>			</ol>
			<div><div><img src="img/B18411_07_02.jpg" alt="" role="presentation" class="calibre4"/>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 7.2 – Several sentences dependency parse visualization</p>
			<h1 id="_idParaDest-179" class="calibre7"><a id="_idTextAnchor183" class="calibre6 pcalibre pcalibre1"/>Visualizing parts of speech</h1>
			<p class="calibre3">In this recipe, we<a id="_idIndexMarker399" class="calibre6 pcalibre pcalibre1"/> visualize part of speech counts. Specifically, we count the number of infinitives and past or present verbs in the book <em class="italic">The Adventures of Sherlock Holmes</em>. This can give us an idea about whether the text mostly talks about past or present events. We could imagine that similar tools could be used to evaluate the quality of a text; for example, a book with very few adjectives but many nouns would not work very well as a fiction book.</p>
			<p class="calibre3">After working through this recipe, you will be able to use the <code>matplotlib</code> package to create bar plots of different verb types, which are tagged using the <code>spacy</code> package.</p>
			<h2 id="_idParaDest-180" class="calibre5"><a id="_idTextAnchor184" class="calibre6 pcalibre pcalibre1"/>Getting ready</h2>
			<p class="calibre3">We will use the <code>spacy</code> package for text analysis and the <code>matplotlib</code> package to create the graph. They are part of the <code>poetry</code> environment and the <code>requirements.txt</code> file.</p>
			<p class="calibre3">The notebook is located at <a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter07/7.2_parts_of_speech.ipynb" class="calibre6 pcalibre pcalibre1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter07/7.2_parts_of_speech.ipynb</a>.</p>
			<h2 id="_idParaDest-181" class="calibre5"><a id="_idTextAnchor185" class="calibre6 pcalibre pcalibre1"/>How to do it...</h2>
			<p class="calibre3">We will create a function that will count the number of verbs by tense and plot each on a bar graph:</p>
			<ol class="calibre13">
				<li class="calibre14">Import the necessary packages:<pre class="source-code">
import spacy
import matplotlib.pyplot as plt</pre></li>				<li class="calibre14">Run the file and language utilities files. The language utilities notebook loads the <strong class="source-inline1">spacy</strong> model, and the file utilities notebook loads the <strong class="source-inline1">read_text_file</strong> function:<pre class="source-code">
%run -i "../util/lang_utils.ipynb"
%run -i "../util/file_utils.ipynb"</pre></li>				<li class="calibre14">Load the text of the Sherlock Holmes book:<pre class="source-code">
text_file = "../data/sherlock_holmes.txt"
text = read_text_file(text_file)</pre></li>				<li class="calibre14">Here, we <a id="_idIndexMarker400" class="calibre6 pcalibre pcalibre1"/>define the verb tag lists, one for present tense and one for past tense. We do not define another list, but use it in the next step, and that is the infinitive verb, which only has one tag, <strong class="source-inline1">VB</strong>. If you went through the <em class="italic">Part-of-speech tagging</em> recipe in <a href="B18411_01.xhtml#_idTextAnchor013" class="calibre6 pcalibre pcalibre1"><em class="italic">Chapter 1</em></a>, you will notice that the tags are different from the <strong class="source-inline1">spacy</strong> tags used there. These tags are more detailed and use the <strong class="source-inline1">tag_</strong> attribute instead of the <strong class="source-inline1">pos_</strong> attribute that is used in the simplified tagset:<pre class="source-code">
past_tags = ["VBD", "VBN"]
present_tags = ["VBG", "VBP", "VBZ"]</pre></li>				<li class="calibre14">In this step, we create the <strong class="source-inline1">visualize_verbs</strong> function. The input to the function is the text and the <strong class="source-inline1">spacy</strong> model. We check each token’s <strong class="source-inline1">tag_</strong> attribute and add the counts of present, past, and infinitive verbs to a dictionary. We then use the <strong class="source-inline1">pyplot</strong> interface to plot those counts in a bar graph. We use the <strong class="source-inline1">bar</strong> function to define the bar graph. The first argument lists the <em class="italic">x</em> coordinates of the bars. The next argument is a list of heights of the bars. We also set the <strong class="source-inline1">align</strong> parameter to “center” and provide the colors for the bars using the <strong class="source-inline1">color</strong> parameter. The <strong class="source-inline1">xticks</strong> function sets the labels for the <em class="italic">x</em> axis. Finally, we use the <strong class="source-inline1">show</strong> function to display the resulting plot:<pre class="source-code">
def visualize_verbs(text, nlp):
    doc = nlp(text)
    verb_dict = {"Inf":0, "Past":0, "Present":0}
    for token in doc:
        if (token.tag_ == "VB"):
            verb_dict["Inf"] = verb_dict["Inf"] + 1
        if (token.tag_ in past_tags):
            verb_dict["Past"] = verb_dict["Past"] + 1
        if (token.tag_ in present_tags):
            verb_dict["Present"] = verb_dict["Present"] + 1
    plt.bar(range(len(verb_dict)),
        list(verb_dict.values()), align='center',
        color=["red", "green", "blue"])
    plt.xticks(range(len(verb_dict)),
        list(verb_dict.keys()))
    plt.show()</pre></li>				<li class="calibre14">Run <a id="_idIndexMarker401" class="calibre6 pcalibre pcalibre1"/>the <strong class="source-inline1">visualize_verbs</strong> function on the text of the Sherlock Holmes book using the small <strong class="source-inline1">spacy</strong> model:<pre class="source-code">
visualize_verbs(text, small_model)</pre><p class="calibre3">This will create the graph in <em class="italic">Figure 7</em><em class="italic">.3</em>. We see that most of the verbs in the book are past tense, which makes sense for a novel. However, there is also a sizable number of present tense verbs, which could be part of direct speech.</p></li>			</ol>
			<div><div><img src="img/B18411_07_03.jpg" alt="" role="presentation" class="calibre4"/>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 7.3 – Infinitive, past, and present verbs in The Adventures of Sherlock Holmes</p>
			<h1 id="_idParaDest-182" class="calibre7"><a id="_idTextAnchor186" class="calibre6 pcalibre pcalibre1"/>Visualizing NER</h1>
			<p class="calibre3"><code>displacy</code> package to create compelling and easy-to-read images.</p>
			<p class="calibre3">After working through this recipe, you will be able to create visualizations of named entities in a text using different formatting options and save the results in a file.</p>
			<h2 id="_idParaDest-183" class="calibre5"><a id="_idTextAnchor187" class="calibre6 pcalibre pcalibre1"/>Getting ready</h2>
			<p class="calibre3">The <code>displaCy</code> library is part of the <code>spacy</code> package. You need at least version 2.0.12 of the <code>spacy</code> package for <code>displaCy</code> to work. The version in the <code>poetry</code> environment and <code>requirements.txt</code> file is 3.6.1.</p>
			<p class="calibre3">The notebook is located at <a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter07/7.3_ner.ipynb" class="calibre6 pcalibre pcalibre1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter07/7.3_ner.ipynb</a>.</p>
			<h2 id="_idParaDest-184" class="calibre5"><a id="_idTextAnchor188" class="calibre6 pcalibre pcalibre1"/>How to do it...</h2>
			<p class="calibre3">We <a id="_idIndexMarker403" class="calibre6 pcalibre pcalibre1"/>will use <code>spacy</code> to parse the sentence and then the <code>displacy</code> engine to visualize the named entities:</p>
			<ol class="calibre13">
				<li class="calibre14">Import both <strong class="source-inline1">spacy</strong> and <strong class="source-inline1">displacy</strong>:<pre class="source-code">
import spacy
from spacy import displacy</pre></li>				<li class="calibre14">Run the language utilities file:<pre class="source-code">
%run -i "../util/lang_utils.ipynb"</pre></li>				<li class="calibre14">Define the text to process:<pre class="source-code">
text = """iPhone 12: Apple makes jump to 5G
Apple has confirmed its iPhone 12 handsets will be its first to work on faster 5G networks.
The company has also extended the range to include a new "Mini" model that has a smaller 5.4in screen.
The US firm bucked a wider industry downturn by increasing its handset sales over the past year.
But some experts say the new features give Apple its best opportunity for growth since 2014, when it revamped its line-up with the iPhone 6.
"5G will bring a new level of performance for downloads and uploads, higher quality video streaming, more responsive gaming,
real-time interactivity and so much more," said chief executive Tim Cook.
There has also been a cosmetic refresh this time round, with the sides of the devices getting sharper, flatter edges.
The higher-end iPhone 12 Pro models also get bigger screens than before and a new sensor to help with low-light photography.
However, for the first time none of the devices will be bundled with headphones or a charger."""</pre></li>				<li class="calibre14">In this<a id="_idIndexMarker404" class="calibre6 pcalibre pcalibre1"/> step, we process the text using the small model. This gives us a <strong class="source-inline1">Doc</strong> object. We then modify the object to contain a title. This title will be part of the NER visualization:<pre class="source-code">
doc = small_model(text)
doc.user_data["title"] = "iPhone 12: Apple makes jump to 5G"</pre></li>				<li class="calibre14">Here, we set up color options for the visualization display. We set green for the <strong class="source-inline1">ORG</strong>-labeled text and yellow for the <strong class="source-inline1">PERSON</strong>-labeled text. We then set the <strong class="source-inline1">options</strong> variable, which contains the colors. Finally, we use the <strong class="source-inline1">render</strong> command to display the visualization. As arguments, we provide the <strong class="source-inline1">Doc</strong> object and the options we previously defined. We also set the <strong class="source-inline1">style</strong> argument to <strong class="source-inline1">"ent"</strong>, as we would like to display just entities. We set the <strong class="source-inline1">jupyter</strong> argument to <strong class="source-inline1">True</strong> in order to display directly in the notebook:<pre class="source-code">
colors = {"ORG": "green", "PERSON":"yellow"}
options = {"colors": colors}
displacy.render(doc, style='ent', options=options, jupyter=True)</pre><p class="calibre3">The output should look like that in <em class="italic">Figure 7</em><em class="italic">.4</em>.</p></li>			</ol>
			<div><div><img src="img/B18411_07_04.jpg" alt="" role="presentation" class="calibre4"/>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 7.4 – Named entities visualization</p>
			<ol class="calibre13">
				<li value="6" class="calibre14">Now we <a id="_idIndexMarker405" class="calibre6 pcalibre pcalibre1"/>save the visualization to an HTML file. We first define the <strong class="source-inline1">path</strong> variable. Then, we use the same <strong class="source-inline1">render</strong> command, but we set the <strong class="source-inline1">jupyter</strong> argument to <strong class="source-inline1">False</strong> this time and assign the output of the command to the <strong class="source-inline1">html</strong> variable. We then open the file, write the HTML, and close the file:<pre class="source-code">
path = "../data/ner_vis.html"
html = displacy.render(doc, style="ent",
    options=options, jupyter=False)
html_file= open(path, "w", encoding="utf-8")
html_file.write(html)
html_file.close()</pre><p class="calibre3">This will create an HTML file with the entities visualization.</p></li>			</ol>
			<h1 id="_idParaDest-185" class="calibre7"><a id="_idTextAnchor189" class="calibre6 pcalibre pcalibre1"/>Creating a confusion matrix plot</h1>
			<p class="calibre3">When<a id="_idIndexMarker406" class="calibre6 pcalibre pcalibre1"/> working with machine learning models, for example, NLP classification models, creating a confusion matrix plot can be a very good tool to see the mistakes that the model makes to then further refine it. The model “confuses” one class for another, hence the name <strong class="bold">confusion matrix</strong>.</p>
			<p class="calibre3">After working through this recipe, you will be able to create an SVM model, evaluate it, and then create a confusion matrix visualization that will tell you in detail which mistakes the model makes.</p>
			<h2 id="_idParaDest-186" class="calibre5"><a id="_idTextAnchor190" class="calibre6 pcalibre pcalibre1"/>Getting ready</h2>
			<p class="calibre3">We will create <a id="_idIndexMarker407" class="calibre6 pcalibre pcalibre1"/>an SVM classifier for the BBC news dataset using the sentence transformer model as the vectorizer. We will then use the <code>ConfusionMatrixDisplay</code> object to create a more informative confusion matrix. The classifier is the same as in the <a href="B18411_04.xhtml#_idTextAnchor106" class="calibre6 pcalibre pcalibre1"><em class="italic">Chapter 4</em></a> recipe <em class="italic">Using SVMs for supervised </em><em class="italic">text classification</em>.</p>
			<p class="calibre3">The dataset is located at <a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/data/bbc_train.json" class="calibre6 pcalibre pcalibre1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/data/bbc_train.json</a> and <a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/data/bbc_test.json" class="calibre6 pcalibre pcalibre1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/data/bbc_test.json</a>.</p>
			<p class="calibre3">The notebook is located at <a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter07/7.4_confusion_matrix.ipynb" class="calibre6 pcalibre pcalibre1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter07/7.4_confusion_matrix.ipynb</a>.</p>
			<h2 id="_idParaDest-187" class="calibre5"><a id="_idTextAnchor191" class="calibre6 pcalibre pcalibre1"/>How to do it...</h2>
			<ol class="calibre13">
				<li class="calibre14">Import the necessary packages and functions:<pre class="source-code">
from sklearn.svm import SVC
from sentence_transformers import SentenceTransformer
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
from sklearn.metrics import ConfusionMatrixDisplay</pre></li>				<li class="calibre14">Run the simple classifier utilities file:<pre class="source-code">
%run -i "../util/util_simple_classifier.ipynb"</pre></li>				<li class="calibre14">Read in the training and test data and shuffle the training data. We shuffle the data so that there are no long sequences of one class, which might either bias the model during training or exclude large chunks of some classes:<pre class="source-code">
train_df = pd.read_json("../data/bbc_train.json")
test_df = pd.read_json("../data/bbc_test.json")
train_df.sample(frac=1)</pre></li>				<li class="calibre14">In this <a id="_idIndexMarker408" class="calibre6 pcalibre pcalibre1"/>step, we load the transformer model and create the <strong class="source-inline1">get_sentence_vector</strong> function. The function takes as arguments the text and the model, then creates and returns the vector. The <strong class="source-inline1">encode</strong> method takes in a list of text, so in order to encode one piece of text, we need to put it into a list, and then get the first element of the <strong class="source-inline1">return</strong> object, since the model also returns a list of encoding vectors:<pre class="source-code">
model = SentenceTransformer('all-MiniLM-L6-v2')
def get_sentence_vector(text, model):
    sentence_embeddings = model.encode([text])
    return sentence_embeddings[0]</pre></li>				<li class="calibre14">Here, we create the <strong class="source-inline1">train_classifier</strong> function. The function takes in vectorized input and the correct answers. It then creates and trains an SVC object and returns it. It could take a few minutes to finish training:<pre class="source-code">
def train_classifier(X_train, y_train):
    clf = SVC(C=0.1, kernel='rbf')
    clf = clf.fit(X_train, y_train)
    return clf</pre></li>				<li class="calibre14">In this step, we train and test the classifier. First, we create a list with the target labels. We then create a <strong class="source-inline1">vectorize</strong> function that uses the <strong class="source-inline1">get_sentence_vector</strong> function but specifies the model to use. We then use the <strong class="source-inline1">create_train_test_data</strong> function from the simple classifier utilities file to get the vectorized input and labels for both the training and test sets. This function takes in the training and test dataframes, the vectorizing method, and<a id="_idIndexMarker409" class="calibre6 pcalibre pcalibre1"/> the name of the column where the text is located. The results are the vectorized training and test data and the true labels for both. Then, we use the <strong class="source-inline1">train_classifier</strong> function to create a trained SVM classifier. We print the classification report for the training data and use the <strong class="source-inline1">test_classifier</strong> function to print the classification report for the test data:<pre class="source-code">
target_names=["tech", "business", "sport",
    "entertainment", "politics"]
vectorize = lambda x: get_sentence_vector(x, model)
(X_train, X_test, y_train, y_test) = create_train_test_data(
    train_df, test_df, vectorize,
    column_name="text_clean")
clf = train_classifier(X_train, y_train)
print(classification_report(train_df["label"],
        y_train, target_names=target_names))
test_classifier(test_df, clf, target_names=target_names)</pre><p class="calibre3">The output should be as follows:</p><pre class="source-code">               precision    recall  f1-score   support
         tech       1.00      1.00      1.00       321
     business       1.00      1.00      1.00       408
        sport       1.00      1.00      1.00       409
entertainment       1.00      1.00      1.00       309
     politics       1.00      1.00      1.00       333
     accuracy                           1.00      1780
    macro avg       1.00      1.00      1.00      1780
 weighted avg       1.00      1.00      1.00      1780
               precision    recall  f1-score   support
         tech       0.97      0.95      0.96        80
     business       0.98      0.97      0.98       102
        sport       0.98      1.00      0.99       102
entertainment       0.96      0.99      0.97        77
     politics       0.98      0.96      0.97        84
     accuracy                           0.98       445
    macro avg       0.97      0.97      0.97       445
 weighted avg       0.98      0.98      0.98       445</pre></li>				<li class="calibre14">Now, we<a id="_idIndexMarker410" class="calibre6 pcalibre pcalibre1"/> create a mapping from number labels to text labels and then create a new column in the test dataframe that shows the text label prediction:<pre class="source-code">
num_to_text_mapping = {0:"tech", 1:"business",
    2:"sport", 3:"entertainment", 4:"politics"}
test_df["pred_label"] = test_df["prediction"].apply(
    lambda x: num_to_text_mapping[x])</pre></li>				<li class="calibre14">In this step, we create a confusion matrix using the <strong class="source-inline1">sklearn</strong> <strong class="source-inline1">confusion_matrix</strong> function. The function takes as input the true labels, the predictions, and the names of the categories. We then create a <strong class="source-inline1">ConfusionMatrixDisplay</strong> object that takes in that confusion matrix and the <a id="_idIndexMarker411" class="calibre6 pcalibre pcalibre1"/>names to display. We then create the confusion matrix plot using the object and display it using the <strong class="source-inline1">matplotlib</strong> library:<pre class="source-code">
cm = confusion_matrix(
    test_df["label_text"],
    test_df["pred_label"], labels=target_names)
disp = ConfusionMatrixDisplay(
    confusion_matrix=cm,
    display_labels=target_names)
disp.plot()
plt.show()</pre><p class="calibre3">The result is shown in <em class="italic">Figure 7</em><em class="italic">.5</em>. The resulting plot clearly shows which classes have overlaps and their number. For example, it is easy to see that there are two examples that are predicted to be about business but are actually about politics.</p></li>			</ol>
			<div><div><img src="img/B18411_07_05.jpg" alt="" role="presentation" class="calibre4"/>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 7.5 – Confusion matrix visualization</p>
			<h1 id="_idParaDest-188" class="calibre7"><a id="_idTextAnchor192" class="calibre6 pcalibre pcalibre1"/>Constructing word clouds</h1>
			<p class="calibre3">Word clouds are<a id="_idIndexMarker412" class="calibre6 pcalibre pcalibre1"/> a nice visualization tool to quickly see topics that are prevalent in a text. They can be used at the preliminary data analysis stage and for illustration purposes. A distinguishing feature of word clouds is that larger-font words signify a more frequent topic, while smaller-font words signify less frequent topics.</p>
			<p class="calibre3">After working<a id="_idIndexMarker413" class="calibre6 pcalibre pcalibre1"/> through this recipe, you will be able to create word clouds from a text and also apply a picture mask on top of the word cloud, which makes for a cool image.</p>
			<p class="calibre3">We will use the text of the book <em class="italic">The Adventures of Sherlock Holmes</em> and the picture mask we will use is a silhouette of Sherlock Holmes’ head.</p>
			<h2 id="_idParaDest-189" class="calibre5"><a id="_idTextAnchor193" class="calibre6 pcalibre pcalibre1"/>Getting ready</h2>
			<p class="calibre3">We will use the <code>wordcloud</code> package for this recipe. In order to display the image, we need the <code>matplotlib</code> package as well. They are both part of the <code>poetry</code> environment and the <code>requirements.txt</code> file.</p>
			<p class="calibre3">The notebook is located at <a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter07/7.5_word_clouds.ipynb" class="calibre6 pcalibre pcalibre1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter07/7.5_word_clouds.ipynb</a>.</p>
			<h2 id="_idParaDest-190" class="calibre5"><a id="_idTextAnchor194" class="calibre6 pcalibre pcalibre1"/>How to do it...</h2>
			<ol class="calibre13">
				<li class="calibre14">Import <a id="_idIndexMarker414" class="calibre6 pcalibre pcalibre1"/>the necessary packages and functions:<pre class="source-code">
import matplotlib.pyplot as plt
from wordcloud import WordCloud, STOPWORDS</pre></li>				<li class="calibre14">Run the file utilities notebook. We will use the <strong class="source-inline1">read_text_file</strong> function from this notebook:<pre class="source-code">
%run -i "../util/file_utils.ipynb"</pre></li>				<li class="calibre14">Read in the book text:<pre class="source-code">
text_file = "../data/sherlock_holmes.txt"
text = read_text_file(text_file)</pre></li>				<li class="calibre14">In this step, we define the <strong class="source-inline1">create_wordcloud</strong> function. The function takes as arguments the text to be processed, stopwords, the filename of where to save the result, and whether to apply a mask over the image (<strong class="source-inline1">None</strong> by default). It creates the <strong class="source-inline1">WordCloud</strong> object, saves it to the file, and then outputs the resulting plot. The options that we provide to the <strong class="source-inline1">WordCloud</strong> object are the minimum font size, the maximum font size, the width and height, the maximum number of words, and the background color:<pre class="source-code">
def create_wordcloud(text, stopwords, filename, 
    apply_mask=None):
    if (apply_mask is not None):
        wordcloud = WordCloud(
            background_color="white", max_words=2000,
            mask=apply_mask, stopwords=stopwords,
            min_font_size=10, max_font_size=100)
        wordcloud.generate(text)
        wordcloud.to_file(filename)
        plt.figure()
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis("off")
        plt.show()
    else:
        wordcloud = WordCloud(min_font_size=10,
            max_font_size=100, stopwords=stopwords,
            width=1000, height=1000, max_words=1000,
            background_color="white").generate(text)
        wordcloud.to_file(filename)
        plt.figure()
        plt.imshow(wordcloud, interpolation="bilinear")
        plt.axis("off")
        plt.show()</pre></li>				<li class="calibre14">Run<a id="_idIndexMarker415" class="calibre6 pcalibre pcalibre1"/> the <strong class="source-inline1">create_wordcloud</strong> function on the text of the Sherlock Holmes book:<pre class="source-code">
create_wordcloud(text, set(STOPWORDS), 
    "../data/sherlock_wc.png")</pre></li>			</ol>
			<p class="img---caption" lang="en-US" xml:lang="en-US">This will save the result in the file located at <code>data/sherlock_wc.png</code> and create the visualization displayed in <em class="italic">Figure 7.6</em> (your results might look slightly different).</p>
			<div><div><img src="img/B18411_07_06.jpg" alt="" role="presentation" class="calibre4"/>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 7.6 – Sherlock Holmes word cloud visualization</p>
			<h2 id="_idParaDest-191" class="calibre5"><a id="_idTextAnchor195" class="calibre6 pcalibre pcalibre1"/>There’s more...</h2>
			<p class="calibre3">We can also <a id="_idIndexMarker416" class="calibre6 pcalibre pcalibre1"/>apply a mask to the word cloud. Here, we will apply a Sherlock Holmes silhouette to the word cloud:</p>
			<ol class="calibre13">
				<li class="calibre14">Do the additional imports:<pre class="source-code">
import numpy as np
from PIL import Image</pre></li>				<li class="calibre14">Read in the mask image and save it as a <strong class="source-inline1">numpy</strong> array:<pre class="source-code">
sherlock_data = Image.open("../data/sherlock.png")
sherlock_mask = np.array(sherlock_data)</pre></li>				<li class="calibre14">Run the function on the text of the Sherlock Holmes book:<pre class="source-code">
create_wordcloud(text, set(STOPWORDS),
    "../data/sherlock_mask.png",
    apply_mask=sherlock_mask)</pre></li>			</ol>
			<p class="calibre3">This will save <a id="_idIndexMarker417" class="calibre6 pcalibre pcalibre1"/>the result in the file located at <code>data/sherlock_mask.png</code> and create the visualization shown in <em class="italic">Figure 7</em><em class="italic">.7</em> (your result might be slightly different):</p>
			<div><div><img src="img/B18411_07_07.jpg" alt="" role="presentation" class="calibre4"/>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 7.7 – Word cloud with mask</p>
			<h2 id="_idParaDest-192" class="calibre5"><a id="_idTextAnchor196" class="calibre6 pcalibre pcalibre1"/>See also</h2>
			<p class="calibre3">Please <a id="_idIndexMarker418" class="calibre6 pcalibre pcalibre1"/>see the <code>wordcloud</code> docs, <a href="https://amueller.github.io/word_cloud/" class="calibre6 pcalibre pcalibre1">https://amueller.github.io/word_cloud/</a>, for more options.</p>
			<h1 id="_idParaDest-193" class="calibre7"><a id="_idTextAnchor197" class="calibre6 pcalibre pcalibre1"/>Visualizing topics from Gensim</h1>
			<p class="calibre3">In<a id="_idIndexMarker419" class="calibre6 pcalibre pcalibre1"/> this recipe, we will visualize the <strong class="bold">Latent Dirichlet Allocation</strong> (<strong class="bold">LDA</strong>) topic model that we created in <a href="B18411_06.xhtml#_idTextAnchor156" class="calibre6 pcalibre pcalibre1"><em class="italic">Chapter 6</em></a>. The visualization will allow us to quickly see words that are most relevant to a topic and the distances<a id="_idIndexMarker420" class="calibre6 pcalibre pcalibre1"/> between topics.</p>
			<p class="calibre3">After working through this recipe, you will be able to load an existing LDA model and create a visualization for its topics, both in Jupyter and saved as an HTML file.</p>
			<h2 id="_idParaDest-194" class="calibre5"><a id="_idTextAnchor198" class="calibre6 pcalibre pcalibre1"/>Getting ready</h2>
			<p class="calibre3">We will use the <code>pyLDAvis</code> package to create the visualization. It is available in the <code>poetry</code> environment and the <code>requirements.txt</code> file.</p>
			<p class="calibre3">We will load the model we created in <a href="B18411_06.xhtml#_idTextAnchor156" class="calibre6 pcalibre pcalibre1"><em class="italic">Chapter 6</em></a> and then use the <code>pyLDAvis</code> package to create the topic model visualization.</p>
			<p class="calibre3">The notebook is located at <a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter07/7.6_topics_gensim.ipynb" class="calibre6 pcalibre pcalibre1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter07/7.6_topics_gensim.ipynb</a>.</p>
			<h2 id="_idParaDest-195" class="calibre5"><a id="_idTextAnchor199" class="calibre6 pcalibre pcalibre1"/>How to do it...</h2>
			<ol class="calibre13">
				<li class="calibre14">Import the necessary packages and functions:<pre class="source-code">
import gensim
import pyLDAvis.gensim</pre></li>				<li class="calibre14">Define the paths to the model files. The model was trained in <a href="B18411_06.xhtml#_idTextAnchor156" class="calibre6 pcalibre pcalibre1"><em class="italic">Chapter 6</em></a>:<pre class="source-code">
model_path = "../models/bbc_gensim/lda.model"
dict_path = "../models/bbc_gensim/id2word.dict"
corpus_path = "../models/bbc_gensim/corpus.mm"</pre></li>				<li class="calibre14">In this step, we load the objects that these paths point to. If you get a <strong class="source-inline1">FileNotFoundError</strong> error at this step, it means that you have not created the dictionary, corpus, and model files. In that case, go back to <a href="B18411_06.xhtml#_idTextAnchor156" class="calibre6 pcalibre pcalibre1"><em class="italic">Chapter 6</em></a>, the <em class="italic">LDA topic modeling with Gensim</em> recipe, and create the model and the accompanying files:<pre class="source-code">
dictionary = gensim.corpora.Dictionary.load(dict_path)
corpus = gensim.corpora.MmCorpus(corpus_path)
lda = gensim.models.ldamodel.LdaModel.load(model_path)</pre></li>				<li class="calibre14">Here, we<a id="_idIndexMarker421" class="calibre6 pcalibre pcalibre1"/> create the <strong class="source-inline1">PreparedData</strong> object <a id="_idIndexMarker422" class="calibre6 pcalibre pcalibre1"/>using the preceding files and save the visualization as HTML. The object is required for the visualization methods:<pre class="source-code">
lda_prepared = pyLDAvis.gensim.prepare(lda, corpus, dictionary)
pyLDAvis.save_html(lda_prepared, '../data/lda-gensim.html')</pre></li>				<li class="calibre14">Here, we enable the Jupyter <strong class="source-inline1">display</strong> option and display the visualization in the notebook. You will see the topics and the words that are important for each topic. To select a particular topic, hover over it with the mouse. You will see the most important words for each topic change while hovering over them:<pre class="source-code">
pyLDAvis.enable_notebook()
pyLDAvis.display(lda_prepared)</pre><p class="calibre3">This will create the visualization in <em class="italic">Figure 7</em><em class="italic">.8</em> (your results might vary):</p></li>			</ol>
			<div><div><img src="img/B18411_07_08.jpg" alt="" role="presentation" class="calibre4"/>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 7.8 – LDA model visualization</p>
			<h2 id="_idParaDest-196" class="calibre5"><a id="_idTextAnchor200" class="calibre6 pcalibre pcalibre1"/>See also</h2>
			<p class="calibre3">Using <code>pyLDAvis</code>, it is also possible to visualize models created using <code>sklearn</code>. See the package documentation for more information: <a href="https://github.com/bmabey/pyLDAvis" class="calibre6 pcalibre pcalibre1">https://github.com/bmabey/pyLDAvis</a>.</p>
			<h1 id="_idParaDest-197" class="calibre7"><a id="_idTextAnchor201" class="calibre6 pcalibre pcalibre1"/>Visualizing topics from BERTopic</h1>
			<p class="calibre3">In this<a id="_idIndexMarker423" class="calibre6 pcalibre pcalibre1"/> recipe, we <a id="_idIndexMarker424" class="calibre6 pcalibre pcalibre1"/>will create and visualize a BERTopic model on the BBC data. There are several visualizations available with the BERTopic package, and we will use several of them.</p>
			<p class="calibre3">In this recipe, we will create a topic model in a similar fashion as in <a href="B18411_06.xhtml#_idTextAnchor156" class="calibre6 pcalibre pcalibre1"><em class="italic">Chapter 6</em></a>, in the <em class="italic">Topic modeling using BERTopic</em> recipe. However, unlike in <a href="B18411_06.xhtml#_idTextAnchor156" class="calibre6 pcalibre pcalibre1"><em class="italic">Chapter 6</em></a>, we will not limit the number<a id="_idIndexMarker425" class="calibre6 pcalibre pcalibre1"/> of topics created, and resulting in more <a id="_idIndexMarker426" class="calibre6 pcalibre pcalibre1"/>than the 5 original topics in the data. It will allow for more interesting visualizations.</p>
			<h2 id="_idParaDest-198" class="calibre5"><a id="_idTextAnchor202" class="calibre6 pcalibre pcalibre1"/>Getting ready</h2>
			<p class="calibre3">We will use the <code>BERTopic</code> package to create the visualization. It is available in the <code>poetry</code> environment.</p>
			<h2 id="_idParaDest-199" class="calibre5"><a id="_idTextAnchor203" class="calibre6 pcalibre pcalibre1"/>How to do it...</h2>
			<ol class="calibre13">
				<li class="calibre14">Import the necessary packages and functions:<pre class="source-code">
import pandas as pd
import numpy as np
from bertopic import BERTopic
from bertopic.representation import KeyBERTInspired</pre></li>				<li class="calibre14">Run the language utilities file:<pre class="source-code">
%run -i "../util/lang_utils.ipynb"</pre></li>				<li class="calibre14">Read in the data:<pre class="source-code">
bbc_df = pd.read_csv("../data/bbc-text.csv")</pre></li>				<li class="calibre14">Here, we <a id="_idIndexMarker427" class="calibre6 pcalibre pcalibre1"/>create a list of training documents from the dataframe object. We then initialize a representation model object. Here, we use the <strong class="source-inline1">KeyBERTInspired</strong> object, which uses BERT to extract the keywords.<p class="calibre3">This object creates the names (representations) for the topics; it does a better job than the default version, which contains lots of stopwords. We then create the main topic model object and fit it to the document set. In this recipe, in contrast to the <em class="italic">Topic modeling using BERTopic</em> recipe in <a href="B18411_06.xhtml#_idTextAnchor156" class="calibre6 pcalibre pcalibre1"><em class="italic">Chapter 6</em></a>, we do not place limits on the number of created topics. This will create a lot more topics:</p><pre class="source-code">
docs = bbc_df["text"].values
representation_model = KeyBERTInspired()
topic_model = BERTopic(
    representation_model=representation_model)
topics, probs = topic_model.fit_transform(docs)</pre></li>				<li class="calibre14">In this step, we <a id="_idIndexMarker428" class="calibre6 pcalibre pcalibre1"/>display the general topic <a id="_idIndexMarker429" class="calibre6 pcalibre pcalibre1"/>visualization. It shows all 42 topics created. If you hover on each circle, you will see the topic representation or name. The representations consist of the top five words in the topic:<pre class="source-code">
topic_model.visualize_topics()</pre><p class="calibre3">This will create the visualization in <em class="italic">Figure 7</em><em class="italic">.9</em> (your results might vary).</p></li>			</ol>
			<div><div><img src="img/B18411_07_09.jpg" alt="" role="presentation" class="calibre4"/>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 7.9 – BERTopic model visualization</p>
			<ol class="calibre13">
				<li value="6" class="calibre14">Here, we<a id="_idIndexMarker430" class="calibre6 pcalibre pcalibre1"/> create a visualization of the topic hierarchy. This<a id="_idIndexMarker431" class="calibre6 pcalibre pcalibre1"/> hierarchy clusters the different topics together if they are related. We first create the hierarchy by using the <strong class="source-inline1">hierarchical_topics</strong> function of the topic model object, and then pass it into the <strong class="source-inline1">visualize_hierarchy</strong> function. The nodes that combine the different topics have their own names that you can see if you hover over them:<pre class="source-code">
hierarchical_topics = topic_model.hierarchical_topics(
    bbc_df["text"])
topic_model.visualize_hierarchy(
    hierarchical_topics=hierarchical_topics)</pre><p class="calibre3">This will create the visualization in <em class="italic">Figure 7</em><em class="italic">.10</em>.</p></li>			</ol>
			<div><div><img src="img/B18411_07_10.jpg" alt="" role="presentation" class="calibre4"/>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 7.10 – BERTopic hierarchical visualization</p>
			<p class="calibre3">If you hover over the nodes, you will see their names.</p>
			<ol class="calibre13">
				<li value="7" class="calibre14">In this step, we <a id="_idIndexMarker432" class="calibre6 pcalibre pcalibre1"/>create a bar chart with the top words for the topics. We specify the <a id="_idIndexMarker433" class="calibre6 pcalibre pcalibre1"/>number of topics to show by using the <strong class="source-inline1">top_n_topics</strong> argument that the <strong class="source-inline1">visualize_barchart</strong> function of the topic model object takes:<pre class="source-code">
topic_model.visualize_barchart(top_n_topics=15)</pre><p class="calibre3">This will create a visualization similar to this:</p></li>			</ol>
			<div><div><img src="img/B18411_07_11.jpg" alt="" role="presentation" class="calibre4"/>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 7.11 – BERTopic word scores</p>
			<ol class="calibre13">
				<li value="8" class="calibre14">Here, we <a id="_idIndexMarker434" class="calibre6 pcalibre pcalibre1"/>create a visualization of individual <a id="_idIndexMarker435" class="calibre6 pcalibre pcalibre1"/>documents in the training set. We provide the list of documents created in <em class="italic">step 4</em> to the <strong class="source-inline1">visualize_documents</strong> function. It clusters the documents<a id="_idIndexMarker436" class="calibre6 pcalibre pcalibre1"/> to the topics. You can see the documents if you hover over the<a id="_idIndexMarker437" class="calibre6 pcalibre pcalibre1"/> individual circles:<pre class="source-code">
topic_model.visualize_documents(docs)</pre><p class="calibre3">The result will be a visualization similar to this:</p></li>			</ol>
			<div><div><img src="img/B18411_07_12.jpg" alt="" role="presentation" class="calibre4"/>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 7.12 – BERTopic document visualization</p>
			<p class="calibre3">If you hover over the nodes, you will see the text of the individual documents.</p>
			<h2 id="_idParaDest-200" class="calibre5"><a id="_idTextAnchor204" class="calibre6 pcalibre pcalibre1"/>See also</h2>
			<ul class="calibre15">
				<li class="calibre14">There are additional <a id="_idIndexMarker438" class="calibre6 pcalibre pcalibre1"/>visualization tools available through BERTopic. See the package documentation for more information: <a href="https://maartengr.github.io/BERTopic/index.html" class="calibre6 pcalibre pcalibre1">https://maartengr.github.io/BERTopic/index.html</a>.</li>
				<li class="calibre14">To learn<a id="_idIndexMarker439" class="calibre6 pcalibre pcalibre1"/> more about <strong class="source-inline1">KeyBERTInspired</strong>, see <a href="https://maartengr.github.io/BERTopic/api/representation/keybert.html" class="calibre6 pcalibre pcalibre1">https://maartengr.github.io/BERTopic/api/representation/keybert.html</a>.</li>
			</ul>
		</div>
	</body></html>