<html><head></head><body>
  <div id="_idContainer056" class="Basic-Text-Frame">
    <h1 class="chapterNumber">3</h1>
    <h1 id="_idParaDest-65" class="chapterTitle">Data Engineering</h1>
    <p class="normal">This chapter will begin exploring the LLM Twin project in more depth. We will learn how to design and implement the data collection pipeline to gather the raw data we will use in all our LLM use cases, such as fine-tuning or inference. As this is not a book on data engineering, we will keep this chapter short and focus only on what is strictly necessary to collect the required raw data. Starting with <em class="italic">Chapter 4</em>, we will concentrate on LLMs and GenAI, exploring its theory and concrete implementation details.</p>
    <p class="normal">When working on toy projects or doing research, you usually have a static dataset with which you work. But in our LLM Twin use case, we want to mimic a real-world scenario where we must gather and curate the data ourselves. Thus, implementing our data pipeline will connect the dots regarding how an end-to-end ML project works. This chapter will explore how to design and implement an <strong class="keyWord">Extract, Transform, Load</strong> (<strong class="keyWord">ETL</strong>) pipeline that<a id="_idIndexMarker139"/> crawls multiple social platforms, such as Medium, Substack, or GitHub, and aggregates the gathered data into a MongoDB data warehouse. We will show you how to implement various crawling methods, standardize the data, and load it into a data warehouse.</p>
    <p class="normal">We will begin by designing the LLM Twin’s data collection pipeline and explaining the architecture of the ETL pipeline. Afterward, we will move directly to implementing the pipeline, starting with ZenML, which will orchestrate the entire process. We will investigate the crawler implementation and understand how to implement a dispatcher layer that instantiates the right crawler class based on the domain of the provided link while following software best practices. Next, we will learn how to implement each crawler individually. Also, we will show you how to implement a data layer on top of MongoDB to structure all our documents and interact with the database.</p>
    <p class="normal">Finally, we will explore how to run the data collection pipeline using ZenML and query the collected data from MongoDB.</p>
    <p class="normal">Thus, in this chapter, we will study the following topics:</p>
    <ul>
      <li class="bulletList">Designing the LLM Twin’s data collection pipeline</li>
      <li class="bulletList">Implementing the LLM Twin’s data collection pipeline</li>
      <li class="bulletList">Gathering raw data into the data warehouse</li>
    </ul>
    <p class="normal">By the end of this chapter, you will know how to design and implement an ETL pipeline to extract, transform, and load raw data ready to be ingested into the ML application.</p>
    <h1 id="_idParaDest-66" class="heading-1">Designing the LLM Twin’s data collection pipeline</h1>
    <p class="normal">Before digging<a id="_idIndexMarker140"/> into the implementation, we must understand the LLM Twin’s data collection ETL architecture, illustrated in <em class="italic">Figure 3.1</em>. We must explore what platforms we will crawl to extract data from and how we will design our data structures and processes. However, the first step is understanding how our data collection pipeline maps to an ETL process.</p>
    <p class="normal">An ETL pipeline involves three fundamental steps:</p>
    <ol>
      <li class="numberedList" value="1">We <strong class="keyWord">extract</strong> data from<a id="_idIndexMarker141"/> various sources. We will crawl data from platforms like Medium, Substack, and GitHub to gather raw data.</li>
      <li class="numberedList">We <strong class="keyWord">transform</strong> this data by cleaning and standardizing it into a consistent format suitable for storage and analysis.</li>
      <li class="numberedList">We <strong class="keyWord">load</strong> the<a id="_idIndexMarker142"/> transformed data into a data warehouse or database.</li>
    </ol>
    <p class="normal">For our project, we use MongoDB as our NoSQL data warehouse. Although this is not a standard <a id="_idIndexMarker143"/>approach, we will explain the reasoning behind this choice shortly.</p>
    <figure class="mediaobject"><img src="../Images/B31105_03_01.png" alt=""/></figure>
    <p class="packt_figref">Figure 3.1: LLM Twin’s data collection ETL pipeline architecture</p>
    <p class="normal">We want to design an ETL pipeline that inputs a user and a list of links as input. Afterward, it crawls each link individually, standardizes the collected content, and saves it under that specific author in a MongoDB data warehouse. </p>
    <p class="normal">Hence, the signature of the data collection pipeline will look as follows:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Input:</strong> A list of links and their associated user (the author)</li>
      <li class="bulletList"><strong class="keyWord">Output:</strong> A list of raw documents stored in the NoSQL data warehouse</li>
    </ul>
    <p class="normal">We will use <code class="inlineCode">user</code> and <code class="inlineCode">author</code> interchangeably, as in most scenarios across the ETL pipeline, a user is the author of the extracted content. However, within the data warehouse, we have only a user collection.</p>
    <p class="normal">The ETL pipeline will <a id="_idIndexMarker144"/>detect the domain of each link, based on which it will call a specialized crawler. We implemented four different crawlers for three different data categories, as seen in <em class="italic">Figure 3.2</em>. First, we will explore the three fundamental data categories we will work with across the book. All our collected documents can be boiled down to an article, repository (or code), and post. It doesn’t matter where the data comes from. We are primarily interested in the document’s format. In most scenarios, we will have to process these data categories differently. Thus, we created a different domain entity for each, where each entity will have its class and collection in MongoDB. As we save the source URL within the document’s metadata, we will still know its source and can reference it in our GenAI use cases.</p>
    <figure class="mediaobject"><img src="../Images/B31105_03_02.png" alt=""/></figure>
    <p class="packt_figref">Figure 3.2: The relationship between the crawlers and the data categories</p>
    <p class="normal">Our codebase<a id="_idIndexMarker145"/> supports four different crawlers:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Medium crawler</strong>: Used to <a id="_idIndexMarker146"/>collect data from Medium. It outputs an article document. It logs in to Medium and crawls the HTML of the article’s link. Then, it extracts, cleans, and normalizes the text from the HTML and loads the standardized text of the article into the NoSQL data warehouse.</li>
      <li class="bulletList"><strong class="keyWord">Custom article crawler</strong>: It performs similar steps to the Medium crawler but is a more generic implementation for collecting articles from various sites. Thus, as it doesn’t implement any particularities of any platform, it doesn’t perform the login step and blindly gathers all the HTML from a particular link. This is enough for articles freely available online, which you can find on Substack and people’s blogs. We will use this crawler as a safety net when the link’s domain isn’t associated with the other supported crawlers. For example, when providing a Substack link, it will default to the custom article crawler, but when providing a Medium URL, it will use the Medium crawler.</li>
      <li class="bulletList"><strong class="keyWord">GitHub crawler</strong>:<strong class="keyWord"> </strong>This collects data from GitHub. It outputs a repository document. It clones the repository, parses the repository file tree, cleans and normalizes the files, and loads them to the database.</li>
      <li class="bulletList"><strong class="keyWord">LinkedIn crawler</strong>:<strong class="keyWord"> </strong>This is used to collect data from LinkedIn. It outputs multiple post <a id="_idIndexMarker147"/>documents. It logs in to LinkedIn, navigates to the user’s feed, and crawls all the user’s latest posts. For each post, it extracts its HTML, cleans and normalizes it, and loads it to MongoDB.</li>
    </ul>
    <p class="normal">In the next section, we will examine each crawler’s implementation in detail. For now, note that each <a id="_idIndexMarker148"/>crawler accesses a specific platform or site in a particular way and extracts HTML from it. Afterward, all the crawlers parse the HTML, extract the text from it, and clean and normalize it so it can be stored in the data warehouse under the same interface.</p>
    <p class="normal">By reducing all the collected data to three data categories and not creating a new data category for every new data source, we can easily extend this architecture to multiple data sources with minimal effort. For example, if we want to start collecting data from X, we only have to implement a new crawler that outputs a post document, and that’s it. The rest of the code will remain untouched. Otherwise, if we introduced the source dimension in the class and document structure, we would have to add code to all downstream layers to support any new data source. For example, we would have to implement a new document class for each new source and adapt the feature pipeline to support it.</p>
    <p class="normal">For our proof of concept, crawling a few hundred documents is enough, but if we want to scale it to a real-world product, we would probably need more data sources to crawl from. LLMs are data-hungry. Thus, you need thousands of documents for ideal results instead of just a few hundred. But in many projects, it’s an excellent strategy to implement an end-to-end project version that isn’t the most accurate and iterate through it later. Thus, by using this architecture, you can easily add more data sources in future iterations to gather a larger dataset. More on <a id="_idIndexMarker149"/>LLM fine-tuning and dataset size will be covered in the next chapter.</p>
    <p class="normal"><strong class="keyWord">How is the ETL process connected to the feature pipeline?</strong> The feature pipeline ingests the<a id="_idIndexMarker150"/> raw data from the MongoDB data warehouse, cleans it further, processes it into features, and stores it in the Qdrant vector DB to make it accessible for the LLM training and inference pipelines. <em class="italic">Chapter 4</em> provides more information on the feature pipeline. The ETL process is independent of the feature pipeline. The two pipelines communicate with each other strictly through the MongoDB data warehouse. Thus, the data collection pipeline can write data for MongoDB, and the feature pipeline can read from it independently and on different schedules.</p>
    <p class="normal"><strong class="keyWord">Why did we use MongoDB as a data warehouse? </strong>Using a transactional database, such <a id="_idIndexMarker151"/>as MongoDB, as a data warehouse is uncommon. However, in our use case, we are working with small amounts of data, which MongoDB can handle. Even if we plan to compute statistics on top of our MongoDB collections, it will work fine at the scale of our LLM Twin’s data (hundreds of documents). We picked MongoDB to store our raw data primarily because of the nature of our unstructured data: text crawled from the internet. By mainly working with unstructured text, selecting a NoSQL database that doesn’t enforce a schema made our development easier and faster. Also, MongoDB is stable and easy to use. Their Python SDK is intuitive. They provide a Docker image that works out of the box locally and a cloud freemium tier that is perfect for proofs of concept, such as the LLM Twin. Thus, we can freely work with it locally and in the cloud. However, when working with big data (millions of documents or more), using a dedicated data warehouse such <a id="_idIndexMarker152"/>as Snowflake or BigQuery will be ideal.</p>
    <p class="normal">Now that we’ve<a id="_idIndexMarker153"/> understood the architecture of the LLM Twin’s data collection pipeline, let’s move on to its implementation.</p>
    <h2 id="_idParaDest-67" class="heading-2">Implementing the LLM Twin’s data collection pipeline</h2>
    <p class="normal">As we<a id="_idIndexMarker154"/> presented in <em class="italic">Chapter 2</em>, the entry point to each pipeline from our LLM Twin project is a ZenML pipeline, which can be configured at runtime through YAML files and run through the ZenML ecosystem. Thus, let’s start by looking into the ZenML <code class="inlineCode">digital_data_etl</code> pipeline. You’ll notice that this is the same pipeline we used as an example in <em class="italic">Chapter 2</em> to illustrate ZenML. But this time, we will dig deeper into the implementation, explaining how the data collection works behind the scenes. After understanding how the pipeline works, we will explore the implementation of each crawler used to collect data from various sites and the MongoDB documents used to store and query data from the data warehouse.</p>
    <h2 id="_idParaDest-68" class="heading-2">ZenML pipeline and steps</h2>
    <p class="normal">In the code snippet<a id="_idIndexMarker155"/> below, we <a id="_idIndexMarker156"/>can see the implementation of the ZenML <code class="inlineCode">digital_data_etl</code> pipeline, which inputs the user’s full name and a list of links that will be crawled under that user (considered the author of the content extracted from those links). Within the function, we call two steps. In the first one, we look up the user in the database based on its full name. Then, we loop through all the links and crawl each independently. The pipeline’s implementation is available in our repository at <code class="inlineCode">pipelines/digital_data_etl.py</code>.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> zenml <span class="hljs-keyword">import</span> pipeline
<span class="hljs-keyword">from</span> steps.etl <span class="hljs-keyword">import</span> crawl_links, get_or_create_user
<span class="hljs-meta">@pipeline</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">digital_data_etl</span>(<span class="hljs-params">user_full_name: </span><span class="hljs-built_in">str</span><span class="hljs-params">, links: </span><span class="hljs-built_in">list</span><span class="hljs-params">[</span><span class="hljs-built_in">str</span><span class="hljs-params">]</span>) -&gt; <span class="hljs-built_in">str</span>:
    user = get_or_create_user(user_full_name)
    last_step = crawl_links(user=user, links=links)
    <span class="hljs-keyword">return</span> last_step.invocation_id
</code></pre>
    <p class="normal"><em class="italic">Figure 3.3</em> shows a run of the <code class="inlineCode">digital_data_etl</code> pipeline on the ZenML dashboard. The next phase is to explore the <code class="inlineCode">get_or_create_user</code> and <code class="inlineCode">crawl_links</code> ZenML steps<a id="_idIndexMarker157"/> individually. The <a id="_idIndexMarker158"/>step implementation is available in our repository at <code class="inlineCode">steps/etl</code>.</p>
    <figure class="mediaobject"><img src="../Images/B31105_03_03.png" alt=""/></figure>
    <p class="packt_figref">Figure 3.3: Example of a digital_data_etl pipeline run from ZenML’s dashboard</p>
    <p class="normal">We will start with the <code class="inlineCode">get_or_create_user</code> ZenML step. We begin by importing the necessary modules and functions used throughout the script.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> loguru <span class="hljs-keyword">import</span> logger
<span class="hljs-keyword">from</span> typing_extensions <span class="hljs-keyword">import</span> Annotated
<span class="hljs-keyword">from</span> zenml <span class="hljs-keyword">import</span> get_step_context, step
<span class="hljs-keyword">from</span> llm_engineering.application <span class="hljs-keyword">import</span> utils
<span class="hljs-keyword">from</span> llm_engineering.domain.documents <span class="hljs-keyword">import</span> UserDocument
</code></pre>
    <p class="normal">Next, we define the function’s signature, which takes a user’s full name as input and retrieves an <a id="_idIndexMarker159"/>existing user or<a id="_idIndexMarker160"/> creates a new one in the MongoDB database if it doesn’t exist:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-meta">@step</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">get_or_create_user</span>(<span class="hljs-params">user_full_name: </span><span class="hljs-built_in">str</span>) -&gt; Annotated[UserDocument, <span class="hljs-string">"user"</span>]:
</code></pre>
    <p class="normal">Using a utility function, we split the full name into first and last names. Then, we attempt to retrieve the user from the database or create a new one if it doesn’t exist. We also retrieve the current step context and add metadata about the user to the output, which will be reflected in the metadata of the <code class="inlineCode">user</code> ZenML output artifact:</p>
    <pre class="programlisting code"><code class="hljs-code">    logger.info(<span class="hljs-string">f"Getting or creating user: </span><span class="hljs-subst">{user_full_name}</span><span class="hljs-string">"</span>)
    first_name, last_name = utils.split_user_full_name(user_full_name)
    user = UserDocument.get_or_create(first_name=first_name, last_name=last_name)
    step_context = get_step_context()
    step_context.add_output_metadata(output_name=<span class="hljs-string">"user"</span>, metadata=_get_metadata(user_full_name, user))
    <span class="hljs-keyword">return</span> user
</code></pre>
    <p class="normal">Additionally, we define a helper function called <code class="inlineCode">_get_metadata()</code>, which builds a dictionary containing the query parameters and the retrieved user information, which will be added as metadata to the user artifact:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">_get_metadata</span>(<span class="hljs-params">user_full_name: </span><span class="hljs-built_in">str</span><span class="hljs-params">, user: UserDocument</span>) -&gt; <span class="hljs-built_in">dict</span>:
    <span class="hljs-keyword">return</span> {
        <span class="hljs-string">"query"</span>: {
            <span class="hljs-string">"user_full_name"</span>: user_full_name,
        },
        <span class="hljs-string">"retrieved"</span>: {
            <span class="hljs-string">"user_id"</span>: <span class="hljs-built_in">str</span>(user.<span class="hljs-built_in">id</span>),
            <span class="hljs-string">"first_name"</span>: user.first_name,
            <span class="hljs-string">"last_name"</span>: user.last_name,
        },
    }
</code></pre>
    <p class="normal">We will move on to the <code class="inlineCode">crawl_links</code> ZenML step, which collects the data from the provided links. The<a id="_idIndexMarker161"/> code begins <a id="_idIndexMarker162"/>by importing essential modules and libraries for web crawling:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> urllib.parse <span class="hljs-keyword">import</span> urlparse
<span class="hljs-keyword">from</span> loguru <span class="hljs-keyword">import</span> logger
<span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm
<span class="hljs-keyword">from</span> typing_extensions <span class="hljs-keyword">import</span> Annotated
<span class="hljs-keyword">from</span> zenml <span class="hljs-keyword">import</span> get_step_context, step
<span class="hljs-keyword">from</span> llm_engineering.application.crawlers.dispatcher <span class="hljs-keyword">import</span> CrawlerDispatcher
<span class="hljs-keyword">from</span> llm_engineering.domain.documents <span class="hljs-keyword">import</span> UserDocument
</code></pre>
    <p class="normal">Following the imports, the main function inputs a list of links written by a specific author. Within this function, a crawler dispatcher is initialized and configured to handle specific domains such as LinkedIn, Medium, and GitHub:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-meta">@step</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">crawl_links</span>(<span class="hljs-params">user: UserDocument, links: </span><span class="hljs-built_in">list</span><span class="hljs-params">[</span><span class="hljs-built_in">str</span><span class="hljs-params">]</span>) -&gt; Annotated[<span class="hljs-built_in">list</span>[<span class="hljs-built_in">str</span>], <span class="hljs-string">"crawled_links"</span>]:
    dispatcher = CrawlerDispatcher.build().register_linkedin().register_medium().register_github()
    logger.info(<span class="hljs-string">f"Starting to crawl </span><span class="hljs-subst">{</span><span class="hljs-built_in">len</span><span class="hljs-subst">(links)}</span><span class="hljs-string"> link(s)."</span>)
</code></pre>
    <p class="normal">The function initializes variables to store the output metadata and count successful crawls. It then iterates over each link. It attempts to crawl and extract data for each link, updating the count of successful crawls and accumulating metadata about each URL:</p>
    <pre class="programlisting code"><code class="hljs-code">    metadata = {}
    successfull_crawls = <span class="hljs-number">0</span>
    <span class="hljs-keyword">for</span> link <span class="hljs-keyword">in</span> tqdm(links):
        successfull_crawl, crawled_domain = _crawl_link(dispatcher, link, user)
        successfull_crawls += successfull_crawl
        metadata = _add_to_metadata(metadata, crawled_domain, successfull_crawl)
</code></pre>
    <p class="normal">After processing all <a id="_idIndexMarker163"/>links, the <a id="_idIndexMarker164"/>function attaches the accumulated metadata to the output artifact:</p>
    <pre class="programlisting code"><code class="hljs-code">    step_context = get_step_context()
    step_context.add_output_metadata(output_name=<span class="hljs-string">"crawled_links"</span>, metadata=metadata)
    logger.info(<span class="hljs-string">f"Successfully crawled </span><span class="hljs-subst">{successfull_crawls}</span><span class="hljs-string"> / </span><span class="hljs-subst">{</span><span class="hljs-built_in">len</span><span class="hljs-subst">(links)}</span><span class="hljs-string"> </span>
<span class="hljs-string">links."</span>)
    <span class="hljs-keyword">return</span> links
</code></pre>
    <p class="normal">The code includes a helper function that attempts to extract information from each link using the appropriate crawler based on the link’s domain. It handles any exceptions that may occur during extraction and returns a tuple indicating the crawl’s success and the link’s domain:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">_crawl_link</span>(<span class="hljs-params">dispatcher: CrawlerDispatcher, link: </span><span class="hljs-built_in">str</span><span class="hljs-params">, user: UserDocument</span>) -&gt; <span class="hljs-built_in">tuple</span>[<span class="hljs-built_in">bool</span>, <span class="hljs-built_in">str</span>]:
    crawler = dispatcher.get_crawler(link)
    crawler_domain = urlparse(link).netloc
    <span class="hljs-keyword">try</span>:
        crawler.extract(link=link, user=user)
        <span class="hljs-keyword">return</span> (<span class="hljs-literal">True</span>, crawler_domain)
    <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
        logger.error(<span class="hljs-string">f"An error occurred while crawling: </span><span class="hljs-subst">{e!s}</span><span class="hljs-string">"</span>)
        <span class="hljs-keyword">return</span> (<span class="hljs-literal">False</span>, crawler_domain)
</code></pre>
    <p class="normal">Another helper <a id="_idIndexMarker165"/>function is<a id="_idIndexMarker166"/> provided to update the metadata dictionary with the results of each crawl:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">_add_to_metadata</span>(<span class="hljs-params">metadata: </span><span class="hljs-built_in">dict</span><span class="hljs-params">, domain: </span><span class="hljs-built_in">str</span><span class="hljs-params">, successfull_crawl: </span><span class="hljs-built_in">bool</span>) -&gt; <span class="hljs-built_in">dict</span>:
    <span class="hljs-keyword">if</span> domain <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> metadata:
        metadata[domain] = {}
    metadata[domain][<span class="hljs-string">"successful"</span>] = metadata.get(domain, {}).get(<span class="hljs-string">"successful"</span>, <span class="hljs-number">0</span>) + successfull_crawl
    metadata[domain][<span class="hljs-string">"total"</span>] = metadata.get(domain, {}).get(<span class="hljs-string">"total"</span>, <span class="hljs-number">0</span>) + <span class="hljs-number">1</span>
    <span class="hljs-keyword">return</span> metadata
</code></pre>
    <p class="normal">As seen in the abovementioned <code class="inlineCode">_crawl_link()</code> function, the <code class="inlineCode">CrawlerDispatcher</code> class knows what<a id="_idIndexMarker167"/> crawler to initialize based on each link’s domain. The logic is then abstracted away under the crawler’s <code class="inlineCode">extract()</code> method. Let’s zoom in on the <code class="inlineCode">CrawlerDispatcher</code> class<a id="_idIndexMarker168"/> to <a id="_idIndexMarker169"/>understand how this works fully.</p>
    <h2 id="_idParaDest-69" class="heading-2">The dispatcher: How do you instantiate the right crawler?</h2>
    <p class="normal">The entry point to<a id="_idIndexMarker170"/> our crawling logic is<a id="_idIndexMarker171"/> the <code class="inlineCode">CrawlerDispatcher</code> class. As illustrated in <em class="italic">Figure 3.4</em>, the dispatcher acts as the intermediate layer between the provided links and the crawlers. It knows what crawler to associate with each URL.</p>
    <p class="normal">The <code class="inlineCode">CrawlerDispatcher</code> class knows how to extract the domain of each link and initialize the proper crawler that collects the data from that site. For example, if it detects the <a href="https://medium.com"><span class="url">https://medium.com</span></a> domain when providing a link to an article, it will build an instance of the <code class="inlineCode">MediumCrawler</code> used to crawl that particular platform. With that in mind, let’s explore the implementation of the <code class="inlineCode">CrawlerDispatcher</code> class.</p>
    <div class="note">
      <p class="normal">All the crawling logic is available in the GitHub repository at <code class="inlineCode">llm_engineering/application/crawlers</code>.</p>
    </div>
    <figure class="mediaobject"><img src="../Images/B31105_03_04.png" alt=""/></figure>
    <p class="packt_figref">Figure 3.4: The relationship between the provided links, the CrawlerDispatcher, and the crawlers</p>
    <p class="normal">We begin by<a id="_idIndexMarker172"/> importing the necessary Python modules<a id="_idIndexMarker173"/> for URL handling and regex, along with importing our crawler classes:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> re
<span class="hljs-keyword">from</span> urllib.parse <span class="hljs-keyword">import</span> urlparse
<span class="hljs-keyword">from</span> loguru <span class="hljs-keyword">import</span> logger
<span class="hljs-keyword">from</span> .base <span class="hljs-keyword">import</span> BaseCrawler
<span class="hljs-keyword">from</span> .custom_article <span class="hljs-keyword">import</span> CustomArticleCrawler
<span class="hljs-keyword">from</span> .github <span class="hljs-keyword">import</span> GithubCrawler
<span class="hljs-keyword">from</span> .linkedin <span class="hljs-keyword">import</span> LinkedInCrawler
<span class="hljs-keyword">from</span> .medium <span class="hljs-keyword">import</span> MediumCrawler
</code></pre>
    <p class="normal">The <code class="inlineCode">CrawlerDispatcher</code> class is defined to manage and dispatch appropriate crawler instances based on given URLs and their domains. Its constructor initializes a registry to store the registered crawlers.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">CrawlerDispatcher</span>:
    <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-literal">None</span>:
        self._crawlers = {}
</code></pre>
    <p class="normal">As we are using the builder creational pattern to instantiate and configure the dispatcher, we define a <code class="inlineCode">build()</code> class method that returns an instance of the dispatcher:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-meta">    @classmethod</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title">build</span>(<span class="hljs-params">cls</span>) -&gt; <span class="hljs-string">"CrawlerDispatcher"</span>:
        dispatcher = cls()
        <span class="hljs-keyword">return</span> dispatcher
</code></pre>
    <p class="normal">The dispatcher<a id="_idIndexMarker174"/> includes methods to register<a id="_idIndexMarker175"/> crawlers for specific platforms like Medium, LinkedIn, and GitHub. These methods use a generic <code class="inlineCode">register()</code> method under the hood to add each crawler to the registry. By returning self, we follow the builder creational pattern (more on the builder pattern: <a href="https://refactoring.guru/design-patterns/builder"><span class="url">https://refactoring.guru/design-patterns/builder</span></a>). We can chain multiple <code class="inlineCode">register_*()</code> methods when instantiating the dispatcher as follows: <code class="inlineCode">CrawlerDispatcher.build().register_linkedin().register_medium()</code>.</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">def</span> <span class="hljs-title">register_medium</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-string">"CrawlerDispatcher"</span>:
        self.register(<span class="hljs-string">"https://medium.com"</span>, MediumCrawler)
        <span class="hljs-keyword">return</span> self
    <span class="hljs-keyword">def</span> <span class="hljs-title">register_linkedin</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-string">"CrawlerDispatcher"</span>:
        self.register(<span class="hljs-string">"https://linkedin.com"</span>, LinkedInCrawler)
        <span class="hljs-keyword">return</span> self
    <span class="hljs-keyword">def</span> <span class="hljs-title">register_github</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-string">"CrawlerDispatcher"</span>:
        self.register(<span class="hljs-string">"https://github.com"</span>, GithubCrawler)
        <span class="hljs-keyword">return</span> self
</code></pre>
    <p class="normal">The generic <code class="inlineCode">register()</code> method normalizes each domain to ensure its format is consistent before it’s added as a key to the <code class="inlineCode">self._crawlers</code> registry of the dispatcher. This is a critical step, as we will use the key of the dictionary as the domain pattern to match future links with a crawler:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">def</span> <span class="hljs-title">register</span>(<span class="hljs-params">self, domain: </span><span class="hljs-built_in">str</span><span class="hljs-params">, crawler: </span><span class="hljs-built_in">type</span><span class="hljs-params">[BaseCrawler]</span>) -&gt; <span class="hljs-literal">None</span>:
        parsed_domain = urlparse(domain)
        domain = parsed_domain.netloc
        self._crawlers[<span class="hljs-string">r"https://(www\.)?{}/*"</span>.<span class="hljs-built_in">format</span>(re.escape(domain))] = crawler
</code></pre>
    <p class="normal">Finally, the <code class="inlineCode">get_crawler()</code> method <a id="_idIndexMarker176"/>determines the <a id="_idIndexMarker177"/>appropriate crawler for a given URL by matching it against the registered domains. If no match is found, it logs a warning and defaults to using the <code class="inlineCode">CustomArticleCrawler</code>.</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">def</span> <span class="hljs-title">get_crawler</span>(<span class="hljs-params">self, url: </span><span class="hljs-built_in">str</span>) -&gt; BaseCrawler:
        <span class="hljs-keyword">for</span> pattern, crawler <span class="hljs-keyword">in</span> self._crawlers.items():
            <span class="hljs-keyword">if</span> re.<span class="hljs-keyword">match</span>(pattern, url):
                <span class="hljs-keyword">return</span> crawler()
        <span class="hljs-keyword">else</span>:
            logger.warning(<span class="hljs-string">f"No crawler found for </span><span class="hljs-subst">{url}</span><span class="hljs-string">. Defaulting to CustomArticleCrawler."</span>)
            <span class="hljs-keyword">return</span> CustomArticleCrawler()
</code></pre>
    <p class="normal">The next step in understanding how the data collection pipeline works is analyzing <a id="_idIndexMarker178"/>each <a id="_idIndexMarker179"/>crawler individually.</p>
    <h2 id="_idParaDest-70" class="heading-2">The crawlers</h2>
    <p class="normal">Before exploring each <a id="_idIndexMarker180"/>crawler’s implementation, we <a id="_idIndexMarker181"/>must present their base class, which defines a unified interface for all the crawlers. As shown in <em class="italic">Figure 3.4</em>, we can implement the dispatcher layer because each crawler follows the same signature. Each class implements the <code class="inlineCode">extract()</code> method, allowing us to leverage OOP techniques such as polymorphism, where we can work with abstract objects without knowing their concrete subclass. For example, in the <code class="inlineCode">_crawl_link()</code> function from the ZenML steps, we had the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">crawler = dispatcher.get_crawler(link)
crawler.extract(link=link, user=user)
</code></pre>
    <p class="normal">Note how we called the <code class="inlineCode">extract()</code> method without caring about what specific type of crawler we instantiated. To conclude, working with abstract interfaces ensures core reusability and ease of extension.</p>
    <h3 id="_idParaDest-71" class="heading-3">Base classes</h3>
    <p class="normal">Now, let’s explore <a id="_idIndexMarker182"/>the <code class="inlineCode">BaseCrawler</code> interface, which<a id="_idIndexMarker183"/> can be found in the repository at <a href="https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/crawlers/base.py"><span class="url">https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/crawlers/base.py</span></a>.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> abc <span class="hljs-keyword">import</span> ABC, abstractmethod
<span class="hljs-keyword">class</span> <span class="hljs-title">BaseCrawler</span>(<span class="hljs-title">ABC</span>):
    model: <span class="hljs-built_in">type</span>[NoSQLBaseDocument]
<span class="hljs-meta">    @abstractmethod</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title">extract</span>(<span class="hljs-params">self, link: </span><span class="hljs-built_in">str</span><span class="hljs-params">, **kwargs</span>) -&gt; <span class="hljs-literal">None</span>: ...
</code></pre>
    <p class="normal">As mentioned above, the interface defines an <code class="inlineCode">extract()</code> method that takes as input a link. Also, it defines a model attribute at the class level that represents the data category document type used to save the extracted data into the MongoDB data warehouse. Doing so allows us to customize each subclass with different data categories while preserving the same attributes at the class level. We will soon explore the <code class="inlineCode">NoSQLBaseDocument</code> class when digging into the document entities.</p>
    <p class="normal">We also extend the <code class="inlineCode">BaseCrawler</code> class with a <code class="inlineCode">BaseSeleniumCrawler</code> class, which implements reusable functionality that uses Selenium to crawl various sites, such as Medium or LinkedIn. <strong class="keyWord">Selenium</strong> is a tool <a id="_idIndexMarker184"/>for automating web browsers. It’s used to interact with web pages programmatically (like logging into LinkedIn, navigating through profiles, etc.). </p>
    <p class="normal">Selenium can programmatically control various browsers such as Chrome, Firefox, or Brave. For these specific platforms, we need Selenium to manipulate the browser programmatically to log in and scroll through the newsfeed or article before being able to extract the entire HTML. For other sites, where <a id="_idIndexMarker185"/>we don’t have to go through the login step <a id="_idIndexMarker186"/>or can directly load the whole page, we can extract the HTML from a particular URL using more straightforward methods than Selenium.</p>
    <div class="note">
      <p class="normal">For the Selenium-based crawlers to work, you must install Chrome on your machine (or a Chromium-based browser such as Brave).</p>
    </div>
    <p class="normal">The code begins by setting up the necessary imports and configurations for web crawling using Selenium and the ChromeDriver initializer. The <code class="inlineCode">chromedriver_autoinstaller</code> ensures that the appropriate version of ChromeDriver is installed and added to the system path, maintaining compatibility with the installed version of your Google Chrome browser (or other Chromium-based browser). Selenium will use the ChromeDriver to communicate with the browser and open a headless session, where we can programmatically manipulate the browser to access various URLs, click on specific elements, such as buttons, or scroll through the newsfeed. Using the <code class="inlineCode">chromedriver_autoinstaller</code>, we ensure we always have the correct ChromeDriver version installed that matches our machine’s Chrome browser version.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> time
<span class="hljs-keyword">from</span> tempfile <span class="hljs-keyword">import</span> mkdtemp
<span class="hljs-keyword">import</span> chromedriver_autoinstaller
<span class="hljs-keyword">from</span> selenium <span class="hljs-keyword">import</span> webdriver
<span class="hljs-keyword">from</span> selenium.webdriver.chrome.options <span class="hljs-keyword">import</span> Options
<span class="hljs-keyword">from</span> llm_engineering.domain.documents <span class="hljs-keyword">import</span> NoSQLBaseDocument
<span class="hljs-comment"># Check if the current version of chromedriver exists</span>
<span class="hljs-comment"># and if it doesn't exist, download it automatically,</span>
<span class="hljs-comment"># then add chromedriver to path</span>
chromedriver_autoinstaller.install()
</code></pre>
    <p class="normal">Next, we define the <code class="inlineCode">BaseSeleniumCrawler</code> class for use cases where we need Selenium to collect the data, such as collecting data from Medium or LinkedIn. </p>
    <p class="normal">Its constructor initializes various Chrome options to optimize performance, enhance security, and ensure a headless browsing environment. These options disable unnecessary<a id="_idIndexMarker187"/> features like GPU rendering, extensions, and <a id="_idIndexMarker188"/>notifications, which can interfere with automated browsing. These are standard configurations when crawling in headless mode:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">BaseSeleniumCrawler</span>(BaseCrawler, ABC):
    <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, scroll_limit: </span><span class="hljs-built_in">int</span><span class="hljs-params"> = </span><span class="hljs-number">5</span>) -&gt; <span class="hljs-literal">None</span>:
        options = webdriver.ChromeOptions()
       
        options.add_argument(<span class="hljs-string">"--no-sandbox"</span>)
        options.add_argument(<span class="hljs-string">"</span><span class="hljs-string">--headless=new"</span>)
        options.add_argument(<span class="hljs-string">"--disable-dev-shm-usage"</span>)
        options.add_argument(<span class="hljs-string">"--log-level=3"</span>)
        options.add_argument(<span class="hljs-string">"--disable-popup-blocking"</span>)
        options.add_argument(<span class="hljs-string">"--disable-notifications"</span>)
        options.add_argument(<span class="hljs-string">"--disable-extensions"</span>)
        options.add_argument(<span class="hljs-string">"--disable-background-networking"</span>)
        options.add_argument(<span class="hljs-string">"--ignore-certificate-errors"</span>)
        options.add_argument(<span class="hljs-string">f"--user-data-dir=</span><span class="hljs-subst">{mkdtemp()}</span><span class="hljs-string">"</span>)
        options.add_argument(<span class="hljs-string">f"--data-path=</span><span class="hljs-subst">{mkdtemp()}</span><span class="hljs-string">"</span>)
        options.add_argument(<span class="hljs-string">f"--disk-cache-dir=</span><span class="hljs-subst">{mkdtemp()}</span><span class="hljs-string">"</span>)
        options.add_argument(<span class="hljs-string">"--remote-debugging-port=9226"</span>)
</code></pre>
    <p class="normal">After configuring the Chrome options, the code allows subclasses to set any additional driver options by calling the <code class="inlineCode">set_extra_driver_options()</code> method. It then initializes the scroll limit and creates a new instance of the Chrome driver with the specified options:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.set_extra_driver_options(options)
        self.scroll_limit = scroll_limit
        self.driver = webdriver.Chrome(
            options=options,
        )
</code></pre>
    <p class="normal">The <code class="inlineCode">BaseSeleniumCrawler</code> class includes placeholder methods for <code class="inlineCode">set_extra_driver_options()</code> and <code class="inlineCode">login()</code>, which subclasses can override to provide specific <a id="_idIndexMarker189"/>functionality. This ensures modularity, as <a id="_idIndexMarker190"/>every platform has a different login page with a different HTML structure:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">def</span> <span class="hljs-title">set_extra_driver_options</span>(<span class="hljs-params">self, options: Options</span>) -&gt; <span class="hljs-literal">None</span>:
        <span class="hljs-keyword">pass</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title">login</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-literal">None</span>:
        <span class="hljs-keyword">pass</span>
</code></pre>
    <p class="normal">Finally, the <code class="inlineCode">scroll_page()</code> method implements a scrolling mechanism to navigate through pages, such as LinkedIn, up to a specified scroll limit. It scrolls to the bottom of the page, waits for new content to load, and repeats the process until it reaches the end of the page or the scroll limit is exceeded. This method is essential for feeds where the content appears as the user scrolls:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">def</span> <span class="hljs-title">scroll_page</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-literal">None</span>:
        <span class="hljs-string">"""Scroll through the LinkedIn page based on the scroll limit."""</span>
        current_scroll = <span class="hljs-number">0</span>
        last_height = self.driver.execute_script(<span class="hljs-string">"return document.body.scrollHeight"</span>)
        <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:
            self.driver.execute_script(<span class="hljs-string">"window.scrollTo(0, document.body.scrollHeight);"</span>)
            time.sleep(<span class="hljs-number">5</span>)
            new_height = self.driver.execute_script(<span class="hljs-string">"return document.body.scrollHeight"</span>)
            <span class="hljs-keyword">if</span> new_height == last_height <span class="hljs-keyword">or</span> (self.scroll_limit <span class="hljs-keyword">and</span> current_scroll &gt;= self.scroll_limit):
                <span class="hljs-keyword">break</span>
            last_height = new_height
            current_scroll += <span class="hljs-number">1</span>
</code></pre>
    <p class="normal">We’ve understood what the base classes of our crawlers look like. Next, we will look into <a id="_idIndexMarker191"/>the<a id="_idIndexMarker192"/> implementation of the following specific crawlers:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">GitHubCrawler(BaseCrawler)</code></li>
      <li class="bulletList"><code class="inlineCode">CustomArticleCrawler(BaseCrawler)</code></li>
      <li class="bulletList"><code class="inlineCode">MediumCrawler(BaseSeleniumCrawler)</code>
        <div class="note">
          <p class="normal">You can find the implementation of the above crawlers in the GitHub repository at <a href="https://github.com/PacktPublishing/LLM-Engineers-Handbook/tree/main/llm_engineering/application/crawlers"><span class="url">https://github.com/PacktPublishing/LLM-Engineers-Handbook/tree/main /llm_engineering/application/crawlers</span></a>.</p>
        </div>
      </li>
    </ul>
    <h3 id="_idParaDest-72" class="heading-3">GitHubCrawler class</h3>
    <p class="normal">The <code class="inlineCode">GithubCrawler</code> class is designed<a id="_idIndexMarker193"/> to scrape GitHub<a id="_idIndexMarker194"/> repositories, extending the functionality of the <code class="inlineCode">BaseCrawler</code>. We don’t have to log in to GitHub through the browser, as we can leverage Git’s clone functionality. Thus, we don’t have to leverage any Selenium functionality. Upon initialization, it sets up a list of patterns to ignore standard files and directories found in GitHub repositories, such as <code class="inlineCode">.git</code>, <code class="inlineCode">.toml</code>, <code class="inlineCode">.lock</code>, and <code class="inlineCode">.png</code>, ensuring that unnecessary files are excluded from the scraping process:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">GithubCrawler</span>(<span class="hljs-title">BaseCrawler</span>):
    model = RepositoryDocument
    <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, ignore=(</span><span class="hljs-string">".git"</span><span class="hljs-params">, </span><span class="hljs-string">".toml"</span><span class="hljs-params">, </span><span class="hljs-string">".lock"</span><span class="hljs-params">, </span><span class="hljs-string">".png"</span><span class="hljs-params">)</span>) -&gt; <span class="hljs-literal">None</span>:
        <span class="hljs-built_in">super</span>().__init__()
        self._ignore = ignore
</code></pre>
    <p class="normal">Next, we implement the <code class="inlineCode">extract()</code> method, where the crawler first checks if the repository has already been processed and stored in the database. If it exists, it exits the method to prevent storing duplicates:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">extract</span>(<span class="hljs-params">self, link: </span><span class="hljs-built_in">str</span><span class="hljs-params">, **kwargs</span>) -&gt; <span class="hljs-literal">None</span>:
    old_model = self.model.find(link=link)
    <span class="hljs-keyword">if</span> old_model <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
        logger.info(<span class="hljs-string">f"Repository already exists in the database: </span><span class="hljs-subst">{link}</span><span class="hljs-string">"</span>)
        <span class="hljs-keyword">return</span>
</code></pre>
    <p class="normal">If the repository <a id="_idIndexMarker195"/>is new, the crawler extracts the repository <a id="_idIndexMarker196"/>name from the link. Then, it creates a temporary directory to clone the repository to ensure that the cloned repository is cleaned up from the local disk after it’s processed:</p>
    <pre class="programlisting code"><code class="hljs-code">    logger.info(<span class="hljs-string">f"Starting scrapping GitHub repository: </span><span class="hljs-subst">{link}</span><span class="hljs-string">"</span>)
    repo_name = link.rstrip(<span class="hljs-string">"/"</span>).split(<span class="hljs-string">"/"</span>)[-<span class="hljs-number">1</span>]
    local_temp = tempfile.mkdtemp()
</code></pre>
    <p class="normal">Within a try block, the crawler changes the current working directory to the <code class="inlineCode">temporary</code> directory and executes the <code class="inlineCode">git clone</code> command in a different process:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">try</span>:
        os.chdir(local_temp)
        subprocess.run([<span class="hljs-string">"git"</span>, <span class="hljs-string">"clone"</span>, link])
</code></pre>
    <p class="normal">After successfully cloning the repository, the crawler constructs the path to the cloned repository. It initializes an empty dictionary used to aggregate the content of the files in a standardized way. It walks through the directory tree, skipping over any directories or files that match the ignore patterns. For each relevant file, it reads the content, removes any spaces, and stores it in the dictionary with<a id="_idIndexMarker197"/> the <a id="_idIndexMarker198"/>file path as the key:</p>
    <pre class="programlisting code"><code class="hljs-code">        repo_path = os.path.join(local_temp, os.listdir(local_temp)[<span class="hljs-number">0</span>])  <span class="hljs-comment"># </span>
        tree = {}
        <span class="hljs-keyword">for</span> root, _, files <span class="hljs-keyword">in</span> os.walk(repo_path):
            <span class="hljs-built_in">dir</span> = root.replace(repo_path, <span class="hljs-string">""</span>).lstrip(<span class="hljs-string">"/"</span>)
            <span class="hljs-keyword">if</span> <span class="hljs-built_in">dir</span>.startswith(self._ignore):
                <span class="hljs-keyword">continue</span>
            <span class="hljs-keyword">for</span> file <span class="hljs-keyword">in</span> files:
                <span class="hljs-keyword">if</span> file.endswith(self._ignore):
                    <span class="hljs-keyword">continue</span>
                file_path = os.path.join(<span class="hljs-built_in">dir</span>, file)
                <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(os.path.join(root, file), <span class="hljs-string">"r"</span>, errors=<span class="hljs-string">"ignore"</span>) <span class="hljs-keyword">as</span> f:
                    tree[file_path] = f.read().replace(<span class="hljs-string">" "</span>, <span class="hljs-string">""</span>)
</code></pre>
    <p class="normal">It then creates a new instance of the <code class="inlineCode">RepositoryDocument</code> model, populating it with the repository content, name, link, platform information, and author details. The instance is then saved to MongoDB:</p>
    <pre class="programlisting code"><code class="hljs-code">        user = kwargs[<span class="hljs-string">"user"</span>]
        instance = self.model(
            content=tree,
            name=repo_name,
            link=link,
            platform=<span class="hljs-string">"github"</span>,
            author_id=user.<span class="hljs-built_in">id</span>,
            author_full_name=user.full_name,
        )
        instance.save()
</code></pre>
    <p class="normal">Finally, whether the scraping succeeds or an exception occurs, the crawler ensures that the temporary directory is removed to clean up any resources used during<a id="_idIndexMarker199"/> the <a id="_idIndexMarker200"/>process:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">except</span> Exception:
        <span class="hljs-keyword">raise</span>
    <span class="hljs-keyword">finally</span>:
        shutil.rmtree(local_temp)
    logger.info(<span class="hljs-string">f"Finished scrapping GitHub repository: </span><span class="hljs-subst">{link}</span><span class="hljs-string">"</span>)
</code></pre>
    <h3 id="_idParaDest-73" class="heading-3">CustomArticleCrawler class</h3>
    <p class="normal">The <code class="inlineCode">CustomArticleCrawler</code> class takes<a id="_idIndexMarker201"/> a different approach to<a id="_idIndexMarker202"/> collecting data from the internet. It leverages the <code class="inlineCode">AsyncHtmlLoader</code> class to read the entire HTML from a link and the <code class="inlineCode">Html2TextTransformer</code> class to extract the text from that HTML. Both classes are made available by the <code class="inlineCode">langchain_community</code> Python package, as seen below, where we import all the necessary Python modules:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> urllib.parse <span class="hljs-keyword">import</span> urlparse
<span class="hljs-keyword">from</span> langchain_community.document_loaders <span class="hljs-keyword">import</span> AsyncHtmlLoader
<span class="hljs-keyword">from</span> langchain_community.document_transformers.html2text <span class="hljs-keyword">import</span> Html2TextTransformer
<span class="hljs-keyword">from</span> loguru <span class="hljs-keyword">import</span> logger
<span class="hljs-keyword">from</span> llm_engineering.domain.documents <span class="hljs-keyword">import</span> ArticleDocument
<span class="hljs-keyword">from</span> .base <span class="hljs-keyword">import</span> BaseCrawler
</code></pre>
    <p class="normal">Next, we define the <code class="inlineCode">CustomArticleCrawler</code> class, which inherits from <code class="inlineCode">BaseCrawler</code>. As before, we don’t need to log in or use the scrolling functionality provided by Selenium. In the <code class="inlineCode">extract</code> method, we first check if the article exists in the database to avoid duplicating content:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">CustomArticleCrawler</span>(<span class="hljs-title">BaseCrawler</span>):
    model = ArticleDocument
    <span class="hljs-keyword">def</span> <span class="hljs-title">extract</span>(<span class="hljs-params">self, link: </span><span class="hljs-built_in">str</span><span class="hljs-params">, **kwargs</span>) -&gt; <span class="hljs-literal">None</span>:
        old_model = self.model.find(link=link)
        <span class="hljs-keyword">if</span> old_model <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
            logger.info(<span class="hljs-string">f"Article already exists in the database: </span><span class="hljs-subst">{link}</span><span class="hljs-string">"</span>)
            <span class="hljs-keyword">return</span>
</code></pre>
    <p class="normal">If the article doesn’t exist, we proceed to scrape it. We use the <code class="inlineCode">AsyncHtmlLoader</code> class to load the HTML from the provided link. After, we transform it into plain text using the <code class="inlineCode">Html2TextTransformer</code> class, which returns a list of documents. We are only interested in the first document. As we delegate the whole logic to these two classes, we don’t control how the content is extracted and parsed. That’s why we used this class as a fallback system for domains where we don’t have anything custom implemented. These two classes follow the LangChain paradigm, which provides high-level functionality that works decently in most scenarios. It is fast to implement but hard to customize. That is one of the reasons why <a id="_idIndexMarker203"/>many <a id="_idIndexMarker204"/>developers avoid using LangChain in production use cases:</p>
    <pre class="programlisting code"><code class="hljs-code">        logger.info(<span class="hljs-string">f"Starting scrapping article: </span><span class="hljs-subst">{link}</span><span class="hljs-string">"</span>)
        loader = AsyncHtmlLoader([link])
        docs = loader.load()
        html2text = Html2TextTransformer()
        docs_transformed = html2text.transform_documents(docs)
        doc_transformed = docs_transformed[<span class="hljs-number">0</span>]
</code></pre>
    <p class="normal">We get the page content from the extracted document, plus relevant metadata such as the <code class="inlineCode">title</code>, <code class="inlineCode">subtitle</code>, <code class="inlineCode">content</code>, and <code class="inlineCode">language</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">        content = {
            <span class="hljs-string">"Title"</span>: doc_transformed.metadata.get(<span class="hljs-string">"title"</span>),
            <span class="hljs-string">"Subtitle"</span>: doc_transformed.metadata.get(<span class="hljs-string">"</span><span class="hljs-string">description"</span>),
            <span class="hljs-string">"Content"</span>: doc_transformed.page_content,
            <span class="hljs-string">"language"</span>: doc_transformed.metadata.get(<span class="hljs-string">"language"</span>),
        }
</code></pre>
    <p class="normal">Next, we parse the URL to determine the platform (or domain) from which the article was scraped:</p>
    <pre class="programlisting code"><code class="hljs-code">        parsed_url = urlparse(link)
        platform = parsed_url.netloc
</code></pre>
    <p class="normal">We then create a <a id="_idIndexMarker205"/>new instance of the article model, populating it with the extracted content. Finally, we save this instance to the MongoDB data warehouse:</p>
    <pre class="programlisting code"><code class="hljs-code">        user = kwargs[<span class="hljs-string">"user"</span>]
        instance = self.model(
            content=content,
            link=link,
            platform=platform,
            author_id=user.<span class="hljs-built_in">id</span>,
            author_full_name=user.full_name,
        )
        instance.save()
        logger.info(<span class="hljs-string">f"Finished scrapping custom article: </span><span class="hljs-subst">{link}</span><span class="hljs-string">"</span>)       
</code></pre>
    <p class="normal">So far, we have seen how to crawl GitHub repositories and random sites using LangChain utility functions. Lastly, we must explore a crawler using Selenium to manipulate the<a id="_idIndexMarker206"/> browser<a id="_idIndexMarker207"/> programmatically. Thus, we will continue with the <code class="inlineCode">MediumCrawler</code> implementation.</p>
    <h3 id="_idParaDest-74" class="heading-3">MediumCrawler class</h3>
    <p class="normal">The code begins <a id="_idIndexMarker208"/>by <a id="_idIndexMarker209"/>importing essential libraries and defining the <code class="inlineCode">MediumCrawler</code> class, which inherits from <code class="inlineCode">BaseSeleniumCrawler</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> bs4 <span class="hljs-keyword">import</span> BeautifulSoup
<span class="hljs-keyword">from</span> loguru <span class="hljs-keyword">import</span> logger
<span class="hljs-keyword">from</span> llm_engineering.domain.documents <span class="hljs-keyword">import</span> ArticleDocument
<span class="hljs-keyword">from</span> .base <span class="hljs-keyword">import</span> BaseSeleniumCrawler
<span class="hljs-keyword">class</span> <span class="hljs-title">MediumCrawler</span>(<span class="hljs-title">BaseSeleniumCrawler</span>):
    model = ArticleDocument
</code></pre>
    <p class="normal">Within the <code class="inlineCode">MediumCrawler</code> class, we leverage the <code class="inlineCode">set_extra_driver_options()</code> method to extend the default driver options used by Selenium:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">def</span> <span class="hljs-title">set_extra_driver_options</span>(<span class="hljs-params">self, options</span>) -&gt; <span class="hljs-literal">None</span>:
        options.add_argument(<span class="hljs-string">r"--profile-directory=Profile 2"</span>)
</code></pre>
    <p class="normal">The <code class="inlineCode">extract()</code> method implements the core functionality, first checking whether the article exists in the database to prevent duplicate entries. </p>
    <p class="normal">If the article is new, the method proceeds to navigate to the article’s link and scroll through the page to ensure all<a id="_idIndexMarker210"/> content is loaded:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">def</span> <span class="hljs-title">extract</span>(<span class="hljs-params">self, link: </span><span class="hljs-built_in">str</span><span class="hljs-params">, **kwargs</span>) -&gt; <span class="hljs-literal">None</span>:
        old_model = self.model.find(link=link)
        <span class="hljs-keyword">if</span> old_model <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
            logger.info(<span class="hljs-string">f"Article already exists in the database: </span><span class="hljs-subst">{link}</span><span class="hljs-string">"</span>)
            <span class="hljs-keyword">return</span>
        logger.info(<span class="hljs-string">f"Starting scrapping Medium article: </span><span class="hljs-subst">{link}</span><span class="hljs-string">"</span>)
        self.driver.get(link)
        self.scroll_page()
</code></pre>
    <p class="normal">After fully loading the page, the method uses <code class="inlineCode">BeautifulSoup</code> to parse the HTML content and extract the article’s title, subtitle, and full text. <code class="inlineCode">BeautifulSoup</code> is a popular Python library for web scraping and parsing HTML or XML documents. Thus, we used it to extract all the HTML elements we needed from the HTML accessed with Selenium. Finally, we aggregate everything into a dictionary:</p>
    <pre class="programlisting code"><code class="hljs-code">        soup = BeautifulSoup(self.driver.page_source, <span class="hljs-string">"html.parser"</span>)
        title = soup.find_all(<span class="hljs-string">"h1"</span>, class_=<span class="hljs-string">"pw-post-title"</span>)
        subtitle = soup.find_all(<span class="hljs-string">"h2"</span>, class_=<span class="hljs-string">"pw-subtitle-paragraph"</span>)
        data = {
            <span class="hljs-string">"Title"</span>: title[<span class="hljs-number">0</span>].string <span class="hljs-keyword">if</span> title <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span>,
            <span class="hljs-string">"Subtitle"</span>: subtitle[<span class="hljs-number">0</span>].string <span class="hljs-keyword">if</span> subtitle <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span>,
            <span class="hljs-string">"Content"</span>: soup.get_text(),
        }
</code></pre>
    <p class="normal">Finally, the method closes the WebDriver to free up resources. It then creates a new <code class="inlineCode">ArticleDocument</code> instance, populates it with the extracted content and user information<a id="_idIndexMarker211"/> provided via <code class="inlineCode">kwargs</code>, and saves it to the <a id="_idIndexMarker212"/>database:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.driver.close()
        user = kwargs[<span class="hljs-string">"user"</span>]
        instance = self.model(
            platform=<span class="hljs-string">"medium"</span>,
            content=data,
            link=link,
            author_id=user.<span class="hljs-built_in">id</span>,
            author_full_name=user.full_name,
        )
        instance.save()
        logger.info(<span class="hljs-string">f"Successfully scraped and saved article: </span><span class="hljs-subst">{link}</span><span class="hljs-string">"</span>)
</code></pre>
    <p class="normal">With that, we conclude the <code class="inlineCode">MediumCrawler</code> implementation. The LinkedIn crawler follows a similar pattern to the Medium one, where it uses Selenium to log in and access the feed of a user’s latest posts. Then, it extracts the posts and scrolls through the feed to load the next page until a limit is hit. You can check the full implementation in our repository at <a href="https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/crawlers/linkedin.py"><span class="url">https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/application/crawlers/linkedin.py</span></a>.</p>
    <p class="normal">With the rise of LLMs, collecting data from the internet has become a critical step in many real-world AI applications. Hence, more high-level tools have appeared in the Python ecosystem, such as Scrapy (<a href="https://github.com/scrapy/scrapy"><span class="url">https://github.com/scrapy/scrapy</span></a>), which crawls websites and extracts structured data from their pages, and Crawl4AI (<a href="https://github.com/unclecode/crawl4ai"><span class="url">https://github.com/unclecode/crawl4ai</span></a>), which is highly specialized in crawling data for LLMs and AI applications.</p>
    <p class="normal">In this section, we’ve looked at implementing three types of crawlers: one that leverages the <code class="inlineCode">git</code> executable in a subprocess to clone GitHub repositories, one that uses LangChain utilities to extract the HTML of a single web page, and one that leverages Selenium for more complex scenarios where we have to navigate through the login page, scroll the article to load the entire HTML, and extract it into text<a id="_idIndexMarker213"/> format. The <a id="_idIndexMarker214"/>last step is understanding how the document classes we’ve used across the chapter, such as the <code class="inlineCode">ArticleDocument</code>, work.</p>
    <h2 id="_idParaDest-75" class="heading-2">The NoSQL data warehouse documents</h2>
    <p class="normal">We had to implement<a id="_idIndexMarker215"/> three <a id="_idIndexMarker216"/>document classes to structure our data categories. These classes define the specific attributes we require for a document, such as the content, author, and source link. It is best practice to structure your data in classes instead of dictionaries, as the attributes we expect for each item are more verbose, reducing run errors. For example, when accessing a value from a Python dictionary, we can never be sure it is present or its type is current. By wrapping our data items with classes, we can ensure each attribute is as expected. </p>
    <p class="normal">By leveraging Python packages such as Pydantic, we have out-of-the-box type validation, which ensures consistency in our datasets. Thus, we modeled the data categories as the following document classes, which we already used in the code up until point:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">ArticleDocument</code> class</li>
      <li class="bulletList"><code class="inlineCode">PostDocument</code> class</li>
      <li class="bulletList"><code class="inlineCode">RepositoryDocument</code> class</li>
    </ul>
    <p class="normal">These are not simple Python data classes or Pydantic models. They support read and write operations on top of the MongoDB data warehouse. To inject the read-and-write functionality into all the document classes without repeating any code, we used the <strong class="keyWord">Object-Document Mapping</strong> (ODM) software pattern, which is based on the <strong class="keyWord">object-relational mapping</strong> (<strong class="keyWord">ORM</strong>) pattern. Thus, let’s first explore ORM, then move <a id="_idIndexMarker217"/>to<a id="_idIndexMarker218"/> ODM, and, finally, dig into our custom ODM implementation and document classes.</p>
    <h3 id="_idParaDest-76" class="heading-3">The ORM and ODM software patterns</h3>
    <p class="normal">Before we talk about<a id="_idIndexMarker219"/> software <a id="_idIndexMarker220"/>patterns, let’s see what ORM is. It’s a technique that lets you query and manipulate data from a database using an object-oriented paradigm. Instead of writing SQL or API-specific queries, you encapsulate all the complexity under an ORM class that knows how to handle all the database operations, most commonly CRUD operations. Thus, working with ORM removes the need to handle the database operations manually and reduces the need to write boilerplate code manually. An ORM interacts with a SQL database, such as PostgreSQL or MySQL.</p>
    <p class="normal">Most modern Python applications use ORMs when interacting with the database. Even though SQL is still a popular choice in the data world, you rarely see raw SQL queries in Python backend components. The most popular Python ORM is SQLAlchemy (<a href="https://www.sqlalchemy.org/"><span class="url">https://www.sqlalchemy.org/</span></a>). Also, with the rise of FastAPI, SQLModel is (<a href="https://github.com/fastapi/sqlmodel"><span class="url">https://github.com/fastapi/sqlmodel</span></a>) a common choice, which is a wrapper over SQLAlchemy that makes the integration easier with FastAPI.</p>
    <p class="normal">For example, using SQLAlchemy, we defined a <code class="inlineCode">User</code> ORM with the ID and name fields. The <code class="inlineCode">User</code> ORM is mapped to the <code class="inlineCode">users</code> table within the SQL database. Thus, when we create a new user and commit it to the database, it is automatically saved <a id="_idIndexMarker221"/>to the <code class="inlineCode">users</code> table. The<a id="_idIndexMarker222"/> same applies to all the CRUD operations on top of the <code class="inlineCode">User</code> class.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> sqlalchemy <span class="hljs-keyword">import</span> Column, Integer, String, create_engine
<span class="hljs-keyword">from</span> sqlalchemy.orm <span class="hljs-keyword">import</span> declarative_base, sessionmaker
   Base = declarative_base()
<span class="hljs-comment"># Define a class that maps to the users table.</span>
   <span class="hljs-keyword">class</span> <span class="hljs-title">User</span>(<span class="hljs-title">Base</span>):
    __tablename__ = <span class="hljs-string">"users"</span>
    <span class="hljs-built_in">id</span> = Column(Integer, primary_key=<span class="hljs-literal">True</span>)
   name = Column(String)
</code></pre>
    <p class="normal">Using the <code class="inlineCode">User</code> ORM, we can quickly insert or query users directly from Python without writing a line of SQL. Note that an ORM usually supports all <strong class="keyWord">CRUD</strong> operations. Here is a code snippet that shows how to save an instance of the User ORM to a SQLite database:</p>
    <pre class="programlisting code"><code class="hljs-code">engine = create_engine(<span class="hljs-string">"sqlite:///:memory:"</span>)
Base.metadata.create_all(engine)
<span class="hljs-comment"># Create a session used to interact with the database.</span>
Session = sessionmaker(bind=engine)
session = Session()
<span class="hljs-comment"># Add a new user.</span>
new_user = User(name=<span class="hljs-string">"Alice"</span>)
session.add(new_user)
session.commit()
</code></pre>
    <p class="normal">Also, this is how we can query a user from the <code class="inlineCode">users</code> SQLite table:</p>
    <pre class="programlisting code"><code class="hljs-code">user = session.query(User).first()
<span class="hljs-keyword">if</span> user:
<span class="hljs-built_in">print</span>(<span class="hljs-string">f"User ID: </span><span class="hljs-subst">{user.</span><span class="hljs-built_in">id</span><span class="hljs-subst">}</span><span class="hljs-string">"</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f"User name: </span><span class="hljs-subst">{user.name}</span><span class="hljs-string">"</span>)
</code></pre>
    <div class="note">
      <p class="normal">Find the entire script and how to run it in the GitHub repository at <code class="inlineCode">code_snippets/03_orm.py</code>.</p>
    </div>
    <p class="normal">The ODM pattern is extremely similar to ORM, but instead of working with SQL databases and tables, it works with NoSQL databases (such as MongoDB) and unstructured collections. As we work with NoSQL databases, the data structure is centered on collections, which store JSON-like documents rather than rows in tables.</p>
    <p class="normal">To conclude, ODM simplifies working with document-based NoSQL databases and maps object-oriented<a id="_idIndexMarker223"/> code to<a id="_idIndexMarker224"/> JSON-like documents. We will implement a light ODM module on top of MongoDB to fully understand how ODM works.</p>
    <h3 id="_idParaDest-77" class="heading-3">Implementing the ODM class</h3>
    <p class="normal">This section <a id="_idIndexMarker225"/>will explore<a id="_idIndexMarker226"/> how to implement an ODM class from scratch. This is an excellent exercise to learn how ODM works and sharpen our skills in writing modular and reusable Python classes. Hence, we will implement a base ODM class called <code class="inlineCode">NoSQLBaseDocument</code>, from which all the other documents will inherit to interact with the MongoDB data warehouse.</p>
    <div class="note">
      <p class="normal">The class can be found in our repository at <code class="inlineCode">llm_engineering/domain/base/nosql.py</code>.</p>
    </div>
    <p class="normal">The code starts by importing essential modules and setting up the database connection. Through the <code class="inlineCode">_database</code> variable, we establish a connection to the database specified in the settings, which is by default called <code class="inlineCode">twin</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> uuid
<span class="hljs-keyword">from</span> abc <span class="hljs-keyword">import</span> ABC
<span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-type">Generic</span>, <span class="hljs-type">Type</span>, TypeVar
<span class="hljs-keyword">from</span> loguru <span class="hljs-keyword">import</span> logger
<span class="hljs-keyword">from</span> pydantic <span class="hljs-keyword">import</span> UUID4, BaseModel, Field
<span class="hljs-keyword">from</span> pymongo <span class="hljs-keyword">import</span> errors
<span class="hljs-keyword">from</span> llm_engineering.domain.exceptions <span class="hljs-keyword">import</span> ImproperlyConfigured
<span class="hljs-keyword">from</span> llm_engineering.infrastructure.db.mongo <span class="hljs-keyword">import</span> connection
<span class="hljs-keyword">from</span> llm_engineering.settings <span class="hljs-keyword">import</span> settings
_database = connection.get_database(settings.DATABASE_NAME)
</code></pre>
    <p class="normal">Next, we define<a id="_idIndexMarker227"/> a type <a id="_idIndexMarker228"/>variable <code class="inlineCode">T</code> bound to the <code class="inlineCode">NoSQLBaseDocument</code> class. The variable leverages Python’s generic module, allowing us to generalize the class’s types. For example, when we implement the <code class="inlineCode">ArticleDocument</code> class, which will inherit from the <code class="inlineCode">NoSQLBaseDocument</code> class, all the instances where <code class="inlineCode">T</code> was used will be replaced with the <code class="inlineCode">ArticleDocument</code> type when analyzing the signature of functions (more on Python generics: <a href="https://realpython.com/python312-typing"><span class="url">https://realpython.com/python312-typing</span></a>). </p>
    <p class="normal">The <code class="inlineCode">NoSQLBaseDocument</code> class is then declared as an abstract base class inheriting from Pydantic’s BaseModel, Python’s Generic (which provides the functionality described earlier), and <code class="inlineCode">ABC</code> (making the class abstract) classes. This class serves as the foundational ODM class:</p>
    <pre class="programlisting code"><code class="hljs-code">T = TypeVar(<span class="hljs-string">"T"</span>, bound=<span class="hljs-string">"NoSQLBaseDocument"</span>)
<span class="hljs-keyword">class</span> <span class="hljs-title">NoSQLBaseDocument</span>(BaseModel, <span class="hljs-type">Generic</span>[T], ABC):
</code></pre>
    <p class="normal">Within the <code class="inlineCode">NoSQLBaseDocument</code> class, an id field is defined as a UUID4, with a default factory generating a unique UUID. The class also implements the <code class="inlineCode">__eq__</code> and <code class="inlineCode">__hash__</code> methods to allow instances to be compared and used in hashed collections like sets or as dictionary keys based on their unique <code class="inlineCode">id</code> attribute:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in">id</span>: UUID4 = Field(default_factory=uuid.uuid4)
<span class="hljs-keyword">def</span> <span class="hljs-title">__eq__</span>(<span class="hljs-params">self, value: </span><span class="hljs-built_in">object</span>) -&gt; <span class="hljs-built_in">bool</span>:
    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(value, self.__class__):
        <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span>
    <span class="hljs-keyword">return</span> self.<span class="hljs-built_in">id</span> == value.<span class="hljs-built_in">id</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">__hash__</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-built_in">int</span>:
    <span class="hljs-keyword">return</span> <span class="hljs-built_in">hash</span>(self.<span class="hljs-built_in">id</span>)
</code></pre>
    <p class="normal">The class provides methods for converting between MongoDB documents and class instances. The <code class="inlineCode">from_mongo()</code> class method transforms a dictionary retrieved from MongoDB <a id="_idIndexMarker229"/>into an <a id="_idIndexMarker230"/>instance of the class. The <code class="inlineCode">to_mongo()</code> instance method converts the model instance into a dictionary suitable for MongoDB insertion:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-meta">@classmethod</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">from_mongo</span>(<span class="hljs-params">cls: </span><span class="hljs-type">Type</span><span class="hljs-params">[T], data: </span><span class="hljs-built_in">dict</span>) -&gt; T:
    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> data:
        <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">"Data is empty."</span>)
    <span class="hljs-built_in">id</span> = data.pop(<span class="hljs-string">"_id"</span>)
    <span class="hljs-keyword">return</span> cls(**<span class="hljs-built_in">dict</span>(data, <span class="hljs-built_in">id</span>=<span class="hljs-built_in">id</span>))
<span class="hljs-keyword">def</span> <span class="hljs-title">to_mongo</span>(<span class="hljs-params">self: T, **kwargs</span>) -&gt; <span class="hljs-built_in">dict</span>:
    exclude_unset = kwargs.pop(<span class="hljs-string">"exclude_unset"</span>, <span class="hljs-literal">False</span>)
    by_alias = kwargs.pop(<span class="hljs-string">"by_alias"</span>, <span class="hljs-literal">True</span>)
    parsed = self.model_dump(exclude_unset=exclude_unset, by_alias=by_alias, **kwargs)
    <span class="hljs-keyword">if</span> <span class="hljs-string">"_id"</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> parsed <span class="hljs-keyword">and</span> <span class="hljs-string">"id"</span> <span class="hljs-keyword">in</span> parsed:
        parsed[<span class="hljs-string">"_id"</span>] = <span class="hljs-built_in">str</span>(parsed.pop(<span class="hljs-string">"id"</span>))
    <span class="hljs-keyword">for</span> key, value <span class="hljs-keyword">in</span> parsed.items():
        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(value, uuid.UUID):
            parsed[key] = <span class="hljs-built_in">str</span>(value)
    <span class="hljs-keyword">return</span> parsed
</code></pre>
    <p class="normal">The <code class="inlineCode">save()</code> method allows an instance of the model to be inserted into a MongoDB collection. It retrieves the appropriate collection, converts the instance into a MongoDB-compatible document leveraging the <code class="inlineCode">to_mongo()</code> method described <a id="_idIndexMarker231"/>above, an<a id="_idIndexMarker232"/>d attempts to insert it into the database, handling any write errors that may occur:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">save</span>(<span class="hljs-params">self: T, **kwargs</span>) -&gt; T | <span class="hljs-literal">None</span>:
    collection = _database[self.get_collection_name()]
    <span class="hljs-keyword">try</span>:
        collection.insert_one(self.to_mongo(**kwargs))
        <span class="hljs-keyword">return</span> self
    <span class="hljs-keyword">except</span> errors.WriteError:
        logger.exception(<span class="hljs-string">"</span><span class="hljs-string">Failed to insert document."</span>)
        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span>
</code></pre>
    <p class="normal">The <code class="inlineCode">get_or_create()</code> class method attempts to find a document in the database matching the provided filter options. If a matching document is found, it is converted into an instance of the class. If not, a new instance is created with the filter options as its initial data and saved to the database:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-meta">@classmethod</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">get_or_create</span>(<span class="hljs-params">cls: </span><span class="hljs-type">Type</span><span class="hljs-params">[T], **filter_options</span>) -&gt; T:
    collection = _database[cls.get_collection_name()]
    <span class="hljs-keyword">try</span>:
        instance = collection.find_one(filter_options)
        <span class="hljs-keyword">if</span> instance:
            <span class="hljs-keyword">return</span> cls.from_mongo(instance)
        new_instance = cls(**filter_options)
        new_instance = new_instance.save()
        <span class="hljs-keyword">return</span> new_instance
    <span class="hljs-keyword">except</span> errors.OperationFailure:
        logger.exception(<span class="hljs-string">f"Failed to retrieve document with filter options: </span><span class="hljs-subst">{filter_options}</span><span class="hljs-string">"</span>)
        <span class="hljs-keyword">raise</span>
</code></pre>
    <p class="normal">The <code class="inlineCode">bulk_insert()</code> class <a id="_idIndexMarker233"/>method allows<a id="_idIndexMarker234"/> multiple documents to be inserted into the database at once:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-meta">@classmethod</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">bulk_insert</span>(<span class="hljs-params">cls: </span><span class="hljs-type">Type</span><span class="hljs-params">[T], documents: </span><span class="hljs-built_in">list</span><span class="hljs-params">[T], **kwargs</span>) -&gt; <span class="hljs-built_in">bool</span>:
    collection = _database[cls.get_collection_name()]
    <span class="hljs-keyword">try</span>:
        collection.insert_many([doc.to_mongo(**kwargs) <span class="hljs-keyword">for</span> doc <span class="hljs-keyword">in</span> documents])
        <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span>
    <span class="hljs-keyword">except</span> (errors.WriteError, errors.BulkWriteError):
logger.error(<span class="hljs-string">f"Failed to insert documents of type </span><span class="hljs-subst">{cls.__name__}</span><span class="hljs-string">"</span>)
        <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span>
</code></pre>
    <p class="normal">The <code class="inlineCode">find()</code> class method searches for a single document in the database that matches the given filter options:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-meta">@classmethod</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">find</span>(<span class="hljs-params">cls: </span><span class="hljs-type">Type</span><span class="hljs-params">[T], **filter_options</span>) -&gt; T | <span class="hljs-literal">None</span>:
    collection = _database[cls.get_collection_name()]
    <span class="hljs-keyword">try</span>:
        instance = collection.find_one(filter_options)
        <span class="hljs-keyword">if</span> instance:
            <span class="hljs-keyword">return</span> cls.from_mongo(instance)
        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span>
    <span class="hljs-keyword">except</span> errors.OperationFailure:
        logger.error(<span class="hljs-string">"Failed to retrieve document."</span>)
        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span>
</code></pre>
    <p class="normal">Similarly, the <code class="inlineCode">bulk_find()</code> class method retrieves multiple documents matching the filter options. It converts each retrieved MongoDB document into a model instance, collecting them into a list:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-meta">@classmethod</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">bulk_find</span>(<span class="hljs-params">cls: </span><span class="hljs-type">Type</span><span class="hljs-params">[T], **filter_options</span>) -&gt; <span class="hljs-built_in">list</span>[T]:
    collection = _database[cls.get_collection_name()]
    <span class="hljs-keyword">try</span>:
        instances = collection.find(filter_options)
        <span class="hljs-keyword">return</span> [document <span class="hljs-keyword">for</span> instance <span class="hljs-keyword">in</span> instances <span class="hljs-keyword">if</span> (document := cls.from_mongo(instance)) <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>]
    <span class="hljs-keyword">except</span> errors.OperationFailure:
        logger.error(<span class="hljs-string">"Failed to retrieve document."</span>)
        <span class="hljs-keyword">return</span> []
</code></pre>
    <p class="normal">Finally, the <code class="inlineCode">get_collection_name()</code> class method determines the name of the MongoDB<a id="_idIndexMarker235"/> collection <a id="_idIndexMarker236"/>associated with the class. It expects the class to have a nested <code class="inlineCode">Settings</code> class with a name attribute specifying the collection name. If this configuration is missing, an <code class="inlineCode">ImproperlyConfigured</code> exception will be raised specifying that the subclass should define a nested <code class="inlineCode">Settings</code> class:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-meta">@classmethod</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">get_collection_name</span>(<span class="hljs-params">cls: </span><span class="hljs-type">Type</span><span class="hljs-params">[T]</span>) -&gt; <span class="hljs-built_in">str</span>:
    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">hasattr</span>(cls, <span class="hljs-string">"Settings"</span>) <span class="hljs-keyword">or</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">hasattr</span>(cls.Settings, <span class="hljs-string">"name"</span>):
        <span class="hljs-keyword">raise</span> ImproperlyConfigured(
            <span class="hljs-string">"Document should define an Settings configuration class with the name of the collection."</span>
        )
    <span class="hljs-keyword">return</span> cls.Settings.name
</code></pre>
    <p class="normal">We can configure each subclass using the nested <code class="inlineCode">Settings</code> class, such as defining the collection name, or anything else specific to that subclass. Within the Python ecosystem, there is an ODM implementation on top of MongoDB, called <code class="inlineCode">mongoengine</code>, which you can find on GitHub. It follows a pattern similar to ours but more comprehensive. We implemented it by ourselves, as it was an excellent exercise to practice writing modular and generic code following best OOP principles, which are <a id="_idIndexMarker237"/>essential <a id="_idIndexMarker238"/>for implementing production-level code.</p>
    <h3 id="_idParaDest-78" class="heading-3">Data categories and user document classes</h3>
    <p class="normal">The last piece of the <a id="_idIndexMarker239"/>puzzle is to see the implementation of the subclasses that inherit from the <code class="inlineCode">NoSQLBaseDocument</code> base class. These are the concrete classes that define our data categories. You’ve seen these classes used across the chapter when working with articles, repositories, and posts within the crawler classes.</p>
    <p class="normal">We begin by importing the essential Python modules and the ODM base class:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> abc <span class="hljs-keyword">import</span> ABC
<span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-type">Optional</span>
<span class="hljs-keyword">from</span> pydantic <span class="hljs-keyword">import</span> UUID4, Field
<span class="hljs-keyword">from</span> .base <span class="hljs-keyword">import</span> NoSQLBaseDocument
<span class="hljs-keyword">from</span> .types <span class="hljs-keyword">import</span> DataCategory
</code></pre>
    <p class="normal">We define an <code class="inlineCode">enum</code> class, where we centralize all our data category types. These variables will act as constants in configuring all our ODM classes throughout the book.</p>
    <div class="note">
      <p class="normal">The class can be found in the repository at <code class="inlineCode">llm_engineering/domain/types.py</code>.</p>
    </div>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> enum <span class="hljs-keyword">import</span> StrEnum
<span class="hljs-keyword">class</span> <span class="hljs-title">DataCategory</span>(<span class="hljs-title">StrEnum</span>):
    PROMPT = <span class="hljs-string">"prompt"</span>
    QUERIES = <span class="hljs-string">"queries"</span>
    INSTRUCT_DATASET_SAMPLES = <span class="hljs-string">"instruct_dataset_samples"</span>
    INSTRUCT_DATASET = <span class="hljs-string">"instruct_dataset"</span>
    PREFERENCE_DATASET_SAMPLES = <span class="hljs-string">"preference_dataset_samples"</span>
    PREFERENCE_DATASET = <span class="hljs-string">"</span><span class="hljs-string">preference_dataset"</span>
    POSTS = <span class="hljs-string">"posts"</span>
    ARTICLES = <span class="hljs-string">"articles"</span>
     REPOSITORIES = <span class="hljs-string">"repositories"</span>
</code></pre>
    <p class="normal">The <code class="inlineCode">Document</code> class is<a id="_idIndexMarker240"/> introduced as an abstract base model for other documents on top of the <code class="inlineCode">NoSQLBaseDocument</code> ODM class. It includes common attributes like content, platform, and author details, providing a standardized structure for documents that will inherit from it:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">Document</span>(NoSQLBaseDocument, ABC):
    content: <span class="hljs-built_in">dict</span>
    platform: <span class="hljs-built_in">str</span>
    author_id: UUID4 = Field(alias=<span class="hljs-string">"author_id"</span>)
    author_full_name: <span class="hljs-built_in">str</span> = Field(alias=<span class="hljs-string">"author_full_name"</span>)
</code></pre>
    <p class="normal">Finally, specific document types are defined by extending the <code class="inlineCode">Document</code> class. The <code class="inlineCode">RepositoryDocument</code>, <code class="inlineCode">PostDocument</code>, and <code class="inlineCode">ArticleDocument</code> classes represent different categories of data, each with unique fields and settings that specify their respective collection names in the database:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">RepositoryDocument</span>(<span class="hljs-title">Document</span>):
    name: <span class="hljs-built_in">str</span>
    link: <span class="hljs-built_in">str</span>
    <span class="hljs-keyword">class</span> <span class="hljs-title">Settings</span>:
        name = DataCategory.REPOSITORIES
<span class="hljs-keyword">class</span> <span class="hljs-title">PostDocument</span>(<span class="hljs-title">Document</span>):
    image: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">str</span>] = <span class="hljs-literal">None</span>
    link: <span class="hljs-built_in">str</span> | <span class="hljs-literal">None</span> = <span class="hljs-literal">None</span>
    <span class="hljs-keyword">class</span> <span class="hljs-title">Settings</span>:
        name = DataCategory.POSTS
<span class="hljs-keyword">class</span> <span class="hljs-title">ArticleDocument</span>(<span class="hljs-title">Document</span>):
    link: <span class="hljs-built_in">str</span>
    <span class="hljs-keyword">class</span> <span class="hljs-title">Settings</span>:
        name = DataCategory.ARTICLES
</code></pre>
    <p class="normal">Finally, we define<a id="_idIndexMarker241"/> the <code class="inlineCode">UserDocument</code> class, which is used to store and query all the users from the LLM Twin project:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">UserDocument</span>(<span class="hljs-title">NoSQLBaseDocument</span>):
    first_name: <span class="hljs-built_in">str</span>
    last_name: <span class="hljs-built_in">str</span>
    <span class="hljs-keyword">class</span> <span class="hljs-title">Settings</span>:
        name = <span class="hljs-string">"users"</span>
<span class="hljs-meta">    @property</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title">full_name</span>(<span class="hljs-params">self</span>):
        <span class="hljs-keyword">return</span> <span class="hljs-string">f"</span><span class="hljs-subst">{self.first_name}</span><span class="hljs-string"> </span><span class="hljs-subst">{self.last_name}</span><span class="hljs-string">"</span>
</code></pre>
    <p class="normal">By implementing the <code class="inlineCode">NoSQLBaseDocument</code> ODM class, we had to focus solely on the fields and specific functionality of each document or domain entity. All the CRUD functionality is delegated to the parent class. Also, by leveraging Pydantic to define the fields, we have out-of-the-box type validation. For example, when creating an instance of the <code class="inlineCode">ArticleDocument</code> class, if the provided link is <code class="inlineCode">None</code> or not a string, it will throw an error signaling that the data is invalid.</p>
    <p class="normal">With that, we’ve finished implementing our data collection pipeline, starting with the ZenML components. Then, we looked into the implementation of the crawlers and, finally, wrapped it up with the ODM class and data category documents. The last step <a id="_idIndexMarker242"/>is to run the data collection pipeline and ingest raw data into the MongoDB data warehouse.</p>
    <h1 id="_idParaDest-79" class="heading-1">Gathering raw data into the data warehouse</h1>
    <p class="normal">ZenML orchestrates<a id="_idIndexMarker243"/> the data collection pipeline. Thus, leveraging ZenML, the data collection pipeline can be run manually, scheduled, or triggered by specific events. Here, we will show you how to run it manually, while we will discuss the other scenarios in <em class="italic">Chapter 11</em> when digging deeper into MLOps.</p>
    <p class="normal">We configured a different pipeline run for each author. We provided a ZenML configuration file for Paul Iusztin’s or Maxime Labonne’s data. To call the data collection pipeline to collect Maxime’s data, for example, you can run the following CLI command:</p>
    <pre class="programlisting con"><code class="hljs-con">poetry poe run-digital-data-etl-maxime
</code></pre>
    <p class="normal">That will call the pipeline with the following ZenML YAML configuration file:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">parameters:</span>
  <span class="hljs-attr">user_full_name:</span> <span class="hljs-string">Maxime</span> <span class="hljs-string">Labonne</span> <span class="hljs-comment"># [First Name(s)] [Last Name]</span>
  <span class="hljs-attr">links:</span>
    <span class="hljs-comment"># Personal Blog</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">https://mlabonne.github.io/blog/posts/2024-07-29_Finetune_Llama31.html</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">https://mlabonne.github.io/blog/posts/2024-07-15_The_Rise_of_Agentic_Data_Generation.html</span>
    <span class="hljs-comment"># Substack</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">https://maximelabonne.substack.com/p/uncensor-any-llm-with-abliteration-d30148b7d43e</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">https://maximelabonne.substack.com/p/create-mixtures-of-experts-with-mergekit-11b318c99562</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">https://maximelabonne.substack.com/p/merge-large-language-models-with-mergekit-2118fb392b54</span>
    <span class="hljs-string">…</span> <span class="hljs-comment"># More Substack links</span>
</code></pre>
    <p class="normal">In <em class="italic">Figure 3.3</em> earlier, we<a id="_idIndexMarker244"/> saw the pipeline’s run DAG and details in ZenML’s dashboard. Meanwhile, <em class="italic">Figure 3.5</em> shows the <code class="inlineCode">user</code> output artifact generated by this data collection pipeline. You can inspect the query <code class="inlineCode">user_full_name</code> and the retrieved <code class="inlineCode">user</code> from the MongoDB database, for which we collected the links in this specific run.</p>
    <figure class="mediaobject"><img src="../Images/B31105_03_05.png" alt=""/></figure>
    <p class="packt_figref">Figure 3.5: Example of the user output artifact after running the data collection pipeline using Maxime’s configuration file</p>
    <p class="normal">Also, in <em class="italic">Figure 3.6</em>, you can <a id="_idIndexMarker245"/>observe the <code class="inlineCode">crawled_links</code> output artifact, which lists all the domains from which we collected data, the total number of links crawled for each domain, and the number of successfully collected links. </p>
    <p class="normal">We want to highlight again the power of these artifacts, as they trace each pipeline’s results and metadata, making it extremely easy to monitor and debug each pipeline run individually.</p>
    <figure class="mediaobject"><img src="../Images/B31105_03_06.png" alt=""/></figure>
    <p class="packt_figref">Figure 3.6: Example of the crawled_links output artifact after running the data collection pipeline using Maxime’s configuration file</p>
    <p class="normal">Now, we can <a id="_idIndexMarker246"/>download the <code class="inlineCode">crawled_links</code> artifact anywhere in our code by running the following code, where the <code class="inlineCode">ID</code> of the artifact can be found in ZenML and is unique for every artifact version:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> zenml.client <span class="hljs-keyword">import</span> Client
artifact = Client().get_artifact_version(<span class="hljs-string">'8349ce09-0693-4e28-8fa2-20f82c76ddec'</span>)
loaded_artifact = artifact.load()
</code></pre>
    <p class="normal">For example, we can easily run the same data collection pipeline but with Paul Iusztin’s YAML configuration, listed below:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">parameters:</span>
  <span class="hljs-attr">user_full_name:</span> <span class="hljs-string">Paul</span> <span class="hljs-string">Iusztin</span> <span class="hljs-comment"># [First Name(s)] [Last Name]</span>
  <span class="hljs-attr">links:</span>
    <span class="hljs-comment"># Medium</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">https://medium.com/decodingml/an-end-to-end-framework-for-production-ready-llm-systems-by-building-your-llm-twin-2cc6bb01141f</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">https://medium.com/decodingml/a-real-time-retrieval-system-for-rag-on-social-media-data-9cc01d50a2a0</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">https://medium.com/decodingml/sota-python-streaming-pipelines-for-fine-tuning-llms-and-rag-in-real-time-82eb07795b87</span>
    <span class="hljs-string">…</span> <span class="hljs-comment"># More Medium links</span>
    <span class="hljs-comment"># Substack</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">https://decodingml.substack.com/p/real-time-feature-pipelines-with?r=1ttoeh</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">https://decodingml.substack.com/p/building-ml-systems-the-right-way?r=1ttoeh</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">https://decodingml.substack.com/p/reduce-your-pytorchs-code-latency?r=1ttoeh</span>
    <span class="hljs-string">…</span> <span class="hljs-comment"># More Substack links</span>
</code></pre>
    <p class="normal">To run the pipeline <a id="_idIndexMarker247"/>using Paul’s configuration, we call the following <code class="inlineCode">poe</code> command:</p>
    <pre class="programlisting con"><code class="hljs-con">poetry poe run-digital-data-etl-paul
</code></pre>
    <p class="normal">That, under the hood, calls the following CLI command that references Paul’s config file:</p>
    <pre class="programlisting con"><code class="hljs-con">poetry run python -m tools.run --run-etl --no-cache --etl-config-filename digital_data_etl_paul_iusztin.yaml
</code></pre>
    <p class="normal">You can find all the configs in the repository in the <code class="inlineCode">configs/</code> directory. Also, using <code class="inlineCode">poe</code>, we configured a command that calls the data collection pipeline for all the supported authors:</p>
    <pre class="programlisting con"><code class="hljs-con">poetry poe run-digital-data-etl
</code></pre>
    <p class="normal">We can easily query the MongoDB data warehouse using our ODM classes. For example, let’s query all the articles collected for Paul Iusztin:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> llm_engineering.domain.documents <span class="hljs-keyword">import</span> ArticleDocument, UserDocument
user = UserDocument.get_or_create(first_name=<span class="hljs-string">"Paul"</span>, last_name=<span class="hljs-string">"Iusztin"</span>)
articles = ArticleDocument.bulk_find(author_id=<span class="hljs-built_in">str</span>(user.<span class="hljs-built_in">id</span>))
<span class="hljs-built_in">print</span>(<span class="hljs-string">f"User ID: </span><span class="hljs-subst">{user.</span><span class="hljs-built_in">id</span><span class="hljs-subst">}</span><span class="hljs-string">"</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f"User name: </span><span class="hljs-subst">{user.first_name}</span><span class="hljs-string"> </span><span class="hljs-subst">{user.last_name}</span><span class="hljs-string">"</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f"Number of articles: </span><span class="hljs-subst">{</span><span class="hljs-built_in">len</span><span class="hljs-subst">(articles)}</span><span class="hljs-string">"</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">"First article link:"</span>, articles[<span class="hljs-number">0</span>].link)
</code></pre>
    <p class="normal">The output of the <a id="_idIndexMarker248"/>code from above is:</p>
    <pre class="programlisting con"><code class="hljs-con">User ID: 900fec95-d621-4315-84c6-52e5229e0b96
User name: Paul Iusztin
Number of articles: 50
First article link: https://medium.com/decodingml/an-end-to-end-framework-for-production-ready-llm-systems-by-building-your-llm-twin-2cc6bb01141f
</code></pre>
    <p class="normal">With only two lines of code, we can query and filter our MongoDB data warehouse using any ODM defined within our project.</p>
    <p class="normal">Also, to ensure that your data collection pipeline works as expected, you can search your MongoDB collections using <a id="_idIndexMarker249"/>your <strong class="keyWord">IDE’s MongoDB plugin, </strong>which you must install separately. For example, you can use this plugin for VSCode: <a href="https://www.mongodb.com/products/tools/vs-code"><span class="url">https://www.mongodb.com/products/tools/vs-code</span></a>. For other IDEs, you can use similar plugins or external NoSQL visualization tools. After connecting to the MongoDB visualization tool, you can connect to our local database using the following URI: <code class="inlineCode">mongodb://llm_engineering:llm_engineering@127.0.0.1:27017</code>. For a cloud MongoDB cluster, you must change the URI, which we will explore in <em class="chapterRef">Chapter 11</em>.</p>
    <p class="normal">And just like that, you’ve learned how to run the data collection pipeline with different ZenML configs and how to visualize the output artifacts of each run. We also looked at how to <a id="_idIndexMarker250"/>query the data warehouse for a particular data category and author. Thus, we’ve finalized our data engineering chapter and can move to the conclusion.</p>
    <h2 id="_idParaDest-80" class="heading-2">Troubleshooting</h2>
    <p class="normal">The raw data stored in <a id="_idIndexMarker251"/>the MongoDB database is central to all future steps. Thus, if you haven’t successfully run the code from this chapter due to any issues with the crawlers, this section provides solutions for fixing potential issues to allow you to move forward.</p>
    <h3 id="_idParaDest-81" class="heading-3">Selenium issues</h3>
    <p class="normal">It is a well-known i<a id="_idIndexMarker252"/>ssue that running Selenium can cause problems due to issues with the browser driver, such as the <code class="inlineCode">ChromeDriver</code>. Thus, if the crawlers that use Selenium, such as the <code class="inlineCode">MediumCrawler</code>, fail due to problems with your <code class="inlineCode">ChromeDriver</code>, you can easily bypass this by commenting out the Medium links added to the data collection YAML configs. To do so, go to the <code class="inlineCode">configs/</code> directory and find all the YAML files that start with <code class="inlineCode">digital_data_etl_*</code>, such as <code class="inlineCode">digital_data_etl_maxime_labonne.yaml</code>. Open them and comment on all the Medium-related URLs, as illustrated in <em class="italic">Figure 3.7</em>. You can leave out the Substack or personal blog URLs as these use the <code class="inlineCode">CustomArticleCrawler</code>, which is not dependent on Selenium.</p>
    <figure class="mediaobject"><img src="../Images/B31105_03_07.png" alt=""/></figure>
    <p class="packt_figref">Figure 3.7: Fix Selenium issues when crawling raw data</p>
    <h3 id="_idParaDest-82" class="heading-3">Import our backed-up data</h3>
    <p class="normal">If nothing works, there <a id="_idIndexMarker253"/>is the possibility of populating the MongoDB database with your backed-up <a id="_idIndexMarker254"/>data saved under the <code class="inlineCode">data/data_warehouse_raw_data directory</code>. This will allow you to proceed to the fine-tuning and inference sections without running the data collection ETL code. To import all the data within this directory, run:</p>
    <pre class="programlisting con"><code class="hljs-con">poetry poe run-import-data-warehouse-from-json
</code></pre>
    <p class="normal">After running the CLI command from above, you will have a one-to-one replica of the dataset we used while developing the code. To ensure the import is completed successfully, you should have 88 articles and 3 users in your MongoDB database.</p>
    <h1 id="_idParaDest-83" class="heading-1">Summary</h1>
    <p class="normal">In this chapter, we’ve learned how to design and build the data collection pipeline for the LLM Twin use case. Instead of relying on static datasets, we collected our custom data to mimic real-world situations, preparing us for real-world challenges in building AI systems.</p>
    <p class="normal">First, we examined the architecture of LLM Twin’s data collection pipeline, which functions as an ETL process. Next, we started digging into the pipeline implementation. We began by understanding how we can orchestrate the pipeline using ZenML. Then, we looked into the crawler implementation. We learned how to crawl data in three ways: using CLI commands in subprocesses or using utility functions from LangChain or Selenium to build custom logic that programmatically manipulates the browser. Finally, we looked into how to build our own ODM class, which we used to define our document class hierarchy, which contains entities such as articles, posts, and repositories.</p>
    <p class="normal">At the end of the chapter, we learned how to run ZenML pipelines with different YAML configuration files and explore the results in the dashboard. We also saw how to interact with the MongoDB data warehouse through the ODM classes.</p>
    <p class="normal">In the next chapter, we will cover the key steps of the RAG feature pipeline, including chunking and embedding documents, ingesting these documents into a vector DB, and applying pre-retrieval optimizations to improve performance. We will also set up the necessary infrastructure programmatically using Pulumi and conclude by deploying the RAG ingestion pipeline to AWS.</p>
    <h1 id="_idParaDest-84" class="heading-1">References</h1>
    <ul>
      <li class="bulletList">Breuss, M. (2023, July 26). <em class="italic">Beautiful Soup: Build a Web Scraper With Python</em>. <a href="https://realpython.com/beautiful-soup-web-scraper-python/"><span class="url">https://realpython.com/beautiful-soup-web-scraper-python/</span></a></li>
      <li class="bulletList">David, D. (2024, July 8). <em class="italic">Guide to Web Scraping with Selenium in 2024</em>. Bright Data. <a href="https://brightdata.com/blog/how-tos/using-selenium-for-web-scraping"><span class="url">https://brightdata.com/blog/how-tos/using-selenium-for-web-scraping</span></a></li>
      <li class="bulletList">Hjelle, G. A. (2023, October 21). <em class="italic">Python 3.12 Preview: Static Typing Improvements</em>. <a href="https://realpython.com/python312-typing/"><span class="url">https://realpython.com/python312-typing/</span></a></li>
      <li class="bulletList"><em class="italic">ORM Quick Start — SQLAlchemy 2.0 documentation</em>. (n.d.). <a href="https://docs.sqlalchemy.org/en/20/orm/quickstart.html "><span class="url">https://docs.sqlalchemy.org/en/20/orm/quickstart.html</span></a></li>
      <li class="bulletList">Ramos, L. P. (2023, August 4). <em class="italic">Python and MongoDB: Connecting to NoSQL Databases</em>. <a href="https://realpython.com/introduction-to-mongodb-and-python/"><span class="url">https://realpython.com/introduction-to-mongodb-and-python/</span></a></li>
      <li class="bulletList">Refactoring.Guru. (2024, January 1). <em class="italic">Builder</em>. <a href="https://refactoring.guru/design-patterns/builder"><span class="url">https://refactoring.guru/design-patterns/builder</span></a></li>
      <li class="bulletList"><em class="italic">What is ETL? A complete guide</em>. (n.d.). Qlik. <a href="https://www.qlik.com/us/etl  "><span class="url">https://www.qlik.com/us/etl</span></a></li>
    </ul>
    <p class="normal"><a href="https://www.qlik.com/us/etl  "/></p>
    <h1 id="_idParaDest-85" class="heading-1">Join our book’s Discord space</h1>
    <p class="normal">Join our community’s Discord space for discussions with the authors and other readers:</p>
    <p class="normal"><a href="https://packt.link/llmeng"><span class="url">https://packt.link/llmeng</span></a></p>
    <p class="normal"><span class="url"><img src="../Images/QR_Code79969828252392890.png" alt=""/></span></p>
  </div>
</body></html>