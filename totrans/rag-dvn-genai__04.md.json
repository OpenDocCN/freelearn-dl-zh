["```py\nimport deeplake\ndataset_path_llm = \"hub://denis76/drone_v2\"\nds_llm = deeplake.load(dataset_path_llm) \n```", "```py\nThis dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/denis76/drone_v2\nhub://denis76/drone_v2 loaded successfully. \n```", "```py\nimport json\nimport pandas as pd\nimport numpy as np\n# Create a dictionary to hold the data\ndata_llm = {}\n# Iterate through the tensors in the dataset\nfor tensor_name in ds_llm.tensors:\n    tensor_data = ds_llm[tensor_name].numpy()\n    # Check if the tensor is multi-dimensional\n    if tensor_data.ndim > 1:\n        # Flatten multi-dimensional tensors\n        data_llm[tensor_name] = [np.array(e).flatten().tolist() for e in tensor_data]\n    else:\n        # Convert 1D tensors directly to lists and decode text\n        if tensor_name == \"text\":\n            data_llm[tensor_name] = [t.tobytes().decode('utf-8') if t else \"\" for t in tensor_data]\n        else:\n            data_llm[tensor_name] = tensor_data.tolist()\n# Create a Pandas DataFrame from the dictionary\ndf_llm = pd.DataFrame(data_llm)\ndf_llm \n```", "```py\nfrom llama_index.core import VectorStoreIndex\nvector_store_index_llm = VectorStoreIndex.from_documents(documents_llm) \n```", "```py\nvector_query_engine_llm = vector_store_index_llm.as_query_engine(similarity_top_k=2, temperature=0.1, num_output=1024) \n```", "```py\nuser_input=\"How do drones identify a truck?\" \n```", "```py\nimport time\nimport textwrap\n#start the timer\nstart_time = time.time()\nllm_response = vector_query_engine_llm.query(user_input)\n# Stop the timer\nend_time = time.time()\n# Calculate and print the execution time\nelapsed_time = end_time - start_time\nprint(f\"Query execution time: {elapsed_time:.4f} seconds\")\nprint(textwrap.fill(str(llm_response), 100)) \n```", "```py\nQuery execution time: 1.5489 seconds \n```", "```py\nDrones can identify a truck using visual detection and tracking methods, which may involve deep neural networks for performance benchmarking. \n```", "```py\nimport deeplake\ndataset_path = 'hub://activeloop/visdrone-det-train'\nds = deeplake.load(dataset_path) # Returns a Deep Lake Dataset but does not download data locally \n```", "```py\nOpening dataset in read-only mode as you don't have write permissions.\nThis dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/activeloop/visdrone-det-train\nhub://activeloop/visdrone-det-train loaded successfully. \n```", "```py\nds.summary() \n```", "```py\nDataset(path='hub://activeloop/visdrone-det-train', read_only=True, tensors=['boxes', 'images', 'labels'])\ntensor    htype            shape              dtype     compression\n------    -----            -----              -----     -----------\nboxes     bbox         (6471, 1:914, 4)       float32          None\nimages    image        (6471, 360:1500,                            \n                        480:2000, 3)          uint8            jpeg\nlabels    class_label  (6471, 1:914)          uint32           None \n```", "```py\nds.visualize() \n```", "```py\nimport pandas as pd\n# Create an empty DataFrame with the defined structure\ndf = pd.DataFrame(columns=['image', 'boxes', 'labels'])\n# Iterate through the samples using enumerate\nfor i, sample in enumerate(ds):\n    # Image data (choose either path or compressed representation)\n    # df.loc[i, 'image'] = sample.images.path  # Store image path\n    df.loc[i, 'image'] = sample.images.tobytes()  # Store compressed image data\n    # Bounding box data (as a list of lists)\n    boxes_list = sample.boxes.numpy(aslist=True)\n    df.loc[i, 'boxes'] = [box.tolist() for box in boxes_list]\n    # Label data (as a list)\n    label_data = sample.labels.data()\n    df.loc[i, 'labels'] = label_data['text']\ndf \n```", "```py\nlabels_list = ds.labels.info['class_names']\nlabels_list \n```", "```py\n['ignored regions',\n 'pedestrian',\n 'people',\n 'bicycle',\n 'car',\n 'van',\n 'truck',\n 'tricycle',\n 'awning-tricycle',\n 'bus',\n 'motor',\n 'others'] \n```", "```py\n# choose an image\nind=0\nimage = ds.images[ind].numpy() # Fetch the first image and return a numpy array \n```", "```py\nimport deeplake\nfrom IPython.display import display\nfrom PIL import Image\nimport cv2  # Import OpenCV\nimage = ds.images[0].numpy()\n# Convert from BGR to RGB (if necessary)\nimage_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n# Create PIL Image and display\nimg = Image.fromarray(image_rgb)\ndisplay(img) \n```", "```py\nlabels = ds.labels[ind].data() # Fetch the labels in the selected image\nprint(labels) \n```", "```py\n{'value': array([1, 1, 7, 1, 1, 1, 1, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 1, 6, 6, 6, 6,\n       1, 1, 1, 1, 1, 1, 6, 6, 3, 6, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 6, 6, 6], dtype=uint32), 'text': ['pedestrian', 'pedestrian', 'tricycle', 'pedestrian', 'pedestrian', 'pedestrian', 'pedestrian', 'truck', 'truck', 'truck', 'truck', 'truck', 'truck', 'truck', 'truck', 'truck', 'truck', 'pedestrian', 'truck', 'truck', 'truck', 'truck', 'pedestrian', 'pedestrian', 'pedestrian', 'pedestrian', 'pedestrian', 'pedestrian', 'truck', 'truck', 'bicycle', 'truck', 'truck', 'pedestrian', 'pedestrian', 'pedestrian', 'pedestrian', 'pedestrian', 'pedestrian', 'pedestrian', 'pedestrian', 'pedestrian', 'pedestrian', 'pedestrian', 'pedestrian', 'truck', 'truck', 'truck']} \n```", "```py\nvalues = labels['value']\ntext_labels = labels['text']\n# Determine the maximum text label length for formatting\nmax_text_length = max(len(label) for label in text_labels)\n# Print the header\nprint(f\"{'Index':<10}{'Label':<{max_text_length + 2}}\")\nprint(\"-\" * (10 + max_text_length + 2))  # Add a separator line\n# Print the indices and labels in two columns\nfor index, label in zip(values, text_labels):\n    print(f\"{index:<10}{label:<{max_text_length + 2}}\") \n```", "```py\nIndex     Label     \n----------------------\n1         pedestrian\n1         pedestrian\n7         tricycle  \n1         pedestrian\n1         pedestrian\n1         pedestrian\n1         pedestrian\n6         truck     \n6         truck    … \n```", "```py\nds.labels[ind].info['class_names'] # class names of the selected image \n```", "```py\nds.labels[ind].info['class_names'] #class names of the selected image \n```", "```py\n['ignored regions',\n 'pedestrian',\n 'people',\n 'bicycle',\n 'car',\n 'van',\n 'truck',\n 'tricycle',\n 'awning-tricycle',\n 'bus',\n 'motor',\n 'others'] \n```", "```py\ndef display_image_with_bboxes(image_data, bboxes, labels, label_name, ind=0):\n    #Displays an image with bounding boxes for a specific label.\n    image_bytes = io.BytesIO(image_data)\n    img = Image.open(image_bytes)\n    # Extract class names specifically for the selected image\n    class_names = ds.labels[ind].info['class_names']\n    # Filter for the specific label (or display all if class names are missing)\n    if class_names is not None:\n        try:\n            label_index = class_names.index(label_name)\n            relevant_indices = np.where(labels == label_index)[0]\n        except ValueError:\n            print(f\"Warning: Label '{label_name}' not found. Displaying all boxes.\")\n            relevant_indices = range(len(labels))\n    else:\n        relevant_indices = []  # No labels found, so display no boxes\n    # Draw bounding boxes\n    draw = ImageDraw.Draw(img)\n    for idx, box in enumerate(bboxes):  # Enumerate over bboxes\n        if idx in relevant_indices:   # Check if this box is relevant\n            x1, y1, w, h = box\n            x2, y2 = x1 + w, y1 + h\n            draw.rectangle([x1, y1, x2, y2], outline=\"red\", width=2)\n            draw.text((x1, y1), label_name, fill=\"red\")\n    # Save the image\n    save_path=\"boxed_image.jpg\"\n    img.save(save_path)\n    display(img) \n```", "```py\nimport io\nfrom PIL import ImageDraw\n# Fetch labels and image data for the selected image\nlabels = ds.labels[ind].data()['value']\nimage_data = ds.images[ind].tobytes()\nbboxes = ds.boxes[ind].numpy()\nibox=\"truck\" # class in image\n# Display the image with bounding boxes for the label chosen\ndisplay_image_with_bboxes(image_data, bboxes, labels, label_name=ibox) \n```", "```py\n# The DataFrame is named 'df'\ndf['doc_id'] = df.index.astype(str)  # Create unique IDs from the row indices \n```", "```py\n# Create documents (extract relevant text for each image's labels)\ndocuments = [] \n```", "```py\nfor _, row in df.iterrows():\n    text_labels = row['labels'] # Each label is now a string\n    text = \" \".join(text_labels) # Join text labels into a single string\n    document = Document(text=text, doc_id=row['doc_id'])\n    documents.append(document) \n```", "```py\n# The DataFrame is named 'df'\ndf['doc_id'] = df.index.astype(str)  # Create unique IDs from the row indices\n# Create documents (extract relevant text for each image's labels)\ndocuments = []\nfor _, row in df.iterrows():\n    text_labels = row['labels'] # Each label is now a string\n    text = \" \".join(text_labels) # Join text labels into a single string\n    document = Document(text=text, doc_id=row['doc_id'])\n    documents.append(document) \n```", "```py\nfrom llama_index.core import GPTVectorStoreIndex\nvector_store_index = GPTVectorStoreIndex.from_documents(documents) \n```", "```py\nvector_store_index.index_struct \n```", "```py\nIndexDict(index_id='4ec313b4-9a1a-41df-a3d8-a4fe5ff6022c', summary=None, nodes_dict={'5e547c1d-0d65-4de6-b33e-a101665751e6': '5e547c1d-0d65-4de6-b33e-a101665751e6', '05f73182-37ed-4567-a855-4ff9e8ae5b8c': '05f73182-37ed-4567-a855-4ff9e8ae5b8c' \n```", "```py\nvector_query_engine = vector_store_index.as_query_engine(similarity_top_k=1, temperature=0.1, num_output=1024) \n```", "```py\nimport time\nstart_time = time.time()\nresponse = vector_query_engine.query(user_input)\n# Stop the timer\nend_time = time.time()\n# Calculate and print the execution time\nelapsed_time = end_time - start_time\nprint(f\"Query execution time: {elapsed_time:.4f} seconds\") \n```", "```py\nQuery execution time: 1.8461 seconds \n```", "```py\nprint(textwrap.fill(str(response), 100)) \n```", "```py\nfrom itertools import groupby\ndef get_unique_words(text):\n    text = text.lower().strip()\n    words = text.split()\n    unique_words = [word for word, _ in groupby(sorted(words))]\n    return unique_words\nfor node in response.source_nodes:\n    print(node.node_id)\n    # Get unique words from the node text:\n    node_text = node.get_text()\n    unique_words = get_unique_words(node_text)\n    print(\"Unique Words in Node Text:\", unique_words) \n```", "```py\n1af106df-c5a6-4f48-ac17-f953dffd2402\nUnique Words in Node Text: ['truck'] \n```", "```py\n# deleting any image previously saved\n!rm /content/boxed_image.jpg \n```", "```py\ndisplay_image_with_bboxes(image_data, bboxes, labels, label_name=ibox) \n```", "```py\nimport io\nfrom PIL import Image\ndef process_and_display(response, df, ds, unique_words):\n    \"\"\"Processes nodes, finds corresponding images in dataset, and displays them with bounding boxes.\n    Args:\n        response: The response object containing source nodes.\n        df: The DataFrame with doc_id information.\n        ds: The dataset containing images, labels, and boxes.\n        unique_words: The list of unique words for filtering.\n    \"\"\"\n…\n            if i == row_index:\n                image_bytes = io.BytesIO(sample.images.tobytes())\n                img = Image.open(image_bytes)\n                labels = ds.labels[i].data()['value']\n                image_data = ds.images[i].tobytes()\n                bboxes = ds.boxes[i].numpy()\n                ibox = unique_words[0]  # class in image\n                display_image_with_bboxes(image_data, bboxes, labels, label_name=ibox)\n# Assuming you have your 'response', 'df', 'ds', and 'unique_words' objects prepared:\nprocess_and_display(response, df, ds, unique_words) \n```", "```py\n# 1.user input=user_input\nprint(user_input)\n# 2.LLM response\nprint(textwrap.fill(str(llm_response), 100))\n# 3.Multimodal response\nimage_path = \"/content/boxed_image.jpg\"\ndisplay_source_image(image_path) \n```", "```py\nHow do drones identify a truck?\nDrones can identify a truck using visual detection and tracking methods, which may involve deep neural networks for performance benchmarking. \n```", "```py\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\ndef calculate_cosine_similarity_with_embeddings(text1, text2):\n    embeddings1 = model.encode(text1)\n    embeddings2 = model.encode(text2)\n    similarity = cosine_similarity([embeddings1], [embeddings2])\n    return similarity[0][0] \n```", "```py\nllm_similarity_score = calculate_cosine_similarity_with_embeddings(user_input, str(llm_response))\nprint(user_input)\nprint(llm_response)\nprint(f\"Cosine Similarity Score: {llm_similarity_score:.3f}\") \n```", "```py\nHow do drones identify a truck?\nHow do drones identify a truck?\nDrones can identify a truck using visual detection and tracking methods, which may involve deep neural networks for performance benchmarking.\nCosine Similarity Score: 0.691 \n```", "```py\nimport base64\nIMAGE_PATH = \"/content/boxed_image.jpg\"\n# Open the image file and encode it as a base64 string\ndef encode_image(image_path):\n    with open(image_path, \"rb\") as image_file:\n        return base64.b64encode(image_file.read()).decode(\"utf-8\")\nbase64_image = encode_image(IMAGE_PATH) \n```", "```py\nfrom openai import OpenAI\n#Set the API key for the client\nclient = OpenAI(api_key=openai.api_key)\nMODEL=\"gpt-4o\" \n```", "```py\nu_word=unique_words[0]\nprint(u_word) \n```", "```py\nresponse = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": f\"You are a helpful assistant that analyzes images that contain {u_word}.\"},\n        {\"role\": \"user\", \"content\": [\n            {\"type\": \"text\", \"text\": f\"Analyze the following image, tell me if there is one {u_word} or more in the bounding boxes and analyze them:\"},\n            {\"type\": \"image_url\", \"image_url\": {\n                \"url\": f\"data:image/png;base64,{base64_image}\"}\n            }\n        ]}\n    ],\n    temperature=0.0,\n)\nresponse_image = response.choices[0].message.content\nprint(response_image) \n```", "```py\nThe image contains two trucks within the bounding boxes. Here is the analysis of each truck:\n1\\. **First Truck (Top Bounding Box)**:\n   - The truck appears to be a flatbed truck.\n   - It is loaded with various materials, possibly construction or industrial supplies.\n   - The truck is parked in an area with other construction materials and equipment.\n2\\. **Second Truck (Bottom Bounding Box)**:\n   - This truck also appears to be a flatbed truck.\n   - It is carrying different types of materials, similar to the first truck.\n   - The truck is situated in a similar environment, surrounded by construction materials and equipment.\nBoth trucks are in a construction or industrial area, likely used for transporting materials and equipment. \n```", "```py\nresp=u_word+\"s\"\nmultimodal_similarity_score = calculate_cosine_similarity_with_embeddings(resp, str(response_image))\nprint(f\"Cosine Similarity Score: {multimodal_similarity_score:.3f}\") \n```", "```py\nCosine Similarity Score: 0.505 \n```", "```py\nscore=(llm_similarity_score+multimodal_similarity_score)/2\nprint(f\"Multimodal, Modular Score: {score:.3f}\") \n```", "```py\nMultimodal, Modular Score: 0.598 \n```"]