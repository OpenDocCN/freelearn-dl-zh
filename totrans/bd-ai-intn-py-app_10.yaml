- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Refining the Semantic Data Model to Improve Accuracy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To effectively use vector search for semantic long-term memory in an intelligent
    application, you must optimize the semantic data model to the application’s needs.
    As the semantic data model uses vector embedding models and vector search, you
    must optimize the contents of the embedded data and the way the data is retrieved.
  prefs: []
  type: TYPE_NORMAL
- en: Refining the semantic data model can lead to significant improvements in retrieval
    accuracy and overall application performance. In **retrieval-augmented generation**
    (**RAG**) applications, an effective semantic data model serves as the foundation
    for a robust retrieval system, which directly informs the quality of the generated
    outputs. The rest of the chapter examines different ways in which you can refine
    the semantic data model and retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Experimenting with different embedding models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning embedding models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Including metadata in the embedded content to maximize semantic relevance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Various techniques to optimize RAG use cases, including query mutation, formatting
    ingested data, and advanced retrieval systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You will need the following technical requirements to run the code in this
    chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: A programming environment with Python 3.x installed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A programming environment capable of running the open source embedding model
    `gte-base-en-v1.5` locally
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An OpenAI API key. To create an API key, refer to the OpenAI documentation at
    [https://platform.openai.com/docs/quickstart/step-2-set-up-your-api-key](https://platform.openai.com/docs/quickstart/step-2-set-up-your-api-key)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Vector embeddings** are the foundation of the semantic data model, serving
    as the machine-interpretable representation of ideas and relationships. Embeddings
    are mathematical representations of objects as points in a multi-dimensional space.
    They act as the glue that connects the various semantic pieces of data in an intelligent
    application. The distance between vectors correlates to semantic similarity. You
    can use this semantic similarity score to retrieve related information that would
    otherwise be difficult to connect. This concept holds true regardless of the specific
    use case, be it RAG, recommendation systems, anomaly detection, or others.'
  prefs: []
  type: TYPE_NORMAL
- en: Having an embedding model better tailored to a use case can improve accuracy
    and performance. Experimenting with different embedding models and fine-tuning
    them on domain-specific data can help identify the best fit for a particular use
    case, further enhancing their effectiveness.
  prefs: []
  type: TYPE_NORMAL
- en: Experimenting with different embedding models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When building intelligent applications, you can experiment with different pre-trained
    embedding models. Different models have varying accuracy, cost, and efficiency.
    Their performance can vary significantly depending on the specific application
    and data. By experimenting with multiple models, developers can identify the best
    fit for their use case.
  prefs: []
  type: TYPE_NORMAL
- en: '*Table 10.1* lists some popular embedding models as of writing in spring 2024
    that are taken from the Hugging Face **Massive Test Embedding Benchmark** (**MTEB**)
    Leaderboard:[1](B22495_10.xhtml#footnote-002)'
  prefs: []
  type: TYPE_NORMAL
- en: '[1](B22495_10.xhtml#footnote-002-backlink) The information from the MTEB Leaderboard
    was taken on April 30, 2024\. ([https://huggingface.co/spaces/mteb/leaderboard](https://huggingface.co/spaces/mteb/leaderboard))'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Model name** | **Developer** | **Is it open** **source?** | **Embedding**
    **length** | **Average** **score**[2](B22495_10.xhtml#footnote-001)[2](B22495_10.xhtml#footnote-001-backlink)
    This score is calculated as an average of a variety of benchmarks. For more information
    about the evaluation metrics used in the benchmark, refer to the *MTEB: Massive
    Text Embedding Benchmark* research paper ([https://arxiv.org/abs/2210.07316](https://arxiv.org/abs/2210.07316)).
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `text-embedding-3-large` | OpenAI | No | 3072 | 64.59 |'
  prefs: []
  type: TYPE_TB
- en: '| `cohere-embed-english-v3.0` | Cohere | No | 1024 | 64.47 |'
  prefs: []
  type: TYPE_TB
- en: '| `gte-base-en-v1.5` | Alibaba | Yes | 768 | 64.11 |'
  prefs: []
  type: TYPE_TB
- en: '| `sentence-t5-large` | Sentence Transformers | Yes | 768 | 57.06 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 10.1: Selected embedding models'
  prefs: []
  type: TYPE_NORMAL
- en: To properly compare different embedding models, you must have a consistent evaluation
    framework. This involves defining a set of relevant evaluation datasets and metrics.
    Use the same evaluation sets and metrics across all models for fair comparison.
    The evaluation datasets should be representative of the relevant application domain.
    An evaluation framework will help you iterate and refine the evaluation process
    over time, incorporating learnings from initial experiments to progressively improve
    the application.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are useful evaluation metrics for using embedding models for
    information retrieval. The metrics are taken from **Ragas**, a framework for RAG
    evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Context precision**: Evaluates whether the retrieved results contain ground-truth
    facts that you would want to answer the input query. Relevant items present in
    the contexts are ranked highly in the retrieved results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Context entities recall**: Evaluates what fraction of the entities from a
    set of ground truths are preset in the retrieved information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ragas supports other RAG evaluation metrics as well, which you can learn more
    about in the Ragas documentation (https://docs.ragas.io/en/stable/).
  prefs: []
  type: TYPE_NORMAL
- en: The following code example uses Ragas and LangChain to evaluate how different
    embedding models perform on the context entities recall metric.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, install the required dependencies in the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code evaluates how the OpenAI `text-embedding-ada-002` and `text-embedding-3-large`
    embedding models perform on the Ragas context entities recall evaluation for a
    sample dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This code outputs results resembling the following to the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from these results, `text-embedding-3-large` yields higher context
    entity recall on this evaluation. The context relevancy score is normalized between
    `0` and `1`, inclusive.
  prefs: []
  type: TYPE_NORMAL
- en: When creating evaluations for your own application, consider using sample data
    that’s relevant to your use case for a better comparison. Also, you will likely
    want to include a representative sample of at least 100 examples.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning embedding models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to experimenting with different pre-trained models, you can fine-tune
    a pre-trained embedding model to optimize it for your use case.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fine-tuning an embedding model can be beneficial in the following scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Domain-specific data**: If the application deals with domain-specific data
    that might not be well captured using an off-the-shelf model, such as legal documents
    or medical records with specialized terminology, fine-tuning can help the model
    better understand and represent the domain-specific concepts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Avoiding undesirable matches**: In cases where there are seemingly similar
    concepts that should be differentiated, fine-tuning can help the model distinguish
    between them. For example, you could fine-tune the model to differentiate between
    *Apple the company* and *apple* *the fruit*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, off-the-shelf embedding models are often highly performant for many
    tasks, especially when combined with the metadata enrichment and RAG optimizations
    discussed later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The available options for fine-tuning an embedding model can vary depending
    on the model and how it is hosted. Managed model hosting providers might only
    expose certain methods for their models, whereas using an open source model can
    provide more flexibility. The **SentenceTransformers** ([https://sbert.net/](https://sbert.net/))
    framework is designed for using and fine-tuning open-source embedding models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally, fine-tuning involves providing similar pairs of sentences, optionally
    including a magnitude of similarity. Alternatively, anchor, positive, and negative
    examples can be provided to guide the fine-tuning process. *Table 10.2* provides
    an overview of anchor, positive, and negative examples, that are used in the subsequent
    code example:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Type** | **Definition** | **Example** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Anchor | The reference text that serves as the starting point for identifying
    similar and dissimilar examples. |'
  prefs: []
  type: TYPE_TB
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Positive | Text that should be represented as similar to the anchor example.
    |'
  prefs: []
  type: TYPE_TB
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Negative | Text that should be represented as dissimilar or different from
    the anchor example. |'
  prefs: []
  type: TYPE_TB
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 10.2: Methods for fine-tuning embedding models'
  prefs: []
  type: TYPE_NORMAL
- en: Here’s a brief code example of using the `SentenceTransformers` and `PyTorch`
    libraries to fine-tune the open source embedding model `gte-base-en-v1.5`.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, install the dependencies in the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Then run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This code outputs a result resembling the following to the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from this example, just the small fine-tuning that was performed,
    increased the vector similarity between the related sentences.
  prefs: []
  type: TYPE_NORMAL
- en: If you would like to learn more about fine-tuning embedding models, a great
    place to start is the *Train and Fine-Tune Sentence Transformers Models Hugging
    Face* blog post by Omar Espejel ([https://huggingface.co/blog/how-to-train-sentence-transformers](https://huggingface.co/blog/how-to-train-sentence-transformers)).
    This blog post includes a more detailed look at fine-tuning an embedding model
    using a similar approach to the one in the preceding code example.
  prefs: []
  type: TYPE_NORMAL
- en: The following section discusses how you can further enhance the semantic data
    model after you have chosen the right data model by embedding relevant metadata
    in the text.
  prefs: []
  type: TYPE_NORMAL
- en: Embedding metadata
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Including **metadata** in embedded content can significantly improve the quality
    of retrieval results by adding greater semantic meaning to it. Metadata creates
    a richer and more meaningful semantic representation of content. Metadata can
    include descriptors such as the type of content, tags, titles, and summaries.
    The following table contains some useful examples of metadata to include in embedded
    content:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Type** | **Example(s)** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Content type | Article, recipe, product review, etc. |'
  prefs: []
  type: TYPE_TB
- en: '| Tags | “dinner”, “Italian”, “vegetarian” |'
  prefs: []
  type: TYPE_TB
- en: '| Title of document | Roasted Garlic and Tomato Pasta |'
  prefs: []
  type: TYPE_TB
- en: '| Summary of document | A simple pasta dish featuring roasted garlic and cherry
    tomatoes in a light sauce |'
  prefs: []
  type: TYPE_TB
- en: 'Table 10.3: Useful types of embedded metadata'
  prefs: []
  type: TYPE_NORMAL
- en: You can also include metadata types that are specific to your application. For
    example, consider creating a RAG chatbot where users ask natural language questions
    and get generated answers on cooking and recipes.
  prefs: []
  type: TYPE_NORMAL
- en: 'You have the following recipe for *Roast Garlic and Tomato Pasta* to include
    in your recipe database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'When creating a vector embedding for the recipe, you could include the following
    metadata before the recipe text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: By including this metadata with the embedded text, you imbue the text with greater
    semantic meaning making it more likely that user queries will capture the correct
    content. This makes relevant user queries have greater cosine similarity scores
    with the text.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table shows the cosine similarity scores between various queries
    and the text with and without metadata using the `BAAI/bge-large-en-v1.5` embedding
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Query text** | **Text without metadata** **similarity score** | **Text
    with metadata** **similarity score** | **Metadata similarity** **score improvement**
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '| `0.7141546` | `0.7306514` | `0.016496778` |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '| `0.71199816` | `0.76754296` | `0.055544794` |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '| `0.60327804` | `0.6559261` | `0.052648067` |'
  prefs: []
  type: TYPE_TB
- en: 'Table 10.4: Comparing cosine similarity of vectors of text with and without
    embedded metadata'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in *Table 10.4*, the text with prepended metadata has a higher
    cosine similarity for a diverse set of relevant queries. This means that the relevant
    content is more likely to be surfaced and used in the RAG chatbot.
  prefs: []
  type: TYPE_NORMAL
- en: Formatting metadata
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When including metadata, you must consider how it is structured to optimize
    processing and interpretation. You should use a machine-readable format that is
    easy to parse and manipulate, such as YAML ([https://yaml.org/spec/1.2.2/](https://yaml.org/spec/1.2.2/)),
    JSON ([https://www.json.org/json-en.html](https://www.json.org/json-en.html)),
    or TOML ([https://toml.io/](https://toml.io/)).
  prefs: []
  type: TYPE_NORMAL
- en: '**YAML** is generally more token-efficient compared to other data formats such
    as **JSON**. This means that using YAML saves on the compute cost of processing
    extra tokens and also represents the same idea with fewer *distraction* tokens
    that could dilute the LLM’s ability to interpret the input and produce a high-quality
    output. YAML also has widespread adoption, so embedding models and LLMs can effectively
    work with it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table demonstrates the comparative token density for the same
    data represented in YAML and JSON using the GPT-4 tokenizer ([https://platform.openai.com/tokenizer](https://platform.openai.com/tokenizer)):'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Format** | **Content** | **Token** **count** |'
  prefs: []
  type: TYPE_TB
- en: '| YAML |'
  prefs: []
  type: TYPE_TB
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '| 60 |'
  prefs: []
  type: TYPE_TB
- en: '| JSON |'
  prefs: []
  type: TYPE_TB
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '| 89 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 10.5: Comparing token length of the same content in YAML and JSON'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in *Table 10.5*, YAML uses approximately two-thirds of tokens
    compared to JSON. The exact difference in token usage depends on the data and
    formatting. YAML generally proves to be a more efficient metadata format than
    JSON.
  prefs: []
  type: TYPE_NORMAL
- en: If including metadata alongside additional text, consider including it as *front
    matter* ([https://jekyllrb.com/docs/front-matter/](https://jekyllrb.com/docs/front-matter/)).
    `---` before and after the metadata.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of front matter preceding Markdown ([https://commonmark.org/help/](https://commonmark.org/help/))
    text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The front matter specification originates from the Jekyll static site builder
    ([https://jekyllrb.com/docs/](https://jekyllrb.com/docs/)). It has since become
    widely adopted across various domains. Given its popularity, language models and
    embedding models should be able to understand its semantic context as metadata
    for the rest of the text. Additionally, libraries are available to easily manipulate
    front matter in relation to the main text content, such as the `python-frontmatter`
    in Python.
  prefs: []
  type: TYPE_NORMAL
- en: The following code example shows how to add front matter to Markdown and print
    out the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, install the `python-frontmatter` package in the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Add front matter to text using the `python-frontmatter` library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'This outputs the following text with front matter to the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The preceding example demonstrates the usefulness of adding front matter as
    a metadata format in your semantic retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: Including static metadata
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For certain types of content or sources, it can be beneficial to include static
    metadata that is the same across all documents. This is a computationally cheap
    and an easy way to consistently include metadata across documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a cookbook chatbot, you could include the cookbook source in the metadata.
    For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: This ensures that every document of a particular type or from a specific source
    contains a consistent base level of metadata. You can then layer on additional
    dynamic metadata that is unique to each specific document, as discussed in the
    following sections. Including static metadata is a low-effort way to provide additional
    semantic context to your documents, aiding in retrieval and interpretation.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting metadata programmatically
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can extract metadata from content using traditional software development
    techniques that do not rely on AI models.
  prefs: []
  type: TYPE_NORMAL
- en: One approach is to extract headers in a document, which can be done with **regular
    expressions** (**regex**) to match header patterns or by parsing the document’s
    **abstract syntax tree** (**AST**) to identify header elements. Extracting and
    including headings as metadata can be useful because headings frequently summarize
    or provide high-level information about the content in that section, thus aiding
    in understanding the semantic context and improving retrieval relevance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Extracting the headers from a Markdown document could create a document with
    metadata resembling the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Generating metadata with LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can use LLMs to generate metadata for your content. Some potential use
    cases for using LLMs to generate metadata include:'
  prefs: []
  type: TYPE_NORMAL
- en: Summarizing the text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting key phrases or terms from the text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying the text into categories
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying the sentiment of the text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recognizing named entities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When selecting an LLM for metadata generation, you may be able to use smaller
    (and therefore faster and cheaper) language models compared to those used for
    other components of your intelligent application.
  prefs: []
  type: TYPE_NORMAL
- en: You can also use traditional **natural language processing** (**NLP**) techniques
    to provide additional metadata. For example, **calculating n-grams** can surface
    the most frequently occurring terms or phrases in the text. Other NLP approaches
    such as **part-of-speech tagging** and **keyword tagging** can also provide useful
    metadata. These approaches typically use small AI models.
  prefs: []
  type: TYPE_NORMAL
- en: You can use the Python NLP libraries, such as `NLTK` or `spaCy`, to extract
    metadata. While using these libraries is generally more compute efficient than
    using an LLM, they generally require fine-tuning, so it’s not worthwhile to use
    them unless your application is running at a scale where the compute requirements
    of an LLM are cost or resource prohibitive.
  prefs: []
  type: TYPE_NORMAL
- en: The following code uses the OpenAI GPT-4o mini LLM to extract the metadata.
    It also uses Pydantic to format the response as JSON.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, install the dependencies in the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, execute the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'This code produces an output resembling the following to the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: As you saw here, LLMs allow you to perform many forms of NLP tasks with prompt
    engineering and minimal technical overhead.
  prefs: []
  type: TYPE_NORMAL
- en: Including metadata with query embedding and ingested content embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to including metadata with the content that you ingest into a vector
    store, you can also include metadata along with the content that you use in your
    search query. By structuring the metadata similarly on both the query and the
    retrieved content, you increase the likelihood of a relevant match using vector
    similarity search.
  prefs: []
  type: TYPE_NORMAL
- en: You can extract metadata for the query using the same strategies as those for
    extracting metadata from the data sources as discussed previously in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, say you’re querying the cookbook chatbot mentioned previously.
    Given the user query `apple pie recipe`, you might want to use the following query
    for vector search:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'A query such as the above will make it more likely to match a recipe with similarly
    structured embedded metadata like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Including structured metadata in the query can act as a kind of *semantic filter*
    to get more accurate search results. The following section examines other techniques
    for improving the accuracy of the data model in RAG applications.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing retrieval-augmented generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Beyond optimizing the semantic data model itself through vector embedding model
    choice and metadata enrichment, there are ways to further refine and improve RAG
    applications. This section covers strategies for optimizing different components
    and stages of the RAG pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Key areas of optimization include query handling, formatting of ingested data,
    retrieval system configuration, and application-level guardrails. Effectively
    optimizing these aspects can lead to significant boosts in the accuracy, relevance,
    and overall performance of RAG applications.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This section covers more advanced techniques than the ones discussed in [*Chapter
    8*](B22495_08.xhtml#_idTextAnchor180), *Implementing Vector Search in* *AI Applications.*
  prefs: []
  type: TYPE_NORMAL
- en: Query mutation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the naive RAG approach, you use direct user input to create the embedding
    used in vector search, perhaps augmented with metadata as discussed earlier in
    the chapter. However, you can drive better search performance by mutating the
    user input using an LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 'Several popular techniques for query mutation include:'
  prefs: []
  type: TYPE_NORMAL
- en: '`My daughter is allergic to nuts. My son is allergic to dairy. What is a vegetarian
    dinner I can make for them?` the LLM-generated step-back search query could be
    *Vegetarian dinner recipe without dairy* *or nuts*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sirloin steak recipe`, the LLM-generated HyDE search query could be *Preheat
    your grill or grill pan to high heat. Pat the sirloin steaks dry and season generously
    with salt and pepper. Drizzle with olive oil and use your hands to coat the steaks
    evenly. Place the steaks on the hot grill and cook for 4-5 minutes per side for
    medium-rare, flipping only once. Use an instant-read thermometer to check for
    doneness (135°F for medium-rare). Transfer the steaks to a cutting board and let
    rest for 5 minutes before slicing against the grain. Serve the juicy sirloin steaks
    with your favorite sides like roasted potatoes, grilled vegetables, or a* *fresh
    salad.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vegan dinner party menu`, the LLM-generated multiple search queries could
    be *Vegan appetizer*, *Vegan dinner main course*, and *Vegan desert*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of these techniques can be optimized for your application’s domain. You
    can even combine them or have an LLM select what is the most appropriate technique
    for a given user query.
  prefs: []
  type: TYPE_NORMAL
- en: However, introducing another point of AI in the application also presents challenges.
    The query mutation may not always work as expected, potentially degrading performance
    in some cases. Additionally, it introduces another component to evaluate and incurs
    the cost of additional AI usage. Any LLM-query mutations should be thoroughly
    evaluated to mitigate unexpected outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting query metadata for pre-filtering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to performing semantic filtering as discussed in the *Embedding
    metadata* section, you can also programmatically filter on metadata before performing
    vector search. This lets you reduce the number of embeddings that you are searching
    over to only examine the subset of total embeddings relevant to a given query.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to select a vector database that contains metadata filtering
    capabilities suitable to your application needs. Metadata filtering capabilities
    vary greatly by vector database. For example, MongoDB Atlas Vector Search supports
    a variety of pre-filter options in the `$vectorSearch` aggregation pipeline stage.
    ([https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-stage/#atlas-vector-search-pre-filter](https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-stage/#atlas-vector-search-pre-filter)).
    In [*Chapter 8*](B22495_08.xhtml#_idTextAnchor180), *Implementing Vector Search
    in AI Applications*, you learned how to set up these pre-filter options with Atlas
    Vector Search Index.
  prefs: []
  type: TYPE_NORMAL
- en: You can use an LLM to extract metadata from a query to use as a filter, like
    how you extract metadata from ingested content, as discussed in the *Embedding
    metadata* section. Alternatively, you could use heuristics to determine filter
    criteria.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, say you are building a cooking chatbot that performs RAG over
    a vector database of recipes and general cooking information such as the popular
    spices in certain cuisines. You could add a metadata filter that only looks at
    the recipe items in the vector database if a user query contains the word `recipe`.
    You can also create so-called *smart* filters that use AI models such as LLMs
    to determine which subsets of the data to include.
  prefs: []
  type: TYPE_NORMAL
- en: Here is a code example of an LLM function that determines what, if any, filter
    to apply to a search query. It also uses Pydantic to format the response as JSON.
  prefs: []
  type: TYPE_NORMAL
- en: The following Python code extracts the topic from a query using the OpenAI LLM
    GPT-4o mini. It also uses Pydantic to format the response as JSON. You can then
    use the extracted topic as a pre-filter, as described in [*Chapter 8*](B22495_08.xhtml#_idTextAnchor180),
    *Implementing Vector Search in* *AI Applications*.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, install the required dependencies in your terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'This outputs the following to your terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: By combining metadata filtering with vector search, your RAG application can
    search more efficiently and accurately. This approach narrows down the search
    space to the most contextually appropriate data, leading to more precise and useful
    results.
  prefs: []
  type: TYPE_NORMAL
- en: Formatting ingested data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When ingesting data to create embeddings, you must consider the format that
    the data is in. Standardizing the data format as much as possible can lead to
    more consistent results.
  prefs: []
  type: TYPE_NORMAL
- en: For longer-form text data, such as technical documentation or reports, you should
    format the ingested and embedded data in a consistent format that includes appropriate
    semantic meaning in a token-dense format. Markdown is a good choice because it
    has high information density per token compared to XML-based formats such as HTML
    or PDFs.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, see the total GPT-4 tokenizer token count for the following content
    represented in plain text, Markdown, and HTML:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Format** | **Content** | **Token** **count** |'
  prefs: []
  type: TYPE_TB
- en: '| Plain Text |'
  prefs: []
  type: TYPE_TB
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '| 81 |'
  prefs: []
  type: TYPE_TB
- en: '| Markdown |'
  prefs: []
  type: TYPE_TB
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '| 83 |'
  prefs: []
  type: TYPE_TB
- en: '| HTML |'
  prefs: []
  type: TYPE_TB
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: '| 138 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 10.6: Token count of different text formats'
  prefs: []
  type: TYPE_NORMAL
- en: How you format ingested data can have a meaningful impact on retrieval quality
    and resource consumption. Generally, plain text or Markdown are effective formats
    for most text-based use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced retrieval systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A variety of advanced retrieval systems have emerged that go beyond simply retrieving
    the nearest match to the query.
  prefs: []
  type: TYPE_NORMAL
- en: All of the following retrieval architectures are experimental as of writing
    in August 2024\. When developing your intelligent application, you should probably
    start with standard vector search retrieval. Optimize standard vector search retrieval
    before using techniques such as filtering and adding semantic metadata before
    you experiment with these advanced retrieval systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Advanced retrieval systems include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Summary retrieval**: Extract a summary from each document and store that
    summary in the vector search index. Retrieve the content of the whole document
    when the embedded version of the summary is matched.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Knowledge graph retrieval**: During data ingestion, create a knowledge graph
    of relations between documents in the vector store. These relationships can be
    created using an LLM. During retrieval, perform an initial semantic search.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Router retrieval**: Use a classifier to determine where a user query should
    be routed to between different data stores.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LlamaIndex has done an excellent job of staying on top of the latest research
    in advanced retrieval systems. To learn more about the various advanced retrieval
    patterns that LlamaIndex supports, refer to the LlamaIndex Query Engine documentation
    ([https://docs.llamaindex.ai/en/stable/examples/query_engine/knowledge_graph_rag_query_engine/](https://docs.llamaindex.ai/en/stable/examples/query_engine/knowledge_graph_rag_query_engine/)).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you explored various techniques for refining your semantic
    data model to improve retrieval accuracy for vector search and RAG. You learned
    how to improve your data model used in information retrieval and RAG. By fine-tuning
    embeddings, you can adjust pre-trained models to improve the accuracy and relevance
    of search results. With embedded metadata, you can improve the vector search quality.
    Finally, RAG optimization ensures that the retrieval process fetches the most
    relevant information.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will examine ways to address common issues in AI application
    development.
  prefs: []
  type: TYPE_NORMAL
