["```py\n#1.Uncomment the following lines if you want to use Google Drive to retrieve your token\nfrom google.colab import drive\ndrive.mount('/content/drive')\nf = open(\"drive/MyDrive/files/hf_token.txt\", \"r\")\naccess_token=f.readline().strip()\nf.close()\n#2.Uncomment the following line if you want to enter your HF token manually\n#access_token =[YOUR HF_TOKEN]\nimport os\nos.environ['HF_TOKEN'] = access_token \n```", "```py\n!pip install datasets==2.20.0 \n```", "```py\n!pip install transformers==4.41.2 \n```", "```py\n!pip install accelerate==0.31.0 \n```", "```py\nfrom transformers import AutoTokenizer\nimport tranformers\nimport torch\nmodel = \"meta-llama/Llama-2-7b-chat-hf\"\ntokenizer = AutoTokenizer.from_pretrained(model) \n```", "```py\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n) \n```", "```py\n!pip install chromadb==0.5.3 \n```", "```py\nSuccessfully installed asgiref-3…onnxruntime-1.18.0… \n```", "```py\n!python -m spacy download en_core_web_md \n```", "```py\n# Start timing before the request\nsession_start_time = time.time() \n```", "```py\n# Import required libraries\nfrom datasets import load_dataset\nimport pandas as pd\n# Load the SciQ dataset from HuggingFace\ndataset = load_dataset(\"sciq\", split=\"train\") \n```", "```py\n# Filter the dataset to include only questions with support and correct answer\nfiltered_dataset = dataset.filter(lambda x: x[\"support\"] != \"\" and x[\"correct_answer\"] != \"\") \n```", "```py\n# Print the number of questions with support\nprint(\"Number of questions with support: \", len(filtered_dataset)) \n```", "```py\nNumber of questions with support:  10481 \n```", "```py\n# Convert the filtered dataset to a pandas DataFrame\ndf = pd.DataFrame(filtered_dataset)\n# Columns to drop\ncolumns_to_drop = ['distractor3', 'distractor1', 'distractor2']\n# Dropping the columns from the DataFrame\ndf.drop(columns=columns_to_drop, inplace=True) \n```", "```py\n# Create a new column 'completion' by merging 'correct_answer' and 'support'\ndf['completion'] = df['correct_answer'] + \" because \" + df['support']\n# Ensure no NaN values are in the 'completion' column\ndf.dropna(subset=['completion'], inplace=True)\ndf \n```", "```py\naerobic because \"Cardio\" has become slang for aerobic exercise that raises your heart rate for an extended amount of time. Cardio can include biking, running, or swimming. Can you guess one of the main organs of the cardiovascular system? Yes, your heart. \n```", "```py\ndf.shape \n```", "```py\n(10481, 4) \n```", "```py\n# Assuming 'df' is your DataFrame\nprint(df.columns) \n```", "```py\nIndex(['question', 'correct_answer', 'support', 'completion'], dtype='object') \n```", "```py\n# Import Chroma and instantiate a client. The default Chroma client is ephemeral, meaning it will not save to disk.\nimport chromadb\nclient = chromadb.Client()\ncollection_name=\"sciq_supports6\" \n```", "```py\n# List all collections\ncollections = client.list_collections()\n# Check if the specific collection exists\ncollection_exists = any(collection.name == collection_name for collection in collections)\nprint(\"Collection exists:\", collection_exists) \n```", "```py\nCollection exists: False \n```", "```py\n# Create a new Chroma collection to store the supporting evidence. We don't need to specify an embedding function, and the default will be used.\nif collection_exists!=True:\n  collection = client.create_collection(collection_name)\nelse:\n  print(\"Collection \", collection_name,\" exists:\", collection_exists) \n```", "```py\n#Printing the dictionary\nresults = collection.get()\nfor result in results:\n    print(result)  # This will print the dictionary for each item \n```", "```py\nids\nembeddings\nmetadatas\ndocuments\nuris\ndata\nincluded \n```", "```py\nmodel_name = \"all-MiniLM-L6-v2\"  # The name of the model to use for embedding and querying \n```", "```py\nldf=len(df)\nnb=ldf  # number of questions to embed and store\nimport time\nstart_time = time.time()  # Start timing before the request\n# Convert Series to list of strings\ncompletion_list = df[\"completion\"][:nb].astype(str).tolist() \n```", "```py\n# Avoiding trying to load data twice in this one run dynamic RAG notebook\nif collection_exists!=True:\n  # Embed and store the first nb supports for this demo\n  collection.add(\n      ids=[str(i) for i in range(0, nb)],  # IDs are just strings\n      documents=completion_list,\n      metadatas=[{\"type\": \"completion\"} for _ in range(0, nb)],\n  ) \n```", "```py\nresponse_time = time.time() - start_time  # Measure response time\nprint(f\"Response Time: {response_time:.2f} seconds\")  # Print response time \n```", "```py\n/root/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz: 100%|██████████| 79.3M/79.3M [00:02<00:00, 31.7MiB/s] \n```", "```py\nResponse Time: 234.25 seconds \n```", "```py\n# Fetch the collection with embeddings included\nresult = collection.get(include=['embeddings'])\n# Extract the first embedding from the result\nfirst_embedding = result['embeddings'][0]\n# If you need to work with the length or manipulate the first embedding:\nembedding_length = len(first_embedding)\nprint(\"First embedding:\", first_embedding)\nprint(\"Embedding length:\", embedding_length) \n```", "```py\nFirst embedding: [0.03689068928360939, -0.05881563201546669, -0.04818134009838104,… \n```", "```py\nEmbedding length: 384 \n```", "```py\ndataset[\"question\"][:nbq]. \n```", "```py\nimport time\nstart_time = time.time()  # Start timing before the request\n# number of retrievals to write\nresults = collection.query(\n    query_texts=df[\"question\"][:nb],\n    n_results=1)\nresponse_time = time.time() - start_time  # Measure response time\nprint(f\"Response Time: {response_time:.2f} seconds\")  # Print response time \n```", "```py\nResponse Time: 199.34 seconds \n```", "```py\nimport spacy\nimport numpy as np\n# Load the pre-trained spaCy language model\nnlp = spacy.load('en_core_web_md')  # Ensure that you've installed this model with 'python -m spacy download en_core_web_md' \n```", "```py\ndef simple_text_similarity(text1, text2):\n    # Convert the texts into spaCy document objects\n    doc1 = nlp(text1)\n    doc2 = nlp(text2)\n\n    # Get the vectors for each document\n    vector1 = doc1.vector\n    vector2 = doc2.vector\n\n    # Compute the cosine similarity between the two vectors\n    # Check for zero vectors to avoid division by zero\n    if np.linalg.norm(vector1) == 0 or np.linalg.norm(vector2) == 0:\n        return 0.0  # Return zero if one of the texts does not have a vector representation\n    else:\n        similarity = np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n        return similarity \n```", "```py\nnbqd = 100  # the number of responses to display, supposing there are more than 100 records\n# Print the question, the original completion, the retrieved document, and compare them\nacc_counter=0\ndisplay_counter=0 \n```", "```py\nfor i, q in enumerate(df['question'][:nb]):\n    original_completion = df['completion'][i]  # Access the original completion for the question\n    retrieved_document = results['documents'][i][0]  # Retrieve the corresponding document\n    similarity_score = simple_text_similarity(original_completion, retrieved_document) \n```", "```py\n if similarity_score > 0.7:\n      acc_counter+=1 \n```", "```py\n display_counter+=1\n    if display_counter<=nbqd or display_counter>nb-nbqd: \n```", "```py\n print(i,\" \", f\"Question: {q}\")\n      print(f\"Retrieved document: {retrieved_document}\")\n      print(f\"Original completion: {original_completion}\")\n      print(f\"Similarity Score: {similarity_score:.2f}\")\n      print()  # Blank line for better readability between entries \n```", "```py\nQuestion: What type of organism is commonly used in preparation of foods such as cheese and yogurt? \n```", "```py\nRetrieved document: lactic acid because Bacteria can be used to make cheese from milk. The bacteria turn the milk sugars into lactic acid. The acid is what causes the milk to curdle to form cheese. Bacteria are also involved in producing other foods. Yogurt is made by using bacteria to ferment milk ( Figure below ). Fermenting cabbage with bacteria produces sauerkraut. \n```", "```py\nOriginal completion: mesophilic organisms because Mesophiles grow best in moderate temperature, typically between 25°C and 40°C (77°F and 104°F). Mesophiles are often found living in or on the bodies of humans or other animals. The optimal growth temperature of many pathogenic mesophiles is 37°C (98°F), the normal human body temperature. Mesophilic organisms have important uses in food preparation, including cheese, yogurt, beer and wine. \n```", "```py\nSimilarity Score: 0.73 \n```", "```py\nif nb>0:\n  acc=acc_counter/nb \n```", "```py\n print(f\"Number of documents: {nb:.2f}\")\n  print(f\"Overall similarity score: {acc:.2f}\") \n```", "```py\nNumber of documents: 10481.00\nOverall similarity score: 1.00 \n```", "```py\n# initial question\nprompt = \"Millions of years ago, plants used energy from the sun to form what?\"\n# variant 1 similar\n#prompt = \"Eons ago, plants used energy from the sun to form what?\"\n# variant 2 divergent\n#prompt = \"Eons ago, plants used sun energy to form what?\" \n```", "```py\nimport time\nimport textwrap\n# Start timing before the request\nstart_time = time.time()\n# Query the collection using the prompt\nresults = collection.query(\n    query_texts=[prompt],  # Use the prompt in a list as expected by the query method\n    n_results=1  # Number of results to retrieve\n)\n# Measure response time\nresponse_time = time.time() - start_time\n# Print response time\nprint(f\"Response Time: {response_time:.2f} seconds\\n\")\n# Check if documents are retrieved\nif results['documents'] and len(results['documents'][0]) > 0:\n    # Use textwrap to format the output for better readability\n    wrapped_question = textwrap.fill(prompt, width=70)  # Wrap text at 70 characters\n    wrapped_document = textwrap.fill(results['documents'][0][0], width=70)\n    # Print formatted results\n    print(f\"Question: {wrapped_question}\")\n    print(\"\\n\")\n    print(f\"Retrieved document: {wrapped_document}\")\n    print()\nelse:\n    print(\"No documents retrieved.\" \n```", "```py\nResponse Time: 0.03 seconds \n```", "```py\nResponse Time: 0.03 seconds\nQuestion: Millions of years ago, plants used energy from the sun to form what?\nRetrieved document: chloroplasts because When ancient plants underwent photosynthesis,\nthey changed energy in sunlight to stored chemical energy in food. The\nplants used the food and so did the organisms that ate the plants.\nAfter the plants and other organisms died, their remains gradually\nchanged to fossil fuels as they were covered and compressed by layers\nof sediments. Petroleum and natural gas formed from ocean organisms\nand are found together. Coal formed from giant tree ferns and other\nswamp plants. \n```", "```py\ndef LLaMA2(prompt):\n    sequences = pipeline(\n        prompt,\n        do_sample=True,\n        top_k=10,\n        num_return_sequences=1,\n        eos_token_id=tokenizer.eos_token_id,\n        max_new_tokens=100, # Control the output length more granularly\n        temperature=0.5,  # Slightly higher for more diversity\n        repetition_penalty=2.0,  # Adjust based on experimentation\n        truncation=True\n    )\n    return sequences \n```", "```py\niprompt='Read the following input and write a summary for beginners.'\nlprompt=iprompt + \" \" + results['documents'][0][0] \n```", "```py\nimport time\nstart_time = time.time()  # Start timing before the request\nresponse=LLaMA2(lprompt) \n```", "```py\nfor seq in response:\n    generated_part = seq['generated_text'].replace(iprompt, '')  # Remove the input part from the output\n\nresponse_time = time.time() - start_time  # Measure response time\nprint(f\"Response Time: {response_time:.2f} seconds\")  # Print response timeLe \n```", "```py\nResponse Time: 5.91 seconds \n```", "```py\nwrapped_response = textwrap.fill(response[0]['generated_text'], width=70)\nprint(wrapped_response) \n```", "```py\nchloroplasts because When ancient plants underwent photosynthesis,\nthey changed energy in sunlight to stored chemical energy in food. The\nplants used the food and so did the organisms that ate the plants.\nAfter the plants and other organisms died, their remains gradually\nchanged to fossil fuels as they were covered and compressed by layers\nof sediments. Petroleum and natural gas formed from ocean organisms\nand are found together. Coal formed from giant tree ferns and other\nswamp plants. Natural Gas: 10% methane (CH4) - mostly derived from\nanaerobic decomposition or fermentation processes involving\nmicroorganism such As those present In wetlands; also contains smaller\namounts Of ethene(C2H6), propiene/propadiene/( C3 H5-7). This is where\nmost petrol comes frm! But there're more complex hydrocarbons like\npentanes & hexans too which can come \n```", "```py\nWrite a nice summary with this text: Question: Millions of years ago, plants used energy from the sun to form what?\nRetrieved document: chloroplasts because When ancient plants underwent photosynthesis,\nthey changed energy in sunlight to stored chemical energy in food. The plants used the food and so did the organisms that ate the plants. After the plants and other organisms died, their remains gradually\nchanged to fossil fuels as they were covered and compressed by layers of sediments. Petroleum and natural gas formed from ocean organisms and are found together. Coal formed from giant tree ferns and other swamp plants. \n```", "```py\nMillions of years ago, plants harnessed energy from the sun through photosynthesis to produce food, storing chemical energy. This energy was vital for the plants themselves and for the organisms that consumed them. Over time, the remains of these plants and animals, buried under sediment, transformed into fossil fuels. Ocean organisms' remains contributed to the formation of petroleum and natural gas, often found together, while the remains of giant tree ferns and swamp plants formed coal. \n```", "```py\n#client.delete_collection(collection_name) \n```", "```py\n# List all collections\ncollections = client.list_collections()\n# Check if the specific collection exists\ncollection_exists = any(collection.name == collection_name for collection in collections)\nprint(\"Collection exists:\", collection_exists) \n```", "```py\nCollection exists: True \n```", "```py\nend_time = time.time() - session_start_time  # Measure response time\nprint(f\"Session preparation time: {response_time:.2f} seconds\")  # Print response time \n```", "```py\nSession preparation time: 780.35 seconds \n```"]