<html><head></head><body><html>&#13;
 <head>&#13;
  <title>&#13;
   Prompt Engineering Guidelines and Best Practices&#13;
  </title>&#13;
 </head>&#13;
 <body>&#13;
  <div class="epub-source">&#13;
   <h1 id="_idParaDest-216">&#13;
    Prompt Engineering Guidelines and Best Practices&#13;
   </h1>&#13;
   <div id="_idContainer115">&#13;
    <p>&#13;
     In this chapter, we embark on an exploration of how the latest advancements in technology are reshaping our interaction with digital tools and applications. As the digital landscape evolves, the traditional interfaces we’ve relied upon for decades are being reimagined, paving the way for more intuitive and efficient forms of communication between humans and machines. At the heart of this transformation is the advent of&#13;
     <a id="_idIndexMarker1071">&#13;
     </a>&#13;
     conversational interfaces powered by&#13;
     <strong class="bold">&#13;
      natural language&#13;
     </strong>&#13;
     (&#13;
     <strong class="bold">&#13;
      NL&#13;
     </strong>&#13;
     ). As a result, understanding how to write effective prompts to customize the behavior of our LlamaIndex components becomes a critical skill in building and improving&#13;
     <span class="No-Break">&#13;
      RAG applications.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Therefore, in this chapter, we’re going to cover the following&#13;
     <span class="No-Break">&#13;
      main topics:&#13;
     </span>&#13;
    </p>&#13;
    <ul>&#13;
     <li>&#13;
      Why prompts are your&#13;
      <span class="No-Break">&#13;
       secret weapon&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      Understanding how LlamaIndex&#13;
      <span class="No-Break">&#13;
       uses prompts&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      Customizing&#13;
      <span class="No-Break">&#13;
       default prompts&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      The golden rules of&#13;
      <span class="No-Break">&#13;
       prompt engineering&#13;
      </span>&#13;
     </li>&#13;
    </ul>&#13;
    <a id="_idTextAnchor216">&#13;
    </a>&#13;
   </div>&#13;
  </div>&#13;
 </body>&#13;
</html>
<html>&#13;
 <head>&#13;
  <title>&#13;
   Technical requirements&#13;
  </title>&#13;
 </head>&#13;
 <body>&#13;
  <div class="epub-source">&#13;
   <h1 id="_idParaDest-217">&#13;
    Technical requirements&#13;
   </h1>&#13;
   <div id="_idContainer115">&#13;
    <p>&#13;
     All code samples from this chapter can be found in the&#13;
     <code class="literal">&#13;
      ch10&#13;
     </code>&#13;
     subfolder of the book’s GitHub&#13;
     <span class="No-Break">&#13;
      repository:&#13;
     </span>&#13;
     <a>&#13;
      <span class="No-Break">&#13;
       https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex&#13;
      </span>&#13;
     </a>&#13;
     <span class="No-Break">&#13;
      .&#13;
     </span>&#13;
    </p>&#13;
    <a id="_idTextAnchor217">&#13;
    </a>&#13;
   </div>&#13;
  </div>&#13;
 </body>&#13;
</html>
<html>&#13;
 <head>&#13;
  <title>&#13;
   Why prompts are your secret weapon&#13;
  </title>&#13;
 </head>&#13;
 <body>&#13;
  <div class="epub-source">&#13;
   <h1 id="_idParaDest-218">&#13;
    Why prompts are your secret weapon&#13;
   </h1>&#13;
   <div id="_idContainer115">&#13;
    <p>&#13;
     I was 6 years old when I started writing my first lines of code using a ZX Spectrum computer. At the time, in the mid-1980s, computers were still a new thing in the world, and not many people&#13;
     <a id="_idIndexMarker1072">&#13;
     </a>&#13;
     understood the extraordinary impact they were going to have on human society. Today, we all live in a reality dominated, and in many ways, driven by technology. The way we relate to technology has also changed fundamentally in the last 40 years. Almost all human activities have come to be touched to a greater or lesser extent by&#13;
     <span class="No-Break">&#13;
      technological progress.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     What hasn’t changed much is the way we interact with technology. With a few notable exceptions – such as the introduction of touch screens and voice interfaces – our interaction with technology has remained almost unchanged. We use, as we did 40 years ago, rudimentary methods to get computers to perform the functions&#13;
     <span class="No-Break">&#13;
      we need.&#13;
     </span>&#13;
    </p>&#13;
    <p class="callout-heading">&#13;
     Clarification&#13;
    </p>&#13;
    <p class="callout">&#13;
     When I say rudimentary, I’m not necessarily referring to the sophistication of the interface itself – although functionally, if we were to compare a modern-day keyboard or mouse, we would find that even here, the advances are not fantastic. I’m referring rather to another aspect that unfortunately continues to stagnate: the bandwidth that our current interfaces&#13;
     <span class="No-Break">&#13;
      can offer.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     The way we currently interact with our technology is long due&#13;
     <span class="No-Break">&#13;
      for replacement.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Let’s go through a simple&#13;
     <span class="No-Break">&#13;
      rationale together:&#13;
     </span>&#13;
    </p>&#13;
    <ul>&#13;
     <li>&#13;
      The computing power offered by IT systems continues to grow at a rapid pace. Even if Moore’s law – see&#13;
      <span class="No-Break">&#13;
       <em class="italic">&#13;
        Figure 10&#13;
       </em>&#13;
      </span>&#13;
      <em class="italic">&#13;
       .1&#13;
      </em>&#13;
      – is arguably no longer considered a valid benchmark, progress is far from slowing&#13;
      <span class="No-Break">&#13;
       down (&#13;
      </span>&#13;
      <a>&#13;
       <span class="No-Break">&#13;
        https://en.wikipedia.org/wiki/Moore%27s_law&#13;
       </span>&#13;
      </a>&#13;
      <span class="No-Break">&#13;
       ).&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      We live in a world dominated almost entirely by applications. At the moment, applications are the layer between the user and the machine that makes our interaction with a computer possible – apps running on local systems, apps running on mobile devices, or apps running in the cloud. Each app offers a very specific set&#13;
      <span class="No-Break">&#13;
       of functionalities.&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      Many apps are designed to run only on specific platforms and cannot be easily ported to other platforms. This means a different app for each&#13;
      <span class="No-Break">&#13;
       specific platform.&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      Many applications overlap in terms of functionality. For a given task there are, in most cases, dozens of different applications that can perform it. So, there is a lot&#13;
      <span class="No-Break">&#13;
       of duplication.&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      Our interaction with technology has remained broadly the same bandwidth as 40 years ago. We use almost the same types of interfaces – keyboard, mouse, touchscreen, gesture- or voice-based – to control&#13;
      <span class="No-Break">&#13;
       application logic.&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      Almost every application&#13;
      <a id="_idIndexMarker1073">&#13;
      </a>&#13;
      comes with its own UI. There is a mandatory learning curve that users have to go through to learn how to operate each application. If we multiply this time by the number of applications that a typical user uses on a regular basis, we find that we actually spend a lot of time learning to use a tool effectively, and this eats into the actual time we spend using the tool to&#13;
      <span class="No-Break">&#13;
       be productive.&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      The number of software applications – including both publicly available applications and those used privately by organizations – is already huge. There are already more than 1 billion applications in the world. That’s without taking into account the fact that an application very often exists in several different versions. And the number&#13;
      <span class="No-Break">&#13;
       is growing.&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      From an evolutionary point of view, the capacity of the human brain has remained unchanged throughout this time. Neuroplasticity gives us a remarkable ability to learn and adapt to new technologies, but unfortunately, evolution itself cannot keep up with&#13;
      <span class="No-Break">&#13;
       technological progress.&#13;
      </span>&#13;
     </li>&#13;
    </ul>&#13;
    <div>&#13;
     <div class="IMG---Figure" id="_idContainer111">&#13;
      <img src="../Images/B21861_10_1.jpg"/>&#13;
     </div>&#13;
    </div>&#13;
    <p class="IMG---Caption" lang="en-US">&#13;
     Figure 10.1 – According to Moore’s law, the number of transistors roughly doubles every 2 years&#13;
    </p>&#13;
    <p>&#13;
     See where I’m aiming? This very specific way of interacting with technology, combined with the rapid&#13;
     <a id="_idIndexMarker1074">&#13;
     </a>&#13;
     evolution of technology, is slowly making us victims of our own success. On the one hand, we have managed to build a huge number of specialized tools capable of solving a huge number of problems. But now, we have a bigger problem: we have so many tools that organizing and using them efficiently has become an extremely complicated process. A new paradigm&#13;
     <span class="No-Break">&#13;
      is needed.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Conversational&#13;
     <a id="_idIndexMarker1075">&#13;
     </a>&#13;
     interfaces, based on&#13;
     <strong class="bold">&#13;
      natural language processing&#13;
     </strong>&#13;
     (&#13;
     <strong class="bold">&#13;
      NLP&#13;
     </strong>&#13;
     ), present themselves as a promising alternative to the current way of interacting with technology. They represent a natural evolution in the way we communicate with our devices. Instead of relying on complex visual interfaces and input methods that require effort and time to learn, conversational interfaces allow us to use NL – the most fundamental and intuitive form of&#13;
     <span class="No-Break">&#13;
      human communication.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     This is where a new core competency in this new paradigm&#13;
     <span class="No-Break">&#13;
      comes in.&#13;
     </span>&#13;
    </p>&#13;
    <p class="callout-heading">&#13;
     Prompt engineering&#13;
    </p>&#13;
    <p class="callout">&#13;
     As human-machine interaction becomes increasingly dependent on NL, the ability to formulate&#13;
     <a id="_idIndexMarker1076">&#13;
     </a>&#13;
     effective prompts that guide&#13;
     <strong class="bold">&#13;
      artificial intelligence&#13;
     </strong>&#13;
     (&#13;
     <strong class="bold">&#13;
      AI&#13;
     </strong>&#13;
     ) algorithms toward&#13;
     <a id="_idIndexMarker1077">&#13;
     </a>&#13;
     desired responses or actions becomes essential. This skill involves not only formulating prompts clearly but also anticipating how different formulations may influence the interpretation and execution of commands by&#13;
     <span class="No-Break">&#13;
      the AI.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Conversational interfaces transform the interaction with technology into a dialogue where linguistic precision and understanding of algorithmic subtleties become key factors in achieving desired outcomes. The ability to interact directly and effectively with computer&#13;
     <a id="_idIndexMarker1078">&#13;
     </a>&#13;
     systems using NL can significantly reduce the barrier between humans and technology. It offers a pathway to democratizing access to technology, making it accessible to a wider range of users, regardless of their&#13;
     <span class="No-Break">&#13;
      technical expertise.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     There are already indications that the intensive use of prompts in our everyday interactions with LLMs can improve even our interpersonal communication skills, as shown, for example, by this study: Liu et al. (2023),&#13;
     <em class="italic">&#13;
      Improving Interpersonal Communication by Simulating Audiences with Language&#13;
     </em>&#13;
     <span class="No-Break">&#13;
      <em class="italic">&#13;
       Models&#13;
      </em>&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      (&#13;
     </span>&#13;
     <a>&#13;
      <span class="No-Break">&#13;
       https://doi.org/10.48550/arXiv.2311.00687&#13;
      </span>&#13;
     </a>&#13;
     <span class="No-Break">&#13;
      ).&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Imagine computer systems that can replace the functionality of dozens or even hundreds of different applications but without the complexity of traditional interfaces. Language interaction: a form of technology where LLMs, augmented with RAG, take the place of applications and operating systems, giving us a universal and much simpler way to use computing power. Without getting too deep into the area of speculation, if I were to make a medium-to-long-term prediction, this is the direction I think we are heading in. In the short term, classical computing systems will continue to prevail. At first, conversational agent-based interfaces will gradually simplify user interaction with them, masking the complexity of the backend application layer. Then, as dedicated AI hardware becomes a commodity, a large part of the applications will be phased out of the ecosystem, and the functionality they provide will be taken over by&#13;
     <span class="No-Break">&#13;
      AI models.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     And I think this whole exposition justifies the title I have chosen for this section. Next, let’s discover together how prompts are used by LlamaIndex for&#13;
     <span class="No-Break">&#13;
      LLM interactions.&#13;
     </span>&#13;
    </p>&#13;
    <a id="_idTextAnchor218">&#13;
    </a>&#13;
   </div>&#13;
  </div>&#13;
 </body>&#13;
</html>
<html>&#13;
 <head>&#13;
  <title>&#13;
   Understanding how LlamaIndex uses prompts&#13;
  </title>&#13;
 </head>&#13;
 <body>&#13;
  <div class="epub-source">&#13;
   <h1 id="_idParaDest-219">&#13;
    Understanding how LlamaIndex uses prompts&#13;
   </h1>&#13;
   <div id="_idContainer115">&#13;
    &#13;
    from llama_index.core import SummaryIndex, SimpleDirectoryReader&#13;
documents = SimpleDirectoryReader("files").load_data()&#13;
summary_index = SummaryIndex.from_documents(documents)&#13;
qe = summary_index.as_query_engine()&#13;
    prompts = qe.get_prompts()&#13;
    for k, p in prompts.items():&#13;
    print(f"Prompt Key: {k}")&#13;
    print("Text:")&#13;
    print(p.get_template())&#13;
    print("\n")&#13;
    <p>&#13;
     In terms of mechanics, a RAG-based application follows exactly the same rules and principles of&#13;
     <a id="_idIndexMarker1079">&#13;
     </a>&#13;
     interaction that a simple user would use in a chat session with an LLM. A major difference comes from the fact that RAG is actually a&#13;
     <a id="_idIndexMarker1080">&#13;
     </a>&#13;
     kind of prompt engineer on steroids. Behind the scenes, for almost every indexing, retrieval, metadata extraction, or final response synthesis operation, the RAG framework programmatically produces prompts. These prompts are enriched with context and then sent to&#13;
     <span class="No-Break">&#13;
      the LLM.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     In LlamaIndex, for each type of operation that requires an LLM, there is a default prompt that is used as a template. Take&#13;
     <code class="literal">&#13;
      TitleExtractor&#13;
     </code>&#13;
     as an example. This is one of the metadata extractors that we already talked about in&#13;
     <a>&#13;
      <span class="No-Break">&#13;
       <em class="italic">&#13;
        Chapter 4&#13;
       </em>&#13;
      </span>&#13;
     </a>&#13;
     ,&#13;
     <em class="italic">&#13;
      Ingesting Data into Our RAG Workflow&#13;
     </em>&#13;
     . The&#13;
     <code class="literal">&#13;
      TitleExtractor&#13;
     </code>&#13;
     class uses two predefined prompt templates to get titles from text nodes inside documents. It does this in&#13;
     <span class="No-Break">&#13;
      two steps:&#13;
     </span>&#13;
    </p>&#13;
    <ol>&#13;
     <li>&#13;
      It gets potential titles from individual text Nodes using the&#13;
      <code class="literal">&#13;
       node_template&#13;
      </code>&#13;
      argument, which creates prompts to generate&#13;
      <span class="No-Break">&#13;
       appropriate titles&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      Combines the individual Node titles into one overall comprehensive title for the whole Document using the&#13;
      <span class="No-Break">&#13;
       <code class="literal">&#13;
        combine_template&#13;
       </code>&#13;
      </span>&#13;
      <span class="No-Break">&#13;
       prompt&#13;
      </span>&#13;
     </li>&#13;
    </ol>&#13;
    <p>&#13;
     The default values for the&#13;
     <code class="literal">&#13;
      TitleExtractor&#13;
     </code>&#13;
     prompts are stored in&#13;
     <span class="No-Break">&#13;
      two constants:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Looking at these two default templates used by&#13;
     <code class="literal">&#13;
      TitleExtractor&#13;
     </code>&#13;
     , we can easily understand how they work. Each template contains a&#13;
     <em class="italic">&#13;
      fixed&#13;
     </em>&#13;
     text part and a&#13;
     <em class="italic">&#13;
      dynamic&#13;
     </em>&#13;
     part, designated by&#13;
     <code class="literal">&#13;
      {context_str}&#13;
     </code>&#13;
     or other variables. That is where LlamaIndex will actually inject the text content of our Nodes during execution, as seen in&#13;
     <span class="No-Break">&#13;
      <em class="italic">&#13;
       Figure 10&#13;
      </em>&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      <em class="italic">&#13;
       .2&#13;
      </em>&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      :&#13;
     </span>&#13;
    </p>&#13;
    <div>&#13;
     <div class="IMG---Figure" id="_idContainer112">&#13;
      <img src="../Images/B21861_10_2.jpg"/>&#13;
     </div>&#13;
    </div>&#13;
    <p class="IMG---Caption" lang="en-US">&#13;
     Figure 10.2 – How prompts are built by injecting variables into a prompt template&#13;
    </p>&#13;
    <p>&#13;
     The prompt templates used by metadata extractors such as&#13;
     <code class="literal">&#13;
      TitleExtractor&#13;
     </code>&#13;
     are defined&#13;
     <a id="_idIndexMarker1081">&#13;
     </a>&#13;
     directly within the&#13;
     <code class="literal">&#13;
      metadata_extractors.py&#13;
     </code>&#13;
     module. The relative path of this module in the LlamaIndex&#13;
     <a id="_idIndexMarker1082">&#13;
     </a>&#13;
     GitHub repository is&#13;
     <code class="literal">&#13;
      llama-index-core/llama_index/core/extractors/metadata_extractors.py&#13;
     </code>&#13;
     . However, this is just an exception as the vast majority of the default templates are defined in two other key modules:&#13;
     <code class="literal">&#13;
      llama-index-core/llama_index/core/prompts/default_prompts.py&#13;
     </code>&#13;
     <span class="No-Break">&#13;
      and&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      <code class="literal">&#13;
       llama-index-core/llama_index/core/prompts/chat_prompts.py&#13;
      </code>&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      .&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Because a RAG workflow built with LlamaIndex can have so many different components that rely on LLM interactions and not all prompt templates can be easily located within the code base, the framework provides a simple method to identify the templates used by a specific component. That method is called&#13;
     <code class="literal">&#13;
      get_prompts()&#13;
     </code>&#13;
     and can be used with agents, retrievers, query engines, response synthesizers, and many other RAG components. Here is a simple example of how we can use it to obtain a list of prompt templates used by a query engine built on top&#13;
     <span class="No-Break">&#13;
      of&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      <code class="literal">&#13;
       SummaryIndex&#13;
      </code>&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      :&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     The first part of the code should be very straightforward at this point. We import&#13;
     <code class="literal">&#13;
      SummaryIndex&#13;
     </code>&#13;
     and&#13;
     <code class="literal">&#13;
      SimpleDirectoryReader&#13;
     </code>&#13;
     and then ingest the two sample files that should have been cloned from our GitHub repository. Once the files have been ingested as Documents, we build an index and a query engine from that index. In this example, we won’t run any queries because we don’t need to. We just want to see the prompts. Therefore, the next step retrieves a dictionary containing the default prompts used within the&#13;
     <span class="No-Break">&#13;
      query engine:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     The dictionary returned by the&#13;
     <code class="literal">&#13;
      get_prompts()&#13;
     </code>&#13;
     method maps keys, which identify the different&#13;
     <a id="_idIndexMarker1083">&#13;
     </a>&#13;
     prompt types used by the query engine, to values&#13;
     <a id="_idIndexMarker1084">&#13;
     </a>&#13;
     that are the actual prompt templates. The last part of the code is responsible for iterating and displaying the keys and their&#13;
     <span class="No-Break">&#13;
      corresponding templates:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     <span class="No-Break">&#13;
      <em class="italic">&#13;
       Figure 10&#13;
      </em>&#13;
     </span>&#13;
     <em class="italic">&#13;
      .3&#13;
     </em>&#13;
     shows the results after running&#13;
     <span class="No-Break">&#13;
      this sample:&#13;
     </span>&#13;
    </p>&#13;
    <div>&#13;
     <div class="IMG---Figure" id="_idContainer113">&#13;
      <img src="../Images/B21861_10_3.jpg"/>&#13;
     </div>&#13;
    </div>&#13;
    <p class="IMG---Caption" lang="en-US">&#13;
     Figure 10.3 – The two prompt templates used by the SummaryIndex query engine&#13;
    </p>&#13;
    <p>&#13;
     Examining the output, we’ll see the two templates used by the query engine:&#13;
     <code class="literal">&#13;
      text_qa_template&#13;
     </code>&#13;
     and&#13;
     <code class="literal">&#13;
      refine_template&#13;
     </code>&#13;
     . You’ll notice that both keys begin with the text&#13;
     <code class="literal">&#13;
      response_synthesizer:&#13;
     </code>&#13;
     . This indicates the exact component of the query engine that actually uses the prompts – in our case, the response synthesizer. Following the same logic, we can use the&#13;
     <code class="literal">&#13;
      get_prompts()&#13;
     </code>&#13;
     method on many other types of RAG components in order to understand prompts used under&#13;
     <span class="No-Break">&#13;
      the hood.&#13;
     </span>&#13;
    </p>&#13;
    <p class="callout-heading">&#13;
     Pro tip&#13;
    </p>&#13;
    <p class="callout">&#13;
     An alternative option to inspect the underlying prompts would be to use an advanced tracing method – such as the one using the Arize AI Phoenix framework, presented in&#13;
     <a>&#13;
      <span class="No-Break">&#13;
       <em class="italic">&#13;
        Chapter 9&#13;
       </em>&#13;
      </span>&#13;
     </a>&#13;
     ,&#13;
     <em class="italic">&#13;
      Customizing and Deploying Our LlamaIndex Project&#13;
     </em>&#13;
     . Phoenix provides a visual representation of the execution flow, making it easier to understand how and when different prompts are used, in addition to displaying the final prompts with the inserted context. One caveat of using that method, though, is that instead of getting the original prompt templates, we’ll see the final prompts – also including any context already inserted in&#13;
     <span class="No-Break">&#13;
      the prompt.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Now that we&#13;
     <a id="_idIndexMarker1085">&#13;
     </a>&#13;
     have a reliable technique for inspecting prompts, the next&#13;
     <a id="_idIndexMarker1086">&#13;
     </a>&#13;
     step explores ways in which can customize them. Building on the title extractor and query engine examples, in the next section, we’ll explore how to customize prompts used by various&#13;
     <span class="No-Break">&#13;
      RAG components.&#13;
     </span>&#13;
    </p>&#13;
    <a id="_idTextAnchor219">&#13;
    </a>&#13;
   </div>&#13;
  </div>&#13;
 </body>&#13;
</html>
<html>&#13;
 <head>&#13;
  <title>&#13;
   Customizing default prompts&#13;
  </title>&#13;
 </head>&#13;
 <body>&#13;
  <div class="epub-source">&#13;
   <h1 id="_idParaDest-220">&#13;
    Customizing default prompts&#13;
   </h1>&#13;
   <div id="_idContainer115">&#13;
    Context information is below.&#13;
---------------------&#13;
{context_str}&#13;
---------------------&#13;
Given the context information &#13;
    from llama_index.core import SummaryIndex, SimpleDirectoryReader&#13;
from llama_index.core import PromptTemplate&#13;
documents = SimpleDirectoryReader("files").load_data()&#13;
summary_index = SummaryIndex.from_documents(documents)&#13;
qe = summary_index.as_query_engine()&#13;
    print(qe.query("Who burned Rome?"))&#13;
print("------------------------")&#13;
    new_qa_template = (&#13;
"Context information is below."&#13;
"---------------------"&#13;
"{context_str}"&#13;
"---------------------"&#13;
"Given the context information "&#13;
"&#13;
    template = PromptTemplate(new_qa_template)&#13;
    qe. Update_prompts(&#13;
    {"response_synthesizer: text_qa_template": template}&#13;
)&#13;
print(qe.query("Who burned Rome?"))&#13;
    from llama_index.core import SimpleDirectoryReader&#13;
from llama_index.core.node_parser import SentenceSplitter&#13;
from llama_index.core.extractors import TitleExtractor&#13;
reader = SimpleDirectoryReader('files')&#13;
documents = reader.load_data()&#13;
parser = SentenceSplitter()&#13;
nodes = parser.get_nodes_from_documents(documents)&#13;
    title_extractor = TitleExtractor(summaries=["self"])&#13;
meta = title_extractor.extract(nodes)&#13;
print("\nFirst title: " +meta[0]['document_title'])&#13;
print("Second title: " +meta[1]['document_title'])&#13;
    First title: "The Enduring Influence of Ancient Rome: Architecture, Engineering, Conquest, and Legacy"&#13;
Second title: "The Enduring Bond: Dogs as Loyal Companions - Exploring the Unbreakable Connection Between Humans and Man's Best Friend"&#13;
    combine_template = (&#13;
    "{context_str}. Based on the above candidate titles "&#13;
    "and content, what is the comprehensive title for "&#13;
    "this document? Keep it under 6 words. Title: "&#13;
)&#13;
title_extractor = TitleExtractor(&#13;
    summaries=["self"],&#13;
    combine_template=combine_template&#13;
)&#13;
meta = title_extractor.extract(nodes)&#13;
print("\nFirst title: "+meta[0]['document_title'])&#13;
print("Second title: "+meta[1]['document_title']&#13;
    First title: "Roman Legacy: Architecture, Engineering, Conquest"&#13;
Second title: "Man's Best Friend: The Enduring Bond"&#13;
    <p>&#13;
     While the default prompts provided by LlamaIndex are designed to work well in most scenarios, there may&#13;
     <a id="_idIndexMarker1087">&#13;
     </a>&#13;
     be instances where customization is necessary or desirable. For example, you might want to adjust prompts to do&#13;
     <span class="No-Break">&#13;
      the following:&#13;
     </span>&#13;
    </p>&#13;
    <ul>&#13;
     <li>&#13;
      Incorporate domain-specific knowledge&#13;
      <span class="No-Break">&#13;
       or terminology&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      Adapt prompts to a particular writing style&#13;
      <span class="No-Break">&#13;
       or tone&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      Modify prompts to prioritize certain types of information&#13;
      <span class="No-Break">&#13;
       or outputs&#13;
      </span>&#13;
     </li>&#13;
     <li>&#13;
      Experiment with different prompt structures to optimize performance&#13;
      <span class="No-Break">&#13;
       or quality&#13;
      </span>&#13;
     </li>&#13;
    </ul>&#13;
    <p>&#13;
     By customizing prompts, we can fine-tune the interaction between the RAG components and the language model, potentially leading to improved accuracy, relevance, and overall effectiveness of&#13;
     <span class="No-Break">&#13;
      our application.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     The good news is that we can modify the behavior of various LlamaIndex components by supplying our own custom prompt templates. The not-so-good news is that contrary to common expectations, writing a good prompt template is not a trivial task. One would have to consider many intricacies such as accuracy, relevance, query formulation, prompt size, output formatting, and others. Because of the involved complexity, the recommended approach for customization is to start with the default prompts and use them as a foundation for making any desired modifications. Changes should be incremental and ideally followed by rigorous evaluation against a diverse set of edge cases. We will have&#13;
     <a id="_idIndexMarker1088">&#13;
     </a>&#13;
     a more detailed discussion about general principles and best practices for writing prompts in the next section. For now, let us focus on the methods used for&#13;
     <span class="No-Break">&#13;
      prompt customization.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     In LlamaIndex, every RAG component that exposes the&#13;
     <code class="literal">&#13;
      get_prompts()&#13;
     </code>&#13;
     method also provides an equivalent for modifying these prompt templates – the&#13;
     <code class="literal">&#13;
      update_prompts()&#13;
     </code>&#13;
     method. So, this is the easiest way to change a particular prompt template. Let’s take our example from the previous section and experiment with a different prompt. This time, we will adapt the&#13;
     <code class="literal">&#13;
      text_qa_template&#13;
     </code>&#13;
     template to also rely on the LLM’s own knowledge when answering the query. The default&#13;
     <code class="literal">&#13;
      text_qa_template&#13;
     </code>&#13;
     template would normally look&#13;
     <span class="No-Break">&#13;
      like this:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     In the following example, we’ll make a very subtle change to this template and see how that will affect the behavior of our query engine. Let’s have a look at&#13;
     <span class="No-Break">&#13;
      the code:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     So far, the code is identical to the previous example, with only one additional import that I will explain in a few moments. This time, though, we’ll first run a query using the default template. We’ll use this response as a&#13;
     <span class="No-Break">&#13;
      reference later:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     It’s now time&#13;
     <a id="_idIndexMarker1089">&#13;
     </a>&#13;
     to change the&#13;
     <code class="literal">&#13;
      prompt_template&#13;
     </code>&#13;
     template. We first define a string containing the&#13;
     <span class="No-Break">&#13;
      new version:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     If you carefully compare the new version with the original template, you’ll notice a subtle but very important change. In this new version, I’m instructing the model to apply not just the knowledge provided in the retrieved context but also use its own knowledge on the matter. It’s time to make use of that new import we added at the beginning of our code. Because the&#13;
     <code class="literal">&#13;
      update_prompts()&#13;
     </code>&#13;
     method requires the prompts to be in the&#13;
     <code class="literal">&#13;
      BasePromptTemplate&#13;
     </code>&#13;
     format, we must first make sure that our new prompt is structured&#13;
     <span class="No-Break">&#13;
      like this:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     We’re now ready to rerun&#13;
     <span class="No-Break">&#13;
      the query:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Let’s have a look at the final output shown in&#13;
     <span class="No-Break">&#13;
      <em class="italic">&#13;
       Figure 10&#13;
      </em>&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      <em class="italic">&#13;
       .4&#13;
      </em>&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      :&#13;
     </span>&#13;
    </p>&#13;
    <div>&#13;
     <div class="IMG---Figure" id="_idContainer114">&#13;
      <img src="../Images/B21861_10_4.jpg"/>&#13;
     </div>&#13;
    </div>&#13;
    <p class="IMG---Caption" lang="en-US">&#13;
     Figure 10.4 – The query output before and after updating the prompt templates&#13;
    </p>&#13;
    <p>&#13;
     As you can see&#13;
     <a id="_idIndexMarker1090">&#13;
     </a>&#13;
     in the output, that slight modification in the&#13;
     <code class="literal">&#13;
      text_qa_template&#13;
     </code>&#13;
     template of the query engine completely changed its behavior. In a similar fashion, instead of changing the answering approach, we could have instructed the LLM to answer in a certain linguistic style, speak in rhymes, or anything else we might need. I think the value this feature provides for a RAG application is pretty clear&#13;
     <span class="No-Break">&#13;
      by now.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Unfortunately, not all LlamaIndex components support the&#13;
     <code class="literal">&#13;
      update_prompts()&#13;
     </code>&#13;
     method. Take, for example, the&#13;
     <code class="literal">&#13;
      TitleExtractor&#13;
     </code>&#13;
     metadata extractor that I mentioned in the previous section. Although metadata extractors do not support the&#13;
     <code class="literal">&#13;
      update_prompts()&#13;
     </code>&#13;
     method, the good news is that we can still change their underlying prompt templates by using arguments. In particular, the two templates used by&#13;
     <code class="literal">&#13;
      TitleExtractor&#13;
     </code>&#13;
     can be customized with the&#13;
     <code class="literal">&#13;
      node_template&#13;
     </code>&#13;
     and&#13;
     <code class="literal">&#13;
      combine_template&#13;
     </code>&#13;
     arguments. Let’s have a look at&#13;
     <span class="No-Break">&#13;
      an example:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     The first part of the example is responsible for ingesting our sample files as Documents and then chunking them into individual Nodes. Let’s extract the titles, first by using the default prompt templates that we saw in the&#13;
     <span class="No-Break">&#13;
      previous section:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     The output&#13;
     <a id="_idIndexMarker1091">&#13;
     </a>&#13;
     so far should be something similar&#13;
     <span class="No-Break">&#13;
      to this:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Next, let’s define a custom prompt template and pass it as an argument to&#13;
     <code class="literal">&#13;
      TitleExtractor&#13;
     </code>&#13;
     for the&#13;
     <span class="No-Break">&#13;
      second run:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Because we’ve added an extra instruction in this custom prompt, the extractor should now generate&#13;
     <a id="_idIndexMarker1092">&#13;
     </a>&#13;
     shorter titles. The output for the second run should be something along the lines of&#13;
     <span class="No-Break">&#13;
      the following:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     After seeing the basic mechanics of prompt customization, it’s time to move on to more&#13;
     <span class="No-Break">&#13;
      advanced methods.&#13;
     </span>&#13;
    </p>&#13;
    <a id="_idTextAnchor220">&#13;
    </a>&#13;
    <h2 id="_idParaDest-221">&#13;
     Using advanced prompting techniques in LlamaIndex&#13;
    </h2>&#13;
    <p>&#13;
     LlamaIndex offers several advanced prompting techniques that enable you to create more&#13;
     <a id="_idIndexMarker1093">&#13;
     </a>&#13;
     customized and expressive prompts, reuse existing prompts, and express certain operations&#13;
     <a id="_idIndexMarker1094">&#13;
     </a>&#13;
     more concisely. These&#13;
     <a id="_idIndexMarker1095">&#13;
     </a>&#13;
     techniques include partial formatting, prompt template variable mappings, and prompt function mappings.&#13;
     <em class="italic">&#13;
      Table 10.1&#13;
     </em>&#13;
     breaks down the purpose and potential use cases for&#13;
     <span class="No-Break">&#13;
      each method:&#13;
     </span>&#13;
    </p>&#13;
    <table class="No-Table-Style _idGenTablePara-1" id="table001-4">&#13;
     <colgroup>&#13;
      <col/>&#13;
      <col/>&#13;
     </colgroup>&#13;
     <tbody>&#13;
      <tr class="No-Table-Style">&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         <span class="No-Break">&#13;
          <strong class="bold">&#13;
           Method&#13;
          </strong>&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         <span class="No-Break">&#13;
          <strong class="bold">&#13;
           Purpose&#13;
          </strong>&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
      </tr>&#13;
      <tr class="No-Table-Style">&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         <span class="No-Break">&#13;
          Partial formatting&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         Allows you to partially format a prompt by filling in some variables but leaving others to be filled in later. This is useful because it allows you to format variables as they become available, rather than maintaining all the required prompt variables until the end. The method is particularly useful in a multi-step RAG scenario that gradually builds the prompt by gathering different&#13;
         <span class="No-Break">&#13;
          user inputs.&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
      </tr>&#13;
      <tr class="No-Table-Style">&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         Prompt template&#13;
         <span class="No-Break">&#13;
          variable mappings&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         They let you specify a mapping between some&#13;
         <em class="italic">&#13;
          expected&#13;
         </em>&#13;
         prompt keys and the keys actually used in your template, enabling you to reuse existing string templates without modifying the template variables. It is similar to creating an&#13;
         <em class="italic">&#13;
          alias&#13;
         </em>&#13;
         for&#13;
         <span class="No-Break">&#13;
          template keys.&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
      </tr>&#13;
      <tr class="No-Table-Style">&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         Prompt&#13;
         <span class="No-Break">&#13;
          function mappings&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
       <td class="No-Table-Style">&#13;
        <p>&#13;
         This feature allows you to dynamically inject certain values, depending on other values or conditions, during query time by passing functions as template variables instead of&#13;
         <span class="No-Break">&#13;
          fixed values.&#13;
         </span>&#13;
        </p>&#13;
       </td>&#13;
      </tr>&#13;
     </tbody>&#13;
    </table>&#13;
    <p class="IMG---Caption" lang="en-US">&#13;
     Table 10.1 – An overview of the more advanced prompting techniques provided by LlamaIndex&#13;
    </p>&#13;
    <p>&#13;
     You’ll&#13;
     <a id="_idIndexMarker1096">&#13;
     </a>&#13;
     find detailed&#13;
     <a id="_idIndexMarker1097">&#13;
     </a>&#13;
     code examples for all three&#13;
     <a id="_idIndexMarker1098">&#13;
     </a>&#13;
     methods in the official LlamaIndex&#13;
     <a id="_idIndexMarker1099">&#13;
     </a>&#13;
     documentation&#13;
     <span class="No-Break">&#13;
      here:&#13;
     </span>&#13;
     <a>&#13;
      <span class="No-Break">&#13;
       https://docs.llamaindex.ai/en/stable/examples/prompts/advanced_prompts.html&#13;
      </span>&#13;
     </a>&#13;
     <span class="No-Break">&#13;
      .&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Having all these new cool gadgets in our knowledge inventory, we can now refine and tailor the dialogue between our application and the LLM, allowing us to customize the behavior of almost any RAG component&#13;
     <span class="No-Break">&#13;
      of LlamaIndex.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     For the final section of this chapter, we move our focus to an important aspect of maximizing our RAG setup’s potential: the art and science of&#13;
     <span class="No-Break">&#13;
      prompt engineering.&#13;
     </span>&#13;
    </p>&#13;
    <a id="_idTextAnchor221">&#13;
    </a>&#13;
   </div>&#13;
  </div>&#13;
 </body>&#13;
</html>
<html>&#13;
 <head>&#13;
  <title>&#13;
   The golden rules of prompt engineering&#13;
  </title>&#13;
 </head>&#13;
 <body>&#13;
  <div class="epub-source">&#13;
   <h1 id="_idParaDest-222">&#13;
    The golden rules of prompt engineering&#13;
   </h1>&#13;
   <div id="_idContainer115">&#13;
    Classify the following reviews as positive or negative sentiment:&#13;
&lt;The food was delicious and the service was excellent!&gt; // Positive&#13;
&lt;I waited over an hour and my meal arrived cold.&gt; // Negative&#13;
&lt;The ambiance was nice but the dishes were overpriced.&gt; //&#13;
Output:&#13;
    There are 15 students in a class. 8 students have dogs as pets.&#13;
If 3 more students get a dog, how many of them would have a dog as a pet then?&#13;
Step 1) Initially there are 15 students and 8 have dogs&#13;
Step 2) 3 more students will get dogs soon&#13;
Step 3) So the final number is the initial 8 students with dogs plus the 3 new students = 8 + 3 = 11&#13;
Therefore, the number of students that would have a dog as a pet is 11.&#13;
A factory makes 100 items daily. On Tuesday, they boost production by 40% for a special order. However, to adjust inventory, they cut Thursday's output by 20% from Tuesday's high. Then, expecting a sales increase, Friday's output rises by 10% over the day before. Calculate the production numbers for Tuesday, Thursday, and Friday.&#13;
    Let's simulate a verbal conversation between three experts who tackle a complex puzzle.&#13;
Each expert outlines one step in their thought process before exchanging insights with the others, without adding any unnecessary remarks. As they progress, any expert who identifies a flaw in their reasoning exits the discussion. The process continues until a solution is found or all available options have been exhausted. The problem they need to solve is:&#13;
"Using only numbers 3, 3, 7, 7 and basic arithmetic operations, is it possible to obtain the value 25?"&#13;
    <p>&#13;
     This section is not intended to serve as a definitive guide to prompt engineering. In fact, the field is an&#13;
     <a id="_idIndexMarker1100">&#13;
     </a>&#13;
     ever-expanding one. Since many LLMs are demonstrating emerging capabilities that were not initially anticipated, it is only natural that our methods of interacting with these linguistic experts will also be refined over time. In other words, as LLMs evolve to better model and understand human nature, we in turn learn new ways of interacting with them. In this section, I aim to present some of the most commonly used techniques in prompt engineering, as well as the basic principles that govern the field. As stated in the previous section, writing a good prompt requires a fine balance between several parameters. Here are some of the most important aspects to consider when building prompts for a&#13;
     <span class="No-Break">&#13;
      RAG application.&#13;
     </span>&#13;
    </p>&#13;
    <a id="_idTextAnchor222">&#13;
    </a>&#13;
    <h2 id="_idParaDest-223">&#13;
     Accuracy and clarity in expression&#13;
    </h2>&#13;
    <p>&#13;
     The prompt should be clear and precise, avoiding ambiguity. The more clearly you state what you&#13;
     <a id="_idIndexMarker1101">&#13;
     </a>&#13;
     need, the more likely you are to get a relevant response. It’s important to articulate the question or task in a way that leaves little room for misinterpretation. Make no assumptions about the model’s ability to understand your message. These assumptions are usually biased and tend to produce hallucinations&#13;
     <span class="No-Break">&#13;
      in return.&#13;
     </span>&#13;
    </p>&#13;
    <a id="_idTextAnchor223">&#13;
    </a>&#13;
    <h2 id="_idParaDest-224">&#13;
     Directiveness&#13;
    </h2>&#13;
    <p>&#13;
     How directive the prompt is can significantly impact the response. A prompt can range from open-ended – encouraging creative or broad responses – to highly specific – requesting&#13;
     <a id="_idIndexMarker1102">&#13;
     </a>&#13;
     a very particular type of answer. The level of directiveness should match the intended outcome. Given that we’re actually building prompt templates that mix a static part with dynamically retrieved content, consider exceptional scenarios and edge cases in which the model might misunderstand the prompt. Use clear instructions or commands (for example,&#13;
     <code class="literal">&#13;
      Summarize&#13;
     </code>&#13;
     ,&#13;
     <code class="literal">&#13;
      Analyze&#13;
     </code>&#13;
     , and&#13;
     <code class="literal">&#13;
      Explain&#13;
     </code>&#13;
     ) to guide the model on the desired task. Our prompts must be broad enough to accommodate varied inputs yet detailed enough to direct the&#13;
     <span class="No-Break">&#13;
      model effectively.&#13;
     </span>&#13;
    </p>&#13;
    <a id="_idTextAnchor224">&#13;
    </a>&#13;
    <h2 id="_idParaDest-225">&#13;
     Context quality&#13;
    </h2>&#13;
    <p>&#13;
     This is a major pain point for building an effective RAG system. Both the quality and structure&#13;
     <a id="_idIndexMarker1103">&#13;
     </a>&#13;
     of our proprietary knowledge base as well as the ability to retrieve the most relevant context from it are very important aspects.&#13;
     <em class="italic">&#13;
      Garbage in, garbage out&#13;
     </em>&#13;
     may be regarded as a general rule applicable to this subject. Try to remove any inconsistencies in the data, special characters that might derail the LLM, duplicate data, and even grammatical errors in the text. These types of quality issues will unfortunately affect both the retrieval and the final response synthesis. Experiment with different retrieval strategies, such as the ones discussed in&#13;
     <a>&#13;
      <span class="No-Break">&#13;
       <em class="italic">&#13;
        Chapter 6&#13;
       </em>&#13;
      </span>&#13;
     </a>&#13;
     ,&#13;
     <em class="italic">&#13;
      Querying Our Data, Part 1 – Context Retrieval&#13;
     </em>&#13;
     . Try different values for&#13;
     <code class="literal">&#13;
      similarity_top_k&#13;
     </code>&#13;
     ,&#13;
     <code class="literal">&#13;
      chunk_size&#13;
     </code>&#13;
     , and&#13;
     <code class="literal">&#13;
      chunk_overlap&#13;
     </code>&#13;
     , as discussed in&#13;
     <a>&#13;
      <span class="No-Break">&#13;
       <em class="italic">&#13;
        Chapter 4&#13;
       </em>&#13;
      </span>&#13;
     </a>&#13;
     ,&#13;
     <em class="italic">&#13;
      Ingesting Data into Our RAG Workflow&#13;
     </em>&#13;
     . Employ re-rankers and Node postprocessors to increase the context quality, as we did in&#13;
     <a>&#13;
      <span class="No-Break">&#13;
       <em class="italic">&#13;
        Chapter 7&#13;
       </em>&#13;
      </span>&#13;
     </a>&#13;
     ,&#13;
     <em class="italic">&#13;
      Querying Our Data, Part 2 – Postprocessing and&#13;
     </em>&#13;
     <span class="No-Break">&#13;
      <em class="italic">&#13;
       Response Synthesis&#13;
      </em>&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      .&#13;
     </span>&#13;
    </p>&#13;
    <a id="_idTextAnchor225">&#13;
    </a>&#13;
    <h2 id="_idParaDest-226">&#13;
     Context quantity&#13;
    </h2>&#13;
    <p>&#13;
     There’s a balance between being concise and offering sufficient detail. A prompt should be brief enough to maintain focus but detailed enough to convey the specific requirements&#13;
     <a id="_idIndexMarker1104">&#13;
     </a>&#13;
     of the task or question. Too little context may result in answers that lack depth or relevance, while too much may confuse&#13;
     <a id="_idIndexMarker1105">&#13;
     </a>&#13;
     the model or lead&#13;
     <span class="No-Break">&#13;
      it off-topic.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     In RAG scenarios, as the amount of context provided in a prompt increase, it’s important to consider the potential impact on the alignment and accuracy of generated responses. While providing more context can be beneficial in many cases, as it gives the language model a broader understanding of the task at hand, there are also risks associated with excessively&#13;
     <span class="No-Break">&#13;
      long prompts.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     For example, when a prompt becomes too long, there is a higher chance of introducing irrelevant or contradictory information. This can lead to misalignment between the intended task and the model’s understanding of it. The model may give too much attention to tangential details or lose focus on the core objective. Maintaining a clear and concise prompt helps ensure that the model stays aligned with the&#13;
     <span class="No-Break">&#13;
      desired output.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Also, as the context grows, the model has to process and consider a larger amount of information. This&#13;
     <a id="_idIndexMarker1106">&#13;
     </a>&#13;
     increased&#13;
     <strong class="bold">&#13;
      cognitive load&#13;
     </strong>&#13;
     can lead to a decrease in accuracy. The model may struggle to identify the most relevant pieces of information or may give undue importance to less significant details. Additionally, longer prompts are more likely to contain ambiguities or inconsistencies, which can further degrade the accuracy of&#13;
     <span class="No-Break">&#13;
      the responses.&#13;
     </span>&#13;
    </p>&#13;
    <p class="callout-heading">&#13;
     Cognitive load in the context of LLMs&#13;
    </p>&#13;
    <p class="callout">&#13;
     Cognitive load refers to the amount of processing effort and resources required by the language model to process, understand, and generate a response based on the provided context. In the case of RAG systems, the cognitive load is directly related to the quantity and complexity of the information present in&#13;
     <span class="No-Break">&#13;
      the prompt.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Implementing Node postprocessors such as&#13;
     <code class="literal">&#13;
      SimilarityPostprocessor&#13;
     </code>&#13;
     or&#13;
     <code class="literal">&#13;
      SentenceEmbeddingOptimizer&#13;
     </code>&#13;
     can partially mitigate this issue by filtering less relevant Nodes or shortening their content, and therefore reducing the final prompt submitted to the LLM. We covered these methods in&#13;
     <a>&#13;
      <span class="No-Break">&#13;
       <em class="italic">&#13;
        Chapter 7&#13;
       </em>&#13;
      </span>&#13;
     </a>&#13;
     ,&#13;
     <em class="italic">&#13;
      Querying Our Data, Part 2 – Postprocessing and Response Synthesis&#13;
     </em>&#13;
     . Moreover, if the retrieved context is inherently long, consider breaking it down into smaller, more&#13;
     <span class="No-Break">&#13;
      manageable chunks.&#13;
     </span>&#13;
    </p>&#13;
    <h3>&#13;
     Context ordering&#13;
    </h3>&#13;
    <p>&#13;
     The overall effectiveness of our RAG pipeline does not rely just on the quantity and quality of context. Especially when dealing with longer context, most LLMs may perform differently&#13;
     <a id="_idIndexMarker1107">&#13;
     </a>&#13;
     when trying to extract the key information from that context, depending on where exactly that key information is placed. A good approach is to structure the prompt hierarchically, with the most critical information at the beginning or at the end. This ensures that the model prioritizes the core instructions and context. That’s where tools such as Node re-rankers or the&#13;
     <code class="literal">&#13;
      LongContextReorder&#13;
     </code>&#13;
     postprocessor may&#13;
     <span class="No-Break">&#13;
      become useful.&#13;
     </span>&#13;
    </p>&#13;
    <p class="callout-heading">&#13;
     Side note&#13;
    </p>&#13;
    <p class="callout">&#13;
     There’s an increasingly popular RAG evaluation technique called the&#13;
     <em class="italic">&#13;
      needle in a haystack test&#13;
     </em>&#13;
     , in which researchers gauge the model’s ability to notice and recall a very specific piece of information from a larger context provided to the LLM. This specific information looks unsuspecting and is usually seamlessly blended into the overall context. In many ways, this method is similar to testing a human’s ability to pay attention to a certain text and then recall key information in&#13;
     <span class="No-Break">&#13;
      that text.&#13;
     </span>&#13;
    </p>&#13;
    <a id="_idTextAnchor226">&#13;
    </a>&#13;
    <h2 id="_idParaDest-227">&#13;
     Required output format&#13;
    </h2>&#13;
    <p>&#13;
     In most cases, when building RAG workflows, we need LLMs to generate structured or semi-structured outputs. In almost all scenarios, we need the output to be predictable in terms of&#13;
     <a id="_idIndexMarker1108">&#13;
     </a>&#13;
     format, size, or language. Sometimes, providing a few&#13;
     <a id="_idIndexMarker1109">&#13;
     </a>&#13;
     examples in our prompt may lead to better responses, but that’s not a silver bullet for all scenarios. That’s were using output parsers and Pydantic programs becomes really important. We talked about these topics in&#13;
     <a>&#13;
      <span class="No-Break">&#13;
       <em class="italic">&#13;
        Chapter 7&#13;
       </em>&#13;
      </span>&#13;
     </a>&#13;
     ,&#13;
     <em class="italic">&#13;
      Querying Our Data, Part 2 – Postprocessing and&#13;
     </em>&#13;
     <span class="No-Break">&#13;
      <em class="italic">&#13;
       Response Synthesis&#13;
      </em>&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      .&#13;
     </span>&#13;
    </p>&#13;
    <a id="_idTextAnchor227">&#13;
    </a>&#13;
    <h2 id="_idParaDest-228">&#13;
     Inference cost&#13;
    </h2>&#13;
    <p>&#13;
     In most cases, we’ll be running our applications within very specific cost constraints. Ignoring&#13;
     <a id="_idIndexMarker1110">&#13;
     </a>&#13;
     token usage would be a clear mistake. So, make&#13;
     <a id="_idIndexMarker1111">&#13;
     </a>&#13;
     sure you’re doing cost estimations, and always keep track of token usage. In addition, you could use tools such as&#13;
     <code class="literal">&#13;
      LongLLMLinguaPostprocessor&#13;
     </code>&#13;
     for prompt compression. We talked about this Node postprocessor in&#13;
     <a>&#13;
      <span class="No-Break">&#13;
       <em class="italic">&#13;
        Chapter 7&#13;
       </em>&#13;
      </span>&#13;
     </a>&#13;
     ,&#13;
     <em class="italic">&#13;
      Querying Our Data, Part 2 – Postprocessing and Response Synthesis&#13;
     </em>&#13;
     . Prompt compression techniques have the potential to improve not only cost efficiency but also the quality of the final response by eliminating redundant information from our context and keeping just&#13;
     <span class="No-Break">&#13;
      key information.&#13;
     </span>&#13;
    </p>&#13;
    <a id="_idTextAnchor228">&#13;
    </a>&#13;
    <h2 id="_idParaDest-229">&#13;
     Overall system latency&#13;
    </h2>&#13;
    <p>&#13;
     While this parameter depends on many factors, bloated, inefficient, or ambiguous prompts can&#13;
     <a id="_idIndexMarker1112">&#13;
     </a>&#13;
     also negatively affect system latency. It’s just like talking to a real person. The longer and less efficient the query, the more&#13;
     <a id="_idIndexMarker1113">&#13;
     </a>&#13;
     processing will be required from the model in order to best understand the actual intent behind the query. Longer processing times will negatively impact the overall&#13;
     <span class="No-Break">&#13;
      user experience.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Prompt engineering is a continuous process of experimentation and iteration. Regularly evaluate the performance of your prompts and refine them based on the results. Remember – this is a long game, and the rules are being constantly re-written. Try to keep your knowledge up to date with the latest advancements and techniques in prompt engineering, as the field is&#13;
     <span class="No-Break">&#13;
      rapidly evolving.&#13;
     </span>&#13;
    </p>&#13;
    <a id="_idTextAnchor229">&#13;
    </a>&#13;
    <h2 id="_idParaDest-230">&#13;
     Choosing the right LLM for the task&#13;
    </h2>&#13;
    <p>&#13;
     In the world of AI, not all LLMs are equal. In&#13;
     <a>&#13;
      <span class="No-Break">&#13;
       <em class="italic">&#13;
        Chapter 9&#13;
       </em>&#13;
      </span>&#13;
     </a>&#13;
     ,&#13;
     <em class="italic">&#13;
      Customizing and Deploying Our LlamaIndex Project&#13;
     </em>&#13;
     , we already saw how easy is to customize different components&#13;
     <a id="_idIndexMarker1114">&#13;
     </a>&#13;
     of our RAG pipeline, including the underlying LLM. But there are actually many options available, so which one should we&#13;
     <a id="_idIndexMarker1115">&#13;
     </a>&#13;
     select for the job? Choosing the&#13;
     <em class="italic">&#13;
      wrong&#13;
     </em>&#13;
     LLM for a particular task will likely cancel many of the efforts we invested in crafting the actual prompts. It’s pretty much like trying to get an answer from the wrong person. If you’re persuasive enough, chances are you’ll get an answer at some point. However, that may not be the answer you were&#13;
     <span class="No-Break">&#13;
      looking for.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     That’s why understanding the different flavors of LLMs and knowing which one qualifies for a given task is essential. Several key characteristics should be useful for our model selection. Let’s look at&#13;
     <span class="No-Break">&#13;
      these next.&#13;
     </span>&#13;
    </p>&#13;
    <h3>&#13;
     Model architecture&#13;
    </h3>&#13;
    <p>&#13;
     Models can have different underlying architectures, and these may determine their inherent&#13;
     <a id="_idIndexMarker1116">&#13;
     </a>&#13;
     capabilities. For example, encoder-only models are specialized in encoding and classifying input text, useful for categorizing text into&#13;
     <a id="_idIndexMarker1117">&#13;
     </a>&#13;
     defined categories, such as with&#13;
     <strong class="bold">&#13;
      Bidirectional Encoder Representations from Transformers&#13;
     </strong>&#13;
     (&#13;
     <strong class="bold">&#13;
      BERT&#13;
     </strong>&#13;
     ), which&#13;
     <a id="_idIndexMarker1118">&#13;
     </a>&#13;
     excels in&#13;
     <strong class="bold">&#13;
      next sentence prediction&#13;
     </strong>&#13;
     (&#13;
     <strong class="bold">&#13;
      NSP&#13;
     </strong>&#13;
     )&#13;
     <span class="No-Break">&#13;
      tasks (&#13;
     </span>&#13;
     <a>&#13;
      <span class="No-Break">&#13;
       https://en.wikipedia.org/wiki/BERT_(language_model)&#13;
      </span>&#13;
     </a>&#13;
     <span class="No-Break">&#13;
      ).&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Encoder-decoder models are capable of both understanding input text and generating responses, making them ideal for text generation and comprehension tasks, such as translation&#13;
     <a id="_idIndexMarker1119">&#13;
     </a>&#13;
     and summarizing articles. One example that fits in this category is&#13;
     <strong class="bold">&#13;
      Bidirectional and Auto-Regressive Transformer&#13;
     </strong>&#13;
     (&#13;
     <span class="No-Break">&#13;
      <strong class="bold">&#13;
       BART&#13;
      </strong>&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      ) (&#13;
     </span>&#13;
     <a>&#13;
      <span class="No-Break">&#13;
       https://huggingface.co/docs/transformers/en/model_doc/bart&#13;
      </span>&#13;
     </a>&#13;
     <span class="No-Break">&#13;
      ).&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Decoder-only models can decode or generate subsequent words or tokens from a given prompt and&#13;
     <a id="_idIndexMarker1120">&#13;
     </a>&#13;
     are primarily used for text generation. Models such as&#13;
     <strong class="bold">&#13;
      Generative Pre-trained Transformer&#13;
     </strong>&#13;
     (&#13;
     <strong class="bold">&#13;
      GPT&#13;
     </strong>&#13;
     ), Mistral, Claude, and LLaMa are superstars in&#13;
     <span class="No-Break">&#13;
      this domain.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     There are&#13;
     <a id="_idIndexMarker1121">&#13;
     </a>&#13;
     also more exotic architectures such as&#13;
     <strong class="bold">&#13;
      Mixture-of-Experts&#13;
     </strong>&#13;
     (&#13;
     <strong class="bold">&#13;
      MoE&#13;
     </strong>&#13;
     ), which essentially leverage a&#13;
     <em class="italic">&#13;
      sparse MoE&#13;
     </em>&#13;
     framework to offer dynamic, token-specific processing – see Shazeer et al. (2017),&#13;
     <em class="italic">&#13;
      Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer&#13;
     </em>&#13;
     (&#13;
     <a>&#13;
      https://doi.org/10.48550/arXiv.1701.06538&#13;
     </a>&#13;
     ). This approach can significantly enhance performance across a range of domains, including mathematics, code generation, and multilingual&#13;
     <a id="_idIndexMarker1122">&#13;
     </a>&#13;
     tasks, as demonstrated by&#13;
     <span class="No-Break">&#13;
      <strong class="bold">&#13;
       Mixtral 8x7B&#13;
      </strong>&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      .&#13;
     </span>&#13;
    </p>&#13;
    <h3>&#13;
     Model size&#13;
    </h3>&#13;
    <p>&#13;
     Model size is&#13;
     <a id="_idIndexMarker1123">&#13;
     </a>&#13;
     another critical factor to consider when selecting an LLM, as it directly impacts both the potential computational cost and the model’s capabilities. The number of parameters within an LLM, ranging from weights to biases adjusted during training, serves as a proxy for understanding the model’s complexity and, by extension, its operational expense. Larger models, such as GPT-4 with its estimated 1.76 trillion parameters, offer profound capabilities but come with higher costs&#13;
     <a id="_idIndexMarker1124">&#13;
     </a>&#13;
     and requirements for computational resources. On the other hand, medium-sized models, typically under 10 billion parameters, strike a balance between affordability and performance, making them suitable for a wide array of applications without breaking&#13;
     <span class="No-Break">&#13;
      the bank.&#13;
     </span>&#13;
    </p>&#13;
    <h3>&#13;
     Inference speed&#13;
    </h3>&#13;
    <p>&#13;
     That’s also a key parameter as it determines how quickly a model can process input and generate output. While larger models may offer enhanced performance in terms of output quality&#13;
     <a id="_idIndexMarker1125">&#13;
     </a>&#13;
     and depth, their inference speed tends to be slower due to the sheer volume of computations required. It’s important to note that inference speed is influenced by various factors beyond just the number of parameters, including the efficiency of the model architecture and the computational infrastructure used. Techniques to reduce inference time, such as model pruning, quantization, and leveraging specialized hardware, can significantly improve the usability of LLMs in&#13;
     <span class="No-Break">&#13;
      real-world applications.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     To make things even more complex, apart from these characteristics, LLMs can be specialized for various tasks or domains, enhancing their performance in specific scenarios. This specialization arises from the type of data and the training objectives used to fine-tune the model. Let’s look at some common&#13;
     <span class="No-Break">&#13;
      specializations next.&#13;
     </span>&#13;
    </p>&#13;
    <h3>&#13;
     Chat models&#13;
    </h3>&#13;
    <p>&#13;
     Chat models are optimized for conversational interactions. They are designed to engage users in dialogue, providing responses that mimic human-like conversation. These models are&#13;
     <a id="_idIndexMarker1126">&#13;
     </a>&#13;
     adept at back-and-forth exchanges and can maintain context over a series&#13;
     <span class="No-Break">&#13;
      of interactions.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     They are the ideal choice for building chatbots or virtual assistants where the interaction is more casual or conversational. These models are used in applications requiring natural, engaging dialogue with users, such as customer service bots, entertainment applications, or virtual companions. As a particular characteristic, they tend to be more open-ended in their responses, aiming to generate replies that are engaging, contextually relevant, and sometimes&#13;
     <span class="No-Break">&#13;
      even entertaining.&#13;
     </span>&#13;
    </p>&#13;
    <h3>&#13;
     Instruct models&#13;
    </h3>&#13;
    <p>&#13;
     Instruct models are fine-tuned to understand and execute specific instructions or queries. They prioritize executing the given task based on the instruction over engaging in a dialogue. That makes them suitable for scenarios where the user needs the model to perform a&#13;
     <a id="_idIndexMarker1127">&#13;
     </a>&#13;
     particular task, such as summarizing a document, generating code based on a prompt, or providing detailed explanations. These models are preferred in educational tools, productivity applications, and anywhere a direct, clear response to a query is needed, such as in the intricate workflow of a&#13;
     <span class="No-Break">&#13;
      RAG application.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     They are more focused on accuracy and relevance to the task at hand rather than maintaining a conversational tone. Their responses are tailored toward fulfilling the user’s request as efficiently and effectively&#13;
     <span class="No-Break">&#13;
      as possible.&#13;
     </span>&#13;
    </p>&#13;
    <h3>&#13;
     Codex models&#13;
    </h3>&#13;
    <p>&#13;
     These models are optimized for understanding and generating code. They have been trained in a&#13;
     <a id="_idIndexMarker1128">&#13;
     </a>&#13;
     vast corpus of programming languages and can assist with coding tasks, debug code, explain code snippets, and even generate entire programs based on a description. This makes them the perfect candidates for integrating into development environments, coding education tools, and anywhere automated coding assistance&#13;
     <span class="No-Break">&#13;
      is beneficial.&#13;
     </span>&#13;
    </p>&#13;
    <h3>&#13;
     Summarization models&#13;
    </h3>&#13;
    <p>&#13;
     Specialized in condensing long texts into shorter summaries while retaining key information&#13;
     <a id="_idIndexMarker1129">&#13;
     </a>&#13;
     and context. These models focus on capturing the essence of the content and presenting it concisely. They are useful for news aggregation services, research, content creation, and any scenario where quick insights from long documents&#13;
     <span class="No-Break">&#13;
      are needed.&#13;
     </span>&#13;
    </p>&#13;
    <h3>&#13;
     Translation models&#13;
    </h3>&#13;
    <p>&#13;
     As the name implies these models are designed to translate text from one language to another. They have&#13;
     <a id="_idIndexMarker1130">&#13;
     </a>&#13;
     been trained on large multilingual datasets to understand and translate between languages with high accuracy, and they are best suited for global communication platforms, content localization, and educational tools aimed at&#13;
     <span class="No-Break">&#13;
      language learners.&#13;
     </span>&#13;
    </p>&#13;
    <h3>&#13;
     Question-answering models&#13;
    </h3>&#13;
    <p>&#13;
     Fine-tuned to understand questions posed in NL and provide accurate answers by referencing&#13;
     <a id="_idIndexMarker1131">&#13;
     </a>&#13;
     provided texts or their vast training data, these models are key in building intelligent search engines, educational aids, and interactive&#13;
     <span class="No-Break">&#13;
      knowledge bases.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     And the list could probably go on with other types of models, fine-tuned for specific domains or applications. Also, keep in mind that because these different specializations tend to enhance or diminish certain capabilities of the model, our carefully crafted prompts may yield inconsistent results. For one model, a prompt may lead to near-perfect responses, while for another it could barely hit an&#13;
     <span class="No-Break">&#13;
      average mark.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     When choosing your LLM, it’s essential to weigh the trade-offs between all these characteristics and the specific requirements of your RAG application. Understanding these aspects helps in selecting a model that not only fits within your budget but also meets your performance and speed expectations. Whether you’re deploying an LLM for real-time applications requiring quick responses or complex tasks demanding deep understanding and generation capabilities, the chosen model will have a profound impact on the outcomes of your LlamaIndex application. But keep in mind that you’re never limited to using a single model for your entire RAG logic. As LlamaIndex gives you endless possibilities for customization, working with a suite of different models can also be an option. You just have to experiment and evaluate until you find the ideal mix and purpose for&#13;
     <span class="No-Break">&#13;
      each one.&#13;
     </span>&#13;
    </p>&#13;
    <a id="_idTextAnchor230">&#13;
    </a>&#13;
    <h2 id="_idParaDest-231">&#13;
     Common methods used for creating effective prompts&#13;
    </h2>&#13;
    <p>&#13;
     While simple prompts can be useful for many tasks, more advanced techniques are often required&#13;
     <a id="_idIndexMarker1132">&#13;
     </a>&#13;
     for complex&#13;
     <a id="_idIndexMarker1133">&#13;
     </a>&#13;
     reasoning or multi-step processes. While definitely not exhaustive, this section covers several powerful prompting techniques that can significantly enhance the performance of language models in our RAG applications. Since there’s already an abundance of study materials, free courses, and plenty of examples available on the web, in case you’re not yet familiar with these methods, take this list as a mere starting point for your future&#13;
     <span class="No-Break">&#13;
      learning path.&#13;
     </span>&#13;
    </p>&#13;
    <h3>&#13;
     Few-shot prompting, also known as k-shot prompting&#13;
    </h3>&#13;
    <p>&#13;
     As described in the paper by Brown et al. (2020),&#13;
     <em class="italic">&#13;
      Language Models are Few-Shot Learners&#13;
     </em>&#13;
     (&#13;
     <a>&#13;
      https://doi.org/10.48550/arXiv.2005.14165&#13;
     </a>&#13;
     ), for complex tasks involving LLMs, few-shot&#13;
     <a id="_idIndexMarker1134">&#13;
     </a>&#13;
     prompting with demonstrations&#13;
     <a id="_idIndexMarker1135">&#13;
     </a>&#13;
     can enable in-context learning and improve performance. This&#13;
     <a id="_idIndexMarker1136">&#13;
     </a>&#13;
     method relies on providing a few examples of the task, along with the expected output, to condition the model. You can experiment with different numbers of examples (for example, one-shot, three-shot, and five-shot) to find the optimal balance, hence the&#13;
     <em class="italic">&#13;
      k-shot&#13;
     </em>&#13;
     <span class="No-Break">&#13;
      alternative name.&#13;
     </span>&#13;
    </p>&#13;
    <p class="callout-heading">&#13;
     What about zero-shot prompting?&#13;
    </p>&#13;
    <p class="callout">&#13;
     For reference,&#13;
     <em class="italic">&#13;
      zero-shot prompting&#13;
     </em>&#13;
     involves presenting a model with a question without any&#13;
     <a id="_idIndexMarker1137">&#13;
     </a>&#13;
     preceding contextual question/answer pairs. This approach is more challenging for the model compared to one-shot or few-shot prompting, due to the absence&#13;
     <span class="No-Break">&#13;
      of context.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     When using few-shot prompting, keep in mind that the format you use for the examples and the distribution of the input text are important factors that can affect performance. While the few-shot prompting method increases the probability of a correct answer for simpler tasks, it may still struggle with more complex reasoning scenarios. Here’s a practical prompt example using&#13;
     <span class="No-Break">&#13;
      this technique:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Providing the model with a few examples in this style enables in-context learning and improves performance on the task without&#13;
     <span class="No-Break">&#13;
      requiring fine-tuning.&#13;
     </span>&#13;
    </p>&#13;
    <h3>&#13;
     Chain-of-Thought (CoT) prompting&#13;
    </h3>&#13;
    <p>&#13;
     First introduced in the paper by Wei et al. (2023),&#13;
     <em class="italic">&#13;
      Chain-of-Thought Prompting Elicits Reasoning in Large Language Models&#13;
     </em>&#13;
     (&#13;
     <a>&#13;
      https://doi.org/10.48550/arXiv.2201.11903&#13;
     </a>&#13;
     ), this method provides impressive results for LLM tasks requiring reasoning or multi-step processes. We can use CoT prompting to encourage the model to break down the problem and show its thought process. We can include&#13;
     <a id="_idIndexMarker1138">&#13;
     </a>&#13;
     examples in our prompts, demonstrating&#13;
     <a id="_idIndexMarker1139">&#13;
     </a>&#13;
     the step-by-step reasoning process in the prompt. Here is a practical&#13;
     <span class="No-Break">&#13;
      prompt example:&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     The first part of the prompt demonstrates the reasoning process, guiding the LLM to better answer the second part – which represents the&#13;
     <span class="No-Break">&#13;
      actual task.&#13;
     </span>&#13;
    </p>&#13;
    <h3>&#13;
     Self-consistency&#13;
    </h3>&#13;
    <p>&#13;
     Self-consistency&#13;
     <a id="_idIndexMarker1140">&#13;
     </a>&#13;
     aims to improve the performance&#13;
     <a id="_idIndexMarker1141">&#13;
     </a>&#13;
     of CoT prompting by sampling multiple, diverse reasoning paths and using the generations to select the most consistent answer. First introduced in the paper by Wang et al. (2023),&#13;
     <em class="italic">&#13;
      Self-Consistency Improves Chain of Thought Reasoning in Language Models&#13;
     </em>&#13;
     (&#13;
     <a>&#13;
      https://doi.org/10.48550/arXiv.2203.11171&#13;
     </a>&#13;
     ), the self-consistency method helps boost performance on tasks involving arithmetic and commonsense reasoning by replacing the more traditional CoT prompting. Self-consistency involves providing few-shot CoT examples, generating multiple reasoning paths, and then selecting the most consistent answer based on&#13;
     <span class="No-Break">&#13;
      these paths.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     This approach&#13;
     <a id="_idIndexMarker1142">&#13;
     </a>&#13;
     acknowledges that language models, like humans, may&#13;
     <a id="_idIndexMarker1143">&#13;
     </a>&#13;
     sometimes make mistakes or take incorrect reasoning steps. However, by leveraging the diversity of reasoning paths and selecting the most consistent answer, self-consistency can potentially provide better answers than&#13;
     <span class="No-Break">&#13;
      CoT prompting.&#13;
     </span>&#13;
    </p>&#13;
    <h3>&#13;
     Tree of Thoughts (ToT) prompting&#13;
    </h3>&#13;
    <p>&#13;
     ToT is a framework that generalizes over CoT prompting and encourages the exploration of thoughts&#13;
     <a id="_idIndexMarker1144">&#13;
     </a>&#13;
     that serve as&#13;
     <a id="_idIndexMarker1145">&#13;
     </a>&#13;
     intermediate steps for general problem-solving with language models. Under the hood, it maintains a&#13;
     <em class="italic">&#13;
      tree of thoughts&#13;
     </em>&#13;
     , where thoughts represent coherent language sequences that serve as intermediate steps toward solving a problem. The language model’s ability to generate and evaluate thoughts is combined with specialized search algorithms to enable systematic exploration of thoughts. ToT prompting involves prompting the language model to evaluate intermediate thoughts as&#13;
     <em class="italic">&#13;
      sure&#13;
     </em>&#13;
     /&#13;
     <em class="italic">&#13;
      maybe&#13;
     </em>&#13;
     /&#13;
     <em class="italic">&#13;
      impossible&#13;
     </em>&#13;
     with regard to reaching the desired solution and then using search algorithms to explore the most promising paths. The method was presented for the first time in the following papers: Yao et al. (2023),&#13;
     <em class="italic">&#13;
      Tree of Thoughts: Deliberate Problem Solving with Large Language Models&#13;
     </em>&#13;
     (&#13;
     <a>&#13;
      https://doi.org/10.48550/arXiv.2305.10601&#13;
     </a>&#13;
     ), and Long et al. (2023),&#13;
     <em class="italic">&#13;
      Large Language Model Guided&#13;
     </em>&#13;
     <span class="No-Break">&#13;
      <em class="italic">&#13;
       Tree-of-Thought&#13;
      </em>&#13;
     </span>&#13;
     <span class="No-Break">&#13;
      (&#13;
     </span>&#13;
     <a>&#13;
      <span class="No-Break">&#13;
       https://doi.org/10.48550/arXiv.2305.08291&#13;
      </span>&#13;
     </a>&#13;
     <span class="No-Break">&#13;
      ).&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Here’s a&#13;
     <span class="No-Break">&#13;
      sample prompt:&#13;
     </span>&#13;
    </p>&#13;
    <h3>&#13;
     Prompt chaining&#13;
    </h3>&#13;
    <p>&#13;
     This method relies on breaking down complex tasks into subtasks and using a chain of prompts, where&#13;
     <a id="_idIndexMarker1146">&#13;
     </a>&#13;
     each prompt’s output serves&#13;
     <a id="_idIndexMarker1147">&#13;
     </a>&#13;
     as an input for the next. Similar to the approach I used for the PITS application in the&#13;
     <code class="literal">&#13;
      training_material_builder.py&#13;
     </code>&#13;
     module, prompt chaining can improve the reliability, transparency, and controllability of the application. By default, in RAG applications, we use separate prompts for retrieving relevant information and generating a final output based on the&#13;
     <span class="No-Break">&#13;
      retrieved context.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     By following these golden rules and methods, you can develop more effective and reliable RAG applications using LlamaIndex and leverage the full potential&#13;
     <span class="No-Break">&#13;
      of LLMs.&#13;
     </span>&#13;
    </p>&#13;
    <a id="_idTextAnchor231">&#13;
    </a>&#13;
   </div>&#13;
  </div>&#13;
 </body>&#13;
</html>
<html>&#13;
 <head>&#13;
  <title>&#13;
   Summary&#13;
  </title>&#13;
 </head>&#13;
 <body>&#13;
  <div class="epub-source">&#13;
   <h1 id="_idParaDest-232">&#13;
    Summary&#13;
   </h1>&#13;
   <div id="_idContainer115">&#13;
    <p>&#13;
     This chapter explored the importance of prompt engineering in building effective RAG applications with LlamaIndex. We learned how to inspect and customize the default prompts used by&#13;
     <span class="No-Break">&#13;
      various components.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     The chapter provided an overview of key principles and best practices for crafting high-quality prompts, as well as advanced prompting techniques. Additionally, it emphasized the significance of choosing the right language model for the task at hand and understanding their different architectures, capabilities,&#13;
     <span class="No-Break">&#13;
      and trade-offs.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     Finally, we talked about some simple yet powerful prompting methods, such as few-shot prompting, CoT prompting, self-consistency, ToT, and prompt chaining to enhance the reasoning and problem-solving abilities of language models. Mastering prompt engineering is crucial for unlocking the full potential of LLMs in&#13;
     <span class="No-Break">&#13;
      RAG applications.&#13;
     </span>&#13;
    </p>&#13;
    <p>&#13;
     As we prepare to wrap up our journey, I invite you to join me in the final chapter of this book, where I will do my best to equip you with some additional learning tools and provide you with a bit of guidance on your future&#13;
     <span class="No-Break">&#13;
      learning path.&#13;
     </span>&#13;
    </p>&#13;
   </div>&#13;
  </div>&#13;
 </body>&#13;
</html></body></html>