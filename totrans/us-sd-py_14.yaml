- en: '14'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '14'
- en: Generating Video Using Stable Diffusion
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用稳定扩散生成视频
- en: Harnessing the power of the Stable Diffusion model, we can generate high-quality
    images using techniques such as LoRA, text embedding, and ControlNet. A natural
    progression from static images is toward dynamic content, that is, videos. Can
    we generate consistent videos using the Stable Diffusion model?
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 利用稳定扩散模型的力量，我们可以通过LoRA、文本嵌入和控制网等技术生成高质量的图像。从静态图像的自然发展是动态内容，即视频。我们能否使用稳定扩散模型生成一致的视频？
- en: The Stable Diffusion model’s UNet architecture, while effective for single-image
    processing, lacks contextual awareness when dealing with multiple images. Consequently,
    generating identical or consistently related images with the same prompt and parameters
    but different seeds is challenging. The resulting images may vary significantly
    in color, shape, or style due to the randomness introduced by the model’s nature.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 稳定扩散模型的 UNet 架构虽然对单图处理有效，但在处理多图时缺乏上下文感知能力。因此，使用相同的提示和参数但不同的种子生成相同或持续相关的图像具有挑战性。由于模型本身的随机性引入，生成的图像在颜色、形状或风格上可能存在显著差异。
- en: One might consider an image-to-image pipeline or a ControlNet approach, where
    a video clip is segmented into individual images, and each image is processed
    sequentially. However, maintaining consistency across the entire sequence, especially
    when applying significant changes (such as transforming a realistic video into
    a cartoon), remains a challenge. Even with pose alignment, the output video may
    still exhibit noticeable flickering.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 一个人可能会考虑图像到图像的管道或ControlNet方法，其中视频片段被分割成单个图像，每个图像依次处理。然而，在整个序列中保持一致性，尤其是在应用重大变化（如将真实视频转换为卡通）时，仍然是一个挑战。即使有姿态对齐，输出视频仍可能表现出明显的闪烁。
- en: 'A breakthrough came with the publication of *AnimateDiff: Animating Your Personalized
    Text-to-Image Diffusion Models without Specific Tuning* [1] by Yuwei Gao and colleagues.
    This work paved the way for generating consistent images from text, thereby enabling
    the creation of short videos.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '随着Yuwei Gao及其同事发表的 *AnimateDiff: Animating Your Personalized Text-to-Image
    Diffusion Models without Specific Tuning* [1] 的出版，取得了突破。这项工作为从文本生成一致图像铺平了道路，从而使得创建短视频成为可能。'
- en: 'In this chapter, we will explore the following:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将探讨以下内容：
- en: The principles of text-to-video generation
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本到视频生成的原理
- en: Practical applications of AnimateDiff
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AnimateDiff 的实际应用
- en: Utilizing Motion LoRA to control animation motion
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用 Motion LoRA 控制动画运动
- en: By the end of this chapter, you will understand the theoretical aspects of video
    generation, the inner workings of AnimateDiff, and why this methodology is effective
    in creating consistent and coherent images. With the provided sample code, you
    will be able to generate a 16-frame video. You can then apply Motion LoRA to manipulate
    the animation’s motion.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将理解视频生成的理论方面、AnimateDiff 的内部工作原理以及为什么这种方法在创建一致和连贯的图像方面是有效的。通过提供的示例代码，你将能够生成一个
    16 帧的视频。然后你可以应用 Motion LoRA 来操纵动画的运动。
- en: Please note that the results of this chapter cannot be fully appreciated in
    a static format like paper or PDF. For the best experience, we encourage you to
    engage with the associated sample code, run it, and observe the generated video.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，本章的结果无法在静态格式如论文或PDF中完全欣赏。为了获得最佳体验，我们鼓励你参与相关的示例代码，运行它，并观察生成的视频。
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In this chapter, we will employ `AnimateDiffPipeline`, available in the `Diffusers`
    library, to generate videos. You won’t need to install any extra tools or packages,
    as Diffusers (after version 0.23.0) offers all the required components and classes.
    Throughout the chapter, I will guide you through the usage of these features.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们将使用 `Diffusers` 库中可用的 `AnimateDiffPipeline` 生成视频。你不需要安装任何额外的工具或包，因为 Diffusers（从版本
    0.23.0 开始）提供了所有必需的组件和类。在整个章节中，我将指导你使用这些功能。
- en: 'To export the result in MP4 video format, you will also need to install the
    `opencv-python` package:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 要以 MP4 视频格式导出结果，你还需要安装 `opencv-python` 包：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Also, note that the `AnimateDiffPipeline` will require at least 8 GB of VRAM
    to generate a 16-frame 256x256 video clip.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请注意，`AnimateDiffPipeline` 生成一个 16 帧 256x256 的视频片段至少需要 8 GB 的 VRAM。
- en: The principles of text-to-video generation
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本到视频生成的原理
- en: The Stable Diffusion UNet, while effective for generating single images, falls
    short when it comes to generating consistent images due to its lack of contextual
    awareness. Researchers have proposed solutions to overcome this limitation, such
    as incorporating temporal information from the preceding one or two frames. However,
    this approach still fails to ensure pixel-level consistency, leading to noticeable
    differences between consecutive images and flickering in the generated video.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Stable Diffusion UNet虽然有效于生成单个图像，但由于其缺乏上下文意识，在生成一致图像方面存在不足。研究人员已经提出了克服这一局限性的解决方案，例如从前一或两个帧中结合时间信息。然而，这种方法仍然无法确保像素级一致性，导致连续图像之间存在明显差异，并在生成的视频中产生闪烁。
- en: 'To address this inconsistency problem, the authors of AnimateDiff trained a
    separated motion model – a zero-initialized convolution side model – similar to
    the ControlNet model. Further, rather than controlling an image, the motion model
    is applied to a series of continuous frames, as shown in *Figure 14**.1*:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个不一致性问题，AnimateDiff的作者训练了一个分离的运动模型——一个类似于ControlNet模型的零初始化卷积侧模型。进一步地，而不是控制一个图像，运动模型被应用于一系列连续的帧，如图*图14.1*所示：
- en: '![Figure 14.1: The architecture of AnimatedDiff](img/B21263_14_01.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图14.1：AnimatedDiff的架构](img/B21263_14_01.jpg)'
- en: 'Figure 14.1: The architecture of AnimatedDiff'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.1：AnimatedDiff的架构
- en: The process involves training a motion modeling module on video datasets to
    extract motion priors while keeping the base Stable Diffusion model frozen. Motion
    priors are the prior knowledge about motion in order to guide the generation or
    customization of videos. During the training stage, a **motion module** (also
    called **Motion UNet**) is added to the Stable Diffusion UNet. Similar to normal
    Stable Diffusion V1.5 UNet training, this Motion UNet will work on all frames
    simultaneously. We can treat them as images in one batch from the same video clip.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程涉及在视频数据集上训练运动建模模块以提取运动先验，同时保持基础Stable Diffusion模型冻结。运动先验是关于运动的知识，以便指导视频的生成或定制。在训练阶段，一个**运动模块**（也称为**运动UNet**）被添加到Stable
    Diffusion UNet中。与正常的Stable Diffusion V1.5 UNet训练类似，这个运动UNet将同时处理所有帧。我们可以将它们视为来自同一视频片段的一批图像。
- en: For instance, if we feed in a video with 16 frames, the motion module with attention
    headers will be trained to consider all 16 frames. If we look into the implementation
    source code, `TransformerTemporalModel` [4] is the core component of `MotionModules`
    [3].
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们输入一个包含16帧的视频，带有注意力头的运动模块将被训练以考虑所有16帧。如果我们查看实现源代码，`TransformerTemporalModel`
    [4]是`MotionModules` [3]的核心组件。
- en: During the inference, the time when we want to generate videos, the motion module
    will be loaded and its weights will be merged into Stable Diffusion UNet. When
    we want to generate a video with 16 frames, the pipeline will first initialize
    16 random latents with Gaussian noise – 𝒩(0,1). Without the motion module, the
    Stable Diffusion UNet will remove noise and generate 16 independent images. However,
    with the help of the motion module with the Transformer attention header built
    inside, the motion UNet attempts to create 16 correlated frames. You may ask,
    why are the images correlated? That is because the frames in the training video
    are correlated. After the denoising stage, the decoder D from the VAE will convert
    the 16 latents into pixel images.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理过程中，当我们想要生成视频时，将加载运动模块并将其权重合并到Stable Diffusion UNet中。当我们想要生成一个包含16帧的视频时，管道将首先使用高斯噪声——𝒩(0,1)初始化16个随机潜在值。如果没有运动模块，Stable
    Diffusion UNet将去除噪声并生成16个独立的图像。然而，借助内置的Transformer注意力头的运动模块的帮助，运动UNet试图创建16个相关的帧。你可能想知道，为什么这些图像是相关的？那是因为训练视频中的帧是相关的。在去噪阶段之后，VAE中的解码器D将把16个潜在值转换为像素图像。
- en: The Motion UNet is responsible for introducing correlations between successive
    frames in the generated video. It is similar to the correlation of different areas
    in one image. This is because the attention header pays attention to different
    parts of the image, and the model learned this knowledge during the training stage.
    Likewise, during the video generation, the model learned the correlation between
    frames during the training stage.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 运动UNet负责在生成的视频中引入连续帧之间的相关性。它类似于一个图像中不同区域的相关性。这是因为注意力头关注图像的不同部分，并且在训练阶段学习了这些知识。同样，在视频生成过程中，模型在训练阶段学习了帧之间的相关性。
- en: At its core, this approach involves designing an attention mechanism that operates
    on a sequence of continuous images. By learning the relationships between frames,
    AnimateDiff can generate more consistent and coherent images from text. Furthermore,
    since the base Stable Diffusion model remains locked, various Stable Diffusion
    extension techniques, such as LoRA, textual embedding, ControlNet, and image-to-image
    generation, can be applied to AnimateDiff as well.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在核心上，这种方法涉及设计一个在连续图像序列上操作的注意力机制。通过学习帧之间的关系，AnimateDiff可以从文本生成更一致和连贯的图像。此外，由于基础Stable
    Diffusion模型保持锁定状态，各种Stable Diffusion扩展技术，如LoRA、文本嵌入、ControlNet和图像到图像生成，也可以应用于AnimateDiff。
- en: Anything that works for standard Stable Diffusion, in theory, should also work
    for AnimateDiff to AnimateDiff as well!
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，任何适用于标准Stable Diffusion的东西，也应该适用于AnimateDiff到AnimateDiff。
- en: Before moving on to the next section, be aware that the AnimateDiff model requires
    a minimum of 12 GB of VRAM to generate a 16-frame, 256x256 video clip. To truly
    grasp the concept, writing code to utilize AnimateDiff is highly recommended.
    Now, let’s proceed to generate a short video (in GIF and MP4 format) using AnimateDiff.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入下一节之前，请注意，AnimateDiff模型生成一个16帧、256x256的视频片段至少需要12 GB的VRAM。为了真正理解这个概念，编写代码来利用AnimateDiff是非常推荐的。现在，让我们使用AnimateDiff生成一个短视频（GIF和MP4格式）。
- en: Practical applications of AnimateDiff
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AnimateDiff的实际应用
- en: The original AnimateDiff code and model were released as a standalone GitHub
    repository [2]. While the author provided sample code and Google Colab to demonstrate
    the results, users still needed to manually pull the code and download the model
    file to use it, being cautious about package versions.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的AnimateDiff代码和模型作为一个独立的GitHub仓库[2]发布。虽然作者提供了示例代码和Google Colab来展示结果，但用户仍然需要手动拉取代码并下载模型文件来使用，同时要小心包的版本。
- en: 'In November 2023, Dhruv Nair [9] merged an AnimateDiff Pipeline for Diffusers,
    allowing users to generate video clips using the AnimateDiff pertained model without
    leaving the `Diffusers` package. Here’s how to use the AnimatedDiff pipeline from
    Diffusers:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 到2023年11月，Dhruv Nair [9] 将AnimateDiff Pipeline合并到Diffusers中，使用户能够在不离开`Diffusers`包的情况下，使用AnimateDiff持久化模型生成视频片段。以下是使用Diffusers中的AnimatedDiff管道的方法：
- en: 'Install this specific version of Diffusers with the integrated AnimateDiff
    code:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用集成的AnimateDiff代码安装这个特定的Diffusers版本：
- en: '[PRE1]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: At the time of writing this chapter, the version of Diffusers with the latest
    AnimateDiff code is 0.23.0\. By specifying this version number, you can ensure
    that the sample code runs smoothly and error-free, as it was tested against this
    particular version.
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在撰写本章时，包含最新AnimateDiff代码的Diffusers版本是0.23.0。通过指定这个版本号，您可以确保示例代码能够顺利且无错误地运行，因为它已经针对这个特定版本进行了测试。
- en: 'You can also try installing the latest version of Diffusers, as it may have
    added more features to the pipeline by the time you read this:'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您还可以尝试安装Diffusers的最新版本，因为到您阅读这篇文档的时候，它可能已经增加了更多功能：
- en: '[PRE2]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Load up the motion adapter. We will use the pre-trained motion adapter model
    from the author of the original paper:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载运动适配器。我们将使用原始论文作者的预训练运动适配器模型：
- en: '[PRE3]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Load up an AnimateDiff pipeline from a Stable Diffusion v1.5-based checkpoint
    model:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从基于Stable Diffusion v1.5的检查点模型加载一个AnimateDiff管道：
- en: '[PRE7]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Use a proper scheduler. The scheduler plays an important role in the process
    of generating coherent images. An comparative study conducted by the author of
    the paper shows different schedulers can lead to different results. Experimentation
    shows that the `EulerAncestralDiscreteScheduler` scheduler with the following
    setting can generate relatively good results:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用合适的调度器。调度器在生成连贯图像的过程中扮演着重要的角色。作者进行的一项比较研究表明，不同的调度器可以导致不同的结果。实验表明，具有以下设置的`EulerAncestralDiscreteScheduler`调度器可以生成相对较好的结果：
- en: '[PRE13]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: To optimize VRAM usage, you can employ two strategies. First, use `pipe.enable_vae_slicing()`
    to configure the VAE to decode one frame at a time, thereby reducing memory consumption.
    Additionally, utilize `pipe.enable_model_cpu_offload()` to offload idle sub-models
    to the CPU, further decreasing VRAM usage.
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了优化VRAM使用，您可以采用两种策略。首先，使用`pipe.enable_vae_slicing()`配置VAE一次解码一帧，从而减少内存消耗。此外，利用`pipe.enable_model_cpu_offload()`将空闲子模型卸载到CPU，进一步降低VRAM使用。
- en: 'Generate coherent images:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成连贯的图像：
- en: '[PRE24]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Now, you should be able to see a GIF file generated using the 16 frames produced
    by AnimateDiff. This GIF uses 16 256x256 images. You can apply the image super-resolution
    techniques introduced in [*Chapter 11*](B21263_11.xhtml#_idTextAnchor214) to upscale
    the image and create a 512x512 GIF. I will not duplicate the code in this chapter.
    It is highly recommended to leverage the skills learned in [*Chapter 11*](B21263_11.xhtml#_idTextAnchor214)
    to further enhance the quality of video generation.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你应该能够看到使用AnimateDiff生成的16帧产生的GIF文件。这个GIF使用了16张256x256的图像。你可以应用在[*第11章*](B21263_11.xhtml#_idTextAnchor214)中介绍的图像超分辨率技术来提升图像并创建一个512x512的GIF。我不会在本章中重复代码。强烈建议利用在[*第11章*](B21263_11.xhtml#_idTextAnchor214)中学到的技能来进一步提高视频生成的质量。
- en: Utilizing Motion LoRA to control animation motion
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用运动LoRA控制动画运动
- en: Besides the motion adapter model, the author of the paper also introduced Motion
    LoRA to control the motion style. Motion LoRA is the same LoRA adapter we introduced
    in [*Chapter 8*](B21263_08.xhtml#_idTextAnchor153). As mentioned before, the AnimateDiff
    pipeline supports all other community-shared LoRAs. You can find these Motion
    LoRAs on the author’s Hugging Face repository [8].
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 除了运动适配器模型之外，论文的作者还介绍了运动LoRA来控制运动风格。运动LoRA与我们[*第8章*](B21263_08.xhtml#_idTextAnchor153)中介绍的相同的LoRA适配器。如前所述，AnimateDiff管道支持所有其他社区共享的LoRA。你可以在作者的Hugging
    Face仓库 [8] 中找到这些运动LoRA。
- en: These Motion LoRAs can be used to control the camera view. Here, we will use
    zoom-in LoRA – `guoyww/animatediff-motion-lora-zoom-in` – as an example. The zoom-in
    will guide the model to generate a video with zoom-in motion.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这些运动LoRA可以用来控制摄像机视角。在这里，我们将使用放大LoRA – `guoyww/animatediff-motion-lora-zoom-in`
    – 作为示例。放大将引导模型生成带有放大运动的视频。
- en: 'The usage is simply one additional line of code:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 使用方法简单，只需额外一行代码：
- en: '[PRE43]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Here is the complete generation code. We are mostly reusing the code from the
    previous section:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是完整的生成代码。我们主要重用了上一节中的代码：
- en: '[PRE44]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: You should see a zoom-in GIF clip is generated under the same folder, named
    `animation_origin_256_w_lora_zoom_in.gif` and an MP4 video clip is generated named
    `animation_origin_256_w_lora_zoom_in.mp4`.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该在同一文件夹下看到一个名为`animation_origin_256_w_lora_zoom_in.gif`的放大GIF剪辑和一个名为`animation_origin_256_w_lora_zoom_in.mp4`的MP4视频剪辑被生成。
- en: Summary
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Every day, the quality and duration of text-to-video samples circulating on
    social networks are improving. It’s likely that by the time you read this chapter,
    the function of the technologies metioned in this chapter will have surpassed
    what was described here. However, one constant is the concept of training a model
    to apply an attention mechanism to a sequence of images.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 每天在社交网络上流传的文本到视频样本的质量和持续时间都在不断提高。很可能在你阅读这一章的时候，本章中提到的技术的功能已经超过了这里所描述的内容。然而，一个不变的是训练模型将注意力机制应用于一系列图像的概念。
- en: At the time of writing, OpenAI’s Sora [9] has just been released. This technology
    can generate high-quality videos based on the Transformer Diffusion architecture.
    This is a similar methodology to that used in AnimatedDiff, which combines the
    Transformer and diffusion models.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，OpenAI的Sora [9]刚刚发布。这项技术可以根据Transformer Diffusion架构生成高质量的视频。这与AnimatedDiff中使用的方法类似，它结合了Transformer和扩散模型。
- en: What sets AnimatedDiff apart is its openness and adaptability. It can be applied
    to any community model with the same base checkpoint version, a feature not currently
    offered by any other solution. Furthermore, the authors of the paper have completely
    open-sourced the code and model.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: AnimatedDiff与众不同的地方在于其开放性和适应性。它可以应用于任何具有相同基础检查点版本的社区模型，这是目前任何其他解决方案都没有提供的功能。此外，论文的作者已经完全开源了代码和模型。
- en: This chapter primarily discussed the challenges of text-to-image generation,
    then introduced AnimatedDiff, explaining how and why it works. We also provided
    a sample code to use the AnimatedDiff pipeline from the Diffusers package to generate
    a GIF clip from 16 coherent images on your own GPU.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 本章主要讨论了文本到图像生成的挑战，然后介绍了AnimatedDiff，解释了它是如何以及为什么能工作的。我们还提供了一个示例代码，展示如何使用Diffusers包中的AnimatedDiff管道在自己的GPU上从16张连贯的图像生成GIF剪辑。
- en: In the next chapter, we will explore the solutions for generating text descriptions
    from an image.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨从图像生成文本描述的解决方案。
- en: References
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai,
    *AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without
    Specific* *Tuning*: [https://arxiv.org/abs/2307.04725](https://arxiv.org/abs/2307.04725)'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 郭宇伟，杨策源，饶安毅，王耀辉，乔宇，林大华，戴博，*AnimateDiff：无需特定* *调整* *的个性化文本到图像扩散模型*：[https://arxiv.org/abs/2307.04725](https://arxiv.org/abs/2307.04725)
- en: 'Original AnimateDiff code repository: [https://github.com/guoyww/AnimateDiff](https://github.com/guoyww/AnimateDiff)'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 原始 AnimateDiff 代码仓库：[https://github.com/guoyww/AnimateDiff](https://github.com/guoyww/AnimateDiff)
- en: 'Diffusers Motion modules implementation: [https://github.com/huggingface/diffusers/blob/3dd4168d4c96c429d2b74c2baaee0678c57578da/src/diffusers/models/unets/unet_motion_model.py#L50](https://github.com/huggingface/diffusers/blob/3dd4168d4c96c429d2b74c2baaee0678c57578da/src/diffusers/models/unets/unet_motion_model.py#L50'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Diffusers 动作模块实现：[https://github.com/huggingface/diffusers/blob/3dd4168d4c96c429d2b74c2baaee0678c57578da/src/diffusers/models/unets/unet_motion_model.py#L50](https://github.com/huggingface/diffusers/blob/3dd4168d4c96c429d2b74c2baaee0678c57578da/src/diffusers/models/unets/unet_motion_model.py#L50)
- en: )
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: 'Hugging Face Diffusers TransformerTemporalModel implementation: [https://github.com/huggingface/diffusers/blob/3dd4168d4c96c429d2b74c2baaee0678c57578da/src/diffusers/models/transformers/transformer_temporal.py#L41](https://github.com/huggingface/diffusers/blob/3dd4168d4c96c429d2b74c2baaee0678c57578da/src/diffusers/models/transformers/transformer_temporal.py#L41)'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hugging Face Diffusers TransformerTemporalModel 实现：[https://github.com/huggingface/diffusers/blob/3dd4168d4c96c429d2b74c2baaee0678c57578da/src/diffusers/models/transformers/transformer_temporal.py#L41](https://github.com/huggingface/diffusers/blob/3dd4168d4c96c429d2b74c2baaee0678c57578da/src/diffusers/models/transformers/transformer_temporal.py#L41)
- en: '[4] Dhruv Nair, [https://github.com/DN6](https://github.com/DN6)'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4] Dhruv Nair，[https://github.com/DN6](https://github.com/DN6)'
- en: 'AnimateDiff proposal pull request: [https://github.com/huggingface/diffusers/pull/5413](https://github.com/huggingface/diffusers/pull/5413)'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: AnimateDiff 提案拉取请求：[https://github.com/huggingface/diffusers/pull/5413](https://github.com/huggingface/diffusers/pull/5413)
- en: 'animatediff-motion-adapter-v1-5-2: [https://huggingface.co/guoyww/animatediff-motion-adapter-v1-5-2](https://huggingface.co/guoyww/animatediff-motion-adapter-v1-5-2)'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: animatediff-motion-adapter-v1-5-2：[https://huggingface.co/guoyww/animatediff-motion-adapter-v1-5-2](https://huggingface.co/guoyww/animatediff-motion-adapter-v1-5-2)
- en: 'Yuwei Guo’s Hugging Face repository: [https://huggingface.co/guoyww](https://huggingface.co/guoyww)'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 郭宇伟的 Hugging Face 仓库：[https://huggingface.co/guoyww](https://huggingface.co/guoyww)
- en: 'Video generation models as world simulators: [https://openai.com/research/video-generation-models-as-world-simulators](https://openai.com/research/video-generation-models-as-world-simulators)'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 视频生成模型作为世界模拟器：[https://openai.com/research/video-generation-models-as-world-simulators](https://openai.com/research/video-generation-models-as-world-simulators)
