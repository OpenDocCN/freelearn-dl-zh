- en: Chapter 4. Self-Organizing Maps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we present a neural network architecture that is suitable
    for unsupervised learning: self-organizing maps, also known as Kohonen networks.
    This particular type of neural network is able to categorize records of data without
    any target output or find a representation of the data in a smaller dimension.
    Throughout this chapter, we are going to explore how this is achieved, as well
    as examples to attest to its capacity. The subtopics of this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks unsupervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Competitive learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kohonen self-organizing maps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One-dimensional SOMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two-dimensional SOMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Problems solved with unsupervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Java implementation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data visualization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Practical problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural networks unsupervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 2](ch02.xhtml "Chapter 2. Getting Neural Networks to Learn"), *Getting
    Neural Networks to Learn* we've been acquainted with unsupervised learning, and
    now we are going to explore the features of this learning paradigm in more detail.
    The mission of unsupervised learning algorithms is to find patterns in datasets,
    where the parameters (weights in the case of neural networks) are adjusted without
    any error measure (there are no target values).
  prefs: []
  type: TYPE_NORMAL
- en: While the supervised algorithms provide an output comparable to the dataset
    that was presented, the unsupervised algorithms do not need to know the output
    values. The fundamentals of unsupervised learning are inspired by the fact that,
    in neurology, similar stimuli produce similar responses. So applying this to artificial
    neural networks, we can say that similar data produces similar outputs, so those
    outputs can be grouped or clustered.
  prefs: []
  type: TYPE_NORMAL
- en: Although this learning may be used in other mathematical fields, such as statistics,
    its core functionality is intended and designed for machine learning problems
    such as data mining, pattern recognition, and so on. Neural networks are a subfield
    in the machine learning discipline, and provided that their structure allows iterative
    learning, they serve as a good framework to apply this concept to.
  prefs: []
  type: TYPE_NORMAL
- en: Most of unsupervised learning applications are aimed at clustering tasks, which
    means that similar data points are to be clustered together, while different data
    points form different clusters. Also, one application that unsupervised learning
    is suitable for is dimensionality reduction or data compression, as long as simpler
    and smaller representations of the data can be found among huge datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unsupervised algorithms are not unique to neural networks, as K-means, expectation
    maximization, and methods of moments are also examples of unsupervised learning
    algorithms. One common feature of all learning algorithms is the absence of mapping
    among variables in the current dataset; instead, one wishes to find a different
    meaning of this data, and that's the goal of any unsupervised learning algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: While in supervised learning algorithms, we usually have a smaller number of
    outputs, for unsupervised learning, there is a need to produce an abstract data
    representation that may require a high number of outputs, but, except for classification
    tasks, their meaning is totally different than the one presented in the supervised
    learning. Usually, each output neuron is responsible for representing a feature
    or a class present in the input data. In most architectures, not all output neurons
    need to be activated at a time; only a restricted set of output neurons may fire,
    meaning that that neuron is able to better represent most of the information being
    fed at the neural input.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One advantage of unsupervised learning over supervised learning is that less
    computational power required by the first for the learning of huge datasets. Time
    consumption grows linearly while for the supervised learning it grows exponentially.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to explore two unsupervised learning algorithms:
    competitive learning and Kohonen self-organizing maps.'
  prefs: []
  type: TYPE_NORMAL
- en: Competitive learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As the name implies, competitive learning handles a competition between the
    output neurons to determine which one is the winner. In competitive learning,
    the winning neuron is usually determined by comparing the weights against the
    inputs (they have the same dimensionality). To facilitate understanding, suppose
    we want to train a single layer neural network with two inputs and four outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Competitive learning](img/B05964_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Every output neuron is then connected to these two inputs, hence for each neuron
    there are two weights.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For this learning, the bias is dropped from the neurons, so the neurons will
    process only the weighted inputs.
  prefs: []
  type: TYPE_NORMAL
- en: The competition starts after the data has been processed by the neurons. The
    winner neuron will be the one whose weights are *near* to the input values. One
    additional difference compared to the supervised learning algorithm is that only
    the winner neuron may update their weights, while the other ones remain unchanged.
    This is the so-called **winner-takes-all** rule. This intention to bring the neuron
    *nearer* to the input that caused it to win the competition.
  prefs: []
  type: TYPE_NORMAL
- en: 'Considering that every input neuron *i* is connected to all output neurons
    *j* through a weight *wij*, in our case, we would have a set of weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Competitive learning](img/B05964_04_01_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Provided that the weights of every neuron have the same dimensionality of the
    input data, let''s consider all the input data points together in a plot with
    the weights of each neuron:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Competitive learning](img/B05964_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In this chart, let''s consider the circles as the data points and the squares
    as the neuron weights. We can see that some data points are closer to certain
    weights, while others are farther but nearer to others. The neural network performs
    computations on the distance on the inputs and the weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Competitive learning](img/B05964_04_02_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The result of this equation will determine how much *stronger* a neuron is
    against its competitors. The neuron whose weight distance to the input is the
    smaller is considered the winner. After many iterations, the weights are driven
    near enough to the data points that give more cause the corresponding neuron to
    win that the changes are either too small or the weights fall in a zig-zag setting.
    Finally, when the network is already trained, the chart takes another shape:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Competitive learning](img/B05964_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As can be seen, the neurons form centroids surrounding the points capable of
    making the corresponding neuron stronger than its competitors.
  prefs: []
  type: TYPE_NORMAL
- en: In an unsupervised neural network, the number of outputs is completely arbitrary.
    Sometimes only some neurons are able to change their weights, while in other cases,
    all the neurons may respond differently to the same input, causing the neural
    network to never learn. In these cases, it is recommended either to review the
    number of output neurons, or consider another type of unsupervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Two stopping conditions are preferable in competitive learning:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Predefined number of epochs: This prevents our algorithm from running for a
    longer time without convergence'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Minimum value of weight update: Prevents the algorithm from running longer
    than necessary'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Competitive layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This type of neural layer is particular, as the outputs won''t be necessarily
    the same as its neuron''s outputs. Only one neuron will be fired at a time, thereby
    requiring a special rule to calculate the outputs. So, let''s create a new class
    called `CompetitiveLayer` that will inherit from `OutputLayer` and starting with
    two new attributes: `winnerNeuron` and `winnerIndex`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This new class of neural layer will override the method `calc()` and add some
    new particular methods to get the weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In the next sections, we will define the class Kohonen for the Kohonen neural
    network. In this class, there will be an `enum` called `distanceCalculation`,
    which will have the different methods to calculate distance. In this chapter (and
    book), we'll stick to the Euclidian distance.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A new class called `ArrayOperations` was created to provide methods that facilitate
    operations with arrays. Functionalities such as getting the index of the maximum
    or minimum or getting a subset of the array are implemented in this class.
  prefs: []
  type: TYPE_NORMAL
- en: 'The distance between the weights of a particular neuron and the input is calculated
    by the method `getWeightDistance( )`, which is called inside the `calc` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The method `getNeuronWeights( )` returns the weights of the neuron corresponding
    to the index passed in the array. Since it is simple and to save space here, we
    invite the reader to see the code to check its implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Kohonen self-organizing maps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This network architecture was created by the Finnish professor Teuvo Kohonen
    at the beginning of the 80s. It consists of one single layer neural network capable
    of providing a *visualization* of the data in one or two dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: In this book, we are going to use Kohonen networks also as a basic competitive
    layer with no links between the neurons. In this case, we are going to consider
    it as zero dimension (0-D).
  prefs: []
  type: TYPE_NORMAL
- en: Theoretically, a Kohonen Network would be able to provide a 3-D (or even in
    more dimensions) representation of the data; however, in printed material such
    as this book, it is not practicable to show 3-D charts without overlapping some
    data. Thus in this book, we are going to deal only with 0-D, 1-D, and 2-D Kohonen
    networks.
  prefs: []
  type: TYPE_NORMAL
- en: Kohonen **Self-Organizing Maps** (**SOMs**), in addition to the traditional
    single layer competitive neural networks (in this book, the 0-D Kohonen network),
    add the concept of neighborhood neurons. A dimensional SOM takes into account
    the index of the neurons in the competitive layer, letting the neighborhood of
    neurons play a relevant role during the learning phase.
  prefs: []
  type: TYPE_NORMAL
- en: 'An SOM has two modes of functioning: mapping and learning. In the mapping mode,
    the input data is classified in the most appropriate neuron, while in the learning
    mode, the input data helps the learning algorithm to build the *map*. This map
    can be interpreted as a lower-dimension representation from a certain dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Extending the neural network code to Kohonen
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In our code, let''s create a new class inherited from `NeuralNet`, since it
    will be a particular type of neural network. This class will be called Kohonen,
    which will use the class `CompetitiveLayer` as the output layer. The following
    class diagram shows how these new classes are arranged:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Extending the neural network code to Kohonen](img/B05964_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Three types of SOMs are covered in this chapter: zero-, one- and two-dimensional.
    These configurations are defined in an `enum MapDimension`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The Kohonen constructor defines the dimension of the Kohonen neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Zero-dimensional SOM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is the pure competitive layer, where the order of the neurons is irrelevant.
    Features such as neighborhood functions are not taken into account. Only the winner
    neuron weights are affected during the learning phase. The map will be composed
    only of unconnected dots.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet define a zero-dimensional SOM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Note the value `0` passed in the argument dim (the last of the constructor).
  prefs: []
  type: TYPE_NORMAL
- en: One-dimensional SOM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This architecture is similar to the network presented in the last section,
    **Competitive learning**, with the addition of neighborhood amongst the output
    neurons:'
  prefs: []
  type: TYPE_NORMAL
- en: '![One-dimensional SOM](img/B05964_04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note that every neuron on the output layer has one or two neighbors. Similarly,
    the neuron that fires the greatest value updates its weights, but in a SOM, the
    neighbor neurons also update their weights in a smaller rate.
  prefs: []
  type: TYPE_NORMAL
- en: The effect of the neighborhood extends the activation area to a wider area of
    the map, provided that all the output neurons must observe an organization, or
    a path in the one-dimensional case. The neighborhood function also allows for
    a better exploration of the properties of the input space, since it forces the
    neural network to keep the connections between neurons, therefore resulting in
    more information in addition to the clusters that are formed.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a plot of the input data points with the neural weights, we can see the
    path formed by the neurons:'
  prefs: []
  type: TYPE_NORMAL
- en: '![One-dimensional SOM](img/B05964_04_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the chart here presented, for simplicity, we plotted only the output weights
    to demonstrate how the map is designed in a (in this case) 2-D space. After training
    over many iterations, the neural network converges to a final shape that represent
    all data points. Provided that structure, a certain set of data may cause the
    Kohonen Network to design another shape in the space. This is a good example of
    dimensionality reduction, since a multidimensional dataset when presented to the
    Self-Organizing Map is able to produce one single line (in the 1-D SOM) that summarizes
    the entire dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'To define a one-dimensional SOM, we need to pass the value `1` as the argument
    `dim`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Two-dimensional SOM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is the most used architecture to demonstrate the Kohonen neural network
    power in a visual way. The output layer is one matrix containing M x N neurons,
    interconnected like a grid:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Two-dimensional SOM](img/B05964_04_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the 2-D SOMs, every neuron now has up to four neighbors (in the square configuration),
    although in some representations, the diagonal neurons may also be considered,
    thus resulting in up to eight neighbors. Hexagonal representations are also useful.
    Let''s see one example of what a 3x3 SOM plot looks like in a 2-D chart (considering
    two input variables):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Two-dimensional SOM](img/B05964_04_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'At first, the untrained Kohonen Network shows a very strange and screwed-up
    shape. The shaping of the weights will depend solely on the input data that is
    going to be fed to the SOM. Let''s see an example of how the map starts to organize
    itself:'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we have the dense data set shown in the following plot:![Two-dimensional
    SOM](img/B05964_04_09.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying SOM, the 2-D shape gradually changes, until it achieves the final configuration:![Two-dimensional
    SOM](img/B05964_04_10.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The final shape of a 2-D SOM may not always be a perfect square; instead, it
    will resemble a shape that could be drawn from the dataset. The neighborhood function
    is one important component in the learning process because it approximates the
    neighbor neurons in the plot, and the structure moves to a configuration that
    is more *organized*.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The grid on a chart is just the more used and didactic. There are other ways
    of showing the SOM diagram, such as the U-matrix and the cluster boundaries.
  prefs: []
  type: TYPE_NORMAL
- en: 2D competitive layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to better represent the neurons of a 2D competitive layer in a grid
    form, we''re creating the CompetitiveLayer2D class, which inherits from `CompetitiveLayer`.
    In this class, we can define the number of neurons in the form of a grid of M
    x N neurons:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The coordinate system in the 2D competitive layer is analogous to the Cartesian.
    Every neuron is assigned a position in the grid, with indexes starting from `0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![2D competitive layer](img/B05964_04_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the illustration above, 12 neurons are arranged in a 3 x 4 grid. Another
    feature added in this class is the indexing of neurons by the position in the
    grid. This allows us to get subsets of neurons (and weights), one entire specific
    row or column of the grid, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: SOM learning algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A self-organizing map aims at classifying the input data by clustering those
    data points that trigger the same response on the output. Initially, the untrained
    network will produce random outputs, but as more examples are presented, the neural
    network identifies which neurons are activated more often and then their *position*
    in the SOM output space is changed. This algorithm is based on competitive learning,
    which means a winner neuron (also known as best matching unit, or BMU) will update
    its weights and its neighbor weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following flowchart illustrates the learning process of a SOM Network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![SOM learning algorithm](img/B05964_04_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The learning resembles a bit those algorithms addressed in [Chapter 2](ch02.xhtml
    "Chapter 2. Getting Neural Networks to Learn"), *Getting Neural Networks to Learn*
    and [Chapter 3](ch03.xhtml "Chapter 3. Perceptrons and Supervised Learning"),
    *Perceptrons and Supervised Learning*. Three major differences are the determination
    of the BMU with the distance, the weight update rule, and the absence of an error
    measure. The distance implies that nearer points should produce similar outputs,
    thus here the criterion to determine the lowest BMU is the neuron which presents
    a lower distance to some data point. This Euclidean distance is usually used,
    and in this book we will apply it for simplicity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![SOM learning algorithm](img/B05964_04_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The weight-to-input distance is calculated by the method `getWeightDistance(
    )` of the `CompetitiveLayer` class for a specific neuron i (argument neuron).
    This method was described above.
  prefs: []
  type: TYPE_NORMAL
- en: Effect of neighboring neurons – the neighborhood function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The weight update rule uses a neighborhood function *Θ(u,v,s,t)* which states
    how much a neighbor neuron u (BMU unit) is close to a neuron *v*. Remember that
    in a dimensional SOM, the BMU neuron is updated together with its neighbor neurons.
    This update is also dependent on a neighborhood radius, which takes into account
    the number of epoch''s s and a reference epoch *t*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Effect of neighboring neurons – the neighborhood function](img/B05964_04_12_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *du,v* is the neuron distance between neurons u and v in the grid. The
    radius is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Effect of neighboring neurons – the neighborhood function](img/B05964_04_12_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, is the initial radius. The effect of the number of epochs (s) and the
    reference epoch (*t*) is the decreasing of the neighborhood radius and thereby
    the effect of neighborhood. This is useful because in the beginning of the training,
    the weights need to be updated more often, because they are usually randomly initialized.
    As the training process continues, the updates need to be weaker, otherwise the
    neural network will continue to change its weights forever and will never converge.
  prefs: []
  type: TYPE_NORMAL
- en: '![Effect of neighboring neurons – the neighborhood function](img/B05964_04_12_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The neighborhood function and the neuron distance are implemented in the `CompetitiveLayer`
    class, with overridden versions for the `CompetitiveLayer2D` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '| CompetitiveLayer | CompetitiveLayer2D |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'The neighborhood radius function is the same for both classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The learning rate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The learning rate also becomes weaker as the training goes on:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The learning rate](img/B05964_04_12_05.jpg)![The learning rate](img/B05964_04_12_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The parameter is the initial learning rate. Finally, considering the neighborhood
    function and the learning rate, the weight update rule is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The learning rate](img/B05964_04_12_07.jpg)![The learning rate](img/B05964_04_12_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *X* *[k]* is the *kth* input, and *W* *[kj]* is the weight connecting
    the *kth* input to the *jth* output.
  prefs: []
  type: TYPE_NORMAL
- en: A new class for competitive learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have a competitive layer, a Kohonen neural network, and defined
    the methods for neighboring functions, let''s create a new class for competitive
    learning. This class will inherit from `LearningAlgorithm` and will receive Kohonen
    objects for learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A new class for competitive learning](img/B05964_04_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As seen in [Chapter 2](ch02.xhtml "Chapter 2. Getting Neural Networks to Learn"),
    *Getting Neural Networks to Learn* a `LearningAlgorithm` object receives a neural
    dataset for training. This property is inherited by the `CompetitiveLearning`
    object, which implements new methods and properties to realize the competitive
    learning procedure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The learning rate, as opposed to the previous algorithms, now changes over
    the training process, and it will be returned by the method `getLearningRate(
    )`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This method is used in the `calcWeightUpdate( )`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The `train( )` method is adapted as well for competitive learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The implementation for the method `appliedNewWeights( )` is analogous to the
    one presented in the previous chapter, with the exception that there is no bias
    and there is only one output layer.
  prefs: []
  type: TYPE_NORMAL
- en: '**Time to play**: SOM applications in action. Now it is time to get hands-on
    and implement the Kohonen neural network in Java. There are many applications
    of self-organizing maps, most of them being in the field of clustering, data abstraction,
    and dimensionality reduction. But the clustering applications are the most interesting
    because of the many possibilities one may apply them on. The real advantage of
    clustering is that there is no need to worry about input/output relationship,
    rather the problem solver may concentrate on the input data. One example of clustering
    application will be explored in [Chapter 7](ch07.xhtml "Chapter 7. Clustering
    Customer Profiles"), *Clustering Customer Profiles*.'
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the SOMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we are going to introduce the plotting feature. Charts can
    be drawn in Java by using the freely available package `JFreeChart` (which can
    be downloaded from [http://www.jfree.org/jfreechart/](http://www.jfree.org/jfreechart/)).
    This package is attached with this chapter''s source code. So, we designed a class
    called **Chart**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The methods implemented in this class are for plotting line and scatter plots.
    The main difference between them lies in the fact that line plots take all data
    series over one x-axis (usually the time axis) where each data series is a line;
    scatter plots, on the other hand, show dots in a 2D plane indicating its position
    in relation to each of the axis. Charts below show graphically the difference
    between them and the codes to generate them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualizing the SOMs](img/B05964_04_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![Visualizing the SOMs](img/B05964_04_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We're suppressing the codes for chart generation (methods `linePlot( )` and
    `scatterPlot( ));` however, in the file `Chart.java`, the reader can find their
    implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Plotting 2D training datasets and neuron weights
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have the methods for plotting charts, let''s plot the training
    dataset and neuron weights. Any 2D dataset can be plotted in the same way shown
    in the diagram of the last section. To plot the weights, we need to get the weights
    of the Kohonen neural network using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'In competitive learning, we can check visually how the weights *move* around
    the dataset space. So we''re going to add a method (`showPlot2DData( )`) to plot
    the dataset and the weights, a property (`plot2DData`) to hold the reference to
    the `ChartFrame`, and a flag (`show2DData`) to determine whether the plot is going
    to be shown for every epoch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'This method will be called from the `train` method at the end of each epoch.
    A property called **sleep** will determine for how many milliseconds the chart
    will be displayed until the next epoch''s chart replaces it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Testing Kohonen learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s now define a Kohonen network and see how it works. First, we''re creating
    a Kohonen with zero dimension:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'By running this code, we get the first plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing Kohonen learning](img/B05964_04_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As the training starts, the weights begin to distribute over the input data
    space, until finally it converges by being distributed uniformly along the input
    data space:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing Kohonen learning](img/B05964_04_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'For one dimension, let''s try something funkier. Let''s create the dataset
    over a cosine function with random noise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'By running the same previous code and changing the object to *kn1*, we get
    a line connecting all the weight points:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing Kohonen learning](img/B05964_04_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As the training continues, the lines tend to be organized along the data wave:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing Kohonen learning](img/B05964_04_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: See the file `Kohonen1DTest.java` if you want to change the initial learning
    rate, maximum number of epochs, and other parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, let's see the two-dimensional Kohonen chart. The code will be a little
    bit different, since now, instead of giving the number of neurons, we're going
    to inform the Kohonen constructor the dimensions of our neural grid.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset used here will be a circle with random noise added:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s construct the two-dimensional Kohonen:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we are using `GaussianInitialization` with mean 500.0 and standard
    deviation 20.0, that is, the weights will be generated at the position (500.0,500.0)
    while the data is centered around (50.0,50.0):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing Kohonen learning](img/B05964_04_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now let''s train the neural network. The neuron weights quickly move to the
    circle in the first epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing Kohonen learning](img/B05964_04_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'By the end of the training, the majority of the weights will be distributed
    over the circle, while in the center there will be an empty space, as the grid
    will be totally stretched out:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing Kohonen learning](img/B05964_04_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we've seen how to apply unsupervised learning algorithms on
    neural networks. We've been presented a new and suitable architecture for that
    end, the self-organizing maps of Kohonen. Unsupervised learning has proved to
    be as powerful as the supervised learning methods, because they concentrate only
    on the input data, without need to make input-output mappings. We've seen graphically
    how the training algorithms are able to drive the weights nearer to the input
    data, thereby playing a role in clustering and dimensionality reduction. In addition
    to these examples, Kohonen SOMs are also able to classify clusters of data, as
    each neuron will provide better responses for a particular set of inputs.
  prefs: []
  type: TYPE_NORMAL
