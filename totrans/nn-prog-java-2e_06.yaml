- en: Chapter 6. Classifying Disease Diagnosis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we have been working with supervised learning for predicting numerical
    values; however, in the real world, numbers are just part of the data addressed.
    Real variables also contain categorical values, which are not purely numerical,
    but describe important features that have influence on the problems neural networks
    are applied to solve. In this chapter, the reader will be presented with a very
    didactic but interesting application involving categorical values and classification:
    disease diagnosis. This chapter digs deeper into classification problems and how
    to represent categorical data, as well as showing how to design a classification
    algorithm using neural networks. The topics covered in this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Foundations of classification problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Categorical data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Confusion matrix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sensibility and specificity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural networks for classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disease diagnosis using neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diagnosis for cancer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diagnosis for diabetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Foundations of classification problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One thing neural networks are really good at is classifying records. The very
    simple perceptron network draws a decision boundary, defining whether a data point
    belongs to one region or another, whereas a region denotes a class. Let''s take
    a look visually on an *x-y* scatter chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Foundations of classification problems](img/B05964_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The dashed lines explicitly separate the points into classes. These points represent
    data records which originally had the corresponding class labels. That means their
    classes were already known, therefore this classification tasks falls in the supervised
    learning category.
  prefs: []
  type: TYPE_NORMAL
- en: 'A classification algorithm seeks to find the boundaries between the classes
    in the data hyperspace. Once the classification boundaries are defined, a new
    data point, with an unknown class, receives a class label according to the boundaries
    defined by the classification algorithm. The figure below shows how a new record
    is classified:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Foundations of classification problems](img/B05964_06_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Based on the current class configuration, the new record's class is the third
    class.
  prefs: []
  type: TYPE_NORMAL
- en: Categorical data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Applications usually lead with the types of data shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Categorical data](img/B05964_06_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Data can be numerical or categorical or, simply speaking, numbers or words.
    Numerical data is represented by a numeric value, from which it can be continuous
    or discrete. This data type has been used so far in this book''s applications.
    Categorical data is a wider class of data that includes words, letters, or even
    numbers, but with a quite different meaning. While numerical data can support
    arithmetic operations, categorical data is only descriptive and cannot be processed
    like numbers, even if the value is a number. An example is the severity degree
    of a disease in a scale (from zero to five, for example). Another property of
    categorical data is that a certain variable has a finite number of values; in
    other words, only a defined set of values can be assigned to a categorical variable.
    A subclass of data inside the categorical is ordinal data. This class is particular
    because the defined values can be sorted in a predefined order. An example is
    adjectives indicating the state or quality of something (bad, fair, good, excellent):'
  prefs: []
  type: TYPE_NORMAL
- en: '| Numerical | Categorical |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Only numbers | Numbers, words, letters, signs |'
  prefs: []
  type: TYPE_TB
- en: '| Can support arithmetic operations | Do not support arithmetic operations
    |'
  prefs: []
  type: TYPE_TB
- en: '| Infinite or undefined range of values | Finite or defined set of values |'
  prefs: []
  type: TYPE_TB
- en: '| Continuous | Discrete | Ordinal | Non-ordinal |'
  prefs: []
  type: TYPE_TB
- en: '| Real values | Integers, decimal | Can be ordered | Cannot be ordered |'
  prefs: []
  type: TYPE_TB
- en: '| Any possible value | Predefined intervals | Can be assigned numbers | Each
    possible value is a flag |'
  prefs: []
  type: TYPE_TB
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that here we are addressing structured data only. In the real world, most
    data is unstructured, including text and multimedia content. Although these types
    of data are also processed in learning from data applications, neural networks
    require them to be transformed into structured data types.
  prefs: []
  type: TYPE_NORMAL
- en: Working with categorical data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Structured data files, such as those used in CSV or Excel, usually contain
    columns of numerical and categorical data. In [Chapter 5](ch05.xhtml "Chapter 5. Forecasting
    Weather"), *Forecasting Weather* we have created the classes `LoadCsv` (for loading
    `csv` files) and DataSet (for storing data from csv), but these classes are prepared
    only for working with numerical data. The simplest way of representing categorical
    value is converting each possible value into a binary column, whereby if the given
    value is presented in the original column, the corresponding binary column will
    have a one as the converted value, otherwise it will be zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Working with categorical data](img/B05964_06_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Ordinal columns can assume the defined values as numerical in the same column;
    however, if the original values are letters or words, they need to be converted
    into numbers via a Java Dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: The strategy described above may be implemented by you as an exercise. Otherwise,
    you would have to handle this manually. In this case, depending on the number
    of data rows, it can be time-consuming.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ve covered that Neural Networks can work as data classifiers by establishing
    decision boundaries onto data in the hyperspace. This boundary can be linear,
    in the case of perceptrons, or nonlinear, in the case of other neural architectures
    such as MLPs, Kohonen, or Adaline. The linear case is based on linear regression,
    on which the classification boundary is a literally a line, as shown in the previous
    figure. If the scatter chart of the data looks like that of the following figure,
    then a nonlinear classification boundary is needed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Logistic regression](img/B05964_06_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Neural Networks are in fact a great nonlinear classifier, and this is achieved
    by the usage of nonlinear activation functions. One nonlinear function that actually
    works well for nonlinear classification is the sigmoid function, whereas the procedure
    for classification using this function is called logistic regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Logistic regression](img/B05964_06_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This function returns values bounded between zero and one. In this function
    α parameter denotes how hard the transition from zero and 1 occurs. The following
    chart shows the difference:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Logistic regression](img/B05964_06_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note that the higher the alpha parameter is, the more the logistic function
    takes a shape of a hard-limiting threshold function, also known as a step function.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple classes versus binary classes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Classification problems usually deal with a multiple class's case, where each
    class is assigned a label. However, a binary classification schema is useful to
    be applied in neural networks. This is because a neural network with a logistic
    function at the output layer can produce only values between `0` and `1`, meaning
    it belongs (1) or does not belong (0) to some class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Nevertheless, there is one approach for multiple classes using binary functions.
    Consider that every class is represented by an output neuron, and whenever that
    output neuron fires, that neuron''s corresponding class is applied on the input
    data record. So let''s suppose a network to classify diseases; each neuron output
    will represent a disease to be applied to some symptom:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Multiple classes versus binary classes](img/B05964_06_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that in that configuration, it would be possible to have multiple diseases
    with the same symptoms, which can happen. However, if only one class would be
    desirable to be chosen, then a schema as the competitive learning algorithm would
    suit more in that case.
  prefs: []
  type: TYPE_NORMAL
- en: Confusion matrix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is no perfect classifier algorithm; all of them are subject to errors
    and biases; however, it is expected that a classification algorithm can correctly
    classify 70-90% of the records.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Very high correct classification rates are not always desirable, because of
    possible biases presented in the input data that might affect the classification
    task, and also there is a risk of overtraining, when only the training data is
    correctly classified.
  prefs: []
  type: TYPE_NORMAL
- en: 'A confusion matrix shows how much of a given class''s records were correctly
    classified and thereby how much were wrongly classified. The following table depicts
    what a confusion matrix may look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Confusion matrix](img/B05964_06_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note that the main diagonal is expected to have the higher values, as the classification
    algorithm will always try to extract meaningful information from the input dataset.
    The sum of all rows must be equal to 100%, because all elements of a given class
    are to be classified in one of the available classes. Note that some classes may
    receive more classifications than expected.
  prefs: []
  type: TYPE_NORMAL
- en: The more a confusion matrix looks like an identity matrix, the better the classification
    algorithm will be.
  prefs: []
  type: TYPE_NORMAL
- en: Sensitivity and specificity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When the classification is binary, the confusion matrix is found to be a simple
    2x2 matrix, and therefore its positions are specially named:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Actual Class | Inferred Class |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Positive (1) | Negative (0) |   |'
  prefs: []
  type: TYPE_TB
- en: '| Positive (1) | True Positive | False Negative |'
  prefs: []
  type: TYPE_TB
- en: '| Negative (0) | False Positive | True Negative |'
  prefs: []
  type: TYPE_TB
- en: In disease diagnosis, which is the subject of this chapter, the concept of a
    binary confusion matrix is applied in the sense that a false diagnosis may be
    either false positive or false negative. The rate of false results can be measured
    by sensitivity and specificity indexes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sensitivity means the true positive rate; it measures how many of the records
    are correctly classified positively:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sensitivity and specificity](img/B05964_06_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Specificity, in turn, means the true negative rate; it indicates the proportion
    of negative record identification:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sensitivity and specificity](img/B05964_06_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: High values of both sensitivity and specificity are desired; however, depending
    on the application field, the sensitivity may carry more meaning.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a confusion matrix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In our code, let''s implement the confusion matrix in the class `NeuralOutputData`.
    The method `calculateConfusionMatrix` below is programmed to consider two neurons
    in the output layer. If the output is 10, then it is *yes* to a confusion matrix;
    if the output is 01, then it is no:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Another method implemented in the `NeuralOutputData` class is called `calculatePerformanceMeasures`.
    It receives as parameter the confusion matrix and it calculates and prints the
    following performance measures of classification:'
  prefs: []
  type: TYPE_NORMAL
- en: Positive class error rate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Negative class error rate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Total error rate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Total accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Precision
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sensibility
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specificity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This method is shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Neural networks for classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Classification tasks can be done by any of the supervised neural networks this
    book has covered so far. However, it is recommended that you use more complex
    architectures such as MLPs. In this chapter, we are going to use the `NeuralNet`
    class to build an MLP with one hidden layer and the sigmoid function at the output.
    Every output neuron will mean a class.
  prefs: []
  type: TYPE_NORMAL
- en: The code used to implement the examples is very similar to the test class (`BackpropagationTest`).
    However, the class `DiagnosisExample` asks which dataset the user would like to
    use and other neural network parameters, such as number of epochs, number of neurons
    in hidden layer, and learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: Disease diagnosis with neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For disease diagnosis, we are going to use the free dataset proben1, which
    is available on the Web ([http://www.filewatcher.com/m/proben1.tar.gz.1782734-0.html](http://www.filewatcher.com/m/proben1.tar.gz.1782734-0.html)).
    Proben1 is a benchmark set of several datasets from different domains. We are
    going to use the cancer and the diabetes datasets. We add a class to run the experiments
    of each case: `DiagnosisExample`.'
  prefs: []
  type: TYPE_NORMAL
- en: Breast cancer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The breast cancer dataset is composed of 10 variables, of which nine are inputs
    and one is a binary output. The dataset has 699 records, but we excluded from
    them 16 which were found to be incomplete, thus we used 683 to train and test
    the neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In real practical problems, it is common to have missing or invalid data. Ideally,
    the classification algorithm must handle these records, but sometimes it is recommended
    that you exclude them, since there would be not enough information to produce
    an accurate result.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table shows a configuration of this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Variable Name | Type | Maximum Value and Minimum Value |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Diagnosis result | OUTPUT | [0; 1] |'
  prefs: []
  type: TYPE_TB
- en: '| Clump Thickness | INPUT #1 | [1; 10] |'
  prefs: []
  type: TYPE_TB
- en: '| Uniformity of Cell Size | INPUT #2 | [1; 10] |'
  prefs: []
  type: TYPE_TB
- en: '| Uniformity of Cell Shape | INPUT #3 | [1; 10] |'
  prefs: []
  type: TYPE_TB
- en: '| Marginal Adhesion | INPUT #4 | [1; 10] |'
  prefs: []
  type: TYPE_TB
- en: '| Single Epithelial Cell Size | INPUT #5 | [1; 10] |'
  prefs: []
  type: TYPE_TB
- en: '| Bare Nuclei | INPUT #6 | [1; 10] |'
  prefs: []
  type: TYPE_TB
- en: '| Bland Chromatin | INPUT #7 | [1; 10] |'
  prefs: []
  type: TYPE_TB
- en: '| Normal Nucleoli | INPUT #8 | [1; 10] |'
  prefs: []
  type: TYPE_TB
- en: '| Mitoses | INPUT #9 | [1; 10] |'
  prefs: []
  type: TYPE_TB
- en: 'So, the proposed neural topology will be that of the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Breast cancer](img/B05964_06_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The dataset division was made as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Training**: 549 records (80%);'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Testing**: 134 records (20%)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As in the previous cases, we performed many experiments to try to find the
    best neural network to classify whether cancer is benign or malignant. So we conducted
    12 different experiments (1,000 epochs per experiment), wherein MSE and accuracy
    values were analyzed. After that, the confusion matrix, sensitivity, and specificity
    were generated with the test dataset and analysis was done. Finally, an analysis
    of generalization was taken. The neural networks involved in the experiments are
    shown in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Experiment | Number of neurons in hidden layer | Learning rate | Activation
    Function |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| #1 | 3 | 0.1 | Hidden Layer: SIGLOGOutput Layer: LINEAR |'
  prefs: []
  type: TYPE_TB
- en: '| #2 | Hidden Layer: HYPERTANOutput Layer: LINEAR |'
  prefs: []
  type: TYPE_TB
- en: '| #3 | 0.5 | Hidden Layer: SIGLOGOutput Layer: LINEAR |'
  prefs: []
  type: TYPE_TB
- en: '| #4 | Hidden Layer: HYPERTANOutput Layer: LINEAR |'
  prefs: []
  type: TYPE_TB
- en: '| #5 | 0.9 | Hidden Layer: SIGLOGOutput Layer: LINEAR |'
  prefs: []
  type: TYPE_TB
- en: '| #6 | Hidden Layer: HYPERTANOutput Layer: LINEAR |'
  prefs: []
  type: TYPE_TB
- en: '| #7 | 5 | 0.1 | Hidden Layer: SIGLOGOutput Layer: LINEAR |'
  prefs: []
  type: TYPE_TB
- en: '| #8 | Hidden Layer: HYPERTANOutput Layer: LINEAR |'
  prefs: []
  type: TYPE_TB
- en: '| #9 | 0.5 | Hidden Layer: SIGLOGOutput Layer: LINEAR |'
  prefs: []
  type: TYPE_TB
- en: '| #10 | Hidden Layer: HYPERTANOutput Layer: LINEAR |'
  prefs: []
  type: TYPE_TB
- en: '| #11 | 0.9 | Hidden Layer: SIGLOGOutput Layer: LINEAR |'
  prefs: []
  type: TYPE_TB
- en: '| #12 | Hidden Layer: HYPERTANOutput Layer: LINEAR |'
  prefs: []
  type: TYPE_TB
- en: 'After each experiment, we collected MSE values (Table X); experiments #4, #8,
    #9, #10, and #11 were equivalents, because they have low MSE values and same total
    accuracy measure (92.25%). Therefore, we selected experiments #4 and #11, because
    they have low MSE values among the five experiments mentioned before:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Experiment | MSE training rate | Total accuracy |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| #1 | 0.01067 | 96.29% |'
  prefs: []
  type: TYPE_TB
- en: '| #2 | 0.00443 | 98.50% |'
  prefs: []
  type: TYPE_TB
- en: '| #3 | 9.99611E-4 | 97.77% |'
  prefs: []
  type: TYPE_TB
- en: '| #4 | 9.99913E-4 | 99.25% |'
  prefs: []
  type: TYPE_TB
- en: '| #5 | 9.99670E-4 | 96.26% |'
  prefs: []
  type: TYPE_TB
- en: '| #6 | 9.92578E-4 | 97.03% |'
  prefs: []
  type: TYPE_TB
- en: '| #7 | 0.01392 | 98.49% |'
  prefs: []
  type: TYPE_TB
- en: '| #8 | 0.00367 | 99.25% |'
  prefs: []
  type: TYPE_TB
- en: '| #9 | 9.99928E-4 | 99.25% |'
  prefs: []
  type: TYPE_TB
- en: '| #10 | 9.99951E-4 | 99.25% |'
  prefs: []
  type: TYPE_TB
- en: '| #11 | 9.99926E-4 | 99.25% |'
  prefs: []
  type: TYPE_TB
- en: '| #12 | NaN | 3.44% |'
  prefs: []
  type: TYPE_TB
- en: 'Graphically, the MSE evolution over time is very fast, as can be seen in the
    following chart of the fourth experiment. Although we used 1,000 epochs to train,
    the experiment stopped earlier, because the minimum overall error (0.001) was
    reached:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Breast cancer](img/B05964_06_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The confusion matrix is shown in the table with the sensibility and specificity
    for both experiments. It is possible to check that measures are the same for both
    experiments:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Experiment | Confusion Matrix | Sensibility | Specificity |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| #4 | [[34.0, 1.0][0.00, 99.0]] | 97.22% | 100.0% |'
  prefs: []
  type: TYPE_TB
- en: '| #11 | [[34.0, 1.0][0.00, 99.0]] | 97.22% | 100.0% |'
  prefs: []
  type: TYPE_TB
- en: 'If we had to choose between models generated by experiments #4 or #11, we recommend
    selecting #4, because it''s simpler than #11 (it has fewer neurons in the hidden
    layer).'
  prefs: []
  type: TYPE_NORMAL
- en: Diabetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An additional example to be explored is the diagnosis of diabetes. This dataset
    has eight inputs and one output, shown in the table below. There are 768 records,
    all complete. However, proben1 states that there are several senseless zero values,
    probably indicating missing data. We''re handling this data as if it was real
    anyway, thereby introducing some errors (or noise) into the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Variable Name | Type | Maximum Value and Minimum Value |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Diagnosis result | OUTPUT | [0; 1] |'
  prefs: []
  type: TYPE_TB
- en: '| Number of times pregnant | INPUT #1 | [0.0; 17] |'
  prefs: []
  type: TYPE_TB
- en: '| Plasma glucose concentration a 2 hours in an oral glucose tolerance test
    | INPUT #2 | [0.0; 199] |'
  prefs: []
  type: TYPE_TB
- en: '| Diastolic blood pressure (mm Hg) | INPUT #3 | [0.0; 122] |'
  prefs: []
  type: TYPE_TB
- en: '| Triceps skin fold thickness (mm) | INPUT #4 | [0.0; 99] |'
  prefs: []
  type: TYPE_TB
- en: '| 2-Hour serum insulin (mu U/ml) | INPUT #5 | [0.0; 744] |'
  prefs: []
  type: TYPE_TB
- en: '| Body mass index (weight in kg/(height in m)^2) | INPUT #6 | [0.0; 67.1] |'
  prefs: []
  type: TYPE_TB
- en: '| Diabetes pedigree function | INPUT #7 | [0.078; 2420] |'
  prefs: []
  type: TYPE_TB
- en: '| Age (years) | INPUT #8 | [21; 81] |'
  prefs: []
  type: TYPE_TB
- en: 'The dataset division was made as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Training**: 617 records (80%)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Test**: 151 records (20%)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To discover the best neural net topology to classify diabetes, we used the
    same schema of neural networks with the same analysis described in the last section.
    However, we''re using multiple class classification in the output layer: two neurons
    in this layer will be used, one for the presence of diabetes and one for absence.'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, the proposed neural architecture looks like that of the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diabetes](img/B05964_06_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The table below shows the MSE training value and accuracy of the first six
    experiments and of the last six experiments:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Experiment | MSE training rate | Total accuracy |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| #1 | 0.00807 | 60.54% |'
  prefs: []
  type: TYPE_TB
- en: '| #2 | 0.00590 | 71.03% |'
  prefs: []
  type: TYPE_TB
- en: '| #3 | 9.99990E-4 | 75.49% |'
  prefs: []
  type: TYPE_TB
- en: '| #4 | 9.98840E-4 | 74.17% |'
  prefs: []
  type: TYPE_TB
- en: '| #5 | 0.00184 | 61.58% |'
  prefs: []
  type: TYPE_TB
- en: '| #6 | 9.82774E-4 | 59.86% |'
  prefs: []
  type: TYPE_TB
- en: '| #7 | 0.00706 | 63.57% |'
  prefs: []
  type: TYPE_TB
- en: '| #8 | 0.00584 | 72.41% |'
  prefs: []
  type: TYPE_TB
- en: '| #9 | 9.99994E-4 | 74.66% |'
  prefs: []
  type: TYPE_TB
- en: '| #10 | 0.01047 | 72.14% |'
  prefs: []
  type: TYPE_TB
- en: '| #11 | 0.00316 | 59.86% |'
  prefs: []
  type: TYPE_TB
- en: '| #12 | 0.43464 | 40.13% |'
  prefs: []
  type: TYPE_TB
- en: 'The fall of the MSE is fast in both cases. However, experiment #9 generates
    an increase of error rate in the first values. It is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diabetes](img/B05964_06_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Analyzing the confusion matrixes, it can be seen that the measures are very
    similar:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Experiment | Confusion Matrix | Sensibility | Specificity |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| #3 | [[35.0, 12.0][25.0, 79.0]] | 74.46% | 75.96% |'
  prefs: []
  type: TYPE_TB
- en: '| #9 | [[34.0, 12.0][26.0, 78.0]] | 73.91% | 75.00% |'
  prefs: []
  type: TYPE_TB
- en: 'One more time, we suggest choosing the simplest model. In the diabetes example,
    it is the artificial neural network generated by experiment #3.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is recommended you explore the class `D` `iagnosisExample` and create a GUI
    to become easy select neural net parameters, as was done in the previous chapter.
    You should try to reuse code already programmed through the inheritance concept.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we've seen two examples of the application of disease diagnosis
    using neural networks. The fundamentals of classification problems were briefly
    reviewed in order to level the knowledge explored in this chapter. Classification
    tasks belong to one of the most used types of supervised tasks in the machine
    learning / data mining fields, and Neural Networks proved to be very appropriate
    to be applied to this type of problem. The reader was also presented with the
    concepts that evaluate the classification tasks, such as sensitivity, specificity,
    and the confusion matrix. These notations are very useful for all classification
    tasks, including those which are handled with other algorithms besides neural
    networks. The next chapter will explore a similar kind of task but using unsupervised
    learning – that means, without expected output data – but the fundamentals presented
    in this chapter will be somewhat helpful.
  prefs: []
  type: TYPE_NORMAL
