<html><head></head><body>
<div id="_idContainer283" class="calibre2">
<h1 class="chapter-number" id="_idParaDest-44"><a id="_idTextAnchor045" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1.1">3</span></h1>
<h1 id="_idParaDest-45" class="calibre4"><a id="_idTextAnchor046" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.2.1">Unleashing Machine Learning Potentials in Natural Language Processing</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.3.1">In this chapter, we will delve into the fundamentals of </span><strong class="bold"><span class="kobospan" id="kobo.4.1">Machine Learning</span></strong><span class="kobospan" id="kobo.5.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.6.1">ML</span></strong><span class="kobospan" id="kobo.7.1">) and preprocessing techniques that are essential for </span><strong class="bold"><span class="kobospan" id="kobo.8.1">natural language processing</span></strong><span class="kobospan" id="kobo.9.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.10.1">NLP</span></strong><span class="kobospan" id="kobo.11.1">) tasks. </span><span class="kobospan" id="kobo.11.2">ML is a powerful tool for building models that can learn from data, and NLP is one of the most exciting and challenging applications </span><span><span class="kobospan" id="kobo.12.1">of ML.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.13.1">By the end of this chapter, you will have gained a comprehensive understanding of data exploration, preprocessing, and data split, know how to deal with imbalanced data techniques, and learned about some of the common ML models required for successful ML, particularly in the context </span><span><span class="kobospan" id="kobo.14.1">of NLP.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.15.1">The following topics will be covered in </span><span><span class="kobospan" id="kobo.16.1">this chapter:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><span><span class="kobospan" id="kobo.17.1">Data exploration</span></span></li>
<li class="calibre15"><a id="_idTextAnchor047" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.18.1">Common </span><span><span class="kobospan" id="kobo.19.1">ML models</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.20.1">Model underfitting </span><span><span class="kobospan" id="kobo.21.1">and overfitting</span></span></li>
<li class="calibre15"><span><span class="kobospan" id="kobo.22.1">Splitting data</span></span></li>
<li class="calibre15"><span><span class="kobospan" id="kobo.23.1">Hyperparameter tuning</span></span></li>
<li class="calibre15"><span><span class="kobospan" id="kobo.24.1">Ensemble models</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.25.1">Handling </span><span><span class="kobospan" id="kobo.26.1">imbalanced data</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.27.1">Dealing with </span><span><span class="kobospan" id="kobo.28.1">correlated data</span></span></li>
</ul>
<h1 id="_idParaDest-46" class="calibre4"><a id="_idTextAnchor048" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.29.1">Technical requirements</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.30.1">Prior knowledge of programming languages, particularly Python, is assumed in this chapter and subsequent chapters of this book. </span><span class="kobospan" id="kobo.30.2">It is also expected that you have already gone through previous chapters to become acquainted with the necessary linear algebra and statistics concepts that will be discussed </span><span><span class="kobospan" id="kobo.31.1">in detail.</span></span></p>
<h1 id="_idParaDest-47" class="calibre4"><a id="_idTextAnchor049" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.32.1">Data exploration</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.33.1">When working in a methodological environment, datasets are often well known and preprocessed, such as</span><a id="_idIndexMarker099" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.34.1"> Kaggle datasets. </span><span class="kobospan" id="kobo.34.2">However, in real-world business environments, one important task is to define the dataset from all possible sources of data, explore the gathered data to find the best method for preprocessing it, and ultimately decide on the ML and natural language models that fit the problem and the underlying data best. </span><span class="kobospan" id="kobo.34.3">This process requires careful consideration and analysis of the data, as well as a thorough understanding of the business problem </span><span><span class="kobospan" id="kobo.35.1">at hand.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.36.1">In NLP, the data can be quite complex, as it often includes text and speech data that can be unstructured and difficult to analyze. </span><span class="kobospan" id="kobo.36.2">This complexity makes preprocessing an essential step in preparing the data for ML models. </span><span class="kobospan" id="kobo.36.3">The first step of any NLP or ML solution starts with exploring the data to learn more about it, which helps us decide on our path to tackle </span><span><span class="kobospan" id="kobo.37.1">the problem.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.38.1">Once the data has been preprocessed, the next step is to explore it to gain a better understanding of its characteristics and structure. </span><span class="kobospan" id="kobo.38.2">Data exploration is an iterative process that involves visualizing and analyzing the data, looking for patterns and relationships, and identifying potential issues or outliers. </span><span class="kobospan" id="kobo.38.3">This process can help us to determine which features are most important for our ML models and identify any potential biases or data quality issues. </span><span class="kobospan" id="kobo.38.4">To streamline data and enhance analysis through ML models, preprocessing methods such as tokenization, stemming, and lemmatization can be employed. </span><span class="kobospan" id="kobo.38.5">In this chapter, we will provide an overview of general preprocessing techniques for ML problems. </span><span class="kobospan" id="kobo.38.6">In the following chapter, we will delve into preprocessing techniques specific to text processing. </span><span class="kobospan" id="kobo.38.7">It is important to note that employing effective preprocessing techniques can significantly enhance the performance and accuracy of ML models, making them more robust </span><span><span class="kobospan" id="kobo.39.1">and reliable.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.40.1">Finally, once the data has been preprocessed and explored, we can start building our ML models. </span><span class="kobospan" id="kobo.40.2">There is no single magical solution that works for all ML problems, so it’s important to carefully consider which models are best suited for the data and the problem at hand. </span><span class="kobospan" id="kobo.40.3">Different types of NLP models exist, encompassing rule-based, statistical, and deep learning models. </span><span class="kobospan" id="kobo.40.4">Each model type possesses unique strengths and weaknesses, underscoring the importance of selecting the most fitting one for the specific problem and dataset </span><span><span class="kobospan" id="kobo.41.1">at hand.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.42.1">Data exploration is an important and initial step in the ML workflow that involves analyzing and understanding the data before building a ML model. </span><span class="kobospan" id="kobo.42.2">The goal of data exploration is to gain insights about the data, identify patterns, detect anomalies, and prepare the data for </span><a id="_idIndexMarker100" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.43.1">modeling. </span><span class="kobospan" id="kobo.43.2">Data exploration helps in choosing the right ML algorithm and determining the best set of features </span><span><span class="kobospan" id="kobo.44.1">to use.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.45.1">Here are some common techniques that are used in </span><span><span class="kobospan" id="kobo.46.1">data exploration:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.47.1">Data visualization</span></strong><span class="kobospan" id="kobo.48.1">: Data visualization involves depicting data through graphical or pictorial formats. </span><span class="kobospan" id="kobo.48.2">It enables</span><a id="_idIndexMarker101" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.49.1"> visual exploration of data, providing insights into its distribution, patterns, and</span><a id="_idIndexMarker102" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.50.1"> relationships. </span><span class="kobospan" id="kobo.50.2">Widely employed techniques in data visualization encompass scatter plots, bar charts, heatmaps, box plots, and </span><span><span class="kobospan" id="kobo.51.1">correlation matrices.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.52.1">Data cleaning</span></strong><span class="kobospan" id="kobo.53.1">: Data cleaning is a</span><a id="_idIndexMarker103" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.54.1"> step of preprocessing where we identify the errors, inconsistencies, and missing values and correct them. </span><span class="kobospan" id="kobo.54.2">It affects the final results of the model since ML models are sensitive to</span><a id="_idIndexMarker104" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.55.1"> errors in the data. </span><span class="kobospan" id="kobo.55.2">Removing duplicates and filling in missing values are some of the common data </span><span><span class="kobospan" id="kobo.56.1">cleaning techniques.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.57.1">Feature engineering</span></strong><span class="kobospan" id="kobo.58.1">: Feature engineering plays a crucial role in optimizing the effectiveness of machine learning</span><a id="_idIndexMarker105" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.59.1"> models by crafting new features from existing data. </span><span class="kobospan" id="kobo.59.2">This process involves not only identifying </span><a id="_idIndexMarker106" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.60.1">pertinent features but also transforming the existing ones and introducing novel features. </span><span class="kobospan" id="kobo.60.2">Various feature engineering techniques, including scaling, normalization, dimensionality reduction, and feature selection, contribute to refining the overall performance of </span><span><span class="kobospan" id="kobo.61.1">the models.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.62.1">Statistical analysis</span></strong><span class="kobospan" id="kobo.63.1">: Statistical analysis utilizes a range of statistical techniques to scrutinize data, revealing valuable insights into its inherent properties. </span><span class="kobospan" id="kobo.63.2">Essential statistical </span><a id="_idIndexMarker107" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.64.1">methods include hypothesis</span><a id="_idIndexMarker108" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.65.1"> testing, regression analysis, and time series analysis, all of which contribute to a comprehensive understanding of the </span><span><span class="kobospan" id="kobo.66.1">data’s characteristics.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.67.1">Domain knowledge</span></strong><span class="kobospan" id="kobo.68.1">: Leveraging domain knowledge entails applying a pre-existing understanding of the </span><a id="_idIndexMarker109" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.69.1">data domain to extract insights and make informed decisions. </span><span class="kobospan" id="kobo.69.2">This knowledge proves valuable </span><a id="_idIndexMarker110" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.70.1">in recognizing pertinent features, interpreting results, and choosing the most suitable ML algorithm for the task </span><span><span class="kobospan" id="kobo.71.1">at hand.</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.72.1">We will explore each of these techniques in the </span><span><span class="kobospan" id="kobo.73.1">following s</span><a id="_idTextAnchor050" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.74.1">ubsections.</span></span></p>
<h2 id="_idParaDest-48" class="calibre7"><a id="_idTextAnchor051" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.75.1">Data visualization</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.76.1">Data visualization is a</span><a id="_idIndexMarker111" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.77.1"> crucial component of machine learning as it allows us to understand and explore complex datasets more easily. </span><span class="kobospan" id="kobo.77.2">It involves creating</span><a id="_idIndexMarker112" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.78.1"> visual representations of data using charts, graphs, and other types of visual aids. </span><span class="kobospan" id="kobo.78.2">By visually presenting data, we can discern patterns, trends, and relationships that might not be readily evident when examining the raw </span><span><span class="kobospan" id="kobo.79.1">data alone.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.80.1">For NLP tasks, data visualization can help us gain insights into the linguistic patterns and structures in text data. </span><span class="kobospan" id="kobo.80.2">For example, we can create word clouds to visualize the frequency of words in a corpus or use heatmaps to display the co-occurrence of words or phrases. </span><span class="kobospan" id="kobo.80.3">We can also use scatter plots and line graphs to visualize changes in sentiment or topic </span><span><span class="kobospan" id="kobo.81.1">over time.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.82.1">One common type of visualization for ML is the scatter plot, which is used to display the relationship between two variables. </span><span class="kobospan" id="kobo.82.2">By plotting the values of two variables on the X and Y axes, we can identify any patterns or trends that exist between them. </span><span class="kobospan" id="kobo.82.3">Scatter plots are particularly useful for identifying clusters or groups of data points that share </span><span><span class="kobospan" id="kobo.83.1">similar characteristics.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.84.1">Another type of visualization that’s frequently employed in ML is the histogram, a tool that illustrates the distribution of a single variable. </span><span class="kobospan" id="kobo.84.2">By grouping data into bins and portraying the frequency of data points in each bin, we can pinpoint the range of values that predominate in the </span><a id="_idIndexMarker113" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.85.1">dataset. </span><span class="kobospan" id="kobo.85.2">Histograms prove useful for detecting outliers or anomalies, and they aid in recognizing areas where the data may exhibit skewness </span><span><span class="kobospan" id="kobo.86.1">or bias.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.87.1">In addition to these basic</span><a id="_idIndexMarker114" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.88.1"> visualizations, ML practitioners often use more advanced techniques, such as dimensionality reduction and network visualizations. </span><span class="kobospan" id="kobo.88.2">Dimensionality reduction techniques, such as </span><strong class="bold"><span class="kobospan" id="kobo.89.1">principal component analysis</span></strong><span class="kobospan" id="kobo.90.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.91.1">PCA</span></strong><span class="kobospan" id="kobo.92.1">) and </span><strong class="bold"><span class="kobospan" id="kobo.93.1">t-distributed stochastic neighbor embedding</span></strong><span class="kobospan" id="kobo.94.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.95.1">t-SNE</span></strong><span class="kobospan" id="kobo.96.1">), are commonly used for </span><a id="_idIndexMarker115" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.97.1">dimensional reduction and to visualize </span><a id="_idIndexMarker116" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.98.1">or analyze the data more easily. </span><span class="kobospan" id="kobo.98.2">Network visualizations, on the other hand, are used to display complex relationships between entities, such as the co-occurrence of words or the connections between so</span><a id="_idTextAnchor052" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.99.1">cial </span><span><span class="kobospan" id="kobo.100.1">media users.</span></span></p>
<h2 id="_idParaDest-49" class="calibre7"><a id="_idTextAnchor053" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.101.1">Data cleaning</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.102.1">Data cleaning, alternatively termed </span><a id="_idIndexMarker117" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.103.1">data cleansing or data scrubbing, involves recognizing and rectifying or eliminating errors, inconsistencies, and inaccuracies </span><a id="_idIndexMarker118" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.104.1">within a dataset. </span><span class="kobospan" id="kobo.104.2">This crucial phase in data preparation for ML significantly influences the accuracy and performance of a model, relying on the quality of the data used for training. </span><span class="kobospan" id="kobo.104.3">Numerous prevalent techniques are employed in data cleaning. </span><span class="kobospan" id="kobo.104.4">Let’s </span><a id="_idTextAnchor054" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.105.1">take a </span><span><span class="kobospan" id="kobo.106.1">closer look.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.107.1">Handling missing values</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.108.1">Missing data is a common problem that occurs in many machine learning projects. </span><span class="kobospan" id="kobo.108.2">Dealing with missing data is important because ML models cannot handle missing data and will either produce errors or</span><a id="_idIndexMarker119" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.109.1"> provide </span><span><span class="kobospan" id="kobo.110.1">inaccurate results.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.111.1">There are several methods for dealing with missing data in </span><span><span class="kobospan" id="kobo.112.1">ML projects:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.113.1">Dropping rows</span></strong><span class="kobospan" id="kobo.114.1">: Addressing missing data can involve a straightforward approach of discarding rows that contain such values. </span><span class="kobospan" id="kobo.114.2">Nevertheless, exercising caution is paramount when employing this method as excessive row removal may result in the loss of valuable data, impacting the overall accuracy of the model. </span><span class="kobospan" id="kobo.114.3">We usually use this method when we have a few rows in our dataset, and we have a few rows with missing values. </span><span class="kobospan" id="kobo.114.4">In this case, removing a few rows can be a good and easy approach to training our model while the final performance will not be </span><span><span class="kobospan" id="kobo.115.1">affected significantly.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.116.1">Dropping columns</span></strong><span class="kobospan" id="kobo.117.1">: Another </span><a id="_idIndexMarker120" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.118.1">approach is to drop the columns that contain missing values. </span><span class="kobospan" id="kobo.118.2">This method can be effective if the missing values are concentrated in a few columns </span><a id="_idIndexMarker121" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.119.1">and if those columns are not important for the analysis. </span><span class="kobospan" id="kobo.119.2">However, dropping important columns can lead to a loss of valuable information. </span><span class="kobospan" id="kobo.119.3">It is better to perform some sort of correlation analysis to see the correlation of the values in these columns with the target class or value before dropping </span><span><span class="kobospan" id="kobo.120.1">these columns.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.121.1">Mean/median/mode imputation</span></strong><span class="kobospan" id="kobo.122.1">: Mean, median, and mode imputation entail substituting missing values with the mean, median, or mode derived from the non-missing values within</span><a id="_idIndexMarker122" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.123.1"> the corresponding column. </span><span class="kobospan" id="kobo.123.2">This method is easy to implement and can be effective when the missing values are few and randomly distributed. </span><span class="kobospan" id="kobo.123.3">However, it can also introduce bias and affect the variability of </span><span><span class="kobospan" id="kobo.124.1">the data.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.125.1">Regression imputation</span></strong><span class="kobospan" id="kobo.126.1">: Regression imputation involves predicting the missing values based on the values of other variables in the dataset. </span><span class="kobospan" id="kobo.126.2">This method can be effective when</span><a id="_idIndexMarker123" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.127.1"> the missing values are related to other variables in the dataset, but it requires a regression model to be built for each column with </span><span><span class="kobospan" id="kobo.128.1">missing values.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.129.1">Multiple imputation</span></strong><span class="kobospan" id="kobo.130.1">: Multiple imputation encompasses generating multiple imputed datasets through statistical models, followed by amalgamating the outcomes to</span><a id="_idIndexMarker124" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.131.1"> produce a conclusive dataset. </span><span class="kobospan" id="kobo.131.2">This approach proves efficacious, particularly when dealing with non-randomly distributed missing values and a substantial number of gaps in </span><span><span class="kobospan" id="kobo.132.1">the dataset.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.133.1">K-nearest neighbor imputation</span></strong><span class="kobospan" id="kobo.134.1">: K-nearest neighbor imputation entails identifying the k-nearest data points to the missing value and utilizing their values to impute the absent value. </span><span class="kobospan" id="kobo.134.2">This method can be effective when the missing values are </span><a id="_idIndexMarker125" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.135.1">clustered together in the dataset. </span><span class="kobospan" id="kobo.135.2">In this approach, we can find the most similar records to the dataset to the record that has the missing value, and then use the mean of the values of those records for that specific record as the </span><span><span class="kobospan" id="kobo.136.1">missed value.</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.137.1">In essence, selecting a method to handle missing data hinges on factors such as the nature and extent of the missing data, analysis objectives, and resource availability. </span><span class="kobospan" id="kobo.137.2">It is crucial to thoughtfully assess the pros and cons of each method and opt for the most suitable approach tailore</span><a id="_idTextAnchor055" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.138.1">d to the </span><span><span class="kobospan" id="kobo.139.1">specific project.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.140.1">Removing duplicates</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.141.1">Eliminating duplicates is a prevalent</span><a id="_idIndexMarker126" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.142.1"> preprocessing measure that’s employed to cleanse datasets by detecting and removing identical records. </span><span class="kobospan" id="kobo.142.2">The occurrence of duplicate records may be attributed to factors such as data entry errors, system glitches, or data merging processes. </span><span class="kobospan" id="kobo.142.3">The presence of duplicates can skew models and yield inaccurate insights. </span><span class="kobospan" id="kobo.142.4">Hence, it is imperative to recognize and eliminate duplicate records to uphold the accuracy and dependability of </span><span><span class="kobospan" id="kobo.143.1">the dataset.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.144.1">There are different methods for removing duplicates in a dataset. </span><span class="kobospan" id="kobo.144.2">The most common method is to compare all the rows of the dataset to identify duplicate records. </span><span class="kobospan" id="kobo.144.3">If two or more rows have the same values in all the columns, they are considered duplicates. </span><span class="kobospan" id="kobo.144.4">In some cases, it may be necessary to compare only a subset of columns if certain columns are more prone </span><span><span class="kobospan" id="kobo.145.1">to duplicates.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.146.1">Another method is to use a unique identifier column to identify duplicates. </span><span class="kobospan" id="kobo.146.2">A unique identifier column is a column that contains unique values for each record, such as an ID number or a combination of unique columns. </span><span class="kobospan" id="kobo.146.3">By comparing the unique identifier column, it is possible to identify and remove duplicate records from </span><span><span class="kobospan" id="kobo.147.1">the dataset.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.148.1">After identifying the duplicate records, the next step is to decide which records to keep and which ones to remove. </span><span class="kobospan" id="kobo.148.2">One</span><a id="_idIndexMarker127" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.149.1"> approach is to keep the first occurrence of a duplicate record and remove all subsequent occurrences. </span><span class="kobospan" id="kobo.149.2">Another approach is to keep the record with the most complete information, or the record with the most </span><span><span class="kobospan" id="kobo.150.1">recent timestamp.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.151.1">It’s crucial to recognize that the removal of duplicates might lead to a reduction in dataset size, potentially affecting the performance of ML models. </span><span class="kobospan" id="kobo.151.2">Consequently, assessing the impact of duplicate removal on both the dataset and the ML model is essential. </span><span class="kobospan" id="kobo.151.3">In some cases, it may be necessary to keep duplicate records if they contain important information that cannot be </span><a id="_idTextAnchor056" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.152.1">obtained from </span><span><span class="kobospan" id="kobo.153.1">other records.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.154.1">Standardizing and transforming data</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.155.1">Standardizing and transforming data is a critical step in preparing data for ML tasks. </span><span class="kobospan" id="kobo.155.2">This process involves scaling and normalizing the numerical features of the dataset to make them easier to</span><a id="_idIndexMarker128" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.156.1"> interpret and compare. </span><span class="kobospan" id="kobo.156.2">The main objective of standardizing </span><a id="_idIndexMarker129" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.157.1">and transforming data is to enhance the accuracy and performance of a ML model by mitigating the influence of features with diverse scales and</span><a id="_idIndexMarker130" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.158.1"> ranges. </span><span class="kobospan" id="kobo.158.2">A widely used method for standardizing data is referred to as “standardization” or “Z-score </span><a id="_idIndexMarker131" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.159.1">normalization.” </span><span class="kobospan" id="kobo.159.2">This technique involves transforming each feature such that it has a mean of zero and a standard deviation of one. </span><span class="kobospan" id="kobo.159.3">The formula for standardization is shown in the </span><span><span class="kobospan" id="kobo.160.1">following equation:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.161.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;′&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/128.png" class="calibre133"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.162.1">Here, </span><em class="italic"><span class="kobospan" id="kobo.163.1">x</span></em><span class="kobospan" id="kobo.164.1"> represents the feature, </span><em class="italic"><span class="kobospan" id="kobo.165.1">mean(x)</span></em><span class="kobospan" id="kobo.166.1"> denotes the mean of the feature, </span><em class="italic"><span class="kobospan" id="kobo.167.1">std(x)</span></em><span class="kobospan" id="kobo.168.1"> indicates the standard deviation of the feature, and </span><em class="italic"><span class="kobospan" id="kobo.169.1">x’</span></em><span class="kobospan" id="kobo.170.1"> represents the new value assigned to the feature. </span><span class="kobospan" id="kobo.170.2">By standardizing the data in this way, the range of each feature is adjusted to be centered around zero, which makes it easier to compare features and prevents features with large values from dominating </span><span><span class="kobospan" id="kobo.171.1">the analysis.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.172.1">Another technique for transforming data is “min-max scaling.” </span><span class="kobospan" id="kobo.172.2">This method rescales the data to a consistent range of values, commonly ranging between 0 and 1. </span><span class="kobospan" id="kobo.172.3">The formula for min-max scaling is </span><span><span class="kobospan" id="kobo.173.1">shown here:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.174.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;′&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/129.png" class="calibre134"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.175.1">In this equation, </span><em class="italic"><span class="kobospan" id="kobo.176.1">x</span></em><span class="kobospan" id="kobo.177.1"> represents the feature, </span><em class="italic"><span class="kobospan" id="kobo.178.1">min(x)</span></em><span class="kobospan" id="kobo.179.1"> signifies the minimum value of the feature, and </span><em class="italic"><span class="kobospan" id="kobo.180.1">max(x)</span></em><span class="kobospan" id="kobo.181.1"> denotes the maximum value of the feature. </span><span class="kobospan" id="kobo.181.2">Min-max scaling proves beneficial when the precise distribution of the data is not crucial, but there is a need to standardize the data for meaningful comparisons across </span><span><span class="kobospan" id="kobo.182.1">different features.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.183.1">Transforming data can also involve changing the distribution of the data. </span><span class="kobospan" id="kobo.183.2">A frequently applied transformation</span><a id="_idIndexMarker132" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.184.1"> is the log transformation, which is employed to alleviate the influence of outliers and skewness within the data. </span><span class="kobospan" id="kobo.184.2">This transformation involves taking the logarithm of the feature values, which can </span><a id="_idIndexMarker133" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.185.1">help to normalize the distribution and reduce the influence of </span><span><span class="kobospan" id="kobo.186.1">extreme values.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.187.1">Overall, standardizing and transforming data constitute a pivotal stage in the data preprocessing workflow for ML endeavors. </span><span class="kobospan" id="kobo.187.2">Through scaling and normalizing features, we can enhance the accuracy and performance of the ML model, rendering the data more interpretable and co</span><a id="_idTextAnchor057" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.188.1">nducive to </span><span><span class="kobospan" id="kobo.189.1">meaningful comparisons.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.190.1">Handling outliers</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.191.1">Outliers are data points that markedly deviate from the rest of the observations in a dataset. </span><span class="kobospan" id="kobo.191.2">Their occurrence may stem from factors such as measurement errors, data corruption, or authentic</span><a id="_idIndexMarker134" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.192.1"> extreme values. </span><span class="kobospan" id="kobo.192.2">The presence of outliers can wield a substantial influence on the outcomes of ML models, introducing distortion to the data and disrupting the relationships between variables. </span><span class="kobospan" id="kobo.192.3">Therefore, handling outliers is an important step in preprocessing data </span><span><span class="kobospan" id="kobo.193.1">for ML.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.194.1">There are several methods for </span><span><span class="kobospan" id="kobo.195.1">handling outliers:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.196.1">Removing outliers</span></strong><span class="kobospan" id="kobo.197.1">: One straightforward approach involves eliminating observations identified as outliers from the dataset. </span><span class="kobospan" id="kobo.197.2">Nevertheless, exercising caution is paramount</span><a id="_idIndexMarker135" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.198.1"> when adopting this method as excessive removal of observations may result in the loss of valuable information and potentially introduce bias to the </span><span><span class="kobospan" id="kobo.199.1">analysis results.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.200.1">Transforming data</span></strong><span class="kobospan" id="kobo.201.1">: Applying mathematical</span><a id="_idIndexMarker136" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.202.1"> functions such as logarithms or square roots to transform the data can mitigate the influence of outliers. </span><span class="kobospan" id="kobo.202.2">For instance, taking the logarithm of a variable can alleviate the impact of extreme values, given the slower rate of increase in the logarithmic scale compared to the </span><span><span class="kobospan" id="kobo.203.1">original values.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.204.1">Winsorizing</span></strong><span class="kobospan" id="kobo.205.1">: Winsorizing is a technique that entails substituting extreme values with the nearest highest or lowest value in the dataset. </span><span class="kobospan" id="kobo.205.2">Employing this method aids in maintaining the sample size and overall distribution of </span><span><span class="kobospan" id="kobo.206.1">the data.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.207.1">Imputing values</span></strong><span class="kobospan" id="kobo.208.1">: Imputation involves replacing missing or extreme values with estimated values derived from the remaining observations in the dataset. </span><span class="kobospan" id="kobo.208.2">For instance, substituting extreme values with the median or mean of the remaining observations is a common </span><span><span class="kobospan" id="kobo.209.1">imputation technique.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.210.1">Using robust statistical methods</span></strong><span class="kobospan" id="kobo.211.1">: Robust statistical methods exhibit lower sensitivity to outliers, leading to more accurate results even in the presence of such extreme values. </span><span class="kobospan" id="kobo.211.2">For instance, opting for the median instead of the mean can effectively diminish the influence of outliers on the </span><span><span class="kobospan" id="kobo.212.1">final results.</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.213.1">It’s crucial to emphasize that selecting an outlier-handling method should be tailored to the unique characteristics of the data and the specific problem at hand. </span><span class="kobospan" id="kobo.213.2">Generally, employing a combination of methods is advisable to address outliers comprehensively, and assessing the impact of each method on the results is essential. </span><span class="kobospan" id="kobo.213.3">Moreover, documenting the steps taken to manage outliers is important for reproducibility and to provide cl</span><a id="_idTextAnchor058" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.214.1">arity on the </span><span><span class="kobospan" id="kobo.215.1">decision-making process.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.216.1">Correcting errors</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.217.1">Rectifying errors during preprocessing stands as a vital stage in readying data for ML. </span><span class="kobospan" id="kobo.217.2">Errors may manifest due to diverse </span><a id="_idIndexMarker137" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.218.1">reasons such as data entry blunders, measurement discrepancies, sensor inaccuracies, or transmission glitches. </span><span class="kobospan" id="kobo.218.2">Correcting errors in data holds paramount significance in guaranteeing that ML models are trained on dependable and precise data, consequently enhancing the accuracy and reliability </span><span><span class="kobospan" id="kobo.219.1">of predictions.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.220.1">Several techniques exist to rectify errors in data. </span><span class="kobospan" id="kobo.220.2">Here are some widely </span><span><span class="kobospan" id="kobo.221.1">utilized methods:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.222.1">Manual inspection</span></strong><span class="kobospan" id="kobo.223.1">: An approach to rectify errors in data involves a manual inspection of the dataset, wherein errors are corrected by hand. </span><span class="kobospan" id="kobo.223.2">This method is frequently employed, particularly when dealing with relatively small and </span><span><span class="kobospan" id="kobo.224.1">manageable datasets.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.225.1">Statistical methods</span></strong><span class="kobospan" id="kobo.226.1">: Statistical methods prove effective in identifying and rectifying errors in data. </span><span class="kobospan" id="kobo.226.2">For instance, when the data adheres to a recognized distribution, statistical techniques such as the Z-score can be employed to detect outliers, which can then be either removed </span><span><span class="kobospan" id="kobo.227.1">or replaced.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.228.1">ML methods</span></strong><span class="kobospan" id="kobo.229.1">: Utilizing ML algorithms facilitates the detection and correction of errors in data. </span><span class="kobospan" id="kobo.229.2">For instance, clustering algorithms prove valuable in pinpointing data points that markedly deviate from the broader dataset. </span><span class="kobospan" id="kobo.229.3">Subsequently, these identified data points can undergo further examination </span><span><span class="kobospan" id="kobo.230.1">and correction.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.231.1">Domain knowledge</span></strong><span class="kobospan" id="kobo.232.1">: Leveraging domain knowledge is instrumental in pinpointing errors within data. </span><span class="kobospan" id="kobo.232.2">For instance, when collecting data from sensors, it becomes feasible to identify and rectify errors by considering the anticipated range of values that the sensor is capable </span><span><span class="kobospan" id="kobo.233.1">of producing.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.234.1">Imputation</span></strong><span class="kobospan" id="kobo.235.1">: Imputation serves as a method to populate missing values in the data. </span><span class="kobospan" id="kobo.235.2">This can be accomplished through various means, including statistical methods such as mean or median imputation, as well as ML algorithms such as k-nearest </span><span><span class="kobospan" id="kobo.236.1">neighbor imputation.</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.237.1">Choosing a technique hinges on factors such as the nature of the data, the dataset’s siz</span><a id="_idTextAnchor059" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.238.1">e, and the resources at </span><a id="_idIndexMarker138" class="calibre5 pcalibre1 pcalibre"/><span><span class="kobospan" id="kobo.239.1">your disposal.</span></span></p>
<h2 id="_idParaDest-50" class="calibre7"><a id="_idTextAnchor060" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.240.1">Feature selection</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.241.1">Feature selection involves choosing the most pertinent features from a dataset for constructing a ML model. </span><span class="kobospan" id="kobo.241.2">The </span><a id="_idIndexMarker139" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.242.1">objective is to decrease the number of features without substantially compromising the model’s accuracy, resulting in enhanced performance, quicker training, and a more straightforward interpretation of </span><span><span class="kobospan" id="kobo.243.1">the model.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.244.1">Several approaches to fea</span><a id="_idTextAnchor061" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.245.1">ture selection exist. </span><span class="kobospan" id="kobo.245.2">Let’s take </span><span><span class="kobospan" id="kobo.246.1">a look.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.247.1">Filter methods</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.248.1">These techniques employ </span><a id="_idIndexMarker140" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.249.1">statistical methods to rank features</span><a id="_idIndexMarker141" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.250.1"> according to their correlation with the target variable. </span><span class="kobospan" id="kobo.250.2">Common methods encompass chi-squared, mutual information, and correlation coefficients. </span><span class="kobospan" id="kobo.250.3">Features are subsequently chosen based on a </span><span><span class="kobospan" id="kobo.251.1">predefined threshold.</span></span></p>
<h4 class="calibre135"><span class="kobospan" id="kobo.252.1">Chi-squared</span></h4>
<p class="calibre6"><span class="kobospan" id="kobo.253.1">The chi-squared test is a widely employed statistical method in ML for feature selection that’s particularly</span><a id="_idIndexMarker142" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.254.1"> effective for categorical variables. </span><span class="kobospan" id="kobo.254.2">This test gauges the dependence between two random variables, providing a P-value that signifies the likelihood of obtaining a result as extreme as or more extreme</span><a id="_idIndexMarker143" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.255.1"> than the </span><span><span class="kobospan" id="kobo.256.1">actual observations.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.257.1">In hypothesis testing, the chi-squared test assesses whether the collected data aligns with the expected data. </span><span class="kobospan" id="kobo.257.2">A small chi-squared test statistic indicates a robust match, while a large statistic implies a weak match. </span><span class="kobospan" id="kobo.257.3">A P-value less than or equal to 0.05 leads to the rejection of the null hypothesis, considering it highly improbable. </span><span class="kobospan" id="kobo.257.4">Conversely, a P-value greater than 0.05 results in accepting or “failing to reject” the null hypothesis. </span><span class="kobospan" id="kobo.257.5">When the P-value hovers around 0.05, further scrutiny of the hypothesis </span><span><span class="kobospan" id="kobo.258.1">is warranted.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.259.1">In feature selection, the chi-squared test evaluates the relationship between each feature and the target variable in the dataset. </span><span class="kobospan" id="kobo.259.2">It determines significance based on whether a statistically significant difference exists between the observed and expected frequencies of the feature, assuming independence between the feature and target. </span><span class="kobospan" id="kobo.259.3">Features with a high chi-squared score exhibit a stronger dependence on the target variable, making them more informative for classification or regression tasks. </span><span class="kobospan" id="kobo.259.4">The formula for calculating the chi-squared is presented in the </span><span><span class="kobospan" id="kobo.260.1">following equation:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.261.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mfrac&gt;&lt;msup&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;msub&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/130.png" class="calibre136"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.262.1">In this equation, </span><span class="kobospan" id="kobo.263.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/131.png" class="calibre137"/></span><span class="kobospan" id="kobo.264.1"> represents the observed value and </span><span class="kobospan" id="kobo.265.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;E&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/132.png" class="calibre138"/></span><span class="kobospan" id="kobo.266.1"> represents the expected value. </span><span class="kobospan" id="kobo.266.2">The computation involves finding the difference between the observed frequency and the expected frequency, squaring the result, and then dividing by the expected frequency. </span><span class="kobospan" id="kobo.266.3">The summation of these values across all categories of the feature yields the overall</span><a id="_idIndexMarker144" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.267.1"> chi-squared statistic for </span><span><span class="kobospan" id="kobo.268.1">that feature.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.269.1">The degrees of freedom for the test relies on the number of categories in the feature and the number of categories in the </span><span><span class="kobospan" id="kobo.270.1">target variable.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.271.1">An exemplary application of</span><a id="_idIndexMarker145" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.272.1"> chi-squared feature selection lies in text classification, particularly in scenarios where the presence or absence of specific words in a document serves as features. </span><span class="kobospan" id="kobo.272.2">The chi-squared test helps identify words strongly associated with a particular class or category of documents, subsequently enabling their use as features in a ML model. </span><span class="kobospan" id="kobo.272.3">In categorical data, especially where the relationship between features and the target variable is non-linear, chi-squared proves to be a valuable method for feature selection. </span><span class="kobospan" id="kobo.272.4">However, its suitability diminishes for continuous or highly correlated features, where alternative feature selection methods may be </span><span><span class="kobospan" id="kobo.273.1">more fitting.</span></span></p>
<h4 class="calibre135"><span class="kobospan" id="kobo.274.1">Mutual information</span></h4>
<p class="calibre6"><span class="kobospan" id="kobo.275.1">Mutual information acts as </span><a id="_idIndexMarker146" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.276.1">a metric to gauge the interdependence of two random variables. </span><span class="kobospan" id="kobo.276.2">In the context of feature selection, it quantifies the information a feature provides about the target variable. </span><span class="kobospan" id="kobo.276.3">The core methodology entails calculating the mutual information between each feature and the target variable, ultimately selecting features with the highest mutual </span><span><span class="kobospan" id="kobo.277.1">information scores.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.278.1">Mathematically, the mutual information between two discrete random variables, </span><em class="italic"><span class="kobospan" id="kobo.279.1">X</span></em><span class="kobospan" id="kobo.280.1"> and </span><em class="italic"><span class="kobospan" id="kobo.281.1">Y</span></em><span class="kobospan" id="kobo.282.1">, can be defined </span><span><span class="kobospan" id="kobo.283.1">as follows:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.284.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi&gt;I&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;mml:mo&gt;;&lt;/mml:mo&gt;&lt;mml:mi&gt;Y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:munder&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;∈&lt;/mml:mo&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:munder&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;∈&lt;/mml:mo&gt;&lt;mml:mi&gt;Y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;log&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;⁡&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" src="image/133.png" class="calibre139"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.285.1">In the given equation, </span><em class="italic"><span class="kobospan" id="kobo.286.1">p(x, y)</span></em><span class="kobospan" id="kobo.287.1"> represents the joint probability mass function of </span><em class="italic"><span class="kobospan" id="kobo.288.1">X</span></em><span class="kobospan" id="kobo.289.1"> and </span><em class="italic"><span class="kobospan" id="kobo.290.1">Y</span></em><span class="kobospan" id="kobo.291.1">, while </span><em class="italic"><span class="kobospan" id="kobo.292.1">p(x)</span></em><span class="kobospan" id="kobo.293.1"> and </span><em class="italic"><span class="kobospan" id="kobo.294.1">p(y)</span></em><span class="kobospan" id="kobo.295.1"> denote the marginal probability mass functions of </span><em class="italic"><span class="kobospan" id="kobo.296.1">X</span></em><span class="kobospan" id="kobo.297.1"> and </span><span><em class="italic"><span class="kobospan" id="kobo.298.1">Y</span></em></span><span><span class="kobospan" id="kobo.299.1">, respectively.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.300.1">In the context of feature selection, mutual information calculation involves treating the feature as </span><em class="italic"><span class="kobospan" id="kobo.301.1">X</span></em><span class="kobospan" id="kobo.302.1"> and the </span><a id="_idIndexMarker147" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.303.1">target variable as </span><em class="italic"><span class="kobospan" id="kobo.304.1">Y</span></em><span class="kobospan" id="kobo.305.1">. </span><span class="kobospan" id="kobo.305.2">By computing the mutual information score for each feature, we can then select features with the </span><span><span class="kobospan" id="kobo.306.1">highest scores.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.307.1">To estimate the probability mass functions needed for calculating mutual information, histogram-based methods can be employed. </span><span class="kobospan" id="kobo.307.2">This involves dividing the range of each variable into a fixed number of bins and estimating the probability mass functions based on the frequencies of observations in each bin. </span><span class="kobospan" id="kobo.307.3">Alternatively, kernel density estimation can be utilized to estimate the probability density functions, and mutual information can then be computed based on the </span><span><span class="kobospan" id="kobo.308.1">estimated densities.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.309.1">In practical applications, mutual information is often employed alongside other feature selection methods, such as chi-squared or correlation-based methods, to enhance the overall performance of the feature </span><span><span class="kobospan" id="kobo.310.1">selection process.</span></span></p>
<h4 class="calibre135"><span class="kobospan" id="kobo.311.1">Correlation coefficients</span></h4>
<p class="calibre6"><span class="kobospan" id="kobo.312.1">Correlation coefficients serve as</span><a id="_idIndexMarker148" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.313.1"> indicators of the strength and direction of the linear relationship between two variables. </span><span class="kobospan" id="kobo.313.2">In the realm of feature selection, these coefficients prove useful in identifying features highly correlated with the target variable, thus serving as potentially </span><span><span class="kobospan" id="kobo.314.1">valuable predictors.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.315.1">The prevalent correlation coefficient employed for feature selection is the Pearson correlation coefficient, also referred to as</span><a id="_idIndexMarker149" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.316.1"> Pearson’s </span><em class="italic"><span class="kobospan" id="kobo.317.1">r</span></em><span class="kobospan" id="kobo.318.1">. </span><span class="kobospan" id="kobo.318.2">Pearson’s r measures the linear relationship between two continuous variables, ranging from -1 (indicating a perfect negative correlation) to 1 (indicating a perfect positive correlation), with 0 denoting no correlation. </span><span class="kobospan" id="kobo.318.3">Its calculation involves dividing the covariance between the two variables by the product of their standard deviations, as depicted in the </span><span><span class="kobospan" id="kobo.319.1">following equation:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.320.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;Y&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;Y&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/134.png" class="calibre140"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.321.1">In the given equation, </span><em class="italic"><span class="kobospan" id="kobo.322.1">X</span></em><span class="kobospan" id="kobo.323.1"> and </span><em class="italic"><span class="kobospan" id="kobo.324.1">Y</span></em><span class="kobospan" id="kobo.325.1"> represent the two variables of interest, </span><em class="italic"><span class="kobospan" id="kobo.326.1">cov()</span></em><span class="kobospan" id="kobo.327.1"> denotes the covariance function, and </span><em class="italic"><span class="kobospan" id="kobo.328.1">std()</span></em><span class="kobospan" id="kobo.329.1"> represents the standard </span><span><span class="kobospan" id="kobo.330.1">deviation function.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.331.1">Utilizing Pearson’s </span><em class="italic"><span class="kobospan" id="kobo.332.1">r</span></em><span class="kobospan" id="kobo.333.1"> for feature selection involves computing the correlation between each feature and the target variable. </span><span class="kobospan" id="kobo.333.2">Features with the highest absolute correlation coefficients are then selected. </span><span class="kobospan" id="kobo.333.3">A high absolute correlation coefficient signifies a strong correlation with the target variable, whether positive or negative. </span><span class="kobospan" id="kobo.333.4">The interpretation of Pearson correlation values and their degree of correlation is outlined in </span><span><em class="italic"><span class="kobospan" id="kobo.334.1">Table 3.1</span></em></span><span><span class="kobospan" id="kobo.335.1">:</span></span></p>
<table class="no-table-style" id="table001-2">
<colgroup class="calibre10">
<col class="calibre11"/>
<col class="calibre11"/>
</colgroup>
<thead class="calibre12">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><strong class="bold"><span class="kobospan" id="kobo.336.1">Pearson </span></strong><span><strong class="bold"><span class="kobospan" id="kobo.337.1">Correlation Value</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold"><span class="kobospan" id="kobo.338.1">Degree </span></strong><span><strong class="bold"><span class="kobospan" id="kobo.339.1">of Correlation</span></strong></span></p>
</td>
</tr>
</thead>
<tbody class="calibre13">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><span class="kobospan" id="kobo.340.1">± 1</span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.341.1">Perfect</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><span class="kobospan" id="kobo.342.1">± 0.50 - ± 1</span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.343.1">High degree</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><span class="kobospan" id="kobo.344.1">± 0.30 - ± </span><span><span class="kobospan" id="kobo.345.1">0.49</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.346.1">Moderate degree</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><span class="kobospan" id="kobo.347.1">&lt; +</span><span><span class="kobospan" id="kobo.348.1">0.29</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.349.1">Low degree</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><span class="kobospan" id="kobo.350.1">0</span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.351.1">No correlation</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.352.1">Table 3 .1 – Pearson correlation values and their degree of correlation</span></p>
<p class="calibre6"><span class="kobospan" id="kobo.353.1">It’s worth noting that Pearson’s </span><em class="italic"><span class="kobospan" id="kobo.354.1">r</span></em><span class="kobospan" id="kobo.355.1"> is only appropriate for identifying linear relationships between variables. </span><span class="kobospan" id="kobo.355.2">If the relationship is nonlinear, or if one or both of the variables are categorical, other </span><a id="_idIndexMarker150" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.356.1">correlation coefficients such as Spearman’s </span><span class="kobospan" id="kobo.357.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;ρ&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/135.png" class="calibre141"/></span><span class="kobospan" id="kobo.358.1">or Kendall’s </span><span class="kobospan" id="kobo.359.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;τ&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/136.png" class="calibre142"/></span><span class="kobospan" id="kobo.360.1"> may be more appropriate. </span><span class="kobospan" id="kobo.360.2">Additionally, it is important to be cautious when interpreting correlation coefficients as a high correlation does not necessarily </span><span><span class="kobospan" id="kobo.361.1">imply causation.</span></span><a id="_idTextAnchor062" class="calibre5 pcalibre1 pcalibre"/></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.362.1">Wrapper methods</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.363.1">These techniques delve into</span><a id="_idIndexMarker151" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.364.1"> subsets of features through iterative model training and testing. </span><span class="kobospan" id="kobo.364.2">Widely known methods encompass forward selection, backward elimination, and recursive feature elimination. </span><span class="kobospan" id="kobo.364.3">While computationally demanding, these methods have the potential to significantly enhance </span><span><span class="kobospan" id="kobo.365.1">model accuracy.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.366.1">A concrete illustration of a wrapper method is </span><strong class="bold"><span class="kobospan" id="kobo.367.1">recursive feature elimination</span></strong><span class="kobospan" id="kobo.368.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.369.1">RFE</span></strong><span class="kobospan" id="kobo.370.1">). </span><span class="kobospan" id="kobo.370.2">Functioning as a backward elimination approach, RFE systematically removes the least important </span><a id="_idIndexMarker152" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.371.1">feature until a predetermined number of features remains. </span><span class="kobospan" id="kobo.371.2">During each iteration, a machine learning model is trained on the existing</span><a id="_idIndexMarker153" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.372.1"> features, and the least important feature is pruned based on its feature importance score. </span><span class="kobospan" id="kobo.372.2">This sequential process persists until the specified number of features is attained. </span><span class="kobospan" id="kobo.372.3">The feature importance score can be extracted from diverse methods, including coefficient values from linear models or feature importance scores derived from decision trees. </span><span class="kobospan" id="kobo.372.4">RFE is a computationally expensive method, but it can be useful when the number of features is very large and there is a need to reduce the feature space. </span><span class="kobospan" id="kobo.372.5">An alternative approach is to have feature selection during the training process, something that’s done via </span><span><span class="kobospan" id="kobo.373.1">embedding methods.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.374.1">Embedded methods</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.375.1">These methods select features</span><a id="_idIndexMarker154" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.376.1"> during the training process of the model. </span><span class="kobospan" id="kobo.376.2">Popular methods include LASSO and ridge regression, decision trees, and </span><span><span class="kobospan" id="kobo.377.1">random forests.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.378.1">LASSO</span></h3>
<p class="calibre6"><strong class="bold"><span class="kobospan" id="kobo.379.1">LASSO</span></strong><span class="kobospan" id="kobo.380.1">, an acronym for </span><strong class="bold"><span class="kobospan" id="kobo.381.1">Least Absolute Shrinkage and Selection Operator</span></strong><span class="kobospan" id="kobo.382.1">, serves as a linear regression technique that’s commonly employed for feature selection in machine learning. </span><span class="kobospan" id="kobo.382.2">Its</span><a id="_idIndexMarker155" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.383.1"> mechanism involves introducing a penalty term to the standard regression loss function. </span><span class="kobospan" id="kobo.383.2">This penalty encourages the model to reduce the coefficients of less important features to zero, effectively eliminating </span><a id="_idIndexMarker156" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.384.1">them from </span><span><span class="kobospan" id="kobo.385.1">the model.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.386.1">The LASSO method proves especially valuable when grappling with high-dimensional data, where the number of features far exceeds the number of samples. </span><span class="kobospan" id="kobo.386.2">In such scenarios, discerning the most crucial features for predicting the target variable can be challenging. </span><span class="kobospan" id="kobo.386.3">LASSO comes to the fore by automatically identifying the most relevant features while simultaneously shrinking the coefficients </span><span><span class="kobospan" id="kobo.387.1">of others.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.388.1">The LASSO method works by finding the solution for the following optimization problem, which is a </span><span><span class="kobospan" id="kobo.389.1">minimization problem:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.390.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;min&lt;/mi&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;w&lt;/mi&gt;&lt;/munder&gt;&lt;msubsup&gt;&lt;mfenced open=&quot;‖&quot; close=&quot;‖&quot;&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;y&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;X&lt;/mi&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;w&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msubsup&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;λ&lt;/mi&gt;&lt;msub&gt;&lt;mfenced open=&quot;‖&quot; close=&quot;‖&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;w&lt;/mi&gt;&lt;/mfenced&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/137.png" class="calibre143"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.391.1">In the given equation, vector </span><em class="italic"><span class="kobospan" id="kobo.392.1">y</span></em><span class="kobospan" id="kobo.393.1"> represents the target variable, </span><em class="italic"><span class="kobospan" id="kobo.394.1">X</span></em><span class="kobospan" id="kobo.395.1"> denotes the feature matrix, </span><em class="italic"><span class="kobospan" id="kobo.396.1">w</span></em><span class="kobospan" id="kobo.397.1"> signifies the vector of regression coefficients, </span><span class="kobospan" id="kobo.398.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;λ&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/138.png" class="calibre144"/></span><span class="kobospan" id="kobo.399.1"> is a hyperparameter dictating the intensity of the penalty term, and </span><span class="kobospan" id="kobo.400.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;w&lt;/mml:mi&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/139.png" class="calibre145"/></span><span class="kobospan" id="kobo.401.1"> stands for the </span><span class="kobospan" id="kobo.402.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;script&quot;&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/140.png" class="calibre146"/></span><span class="kobospan" id="kobo.403.1">norm of the coefficients (that is, the sum of their </span><span><span class="kobospan" id="kobo.404.1">absolute values).</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.405.1">The inclusion of the </span><span class="kobospan" id="kobo.406.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;script&quot;&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/141.png" class="calibre147"/></span><span class="kobospan" id="kobo.407.1"> penalty term in the objective function prompts the model to precisely zero out certain coefficients, essentially eliminating the associated features from the model. </span><span class="kobospan" id="kobo.407.2">The degree of penalty strength is governed by the </span><span class="kobospan" id="kobo.408.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;λ&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/142.png" class="calibre148"/></span><span class="kobospan" id="kobo.409.1"> hyperparameter, which can be fine-tuned through the use </span><span><span class="kobospan" id="kobo.410.1">of cross-validation.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.411.1">LASSO has several advantages over other feature selection methods, such as its ability to handle correlated features and its ability to perform feature selection and regression simultaneously. </span><span class="kobospan" id="kobo.411.2">However, LASSO has some limitations, such as its tendency to select only one feature from a group of correlated features, and its performance may deteriorate if the number of features is much larger than the number </span><span><span class="kobospan" id="kobo.412.1">of samples.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.413.1">Consider the application of LASSO for feature selection in predicting house prices. </span><span class="kobospan" id="kobo.413.2">Imagine a dataset encompassing details about houses – such as the number of bedrooms, lot size, construction year, and so on – alongside their respective sale prices. </span><span class="kobospan" id="kobo.413.3">Employing LASSO, we </span><a id="_idIndexMarker157" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.414.1">can pinpoint the most crucial features to predict the sale price while concurrently fitting a linear regression model to the dataset. </span><span class="kobospan" id="kobo.414.2">The </span><a id="_idIndexMarker158" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.415.1">outcome is a model that’s ready to forecast the sale price of a new house based on </span><span><span class="kobospan" id="kobo.416.1">its features.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.417.1">Ridge regression</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.418.1">Ridge regression, a linear </span><a id="_idIndexMarker159" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.419.1">regression method applicable to feature selection, closely resembles ordinary least squares regression but introduces a penalty term to the cost function to </span><span><span class="kobospan" id="kobo.420.1">counter overfitting.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.421.1">In ridge regression, the cost function </span><a id="_idIndexMarker160" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.422.1">undergoes modification with the inclusion of a penalty term directly proportional to the square of the coefficients’ magnitude. </span><span class="kobospan" id="kobo.422.2">This penalty term is regulated by a hyperparameter, often denoted as </span><span class="kobospan" id="kobo.423.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;λ&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/138.png" class="calibre149"/></span><span class="kobospan" id="kobo.424.1"> or </span><span class="kobospan" id="kobo.425.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;α&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/144.png" class="calibre150"/></span><span class="kobospan" id="kobo.426.1"> dictating the regularization strength. </span><span class="kobospan" id="kobo.426.2">When </span><span class="kobospan" id="kobo.427.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;α&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/144.png" class="calibre150"/></span><span class="kobospan" id="kobo.428.1"> is set to zero, ridge regression reverts to ordinary least </span><span><span class="kobospan" id="kobo.429.1">squares regression.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.430.1">The penalty term’s impact manifests in shrinking the coefficients’ magnitude toward zero. </span><span class="kobospan" id="kobo.430.2">This proves beneficial in mitigating overfitting, discouraging the model from excessively relying on any single feature. </span><span class="kobospan" id="kobo.430.3">In effect, the penalty term acts as a form of feature selection by reducing the importance of less </span><span><span class="kobospan" id="kobo.431.1">relevant features.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.432.1">The equation for the ridge regression loss function is </span><span><span class="kobospan" id="kobo.433.1">as follows:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.434.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;min&lt;/mi&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;w&lt;/mi&gt;&lt;/munder&gt;&lt;msubsup&gt;&lt;mfenced open=&quot;‖&quot; close=&quot;‖&quot;&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;y&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;X&lt;/mi&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;w&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msubsup&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;α&lt;/mi&gt;&lt;msub&gt;&lt;mfenced open=&quot;‖&quot; close=&quot;‖&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;w&lt;/mi&gt;&lt;/mfenced&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/146.png" class="calibre151"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.435.1">Here, we have </span><span><span class="kobospan" id="kobo.436.1">the following:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><em class="italic"><span class="kobospan" id="kobo.437.1">N</span></em><span class="kobospan" id="kobo.438.1"> is the number of samples in the </span><span><span class="kobospan" id="kobo.439.1">training set.</span></span></li>
<li class="calibre15"><em class="italic"><span class="kobospan" id="kobo.440.1">y</span></em><span class="kobospan" id="kobo.441.1"> is the column vector of target values of </span><span><span class="kobospan" id="kobo.442.1">size </span></span><span><em class="italic"><span class="kobospan" id="kobo.443.1">N</span></em></span><span><span class="kobospan" id="kobo.444.1">.</span></span></li>
<li class="calibre15"><em class="italic"><span class="kobospan" id="kobo.445.1">X</span></em><span class="kobospan" id="kobo.446.1"> is the design matrix of </span><span><span class="kobospan" id="kobo.447.1">input features.</span></span></li>
<li class="calibre15"><em class="italic"><span class="kobospan" id="kobo.448.1">w</span></em><span class="kobospan" id="kobo.449.1"> is the vector of regression coefficients to </span><span><span class="kobospan" id="kobo.450.1">be estimated.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.451.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;α&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/147.png" class="calibre152"/></span><span class="kobospan" id="kobo.452.1"> is the regularization parameter that controls the strength of the penalty term. </span><span class="kobospan" id="kobo.452.2">It is a hyperparameter that needs to </span><span><span class="kobospan" id="kobo.453.1">be tuned.</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.454.1">The first term in the loss function measures the mean squared error between the predicted values and the true values. </span><span class="kobospan" id="kobo.454.2">The second term is the </span><span class="kobospan" id="kobo.455.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;script&quot;&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/148.png" class="calibre153"/></span><span class="kobospan" id="kobo.456.1"> penalty term that shrinks the coefficients toward zero. </span><span class="kobospan" id="kobo.456.2">The ridge regression algorithm finds the values of the regression coefficients that </span><a id="_idIndexMarker161" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.457.1">minimize this loss function. </span><span class="kobospan" id="kobo.457.2">By tuning the regularization parameter, </span><span class="kobospan" id="kobo.458.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;α&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/149.png" class="calibre154"/></span><span class="kobospan" id="kobo.459.1">, we can control the bias-variance trade-off </span><a id="_idIndexMarker162" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.460.1">of the model, with higher alpha values leading to more regularization and </span><span><span class="kobospan" id="kobo.461.1">lower overfitting.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.462.1">Ridge regression can be used for feature selection by examining the magnitudes of the coefficients produced by the model. </span><span class="kobospan" id="kobo.462.2">Features with coefficients that are close to zero or smaller are considered less important and can be dropped from the model. </span><span class="kobospan" id="kobo.462.3">The value of </span><span class="kobospan" id="kobo.463.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;α&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/150.png" class="calibre155"/></span><span class="kobospan" id="kobo.464.1"> can be tuned using cross-validation to find the optimal balance between model complexity </span><span><span class="kobospan" id="kobo.465.1">and accuracy.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.466.1">One of the main advantages of ridge regression is its ability to handle multicollinearity, which occurs when there are strong correlations between the independent variables. </span><span class="kobospan" id="kobo.466.2">In such cases, ordinary least squares regression can produce unstable and unreliable coefficient estimates, but ridge regression can help stabilize the estimates and improve the overall performance of </span><span><span class="kobospan" id="kobo.467.1">the model.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.468.1">Choosing LASSO or ridge regression</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.469.1">Ridge regression and LASSO </span><a id="_idIndexMarker163" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.470.1">are both regularization techniques that are used in linear regression to prevent overfitting of the model by </span><a id="_idIndexMarker164" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.471.1">penalizing the model’s coefficients. </span><span class="kobospan" id="kobo.471.2">While both methods seek to prevent overfitting, they differ in their approach to how the coefficients </span><span><span class="kobospan" id="kobo.472.1">are penalized.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.473.1">Ridge regression adds a penalty term to the </span><strong class="bold"><span class="kobospan" id="kobo.474.1">sum of squared errors</span></strong><span class="kobospan" id="kobo.475.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.476.1">SSE</span></strong><span class="kobospan" id="kobo.477.1">) that is proportional to the square of the magnitude of the coefficients. </span><span class="kobospan" id="kobo.477.2">The penalty term is controlled by a regularization parameter (</span><span class="kobospan" id="kobo.478.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;α&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/151.png" class="calibre156"/></span><span class="kobospan" id="kobo.479.1">), which determines the amount of shrinkage applied to the coefficients. </span><span class="kobospan" id="kobo.479.2">This penalty term shrinks the values of the coefficients toward zero but does not set them exactly to zero. </span><span class="kobospan" id="kobo.479.3">Therefore, ridge regression can be used to reduce the impact of irrelevant features in a model, but it will not eliminate </span><span><span class="kobospan" id="kobo.480.1">them completely.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.481.1">On the other hand, LASSO also adds a penalty term to the SSE, but the penalty term is proportional to the absolute value of the coefficients. </span><span class="kobospan" id="kobo.481.2">Like ridge, LASSO also has a regularization parameter (</span><span class="kobospan" id="kobo.482.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;λ&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/152.png" class="calibre157"/></span><span class="kobospan" id="kobo.483.1">) that determines the amount of shrinkage applied to the coefficients. </span><span class="kobospan" id="kobo.483.2">However, LASSO has a unique property of setting some of the coefficients exactly to zero when the regularization parameter is sufficiently high. </span><span class="kobospan" id="kobo.483.3">Therefore, LASSO can be used for feature selection as it can eliminate irrelevant features and set their corresponding coefficients </span><span><span class="kobospan" id="kobo.484.1">to zero.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.485.1">In general, if the dataset has many</span><a id="_idIndexMarker165" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.486.1"> features and a small number of them are expected to be important, LASSO regression is a better choice as it will set the coefficients of irrelevant features to zero, leading to a simpler and more interpretable model. </span><span class="kobospan" id="kobo.486.2">On the other hand, if most of the features in the dataset are expected to be relevant, ridge regression is a better choice as it will shrink the coefficients toward zero but not set them exactly to zero, preserving all the features in </span><span><span class="kobospan" id="kobo.487.1">the model.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.488.1">However, it is important to note that the optimal choice between ridge and LASSO depends on the specific problem and dataset, and it is often recommended to try both and compare their performance using </span><span><span class="kobospan" id="kobo.489.1">cross-validat</span><a id="_idTextAnchor063" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.490.1">ion techniques.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.491.1">Dimensionality reduction techniques</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.492.1">These methods transform the features into </span><a id="_idIndexMarker166" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.493.1">a lower-dimensional space while retaining as much information as possible. </span><span class="kobospan" id="kobo.493.2">Popular </span><a id="_idIndexMarker167" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.494.1">methods include PCA, </span><strong class="bold"><span class="kobospan" id="kobo.495.1">linear discriminant analysis</span></strong><span class="kobospan" id="kobo.496.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.497.1">LDA</span></strong><span class="kobospan" id="kobo.498.1">), </span><span><span class="kobospan" id="kobo.499.1">and </span></span><span><strong class="bold"><span class="kobospan" id="kobo.500.1">t-</span></strong></span><span><span class="kobospan" id="kobo.501.1">SNE.</span></span></p>
<h4 class="calibre135"><span class="kobospan" id="kobo.502.1">PCA</span></h4>
<p class="calibre6"><span class="kobospan" id="kobo.503.1">PCA is a widely used technique</span><a id="_idIndexMarker168" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.504.1"> in machine learning for reducing the dimensionality of large datasets while retaining most of the important information. </span><span class="kobospan" id="kobo.504.2">The basic idea of PCA is to transform a set of </span><a id="_idIndexMarker169" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.505.1">correlated variables into a set of uncorrelated variables known as </span><span><span class="kobospan" id="kobo.506.1">principal components.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.507.1">The goal of PCA is to identify the directions of maximum variance in the data and project the data in these directions, reducing the dimensionality of the data. </span><span class="kobospan" id="kobo.507.2">The principal components are sorted in order of the amount of variance they explain, with the first principal component explaining the most variance in </span><span><span class="kobospan" id="kobo.508.1">the data.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.509.1">The PCA algorithm involves the </span><span><span class="kobospan" id="kobo.510.1">following steps:</span></span></p>
<ol class="calibre16">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.511.1">Standardize the data</span></strong><span class="kobospan" id="kobo.512.1">: PCA requires the data to be standardized – that is, each feature must have zero mean and </span><span><span class="kobospan" id="kobo.513.1">unit variance.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.514.1">Compute the covariance matrix</span></strong><span class="kobospan" id="kobo.515.1">: The covariance matrix is a square matrix that measures the linear relationships between pairs of features in </span><span><span class="kobospan" id="kobo.516.1">the data.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.517.1">Compute the eigenvectors and eigenvalues of the covariance matrix</span></strong><span class="kobospan" id="kobo.518.1">: The eigenvectors represent the primary directions of the highest variance within the dataset, while the eigenvalues quantify the extent of variance elucidated by </span><span><span class="kobospan" id="kobo.519.1">each eigenvector.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.520.1">Select the number of principal components</span></strong><span class="kobospan" id="kobo.521.1">: The number of principal components to retain can be determined by analyzing the eigenvalues and selecting the top </span><em class="italic"><span class="kobospan" id="kobo.522.1">k</span></em><span class="kobospan" id="kobo.523.1"> eigenvectors that explain the </span><span><span class="kobospan" id="kobo.524.1">most variance.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.525.1">Project the data onto the selected principal components</span></strong><span class="kobospan" id="kobo.526.1">: The original data is projected onto the selected principal components, resulting in a lower-dimensional representation of </span><span><span class="kobospan" id="kobo.527.1">the data.</span></span></li>
</ol>
<p class="calibre6"><span class="kobospan" id="kobo.528.1">PCA can be used for feature</span><a id="_idIndexMarker170" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.529.1"> selection by selecting the top </span><em class="italic"><span class="kobospan" id="kobo.530.1">k</span></em><span class="kobospan" id="kobo.531.1"> principal components that explain the most variance in the data. </span><span class="kobospan" id="kobo.531.2">This can be useful for reducing the dimensionality of high-dimensional datasets and improving the performance of machine learning models. </span><span class="kobospan" id="kobo.531.3">However, it’s important to note that PCA may not always lead to improved performance, especially if the data is already low-dimensional or if the features are not highly correlated. </span><span class="kobospan" id="kobo.531.4">It’s also important to consider the interpretability of the selected principal components as they may not always correspond to meaningful features in </span><span><span class="kobospan" id="kobo.532.1">the data.</span></span></p>
<h4 class="calibre135"><span class="kobospan" id="kobo.533.1">LDA</span></h4>
<p class="calibre6"><span class="kobospan" id="kobo.534.1">LDA is a dimensionality </span><a id="_idIndexMarker171" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.535.1">reduction technique that’s used for </span><a id="_idIndexMarker172" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.536.1">feature selection in machine learning. </span><span class="kobospan" id="kobo.536.2">It is often used in classification tasks to reduce the number of features by transforming them into a lower-dimensional space while retaining as much class-discriminatory information </span><span><span class="kobospan" id="kobo.537.1">as possible.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.538.1">In LDA, the goal is to find a linear combination of the original features that maximizes the separation between classes. </span><span class="kobospan" id="kobo.538.2">The input to LDA is a dataset of labeled examples, where each example is a feature vector with a corresponding class label. </span><span class="kobospan" id="kobo.538.3">The output of LDA is a set of linear combinations of the original features, which can be used as new features in a machine </span><span><span class="kobospan" id="kobo.539.1">learning model.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.540.1">To perform LDA, the first step is to compute the mean and covariance matrix of each class. </span><span class="kobospan" id="kobo.540.2">The overall mean and covariance matrix are then calculated from the class means and covariance matrices. </span><span class="kobospan" id="kobo.540.3">The goal is to project the data onto a lower-dimensional space while </span><a id="_idIndexMarker173" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.541.1">still retaining the class information. </span><span class="kobospan" id="kobo.541.2">This is achieved by finding the eigenvectors and eigenvalues of the covariance matrix, sorting them in descending order of the eigenvalues, and selecting the top </span><em class="italic"><span class="kobospan" id="kobo.542.1">k</span></em><span class="kobospan" id="kobo.543.1"> eigenvectors that correspond to the </span><em class="italic"><span class="kobospan" id="kobo.544.1">k</span></em><span class="kobospan" id="kobo.545.1"> largest eigenvalues. </span><span class="kobospan" id="kobo.545.2">The selected eigenvectors form the basis for the new </span><span><span class="kobospan" id="kobo.546.1">feature space.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.547.1">The LDA algorithm can be summarized in the </span><span><span class="kobospan" id="kobo.548.1">following steps:</span></span></p>
<ol class="calibre16">
<li class="calibre15"><span class="kobospan" id="kobo.549.1">Compute the mean vector of </span><span><span class="kobospan" id="kobo.550.1">each class.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.551.1">Compute the covariance matrix of </span><span><span class="kobospan" id="kobo.552.1">each class.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.553.1">Compute the overall mean vector and overall </span><span><span class="kobospan" id="kobo.554.1">covariance matrix.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.555.1">Compute the between-class </span><span><span class="kobospan" id="kobo.556.1">scatter matrix.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.557.1">Compute the within-class </span><span><span class="kobospan" id="kobo.558.1">scatter matrix.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.559.1">Compute the eigenvectors and eigenvalues of the matrix using the </span><span><span class="kobospan" id="kobo.560.1">following equation:</span></span></li>
</ol>
<p class="calibre6"><span class="kobospan" id="kobo.561.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;S&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;*&lt;/mml:mi&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;S&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/153.png" class="calibre158"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.562.1">Here, </span><span class="kobospan" id="kobo.563.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;S&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/154.png" class="calibre159"/></span><span class="kobospan" id="kobo.564.1"> is the within-class scatter matrix and </span><span class="kobospan" id="kobo.565.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;S&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/155.png" class="calibre160"/></span><span class="kobospan" id="kobo.566.1"> is the between-class </span><span><span class="kobospan" id="kobo.567.1">scatter matrix.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.568.1">7.	</span><span class="kobospan" id="kobo.568.2">Select the top </span><em class="italic"><span class="kobospan" id="kobo.569.1">k</span></em><span class="kobospan" id="kobo.570.1"> eigenvectors with the highest eigenvalues as the new </span><span><span class="kobospan" id="kobo.571.1">feature space.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.572.1">LDA is particularly useful when the number of features is large and the number of examples is small. </span><span class="kobospan" id="kobo.572.2">It can be used in a </span><a id="_idIndexMarker174" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.573.1">variety of applications, including image recognition, speech</span><a id="_idIndexMarker175" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.574.1"> recognition, and NLP. </span><span class="kobospan" id="kobo.574.2">However, it assumes that the classes are normally distributed and that the class covariance matrices are equal, which may not always be the case </span><span><span class="kobospan" id="kobo.575.1">in practice.</span></span></p>
<h4 class="calibre135"><span class="kobospan" id="kobo.576.1">t-SNE</span></h4>
<p class="calibre6"><span class="kobospan" id="kobo.577.1">t-SNE is a dimensionality</span><a id="_idIndexMarker176" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.578.1"> reduction technique that’s used for visualizing high-dimensional data in a low-dimensional space, often used for feature selection. </span><span class="kobospan" id="kobo.578.2">It was developed by Laurens van der Maaten and Geoffrey </span><a id="_idIndexMarker177" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.579.1">Hinton </span><span><span class="kobospan" id="kobo.580.1">in 2008.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.581.1">The basic idea behind t-SNE is to preserve the pairwise similarities of data points in a low-dimensional space, as opposed to preserving the distances between them. </span><span class="kobospan" id="kobo.581.2">In other words, it tries to retain the local structure of the data while discarding the global structure. </span><span class="kobospan" id="kobo.581.3">This can be useful in situations where the high-dimensional data is difficult to visualize, but there may be meaningful patterns and relationships among the </span><span><span class="kobospan" id="kobo.582.1">data points.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.583.1">t-SNE starts by calculating the pairwise similarity between each pair of data points in the high-dimensional space. </span><span class="kobospan" id="kobo.583.2">The similarity is usually measured using a Gaussian kernel, which gives higher weights to nearby points and lower weights to distant points. </span><span class="kobospan" id="kobo.583.3">The similarity matrix is then converted into a probability distribution using a softmax function. </span><span class="kobospan" id="kobo.583.4">This distribution is used to create a low-dimensional space, typically 2D </span><span><span class="kobospan" id="kobo.584.1">or 3D.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.585.1">In the low-dimensional space, t-SNE again calculates the pairwise similarities between each pair of data points, but this time using a student’s t-distribution instead of a Gaussian distribution. </span><span class="kobospan" id="kobo.585.2">The t-distribution has heavier tails than the Gaussian distribution, which helps to better preserve the local structure of the data. </span><span class="kobospan" id="kobo.585.3">t-SNE then adjusts the position of the points in the low-dimensional space to minimize the difference between the pairwise similarities in the high-dimensional space and the pairwise similarities in the </span><span><span class="kobospan" id="kobo.586.1">low-dimensional space.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.587.1">t-SNE is a powerful technique for visualizing high-dimensional data by reducing it to a low-dimensional space. </span><span class="kobospan" id="kobo.587.2">However, it is not typically used for feature selection as its primary purpose is to create visualizations of </span><span><span class="kobospan" id="kobo.588.1">complex datasets.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.589.1">Instead, t-SNE can be</span><a id="_idIndexMarker178" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.590.1"> used to help identify clusters of data points that share similar features, which may be useful in identifying </span><a id="_idIndexMarker179" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.591.1">groups of features that are important for a particular task. </span><span class="kobospan" id="kobo.591.2">For example, suppose you have a dataset of customer demographics and purchase history, and you want to identify groups of customers that are similar based on their purchasing behavior. </span><span class="kobospan" id="kobo.591.3">You could use t-SNE to reduce the high-dimensional feature space to two dimensions, and then plot the resulting data points on a scatter plot. </span><span class="kobospan" id="kobo.591.4">By examining the plot, you might be able to identify clusters of customers with similar purchasing behavior, which could then inform your feature selection process. </span><span class="kobospan" id="kobo.591.5">Here’s a sample t-SNE for the </span><span><span class="kobospan" id="kobo.592.1">MNIST dataset:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer168">
<span class="kobospan" id="kobo.593.1"><img alt="Figure 3.1 – t-SNE on the MNIST dataset" src="image/B18949_03_1.jpg" class="calibre3"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.594.1">Figure 3.1 – t-SNE on the MNIST dataset</span></p>
<p class="calibre6"><span class="kobospan" id="kobo.595.1">It’s worth noting that t-SNE is primarily a visualization tool and should not be used as the sole method for feature selection. </span><span class="kobospan" id="kobo.595.2">Instead, it can be used in conjunction with other techniques, such as</span><a id="_idIndexMarker180" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.596.1"> LDA or PCA, to gain a more complete understanding of the underlying structure of </span><span><span class="kobospan" id="kobo.597.1">your data.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.598.1">The choice of feature</span><a id="_idIndexMarker181" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.599.1"> selection method depends on the nature of the data, the size of the dataset, the complexity of the model, and the computational resources available. </span><span class="kobospan" id="kobo.599.2">It is important to carefully evaluate the performance of the model after feature selection to ensure that important information has not been lost. </span><span class="kobospan" id="kobo.599.3">Another important process is feature engineering, which is about transforming or selecting features fo</span><a id="_idTextAnchor064" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.600.1">r the machine </span><span><span class="kobospan" id="kobo.601.1">learning models.</span></span></p>
<h2 id="_idParaDest-51" class="calibre7"><a id="_idTextAnchor065" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.602.1">Feature engineering</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.603.1">Feature engineering is the </span><a id="_idIndexMarker182" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.604.1">process of selecting, transforming, and extracting features from raw data to improve the performance of machine learning models. </span><span class="kobospan" id="kobo.604.2">Features are the individual measurable properties or characteristics of the data that can be used to make predictions </span><span><span class="kobospan" id="kobo.605.1">or classifications.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.606.1">One common technique in </span><a id="_idIndexMarker183" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.607.1">feature engineering is feature selection, which involves selecting a subset of relevant features from the original dataset to improve the model’s accuracy and reduce its complexity. </span><span class="kobospan" id="kobo.607.2">This can be done through statistical methods such as correlation analysis or feature importance ranking using decision trees or </span><span><span class="kobospan" id="kobo.608.1">random forests.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.609.1">Another technique in feature engineering is feature extraction, which involves transforming the raw data into a new set of features that may be more useful for the model. </span><span class="kobospan" id="kobo.609.2">The primary distinction between feature selection and feature engineering lies in their approaches: while feature selection retains a subset of the original features without modifying the selected features, feature engineering algorithms reconfigure and transform the data into a new feature space. </span><span class="kobospan" id="kobo.609.3">Feature engineering can be done through techniques such as dimensionality reduction, PCA, or t-SNE. </span><span class="kobospan" id="kobo.609.4">Feature selection and extraction were explained in detail in the previous </span><span><span class="kobospan" id="kobo.610.1">subsection (3-1-3).</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.611.1">Feature scaling is another important technique in feature engineering that involves scaling the values of features to the same range, typically between 0 and 1 or -1 and 1. </span><span class="kobospan" id="kobo.611.2">This is done to prevent certain features from dominating others in the model and to ensure that the algorithm can converge quickly during training. </span><span class="kobospan" id="kobo.611.3">When the features in the dataset have different scales, this can lead to issues when using certain machine learning algorithms that are sensitive to the relative magnitudes of the features. </span><span class="kobospan" id="kobo.611.4">Feature scaling can help to address this problem by ensuring that all features are on a similar scale. </span><span class="kobospan" id="kobo.611.5">Common methods for feature scaling include min-max scaling, Z-score scaling, and scaling by the maximum </span><span><span class="kobospan" id="kobo.612.1">absolute value.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.613.1">There are several common methods for </span><span><span class="kobospan" id="kobo.614.1">feature scaling:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.615.1">Min-max scaling</span></strong><span class="kobospan" id="kobo.616.1">: Also known as normalization, this technique scales the values of the feature to be</span><a id="_idIndexMarker184" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.617.1"> between a specified range, typically between 0 and 1 (for regular machine </span><a id="_idIndexMarker185" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.618.1">learning models, and sometimes -1 and 1 for deep learning models). </span><span class="kobospan" id="kobo.618.2">The formula for min-max scaling is </span><span><span class="kobospan" id="kobo.619.1">shown here:</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.620.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/156.png" class="calibre161"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.621.1">Here, </span><em class="italic"><span class="kobospan" id="kobo.622.1">x</span></em><span class="kobospan" id="kobo.623.1"> is the original feature value, </span><em class="italic"><span class="kobospan" id="kobo.624.1">min(x)</span></em><span class="kobospan" id="kobo.625.1"> is the minimum value of the feature, and </span><em class="italic"><span class="kobospan" id="kobo.626.1">max(x)</span></em><span class="kobospan" id="kobo.627.1"> is the maximum value of </span><span><span class="kobospan" id="kobo.628.1">the feature.</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.629.1">Standardization</span></strong><span class="kobospan" id="kobo.630.1">: This</span><a id="_idIndexMarker186" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.631.1"> technique transforms the feature </span><a id="_idIndexMarker187" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.632.1">values to have a mean of 0 and a standard deviation of 1. </span><span class="kobospan" id="kobo.632.2">Standardization is less affected by outliers in the data than min-max scaling. </span><span class="kobospan" id="kobo.632.3">The formula for standardization is </span><span><span class="kobospan" id="kobo.633.1">shown here:</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.634.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/157.png" class="calibre162"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.635.1">Here, </span><em class="italic"><span class="kobospan" id="kobo.636.1">x</span></em><span class="kobospan" id="kobo.637.1"> is the original feature value, </span><em class="italic"><span class="kobospan" id="kobo.638.1">mean(x)</span></em><span class="kobospan" id="kobo.639.1"> is the mean of the feature, and </span><em class="italic"><span class="kobospan" id="kobo.640.1">std(x)</span></em><span class="kobospan" id="kobo.641.1"> is the standard deviation of </span><span><span class="kobospan" id="kobo.642.1">the feature.</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.643.1">Robust scaling</span></strong><span class="kobospan" id="kobo.644.1">: This technique is similar to standardization but uses the median and </span><strong class="bold"><span class="kobospan" id="kobo.645.1">interquartile range</span></strong><span class="kobospan" id="kobo.646.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.647.1">IQR</span></strong><span class="kobospan" id="kobo.648.1">) instead of </span><a id="_idIndexMarker188" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.649.1">the mean and standard deviation. </span><span class="kobospan" id="kobo.649.2">Robust scaling is useful when the data contains outliers that would </span><a id="_idIndexMarker189" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.650.1">significantly affect the mean and standard deviation. </span><span class="kobospan" id="kobo.650.2">The formula for robust scaling is </span><span><span class="kobospan" id="kobo.651.1">shown here:</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.652.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/158.png" class="calibre163"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.653.1">Here, </span><em class="italic"><span class="kobospan" id="kobo.654.1">x</span></em><span class="kobospan" id="kobo.655.1"> is the original feature value, </span><em class="italic"><span class="kobospan" id="kobo.656.1">median(x)</span></em><span class="kobospan" id="kobo.657.1"> is the median of the feature, </span><em class="italic"><span class="kobospan" id="kobo.658.1">Q1(x)</span></em><span class="kobospan" id="kobo.659.1"> is the first quartile of the</span><a id="_idIndexMarker190" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.660.1"> feature, and </span><em class="italic"><span class="kobospan" id="kobo.661.1">Q3(x)</span></em><span class="kobospan" id="kobo.662.1"> is the third quartile of </span><span><span class="kobospan" id="kobo.663.1">the feature.</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.664.1">Log transformation</span></strong><span class="kobospan" id="kobo.665.1">: This technique is used when the data is highly skewed or has a long tail. </span><span class="kobospan" id="kobo.665.2">By taking the logarithm of the feature values, the distribution can be </span><a id="_idIndexMarker191" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.666.1">made more normal or symmetric, which can</span><a id="_idIndexMarker192" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.667.1"> improve the performance of some machine learning algorithms. </span><span class="kobospan" id="kobo.667.2">The formula for log transformation is </span><span><span class="kobospan" id="kobo.668.1">shown here:</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.669.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/159.png" class="calibre164"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.670.1">Here, </span><em class="italic"><span class="kobospan" id="kobo.671.1">x</span></em><span class="kobospan" id="kobo.672.1"> is the original </span><span><span class="kobospan" id="kobo.673.1">feature value.</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.674.1">Power transformation</span></strong><span class="kobospan" id="kobo.675.1">: This technique is similar to log transformation but allows for a broader range of transformations. </span><span class="kobospan" id="kobo.675.2">The most common power transformation is the Box-Cox</span><a id="_idIndexMarker193" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.676.1"> transformation, which raises the feature values to a power that is determined using maximum likelihood estimation. </span><span class="kobospan" id="kobo.676.2">The formula for the Box-Cox transformation is </span><span><span class="kobospan" id="kobo.677.1">shown here:</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.678.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;λ&lt;/mi&gt;&lt;/msup&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;λ&lt;/mi&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/160.png" class="calibre165"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.679.1">Here, </span><em class="italic"><span class="kobospan" id="kobo.680.1">x</span></em><span class="kobospan" id="kobo.681.1"> is the original feature value, and </span><span class="kobospan" id="kobo.682.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;λ&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/161.png" class="calibre166"/></span><span class="kobospan" id="kobo.683.1"> is the power parameter that is estimated using </span><span><span class="kobospan" id="kobo.684.1">maximum likelihood.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.685.1">These are some of the most common methods for feature scaling in machine learning. </span><span class="kobospan" id="kobo.685.2">The choice of method depends on the distribution of the data, the machine learning algorithm being used, and the specific requirements of </span><span><span class="kobospan" id="kobo.686.1">the problem.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.687.1">One final technique in feature engineering is feature construction, which involves creating new features by combining or transforming existing ones. </span><span class="kobospan" id="kobo.687.2">This can be done through techniques such as polynomial expansion, logarithmic transformation, or </span><span><span class="kobospan" id="kobo.688.1">interaction terms.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.689.1">Polynomial expansion</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.690.1">Polynomial expansion is a</span><a id="_idIndexMarker194" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.691.1"> feature construction technique that involves creating new features by taking polynomial combinations of existing features. </span><span class="kobospan" id="kobo.691.2">This technique is commonly used in machine learning to model nonlinear relationships between features and the </span><span><span class="kobospan" id="kobo.692.1">target variable.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.693.1">The idea behind polynomial</span><a id="_idIndexMarker195" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.694.1"> expansion is to create new features by raising the existing features to different powers and taking their products. </span><span class="kobospan" id="kobo.694.2">For example, suppose we have a single feature, </span><em class="italic"><span class="kobospan" id="kobo.695.1">x</span></em><span class="kobospan" id="kobo.696.1">. </span><span class="kobospan" id="kobo.696.2">We can create new features by taking the square of </span><em class="italic"><span class="kobospan" id="kobo.697.1">x (</span></em><em class="italic"><span class="kobospan" id="kobo.698.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/162.png" class="calibre167"/></span></em><em class="italic"><span class="kobospan" id="kobo.699.1">)</span></em><span class="kobospan" id="kobo.700.1">. </span><span class="kobospan" id="kobo.700.2">We can also create higher-order polynomial features by taking </span><em class="italic"><span class="kobospan" id="kobo.701.1">x</span></em><span class="kobospan" id="kobo.702.1"> to even higher powers, such as </span><span class="kobospan" id="kobo.703.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/163.png" class="calibre168"/></span><em class="italic"><span class="kobospan" id="kobo.704.1">, </span></em><em class="italic"><span class="kobospan" id="kobo.705.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;4&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/164.png" class="calibre169"/></span></em><span class="kobospan" id="kobo.706.1">, and so on. </span><span class="kobospan" id="kobo.706.2">In general, we can create polynomial features of degree </span><em class="italic"><span class="kobospan" id="kobo.707.1">d</span></em><span class="kobospan" id="kobo.708.1"> by taking all possible combinations of products and powers of the original features up to </span><span><span class="kobospan" id="kobo.709.1">degree </span></span><span><em class="italic"><span class="kobospan" id="kobo.710.1">d</span></em></span><span><span class="kobospan" id="kobo.711.1">.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.712.1">In addition to creating polynomial features from a single feature, we can also create polynomial features from multiple features. </span><span class="kobospan" id="kobo.712.2">For example, suppose we have two features, </span><span class="kobospan" id="kobo.713.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/165.png" class="calibre170"/></span><span class="kobospan" id="kobo.714.1"> and </span><span class="kobospan" id="kobo.715.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/166.png" class="calibre171"/></span><span class="kobospan" id="kobo.716.1">. </span><span class="kobospan" id="kobo.716.2">We can create new polynomial features by taking their products (</span><span class="kobospan" id="kobo.717.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/167.png" class="calibre172"/></span><span class="kobospan" id="kobo.718.1">) and raising them to different powers ( </span><span class="kobospan" id="kobo.719.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="image/168.png" class="calibre173"/></span><span class="kobospan" id="kobo.720.1">, </span><span class="kobospan" id="kobo.721.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="image/169.png" class="calibre174"/></span><span class="kobospan" id="kobo.722.1">, and so on). </span><span class="kobospan" id="kobo.722.2">Again, we can create polynomial features of any degree by taking all possible combinations of products and powers of the </span><span><span class="kobospan" id="kobo.723.1">original features.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.724.1">One important consideration when using polynomial expansion is that it can quickly lead to a large number of features, especially for high degrees of polynomials. </span><span class="kobospan" id="kobo.724.2">This can make the resulting model more complex and harder to interpret, and can also lead to overfitting if the number of features is not properly controlled. </span><span class="kobospan" id="kobo.724.3">To address this issue, it is common to use regularization techniques or feature selection methods to select a subset of the most informative </span><span><span class="kobospan" id="kobo.725.1">polynomial features.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.726.1">Overall, polynomial expansion is a powerful feature construction technique that can help capture complex nonlinear relationships between features and the target variable. </span><span class="kobospan" id="kobo.726.2">However, it should be used with caution and with appropriate regularization or feature selection to avoid overfitting and maintain </span><span><span class="kobospan" id="kobo.727.1">model interpretability.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.728.1">For example, in a regression problem, you might have a dataset with a single feature, say </span><em class="italic"><span class="kobospan" id="kobo.729.1">x</span></em><span class="kobospan" id="kobo.730.1">, and you want to fit a model that can capture the relationship between </span><em class="italic"><span class="kobospan" id="kobo.731.1">x</span></em><span class="kobospan" id="kobo.732.1"> and the target variable, </span><em class="italic"><span class="kobospan" id="kobo.733.1">y</span></em><span class="kobospan" id="kobo.734.1">. </span><span class="kobospan" id="kobo.734.2">However, the relationship between </span><em class="italic"><span class="kobospan" id="kobo.735.1">x</span></em><span class="kobospan" id="kobo.736.1"> and </span><em class="italic"><span class="kobospan" id="kobo.737.1">y</span></em><span class="kobospan" id="kobo.738.1"> may not be linear, and a simple linear model may not be sufficient. </span><span class="kobospan" id="kobo.738.2">In this case, polynomial expansion can be used to create additional features that capture the non-linear relationship between </span><em class="italic"><span class="kobospan" id="kobo.739.1">x</span></em> <span><span class="kobospan" id="kobo.740.1">and </span></span><span><em class="italic"><span class="kobospan" id="kobo.741.1">y</span></em></span><span><span class="kobospan" id="kobo.742.1">.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.743.1">To illustrate, let’s say you have a dataset with a single feature, </span><em class="italic"><span class="kobospan" id="kobo.744.1">x</span></em><span class="kobospan" id="kobo.745.1">, and a target variable, </span><em class="italic"><span class="kobospan" id="kobo.746.1">y</span></em><span class="kobospan" id="kobo.747.1">, and you want to fit</span><a id="_idIndexMarker196" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.748.1"> a polynomial regression model. </span><span class="kobospan" id="kobo.748.2">The goal is to find a function, </span><em class="italic"><span class="kobospan" id="kobo.749.1">f(x)</span></em><span class="kobospan" id="kobo.750.1">, that minimizes the difference between the predicted </span><a id="_idIndexMarker197" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.751.1">and actual values </span><span><span class="kobospan" id="kobo.752.1">of </span></span><span><em class="italic"><span class="kobospan" id="kobo.753.1">y</span></em></span><span><span class="kobospan" id="kobo.754.1">.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.755.1">Polynomial expansion can be used to create additional features based on </span><em class="italic"><span class="kobospan" id="kobo.756.1">x</span></em><span class="kobospan" id="kobo.757.1">, such as </span><span class="kobospan" id="kobo.758.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/170.png" class="calibre175"/></span><span class="kobospan" id="kobo.759.1">, </span><span class="kobospan" id="kobo.760.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/171.png" class="calibre175"/></span><span class="kobospan" id="kobo.761.1">, and so on. </span><span class="kobospan" id="kobo.761.2">This can be done using libraries such as </span><strong class="source-inline"><span class="kobospan" id="kobo.762.1">scikit-learn</span></strong><span class="kobospan" id="kobo.763.1">, which has a </span><strong class="source-inline"><span class="kobospan" id="kobo.764.1">PolynomialFeatures</span></strong><span class="kobospan" id="kobo.765.1"> function that can automatically generate polynomial features of a </span><span><span class="kobospan" id="kobo.766.1">specified degree.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.767.1">By adding these polynomial features, the model becomes more expressive and can capture the non-linear relationship between </span><em class="italic"><span class="kobospan" id="kobo.768.1">x</span></em><span class="kobospan" id="kobo.769.1"> and </span><em class="italic"><span class="kobospan" id="kobo.770.1">y</span></em><span class="kobospan" id="kobo.771.1">. </span><span class="kobospan" id="kobo.771.2">However, it’s important to be careful not to overfit the data as adding too many polynomial features can lead to a model that is overly complex and performs poorly on new, </span><span><span class="kobospan" id="kobo.772.1">unseen data.</span></span></p>
</div>


<div id="_idContainer283" class="calibre2">
<h3 class="calibre8"><span class="kobospan" id="kobo.773.1">Logarithmic transformation</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.774.1">Logarithmic transformation is</span><a id="_idIndexMarker198" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.775.1"> a common feature engineering technique that’s used in data preprocessing. </span><span class="kobospan" id="kobo.775.2">The goal of logarithmic transformation is to </span><a id="_idIndexMarker199" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.776.1">make data less skewed and more symmetric by applying a logarithmic function to the features. </span><span class="kobospan" id="kobo.776.2">This technique can be particularly useful for features that are skewed, such as those with a long tail of </span><span><span class="kobospan" id="kobo.777.1">high values.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.778.1">The logarithmic transformation is defined as an equation taking the natural logarithm of </span><span><span class="kobospan" id="kobo.779.1">the data:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.780.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/172.png" class="calibre176"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.781.1">Here, </span><em class="italic"><span class="kobospan" id="kobo.782.1">y</span></em><span class="kobospan" id="kobo.783.1"> is the transformed data and </span><em class="italic"><span class="kobospan" id="kobo.784.1">x</span></em><span class="kobospan" id="kobo.785.1"> is the original data. </span><span class="kobospan" id="kobo.785.2">The logarithmic function maps the original data to a new space, where the relationship between the values is preserved but the scale is compressed. </span><span class="kobospan" id="kobo.785.3">The logarithmic transformation is particularly useful for features with large ranges or that are distributed exponentially, such as the prices</span><a id="_idIndexMarker200" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.786.1"> of products or the incomes </span><span><span class="kobospan" id="kobo.787.1">of individuals.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.788.1">One of the benefits of the logarithmic transformation is that it can help normalize data and make it more suitable for certain machine learning algorithms that assume normally distributed data. </span><span class="kobospan" id="kobo.788.2">Additionally, logarithmic transformation can reduce the impact of outliers on the data, which can help improve the performance of </span><span><span class="kobospan" id="kobo.789.1">some models.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.790.1">It’s important to note that the logarithmic transformation</span><a id="_idIndexMarker201" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.791.1"> is not appropriate for all types of data. </span><span class="kobospan" id="kobo.791.2">For example, if the data includes zero or negative values, the logarithmic transformation cannot be applied directly. </span><span class="kobospan" id="kobo.791.3">In these cases, a modified logarithmic transformation, such as adding a constant before taking the logarithm, may be used. </span><span class="kobospan" id="kobo.791.4">Overall, logarithmic transformation is a useful technique for feature engineering that can help improve the performance of machine learning models, especially when dealing with skewed or exponentially </span><span><span class="kobospan" id="kobo.792.1">distributed data.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.793.1">In summary, feature engineering is a critical step in the machine learning pipeline as it can significantly impact the performance and interpretability of the resulting models. </span><span class="kobospan" id="kobo.793.2">Effective feature engineering requires domain knowledge, creativity, and an iterative process of testing and refining different techniques until the optimal set of features </span><span><span class="kobospan" id="kobo.794.1">is identified.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.795.1">Interaction terms</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.796.1">In feature construction, interaction</span><a id="_idIndexMarker202" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.797.1"> terms refer to creating new features by combining two or more existing features in a dataset through multiplication, division, or other mathematical operations. </span><span class="kobospan" id="kobo.797.2">These new features capture the interaction </span><a id="_idIndexMarker203" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.798.1">or relationship between the original features, and they can help improve the accuracy of machine </span><span><span class="kobospan" id="kobo.799.1">learning models.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.800.1">For example, in a dataset of real estate prices, you might have features such as the number of bedrooms, the number of bathrooms, and the square footage of the property. </span><span class="kobospan" id="kobo.800.2">By themselves, these features provide some information about the price of the property, but they do not capture any interaction effects between the features. </span><span class="kobospan" id="kobo.800.3">However, by creating an interaction term between the number of bedrooms and the square footage, you can capture the idea that larger properties with more bedrooms tend to be more expensive than smaller ones with the same number </span><span><span class="kobospan" id="kobo.801.1">of bedrooms.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.802.1">In practice, interaction terms are created by multiplying or dividing two or more features together. </span><span class="kobospan" id="kobo.802.2">For example, if we have two features, </span><em class="italic"><span class="kobospan" id="kobo.803.1">x</span></em><span class="kobospan" id="kobo.804.1"> and </span><em class="italic"><span class="kobospan" id="kobo.805.1">y</span></em><span class="kobospan" id="kobo.806.1">, we can create an interaction term by multiplying </span><a id="_idIndexMarker204" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.807.1">them together: </span><em class="italic"><span class="kobospan" id="kobo.808.1">xy</span></em><span class="kobospan" id="kobo.809.1">. </span><span class="kobospan" id="kobo.809.2">We can also create</span><a id="_idIndexMarker205" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.810.1"> interaction terms by dividing one feature by </span><span><span class="kobospan" id="kobo.811.1">another: </span></span><span><em class="italic"><span class="kobospan" id="kobo.812.1">x/y</span></em></span><span><span class="kobospan" id="kobo.813.1">.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.814.1">When creating interaction terms, it is important to consider which features to combine and how to combine them. </span><span class="kobospan" id="kobo.814.2">Here are some </span><span><span class="kobospan" id="kobo.815.1">common techniques:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.816.1">Domain knowledge</span></strong><span class="kobospan" id="kobo.817.1">: Use domain knowledge or expert intuition to identify which features are likely to interact and how they </span><span><span class="kobospan" id="kobo.818.1">might interact.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.819.1">Pairwise combinations</span></strong><span class="kobospan" id="kobo.820.1">: Create interaction terms by pairwise combining all pairs of features in the dataset. </span><span class="kobospan" id="kobo.820.2">This can be computationally expensive, but it can help identify potential </span><span><span class="kobospan" id="kobo.821.1">interaction effects.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.822.1">PCA</span></strong><span class="kobospan" id="kobo.823.1">: Use PCA to identify the most important combinations of features, and create interaction terms based on </span><span><span class="kobospan" id="kobo.824.1">these combinations.</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.825.1">Overall, interaction terms are a powerful tool in feature construction that can help capture complex relationships between features and improve the accuracy of machine learning models. </span><span class="kobospan" id="kobo.825.2">However, it is important to be careful when creating interaction terms as too many or poorly chosen terms can le</span><a id="_idTextAnchor066" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.826.1">ad to overfitting or decreased </span><span><span class="kobospan" id="kobo.827.1">model interpretability.</span></span></p>
<h1 id="_idParaDest-52" class="calibre4"><a id="_idTextAnchor067" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.828.1">Common machine learning models</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.829.1">Here, we will explain some of the most common machine learning models, as well as their advantages and disadvantages. </span><span class="kobospan" id="kobo.829.2">Knowing this information will help you pick the best model for the</span><a id="_idTextAnchor068" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.830.1"> problem and be able to improve the </span><span><span class="kobospan" id="kobo.831.1">implemented model.</span></span></p>
<h2 id="_idParaDest-53" class="calibre7"><a id="_idTextAnchor069" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.832.1">Linear regression</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.833.1">Linear regression is a type of supervised learning algorithm that’s used to model the relationship between a dependent variable and one or more independent variables. </span><span class="kobospan" id="kobo.833.2">It assumes a linear relationship </span><a id="_idIndexMarker206" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.834.1">between the input features and the </span><a id="_idIndexMarker207" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.835.1">output. </span><span class="kobospan" id="kobo.835.2">The goal of linear regression is to find the best-fit line that predicts the value of the dependent variable based on the </span><span><span class="kobospan" id="kobo.836.1">independent variables.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.837.1">The equation for a simple linear regression</span><a id="_idIndexMarker208" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.838.1"> with one independent variable (also called a </span><strong class="bold"><span class="kobospan" id="kobo.839.1">simple linear equation</span></strong><span class="kobospan" id="kobo.840.1">) is </span><span><span class="kobospan" id="kobo.841.1">as follows:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.842.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/173.png" class="calibre177"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.843.1">Here, we have </span><span><span class="kobospan" id="kobo.844.1">the following:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><em class="italic"><span class="kobospan" id="kobo.845.1">y</span></em><span class="kobospan" id="kobo.846.1"> is the dependent variable (the variable we want </span><span><span class="kobospan" id="kobo.847.1">to predict)</span></span></li>
<li class="calibre15"><em class="italic"><span class="kobospan" id="kobo.848.1">x</span></em><span class="kobospan" id="kobo.849.1"> is the independent variable (the </span><span><span class="kobospan" id="kobo.850.1">input variable)</span></span></li>
<li class="calibre15"><em class="italic"><span class="kobospan" id="kobo.851.1">m</span></em><span class="kobospan" id="kobo.852.1"> is the slope of the line (how much </span><em class="italic"><span class="kobospan" id="kobo.853.1">y</span></em><span class="kobospan" id="kobo.854.1"> changes when </span><span><em class="italic"><span class="kobospan" id="kobo.855.1">x</span></em></span><span><span class="kobospan" id="kobo.856.1"> changes)</span></span></li>
<li class="calibre15"><em class="italic"><span class="kobospan" id="kobo.857.1">b</span></em><span class="kobospan" id="kobo.858.1"> is the y-intercept (where the line intercepts the </span><em class="italic"><span class="kobospan" id="kobo.859.1">Y</span></em><span class="kobospan" id="kobo.860.1">-axis when </span><em class="italic"><span class="kobospan" id="kobo.861.1">x</span></em><span class="kobospan" id="kobo.862.1"> = </span><span><span class="kobospan" id="kobo.863.1">0)</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.864.1">The goal of linear regression is to find </span><a id="_idIndexMarker209" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.865.1">the values of </span><em class="italic"><span class="kobospan" id="kobo.866.1">m</span></em><span class="kobospan" id="kobo.867.1"> and </span><em class="italic"><span class="kobospan" id="kobo.868.1">b</span></em><span class="kobospan" id="kobo.869.1"> that minimize the difference between the predicted values and the actual values of the dependent variable. </span><span class="kobospan" id="kobo.869.2">This difference is typically measured using a cost function, such as mean squared error or mean </span><span><span class="kobospan" id="kobo.870.1">absolute error.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.871.1">Multiple linear regression is an </span><a id="_idIndexMarker210" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.872.1">extension of simple linear regression, where there are multiple independent variables. </span><span class="kobospan" id="kobo.872.2">The equation for multiple linear regression is </span><span><span class="kobospan" id="kobo.873.1">shown here:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.874.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msub&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/174.png" class="calibre178"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.875.1">Here we have </span><span><span class="kobospan" id="kobo.876.1">the following:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><em class="italic"><span class="kobospan" id="kobo.877.1">y</span></em><span class="kobospan" id="kobo.878.1"> is the </span><span><span class="kobospan" id="kobo.879.1">dependent variable</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.880.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/175.png" class="calibre179"/></span><span class="kobospan" id="kobo.881.1"> are the </span><span><span class="kobospan" id="kobo.882.1">independent variables</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.883.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/176.png" class="calibre180"/></span><span class="kobospan" id="kobo.884.1"> is the y-intercept (when all the independent variables are equal </span><span><span class="kobospan" id="kobo.885.1">to 0)</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.886.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/177.png" class="calibre181"/></span><span class="kobospan" id="kobo.887.1"> are the coefficients (how much </span><em class="italic"><span class="kobospan" id="kobo.888.1">y</span></em><span class="kobospan" id="kobo.889.1"> changes when each independent </span><span><span class="kobospan" id="kobo.890.1">variable changes)</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.891.1">Similar to simple linear regression, the goal of multiple linear regression is to find the values of </span><span class="kobospan" id="kobo.892.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/178.png" class="calibre182"/></span><span><span class="kobospan" id="kobo.893.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/179.png" class="calibre183"/></span></span><span class="kobospan" id="kobo.894.1"> that minimize the difference between the predicted values and the actual values of</span><a id="_idIndexMarker211" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.895.1"> the </span><span><span class="kobospan" id="kobo.896.1">dependent variable.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.897.1">The advantages of linear</span><a id="_idIndexMarker212" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.898.1"> regression are </span><span><span class="kobospan" id="kobo.899.1">as follows:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><span class="kobospan" id="kobo.900.1">It’s simple and easy </span><span><span class="kobospan" id="kobo.901.1">to</span></span><span><a id="_idIndexMarker213" class="calibre5 pcalibre1 pcalibre"/></span><span><span class="kobospan" id="kobo.902.1"> understand</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.903.1">It can be used to model a wide range of relationships between the dependent and </span><span><span class="kobospan" id="kobo.904.1">independent variables</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.905.1">It’s computationally efficient, making it fast and suitable for </span><span><span class="kobospan" id="kobo.906.1">large datasets</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.907.1">It provides interpretable results, allowing for the analysis of the impact of each independent variable on the </span><span><span class="kobospan" id="kobo.908.1">dependent variable</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.909.1">The disadvantages of linear</span><a id="_idIndexMarker214" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.910.1"> regression are </span><span><span class="kobospan" id="kobo.911.1">as follows:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><span class="kobospan" id="kobo.912.1">It assumes a linear relationship between the input features and the output, which may not always be the case in </span><span><span class="kobospan" id="kobo.913.1">real-world data</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.914.1">It may not capture complex non-linear relationships between the input features and </span><span><span class="kobospan" id="kobo.915.1">the output</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.916.1">It’s sensitive to outliers and influential observations, which can affect the accuracy of </span><span><span class="kobospan" id="kobo.917.1">the model</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.918.1">It assumes that the errors are normally distributed wit</span><a id="_idTextAnchor070" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.919.1">h constant variance, which may not always hold true </span><span><span class="kobospan" id="kobo.920.1">in practice</span></span></li>
</ul>
<h2 id="_idParaDest-54" class="calibre7"><a id="_idTextAnchor071" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.921.1">Logistic regression</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.922.1">Logistic regression is a popular machine learning algorithm that’s used for classification problems. </span><span class="kobospan" id="kobo.922.2">Unlike</span><a id="_idIndexMarker215" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.923.1"> linear regression, which is used for predicting continuous values, logistic regression is used for predicting discrete outcomes, typically binary outcomes (0 </span><span><span class="kobospan" id="kobo.924.1">or 1).</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.925.1">The goal of logistic regression is to estimate the probability of a certain outcome based on one or more input variables. </span><span class="kobospan" id="kobo.925.2">The output of logistic regression is a probability score, which can be</span><a id="_idIndexMarker216" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.926.1"> converted into a binary class label by applying a threshold value. </span><span class="kobospan" id="kobo.926.2">The threshold value can be adjusted to balance between precision and recall based on the specific requirements of </span><span><span class="kobospan" id="kobo.927.1">the problem.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.928.1">The logistic regression model assumes that the relationship between the input variables and the output variable is linear in the logit (log odds) space. </span><span class="kobospan" id="kobo.928.2">The logit function is defined </span><span><span class="kobospan" id="kobo.929.1">as follows:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.930.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/180.png" class="calibre184"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.931.1">Here, </span><em class="italic"><span class="kobospan" id="kobo.932.1">p</span></em><span class="kobospan" id="kobo.933.1"> is the probability of the positive outcome (that is, the probability of the </span><span><span class="kobospan" id="kobo.934.1">event occurring).</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.935.1">The logistic regression model can be represented mathematically </span><span><span class="kobospan" id="kobo.936.1">as follows:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.937.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;*&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;*&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mo&gt;..&lt;/mo&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msub&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;*&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/181.png" class="calibre185"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.938.1">Here, </span><span class="kobospan" id="kobo.939.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mo&gt;…&lt;/mo&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/182.png" class="calibre186"/></span><span class="kobospan" id="kobo.940.1"> are the coefficients of the model, </span><span class="kobospan" id="kobo.941.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mo&gt;…&lt;/mo&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/183.png" class="calibre187"/></span><span class="kobospan" id="kobo.942.1"> are the input variables, and </span><em class="italic"><span class="kobospan" id="kobo.943.1">logit(p)</span></em><span class="kobospan" id="kobo.944.1"> is the logit function of the probability of a </span><span><span class="kobospan" id="kobo.945.1">positive outcome.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.946.1">The logistic regression model is trained using a dataset of labeled examples, where each example consists of a set of input variables and a binary label indicating whether the positive outcome occurred or not. </span><span class="kobospan" id="kobo.946.2">The coefficients of the model are estimated using maximum likelihood estimation, which seeks to find the values of the coefficients that maximize the likelihood of the </span><span><span class="kobospan" id="kobo.947.1">observed data.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.948.1">The advantages of logistic regression are </span><span><span class="kobospan" id="kobo.949.1">as follows:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.950.1">Interpretable</span></strong><span class="kobospan" id="kobo.951.1">: The coefficients of the model can be interpreted as the change in the log odds of the positive outcome associated with a unit change in the corresponding</span><a id="_idIndexMarker217" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.952.1"> input variable, making it easy to understand the impact of each input variable on the predicted </span><a id="_idIndexMarker218" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.953.1">probability of the </span><span><span class="kobospan" id="kobo.954.1">positive outcome</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.955.1">Computationally efficient</span></strong><span class="kobospan" id="kobo.956.1">: Logistic</span><a id="_idIndexMarker219" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.957.1"> regression is a simple algorithm that can be trained quickly on </span><span><span class="kobospan" id="kobo.958.1">large datasets</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.959.1">Works well with small datasets</span></strong><span class="kobospan" id="kobo.960.1">: Logistic regression can be effective even with a small number of observations, provided that the input variables are relevant to the </span><span><span class="kobospan" id="kobo.961.1">prediction task</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.962.1">The disadvantages of logistic regression are </span><span><span class="kobospan" id="kobo.963.1">as follows:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.964.1">Assumes linearity</span></strong><span class="kobospan" id="kobo.965.1">: Logistic regression assumes a linear relationship between the input variables </span><a id="_idIndexMarker220" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.966.1">and the logit of the probability of the positive outcome, which may not always be the case in </span><span><span class="kobospan" id="kobo.967.1">real-world datasets</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.968.1">May suffer from overfitting</span></strong><span class="kobospan" id="kobo.969.1">: If the number of input variables is large compared to the number of observations, the model may suffer from overfitting, leading to poor generalization performance on </span><span><span class="kobospan" id="kobo.970.1">new data</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.971.1">Not suitable for non-linear problems</span></strong><span class="kobospan" id="kobo.972.1">: Logistic regression is a linear algorithm and is not suitable for problems where the relations</span><a id="_idTextAnchor072" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.973.1">hip between the input variables and the output variable </span><span><span class="kobospan" id="kobo.974.1">is non-linear</span></span></li>
</ul>
<h2 id="_idParaDest-55" class="calibre7"><a id="_idTextAnchor073" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.975.1">Decision trees</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.976.1">Decision trees are a type </span><a id="_idIndexMarker221" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.977.1">of supervised learning algorithm used for classification and</span><a id="_idIndexMarker222" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.978.1"> regression analysis. </span><span class="kobospan" id="kobo.978.2">A decision tree consists of a series of nodes that represent decision points, each of which has one or more branches that lead to other decision points or a </span><span><span class="kobospan" id="kobo.979.1">final prediction.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.980.1">In a classification problem, each leaf node of the tree represents a class label, while in a regression problem, each leaf node represents a numerical value. </span><span class="kobospan" id="kobo.980.2">The process of building a decision tree involves choosing a sequence of attributes that best splits the data into subsets that are more homogenous concerning the target variable. </span><span class="kobospan" id="kobo.980.3">This process is </span><a id="_idIndexMarker223" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.981.1">typically repeated recursively for each subset until a stopping criterion is met, such as a minimum number of instances in each subset or a maximum depth of </span><span><span class="kobospan" id="kobo.982.1">the tree.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.983.1">The equations for decision trees involve calculating the information gain (or another splitting criterion, such as Gini impurity or entropy) for each potential split at each decision point. </span><span class="kobospan" id="kobo.983.2">The</span><a id="_idIndexMarker224" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.984.1"> attribute with the highest information gain is selected as the split criterion for that node. </span><span class="kobospan" id="kobo.984.2">The conceptual formula for information gain is </span><span><span class="kobospan" id="kobo.985.1">shown here:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.986.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;I&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mo&gt;[&lt;/mo&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;o&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;f&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;′&lt;/mo&gt;&lt;/mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;]&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/184.png" class="calibre188"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.987.1">Here, </span><em class="italic"><span class="kobospan" id="kobo.988.1">entropy</span></em><span class="kobospan" id="kobo.989.1"> is a measure of the impurity or randomness of a system. </span><span class="kobospan" id="kobo.989.2">In the context of decision trees, entropy is used to measure the impurity of a node in </span><span><span class="kobospan" id="kobo.990.1">the tree.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.991.1">The </span><em class="italic"><span class="kobospan" id="kobo.992.1">entropy</span></em><span class="kobospan" id="kobo.993.1"> of a node is calculated </span><span><span class="kobospan" id="kobo.994.1">as follows:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.995.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;/munderover&gt;&lt;mrow&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;/mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;msub&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/185.png" class="calibre189"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.996.1">Here, </span><em class="italic"><span class="kobospan" id="kobo.997.1">c</span></em><span class="kobospan" id="kobo.998.1"> is the number of classes and </span><span class="kobospan" id="kobo.999.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/186.png" class="calibre190"/></span><span class="kobospan" id="kobo.1000.1"> is the proportion of the samples that belong to class </span><em class="italic"><span class="kobospan" id="kobo.1001.1">i</span></em><span class="kobospan" id="kobo.1002.1"> in </span><span><span class="kobospan" id="kobo.1003.1">the node.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1004.1">The entropy of a node ranges from 0 to 1, with 0 indicating a pure node (that is, all samples belong to the same class) and 1 indicating a node that is evenly split between </span><span><span class="kobospan" id="kobo.1005.1">all classes.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1006.1">In a decision tree, the entropy of a node is used to determine the splitting criterion for the tree. </span><span class="kobospan" id="kobo.1006.2">The idea is to split the node into two or more child nodes such that the entropy of the child nodes is lower than the entropy of the parent node. </span><span class="kobospan" id="kobo.1006.3">The split with the lowest entropy is chosen as the </span><span><span class="kobospan" id="kobo.1007.1">best split.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1008.1">Please note that the choice of the next node in the decision tree differs based on the underlying algorithm – for example, CART, ID3, or C4.5. </span><span class="kobospan" id="kobo.1008.2">What we explained here was CART, which uses Gini impurity and entropy to split </span><span><span class="kobospan" id="kobo.1009.1">the data.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1010.1">The advantage of using</span><a id="_idIndexMarker225" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1011.1"> entropy as a splitting criterion is that it can handle both binary and multi-class classification problems. </span><span class="kobospan" id="kobo.1011.2">It is also relatively computationally efficient compared to </span><a id="_idIndexMarker226" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1012.1">other splitting criteria. </span><span class="kobospan" id="kobo.1012.2">However, one disadvantage of using entropy is that it tends to create biased trees in favor of attributes with </span><span><span class="kobospan" id="kobo.1013.1">many categories.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1014.1">Here are some of the advantages of </span><span><span class="kobospan" id="kobo.1015.1">decision trees:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><span class="kobospan" id="kobo.1016.1">Easy to understand and interpret, even </span><span><span class="kobospan" id="kobo.1017.1">for non-experts</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1018.1">Can handle both categorical and </span><span><span class="kobospan" id="kobo.1019.1">numerical data</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1020.1">Can handle missing data </span><span><span class="kobospan" id="kobo.1021.1">and outliers</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1022.1">Can be used for </span><span><span class="kobospan" id="kobo.1023.1">feature selection</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1024.1">Can be combined with other models in ensemble methods, such as </span><span><span class="kobospan" id="kobo.1025.1">random forests</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.1026.1">Here are some of the disadvantages of </span><span><span class="kobospan" id="kobo.1027.1">decision trees:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><span class="kobospan" id="kobo.1028.1">Can be prone to overfitting, especially if the tree is too deep </span><span><span class="kobospan" id="kobo.1029.1">or complex</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1030.1">Can be sensitive to small changes in the data or the way the tree </span><span><span class="kobospan" id="kobo.1031.1">is built</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1032.1">Can be biased toward features with many categories or</span><a id="_idTextAnchor074" class="calibre5 pcalibre1 pcalibre"/> <span><span class="kobospan" id="kobo.1033.1">high cardinality</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1034.1">Can have problems with rare events or </span><span><span class="kobospan" id="kobo.1035.1">imbalanced datasets</span></span></li>
</ul>
<h2 id="_idParaDest-56" class="calibre7"><a id="_idTextAnchor075" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1036.1">Random forest</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.1037.1">Random forest is an </span><a id="_idIndexMarker227" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1038.1">ensemble learning method that’s versatile and can perform classification and regression tasks. </span><span class="kobospan" id="kobo.1038.2">It operates by generating multiple decision trees during training, predicting the target class for classification based on the majority of the trees, and the predicted value based on the mean prediction by trees for regression tasks. </span><span class="kobospan" id="kobo.1038.3">The algorithm for constructing a random forest can be summarized</span><a id="_idIndexMarker228" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1039.1"> in the </span><span><span class="kobospan" id="kobo.1040.1">following steps:</span></span></p>
<ol class="calibre16">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.1041.1">Bootstrap sampling</span></strong><span class="kobospan" id="kobo.1042.1">: Randomly select a subset of the data with replacement to create a new dataset that’s the same size as the </span><span><span class="kobospan" id="kobo.1043.1">original dataset.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.1044.1">Feature selection</span></strong><span class="kobospan" id="kobo.1045.1">: Randomly select a subset of the features (columns) for each split when building a decision tree. </span><span class="kobospan" id="kobo.1045.2">This helps to create diversity in the trees and </span><span><span class="kobospan" id="kobo.1046.1">reduce overfitting.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.1047.1">Tree building</span></strong><span class="kobospan" id="kobo.1048.1">: Construct a decision tree for each bootstrap sample and feature subset. </span><span class="kobospan" id="kobo.1048.2">The decision tree is constructed recursively by splitting the data based on the selected features until a stopping criterion is met (for example, maximum depth or minimum number of samples in a </span><span><span class="kobospan" id="kobo.1049.1">leaf node).</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.1050.1">Ensemble learning</span></strong><span class="kobospan" id="kobo.1051.1">: Combine the predictions of all decision trees to make a final prediction. </span><span class="kobospan" id="kobo.1051.2">For classification, the class that receives the most votes from the decision trees is the final prediction. </span><span class="kobospan" id="kobo.1051.3">For regression, the average of the predictions from all decision trees is the </span><span><span class="kobospan" id="kobo.1052.1">final prediction.</span></span></li>
</ol>
<p class="calibre6"><span class="kobospan" id="kobo.1053.1">The random forest algorithm can be expressed mathematically </span><span><span class="kobospan" id="kobo.1054.1">as follows.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1055.1">Given a dataset, </span><em class="italic"><span class="kobospan" id="kobo.1056.1">D</span></em><span class="kobospan" id="kobo.1057.1">, with </span><em class="italic"><span class="kobospan" id="kobo.1058.1">N</span></em><span class="kobospan" id="kobo.1059.1"> samples and </span><em class="italic"><span class="kobospan" id="kobo.1060.1">M</span></em><span class="kobospan" id="kobo.1061.1"> features, we create </span><em class="italic"><span class="kobospan" id="kobo.1062.1">T</span></em><span class="kobospan" id="kobo.1063.1"> decision trees </span><span><span class="kobospan" id="kobo.1064.1">{</span></span><span><span class="kobospan" id="kobo.1065.1">T</span></span><span><span class="kobospan" id="kobo.1066.1">r</span></span><span><span class="kobospan" id="kobo.1067.1">e</span></span><span> </span><span><span class="kobospan" id="kobo.1068.1">e</span></span><span><span class="kobospan" id="kobo.1069.1"> </span></span><span><span class="kobospan" id="kobo.1070.1">1</span></span><span><span class="kobospan" id="kobo.1071.1">,</span></span><span> </span><span><span class="kobospan" id="kobo.1072.1">T</span></span><span><span class="kobospan" id="kobo.1073.1">r</span></span><span><span class="kobospan" id="kobo.1074.1">e</span></span><span> </span><span><span class="kobospan" id="kobo.1075.1">e</span></span><span><span class="kobospan" id="kobo.1076.1"> </span></span><span><span class="kobospan" id="kobo.1077.1">2</span></span><span><span class="kobospan" id="kobo.1078.1">,</span></span><span> </span><span><span class="kobospan" id="kobo.1079.1">…</span></span><span> </span><span><span class="kobospan" id="kobo.1080.1">,</span></span><span> </span><span><span class="kobospan" id="kobo.1081.1">T</span></span><span><span class="kobospan" id="kobo.1082.1">r</span></span><span><span class="kobospan" id="kobo.1083.1">e</span></span><span> </span><span><span class="kobospan" id="kobo.1084.1">e</span></span><span><span class="kobospan" id="kobo.1085.1"> </span></span><span><span class="kobospan" id="kobo.1086.1">T</span></span><span><span class="kobospan" id="kobo.1087.1">}</span></span><span class="kobospan" id="kobo.1088.1"> by applying the preceding steps. </span><span class="kobospan" id="kobo.1088.2">Each decision tree is constructed using a bootstrap sample of the data, </span><em class="italic"><span class="kobospan" id="kobo.1089.1">D’</span></em><span class="kobospan" id="kobo.1090.1">, with size </span><em class="italic"><span class="kobospan" id="kobo.1091.1">N’ (N’ &lt;= N)</span></em><span class="kobospan" id="kobo.1092.1"> and a subset of the features, </span><em class="italic"><span class="kobospan" id="kobo.1093.1">F’</span></em><span class="kobospan" id="kobo.1094.1">, with size </span><em class="italic"><span class="kobospan" id="kobo.1095.1">m (m &lt;= M)</span></em><span class="kobospan" id="kobo.1096.1">. </span><span class="kobospan" id="kobo.1096.2">For each split in the decision tree, we randomly select </span><em class="italic"><span class="kobospan" id="kobo.1097.1">k (k &lt; m)</span></em><span class="kobospan" id="kobo.1098.1"> features from </span><em class="italic"><span class="kobospan" id="kobo.1099.1">F’</span></em><span class="kobospan" id="kobo.1100.1"> and choose the best feature to split the data based on an impurity measure (for example, Gini index or entropy). </span><span class="kobospan" id="kobo.1100.2">The decision tree is built until a stopping criterion is met (for example, the maximum depth or minimum number of samples in a </span><span><span class="kobospan" id="kobo.1101.1">leaf node).</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1102.1">The final prediction, </span><span class="kobospan" id="kobo.1103.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:math&gt;" src="image/187.png" class="calibre191"/></span><span class="kobospan" id="kobo.1104.1">, for a new sample, </span><em class="italic"><span class="kobospan" id="kobo.1105.1">x</span></em><span class="kobospan" id="kobo.1106.1">, is obtained by aggregating the predictions from all </span><span><span class="kobospan" id="kobo.1107.1">decision trees.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1108.1">For classification, </span><span class="kobospan" id="kobo.1109.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:math&gt;" src="image/188.png" class="calibre192"/></span><span class="kobospan" id="kobo.1110.1"> is the class that receives the most votes from all </span><span><span class="kobospan" id="kobo.1111.1">decision trees:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1112.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mover&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/msub&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/munder&gt;&lt;mrow&gt;&lt;mi&gt;I&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/189.png" class="calibre193"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1113.1">Here, </span><span class="kobospan" id="kobo.1114.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/190.png" class="calibre194"/></span><span class="kobospan" id="kobo.1115.1"> is the prediction of the </span><em class="italic"><span class="kobospan" id="kobo.1116.1">j-th</span></em><span class="kobospan" id="kobo.1117.1"> decision tree for the </span><em class="italic"><span class="kobospan" id="kobo.1118.1">i-th</span></em><span class="kobospan" id="kobo.1119.1"> sample, and </span><em class="italic"><span class="kobospan" id="kobo.1120.1">I()</span></em><span class="kobospan" id="kobo.1121.1"> is the indicator function that returns 1 if the condition is true and </span><span><span class="kobospan" id="kobo.1122.1">0 otherwise.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1123.1">For regression, </span><span class="kobospan" id="kobo.1124.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:math&gt;" src="image/188.png" class="calibre192"/></span><span class="kobospan" id="kobo.1125.1"> is the average </span><a id="_idIndexMarker229" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1126.1">of the predictions from all </span><span><span class="kobospan" id="kobo.1127.1">decision trees:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1128.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mover&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/munderover&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/192.png" class="calibre195"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1129.1">Here, </span><span class="kobospan" id="kobo.1130.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/193.png" class="calibre196"/></span><span class="kobospan" id="kobo.1131.1"> is the prediction of the </span><em class="italic"><span class="kobospan" id="kobo.1132.1">i-th</span></em><span class="kobospan" id="kobo.1133.1"> decision tree for the new </span><span><span class="kobospan" id="kobo.1134.1">sample, </span></span><span><em class="italic"><span class="kobospan" id="kobo.1135.1">x</span></em></span><span><span class="kobospan" id="kobo.1136.1">.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1137.1">In summary, random forest is a powerful machine learning algorithm that can handle high-dimensional and noisy datasets. </span><span class="kobospan" id="kobo.1137.2">It works by constructing multiple decision trees using bootstrap samples of the data and feature subsets, and then aggregating the predictions of all decision trees to make a final prediction. </span><span class="kobospan" id="kobo.1137.3">The algorithm is scalable, easy to use, and provides a measure of feature importance, making it a popular choice for many machine </span><span><span class="kobospan" id="kobo.1138.1">learning applications.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1139.1">The advantages of random</span><a id="_idIndexMarker230" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1140.1"> forests are </span><span><span class="kobospan" id="kobo.1141.1">as follows:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.1142.1">Robustness</span></strong><span class="kobospan" id="kobo.1143.1">: Random forest is a very robust algorithm that can handle a variety of input data types, including numerical, categorical, and </span><span><span class="kobospan" id="kobo.1144.1">ordinal data</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.1145.1">Feature selection</span></strong><span class="kobospan" id="kobo.1146.1">: Random forest can rank the importance of features, allowing users to identify the most important features for classification or </span><span><span class="kobospan" id="kobo.1147.1">regression tasks</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.1148.1">Overfitting</span></strong><span class="kobospan" id="kobo.1149.1">: Random forest has a built-in mechanism for reducing overfitting, called bagging, which </span><a id="_idIndexMarker231" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1150.1">helps to generalize well on </span><span><span class="kobospan" id="kobo.1151.1">new data</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.1152.1">Scalability</span></strong><span class="kobospan" id="kobo.1153.1">: Random forest can handle large datasets with a high number of features, making it a good choice for big </span><span><span class="kobospan" id="kobo.1154.1">data applications</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.1155.1">Outliers</span></strong><span class="kobospan" id="kobo.1156.1">: Random forest is robust to the presence of outliers as it is based on decision trees, which can handle </span><span><span class="kobospan" id="kobo.1157.1">outliers effectively</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.1158.1">The disadvantages of random</span><a id="_idIndexMarker232" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1159.1"> forests are </span><span><span class="kobospan" id="kobo.1160.1">as follows:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.1161.1">Interpretability</span></strong><span class="kobospan" id="kobo.1162.1">: Random forest models can be difficult to interpret as they are based on an ensemble of </span><span><span class="kobospan" id="kobo.1163.1">decision trees</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.1164.1">Training time</span></strong><span class="kobospan" id="kobo.1165.1">: The training time of a random forest can be longer than other simpler algorithms, especially when the number of trees in the ensemble </span><span><span class="kobospan" id="kobo.1166.1">is large</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.1167.1">Memory usage</span></strong><span class="kobospan" id="kobo.1168.1">: Random forest requires more memory than some other algorithms as it has to store the decision trees </span><span><span class="kobospan" id="kobo.1169.1">in memory</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.1170.1">Bias</span></strong><span class="kobospan" id="kobo.1171.1">: Random forest can suffer from bias if the data is imbalanced or if the target variable has a </span><span><span class="kobospan" id="kobo.1172.1">high cardinality</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.1173.1">Overfitting</span></strong><span class="kobospan" id="kobo.1174.1">: Although random forest is designed to prevent overfitting, it is still possible to overfit the model if the hyperparameters are not </span><span><span class="kobospan" id="kobo.1175.1">properly tuned</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.1176.1">Overall, random forest is a powerful machine learning algorithm that has many advantages, but it is important </span><a id="_idTextAnchor076" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1177.1">to carefully consider its limitations before applying it to a </span><span><span class="kobospan" id="kobo.1178.1">particular problem.</span></span></p>
<h2 id="_idParaDest-57" class="calibre7"><a id="_idTextAnchor077" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1179.1">Support vector machines (SVMs)</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.1180.1">SVMs are considered</span><a id="_idIndexMarker233" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1181.1"> robust supervised learning algorithms that can perform both classification and regression tasks. </span><span class="kobospan" id="kobo.1181.2">They excel in scenarios with intricate decision boundaries, surpassing the limitations of linear models. </span><span class="kobospan" id="kobo.1181.3">At their core, SVMs aim to identify a hyperplane within a multi-dimensional space that maximally segregates the classes. </span><span class="kobospan" id="kobo.1181.4">This hyperplane is positioned to maximize the distance between itself and the closest points from each class, known as support vectors. </span><span class="kobospan" id="kobo.1181.5">Here’s how SVMs work for a binary classification problem. </span><span class="kobospan" id="kobo.1181.6">Given a set of training data, </span><span class="kobospan" id="kobo.1182.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mo&gt;{&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mo&gt;...&lt;/mo&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;}&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/194.png" class="calibre197"/></span><span class="kobospan" id="kobo.1183.1">, where </span><span class="kobospan" id="kobo.1184.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/195.png" class="calibre198"/></span><span class="kobospan" id="kobo.1185.1"> is a d-dimensional feature vector and </span><span class="kobospan" id="kobo.1186.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/196.png" class="calibre199"/></span><span class="kobospan" id="kobo.1187.1"> is the binary class label (+1 or -1), the goal of an SVM is to find a hyperplane that separates the two classes with the largest margin. </span><span class="kobospan" id="kobo.1187.2">The margin is defined as the distance between the hyperplane and the closest data points from </span><span><span class="kobospan" id="kobo.1188.1">each class:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer210">
<span class="kobospan" id="kobo.1189.1"><img alt="Figure 3.2 – SVM margins" src="image/B18949_03_2.jpg" class="calibre3"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.1190.1">Figure 3.2 – SVM margins</span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1191.1">The hyperplane is defined by a weight vector, </span><em class="italic"><span class="kobospan" id="kobo.1192.1">w</span></em><span class="kobospan" id="kobo.1193.1">, and a bias term, </span><em class="italic"><span class="kobospan" id="kobo.1194.1">b</span></em><span class="kobospan" id="kobo.1195.1">, such that for any new data point, </span><em class="italic"><span class="kobospan" id="kobo.1196.1">x</span></em><span class="kobospan" id="kobo.1197.1">, the </span><a id="_idIndexMarker234" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1198.1">predicted class label, </span><em class="italic"><span class="kobospan" id="kobo.1199.1">y</span></em><span class="kobospan" id="kobo.1200.1">, is given by the </span><span><span class="kobospan" id="kobo.1201.1">following equation:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1202.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/197.png" class="calibre200"/></span><span class="kobospan" id="kobo.1203.1">+</span><span><span class="kobospan" id="kobo.1204.1">b)</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1205.1">Here, </span><em class="italic"><span class="kobospan" id="kobo.1206.1">sign</span></em><span class="kobospan" id="kobo.1207.1"> is the sign function, which returns +1 if the argument is positive and -</span><span><span class="kobospan" id="kobo.1208.1">1 otherwise.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1209.1">The objective function of an SVM is to minimize the classification error subject to the constraint that the margin is maximized. </span><span class="kobospan" id="kobo.1209.2">This can be formulated as an </span><span><span class="kobospan" id="kobo.1210.1">optimization problem:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1211.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;msup&gt;&lt;mrow&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;w&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;/mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/198.png" class="calibre201"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1212.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;w&lt;/mi&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msup&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;≥&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1,2&lt;/mn&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mo&gt;...&lt;/mo&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/199.png" class="calibre202"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1213.1">Here, </span><span class="kobospan" id="kobo.1214.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mrow&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;w&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;/mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/200.png" class="calibre203"/></span><span class="kobospan" id="kobo.1215.1"> is the squared Euclidean norm of the weight vector, </span><em class="italic"><span class="kobospan" id="kobo.1216.1">w</span></em><span class="kobospan" id="kobo.1217.1">. </span><span class="kobospan" id="kobo.1217.2">The constraints ensure that all data points are correctly classified and that the margin </span><span><span class="kobospan" id="kobo.1218.1">is maximized.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1219.1">Here are some of the advantages </span><span><span class="kobospan" id="kobo.1220.1">of SVMs:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><span class="kobospan" id="kobo.1221.1">Effective in high-dimensional</span><a id="_idIndexMarker235" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1222.1"> spaces, which is useful when the number of features </span><span><span class="kobospan" id="kobo.1223.1">is large</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1224.1">Can be used for both classification and </span><span><span class="kobospan" id="kobo.1225.1">regression tasks</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1226.1">Works well with both linearly separable and non-linearly </span><span><span class="kobospan" id="kobo.1227.1">separable data</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1228.1">Can handle outliers well due to the use of the </span><span><span class="kobospan" id="kobo.1229.1">margin concept</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1230.1">Has a regularization parameter that allows you to </span><span><span class="kobospan" id="kobo.1231.1">control overfitting</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.1232.1">Here are some of the </span><a id="_idIndexMarker236" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1233.1">disadvantages </span><span><span class="kobospan" id="kobo.1234.1">of SVMs:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><span class="kobospan" id="kobo.1235.1">Can be sensitive to the choice of kernel function, which can greatly affect the performance of </span><span><span class="kobospan" id="kobo.1236.1">the model</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1237.1">Computationally intensive for </span><span><span class="kobospan" id="kobo.1238.1">large datasets</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1239.1">It can be difficult to interpret the resul</span><a id="_idTextAnchor078" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1240.1">ts of an </span><span><span class="kobospan" id="kobo.1241.1">SVM model</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1242.1">Requires careful tuning of parameters to achieve </span><span><span class="kobospan" id="kobo.1243.1">good performance</span></span></li>
</ul>
<h2 id="_idParaDest-58" class="calibre7"><a id="_idTextAnchor079" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1244.1">Neural networks and transformers</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.1245.1">Neural networks and transformers are both powerful machine learning models that are used for a variety of tasks, such as image classification, NLP, and </span><span><span class="kobospan" id="kobo.1246.1">speech recognition.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.1247.1">Neural networks</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.1248.1">Neural networks draw</span><a id="_idIndexMarker237" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1249.1"> inspiration from the structure and functioning of the human brain. </span><span class="kobospan" id="kobo.1249.2">They represent a category of machine learning models that are proficient in various tasks such as classification, regression, and more. </span><span class="kobospan" id="kobo.1249.3">Comprising multiple layers of interconnected </span><a id="_idIndexMarker238" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1250.1">nodes known as neurons, these networks adeptly process and manipulate data. </span><span class="kobospan" id="kobo.1250.2">The output of each layer is fed into the next layer, creating a hierarchy of feature representations. </span><span class="kobospan" id="kobo.1250.3">The input to the first layer is the raw data, and the output of the final layer is the prediction. </span><span class="kobospan" id="kobo.1250.4">A simple neural network for detecting the gender of a person based on their height and weight is shown in </span><span><em class="italic"><span class="kobospan" id="kobo.1251.1">Figure 3</span></em></span><span><em class="italic"><span class="kobospan" id="kobo.1252.1">.3</span></em></span><span><span class="kobospan" id="kobo.1253.1">:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer215">
<span class="kobospan" id="kobo.1254.1"><img alt="Figure 3.3 – Simple neural network" src="image/B18949_03_3.jpg" class="calibre3"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.1255.1">Figure 3.3 – Simple neural network</span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1256.1">The operation of a single neuron</span><a id="_idIndexMarker239" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1257.1"> in a neural network can be represented by the </span><span><span class="kobospan" id="kobo.1258.1">following equation:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1259.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/munderover&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/201.png" class="calibre204"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1260.1">Here, </span><span class="kobospan" id="kobo.1261.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/202.png" class="calibre205"/></span><span class="kobospan" id="kobo.1262.1"> is the input values, </span><span class="kobospan" id="kobo.1263.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/203.png" class="calibre206"/></span><span class="kobospan" id="kobo.1264.1"> is the weights of the connections between the neurons, </span><em class="italic"><span class="kobospan" id="kobo.1265.1">b</span></em><span class="kobospan" id="kobo.1266.1"> is the bias term, and </span><em class="italic"><span class="kobospan" id="kobo.1267.1">f</span></em><span class="kobospan" id="kobo.1268.1"> is the activation function. </span><span class="kobospan" id="kobo.1268.2">The activation function applies a non-linear transformation to the weighted sum of the inputs and </span><span><span class="kobospan" id="kobo.1269.1">bias term.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1270.1">Training a neural network involves adjusting the weights and biases of the neurons to minimize a loss function. </span><span class="kobospan" id="kobo.1270.2">This is typically done using an optimization algorithm such as stochastic </span><span><span class="kobospan" id="kobo.1271.1">gradient descent.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1272.1">The advantages of neural networks include their ability to learn complex non-linear relationships between input and output data, their ability to automatically extract meaningful features from raw data, and their scalability to </span><span><span class="kobospan" id="kobo.1273.1">large datasets.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1274.1">The disadvantages of neural networks include their high computational and memory requirements, their sensitivity to hyperparameter tuning, and the difficulty of interpreting their </span><span><span class="kobospan" id="kobo.1275.1">internal representations.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.1276.1">Transformers</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.1277.1">Transformers are a type of </span><a id="_idIndexMarker240" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1278.1">neural network architecture that is particularly well suited to sequential data such as text or speech. </span><span class="kobospan" id="kobo.1278.2">They were introduced in the context of NLP and have since been applied to a wide range </span><span><span class="kobospan" id="kobo.1279.1">of tasks.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1280.1">The core component of a transformer is the self-attention mechanism, which allows the model to attend to different parts of the input sequence when computing the output. </span><span class="kobospan" id="kobo.1280.2">The self-attention mechanism is based on a dot product between a query vector, a set of key vectors, and a set of value vectors. </span><span class="kobospan" id="kobo.1280.3">The resulting attention weights are used to weight the values, which are then combined to produce </span><span><span class="kobospan" id="kobo.1281.1">the output.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1282.1">The self-attention operation can be represented by the </span><span><span class="kobospan" id="kobo.1283.1">following equations:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1284.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/204.png" class="calibre207"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1285.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/205.png" class="calibre208"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1286.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/206.png" class="calibre209"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1287.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;msup&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;msqrt&gt;&lt;msub&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;/msub&gt;&lt;/msqrt&gt;&lt;/mfrac&gt;&lt;/mfenced&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/207.png" class="calibre210"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1288.1">Here, </span><em class="italic"><span class="kobospan" id="kobo.1289.1">X</span></em><span class="kobospan" id="kobo.1290.1"> is the input sequence, </span><span class="kobospan" id="kobo.1291.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;Q&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/208.png" class="calibre211"/></span><span class="kobospan" id="kobo.1292.1">, </span><span class="kobospan" id="kobo.1293.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;K&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/209.png" class="calibre212"/></span><span class="kobospan" id="kobo.1294.1">, and </span><span class="kobospan" id="kobo.1295.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;V&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/210.png" class="calibre213"/></span><span class="kobospan" id="kobo.1296.1"> are learned projection matrices for the query, key, and value vectors, respectively, </span><span class="kobospan" id="kobo.1297.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;K&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/211.png" class="calibre214"/></span><span class="kobospan" id="kobo.1298.1"> is the dimensionality of the key vectors, and </span><span class="kobospan" id="kobo.1299.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;Q&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/212.png" class="calibre215"/></span><span class="kobospan" id="kobo.1300.1"> is a learned projection matrix that maps the output of the attention mechanism to the </span><span><span class="kobospan" id="kobo.1301.1">final output.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1302.1">The advantages of transformers include their ability to handle variable-length input sequences, their ability to capture long-range dependencies in the data, and their state-of-the-art performance on many </span><span><span class="kobospan" id="kobo.1303.1">NLP tasks.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1304.1">The disadvantages of transformers include their high computational and memory requirements, their sensitivity to hyperparameter tuning, and their difficulty in handling tasks that require explicit modeling of </span><span><span class="kobospan" id="kobo.1305.1">sequential dynamics.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1306.1">These are just a few of the most popular machine learning models. </span><span class="kobospan" id="kobo.1306.2">The choice of model depends on the problem at hand, the size and quality of the data, and the desired outcome. </span><span class="kobospan" id="kobo.1306.3">Now that </span><a id="_idIndexMarker241" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1307.1">we have explored the most common machine learning models, we wil</span><a id="_idTextAnchor080" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1308.1">l explain model underfitting and overfitting, which happens during the </span><span><span class="kobospan" id="kobo.1309.1">training process.</span></span></p>
<h1 id="_idParaDest-59" class="calibre4"><a id="_idTextAnchor081" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1310.1">Model underfitting and overfitting</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.1311.1">In machine learning, the ultimate</span><a id="_idIndexMarker242" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1312.1"> goal is to build a model that can generalize well on unseen data. </span><span class="kobospan" id="kobo.1312.2">However, sometimes, a model can fail to achieve this goal due to either underfitting </span><span><span class="kobospan" id="kobo.1313.1">or overfitting.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1314.1">Underfitting occurs when a model is</span><a id="_idIndexMarker243" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1315.1"> too simple to capture the underlying patterns in the data. </span><span class="kobospan" id="kobo.1315.2">In other words, the model can’t learn the relationship between the features and the target variable properly. </span><span class="kobospan" id="kobo.1315.3">This can result in poor performance on both the training and testing data. </span><span class="kobospan" id="kobo.1315.4">For example, in </span><span><em class="italic"><span class="kobospan" id="kobo.1316.1">Figure 3</span></em></span><em class="italic"><span class="kobospan" id="kobo.1317.1">.4</span></em><span class="kobospan" id="kobo.1318.1">, we can see that the model is underfitted, and it cannot present the data very well. </span><span class="kobospan" id="kobo.1318.2">This is not what we like in machine learning models, and we usually like to see a precise model, as shown in </span><span><em class="italic"><span class="kobospan" id="kobo.1319.1">Figure 3</span></em></span><span><em class="italic"><span class="kobospan" id="kobo.1320.1">.5</span></em></span><span><span class="kobospan" id="kobo.1321.1">:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer228">
<span class="kobospan" id="kobo.1322.1"><img alt="Figure 3.4 – The machine learning model underfitting on the training data" src="image/B18949_03_4.jpg" class="calibre3"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.1323.1">Figure 3.4 – The machine learning model underfitting on the training data</span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1324.1">Underfitting happens when the model is</span><a id="_idIndexMarker244" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1325.1"> not trained well, or the model complexity is not</span><a id="_idIndexMarker245" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1326.1"> enough to catch the underlying pattern in the data. </span><span class="kobospan" id="kobo.1326.2">To solve this problem, we can use more complex models, and continue the </span><span><span class="kobospan" id="kobo.1327.1">training process:</span></span></p>
<p class="calibre6"> </p>
<div class="calibre2">
<div class="img---figure" id="_idContainer229">
<span class="kobospan" id="kobo.1328.1"><img alt="Figure 3.5 – Optimal fitting of the machine learning model on the training data" src="image/B18949_03_5.jpg" class="calibre3"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.1329.1">Figure 3.5 – Optimal fitting of the machine learning model on the training data</span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1330.1">Optimal fitting happens when the</span><a id="_idIndexMarker246" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1331.1"> model captures the pattern in the data pretty well but </span><a id="_idIndexMarker247" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1332.1">does not overfit every single sample. </span><span class="kobospan" id="kobo.1332.2">This helps the model work better on </span><span><span class="kobospan" id="kobo.1333.1">unseen data:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer230">
<span class="kobospan" id="kobo.1334.1"><img alt="Figure 3.6 – Overfitting the model on the training data" src="image/B18949_03_6.jpg" class="calibre3"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.1335.1">Figure 3.6 – Overfitting the model on the training data</span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1336.1">On the other hand, overfitting </span><a id="_idIndexMarker248" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1337.1">occurs when a model is too complex and fits the training data too closely, which can lead to poor generalization on new, unseen data, as </span><a id="_idIndexMarker249" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1338.1">shown in </span><span><em class="italic"><span class="kobospan" id="kobo.1339.1">Figure 3</span></em></span><em class="italic"><span class="kobospan" id="kobo.1340.1">.6</span></em><span class="kobospan" id="kobo.1341.1">. </span><span class="kobospan" id="kobo.1341.2">This happens when the model learns the noise or random fluctuations in the training data, rather than the underlying patterns. </span><span class="kobospan" id="kobo.1341.3">In other words, the model becomes too specialized for the training data and does not perform well on the testing data. </span><span class="kobospan" id="kobo.1341.4">As shown in the preceding figure, the model is overfitted, and the model tried to predict every single sample very precisely. </span><span class="kobospan" id="kobo.1341.5">The problem with this model is that it does not learn the general pattern, and learns the pattern of each individual sample, which makes it work poorly when facing new, </span><span><span class="kobospan" id="kobo.1342.1">unseen records.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1343.1">A useful way to understand the</span><a id="_idIndexMarker250" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1344.1"> trade-off between underfitting and overfitting is through the bias-variance</span><a id="_idIndexMarker251" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1345.1"> trade-off. </span><span class="kobospan" id="kobo.1345.2">Bias refers to the difference between the predicted values of the model and the actual values in the training data. </span><span class="kobospan" id="kobo.1345.3">A high bias means that the model is not complex enough to capture the underlying patterns in the data and underfits the data (</span><span><em class="italic"><span class="kobospan" id="kobo.1346.1">Figure 3</span></em></span><em class="italic"><span class="kobospan" id="kobo.1347.1">.7</span></em><span class="kobospan" id="kobo.1348.1">). </span><span class="kobospan" id="kobo.1348.2">An underfit model has poor performance on both the training and </span><span><span class="kobospan" id="kobo.1349.1">testing data:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer231">
<span class="kobospan" id="kobo.1350.1"><img alt="Figure 3.7 – High bias" src="image/B18949_03_7.jpg" class="calibre3"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.1351.1">Figure 3.7 – High bias</span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1352.1">Variance, on the other hand, refers to</span><a id="_idIndexMarker252" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1353.1"> the sensitivity of the model to small fluctuations</span><a id="_idIndexMarker253" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1354.1"> in the training data. </span><span class="kobospan" id="kobo.1354.2">A high variance means that the model is overly complex and overfits the data, which leads to poor generalization performance on new data. </span><span class="kobospan" id="kobo.1354.3">An overfit model has good performance on the training data but poor performance on the </span><span><span class="kobospan" id="kobo.1355.1">testing data:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer232">
<span class="kobospan" id="kobo.1356.1"><img alt="Figure 3.8 – Just right (not high bias, not high variance)" src="image/B18949_03_8.jpg" class="calibre3"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.1357.1">Figure 3.8 – Just right (not high bias, not high variance)</span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1358.1">To strike a balance between bias</span><a id="_idIndexMarker254" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1359.1"> and variance, we need to choose a model that is </span><a id="_idIndexMarker255" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1360.1">neither too simple nor too complex. </span><span class="kobospan" id="kobo.1360.2">As mentioned previously, this is often referred to as the bias-variance trade-off (</span><span><em class="italic"><span class="kobospan" id="kobo.1361.1">Figure 3</span></em></span><em class="italic"><span class="kobospan" id="kobo.1362.1">.8</span></em><span class="kobospan" id="kobo.1363.1">). </span><span class="kobospan" id="kobo.1363.2">A model with a high bias and low variance can be improved by increasing the complexity of the model, while a model with a high variance and low bias can be improved by decreasing the complexity of </span><span><span class="kobospan" id="kobo.1364.1">the model:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer233">
<span class="kobospan" id="kobo.1365.1"><img alt="Figure 3.9 – High variance" src="image/B18949_03_9.jpg" class="calibre3"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.1366.1">Figure 3.9 – High variance</span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1367.1">There are several methods to reduce bias and variance in a model. </span><span class="kobospan" id="kobo.1367.2">One common approach is regularization, which</span><a id="_idIndexMarker256" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1368.1"> adds a penalty term to the loss function to control the complexity of the model. </span><span class="kobospan" id="kobo.1368.2">Another approach is to use ensembles, which combine</span><a id="_idIndexMarker257" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1369.1"> multiple models to improve the overall performance by reducing the variance. </span><span class="kobospan" id="kobo.1369.2">Cross-validation can also be used to evaluate the model’s performance and tune its hyperparameters to find the optimal balance between bias </span><span><span class="kobospan" id="kobo.1370.1">and variance.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1371.1">Overall, understanding bias and variance is crucial in machine learning as it helps us to choose an appropriate model and identify the sources of error in </span><span><span class="kobospan" id="kobo.1372.1">the model.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1373.1">Bias refers to the error that is introduced by approximating a real-world problem with a simplified model. </span><span class="kobospan" id="kobo.1373.2">Variance, on the other hand, refers to the error that is introduced by the model’s sensitivity to small fluctuations in the </span><span><span class="kobospan" id="kobo.1374.1">training data.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1375.1">When a model has high bias</span><a id="_idIndexMarker258" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1376.1"> and low variance, it is underfitting. </span><span class="kobospan" id="kobo.1376.2">This means that the model is not capturing the </span><a id="_idIndexMarker259" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1377.1">complexity of the problem and is making overly simplistic assumptions. </span><span class="kobospan" id="kobo.1377.2">When a model has low bias and high variance, it is overfitting. </span><span class="kobospan" id="kobo.1377.3">This means that the model is too sensitive to the training data and is fitting the noise instead of the </span><span><span class="kobospan" id="kobo.1378.1">underlying patterns.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1379.1">To overcome underfitting, we can try increasing the complexity of the model, adding more features, or using a more sophisticated algorithm. </span><span class="kobospan" id="kobo.1379.2">To prevent overfitting, several methods can </span><span><span class="kobospan" id="kobo.1380.1">be used:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.1381.1">Cross-validation</span></strong><span class="kobospan" id="kobo.1382.1">: Assessing the </span><a id="_idIndexMarker260" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1383.1">performance of machine learning models is essential. </span><span class="kobospan" id="kobo.1383.2">Cross-validation serves as a method for assessing the effectiveness of a machine learning model. </span><span class="kobospan" id="kobo.1383.3">It entails training the model on one portion of the data and testing it on another. </span><span class="kobospan" id="kobo.1383.4">By employing distinct subsets for training and evaluation, cross-validation mitigates the risk of overfitting. </span><span class="kobospan" id="kobo.1383.5">Further elaboration on this technique will be provided in the subsequent section on </span><span><span class="kobospan" id="kobo.1384.1">data splitting.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.1385.1">Regularization</span></strong><span class="kobospan" id="kobo.1386.1">: Regularization is </span><a id="_idIndexMarker261" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1387.1">a technique that’s used to add a penalty term to the loss function during training, which helps to reduce the complexity of the model and prevent overfitting. </span><span class="kobospan" id="kobo.1387.2">There are different types of regularization, including L1 regularization (LASSO), L2 regularization (ridge), and elastic </span><span><span class="kobospan" id="kobo.1388.1">net regularization.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.1389.1">Early stopping</span></strong><span class="kobospan" id="kobo.1390.1">: Early stopping</span><a id="_idIndexMarker262" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1391.1"> is a technique that’s used to stop the training process when the performance of the model on the validation data starts to degrade. </span><span class="kobospan" id="kobo.1391.2">This helps to prevent overfitting by stopping the model from continuing to learn from the training data when it has already reached its maximum performance. </span><span class="kobospan" id="kobo.1391.3">This technique is usually used in iterative algorithms such as deep learning methods, where the model is being trained for multiple iterations (epochs). </span><span class="kobospan" id="kobo.1391.4">To use early stopping, we usually train the model while evaluating the model performance on the training and validation subsets. </span><span class="kobospan" id="kobo.1391.5">The model’s performance usually improves on the training set with more</span><a id="_idIndexMarker263" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1392.1"> training, but since the model has not seen the validation</span><a id="_idIndexMarker264" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1393.1"> set, the validation error usually decreases initially and at some point, starts increasing again. </span><span class="kobospan" id="kobo.1393.2">This point is where the model starts overfitting. </span><span class="kobospan" id="kobo.1393.3">By visualizing the training and validation error of the model during training, we can identify and stop the model at this point (</span><span><em class="italic"><span class="kobospan" id="kobo.1394.1">Figure 3</span></em></span><span><em class="italic"><span class="kobospan" id="kobo.1395.1">.10</span></em></span><span><span class="kobospan" id="kobo.1396.1">):</span></span></li>
</ul>
<div class="calibre2">
<div class="img---figure" id="_idContainer234">
<span class="kobospan" id="kobo.1397.1"><img alt="Figure 3.10 – Early stopping" src="image/B18949_03_10.jpg" class="calibre3"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.1398.1">Figure 3.10 – Early stopping</span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.1399.1">Dropout</span></strong><span class="kobospan" id="kobo.1400.1">: Dropout is a technique in deep learning models that is used to randomly drop out some neurons during training, which helps to prevent the model from relying too heavily </span><a id="_idIndexMarker265" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1401.1">on a small subset of features or neurons and overfitting the training data. </span><span class="kobospan" id="kobo.1401.2">By dropping the weight of neurons in the model during the process, we make the model learn the general pattern and prevent it from memorizing the</span><a id="_idIndexMarker266" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1402.1"> training </span><span><span class="kobospan" id="kobo.1403.1">data (overfitting).</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.1404.1">Data augmentation</span></strong><span class="kobospan" id="kobo.1405.1">: Data augmentation</span><a id="_idIndexMarker267" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1406.1"> is a method that we can use to artificially expand the training data size by applying transformations, such as rotation, scaling, and flipping, to the existing dataset, which helps us to extend our training data. </span><span class="kobospan" id="kobo.1406.2">This strategy aids in mitigating overfitting by offering the model a more diverse set of examples to </span><span><span class="kobospan" id="kobo.1407.1">learn from.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.1408.1">Ensemble methods</span></strong><span class="kobospan" id="kobo.1409.1">: Ensemble methods are techniques that are used to combine multiple models to</span><a id="_idIndexMarker268" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1410.1"> improve their performance and prevent overfitting. </span><span class="kobospan" id="kobo.1410.2">This can be done by using techniques such as bagging, boosting, </span><span><span class="kobospan" id="kobo.1411.1">or stacking.</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.1412.1">By using these techniques, it is</span><a id="_idIndexMarker269" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1413.1"> possible to prevent overfitting and build models that generalize well to new, unseen data. </span><span class="kobospan" id="kobo.1413.2">In practice, it is important to monitor both the training and testing performance of the model and make adjustments accordingly to achieve the best possible gen</span><a id="_idTextAnchor082" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1414.1">eralization performance. </span><span class="kobospan" id="kobo.1414.2">We will explain how to split our data into training and testing in the </span><span><span class="kobospan" id="kobo.1415.1">next section.</span></span></p>
<h1 id="_idParaDest-60" class="calibre4"><a id="_idTextAnchor083" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1416.1">Splitting data</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.1417.1">When developing a </span><a id="_idIndexMarker270" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1418.1">machine learning model, it’s important to split the data into training, validation, and test sets; this is called data splitting. </span><span class="kobospan" id="kobo.1418.2">This is done to evaluate the performance of the model on new, unseen data and to </span><span><span class="kobospan" id="kobo.1419.1">prevent overfitting.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1420.1">The most common method for splitting the data is the train-test split, which splits the data into two sets: the training set, which is used to train the model, and the test set, which is used to evaluate the performance of the model. </span><span class="kobospan" id="kobo.1420.2">The data is randomly divided into two sets, with a typical split being 80% of the data for training and 20% for testing. </span><span class="kobospan" id="kobo.1420.3">Using this approach the model will be trained using the majority of the data (training data) and then tested on the remaining data (test set). </span><span class="kobospan" id="kobo.1420.4">Using this approach, we can ensure that the model’s performance is based on new, </span><span><span class="kobospan" id="kobo.1421.1">unseen data.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1422.1">Most of the time in machine learning model development, we have a set of hyperparameters for </span><a id="_idIndexMarker271" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1423.1">our model that we like to tune (we will explain hyperparameter tuning in the next subsection). </span><span class="kobospan" id="kobo.1423.2">In this case, we like to make sure that the performance that we get on the test set is reliable and not just by chance based on a set of hyperparameters. </span><span class="kobospan" id="kobo.1423.3">In this case, based on the size of our training data, we can divide the data into 60%, 20%, and 20% (or 70%, 15%, and 15%) for training, validation, and testing. </span><span class="kobospan" id="kobo.1423.4">In this case, we train the model on the training data and select the set of hyperparameters that give us the best performance on the validation set. </span><span class="kobospan" id="kobo.1423.5">We then report the actual model performance on the test set, which has not been seen or used before during model training or </span><span><span class="kobospan" id="kobo.1424.1">hyperparameter selection.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1425.1">A more advanced method for splitting the data, especially when the size of our training data is limited, is k-fold cross-validation. </span><span class="kobospan" id="kobo.1425.2">In this method, the data is split into </span><em class="italic"><span class="kobospan" id="kobo.1426.1">k</span></em><span class="kobospan" id="kobo.1427.1"> equally sized “folds,” and the model is trained and tested </span><em class="italic"><span class="kobospan" id="kobo.1428.1">k</span></em><span class="kobospan" id="kobo.1429.1"> times, with each fold being used as the test set once and the remaining folds used as the training set. </span><span class="kobospan" id="kobo.1429.2">The results of each fold are then averaged to get an overall measure of the model’s performance. </span><span class="kobospan" id="kobo.1429.3">K-fold cross-validation is useful for small datasets where the train-test split may result in a large variance in performance evaluation. </span><span class="kobospan" id="kobo.1429.4">In this case, we report the average, minimum, and maximum performance of the model on each of the </span><em class="italic"><span class="kobospan" id="kobo.1430.1">k</span></em><span class="kobospan" id="kobo.1431.1"> folds, as shown in </span><span><em class="italic"><span class="kobospan" id="kobo.1432.1">Figure 3</span></em></span><span><em class="italic"><span class="kobospan" id="kobo.1433.1">.11</span></em></span><span><span class="kobospan" id="kobo.1434.1">.</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer235">
<span class="kobospan" id="kobo.1435.1"><img alt="Figure 3.11 – K-fold cross-validation" src="image/B18949_03_11.jpg" class="calibre3"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.1436.1">Figure 3.11 – K-fold cross-validation</span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1437.1">Another variant of k-fold cross-validation is stratified k-fold cross-validation, which ensures that the distribution of the target variable is consistent across all folds. </span><span class="kobospan" id="kobo.1437.2">This is useful when dealing with imbalanced datasets, where the number of instances of one class is much smaller than </span><span><span class="kobospan" id="kobo.1438.1">the others.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1439.1">Time series data</span><a id="_idIndexMarker272" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1440.1"> requires special attention when splitting. </span><span class="kobospan" id="kobo.1440.2">In this case, we typically use a method called time </span><a id="_idIndexMarker273" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1441.1">series cross-validation, which preserves the temporal order of the data. </span><span class="kobospan" id="kobo.1441.2">In this method, the data is split into multiple segments, with each segment representing a fixed time interval. </span><span class="kobospan" id="kobo.1441.3">The model is then trained on the past data and tested on the future data. </span><span class="kobospan" id="kobo.1441.4">This helps to evaluate the performance of the model in real-world scenarios. </span><span class="kobospan" id="kobo.1441.5">You can see an example of how to split the data in a time series problem in </span><span><em class="italic"><span class="kobospan" id="kobo.1442.1">Figure 3</span></em></span><span><em class="italic"><span class="kobospan" id="kobo.1443.1">.12</span></em></span><span><span class="kobospan" id="kobo.1444.1">:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer236">
<span class="kobospan" id="kobo.1445.1"><img alt="Figure 3.12 – Time series data splitting" src="image/B18949_03_12.jpg" class="calibre3"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.1446.1">Figure 3.12 – Time series data splitting</span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1447.1">In all cases, it’s important to</span><a id="_idIndexMarker274" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1448.1"> ensure that the split is done randomly but with the same random seed each time to ensure the reproducibility of the results. </span><span class="kobospan" id="kobo.1448.2">It’s also important to ensure that the split is representative of the underlying data – that is, the distribution of the target variable should be consistent across all sets. </span><span class="kobospan" id="kobo.1448.3">Once we have split the data into different subsets for training and testing our model, we can try to find the best</span><a id="_idTextAnchor084" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1449.1"> set of hyperparameters for our model. </span><span class="kobospan" id="kobo.1449.2">This process is called hyperparameter tuning and will be </span><span><span class="kobospan" id="kobo.1450.1">explained next.</span></span></p>
<h1 id="_idParaDest-61" class="calibre4"><a id="_idTextAnchor085" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1451.1">Hyperparameter tuning</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.1452.1">Hyperparameter tuning is an important step in the machine learning process that involves selecting the best set of </span><a id="_idIndexMarker275" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1453.1">hyperparameters for a given model. </span><span class="kobospan" id="kobo.1453.2">Hyperparameters are values that are set before the training process begins and can have a significant impact on the model’s performance. </span><span class="kobospan" id="kobo.1453.3">Examples of hyperparameters include learning rate, regularization strength, number of hidden layers in a neural network, and </span><span><span class="kobospan" id="kobo.1454.1">many others.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1455.1">The process of hyperparameter tuning involves selecting the best combination of hyperparameters that results in the optimal performance of the model. </span><span class="kobospan" id="kobo.1455.2">This is typically done by searching through a predefined set of hyperparameters and evaluating their performance on a </span><span><span class="kobospan" id="kobo.1456.1">validation set.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1457.1">There are several methods for hyperparameter tuning, including grid search, random search, and Bayesian optimization. </span><span class="kobospan" id="kobo.1457.2">Grid search involves creating a grid of all possible hyperparameter combinations and evaluating each one on a validation set to determine the optimal </span><a id="_idIndexMarker276" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1458.1">set of hyperparameters. </span><span class="kobospan" id="kobo.1458.2">Random search, on the other hand, randomly samples hyperparameters from a predefined distribution and evaluates their performance on a </span><span><span class="kobospan" id="kobo.1459.1">validation set.</span></span></p>
<p class="calibre6"><strong class="bold"><span class="kobospan" id="kobo.1460.1">Random search</span></strong><span class="kobospan" id="kobo.1461.1"> and </span><strong class="bold"><span class="kobospan" id="kobo.1462.1">grid search</span></strong><span class="kobospan" id="kobo.1463.1"> are </span><a id="_idIndexMarker277" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1464.1">methods that are used to search the search space, entirely or randomly, without </span><a id="_idIndexMarker278" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1465.1">considering previous hyperparameter results. </span><span class="kobospan" id="kobo.1465.2">Thus, these methods are inefficient. </span><span class="kobospan" id="kobo.1465.3">An alternative Bayesian optimization method has been proposed that iteratively computes the posterior distribution of the function and considers past evaluations to find the best hyperparameters. </span><span class="kobospan" id="kobo.1465.4">Using this approach, we can find the best set of hyperparameters with </span><span><span class="kobospan" id="kobo.1466.1">less iterations.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1467.1">Bayesian optimization utilizes past evaluations to probabilistically map hyperparameters to objective function scores, as demonstrated in the </span><span><span class="kobospan" id="kobo.1468.1">following equation:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1469.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/213.png" class="calibre216"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1470.1">Here are the steps Bayesian </span><span><span class="kobospan" id="kobo.1471.1">optimization undertakes:</span></span></p>
<ol class="calibre16">
<li class="calibre15"><span class="kobospan" id="kobo.1472.1">It develops a surrogate probabilistic model for the </span><span><span class="kobospan" id="kobo.1473.1">objective function.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1474.1">It identifies the optimal hyperparameters based on </span><span><span class="kobospan" id="kobo.1475.1">the surrogate.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1476.1">It utilizes these hyperparameters in the actual </span><span><span class="kobospan" id="kobo.1477.1">objective function.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1478.1">It updates the surrogate model to integrate the </span><span><span class="kobospan" id="kobo.1479.1">latest results.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1480.1">It reiterates </span><em class="italic"><span class="kobospan" id="kobo.1481.1">steps 2</span></em><span class="kobospan" id="kobo.1482.1"> to </span><em class="italic"><span class="kobospan" id="kobo.1483.1">4</span></em><span class="kobospan" id="kobo.1484.1"> until it reaches the maximum iteration count or </span><span><span class="kobospan" id="kobo.1485.1">time limit.</span></span></li>
</ol>
<p class="calibre6"><strong class="bold"><span class="kobospan" id="kobo.1486.1">Sequential model-based optimization</span></strong><span class="kobospan" id="kobo.1487.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.1488.1">SMBO</span></strong><span class="kobospan" id="kobo.1489.1">) methods are a formalization of Bayesian optimization, with trials run</span><a id="_idIndexMarker279" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1490.1"> one after another, trying better hyperparameters each time and updating a probability model (surrogate). </span><span class="kobospan" id="kobo.1490.2">SMBO methods differ in </span><em class="italic"><span class="kobospan" id="kobo.1491.1">steps 3</span></em><span class="kobospan" id="kobo.1492.1"> and </span><em class="italic"><span class="kobospan" id="kobo.1493.1">4</span></em><span class="kobospan" id="kobo.1494.1"> – specifically, how they build a surrogate of the objective function and the criteria used to select the next hyperparameters. </span><span class="kobospan" id="kobo.1494.2">These variants include Gaussian processes, random forest regressions, and tree-structured Parzen estimators, </span><span><span class="kobospan" id="kobo.1495.1">among others.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1496.1">In low-dimensional problems with numerical hyperparameters, Bayesian optimization is considered the best available hyperparameter optimization method. </span><span class="kobospan" id="kobo.1496.2">However, it is restricted to problems of </span><span><span class="kobospan" id="kobo.1497.1">moderate dimension.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1498.1">In addition to these methods, there are also several libraries available that automate the process of hyperparameter tuning. </span><span class="kobospan" id="kobo.1498.2">Examples of these libraries include scikit-learn’s </span><strong class="source-inline"><span class="kobospan" id="kobo.1499.1">GridSearchCV</span></strong><span class="kobospan" id="kobo.1500.1"> and </span><strong class="source-inline"><span class="kobospan" id="kobo.1501.1">RandomizedSearchCV</span></strong><span class="kobospan" id="kobo.1502.1">, </span><strong class="source-inline"><span class="kobospan" id="kobo.1503.1">Keras Tuner</span></strong><span class="kobospan" id="kobo.1504.1">, and </span><strong class="source-inline"><span class="kobospan" id="kobo.1505.1">Optuna</span></strong><span class="kobospan" id="kobo.1506.1">. </span><span class="kobospan" id="kobo.1506.2">These libraries allow for efficient hyperparameter tuning and can significantly improve the performance of machine </span><span><span class="kobospan" id="kobo.1507.1">learning models.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1508.1">Hyperparameter optimization in machine learning can be a complex and time-consuming process. </span><span class="kobospan" id="kobo.1508.2">Two primary complexity challenges arise in the search process: the trial execution time and the complexity of the search space, including the number of evaluated hyperparameter combinations. </span><span class="kobospan" id="kobo.1508.3">In deep learning, these challenges are especially pertinent due to the extensive search space and the utilization of large </span><span><span class="kobospan" id="kobo.1509.1">training sets.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1510.1">To address these issues and reduce the search space, some standard techniques may be used. </span><span class="kobospan" id="kobo.1510.2">For example, reducing the size of the training dataset based on statistical sampling or applying feature selection techniques can help reduce the execution time of each trial. </span><span class="kobospan" id="kobo.1510.3">Additionally, identifying the most important hyperparameters for optimization and using additional objective functions beyond just accuracy, such as the number of operations or optimization time, can help reduce the complexity of the </span><span><span class="kobospan" id="kobo.1511.1">search space.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1512.1">By combining accuracy with visualization through a deconvolution network, researchers have achieved superior results. </span><span class="kobospan" id="kobo.1512.2">However, it’s important to note that these techniques are not exhaustive, and the best approach may depend on the specific problem </span><span><span class="kobospan" id="kobo.1513.1">at hand.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1514.1">Another common approach</span><a id="_idIndexMarker280" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1515.1"> for improving model performance is to use multiple</span><a id="_idTextAnchor086" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1516.1"> models in parallel; these are called ensemble models. </span><span class="kobospan" id="kobo.1516.2">They are very useful in dealing with machine </span><span><span class="kobospan" id="kobo.1517.1">learning problems.</span></span></p>
</div>


<div id="_idContainer283" class="calibre2">
<h1 id="_idParaDest-62" class="calibre4"><a id="_idTextAnchor087" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1518.1">Ensemble models</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.1519.1">Ensemble modeling is a</span><a id="_idIndexMarker281" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1520.1"> technique in machine learning that combines the predictions of multiple models to improve overall performance. </span><span class="kobospan" id="kobo.1520.2">The idea behind ensemble models is that multiple models can be better than a single model as different models may capture differen</span><a id="_idTextAnchor088" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1521.1">t patterns in </span><span><span class="kobospan" id="kobo.1522.1">the data.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1523.1">There are several types of ensemble models, all of which we’ll cover in the </span><span><span class="kobospan" id="kobo.1524.1">following sections.</span></span></p>
<h2 id="_idParaDest-63" class="calibre7"><a id="_idTextAnchor089" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1525.1">Bagging</span></h2>
<p class="calibre6"><strong class="bold"><span class="kobospan" id="kobo.1526.1">Bootstrap aggregating</span></strong><span class="kobospan" id="kobo.1527.1">, also known as </span><strong class="bold"><span class="kobospan" id="kobo.1528.1">bagging</span></strong><span class="kobospan" id="kobo.1529.1">, is an ensemble method that combines multiple independent </span><a id="_idIndexMarker282" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1530.1">models trained on different subsets of the training </span><a id="_idIndexMarker283" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1531.1">data to reduce variance and improve </span><span><span class="kobospan" id="kobo.1532.1">model generalization.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1533.1">The bagging algorithm can be summarized </span><span><span class="kobospan" id="kobo.1534.1">as follows:</span></span></p>
<ol class="calibre16">
<li class="calibre15"><span class="kobospan" id="kobo.1535.1">Given a training dataset of size </span><em class="italic"><span class="kobospan" id="kobo.1536.1">n</span></em><span class="kobospan" id="kobo.1537.1">, create </span><em class="italic"><span class="kobospan" id="kobo.1538.1">m</span></em><span class="kobospan" id="kobo.1539.1"> bootstrap samples of size </span><em class="italic"><span class="kobospan" id="kobo.1540.1">n</span></em><span class="kobospan" id="kobo.1541.1"> (that is, sample </span><em class="italic"><span class="kobospan" id="kobo.1542.1">n</span></em><span class="kobospan" id="kobo.1543.1"> instances with replacement </span><span><em class="italic"><span class="kobospan" id="kobo.1544.1">m</span></em></span><span><span class="kobospan" id="kobo.1545.1"> times).</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1546.1">Train a base model (for example, a decision tree) on each bootstrap </span><span><span class="kobospan" id="kobo.1547.1">sample independently.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1548.1">Aggregate the predictions of all base models to obtain the ensemble prediction. </span><span class="kobospan" id="kobo.1548.2">This can be done by either taking the majority vote (in the case of classification) or the average (in the case </span><span><span class="kobospan" id="kobo.1549.1">of regression).</span></span></li>
</ol>
<p class="calibre6"><span class="kobospan" id="kobo.1550.1">The bagging algorithm is particularly effective when the base models are unstable (that is, have high variance), such as decision trees, and when the training dataset </span><span><span class="kobospan" id="kobo.1551.1">is small.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1552.1">The equation for aggregating the predictions of the base models depends on the type of problem (classification or regression). </span><span class="kobospan" id="kobo.1552.2">For classification, the ensemble prediction is obtained by taking the </span><span><span class="kobospan" id="kobo.1553.1">majority vote:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1554.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;Y&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/msub&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/munderover&gt;&lt;mrow&gt;&lt;mi&gt;I&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/214.png" class="calibre217"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1555.1">Here, </span><span class="kobospan" id="kobo.1556.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/215.png" class="calibre218"/></span><span class="kobospan" id="kobo.1557.1"> is the predicted class of the </span><span class="kobospan" id="kobo.1558.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/216.png" class="calibre219"/></span><span class="kobospan" id="kobo.1559.1"> base model for the </span><span class="kobospan" id="kobo.1560.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/217.png" class="calibre220"/></span><span class="kobospan" id="kobo.1561.1"> instance and </span><em class="italic"><span class="kobospan" id="kobo.1562.1">I()</span></em><span class="kobospan" id="kobo.1563.1"> is the indicator function (equal to 1 if x is true, and </span><span><span class="kobospan" id="kobo.1564.1">0 otherwise).</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1565.1">For regression, the ensemble prediction is obtained by taking the </span><span><span class="kobospan" id="kobo.1566.1">average score:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1567.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;Y&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/munderover&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/218.png" class="calibre221"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1568.1">Here, </span><span class="kobospan" id="kobo.1569.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/193.png" class="calibre196"/></span><span class="kobospan" id="kobo.1570.1"> is the predicted value of the </span><span class="kobospan" id="kobo.1571.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/220.png" class="calibre222"/></span> <span><span class="kobospan" id="kobo.1572.1">base model.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1573.1">The advantages of bagging are </span><a id="_idIndexMarker284" class="calibre5 pcalibre1 pcalibre"/><span><span class="kobospan" id="kobo.1574.1">as follows:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><span class="kobospan" id="kobo.1575.1">Improved model generalization by reducing variance </span><span><span class="kobospan" id="kobo.1576.1">and overfitting</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1577.1">Ability to handle high-dimensional datasets with </span><span><span class="kobospan" id="kobo.1578.1">complex relationships</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1579.1">Can be used with a variety of </span><span><span class="kobospan" id="kobo.1580.1">base models</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.1581.1">The disadvantages of bagging are </span><span><span class="kobospan" id="kobo.1582.1">as follows:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><span class="kobospan" id="kobo.1583.1">Increased model complexity and computation time due to the use of multiple </span><span><span class="kobospan" id="kobo.1584.1">base models</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1585.1">It can sometimes lead to overfitting if the base mod</span><a id="_idTextAnchor090" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1586.1">els are too complex or the dataset is </span><span><span class="kobospan" id="kobo.1587.1">too small</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1588.1">It does not work well when the base models are highly correlated </span><span><span class="kobospan" id="kobo.1589.1">or biased</span></span></li>
</ul>
<h2 id="_idParaDest-64" class="calibre7"><a id="_idTextAnchor091" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1590.1">Boosting</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.1591.1">Boosting is another popular </span><a id="_idIndexMarker285" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1592.1">ensemble learning technique that aims to improve the performance of weak classifiers by combining them into a stronger classifier. </span><span class="kobospan" id="kobo.1592.2">Unlike bagging, boosting</span><a id="_idIndexMarker286" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1593.1"> focuses on iteratively improving the accuracy of the classifier by adjusting the weights of the training examples. </span><span class="kobospan" id="kobo.1593.2">The basic idea behind boosting is to learn from the mistakes of the previous weak classifiers and to put more emphasis on the examples that were incorrectly classified in the </span><span><span class="kobospan" id="kobo.1594.1">previous iteration.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1595.1">There are several boosting algorithms, but one of the most popular ones is AdaBoost (short for adaptive</span><a id="_idIndexMarker287" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1596.1"> boosting). </span><span class="kobospan" id="kobo.1596.2">The AdaBoost algorithm works </span><span><span class="kobospan" id="kobo.1597.1">as follows:</span></span></p>
<ol class="calibre16">
<li class="calibre15"><span class="kobospan" id="kobo.1598.1">First, it initializes</span><a id="_idIndexMarker288" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1599.1"> the weights of</span><a id="_idIndexMarker289" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1600.1"> the training examples to </span><span><span class="kobospan" id="kobo.1601.1">be equal.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1602.1">Then, it trains a weak classifier on the </span><span><span class="kobospan" id="kobo.1603.1">training set.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1604.1">Next, it computes the weighted error rate of the </span><span><span class="kobospan" id="kobo.1605.1">weak classifier.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1606.1">After, it computes the importance of the weak classifier based on its weighted </span><span><span class="kobospan" id="kobo.1607.1">error rate.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1608.1">Then, it increases the weights of the examples that were misclassified by the </span><span><span class="kobospan" id="kobo.1609.1">weak classifier.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1610.1">Once it’s done this, it normalizes the weights of the examples so that they sum up </span><span><span class="kobospan" id="kobo.1611.1">to one.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1612.1">It repeats </span><em class="italic"><span class="kobospan" id="kobo.1613.1">steps 2</span></em><span class="kobospan" id="kobo.1614.1"> to </span><em class="italic"><span class="kobospan" id="kobo.1615.1">6</span></em><span class="kobospan" id="kobo.1616.1"> for a predetermined number of iterations or until the desired accuracy </span><span><span class="kobospan" id="kobo.1617.1">is achieved.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1618.1">Finally, it combines the weak classifiers into a strong classifier by assigning weights to them based on </span><span><span class="kobospan" id="kobo.1619.1">their importance.</span></span></li>
</ol>
<p class="calibre6"><span class="kobospan" id="kobo.1620.1">The final classifier is a weighted combination of the weak classifiers. </span><span class="kobospan" id="kobo.1620.2">The importance of each weak classifier is determined by its weighted error rate, which is computed as </span><span><span class="kobospan" id="kobo.1621.1">an equation:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1622.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/msubsup&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mi&gt;I&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/msubsup&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/221.png" class="calibre223"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1623.1">Here, </span><em class="italic"><span class="kobospan" id="kobo.1624.1">m</span></em><span class="kobospan" id="kobo.1625.1"> is the index of the weak classifier, </span><em class="italic"><span class="kobospan" id="kobo.1626.1">N</span></em><span class="kobospan" id="kobo.1627.1"> is the number of training examples, </span><span class="kobospan" id="kobo.1628.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/222.png" class="calibre224"/></span><span class="kobospan" id="kobo.1629.1"> is the weight of the </span><span class="kobospan" id="kobo.1630.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/220.png" class="calibre225"/></span><span class="kobospan" id="kobo.1631.1"> training example, </span><span class="kobospan" id="kobo.1632.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/196.png" class="calibre226"/></span><span class="kobospan" id="kobo.1633.1"> is the true label of the </span><span class="kobospan" id="kobo.1634.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/220.png" class="calibre225"/></span><span class="kobospan" id="kobo.1635.1"> training example, </span><span class="kobospan" id="kobo.1636.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:math&gt;" src="image/226.png" class="calibre227"/></span><span class="kobospan" id="kobo.1637.1"> is the prediction of the </span><span class="kobospan" id="kobo.1638.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/227.png" class="calibre228"/></span><span class="kobospan" id="kobo.1639.1"> weak classifier for the </span><span class="kobospan" id="kobo.1640.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/220.png" class="calibre222"/></span><span class="kobospan" id="kobo.1641.1"> training example, and </span><span class="kobospan" id="kobo.1642.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;I&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mfenced&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/229.png" class="calibre229"/></span><span class="kobospan" id="kobo.1643.1"> is an indicator function that returns 1 if the prediction of the weak classifier is incorrect and </span><span><span class="kobospan" id="kobo.1644.1">0 otherwise.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1645.1">The importance of the weak classifier is computed by the </span><span><span class="kobospan" id="kobo.1646.1">following equation:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1647.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;α&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;ln&lt;/mi&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;/mrow&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;/mrow&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/msub&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/230.png" class="calibre230"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1648.1">The weights of the examples are updated based on </span><span><span class="kobospan" id="kobo.1649.1">their importance:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1650.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;α&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/msub&gt;&lt;mi&gt;I&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/231.png" class="calibre231"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1651.1">The final classifier is then obtained by combining the </span><span><span class="kobospan" id="kobo.1652.1">weak classifiers:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1653.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;H&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;M&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;α&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/232.png" class="calibre232"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1654.1">Here, </span><em class="italic"><span class="kobospan" id="kobo.1655.1">M</span></em><span class="kobospan" id="kobo.1656.1"> is the total number of weak </span><a id="_idIndexMarker290" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1657.1">classifiers, </span><span class="kobospan" id="kobo.1658.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:math&gt;" src="image/233.png" class="calibre233"/></span><span class="kobospan" id="kobo.1659.1"> is the prediction of the </span><em class="italic"><span class="kobospan" id="kobo.1660.1">m-th</span></em><span class="kobospan" id="kobo.1661.1"> weak classifier, and </span><strong class="source-inline"><span class="kobospan" id="kobo.1662.1">sign()</span></strong><span class="kobospan" id="kobo.1663.1"> is a function that returns +1 if its argument is positive and -</span><span><span class="kobospan" id="kobo.1664.1">1 otherwise.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1665.1">Let’s look at some of the </span><a id="_idIndexMarker291" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1666.1">advantages </span><span><span class="kobospan" id="kobo.1667.1">of boosting:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><span class="kobospan" id="kobo.1668.1">Boosting can improve the accuracy of weak classifiers and can lead to a significant improvement </span><span><span class="kobospan" id="kobo.1669.1">in performance</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1670.1">Boosting is relatively easy to implement and can be applied to a wide range of </span><span><span class="kobospan" id="kobo.1671.1">classification problems</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1672.1">Boosting can handle noisy data and reduce the risk </span><span><span class="kobospan" id="kobo.1673.1">of overfitting</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.1674.1">Here are some of the disadvantages </span><span><span class="kobospan" id="kobo.1675.1">of boosting:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><span class="kobospan" id="kobo.1676.1">Boosting can be sensitive to outliers and can overfit to </span><span><span class="kobospan" id="kobo.1677.1">noisy data</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1678.1">Boosting can be computationally expensive, espe</span><a id="_idTextAnchor092" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1679.1">cially when dealing with </span><span><span class="kobospan" id="kobo.1680.1">large datasets</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1681.1">Boosting can be </span><a id="_idIndexMarker292" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1682.1">difficult to interpret as it involves combining multiple </span><span><span class="kobospan" id="kobo.1683.1">weak classifiers</span></span></li>
</ul>
<h2 id="_idParaDest-65" class="calibre7"><a id="_idTextAnchor093" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1684.1">Stacking</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.1685.1">Stacking is another</span><a id="_idIndexMarker293" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1686.1"> popular ensemble learning technique that combines the</span><a id="_idIndexMarker294" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1687.1"> predictions of multiple base models by training a higher-level model on their predictions. </span><span class="kobospan" id="kobo.1687.2">The idea behind stacking is to leverage the strengths of different base models to achieve better </span><span><span class="kobospan" id="kobo.1688.1">predictive performance.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1689.1">Here’s how </span><span><span class="kobospan" id="kobo.1690.1">stacking works:</span></span></p>
<ol class="calibre16">
<li class="calibre15"><span class="kobospan" id="kobo.1691.1">Divide the training data into two parts: the first part is used to train the base models, while the second part is used to create a new dataset of predictions from the </span><span><span class="kobospan" id="kobo.1692.1">base models.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1693.1">Train multiple base models on the first part of the </span><span><span class="kobospan" id="kobo.1694.1">training data.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1695.1">Use the trained base models to make predictions on the second part of the training data to create a new dataset </span><span><span class="kobospan" id="kobo.1696.1">of predictions.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1697.1">Train a higher-level model (also known as a metamodel or blender) on the new dataset </span><span><span class="kobospan" id="kobo.1698.1">of predictions.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1699.1">Use the trained higher-level model to make predictions on the </span><span><span class="kobospan" id="kobo.1700.1">test data.</span></span></li>
</ol>
<p class="calibre6"><span class="kobospan" id="kobo.1701.1">The higher-level model is typically a simple model such as a linear regression, logistic regression, or a decision tree. </span><span class="kobospan" id="kobo.1701.2">The idea is to use the predictions of the base models as input features for the higher-l</span><a id="_idTextAnchor094" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1702.1">evel model. </span><span class="kobospan" id="kobo.1702.2">This way, the higher-level model learns to combine the predictions of the base models to make more </span><span><span class="kobospan" id="kobo.1703.1">accurate predictions.</span></span></p>
<h2 id="_idParaDest-66" class="calibre7"><a id="_idTextAnchor095" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1704.1">Random forests</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.1705.1">One of the most </span><a id="_idIndexMarker295" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1706.1">commonly known ensemble models is random forest, where the model combines the predictions of multiple decision</span><a id="_idIndexMarker296" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1707.1"> trees and ou</span><a id="_idTextAnchor096" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1708.1">tputs the predictions. </span><span class="kobospan" id="kobo.1708.2">This is usually more accurate and prone to overfitting. </span><span class="kobospan" id="kobo.1708.3">We elaborated on Random Forest earlier in </span><span><span class="kobospan" id="kobo.1709.1">this chapter.</span></span></p>
<h2 id="_idParaDest-67" class="calibre7"><a id="_idTextAnchor097" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1710.1">Gradient boosting</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.1711.1">Gradient boosting is another</span><a id="_idIndexMarker297" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1712.1"> ensemble model that can be used for classification </span><a id="_idIndexMarker298" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1713.1">and regression tasks. </span><span class="kobospan" id="kobo.1713.2">It works by getting a weak classifier (such as a simple tree), and in each step tries to improve this weak classifier to build a better model. </span><span class="kobospan" id="kobo.1713.3">The main idea here is that the model tries to focus on its mistakes in each step and improve itself by fitting the model by correcting the errors made in </span><span><span class="kobospan" id="kobo.1714.1">previous trees.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1715.1">During each iteration, the algorithm computes the negative gradient of the loss function concerning the predicted values, followed by fitting a decision tree to these negative gradient values. </span><span class="kobospan" id="kobo.1715.2">The predictions of the new tree are then combined with the predictions of the previous trees, using a learning rate parameter that controls the contribution of each tree to the </span><span><span class="kobospan" id="kobo.1716.1">final prediction.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1717.1">The overall prediction of the gradient boosting model is obtained by summing up the predictions of all the trees, which are weighted by their respective </span><span><span class="kobospan" id="kobo.1718.1">learning rates.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1719.1">Let’s take a look at the equation for the gradient </span><span><span class="kobospan" id="kobo.1720.1">boosting algorithm.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1721.1">First, we initialize the model with a </span><span><span class="kobospan" id="kobo.1722.1">constant value:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1723.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;/msub&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/munderover&gt;&lt;mrow&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/234.png" class="calibre234"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1724.1">Here, </span><em class="italic"><span class="kobospan" id="kobo.1725.1">c</span></em><span class="kobospan" id="kobo.1726.1"> is a constant, </span><span class="kobospan" id="kobo.1727.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/196.png" class="calibre235"/></span><span class="kobospan" id="kobo.1728.1"> is the true label of the </span><em class="italic"><span class="kobospan" id="kobo.1729.1">i-th</span></em><span class="kobospan" id="kobo.1730.1"> sample, </span><em class="italic"><span class="kobospan" id="kobo.1731.1">N</span></em><span class="kobospan" id="kobo.1732.1"> is the number of samples, and </span><em class="italic"><span class="kobospan" id="kobo.1733.1">L</span></em><span class="kobospan" id="kobo.1734.1"> is the loss function, which is used to measure the error between the predicted and </span><span><span class="kobospan" id="kobo.1735.1">true labels.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1736.1">At each iteration, </span><em class="italic"><span class="kobospan" id="kobo.1737.1">m</span></em><span class="kobospan" id="kobo.1738.1">, the algorithm fits a decision tree to the negative gradient values of the loss function concerning the predicted values, </span><span class="kobospan" id="kobo.1739.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mo&gt;∇&lt;/mo&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/236.png" class="calibre236"/></span><span class="kobospan" id="kobo.1740.1">. </span><span class="kobospan" id="kobo.1740.2">The decision tree predicts the </span><a id="_idIndexMarker299" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1741.1">negative gradient values, which are then</span><a id="_idIndexMarker300" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1742.1"> used to update the predictions of the model via the </span><span><span class="kobospan" id="kobo.1743.1">following equation:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1744.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;η&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;*&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/237.png" class="calibre237"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1745.1">Here, </span><span class="kobospan" id="kobo.1746.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/238.png" class="calibre238"/></span><span class="kobospan" id="kobo.1747.1"> is the prediction of the model at the previous iteration, </span><em class="italic"><span class="kobospan" id="kobo.1748.1">η</span></em><span class="kobospan" id="kobo.1749.1"> is the learning rate, and </span><span class="kobospan" id="kobo.1750.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:math&gt;" src="image/239.png" class="calibre239"/></span><span class="kobospan" id="kobo.1751.1"> is the prediction of the decision tree at the </span><span><span class="kobospan" id="kobo.1752.1">current iteration.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1753.1">The final prediction of the model is obtained by combining the predictions of all </span><span><span class="kobospan" id="kobo.1754.1">the trees:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1755.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;/munderover&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;η&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/msub&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;*&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/240.png" class="calibre240"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1756.1">Here, </span><em class="italic"><span class="kobospan" id="kobo.1757.1">M</span></em><span class="kobospan" id="kobo.1758.1"> is the total number of trees in the model and </span><span class="kobospan" id="kobo.1759.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;η&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/241.png" class="calibre241"/></span><span class="kobospan" id="kobo.1760.1"> and </span><span class="kobospan" id="kobo.1761.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:math&gt;" src="image/242.png" class="calibre242"/></span><span class="kobospan" id="kobo.1762.1"> are the learning rate and prediction of the </span><em class="italic"><span class="kobospan" id="kobo.1763.1">m-th</span></em> <span><span class="kobospan" id="kobo.1764.1">tree, respectively.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1765.1">Let’s look at some of the advantages of </span><span><span class="kobospan" id="kobo.1766.1">gradient boosting:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><span class="kobospan" id="kobo.1767.1">High </span><span><span class="kobospan" id="kobo.1768.1">prediction accuracy</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1769.1">Handles both regression and </span><span><span class="kobospan" id="kobo.1770.1">classification problems</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1771.1">Can handle missing data </span><span><span class="kobospan" id="kobo.1772.1">and outliers</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1773.1">Can be used with various </span><span><span class="kobospan" id="kobo.1774.1">loss functions</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1775.1">Can handle </span><span><span class="kobospan" id="kobo.1776.1">high-dimensional data</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.1777.1">Now, let’s look at some of </span><span><span class="kobospan" id="kobo.1778.1">the disadvantages:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><span class="kobospan" id="kobo.1779.1">Sensitive to overfitting, especially when the number of trees </span><span><span class="kobospan" id="kobo.1780.1">is large</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1781.1">Computationally expensive and time-consuming to train, especially for </span><span><span class="kobospan" id="kobo.1782.1">la</span><a id="_idTextAnchor098" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1783.1">rge datasets</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1784.1">Requires careful tuning of hyperparameters, such as the number of trees, the learning rate, and the maximum depth of </span><span><span class="kobospan" id="kobo.1785.1">the trees</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.1786.1">With that, we have reviewed </span><a id="_idIndexMarker301" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1787.1">the ensemble models that can</span><a id="_idIndexMarker302" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1788.1"> help us improve our model performance. </span><span class="kobospan" id="kobo.1788.2">However, sometimes, our dataset has some features that we need to consider before we apply machine learning models. </span><span class="kobospan" id="kobo.1788.3">One common case is when we have an </span><span><span class="kobospan" id="kobo.1789.1">imbalanced dataset.</span></span></p>
<h1 id="_idParaDest-68" class="calibre4"><a id="_idTextAnchor099" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1790.1">Handling imbalanced data</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.1791.1">In most real-world problems, our</span><a id="_idIndexMarker303" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1792.1"> data is imbalanced, which means that the distribution of records from different classes (such as patients with and without cancer) is different. </span><span class="kobospan" id="kobo.1792.2">Handling imbalanced datasets is an important task in machine learning as it is common to have datasets with uneven class distribution. </span><span class="kobospan" id="kobo.1792.3">In such cases, the minority class is often under-represented, which can cause poor model performance and biased predictions. </span><span class="kobospan" id="kobo.1792.4">The reason behind this is that machine learning methods are trying to optimize their fitness function to minimize the error in the training set. </span><span class="kobospan" id="kobo.1792.5">Now, let’s say that we have 99% of the data from the positive class and 1% from the negative class. </span><span class="kobospan" id="kobo.1792.6">In this case, if the model predicts all records as positive, the error will be 1%; however, this model is not useful for us. </span><span class="kobospan" id="kobo.1792.7">That’s why, if we have an imbalanced dataset, we need to use various methods to handle imbalanced data. </span><span class="kobospan" id="kobo.1792.8">In general, we can have three categories of methods to handle </span><span><span class="kobospan" id="kobo.1793.1">imbalanced datasets:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.1794.1">Undersampling</span></strong><span class="kobospan" id="kobo.1795.1">: A very simple method that comes to mind is to use fewer training records from the </span><a id="_idIndexMarker304" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1796.1">majority class. </span><span class="kobospan" id="kobo.1796.2">This method works, but we need to consider that by using less training data, we are feeding less information to the model causes to have a less robust training and </span><span><span class="kobospan" id="kobo.1797.1">final model.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.1798.1">Resampling</span></strong><span class="kobospan" id="kobo.1799.1">: Resampling</span><a id="_idIndexMarker305" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1800.1"> methods involve modifying the original dataset to create a balanced distribution. </span><span class="kobospan" id="kobo.1800.2">This can be achieved by either oversampling the minority class (creating more samples of the minority class) or undersampling the majority class (removing samples from the majority class). </span><span class="kobospan" id="kobo.1800.3">Oversampling techniques include </span><strong class="bold"><span class="kobospan" id="kobo.1801.1">random oversampling</span></strong><span class="kobospan" id="kobo.1802.1">, </span><strong class="bold"><span class="kobospan" id="kobo.1803.1">Synthetic Minority Oversampling Technique</span></strong><span class="kobospan" id="kobo.1804.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.1805.1">SMOTE</span></strong><span class="kobospan" id="kobo.1806.1">), and </span><strong class="bold"><span class="kobospan" id="kobo.1807.1">Adaptive Synthetic Sampling</span></strong><span class="kobospan" id="kobo.1808.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.1809.1">ADASYN</span></strong><span class="kobospan" id="kobo.1810.1">). </span><span class="kobospan" id="kobo.1810.2">Undersampling techniques include </span><strong class="bold"><span class="kobospan" id="kobo.1811.1">random undersampling</span></strong><span class="kobospan" id="kobo.1812.1">, </span><strong class="bold"><span class="kobospan" id="kobo.1813.1">Tomek links</span></strong><span class="kobospan" id="kobo.1814.1">, and </span><span><strong class="bold"><span class="kobospan" id="kobo.1815.1">cluster cen</span><a id="_idTextAnchor100" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1816.1">troids</span></strong></span><span><span class="kobospan" id="kobo.1817.1">.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.1818.1">Handling imbalanced datasets in machine learning models</span></strong><span class="kobospan" id="kobo.1819.1">: Such as modifying cost</span><a id="_idIndexMarker306" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1820.1"> function, or modified batching in deep </span><span><span class="kobospan" id="kobo.1821.1">learning models.</span></span></li>
</ul>
<h2 id="_idParaDest-69" class="calibre7"><a id="_idTextAnchor101" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1822.1">SMOTE</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.1823.1">SMOTE is a widely used </span><a id="_idIndexMarker307" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1824.1">algorithm for handling imbalanced datasets in machine learning. </span><span class="kobospan" id="kobo.1824.2">It is a synthetic data generation technique that creates new, synthetic samples in the minority class by interpolating between existing samples. </span><span class="kobospan" id="kobo.1824.3">SMOTE works by identifying the k-nearest neighbors of a minority </span><a id="_idIndexMarker308" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1825.1">class sample and then generating new samples along the line segments that connect </span><span><span class="kobospan" id="kobo.1826.1">these neighbors.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1827.1">Here are the steps of the </span><span><span class="kobospan" id="kobo.1828.1">SMOTE algorithm:</span></span></p>
<ol class="calibre16">
<li class="calibre15"><span class="kobospan" id="kobo.1829.1">Select a minority class </span><span><span class="kobospan" id="kobo.1830.1">sample, </span></span><span><em class="italic"><span class="kobospan" id="kobo.1831.1">x</span></em></span><span><span class="kobospan" id="kobo.1832.1">.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1833.1">Choose one of its k-nearest </span><span><span class="kobospan" id="kobo.1834.1">neighbors, </span></span><span><em class="italic"><span class="kobospan" id="kobo.1835.1">x’</span></em></span><span><span class="kobospan" id="kobo.1836.1">.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1837.1">Generate a synthetic sample by interpolating between </span><em class="italic"><span class="kobospan" id="kobo.1838.1">x</span></em><span class="kobospan" id="kobo.1839.1"> and </span><em class="italic"><span class="kobospan" id="kobo.1840.1">x’</span></em><span class="kobospan" id="kobo.1841.1">. </span><span class="kobospan" id="kobo.1841.2">To do this, choose a random number, </span><em class="italic"><span class="kobospan" id="kobo.1842.1">r</span></em><span class="kobospan" id="kobo.1843.1">, between 0 and 1, and then calculate the synthetic sample, </span><span><span class="kobospan" id="kobo.1844.1">as follows:</span></span></li>
</ol>
<p class="calibre6"><span class="kobospan" id="kobo.1845.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;*&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;′&lt;/mo&gt;&lt;/mrow&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/243.png" class="calibre243"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1846.1">This creates a new sample that is somewhere between </span><em class="italic"><span class="kobospan" id="kobo.1847.1">x</span></em><span class="kobospan" id="kobo.1848.1"> and </span><em class="italic"><span class="kobospan" id="kobo.1849.1">x’</span></em><span class="kobospan" id="kobo.1850.1">, but not the same as </span><span><span class="kobospan" id="kobo.1851.1">either one.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1852.1">4.	</span><span class="kobospan" id="kobo.1852.2">Repeat </span><em class="italic"><span class="kobospan" id="kobo.1853.1">steps 1</span></em><span class="kobospan" id="kobo.1854.1"> to </span><em class="italic"><span class="kobospan" id="kobo.1855.1">3</span></em><span class="kobospan" id="kobo.1856.1"> until the desired number of synthetic samples has </span><span><span class="kobospan" id="kobo.1857.1">been generated.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1858.1">Here are the advantages</span><a id="_idIndexMarker309" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1859.1"> and disadvantages </span><span><span class="kobospan" id="kobo.1860.1">of SMOTE:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><span class="kobospan" id="kobo.1861.1">It helps to address the problem of class imbalance by creating synthetic samples in the </span><span><span class="kobospan" id="kobo.1862.1">minority class.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1863.1">SMOTE can be combined with other techniques, such as random undersampling or Tomek links, to further improve the balance of </span><span><span class="kobospan" id="kobo.1864.1">the dataset.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1865.1">SMOTE can be applied to both categorical and </span><span><span class="kobospan" id="kobo.1866.1">numerical data.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1867.1">SMOTE can sometimes create synthetic samples that are unrealistic or noisy, leading </span><span><span class="kobospan" id="kobo.1868.1">to overfitting.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1869.1">SMOTE can sometimes cause the decision boundary to be too sensitive to the minority class, leading to poor performance of the </span><span><span class="kobospan" id="kobo.1870.1">majority class.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1871.1">SMOTE can be computationally expensive for </span><span><span class="kobospan" id="kobo.1872.1">large datasets.</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.1873.1">Here is an example of</span><a id="_idIndexMarker310" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1874.1"> SMOTE in action. </span><span class="kobospan" id="kobo.1874.2">Suppose we have a dataset with two classes: the majority class (class 0) has 900 samples, and the minority class (class 1) has 100 samples. </span><span class="kobospan" id="kobo.1874.3">We want to use SMOTE to generate synthetic samples for the </span><span><span class="kobospan" id="kobo.1875.1">minority class:</span></span></p>
<ol class="calibre16">
<li class="calibre15"><span class="kobospan" id="kobo.1876.1">We select a minority class </span><span><span class="kobospan" id="kobo.1877.1">sample, </span></span><span><em class="italic"><span class="kobospan" id="kobo.1878.1">x</span></em></span><span><span class="kobospan" id="kobo.1879.1">.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1880.1">We choose one of its k-nearest </span><span><span class="kobospan" id="kobo.1881.1">neighbors, </span></span><span><em class="italic"><span class="kobospan" id="kobo.1882.1">x’</span></em></span><span><span class="kobospan" id="kobo.1883.1">.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1884.1">We generate a synthetic sample by interpolating between </span><em class="italic"><span class="kobospan" id="kobo.1885.1">x</span></em><span class="kobospan" id="kobo.1886.1"> and </span><em class="italic"><span class="kobospan" id="kobo.1887.1">x’</span></em><span class="kobospan" id="kobo.1888.1"> using a random </span><span><span class="kobospan" id="kobo.1889.1">number, </span></span><span><em class="italic"><span class="kobospan" id="kobo.1890.1">r</span></em></span><span><span class="kobospan" id="kobo.1891.1">:</span></span></li>
</ol>
<p class="calibre6"><span class="kobospan" id="kobo.1892.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;*&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;′&lt;/mo&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/244.png" class="calibre244"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1893.1">For example, suppose </span><em class="italic"><span class="kobospan" id="kobo.1894.1">x</span></em><span class="kobospan" id="kobo.1895.1"> is (</span><em class="italic"><span class="kobospan" id="kobo.1896.1">1, 2</span></em><span class="kobospan" id="kobo.1897.1">), </span><em class="italic"><span class="kobospan" id="kobo.1898.1">x’</span></em><span class="kobospan" id="kobo.1899.1"> is (</span><em class="italic"><span class="kobospan" id="kobo.1900.1">3, 4</span></em><span class="kobospan" id="kobo.1901.1">), and </span><em class="italic"><span class="kobospan" id="kobo.1902.1">r</span></em><span class="kobospan" id="kobo.1903.1"> is </span><em class="italic"><span class="kobospan" id="kobo.1904.1">0.5</span></em><span class="kobospan" id="kobo.1905.1">. </span><span class="kobospan" id="kobo.1905.2">In this case, the new sample is </span><span><span class="kobospan" id="kobo.1906.1">as follows:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1907.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;0.5&lt;/mn&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;*&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mn&gt;4&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/245.png" class="calibre245"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1908.1">4.	</span><span class="kobospan" id="kobo.1908.2">We repeat </span><em class="italic"><span class="kobospan" id="kobo.1909.1">steps 1</span></em><span class="kobospan" id="kobo.1910.1"> to </span><em class="italic"><span class="kobospan" id="kobo.1911.1">3</span></em><span class="kobospan" id="kobo.1912.1"> until we have generated the desired number of synthetic samples. </span><span class="kobospan" id="kobo.1912.2">For example, suppose we want to generate 100 synthetic samples. </span><span class="kobospan" id="kobo.1912.3">We repeat </span><em class="italic"><span class="kobospan" id="kobo.1913.1">steps 1</span></em><span class="kobospan" id="kobo.1914.1"> to </span><em class="italic"><span class="kobospan" id="kobo.1915.1">3</span></em><span class="kobospan" id="kobo.1916.1"> for each of the 100 minority class </span><a id="_idTextAnchor102" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1917.1">samples and then</span><a id="_idIndexMarker311" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1918.1"> combine the original minority class samples with the synthetic samples to create a balanced dataset with 200 samples in </span><span><span class="kobospan" id="kobo.1919.1">each class.</span></span></p>
<h2 id="_idParaDest-70" class="calibre7"><a id="_idTextAnchor103" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1920.1">The NearMiss algorithm</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.1921.1">The </span><strong class="source-inline"><span class="kobospan" id="kobo.1922.1">NearMiss</span></strong><span class="kobospan" id="kobo.1923.1"> algorithm is a</span><a id="_idIndexMarker312" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1924.1"> technique for balancing class distribution by undersampling (removing) the records from the major class. </span><span class="kobospan" id="kobo.1924.2">When two </span><a id="_idIndexMarker313" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1925.1">classes have records that are very close to each other, eliminating some of the records from the majority class increases the distance between the two classes, which helps the classification process. </span><span class="kobospan" id="kobo.1925.2">To avoid information loss problems in the majority of undersampling methods, near-miss methods are </span><span><span class="kobospan" id="kobo.1926.1">widely used.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1927.1">The working of nearest-neighbor methods is based on the </span><span><span class="kobospan" id="kobo.1928.1">following steps:</span></span></p>
<ol class="calibre16">
<li class="calibre15"><span class="kobospan" id="kobo.1929.1">Find the distances between all records from the major class and minor class. </span><span class="kobospan" id="kobo.1929.2">Our goal is to undersample the records from the </span><span><span class="kobospan" id="kobo.1930.1">major class.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1931.1">Choose </span><em class="italic"><span class="kobospan" id="kobo.1932.1">n</span></em><span class="kobospan" id="kobo.1933.1"> records from the major class that are closest to the </span><span><span class="kobospan" id="kobo.1934.1">minor class.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1935.1">If there are </span><em class="italic"><span class="kobospan" id="kobo.1936.1">k</span></em><span class="kobospan" id="kobo.1937.1"> records in the minor class, the nearest method will return </span><em class="italic"><span class="kobospan" id="kobo.1938.1">k*n</span></em><span class="kobospan" id="kobo.1939.1"> records from the </span><span><span class="kobospan" id="kobo.1940.1">major class.</span></span></li>
</ol>
<p class="calibre6"><span class="kobospan" id="kobo.1941.1">There are three variations of applying the </span><strong class="source-inline"><span class="kobospan" id="kobo.1942.1">NearMiss</span></strong><span class="kobospan" id="kobo.1943.1"> algorithm that we can use to find the </span><em class="italic"><span class="kobospan" id="kobo.1944.1">n</span></em><span class="kobospan" id="kobo.1945.1"> closest records in the </span><span><span class="kobospan" id="kobo.1946.1">major class:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><span class="kobospan" id="kobo.1947.1">We can select the records of the major class for which the average distances to the k-closest records of the minor class are </span><span><span class="kobospan" id="kobo.1948.1">the smallest.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1949.1">We can select the records of the major class for which the average distances to the k-farthest records of the minor class are </span><span><span class="kobospan" id="kobo.1950.1">the smallest.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1951.1">We can implement two </span><a id="_idIndexMarker314" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1952.1">steps. </span><span class="kobospan" id="kobo.1952.2">In the first step, for each record from the minor class, their </span><em class="italic"><span class="kobospan" id="kobo.1953.1">M</span></em><span class="kobospan" id="kobo.1954.1"> nearest</span><a id="_idTextAnchor104" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1955.1"> neighbors will be stored. </span><span class="kobospan" id="kobo.1955.2">Then, the records from the major class are selected </span><a id="_idIndexMarker315" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1956.1">such that the average distance to the </span><em class="italic"><span class="kobospan" id="kobo.1957.1">N</span></em><span class="kobospan" id="kobo.1958.1"> nearest neighbors is </span><span><span class="kobospan" id="kobo.1959.1">the largest.</span></span></li>
</ul>
<h2 id="_idParaDest-71" class="calibre7"><a id="_idTextAnchor105" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1960.1">Cost-sensitive learning</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.1961.1">Cost-sensitive learning is a method that’s used to train machine learning models on imbalanced datasets. </span><span class="kobospan" id="kobo.1961.2">In</span><a id="_idIndexMarker316" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1962.1"> imbalanced datasets, the number of examples in one class (usually the minority class) is much lower than in the other class (usually the majority class). </span><span class="kobospan" id="kobo.1962.2">Cost-sensitive learning involves assigning misclassification costs </span><a id="_idIndexMarker317" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1963.1">to the model that differ based on the class being predicted, which can help the model focus more on correctly classifying the </span><span><span class="kobospan" id="kobo.1964.1">minority class.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1965.1">Let’s assume we have a binary classification problem with two classes, positive and negative. </span><span class="kobospan" id="kobo.1965.2">In cost-sensitive learning, we assign different costs to different types of errors. </span><span class="kobospan" id="kobo.1965.3">For example, we may assign a higher cost to misclassifying a positive example as negative because in an imbalanced dataset, the positive class is the minority class, and misclassifying positive examples can have a greater impact on the performance of </span><span><span class="kobospan" id="kobo.1966.1">the model.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1967.1">We can assign costs in the form of a </span><span><span class="kobospan" id="kobo.1968.1">confusion matrix:</span></span></p>
<table class="no-table-style" id="table002">
<colgroup class="calibre10">
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
</colgroup>
<thead class="calibre12">
<tr class="no-table-style1">
<td class="no-table-style2"/>
<td class="no-table-style2">
<p class="calibre6"><span><strong class="bold"><span class="kobospan" id="kobo.1969.1">Predicted Positive</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><strong class="bold"><span class="kobospan" id="kobo.1970.1">Predicted Negative</span></strong></span></p>
</td>
</tr>
</thead>
<tbody class="calibre13">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><span><strong class="bold"><span class="kobospan" id="kobo.1971.1">Actual Positive</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><strong class="source-inline"><span class="kobospan" id="kobo.1972.1">TP_cost</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><strong class="source-inline"><span class="kobospan" id="kobo.1973.1">FN_cost</span></strong></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><span><strong class="bold"><span class="kobospan" id="kobo.1974.1">Actual Negative</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><strong class="source-inline"><span class="kobospan" id="kobo.1975.1">FP_cost</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><strong class="source-inline"><span class="kobospan" id="kobo.1976.1">TN_cost</span></strong></span></p>
</td>
</tr>
</tbody>
</table>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.1977.1">Table 3.2 – Confusion matrix costs</span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1978.1">Here, </span><strong class="source-inline"><span class="kobospan" id="kobo.1979.1">TP_cost</span></strong><span class="kobospan" id="kobo.1980.1">, </span><strong class="source-inline"><span class="kobospan" id="kobo.1981.1">FN_cost</span></strong><span class="kobospan" id="kobo.1982.1">, </span><strong class="source-inline"><span class="kobospan" id="kobo.1983.1">FP_cost</span></strong><span class="kobospan" id="kobo.1984.1">, and </span><strong class="source-inline"><span class="kobospan" id="kobo.1985.1">TN_cost</span></strong><span class="kobospan" id="kobo.1986.1"> are the costs associated with true positives, false negatives, false positives, and true </span><span><span class="kobospan" id="kobo.1987.1">negatives, respectively.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1988.1">To incorporate the cost matrix into</span><a id="_idIndexMarker318" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1989.1"> the training process, we can modify the standard loss function that the </span><a id="_idIndexMarker319" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1990.1">model optimizes during training. </span><span class="kobospan" id="kobo.1990.2">One common cost-sensitive loss function is the weighted cross-entropy loss, which is defined </span><span><span class="kobospan" id="kobo.1991.1">as follows:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1992.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;*&lt;/mi&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;*&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mover&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;*&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;*&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mover&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/246.png" class="calibre246"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1993.1">Here, </span><em class="italic"><span class="kobospan" id="kobo.1994.1">y</span></em><span class="kobospan" id="kobo.1995.1"> is the true label (either 0 or 1), </span><span class="kobospan" id="kobo.1996.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:math&gt;" src="image/247.png" class="calibre247"/></span><span class="kobospan" id="kobo.1997.1"> is the predicted probability of the positive class, and </span><span class="kobospan" id="kobo.1998.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/248.png" class="calibre248"/></span><span class="kobospan" id="kobo.1999.1">and </span><span class="kobospan" id="kobo.2000.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/249.png" class="calibre249"/></span><span class="kobospan" id="kobo.2001.1"> are weights that are assigned to the positive and negative </span><span><span class="kobospan" id="kobo.2002.1">classes, respectively.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.2003.1">The weights, </span><span class="kobospan" id="kobo.2004.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/250.png" class="calibre250"/></span><span class="kobospan" id="kobo.2005.1">and </span><span class="kobospan" id="kobo.2006.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/251.png" class="calibre251"/></span><span class="kobospan" id="kobo.2007.1">, can be determined by the costs assigned in the confusion matrix. </span><span class="kobospan" id="kobo.2007.2">For example, if we assign a higher cost to false negatives (that is, misclassifying a positive example as negative), we may set </span><span class="kobospan" id="kobo.2008.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/252.png" class="calibre252"/></span><span class="kobospan" id="kobo.2009.1">to a higher value </span><span><span class="kobospan" id="kobo.2010.1">than </span></span><span><span class="kobospan" id="kobo.2011.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/249.png" class="calibre249"/></span></span><span><span class="kobospan" id="kobo.2012.1">.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.2013.1">Cost-sensitive learning can also be used with other types of models, such as decision trees and SVMs. </span><span class="kobospan" id="kobo.2013.2">The concept of assigning costs to different types of errors can be applied in various ways to improve the performance of a model on imbalanced datasets. </span><span class="kobospan" id="kobo.2013.3">However, it’s important to carefully select the appropriate cost matrix and loss function based on the specific characteristics of the dataset and the problem </span><span><span class="kobospan" id="kobo.2014.1">being solved:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.2015.1">Ensemble techniques</span></strong><span class="kobospan" id="kobo.2016.1">: Ensemble techniques combine multiple models to improve predictive performance. </span><span class="kobospan" id="kobo.2016.2">In</span><a id="_idIndexMarker320" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.2017.1"> imbalanced datasets, an ensemble of models can be trained on different subsets of the dataset, ensuring that each model is trained on both the minority and majority classes. </span><span class="kobospan" id="kobo.2017.2">Examples of ensemble techniques for imbalanced datasets include bagging </span><span><span class="kobospan" id="kobo.2018.1">and boosting.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.2019.1">Anomaly detection</span></strong><span class="kobospan" id="kobo.2020.1">: Anomaly detection techniques can be used to identify the minority class as an anomaly in</span><a id="_idIndexMarker321" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.2021.1"> the dataset. </span><span class="kobospan" id="kobo.2021.2">These techniques aim to identify rare events that are significantly different from the majority class. </span><span class="kobospan" id="kobo.2021.3">The identified </span><a id="_idIndexMarker322" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.2022.1">samples can then be used to train the model on</span><a id="_idIndexMarker323" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.2023.1"> the </span><span><span class="kobospan" id="kobo.2024.1">minority c</span><a id="_idTextAnchor106" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.2025.1">lass.</span></span></li>
</ul>
<h2 id="_idParaDest-72" class="calibre7"><a id="_idTextAnchor107" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.2026.1">Data augmentation</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.2027.1">The idea behind data </span><a id="_idIndexMarker324" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.2028.1">augmentation is to generate new examples by applying transformations to the original ones, while still retaining the label. </span><span class="kobospan" id="kobo.2028.2">These transformations can include rotation, translation, scaling, flipping, and adding noise, among others. </span><span class="kobospan" id="kobo.2028.3">This can be particularly useful for imbalanced datasets, where the number of examples in one class is much smaller than in </span><span><span class="kobospan" id="kobo.2029.1">the other.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.2030.1">In the context of imbalanced datasets, data augmentation can be used to create new examples of the minority class, effectively balancing the dataset. </span><span class="kobospan" id="kobo.2030.2">This can be done by applying the same set of transformations to the minority class examples, creating a new set of examples that are still representative of the minority class but are slightly different from the </span><span><span class="kobospan" id="kobo.2031.1">original ones.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.2032.1">The equations that are involved in data augmentation are relatively simple as they are based on applying transformation functions to the original examples. </span><span class="kobospan" id="kobo.2032.2">For example, to rotate an image by a certain angle, we can use a </span><span><span class="kobospan" id="kobo.2033.1">rotation matrix:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.2034.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;′&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/254.png" class="calibre253"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.2035.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;′&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/255.png" class="calibre254"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.2036.1">Here, </span><em class="italic"><span class="kobospan" id="kobo.2037.1">x</span></em><span class="kobospan" id="kobo.2038.1"> and </span><em class="italic"><span class="kobospan" id="kobo.2039.1">y</span></em><span class="kobospan" id="kobo.2040.1"> are the original coordinates of a pixel in the image, </span><em class="italic"><span class="kobospan" id="kobo.2041.1">x’</span></em><span class="kobospan" id="kobo.2042.1"> and </span><em class="italic"><span class="kobospan" id="kobo.2043.1">y’</span></em><span class="kobospan" id="kobo.2044.1"> are the new coordinates after rotation, and </span><span class="kobospan" id="kobo.2045.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/256.png" class="calibre255"/></span><span class="kobospan" id="kobo.2046.1"> is the angle </span><span><span class="kobospan" id="kobo.2047.1">of rotation.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.2048.1">Similarly, to apply translation, we can simply shift the image by a certain number </span><span><span class="kobospan" id="kobo.2049.1">of pixels:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.2050.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;′&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/257.png" class="calibre256"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.2051.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;′&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/258.png" class="calibre257"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.2052.1">Here, </span><em class="italic"><span class="kobospan" id="kobo.2053.1">dx</span></em><span class="kobospan" id="kobo.2054.1"> and </span><em class="italic"><span class="kobospan" id="kobo.2055.1">dy</span></em><span class="kobospan" id="kobo.2056.1"> are the horizontal and vertical </span><span><span class="kobospan" id="kobo.2057.1">shifts, respectively.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.2058.1">Data augmentation can be a powerful technique for addressing imbalanced datasets as it can create new examples that are representative of the minority class, while still</span><a id="_idIndexMarker325" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.2059.1"> preserving the label information. </span><span class="kobospan" id="kobo.2059.2">However, it is important to be careful when applying data augmentation as it can also introduce noise and artifacts in the data, and can lead to overfitting if not </span><span><span class="kobospan" id="kobo.2060.1">done properly.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.2061.1">In conclusion, handling imbalanced datasets is an important aspect of machine learning. </span><span class="kobospan" id="kobo.2061.2">There are several techniques available to handle imbalanced datasets, each with its advantages and disadvantages. </span><span class="kobospan" id="kobo.2061.3">The choice of technique depends on the dataset, the problem, and the available resources. </span><span class="kobospan" id="kobo.2061.4">Besides having imbalanced data, in the case of working on time series data, we might face correlated data. </span><span class="kobospan" id="kobo.2061.5">We’ll take a closer look at </span><span><span class="kobospan" id="kobo.2062.1">this</span><a id="_idTextAnchor108" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.2063.1"> next.</span></span></p>
<h1 id="_idParaDest-73" class="calibre4"><a id="_idTextAnchor109" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.2064.1">Dealing with correlated data</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.2065.1">Dealing with correlated time series data in machine learning models can be challenging as traditional techniques </span><a id="_idIndexMarker326" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.2066.1">such as random sampling can introduce biases and overlook dependencies between data points. </span><span class="kobospan" id="kobo.2066.2">Here are some approaches that </span><span><span class="kobospan" id="kobo.2067.1">can help:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.2068.1">Time series cross-validation</span></strong><span class="kobospan" id="kobo.2069.1">: Time series data is often dependent on past values and it’s important to preserve this relationship during model training and evaluation. </span><span class="kobospan" id="kobo.2069.2">Time series cross-validation involves splitting the data into multiple folds, with each fold consisting of a continuous block of time. </span><span class="kobospan" id="kobo.2069.3">This approach ensures that the model is trained on past data and evaluated on future data, which better simulates how the model will perform in </span><span><span class="kobospan" id="kobo.2070.1">real-world scenarios.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.2071.1">Feature engineering</span></strong><span class="kobospan" id="kobo.2072.1">: Correlated time series data can be difficult to model with traditional machine learning algorithms. </span><span class="kobospan" id="kobo.2072.2">Feature engineering can help transform the data into a more suitable format. </span><span class="kobospan" id="kobo.2072.3">Examples of feature engineering for time series data include creating lags or differences in the time series, aggregating data into time buckets or windows, and creating rolling statistics such as </span><span><span class="kobospan" id="kobo.2073.1">moving averages.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.2074.1">Time series-specific models</span></strong><span class="kobospan" id="kobo.2075.1">: There are several models specifically designed for time series data, such as </span><strong class="bold"><span class="kobospan" id="kobo.2076.1">AutoRegressive Integrated Moving Average</span></strong><span class="kobospan" id="kobo.2077.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.2078.1">ARIMA</span></strong><span class="kobospan" id="kobo.2079.1">), </span><strong class="bold"><span class="kobospan" id="kobo.2080.1">Seasonal ARIMA</span></strong><span class="kobospan" id="kobo.2081.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.2082.1">SARIMA</span></strong><span class="kobospan" id="kobo.2083.1">), </span><strong class="bold"><span class="kobospan" id="kobo.2084.1">Prophet</span></strong><span class="kobospan" id="kobo.2085.1">, and </span><strong class="bold"><span class="kobospan" id="kobo.2086.1">Long Short-Term Memory</span></strong><span class="kobospan" id="kobo.2087.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.2088.1">LSTM</span></strong><span class="kobospan" id="kobo.2089.1">) networks. </span><span class="kobospan" id="kobo.2089.2">These models are designed to capture the dependencies and patterns in time series data and may outperform traditional machine </span><span><span class="kobospan" id="kobo.2090.1">learning models.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.2091.1">Time series preprocessing techniques</span></strong><span class="kobospan" id="kobo.2092.1">: Time series data can be preprocessed to remove </span><a id="_idIndexMarker327" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.2093.1">correlations and make the data more suitable for machine learning models. </span><span class="kobospan" id="kobo.2093.2">Techniques such as differencing, detrending, and normalization can help remove trends and seasonal components from the data, which can help </span><span><span class="kobospan" id="kobo.2094.1">reduce correlations.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.2095.1">Dimensionality reduction techniques</span></strong><span class="kobospan" id="kobo.2096.1">: Correlated time series data can have a high dimensionality, which can make modeling difficult. </span><span class="kobospan" id="kobo.2096.2">Dimensionality reduction techniques such as PCA or autoencoders can help reduce the number of variables in the data while preserving the most </span><span><span class="kobospan" id="kobo.2097.1">important information.</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.2098.1">In general, it’s important to approach time series data with techniques that preserve the temporal dependencies and patterns in the data. </span><span class="kobospan" id="kobo.2098.2">This can require specialized modeling techniques and </span><span><span class="kobospan" id="kobo.2099.1">preprocessi</span><a id="_idTextAnchor110" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.2100.1">ng steps.</span></span></p>
<h1 id="_idParaDest-74" class="calibre4"><a id="_idTextAnchor111" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.2101.1">Summary</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.2102.1">In this chapter, we learned about various concepts related to machine learning, starting with data exploration and preprocessing techniques. </span><span class="kobospan" id="kobo.2102.2">We then explored various machine learning models, such as logistic regression, decision trees, support vector machines, and random forests, along with their strengths and weaknesses. </span><span class="kobospan" id="kobo.2102.3">We also discussed the importance of splitting data into training and test sets, as well as techniques for handling </span><span><span class="kobospan" id="kobo.2103.1">imbalanced datasets.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.2104.1">The chapter also covered the concepts of model bias, variance, underfitting, and overfitting, and how to diagnose and address these issues. </span><span class="kobospan" id="kobo.2104.2">We also explored ensemble methods such as bagging, boosting, and stacking, which can improve model performance by combining the predictions of </span><span><span class="kobospan" id="kobo.2105.1">multiple models.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.2106.1">Finally, we learned about the limitations and challenges of machine learning, including the need for large amounts of high-quality data, the risk of bias and unfairness, and the difficulty of interpreting complex models. </span><span class="kobospan" id="kobo.2106.2">Despite these challenges, machine learning offers powerful tools for solving a wide range of problems and has the potential to transform many industries </span><span><span class="kobospan" id="kobo.2107.1">and fields.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.2108.1">In the next chapter, we will discuss text preprocessing, which is required for text to be used by machine </span><span><span class="kobospan" id="kobo.2109.1">learning models.</span></span></p>
<h1 id="_idParaDest-75" class="calibre4"><a id="_idTextAnchor112" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.2110.1">References</span></h1>
<ul class="calibre14">
<li class="calibre15"><span class="kobospan" id="kobo.2111.1">Shahriari, B., Swersky, K., Wang, Z., Adams, R.P., de Freitas, N.: </span><em class="italic"><span class="kobospan" id="kobo.2112.1">Taking the human out of the loop: A review of Bayesian optimization. </span><span class="kobospan" id="kobo.2112.2">Proceedings of the IEEE 104(1), 148–175 (2016). </span></em><span><em class="italic"><span class="kobospan" id="kobo.2113.1">DOI 10.1109/JPROC.2015.2494218</span></em></span><span><span class="kobospan" id="kobo.2114.1">.</span></span></li>
</ul>
</div>
</body></html>