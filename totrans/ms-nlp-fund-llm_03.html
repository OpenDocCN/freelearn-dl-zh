<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-44"><a id="_idTextAnchor045" class="calibre5 pcalibre1 pcalibre"/>3</h1>
<h1 id="_idParaDest-45" class="calibre4"><a id="_idTextAnchor046" class="calibre5 pcalibre1 pcalibre"/>Unleashing Machine Learning Potentials in Natural Language Processing</h1>
<p class="calibre6">In this chapter, we will delve into the fundamentals of <strong class="bold">Machine Learning</strong> (<strong class="bold">ML</strong>) and preprocessing techniques that are essential for <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>) tasks. ML is a powerful tool for building models that can learn from data, and NLP is one of the most exciting and challenging applications of ML.</p>
<p class="calibre6">By the end of this chapter, you will have gained a comprehensive understanding of data exploration, preprocessing, and data split, know how to deal with imbalanced data techniques, and learned about some of the common ML models required for successful ML, particularly in the context of NLP.</p>
<p class="calibre6">The following topics will be covered in this chapter:</p>
<ul class="calibre14">
<li class="calibre15">Data exploration</li>
<li class="calibre15"><a id="_idTextAnchor047" class="calibre5 pcalibre1 pcalibre"/>Common ML models</li>
<li class="calibre15">Model underfitting and overfitting</li>
<li class="calibre15">Splitting data</li>
<li class="calibre15">Hyperparameter tuning</li>
<li class="calibre15">Ensemble models</li>
<li class="calibre15">Handling imbalanced data</li>
<li class="calibre15">Dealing with correlated data</li>
</ul>
<h1 id="_idParaDest-46" class="calibre4"><a id="_idTextAnchor048" class="calibre5 pcalibre1 pcalibre"/>Technical requirements</h1>
<p class="calibre6">Prior knowledge of programming languages, particularly Python, is assumed in this chapter and subsequent chapters of this book. It is also expected that you have already gone through previous chapters to become acquainted with the necessary linear algebra and statistics concepts that will be discussed in detail.</p>
<h1 id="_idParaDest-47" class="calibre4"><a id="_idTextAnchor049" class="calibre5 pcalibre1 pcalibre"/>Data exploration</h1>
<p class="calibre6">When working in a methodological environment, datasets are often well known and preprocessed, such as<a id="_idIndexMarker099" class="calibre5 pcalibre1 pcalibre"/> Kaggle datasets. However, in real-world business environments, one important task is to define the dataset from all possible sources of data, explore the gathered data to find the best method for preprocessing it, and ultimately decide on the ML and natural language models that fit the problem and the underlying data best. This process requires careful consideration and analysis of the data, as well as a thorough understanding of the business problem at hand.</p>
<p class="calibre6">In NLP, the data can be quite complex, as it often includes text and speech data that can be unstructured and difficult to analyze. This complexity makes preprocessing an essential step in preparing the data for ML models. The first step of any NLP or ML solution starts with exploring the data to learn more about it, which helps us decide on our path to tackle the problem.</p>
<p class="calibre6">Once the data has been preprocessed, the next step is to explore it to gain a better understanding of its characteristics and structure. Data exploration is an iterative process that involves visualizing and analyzing the data, looking for patterns and relationships, and identifying potential issues or outliers. This process can help us to determine which features are most important for our ML models and identify any potential biases or data quality issues. To streamline data and enhance analysis through ML models, preprocessing methods such as tokenization, stemming, and lemmatization can be employed. In this chapter, we will provide an overview of general preprocessing techniques for ML problems. In the following chapter, we will delve into preprocessing techniques specific to text processing. It is important to note that employing effective preprocessing techniques can significantly enhance the performance and accuracy of ML models, making them more robust and reliable.</p>
<p class="calibre6">Finally, once the data has been preprocessed and explored, we can start building our ML models. There is no single magical solution that works for all ML problems, so it’s important to carefully consider which models are best suited for the data and the problem at hand. Different types of NLP models exist, encompassing rule-based, statistical, and deep learning models. Each model type possesses unique strengths and weaknesses, underscoring the importance of selecting the most fitting one for the specific problem and dataset at hand.</p>
<p class="calibre6">Data exploration is an important and initial step in the ML workflow that involves analyzing and understanding the data before building a ML model. The goal of data exploration is to gain insights about the data, identify patterns, detect anomalies, and prepare the data for <a id="_idIndexMarker100" class="calibre5 pcalibre1 pcalibre"/>modeling. Data exploration helps in choosing the right ML algorithm and determining the best set of features to use.</p>
<p class="calibre6">Here are some common techniques that are used in data exploration:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Data visualization</strong>: Data visualization involves depicting data through graphical or pictorial formats. It enables<a id="_idIndexMarker101" class="calibre5 pcalibre1 pcalibre"/> visual exploration of data, providing insights into its distribution, patterns, and<a id="_idIndexMarker102" class="calibre5 pcalibre1 pcalibre"/> relationships. Widely employed techniques in data visualization encompass scatter plots, bar charts, heatmaps, box plots, and correlation matrices.</li>
<li class="calibre15"><strong class="bold">Data cleaning</strong>: Data cleaning is a<a id="_idIndexMarker103" class="calibre5 pcalibre1 pcalibre"/> step of preprocessing where we identify the errors, inconsistencies, and missing values and correct them. It affects the final results of the model since ML models are sensitive to<a id="_idIndexMarker104" class="calibre5 pcalibre1 pcalibre"/> errors in the data. Removing duplicates and filling in missing values are some of the common data cleaning techniques.</li>
<li class="calibre15"><strong class="bold">Feature engineering</strong>: Feature engineering plays a crucial role in optimizing the effectiveness of machine learning<a id="_idIndexMarker105" class="calibre5 pcalibre1 pcalibre"/> models by crafting new features from existing data. This process involves not only identifying <a id="_idIndexMarker106" class="calibre5 pcalibre1 pcalibre"/>pertinent features but also transforming the existing ones and introducing novel features. Various feature engineering techniques, including scaling, normalization, dimensionality reduction, and feature selection, contribute to refining the overall performance of the models.</li>
<li class="calibre15"><strong class="bold">Statistical analysis</strong>: Statistical analysis utilizes a range of statistical techniques to scrutinize data, revealing valuable insights into its inherent properties. Essential statistical <a id="_idIndexMarker107" class="calibre5 pcalibre1 pcalibre"/>methods include hypothesis<a id="_idIndexMarker108" class="calibre5 pcalibre1 pcalibre"/> testing, regression analysis, and time series analysis, all of which contribute to a comprehensive understanding of the data’s characteristics.</li>
<li class="calibre15"><strong class="bold">Domain knowledge</strong>: Leveraging domain knowledge entails applying a pre-existing understanding of the <a id="_idIndexMarker109" class="calibre5 pcalibre1 pcalibre"/>data domain to extract insights and make informed decisions. This knowledge proves valuable <a id="_idIndexMarker110" class="calibre5 pcalibre1 pcalibre"/>in recognizing pertinent features, interpreting results, and choosing the most suitable ML algorithm for the task at hand.</li>
</ul>
<p class="calibre6">We will explore each of these techniques in the following s<a id="_idTextAnchor050" class="calibre5 pcalibre1 pcalibre"/>ubsections.</p>
<h2 id="_idParaDest-48" class="calibre7"><a id="_idTextAnchor051" class="calibre5 pcalibre1 pcalibre"/>Data visualization</h2>
<p class="calibre6">Data visualization is a<a id="_idIndexMarker111" class="calibre5 pcalibre1 pcalibre"/> crucial component of machine learning as it allows us to understand and explore complex datasets more easily. It involves creating<a id="_idIndexMarker112" class="calibre5 pcalibre1 pcalibre"/> visual representations of data using charts, graphs, and other types of visual aids. By visually presenting data, we can discern patterns, trends, and relationships that might not be readily evident when examining the raw data alone.</p>
<p class="calibre6">For NLP tasks, data visualization can help us gain insights into the linguistic patterns and structures in text data. For example, we can create word clouds to visualize the frequency of words in a corpus or use heatmaps to display the co-occurrence of words or phrases. We can also use scatter plots and line graphs to visualize changes in sentiment or topic over time.</p>
<p class="calibre6">One common type of visualization for ML is the scatter plot, which is used to display the relationship between two variables. By plotting the values of two variables on the X and Y axes, we can identify any patterns or trends that exist between them. Scatter plots are particularly useful for identifying clusters or groups of data points that share similar characteristics.</p>
<p class="calibre6">Another type of visualization that’s frequently employed in ML is the histogram, a tool that illustrates the distribution of a single variable. By grouping data into bins and portraying the frequency of data points in each bin, we can pinpoint the range of values that predominate in the <a id="_idIndexMarker113" class="calibre5 pcalibre1 pcalibre"/>dataset. Histograms prove useful for detecting outliers or anomalies, and they aid in recognizing areas where the data may exhibit skewness or bias.</p>
<p class="calibre6">In addition to these basic<a id="_idIndexMarker114" class="calibre5 pcalibre1 pcalibre"/> visualizations, ML practitioners often use more advanced techniques, such as dimensionality reduction and network visualizations. Dimensionality reduction techniques, such as <strong class="bold">principal component analysis</strong> (<strong class="bold">PCA</strong>) and <strong class="bold">t-distributed stochastic neighbor embedding</strong> (<strong class="bold">t-SNE</strong>), are commonly used for <a id="_idIndexMarker115" class="calibre5 pcalibre1 pcalibre"/>dimensional reduction and to visualize <a id="_idIndexMarker116" class="calibre5 pcalibre1 pcalibre"/>or analyze the data more easily. Network visualizations, on the other hand, are used to display complex relationships between entities, such as the co-occurrence of words or the connections between so<a id="_idTextAnchor052" class="calibre5 pcalibre1 pcalibre"/>cial media users.</p>
<h2 id="_idParaDest-49" class="calibre7"><a id="_idTextAnchor053" class="calibre5 pcalibre1 pcalibre"/>Data cleaning</h2>
<p class="calibre6">Data cleaning, alternatively termed <a id="_idIndexMarker117" class="calibre5 pcalibre1 pcalibre"/>data cleansing or data scrubbing, involves recognizing and rectifying or eliminating errors, inconsistencies, and inaccuracies <a id="_idIndexMarker118" class="calibre5 pcalibre1 pcalibre"/>within a dataset. This crucial phase in data preparation for ML significantly influences the accuracy and performance of a model, relying on the quality of the data used for training. Numerous prevalent techniques are employed in data cleaning. Let’s <a id="_idTextAnchor054" class="calibre5 pcalibre1 pcalibre"/>take a closer look.</p>
<h3 class="calibre8">Handling missing values</h3>
<p class="calibre6">Missing data is a common problem that occurs in many machine learning projects. Dealing with missing data is important because ML models cannot handle missing data and will either produce errors or<a id="_idIndexMarker119" class="calibre5 pcalibre1 pcalibre"/> provide inaccurate results.</p>
<p class="calibre6">There are several methods for dealing with missing data in ML projects:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Dropping rows</strong>: Addressing missing data can involve a straightforward approach of discarding rows that contain such values. Nevertheless, exercising caution is paramount when employing this method as excessive row removal may result in the loss of valuable data, impacting the overall accuracy of the model. We usually use this method when we have a few rows in our dataset, and we have a few rows with missing values. In this case, removing a few rows can be a good and easy approach to training our model while the final performance will not be affected significantly.</li>
<li class="calibre15"><strong class="bold">Dropping columns</strong>: Another <a id="_idIndexMarker120" class="calibre5 pcalibre1 pcalibre"/>approach is to drop the columns that contain missing values. This method can be effective if the missing values are concentrated in a few columns <a id="_idIndexMarker121" class="calibre5 pcalibre1 pcalibre"/>and if those columns are not important for the analysis. However, dropping important columns can lead to a loss of valuable information. It is better to perform some sort of correlation analysis to see the correlation of the values in these columns with the target class or value before dropping these columns.</li>
<li class="calibre15"><strong class="bold">Mean/median/mode imputation</strong>: Mean, median, and mode imputation entail substituting missing values with the mean, median, or mode derived from the non-missing values within<a id="_idIndexMarker122" class="calibre5 pcalibre1 pcalibre"/> the corresponding column. This method is easy to implement and can be effective when the missing values are few and randomly distributed. However, it can also introduce bias and affect the variability of the data.</li>
<li class="calibre15"><strong class="bold">Regression imputation</strong>: Regression imputation involves predicting the missing values based on the values of other variables in the dataset. This method can be effective when<a id="_idIndexMarker123" class="calibre5 pcalibre1 pcalibre"/> the missing values are related to other variables in the dataset, but it requires a regression model to be built for each column with missing values.</li>
<li class="calibre15"><strong class="bold">Multiple imputation</strong>: Multiple imputation encompasses generating multiple imputed datasets through statistical models, followed by amalgamating the outcomes to<a id="_idIndexMarker124" class="calibre5 pcalibre1 pcalibre"/> produce a conclusive dataset. This approach proves efficacious, particularly when dealing with non-randomly distributed missing values and a substantial number of gaps in the dataset.</li>
<li class="calibre15"><strong class="bold">K-nearest neighbor imputation</strong>: K-nearest neighbor imputation entails identifying the k-nearest data points to the missing value and utilizing their values to impute the absent value. This method can be effective when the missing values are <a id="_idIndexMarker125" class="calibre5 pcalibre1 pcalibre"/>clustered together in the dataset. In this approach, we can find the most similar records to the dataset to the record that has the missing value, and then use the mean of the values of those records for that specific record as the missed value.</li>
</ul>
<p class="calibre6">In essence, selecting a method to handle missing data hinges on factors such as the nature and extent of the missing data, analysis objectives, and resource availability. It is crucial to thoughtfully assess the pros and cons of each method and opt for the most suitable approach tailore<a id="_idTextAnchor055" class="calibre5 pcalibre1 pcalibre"/>d to the specific project.</p>
<h3 class="calibre8">Removing duplicates</h3>
<p class="calibre6">Eliminating duplicates is a prevalent<a id="_idIndexMarker126" class="calibre5 pcalibre1 pcalibre"/> preprocessing measure that’s employed to cleanse datasets by detecting and removing identical records. The occurrence of duplicate records may be attributed to factors such as data entry errors, system glitches, or data merging processes. The presence of duplicates can skew models and yield inaccurate insights. Hence, it is imperative to recognize and eliminate duplicate records to uphold the accuracy and dependability of the dataset.</p>
<p class="calibre6">There are different methods for removing duplicates in a dataset. The most common method is to compare all the rows of the dataset to identify duplicate records. If two or more rows have the same values in all the columns, they are considered duplicates. In some cases, it may be necessary to compare only a subset of columns if certain columns are more prone to duplicates.</p>
<p class="calibre6">Another method is to use a unique identifier column to identify duplicates. A unique identifier column is a column that contains unique values for each record, such as an ID number or a combination of unique columns. By comparing the unique identifier column, it is possible to identify and remove duplicate records from the dataset.</p>
<p class="calibre6">After identifying the duplicate records, the next step is to decide which records to keep and which ones to remove. One<a id="_idIndexMarker127" class="calibre5 pcalibre1 pcalibre"/> approach is to keep the first occurrence of a duplicate record and remove all subsequent occurrences. Another approach is to keep the record with the most complete information, or the record with the most recent timestamp.</p>
<p class="calibre6">It’s crucial to recognize that the removal of duplicates might lead to a reduction in dataset size, potentially affecting the performance of ML models. Consequently, assessing the impact of duplicate removal on both the dataset and the ML model is essential. In some cases, it may be necessary to keep duplicate records if they contain important information that cannot be <a id="_idTextAnchor056" class="calibre5 pcalibre1 pcalibre"/>obtained from other records.</p>
<h3 class="calibre8">Standardizing and transforming data</h3>
<p class="calibre6">Standardizing and transforming data is a critical step in preparing data for ML tasks. This process involves scaling and normalizing the numerical features of the dataset to make them easier to<a id="_idIndexMarker128" class="calibre5 pcalibre1 pcalibre"/> interpret and compare. The main objective of standardizing <a id="_idIndexMarker129" class="calibre5 pcalibre1 pcalibre"/>and transforming data is to enhance the accuracy and performance of a ML model by mitigating the influence of features with diverse scales and<a id="_idIndexMarker130" class="calibre5 pcalibre1 pcalibre"/> ranges. A widely used method for standardizing data is referred to as “standardization” or “Z-score <a id="_idIndexMarker131" class="calibre5 pcalibre1 pcalibre"/>normalization.” This technique involves transforming each feature such that it has a mean of zero and a standard deviation of one. The formula for standardization is shown in the following equation:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;′&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/128.png" class="calibre133"/></p>
<p class="calibre6">Here, <em class="italic">x</em> represents the feature, <em class="italic">mean(x)</em> denotes the mean of the feature, <em class="italic">std(x)</em> indicates the standard deviation of the feature, and <em class="italic">x’</em> represents the new value assigned to the feature. By standardizing the data in this way, the range of each feature is adjusted to be centered around zero, which makes it easier to compare features and prevents features with large values from dominating the analysis.</p>
<p class="calibre6">Another technique for transforming data is “min-max scaling.” This method rescales the data to a consistent range of values, commonly ranging between 0 and 1. The formula for min-max scaling is shown here:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;′&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/129.png" class="calibre134"/></p>
<p class="calibre6">In this equation, <em class="italic">x</em> represents the feature, <em class="italic">min(x)</em> signifies the minimum value of the feature, and <em class="italic">max(x)</em> denotes the maximum value of the feature. Min-max scaling proves beneficial when the precise distribution of the data is not crucial, but there is a need to standardize the data for meaningful comparisons across different features.</p>
<p class="calibre6">Transforming data can also involve changing the distribution of the data. A frequently applied transformation<a id="_idIndexMarker132" class="calibre5 pcalibre1 pcalibre"/> is the log transformation, which is employed to alleviate the influence of outliers and skewness within the data. This transformation involves taking the logarithm of the feature values, which can <a id="_idIndexMarker133" class="calibre5 pcalibre1 pcalibre"/>help to normalize the distribution and reduce the influence of extreme values.</p>
<p class="calibre6">Overall, standardizing and transforming data constitute a pivotal stage in the data preprocessing workflow for ML endeavors. Through scaling and normalizing features, we can enhance the accuracy and performance of the ML model, rendering the data more interpretable and co<a id="_idTextAnchor057" class="calibre5 pcalibre1 pcalibre"/>nducive to meaningful comparisons.</p>
<h3 class="calibre8">Handling outliers</h3>
<p class="calibre6">Outliers are data points that markedly deviate from the rest of the observations in a dataset. Their occurrence may stem from factors such as measurement errors, data corruption, or authentic<a id="_idIndexMarker134" class="calibre5 pcalibre1 pcalibre"/> extreme values. The presence of outliers can wield a substantial influence on the outcomes of ML models, introducing distortion to the data and disrupting the relationships between variables. Therefore, handling outliers is an important step in preprocessing data for ML.</p>
<p class="calibre6">There are several methods for handling outliers:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Removing outliers</strong>: One straightforward approach involves eliminating observations identified as outliers from the dataset. Nevertheless, exercising caution is paramount<a id="_idIndexMarker135" class="calibre5 pcalibre1 pcalibre"/> when adopting this method as excessive removal of observations may result in the loss of valuable information and potentially introduce bias to the analysis results.</li>
<li class="calibre15"><strong class="bold">Transforming data</strong>: Applying mathematical<a id="_idIndexMarker136" class="calibre5 pcalibre1 pcalibre"/> functions such as logarithms or square roots to transform the data can mitigate the influence of outliers. For instance, taking the logarithm of a variable can alleviate the impact of extreme values, given the slower rate of increase in the logarithmic scale compared to the original values.</li>
<li class="calibre15"><strong class="bold">Winsorizing</strong>: Winsorizing is a technique that entails substituting extreme values with the nearest highest or lowest value in the dataset. Employing this method aids in maintaining the sample size and overall distribution of the data.</li>
<li class="calibre15"><strong class="bold">Imputing values</strong>: Imputation involves replacing missing or extreme values with estimated values derived from the remaining observations in the dataset. For instance, substituting extreme values with the median or mean of the remaining observations is a common imputation technique.</li>
<li class="calibre15"><strong class="bold">Using robust statistical methods</strong>: Robust statistical methods exhibit lower sensitivity to outliers, leading to more accurate results even in the presence of such extreme values. For instance, opting for the median instead of the mean can effectively diminish the influence of outliers on the final results.</li>
</ul>
<p class="calibre6">It’s crucial to emphasize that selecting an outlier-handling method should be tailored to the unique characteristics of the data and the specific problem at hand. Generally, employing a combination of methods is advisable to address outliers comprehensively, and assessing the impact of each method on the results is essential. Moreover, documenting the steps taken to manage outliers is important for reproducibility and to provide cl<a id="_idTextAnchor058" class="calibre5 pcalibre1 pcalibre"/>arity on the decision-making process.</p>
<h3 class="calibre8">Correcting errors</h3>
<p class="calibre6">Rectifying errors during preprocessing stands as a vital stage in readying data for ML. Errors may manifest due to diverse <a id="_idIndexMarker137" class="calibre5 pcalibre1 pcalibre"/>reasons such as data entry blunders, measurement discrepancies, sensor inaccuracies, or transmission glitches. Correcting errors in data holds paramount significance in guaranteeing that ML models are trained on dependable and precise data, consequently enhancing the accuracy and reliability of predictions.</p>
<p class="calibre6">Several techniques exist to rectify errors in data. Here are some widely utilized methods:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Manual inspection</strong>: An approach to rectify errors in data involves a manual inspection of the dataset, wherein errors are corrected by hand. This method is frequently employed, particularly when dealing with relatively small and manageable datasets.</li>
<li class="calibre15"><strong class="bold">Statistical methods</strong>: Statistical methods prove effective in identifying and rectifying errors in data. For instance, when the data adheres to a recognized distribution, statistical techniques such as the Z-score can be employed to detect outliers, which can then be either removed or replaced.</li>
<li class="calibre15"><strong class="bold">ML methods</strong>: Utilizing ML algorithms facilitates the detection and correction of errors in data. For instance, clustering algorithms prove valuable in pinpointing data points that markedly deviate from the broader dataset. Subsequently, these identified data points can undergo further examination and correction.</li>
<li class="calibre15"><strong class="bold">Domain knowledge</strong>: Leveraging domain knowledge is instrumental in pinpointing errors within data. For instance, when collecting data from sensors, it becomes feasible to identify and rectify errors by considering the anticipated range of values that the sensor is capable of producing.</li>
<li class="calibre15"><strong class="bold">Imputation</strong>: Imputation serves as a method to populate missing values in the data. This can be accomplished through various means, including statistical methods such as mean or median imputation, as well as ML algorithms such as k-nearest neighbor imputation.</li>
</ul>
<p class="calibre6">Choosing a technique hinges on factors such as the nature of the data, the dataset’s siz<a id="_idTextAnchor059" class="calibre5 pcalibre1 pcalibre"/>e, and the resources at <a id="_idIndexMarker138" class="calibre5 pcalibre1 pcalibre"/>your disposal.</p>
<h2 id="_idParaDest-50" class="calibre7"><a id="_idTextAnchor060" class="calibre5 pcalibre1 pcalibre"/>Feature selection</h2>
<p class="calibre6">Feature selection involves choosing the most pertinent features from a dataset for constructing a ML model. The <a id="_idIndexMarker139" class="calibre5 pcalibre1 pcalibre"/>objective is to decrease the number of features without substantially compromising the model’s accuracy, resulting in enhanced performance, quicker training, and a more straightforward interpretation of the model.</p>
<p class="calibre6">Several approaches to fea<a id="_idTextAnchor061" class="calibre5 pcalibre1 pcalibre"/>ture selection exist. Let’s take a look.</p>
<h3 class="calibre8">Filter methods</h3>
<p class="calibre6">These techniques employ <a id="_idIndexMarker140" class="calibre5 pcalibre1 pcalibre"/>statistical methods to rank features<a id="_idIndexMarker141" class="calibre5 pcalibre1 pcalibre"/> according to their correlation with the target variable. Common methods encompass chi-squared, mutual information, and correlation coefficients. Features are subsequently chosen based on a predefined threshold.</p>
<h4 class="calibre135">Chi-squared</h4>
<p class="calibre6">The chi-squared test is a widely employed statistical method in ML for feature selection that’s particularly<a id="_idIndexMarker142" class="calibre5 pcalibre1 pcalibre"/> effective for categorical variables. This test gauges the dependence between two random variables, providing a P-value that signifies the likelihood of obtaining a result as extreme as or more extreme<a id="_idIndexMarker143" class="calibre5 pcalibre1 pcalibre"/> than the actual observations.</p>
<p class="calibre6">In hypothesis testing, the chi-squared test assesses whether the collected data aligns with the expected data. A small chi-squared test statistic indicates a robust match, while a large statistic implies a weak match. A P-value less than or equal to 0.05 leads to the rejection of the null hypothesis, considering it highly improbable. Conversely, a P-value greater than 0.05 results in accepting or “failing to reject” the null hypothesis. When the P-value hovers around 0.05, further scrutiny of the hypothesis is warranted.</p>
<p class="calibre6">In feature selection, the chi-squared test evaluates the relationship between each feature and the target variable in the dataset. It determines significance based on whether a statistically significant difference exists between the observed and expected frequencies of the feature, assuming independence between the feature and target. Features with a high chi-squared score exhibit a stronger dependence on the target variable, making them more informative for classification or regression tasks. The formula for calculating the chi-squared is presented in the following equation:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mfrac&gt;&lt;msup&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;msub&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/130.png" class="calibre136"/></p>
<p class="calibre6">In this equation, <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/131.png" class="calibre137"/> represents the observed value and <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;E&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/132.png" class="calibre138"/> represents the expected value. The computation involves finding the difference between the observed frequency and the expected frequency, squaring the result, and then dividing by the expected frequency. The summation of these values across all categories of the feature yields the overall<a id="_idIndexMarker144" class="calibre5 pcalibre1 pcalibre"/> chi-squared statistic for that feature.</p>
<p class="calibre6">The degrees of freedom for the test relies on the number of categories in the feature and the number of categories in the target variable.</p>
<p class="calibre6">An exemplary application of<a id="_idIndexMarker145" class="calibre5 pcalibre1 pcalibre"/> chi-squared feature selection lies in text classification, particularly in scenarios where the presence or absence of specific words in a document serves as features. The chi-squared test helps identify words strongly associated with a particular class or category of documents, subsequently enabling their use as features in a ML model. In categorical data, especially where the relationship between features and the target variable is non-linear, chi-squared proves to be a valuable method for feature selection. However, its suitability diminishes for continuous or highly correlated features, where alternative feature selection methods may be more fitting.</p>
<h4 class="calibre135">Mutual information</h4>
<p class="calibre6">Mutual information acts as <a id="_idIndexMarker146" class="calibre5 pcalibre1 pcalibre"/>a metric to gauge the interdependence of two random variables. In the context of feature selection, it quantifies the information a feature provides about the target variable. The core methodology entails calculating the mutual information between each feature and the target variable, ultimately selecting features with the highest mutual information scores.</p>
<p class="calibre6">Mathematically, the mutual information between two discrete random variables, <em class="italic">X</em> and <em class="italic">Y</em>, can be defined as follows:</p>
<p class="calibre6"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi&gt;I&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;mml:mo&gt;;&lt;/mml:mo&gt;&lt;mml:mi&gt;Y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:munder&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;∈&lt;/mml:mo&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:munder&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;∈&lt;/mml:mo&gt;&lt;mml:mi&gt;Y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;log&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;⁡&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" src="img/133.png" class="calibre139"/></p>
<p class="calibre6">In the given equation, <em class="italic">p(x, y)</em> represents the joint probability mass function of <em class="italic">X</em> and <em class="italic">Y</em>, while <em class="italic">p(x)</em> and <em class="italic">p(y)</em> denote the marginal probability mass functions of <em class="italic">X</em> and <em class="italic">Y</em>, respectively.</p>
<p class="calibre6">In the context of feature selection, mutual information calculation involves treating the feature as <em class="italic">X</em> and the <a id="_idIndexMarker147" class="calibre5 pcalibre1 pcalibre"/>target variable as <em class="italic">Y</em>. By computing the mutual information score for each feature, we can then select features with the highest scores.</p>
<p class="calibre6">To estimate the probability mass functions needed for calculating mutual information, histogram-based methods can be employed. This involves dividing the range of each variable into a fixed number of bins and estimating the probability mass functions based on the frequencies of observations in each bin. Alternatively, kernel density estimation can be utilized to estimate the probability density functions, and mutual information can then be computed based on the estimated densities.</p>
<p class="calibre6">In practical applications, mutual information is often employed alongside other feature selection methods, such as chi-squared or correlation-based methods, to enhance the overall performance of the feature selection process.</p>
<h4 class="calibre135">Correlation coefficients</h4>
<p class="calibre6">Correlation coefficients serve as<a id="_idIndexMarker148" class="calibre5 pcalibre1 pcalibre"/> indicators of the strength and direction of the linear relationship between two variables. In the realm of feature selection, these coefficients prove useful in identifying features highly correlated with the target variable, thus serving as potentially valuable predictors.</p>
<p class="calibre6">The prevalent correlation coefficient employed for feature selection is the Pearson correlation coefficient, also referred to as<a id="_idIndexMarker149" class="calibre5 pcalibre1 pcalibre"/> Pearson’s <em class="italic">r</em>. Pearson’s r measures the linear relationship between two continuous variables, ranging from -1 (indicating a perfect negative correlation) to 1 (indicating a perfect positive correlation), with 0 denoting no correlation. Its calculation involves dividing the covariance between the two variables by the product of their standard deviations, as depicted in the following equation:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;Y&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;Y&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/134.png" class="calibre140"/></p>
<p class="calibre6">In the given equation, <em class="italic">X</em> and <em class="italic">Y</em> represent the two variables of interest, <em class="italic">cov()</em> denotes the covariance function, and <em class="italic">std()</em> represents the standard deviation function.</p>
<p class="calibre6">Utilizing Pearson’s <em class="italic">r</em> for feature selection involves computing the correlation between each feature and the target variable. Features with the highest absolute correlation coefficients are then selected. A high absolute correlation coefficient signifies a strong correlation with the target variable, whether positive or negative. The interpretation of Pearson correlation values and their degree of correlation is outlined in <em class="italic">Table 3.1</em>:</p>
<table class="no-table-style" id="table001-2">
<colgroup class="calibre10">
<col class="calibre11"/>
<col class="calibre11"/>
</colgroup>
<thead class="calibre12">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><strong class="bold">Pearson </strong><strong class="bold">Correlation Value</strong></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold">Degree </strong><strong class="bold">of Correlation</strong></p>
</td>
</tr>
</thead>
<tbody class="calibre13">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6">± 1</p>
</td>
<td class="no-table-style2">
<p class="calibre6">Perfect</p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6">± 0.50 - ± 1</p>
</td>
<td class="no-table-style2">
<p class="calibre6">High degree</p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6">± 0.30 - ± 0.49</p>
</td>
<td class="no-table-style2">
<p class="calibre6">Moderate degree</p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6">&lt; +0.29</p>
</td>
<td class="no-table-style2">
<p class="calibre6">Low degree</p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6">0</p>
</td>
<td class="no-table-style2">
<p class="calibre6">No correlation</p>
</td>
</tr>
</tbody>
</table>
<p class="img---caption" lang="en-US" xml:lang="en-US">Table 3 .1 – Pearson correlation values and their degree of correlation</p>
<p class="calibre6">It’s worth noting that Pearson’s <em class="italic">r</em> is only appropriate for identifying linear relationships between variables. If the relationship is nonlinear, or if one or both of the variables are categorical, other <a id="_idIndexMarker150" class="calibre5 pcalibre1 pcalibre"/>correlation coefficients such as Spearman’s <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;ρ&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="img/135.png" class="calibre141"/>or Kendall’s <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;τ&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="img/136.png" class="calibre142"/> may be more appropriate. Additionally, it is important to be cautious when interpreting correlation coefficients as a high correlation does not necessarily imply causation.<a id="_idTextAnchor062" class="calibre5 pcalibre1 pcalibre"/></p>
<h3 class="calibre8">Wrapper methods</h3>
<p class="calibre6">These techniques delve into<a id="_idIndexMarker151" class="calibre5 pcalibre1 pcalibre"/> subsets of features through iterative model training and testing. Widely known methods encompass forward selection, backward elimination, and recursive feature elimination. While computationally demanding, these methods have the potential to significantly enhance model accuracy.</p>
<p class="calibre6">A concrete illustration of a wrapper method is <strong class="bold">recursive feature elimination</strong> (<strong class="bold">RFE</strong>). Functioning as a backward elimination approach, RFE systematically removes the least important <a id="_idIndexMarker152" class="calibre5 pcalibre1 pcalibre"/>feature until a predetermined number of features remains. During each iteration, a machine learning model is trained on the existing<a id="_idIndexMarker153" class="calibre5 pcalibre1 pcalibre"/> features, and the least important feature is pruned based on its feature importance score. This sequential process persists until the specified number of features is attained. The feature importance score can be extracted from diverse methods, including coefficient values from linear models or feature importance scores derived from decision trees. RFE is a computationally expensive method, but it can be useful when the number of features is very large and there is a need to reduce the feature space. An alternative approach is to have feature selection during the training process, something that’s done via embedding methods.</p>
<h3 class="calibre8">Embedded methods</h3>
<p class="calibre6">These methods select features<a id="_idIndexMarker154" class="calibre5 pcalibre1 pcalibre"/> during the training process of the model. Popular methods include LASSO and ridge regression, decision trees, and random forests.</p>
<h3 class="calibre8">LASSO</h3>
<p class="calibre6"><strong class="bold">LASSO</strong>, an acronym for <strong class="bold">Least Absolute Shrinkage and Selection Operator</strong>, serves as a linear regression technique that’s commonly employed for feature selection in machine learning. Its<a id="_idIndexMarker155" class="calibre5 pcalibre1 pcalibre"/> mechanism involves introducing a penalty term to the standard regression loss function. This penalty encourages the model to reduce the coefficients of less important features to zero, effectively eliminating <a id="_idIndexMarker156" class="calibre5 pcalibre1 pcalibre"/>them from the model.</p>
<p class="calibre6">The LASSO method proves especially valuable when grappling with high-dimensional data, where the number of features far exceeds the number of samples. In such scenarios, discerning the most crucial features for predicting the target variable can be challenging. LASSO comes to the fore by automatically identifying the most relevant features while simultaneously shrinking the coefficients of others.</p>
<p class="calibre6">The LASSO method works by finding the solution for the following optimization problem, which is a minimization problem:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;min&lt;/mi&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;w&lt;/mi&gt;&lt;/munder&gt;&lt;msubsup&gt;&lt;mfenced open=&quot;‖&quot; close=&quot;‖&quot;&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;y&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;X&lt;/mi&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;w&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msubsup&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;λ&lt;/mi&gt;&lt;msub&gt;&lt;mfenced open=&quot;‖&quot; close=&quot;‖&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;w&lt;/mi&gt;&lt;/mfenced&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/137.png" class="calibre143"/></p>
<p class="calibre6">In the given equation, vector <em class="italic">y</em> represents the target variable, <em class="italic">X</em> denotes the feature matrix, <em class="italic">w</em> signifies the vector of regression coefficients, <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;λ&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="img/138.png" class="calibre144"/> is a hyperparameter dictating the intensity of the penalty term, and <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;w&lt;/mml:mi&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/139.png" class="calibre145"/> stands for the <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;script&quot;&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/140.png" class="calibre146"/>norm of the coefficients (that is, the sum of their absolute values).</p>
<p class="calibre6">The inclusion of the <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;script&quot;&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/141.png" class="calibre147"/> penalty term in the objective function prompts the model to precisely zero out certain coefficients, essentially eliminating the associated features from the model. The degree of penalty strength is governed by the <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;λ&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="img/142.png" class="calibre148"/> hyperparameter, which can be fine-tuned through the use of cross-validation.</p>
<p class="calibre6">LASSO has several advantages over other feature selection methods, such as its ability to handle correlated features and its ability to perform feature selection and regression simultaneously. However, LASSO has some limitations, such as its tendency to select only one feature from a group of correlated features, and its performance may deteriorate if the number of features is much larger than the number of samples.</p>
<p class="calibre6">Consider the application of LASSO for feature selection in predicting house prices. Imagine a dataset encompassing details about houses – such as the number of bedrooms, lot size, construction year, and so on – alongside their respective sale prices. Employing LASSO, we <a id="_idIndexMarker157" class="calibre5 pcalibre1 pcalibre"/>can pinpoint the most crucial features to predict the sale price while concurrently fitting a linear regression model to the dataset. The <a id="_idIndexMarker158" class="calibre5 pcalibre1 pcalibre"/>outcome is a model that’s ready to forecast the sale price of a new house based on its features.</p>
<h3 class="calibre8">Ridge regression</h3>
<p class="calibre6">Ridge regression, a linear <a id="_idIndexMarker159" class="calibre5 pcalibre1 pcalibre"/>regression method applicable to feature selection, closely resembles ordinary least squares regression but introduces a penalty term to the cost function to counter overfitting.</p>
<p class="calibre6">In ridge regression, the cost function <a id="_idIndexMarker160" class="calibre5 pcalibre1 pcalibre"/>undergoes modification with the inclusion of a penalty term directly proportional to the square of the coefficients’ magnitude. This penalty term is regulated by a hyperparameter, often denoted as <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;λ&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="img/138.png" class="calibre149"/> or <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;α&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="img/144.png" class="calibre150"/> dictating the regularization strength. When <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;α&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="img/144.png" class="calibre150"/> is set to zero, ridge regression reverts to ordinary least squares regression.</p>
<p class="calibre6">The penalty term’s impact manifests in shrinking the coefficients’ magnitude toward zero. This proves beneficial in mitigating overfitting, discouraging the model from excessively relying on any single feature. In effect, the penalty term acts as a form of feature selection by reducing the importance of less relevant features.</p>
<p class="calibre6">The equation for the ridge regression loss function is as follows:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;min&lt;/mi&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;w&lt;/mi&gt;&lt;/munder&gt;&lt;msubsup&gt;&lt;mfenced open=&quot;‖&quot; close=&quot;‖&quot;&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;y&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;X&lt;/mi&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;w&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msubsup&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;α&lt;/mi&gt;&lt;msub&gt;&lt;mfenced open=&quot;‖&quot; close=&quot;‖&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;w&lt;/mi&gt;&lt;/mfenced&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/146.png" class="calibre151"/></p>
<p class="calibre6">Here, we have the following:</p>
<ul class="calibre14">
<li class="calibre15"><em class="italic">N</em> is the number of samples in the training set.</li>
<li class="calibre15"><em class="italic">y</em> is the column vector of target values of size <em class="italic">N</em>.</li>
<li class="calibre15"><em class="italic">X</em> is the design matrix of input features.</li>
<li class="calibre15"><em class="italic">w</em> is the vector of regression coefficients to be estimated.</li>
<li class="calibre15"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;α&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="img/147.png" class="calibre152"/> is the regularization parameter that controls the strength of the penalty term. It is a hyperparameter that needs to be tuned.</li>
</ul>
<p class="calibre6">The first term in the loss function measures the mean squared error between the predicted values and the true values. The second term is the <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;script&quot;&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/148.png" class="calibre153"/> penalty term that shrinks the coefficients toward zero. The ridge regression algorithm finds the values of the regression coefficients that <a id="_idIndexMarker161" class="calibre5 pcalibre1 pcalibre"/>minimize this loss function. By tuning the regularization parameter, <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;α&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="img/149.png" class="calibre154"/>, we can control the bias-variance trade-off <a id="_idIndexMarker162" class="calibre5 pcalibre1 pcalibre"/>of the model, with higher alpha values leading to more regularization and lower overfitting.</p>
<p class="calibre6">Ridge regression can be used for feature selection by examining the magnitudes of the coefficients produced by the model. Features with coefficients that are close to zero or smaller are considered less important and can be dropped from the model. The value of <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;α&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="img/150.png" class="calibre155"/> can be tuned using cross-validation to find the optimal balance between model complexity and accuracy.</p>
<p class="calibre6">One of the main advantages of ridge regression is its ability to handle multicollinearity, which occurs when there are strong correlations between the independent variables. In such cases, ordinary least squares regression can produce unstable and unreliable coefficient estimates, but ridge regression can help stabilize the estimates and improve the overall performance of the model.</p>
<h3 class="calibre8">Choosing LASSO or ridge regression</h3>
<p class="calibre6">Ridge regression and LASSO <a id="_idIndexMarker163" class="calibre5 pcalibre1 pcalibre"/>are both regularization techniques that are used in linear regression to prevent overfitting of the model by <a id="_idIndexMarker164" class="calibre5 pcalibre1 pcalibre"/>penalizing the model’s coefficients. While both methods seek to prevent overfitting, they differ in their approach to how the coefficients are penalized.</p>
<p class="calibre6">Ridge regression adds a penalty term to the <strong class="bold">sum of squared errors</strong> (<strong class="bold">SSE</strong>) that is proportional to the square of the magnitude of the coefficients. The penalty term is controlled by a regularization parameter (<img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;α&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="img/151.png" class="calibre156"/>), which determines the amount of shrinkage applied to the coefficients. This penalty term shrinks the values of the coefficients toward zero but does not set them exactly to zero. Therefore, ridge regression can be used to reduce the impact of irrelevant features in a model, but it will not eliminate them completely.</p>
<p class="calibre6">On the other hand, LASSO also adds a penalty term to the SSE, but the penalty term is proportional to the absolute value of the coefficients. Like ridge, LASSO also has a regularization parameter (<img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;λ&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="img/152.png" class="calibre157"/>) that determines the amount of shrinkage applied to the coefficients. However, LASSO has a unique property of setting some of the coefficients exactly to zero when the regularization parameter is sufficiently high. Therefore, LASSO can be used for feature selection as it can eliminate irrelevant features and set their corresponding coefficients to zero.</p>
<p class="calibre6">In general, if the dataset has many<a id="_idIndexMarker165" class="calibre5 pcalibre1 pcalibre"/> features and a small number of them are expected to be important, LASSO regression is a better choice as it will set the coefficients of irrelevant features to zero, leading to a simpler and more interpretable model. On the other hand, if most of the features in the dataset are expected to be relevant, ridge regression is a better choice as it will shrink the coefficients toward zero but not set them exactly to zero, preserving all the features in the model.</p>
<p class="calibre6">However, it is important to note that the optimal choice between ridge and LASSO depends on the specific problem and dataset, and it is often recommended to try both and compare their performance using cross-validat<a id="_idTextAnchor063" class="calibre5 pcalibre1 pcalibre"/>ion techniques.</p>
<h3 class="calibre8">Dimensionality reduction techniques</h3>
<p class="calibre6">These methods transform the features into <a id="_idIndexMarker166" class="calibre5 pcalibre1 pcalibre"/>a lower-dimensional space while retaining as much information as possible. Popular <a id="_idIndexMarker167" class="calibre5 pcalibre1 pcalibre"/>methods include PCA, <strong class="bold">linear discriminant analysis</strong> (<strong class="bold">LDA</strong>), and <strong class="bold">t-</strong>SNE.</p>
<h4 class="calibre135">PCA</h4>
<p class="calibre6">PCA is a widely used technique<a id="_idIndexMarker168" class="calibre5 pcalibre1 pcalibre"/> in machine learning for reducing the dimensionality of large datasets while retaining most of the important information. The basic idea of PCA is to transform a set of <a id="_idIndexMarker169" class="calibre5 pcalibre1 pcalibre"/>correlated variables into a set of uncorrelated variables known as principal components.</p>
<p class="calibre6">The goal of PCA is to identify the directions of maximum variance in the data and project the data in these directions, reducing the dimensionality of the data. The principal components are sorted in order of the amount of variance they explain, with the first principal component explaining the most variance in the data.</p>
<p class="calibre6">The PCA algorithm involves the following steps:</p>
<ol class="calibre16">
<li class="calibre15"><strong class="bold">Standardize the data</strong>: PCA requires the data to be standardized – that is, each feature must have zero mean and unit variance.</li>
<li class="calibre15"><strong class="bold">Compute the covariance matrix</strong>: The covariance matrix is a square matrix that measures the linear relationships between pairs of features in the data.</li>
<li class="calibre15"><strong class="bold">Compute the eigenvectors and eigenvalues of the covariance matrix</strong>: The eigenvectors represent the primary directions of the highest variance within the dataset, while the eigenvalues quantify the extent of variance elucidated by each eigenvector.</li>
<li class="calibre15"><strong class="bold">Select the number of principal components</strong>: The number of principal components to retain can be determined by analyzing the eigenvalues and selecting the top <em class="italic">k</em> eigenvectors that explain the most variance.</li>
<li class="calibre15"><strong class="bold">Project the data onto the selected principal components</strong>: The original data is projected onto the selected principal components, resulting in a lower-dimensional representation of the data.</li>
</ol>
<p class="calibre6">PCA can be used for feature<a id="_idIndexMarker170" class="calibre5 pcalibre1 pcalibre"/> selection by selecting the top <em class="italic">k</em> principal components that explain the most variance in the data. This can be useful for reducing the dimensionality of high-dimensional datasets and improving the performance of machine learning models. However, it’s important to note that PCA may not always lead to improved performance, especially if the data is already low-dimensional or if the features are not highly correlated. It’s also important to consider the interpretability of the selected principal components as they may not always correspond to meaningful features in the data.</p>
<h4 class="calibre135">LDA</h4>
<p class="calibre6">LDA is a dimensionality <a id="_idIndexMarker171" class="calibre5 pcalibre1 pcalibre"/>reduction technique that’s used for <a id="_idIndexMarker172" class="calibre5 pcalibre1 pcalibre"/>feature selection in machine learning. It is often used in classification tasks to reduce the number of features by transforming them into a lower-dimensional space while retaining as much class-discriminatory information as possible.</p>
<p class="calibre6">In LDA, the goal is to find a linear combination of the original features that maximizes the separation between classes. The input to LDA is a dataset of labeled examples, where each example is a feature vector with a corresponding class label. The output of LDA is a set of linear combinations of the original features, which can be used as new features in a machine learning model.</p>
<p class="calibre6">To perform LDA, the first step is to compute the mean and covariance matrix of each class. The overall mean and covariance matrix are then calculated from the class means and covariance matrices. The goal is to project the data onto a lower-dimensional space while <a id="_idIndexMarker173" class="calibre5 pcalibre1 pcalibre"/>still retaining the class information. This is achieved by finding the eigenvectors and eigenvalues of the covariance matrix, sorting them in descending order of the eigenvalues, and selecting the top <em class="italic">k</em> eigenvectors that correspond to the <em class="italic">k</em> largest eigenvalues. The selected eigenvectors form the basis for the new feature space.</p>
<p class="calibre6">The LDA algorithm can be summarized in the following steps:</p>
<ol class="calibre16">
<li class="calibre15">Compute the mean vector of each class.</li>
<li class="calibre15">Compute the covariance matrix of each class.</li>
<li class="calibre15">Compute the overall mean vector and overall covariance matrix.</li>
<li class="calibre15">Compute the between-class scatter matrix.</li>
<li class="calibre15">Compute the within-class scatter matrix.</li>
<li class="calibre15">Compute the eigenvectors and eigenvalues of the matrix using the following equation:</li>
</ol>
<p class="calibre6"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;S&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;*&lt;/mml:mi&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;S&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/153.png" class="calibre158"/></p>
<p class="calibre6">Here, <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;S&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/154.png" class="calibre159"/> is the within-class scatter matrix and <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;S&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/155.png" class="calibre160"/> is the between-class scatter matrix.</p>
<p class="calibre6">7.	Select the top <em class="italic">k</em> eigenvectors with the highest eigenvalues as the new feature space.</p>
<p class="calibre6">LDA is particularly useful when the number of features is large and the number of examples is small. It can be used in a <a id="_idIndexMarker174" class="calibre5 pcalibre1 pcalibre"/>variety of applications, including image recognition, speech<a id="_idIndexMarker175" class="calibre5 pcalibre1 pcalibre"/> recognition, and NLP. However, it assumes that the classes are normally distributed and that the class covariance matrices are equal, which may not always be the case in practice.</p>
<h4 class="calibre135">t-SNE</h4>
<p class="calibre6">t-SNE is a dimensionality<a id="_idIndexMarker176" class="calibre5 pcalibre1 pcalibre"/> reduction technique that’s used for visualizing high-dimensional data in a low-dimensional space, often used for feature selection. It was developed by Laurens van der Maaten and Geoffrey <a id="_idIndexMarker177" class="calibre5 pcalibre1 pcalibre"/>Hinton in 2008.</p>
<p class="calibre6">The basic idea behind t-SNE is to preserve the pairwise similarities of data points in a low-dimensional space, as opposed to preserving the distances between them. In other words, it tries to retain the local structure of the data while discarding the global structure. This can be useful in situations where the high-dimensional data is difficult to visualize, but there may be meaningful patterns and relationships among the data points.</p>
<p class="calibre6">t-SNE starts by calculating the pairwise similarity between each pair of data points in the high-dimensional space. The similarity is usually measured using a Gaussian kernel, which gives higher weights to nearby points and lower weights to distant points. The similarity matrix is then converted into a probability distribution using a softmax function. This distribution is used to create a low-dimensional space, typically 2D or 3D.</p>
<p class="calibre6">In the low-dimensional space, t-SNE again calculates the pairwise similarities between each pair of data points, but this time using a student’s t-distribution instead of a Gaussian distribution. The t-distribution has heavier tails than the Gaussian distribution, which helps to better preserve the local structure of the data. t-SNE then adjusts the position of the points in the low-dimensional space to minimize the difference between the pairwise similarities in the high-dimensional space and the pairwise similarities in the low-dimensional space.</p>
<p class="calibre6">t-SNE is a powerful technique for visualizing high-dimensional data by reducing it to a low-dimensional space. However, it is not typically used for feature selection as its primary purpose is to create visualizations of complex datasets.</p>
<p class="calibre6">Instead, t-SNE can be<a id="_idIndexMarker178" class="calibre5 pcalibre1 pcalibre"/> used to help identify clusters of data points that share similar features, which may be useful in identifying <a id="_idIndexMarker179" class="calibre5 pcalibre1 pcalibre"/>groups of features that are important for a particular task. For example, suppose you have a dataset of customer demographics and purchase history, and you want to identify groups of customers that are similar based on their purchasing behavior. You could use t-SNE to reduce the high-dimensional feature space to two dimensions, and then plot the resulting data points on a scatter plot. By examining the plot, you might be able to identify clusters of customers with similar purchasing behavior, which could then inform your feature selection process. Here’s a sample t-SNE for the MNIST dataset:</p>
<div><div><img alt="Figure 3.1 – t-SNE on the MNIST dataset" src="img/B18949_03_1.jpg" class="calibre3"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.1 – t-SNE on the MNIST dataset</p>
<p class="calibre6">It’s worth noting that t-SNE is primarily a visualization tool and should not be used as the sole method for feature selection. Instead, it can be used in conjunction with other techniques, such as<a id="_idIndexMarker180" class="calibre5 pcalibre1 pcalibre"/> LDA or PCA, to gain a more complete understanding of the underlying structure of your data.</p>
<p class="calibre6">The choice of feature<a id="_idIndexMarker181" class="calibre5 pcalibre1 pcalibre"/> selection method depends on the nature of the data, the size of the dataset, the complexity of the model, and the computational resources available. It is important to carefully evaluate the performance of the model after feature selection to ensure that important information has not been lost. Another important process is feature engineering, which is about transforming or selecting features fo<a id="_idTextAnchor064" class="calibre5 pcalibre1 pcalibre"/>r the machine learning models.</p>
<h2 id="_idParaDest-51" class="calibre7"><a id="_idTextAnchor065" class="calibre5 pcalibre1 pcalibre"/>Feature engineering</h2>
<p class="calibre6">Feature engineering is the <a id="_idIndexMarker182" class="calibre5 pcalibre1 pcalibre"/>process of selecting, transforming, and extracting features from raw data to improve the performance of machine learning models. Features are the individual measurable properties or characteristics of the data that can be used to make predictions or classifications.</p>
<p class="calibre6">One common technique in <a id="_idIndexMarker183" class="calibre5 pcalibre1 pcalibre"/>feature engineering is feature selection, which involves selecting a subset of relevant features from the original dataset to improve the model’s accuracy and reduce its complexity. This can be done through statistical methods such as correlation analysis or feature importance ranking using decision trees or random forests.</p>
<p class="calibre6">Another technique in feature engineering is feature extraction, which involves transforming the raw data into a new set of features that may be more useful for the model. The primary distinction between feature selection and feature engineering lies in their approaches: while feature selection retains a subset of the original features without modifying the selected features, feature engineering algorithms reconfigure and transform the data into a new feature space. Feature engineering can be done through techniques such as dimensionality reduction, PCA, or t-SNE. Feature selection and extraction were explained in detail in the previous subsection (3-1-3).</p>
<p class="calibre6">Feature scaling is another important technique in feature engineering that involves scaling the values of features to the same range, typically between 0 and 1 or -1 and 1. This is done to prevent certain features from dominating others in the model and to ensure that the algorithm can converge quickly during training. When the features in the dataset have different scales, this can lead to issues when using certain machine learning algorithms that are sensitive to the relative magnitudes of the features. Feature scaling can help to address this problem by ensuring that all features are on a similar scale. Common methods for feature scaling include min-max scaling, Z-score scaling, and scaling by the maximum absolute value.</p>
<p class="calibre6">There are several common methods for feature scaling:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Min-max scaling</strong>: Also known as normalization, this technique scales the values of the feature to be<a id="_idIndexMarker184" class="calibre5 pcalibre1 pcalibre"/> between a specified range, typically between 0 and 1 (for regular machine <a id="_idIndexMarker185" class="calibre5 pcalibre1 pcalibre"/>learning models, and sometimes -1 and 1 for deep learning models). The formula for min-max scaling is shown here:</li>
</ul>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/156.png" class="calibre161"/></p>
<p class="calibre6">Here, <em class="italic">x</em> is the original feature value, <em class="italic">min(x)</em> is the minimum value of the feature, and <em class="italic">max(x)</em> is the maximum value of the feature.</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Standardization</strong>: This<a id="_idIndexMarker186" class="calibre5 pcalibre1 pcalibre"/> technique transforms the feature <a id="_idIndexMarker187" class="calibre5 pcalibre1 pcalibre"/>values to have a mean of 0 and a standard deviation of 1. Standardization is less affected by outliers in the data than min-max scaling. The formula for standardization is shown here:</li>
</ul>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/157.png" class="calibre162"/></p>
<p class="calibre6">Here, <em class="italic">x</em> is the original feature value, <em class="italic">mean(x)</em> is the mean of the feature, and <em class="italic">std(x)</em> is the standard deviation of the feature.</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Robust scaling</strong>: This technique is similar to standardization but uses the median and <strong class="bold">interquartile range</strong> (<strong class="bold">IQR</strong>) instead of <a id="_idIndexMarker188" class="calibre5 pcalibre1 pcalibre"/>the mean and standard deviation. Robust scaling is useful when the data contains outliers that would <a id="_idIndexMarker189" class="calibre5 pcalibre1 pcalibre"/>significantly affect the mean and standard deviation. The formula for robust scaling is shown here:</li>
</ul>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/158.png" class="calibre163"/></p>
<p class="calibre6">Here, <em class="italic">x</em> is the original feature value, <em class="italic">median(x)</em> is the median of the feature, <em class="italic">Q1(x)</em> is the first quartile of the<a id="_idIndexMarker190" class="calibre5 pcalibre1 pcalibre"/> feature, and <em class="italic">Q3(x)</em> is the third quartile of the feature.</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Log transformation</strong>: This technique is used when the data is highly skewed or has a long tail. By taking the logarithm of the feature values, the distribution can be <a id="_idIndexMarker191" class="calibre5 pcalibre1 pcalibre"/>made more normal or symmetric, which can<a id="_idIndexMarker192" class="calibre5 pcalibre1 pcalibre"/> improve the performance of some machine learning algorithms. The formula for log transformation is shown here:</li>
</ul>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/159.png" class="calibre164"/></p>
<p class="calibre6">Here, <em class="italic">x</em> is the original feature value.</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Power transformation</strong>: This technique is similar to log transformation but allows for a broader range of transformations. The most common power transformation is the Box-Cox<a id="_idIndexMarker193" class="calibre5 pcalibre1 pcalibre"/> transformation, which raises the feature values to a power that is determined using maximum likelihood estimation. The formula for the Box-Cox transformation is shown here:</li>
</ul>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;λ&lt;/mi&gt;&lt;/msup&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;λ&lt;/mi&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/160.png" class="calibre165"/></p>
<p class="calibre6">Here, <em class="italic">x</em> is the original feature value, and <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;λ&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="img/161.png" class="calibre166"/> is the power parameter that is estimated using maximum likelihood.</p>
<p class="calibre6">These are some of the most common methods for feature scaling in machine learning. The choice of method depends on the distribution of the data, the machine learning algorithm being used, and the specific requirements of the problem.</p>
<p class="calibre6">One final technique in feature engineering is feature construction, which involves creating new features by combining or transforming existing ones. This can be done through techniques such as polynomial expansion, logarithmic transformation, or interaction terms.</p>
<h3 class="calibre8">Polynomial expansion</h3>
<p class="calibre6">Polynomial expansion is a<a id="_idIndexMarker194" class="calibre5 pcalibre1 pcalibre"/> feature construction technique that involves creating new features by taking polynomial combinations of existing features. This technique is commonly used in machine learning to model nonlinear relationships between features and the target variable.</p>
<p class="calibre6">The idea behind polynomial<a id="_idIndexMarker195" class="calibre5 pcalibre1 pcalibre"/> expansion is to create new features by raising the existing features to different powers and taking their products. For example, suppose we have a single feature, <em class="italic">x</em>. We can create new features by taking the square of <em class="italic">x (</em><em class="italic"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/162.png" class="calibre167"/></em><em class="italic">)</em>. We can also create higher-order polynomial features by taking <em class="italic">x</em> to even higher powers, such as <img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/163.png" class="calibre168"/><em class="italic">, </em><em class="italic"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;4&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/164.png" class="calibre169"/></em>, and so on. In general, we can create polynomial features of degree <em class="italic">d</em> by taking all possible combinations of products and powers of the original features up to degree <em class="italic">d</em>.</p>
<p class="calibre6">In addition to creating polynomial features from a single feature, we can also create polynomial features from multiple features. For example, suppose we have two features, <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/165.png" class="calibre170"/> and <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/166.png" class="calibre171"/>. We can create new polynomial features by taking their products (<img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/167.png" class="calibre172"/>) and raising them to different powers ( <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="img/168.png" class="calibre173"/>, <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="img/169.png" class="calibre174"/>, and so on). Again, we can create polynomial features of any degree by taking all possible combinations of products and powers of the original features.</p>
<p class="calibre6">One important consideration when using polynomial expansion is that it can quickly lead to a large number of features, especially for high degrees of polynomials. This can make the resulting model more complex and harder to interpret, and can also lead to overfitting if the number of features is not properly controlled. To address this issue, it is common to use regularization techniques or feature selection methods to select a subset of the most informative polynomial features.</p>
<p class="calibre6">Overall, polynomial expansion is a powerful feature construction technique that can help capture complex nonlinear relationships between features and the target variable. However, it should be used with caution and with appropriate regularization or feature selection to avoid overfitting and maintain model interpretability.</p>
<p class="calibre6">For example, in a regression problem, you might have a dataset with a single feature, say <em class="italic">x</em>, and you want to fit a model that can capture the relationship between <em class="italic">x</em> and the target variable, <em class="italic">y</em>. However, the relationship between <em class="italic">x</em> and <em class="italic">y</em> may not be linear, and a simple linear model may not be sufficient. In this case, polynomial expansion can be used to create additional features that capture the non-linear relationship between <em class="italic">x</em> and <em class="italic">y</em>.</p>
<p class="calibre6">To illustrate, let’s say you have a dataset with a single feature, <em class="italic">x</em>, and a target variable, <em class="italic">y</em>, and you want to fit<a id="_idIndexMarker196" class="calibre5 pcalibre1 pcalibre"/> a polynomial regression model. The goal is to find a function, <em class="italic">f(x)</em>, that minimizes the difference between the predicted <a id="_idIndexMarker197" class="calibre5 pcalibre1 pcalibre"/>and actual values of <em class="italic">y</em>.</p>
<p class="calibre6">Polynomial expansion can be used to create additional features based on <em class="italic">x</em>, such as <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="img/170.png" class="calibre175"/>, <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="img/171.png" class="calibre175"/>, and so on. This can be done using libraries such as <code>scikit-learn</code>, which has a <code>PolynomialFeatures</code> function that can automatically generate polynomial features of a specified degree.</p>
<p class="calibre6">By adding these polynomial features, the model becomes more expressive and can capture the non-linear relationship between <em class="italic">x</em> and <em class="italic">y</em>. However, it’s important to be careful not to overfit the data as adding too many polynomial features can lead to a model that is overly complex and performs poorly on new, unseen data.</p>
</div>


<div><h3 class="calibre8">Logarithmic transformation</h3>
<p class="calibre6">Logarithmic transformation is<a id="_idIndexMarker198" class="calibre5 pcalibre1 pcalibre"/> a common feature engineering technique that’s used in data preprocessing. The goal of logarithmic transformation is to <a id="_idIndexMarker199" class="calibre5 pcalibre1 pcalibre"/>make data less skewed and more symmetric by applying a logarithmic function to the features. This technique can be particularly useful for features that are skewed, such as those with a long tail of high values.</p>
<p class="calibre6">The logarithmic transformation is defined as an equation taking the natural logarithm of the data:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/172.png" class="calibre176"/></p>
<p class="calibre6">Here, <em class="italic">y</em> is the transformed data and <em class="italic">x</em> is the original data. The logarithmic function maps the original data to a new space, where the relationship between the values is preserved but the scale is compressed. The logarithmic transformation is particularly useful for features with large ranges or that are distributed exponentially, such as the prices<a id="_idIndexMarker200" class="calibre5 pcalibre1 pcalibre"/> of products or the incomes of individuals.</p>
<p class="calibre6">One of the benefits of the logarithmic transformation is that it can help normalize data and make it more suitable for certain machine learning algorithms that assume normally distributed data. Additionally, logarithmic transformation can reduce the impact of outliers on the data, which can help improve the performance of some models.</p>
<p class="calibre6">It’s important to note that the logarithmic transformation<a id="_idIndexMarker201" class="calibre5 pcalibre1 pcalibre"/> is not appropriate for all types of data. For example, if the data includes zero or negative values, the logarithmic transformation cannot be applied directly. In these cases, a modified logarithmic transformation, such as adding a constant before taking the logarithm, may be used. Overall, logarithmic transformation is a useful technique for feature engineering that can help improve the performance of machine learning models, especially when dealing with skewed or exponentially distributed data.</p>
<p class="calibre6">In summary, feature engineering is a critical step in the machine learning pipeline as it can significantly impact the performance and interpretability of the resulting models. Effective feature engineering requires domain knowledge, creativity, and an iterative process of testing and refining different techniques until the optimal set of features is identified.</p>
<h3 class="calibre8">Interaction terms</h3>
<p class="calibre6">In feature construction, interaction<a id="_idIndexMarker202" class="calibre5 pcalibre1 pcalibre"/> terms refer to creating new features by combining two or more existing features in a dataset through multiplication, division, or other mathematical operations. These new features capture the interaction <a id="_idIndexMarker203" class="calibre5 pcalibre1 pcalibre"/>or relationship between the original features, and they can help improve the accuracy of machine learning models.</p>
<p class="calibre6">For example, in a dataset of real estate prices, you might have features such as the number of bedrooms, the number of bathrooms, and the square footage of the property. By themselves, these features provide some information about the price of the property, but they do not capture any interaction effects between the features. However, by creating an interaction term between the number of bedrooms and the square footage, you can capture the idea that larger properties with more bedrooms tend to be more expensive than smaller ones with the same number of bedrooms.</p>
<p class="calibre6">In practice, interaction terms are created by multiplying or dividing two or more features together. For example, if we have two features, <em class="italic">x</em> and <em class="italic">y</em>, we can create an interaction term by multiplying <a id="_idIndexMarker204" class="calibre5 pcalibre1 pcalibre"/>them together: <em class="italic">xy</em>. We can also create<a id="_idIndexMarker205" class="calibre5 pcalibre1 pcalibre"/> interaction terms by dividing one feature by another: <em class="italic">x/y</em>.</p>
<p class="calibre6">When creating interaction terms, it is important to consider which features to combine and how to combine them. Here are some common techniques:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Domain knowledge</strong>: Use domain knowledge or expert intuition to identify which features are likely to interact and how they might interact.</li>
<li class="calibre15"><strong class="bold">Pairwise combinations</strong>: Create interaction terms by pairwise combining all pairs of features in the dataset. This can be computationally expensive, but it can help identify potential interaction effects.</li>
<li class="calibre15"><strong class="bold">PCA</strong>: Use PCA to identify the most important combinations of features, and create interaction terms based on these combinations.</li>
</ul>
<p class="calibre6">Overall, interaction terms are a powerful tool in feature construction that can help capture complex relationships between features and improve the accuracy of machine learning models. However, it is important to be careful when creating interaction terms as too many or poorly chosen terms can le<a id="_idTextAnchor066" class="calibre5 pcalibre1 pcalibre"/>ad to overfitting or decreased model interpretability.</p>
<h1 id="_idParaDest-52" class="calibre4"><a id="_idTextAnchor067" class="calibre5 pcalibre1 pcalibre"/>Common machine learning models</h1>
<p class="calibre6">Here, we will explain some of the most common machine learning models, as well as their advantages and disadvantages. Knowing this information will help you pick the best model for the<a id="_idTextAnchor068" class="calibre5 pcalibre1 pcalibre"/> problem and be able to improve the implemented model.</p>
<h2 id="_idParaDest-53" class="calibre7"><a id="_idTextAnchor069" class="calibre5 pcalibre1 pcalibre"/>Linear regression</h2>
<p class="calibre6">Linear regression is a type of supervised learning algorithm that’s used to model the relationship between a dependent variable and one or more independent variables. It assumes a linear relationship <a id="_idIndexMarker206" class="calibre5 pcalibre1 pcalibre"/>between the input features and the <a id="_idIndexMarker207" class="calibre5 pcalibre1 pcalibre"/>output. The goal of linear regression is to find the best-fit line that predicts the value of the dependent variable based on the independent variables.</p>
<p class="calibre6">The equation for a simple linear regression<a id="_idIndexMarker208" class="calibre5 pcalibre1 pcalibre"/> with one independent variable (also called a <strong class="bold">simple linear equation</strong>) is as follows:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/173.png" class="calibre177"/></p>
<p class="calibre6">Here, we have the following:</p>
<ul class="calibre14">
<li class="calibre15"><em class="italic">y</em> is the dependent variable (the variable we want to predict)</li>
<li class="calibre15"><em class="italic">x</em> is the independent variable (the input variable)</li>
<li class="calibre15"><em class="italic">m</em> is the slope of the line (how much <em class="italic">y</em> changes when <em class="italic">x</em> changes)</li>
<li class="calibre15"><em class="italic">b</em> is the y-intercept (where the line intercepts the <em class="italic">Y</em>-axis when <em class="italic">x</em> = 0)</li>
</ul>
<p class="calibre6">The goal of linear regression is to find <a id="_idIndexMarker209" class="calibre5 pcalibre1 pcalibre"/>the values of <em class="italic">m</em> and <em class="italic">b</em> that minimize the difference between the predicted values and the actual values of the dependent variable. This difference is typically measured using a cost function, such as mean squared error or mean absolute error.</p>
<p class="calibre6">Multiple linear regression is an <a id="_idIndexMarker210" class="calibre5 pcalibre1 pcalibre"/>extension of simple linear regression, where there are multiple independent variables. The equation for multiple linear regression is shown here:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msub&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/174.png" class="calibre178"/></p>
<p class="calibre6">Here we have the following:</p>
<ul class="calibre14">
<li class="calibre15"><em class="italic">y</em> is the dependent variable</li>
<li class="calibre15"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/175.png" class="calibre179"/> are the independent variables</li>
<li class="calibre15"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/176.png" class="calibre180"/> is the y-intercept (when all the independent variables are equal to 0)</li>
<li class="calibre15"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/177.png" class="calibre181"/> are the coefficients (how much <em class="italic">y</em> changes when each independent variable changes)</li>
</ul>
<p class="calibre6">Similar to simple linear regression, the goal of multiple linear regression is to find the values of <img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/178.png" class="calibre182"/><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/179.png" class="calibre183"/> that minimize the difference between the predicted values and the actual values of<a id="_idIndexMarker211" class="calibre5 pcalibre1 pcalibre"/> the dependent variable.</p>
<p class="calibre6">The advantages of linear<a id="_idIndexMarker212" class="calibre5 pcalibre1 pcalibre"/> regression are as follows:</p>
<ul class="calibre14">
<li class="calibre15">It’s simple and easy to<a id="_idIndexMarker213" class="calibre5 pcalibre1 pcalibre"/> understand</li>
<li class="calibre15">It can be used to model a wide range of relationships between the dependent and independent variables</li>
<li class="calibre15">It’s computationally efficient, making it fast and suitable for large datasets</li>
<li class="calibre15">It provides interpretable results, allowing for the analysis of the impact of each independent variable on the dependent variable</li>
</ul>
<p class="calibre6">The disadvantages of linear<a id="_idIndexMarker214" class="calibre5 pcalibre1 pcalibre"/> regression are as follows:</p>
<ul class="calibre14">
<li class="calibre15">It assumes a linear relationship between the input features and the output, which may not always be the case in real-world data</li>
<li class="calibre15">It may not capture complex non-linear relationships between the input features and the output</li>
<li class="calibre15">It’s sensitive to outliers and influential observations, which can affect the accuracy of the model</li>
<li class="calibre15">It assumes that the errors are normally distributed wit<a id="_idTextAnchor070" class="calibre5 pcalibre1 pcalibre"/>h constant variance, which may not always hold true in practice</li>
</ul>
<h2 id="_idParaDest-54" class="calibre7"><a id="_idTextAnchor071" class="calibre5 pcalibre1 pcalibre"/>Logistic regression</h2>
<p class="calibre6">Logistic regression is a popular machine learning algorithm that’s used for classification problems. Unlike<a id="_idIndexMarker215" class="calibre5 pcalibre1 pcalibre"/> linear regression, which is used for predicting continuous values, logistic regression is used for predicting discrete outcomes, typically binary outcomes (0 or 1).</p>
<p class="calibre6">The goal of logistic regression is to estimate the probability of a certain outcome based on one or more input variables. The output of logistic regression is a probability score, which can be<a id="_idIndexMarker216" class="calibre5 pcalibre1 pcalibre"/> converted into a binary class label by applying a threshold value. The threshold value can be adjusted to balance between precision and recall based on the specific requirements of the problem.</p>
<p class="calibre6">The logistic regression model assumes that the relationship between the input variables and the output variable is linear in the logit (log odds) space. The logit function is defined as follows:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/180.png" class="calibre184"/></p>
<p class="calibre6">Here, <em class="italic">p</em> is the probability of the positive outcome (that is, the probability of the event occurring).</p>
<p class="calibre6">The logistic regression model can be represented mathematically as follows:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;*&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;*&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mo&gt;..&lt;/mo&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msub&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;*&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/181.png" class="calibre185"/></p>
<p class="calibre6">Here, <img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mo&gt;…&lt;/mo&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/182.png" class="calibre186"/> are the coefficients of the model, <img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mo&gt;…&lt;/mo&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/183.png" class="calibre187"/> are the input variables, and <em class="italic">logit(p)</em> is the logit function of the probability of a positive outcome.</p>
<p class="calibre6">The logistic regression model is trained using a dataset of labeled examples, where each example consists of a set of input variables and a binary label indicating whether the positive outcome occurred or not. The coefficients of the model are estimated using maximum likelihood estimation, which seeks to find the values of the coefficients that maximize the likelihood of the observed data.</p>
<p class="calibre6">The advantages of logistic regression are as follows:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Interpretable</strong>: The coefficients of the model can be interpreted as the change in the log odds of the positive outcome associated with a unit change in the corresponding<a id="_idIndexMarker217" class="calibre5 pcalibre1 pcalibre"/> input variable, making it easy to understand the impact of each input variable on the predicted <a id="_idIndexMarker218" class="calibre5 pcalibre1 pcalibre"/>probability of the positive outcome</li>
<li class="calibre15"><strong class="bold">Computationally efficient</strong>: Logistic<a id="_idIndexMarker219" class="calibre5 pcalibre1 pcalibre"/> regression is a simple algorithm that can be trained quickly on large datasets</li>
<li class="calibre15"><strong class="bold">Works well with small datasets</strong>: Logistic regression can be effective even with a small number of observations, provided that the input variables are relevant to the prediction task</li>
</ul>
<p class="calibre6">The disadvantages of logistic regression are as follows:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Assumes linearity</strong>: Logistic regression assumes a linear relationship between the input variables <a id="_idIndexMarker220" class="calibre5 pcalibre1 pcalibre"/>and the logit of the probability of the positive outcome, which may not always be the case in real-world datasets</li>
<li class="calibre15"><strong class="bold">May suffer from overfitting</strong>: If the number of input variables is large compared to the number of observations, the model may suffer from overfitting, leading to poor generalization performance on new data</li>
<li class="calibre15"><strong class="bold">Not suitable for non-linear problems</strong>: Logistic regression is a linear algorithm and is not suitable for problems where the relations<a id="_idTextAnchor072" class="calibre5 pcalibre1 pcalibre"/>hip between the input variables and the output variable is non-linear</li>
</ul>
<h2 id="_idParaDest-55" class="calibre7"><a id="_idTextAnchor073" class="calibre5 pcalibre1 pcalibre"/>Decision trees</h2>
<p class="calibre6">Decision trees are a type <a id="_idIndexMarker221" class="calibre5 pcalibre1 pcalibre"/>of supervised learning algorithm used for classification and<a id="_idIndexMarker222" class="calibre5 pcalibre1 pcalibre"/> regression analysis. A decision tree consists of a series of nodes that represent decision points, each of which has one or more branches that lead to other decision points or a final prediction.</p>
<p class="calibre6">In a classification problem, each leaf node of the tree represents a class label, while in a regression problem, each leaf node represents a numerical value. The process of building a decision tree involves choosing a sequence of attributes that best splits the data into subsets that are more homogenous concerning the target variable. This process is <a id="_idIndexMarker223" class="calibre5 pcalibre1 pcalibre"/>typically repeated recursively for each subset until a stopping criterion is met, such as a minimum number of instances in each subset or a maximum depth of the tree.</p>
<p class="calibre6">The equations for decision trees involve calculating the information gain (or another splitting criterion, such as Gini impurity or entropy) for each potential split at each decision point. The<a id="_idIndexMarker224" class="calibre5 pcalibre1 pcalibre"/> attribute with the highest information gain is selected as the split criterion for that node. The conceptual formula for information gain is shown here:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;I&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mo&gt;[&lt;/mo&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;o&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;f&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;′&lt;/mo&gt;&lt;/mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;]&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/184.png" class="calibre188"/></p>
<p class="calibre6">Here, <em class="italic">entropy</em> is a measure of the impurity or randomness of a system. In the context of decision trees, entropy is used to measure the impurity of a node in the tree.</p>
<p class="calibre6">The <em class="italic">entropy</em> of a node is calculated as follows:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;/munderover&gt;&lt;mrow&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;/mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;msub&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/185.png" class="calibre189"/></p>
<p class="calibre6">Here, <em class="italic">c</em> is the number of classes and <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/186.png" class="calibre190"/> is the proportion of the samples that belong to class <em class="italic">i</em> in the node.</p>
<p class="calibre6">The entropy of a node ranges from 0 to 1, with 0 indicating a pure node (that is, all samples belong to the same class) and 1 indicating a node that is evenly split between all classes.</p>
<p class="calibre6">In a decision tree, the entropy of a node is used to determine the splitting criterion for the tree. The idea is to split the node into two or more child nodes such that the entropy of the child nodes is lower than the entropy of the parent node. The split with the lowest entropy is chosen as the best split.</p>
<p class="calibre6">Please note that the choice of the next node in the decision tree differs based on the underlying algorithm – for example, CART, ID3, or C4.5. What we explained here was CART, which uses Gini impurity and entropy to split the data.</p>
<p class="calibre6">The advantage of using<a id="_idIndexMarker225" class="calibre5 pcalibre1 pcalibre"/> entropy as a splitting criterion is that it can handle both binary and multi-class classification problems. It is also relatively computationally efficient compared to <a id="_idIndexMarker226" class="calibre5 pcalibre1 pcalibre"/>other splitting criteria. However, one disadvantage of using entropy is that it tends to create biased trees in favor of attributes with many categories.</p>
<p class="calibre6">Here are some of the advantages of decision trees:</p>
<ul class="calibre14">
<li class="calibre15">Easy to understand and interpret, even for non-experts</li>
<li class="calibre15">Can handle both categorical and numerical data</li>
<li class="calibre15">Can handle missing data and outliers</li>
<li class="calibre15">Can be used for feature selection</li>
<li class="calibre15">Can be combined with other models in ensemble methods, such as random forests</li>
</ul>
<p class="calibre6">Here are some of the disadvantages of decision trees:</p>
<ul class="calibre14">
<li class="calibre15">Can be prone to overfitting, especially if the tree is too deep or complex</li>
<li class="calibre15">Can be sensitive to small changes in the data or the way the tree is built</li>
<li class="calibre15">Can be biased toward features with many categories or<a id="_idTextAnchor074" class="calibre5 pcalibre1 pcalibre"/> high cardinality</li>
<li class="calibre15">Can have problems with rare events or imbalanced datasets</li>
</ul>
<h2 id="_idParaDest-56" class="calibre7"><a id="_idTextAnchor075" class="calibre5 pcalibre1 pcalibre"/>Random forest</h2>
<p class="calibre6">Random forest is an <a id="_idIndexMarker227" class="calibre5 pcalibre1 pcalibre"/>ensemble learning method that’s versatile and can perform classification and regression tasks. It operates by generating multiple decision trees during training, predicting the target class for classification based on the majority of the trees, and the predicted value based on the mean prediction by trees for regression tasks. The algorithm for constructing a random forest can be summarized<a id="_idIndexMarker228" class="calibre5 pcalibre1 pcalibre"/> in the following steps:</p>
<ol class="calibre16">
<li class="calibre15"><strong class="bold">Bootstrap sampling</strong>: Randomly select a subset of the data with replacement to create a new dataset that’s the same size as the original dataset.</li>
<li class="calibre15"><strong class="bold">Feature selection</strong>: Randomly select a subset of the features (columns) for each split when building a decision tree. This helps to create diversity in the trees and reduce overfitting.</li>
<li class="calibre15"><strong class="bold">Tree building</strong>: Construct a decision tree for each bootstrap sample and feature subset. The decision tree is constructed recursively by splitting the data based on the selected features until a stopping criterion is met (for example, maximum depth or minimum number of samples in a leaf node).</li>
<li class="calibre15"><strong class="bold">Ensemble learning</strong>: Combine the predictions of all decision trees to make a final prediction. For classification, the class that receives the most votes from the decision trees is the final prediction. For regression, the average of the predictions from all decision trees is the final prediction.</li>
</ol>
<p class="calibre6">The random forest algorithm can be expressed mathematically as follows.</p>
<p class="calibre6">Given a dataset, <em class="italic">D</em>, with <em class="italic">N</em> samples and <em class="italic">M</em> features, we create <em class="italic">T</em> decision trees {Tre e 1, Tre e 2, … , Tre e T} by applying the preceding steps. Each decision tree is constructed using a bootstrap sample of the data, <em class="italic">D’</em>, with size <em class="italic">N’ (N’ &lt;= N)</em> and a subset of the features, <em class="italic">F’</em>, with size <em class="italic">m (m &lt;= M)</em>. For each split in the decision tree, we randomly select <em class="italic">k (k &lt; m)</em> features from <em class="italic">F’</em> and choose the best feature to split the data based on an impurity measure (for example, Gini index or entropy). The decision tree is built until a stopping criterion is met (for example, the maximum depth or minimum number of samples in a leaf node).</p>
<p class="calibre6">The final prediction, <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:math&gt;" src="img/187.png" class="calibre191"/>, for a new sample, <em class="italic">x</em>, is obtained by aggregating the predictions from all decision trees.</p>
<p class="calibre6">For classification, <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:math&gt;" src="img/188.png" class="calibre192"/> is the class that receives the most votes from all decision trees:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mover&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/msub&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/munder&gt;&lt;mrow&gt;&lt;mi&gt;I&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/189.png" class="calibre193"/></p>
<p class="calibre6">Here, <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/190.png" class="calibre194"/> is the prediction of the <em class="italic">j-th</em> decision tree for the <em class="italic">i-th</em> sample, and <em class="italic">I()</em> is the indicator function that returns 1 if the condition is true and 0 otherwise.</p>
<p class="calibre6">For regression, <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:math&gt;" src="img/188.png" class="calibre192"/> is the average <a id="_idIndexMarker229" class="calibre5 pcalibre1 pcalibre"/>of the predictions from all decision trees:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mover&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/munderover&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/192.png" class="calibre195"/></p>
<p class="calibre6">Here, <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/193.png" class="calibre196"/> is the prediction of the <em class="italic">i-th</em> decision tree for the new sample, <em class="italic">x</em>.</p>
<p class="calibre6">In summary, random forest is a powerful machine learning algorithm that can handle high-dimensional and noisy datasets. It works by constructing multiple decision trees using bootstrap samples of the data and feature subsets, and then aggregating the predictions of all decision trees to make a final prediction. The algorithm is scalable, easy to use, and provides a measure of feature importance, making it a popular choice for many machine learning applications.</p>
<p class="calibre6">The advantages of random<a id="_idIndexMarker230" class="calibre5 pcalibre1 pcalibre"/> forests are as follows:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Robustness</strong>: Random forest is a very robust algorithm that can handle a variety of input data types, including numerical, categorical, and ordinal data</li>
<li class="calibre15"><strong class="bold">Feature selection</strong>: Random forest can rank the importance of features, allowing users to identify the most important features for classification or regression tasks</li>
<li class="calibre15"><strong class="bold">Overfitting</strong>: Random forest has a built-in mechanism for reducing overfitting, called bagging, which <a id="_idIndexMarker231" class="calibre5 pcalibre1 pcalibre"/>helps to generalize well on new data</li>
<li class="calibre15"><strong class="bold">Scalability</strong>: Random forest can handle large datasets with a high number of features, making it a good choice for big data applications</li>
<li class="calibre15"><strong class="bold">Outliers</strong>: Random forest is robust to the presence of outliers as it is based on decision trees, which can handle outliers effectively</li>
</ul>
<p class="calibre6">The disadvantages of random<a id="_idIndexMarker232" class="calibre5 pcalibre1 pcalibre"/> forests are as follows:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Interpretability</strong>: Random forest models can be difficult to interpret as they are based on an ensemble of decision trees</li>
<li class="calibre15"><strong class="bold">Training time</strong>: The training time of a random forest can be longer than other simpler algorithms, especially when the number of trees in the ensemble is large</li>
<li class="calibre15"><strong class="bold">Memory usage</strong>: Random forest requires more memory than some other algorithms as it has to store the decision trees in memory</li>
<li class="calibre15"><strong class="bold">Bias</strong>: Random forest can suffer from bias if the data is imbalanced or if the target variable has a high cardinality</li>
<li class="calibre15"><strong class="bold">Overfitting</strong>: Although random forest is designed to prevent overfitting, it is still possible to overfit the model if the hyperparameters are not properly tuned</li>
</ul>
<p class="calibre6">Overall, random forest is a powerful machine learning algorithm that has many advantages, but it is important <a id="_idTextAnchor076" class="calibre5 pcalibre1 pcalibre"/>to carefully consider its limitations before applying it to a particular problem.</p>
<h2 id="_idParaDest-57" class="calibre7"><a id="_idTextAnchor077" class="calibre5 pcalibre1 pcalibre"/>Support vector machines (SVMs)</h2>
<p class="calibre6">SVMs are considered<a id="_idIndexMarker233" class="calibre5 pcalibre1 pcalibre"/> robust supervised learning algorithms that can perform both classification and regression tasks. They excel in scenarios with intricate decision boundaries, surpassing the limitations of linear models. At their core, SVMs aim to identify a hyperplane within a multi-dimensional space that maximally segregates the classes. This hyperplane is positioned to maximize the distance between itself and the closest points from each class, known as support vectors. Here’s how SVMs work for a binary classification problem. Given a set of training data, <img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mo&gt;{&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mo&gt;...&lt;/mo&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;}&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/194.png" class="calibre197"/>, where <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/195.png" class="calibre198"/> is a d-dimensional feature vector and <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/196.png" class="calibre199"/> is the binary class label (+1 or -1), the goal of an SVM is to find a hyperplane that separates the two classes with the largest margin. The margin is defined as the distance between the hyperplane and the closest data points from each class:</p>
<div><div><img alt="Figure 3.2 – SVM margins" src="img/B18949_03_2.jpg" class="calibre3"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.2 – SVM margins</p>
<p class="calibre6">The hyperplane is defined by a weight vector, <em class="italic">w</em>, and a bias term, <em class="italic">b</em>, such that for any new data point, <em class="italic">x</em>, the <a id="_idIndexMarker234" class="calibre5 pcalibre1 pcalibre"/>predicted class label, <em class="italic">y</em>, is given by the following equation:</p>
<p class="calibre6"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="img/197.png" class="calibre200"/>+b)</p>
<p class="calibre6">Here, <em class="italic">sign</em> is the sign function, which returns +1 if the argument is positive and -1 otherwise.</p>
<p class="calibre6">The objective function of an SVM is to minimize the classification error subject to the constraint that the margin is maximized. This can be formulated as an optimization problem:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;msup&gt;&lt;mrow&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;w&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;/mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/198.png" class="calibre201"/></p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;w&lt;/mi&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msup&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;≥&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1,2&lt;/mn&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mo&gt;...&lt;/mo&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/199.png" class="calibre202"/></p>
<p class="calibre6">Here, <img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mrow&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;w&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;/mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/200.png" class="calibre203"/> is the squared Euclidean norm of the weight vector, <em class="italic">w</em>. The constraints ensure that all data points are correctly classified and that the margin is maximized.</p>
<p class="calibre6">Here are some of the advantages of SVMs:</p>
<ul class="calibre14">
<li class="calibre15">Effective in high-dimensional<a id="_idIndexMarker235" class="calibre5 pcalibre1 pcalibre"/> spaces, which is useful when the number of features is large</li>
<li class="calibre15">Can be used for both classification and regression tasks</li>
<li class="calibre15">Works well with both linearly separable and non-linearly separable data</li>
<li class="calibre15">Can handle outliers well due to the use of the margin concept</li>
<li class="calibre15">Has a regularization parameter that allows you to control overfitting</li>
</ul>
<p class="calibre6">Here are some of the <a id="_idIndexMarker236" class="calibre5 pcalibre1 pcalibre"/>disadvantages of SVMs:</p>
<ul class="calibre14">
<li class="calibre15">Can be sensitive to the choice of kernel function, which can greatly affect the performance of the model</li>
<li class="calibre15">Computationally intensive for large datasets</li>
<li class="calibre15">It can be difficult to interpret the resul<a id="_idTextAnchor078" class="calibre5 pcalibre1 pcalibre"/>ts of an SVM model</li>
<li class="calibre15">Requires careful tuning of parameters to achieve good performance</li>
</ul>
<h2 id="_idParaDest-58" class="calibre7"><a id="_idTextAnchor079" class="calibre5 pcalibre1 pcalibre"/>Neural networks and transformers</h2>
<p class="calibre6">Neural networks and transformers are both powerful machine learning models that are used for a variety of tasks, such as image classification, NLP, and speech recognition.</p>
<h3 class="calibre8">Neural networks</h3>
<p class="calibre6">Neural networks draw<a id="_idIndexMarker237" class="calibre5 pcalibre1 pcalibre"/> inspiration from the structure and functioning of the human brain. They represent a category of machine learning models that are proficient in various tasks such as classification, regression, and more. Comprising multiple layers of interconnected <a id="_idIndexMarker238" class="calibre5 pcalibre1 pcalibre"/>nodes known as neurons, these networks adeptly process and manipulate data. The output of each layer is fed into the next layer, creating a hierarchy of feature representations. The input to the first layer is the raw data, and the output of the final layer is the prediction. A simple neural network for detecting the gender of a person based on their height and weight is shown in <em class="italic">Figure 3</em><em class="italic">.3</em>:</p>
<div><div><img alt="Figure 3.3 – Simple neural network" src="img/B18949_03_3.jpg" class="calibre3"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.3 – Simple neural network</p>
<p class="calibre6">The operation of a single neuron<a id="_idIndexMarker239" class="calibre5 pcalibre1 pcalibre"/> in a neural network can be represented by the following equation:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/munderover&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/201.png" class="calibre204"/></p>
<p class="calibre6">Here, <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/202.png" class="calibre205"/> is the input values, <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/203.png" class="calibre206"/> is the weights of the connections between the neurons, <em class="italic">b</em> is the bias term, and <em class="italic">f</em> is the activation function. The activation function applies a non-linear transformation to the weighted sum of the inputs and bias term.</p>
<p class="calibre6">Training a neural network involves adjusting the weights and biases of the neurons to minimize a loss function. This is typically done using an optimization algorithm such as stochastic gradient descent.</p>
<p class="calibre6">The advantages of neural networks include their ability to learn complex non-linear relationships between input and output data, their ability to automatically extract meaningful features from raw data, and their scalability to large datasets.</p>
<p class="calibre6">The disadvantages of neural networks include their high computational and memory requirements, their sensitivity to hyperparameter tuning, and the difficulty of interpreting their internal representations.</p>
<h3 class="calibre8">Transformers</h3>
<p class="calibre6">Transformers are a type of <a id="_idIndexMarker240" class="calibre5 pcalibre1 pcalibre"/>neural network architecture that is particularly well suited to sequential data such as text or speech. They were introduced in the context of NLP and have since been applied to a wide range of tasks.</p>
<p class="calibre6">The core component of a transformer is the self-attention mechanism, which allows the model to attend to different parts of the input sequence when computing the output. The self-attention mechanism is based on a dot product between a query vector, a set of key vectors, and a set of value vectors. The resulting attention weights are used to weight the values, which are then combined to produce the output.</p>
<p class="calibre6">The self-attention operation can be represented by the following equations:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/204.png" class="calibre207"/></p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/205.png" class="calibre208"/></p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/206.png" class="calibre209"/></p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;msup&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;msqrt&gt;&lt;msub&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;/msub&gt;&lt;/msqrt&gt;&lt;/mfrac&gt;&lt;/mfenced&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/207.png" class="calibre210"/></p>
<p class="calibre6">Here, <em class="italic">X</em> is the input sequence, <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;Q&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/208.png" class="calibre211"/>, <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;K&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/209.png" class="calibre212"/>, and <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;V&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/210.png" class="calibre213"/> are learned projection matrices for the query, key, and value vectors, respectively, <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;K&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/211.png" class="calibre214"/> is the dimensionality of the key vectors, and <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;Q&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/212.png" class="calibre215"/> is a learned projection matrix that maps the output of the attention mechanism to the final output.</p>
<p class="calibre6">The advantages of transformers include their ability to handle variable-length input sequences, their ability to capture long-range dependencies in the data, and their state-of-the-art performance on many NLP tasks.</p>
<p class="calibre6">The disadvantages of transformers include their high computational and memory requirements, their sensitivity to hyperparameter tuning, and their difficulty in handling tasks that require explicit modeling of sequential dynamics.</p>
<p class="calibre6">These are just a few of the most popular machine learning models. The choice of model depends on the problem at hand, the size and quality of the data, and the desired outcome. Now that <a id="_idIndexMarker241" class="calibre5 pcalibre1 pcalibre"/>we have explored the most common machine learning models, we wil<a id="_idTextAnchor080" class="calibre5 pcalibre1 pcalibre"/>l explain model underfitting and overfitting, which happens during the training process.</p>
<h1 id="_idParaDest-59" class="calibre4"><a id="_idTextAnchor081" class="calibre5 pcalibre1 pcalibre"/>Model underfitting and overfitting</h1>
<p class="calibre6">In machine learning, the ultimate<a id="_idIndexMarker242" class="calibre5 pcalibre1 pcalibre"/> goal is to build a model that can generalize well on unseen data. However, sometimes, a model can fail to achieve this goal due to either underfitting or overfitting.</p>
<p class="calibre6">Underfitting occurs when a model is<a id="_idIndexMarker243" class="calibre5 pcalibre1 pcalibre"/> too simple to capture the underlying patterns in the data. In other words, the model can’t learn the relationship between the features and the target variable properly. This can result in poor performance on both the training and testing data. For example, in <em class="italic">Figure 3</em><em class="italic">.4</em>, we can see that the model is underfitted, and it cannot present the data very well. This is not what we like in machine learning models, and we usually like to see a precise model, as shown in <em class="italic">Figure 3</em><em class="italic">.5</em>:</p>
<div><div><img alt="Figure 3.4 – The machine learning model underfitting on the training data" src="img/B18949_03_4.jpg" class="calibre3"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.4 – The machine learning model underfitting on the training data</p>
<p class="calibre6">Underfitting happens when the model is<a id="_idIndexMarker244" class="calibre5 pcalibre1 pcalibre"/> not trained well, or the model complexity is not<a id="_idIndexMarker245" class="calibre5 pcalibre1 pcalibre"/> enough to catch the underlying pattern in the data. To solve this problem, we can use more complex models, and continue the training process:</p>
<p class="calibre6"> </p>
<div><div><img alt="Figure 3.5 – Optimal fitting of the machine learning model on the training data" src="img/B18949_03_5.jpg" class="calibre3"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.5 – Optimal fitting of the machine learning model on the training data</p>
<p class="calibre6">Optimal fitting happens when the<a id="_idIndexMarker246" class="calibre5 pcalibre1 pcalibre"/> model captures the pattern in the data pretty well but <a id="_idIndexMarker247" class="calibre5 pcalibre1 pcalibre"/>does not overfit every single sample. This helps the model work better on unseen data:</p>
<div><div><img alt="Figure 3.6 – Overfitting the model on the training data" src="img/B18949_03_6.jpg" class="calibre3"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.6 – Overfitting the model on the training data</p>
<p class="calibre6">On the other hand, overfitting <a id="_idIndexMarker248" class="calibre5 pcalibre1 pcalibre"/>occurs when a model is too complex and fits the training data too closely, which can lead to poor generalization on new, unseen data, as <a id="_idIndexMarker249" class="calibre5 pcalibre1 pcalibre"/>shown in <em class="italic">Figure 3</em><em class="italic">.6</em>. This happens when the model learns the noise or random fluctuations in the training data, rather than the underlying patterns. In other words, the model becomes too specialized for the training data and does not perform well on the testing data. As shown in the preceding figure, the model is overfitted, and the model tried to predict every single sample very precisely. The problem with this model is that it does not learn the general pattern, and learns the pattern of each individual sample, which makes it work poorly when facing new, unseen records.</p>
<p class="calibre6">A useful way to understand the<a id="_idIndexMarker250" class="calibre5 pcalibre1 pcalibre"/> trade-off between underfitting and overfitting is through the bias-variance<a id="_idIndexMarker251" class="calibre5 pcalibre1 pcalibre"/> trade-off. Bias refers to the difference between the predicted values of the model and the actual values in the training data. A high bias means that the model is not complex enough to capture the underlying patterns in the data and underfits the data (<em class="italic">Figure 3</em><em class="italic">.7</em>). An underfit model has poor performance on both the training and testing data:</p>
<div><div><img alt="Figure 3.7 – High bias" src="img/B18949_03_7.jpg" class="calibre3"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.7 – High bias</p>
<p class="calibre6">Variance, on the other hand, refers to<a id="_idIndexMarker252" class="calibre5 pcalibre1 pcalibre"/> the sensitivity of the model to small fluctuations<a id="_idIndexMarker253" class="calibre5 pcalibre1 pcalibre"/> in the training data. A high variance means that the model is overly complex and overfits the data, which leads to poor generalization performance on new data. An overfit model has good performance on the training data but poor performance on the testing data:</p>
<div><div><img alt="Figure 3.8 – Just right (not high bias, not high variance)" src="img/B18949_03_8.jpg" class="calibre3"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.8 – Just right (not high bias, not high variance)</p>
<p class="calibre6">To strike a balance between bias<a id="_idIndexMarker254" class="calibre5 pcalibre1 pcalibre"/> and variance, we need to choose a model that is <a id="_idIndexMarker255" class="calibre5 pcalibre1 pcalibre"/>neither too simple nor too complex. As mentioned previously, this is often referred to as the bias-variance trade-off (<em class="italic">Figure 3</em><em class="italic">.8</em>). A model with a high bias and low variance can be improved by increasing the complexity of the model, while a model with a high variance and low bias can be improved by decreasing the complexity of the model:</p>
<div><div><img alt="Figure 3.9 – High variance" src="img/B18949_03_9.jpg" class="calibre3"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.9 – High variance</p>
<p class="calibre6">There are several methods to reduce bias and variance in a model. One common approach is regularization, which<a id="_idIndexMarker256" class="calibre5 pcalibre1 pcalibre"/> adds a penalty term to the loss function to control the complexity of the model. Another approach is to use ensembles, which combine<a id="_idIndexMarker257" class="calibre5 pcalibre1 pcalibre"/> multiple models to improve the overall performance by reducing the variance. Cross-validation can also be used to evaluate the model’s performance and tune its hyperparameters to find the optimal balance between bias and variance.</p>
<p class="calibre6">Overall, understanding bias and variance is crucial in machine learning as it helps us to choose an appropriate model and identify the sources of error in the model.</p>
<p class="calibre6">Bias refers to the error that is introduced by approximating a real-world problem with a simplified model. Variance, on the other hand, refers to the error that is introduced by the model’s sensitivity to small fluctuations in the training data.</p>
<p class="calibre6">When a model has high bias<a id="_idIndexMarker258" class="calibre5 pcalibre1 pcalibre"/> and low variance, it is underfitting. This means that the model is not capturing the <a id="_idIndexMarker259" class="calibre5 pcalibre1 pcalibre"/>complexity of the problem and is making overly simplistic assumptions. When a model has low bias and high variance, it is overfitting. This means that the model is too sensitive to the training data and is fitting the noise instead of the underlying patterns.</p>
<p class="calibre6">To overcome underfitting, we can try increasing the complexity of the model, adding more features, or using a more sophisticated algorithm. To prevent overfitting, several methods can be used:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Cross-validation</strong>: Assessing the <a id="_idIndexMarker260" class="calibre5 pcalibre1 pcalibre"/>performance of machine learning models is essential. Cross-validation serves as a method for assessing the effectiveness of a machine learning model. It entails training the model on one portion of the data and testing it on another. By employing distinct subsets for training and evaluation, cross-validation mitigates the risk of overfitting. Further elaboration on this technique will be provided in the subsequent section on data splitting.</li>
<li class="calibre15"><strong class="bold">Regularization</strong>: Regularization is <a id="_idIndexMarker261" class="calibre5 pcalibre1 pcalibre"/>a technique that’s used to add a penalty term to the loss function during training, which helps to reduce the complexity of the model and prevent overfitting. There are different types of regularization, including L1 regularization (LASSO), L2 regularization (ridge), and elastic net regularization.</li>
<li class="calibre15"><strong class="bold">Early stopping</strong>: Early stopping<a id="_idIndexMarker262" class="calibre5 pcalibre1 pcalibre"/> is a technique that’s used to stop the training process when the performance of the model on the validation data starts to degrade. This helps to prevent overfitting by stopping the model from continuing to learn from the training data when it has already reached its maximum performance. This technique is usually used in iterative algorithms such as deep learning methods, where the model is being trained for multiple iterations (epochs). To use early stopping, we usually train the model while evaluating the model performance on the training and validation subsets. The model’s performance usually improves on the training set with more<a id="_idIndexMarker263" class="calibre5 pcalibre1 pcalibre"/> training, but since the model has not seen the validation<a id="_idIndexMarker264" class="calibre5 pcalibre1 pcalibre"/> set, the validation error usually decreases initially and at some point, starts increasing again. This point is where the model starts overfitting. By visualizing the training and validation error of the model during training, we can identify and stop the model at this point (<em class="italic">Figure 3</em><em class="italic">.10</em>):</li>
</ul>
<div><div><img alt="Figure 3.10 – Early stopping" src="img/B18949_03_10.jpg" class="calibre3"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.10 – Early stopping</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Dropout</strong>: Dropout is a technique in deep learning models that is used to randomly drop out some neurons during training, which helps to prevent the model from relying too heavily <a id="_idIndexMarker265" class="calibre5 pcalibre1 pcalibre"/>on a small subset of features or neurons and overfitting the training data. By dropping the weight of neurons in the model during the process, we make the model learn the general pattern and prevent it from memorizing the<a id="_idIndexMarker266" class="calibre5 pcalibre1 pcalibre"/> training data (overfitting).</li>
<li class="calibre15"><strong class="bold">Data augmentation</strong>: Data augmentation<a id="_idIndexMarker267" class="calibre5 pcalibre1 pcalibre"/> is a method that we can use to artificially expand the training data size by applying transformations, such as rotation, scaling, and flipping, to the existing dataset, which helps us to extend our training data. This strategy aids in mitigating overfitting by offering the model a more diverse set of examples to learn from.</li>
<li class="calibre15"><strong class="bold">Ensemble methods</strong>: Ensemble methods are techniques that are used to combine multiple models to<a id="_idIndexMarker268" class="calibre5 pcalibre1 pcalibre"/> improve their performance and prevent overfitting. This can be done by using techniques such as bagging, boosting, or stacking.</li>
</ul>
<p class="calibre6">By using these techniques, it is<a id="_idIndexMarker269" class="calibre5 pcalibre1 pcalibre"/> possible to prevent overfitting and build models that generalize well to new, unseen data. In practice, it is important to monitor both the training and testing performance of the model and make adjustments accordingly to achieve the best possible gen<a id="_idTextAnchor082" class="calibre5 pcalibre1 pcalibre"/>eralization performance. We will explain how to split our data into training and testing in the next section.</p>
<h1 id="_idParaDest-60" class="calibre4"><a id="_idTextAnchor083" class="calibre5 pcalibre1 pcalibre"/>Splitting data</h1>
<p class="calibre6">When developing a <a id="_idIndexMarker270" class="calibre5 pcalibre1 pcalibre"/>machine learning model, it’s important to split the data into training, validation, and test sets; this is called data splitting. This is done to evaluate the performance of the model on new, unseen data and to prevent overfitting.</p>
<p class="calibre6">The most common method for splitting the data is the train-test split, which splits the data into two sets: the training set, which is used to train the model, and the test set, which is used to evaluate the performance of the model. The data is randomly divided into two sets, with a typical split being 80% of the data for training and 20% for testing. Using this approach the model will be trained using the majority of the data (training data) and then tested on the remaining data (test set). Using this approach, we can ensure that the model’s performance is based on new, unseen data.</p>
<p class="calibre6">Most of the time in machine learning model development, we have a set of hyperparameters for <a id="_idIndexMarker271" class="calibre5 pcalibre1 pcalibre"/>our model that we like to tune (we will explain hyperparameter tuning in the next subsection). In this case, we like to make sure that the performance that we get on the test set is reliable and not just by chance based on a set of hyperparameters. In this case, based on the size of our training data, we can divide the data into 60%, 20%, and 20% (or 70%, 15%, and 15%) for training, validation, and testing. In this case, we train the model on the training data and select the set of hyperparameters that give us the best performance on the validation set. We then report the actual model performance on the test set, which has not been seen or used before during model training or hyperparameter selection.</p>
<p class="calibre6">A more advanced method for splitting the data, especially when the size of our training data is limited, is k-fold cross-validation. In this method, the data is split into <em class="italic">k</em> equally sized “folds,” and the model is trained and tested <em class="italic">k</em> times, with each fold being used as the test set once and the remaining folds used as the training set. The results of each fold are then averaged to get an overall measure of the model’s performance. K-fold cross-validation is useful for small datasets where the train-test split may result in a large variance in performance evaluation. In this case, we report the average, minimum, and maximum performance of the model on each of the <em class="italic">k</em> folds, as shown in <em class="italic">Figure 3</em><em class="italic">.11</em>.</p>
<div><div><img alt="Figure 3.11 – K-fold cross-validation" src="img/B18949_03_11.jpg" class="calibre3"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.11 – K-fold cross-validation</p>
<p class="calibre6">Another variant of k-fold cross-validation is stratified k-fold cross-validation, which ensures that the distribution of the target variable is consistent across all folds. This is useful when dealing with imbalanced datasets, where the number of instances of one class is much smaller than the others.</p>
<p class="calibre6">Time series data<a id="_idIndexMarker272" class="calibre5 pcalibre1 pcalibre"/> requires special attention when splitting. In this case, we typically use a method called time <a id="_idIndexMarker273" class="calibre5 pcalibre1 pcalibre"/>series cross-validation, which preserves the temporal order of the data. In this method, the data is split into multiple segments, with each segment representing a fixed time interval. The model is then trained on the past data and tested on the future data. This helps to evaluate the performance of the model in real-world scenarios. You can see an example of how to split the data in a time series problem in <em class="italic">Figure 3</em><em class="italic">.12</em>:</p>
<div><div><img alt="Figure 3.12 – Time series data splitting" src="img/B18949_03_12.jpg" class="calibre3"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.12 – Time series data splitting</p>
<p class="calibre6">In all cases, it’s important to<a id="_idIndexMarker274" class="calibre5 pcalibre1 pcalibre"/> ensure that the split is done randomly but with the same random seed each time to ensure the reproducibility of the results. It’s also important to ensure that the split is representative of the underlying data – that is, the distribution of the target variable should be consistent across all sets. Once we have split the data into different subsets for training and testing our model, we can try to find the best<a id="_idTextAnchor084" class="calibre5 pcalibre1 pcalibre"/> set of hyperparameters for our model. This process is called hyperparameter tuning and will be explained next.</p>
<h1 id="_idParaDest-61" class="calibre4"><a id="_idTextAnchor085" class="calibre5 pcalibre1 pcalibre"/>Hyperparameter tuning</h1>
<p class="calibre6">Hyperparameter tuning is an important step in the machine learning process that involves selecting the best set of <a id="_idIndexMarker275" class="calibre5 pcalibre1 pcalibre"/>hyperparameters for a given model. Hyperparameters are values that are set before the training process begins and can have a significant impact on the model’s performance. Examples of hyperparameters include learning rate, regularization strength, number of hidden layers in a neural network, and many others.</p>
<p class="calibre6">The process of hyperparameter tuning involves selecting the best combination of hyperparameters that results in the optimal performance of the model. This is typically done by searching through a predefined set of hyperparameters and evaluating their performance on a validation set.</p>
<p class="calibre6">There are several methods for hyperparameter tuning, including grid search, random search, and Bayesian optimization. Grid search involves creating a grid of all possible hyperparameter combinations and evaluating each one on a validation set to determine the optimal <a id="_idIndexMarker276" class="calibre5 pcalibre1 pcalibre"/>set of hyperparameters. Random search, on the other hand, randomly samples hyperparameters from a predefined distribution and evaluates their performance on a validation set.</p>
<p class="calibre6"><strong class="bold">Random search</strong> and <strong class="bold">grid search</strong> are <a id="_idIndexMarker277" class="calibre5 pcalibre1 pcalibre"/>methods that are used to search the search space, entirely or randomly, without <a id="_idIndexMarker278" class="calibre5 pcalibre1 pcalibre"/>considering previous hyperparameter results. Thus, these methods are inefficient. An alternative Bayesian optimization method has been proposed that iteratively computes the posterior distribution of the function and considers past evaluations to find the best hyperparameters. Using this approach, we can find the best set of hyperparameters with less iterations.</p>
<p class="calibre6">Bayesian optimization utilizes past evaluations to probabilistically map hyperparameters to objective function scores, as demonstrated in the following equation:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/213.png" class="calibre216"/></p>
<p class="calibre6">Here are the steps Bayesian optimization undertakes:</p>
<ol class="calibre16">
<li class="calibre15">It develops a surrogate probabilistic model for the objective function.</li>
<li class="calibre15">It identifies the optimal hyperparameters based on the surrogate.</li>
<li class="calibre15">It utilizes these hyperparameters in the actual objective function.</li>
<li class="calibre15">It updates the surrogate model to integrate the latest results.</li>
<li class="calibre15">It reiterates <em class="italic">steps 2</em> to <em class="italic">4</em> until it reaches the maximum iteration count or time limit.</li>
</ol>
<p class="calibre6"><strong class="bold">Sequential model-based optimization</strong> (<strong class="bold">SMBO</strong>) methods are a formalization of Bayesian optimization, with trials run<a id="_idIndexMarker279" class="calibre5 pcalibre1 pcalibre"/> one after another, trying better hyperparameters each time and updating a probability model (surrogate). SMBO methods differ in <em class="italic">steps 3</em> and <em class="italic">4</em> – specifically, how they build a surrogate of the objective function and the criteria used to select the next hyperparameters. These variants include Gaussian processes, random forest regressions, and tree-structured Parzen estimators, among others.</p>
<p class="calibre6">In low-dimensional problems with numerical hyperparameters, Bayesian optimization is considered the best available hyperparameter optimization method. However, it is restricted to problems of moderate dimension.</p>
<p class="calibre6">In addition to these methods, there are also several libraries available that automate the process of hyperparameter tuning. Examples of these libraries include scikit-learn’s <code>GridSearchCV</code> and <code>RandomizedSearchCV</code>, <code>Keras Tuner</code>, and <code>Optuna</code>. These libraries allow for efficient hyperparameter tuning and can significantly improve the performance of machine learning models.</p>
<p class="calibre6">Hyperparameter optimization in machine learning can be a complex and time-consuming process. Two primary complexity challenges arise in the search process: the trial execution time and the complexity of the search space, including the number of evaluated hyperparameter combinations. In deep learning, these challenges are especially pertinent due to the extensive search space and the utilization of large training sets.</p>
<p class="calibre6">To address these issues and reduce the search space, some standard techniques may be used. For example, reducing the size of the training dataset based on statistical sampling or applying feature selection techniques can help reduce the execution time of each trial. Additionally, identifying the most important hyperparameters for optimization and using additional objective functions beyond just accuracy, such as the number of operations or optimization time, can help reduce the complexity of the search space.</p>
<p class="calibre6">By combining accuracy with visualization through a deconvolution network, researchers have achieved superior results. However, it’s important to note that these techniques are not exhaustive, and the best approach may depend on the specific problem at hand.</p>
<p class="calibre6">Another common approach<a id="_idIndexMarker280" class="calibre5 pcalibre1 pcalibre"/> for improving model performance is to use multiple<a id="_idTextAnchor086" class="calibre5 pcalibre1 pcalibre"/> models in parallel; these are called ensemble models. They are very useful in dealing with machine learning problems.</p>
</div>


<div><h1 id="_idParaDest-62" class="calibre4"><a id="_idTextAnchor087" class="calibre5 pcalibre1 pcalibre"/>Ensemble models</h1>
<p class="calibre6">Ensemble modeling is a<a id="_idIndexMarker281" class="calibre5 pcalibre1 pcalibre"/> technique in machine learning that combines the predictions of multiple models to improve overall performance. The idea behind ensemble models is that multiple models can be better than a single model as different models may capture differen<a id="_idTextAnchor088" class="calibre5 pcalibre1 pcalibre"/>t patterns in the data.</p>
<p class="calibre6">There are several types of ensemble models, all of which we’ll cover in the following sections.</p>
<h2 id="_idParaDest-63" class="calibre7"><a id="_idTextAnchor089" class="calibre5 pcalibre1 pcalibre"/>Bagging</h2>
<p class="calibre6"><strong class="bold">Bootstrap aggregating</strong>, also known as <strong class="bold">bagging</strong>, is an ensemble method that combines multiple independent <a id="_idIndexMarker282" class="calibre5 pcalibre1 pcalibre"/>models trained on different subsets of the training <a id="_idIndexMarker283" class="calibre5 pcalibre1 pcalibre"/>data to reduce variance and improve model generalization.</p>
<p class="calibre6">The bagging algorithm can be summarized as follows:</p>
<ol class="calibre16">
<li class="calibre15">Given a training dataset of size <em class="italic">n</em>, create <em class="italic">m</em> bootstrap samples of size <em class="italic">n</em> (that is, sample <em class="italic">n</em> instances with replacement <em class="italic">m</em> times).</li>
<li class="calibre15">Train a base model (for example, a decision tree) on each bootstrap sample independently.</li>
<li class="calibre15">Aggregate the predictions of all base models to obtain the ensemble prediction. This can be done by either taking the majority vote (in the case of classification) or the average (in the case of regression).</li>
</ol>
<p class="calibre6">The bagging algorithm is particularly effective when the base models are unstable (that is, have high variance), such as decision trees, and when the training dataset is small.</p>
<p class="calibre6">The equation for aggregating the predictions of the base models depends on the type of problem (classification or regression). For classification, the ensemble prediction is obtained by taking the majority vote:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;Y&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/msub&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/munderover&gt;&lt;mrow&gt;&lt;mi&gt;I&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/214.png" class="calibre217"/></p>
<p class="calibre6">Here, <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/215.png" class="calibre218"/> is the predicted class of the <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/216.png" class="calibre219"/> base model for the <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/217.png" class="calibre220"/> instance and <em class="italic">I()</em> is the indicator function (equal to 1 if x is true, and 0 otherwise).</p>
<p class="calibre6">For regression, the ensemble prediction is obtained by taking the average score:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;Y&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/munderover&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/218.png" class="calibre221"/></p>
<p class="calibre6">Here, <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/193.png" class="calibre196"/> is the predicted value of the <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/220.png" class="calibre222"/> base model.</p>
<p class="calibre6">The advantages of bagging are <a id="_idIndexMarker284" class="calibre5 pcalibre1 pcalibre"/>as follows:</p>
<ul class="calibre14">
<li class="calibre15">Improved model generalization by reducing variance and overfitting</li>
<li class="calibre15">Ability to handle high-dimensional datasets with complex relationships</li>
<li class="calibre15">Can be used with a variety of base models</li>
</ul>
<p class="calibre6">The disadvantages of bagging are as follows:</p>
<ul class="calibre14">
<li class="calibre15">Increased model complexity and computation time due to the use of multiple base models</li>
<li class="calibre15">It can sometimes lead to overfitting if the base mod<a id="_idTextAnchor090" class="calibre5 pcalibre1 pcalibre"/>els are too complex or the dataset is too small</li>
<li class="calibre15">It does not work well when the base models are highly correlated or biased</li>
</ul>
<h2 id="_idParaDest-64" class="calibre7"><a id="_idTextAnchor091" class="calibre5 pcalibre1 pcalibre"/>Boosting</h2>
<p class="calibre6">Boosting is another popular <a id="_idIndexMarker285" class="calibre5 pcalibre1 pcalibre"/>ensemble learning technique that aims to improve the performance of weak classifiers by combining them into a stronger classifier. Unlike bagging, boosting<a id="_idIndexMarker286" class="calibre5 pcalibre1 pcalibre"/> focuses on iteratively improving the accuracy of the classifier by adjusting the weights of the training examples. The basic idea behind boosting is to learn from the mistakes of the previous weak classifiers and to put more emphasis on the examples that were incorrectly classified in the previous iteration.</p>
<p class="calibre6">There are several boosting algorithms, but one of the most popular ones is AdaBoost (short for adaptive<a id="_idIndexMarker287" class="calibre5 pcalibre1 pcalibre"/> boosting). The AdaBoost algorithm works as follows:</p>
<ol class="calibre16">
<li class="calibre15">First, it initializes<a id="_idIndexMarker288" class="calibre5 pcalibre1 pcalibre"/> the weights of<a id="_idIndexMarker289" class="calibre5 pcalibre1 pcalibre"/> the training examples to be equal.</li>
<li class="calibre15">Then, it trains a weak classifier on the training set.</li>
<li class="calibre15">Next, it computes the weighted error rate of the weak classifier.</li>
<li class="calibre15">After, it computes the importance of the weak classifier based on its weighted error rate.</li>
<li class="calibre15">Then, it increases the weights of the examples that were misclassified by the weak classifier.</li>
<li class="calibre15">Once it’s done this, it normalizes the weights of the examples so that they sum up to one.</li>
<li class="calibre15">It repeats <em class="italic">steps 2</em> to <em class="italic">6</em> for a predetermined number of iterations or until the desired accuracy is achieved.</li>
<li class="calibre15">Finally, it combines the weak classifiers into a strong classifier by assigning weights to them based on their importance.</li>
</ol>
<p class="calibre6">The final classifier is a weighted combination of the weak classifiers. The importance of each weak classifier is determined by its weighted error rate, which is computed as an equation:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/msubsup&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mi&gt;I&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/msubsup&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/221.png" class="calibre223"/></p>
<p class="calibre6">Here, <em class="italic">m</em> is the index of the weak classifier, <em class="italic">N</em> is the number of training examples, <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/222.png" class="calibre224"/> is the weight of the <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/220.png" class="calibre225"/> training example, <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/196.png" class="calibre226"/> is the true label of the <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/220.png" class="calibre225"/> training example, <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:math&gt;" src="img/226.png" class="calibre227"/> is the prediction of the <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/227.png" class="calibre228"/> weak classifier for the <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/220.png" class="calibre222"/> training example, and <img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;I&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mfenced&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/229.png" class="calibre229"/> is an indicator function that returns 1 if the prediction of the weak classifier is incorrect and 0 otherwise.</p>
<p class="calibre6">The importance of the weak classifier is computed by the following equation:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;α&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;ln&lt;/mi&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;/mrow&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;/mrow&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/msub&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/230.png" class="calibre230"/></p>
<p class="calibre6">The weights of the examples are updated based on their importance:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;α&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/msub&gt;&lt;mi&gt;I&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/231.png" class="calibre231"/></p>
<p class="calibre6">The final classifier is then obtained by combining the weak classifiers:</p>
<p class="calibre6"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;H&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;M&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;α&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="img/232.png" class="calibre232"/></p>
<p class="calibre6">Here, <em class="italic">M</em> is the total number of weak <a id="_idIndexMarker290" class="calibre5 pcalibre1 pcalibre"/>classifiers, <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:math&gt;" src="img/233.png" class="calibre233"/> is the prediction of the <em class="italic">m-th</em> weak classifier, and <code>sign()</code> is a function that returns +1 if its argument is positive and -1 otherwise.</p>
<p class="calibre6">Let’s look at some of the <a id="_idIndexMarker291" class="calibre5 pcalibre1 pcalibre"/>advantages of boosting:</p>
<ul class="calibre14">
<li class="calibre15">Boosting can improve the accuracy of weak classifiers and can lead to a significant improvement in performance</li>
<li class="calibre15">Boosting is relatively easy to implement and can be applied to a wide range of classification problems</li>
<li class="calibre15">Boosting can handle noisy data and reduce the risk of overfitting</li>
</ul>
<p class="calibre6">Here are some of the disadvantages of boosting:</p>
<ul class="calibre14">
<li class="calibre15">Boosting can be sensitive to outliers and can overfit to noisy data</li>
<li class="calibre15">Boosting can be computationally expensive, espe<a id="_idTextAnchor092" class="calibre5 pcalibre1 pcalibre"/>cially when dealing with large datasets</li>
<li class="calibre15">Boosting can be <a id="_idIndexMarker292" class="calibre5 pcalibre1 pcalibre"/>difficult to interpret as it involves combining multiple weak classifiers</li>
</ul>
<h2 id="_idParaDest-65" class="calibre7"><a id="_idTextAnchor093" class="calibre5 pcalibre1 pcalibre"/>Stacking</h2>
<p class="calibre6">Stacking is another<a id="_idIndexMarker293" class="calibre5 pcalibre1 pcalibre"/> popular ensemble learning technique that combines the<a id="_idIndexMarker294" class="calibre5 pcalibre1 pcalibre"/> predictions of multiple base models by training a higher-level model on their predictions. The idea behind stacking is to leverage the strengths of different base models to achieve better predictive performance.</p>
<p class="calibre6">Here’s how stacking works:</p>
<ol class="calibre16">
<li class="calibre15">Divide the training data into two parts: the first part is used to train the base models, while the second part is used to create a new dataset of predictions from the base models.</li>
<li class="calibre15">Train multiple base models on the first part of the training data.</li>
<li class="calibre15">Use the trained base models to make predictions on the second part of the training data to create a new dataset of predictions.</li>
<li class="calibre15">Train a higher-level model (also known as a metamodel or blender) on the new dataset of predictions.</li>
<li class="calibre15">Use the trained higher-level model to make predictions on the test data.</li>
</ol>
<p class="calibre6">The higher-level model is typically a simple model such as a linear regression, logistic regression, or a decision tree. The idea is to use the predictions of the base models as input features for the higher-l<a id="_idTextAnchor094" class="calibre5 pcalibre1 pcalibre"/>evel model. This way, the higher-level model learns to combine the predictions of the base models to make more accurate predictions.</p>
<h2 id="_idParaDest-66" class="calibre7"><a id="_idTextAnchor095" class="calibre5 pcalibre1 pcalibre"/>Random forests</h2>
<p class="calibre6">One of the most <a id="_idIndexMarker295" class="calibre5 pcalibre1 pcalibre"/>commonly known ensemble models is random forest, where the model combines the predictions of multiple decision<a id="_idIndexMarker296" class="calibre5 pcalibre1 pcalibre"/> trees and ou<a id="_idTextAnchor096" class="calibre5 pcalibre1 pcalibre"/>tputs the predictions. This is usually more accurate and prone to overfitting. We elaborated on Random Forest earlier in this chapter.</p>
<h2 id="_idParaDest-67" class="calibre7"><a id="_idTextAnchor097" class="calibre5 pcalibre1 pcalibre"/>Gradient boosting</h2>
<p class="calibre6">Gradient boosting is another<a id="_idIndexMarker297" class="calibre5 pcalibre1 pcalibre"/> ensemble model that can be used for classification <a id="_idIndexMarker298" class="calibre5 pcalibre1 pcalibre"/>and regression tasks. It works by getting a weak classifier (such as a simple tree), and in each step tries to improve this weak classifier to build a better model. The main idea here is that the model tries to focus on its mistakes in each step and improve itself by fitting the model by correcting the errors made in previous trees.</p>
<p class="calibre6">During each iteration, the algorithm computes the negative gradient of the loss function concerning the predicted values, followed by fitting a decision tree to these negative gradient values. The predictions of the new tree are then combined with the predictions of the previous trees, using a learning rate parameter that controls the contribution of each tree to the final prediction.</p>
<p class="calibre6">The overall prediction of the gradient boosting model is obtained by summing up the predictions of all the trees, which are weighted by their respective learning rates.</p>
<p class="calibre6">Let’s take a look at the equation for the gradient boosting algorithm.</p>
<p class="calibre6">First, we initialize the model with a constant value:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;/msub&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/munderover&gt;&lt;mrow&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/234.png" class="calibre234"/></p>
<p class="calibre6">Here, <em class="italic">c</em> is a constant, <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/196.png" class="calibre235"/> is the true label of the <em class="italic">i-th</em> sample, <em class="italic">N</em> is the number of samples, and <em class="italic">L</em> is the loss function, which is used to measure the error between the predicted and true labels.</p>
<p class="calibre6">At each iteration, <em class="italic">m</em>, the algorithm fits a decision tree to the negative gradient values of the loss function concerning the predicted values, <img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mo&gt;∇&lt;/mo&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/236.png" class="calibre236"/>. The decision tree predicts the <a id="_idIndexMarker299" class="calibre5 pcalibre1 pcalibre"/>negative gradient values, which are then<a id="_idIndexMarker300" class="calibre5 pcalibre1 pcalibre"/> used to update the predictions of the model via the following equation:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;η&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;*&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/237.png" class="calibre237"/></p>
<p class="calibre6">Here, <img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/238.png" class="calibre238"/> is the prediction of the model at the previous iteration, <em class="italic">η</em> is the learning rate, and <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:math&gt;" src="img/239.png" class="calibre239"/> is the prediction of the decision tree at the current iteration.</p>
<p class="calibre6">The final prediction of the model is obtained by combining the predictions of all the trees:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;/munderover&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;η&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/msub&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;*&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/240.png" class="calibre240"/></p>
<p class="calibre6">Here, <em class="italic">M</em> is the total number of trees in the model and <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;η&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/241.png" class="calibre241"/> and <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:math&gt;" src="img/242.png" class="calibre242"/> are the learning rate and prediction of the <em class="italic">m-th</em> tree, respectively.</p>
<p class="calibre6">Let’s look at some of the advantages of gradient boosting:</p>
<ul class="calibre14">
<li class="calibre15">High prediction accuracy</li>
<li class="calibre15">Handles both regression and classification problems</li>
<li class="calibre15">Can handle missing data and outliers</li>
<li class="calibre15">Can be used with various loss functions</li>
<li class="calibre15">Can handle high-dimensional data</li>
</ul>
<p class="calibre6">Now, let’s look at some of the disadvantages:</p>
<ul class="calibre14">
<li class="calibre15">Sensitive to overfitting, especially when the number of trees is large</li>
<li class="calibre15">Computationally expensive and time-consuming to train, especially for la<a id="_idTextAnchor098" class="calibre5 pcalibre1 pcalibre"/>rge datasets</li>
<li class="calibre15">Requires careful tuning of hyperparameters, such as the number of trees, the learning rate, and the maximum depth of the trees</li>
</ul>
<p class="calibre6">With that, we have reviewed <a id="_idIndexMarker301" class="calibre5 pcalibre1 pcalibre"/>the ensemble models that can<a id="_idIndexMarker302" class="calibre5 pcalibre1 pcalibre"/> help us improve our model performance. However, sometimes, our dataset has some features that we need to consider before we apply machine learning models. One common case is when we have an imbalanced dataset.</p>
<h1 id="_idParaDest-68" class="calibre4"><a id="_idTextAnchor099" class="calibre5 pcalibre1 pcalibre"/>Handling imbalanced data</h1>
<p class="calibre6">In most real-world problems, our<a id="_idIndexMarker303" class="calibre5 pcalibre1 pcalibre"/> data is imbalanced, which means that the distribution of records from different classes (such as patients with and without cancer) is different. Handling imbalanced datasets is an important task in machine learning as it is common to have datasets with uneven class distribution. In such cases, the minority class is often under-represented, which can cause poor model performance and biased predictions. The reason behind this is that machine learning methods are trying to optimize their fitness function to minimize the error in the training set. Now, let’s say that we have 99% of the data from the positive class and 1% from the negative class. In this case, if the model predicts all records as positive, the error will be 1%; however, this model is not useful for us. That’s why, if we have an imbalanced dataset, we need to use various methods to handle imbalanced data. In general, we can have three categories of methods to handle imbalanced datasets:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Undersampling</strong>: A very simple method that comes to mind is to use fewer training records from the <a id="_idIndexMarker304" class="calibre5 pcalibre1 pcalibre"/>majority class. This method works, but we need to consider that by using less training data, we are feeding less information to the model causes to have a less robust training and final model.</li>
<li class="calibre15"><strong class="bold">Resampling</strong>: Resampling<a id="_idIndexMarker305" class="calibre5 pcalibre1 pcalibre"/> methods involve modifying the original dataset to create a balanced distribution. This can be achieved by either oversampling the minority class (creating more samples of the minority class) or undersampling the majority class (removing samples from the majority class). Oversampling techniques include <strong class="bold">random oversampling</strong>, <strong class="bold">Synthetic Minority Oversampling Technique</strong> (<strong class="bold">SMOTE</strong>), and <strong class="bold">Adaptive Synthetic Sampling</strong> (<strong class="bold">ADASYN</strong>). Undersampling techniques include <strong class="bold">random undersampling</strong>, <strong class="bold">Tomek links</strong>, and <strong class="bold">cluster cen<a id="_idTextAnchor100" class="calibre5 pcalibre1 pcalibre"/>troids</strong>.</li>
<li class="calibre15"><strong class="bold">Handling imbalanced datasets in machine learning models</strong>: Such as modifying cost<a id="_idIndexMarker306" class="calibre5 pcalibre1 pcalibre"/> function, or modified batching in deep learning models.</li>
</ul>
<h2 id="_idParaDest-69" class="calibre7"><a id="_idTextAnchor101" class="calibre5 pcalibre1 pcalibre"/>SMOTE</h2>
<p class="calibre6">SMOTE is a widely used <a id="_idIndexMarker307" class="calibre5 pcalibre1 pcalibre"/>algorithm for handling imbalanced datasets in machine learning. It is a synthetic data generation technique that creates new, synthetic samples in the minority class by interpolating between existing samples. SMOTE works by identifying the k-nearest neighbors of a minority <a id="_idIndexMarker308" class="calibre5 pcalibre1 pcalibre"/>class sample and then generating new samples along the line segments that connect these neighbors.</p>
<p class="calibre6">Here are the steps of the SMOTE algorithm:</p>
<ol class="calibre16">
<li class="calibre15">Select a minority class sample, <em class="italic">x</em>.</li>
<li class="calibre15">Choose one of its k-nearest neighbors, <em class="italic">x’</em>.</li>
<li class="calibre15">Generate a synthetic sample by interpolating between <em class="italic">x</em> and <em class="italic">x’</em>. To do this, choose a random number, <em class="italic">r</em>, between 0 and 1, and then calculate the synthetic sample, as follows:</li>
</ol>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;*&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;′&lt;/mo&gt;&lt;/mrow&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/243.png" class="calibre243"/></p>
<p class="calibre6">This creates a new sample that is somewhere between <em class="italic">x</em> and <em class="italic">x’</em>, but not the same as either one.</p>
<p class="calibre6">4.	Repeat <em class="italic">steps 1</em> to <em class="italic">3</em> until the desired number of synthetic samples has been generated.</p>
<p class="calibre6">Here are the advantages<a id="_idIndexMarker309" class="calibre5 pcalibre1 pcalibre"/> and disadvantages of SMOTE:</p>
<ul class="calibre14">
<li class="calibre15">It helps to address the problem of class imbalance by creating synthetic samples in the minority class.</li>
<li class="calibre15">SMOTE can be combined with other techniques, such as random undersampling or Tomek links, to further improve the balance of the dataset.</li>
<li class="calibre15">SMOTE can be applied to both categorical and numerical data.</li>
<li class="calibre15">SMOTE can sometimes create synthetic samples that are unrealistic or noisy, leading to overfitting.</li>
<li class="calibre15">SMOTE can sometimes cause the decision boundary to be too sensitive to the minority class, leading to poor performance of the majority class.</li>
<li class="calibre15">SMOTE can be computationally expensive for large datasets.</li>
</ul>
<p class="calibre6">Here is an example of<a id="_idIndexMarker310" class="calibre5 pcalibre1 pcalibre"/> SMOTE in action. Suppose we have a dataset with two classes: the majority class (class 0) has 900 samples, and the minority class (class 1) has 100 samples. We want to use SMOTE to generate synthetic samples for the minority class:</p>
<ol class="calibre16">
<li class="calibre15">We select a minority class sample, <em class="italic">x</em>.</li>
<li class="calibre15">We choose one of its k-nearest neighbors, <em class="italic">x’</em>.</li>
<li class="calibre15">We generate a synthetic sample by interpolating between <em class="italic">x</em> and <em class="italic">x’</em> using a random number, <em class="italic">r</em>:</li>
</ol>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;*&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;′&lt;/mo&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/244.png" class="calibre244"/></p>
<p class="calibre6">For example, suppose <em class="italic">x</em> is (<em class="italic">1, 2</em>), <em class="italic">x’</em> is (<em class="italic">3, 4</em>), and <em class="italic">r</em> is <em class="italic">0.5</em>. In this case, the new sample is as follows:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;0.5&lt;/mn&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;*&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mn&gt;4&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/245.png" class="calibre245"/></p>
<p class="calibre6">4.	We repeat <em class="italic">steps 1</em> to <em class="italic">3</em> until we have generated the desired number of synthetic samples. For example, suppose we want to generate 100 synthetic samples. We repeat <em class="italic">steps 1</em> to <em class="italic">3</em> for each of the 100 minority class <a id="_idTextAnchor102" class="calibre5 pcalibre1 pcalibre"/>samples and then<a id="_idIndexMarker311" class="calibre5 pcalibre1 pcalibre"/> combine the original minority class samples with the synthetic samples to create a balanced dataset with 200 samples in each class.</p>
<h2 id="_idParaDest-70" class="calibre7"><a id="_idTextAnchor103" class="calibre5 pcalibre1 pcalibre"/>The NearMiss algorithm</h2>
<p class="calibre6">The <code>NearMiss</code> algorithm is a<a id="_idIndexMarker312" class="calibre5 pcalibre1 pcalibre"/> technique for balancing class distribution by undersampling (removing) the records from the major class. When two <a id="_idIndexMarker313" class="calibre5 pcalibre1 pcalibre"/>classes have records that are very close to each other, eliminating some of the records from the majority class increases the distance between the two classes, which helps the classification process. To avoid information loss problems in the majority of undersampling methods, near-miss methods are widely used.</p>
<p class="calibre6">The working of nearest-neighbor methods is based on the following steps:</p>
<ol class="calibre16">
<li class="calibre15">Find the distances between all records from the major class and minor class. Our goal is to undersample the records from the major class.</li>
<li class="calibre15">Choose <em class="italic">n</em> records from the major class that are closest to the minor class.</li>
<li class="calibre15">If there are <em class="italic">k</em> records in the minor class, the nearest method will return <em class="italic">k*n</em> records from the major class.</li>
</ol>
<p class="calibre6">There are three variations of applying the <code>NearMiss</code> algorithm that we can use to find the <em class="italic">n</em> closest records in the major class:</p>
<ul class="calibre14">
<li class="calibre15">We can select the records of the major class for which the average distances to the k-closest records of the minor class are the smallest.</li>
<li class="calibre15">We can select the records of the major class for which the average distances to the k-farthest records of the minor class are the smallest.</li>
<li class="calibre15">We can implement two <a id="_idIndexMarker314" class="calibre5 pcalibre1 pcalibre"/>steps. In the first step, for each record from the minor class, their <em class="italic">M</em> nearest<a id="_idTextAnchor104" class="calibre5 pcalibre1 pcalibre"/> neighbors will be stored. Then, the records from the major class are selected <a id="_idIndexMarker315" class="calibre5 pcalibre1 pcalibre"/>such that the average distance to the <em class="italic">N</em> nearest neighbors is the largest.</li>
</ul>
<h2 id="_idParaDest-71" class="calibre7"><a id="_idTextAnchor105" class="calibre5 pcalibre1 pcalibre"/>Cost-sensitive learning</h2>
<p class="calibre6">Cost-sensitive learning is a method that’s used to train machine learning models on imbalanced datasets. In<a id="_idIndexMarker316" class="calibre5 pcalibre1 pcalibre"/> imbalanced datasets, the number of examples in one class (usually the minority class) is much lower than in the other class (usually the majority class). Cost-sensitive learning involves assigning misclassification costs <a id="_idIndexMarker317" class="calibre5 pcalibre1 pcalibre"/>to the model that differ based on the class being predicted, which can help the model focus more on correctly classifying the minority class.</p>
<p class="calibre6">Let’s assume we have a binary classification problem with two classes, positive and negative. In cost-sensitive learning, we assign different costs to different types of errors. For example, we may assign a higher cost to misclassifying a positive example as negative because in an imbalanced dataset, the positive class is the minority class, and misclassifying positive examples can have a greater impact on the performance of the model.</p>
<p class="calibre6">We can assign costs in the form of a confusion matrix:</p>
<table class="no-table-style" id="table002">
<colgroup class="calibre10">
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
</colgroup>
<thead class="calibre12">
<tr class="no-table-style1">
<td class="no-table-style2"/>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold">Predicted Positive</strong></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold">Predicted Negative</strong></p>
</td>
</tr>
</thead>
<tbody class="calibre13">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><strong class="bold">Actual Positive</strong></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><code>TP_cost</code></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><code>FN_cost</code></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><strong class="bold">Actual Negative</strong></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><code>FP_cost</code></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><code>TN_cost</code></p>
</td>
</tr>
</tbody>
</table>
<p class="img---caption" lang="en-US" xml:lang="en-US">Table 3.2 – Confusion matrix costs</p>
<p class="calibre6">Here, <code>TP_cost</code>, <code>FN_cost</code>, <code>FP_cost</code>, and <code>TN_cost</code> are the costs associated with true positives, false negatives, false positives, and true negatives, respectively.</p>
<p class="calibre6">To incorporate the cost matrix into<a id="_idIndexMarker318" class="calibre5 pcalibre1 pcalibre"/> the training process, we can modify the standard loss function that the <a id="_idIndexMarker319" class="calibre5 pcalibre1 pcalibre"/>model optimizes during training. One common cost-sensitive loss function is the weighted cross-entropy loss, which is defined as follows:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;*&lt;/mi&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;*&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mover&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;*&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;*&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mover&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/246.png" class="calibre246"/></p>
<p class="calibre6">Here, <em class="italic">y</em> is the true label (either 0 or 1), <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:math&gt;" src="img/247.png" class="calibre247"/> is the predicted probability of the positive class, and <img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/248.png" class="calibre248"/>and <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/249.png" class="calibre249"/> are weights that are assigned to the positive and negative classes, respectively.</p>
<p class="calibre6">The weights, <img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/250.png" class="calibre250"/>and <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/251.png" class="calibre251"/>, can be determined by the costs assigned in the confusion matrix. For example, if we assign a higher cost to false negatives (that is, misclassifying a positive example as negative), we may set <img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/252.png" class="calibre252"/>to a higher value than <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/249.png" class="calibre249"/>.</p>
<p class="calibre6">Cost-sensitive learning can also be used with other types of models, such as decision trees and SVMs. The concept of assigning costs to different types of errors can be applied in various ways to improve the performance of a model on imbalanced datasets. However, it’s important to carefully select the appropriate cost matrix and loss function based on the specific characteristics of the dataset and the problem being solved:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Ensemble techniques</strong>: Ensemble techniques combine multiple models to improve predictive performance. In<a id="_idIndexMarker320" class="calibre5 pcalibre1 pcalibre"/> imbalanced datasets, an ensemble of models can be trained on different subsets of the dataset, ensuring that each model is trained on both the minority and majority classes. Examples of ensemble techniques for imbalanced datasets include bagging and boosting.</li>
<li class="calibre15"><strong class="bold">Anomaly detection</strong>: Anomaly detection techniques can be used to identify the minority class as an anomaly in<a id="_idIndexMarker321" class="calibre5 pcalibre1 pcalibre"/> the dataset. These techniques aim to identify rare events that are significantly different from the majority class. The identified <a id="_idIndexMarker322" class="calibre5 pcalibre1 pcalibre"/>samples can then be used to train the model on<a id="_idIndexMarker323" class="calibre5 pcalibre1 pcalibre"/> the minority c<a id="_idTextAnchor106" class="calibre5 pcalibre1 pcalibre"/>lass.</li>
</ul>
<h2 id="_idParaDest-72" class="calibre7"><a id="_idTextAnchor107" class="calibre5 pcalibre1 pcalibre"/>Data augmentation</h2>
<p class="calibre6">The idea behind data <a id="_idIndexMarker324" class="calibre5 pcalibre1 pcalibre"/>augmentation is to generate new examples by applying transformations to the original ones, while still retaining the label. These transformations can include rotation, translation, scaling, flipping, and adding noise, among others. This can be particularly useful for imbalanced datasets, where the number of examples in one class is much smaller than in the other.</p>
<p class="calibre6">In the context of imbalanced datasets, data augmentation can be used to create new examples of the minority class, effectively balancing the dataset. This can be done by applying the same set of transformations to the minority class examples, creating a new set of examples that are still representative of the minority class but are slightly different from the original ones.</p>
<p class="calibre6">The equations that are involved in data augmentation are relatively simple as they are based on applying transformation functions to the original examples. For example, to rotate an image by a certain angle, we can use a rotation matrix:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;′&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/254.png" class="calibre253"/></p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;′&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/255.png" class="calibre254"/></p>
<p class="calibre6">Here, <em class="italic">x</em> and <em class="italic">y</em> are the original coordinates of a pixel in the image, <em class="italic">x’</em> and <em class="italic">y’</em> are the new coordinates after rotation, and <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="img/256.png" class="calibre255"/> is the angle of rotation.</p>
<p class="calibre6">Similarly, to apply translation, we can simply shift the image by a certain number of pixels:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;′&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/257.png" class="calibre256"/></p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;′&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/258.png" class="calibre257"/></p>
<p class="calibre6">Here, <em class="italic">dx</em> and <em class="italic">dy</em> are the horizontal and vertical shifts, respectively.</p>
<p class="calibre6">Data augmentation can be a powerful technique for addressing imbalanced datasets as it can create new examples that are representative of the minority class, while still<a id="_idIndexMarker325" class="calibre5 pcalibre1 pcalibre"/> preserving the label information. However, it is important to be careful when applying data augmentation as it can also introduce noise and artifacts in the data, and can lead to overfitting if not done properly.</p>
<p class="calibre6">In conclusion, handling imbalanced datasets is an important aspect of machine learning. There are several techniques available to handle imbalanced datasets, each with its advantages and disadvantages. The choice of technique depends on the dataset, the problem, and the available resources. Besides having imbalanced data, in the case of working on time series data, we might face correlated data. We’ll take a closer look at this<a id="_idTextAnchor108" class="calibre5 pcalibre1 pcalibre"/> next.</p>
<h1 id="_idParaDest-73" class="calibre4"><a id="_idTextAnchor109" class="calibre5 pcalibre1 pcalibre"/>Dealing with correlated data</h1>
<p class="calibre6">Dealing with correlated time series data in machine learning models can be challenging as traditional techniques <a id="_idIndexMarker326" class="calibre5 pcalibre1 pcalibre"/>such as random sampling can introduce biases and overlook dependencies between data points. Here are some approaches that can help:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Time series cross-validation</strong>: Time series data is often dependent on past values and it’s important to preserve this relationship during model training and evaluation. Time series cross-validation involves splitting the data into multiple folds, with each fold consisting of a continuous block of time. This approach ensures that the model is trained on past data and evaluated on future data, which better simulates how the model will perform in real-world scenarios.</li>
<li class="calibre15"><strong class="bold">Feature engineering</strong>: Correlated time series data can be difficult to model with traditional machine learning algorithms. Feature engineering can help transform the data into a more suitable format. Examples of feature engineering for time series data include creating lags or differences in the time series, aggregating data into time buckets or windows, and creating rolling statistics such as moving averages.</li>
<li class="calibre15"><strong class="bold">Time series-specific models</strong>: There are several models specifically designed for time series data, such as <strong class="bold">AutoRegressive Integrated Moving Average</strong> (<strong class="bold">ARIMA</strong>), <strong class="bold">Seasonal ARIMA</strong> (<strong class="bold">SARIMA</strong>), <strong class="bold">Prophet</strong>, and <strong class="bold">Long Short-Term Memory</strong> (<strong class="bold">LSTM</strong>) networks. These models are designed to capture the dependencies and patterns in time series data and may outperform traditional machine learning models.</li>
<li class="calibre15"><strong class="bold">Time series preprocessing techniques</strong>: Time series data can be preprocessed to remove <a id="_idIndexMarker327" class="calibre5 pcalibre1 pcalibre"/>correlations and make the data more suitable for machine learning models. Techniques such as differencing, detrending, and normalization can help remove trends and seasonal components from the data, which can help reduce correlations.</li>
<li class="calibre15"><strong class="bold">Dimensionality reduction techniques</strong>: Correlated time series data can have a high dimensionality, which can make modeling difficult. Dimensionality reduction techniques such as PCA or autoencoders can help reduce the number of variables in the data while preserving the most important information.</li>
</ul>
<p class="calibre6">In general, it’s important to approach time series data with techniques that preserve the temporal dependencies and patterns in the data. This can require specialized modeling techniques and preprocessi<a id="_idTextAnchor110" class="calibre5 pcalibre1 pcalibre"/>ng steps.</p>
<h1 id="_idParaDest-74" class="calibre4"><a id="_idTextAnchor111" class="calibre5 pcalibre1 pcalibre"/>Summary</h1>
<p class="calibre6">In this chapter, we learned about various concepts related to machine learning, starting with data exploration and preprocessing techniques. We then explored various machine learning models, such as logistic regression, decision trees, support vector machines, and random forests, along with their strengths and weaknesses. We also discussed the importance of splitting data into training and test sets, as well as techniques for handling imbalanced datasets.</p>
<p class="calibre6">The chapter also covered the concepts of model bias, variance, underfitting, and overfitting, and how to diagnose and address these issues. We also explored ensemble methods such as bagging, boosting, and stacking, which can improve model performance by combining the predictions of multiple models.</p>
<p class="calibre6">Finally, we learned about the limitations and challenges of machine learning, including the need for large amounts of high-quality data, the risk of bias and unfairness, and the difficulty of interpreting complex models. Despite these challenges, machine learning offers powerful tools for solving a wide range of problems and has the potential to transform many industries and fields.</p>
<p class="calibre6">In the next chapter, we will discuss text preprocessing, which is required for text to be used by machine learning models.</p>
<h1 id="_idParaDest-75" class="calibre4"><a id="_idTextAnchor112" class="calibre5 pcalibre1 pcalibre"/>References</h1>
<ul class="calibre14">
<li class="calibre15">Shahriari, B., Swersky, K., Wang, Z., Adams, R.P., de Freitas, N.: <em class="italic">Taking the human out of the loop: A review of Bayesian optimization. Proceedings of the IEEE 104(1), 148–175 (2016). </em><em class="italic">DOI 10.1109/JPROC.2015.2494218</em>.</li>
</ul>
</div>
</body></html>