<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div id="_idContainer054">&#13;
    <h1 class="chapterNumber"><a id="_idTextAnchor023"/>4</h1>&#13;
    <h1 id="_idParaDest-56" class="chapterTitle">Building Your Neo4j Graph with Movies Dataset</h1>&#13;
    <p class="normal">In the previous chapters, we learned how knowledge graphs have emerged as a transformative tool, offering a structured way to connect diverse data points, enabling smarter search, recommendations, and inference capabilities across a wide range of domains.</p>&#13;
    <p class="normal">Knowledge graphs excel at capturing complex relationships between entities, making them indispensable for applications that require deep contextual understanding.</p>&#13;
    <p class="normal">Neo4j, with its state-of-the-art graph database technology, stands out as a leading platform for building and managing knowledge graphs. As we saw in the previous chapter, unlike traditional relational databases, Neo4j is designed to handle highly connected data with ease, allowing for more intuitive querying and faster retrieval of insights. This makes it an ideal choice for developers and data scientists looking to transform raw, unstructured data into meaningful insights that can drive AI-powered applications.</p>&#13;
    <p class="normal">In this chapter, we are going to cover the following main topics:</p>&#13;
    <ul>&#13;
      <li class="bulletList">Design considerations for a Neo4j graph for an efficient search</li>&#13;
      <li class="bulletList">Utilizing a movies dataset</li>&#13;
      <li class="bulletList">Building your movie knowledge graph with code examples</li>&#13;
      <li class="bulletList">Beyond the basics: advanced Cypher techniques for complex graph structures</li>&#13;
    </ul>&#13;
    <h1 id="_idParaDest-57" class="heading-1">Technical requirements</h1>&#13;
    <p class="normal">To successfully work through the exercises in this chapter, you will need the following tools:</p>&#13;
    <ul>&#13;
      <li class="bulletList"><strong class="screenText">Neo4j AuraDB</strong>: You can use Neo4j AuraDB, the cloud version of Neo4j, available at <a href="https://neo4j.com/aura"><span class="url">https://neo4j.com/aura</span></a>.</li>&#13;
      <li class="bulletList"><strong class="screenText">Cypher query language</strong>: Familiarity with the Cypher query language is essential, as we will be using Cypher extensively to create and query the graph. You can find out more about Cypher syntax in the Cypher query language documentation: <a href="https://neo4j.com/docs/cypher/"><span class="url">https://neo4j.com/docs/cypher/</span></a>.</li>&#13;
      <li class="bulletList"><strong class="screenText">Python</strong>: You will need Python 3.x installed on your system. Python is used for scripting and interacting with the Neo4j database. You can download Python from the official Python website: <a href="https://www.python.org/downloads/"><span class="url">https://www.python.org/downloads/</span></a>.</li>&#13;
      <li class="bulletList"><strong class="screenText">Python libraries</strong>:<ul>&#13;
          <li class="bulletList"><strong class="screenText">Neo4j Driver for Python</strong>: Install the Neo4j Python driver to connect to the Neo4j database using Python. You can install it via <code class="inlineCode">pip</code>:&#13;
            <pre class="programlisting code"><code class="hljs-code">pip install neo4j&#13;
</code></pre>&#13;
          </li>&#13;
          <li class="bulletList"><strong class="screenText">pandas</strong>: This library will be used for data manipulation and analysis. Install it via <code class="inlineCode">pip</code>:&#13;
            <pre class="programlisting code"><code class="hljs-code">pip install pandas&#13;
</code></pre>&#13;
          </li>&#13;
        </ul>&#13;
      </li>&#13;
      <li class="bulletList"><strong class="screenText">Integrated Development Environment (IDE)</strong>: An IDE such as PyCharm, VS Code, or Jupyter Notebook is recommended for writing and managing your Python code efficiently.</li>&#13;
      <li class="bulletList"><strong class="screenText">Git and GitHub</strong>: Basic knowledge of Git is required for version control. You will also need a GitHub account to access the code repository for this chapter.</li>&#13;
      <li class="bulletList"><strong class="screenText">Movies dataset</strong>: <strong class="keyWord">The Movie Database</strong> (<strong class="keyWord">TMDb</strong>) is required, available on Kaggle: <a href="https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset/"><span class="url">https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset/</span></a>.</li>&#13;
      <li class="bulletList">This dataset is a derivative of the <strong class="keyWord">Movie Lens Datasets</strong> (F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4: 19:1–19:19.<a href="https://doi.org/10.1145/2827872"><span class="url">https://doi.org/10.1145/2827872</span></a>).</li>&#13;
      <li class="bulletList">Some of the data files, such as <code class="inlineCode">credits.csv</code> and <code class="inlineCode">ratings.csv</code>, may not be available on GitHub due to storage constraints. However, you can access all the raw data files from a GCS bucket.</li>&#13;
    </ul>&#13;
    <p class="normal">All the code for this chapter is available in the following GitHub repository: <a href="https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/tree/main/ch4"><span class="url">https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/tree/main/ch4</span></a>.</p>&#13;
    <p class="normal">The folder includes all the necessary files and scripts to help you build your Neo4j graph using the <a id="_idTextAnchor024"/>movies dataset and Cypher code.</p>&#13;
    <p class="normal">Make sure to clone or download the repository to follow along with the code examples provided in this chapter.</p>&#13;
    <p class="normal">The GitHub repository contains the path to GCS to access the raw data files.</p>&#13;
    <h1 id="_idParaDest-58" class="heading-1">Design considerations for a Neo4j graph for an efficient search</h1>&#13;
    <p class="normal">A <a id="_idIndexMarker183"/>well-designed Neo4j graph ensures that your search functionality is not only accurate but also efficient, enabling the quick retrieval of relevant information. The way data is organized in a graph directly impacts the performance and relevance of search results, making it crucial to understand the principles of effective graph modeling.</p>&#13;
    <p class="normal">This section will delve into the importance of structuring your Neo4j graph correctly, how it influences the search process, and the key considerations you need to keep in mind while designing your graph model.</p>&#13;
    <h2 id="_idParaDest-59" class="heading-2">Considerations while defining node and relationship types</h2>&#13;
    <p class="normal">Recall <a id="_idIndexMarker184"/>from <a href="Preface.xhtml#_idTextAnchor012"><em class="italic">Chapter 3</em></a> that the foundation of any Neo4j graph is built upon <strong class="keyWord">nodes</strong> and <strong class="keyWord">relationships</strong>. Nodes <a id="_idIndexMarker185"/>represent entities, such as movies or people (e.g., actors or directors), while relationships define how these entities are connected. The types of nodes and relationships you choose play a crucial role in determining the effectiveness of your search queries.</p>&#13;
    <p class="normal">In<a id="_idTextAnchor025"/> a movies dataset, nodes could traditionally represent distinct entities such as <code class="inlineCode">Movies</code>, <code class="inlineCode">Actors</code>, <code class="inlineCode">Directors</code>, and <code class="inlineCode">Genres</code>. Relationships would then define how these nodes interact, such as <code class="inlineCode">ACTED_IN</code>, <code class="inlineCode">DIRECTED</code>, or <code class="inlineCode">BELONGS_TO</code>. However, there is an alternative and often more efficient approach—consolidating similar entities under a single node type.</p>&#13;
    <p class="normal">Instead of creating separate nodes for <code class="inlineCode">Actors</code> and <code class="inlineCode">Directors</code>, you can create a single <code class="inlineCode">Person</code> node. The characteristic of each <code class="inlineCode">Person</code> node—whether they are an actor, a director, or both—is then defined by the type of relationship it has with the <code class="inlineCode">Movie</code> node. For example, a <code class="inlineCode">Person</code> node connected to a <code class="inlineCode">Movie</code> node by an <code class="inlineCode">ACTED_IN</code> relationship signifies that the person is an actor in that movie. Similarly, a <code class="inlineCode">DIRECTED</code> relationship <a id="_idIndexMarker186"/>indicates that the person directed the movie. We will be creating the complete graph in upcoming sections.</p>&#13;
    <p class="normal">But first, let’s talk about why this approach is better. As we demonstrated in <a href="Preface.xhtml#_idTextAnchor012"><em class="italic">Chapter 3</em></a>, this approach results in the following:</p>&#13;
    <ul>&#13;
      <li class="bulletList"><strong class="screenText">Simplified data model</strong>: By using a single <code class="inlineCode">Person</code> node to represent both actors and directors, your data model becomes more streamlined. This reduces the complexity of the graph and makes it easier to understand and manage.</li>&#13;
      <li class="bulletList"><strong class="screenText">Enhanced query performance</strong>: With fewer node types, the graph database can more efficiently traverse relationships during queries. This is because the database engine has fewer distinct entities to differentiate between, leading to faster query execution times.</li>&#13;
      <li class="bulletList"><strong class="screenText">Reduced redundancy</strong>: A unified <code class="inlineCode">Person</code> node eliminates the need to duplicate information. In cases where a person is both an actor and a director, you avoid creating two separate nodes with overlapping data, thus minimizing redundancy, and saving storage space.</li>&#13;
      <li class="bulletList"><strong class="screenText">Flexible relationship definitions</strong>: This approach allows for more flexible and granular relationship definitions. If a person has multiple roles in different movies (e.g., acting in one and directing another), the relationships can clearly distinguish these roles without needing to create multiple nodes.</li>&#13;
      <li class="bulletList"><strong class="screenText">Easier maintenance and scalability</strong>: As your dataset grows, maintaining a simpler node structure becomes increasingly important. Adding new roles or relationships becomes more<a id="_idIndexMarker187"/> straightforward when you are working with a unified node type.</li>&#13;
    </ul>&#13;
    <p class="normal">By carefully selecting and defining these types and relationships, you create a graph structure that mirrors real-world connections. This makes your search queries more intuitive, the results more meaningful, and the overall system more efficient.</p>&#13;
    <h2 id="_idParaDest-60" class="heading-2">Applying indexing and constraints on search performance</h2>&#13;
    <p class="normal">As your Neo4j graph grows, the importance of <strong class="keyWord">indexing</strong> and applying <strong class="keyWord">constraints</strong> becomes paramount. <strong class="keyWord">Indexes</strong> allow <a id="_idIndexMarker188"/>Neo4j to quickly locate the <a id="_idIndexMarker189"/>starting points for queries, drastically improving search performance, especially in large datasets. Constraints, however, ensure data integrity by preventing the creation of duplicate nodes or invalid relationships.</p>&#13;
    <p class="normal">In the context <a id="_idTextAnchor026"/>of our movies dataset, where we use a unified <code class="inlineCode">Person</code> node for both actors and directors, indexing becomes even more crucial. You might <strong class="keyWord">index</strong> nodes based on properties such as <code class="inlineCode">person_name</code> or <code class="inlineCode">role</code>, ensuring that searches for specific people or their roles in movies return results swiftly. For example, you could <a id="_idIndexMarker190"/>index the role property on the relationships (e.g., <code class="inlineCode">ACTED_IN</code> or <code class="inlineCode">DIRECTED</code>) to quickly filter people by their involvement in a particular movie.</p>&#13;
    <p class="normal">Constraints are also <a id="_idIndexMarker191"/>essential to maintaining the integrity of your graph. Let’s look at some of these constraints. The constraints should be carefully designed based on the nature of your dataset and application requirements—they are not a one-size-fits-all solution.</p>&#13;
    <p class="normal">The following are some example statements that demonstrate how to create constraints and indexes tailored for a movie dataset. These examples include common scenarios such as ensuring the uniqueness of person identifiers and optimizing search performance across node and relationship properties. Depending on your specific use case and data quality, you can adapt these patterns to enforce data integrity and improve query speed:</p>&#13;
    <ul>&#13;
      <li class="bulletList">Unique <a id="_idIndexMarker192"/>constraint on <code class="inlineCode">person_name</code> (for a simplified use case). In many cases—such as our movie dataset, where we assume each person has a unique name—you might enforce a unique constraint on the <code class="inlineCode">person_name</code> property to ensure that each individual is represented by a single node, even if they take on multiple roles (e.g., actor and director) across different movies. Here is how you can do this:&#13;
        <pre class="programlisting code"><code class="hljs-code">CREATE CONSTRAINT unique_person_name IF NOT EXISTS&#13;
FOR (p:Person)&#13;
REQUIRE p.person_name IS UNIQUE;&#13;
</code></pre>&#13;
      </li>&#13;
    </ul>&#13;
    <p class="normal">This helps prevent the accidental creation of duplicate nodes and keeps your graph clean and efficient.</p>&#13;
    <ul>&#13;
      <li class="bulletList">Unique <a id="_idIndexMarker193"/>constraint on a more reliable ID (e.g., <code class="inlineCode">person_id</code>). The uniqueness constraint in the previous scenarios is based on assumptions about your data. In real-world scenarios, it is common to encounter different individuals with the same name.</li>&#13;
    </ul>&#13;
    <p class="normal">In such cases, you should use a more reliable identifier, such as a <code class="inlineCode">person_id</code> value from an external source (e.g., <strong class="screenText">Internet Movie Database</strong> (<strong class="screenText">IMDb</strong>) or <strong class="screenText">TMDb</strong>) to enforce uniqueness. The following Cypher code shows how to achieve this:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code">CREATE CONSTRAINT unique_person_id IF NOT EXISTS&#13;
FOR (p:Person)&#13;
REQUIRE p.person_id IS UNIQUE;&#13;
</code></pre>&#13;
    <ul>&#13;
      <li class="bulletList">Index<a id="_idIndexMarker194"/> on <code class="inlineCode">person_name</code> (for faster lookup if uniqueness is not enforced). If you’re not enforcing uniqueness but still frequently search for people by name, an index on the <code class="inlineCode">person_name</code> property can significantly improve query performance. This allows Neo4j to quickly locate <code class="inlineCode">Person</code> nodes based on their names:&#13;
        <pre class="programlisting code"><code class="hljs-code">CREATE INDEX person_name_index IF NOT EXISTS&#13;
FOR (p:Person)&#13;
ON (p.person_name);&#13;
</code></pre>&#13;
      </li>&#13;
      <li class="bulletList">Index on the <code class="inlineCode">title</code> property of <code class="inlineCode">Movie</code>. Movies are often queried by title—especially in recommendation systems or search functionalities. Indexing the <code class="inlineCode">title</code> property <a id="_idIndexMarker195"/>ensures quick lookups when users search for specific movies:&#13;
        <pre class="programlisting code"><code class="hljs-code">CREATE INDEX movie_title_index IF NOT EXISTS&#13;
FOR (m:Movie)&#13;
ON (m.title);&#13;
</code></pre>&#13;
      </li>&#13;
      <li class="bulletList">Index on<a id="_idIndexMarker196"/> the <code class="inlineCode">role</code> property in the <code class="inlineCode">ACTED_IN</code> relationship. If your application requires filtering actors by their specific roles in movies (e.g., lead or cameo), indexing the <code class="inlineCode">role</code> property on the <code class="inlineCode">ACTED_IN</code> relationship helps speed up those queries by avoiding full scans of all relationships:&#13;
        <pre class="programlisting code"><code class="hljs-code">CREATE INDEX acted_in_role_index IF NOT EXISTS&#13;
FOR (<a id="_idTextAnchor027"/>)-[r:ACTED_IN]-()&#13;
ON (r.role);&#13;
</code></pre>&#13;
        <div class="note">&#13;
          <p class="normal"><strong class="keyWord">Note</strong></p>&#13;
          <p class="normal">Neo4j only supports relationship property indexes in version <code class="inlineCode">5.x</code> and above.</p>&#13;
        </div>&#13;
      </li>&#13;
    </ul>&#13;
    <p class="normal">Properly <a id="_idIndexMarker197"/>implemented indexing and constraints make your graph more resilient and your search processes faster and more reliable. This not only enhances the user experience but also reduces the computational load on your system, allowing for more scalable solutions.</p>&#13;
    <p class="normal">In the next section, we will explore how to harness the power of open data <a id="_idTextAnchor028"/>by utilizing a movies dataset to build your graph.</p>&#13;
    <h1 id="_idParaDest-61" class="heading-1">Utilizing a movies dataset</h1>&#13;
    <p class="normal">In this section, we <a id="_idIndexMarker198"/>will focus on utilizing <strong class="screenText">TMDb</strong>, a comprehensive collection of metadata made available on Kaggle: <a href="https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset/"><span class="url">https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset/</span></a>. This dataset includes a wide range of information about movies, such as titles, genres, cast, crew, release dates, and ratings. With over 45,000 movies and detailed information about the people involved in their<a id="_idTextAnchor029"/> creation, this dataset provides a robust foundation for building a Neo4j graph that captures the complex relationships within the film industry.</p>&#13;
    <p class="normal">You will use this dataset to model the data as a knowledge graph, learning about data integration in a practical context. You will learn how to source, prepare, and import this data into Neo4j.</p>&#13;
    <p class="normal">When working with <a id="_idIndexMarker199"/>large datasets such as TMDb, it is crucial to ensure that the data is clean, consistent, and properly structured before integrating it into your Neo4j graph. Raw data, while rich in information, often contains inconsistencies, redundancies, and complex structures that can hinder the performance and accuracy of your knowledge graph. This is where data normalization and cleaning come into play.</p>&#13;
    <h2 id="_idParaDest-62" class="heading-2">Why normalize and clean data?</h2>&#13;
    <p class="normal">Maintaining a<a id="_idIndexMarker200"/> clean and normalized dataset is crucial when building a Neo4j graph, as it <a id="_idIndexMarker201"/>directly impacts the quality and performance of your application. By normalizing and cleaning your data, you ensure consistency, improve efficiency, and create a scalable foundation for analysis. Here is<a id="_idTextAnchor030"/> why each of these steps matters:</p>&#13;
    <ul>&#13;
      <li class="bulletList"><strong class="screenText">Consistency</strong>: Raw data can come with variations in how similar information is recorded. For example, movie genres might be listed in different formats or contain duplicates. Normalizing data ensures that similar data points are recorded in a consistent format, making it easier to query and analyze. However, tackling these issues in real-world datasets can be challenging. Neo4j helps address problems such as entity linkage and deduplication through powerful features such as Cypher pattern matching, APOC procedures for merging nodes and cleaning up duplicates, and the Graph Data Science library, which includes node similarity algorithms to identify and consolidate related entities. These capabilities enable you to build a clean, reliable graph that reflects the true structure of your data.</li>&#13;
      <li class="bulletList"><strong class="screenText">Efficiency</strong>: Normalizing data reduces redundancy, which can improve the efficiency of your Neo4j graph. By organizing data into a standardized format, you minimize the storage requirements and optimize the performance of your queries.</li>&#13;
      <li class="bulletList"><strong class="screenText">Accuracy</strong>: Cleaning data involves removing or correcting inaccurate records. This step is essential to ensure that the insights derived from your graph are based on accurate and reliable data.</li>&#13;
      <li class="bulletList"><strong class="screenText">Scalability</strong>: A <a id="_idIndexMarker202"/>clean and normalized dataset is easier to scale. As <a id="_idIndexMarker203"/>your dataset grows, maintaining a standardized structure ensures that the graph remains manageable and performs well under increasing loads.</li>&#13;
    </ul>&#13;
    <p class="normal">Let us move on to cleaning and normalizing CSV files next.</p>&#13;
    <h2 id="_idParaDest-63" class="heading-2">Cleaning and normalizing the CSV files</h2>&#13;
    <p class="normal">Now, we will clean and normalize each <a id="_idIndexMarker204"/>CSV file included in TMDb. The available CSV files in our dataset are as follows:</p>&#13;
    <ul>&#13;
      <li class="bulletList"><code class="inlineCode">credits.csv</code>: This file contains<a id="_idIndexMarker205"/> detailed information about the cast and crew for each movie in our dataset, presented as a stringified JSON object. For our purposes, we will focus specifically on extracting the relevant details related to characters, actors, directors, and producers:&#13;
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Load the CSV file</span>&#13;
df = pd.read_csv(<span class="hljs-string">'./raw_data/credits.csv'</span>)&#13;
<span class="hljs-comment"># Function to extract relevant cast information</span>&#13;
<span class="hljs-keyword">def</span> <span class="hljs-title">extract_cast</span>(<span class="hljs-params">cast_str</span>):&#13;
    cast_list = ast.literal_eval(cast_str)&#13;
    <span class="hljs-keyword">return</span> [&#13;
        {&#13;
            <span class="hljs-string">'</span><span class="hljs-string">actor_id'</span>: c[<span class="hljs-string">'id'</span>],&#13;
            <span class="hljs-string">'name'</span>: c[<span class="hljs-string">'name'</span>],&#13;
            <span class="hljs-string">'character'</span>: c[<span class="hljs-string">'character'</span>],&#13;
            <span class="hljs-string">'cast_id'</span>: c[<span class="hljs-string">'cast_id'</span>]&#13;
        }&#13;
        <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> cast_list&#13;
    ]&#13;
<span class="hljs-comment"># Function to extract relevant crew information</span>&#13;
<span class="hljs-keyword">def</span> <span class="hljs-title">extract_crew</span>(<span class="hljs-params">crew_str</span>):&#13;
    crew_list = ast.literal_eval(crew_str)&#13;
    relevant_jobs = [<span class="hljs-string">'Director'</span>, <span class="hljs-string">'Producer'</span>]&#13;
    <span class="hljs-keyword">return</span> [&#13;
        {&#13;
            <span class="hljs-string">'crew_id'</span>: c[<span class="hljs-string">'id'</span>],&#13;
            <span class="hljs-string">'name'</span>: c[<span class="hljs-string">'</span><span class="hljs-string">name'</span>],&#13;
            <span class="hljs-string">'job'</span>: c[<span class="hljs-string">'job'</span>]&#13;
        }&#13;
        <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> crew_list <span class="hljs-keyword">if</span> c[<span class="hljs-string">'job'</span>] <span class="hljs-keyword">in</span> relevant_jobs&#13;
    ]&#13;
<span class="hljs-comment"># Apply the extraction functions to each row</span>&#13;
df[<span class="hljs-string">'cast'</span>] = df[<span class="hljs-string">'cast'</span>].apply(extract_cast)&#13;
df[<span class="hljs-string">'crew'</span>] = df[<span class="hljs-string">'crew'</span>].apply(extract_crew)&#13;
<span class="hljs-comment"># Explode the lists into separate rows</span>&#13;
df_cast = df.explode(<span class="hljs-string">'cast'</span>).dropna(subset=[<span class="hljs-string">'cast'</span>])&#13;
df_crew = df.explode(<span class="hljs-string">'crew'</span>).dropna(subset=[<span class="hljs-string">'crew'</span>])&#13;
<span class="hljs-comment"># Normalize the exploded data</span>&#13;
df_cast_normalized = pd.json_normalize(df_cast[<span class="hljs-string">'cast'</span>])&#13;
df_crew_normalized = pd.json_normalize(df_crew[<span class="hljs-string">'crew'</span>])&#13;
<span class="hljs-comment"># Reset index to avoid duplicate indices</span>&#13;
df_cast_normalized = df_cast_normalized.reset_index(drop=<span class="hljs-literal">True</span>)&#13;
df_crew_normalized = df_crew_normalized.reset_index(drop=<span class="hljs-literal">True</span>)&#13;
<span class="hljs-comment"># Drop duplicate rows if any</span>&#13;
df_cast_normalized = df_cast_normalized.drop_duplicates()&#13;
df_crew_normalized = df_crew_normalized.drop_duplicates()&#13;
<span class="hljs-comment"># Add the movie ID back to the normalized DataFrames</span>&#13;
df_cast_normalized[<span class="hljs-string">'tmdbId'</span>] = df_cast.reset_index(drop=<span class="hljs-literal">True</span>)[<span class="hljs-string">'id'</span>]&#13;
df_crew_normalized[<span class="hljs-string">'tmdbId'</span>] = df_crew.reset_index(drop=<span class="hljs-literal">True</span>)[<span class="hljs-string">'id'</span>]&#13;
<span class="hljs-comment"># Save the normalized data with the updated column names</span>&#13;
df_cast_normalized.to_csv(&#13;
    os.path.join(output_dir, <span class="hljs-string">'normalized_cast.csv'</span>),&#13;
    index=<span class="hljs-literal">False</span>&#13;
)&#13;
df_crew_normalized.to_csv(&#13;
    os.path.join(output_dir, <span class="hljs-string">'normalized_crew.csv'</span>),&#13;
    index=<span class="hljs-literal">False</span>&#13;
)&#13;
<span class="hljs-comment"># Display a sample of the output for verification</span>&#13;
<span class="hljs-built_in">print</span>(<span class="hljs-string">"Sample of normalized cast data:"</span>)&#13;
<span class="hljs-built_in">print</span>(df_cast_normalized.head())&#13;
<span class="hljs-built_in">print</span>(<span class="hljs-string">"Sample of normalized crew data:"</span>)&#13;
<span class="hljs-built_in">print</span>(df_crew_normalized.head())&#13;
</code></pre>&#13;
      </li>&#13;
      <li class="bulletList"><code class="inlineCode">keywords.csv</code>: This file <a id="_idIndexMarker206"/>contains the movie plot keywords for each movie in the dataset. The keywords are essential for categorizing and identifying thematic elements within the movies, which can be used for various purposes, such<a id="_idIndexMarker207"/> as search, recommendation, and content analysis:&#13;
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Load the CSV file</span>&#13;
df = pd.read_csv(<span class="hljs-string">'</span><span class="hljs-string">./raw_data/keywords.csv'</span>)  <span class="hljs-comment"># Update the path as necessary</span>&#13;
<span class="hljs-comment"># Function to extract and normalize keywords</span>&#13;
<span class="hljs-keyword">def</span> <span class="hljs-title">normalize_keywords</span>(<span class="hljs-params">keyword_str</span>):&#13;
    <span class="hljs-keyword">if</span> pd.isna(keyword_str) <span class="hljs-keyword">or</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(keyword_str, <span class="hljs-built_in">str</span>):  <span class="hljs-comment"># Check if the value is NaN or not a string</span>&#13;
        <span class="hljs-keyword">return</span> []&#13;
    <span class="hljs-comment"># Convert the stringified JSON object into a list of dictionaries</span>&#13;
    keyword_list = ast.literal_eval(keyword_str)&#13;
    <span class="hljs-comment"># Extract the 'name' of each keyword and return them as a list</span>&#13;
    <span class="hljs-keyword">return</span> [kw[<span class="hljs-string">'name'</span>] <span class="hljs-keyword">for</span> kw <span class="hljs-keyword">in</span> keyword_list]&#13;
<span class="hljs-comment"># Apply the normalization function to the 'keywords' column</span>&#13;
df[<span class="hljs-string">'keywords'</span>] = df[<span class="hljs-string">'keywords'</span>].apply(normalize_keywords)&#13;
<span class="hljs-comment"># Combine all keywords for each tmdbId into a single row</span>&#13;
df_keywords_aggregated = df.groupby(<span class="hljs-string">'id'</span>, as_index=<span class="hljs-literal">False</span>).agg({&#13;
    <span class="hljs-string">'keywords'</span>: <span class="hljs-keyword">lambda</span> x: <span class="hljs-string">', '</span>.join(<span class="hljs-built_in">sum</span>(x, []))&#13;
})&#13;
<span class="hljs-comment"># Rename the 'id' column to 'tmdbId'</span>&#13;
df_keywords_aggregated.rename(&#13;
    columns={<span class="hljs-string">'id'</span>: <span class="hljs-string">'tmdbId'</span>}, inplace=<span class="hljs-literal">True</span>&#13;
)&#13;
<span class="hljs-comment"># Save the aggregated DataFrame to a new CSV file</span>&#13;
df_keywords_aggregated.to_csv(&#13;
    os.path.join(output_dir, <span class="hljs-string">'normalized_keywords.csv'</span>),&#13;
    index=<span class="hljs-literal">False</span>&#13;
)&#13;
<span class="hljs-comment"># Display the first few rows of the aggregated DataFrame for verification</span>&#13;
<span class="hljs-built_in">print</span>(df_keywords_aggregated.head())&#13;
</code></pre>&#13;
      </li>&#13;
      <li class="bulletList"><code class="inlineCode">links.csv</code>: This file <a id="_idIndexMarker208"/>contains essential metadata that links each movie in the full <strong class="screenText">MovieLens dataset</strong> to its corresponding entries in both TMDb and IMDB. This file serves as a crucial bridge for connecting the MovieLens dataset with external movie databases, enabling enriched data integration and further analysis. However, for this use case, we skipped processing the <code class="inlineCode">links.csv</code> file, as it is not essential <a id="_idIndexMarker209"/>to our current analysis. Our focus will remain on other CSV files more directly relevant to our project’s objectives. The data contained in <code class="inlineCode">links.csv</code> can still be useful for future projects that require integration with external databases, but it will not be utilized in this instance.</li>&#13;
      <li class="bulletList"><code class="inlineCode">links_small.csv</code>: This file contains the TMDb and IMDb IDs for a small subset of 9,000 movies from the full MovieLens dataset. While this file provides a streamlined version of the links for a smaller selection of movies, we will not be using this file, as we are already utilizing the full dataset from Kaggle, which includes all available movies. This file is typically useful for scenarios where a more manageable, smaller dataset is needed, but for our purposes, the full data set is preferred for comprehensive analysis and integration.</li>&#13;
      <li class="bulletList"><code class="inlineCode">movies_metadata.csv</code>: This file is a comprehensive dataset containing detailed information on 45,000 movies featured in the full MovieLens dataset. This file includes various features such as posters, backdrops, budgets, revenue, release dates, languages, production countries, and companies, among others. To efficiently organize and analyze this data, we <span id="page12-2" role="doc-pagebreak" aria-label="82" epub:type="pagebreak"/>will normalize the <code class="inlineCode">movies_metadata.csv</code> file into multiple CSV files, each representing a relevant node in our dataset. These nodes include genres, production companies, production countries, and spoken languages. By breaking down the data into these separate files, we can more easily manage and utilize the rich information contained within this dataset. Let’s see how.<ol>&#13;
          <li class="numberedList" value="1">Begin with necessary imports.&#13;
            <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd&#13;
<span class="hljs-keyword">import</span> ast&#13;
<span class="hljs-comment"># Load the CSV file</span>&#13;
df = pd.read_csv(<span class="hljs-string">'./raw_data/movies_metadata.csv'</span>)  <span class="hljs-comment"># Update the path as necessary</span>&#13;
</code></pre>&#13;
          </li>&#13;
          <li class="numberedList">Extract and <a id="_idIndexMarker210"/>normalize genres, production companies, countries, and<a id="_idIndexMarker211"/> spoken languages. We will demonstrate this step for genres and production companies. The rest of the code is available on <a href="https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/tree/main/ch4"><span class="url">https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/tree/main/ch4</span></a><a href="https://github.com/PacktPublishing/Gen-AI-with-Neo4j-Knowledge-Graphs-Vector-Search/tree/main/ch4%0D%0A"/>&#13;
            <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Function to extract and normalize genres</span>&#13;
<span class="hljs-keyword">def</span> <span class="hljs-title">extract_genres</span>(<span class="hljs-params">genres_str</span>):&#13;
    <span class="hljs-keyword">if</span> pd.isna(genres_str) <span class="hljs-keyword">or</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(&#13;
        genres_str, <span class="hljs-built_in">str</span>&#13;
    ):&#13;
        <span class="hljs-keyword">return</span> []&#13;
    genres_list = ast.literal_eval(genres_str)&#13;
    <span class="hljs-keyword">return</span> [&#13;
        {<span class="hljs-string">'genre_id'</span>: <span class="hljs-built_in">int</span>(g[<span class="hljs-string">'id'</span>]), <span class="hljs-string">'genre_name'</span>: g[<span class="hljs-string">'name'</span>]}&#13;
        <span class="hljs-keyword">for</span> g <span class="hljs-keyword">in</span> genres_list&#13;
    ]&#13;
<span class="hljs-comment"># Function to extract and normalize production companies</span>&#13;
<span class="hljs-keyword">def</span> <span class="hljs-title">extract_production_companies</span>(<span class="hljs-params">companies_str</span>):&#13;
    <span class="hljs-keyword">if</span> pd.isna(companies_str) <span class="hljs-keyword">or</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(&#13;
        companies_str, <span class="hljs-built_in">str</span>&#13;
    ):&#13;
        <span class="hljs-keyword">return</span> []&#13;
    companies_list = ast.literal_eval(companies_str)&#13;
    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(companies_list, <span class="hljs-built_in">list</span>):&#13;
        <span class="hljs-keyword">return</span> [&#13;
            {<span class="hljs-string">'company_id'</span>: <span class="hljs-built_in">int</span>(c[<span class="hljs-string">'id'</span>]),&#13;
                <span class="hljs-string">'company_name'</span>: c[<span class="hljs-string">'name'</span>]&#13;
            }&#13;
            <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> companies_list&#13;
        ]&#13;
    <span class="hljs-keyword">return</span> []&#13;
</code></pre>&#13;
          </li>&#13;
          <li class="numberedList">Apply the<a id="_idIndexMarker212"/> extraction<a id="_idIndexMarker213"/> functions.&#13;
            <pre class="programlisting code"><code class="hljs-code">df[<span class="hljs-string">'genres'</span>] = df[<span class="hljs-string">'genres'</span>].apply(extract_genres)&#13;
df[<span class="hljs-string">'production_companies'</span>] = \&#13;
    df[<span class="hljs-string">'production_companies'</span>].apply(&#13;
        extract_production_companies&#13;
    )&#13;
df[<span class="hljs-string">'production_countries'</span>] = \&#13;
    df[<span class="hljs-string">'production_countries'</span>].apply(&#13;
        extract_production_countries&#13;
    )&#13;
df[<span class="hljs-string">'spoken_languages'</span>] = df[<span class="hljs-string">'spoken_languages'</span>].apply(&#13;
    extract_spoken_languages&#13;
)&#13;
<span class="hljs-comment"># Explode lists into rows</span>&#13;
df_genres = df.explode(<span class="hljs-string">'genres'</span>).dropna(subset=[<span class="hljs-string">'genres'</span>])&#13;
df_companies = df.explode(<span class="hljs-string">'production_companies'</span>).dropna(&#13;
    subset=[<span class="hljs-string">'production_companies'</span>]&#13;
)&#13;
df_countries = df.explode(<span class="hljs-string">'production_countries'</span>).dropna(&#13;
    subset=[<span class="hljs-string">'</span><span class="hljs-string">production_countries'</span>]&#13;
)&#13;
df_languages = df.explode(<span class="hljs-string">'spoken_languages'</span>).dropna(&#13;
    subset=[<span class="hljs-string">'spoken_languages'</span>]&#13;
)&#13;
</code></pre>&#13;
          </li>&#13;
          <li class="numberedList">Normalize the <a id="_idIndexMarker214"/>exploded data. Let’s <a id="_idIndexMarker215"/>do this for genres.&#13;
            <pre class="programlisting code"><code class="hljs-code">df_genres_normalized = pd.json_normalize(df_genres[<span class="hljs-string">'genres'</span>])&#13;
<span class="hljs-comment"># Reset index to avoid duplicate indices</span>&#13;
df_genres_normalized = \&#13;
    df_genres_normalized.reset_index(drop=<span class="hljs-literal">True</span>)&#13;
<span class="hljs-comment"># Add the movie ID back to the normalized DataFrames as 'tmdbId'</span>&#13;
df_genres_normalized[<span class="hljs-string">'tmdbId'</span>] = df_genres.reset_index(&#13;
    drop=<span class="hljs-literal">True</span>&#13;
)[<span class="hljs-string">'</span><span class="hljs-string">id'</span>]&#13;
<span class="hljs-comment"># Ensure that 'company_id' and similar fields are treated as integers</span>&#13;
df_genres_normalized[<span class="hljs-string">'genre_id'</span>] = \&#13;
    df_genres_normalized[<span class="hljs-string">'genre_id'</span>].astype(<span class="hljs-built_in">int</span>)&#13;
<span class="hljs-comment"># Save the normalized data with the updated column names</span>&#13;
df_genres_normalized.to_csv(&#13;
    os.path.join(output_dir, <span class="hljs-string">'normalized_genres.csv'</span>),&#13;
    index=<span class="hljs-literal">False</span>&#13;
)&#13;
</code></pre>&#13;
          </li>&#13;
          <li class="numberedList">Next up, extract the collection name.&#13;
            <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># For the movies, including "Belongs to Collection" within the same CSV</span>&#13;
<span class="hljs-comment"># Extract only the "name" from "belongs_to_collection" and include additional fields</span>&#13;
<span class="hljs-keyword">def</span> <span class="hljs-title">extract_collection_name</span>(<span class="hljs-params">collection_str</span>):&#13;
    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(collection_str, <span class="hljs-built_in">str</span>):&#13;
        <span class="hljs-keyword">try</span>:&#13;
            collection_dict = \&#13;
                ast.literal_eval(collection_str)&#13;
            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(collection_dict, <span class="hljs-built_in">dict</span>):&#13;
                <span class="hljs-keyword">return</span> collection_dict.get(<span class="hljs-string">'name'</span>, <span class="hljs-string">"None"</span>)&#13;
        <span class="hljs-keyword">except</span> (ValueError, SyntaxError):  <span class="hljs-comment"># Handle cases where string parsing fails</span>&#13;
            <span class="hljs-keyword">return</span> <span class="hljs-string">"None"</span>&#13;
    <span class="hljs-keyword">return</span> <span class="hljs-string">"None"</span>&#13;
df_movies = df[&#13;
    [&#13;
        <span class="hljs-string">'id'</span>, <span class="hljs-string">'</span><span class="hljs-string">original_title'</span>, <span class="hljs-string">'adult'</span>, <span class="hljs-string">'budget'</span>, <span class="hljs-string">'imdb_id'</span>,&#13;
        <span class="hljs-string">'original_language'</span>, <span class="hljs-string">'revenue'</span>, <span class="hljs-string">'tagline'</span>, <span class="hljs-string">'title'</span>,&#13;
        <span class="hljs-string">'release_date'</span>, <span class="hljs-string">'runtime'</span>, <span class="hljs-string">'</span><span class="hljs-string">overview'</span>,&#13;
        <span class="hljs-string">'belongs_to_collection'</span>&#13;
    ]&#13;
].copy()&#13;
df_movies[<span class="hljs-string">'belongs_to_collection'</span>] = \&#13;
    df_movies[<span class="hljs-string">'belongs_to_collection'</span>].apply(&#13;
        extract_collection_name&#13;
    )&#13;
df_movies[<span class="hljs-string">'adult'</span>] = df_movies[<span class="hljs-string">'adult'</span>].apply(&#13;
    <span class="hljs-keyword">lambda</span> x: <span class="hljs-number">1</span> <span class="hljs-keyword">if</span> x == <span class="hljs-string">'</span><span class="hljs-string">TRUE'</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span>&#13;
)  <span class="hljs-comment"># Convert 'adult' to integer</span>&#13;
<span class="hljs-comment"># Rename 'id' to 'tmdbId'</span>&#13;
df_movies.rename(columns={<span class="hljs-string">'id'</span>: <span class="hljs-string">'tmdbId'</span>}, inplace=<span class="hljs-literal">True</span>)  <span class="hljs-comment"># Rename 'id' to 'tmdbId'</span>&#13;
<span class="hljs-comment"># Save the movies to a separate CSV, including the extracted fields</span>&#13;
df_movies.to_csv(&#13;
    <span class="hljs-string">'</span><span class="hljs-string">./normalized_data/normalized_movies.csv'</span>, index=<span class="hljs-literal">False</span>&#13;
)&#13;
</code></pre>&#13;
          </li>&#13;
        </ol>&#13;
      </li>&#13;
      <li class="bulletList"><code class="inlineCode">ratings.csv</code>: This file<a id="_idIndexMarker216"/> is the full MovieLens dataset, consisting of 26 million ratings <a id="_idTextAnchor031"/>and 750,000 tag applications from 270,000 users on all 45,000 movies in this dataset. This comprehensive dataset provides detailed user interaction data, which we will use directly without the need for normalization. However, for this use case, we have decided to skip processing the <code class="inlineCode">ratings.csv</code> file. While it provides extensive user interaction data, it is not essential to our current analysis and objectives. We are focusing on other CSV files that are more directly relevant to our project. The data in <code class="inlineCode">ratings.csv</code> can still be valuable for future projects that require a deep dive into user ratings and interactions, but it will not be utilized in this instance.</li>&#13;
      <li class="bulletList"><code class="inlineCode">ratings_small.csv</code>: This file is a smaller subset of the <code class="inlineCode">ratings.csv</code> file, containing 100,000 ratings from 700 users on 9,000 movies. We will be using <code class="inlineCode">ratings_small.csv</code> instead of<a id="_idIndexMarker217"/> focusing on the full dataset provided in <code class="inlineCode">ratings.csv</code>.</li>&#13;
    </ul>&#13;
    <p class="normal">Through this process, we have learned how to transform raw, unstructured data into clean, normalized datasets that are now primed for integration into your Neo4j graph. This preparation paves the way for constructing a robust, efficient, and effective AI-powered search and recommendation system. In the next section, we will take these normalized CSV files and use Cypher code to build a knowledge graph, unlocking the full potential of our dataset.</p>&#13;
    <h1 id="_idParaDest-64" class="heading-1">Building your movie knowledge graph with code examples</h1>&#13;
    <p class="normal">In this section, we will import<a id="_idIndexMarker218"/> your normalized datasets into Neo4j and transform them into a fully functional knowledge graph.</p>&#13;
    <h2 id="_idParaDest-65" class="heading-2">Setting up your AuraDB free instance</h2>&#13;
    <p class="normal">To start building your knowledge graph with Neo4j, you will first need to set up an AuraDB Free instance. AuraDB Free<a id="_idIndexMarker219"/> is a cloud-hosted Neo4j database that allows you to quickly get started without worrying about local installations or infrastructure management.</p>&#13;
    <p class="normal">Follow these steps to <a id="_idIndexMarker220"/>create your instance:</p>&#13;
    <ol>&#13;
      <li class="numberedList" value="1">Visit <a href="https://console.neo4j.io"><span class="url">https://console.neo4j.io</span></a>.</li>&#13;
      <li class="numberedList">Log in with your Google account or with email.</li>&#13;
      <li class="numberedList">Click <strong class="screenText">Create Free Instance</strong>.</li>&#13;
      <li class="numberedList">While the instance is being provisioned, a pop-up window will appear showing the connection credentials for your database.</li>&#13;
    </ol>&#13;
    <p class="normal">Make sure to download and securely save the following details from the popup—these are essential for connecting your application to Neo4j:<a id="_idTextAnchor032"/></p>&#13;
    <pre class="programlisting code"><code class="hljs-code">NEO4J_URI=neo4j+s://&lt;your-instance-id&gt;.databases.neo4j.io&#13;
NEO4J_USERNAME=neo4j&#13;
NEO4J_PASSWORD=&lt;your-generated-password&gt;&#13;
AURA_INSTANCEID=&lt;your-instance-id&gt;&#13;
AURA_INSTANCENAME=&lt;your-instance-name&gt;&#13;
</code></pre>&#13;
    <p class="normal">With your AuraDB Free instance set up, you are now ready to import your normalized datasets and start building your knowledge graph using Cypher code. In the following section, we will guide you through importing data and constructing relationships within your graph.</p>&#13;
    <h2 id="_idParaDest-66" class="heading-2">Importing your data into AuraDB</h2>&#13;
    <p class="normal">Now that your <a id="_idIndexMarker221"/>AuraDB Free instance is up and running, it is time to import your normalized datasets and build your knowledge graph. In this section, we will walk through preparing your CSV files, setting up indexes and constraints, importing data, and creating relationships—all through a Python script:</p>&#13;
    <ol>&#13;
      <li class="numberedList" value="1">Prepare your CSV files for import.</li>&#13;
      <li class="numberedList">Ensure that the CSV files you generated (e.g., <code class="inlineCode">normalized_movies.csv</code>, <code class="inlineCode">normalized_genres.csv</code>, etc.) are ready for import. These files should be clean, well structured, and hosted at accessible URLs. In this case, the <code class="inlineCode">graph_build.py</code> script fetches files from public cloud storage (for example, <a href="https://storage.googleapis.com/movies-packt/normalized_movies.csv"><span class="url">https://storage.googleapis.com/movies-packt/normalized_movies.csv</span></a>), so you do not need to upload them manually anywhere.</li>&#13;
      <li class="numberedList">Add indexes and constraints to optimize graph query retrieval.</li>&#13;
    </ol>&#13;
    <p class="normal">Before loading data, it is critical to create unique constraints and indexes to ensure integrity and optimize query performance. The script includes Cypher commands to do the following:</p>&#13;
    <ul>&#13;
      <li class="bulletList">Ensure uniqueness on IDs such as <code class="inlineCode">tmdbId</code>, <code class="inlineCode">movieId</code>, and <code class="inlineCode">company_id</code></li>&#13;
      <li class="bulletList">Create indexes on properties such as <code class="inlineCode">actor_id</code>, <code class="inlineCode">crew_id</code>, and <code class="inlineCode">user_id</code></li>&#13;
    </ul>&#13;
    <p class="normal">Here is how you can create indexes and constraints:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code">"CREATE CONSTRAINT unique_tmdb_id IF NOT EXISTS FOR (m:Movie) REQUIRE m.tmdbId IS UNIQUE;",&#13;
"CREATE CONSTRAINT unique_movie_id IF NOT EXISTS FOR (m:Movie) REQUIRE m.movieId IS UNIQUE;",&#13;
"CREATE CONSTRAINT unique_prod_id IF NOT EXISTS FOR (p:ProductionCompany) REQUIRE p.company_id IS UNIQUE;",&#13;
"CREATE CONSTRAINT unique_genre_id IF NOT EXISTS FOR (g:Genre) REQUIRE g.genre_id IS UNIQUE;",&#13;
"CREATE CONSTRAINT unique_lang_id IF NOT EXISTS FOR (l:SpokenLanguage) REQUIRE l.language_code IS UNIQUE;",&#13;
"CREATE CONSTRAINT unique_country_id IF NOT EXISTS FOR (c:Country) REQUIRE c.country_code IS UNIQUE;",&#13;
"CREATE INDEX actor_id IF NOT EXISTS FOR (p:Person) ON (p.actor_id);",&#13;
"CREATE INDEX crew_id IF NOT EXISTS FOR (p:Person) ON (p.crew_id);",&#13;
"CREATE INDEX movieId IF NOT EXISTS FOR (m:Movie) ON (m.movieId);",&#13;
"CREATE INDEX user_id IF NOT EXISTS FOR (p:Person) ON (p.user_id);"&#13;
</code></pre>&#13;
    <ol>&#13;
      <li class="numberedList" value="4">Import data and create nodes.</li>&#13;
    </ol>&#13;
    <p class="normal">After adding constraints and indexes, the script loads the nodes from their respective CSVs:</p>&#13;
    <ul>&#13;
      <li class="bulletList"><code class="inlineCode">load_movies()</code> adds all movie metadata</li>&#13;
      <li class="bulletList"><code class="inlineCode">load_genres()</code>, <code class="inlineCode">load_production_companies()</code>, <code class="inlineCode">load_countries()</code>, and others create related nodes, such as <code class="inlineCode">Genre</code>, <code class="inlineCode">ProductionCompany</code>, <code class="inlineCode">Country</code>, and <code class="inlineCode">SpokenLanguage</code></li>&#13;
      <li class="bulletList">Person-related data is added using <code class="inlineCode">load_person_actors()</code> and <code class="inlineCode">load_person_crew()</code></li>&#13;
    </ul>&#13;
    <p class="normal">Additional <a id="_idIndexMarker222"/>properties are added via <code class="inlineCode">load_links()</code>, <code class="inlineCode">load_keywords()</code>, and <code class="inlineCode">load_ratings()</code></p>&#13;
    <p class="normal">Take the following example:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code">graph.load_movies('https://storage.googleapis.com/movies-packt/normalized_movies.csv', movie_limit)&#13;
</code></pre>&#13;
    <ol>&#13;
      <li class="numberedList" value="5">Create relationships.</li>&#13;
    </ol>&#13;
    <p class="normal">As each loader function runs, it not only creates nodes but also establishes meaningful relationships:</p>&#13;
    <ul>&#13;
      <li class="bulletList"><code class="inlineCode">HAS_GENRE</code> between <code class="inlineCode">Movie</code> and <code class="inlineCode">Genre</code></li>&#13;
      <li class="bulletList"><code class="inlineCode">PRODUCED_BY</code> between <code class="inlineCode">Movie</code> and <code class="inlineCode">ProductionCompany</code></li>&#13;
      <li class="bulletList"><code class="inlineCode">HAS_LANGUAGE</code> between <code class="inlineCode">Movie and SpokenLanguage</code>, <code class="inlineCode">PRODUCED_IN </code>between<code class="inlineCode"> Movie</code> and <code class="inlineCode">Country</code>, <code class="inlineCode">ACTED_IN</code>, <code class="inlineCode">DIRECTED</code>, <code class="inlineCode">PRODUCED</code> between <code class="inlineCode">Movie</code> and <code class="inlineCode">Person</code>, and <code class="inlineCode">RATED </code>between <code class="inlineCode">Movie</code> and <code class="inlineCode">User</code>, among others.</li>&#13;
    </ul>&#13;
    <ol>&#13;
      <li class="numberedList" value="6">Run the full script.</li>&#13;
    </ol>&#13;
    <p class="normal">Before running the script, ensure you have the Neo4j Python driver installed. You can install it using <code class="inlineCode">pip</code>.</p>&#13;
    <pre class="programlisting con"><code class="hljs-con"> pip install neo4j&#13;
</code></pre>&#13;
    <p class="normal">To run the entire graph-building process, simply execute the following:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con">python graph_build.py&#13;
</code></pre>&#13;
    <p class="normal">This script <a id="_idIndexMarker223"/>performs the following, in order:</p>&#13;
    <ul>&#13;
      <li class="bulletList">Connects to your AuraDB instance using credentials from the <code class="inlineCode">.env</code> file</li>&#13;
      <li class="bulletList">Cleans up the database</li>&#13;
      <li class="bulletList">Adds indexes and constraints</li>&#13;
      <li class="bulletList">Loads all node data and relationships in bulk using hosted CSVs</li>&#13;
    </ul>&#13;
    <p class="normal">Please refer to the complete script available here: <a href="https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch4/graph_build.py"><span class="url">https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/blob/main/ch4/graph_build.py</span></a>.</p>&#13;
    <p class="normal">Once complete, verify your import using Neo4j Browser:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code">MATCH (m:Movie)-[:HAS_GENRE]-&gt;(g:Genre)&#13;
RETURN m.title, g.genre_name&#13;
LIMIT 10;&#13;
</code></pre>&#13;
    <p class="normal"><em class="italic">Figure 4.1</em> illustrates <a id="_idIndexMarker224"/>a connected movie graph with over 90K nodes and 320K+ relationships. Nodes such as <code class="inlineCode">Movie</code>, <code class="inlineCode">Genre</code>, <code class="inlineCode">Person</code>, and <code class="inlineCode">ProductionCompany</code> are represented with distinct colors, while relationships suc<a id="_idTextAnchor033"/>h as <code class="inlineCode">ACTED_IN</code>, <code class="inlineCode">HAS_GENRE</code>, and <code class="inlineCode">PRODUCED_BY</code> showcase the web of interconnected metadata.</p>&#13;
    <figure class="mediaobject"><img src="../Images/B31107_04_01.png" alt="Figure 4.1 — Neo4j graph of movies dataset" width="937" height="933"/></figure>&#13;
    <p class="packt_figref">Figure 4.1 — Neo4j graph of movies dataset</p>&#13;
    <p class="normal">With your data successfully imported and your knowledge graph fully constructed using Python and Cypher, you are now ready to dive into building a GenAI-powered search application in the next chapter. In the following section, we will dive into advanced Cypher techniques that empower you to handle intricate relationships and derive deeper insights from your data.</p>&#13;
    <h1 id="_idParaDest-67" class="heading-1">Beyond the basics: advanced Cypher techniques for complex <a id="_idTextAnchor034"/>graph structures</h1>&#13;
    <p class="normal">As your knowledge graph grows in size and complexity, so do the demands on your querying and data management capabilities. Cypher, Neo4j’s powerful query language, offers a range of advanced features designed to handle complex graph structures and enable more sophisticated data analysis. In this section, we will explore these advanced Cypher<a id="_idIndexMarker225"/> techniques, including <strong class="keyWord">path patterns</strong>, <strong class="keyWord">variable-length relationships</strong>, subqueries, and graph algorithms. Understanding these techniques, will help you <a id="_idIndexMarker226"/>efficiently manage intricate relationships, perform deeper analyses, and unlock the full potential of your knowledge graph for advanced use cases.</p>&#13;
    <p class="normal">Let us explore these key advanced Cypher techniques:</p>&#13;
    <ul>&#13;
      <li class="bulletList"><strong class="screenText">Variable-length relationships</strong>:  Variable-length relationships in Cypher allow you to match<a id="_idIndexMarker227"/> paths of varying<a id="_idIndexMarker228"/> lengths between nodes. This is particularly useful when exploring hierarchical structures or networks with multiple degrees of separation. An example is finding all movies connected to a specific actor within three degrees of separation:&#13;
        <pre class="programlisting code"><code class="hljs-code">MA<a id="_idTextAnchor035"/>TCH (a:Actor {name: 'Tom Hanks'})-[:ACTED_IN*1..<a id="_idTextAnchor036"/>3]-(m:Movie)&#13;
RETURN DISTINCT m.title;&#13;
</code></pre>&#13;
        <ul>&#13;
          <li class="bulletList">Here, <code class="inlineCode">*1..3</code> specifies that the relationship path can be between 1 and 3 steps long.</li>&#13;
          <li class="bulletList"><strong class="screenText">Use cases</strong>: Variable-length relationships are ideal for scenarios such as social network analysis, where you want to find all people within a certain degree of connection, or in hierarchical datasets where you want to explore parent-child relationships <a id="_idIndexMarker229"/>across multiple<a id="_idIndexMarker230"/> levels.</li>&#13;
        </ul>&#13;
      </li>&#13;
      <li class="bulletList"><strong class="screenText">Pattern matching with path patterns</strong>: You can<a id="_idIndexMarker231"/> create <strong class="keyWord">named path patterns</strong> as well as <a id="_idIndexMarker232"/>chain the paths in Neo4j.<ul>&#13;
          <li class="bulletList"><strong class="screenText">Defining path patterns</strong>: Cypher allows you to define named path patterns that can be reused throughout your queries. This makes your queries more readable and allows you to encapsulate complex relationships in a single pattern. Take the following example:&#13;
            <pre class="programlisting code"><code class="hljs-code">MATCH path = (a:Actor)-[:ACTED_IN]-&gt;(m:Movie)&#13;
RETURN path;&#13;
</code></pre>&#13;
          </li>&#13;
        </ul>&#13;
        <p class="normal">Here, <code class="inlineCode">path</code> is a named path pattern that can be reused in subsequent operations or subqueries.</p>&#13;
        <ul>&#13;
          <li class="bulletList"><strong class="keyWord">Chaining path patterns</strong>: Cypher allows you to <a id="_idIndexMarker233"/>combine multiple path patterns to perform complex traversals within the graph. This is especially useful when trying to uncover indirect relationships or discover multiple paths that satisfy specific criteria.</li>&#13;
        </ul>&#13;
        <p class="normal">An example is exploring collaborations in the movies dataset.</p>&#13;
        <p class="normal">Let’s say we want to find movies where an actor has worked with a director they’ve previously collaborated with, possibly through another movie. This involves chaining paths from an actor to a movie, then to a director, and seeing whether there’s another movie connecting the same actor-director pair:</p>&#13;
        <pre class="programlisting code"><code class="hljs-code">MATCH (a:Actor {name: "Tom Hanks"})-[:ACTED_IN]-&gt;(m1:Movie)&lt;-[:DIRECTED_BY]-(d:Director) MATCH (a)-[:ACTED_IN]-&gt;(m2:Movie)&lt;-[:DIRECTED_BY]-(d)&#13;
WHERE m1 &lt;&gt; m2&#13;
RETURN a.name AS actor, d.name AS director, collect(DISTINCT m1.title) + collect(DISTINCT m2.title) AS movies&#13;
</code></pre>&#13;
        <p class="normal">This kind of pattern chaining is extremely helpful in identifying professional relationships, recurring <a id="_idIndexMarker234"/>collaborations, or analyzing indirect influence in networks.</p>&#13;
      </li>&#13;
      <li class="bulletList"><strong class="keyWord">Subqueries</strong> and <strong class="keyWord">procedural logic</strong>: You can use<a id="_idIndexMarker235"/> subqueries<a id="_idIndexMarker236"/> and procedures to process<a id="_idIndexMarker237"/> complex queries. Here is how:<ul>&#13;
          <li class="bulletList"><strong class="screenText">Using subqueries for modular queries</strong>: Subqueries in Cypher allow you to break down complex queries into modular, reusable components. This is particularly helpful when dealing with large graphs or when you need to perform multiple operations on the same dataset. Take the fol<a id="_idTextAnchor037"/>lowing example:&#13;
            <pre class="programlisting code"><code class="hljs-code">CALL {&#13;
  MATCH (m:Movie)-[:HAS_GENRE]-&gt;(g:Genre {name: 'Action'})&#13;
  RETURN m&#13;
}&#13;
MATCH (m)-[:DIRECTED_BY]-&gt;(d:Director)&#13;
RETURN d.name, COUNT(m) AS action_movies_directed;&#13;
</code></pre>&#13;
          </li>&#13;
        </ul>&#13;
        <p class="normal">Here, the subquery retrieves all action movies, and the outer query matches these movies to their directors.</p>&#13;
        <ul>&#13;
          <li class="bulletList"><strong class="screenText">Procedural logic with </strong><strong class="keyWord">CALL</strong>: The <code class="inlineCode">CALL</code> clause in Cypher allows you to invoke procedures and use the results in further queries. This is essential for advanced <a id="_idIndexMarker238"/>data processing, such as running graph algorithms or invoking custom <a id="_idIndexMarker239"/>procedures.</li>&#13;
        </ul>&#13;
        <p class="normal">We’ve already applied this in our own implementation in the <code class="inlineCode">graph_build.py</code> file, specifically in the <code class="inlineCode">load_ratings()</code> function. Here, we use the <code class="inlineCode">CALL { ... } IN TRANSACTIONS</code> pattern to efficiently load large datasets by processing them in chunks of 50,000 rows:</p>&#13;
        <pre class="programlisting code"><code class="hljs-code">LOAD CSV WITH HEADERS FROM $csvFile AS row&#13;
CALL (row) {&#13;
  MATCH (m:Movie {movieId: toInteger(row.movieId)})&#13;
  WITH m, row&#13;
  MERGE (p:Person {user_id: toInteger(row.userId)})&#13;
  ON CREATE SET p.role = 'user'&#13;
  MERGE (p)-[r:RATED]-&gt;(m)&#13;
  ON CREATE SET r.rating = toFloat(row.rating), r.timestamp = toInteger(row.timestamp)&#13;
} IN TRANSACTIONS OF 50000 ROWS;&#13;
</code></pre>&#13;
        <p class="normal">This approach allows us to handle massive CSV imports while maintaining performance and transactional integrity—just one of the many powerful use cases for <code class="inlineCode">CALL</code> in real-world graph applications.</p>&#13;
      </li>&#13;
      <li class="bulletList"><strong class="screenText">Working with </strong><strong class="keyWord">nested queries:</strong> In <a id="_idIndexMarker240"/>complex graph structures, you might need to <strong class="screenText">combine results from multiple queries</strong>. Cypher<a id="_idIndexMarker241"/> allows you to nest queries, passing results from one query into another, which is useful for filtering or refining results based on multiple criteria. Take the following example:&#13;
        <pre class="programlisting code"><code class="hljs-code">MATCH (m:Movie)&#13;
WHERE m.revenue &gt; 100000000&#13;
CALL {&#13;
  WITH m&#13;
  MATCH (m)-[:HAS_GENRE]-&gt;(g:Genre)&#13;
  RETURN g.name AS genre&#13;
}&#13;
RETURN m.title, genre;&#13;
</code></pre>&#13;
      </li>&#13;
    </ul>&#13;
    <p class="normal">Here, the nested query refines the results by filtering movies based on revenue and then finding their associated genres.</p>&#13;
    <p class="normal">These Cypher techniques empower you to tackle complex graph structures, enabling deeper insights and more sophisticated analyses. You can refer to <a href="https://neo4j.com/docs/cypher-manual/current/appendix/tutorials/advanced-query-tuning/"><span class="url">https://neo4j.com/docs/cypher-manual/current/appendix/tutorials/advanced-query-tuning/</span></a> to explore some of these techniques further.</p>&#13;
    <h1 id="_idParaDest-68" class="heading-1">Summary</h1>&#13;
    <p class="normal">In this chapter, we worked on transforming raw, semi-structured data into clean, normalized datasets, ready for integration into our knowledge graph. We then explored the best practices in graph modeling, focusing on how to structure your nodes and relationships to enhance search efficiency and ensure your graph remains scalable and performant. Following this, we tackled other Cypher techniques, equipping you with the skills to handle variable-length relationships, pattern matching, subqueries, and graph algorithms. You are now well prepared to build a knowledge graph-driven search that can handle even the most intricate data relationships.</p>&#13;
    <p class="normal">In the next chapter, we will take a step further by exploring how to integrate Haystack into Neo4j. This practical guide will show you how to build powerful search functionalities within your knowledge graph, allowing you to leverage the full potential of both Neo4j and Haystack for intelligent search solutions.</p>&#13;
  </div>&#13;
</div></div></body></html>