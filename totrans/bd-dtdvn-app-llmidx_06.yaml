- en: <title>Indexing our PITS study materials – hands-on</title>
  prefs: []
  type: TYPE_NORMAL
- en: Indexing our PITS study materials – hands-on
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'from llama_index.core import (     VectorStoreIndex, TreeIndex, load_index_from_storage)
    from llama_index.core import StorageContext from global_settings import INDEX_STORAGE
    from document_uploader import ingest_documents def build_indexes(nodes):     try:
            storage_context = StorageContext.from_defaults(             persist_dir=INDEX_STORAGE
            )         vector_index = load_index_from_storage(             storage_context,
    index_id="vector"         )         tree_index = load_index_from_storage(             storage_context,
    index_id="tree"         )         print("All indices loaded from storage.")     except
    Exception as e:         print(f"Error occurred while loading indices: {e}")         storage_context
    = StorageContext.from_defaults()         vector_index = VectorStoreIndex(             nodes,
    storage_context=storage_context         )         vector_index.set_index_id("vector")
            tree_index = TreeIndex(             nodes, storage_context=storage_context
            )         tree_index.set_index_id("tree")         storage_context.persist(
                persist_dir=INDEX_STORAGE         )         print("New indexes created
    and persisted.")     return vector_index, tree_index'
  prefs: []
  type: TYPE_NORMAL
- en: With a solid understanding of how indexing works in LlamaIndex, we’re now ready
    to implement the indexing logic in our tutoring application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create the `index_builder.py` module. This module takes care of Index
    creation. In the current implementation, it creates two Indexes: a `VectorStoreIndex`
    and a `TreeIndex` . As you can see, this is a very basic implementation and there
    is definitely room for improvement. Let’s handle the imports first:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll implement our Index building function:'
  prefs: []
  type: TYPE_NORMAL
- en: We first check to see whether the Indexes have already been persisted to disk.
    If affirmative, then we leverage persistence to avoid the additional cost of rebuilding
    them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note on: Notice the usage of index_id'
  prefs: []
  type: TYPE_NORMAL
- en: Because we have persisted more than one Index in the same storage folder – `INDEX_STORAGE`
    – when using `load_index_from_storage` , we need to specify their individual IDs
    so that LlamaIndex can identify the correct Index.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we cannot find them in the `INDEX_STORAGE` folder, we proceed to build them
    from the nodes. We also set an ID for each Index using `set_index_id` so that
    we can load them correctly in future sessions:'
  prefs: []
  type: TYPE_NORMAL
- en: The `build_indexes` function returns the two Index objects that we’ll use later
    in our application.
  prefs: []
  type: TYPE_NORMAL
- en: That’s it for now. We’ll take the next steps during *Chapter 6* , *Querying
    Our Data, Part 1 –* *Context Retrieval* .
  prefs: []
  type: TYPE_NORMAL
- en: <title>Summary</title>
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored various indexing strategies and architectures within
    LlamaIndex. Indexes provide essential capabilities for building performant RAG
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the chapter, we looked at the `VectorStoreIndex` , which is the most
    commonly used Index type. We also gained an understanding of embeddings, vector
    stores, similarity search, and storage contexts. These are key concepts related
    to the `VectorStoreIndex` .
  prefs: []
  type: TYPE_NORMAL
- en: We also covered other Index types such as `SummaryIndex` for simple linear scans,
    `KeywordTableIndex` for keyword search, `TreeIndex` for hierarchical data, and
    `KnowledgeGraphIndex` for relationship-based queries. `ComposableGraph` was introduced
    as a tool for building multi-level Indexes, and cost estimation techniques were
    discussed together with best practices.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, this chapter provided an overview of indexing capabilities in LlamaIndex,
    laying the foundation for building sophisticated and efficient RAG applications.
  prefs: []
  type: TYPE_NORMAL
- en: See you in *Chapter 6* , where we’ll discuss methods for querying our data in
    LlamaIndex.
  prefs: []
  type: TYPE_NORMAL
- en: '<title>Part 3: Retrieving and Working with Indexed Data</title>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 3: Retrieving and Working with Indexed Data'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This part progresses from exploring LlamaIndex’s querying capabilities within
    a RAG workflow, focusing on retrieval mechanisms, query mechanics, and advanced
    retrieval strategies, to refining these queries through post-processing techniques
    and integrating them into comprehensive query engines. It culminates in a practical
    examination of building chatbots and intelligent agents, covering various engine
    modes, agent architectures, and the implementation of conversational features,
    thereby equipping you with the knowledge to create dynamic, conversational RAG
    interfaces.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 6* , *Querying Our Data, Part 1 – Context Retrieval*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Chapter 7* , *Querying Our Data, Part 2 – Postprocessing and Response Synthesis*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Chapter 8* , *Building Chatbots and Agents with LlamaIndex*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
