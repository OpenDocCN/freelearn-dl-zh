<html><head></head><body>
<div><div><div><h1 id="_idParaDest-121" class="chapter-number"><a id="_idTextAnchor120"/>6</h1>
			<h1 id="_idParaDest-122"><a id="_idTextAnchor121"/>Adding Memories to Your AI Application</h1>
			<p>In the previous chapter, we learned how to use planners to give our users the ability to ask our application to perform actions that we did not program explicitly. In this chapter, we are going to learn how to use external data, so that we can bring recent information and keep information between user sessions. For now, we are going to use small amounts of external data that our users may have given us by saving it to <strong class="bold">memory</strong>. Learning how to use memory will enable us to greatly expand the capabilities of AI models.</p>
			<p>This is a building block for the next chapter, in which we are going to learn techniques to use amounts of data that vastly exceed the context window of existing models. As you may remember, a <em class="italic">context window</em> is the maximum size of the input you can send to an AI service. By using memory, you can save a large amount of data and only send portions of the data in each call.</p>
			<p>We will start by understanding how LLMs convert words into meaning by using <strong class="bold">embeddings</strong> and then compare phrases with similar meanings to recall data from memory. Later in the chapter, we will see how to keep historical data in a chat application.</p>
			<p>In this chapter, we’ll be covering the following topics :</p>
			<ul>
				<li>Creating embeddings for text data</li>
				<li>Storing data in memory and recovering it to use in your prompts</li>
				<li>Using a plugin to keep track of a chat that your user is having with your application</li>
				<li>Using summarization to keep track of long chats</li>
			</ul>
			<p>By the end of the chapter, you will have learned how to help your application remember information entered by the user and retrieve it when needed.</p>
			<h1 id="_idParaDest-123"><a id="_idTextAnchor122"/>Technical requirements</h1>
			<p>To complete this chapter, you will need to have a recent, supported version of your preferred Python or C# development environment:</p>
			<ul>
				<li>For Python, the minimum supported version is Python 3.10, and the recommended version is Python 3.11</li>
				<li>For C#, the minimum supported version is .NET 8</li>
			</ul>
			<p>In this chapter, we will call OpenAI services. Given the amount that companies spend on training these LLMs, it’s no surprise that using these services is not free. You will need an <strong class="bold">OpenAI API</strong> key, either directly through <strong class="bold">OpenAI</strong> or <strong class="bold">Microsoft</strong>, via the <strong class="bold">Azure </strong><strong class="bold">OpenAI</strong> service.</p>
			<p>If you are using .NET, the code for this chapter is at <a href="https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/dotnet/ch6">https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/dotnet/ch6</a>.</p>
			<p>If you are using Python, the code for this chapter is at <a href="https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/python/ch6">https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/python/ch6</a>.</p>
			<p>You can install the required packages by going to the GitHub repository and using the following: <code>pip install -</code><code>r requirements.txt</code>.</p>
			<h1 id="_idParaDest-124"><a id="_idTextAnchor123"/>Defining memory and embeddings</h1>
			<p>LLMs provided by AI services such as OpenAI are <strong class="bold">stateless</strong>, meaning they don’t retain any memory of <a id="_idIndexMarker412"/>previous interactions. When you submit a request, the request itself contains all the information the model will use to respond. Any previous requests you submitted have already been forgotten by the model. While this stateless nature allows for many useful applications, some situations require the model to consider more context across multiple requests.</p>
			<p>Despite their immense computing power, most LLMs can only work with small amounts of text, about one page at a time, although this has been increasing recently — the new GPT-4 Turbo, released in November 2023, can receive 128,000 tokens as input, which is about 200 pages of text. Sometimes, however, there are applications that require a model to consider more than 200 pages of text — for example, a model that answers questions about a large collection of academic papers.</p>
			<p>Memories are a powerful way to help Semantic Kernel work by providing more context for your requests. We <a id="_idIndexMarker413"/>add memory to Semantic Kernel by using a concept called <strong class="bold">semantic memory search</strong>, where textual information is represented <a id="_idIndexMarker414"/>by vectors of numbers called <strong class="bold">embeddings</strong>. Since the inception of computing, text has been converted into numbers to help computers compare different texts. For example, the ASCII table that converts letters into numbers was first published in 1963. LLMs convert much more than a single character at a time, using embeddings.</p>
			<p>Embeddings take words and phrases as inputs and output a long list of numbers to represent them. The length of the list varies depending on the embedding model. Importantly, words and phrases with similar meanings are close to each other in a numerical sense; if one were to calculate a distance between the numeric components of the embeddings of two similar phrases, it would be smaller than the distance between two sentences with very different meanings.</p>
			<p>We will see a complete example in the <em class="italic">Embeddings in action</em> section, but for a quick example, the difference between the one-word phrases “<em class="italic">queen</em>” and “<em class="italic">king</em>” is much smaller than the difference between the one-word phrases “<em class="italic">camera</em>” and “<em class="italic">dog.</em>”</p>
			<p>Let’s take a deeper look.</p>
			<h2 id="_idParaDest-125"><a id="_idTextAnchor124"/>How does semantic memory work?</h2>
			<p>Embeddings are a way of representing words or other data as vectors in a high-dimensional <a id="_idIndexMarker415"/>space. Embeddings are useful for AI models because they can capture the meaning and context of words or data in a way that computers can process. An embedding model takes a sentence, paragraph, or even some pages of text and outputs the corresponding embedding vector:</p>
			<div><div><img src="img/B21826_06_1.jpg" alt="Figure 6.1 – Embedding model" width="1189" height="282"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.1 – Embedding model</p>
			<p>The current highest-performing embedding model that OpenAI makes available to users is called <code>text-embedding-3-large</code> and can convert an input of up to 8,191 tokens (about 12 pages of text) into a vector of 3,072 dimensions represented by real numbers.</p>
			<p>OpenAI also makes additional embedding models available at different prices and performance points, such as <code>text-embedding-3-small</code> and <code>text-embedding-ada-2</code>. At the time of writing, the <code>text-embedding-3-small</code> model offers better performance than the <code>text-embedding-ada-2</code> model, and it’s five times cheaper.</p>
			<p>You, as a developer, can store text data (including user requests and responses provided by the AI service) as embedding vectors. It’s important to know that this will not necessarily make the data smaller. For a given embedding model, embedding vectors will always be the same length. For example, for the <code>text-embedding-3-large</code> model, the embedding vector length is always 3,072. If you store the word “<em class="italic">No</em>” using this model, it will <a id="_idIndexMarker416"/>use a vector of 3,072 real numbers that would take 12,228 bytes of memory, a lot more than the string “<code>No</code>”, which can usually be stored in two bytes. On the other hand, if you embed 12 pages of text, their embedding vector length will also be 3,072 and take 12,228 bytes of memory.</p>
			<p>You can use embeddings to recall context that has been given to your application long before a request is made. For example, you could store all the chats you have had with a user in a database. If the user told you months ago that their favorite city is Paris in France, this information can be saved. Later, when the user asks what the biggest attraction is in the city they like the most, you can search for their favorite city in the database you created.</p>
			<p class="callout-heading">How are vector databases different from KernelContext?</p>
			<p class="callout">In previous chapters, we used variables of the type <code>KernelContext</code> to pass information <a id="_idIndexMarker417"/>to functions. A <code>KernelContext</code> variable can be serialized to disk, and therefore, you could use it to store and remember things that your application has already been told.</p>
			<p class="callout">The difference is that a <code>KernelContext</code> variable is a collection of key/value pairs. For each piece of information you store, you have to provide a key, and later, you have to use the same key to retrieve it. Vector databases, on the other hand, retrieve information by similarity, so you can retrieve a piece of information even if you don’t know the key used to store it.</p>
			<p class="callout">Another difference is that, if you want, a vector database can return just the subset of information that is similar to the information you requested, while if you have a <code>KernelContext</code> variable, you need to keep all information available all the time, which may cause performance and capacity issues when you have a lot of information.</p>
			<p>As the user chats with your application, you can store each command the user types in the chat as an <a id="_idIndexMarker418"/>embedding, its numerical representation. Then, when the user enters a new command, you can search through everything the user has ever typed before by comparing the embeddings of what the user just entered to things that they have entered before.</p>
			<p>Because the application is using embedding representations that encode meaning, the user may have said “<em class="italic">my favorite city is Paris</em>” months ago and may ask “<em class="italic">what’s the biggest attraction in the city I like the most</em>” now. A string search would not find a match between “<em class="italic">city I like the most</em>” and “<em class="italic">favorite city</em>,” but these two sentences will have embedding vectors that are close to each other, and a semantic search would return “<em class="italic">favorite city</em>” as a close match for “<em class="italic">city I like the most</em>.” In this case, a close match is exactly what you need.</p>
			<p>Let’s see how to create embeddings with an example.</p>
			<h2 id="_idParaDest-126"><a id="_idTextAnchor125"/>Embeddings in action</h2>
			<p>This subsection only has Python code, because OpenAI does not provide a C# API and we would <a id="_idIndexMarker419"/>need to call the REST API directly. This subsection will show embedding values to help you understand the embedding concepts, but you don’t need to implement the code to understand it.</p>
			<p>To start, we will need to import some libraries. In Python, linear algebra calculations are in the <code>numpy</code> library, so we need to import that:</p>
			<pre class="source-code">
from openai import OpenAI
from typing import Tuple
import numpy as np
from numpy.linalg import norm
import os</pre>			<p>Here, we will generate embeddings for three sentences and compare them to one another.</p>
			<p>First, we will write a function (<code>get_embedding</code>) that generates the embeddings:</p>
			<pre class="source-code">
def get_embedding(text: str) -&gt; Tuple[float]:
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    response = client.embeddings.create(
        input=text,
        model="text-embedding-3-small"
    )
    return response.data[0].embedding</pre>			<p>The preceding <a id="_idIndexMarker420"/>function is a straightforward function call, simply instantiating a connection to OpenAI and calling the <code>embeddings.create</code> method, using the <code>text-embedding-3-small</code> model.</p>
			<p>Then, to compare <a id="_idIndexMarker421"/>how similar one embedding is to another, we will use a <code>0.0</code> and <code>1.0</code>, with <code>1.0</code> meaning they are very similar.</p>
			<pre class="source-code">
def similarity(A: np.array, B: np.array) -&gt; float:
    # compute cosine similarity
    cosine = np.dot(A,B)/(norm(A)*norm(B))
    return cosine</pre>			<p>The last step is to call the functions with the phrases we want to check:</p>
			<pre class="source-code">
if __name__ == "__main__":
    load_dotenv()
    king = get_embedding("The king has been crowned")
    queen = get_embedding("The queen has been crowned")
    linkedin = get_embedding("LinkedIn is a social media platform for professionals")
    print(similarity(king, queen))
    print(similarity(king, linkedin))
    print(similarity(queen, linkedin))</pre>			<p>The three phrases are <code>"The king has been crowned"</code>, <code>"The queen has been crowned"</code>, and <code>"LinkedIn is a social media platform for professionals"</code>. We expect the first and second phrases to be similar, and both to be different from the third phrase.</p>
			<p>As expected, this is <a id="_idIndexMarker422"/>what we get, remembering that the numbers go between <code>0.0</code> and <code>1.0</code>, with <code>0.0</code> meaning dissimilar and <code>1.0</code> meaning perfect match:</p>
			<pre class="console">
0.8684853246664367
0.028215574794606412
0.046607036099519175</pre>			<p>If you want to see the embeddings themselves, you can print them, remembering that they use 1,536 real numbers for the <code>text-embedding-3-small</code> model.</p>
			<p>The following code prints the first 10 embedding values:</p>
			<pre class="source-code">
    for i in range(0, 10):
        print(king[i])</pre>			<p>Here’s the result:</p>
			<pre class="console">
-0.009829566814005375
-0.009656181558966637
0.024287164211273193
0.01408415473997593
-0.03662413731217384
-0.0040411921218037605
-0.00032176158856600523
0.046813808381557465
-0.03235621005296707
-0.04099876061081886</pre>			<p>Now that <a id="_idIndexMarker423"/>we understand a little more about how embeddings work, let’s see how to use them with LLMs.</p>
			<h1 id="_idParaDest-127"><a id="_idTextAnchor126"/>Using memory within chats and LLMs</h1>
			<p>As we have seen before, models have a size limit called a context window. The size limit includes both the prompt with the user request and the response. The default context window for <a id="_idIndexMarker424"/>a model such as GPT-3.5, for example, is 4,096 bytes, meaning <a id="_idIndexMarker425"/>that both your prompt, including the user request, and the answer that GPT-3.5 provides can have at most 4,096 bytes; otherwise, you will get an error, or the response will cut off in the middle.</p>
			<p>If your <a id="_idIndexMarker426"/>application uses a lot of text data, for example, a 10,000-page operating manual, or allows people to search and ask questions about a <a id="_idIndexMarker427"/>database of hundreds of documents with each one having 50 pages, you need to find a way of including just the relevant portion of this large dataset with your prompt. Otherwise, the prompt alone could be larger than the context window, resulting in an error, or the remaining context window could be so short that there would be no space for the model to provide a good answer.</p>
			<p>One way in which you could work around this problem is by summarizing each page into a shorter paragraph and then generating an embedding vector for each summary. Instead of including all the pages in your prompt, you can use something such as cosine similarity to search for the relevant pages by comparing their embeddings with the request embeddings, and then include only the summaries of the relevant pages in the prompt, saving a lot of space.</p>
			<p>Another reason to use memory is to keep data between sessions, or even between prompts. For example, as we suggested in the <em class="italic">How does semantic memory work?</em> section, your user may have told you that their favorite city is Paris, and when they ask for a guide for their favorite city, you don’t need to ask again; you just need to search for their favorite city.</p>
			<p>To find the data in the memory that is relevant to our prompt, we could use something such as the cosine distance shown in the previous section. In practice, the Semantic Kernel SDK <a id="_idIndexMarker428"/>already provides you with a search function, so you <a id="_idIndexMarker429"/>don’t need to implement it yourself. In addition, you <a id="_idIndexMarker430"/>can use several third-party vector databases, each <a id="_idIndexMarker431"/>with its own search functions.</p>
			<p>Here’s a list of all databases that you can use out of the box:</p>
			<table id="table001-2" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Database name</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Python</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">C#</strong></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Azure Cosmos DB for MongoDB vCore</p>
						</td>
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p>✅</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Azure AI Search</p>
						</td>
						<td class="No-Table-Style">
							<p>✅</p>
						</td>
						<td class="No-Table-Style">
							<p>✅</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Azure PostgreSQL Server</p>
						</td>
						<td class="No-Table-Style">
							<p>✅</p>
						</td>
						<td class="No-Table-Style"/>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Chroma</p>
						</td>
						<td class="No-Table-Style">
							<p>✅</p>
						</td>
						<td class="No-Table-Style">
							<p>✅</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>DuckDB</p>
						</td>
						<td class="No-Table-Style">
							<p>✅</p>
						</td>
						<td class="No-Table-Style"/>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Milvus</p>
						</td>
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p>✅</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>MongoDB Atlas Vector Search</p>
						</td>
						<td class="No-Table-Style">
							<p>✅</p>
						</td>
						<td class="No-Table-Style">
							<p>✅</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Pinecone</p>
						</td>
						<td class="No-Table-Style">
							<p>✅</p>
						</td>
						<td class="No-Table-Style">
							<p>✅</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>PostgreSQL</p>
						</td>
						<td class="No-Table-Style">
							<p>✅</p>
						</td>
						<td class="No-Table-Style">
							<p>✅</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Qdrant</p>
						</td>
						<td class="No-Table-Style">
							<p>✅</p>
						</td>
						<td class="No-Table-Style"/>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Redis</p>
						</td>
						<td class="No-Table-Style">
							<p>✅</p>
						</td>
						<td class="No-Table-Style"/>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>SQLite</p>
						</td>
						<td class="No-Table-Style">
							<p>✅</p>
						</td>
						<td class="No-Table-Style"/>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Weaviate</p>
						</td>
						<td class="No-Table-Style">
							<p>✅</p>
						</td>
						<td class="No-Table-Style">
							<p>✅</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 6.1 — Vector databases and compatibility with Semantic Kernel</p>
			<p>In <a id="_idIndexMarker432"/>addition to the databases listed, there’s another one, called <code>VolatileMemoryStore</code>, which <a id="_idIndexMarker433"/>represents the RAM of <a id="_idIndexMarker434"/>the machine you’re running your code on. That database is <a id="_idIndexMarker435"/>not persistent, and its contents are discarded <a id="_idIndexMarker436"/>when the code finishes running, but it’s fast and free and can be easily used during development.</p>
			<h2 id="_idParaDest-128"><a id="_idTextAnchor127"/>Using memory with Microsoft Semantic Kernel</h2>
			<p>In the following example, we will store some information about the user and then use the <code>TextMemorySkill</code> core skill to retrieve it directly inside a prompt. Core skills are skills that <a id="_idIndexMarker437"/>come out of the box with Semantic Kernel. <code>TextMemorySkill</code> has functions to put text into memory and retrieve it.</p>
			<p>In the <a id="_idIndexMarker438"/>following example, our use case will be of a user who tells us their favorite city and favorite activity. We will save those to memory and then we will retrieve them and provide an itinerary based on the saved information.</p>
			<p>We start by importing the libraries that we usually import, plus a few memory libraries that will be described later.</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Python</p>
			<pre class="source-code">
import asyncio
import semantic_kernel as sk
from semantic_kernel.connectors.ai.open_ai import OpenAITextEmbedding, OpenAIChatCompletion, OpenAIChatPromptExecutionSettings
from semantic_kernel.functions import KernelArguments, KernelFunction
from semantic_kernel.prompt_template import PromptTemplateConfig
from semantic_kernel.utils.settings import openai_settings_from_dot_env
<strong class="bold">from semantic_kernel.memory.volatile_memory_store import VolatileMemoryStore</strong>
<strong class="bold">from semantic_kernel.memory.semantic_text_memory import SemanticTextMemory</strong>
<strong class="bold">from semantic_kernel.core_plugins.text_memory_plugin import TextMemoryPlugin</strong></pre>			<p class="SC---Heading" lang="en-US" xml:lang="en-US">C#</p>
			<pre class="source-code">
using Microsoft.SemanticKernel;
using Microsoft.SemanticKernel.Memory;
<strong class="bold">using Microsoft.SemanticKernel.Plugins.Memory;</strong>
using Microsoft.SemanticKernel.Connectors.OpenAI;
#pragma warning disable SKEXP0003, SKEXP0011, SKEXP0052</pre>			<p>Note that <a id="_idIndexMarker439"/>the memory functions in Python <a id="_idIndexMarker440"/>are asynchronous, so we must include the <code>asyncio</code> library. Also, at the time of writing, the memory functions in C# are marked as experimental, so you have to disable the experimental warnings with the <code>#</code><code>pragma</code> command.</p>
			<p>Now, let’s create a kernel:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Python</p>
			<pre class="source-code">
def create_kernel() -&gt; tuple[sk.Kernel, OpenAITextEmbedding]:
    api_key, org_id =  openai_settings_from_dot_env()
    kernel = sk.Kernel()
    gpt = OpenAIChatCompletion(ai_model_id="gpt-4-turbo-preview", api_key=api_key, org_id=org_id, service_id="gpt4")
    emb = OpenAITextEmbedding(ai_model_id="text-embedding-ada-002", api_key=api_key, org_id=org_id, service_id="emb")
    kernel.add_service(emb)
    kernel.add_service(gpt)
    return kernel, emb
async def main():
    kernel, emb = create_kernel()
    memory = SemanticTextMemory(storage=VolatileMemoryStore(), embeddings_generator=emb)
    kernel.add_plugin(TextMemoryPlugin(memory), "TextMemoryPlugin")</pre>			<p class="SC---Heading" lang="en-US" xml:lang="en-US">C#</p>
			<pre class="source-code">
var builder = Kernel.CreateBuilder();
builder.AddOpenAIChatCompletion("gpt-4-turbo-preview", apiKey, orgId);
var kernel = builder.Build();
var memoryBuilder = new MemoryBuilder();
memoryBuilder.WithMemoryStore(new VolatileMemoryStore());
memoryBuilder.WithOpenAITextEmbeddingGeneration("text-embedding-3-small", apiKey);
var memory = memoryBuilder.Build();</pre>			<p>Note that we added three items to our kernel:</p>
			<ul>
				<li>An embedding model, which will help us load things into memory. For C#, we can use <code>text-embedding-3-small</code>, but at the time of writing, even though Python can use <code>text-embedding-3-small</code> as we did in the previous section, the core Python plugins only work with model <code>text-embedding-ada-002</code>.</li>
				<li>Memory storage; in this case, <code>VolatileMemoryStore</code>, which just stores data temporarily in your computer’s RAM.</li>
				<li>A GPT model to generate the itinerary; we’re using GPT-4</li>
			</ul>
			<p>Also, note <a id="_idIndexMarker441"/>that in C#, the memory and the <a id="_idIndexMarker442"/>kernel are built in separate commands, while in Python, they are all built together.</p>
			<p>Now, let’s create a function that adds data to the memory:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Python</p>
			<pre class="source-code">
 async def add_to_memory(memory: SemanticTextMemory, id: str, text: str):
    await memory.save_information(collection="generic", id=id, text=text)</pre>			<p class="SC---Heading" lang="en-US" xml:lang="en-US">C#</p>
			<pre class="source-code">
const string MemoryCollectionName = "default";
await memory.SaveInformationAsync(MemoryCollectionName, id: "1", text: "My favorite city is Paris");
await memory.SaveInformationAsync(MemoryCollectionName, id: "2", text: "My favorite activity is visiting museums");</pre>			<p>The function to add data to memory simply calls <code>memory.save_information</code> in Python and <code>memory.SaveInformationAsync</code> in C#. You can keep different groups of information separate by using collections, but in our simple case, we’re just going to use <code>"generic"</code> for Python and <code>"default"</code> for C#, as those are the default collections for the plugins. The <code>id</code> parameter does not have to mean anything, but it must be unique by item. If you save multiple items using the same <code>id</code> parameter, the last saved item will overwrite the previous ones. It’s common to generate GUIDs to ensure some level of uniqueness, but if you are just going to add a few items manually, you can manually ensure that the ids are different.</p>
			<p>We can <a id="_idIndexMarker443"/>now create a function that <a id="_idIndexMarker444"/>generates a travel itinerary:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Python</p>
			<pre class="source-code">
async def tour(kernel: sk.Kernel) -&gt; KernelFunction:
    prompt = """
    Information about me, from previous conversations:
    - {{$city}} {{recall $city}}
    - {{$activity}} {{recall $activity}}
    """.strip()
    execution_settings = kernel.get_service("gpt4").instantiate_prompt_execution_settings(service_id="gpt4")
    execution_settings.max_tokens = 4000
    execution_settings.temperature = 0.8
    prompt_template_config = PromptTemplateConfig(template=prompt, execution_settings=execution_settings)
    chat_func = kernel.add_function(
        function_name="chat_with_memory",
        plugin_name="TextMemoryPlugin",
        prompt_template_config=prompt_template_config,
    )
    return chat_func</pre>			<p class="SC---Heading" lang="en-US" xml:lang="en-US">C#</p>
			<pre class="source-code">
kernel.ImportPluginFromObject(new TextMemoryPlugin(memory));
const string prompt = @"
Information about me, from previous conversations:
- {{$city}} {{recall $city}}
- {{$activity}} {{recall $activity}}
Generate a personalized tour of activities for me to do when I have a free day in my favorite city. I just want to do my favorite activity.
";
var f = kernel.CreateFunctionFromPrompt(prompt, new OpenAIPromptExecutionSettings { MaxTokens = 2000, Temperature = 0.8 });
var context = new KernelArguments();
context["city"] = "What is my favorite city?";
context["activity"] = "What is my favorite activity?";
context[TextMemoryPlugin.CollectionParam] = MemoryCollectionName;</pre>			<p><code>TextMemoryPlugin</code> gives the ability to use <code>{{recall $question}}</code> to retrieve the contents of the memory inside a prompt without you needing to write any code.</p>
			<p>For example, assume that we loaded <code>My favorite city is Paris</code> in our memory. When <a id="_idIndexMarker445"/>we load the <code>$city</code> variable <a id="_idIndexMarker446"/>with <code>"What's my favorite city"</code> and write <code>{{$city}} {{recall $city}}</code> inside the prompt, Semantic Kernel will replace that line with <code>"What's my favorite city? My favorite city is Paris"</code> inside the prompt.</p>
			<p class="callout-heading">Storing data in memory</p>
			<p class="callout">Note that we didn’t use a meaningful key name when storing the data in memory (we used <code>"1"</code> and <code>"2"</code>). You also don’t need to classify the information before storing it. Some applications simply store everything as they comes, while others use a semantic function to ask Semantic Kernel whether the user input contains personalization information and store it in cases where it does.</p>
			<p>Now, let’s load the memory and call the prompt:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Python</p>
			<pre class="source-code">
    await add_to_memory(memory, id="1", text="My favorite city is Paris")
    await add_to_memory(memory, id="2", text="My favorite activity is visiting museums")
    f = await tour(kernel)
    args = KernelArguments()
    args["city"] = "My favorite city is Paris"
    args["activity"] = "My favorite activity is visiting museums"
    answer = await kernel.invoke(f, arguments=args)
    print(answer)
if __name__ == "__main__":
    asyncio.run(main())</pre>			<p class="SC---Heading" lang="en-US" xml:lang="en-US">C#</p>
			<pre class="source-code">
await memory.SaveInformationAsync(MemoryCollectionName, id: "1", text: "My favorite city is Paris");
await memory.SaveInformationAsync(MemoryCollectionName, id: "2", text: "My favorite activity is visiting museums");
var result = await f.InvokeAsync(kernel, context);
Console.WriteLine(result);</pre>			<p>In the <a id="_idIndexMarker447"/>code, we load the information <a id="_idIndexMarker448"/>to memory using the <code>add_to_memory</code> function and immediately call our semantic function <code>f</code>. If you are using any memory store other than <code>VolatileMemoryStore</code>, you don’t need to implement <a id="_idIndexMarker449"/>these two steps in the same session. We will see an example of persisting memory in our <strong class="bold">RAG</strong> (<strong class="bold">retrieval-augmented generation</strong>) example in <a href="B21826_07.xhtml#_idTextAnchor132"><em class="italic">Chapter 7</em></a>.</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Results</p>
			<p>Note that the model recalled that the user’s favorite city is Paris and that their favorite activity is going to museums:</p>
			<pre class="console">
Given your love for Paris and visiting museums, here's a personalized itinerary for a fulfilling day exploring some of the most iconic and enriching museums in the City of Light:
**Morning: Musée du Louvre**
Start your day at the Louvre, the world's largest art museum and a historic monument in Paris. Home to thousands of works of art, including the Mona Lisa and the Venus de Milo, the Louvre offers an unparalleled experience for art lovers. Arrive early to beat the crowds and spend your morning marveling at the masterpieces from across the world. Don't forget to walk through the Tuileries Garden nearby for a peaceful stroll.
**Afternoon: Musée d'Orsay**
Next, head to the Musée d'Orsay, located on the left bank of the Seine. Housed in the former Gare d'Orsay, a Beaux-Arts railway station, the museum holds the largest collection of Impressionist and Post-Impressionist masterpieces in the world. Spend your afternoon admiring works by Monet, Van Gogh, Renoir, and many others.
**Late Afternoon: Musée de l'Orangerie**
Conclude your day of museum visits at the Musée de l'Orangerie, located in the corner of the Tuileries Gardens. This gallery is famous for housing eight large Water Lilies murals by Claude Monet, displayed in two oval rooms offering a breathtaking panorama of Monet's garden-inspired masterpieces. The museum also contains works by Cézanne, Matisse, Picasso, and Rousseau, among others.
**Evening: Seine River Walk and Dinner**
After an enriching day of art, take a leisurely walk along the Seine River. The riverside offers a picturesque view of Paris as the city lights begin to sparkle. For dinner, choose one of the numerous bistros or restaurants along the river or in the nearby neighborhoods to enjoy classic French cuisine, reflecting on the beautiful artworks and memories created throughout the day.</pre>			<p>Note that <a id="_idIndexMarker450"/>if you pay for a subscription <a id="_idIndexMarker451"/>to any of the vector database providers listed in <em class="italic">Table 6.1</em>, you can simply replace the <code>VolatileMemoryStore</code> constructor with their constructor; for example, if you are using Pinecone, you will use <code>Pinecone(apiKey)</code>, and the memory will be persisted in that database and available to the user the next time they run your application. We will see an example with Azure AI Search in <a href="B21826_07.xhtml#_idTextAnchor132"><em class="italic">Chapter 7</em></a>.</p>
			<p>Now, let’s see how we can use memory in a chat with a user.</p>
			<h2 id="_idParaDest-129"><a id="_idTextAnchor128"/>Using memory in chats</h2>
			<p>Memory is typically used in chat-based applications. All the applications that we built in earlier <a id="_idIndexMarker452"/>chapters were <em class="italic">one-shot</em> — all the information required to complete a task is part of the request submitted by the user plus whatever <a id="_idIndexMarker453"/>modifications we make to the prompt in our own code, for example, by including the user-submitted prompt inside a variable in <code>skprompt.txt</code>, or modifying their prompt using string manipulation. All questions and answers that happened before are ignored. We say that the AI service is <em class="italic">stateless</em>.</p>
			<p>Sometimes, however, we want the AI service to remember requests that have been made before. For example, if I ask the app about the largest city in India by population, the application will respond that it is <em class="italic">Mumbai</em>. If I then ask “<em class="italic">how is the temperature there in the summer</em>,” I would expect the application to realize that I’m asking about the temperature in Mumbai, even though my second prompt does not include the name of the city.</p>
			<p>As we mentioned in <a href="B21826_01.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, the brute-force solution is to simply repeat the whole history of the chat with every new request. Therefore, when the second request is submitted by the user, we could silently attach their first request and the response our AI service provided to it and then submit everything together again to the AI service.</p>
			<p>Let’s see how to do this next. We start with the usual imports:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Python</p>
			<pre class="source-code">
import asyncio
import semantic_kernel as sk
from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion, OpenAIChatPromptExecutionSettings
from semantic_kernel.functions import KernelFunction
from semantic_kernel.prompt_template import PromptTemplateConfig, InputVariable
from semantic_kernel.core_plugins import ConversationSummaryPlugin
from semantic_kernel.contents.chat_history import ChatHistory
from semantic_kernel.utils.settings import openai_settings_from_dot_env</pre>			<p class="SC---Heading" lang="en-US" xml:lang="en-US">C#</p>
			<pre class="source-code">
using Microsoft.SemanticKernel;
using Microsoft.SemanticKernel.Connectors.OpenAI;
using Microsoft.SemanticKernel.Plugins.Core;
#pragma warning disable SKEXP0003, SKEXP0011, SKEXP0052, SKEXP0050</pre>			<p>Note that <a id="_idIndexMarker454"/>in C#, since several components of the Semantic <a id="_idIndexMarker455"/>Kernel package are still in pre-release, you need to disable the experimental warnings using a <code>#</code><code>pragma</code> directive.</p>
			<p>After importing the libraries, we create the kernel:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Python</p>
			<pre class="source-code">
def create_kernel() -&gt; sk.Kernel:
    api_key, org_id =  openai_settings_from_dot_env()
    kernel = sk.Kernel()
    gpt = OpenAIChatCompletion(ai_model_id="gpt-4-turbo-preview", api_key=api_key, org_id=org_id, service_id="gpt4")
    kernel.add_service(gpt)
    # The following execution settings are used for the ConversationSummaryPlugin
    execution_settings = OpenAIChatPromptExecutionSettings(
        service_id="gpt4", max_tokens=ConversationSummaryPlugin._max_tokens, temperature=0.1, top_p=0.5)
    prompt_template_config = PromptTemplateConfig(
        template=ConversationSummaryPlugin._summarize_conversation_prompt_template,
        description="Given a section of a conversation transcript, summarize it",
        execution_settings=execution_settings,
    )
    # Import the ConversationSummaryPlugin
    kernel.add_plugin(
        ConversationSummaryPlugin(kernel=kernel, prompt_template_config=prompt_template_config),
        plugin_name="ConversationSummaryPlugin",
    )
    return kernel</pre>			<p class="SC---Heading" lang="en-US" xml:lang="en-US">C#</p>
			<pre class="source-code">
var (apiKey, orgId) = Settings.LoadFromFile();
var builder = Kernel.CreateBuilder();
builder.AddOpenAIChatCompletion("gpt-4-turbo-preview", apiKey, orgId);
var kernel = builder.Build();
kernel.ImportPluginFromObject(new ConversationSummaryPlugin());</pre>			<p>Our kernel <a id="_idIndexMarker456"/>just needs a chat completion service. I’m using GPT-4, but GPT-3.5 also works. I am also adding <code>ConversationSummaryPlugin</code>, which will be used in the last subsection of this chapter, <em class="italic">Reducing history size with summarization</em>. We will explain it in detail later, but as the <a id="_idIndexMarker457"/>name implies, it summarizes conversations.</p>
			<p>Now, let’s create the main chat function:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Python</p>
			<pre class="source-code">
async def create_chat_function(kernel: sk.Kernel) -&gt; KernelFunction:
    # Create the prompt
    prompt = """
    User: {{$request}}
    Assistant:  """
    # These execution settings are tied to the chat function, created below.
    execution_settings = kernel.get_service("gpt4").instantiate_prompt_execution_settings(service_id="gpt4")
    chat_prompt_template_config = PromptTemplateConfig(
        template=prompt,
        description="Chat with the assistant",
        execution_settings=execution_settings,
        input_variables=[
            InputVariable(name="request", description="The user input", is_required=True),
            InputVariable(name="history", description="The history of the conversation", is_required=True),
        ],
    )
    # Create the function
    chat_function = kernel.add_function(
        prompt=prompt,
        plugin_name="Summarize_Conversation",
        function_name="Chat",
        description="Chat with the assistant",
        prompt_template_config=chat_prompt_template_config,)
    return chat_function</pre>			<p class="SC---Heading" lang="en-US" xml:lang="en-US">C#</p>
			<pre class="source-code">
const string prompt = @"
Chat history:
{{$history}}
User: {{$userInput}}
Assistant:";
var executionSettings = new OpenAIPromptExecutionSettings {MaxTokens = 2000,Temperature = 0.8,};
var chatFunction = kernel.CreateFunctionFromPrompt(prompt, executionSettings);
var history = "";
var arguments = new KernelArguments();
arguments["history"] = history;</pre>			<p>Now, let’s <a id="_idIndexMarker458"/>write the <a id="_idIndexMarker459"/>main loop of our program:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Python</p>
			<pre class="source-code">
async def main():
    kernel = create_kernel()
    history = ChatHistory()
    chat_function = await create_chat_function(kernel)
    while True:
        try:
            request = input("User:&gt; ")
        except KeyboardInterrupt:
            print("\n\nExiting chat...")
            return False
        except EOFError:
            print("\n\nExiting chat...")
            return False
        if request == "exit":
            print("\n\nExiting chat...")
            return False
        result = await kernel.invoke(
            chat_function,
            request=request,
            history=history,
        )
        # Add the request to the history
        history.add_user_message(request)
        history.add_assistant_message(str(result))
        print(f"Assistant:&gt; {result}")
if __name__ == "__main__":
    asyncio.run(main())</pre>			<p class="SC---Heading" lang="en-US" xml:lang="en-US">C#</p>
			<pre class="source-code">
var chatting = true;
while (chatting) {
    Console.Write("User: ");
    var input = Console.ReadLine();
    if (input == null) {break;}
    input = input.Trim();
    if (input == "exit") {break;}
    arguments["userInput"] = input;
    var answer = await chatFunction.InvokeAsync(kernel, arguments);
    var result = $"\nUser: {input}\nAssistant: {answer}\n";
    history += result;
    arguments["history"] = history;
    // Show the bot response
    Console.WriteLine(result);
}</pre>			<p>The main loop of our program runs until the user enters the word <code>"exit"</code>. Otherwise, we submit the user request to the AI service, collect its answer, and add both to the <code>history</code> variable, which we also submit as part of our request.</p>
			<p>Although this solves the problem of always having the whole history, it becomes prohibitively <a id="_idIndexMarker460"/>expensive as prompts start to get larger and larger. When the user submits their request number <em class="italic">N</em>, the <code>history</code> variable contains their <a id="_idIndexMarker461"/>requests <code>1</code>, …, <em class="italic">N</em>-<code>1</code>, and the chatbot answers <code>1</code>, …, <em class="italic">N</em>-<code>1</code> along with it. For large <em class="italic">N</em>, in addition to being expensive, this can exceed the context window of the AI service and you will get an error.</p>
			<p>The solution is to only pass a summary of the history to the AI service. It’s fortunate that summarizing conversations is something that even older models can do very well. Let’s see how to easily do it with Semantic Kernel.</p>
			<h2 id="_idParaDest-130"><a id="_idTextAnchor129"/>Reducing history size with summarization</h2>
			<p>If you want to reduce the prompt without losing much context, you can use the AI service to <a id="_idIndexMarker462"/>summarize what has already happened in the conversation. To do so, you can use the <code>SummarizeConversation</code> function of <code>ConversationSummaryPlugin</code> that we imported <a id="_idIndexMarker463"/>when creating the kernel. Now, instead of repeating the whole history in the prompt, the summary will have up to 1,000 tokens regardless of the conversation size, which should be plenty for most use cases. To summarize the history in the <code>$history</code> variable, simply call <code>{{ConversationSummaryPlugin.SummarizeConversation $history}}</code> in your prompt.</p>
			<p>It is still possible to lose details after too much summarization. If you try to summarize 1,000 pages in 1,000 words, something will be lost. To prevent this problem, most applications have limits on how long conversations can go. For example, at the time of writing, Microsoft Copilot conversations have a limit of 20 interactions, and you must restart the conversation (with an empty memory) after that.</p>
			<p>The change to the code is shown as follows; you just need to change the contents of the <code>prompt</code> variable. The change will add a conversation summary to the prompt, which will remind the LLM of everything that went on before. The summary will not be displayed to the user.</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Python</p>
			<pre class="source-code">
prompt = """
Chat history:
<strong class="bold">{{ConversationSummaryPlugin.SummarizeConversation $history}}</strong>
User: {{$userInput}}
Assistant:
"""</pre>			<p class="SC---Heading" lang="en-US" xml:lang="en-US">C#</p>
			<pre class="source-code">
const string prompt = @"
Chat history:
<strong class="bold">{{ConversationSummaryPlugin.SummarizeConversation $history}}</strong>
User: {{$userInput}}
ChatBot:";</pre>			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Results</p>
			<pre class="console">
User:&gt; What is the largest city by population in Western Europe?
Assistant:&gt; The largest city by population in Western Europe is London, United Kingdom.</pre>			<p>Note that <a id="_idIndexMarker464"/>in the next question, I’ll use the word <code>there</code> to mean London. Since the history summary is included as part of the conversation, even though my next question doesn’t explicitly name London, the prompt that goes to the AI contains that information:</p>
			<pre class="console">
User:&gt; Are there any famous people who lived there?
Assistant:&gt; Yes, London has been home to many famous people throughout history. Some notable individuals include:
1. **William Shakespeare** - The renowned playwright and poet lived in London for most of his career.
2. **Charles Dickens** - The famous novelist, known for works like "Oliver Twist" and "A Christmas Carol," spent much of his life in London.
3. **Winston Churchill** - The iconic British Prime Minister during World War II was born in Woodstock but lived and worked in London for much of his life.</pre>			<p>Note that the answer to the previous question was correct. Shakespeare, Dickens, and Churchill <a id="_idIndexMarker465"/>all lived in London. Now, I’ll refer to Shakespeare just by its position on the list and to London simply as <code>that city</code>, and because we’re keeping track of history, the kernel will know that I mean Shakespeare and London:</p>
			<pre class="console">
User:&gt; What is a famous play from the first one set in that city?
Assistant:&gt; A famous play from William Shakespeare that is set in London is "Henry V." This historical play, part of Shakespeare's series on the English monarchs, includes scenes that are set in London, notably the English court. It portrays the events before and after the Battle of Agincourt during the Hundred Years' War, with significant portions reflecting on the life and times in London during the early 15th century.</pre>			<p>Again, the AI gets the answer correct. The <code>Henry V</code> play is actually set in London.</p>
			<p>Let’s exit the chat:</p>
			<pre class="console">
User:&gt; exit|
Exiting chat...</pre>			<h1 id="_idParaDest-131"><a id="_idTextAnchor130"/>Summary</h1>
			<p>In this chapter, we learned how to add and retrieve information from memory, and how to easily include the memory in your prompts. LLMs are stateless and limited by their prompt sizes, and in this chapter, we learned techniques to save information between sessions and reduce prompt sizes while still including relevant portions of the conversation in the prompt.</p>
			<p>In the next chapter, we will see how to use a vector database to retrieve a lot more information from memory and use a technique called <strong class="bold">retrieval-augmented generation</strong> (<strong class="bold">RAG</strong>) to organize and present that information in a useful way. This technique is often used in enterprise applications, as you trade off a little bit of the creativity offered by LLMs, but get back additional precision, the ability to show references, and the ability to use a lot of data that you own and have control over.</p>
			<p>For our application, we are going to load thousands of academic articles into a vector database and have Semantic Kernel search for a topic and summarize the research for us.</p>
		</div>
	</div>
</div>


<div><div><div><h1 id="_idParaDest-132" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor131"/>Part 3: Real-World Use Cases</h1>
		</div>
		<div><p>In this part, we see how Semantic Kernel can be used in real-world problems. We learn how using the retrieval-augmented generation (RAG) technique can allow AI models to use large amounts of data, including very recent data that was not available when the AI service was trained. We conclude by learning how to use ChatGPT to distribute an application we wrote to hundreds of millions of users.</p>
			<p>This part includes the following chapters:</p>
			<ul>
				<li><a href="B21826_07.xhtml#_idTextAnchor132"><em class="italic">Chapter 7</em></a>, <em class="italic">Real-World Use Case – Retrieval-Augmented Generation</em></li>
				<li><a href="B21826_08.xhtml#_idTextAnchor143"><em class="italic">Chapter 8</em></a>, <em class="italic">Real-World Use Case – Making Your Application Available on ChatGPT</em></li>
			</ul>
		</div>
		<div><div></div>
		</div>
		<div><div></div>
		</div>
	</div>
</div>
</body></html>