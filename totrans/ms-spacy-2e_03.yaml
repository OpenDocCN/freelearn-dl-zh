- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Extracting Linguistic Features
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提取语言特征
- en: This chapter is a deep dive into the full power of spaCy. You will discover
    the linguistic features, including spaCy’s most used features such as the **part-of-speech**
    ( **POS** ) **tagger** , the **dependency parser** , the **named entity recognizer**
    , and **merging/splitting** features.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章深入探讨了spaCy的全部功能。您将发现语言特征，包括spaCy最常用的功能，如**词性**（**POS**）**标注器**、**依存句法分析器**、**命名实体识别器**和**合并/分割**功能。
- en: First, you’ll learn about the POS tag concept, how the spaCy POS tagger functions,
    and how to place POS tags into your **natural-language understanding** ( **NLU**
    ) applications. Next, you’ll learn a structured way to represent the sentence
    syntax through the dependency parser. You’ll learn about the dependency labels
    of spaCy and how to interpret the spaCy dependency labeler results with revealing
    examples. Then, you’ll learn a very important NLU concept that lies at the heart
    of many **natural language processing** ( **NLP** ) applications— **named entity
    recognition** ( **NER** ). We’ll go over examples of how to extract information
    from the text using NER. Finally, you’ll learn how to merge and split the entities
    you extracted.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您将了解词性标注的概念，spaCy词性标注器的功能以及如何将词性标注放入您的**自然语言理解**（**NLU**）应用中。接下来，您将学习一种结构化的方法来通过依存句法分析器表示句子语法。您将了解spaCy的依存标签以及如何通过揭示性例子来解释spaCy依存标签分析器的结果。然后，您将学习一个非常重要的NLU概念，它是许多**自然语言处理**（**NLP**）应用的核心——**命名实体识别**（**NER**）。我们将通过NER从文本中提取信息的例子进行讲解。最后，您将学习如何合并和分割提取的实体。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: What is POS tagging?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是词性标注（POS tagging）？
- en: Introduction to dependency parsing
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 依存句法分析简介
- en: Introducing NER
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍命名实体识别（NER）
- en: Merging and splitting tokens
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 合并和分割标记
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The code of this chapter can be found at [https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition](https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition)
    .
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在[https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition](https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition)找到。
- en: What is POS tagging?
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是词性标注（POS tagging）？
- en: 'A part of speech is a syntactic category in which every word falls into a category
    according to its function in a sentence. For example, English has nine main categories:
    verb, noun, pronoun, determiner, adjective, adverb, preposition, conjunction,
    and interjection. We can describe the functions of each category as follows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 词性是一个句法类别，每个词都根据其在句子中的功能落入一个类别。例如，英语有九个主要类别：动词、名词、代词、限定词、形容词、副词、介词、连词和感叹词。我们可以如下描述每个类别的功能：
- en: '**Verb** : Expresses an action or a state of being'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动词**：表达动作或存在状态'
- en: '**Noun** : Identifies a person, a place, or a thing, or names a particular
    of one of these (a proper noun)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**名词**：指代人、地点或事物，或命名这些事物中的特定一个（专有名词）'
- en: '**Pronoun** : Can replace a noun or noun phrase'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代词**：可以替换名词或名词短语'
- en: '**Determiner** : Is placed in front of a noun to express a quantity or clarify
    what the noun refers to—briefly, a noun introducer'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**限定词**：置于名词之前，表示数量或明确名词所指的内容——简而言之，名词引入者'
- en: '**Adjective** : Modifies a noun or a pronoun'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**形容词**：修饰名词或代词'
- en: '**Adverb** : Modifies a verb, an adjective, or another adverb'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**副词**：修饰动词、形容词或另一个副词'
- en: '**Preposition** : Connects a noun/pronoun to other parts of the sentence'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**介词**：连接名词/代词与其他句子部分'
- en: '**Conjunction** : Glues words, clauses, and sentences together'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**连词**：将单词、子句和句子粘合在一起'
- en: '**Interjection** : Expresses emotion in a sudden and exclamatory way'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**感叹词**：以突然和感叹的方式表达情感'
- en: This core set of categories, without any language-specific morphological or
    syntactic features, are called **universal tags** . spaCy captures universal tags
    via the **pos_** **feature** .
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这个核心类别集，没有任何语言特定的形态学或句法特征，被称为**通用标签**。spaCy通过**pos_****特征**捕获通用标签。
- en: Throughout the book, we are providing examples in the English language, so we’ll
    only focus on English. Different languages offer different tagsets. You can see
    the label schemes used by each model by selecting the model language at [https://spacy.io/models](https://spacy.io/models)
    and clicking on the **Label** **Scheme** button.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中，我们提供英语语言的例子，因此我们只会关注英语。不同的语言提供不同的标签集。您可以通过选择模型语言在[https://spacy.io/models](https://spacy.io/models)并点击**标签方案**按钮来查看每个模型使用的标签方案。
- en: Each supported language of spaCy admits its own fine-grained tagset and tagging
    scheme. This language specific tagging scheme usually covers morphological features,
    tenses and aspects of verbs, number of nouns (singular/plural), person and number
    information of pronouns (first-, second-, or third-person, singular or plural),
    pronoun type (personal, demonstrative, or interrogative), adjective type (comparative
    or superlative), and so on.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 支持的语言都有自己的细粒度标签集和标注方案。这种语言特定的标注方案通常包括形态学特征、动词的时态和语气、名词的数量（单数/复数）、代词的人称和数量信息（第一、第二或第三人称，单数或复数）、代词类型（人称、指示或疑问）、形容词类型（比较级或最高级）等等。
- en: spaCy supports fine-grained POS tags to answer language-specific needs. The
    **tag_feature** corresponds to the fine-grained tags. Don’t worry if you haven’t
    worked with POS tags before, as you’ll become familiar by practicing with the
    help of our examples. You can also always call **spacy.explain()** on the tags.
    We usually call **spacy.explain()** in two ways, either directly on the tag name
    string or with **token.tag_** .
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 支持细粒度词性标注以满足特定语言的需求。**tag_feature** 对应于细粒度标签。如果你之前没有使用过词性标注，不要担心，通过我们的示例练习，你会变得熟悉。你还可以始终在标签上调用
    **spacy.explain()**。我们通常有两种调用 **spacy.explain()** 的方式，要么直接在标签名称字符串上，要么使用 **token.tag_**。
- en: 'Let’s do that in practice with some coding. By this point, you should already
    have the spaCy library and the English model installed, but if not, you can just
    run **python –m pip install spacy** and then **python –m spacy download en_core_web_sm**
    . All set; let’s get started:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在实践中通过一些编码来实现这一点。到这一点，你应该已经安装了 spaCy 库和英语模型，如果没有，只需运行 **python –m pip install
    spacy** 然后运行 **python –m spacy download en_core_web_sm**。一切准备就绪；让我们开始吧：
- en: 'First, let’s import spaCy and explain what the **NNS** tag means:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们导入 spaCy 并解释一下 **NNS** 标签的含义：
- en: '[PRE0]'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now let’s load the English model and process a sentence:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们加载英语模型并处理一个句子：
- en: '[PRE1]'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can print the text and the POS tag of each token:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以打印出每个标记的文本和词性标注：
- en: '[PRE2]'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can also print the explanation using **token.tag_** as input:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们也可以使用 **token.tag_** 作为输入来打印解释：
- en: '[PRE3]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If you want to know more about POS, you can read more about it at *Eight Parts
    of* *Speech* : [http://www.butte.edu/departments/cas/tipsheets/grammar/parts_of_speech.html](http://www.butte.edu/departments/cas/tipsheets/grammar/parts_of_speech.html)
    .'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于词性的信息，你可以在 *Eight Parts of* *Speech*：[http://www.butte.edu/departments/cas/tipsheets/grammar/parts_of_speech.html](http://www.butte.edu/departments/cas/tipsheets/grammar/parts_of_speech.html)
    上了解更多。
- en: POS tagging offers a very basic syntactic understanding of the sentence. POS
    tags are used in NLU when we want to find the verbs and the nouns in a sentence
    or better disambiguate some words for their meanings (more on this subject soon).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 词性标注提供了对句子非常基本的句法理解。当我们想要在句子中找到动词和名词或更好地消除一些词的意义歧义时，会使用词性标注（关于这个话题很快就会有更多介绍）。
- en: Each word is tagged by a POS tag depending on its context—the other surrounding
    words and their POS tags. POS taggers are sequential statistical models, which
    means that *a tag of a word depends on the word-neighbor tokens, their tags, and
    the word itself* . POS tagging has always been done in different forms. **Sequence-to-sequence
    learning** ( **Seq2seq** ) started with **Hidden Markov Models** ( **HMMs** )
    and evolved to neural network models, usually using **Long Short-Term Memory**
    ( **LSTM** ) cells.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 每个词根据其上下文——其他周围的词及其词性标注——被标注为一个词性标签。词性标注器是顺序统计模型，这意味着“一个词的标签取决于其邻近的词、它们的标签以及这个词本身”。词性标注一直以不同的形式存在。**序列到序列学习**（**Seq2seq**）始于**隐马尔可夫模型**（**HMMs**），并演变为使用**长短期记忆**（**LSTM**）单元的神经网络模型。
- en: Many NLU applications still need to know the word type for better accuracy.
    Syntactic information can be used in a traditional task called **Word-Sense Disambiguation**
    ( **WSD** ). Let’s see how to tackle it with the help of the spaCy tagger in the
    next section.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 许多自然语言理解（NLU）应用仍然需要知道词的类型以获得更高的准确性。句法信息可以用于一个传统的任务，称为**词语消歧**（**WSD**）。让我们在下一节中看看如何使用
    spaCy 标注器来处理它。
- en: Word-Sense Disambiguation (WSD)
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词语消歧（WSD）
- en: 'WSD is a classical NLU problem of deciding in which sense a particular word
    is used in a sentence. A word can have many senses—for instance, consider the
    word bass. Here are some senses we can think of:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 词语消歧是自然语言理解中的一个经典问题，它决定一个特定的词在句子中使用的意义。一个词可以有多个意义——例如，考虑一下“bass”这个词。以下是我们能想到的一些意义：
- en: Bass—sea bass, fish
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低音——海鲈鱼，鱼类
- en: Bass—lowest masculine voice
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低音——最低的男性声音
- en: Bass—male singer with the lowest voice range
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低音——音域最低的男歌手
- en: 'Determining the sense of the word can be crucial in search engines, machine
    translation, and question-answering systems. For the preceding example, bass,
    a POS tagger is unfortunately not much help as the tagger labels all senses with
    a noun tag. We need more than a POS tagger. How about the word *beat* ? Let’s
    have a look at this here:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在搜索引擎、机器翻译和问答系统中，确定词义可能至关重要。对于先前的例子，不幸的是，词性标注器并没有太大的帮助，因为标注器将所有词义都标记为名词。我们需要的不只是一个词性标注器。关于“beat”这个词怎么样？让我们看看这里：
- en: Beat—to strike violently ( **verb** ( **V** ))
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beat—猛烈地打击（**动词**（**V**））
- en: Beat—to defeat someone else in a game or a competition ( **verb** ( **V** ))
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beat—在游戏或比赛中击败某人（**动词**（**V**））
- en: Beat—rhythm in music or poetry ( **noun** ( **N** ))
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beat—音乐或诗歌中的节奏（**名词**（**N**））
- en: Beat—bird wing movement ( **noun** ( **N** ))
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beat—鸟的翅膀运动（**名词**（**N**））
- en: Beat—completely exhausted ( **adjective** ( **ADJ** ))
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beat—完全筋疲力尽（**形容词**（**ADJ**））
- en: Here, POS tagging can help a lot indeed. The *ADJ tag* determines the word sense
    definitely; if the word beat is tagged as ADJ, it identifies the sense *completely
    exhausted* . This is not true for the V and N tags; if the word beat is labeled
    with a V tag, its sense can be to *strike violently* or to *defeat someone else*
    . WSD is an open problem, and many statistical models are proposed. However, if
    you need a quick prototype, you can tackle this problem in some cases (such as
    in the preceding example) with the help of the spaCy tagger.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，词性标注确实能帮上大忙。**形容词标签**决定了词义的确切性；如果“beat”被标记为形容词，它表示的词义是**完全筋疲力尽**。对于动词和名词标签来说则不是这样；如果“beat”被标记为动词标签，它的词义可以是**猛烈地打击**或**击败某人**。词义消歧是一个开放性问题，已经提出了许多统计模型。然而，如果你需要一个快速的原型，你可以在某些情况下（例如在先前的例子中）借助spaCy标签器来解决这个问题。
- en: Verb tense and aspect in NLU applications
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NLU应用中的动词时态和体
- en: In the previous chapter, we used the example of the travel agency application
    where we got the base forms (which are freed from verb tense and aspect) of the
    verbs by using **lemmatization** . In this subsection, we’ll focus on how to use
    the verb tense and aspect information that we lost during the lemmatization process.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们使用了旅行社应用的例子，我们通过使用**词形还原**得到了动词的基础形式（这些形式摆脱了动词的时态和体）。在本小节中，我们将重点介绍如何使用在词形还原过程中丢失的动词时态和体信息。
- en: '**Verb tense** and **aspect** are maybe the most interesting information that
    verbs provide us, telling us when the action happened in time and if the action
    of the verb is finished or ongoing. Tense and aspect together indicate a verb’s
    reference to the current time. English has three basic tenses: past, present,
    and future. A verb tense can express different aspects and forms, such as the
    simple, progressive/continuous, perfect, perfect continuous, and future tenses.
    For instance, in the sentence *I’m eating* , the action *eat* happens in the present
    and is ongoing. Hence, we describe this verb as *present progressive/continuous*
    .'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**动词时态**和**体**可能是动词为我们提供的最有趣的信息，告诉我们动作何时发生以及动作是否完成或正在进行。时态和体一起表明动词对当前时间的参照。英语有三种基本时态：过去、现在和将来。动词时态可以表达不同的体和形式，如简单、进行/连续、完成、完成进行和将来时态。例如，在句子*I’m
    eating*中，动作*eat*发生在现在并且正在进行。因此，我们描述这个动词为**现在进行/连续**。'
- en: 'However, how do we use this information in our travel agency NLU? Consider
    the following customer sentences that can be directed to our NLU application:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们如何在我们的旅行社NLU中使用这些信息？考虑以下可以指向我们的NLU应用的客户句子：
- en: I flew to Rome
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我飞往罗马
- en: I’m flying to Rome
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我正在飞往罗马
- en: I will fly to Rome
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我将飞往罗马
- en: 'In all the sentences, the action is to *fly* : however, only some sentences
    state the intent to make a ticket booking. Let’s imagine these sentences with
    a surrounding context, as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有句子中，动作是**飞**：然而，只有一些句子表明有订票的意图。让我们想象以下带有周围上下文的句子：
- en: I flew to Rome three days ago. I still didn’t get the bill, please send it ASAP.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我三天前飞往罗马。我还没收到账单，请尽快发送。
- en: I’m flying to Rome next week. Can you check flight availability?
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我下周将飞往罗马。你能查一下航班可用性吗？
- en: I will fly to Rome next week. Can you check the flights?
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我下周将飞往罗马。你能查一下航班吗？
- en: 'At a quick glance, the past form of the verb fly doesn’t indicate a booking
    intent. Rather, it directs to either a customer complaint or customer service
    issues. The *infinitive* and *present progressive* forms, on the other hand, point
    to booking intent. We can use the POS tags to filter these intents by filtering
    the sentences that have the **VBG** tag (a verb in present progressive form) or
    the **VB** tag (a verb in base/infinitive form). Let’s do that in the following
    code segment:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 从表面上看，动词 fly 的过去式并不表明预订意图。相反，它指向客户投诉或客户服务问题。另一方面，**不定式**和**现在进行时**形式则指向预订意图。我们可以使用词性标注来通过过滤具有
    **VBG** 标签（现在进行时态的动词）或 **VB** 标签（基本/不定式形式的动词）的句子来过滤这些意图。让我们在下面的代码段中这样做：
- en: '[PRE4]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We have covered one semantic and one morphological task—WSD and tense/aspect
    of verbs. We’ll now continue with another syntactic concept—dependency parsing.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经涵盖了一个语义任务和一个形态学任务——WSD 和动词的时态/体。现在我们将继续另一个句法概念——依存句法分析。
- en: Introduction to dependency parsing
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 依存句法分析简介
- en: With spaCy’s dependency parser, we can *represent the syntactic structure of
    sentences* . In the previous section, we focused on POS tags, which categorize
    words syntactically. While POS tags offer insights into the tags of neighboring
    words, they don’t reveal the relationships between words that are not directly
    adjacent in a sentence.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 spaCy 的依存句法分析器，我们可以**表示句子的句法结构**。在前一节中，我们关注了词性标注，它将词按照句法进行分类。虽然词性标注可以提供关于相邻词标签的见解，但它并没有揭示句子中非直接相邻词之间的关系。
- en: As the name suggests, dependency parsing involves analyzing sentence structures
    via dependencies between the tokens. The dependency parser tags syntactic relations
    between tokens of the sentence and connects tokens that are syntactically related.
    A dependency or a dependency relation is a directed link between two tokens.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名所示，依存句法分析涉及通过标记之间的依存关系来分析句子结构。依存句法分析器为句子中的标记之间的句法关系进行标记，并连接句法相关的标记。依存关系或依存关系是一个两个标记之间的有向链接。
- en: We can visualize the dependency parsing as a tree, as illustrated in *Figure
    3* *.1* .
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将依存句法分析可视化为一棵树，如图 *图 3.1* 所示。
- en: '![Figure 3.1 – The result of running dependency parsing on a sentence](img/B22441_03_01.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.1 – 在句子上运行依存句法分析的结果](img/B22441_03_01.jpg)'
- en: Figure 3.1 – The result of running dependency parsing on a sentence
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1 – 在句子上运行依存句法分析的结果
- en: Let’s learn more about these dependency relations in the next section.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在下一节中了解更多关于这些依存关系的信息。
- en: Dependency relations
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 依存关系
- en: Many statistical methods in NLP revolve around vector representations of words
    and treat a sentence as a sequence of words. As you can see in *Figure 3* *.1*
    , a sentence is more than a sequence of tokens—it has a structure. Every word
    in a sentence has a well-defined role, such as verb, subject, object, and so on;
    hence, sentences have a structure. This structure is used extensively in chatbots,
    question-answering, and machine translation.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: NLP 中的许多统计方法都围绕单词的向量表示展开，并将句子视为单词的序列。如图 *图 3.1* 所示，句子不仅仅是标记的序列——它具有结构。句子中的每个单词都有一个明确定义的角色，例如动词、主语、宾语等；因此，句子具有结构。这种结构在聊天机器人、问答系统和机器翻译中得到广泛应用。
- en: 'The most useful application that first comes to mind is determining the sentence
    object and subject. Again, let’s go back to our travel agency application. Imagine
    a customer is complaining about the service. Compare the two sentences *I forwarded
    you the email* and *You forwarded me the email* ; if we eliminate the stop words
    *I* , *you* , *me* , and *the* , this is what remains:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 最有用的应用之一是确定句子中的宾语和主语。再次，让我们回到我们的旅行社应用程序。想象一下，一位客户正在抱怨服务。比较以下两个句子：“我把邮件转发给你”和“你把邮件转发给我”；如果我们消除了停用词“我”、“你”、“我”和“的”，剩下的就是：
- en: I forwarded you the email -> forwarded email
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我把邮件转发给你 -> 转发邮件
- en: You forwarded me the email -> forwarded email
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你把邮件转发给我 -> 转发邮件
- en: Though the remaining parts of the sentences are identical, the two sentences
    have very different meanings and require different answers. In the first sentence,
    the sentence subject is *I* (then, the answer most probably will start with *you*
    ), and the second sentence’s subject is *you* (which will end up in an *I* answer).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管句子的剩余部分相同，但这两个句子具有非常不同的含义，需要不同的答案。在第一句中，句子主语是**我**（因此，答案很可能会以**你**开头），而第二句的主语是**你**（这将导致以**我**结尾的答案）。
- en: The dependency parser helps us to go deeper into the sentence syntax and semantics,
    going beyond the analysis of the words by themselves. Let’s explore more, starting
    from the dependency relations.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 依存分析器帮助我们更深入地了解句子句法和语义，超越了仅分析单词本身的分析。让我们从依存关系开始探索更多内容。
- en: Syntactic relations
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 句法关系
- en: 'spaCy assigns each token a dependency label, just as with other linguistic
    features such as a lemma or a POS tag. spaCy shows dependency relations with directed
    arcs. *Figure 3* *.2* shows an example of a dependency relation between a noun
    and the adjective that qualifies the noun:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy为每个标记分配一个依存标签，就像其他语言特征（如词元或POS标签）一样。spaCy使用有向弧显示依存关系。*图3* *.2*显示了名词与其修饰形容词之间的依存关系示例：
- en: '![Figure 3.2 – Dependency relation between a noun and its adjective](img/B22441_03_02.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图3.2 – 名词与其形容词之间的依存关系](img/B22441_03_02.jpg)'
- en: Figure 3.2 – Dependency relation between a noun and its adjective
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2 – 名词与其形容词之间的依存关系
- en: 'A dependency label describes the type of syntactic relation between two tokens
    as follows: one of the tokens is the **syntactic parent** (called the **head**
    ) and the other is its **dependent** (called the **child** ). In the preceding
    example, **flower** is the head and **blue** is its dependent/child.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 一个依存标签描述了两个标记之间的句法关系如下：其中一个标记是**句法父节点**（称为**头**）而另一个是其**依存项**（称为**子节点**）。在先前的例子中，**flower**是头，**blue**是其依存项/子节点。
- en: 'The dependency label is assigned to the child. Token objects have **dep** (int
    value) and **dep_** (unicode, human-readable string) properties that hold the
    dependency label, as illustrated in the following code snippet:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 依存标签被分配给子节点。标记对象有**dep**（整数值）和**dep_**（Unicode，可读字符串）属性，它们持有依存标签，如下面的代码片段所示：
- en: '[PRE5]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'In this example, we iterated over the tokens and printed their text and dependency
    labels. Let’s understand these dependency labels:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们遍历了标记并打印了它们的文本和依存标签。让我们了解这些依存标签：
- en: '**blue** had the **amod** label assigned. **amod** is the dependency label
    for an adjective-noun relation.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**blue**被分配了**amod**标签。**amod**是形容词-名词关系的依存标签。'
- en: '**flower** is the **ROOT** . **ROOT** is a special label in the dependency
    tree; it is assigned to the main verb of a sentence. If we’re processing a phrase
    (not a full sentence), the **ROOT** label is assigned to the root of the phrase,
    which is the head noun of the phrase. In the **blue flower** phrase, the head
    noun, **flower** , is the root of the phrase.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**flower**是**ROOT**。**ROOT**是依存树中的一个特殊标签；它被分配给句子的主要动词。如果我们处理的是一个短语（而不是完整的句子），则**ROOT**标签被分配给短语的根，即短语的中心名词。在**blue
    flower**短语中，中心名词**flower**是短语的根。'
- en: Each sentence or phrase has exactly one root, and it’s the root of the parse
    tree.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个句子或短语恰好有一个根，它是分析树的根。
- en: Tree nodes can have more than one child, but each node can only have one parent
    (due to tree restrictions, and trees containing no cycles). In other words, every
    token has exactly one head, but a parent can have several children. This is the
    reason why the dependency label is assigned to the dependent node.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 树节点可以有多个子节点，但每个节点只能有一个父节点（由于树的限制，以及不包含循环的树）。换句话说，每个标记恰好有一个头，但父节点可以有多个子节点。这就是为什么依存标签被分配给依存节点的原因。
- en: 'Let’s see a list of the most common and useful dependency labels, then we’ll
    see how exactly they link tokens to each other. Here’s the list first:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看最常见的和最有用的依存标签列表，然后我们将了解它们如何确切地链接标记。首先是这个列表：
- en: '**amod** : Adjectival modifier'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**amod** : 形容词修饰符'
- en: '**aux** : Auxiliary'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**aux** : 辅助'
- en: '**compound** : Compound'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**compound** : 复合'
- en: '**dative** : Dative object'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**dative** : 宾格宾语'
- en: '**det** : Determiner'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**det** : 限定词'
- en: '**dobj** : Direct object'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**dobj** : 直接宾语'
- en: '**nsubj** : Nominal subject'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**nsubj** : 名词主语'
- en: '**nsubjpass** : Nominal subject, passive'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**nsubjpass** : 名词主语，被动'
- en: '**nummod** : Numeric modifier'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**nummod** : 数量修饰符'
- en: '**poss** : Possessive modifier'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**poss** : 所有权修饰符'
- en: '**root** : The root'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**root** : 根'
- en: 'Let’s see examples of how these labels are used and what relation they express:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这些标签的使用示例以及它们表达的关系：
- en: '**amod** is an adjectival modifier. As understood from the name, this relation
    modifies the noun (or pronoun). In *Figure 3* *.3,* we see white modify sheep:'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**amod**是一个形容词修饰符。从其名称可以理解，这种关系修饰名词（或代词）。在*图3* *.3*中，我们看到白色修饰绵羊：'
- en: '![Figure 3.3 – amod relation](img/B22441_03_03.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图3.3 – amod关系](img/B22441_03_03.jpg)'
- en: Figure 3.3 – amod relation
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3 – amod关系
- en: '**aux** is what you might guess: it’s the dependency relation between an auxiliary
    verb and its main verb. The dependent is an auxiliary verb, and the head is the
    main verb. In *Figure 3* *.4* , we see that **has** is the auxiliary verb of the
    main **gone** verb:'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**aux**是你可能猜到的：它是助动词与其主要动词之间的依赖关系。依赖项是一个助动词，中心项是主要动词。在*图3.4*中，我们看到**has**是主要动词**gone**的助动词：'
- en: '![Figure 3.4 – aux relation](img/B22441_03_04.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图3.4 – aux关系](img/B22441_03_04.jpg)'
- en: Figure 3.4 – aux relation
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4 – aux关系
- en: '**compound** is used for the noun compounds; the second noun is modified by
    the first noun. In *Figure 3* *.5* , **phone book** is a noun compound and the
    **phone** noun modifies the **book** noun:'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**compound**用于名词复合；第二个名词由第一个名词修饰。在*图3.5*中，**phone book**是一个名词复合，**phone**名词修饰**book**名词：'
- en: '![Figure 3.5 – compound relation](img/B22441_03_05.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图3.5 – 复合关系](img/B22441_03_05.jpg)'
- en: Figure 3.5 – compound relation
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5 – 复合关系
- en: 'The **det** relation links a determiner (the dependent) to the noun it qualifies
    (its head). In *Figure 3* *.6* , **the** is the determiner of the noun **girl**
    in this sentence:'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**det**关系将限定词（依赖项）与其修饰的名词（其中心项）联系起来。在*图3.6*中，**the**是这句话中名词**girl**的限定词：'
- en: '![Figure 3.6 – det relation](img/B22441_03_06.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图3.6 – det关系](img/B22441_03_06.jpg)'
- en: Figure 3.6 – det relation
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.6 – det关系
- en: Next, we’ll look into two object relations, **dative** and **dobj** . The **dobj**
    relation is between the verb and its direct object. A sentence can have more than
    one object (such as in the following example); a direct object is the object that
    the verb acts upon, and the others are called **indirect objects** .
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨两种宾语关系，**dative**和**dobj**。**dobj**关系在动词和它的直接宾语之间。一个句子可以有一个以上的宾语（如下面的例子所示）；直接宾语是动词所作用的对象，其他的是称为**间接宾语**。
- en: 'A direct object is generally marked with the **accusative case** . A **dative**
    relation points to a **dative object** , which receives an indirect action from
    the verb. In the sentence shown in *Figure 3* *.7* , the indirect object is **me**
    and the direct object is **book** :'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 直接宾语通常用**宾格**来标记。**dative**关系指向**dative宾语**，它从动词那里接受间接动作。在*图3.7*中显示的句子中，间接宾语是**me**，直接宾语是**book**：
- en: '![Figure 3.7 – The direct and indirect objects of the sentence](img/B22441_03_07.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![图3.7 – 句子的直接和间接宾语](img/B22441_03_07.jpg)'
- en: Figure 3.7 – The direct and indirect objects of the sentence
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.7 – 句子的直接和间接宾语
- en: '**nsubj** and **nsubjposs** are two relations that are related to the nominal
    sentence subject. The subject of the sentence is the one that committed the action.
    A passive subject is still the subject, but we mark it with **nsubjposs** . In
    *Figure 3* *.8* , **Mary** is the nominal subject of the sentence:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '**nsubj**和**nsubjposs**是两种与名词性句子主语相关的关联。句子的主语是执行动作的人。被动主语仍然是主语，但我们用**nsubjposs**来标记它。在*图3.8*中，**Mary**是句子的名词主语：'
- en: '![Figure 3.8 – nsubj relation](img/B22441_03_08.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图3.8 – nsubj关系](img/B22441_03_08.jpg)'
- en: Figure 3.8 – nsubj relation
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.8 – nsubj关系
- en: '**you** is the passive nominal subject of the sentence in *Figure 3* *.9* :'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '*图3.9*中的句子是**you**的被动名词主语：'
- en: '![Figure 3.9 – nsubjpass relation](img/B22441_03_09.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图3.9 – nsubjpass关系](img/B22441_03_09.jpg)'
- en: Figure 3.9 – nsubjpass relation
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.9 – nsubjpass关系
- en: 'We have now covered sentence subject and object relations. Now, we’ll discover
    two modifier relations; one is the **nummod (numeric modifier)** and the other
    is the **poss (possessive modifier)** . A numeric modifier modifies the meaning
    of the head noun by a number or quantity. In the sentence in *Figure 3* *.10*
    , **nummod** is easy to spot; it’s between **3** and **books** :'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经涵盖了句子主语和宾语关系。现在，我们将发现两种修饰关系；一个是**nummod（数字修饰语**），另一个是**poss（拥有性修饰语**）。数字修饰语通过数字或数量来修饰中心名词的意义。在*图3.10*中的句子中，**nummod**很容易找到；它在**3**和**books**之间：
- en: '![Figure 3.10 – nummod relation](img/B22441_03_10.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图3.10 – nummod关系](img/B22441_03_10.jpg)'
- en: Figure 3.10 – nummod relation
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.10 – nummod关系
- en: 'A possessive modifier happens either between a *possessive pronoun* and a noun
    or a *possessive’s* and a noun. In the sentence shown in *Figure 3* *.11* , **my**
    is a possessive marker on the noun **book** :'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有性修饰语发生在所有格代词和名词之间，或者所有格的所有格和名词之间。在*图3.11*中显示的句子中，**my**是名词**book**的拥有性标记：
- en: '![Figure 3.11 – poss relation between my and book](img/B22441_03_11.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图3.11 – my和book之间的poss关系](img/B22441_03_11.jpg)'
- en: Figure 3.11 – poss relation between my and book
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.11 – my和book之间的poss关系
- en: Last, but not least, **is** is the **root** label, which is not a real relation
    but is a marker for the sentence verb. A root word has no real parent in the syntactic
    tree; the root is the main verb of the sentence. In the preceding sentence, the
    main verb is the **is** auxiliary verb. Notice that the root node has no incoming
    arc—that is, no parent.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，**is**是**根**标签，它不是一个真正的关系，而是句子动词的标记。根词在句法树中没有真正的父节点；根是句子的主要动词。在前面的句子中，主要动词是**is**助动词。请注意，根节点没有进入弧线——也就是说，没有父节点。
- en: 'These are the most useful labels for our NLU purposes. Now let’s practice how
    we can make use of dependency labels. **token.dep_** includes the dependency label
    of the dependent token. The **token.head** property points to the head/parent
    token. Only the root token does not have a parent; spaCy points to the token itself
    in this case. Let’s bisect the example sentence from *Figure 3* *.11* , as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是我们NLU目的中最有用的标签。现在让我们练习如何利用依存关系标签。**token.dep_**包括从属标记的依存关系标签。**token.head**属性指向头/父标记。只有根标记没有父节点；spaCy在这种情况下指向标记本身。让我们从*图3.11*的示例句子中分割出以下内容：
- en: '[PRE6]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We iterated over the tokens and printed the fine-grained POS tag and the dependency
    label. **is** is the main verb of the sentence and is labeled by the **ROOT**
    label. **This** is the subject of the sentence. We can go one level deeper and
    print the token heads this time, as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遍历了标记并打印了细粒度的词性标注和依存关系标签。**is**是句子的主要动词，并标记为**ROOT**标签。**这**是句子的主语。我们可以深入一级并打印这次标记的头，如下所示：
- en: '[PRE7]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We can see that the verb **is** is the head of all the tokens except the **my**
    one, which has **book** as the head. Let’s examine the dependency tree of a longer
    and more complicated sentence, as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到动词**is**是除了**my**之外所有标记的头，而**my**的头是**book**。让我们检查一个更长、更复杂的句子的依存关系树，如下所示：
- en: '[PRE8]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As we can see in *Figure 3* *.12* , we locate the main verb and the **trying**
    root (it has no incoming arcs). The left side of the word **trying** looks manageable,
    but the right side has a chain of arcs. Let’s start with the left side. The **we**
    pronoun is labeled with **nsubj** . Hence, this is the nominal subject of the
    sentence. The other left arc, labeled **aux** , points to the **trying** dependent
    ( **are** ), which is the auxiliary verb of the main verb, **trying** .
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图3.12*所示，我们可以定位到主要动词和**尝试**的根（它没有进入弧线）。**尝试**这个词的左侧看起来可以管理，但右侧有一系列弧线。让我们从左侧开始。**我们**这个代词被标记为**nsubj**。因此，这是句子的名词主语。另一个左侧的弧线，标记为**aux**，指向**尝试**的从属（**are**），它是主要动词**尝试**的助动词。
- en: '![Figure 3.12 – A complicated parsing example](img/B22441_03_12.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图3.12 – 一个复杂的解析示例](img/B22441_03_12.jpg)'
- en: Figure 3.12 – A complicated parsing example
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.12 – 一个复杂的解析示例
- en: Now let’s see what is happening on the right side. **trying** is attached to
    the second verb, **understand** , via an **xcomp** relation. The **xcomp** (or
    open complement) relation of a verb is a clause without its own subject. Here,
    the **to understand the difference** clause has no subject, so it’s an open complement.
    We follow the **dobj** arc from the second verb, **understand** , and land on
    the noun, **difference** , which is the direct object of the **to understand the**
    **difference** clause.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看右侧发生了什么。**尝试**通过一个**xcomp**关系附加到第二个动词**理解**上。动词的**xcomp**（或开放补语）关系是一个没有自己主语的子句。在这里，**理解差异**这个子句没有主语，所以它是一个开放补语。我们跟随从第二个动词**理解**出发的**dobj**弧线，最终落在名词**差异**上，它是**to
    understand the difference**子句的直接宾语。
- en: The *displaCy* online demo ( [https://demos.explosion.ai/displacy](https://demos.explosion.ai/displacy)
    ) is a great tool for you to try your own example sentences and see the parsing
    results. This section has a solid foundation for general linguistics and information
    extraction for the pattern-matching exercises we will see in [*Chapter 4*](B22441_04.xhtml#_idTextAnchor056)
    .
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '*displaCy*在线演示（[https://demos.explosion.ai/displacy](https://demos.explosion.ai/displacy)）是一个很好的工具，你可以用它来尝试自己的示例句子并查看解析结果。本节为我们将在[*第4章*](B22441_04.xhtml#_idTextAnchor056)中看到的模式匹配练习提供了坚实的基础。'
- en: 'To finish this chapter, let’s learn about a very famous NLP task: NER.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成这一章，让我们了解一个非常著名的NLP任务：NER。
- en: Introducing NER
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍NER
- en: What is a **named entity** ? A named entity is a real-world object that we can
    refer to by a proper name or quantity of interest. It can be a person, a place
    (city, country, landmark, or famous building), an organization, a company, a product,
    dates, times, percentages, monetary amounts, a drug, or a disease name. Some examples
    are Alicia Keys, Paris, France, Brandenburg Gate, WHO, Google, Porsche Cayenne,
    and so on.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是**命名实体**？命名实体是我们可以通过一个专有名称或感兴趣的数量来指代的现实世界中的对象。它可以是一个人、一个地方（城市、国家、地标或著名建筑）、一个组织、一家公司、一个产品、日期、时间、百分比、货币金额、一种药物或疾病名称。一些例子包括Alicia
    Keys、巴黎、法国、勃兰登堡门、世界卫生组织、谷歌、保时捷卡宴等。
- en: A named entity always points to a specific object, and that object is distinguishable
    via the corresponding named entity. For instance, if we tag the **Paris is the
    capital of France** sentence, we parse **Paris** and **France** as named entities,
    but not the word **capital** . The reason is that **capital** does not point to
    a specific object; it’s a general name for many objects.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 命名实体总是指向一个特定的对象，而这个对象可以通过相应的命名实体来区分。例如，如果我们标记“**巴黎是法国的首都**”这个句子，我们会将**巴黎**和**法国**解析为命名实体，但不会将单词**首都**解析为命名实体。原因是**首都**并不指向一个特定的对象；它是许多对象的通用名称。
- en: NER categorization is a bit different from POS categorization. Here, the number
    of categories is as high as we want. The most common categories are person, location,
    and organization and are supported by almost every usable NER tagger.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: NER分类与POS分类略有不同。在这里，分类的数量可以高达我们想要的。最常见的分类是人物、地点和组织，并且几乎每个可用的NER标记器都支持这些分类。
- en: 'spaCy supports a wide range of entity types. Which ones you use depends on
    your corpus. If you process financial text, you most probably use **MONEY** and
    **PERCENTAGE** more often than **WORK_OF_ART** . Here is a list of the entity
    types supported by spaCy (sourced from [https://towardsdatascience.com/explorations-in-named-entity-recognition-and-was-eleanor-roosevelt-right-671271117218](https://towardsdatascience.com/explorations-in-named-entity-recognition-and-was-eleanor-roosevelt-right-671271117218)
    ):'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy支持广泛的实体类型。你使用哪些类型取决于你的语料库。如果你处理财务文本，你很可能比**WORK_OF_ART**更频繁地使用**MONEY**和**PERCENTAGE**。以下是spaCy支持的实体类型列表（来源：[https://towardsdatascience.com/explorations-in-named-entity-recognition-and-was-eleanor-roosevelt-right-671271117218](https://towardsdatascience.com/explorations-in-named-entity-recognition-and-was-eleanor-roosevelt-right-671271117218)）：
- en: '**PERSON** : People, including fictional'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PERSON**：人物，包括虚构人物'
- en: '**NORP** : Nationalities or religious or political groups'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NORP**：民族或宗教或政治团体'
- en: '**FAC** : Buildings, airports, highways, bridges, and so on'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**FAC**：建筑物、机场、高速公路、桥梁等'
- en: '**ORG** : Companies, agencies, institutions, and so on'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ORG**：公司、机构、机构等'
- en: '**GPE** : Countries, cities, states'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GPE**：国家、城市、州'
- en: '**LOC** : Non-GPE locations, mountain ranges, bodies of water'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LOC**：非GPE地点、山脉、水体等'
- en: '**PRODUCT** : Objects, vehicles, foods, and so on ( not services)'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PRODUCT**：物体、车辆、食品等（不包括服务）'
- en: '**EVENT** : Named hurricanes, battles, wars, sports events, and so on'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**EVENT**：命名的飓风、战役、战争、体育赛事等'
- en: '**WORK_OF_ART** : Titles of books, songs, and so on'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**WORK_OF_ART**：书籍、歌曲等的标题'
- en: '**LAW** : Named documents made into laws'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LAW**：成为法律的命名文件'
- en: '**LANGUAGE** : Any named language'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LANGUAGE**：任何命名的语言'
- en: '**DATE** : Absolute or relative dates or periods'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DATE**：绝对或相对日期或时期'
- en: '**TIME** : Times smaller than a day'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TIME**：小于一天的时间'
- en: '**PERCENT** : Percentage, including %'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PERCENT**：百分比，包括%'
- en: '**MONEY** : Monetary values, including unit'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MONEY**：货币价值，包括单位'
- en: '**QUANTITY** : Measurements, as of weight or distance'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**QUANTITY**：测量值，如重量或距离'
- en: '**ORDINAL** : first, second, and so on'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ORDINAL**：第一、第二等'
- en: '**CARDINAL** : Numerals that do not fall under another type'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CARDINAL**：不属于其他类型的数词'
- en: Just as with the POS tagger statistical models, NER models are also sequential
    models. The very first modern NER tagger model is a **Conditional Random Field**
    ( **CRF** ). CRFs are sequence classifiers used for structured prediction problems
    such as labeling and parsing. The current state-of-the-art NER tagging is achieved
    with the **transformers** architecture (more on that in [*Chapter 6*](B22441_06.xhtml#_idTextAnchor087)
    ).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 就像POS标记器的统计模型一样，NER模型也是顺序模型。第一个现代NER标记器模型是**条件随机字段**（**CRF**）。CRF是用于结构化预测问题（如标记和解析）的序列分类器。当前最先进的NER标记是使用**transformers**架构实现的（更多内容请参阅[*第6章*](B22441_06.xhtml#_idTextAnchor087)）。
- en: 'Named entities in a doc are available via the **doc.ents** property. **doc.ents**
    is a list of **Span** objects, as illustrated in the following code snippet:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 文档中的命名实体可以通过**doc.ents**属性访问。**doc.ents**是一个**Span**对象的列表，如下面的代码片段所示：
- en: '[PRE9]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: spaCy also tags each token with the entity type. The type of the named entity
    is available via **token.ent_type** (int) and **token.ent_type_** (unicode). If
    the token is not a named entity, then **token.ent_type_** is just an empty string.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy也为每个标记标记了实体类型。命名实体的类型可以通过**token.ent_type**（整数）和**token.ent_type_**（Unicode）访问。如果标记不是命名实体，那么**token.ent_type_**只是一个空字符串。
- en: 'Just as for POS tags and dependency labels, we can call **spacy.explain()**
    on the tag string or on **token.ent_type_** , as follows:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 正如对于词性和依存标签一样，我们可以在标签字符串或**token.ent_type_**上调用**spacy.explain()**，如下所示：
- en: '[PRE10]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: After tagging tokens with different syntactical features, we sometimes want
    to merge or split entities into fewer or more tokens. In the next section, we
    will see how this merging and splitting is done.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在标记不同的句法特征后，我们有时希望将实体合并或拆分为更少或更多的标记。在下一节中，我们将看到这种合并和拆分是如何进行的。
- en: Merging and splitting tokens
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 合并和拆分标记
- en: 'In some cases, we want to unite or split multiword named entities. For example,
    this is needed when the tokenizer does not perform so well on some unusual tokens,
    and you need to split them by hand. In this subsection, we’ll cover a very practical
    remedy for our multiword expressions, multiword named entities, and typos: **doc.retokenize**
    .'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，我们希望合并或拆分多词命名实体。例如，当分词器在某些不寻常的标记上表现不佳，需要手动拆分时，这就很有必要。在本小节中，我们将介绍一种针对我们的多词表达式、多词命名实体和错别字的非常实用的补救方法：**doc.retokenize**。
- en: '**doc.retokenize** is used in a context manager and it’s the correct tool for
    merging and splitting the spans of **doc** objects. The **retokenizer.merge()**
    method should receive the spans to merge and the attributes to set on these merged
    tokens. Let’s see an example of retokenization by merging a multiword named entity,
    as follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '**doc.retokenize**用于上下文管理器，并且是合并和拆分**doc**对象跨度范围的正确工具。**retokenizer.merge()**方法应该接收要合并的跨度以及要设置在这些合并标记上的属性。让我们看看以下如何通过合并多词命名实体进行重新分词的示例：'
- en: 'First, let’s create a **doc** from the sentence and print the entities:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们从句子中创建一个**doc**并打印实体：
- en: '[PRE11]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now let’s see how spaCy separated the tokens:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们看看spaCy是如何分离标记的：
- en: '[PRE12]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We want to merge tokens on positions **3** and **4** and de span slices are
    set as **(value_included, value_not_included)** , so we will slice **doc[3:5]**
    and set the **LEMMA** of this new token:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们希望合并位置**3**和**4**的标记，并且分割的切片设置为**（包含值，不包含值）**，因此我们将分割**doc[3:5]**并设置这个新标记的**LEMMA**：
- en: '[PRE13]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let’s print the tokens again to see whether the merge worked:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们再次打印标记，看看合并是否成功：
- en: '[PRE14]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Merging the tokens worked well, but how about splitting a multiword token into
    several tokens? In this setting, either there’s a typo in the text you want to
    fix, or the custom tokenization is not satisfactory for your specific sentence.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 合并标记工作得很好，但拆分多词标记为几个标记又会如何呢？在这个设置中，要么是想要修复的文本中存在错别字，要么是自定义分词对于你的特定句子来说不满意。
- en: 'Splitting a span is a bit more complicated than merging a span because of the
    following reasons:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 拆分跨度比合并跨度要复杂一些，原因如下：
- en: We are changing the dependency tree
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们正在改变依存句法树
- en: We need to assign new POS tags, dependency labels, and necessary token attributes
    to the new tokens
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要为新标记分配新的词性标签、依存标签和必要的标记属性
- en: 'Basically, we need to think about how to assign linguistic features to the
    new tokens we created. Let’s see how to deal with the new tokens with an example
    of how to fix a typo, as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，我们需要考虑如何为新创建的标记分配语言特征。让我们通过以下如何修复错别字的示例来了解如何处理新标记：
- en: '[PRE15]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '*Figure 3* *.13* shows what the dependency tree looks like before the splitting
    operation.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '*图3* *.13* 展示了在分割操作之前依存句法树的样子。'
- en: '![Figure 3.13 – Sample sentence’s dependency tree before retokenization](img/B22441_03_13.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![图3.13 – 在重新分词之前样本句子的依存句法树](img/B22441_03_13.jpg)'
- en: Figure 3.13 – Sample sentence’s dependency tree before retokenization
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.13 – 在重新分词之前样本句子的依存句法树
- en: 'Now, we will split the **doc[3]** , **NewHampshire** , into two tokens: **New**
    and **Hampshire** . We will give fine-grained POS tags and dependency labels to
    the new tokens via the **attrs** dictionary to the **retokenize.split()** method.
    We will also rearrange the dependency tree by passing the new tokens’ parents
    via the **heads** parameter. While arranging the heads, there are two things to
    consider, as outlined here:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将 **doc[3]** ， **NewHampshire** ，拆分为两个标记：**New** 和 **Hampshire** 。我们将通过将新的标记的依赖关系通过
    **heads** 参数传递给 **retokenize.split()** 方法，为新标记提供细粒度的 POS 标签和依赖标签。我们还将通过传递新标记的父节点通过
    **heads** 参数来重新排列依赖树。在安排头部时，有两个事情需要考虑，如下所述：
- en: Firstly, when you provide a relative position, such as **(doc[3], 1)** , this
    means that the head of **doc[3]** will be the token that is *one position ahead*
    —in this case, that’s **doc[4]** .
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，当你提供一个相对位置，例如 **(doc[3], 1)** ，这意味着 **doc[3]** 的头部将是 *前一个位置* 的标记——在这种情况下，那就是
    **doc[4]** 。
- en: Secondly, if you give an absolute position, it means the position in the original
    **Doc** object. In the following code snippet, the second item in the **heads**
    list indicates that the head of the **Hampshire** token is the second token in
    the original Doc, which is the **in** token.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 其次，如果你提供一个绝对位置，这意味着在原始 **Doc** 对象中的位置。在下面的代码片段中，**heads** 列表中的第二个项目表示 **Hampshire**
    标记的头是原始 Doc 中的第二个标记，即 **in** 标记。
- en: 'Let’s do this in the code:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在代码中实现这个功能：
- en: '[PRE16]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '*Figure 3* *.14* shows what the dependency tree looks like after the splitting
    operation.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 3* *.14* 展示了拆分操作后依赖树的样子。'
- en: '![Figure 3.14 – Dependency tree after the splitting operation](img/B22441_03_14.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.14 – 拆分操作后的依赖树](img/B22441_03_14.jpg)'
- en: Figure 3.14 – Dependency tree after the splitting operation
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.14 – 拆分操作后的依赖树
- en: You can apply merging and splitting operations onto any span, not only the named
    entity spans. The most important part here is to correctly arrange the new dependency
    tree and the linguistic attributes.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将合并和拆分操作应用于任何跨度，而不仅仅是命名实体跨度。这里最重要的部分是正确安排新的依赖树和语言属性。
- en: Summary
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'This chapter gave you details of spaCy’s linguistic features and how to use
    them. You learned about POS tagging and applications and learned about an important
    yet not-so-well-known and well-used feature of spaCy—dependency labels. Then,
    we discovered a famous NLU tool and concept: NER. We saw how to do named entity
    extraction, again via examples. We finalized this chapter with a handy tool for
    merging and splitting spans.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 本章为您提供了 spaCy 的语言特征及其使用方法的详细信息。您了解了 POS 标记和其应用，并了解了 spaCy 中的一个重要但不太为人所知且使用较少的特征——依赖标签。然后，我们发现了一个著名的
    NLU 工具和概念：NER。我们通过示例了解了如何进行命名实体提取。最后，我们以一个方便的工具来合并和拆分跨度结束了本章。
- en: What’s next? In the next chapter, we will discover how to use these linguistic
    features to extract information using the **Matcher** , **PhraseMatcher** , and
    **SpanRuler** classes.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是什么？在下一章中，我们将发现如何使用这些语言特征通过 **Matcher** ， **PhraseMatcher** ，和 **SpanRuler**
    类来提取信息。
