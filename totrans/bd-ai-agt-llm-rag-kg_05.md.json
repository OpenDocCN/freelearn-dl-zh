["```py\n    text_splitter = NLTKTextSplitter(chunk_size=1500)\n    def split_overview(overview):\n        if pd.isna(overview):\n            return []\n        return text_splitter.split_text(str(overview))\n    df['chunks'] = df['text_column'].apply(split_overview)\n    ```", "```py\n        embedder = SentenceTransformer('all-MiniLM-L6-v2')\n        def encode_chunk(chunk):\n            if not isinstance(chunk, str) or chunk.strip() == \"\":\n                return None\n            return embedder.encode(chunk).tolist()\n        chunked_df['embeddings'] = chunked_df['chunks'].apply(encode_chunk)\n        ```", "```py\n        chunked_df.dropna(subset=['embeddings'], inplace=True)\n        client = chromadb.Client()\n        collection = client.create_collection(name='movies')\n        for idx, row in chunked_df.iterrows():\n            collection.add(\n                ids=[str(idx)],\n                embeddings=[row['embeddings']],\n                metadatas=[{\n                    'original_title': row['original_title'],\n                    'chunk': row['chunks']\n                }]\n            )\n        ```", "```py\n        def retrieve_documents(query, collection, top_k=5):\n            query_embedding = embedder.encode(query).tolist()\n            results = collection.query( query_embeddings=[query_embedding], n_results=top_k )\n            chunks = []\n            titles = []\n            for document in results['metadatas'][0]:\n                chunks.append(document['chunk'])\n                titles.append(document['original_title'])\n            return chunks, titles\n        ```", "```py\n    tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n    model = AutoModelForCausalLM.from_pretrained(\n        \"mistralai/Mistral-7B-Instruct-v0.1\",\n        device_map='auto')\n    text_generation_pipeline = pipeline(\n        model=model,\n        tokenizer=tokenizer,\n        task=\"text-generation\",\n        return_full_text=True,\n        max_new_tokens=800)\n    def generate_answer(query, chunks, titles, text_generation_pipeline):\n        context = \"\\n\\n\".join([f\"Title: {title}\\nChunk: {chunk}\" for title, chunk in zip(titles, chunks)])\n        prompt = f\"\"\"[INST]\n        Instruction: You're an expert in movie suggestions. Your task is to analyze carefully the context and come up with an exhaustive answer to the following question:\n        {query}\n        Here is the context to help you:\n        {context}\n        [/INST]\"\"\"\n        generated_text = text_generation_pipeline(prompt)[0]['generated_text']\n        return generated_text\n    ```", "```py\nclient = chromadb.Client()\ncollection = client.get_collection(name='movies')\nquery = \"What are some good movies to watch on a rainy day?\"\ntop_k = 5\nchunks, titles = retrieve_documents(query, collection, top_k)\nprint(f\"Retrieved Chunks: {chunks}\")\nprint(f\"Retrieved Titles: {titles}\")\nif chunks and titles:\n    answer = generate_answer(query, chunks, titles, text_generation_pipeline)\n    print(answer)\nelse:\n    print(\"No relevant documents found to generate an answer.\")\n```"]