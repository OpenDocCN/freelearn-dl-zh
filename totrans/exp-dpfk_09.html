<html><head></head><body>
		<div id="_idContainer097">
			<h1 id="_idParaDest-151" class="chapter-number"><a id="_idTextAnchor152"/>9</h1>
			<h1 id="_idParaDest-152"><a id="_idTextAnchor153"/>The Future of Generative AI</h1>
			<p>While it sometimes might feel like we’re already living in the future with deepfakes and AI-generated images, the technology behind them is really just beginning to take off. As we move forward, the capabilities of these generative AIs will only become <span class="No-Break">more powerful.</span></p>
			<p>This chapter is not unbounded futurism but will instead look at specific generative AIs and where they are improving. We will examine the following technologies and how they are changing. We’ll discuss the future of the following areas <span class="No-Break">of AI:</span></p>
			<ul>
				<li><span class="No-Break">Generating text</span></li>
				<li>Improving <span class="No-Break">image quality</span></li>
				<li>Text-guided <span class="No-Break">image generation</span></li>
				<li><span class="No-Break">Generating sound</span></li>
				<li><span class="No-Break">Deepfakes</span></li>
			</ul>
			<h1 id="_idParaDest-153"><a id="_idTextAnchor154"/>Generating text</h1>
			<p>Recently, text generation<a id="_idIndexMarker399"/> models made a major impact when they came into the public consciousness with OpenAI’s success with ChatGPT in 2022. However, text generation<a id="_idIndexMarker400"/> was among the first uses of AI. Eliza was the first <strong class="bold">chatbot</strong> ever developed, back in 1966, before all but the most technically inclined people had even seen a computer themselves. The personal computer wouldn’t even be invented for another 5 years, in 1971. However, it’s only recently that truly impressive chatbots have <span class="No-Break">been developed.</span></p>
			<h2 id="_idParaDest-154"><a id="_idTextAnchor155"/>Recent developments</h2>
			<p>A type of model called <strong class="bold">transformers</strong> is responsible for the recent burst<a id="_idIndexMarker401"/> in language models. Transformers<a id="_idIndexMarker402"/> are neural networks that are comprised<a id="_idIndexMarker403"/> entirely of a layer called an <strong class="bold">attention layer</strong>. Attention layers work sort of like a spotlight, focusing on the part of the data that is most likely to be important. This lets transformers (and other models that use attention layers) be a lot deeper without losing “focus” (though that’s not actually the technical term for it, it’s a good metaphor for <span class="No-Break">the effect).</span></p>
			<p>Thanks to the ability to make very deep models, transformers excel at tasks that need to pull meaning from complicated structures. This is a perfect match for language tasks and other long sequences where a word’s meaning depends heavily on those words around it (think about the difference between “<em class="italic">I ate an orange fruit</em>” and “<em class="italic">I ate an orange</em>”). Transformers are used heavily for tasks such as translation, but they also are excellent for other tasks such as answering questions, summarizing information, and the subject of this section: <span class="No-Break">text generation.</span></p>
			<p>Most language tasks require not just the ability to “understand” the text but also to create new text. For example, in a summarizing task, you cannot simply take the important words from a document and copy them out to the user. The words would be meaningless, as they’d be in an arbitrary order and have none of the important context that language is built on. So, these models must also create realistic and understandable output in the form of properly constructed sentences for the user <span class="No-Break">to interpret.</span></p>
			<p class="callout-heading">Author’s note</p>
			<p class="callout">There is a lot of debate as to whether you can use a uniquely human concept of understanding to describe anything a computer does. One famous<a id="_idIndexMarker404"/> example is the so-called <strong class="bold">Chinese Room</strong> thought experiment by <span class="No-Break">John Searle.</span></p>
			<p class="callout">The basics of the thought experiment are this: imagine a room where there is a huge array of filing cabinets. In each of these filing cabinets, there are cards that pair a certain input to a certain output. The room can take as input and output other cards on which Chinese characters are written. Inside the room, there is someone (or something) who knows no Chinese, but by matching the incoming cards with the cards in the filing cabinets, then copying the card’s output characters onto another card, can output <span class="No-Break">perfect Chinese.</span></p>
			<p class="callout">In that context, can you say that the machine (or the person or thing inside the machine) “understands” Chinese? What about the room itself? The question has led to a lively debate among philosophers, computer scientists, linguists, <span class="No-Break">and more.</span></p>
			<h2 id="_idParaDest-155"><a id="_idTextAnchor156"/>Building sentences</h2>
			<p>Eliza, that first chatbot, built its sentences<a id="_idIndexMarker405"/> by simply repeating back words from the input sentences into one of a number of pre-built sentences that were written carefully to let those words be put into them. This process of parroting parts of the user’s responses back was good enough to serve as a very simple Freudian counselor, which many people felt helped them to get through <span class="No-Break">some situations.</span></p>
			<p>Today’s generative models instead build sentences with extremely advanced sentence structures. They do this by building sentences in a way not unlike a jigsaw puzzle. The model uses a concept called a <strong class="bold">beam</strong> to search for the word that fits best<a id="_idIndexMarker406"/> into a given part of a sentence, much like how a jigsaw solver searches over a set of pieces to find the ones that best fit the available space. Unfortunately, this is where the metaphor becomes strained. A single beam keeps track of just one sequence, and unlike a normal jigsaw, a sentence can be made of a near-infinite combination of words. Because of this, a model needs to keep track of several beams as the best scoring beam may change as the sentence gets completed. More beams lead to a better-generated sentence, both <strong class="bold">grammatically</strong> and <strong class="bold">semantically</strong> (that is, both the structure and meaning) but require more computation as the model solves a larger number of sentences at the <span class="No-Break">same time.</span></p>
			<h2 id="_idParaDest-156"><a id="_idTextAnchor157"/>The future of text generation</h2>
			<p>As we move into the future, these models<a id="_idIndexMarker407"/> will improve in two ways; computational availability will increase, making it feasible to get larger models with larger beam searches, and design efficiency will allow for better-designed models that can do more with the same resources. You can think of this as something like packing boxes into a truck: you can only fit so many boxes in a truck until you need to either get a bigger truck or find a way to make the boxes smaller. Transformers are an example of design efficiency, while the fact that computers always get faster and better is an example of <span class="No-Break">computational availability.</span></p>
			<p>It may seem that text generation has endless grounds for improvement, but there are questions as to how sustainable the long-term growth of language<a id="_idIndexMarker408"/> models may be. There are concerns that we might reach the limits of <strong class="bold">large language models</strong> (<strong class="bold">LLMs</strong>) relatively quickly. Multiple constraints exist that may limit <span class="No-Break">the models.</span></p>
			<p>The first limit on language models is that the largest models already require weeks or months to train, spread across thousands of powerful computers. The biggest limitations of these types of models may be economics. There simply might not be enough demand to keep growing the models as the costs continue <span class="No-Break">to skyrocket.</span></p>
			<p>The second major limit to language models is far harder to get past: there simply might not be enough data to keep growing. LLMs require a massive amount of data to be able to learn languages, and we are quickly approaching the point where they could be trained on the entire output of humanity’s written history and still not have enough data to fully train the model. Deepmind published (<a href="https://arxiv.org/abs/2203.15556">https://arxiv.org/abs/2203.15556</a>) a study of LLMs that found that the compute and data needed to completely train a model must scale up in tandem. That means that if you double the amount of compute a model needs to train, you should also double the data. This means that it’s entirely possible that models will soon be limited in how well they can learn language because they’ve been trained on everything ever written <span class="No-Break">by humanity.</span></p>
			<p>Both of these problems have potential solutions, but they’re nowhere near as easy to come by as throwing more compute or smarter model designs at the problem. You might ask something such as “<em class="italic">but how can humans learn language without reading every text humanity has ever written</em>?” or something similar. The reason is that humans are fully “intelligent,” a term whose definition seems to change every time something that is not human gets close to it. In the end, it’s possible that we will be unable to get a computer to fully understand language until we can get them to “understand” in a way that philosophers cannot debate. Until then, language models need a lot more data than humans to reach high levels of <span class="No-Break">language use.</span></p>
			<p>Text generation<a id="_idIndexMarker409"/> may have been one of the first uses of AI, but arguably a more significant use until recently has been improving <span class="No-Break">image quality.</span></p>
			<h1 id="_idParaDest-157"><a id="_idTextAnchor158"/>Improving image quality</h1>
			<p>The earliest known image<a id="_idIndexMarker410"/> that was taken through mechanical means is the <em class="italic">Niépce héliographie</em>, which was taken by Joseph Nicéphore Niépce in 1827. It was taken through the window of his workshop by exposing a pewter plate covered in a thin layer of a concoction made from lavender and bitumen. This plate was exposed to sunlight for several days to create a blurry, <span class="No-Break">monochrome image.</span></p>
			<div>
				<div id="_idContainer094" class="IMG---Figure">
					<img src="image/B17535_09_001.jpg" alt="Figure 9.1 – The ﻿Niépce Heliograph taken by Joseph Nicéphore Niépce in 1827"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.1 – The Niépce Heliograph taken by Joseph Nicéphore Niépce in 1827</p>
			<p>Since then, images have gotten<a id="_idIndexMarker411"/> better at capturing reality, but the process has not been <em class="italic">perfected</em>. There are always various limitations that mean that the images aren’t quite accurate in color, cannot capture all the details, or cause distortions in the image. A truly perfect image is mindboggling to even consider: from a single perfect image, you’d be able to zoom into any atom even on the other side of the universe and our images simply cannot accomplish that level <span class="No-Break">of detail.</span></p>
			<h2 id="_idParaDest-158"><a id="_idTextAnchor159"/>Various tactics</h2>
			<p>Sometimes, the limitations<a id="_idIndexMarker412"/> of the image are more than we’re willing to accept. In this case, we can try to “improve” the image by modifying it to look more like what we want. Some of these edits are relatively easy. Perhaps your photo was taken at an angle, and everything is crooked. In the age of physical photo prints, this could be fixed with nothing but a pair of scissors (and maybe a mat to hide the smaller size). Many cameras with a flash attached have built-in “red eye” removal, which masks the reflection of the flash in people’s eyes by firing the flash twice in quick succession, once to make your eyes adjust to the flash and the second to actually take <span class="No-Break">the photo.</span></p>
			<p>Now you can fix both of these problems in software, and you can fix a lot more than crooked photos or a bright light reflecting off the back of your eye. Modern editing software can even “fix” things that people would argue aren’t actually broken. There is an ongoing discussion in society whether the images on the covers of magazines should be “brushed up” to make the subject more attractive or whether the process creates more harm than good by perpetuating an unrealistic <span class="No-Break">body image.</span></p>
			<p>As we’ve reached the age of neural networks, the automatic improvement of photos has become easier than ever. There are numerous services and tools offering to improve photos, whether they have been newly taken or are older. They work on a variety of techniques but fall into several different categories, including <strong class="bold">upscaling</strong>, <strong class="bold">color correction</strong> (or <strong class="bold">grading</strong>), and <strong class="bold">denoising</strong>. Let’s look at each of <span class="No-Break">these briefly.</span></p>
			<h3>Upscaling</h3>
			<p>Upscaling<a id="_idIndexMarker413"/> involves taking an image <a id="_idIndexMarker414"/>and scaling it up. That is, making an image have a higher resolution. Modern upscaling AIs have the ability to not just increase the resolution but also the fidelity – they can make images both bigger and better. This innovation comes from learning from lots of other images about what objects, textures, and images look like both large and shrunk down. The model, after being fed an artificially downscaled image, is asked to recreate the original image and is then reviewed on how well <span class="No-Break">it did.</span></p>
			<p>Making an image larger and higher in fidelity can be a balancing act. It’s possible to make images <em class="italic">too</em> sharp or fill in data that shouldn’t be there. For example, you don’t want to add wrinkles to a child, but a photo of your grandmother without her wrinkles just wouldn’t look right. For this reason, most upscaling has some sort of slider or other control to set how much additional fidelity you want the AI to give your images, letting you choose case by case what level of detail should <span class="No-Break">be added.</span></p>
			<p>There is one type of upscaling<a id="_idIndexMarker415"/> that sidesteps this limitation, and that is <strong class="bold">domain-specific upscaling</strong>. A domain-specific upscaler is one that upscales only one type of content. A recently popular example of this is face upscaling. Humans are hardwired to see faces, and we will notice any issues with them very easily. But when you take an upscaler and train it on nothing but faces, it’s able to learn specifically about faces and get far higher fidelity results when doing that one task instead of having to worry about upscaling everything. This lets the AI learn that “old people” tend to have wrinkles, something that a general purpose upscaler just doesn’t have the space <span class="No-Break">to learn.</span></p>
			<p>The techniques of domain-specific upscaling<a id="_idIndexMarker416"/> require additional processes such as detection<a id="_idIndexMarker417"/> and compositing to detect the objects that the upscaler can improve and ensure that they get put back into the image correctly. This does lead to a different potential problem where the objects are of a higher quality than the rest of the image, but unless the difference is drastic, people don’t seem to notice <span class="No-Break">very much.</span></p>
			<h3>Color correction</h3>
			<p>Color correction<a id="_idIndexMarker418"/> is a process<a id="_idIndexMarker419"/> that takes place in every camera device anywhere. Taking an image from the camera sensor and turning it into an image that we can look at requires many different processes, one of which is color correction – if for nothing else than to correct for the color of the lighting that the picture was taken in. Our eyes and brains automatically correct for light color, but cameras need to do it explicitly. Even once a photo is taken, that doesn’t mean that color correction is done. Most professional photographers take pictures in a “raw” format that allows them to set the colors exactly, without the loss of converting them <span class="No-Break">multiple times.</span></p>
			<p class="callout-heading">Author’s note</p>
			<p class="callout">One lie that our cameras tell us is that an image is made up of many pixels that have red, blue, and green elements. In reality, our cameras take separate images of each color <a id="_idIndexMarker420"/>and then join them together in a process called <strong class="bold">debayering</strong>. In fact, most cameras cheat even more by having twice as many green sub-pixels as red or blue, as human eyes are more sensitive to details in green. The problem is that these pixels never line up perfectly when re-aligned, and poorly done debayering means that your images will have weird color artifacts at the edges of <span class="No-Break">the colors.</span></p>
			<p>However, color is not just an objective thing. Think about the film <em class="italic">The Matrix</em>. Have you ever noticed how green everything is inside the Matrix? That was done<a id="_idIndexMarker421"/> on purpose. This use of color is called “grading” (though confusingly, sometimes, correction is also called grading). Humans are weirdly connected to colors on an emotional level, and changing them can tweak our emotional connection to an image. For example, think about how we say “blue” to mean “sad” or how an image full of browns and reds might make you feel a chill like autumn has <span class="No-Break">just begun.</span></p>
			<p>Both of these color<a id="_idIndexMarker422"/> tasks can be performed by AI. There are AI tools<a id="_idIndexMarker423"/> out there that can convert an image to match a particular emotional style or to match another image that you have. Here, the trick for the AI is not in changing the colors of an image, it’s in changing them in a particular way that will be pleasing to the humans involved. You probably don’t want to turn your green shoes into red just because you ask the AI to turn the photo to fall colors, but the green leaves need to <span class="No-Break">be adjusted.</span></p>
			<h3>Denoising</h3>
			<p>Noise is another<a id="_idIndexMarker424"/> of the inevitable issues with pictures that today’s cameras<a id="_idIndexMarker425"/> do their best to hide, but it’s a part of any image you take – especially in dark conditions. Noise reduction in cameras usually works by applying a slight blur to all pixels in the image (often before the color correction happens). This can help to smooth out any noise coming from the sensor, but it can’t do a perfect job. Fancier techniques have become extremely common since modern cameras now have substantial computer power in them (and many are in our phones). In fact, most of today’s phone processors have some sort of AI acceleration just so that they can improve the photos <span class="No-Break">being taken.</span></p>
			<p>Denoising can be done in many different ways, but a recent innovation is diffusion models. Diffusion models<a id="_idIndexMarker426"/> are really just using a <strong class="bold">diffusion</strong> process to train a neural network – one in which the image is given to the model with blur or noise added and it’s tasked with providing the original image. This process can be made deeper, by repeating the diffusion process multiple times and asking the model to clean it up each time. This allows a model to create incredibly detailed images from very little information. Unfortunately, information loss is information loss, and while diffusion models can create new information, restoring the original information is impossible without some kind of guidance. This type of AI denoising is actually responsible for one of the innovations<a id="_idIndexMarker427"/> we’ll talk about<a id="_idIndexMarker428"/> more in the <em class="italic">Text-guided image </em><span class="No-Break"><em class="italic">generation</em></span><span class="No-Break"> section.</span></p>
			<h2 id="_idParaDest-159"><a id="_idTextAnchor160"/>The future of image quality upgrading</h2>
			<p>There is a fundamental question<a id="_idIndexMarker429"/> that limits what an AI can do for image quality. Is it okay for the AI to “invent” a new detail that doesn’t belong there? If the answer is yes, then there is no practical limit to what AI upscaling can accomplish. It’s possible that we could have AI upscalers that generate the very atoms of a person’s skin if we zoom in far enough. Unfortunately, the answer is generally considered to be “no.” We are picky about our images and want them to be not just detailed, but accurate. This means that quality and detail will always be limited, and there isn’t a whole lot we can do to do to <span class="No-Break">avoid that.</span></p>
			<p>That’s not to say that it’s impossible to improve further. It’s possible that we might be able to make AIs capable of getting the details they need from other sources. For example, right now, we have video upscalers that can upscale a frame of video by borrowing data from the frames that surround them. This lets the AI look beyond the current frame to find patterns and details that it can copy back to the working frame. You might be able to upscale your old family home videos by providing photographs to fill in the missing quality. Additionally, you might have an AI that learns what a given person looked like at various points in their life (for example, your school yearbooks and family photos) that can then interpolate that data to fill in specific times of <span class="No-Break">your video.</span></p>
			<p>The quest for higher quality will never end, and people will always want to see the next improvement. That said, there are practical limits that prevent a given image from being improved forever. However, what if you don’t care about accuracy, and you just want the greatest quality image possible? Maybe an image that is created from just a sentence or two? Well, th<a id="_idTextAnchor161"/>at’s what we’ll look <span class="No-Break">at next.</span></p>
			<h1 id="_idParaDest-160"><a id="_idTextAnchor162"/>Text-guided image generation</h1>
			<p>Text-guided image generation<a id="_idIndexMarker430"/> is an interesting category of generative AI. OpenAI had several developers release a paper called <em class="italic">Learning Transferable Visual Models From Natural Language Supervision</em> (https://arxiv.org/abs/2103.00020). Though I prefer the summary title they posted on their blog <em class="italic">CLIP: Connecting Text and Images</em>. CLIP was mentioned in <a href="B17535_08.xhtml#_idTextAnchor136"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>, <em class="italic">Applying the Lessons of Deepfakes</em>, but we’ll talk about<a id="_idIndexMarker431"/> it some <span class="No-Break">more here.</span></p>
			<h2 id="_idParaDest-161"><a id="_idTextAnchor163"/>CLIP</h2>
			<p>CLIP is actually a pair of neural network<a id="_idIndexMarker432"/> encoders. One is trained on images while the other is trained on text. So far, this isn’t very unusual. The real trick comes from how the two are linked. Essentially, both encoders are passed data from the same image; the image encoder gets the image, the text encoder gets the image’s description, and then the encoding they generate is compared to each other. This training methodology effectively trains two separate models to create the same output given <span class="No-Break">related inputs.</span></p>
			<p>That might still sound confusing, so let’s look at it another way. Imagine a room where there are hundreds of little boxes in a grid. Two men take turns in the room; one is given an image, and the other one is given a sentence. They are tasked with placing their objects in one of the boxes but with no information as to what is “correct”. They can study their own objects but then must pick one of the boxes. Then, they’re scored based on how close they were to each other’s placement and only on that similarity, with no assumptions that there was a better choice than the ones the two <span class="No-Break">men picked.</span></p>
			<p>The idea is that, eventually, they’ll settle on a set of rules where both the image and the text description of the object will be placed in the same boxes. This metaphor is very close to how it actually works, except instead of just putting the object in one box, they score the text and image on how they fit into each of the hundreds <span class="No-Break">of boxes.</span></p>
			<p>When fully trained, similar images should score similarly, similar text descriptions should be scored similarly, and matching image and text descriptions should be <span class="No-Break">scored similarly:</span></p>
			<div>
				<div id="_idContainer095" class="IMG---Figure">
					<img src="image/B17535_09_002.jpg" alt="Figure 9.2 – An example of CLIP’s shared latent space (Mountain photo by Rohit Tandon via Unsplash)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.2 – An example of CLIP’s shared latent space (Mountain photo by Rohit Tandon via Unsplash)</p>
			<h2 id="_idParaDest-162"><a id="_idTextAnchor164"/>Image generation with CLIP</h2>
			<p>So, now we have a model<a id="_idIndexMarker433"/> that turns an image and its associated text into a <strong class="bold">shared latent space</strong>. How does this generate images? Well, that’s where<a id="_idIndexMarker434"/> other tricks come in. We’re going to gloss over the specifics here because there are so many competing models that have come out recently that all do things a little differently and writing about each one in depth would be a book of its own, but the basic idea is similar between them, so let’s go <span class="No-Break">with that.</span></p>
			<p>Okay, so to train the model, the first step is to get the CLIP latent. Some models use both the image and the text embeddings, some go for one or the other at random, while others pick just the text embedding. No matter which latent they use, the process is similar. After generating the CLIP embedding, a model is trained to turn that embedding into <span class="No-Break">an image.</span></p>
			<p>Here is a quick summary of just a few of the big text-to-image models on the market <span class="No-Break">right now:</span></p>
			<ul>
				<li>Stability AI’s <strong class="bold">Stable Diffusion</strong> uses a repeating diffusion process<a id="_idIndexMarker435"/> that starts with a random noise pattern (or, in image-to-image mode, an image) and then iterates the diffusion process over and over in the latent space and uses a final decoder to convert the embedding back into <span class="No-Break">an image</span></li>
				<li>Google’s <strong class="bold">Imagen</strong> works with a similar method, but the diffusion<a id="_idIndexMarker436"/> process occurs in the image’s pixel space, and there is no need for a final conversion back into an <span class="No-Break">image space</span></li>
				<li>OpenAI’s <strong class="bold">Dall-E</strong> is, instead, a transformer-based model<a id="_idIndexMarker437"/> that generates an image by passing the text representation through a number of attention layers and other layers to create the <span class="No-Break">image output</span></li>
			</ul>
			<p>The effective result in each case<a id="_idIndexMarker438"/> is an image that should match the text given to guide <span class="No-Break">the generation.</span></p>
			<p>So, where will image generation go <span class="No-Break">from here?</span></p>
			<h2 id="_idParaDest-163"><a id="_idTextAnchor165"/>The future of image generation</h2>
			<p>Many other techniques<a id="_idIndexMarker439"/> have been developed and announced, and just keeping up with the various releases is a significant endeavor beyond the scope of this book. The really incredible thing about all these different techniques is that they were all developed in rapid succession. The pace seems to be continually accelerating and doubtless shortly after this book comes out, all of this information will be superseded by a <span class="No-Break">new innovation.</span></p>
			<p>This field is rapidly developing, so it’s impossible to predict where future limitations might show up. It’s possible that we’re near the tail end of this current burst of development, and we’ll see a quite few years before a new innovation. But it’s also possible that we’ll see a rapid development of new functionality that makes today’s image generators look primitive within a <span class="No-Break">few months.</span></p>
			<p>The limitations that abound right now are not about resolution, detail, or even quality: it’s simply a matter of learning the best way to get results. Thanks to the public distribution of Stable Diffusion, there is a huge community of people experimenting with the technology, and it’s nearly impossible to go more than a couple of days without some new technique<a id="_idIndexMarker440"/> or idea coming<a id="_idIndexMarker441"/> forward that promises<a id="_idIndexMarker442"/> new frontiers. Things such as <strong class="bold">Textual Inversion</strong>, <strong class="bold">LoRA</strong>, and <strong class="bold">DreamBooth</strong> offer new ways to tune Stable Diffusion results on consumer hardware, while new functionality such as <strong class="bold">inpainting</strong> and <strong class="bold">outpainting</strong> allow you to fill in<a id="_idIndexMarker443"/> or expand<a id="_idIndexMarker444"/> an <span class="No-Break">image, respectively.</span></p>
			<p>As time goes on, we’ll get more comfortable<a id="_idIndexMarker445"/> with using these image-generation models, and we’ll be better able to design them to maximize their performance. At that point, we’ll be able to predict what limits image generation has. At this point, we’re just too close to the mountain directly in front of us to see <span class="No-Break">the top.</span></p>
			<h1 id="_idParaDest-164"><a id="_idTextAnchor166"/>Generating sound</h1>
			<p>Sound generation<a id="_idIndexMarker446"/> is another one of those fields we could keep subdividing down and down until all the room we have in the book is taken up by headings listing the different methods of sound generation. For the sake of brevity, we’ll group them all here and cover a few big <span class="No-Break">subfields instead.</span></p>
			<h2 id="_idParaDest-165"><a id="_idTextAnchor167"/>Voice swapping</h2>
			<p>The first thing that most people<a id="_idIndexMarker447"/> think about when they learn<a id="_idIndexMarker448"/> they can swap faces is the question of whether they can swap voices too. The answer is quite unsatisfying: yes, but you probably don’t want to. There are AIs out there that can swap voices but they all suffer from various problems: from sounding like a robot, to lacking inflection, to not matching the person involved, to being very expensive and exclusive. If you’re doing anything with even moderate production value, you’ll get much better use out of <em class="italic">natural</em> intelligence: finding an impressionist who can do an impersonation of the voice. AI technology is just not there (yet). Even <em class="italic">Fall</em>, a movie that famously did “voice deepfakes” actually just re-recorded the voices in the studio (a process called automated dialog replacement or looping), and then the AI was used to adjust their mouths to match the <span class="No-Break">new dialog.</span></p>
			<p>Voice-swapping technology will undoubtedly improve, and there are companies that offer very high-quality voice models for various projects, but they’re very expensive and still a niche use case. However, if the demand remains, it’s inevitable that someone will come out with a new technique or improvement that will make high-quality voice swapping<a id="_idIndexMarker449"/> as easy<a id="_idIndexMarker450"/> as <span class="No-Break">other deepfakes.</span></p>
			<h2 id="_idParaDest-166"><a id="_idTextAnchor168"/>Text-guided music generation</h2>
			<p>This technique works a lot like<a id="_idIndexMarker451"/> text-to-image<a id="_idIndexMarker452"/> generation. In fact, most of the implementations actually use a text-to-image generator under the hood. The basic technique is to turn audio into a spectrogram image of the audio and then train a text-to-image model on it. This means that the same image generators can be used to generate audio spectrograms that can then be converted back into an <span class="No-Break">audio stream.</span></p>
			<p>This technique is lossy and limits the quality of the output because the spectrogram image can only hold so much frequency. In addition, there is the problem that spectrogram images represent just a small slice of time. This means that generating the audio has to happen in small chunks a few seconds long. Riffusion (an audio generation model built on Stable Diffusion) was trained on spectrograms about 8 seconds long. This means that even if you come up with tricks such as joining multiple spectrograms, you’re going to have a new generation every 8 seconds or so. It’s possible to increase both the time slice and frequency by increasing the resolution of the image underlying the generation, but the limits would still be there, just higher. It’s unlikely that that current technology will be able to generate full length songs of 3-4 minutes even with reasonable advances <span class="No-Break">in technique.</span></p>
			<div>
				<div id="_idContainer096" class="IMG---Figure">
					<img src="image/B17535_09_003.jpg" alt="Figure 9.3 – An example of a spectrogram made by Riffusion"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.3 – An example of a spectrogram made by Riffusion</p>
			<p>Music that has low-frequency resolution and changes significantly every few seconds is a far cry from what we, typically, expect as music listeners. It’s likely that someone will create a text-to-music generator that will not involve the audio-image conversion process, but that would be a new creation that will probably have its own quirks and is getting into the speculative futurism<a id="_idIndexMarker453"/> that we want to avoid<a id="_idIndexMarker454"/> in <span class="No-Break">this chapter.</span></p>
			<h2 id="_idParaDest-167"><a id="_idTextAnchor169"/>The future of sound generation</h2>
			<p>It’s all but inevitable that someone<a id="_idIndexMarker455"/> will create a tool or technique that will blow open the field of sound generation in the same ways that convolutional layers and diffusion models have done for images and transformers have done for language. When that happens, we’ll see a gold rush-style burst of innovation as people adopt and extend the new technology with the same improvements that we’ve seen in the image and text generation fields. Unfortunately, for now, we cannot rely on the great innovations of the future, so we must ground ourselves in the now or the <span class="No-Break">near future.</span></p>
			<p>Modern audio techniques rely on autoencoders or spectrograms to bring the audio into a form that is usable by the AI generation techniques that we have access to now. To this end, it’s possible that these methods could be improved to add more detail or information. For example, usually, spectrograms are done in black and white. What if color were added in a way that more than just linearly increased the amount that could be generated? For example, what if red were used for a repeating melody, blue for another motif, and green to show how the two interact at a much larger timescale? Could this, when paired with larger spectrograms, bring modern sound generation to the point where an entire high-quality song could be generated <span class="No-Break">at once?</span></p>
			<p>What if we spent the time to develop a coding system for a spoken voice that turned it into a written form that could be learned and trained into a text-generation model? If we could define an accent as some combination of how the written form gets turned into audio, could we make an AI that could copy accents like they can painting styles? There are a lot of ways that we might be able to modify audio into a form that our current AI models excel at that would enable improvements of current sound-generation AI across <span class="No-Break">the board.</span></p>
			<p>Not even counting the revolutionary improvements from a whole new paradigm type akin to transformers or convolutions, I think that there are years of substantial evolution available in audio if computer scientists were to put as much attention into audio as they have images<a id="_idIndexMarker456"/> and <span class="No-Break">video recently.</span></p>
			<h1 id="_idParaDest-168"><a id="_idTextAnchor170"/>Deepfakes</h1>
			<p>Of course, this whole book<a id="_idIndexMarker457"/> has been about the past and present of deepfakes, so it makes sense to circle back to their future at the end. There is a lot we can learn about the future of deepfakes from the other AI mentioned in this chapter. This is because, sneakily, all the parts of this chapter have been building up to this section. Deepfakes are, after all, an image generation AI that works on domain-specific images with a <span class="No-Break">shared embedding.</span></p>
			<p>Every area that we’ve explored in this chapter can be used to improve deepfakes, so let’s approach them one at <span class="No-Break">a time.</span></p>
			<h2 id="_idParaDest-169"><a id="_idTextAnchor171"/>Sound generation</h2>
			<p>This one is quite simple<a id="_idIndexMarker458"/> and obvious. The next step after swapping a face would be to swap<a id="_idIndexMarker459"/> the voice too. If we could get a solid voice swap, then deepfakes would be taken to a whole new capability. Making music or other effects could also be useful if you were making a movie without any other people helping, but their utility would be otherwise limited (in deepfakes, other industries would doubtlessly have more use <span class="No-Break">for them).</span></p>
			<h2 id="_idParaDest-170"><a id="_idTextAnchor172"/>Text-guided image generation</h2>
			<p>Right now, you could consider <a id="_idIndexMarker460"/>deepfakes to be “face-guided” and perhaps<a id="_idIndexMarker461"/> that is the best solution, as it lets an actor’s performance shine through, even if they’re wearing a different actor’s face. But text-guided postprocessing could definitely fill a need. There is a famous scene in the film <em class="italic">Blade Runner</em> where the detective, Deckard, examines an image using voice commands to explore the environment (before doing the impossible and shifting the view to the side, which could only happen by inventing new data and wouldn’t be usable for finding clues). This could be seen as a prototype workflow for image modification with text-guided <span class="No-Break">image generation.</span></p>
			<p>Instead of generating a whole new image, we could modify parts of it using masks or filters and combine the AI changes back with the original image. This lets us do things such as write (or say) “<em class="italic">make him smile more</em>” to generate modifications to our image without replacing the whole image. This leverages the power of text guidance for some things, while still letting the face-guided <span class="No-Break">swap happen.</span></p>
			<h2 id="_idParaDest-171"><a id="_idTextAnchor173"/>Improving image quality</h2>
			<p>It is easy to see how improving<a id="_idIndexMarker462"/> image quality could improve deepfakes: you<a id="_idIndexMarker463"/> could improve your input data or output results to be more accurate, of higher quality, or for color correction. Image quality is critical in creating a successful deepfake, but some sources simply don’t provide the data that you really need to get a quality deepfake. For example, Albert Einstein, unfortunately, passed away before HD video cameras could capture all the data that we want for a standard deepfake. Photos and some videos do exist that we could use, but those sources are low-quality, are mostly monochrome, and would generally make for a poor swap. If we could improve that data enough to train a model, we could then bootstrap to create a whole new view of <span class="No-Break">Albert Einstein.</span></p>
			<p>In particular, domain-specific tools will be beneficial to deepfakes. There are already face-specific upscalers that can be used on Faceswap's output to increase the effective resolution of the deepfakes process. Especially when used with very high-quality videos, upscaling the output can bridge the gap between<a id="_idIndexMarker464"/> computational power and the resolution<a id="_idIndexMarker465"/> of the <span class="No-Break">swapped face.</span></p>
			<h2 id="_idParaDest-172"><a id="_idTextAnchor174"/>Text generation</h2>
			<p>This one seems<a id="_idIndexMarker466"/> like it might be the least important for deepfakes, and<a id="_idIndexMarker467"/> in a way, the “generation” part is, yes. But techniques and technologies such as CLIP could be brought into deepfakes in new and unique ways. For example, what if we got rid of the general-purpose face encoder that is a part of all deepfakes and replaced it with a new encoder that was trained like CLIP, contrasting with various faces and data? We might be able to cut one of the largest parts of the model to run separately, allowing us to focus all our energy (and compute) on the decoder that builds the face back instead of wasting time and energy training an encoder just to give us an embedding to feed into <span class="No-Break">the decoder.</span></p>
			<h2 id="_idParaDest-173"><a id="_idTextAnchor175"/>The future of deepfakes</h2>
			<p>I think that with all the innovations<a id="_idIndexMarker468"/> that have come to generative AI recently, we can be sure that deepfakes have not reached the end of the line. There are innovations in these other domains that are just waiting to be implemented in deepfakes. These improvements will allow deepfakes to do new and interesting things while still keeping the essence of what a deepfake is – the swapping of one face <span class="No-Break">with another.</span></p>
			<p>We’ve not yet seen the end of deepfakes in movies. In fact, it seems likely that deepfakes in movies will only grow as the technology becomes more mature, capable, and available. It seems reasonable that entire actors may be deepfaked in the future just to keep a character from changing due to growing older or passing away. Imagine a sequel to <em class="italic">Gone with the Wind</em> done entirely with the original actors animated by AI under the complete direction of the director. It’s feasible that we may see that before too long if AI continues to grow <span class="No-Break">and advance.</span></p>
			<p>Deepfakes are not unbounded; there are still problems to solve, such as resolution, training time, and even data collection. But it’s now possible to make a deepfake face in a resolution higher than early Blu-rays with data collected from black-and-white movies, colorized and cleanly upscaled. Where will we be in 10 years? Or even 20 years? How long until you don’t even bother to get out of bed<a id="_idIndexMarker469"/> to attend a video call where your full body is generated and matched <span class="No-Break">to you?</span></p>
			<h2 id="_idParaDest-174"><a id="_idTextAnchor176"/>The future of AI ethics</h2>
			<p>Our guidelines<a id="_idIndexMarker470"/> from <a href="B17535_03.xhtml#_idTextAnchor054"><span class="No-Break"><em class="italic">Chapter 3</em></span></a><em class="italic">, Examining Deepfake Ethics and Dangers</em>, are still a solid foundation<a id="_idIndexMarker471"/> for ethics (after all, being nice to people never goes out of style), but as we move into the future, we’ll approach entirely new challenges that will need their own <span class="No-Break">ethical guidelines.</span></p>
			<p>When, for example, is it acceptable to replace an actor with a deepfake? Is it acceptable to replace a hard-working human with an AI that is just puppeted by the director? What if it’s a role that nobody wants to (or can) play? <em class="italic">Gollum</em> in Peter Jackson’s <em class="italic">Lord of the Rings</em> was played by the incredibly talented character actor <em class="italic">Andy Serkis</em> (Serkis also played Snoke in Disney’s <em class="italic">Star Wars</em> sequels and many other digital-first characters). Would it be ethical to replace him with <span class="No-Break">an AI?</span></p>
			<p>Chances are that an entirely AI character would be considered ethical by many (probably not the actors, though). But what if a director decided that an actress’s voice wasn’t “girly” enough and replaced it with a squeaky valley girl's voice without the actress’s permission? If it’s the director’s creative vision to replace the actor playing the bad guy with a darker-skinned person? What about the example in the previous section where you replace your pajamas with a well-coordinated outfit for your video call? These questions are harder to answer, and it’s something that we’ll have to evaluate together as society <span class="No-Break">moves forward.</span></p>
			<h1 id="_idParaDest-175"><a id="_idTextAnchor177"/>Summary</h1>
			<p>Generative AI has a huge history and a tremendous future. We’re standing before a vast plane where anything is possible, and we just have to go toward it. That said, not everything is visible today, and we must temper our expectations. The main challenges are the limitations of our computers, time, and research. If we dedicate our time and efforts to solving some of AI’s limitations, we’ll inevitably come up with brand-new leaps that will help us move forward. Even without huge revolutionary improvements though, there are a lot of smaller evolutionary improvements that we can make to improve the capabilities of <span class="No-Break">these models.</span></p>
			<p>The biggest driver of innovation is need. Having more and more people using generative AI and putting it toward novel uses will create the economic and social pushes that generative AI needs to continue being improved on into <span class="No-Break">the future.</span></p>
			<p>This book has all been about getting us to this point where we, the authors, can invite you to go forward and help AI move toward this generative future that, hopefully, we can all see just over <span class="No-Break">that horizon.</span></p>
		</div>
		<div>
			<div id="_idContainer098" class="IMG---Figure">
			</div>
		</div>
	<div style="width:100%; margin-top:20px; "><div style="text-align:left; padding:10px" aria-hidden="true"><span style="font-size: 0.75em">EBSCOhost - printed on 11/27/2023 6:20 AM via . All use subject to <a target="_blank" href="https://www.ebsco.com/terms-of-use">https://www.ebsco.com/terms-of-use</a></span></div></div>
</body></html>