- en: '1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Defining a Business-Ready Generative AI System
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Implementing a **generative AI system** (**GenAISys**) in an organization doesn’t
    stop at simply integrating a standalone model such as GPT, Grok, Llama, or Gemini
    via an API. While this is often a starting point, we often mistake it as the finish
    line. The rising demand for AI, as it expands across all domains, calls for the
    implementation of advanced AI systems that go beyond simply integrating a prebuilt
    model.
  prefs: []
  type: TYPE_NORMAL
- en: A business-ready GenAISys should provide ChatGPT-grade functionality in an organization,
    but also go well beyond it. Its capabilities and features must include **natural
    language understanding** (**NLU**), contextual awareness through memory retention
    across dialogues in a chat session, and agentic functions such as autonomous image,
    audio, and document analysis and generation. Think of a generative AI model as
    an entity with a wide range of functions, including AI agents as agentic co-workers.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will begin the chapter by defining what a business-ready GenAISys is. From
    there, we’ll focus on the central role of a generative AI model, such as GPT-4o,
    that can both orchestrate and execute tasks. Building on that, we will lay the
    groundwork for contextual awareness and memory retention, discussing four types
    of generative AI memory: memoryless, short-term, long-term, and multiple sessions.
    We will also define a new approach to **retrieval-augmented generation** (**RAG**)
    that introduces an additional dimension to data retrieval: instruction and agentic
    reasoning scenarios. Adding instructions stored in a vector store takes RAG to
    another level by retrieving instructions that we can add to a prompt. In parallel,
    we will examine a critical component of a GenAISys: human roles. We will see how,
    throughout its life cycle, an AI system requires human expertise. Additionally,
    we will define several levels of implementation to adapt the scope and scale of
    a GenAISys, not only to business requirements but also to available budgets and
    resources.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we’ll illustrate how contextual awareness and memory retention can
    be implemented using OpenAI’s LLM and multimodal API. A GenAISys cannot work without
    solid memory retention functionality—without memory, there’s no context, and without
    context, there’s no sustainable generation. Throughout this book, we will create
    modules for memoryless, short-term, long-term, and multisession types depending
    on the task at hand. By the end of this chapter, you will have acquired a clear
    conceptual framework for what makes an AI system business-ready and practical
    experience in building the first bricks of an AI controller.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a nutshell, this chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Components of a business-ready GenAISys
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AI controllers and agentic functionality (model-agnostic)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hybrid human roles and collaboration with AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Business opportunities and scope
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contextual awareness through memory retention
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s begin by defining what a business-ready GenAISys is.
  prefs: []
  type: TYPE_NORMAL
- en: Components of a business-ready GenAISys
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A business-ready GenAISys is a modular orchestrator that seamlessly integrates
    standard AI models with multifunctional frameworks to deliver hybrid intelligence.
    By combining generative AI with agentic functionality, RAG, **machine learning**
    (**ML**), web search, non-AI operations, and multiple-session memory systems,
    we are able to deliver scalable and adaptive solutions for diverse and complex
    tasks. Take ChatGPT, for example; people use the name “ChatGPT” interchangeably
    for the generative AI model as well as for the application itself. However, behind
    the chat interface, tools such as ChatGPT and Gemini are part of larger systems—online
    copilots—that are fully integrated and managed by intelligent AI controllers to
    provide a smooth user experience.
  prefs: []
  type: TYPE_NORMAL
- en: 'It was Tomczak (2024) who took us from thinking of generative AI models as
    a collective entity to considering complex GenAISys architectures. His paper uses
    the term “GenAISys” to describe these more complex platforms. Our approach in
    this book will be to expand the horizon of a GenAISys to include advanced AI controller
    functionality and human roles in a business-ready ecosystem. There is no single
    silver-bullet architecture for a GenAISys. However, in this section, we’ll define
    the main components necessary to attain ChatGPT-level functionality. These include
    a generative AI model, memory retention functions, modular RAG, and multifunctional
    capabilities. How each component contributes to the GenAISys framework is illustrated
    in *Figure 1.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.1: GenAISys, the AI controller, and human roles](img/B32304_01_1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.1: GenAISys, the AI controller, and human roles'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now define the architecture of the AI controllers and human roles that
    make up a GenAISys.
  prefs: []
  type: TYPE_NORMAL
- en: AI controllers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At the heart of a business-ready GenAISys is an **AI controller** that activates
    custom ChatGPT-level features based on the context of the input. Unlike traditional
    pipelines with predetermined task sequences, the AI controller operates without
    a fixed order, dynamically adapting tasks—such as web search, image analysis,
    and text generation—based on the specific context of each input. This agentic
    context-driven approach enables the AI controller to orchestrate various components
    seamlessly, ensuring effective and coherent performance of the generative AI model.
  prefs: []
  type: TYPE_NORMAL
- en: 'A lot of work is required to achieve effective results with a custom ChatGPT-grade
    AI controller. However, the payoff is a new class of AI systems that can withstand
    real-world pressure and produce tangible business results. A solid AI controller
    ecosystem can support use cases across multiple domains: customer support automation,
    sales lead generation, production optimization (services and manufacturing), healthcare
    response support, supply chain optimization, and any other domain the market will
    take you! A GenAISys, thus, requires an AI controller to orchestrate multiple
    pipelines, such as contextual awareness to understand the intent of the prompt
    and memory retention to support continuity across sessions.'
  prefs: []
  type: TYPE_NORMAL
- en: The GenAISys must also define human roles, which determine which functions and
    data can be accessed. Before we move on to human roles, however, let’s first break
    down the key components that power the AI controller. As shown in *Figure 1.1*,
    the generative AI model, memory, modular RAG, and multifunctional capabilities
    each play vital roles in enabling flexible, context-driven orchestration. Let’s
    explore how these elements work together to build a business-ready GenAISys. We
    will first define the role of the generative AI model.
  prefs: []
  type: TYPE_NORMAL
- en: Model-agnostic approach to generative AI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When we build a sustainable GenAISys, we need model *interchangeability*—the
    flexibility to swap out the underlying model as needed. A generative AI model
    should serve as a component within the system, not as the core that the system
    is built around. That way, if our model is deprecated or requires updating, or
    we simply find a better-performing one, we can simply replace it with another
    that better fits our project.
  prefs: []
  type: TYPE_NORMAL
- en: As such, the generative AI model can be OpenAI’s GPT, Google’s Gemini, Meta’s
    Llama, xAI’s Grok, or any Hugging Face model, as long as it supports the required
    tasks. Ideally, we should choose a multipurpose, multimodal model that encompasses
    text, vision, and reasoning abilities. Bommasani et al. (2021) provide a comprehensive
    analysis of such foundation models, whose scope reaches beyond LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'A generative AI model has two main functions, as shown in *Figure 1.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Orchestrates** by determining which tasks need to be triggered based on the
    input. This input can be a user prompt or a system request from another function
    in the pipeline. The orchestration function agent can trigger web search, document
    parsing, image generation, RAG, ML functions, non-AI functions, and any other
    function integrated into the GenAISys.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Executes** the tasks requested by the orchestration layer or executes a task
    directly based on the input. For example, a simple query such as requesting the
    capital of the US will not necessarily require complex functionality. However,
    a request for document analysis might require several functions (chunking, embedding,
    storing, and retrieving).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 1.2: A generative AI model to orchestrate or execute tasks](img/B32304_01_2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.2: A generative AI model to orchestrate or execute tasks'
  prefs: []
  type: TYPE_NORMAL
- en: Notice that *Figure 1.2* has a unique feature. There are no arrows directing
    the input, orchestration, and execution components. Unlike traditional hardcoded
    linear pipelines, a flexible GenAISys has its components unordered. We build the
    components and then let automated scenarios selected by the orchestration function
    order the tasks dynamically.
  prefs: []
  type: TYPE_NORMAL
- en: 'This flexibility ensures the system’s adaptability to a wide range of tasks.
    We will not be able to build a system that solves every task, but we can build
    one that satisfies a wide range of tasks within a company. Here are two example
    workflows that illustrate how a GenAISys can dynamically sequence tasks based
    on the roles involved:'
  prefs: []
  type: TYPE_NORMAL
- en: Human roles can be configured so that, in some cases, the user input executes
    a simple API call to provide a straightforward response, such as requesting the
    capital of a country. In this case, the generative AI model executes a request
    directly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: System roles can be configured dynamically to orchestrate a set of instructions,
    such as searching the web first and then summarizing the web page. In this case,
    the system goes through an orchestration process to produce an output.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The possibilities are unlimited; however, all the scenarios will rely on the
    memory to ensure consistent, context-aware behavior. Let’s look at memory next.
  prefs: []
  type: TYPE_NORMAL
- en: Building the memory of a GenAISys
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Advanced generative AI models such as OpenAI’s GPT, Meta’s Llama, xAI’s Grok,
    Google’s Gemini, and many Hugging Face variants are *context-driven* regardless
    of their specific version or performance level. You will choose the model based
    on your project, but the basic rule remains simple:'
  prefs: []
  type: TYPE_NORMAL
- en: No-context => No meaningful generation
  prefs: []
  type: TYPE_NORMAL
- en: When we use ChatGPT or any other copilot, we have nothing to worry about as
    contextual memory is handled for us. We just start a dialogue, and things run
    smoothly as we adapt our prompt to the level of responses we are obtaining. However,
    when we develop a system with a generative AI API from scratch, we have to explicitly
    build contextual awareness and memory retention.
  prefs: []
  type: TYPE_NORMAL
- en: 'Four approaches stand out among the wide range of possible memory retention
    strategies with an API:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Stateless and memoryless session**: A request is sent to the API, and a response
    is returned with no memory retention functionality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Short-term memory session**: The exchanges between the requests and responses
    are stored in memory during the session but not beyond.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Long-term memory of multiple sessions**: The exchanges between the requests
    and responses are stored in memory and memorized even after the session ends.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Long-term memory of multiple cross-topic sessions**: This feature links the
    long-term memory of multiple sessions to other sessions. Each session is assigned
    a role: a system or multiple users. This feature is not standard in platforms
    such as ChatGPT but is essential for workflow management within organizations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 1.3* sums up these four memory architectures. We’ll demonstrate each
    configuration in Python using GPT-4o in the upcoming section, *Contextual awareness
    and memory retention*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.3: Four different GenAISys memory configurations](img/B32304_01_3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.3: Four different GenAISys memory configurations'
  prefs: []
  type: TYPE_NORMAL
- en: 'These four memory types serve as a starting point that can be expanded as necessary
    when developing a GenAISys. However, practical implementations often require additional
    functionality, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Human roles** to define users or groups of users that can access session
    history or sets of sessions on multiple topics. This will take us beyond ChatGPT-level
    platforms. We will introduce this aspect in [*Chapter 2*](Chapter_2.xhtml#_idTextAnchor055),
    *Building the Generative AI Controller*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Storage strategies** to define what we need to store and what we need to
    discard. We will introduce storage strategies and take this concept further with
    a Pinecone vector store in [*Chapter 3*](Chapter_3.xhtml#_idTextAnchor085), *Integrating
    Dynamic RAG into the GenAISys*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are native distinctions between two key categories of memorization in
    generative models:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Semantic memory**, which contains facts such as hard science'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Episodic memory**, which contains personal timestamped memories such as personal
    events in time and business meetings'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can see that building a GenAISys’s memory requires careful design and deliberate
    development to implement ChatGPT-grade memory and additional memory configurations,
    such as long-term, cross-topic sessions. The ultimate goal, however, of this advanced
    memory system is to enhance the model’s contextual awareness. While generative
    AI models such as GPT-4o have inbuilt contextual awareness, to expand the scope
    of a context-driven system such as the GenAISys we’re building, we need to integrate
    advanced RAG functionality.
  prefs: []
  type: TYPE_NORMAL
- en: RAG as an agentic multifunction co-orchestrator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we explain the motivations for using RAG for three core functions
    within a GenAISys:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Knowledge retrieval:** Retrieving targeted, nuanced information'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Context window optimization:** Engineering optimized prompts'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Agentic orchestration of multifunctional capabilities**: Triggering functions
    dynamically'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s begin with knowledge retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Knowledge retrieval
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Generative AI models excel when it comes to revealing parametric knowledge
    that they have learned, which is embedded in their weights. This knowledge is
    learned during training and embedded in models such as GPT, Llama, Grok, and Gemini.
    However, that knowledge stops at the cutoff date when no additional data is fed
    to the model. At that point, to update or supplement it, we have two options:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Implicit knowledge**: Fine-tune the model so that more trained knowledge
    is added to its weights (parametric). This process can be challenging if you are
    working with dynamic data that changes daily, such as weather forecasts, newsfeeds,
    or social media messages. It also comes with costs and risks if the fine-tuning
    process doesn’t work that well for your data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Explicit knowledge**: Store the data in files or embed data in vector stores.
    The knowledge will then be structured, accessible, traceable, and updated. We
    can then retrieve the information with advanced queries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s important to note here that static implicit knowledge cannot scale effectively
    without dynamic explicit knowledge. More on that in the upcoming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Context window optimization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Generative AI models are expanding the boundaries of context windows. For example,
    at the time of writing, the following are the supported context lengths:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Llama 4 Scout: 10 million tokens'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gemini 2.0 Pro Experimental: 2 million tokens'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Claude 3.7 Sonnet: 200,000 tokens'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GPT-4o: 128,000 tokens'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While impressive, these large context windows can be expensive in terms of token
    costs and compute. Furthermore, the main issue is that their precision diminishes
    when the context becomes too large. Also, we don’t need the largest context window
    but only the one that best fits our project. This can justify implementing RAG
    if necessary to optimize a project.
  prefs: []
  type: TYPE_NORMAL
- en: The chunking process of RAG splits large content into more nuanced groups of
    tokens. When we embed these chunks, they become vectors that can be stored and
    efficiently retrieved from vector stores. This approach ensures we use only the
    most relevant context per task, minimizing token usage and maximizing response
    quality. Thus, we can rely on generative AI capabilities for parametric implicit
    knowledge and RAG for large volumes of explicit non-parametric data in vector
    stores. We can take RAG further and use the method as an orchestrator.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Agentic orchestrator of multifunctional capabilities
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The AI controller bridges with RAG through the generative AI model. RAG is used
    to augment the model’s input with a flexible range of instructions. Now, using
    RAG to retrieve instructions might seem counterintuitive at first—but think about
    it. If we store instructions as vectors and retrieve the best set for a task,
    we get a fast, adaptable way to enable agentic functionality, generate effective
    results, and avoid the need to fine-tune the model every time we change our instruction
    strategies for how we want it to behave.
  prefs: []
  type: TYPE_NORMAL
- en: 'These instructions act as optimized prompts, tailored to the task at hand.
    In this sense, RAG becomes part of the orchestration layer of the AI system. A
    vector store such as Pinecone can store and return this functional information,
    as illustrated in *Figure 1.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.4: RAG orchestration functionality](img/B32304_01_4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.4: RAG orchestration functionality'
  prefs: []
  type: TYPE_NORMAL
- en: 'The orchestration of these scenarios is performed through the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scenario retrieval**: The AI controller will receive structure instructions
    (scenarios) from a vector database, such as Pinecone, adapted to the user’s query'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dynamic task activation**: Each scenario specifies a series of tasks, such
    as web search, ML algorithms, standard SQL queries, or any function we need'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Adding classical functions and ML functionality to the GenAISys enhances its
    capabilities dramatically. The modular architecture of a GenAISys makes this multifunctional
    approach effective, as in the following use cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Web search** to perform real-time searches to augment inputs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Document analysis** to process documents and populate the vector store'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Document search** to retrieve parts of the processed documents from the vector
    store'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ML** such as **K-means clustering** (**KMC**) to group data and **k-nearest
    neighbors** (**KNN**) for similarity searches'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SQL queries** to execute rule-based retrieval on structured datasets'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any other function required for your project or workflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RAG remains a critical component of a GenAISys, which we will build into our
    GenAISys in [*Chapter 3*](Chapter_3.xhtml#_idTextAnchor085), *Integrating Dynamic
    RAG into the GenAISys*. In [*Chapter 3*](Chapter_3.xhtml#_idTextAnchor085)*, Integrating
    Dynamic RAG into the GenAISys*, we will also enhance the system with multifunctional
    features.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll now move on to the human roles, which form the backbone of any GenAISys.
  prefs: []
  type: TYPE_NORMAL
- en: Human roles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Contrary to popular belief, the successful deployment and operation of a GenAISys—such
    as the ChatGPT platform—relies heavily on human involvement throughout its entire
    life cycle. While these tools may seem to handle complex tasks effortlessly, behind
    the scenes are multiple layers of human expertise, oversight, and coordination
    that make their smooth operation possible.
  prefs: []
  type: TYPE_NORMAL
- en: Software professionals must first design the architecture, process massive datasets,
    and fine-tune the system on million-dollar servers equipped with cutting-edge
    compute resources. After deployment, large teams are required to monitor, validate,
    and interpret system outputs—continuously adapting them in response to errors,
    emerging technologies, and regulatory changes. On top of that, when it comes to
    deploying these systems within organizations—whether inside corporate intranets,
    public-facing websites, research environments, or learning management systems—it
    takes cross-functional coordination efforts across multiple domains.
  prefs: []
  type: TYPE_NORMAL
- en: These tasks require high levels of expertise and qualified teams. Humans are,
    therefore, not just irreplaceable; they are critical! They are architects, supervisors,
    curators, and guardians of the AI systems they create and maintain.
  prefs: []
  type: TYPE_NORMAL
- en: GenAISys implementation and governance teams
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Implementing a GenAISys requires technical skills and teamwork to gain the
    support of end users. It’s a collaborative challenge between AI controller design,
    user roles, and expectations. To anyone who thinks that deploying a real-world
    AI system is just about getting access to a model—such as the latest GPT, Llama,
    or Gemini—a close look at the resources required will reveal the true challenges.
    A massive number of human resources might be involved in the development, deployment,
    and maintenance of an AI system. Of course, not every organization will need all
    of these roles, but we must recognize the range of skills involved, such as the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Project manager** (**PM**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Product manager
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Program manager
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ML engineer** (**MLE**)/data scientist'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Software developer/**backend engineer** (**BE**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cloud engineer** (**CE**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data engineer** (**DE**) and privacy manager'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: UI/UX designer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compliance and regulatory officer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Legal counsel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security engineer** (**SE**) and security officer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Subject-matter experts for each domain-specific deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Quality assurance engineer** (**QAE**) and tester'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical documentation writer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: System maintenance and support technician
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: User support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trainer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are just examples—just enough to show how many different roles are involved
    in building and operating a full-scale GenAISys. *Figure 1.5* shows that designing
    and implementing a GenAISys is a continual process, where human resources are
    needed at every stage.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.5: A GenAISys life cycle](img/B32304_01_5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.5: A GenAISys life cycle'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that a GenAISys life cycle is a never-ending process:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Business requirements** will continually evolve with market constraints'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GenAISys** design will have to adapt with each business shift'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AI controller** specifications must adapt to technological progress'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Implementation** must adapt to ever-changing business specifications'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**User feedback** will drive continual improvement'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-world AI relies heavily on human abilities—the kind of contextual and technical
    understanding that AI alone cannot replicate. AI can automate a wide range of
    tasks effectively. But it’s humans who bring the deep insight needed to align
    those systems with real business goals.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take this further and look at a RACI heatmap to show why humans are a
    critical component of a GenAISys.
  prefs: []
  type: TYPE_NORMAL
- en: GenAISys RACI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Organizing a GenAISys project requires human resources that go far beyond what
    AI automation alone can provide. **RACI** is a responsibility assignment matrix
    that helps define roles and responsibilities for each task or decision by identifying
    who is **Responsible**, **Accountable**, **Consulted**, and **Informed**. RACI
    is ideal for managing the complexity of building a GenAISys. It adds structure
    to the growing list of human roles required during the system’s life cycle and
    provides a pragmatic framework for coordinating their involvement.
  prefs: []
  type: TYPE_NORMAL
- en: 'As in any complex project, teams working on a GenAISys need to collaborate
    across disciplines, and RACI helps define who does what. Each letter in RACI stands
    for a specific type of role:'
  prefs: []
  type: TYPE_NORMAL
- en: '**R (Responsible):** The person(s) who works actively on the task. They are
    responsible for the proper completion of the work. For example, an MLE may be
    responsible for processing datasets with ML algorithms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A (Accountable):** The person(s) answerable for the success or failure of
    a task. They oversee the task that somebody else is responsible for carrying out.
    For example, the **product owner** (**PO**) will have to make sure that the MLE’s
    task is done on time and in compliance with the specifications. If not, the PO
    will be accountable for the failure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**C (Consulted)**: The person(s) providing input, advice, and feedback to help
    the others in a team. They are not responsible for executing the work. For example,
    a subject-matter expert in retail may help the MLE understand the goal of an ML
    algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**I (Informed)**: The person(s) kept in the loop about the progress or outcome
    of a task. They don’t participate in the task but want to be simply informed or
    need to make decisions. For example, a **data privacy officer** (**DPO**) would
    like to be informed about a system’s security functionality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A RACI heatmap typically contains legends for each human role in a project.
    Let’s build a heatmap with the following roles:'
  prefs: []
  type: TYPE_NORMAL
- en: The **MLE** develops and integrates AI models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **DE** designs data management pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **BE** builds API interactions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **frontend engineer** (**FE**) develops end user features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **UI/UX designer** designs user interfaces
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **CE/DevOps engineer** manages cloud infrastructure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **prompt engineer** (**PE**) designs optimal prompts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **SE** handles secure data and access
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **DPO** manages data governance and regulation compliance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **legal/compliance officer** (**LC**) reviews the legal scope of a project
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **QAE** tests the GenAISys
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **PO** defines the scope and scale of a product
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **PM** coordinates resources and timelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **technical writer** (**TW**) produces documentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **vendor manager** (**VM**) communicates with external vendors and service
    providers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Not every GenAISys project will include all of these roles, but depending on
    the scope and scale of the project, many of them will be critical. Now, let’s
    list the key responsibilities of the roles defined above in a typical generative
    AI project:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model**: AI model development'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Controller**: Orchestration of APIs and multimodal components'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pipelines**: Data processing and integration workflows'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**UI/UX**: User interface and experience design'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security**: Data protection and access control'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DevOps**: Infrastructure, scaling, and monitoring'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prompts**: Designing and optimizing model interactions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**QA**: Testing and quality assurance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ve defined the roles and the tasks. Now, we can show how they can be mapped
    to a real-world scenario. *Figure 1.6* illustrates an example RACI heatmap for
    a GenAISys.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.6: Example of a RACI heatmap](img/B32304_01_6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.6: Example of a RACI heatmap'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, in this heatmap, the MLE has the following responsibilities:'
  prefs: []
  type: TYPE_NORMAL
- en: (**R**)esponsible and (**A**)ccountable for the model, which could be GPT-4o.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (**R**)esponsible and (**A**)ccountable for the prompts for the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (**C**)onsulted as an expert for the controller, the pipeline, and testing (QA)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (**I**)nformed about the UI/UX, security, and DevOps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can sum it up with one simple rule for a GenAISys:'
  prefs: []
  type: TYPE_NORMAL
- en: No humans -> no system!
  prefs: []
  type: TYPE_NORMAL
- en: We can see that *we* are necessary during the whole life cycle of a GenAISys,
    from design to maintenance and support, including continual evolutions to keep
    up with user feedback. Humans have been and will be here for a long time! Next,
    let’s explore the business opportunities that a GenAISys can unlock.
  prefs: []
  type: TYPE_NORMAL
- en: Business opportunities and scope
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: More often than not, we will not have access to the incredible billion-dollar
    resources of OpenAI, Meta, xAI, or Microsoft Azure to build ChatGPT-like platforms.
    The previous section showed that beneath a ChatGPT-like, seemingly simple, seamless
    interface, there is a complex layer of expensive infrastructure, rare talent,
    and continuous improvement and evolution that absorb resources only large corporations
    can afford. Therefore, a smarter path from the start is to determine which project
    category we are in and leverage the power of existing modules and libraries to
    build our GenAISys. Whatever the use case, such as marketing, finance, production,
    or support, we need to find the right scope and scale to implement a realistic
    GenAISys.
  prefs: []
  type: TYPE_NORMAL
- en: The first step of any GenAISys is to define the project’s goal (opportunity),
    including its scope and scale, as we mentioned. During this step, you will assess
    the risks, such as costs, confidentiality, and resource availability (risk management).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can classify GenAISys projects into three main business implementation types
    depending on our resources, our objectives, the complexity of our use case, and
    our budget. These are illustrated in *Figure 1.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hybrid approach**: Leveraging existing AI platforms'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Small scope and scale**: A focused GenAISys'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Full-scale generative multi-agent AI system**: A complete ChatGPT-level generative
    AI platform'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 1.7: The three main GenAISys business implementations](img/B32304_01_7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.7: The three main GenAISys business implementations'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin with a hybrid approach, a practical way to deliver business results
    without overbuilding.
  prefs: []
  type: TYPE_NORMAL
- en: Hybrid approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A hybrid framework enables you to minimize development costs and time by combining
    ready-to-use SaaS platforms with custom-built components developed only when necessary,
    such as web search and data cleansing. This way, you can leverage the power of
    generative AI without developing everything from scratch. Let’s go through the
    key characteristics and a few example use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Key characteristics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Relying on proven web services such as OpenAI’s GPT API, AWS, Google AI, or
    Microsoft Azure. These platforms provide the core generative functionality.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customizing your project by integrating domain-specific vector stores and your
    organization’s proprietary datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Focusing development on targeted functionality, such as customer support automation
    or marketing campaign generation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use case examples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Implementing a domain-specific vector store to handle legal, medical, or product-related
    customer queries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building customer support on a social media platform with real-time capabilities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This category offers the ability to do more with less—in terms of both cost
    and development effort. A hybrid system can be a standalone GenAISys or a subsystem
    within a larger generative AI platform where full-scale development isn’t necessary.
    Let’s now look at how a small-scope, small-scale GenAISys can take us even further.
  prefs: []
  type: TYPE_NORMAL
- en: Small scope and scale
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A small-scale GenAISys might include an intelligent, GenAI-driven AI controller
    connected to a vector store. This setup allows the system to retrieve data, trigger
    instructions, and call additional functionality such as web search or ML—without
    needing full-scale infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Key characteristics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A clearly defined profitable system designed to achieve reasonable objectives
    with optimal development time and cost
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The AI controller orchestrates instruction scenarios that, in turn, trigger
    RAG, web search, image analysis, and additional custom tasks that fit your needs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The focus is on high-priority, productive features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use case examples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A GenAISys for document retrieval and summarization for any type of document
    with nuanced analysis through chunked and embedded content
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Augmenting a model such as GPT or Llama with real-time web search to bypass
    its data cutoff date—ideal for applications such as weather forecasting or news
    monitoring that don’t need continual fine-tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This category takes us a step beyond the hybrid approach, while still staying
    realistic and manageable for small to mid-sized businesses or even individual
    departments within large organizations.
  prefs: []
  type: TYPE_NORMAL
- en: Full-scale GenAISys
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you’re working in a team of experts within an organization that has a large
    budget and advanced infrastructure, this category is for you. Your team can build
    a full-scale GenAISys that begins to approach the capabilities of ChatGPT-grade
    platforms.
  prefs: []
  type: TYPE_NORMAL
- en: Key characteristics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A full-blown AI controller that manages and orchestrates complex automated workflows,
    including RAG, instruction scenarios, multimodal functionality, and real-time
    data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Requires significant computing resources and highly skilled development teams
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Think of the GenAISys we’re building in this book as an alpha version—a template
    that can be cloned, configured, and deployed anywhere in the organization as often
    as needed.
  prefs: []
  type: TYPE_NORMAL
- en: Use case examples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'GenAISys is already present in healthcare to assist with patient diagnosis
    and disease prevention. The Institut Curie in Paris, for example, has a very advanced
    AI research team: [https://institut-curie.org/](https://institut-curie.org/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many large organizations have begun implementing GenAISys for fraud detection,
    weather predictions, and legal expertise.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can join one of these large organizations that have the resources to build
    a sustainable GenAISys, whether it be on a cloud platform, local servers, or both.
  prefs: []
  type: TYPE_NORMAL
- en: The three categories—hybrid, small scale, and full scale—offer distinct paths
    for building a GenAISys, depending on your organization’s goals, budget, and technical
    capabilities. In this book, we’ll explore the critical components that make up
    a GenAISys. By the end, you’ll be equipped to contribute to any of these categories
    and offer realistic, technically grounded recommendations for the projects you
    work on.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now lift the hood and begin building contextual awareness and memory retention
    in code.
  prefs: []
  type: TYPE_NORMAL
- en: Contextual awareness and memory retention
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we’ll begin implementing simulations of contextual awareness
    and memory retention in Python to illustrate the concepts introduced in the *Building
    the memory of a GenAISys* section. The goal is to demonstrate practical ways to
    manage context and memory—two features that are becoming increasingly critical
    as generative AI platforms evolve.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the `Contextual_Awareness_and_Memory_Retention.ipynb` file located in
    the `chapter01` folder of the GitHub repository ([https://github.com/Denis2054/Building-Business-Ready-Generative-AI-Systems/tree/main](https://github.com/Denis2054/Building-Business-Ready-Generative-AI-Systems/tree/main)).
    You’ll see that the notebook is divided into five main sections:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Setting up the environment**, building reusable functions, and storing them
    in the `commons` directory of the repository, so we can reuse them when necessary
    throughout the book'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stateless and memoryless session** with semantic and episodic memory'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Short-term memory session** for context awareness during a session'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Long-term memory across multiple sessions** for context retention across
    different sessions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Long-term memory of multiple cross-topic sessions**, expanding long-term
    memory over formerly separate sessions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The goal is to illustrate each type of memory in an explicit process. These
    examples are intentionally kept manual for now, but they will be automated and
    managed by the AI controller we will begin to build in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Due to the probabilistic nature of generative models, you may observe different
    outputs for the same prompt across runs. Make sure to run the entire notebook
    in a single session, as memory retention in this notebook is explicit in different
    cells. In [*Chapter 2*](Chapter_2.xhtml#_idTextAnchor055), this functionality
    will become persistent and fully managed by the AI controller
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to install the environment.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will need a `commons` directory for our GenAISys project. This directory
    will contain the main modules and libraries needed across all notebooks in this
    book’s GitHub repository. The motivation is to focus on designing the system for
    maintenance and support. As such, by grouping the main modules and libraries in
    one directory, we can zero in on a resource that requires our attention instead
    of repeating the setup steps in every notebook. Furthermore, this section will
    serve as a reference point for all the notebooks in this book’s GitHub repository.
    We’ll only describe the downloading of each resource once and then reuse them
    throughout the book to build our educational GenAISys.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we can download the notebook resources from the `commons` directory and
    install the requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to download `grequests.py`, a utility script we will use
    throughout the book. It contains a function to download the files we need directly
    from GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/3-PPMUMLAP0325.png)**Quick tip**: Enhance your coding experience with
    the **AI Code Explainer** and **Quick Copy** features. Open this book in the next-gen
    Packt Reader. Click the **Copy** button'
  prefs: []
  type: TYPE_NORMAL
- en: (**1**) to quickly copy code into your coding environment, or click the **Explain**
    button
  prefs: []
  type: TYPE_NORMAL
- en: (**2**) to get the AI assistant to explain a block of code to you.
  prefs: []
  type: TYPE_NORMAL
- en: '![A white background with a black text  AI-generated content may be incorrect.](img/image_%282%29.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/4.png)**The next-gen Packt Reader** is included for free with the purchase
    of this book. Scan the QR code OR visit [packtpub.com/unlock](http://packtpub.com/unlock),
    then use the search bar to find this book by name. Double-check the edition shown
    to make sure you get the right one.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A qr code on a white background  AI-generated content may be incorrect.](img/Unlock_Code1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The goal of this script is to download a file from any directory of the repository
    by calling the `download` function from `grequests`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This function uses a `curl` command to download files from a specified directory
    and filename. It also includes basic error handling in case of command execution
    failures.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code begins by importing `subprocess` to handle paths and commands. The
    `download` function contains two parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '`directory`: The subdirectory of the GitHub repository where the file is stored'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`filename`: The name of the file to download'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The base URL for the GitHub repository is then defined, pointing to the raw
    files we will need:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We now need to define the file’s full URL with the `directory` and `filename`
    parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The function now defines the `curl` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the `download` command is executed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '`check=True` activates an exception if the `curl` command fails'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`shell=True` runs the command through the shell'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `try-except` block is used to handle errors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We now have a standalone download script that we’ll use throughout the book.
    Let’s go ahead and download the resources we need for this program.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading OpenAI resources
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We need three resources for this notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '`requirements01.py` to install the precise OpenAI version we want'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`openai_setup.py` to initialize the OpenAI API key'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`openai_api_py` contains a reusable function for calling the GPT-4o model,
    so you don’t need to rewrite the same code across multiple cells or notebooks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will be reusing the same functions throughout the book for standard OpenAI
    API calls. You can come back to this section any time you want to revisit the
    installation process. Other scenarios will be added to the `commons` directory
    when necessary.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can download these files with the `download()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The first resource is `requirements01.py`.
  prefs: []
  type: TYPE_NORMAL
- en: Installing OpenAI
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`requirements01.py` makes sure that a specific version of the OpenAI library
    is installed to avoid conflicts with other installed libraries. The code thus
    uninstalls existing versions, force-installs the specified version requested,
    and verifies the result. The function executes the installation with error handling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The first step for the function is to uninstall the current OpenAI library,
    if there is one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The function then installs a specific version of OpenAI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the function verifies that OpenAI is properly installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The output at the end of the function should be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We can now initialize the OpenAI API key.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI API key initialization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There are two methods to initialize the OpenAI API key in the notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Using Google Colab secrets**: Click on the key icon in the left pane in Google
    Colab, as shown in *Figure 1.8*, then click on **Add new secret** and add your
    key with the name of the key variable you will use in the notebook:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.8: Add a new Google secret key](img/B32304_01_8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.8: Add a new Google secret key'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we can use Google’s function to initialize the key by calling it in our
    `openai_setup` function in `openai_setup.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This method is activated if `google_secrets` is set to `True`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '**Custom secure method**: You can also choose a custom method or enter the
    key in the code by setting `google_secrets` to `False`, uncommenting the following
    code, and entering your API key directly, or any method of your choice:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In both cases, the code will create an environment variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The OpenAI API key is initialized. We will now import a custom OpenAI API call.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI API call
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The goal next is to create an OpenAI API call function in `openai_api.py` that
    we can import in two lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The function is thus built to receive four variables when making the call and
    display them seamlessly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The parameters in this function are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`input`: Contains the input (user or system), for example, `Where is Hawaii?`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mrole`: Defines the system’s role, for example, `You are a geology expert.`
    or simply `System.`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mcontent`: Is what we expect the system to be, for example, `You are a geology
    expert.`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`user_role`: Defines the role of the user, for example, `user`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The first part of the code in the function defines the model we will be using
    in this notebook and creates a message object for the API call with the parameters
    we sent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We then define the API call parameters in a dictionary for this notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The dictionary parameters are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`temperature`: Controls the randomness of a response. `0` will produce deterministic
    responses. Higher values(e.g., `0.7`) will produce more creative responses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_tokens`: Limits the maximum number of tokens of a response.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`top_p`: Produces nucleus sampling. It controls the diversity of a response
    by sampling from the top tokens with a cumulative probability of 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`frequency_penalty`: Reduces the repetition of tokens to avoid redundancies.
    `0` will apply no penalty, and `2` a strong penalty. In this case, `0` is sufficient
    because of the high performance of the OpenAI model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`presence_penalty`: Encourages new content by penalizing existing content to
    avoid redundancies. It applies to the same values as for the frequency penalty.
    In this case, due to the high performance of the OpenAI model, it doesn’t require
    this control.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We then initialize the OpenAI client to create an instance for the API calls:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we make the API call by sending the model, the message object, and
    the unpacked parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The function ends by returning the content of the API’s response that we need:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This function will help us focus on the GenAISys architecture without having
    to overload the notebook with repetitive libraries and functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the notebook, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The program provides the input, roles, and message content to the function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`messages_obj` contains the conversation history'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The parameters for the API’s behavior are defined in the `params` dictionary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An API call is made to the OpenAI model using the OpenAI client
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The function returns only the AI’s response content
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A GenAISys will contain many components—including a generative model. You can
    choose the one that fits your project. In this book, the models are used for educational
    purposes only, not as endorsements or recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now build and run a stateless and memoryless session.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Stateless and memoryless session
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A stateless and memoryless session is useful if we only want a single and temporary
    exchange with no stored information between requests. The examples in this section
    are both stateless and memoryless:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Stateless* indicates that each request will be processed independently'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Memoryless* means that there is no mechanism to remember past exchanges'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s begin with a semantic query.
  prefs: []
  type: TYPE_NORMAL
- en: Semantic query
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This request expects a purely semantic, factual response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we call the OpenAI API function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the response is purely semantic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The next query is episodic.
  prefs: []
  type: TYPE_NORMAL
- en: Episodic query with a semantic undertone
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The query in this example is episodic and draws on personal experience. However,
    there is a semantic undertone because of the description of Hawaii. Here’s the
    message, which is rather poetic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '`mcontent` is reused from the semantic query example (“You are an expert in
    geology”), but in this case, it doesn’t significantly influence the response.
    Since the user input is highly personal and narrative-driven, the system prompt
    plays a minimal role.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We could insert external information before the function call if necessary.
    For example, we could add some information from another source, such as a text
    message received that day from a family member:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we call the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We see that the response is mostly episodic with some semantic information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Stateless and memoryless verification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We added no memory retention functionality earlier, making the dialogue stateless.
    Let’s check:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'When we call the function, our dialogue will be forgotten:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The output confirms that the session is memoryless:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The API call is stateless because the OpenAI API does not retain memory between
    requests. If we were using ChatGPT directly, the exchanges would be memorized
    within that session. This has a critical impact on implementation. It means we
    have to build our own memory mechanisms to give GenAISys stateful behavior. Let’s
    start with the first layer: short-term memory.'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Short-term memory session
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The goal of this section is to emulate a short-term memory session using a
    two-step process:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we initiate a session that goes from user input to a response:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: User input => Generative model API call => Response
  prefs: []
  type: TYPE_NORMAL
- en: 'To achieve this first step, we run the session up to the response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The response’s output is stored in `response`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to feed the previous interaction into the next prompt, along
    with a follow-up question:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Explain the situation: `The current dialog session is:`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Add the user’s initial input: `Hawai is on a geological volcano system. Explain:`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Add the response we obtained in the previous call
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Add the user’s new input: `Sum up your previous response in a short sentence
    in a maximum of 20 words.`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The goal here is to compress the session log. We won’t always need to compress
    dialogues, but in longer sessions, large context windows can pile up quickly.
    This technique helps keep the token count low, which matters for both cost and
    performance. In this particular case, we’re only managing one response, so we
    could keep the entire interaction in memory if we wanted to. Still, this example
    introduces a useful habit for scaling up.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the prompt is assembled:'
  prefs: []
  type: TYPE_NORMAL
- en: Call the API function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Display the response
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The scenario is illustrated in the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The output provides a nice, short summary of the dialogue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: This functionality wasn’t strictly necessary here, but it sets us up for the
    longer dialogues we’ll encounter later in the book. Next, let’s build a long-term
    simulation of multiple sessions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keep in mind: Since the session is still in-memory only, the conversation would
    be lost if the notebook disconnects. Nothing is stored on disk or in a database
    yet.'
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Long-term memory of multiple sessions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we’re simulating long-term memory by continuing a conversation
    from an earlier session. The difference here is that we’re not just *remembering*
    a dialogue from a single session—we’re *reusing* content from a past session to
    extend the conversation. At this point, the term “session” takes on a broader
    meaning. In a traditional copilot scenario, one user interacts with one model
    in one self-contained session. Here, we’re blending sessions and supporting multiple
    sub-sessions. Multiple users can interact with the model in a shared environment,
    effectively creating a single global session with branching memory threads. Think
    of the model as a guest in an ongoing Zoom or Teams meeting. You can ask the AI
    guest to participate or stay quiet—and when it joins, it may need a recap.
  prefs: []
  type: TYPE_NORMAL
- en: 'To avoid repeating the first steps of the past conversation, we’re reusing
    the content from the short-term memory session we just ran. Let’s assume the previous
    session is over, but we still want to continue from where we left off:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The output contains the final response from our short-term memory session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The process in this section will build on the previous session, similar to
    how you’d revisit a conversation with an online copilot after some time away:'
  prefs: []
  type: TYPE_NORMAL
- en: Save previous session => Load previous session => Add it to the new session’s
    scenario
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first test whether the API remembers anything on its own:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows that it forgot the conversation we were in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The API forgot the previous call because stateless APIs don’t retain past dialogue.
    It’s up to us to decide what to include in the prompt. We have a few choices:'
  prefs: []
  type: TYPE_NORMAL
- en: Do we want to remember everything with a large consumption of tokens?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do we want to summarize parts or all of the previous conversations?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In a real GenAISys, when an input triggers a request, the AI controller decides
    which is the best strategy to apply to a task. The code now associates the previous
    session’s context and memory with a new request:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The response shows that the system now remembers the past session and has enough
    information to provide an acceptable output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Let’s now build a long-term simulation of multiple sessions across different
    topics.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Long-term memory of multiple cross-topic sessions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section illustrates how to merge two separate sessions into one. This isn’t
    something standard ChatGPT-like platforms offer. Typically, when we start a new
    topic, the copilot only remembers what’s happened in the current session. But
    in a corporate environment, we may need more flexibility—especially when multiple
    users are collaborating. In such cases, the AI controller can be configured to
    allow groups of users to view and merge sessions generated by others in the same
    group.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say we want to sum up two separate conversations—one about Hawaii’s volcanic
    systems, and another about organizing a geological field trip to Arizona. We begin
    by saving the previous long-term memory session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we can start a separate multi-user sub-session from another location,
    Arizona:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'We now expect a response on Arizona, leaving Hawaii out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The response is acceptable. Now, let’s simulate long-term memory across multiple
    topics by combining both sessions and prompting the system to summarize them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The system’s output shows that the long-term memory of the system is effective.
    We see that the first part is about Hawaii:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Then the response continues to the part about Arizona:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: We’ve now covered the core memory modes of GenAISys—from stateless and short-term
    memory to multi-user, multi-topic long-term memory. Let’s now summarize the chapter’s
    journey and move to the next level!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A business-ready GenAISys offers functionality on par with ChatGPT-like platforms.
    It brings together generative AI models, agentic features, RAG, memory retention,
    and a range of ML and non-AI functions—all coordinated by an AI controller. Unlike
    traditional pipelines, the controller doesn’t follow a fixed sequence of steps.
    Instead, it orchestrates tasks dynamically, adapting to the context.
  prefs: []
  type: TYPE_NORMAL
- en: A GenAISys typically runs on a model such as GPT-4o—or whichever model best
    fits your use case. But as we’ve seen, just having access to an API isn’t enough.
    Contextual awareness and memory retention are essential. While ChatGPT-like tools
    offer these features by default, we have to build them ourselves when creating
    custom systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'We explored four types of memory: memoryless, short-term, long-term, and cross-topic.
    We also distinguished semantic memory (facts) from episodic memory (personal,
    time-stamped information). Context awareness depends heavily on memory—but context
    windows have limits. Even if we increase the window size, models can still miss
    the nuance in complex tasks. That’s where advanced RAG comes in—breaking down
    content into smaller chunks, embedding them, and storing them in vector stores
    such as Pinecone. This expands what the system can “remember” and use for reasoning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We also saw that no matter how advanced GenAISys becomes, it can’t function
    without human expertise. From design to deployment, maintenance, and iteration,
    people remain critical throughout the system’s life cycle. We then outlined three
    real-world implementation models based on available resources and goals: hybrid
    systems that leverage existing AI platforms, small-scale systems for targeted
    business needs, and full-scale systems built for ChatGPT-grade performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we got hands-on—building a series of memory simulation modules in
    Python using GPT-4o. These examples laid the groundwork for what comes next: the
    AI controller that will manage memory, context, and orchestration across your
    GenAISys. We are now ready to build a GenAISys AI controller!'
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Is an API generative AI model such as GPT an AI controller? (Yes or No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does a memoryless session remember the last exchange(s)? (Yes or No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is RAG used to optimize context windows? (Yes or No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Are human roles important for the entire life cycle of a GenAISys? (Yes or No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can an AI controller run tasks dynamically? (Yes or No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is a small-scale GenAISys built with a limited number of key features? (Yes
    or No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does a full-scale ChatGPT-like system require huge resources? (Yes or No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is long-term memory necessary across multiple sessions? (Yes or No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do vector stores such as Pinecone support knowledge and AI controller functions?
    (Yes or No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can a GenAISys function without contextual awareness? (Yes or No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Tomczak, J. M. (2024). *Generative AI Systems: A Systems-based Perspective
    on Generative AI.* [https://arxiv.org/pdf/2407.11001](https://arxiv.org/pdf/2407.11001)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zewe, A. (2023, November 9). *Explained: Generative AI.* MIT News. Retrieved
    from [https://news.mit.edu/2023/explained-generative-ai-1109](https://news.mit.edu/2023/explained-generative-ai-1109)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenAI models: [https://platform.openai.com/docs/models](https://platform.openai.com/docs/models)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S.,
    ... & Liang, P. (2021). *On the Opportunities and Risks of Foundation Models.*
    arXiv preprint arXiv:2108.07258\. Retrieved from [https://arxiv.org/abs/2108.07258](https://arxiv.org/abs/2108.07258)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feuerriegel, S., Hartmann, J., Janiesch, C., & Zschech, P. (2023). *Generative
    AI. Business & Information Systems Engineering*. [https://doi.org/10.1007/s12599-023-00834-7](https://doi.org/10.1007/s12599-023-00834-7
    )
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Eloundou et al. (2023). *GPTs are GPTs: An Early Look at the Labor Market Impact
    Potential of Large Language Models.* [https://arxiv.org/abs/2303.10130](https://arxiv.org/abs/2303.10130)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Unlock this book’s exclusive benefits now
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: Scan this QR code or go to [packtpub.com/unlock](http://packtpub.com/unlock),
    then search for this book by name. | ![A qr code on a white background  AI-generated
    content may be incorrect.](img/Unlock.png) |
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| *Note: Keep your purchase invoice ready before you start.* |'
  prefs:
  - PREF_IND
  type: TYPE_TB
