- en: Modern Methods for Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We now know how to convert text strings to numerical vectors that capture some
    meaning. In this chapter, we will look at how to use those with embedding. Embedding
    is the more frequently used term for word vectors and numerical representations.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we are still following the broad outline from our first, that
    is, text→ representations → models→ evaluation → deployment.
  prefs: []
  type: TYPE_NORMAL
- en: We will continue working with text classification as our example task. This
    is mainly because it's a simple task for demonstration, but we can also extend
    almost all of the ideas in this book to solve other problems. The main focus ahead,
    however, is machine learning for text classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'To sum up, in this chapter we will be looking at the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment analysis as a specific class and example of text classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simple classifiers and how to optimize them for your datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensemble methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning for text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are at least 10 to 20 machine learning techniques that are well known
    in the community, ranging from SVMs to several regressions and gradient boosting
    machines. We will select a small taste of these.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/290de774-4008-41d8-91dd-3e7027b94f3b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [https://www.kaggle.com/surveys/2017.](https://www.kaggle.com/surveys/2017)'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding graph shows the most popular machine learning techniques used
    by Kagglers.
  prefs: []
  type: TYPE_NORMAL
- en: We met **Logistic Regression** in the first chapter while working the 20 newsgroups
    dataset. We will revisit **Logistic Regression** and introduce **Naive Bayes**,
    **SVM**, **Decision Trees**, **Random Forests**, and **XgBoost**. **XgBoost**
    is a popular algorithm used by several Kaggle winners to achieve award-winning
    results. We will use the scikit-learn and XGBoost packages in Python to see the
    previous example in code.
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment analysis as text classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A popular use of classifiers is in sentiment analysis. The end objective here
    is to determine the subjective value of a text document, which is essentially
    how positive or negative the content of a text document is. This is particularly
    handy for quickly understanding what the tone is of, say, the movie you are producing
    or the book you want to read.
  prefs: []
  type: TYPE_NORMAL
- en: Simple classifiers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's begin by simply trying a few machine learning classifiers such as Logistic
    Regression, Naive Bayes, and Decision Trees. We'll then move on and try the Random
    Forest and Extra Trees classifiers. For all of these implementations, we won't
    use anything except scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing simple classifiers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can tweak these simple classifiers to improve their performance. For this,
    the most common method is to try several slightly different versions of the classifier.
    We do this by changing the parameters of our classifier.
  prefs: []
  type: TYPE_NORMAL
- en: We will learn how to automate this search process for the best classifier parameters
    using `GridSearch` and `RandomizedSearch`.
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having an ensemble of several different classifiers means we will be using a
    group of models. Ensembling is a very popular and easy to understand machine learning
    technique, and is part of almost every winning Kaggle competition.
  prefs: []
  type: TYPE_NORMAL
- en: Despite initial concerns that this process might be slow, some teams working
    on commercial software have begun using ensemble methods in production software
    as well. This is because it requires very little overhead, is easy to parallelize,
    and allows for a built-in fallback of using a single model.
  prefs: []
  type: TYPE_NORMAL
- en: We will look at some of the simplest ensembling techniques based on simple majority,
    also known as voting ensemble, and will then build using that.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, this machine learning for NLP section covers simple classifiers,
    parameter optimization, and ensemble methods.
  prefs: []
  type: TYPE_NORMAL
- en: Getting the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will programmatically download the data using Python''s standard inbuilt
    toolkit called `urlretrieve` from `urllib.request`. The following is our download-from-internet
    piece:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'If you are using the fastAI environment, all of these imports work. The second
    block simply sets up Tqdm for us to visualize the download progress. Let''s now
    download the data using `urlretrieve`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s download some data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now extract the preceding files and see what the directory contains:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that we prefer to use `Path from pathlib` over the `os.path` functionality.
    This make it more platform-agnostic as well as Pythonic. This really badly written
    utility tells us that there are at least two folders: `train` and `test`. Each
    of these folders, in turn, has at least three folders, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The `pos` and `neg` folders contain reviews, which are positive and negative
    respectively. The `unsup` folder stands for unsupervised. These folders are useful
    for building language models, especially for deep learning, but we will not use
    that here. Similarly, the `all` folder is redundant because those reviews are
    repeated in either the `pos` or `neg` folder.
  prefs: []
  type: TYPE_NORMAL
- en: Reading data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s read the following data into a Pandas `DataFrame` with the appropriate
    labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This function reads the files for a particular `train` or `test` split, both
    positive and negative, for the IMDb dataset. Each split is a `DataFrame` with
    two columns: `text` and `label`. The `label` column gives us our target value,
    or `y`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now read the data in the corresponding `DataFrame` and then split it
    into the following four variables: `X_train`, `y_train`, `X_test`, and `y_test`.'
  prefs: []
  type: TYPE_NORMAL
- en: Simple classifiers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to try some of our classifiers, let''s get the basic imports out of
    the way, as shown in the following code. Here, we will be importing the rest of
    the classifiers as we need them. This ability to import things later is important
    for ensuring we don''t import too many unnecessary components into memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Since this section is simply for illustration purposes, we will use the simplest
    feature extraction steps, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Bag of words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TF-IDF
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We encourage you to try the code examples with better text vectorization (for
    example, using direct GloVe or word2vec lookups).
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s now simply replicate the simple logistic regression we did in [Chapter
    1](5625152b-6870-44b1-a39f-5a79bcc675d9.xhtml), *Getting Started with Text Classification*,
    but on our custom dataset, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As you can see in the preceding snippet, `lr_clf` becomes our classifier pipeline.
    We saw the pipeline in our introductory section. A pipeline allows us to queue
    multiple operations in one single Python object.
  prefs: []
  type: TYPE_NORMAL
- en: We are able to call functions such as `fit`, `predict`, and `fit_transform`
    on our `Pipeline` objects because a pipeline automatically calls the corresponding
    function of the last component in the list.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'As mentioned earlier, we are calling the `predict` function on our pipeline.
    The test reviews go through under the same pre-processing steps, `CountVectorizer()`
    and `TfidfTransformer()`, as the reviews during training, as shown in the following
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The ease and simplicity of this process makes `Pipeline` one of the most frequently
    used abstractions in software-grade machine learning. However, users might prefer
    to execute each step independently, or build their own pipeline equivalents in
    some research or experimentation use cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: How do we find our model accuracy? Well, let's take a quick look at what is
    happening in the preceding line.
  prefs: []
  type: TYPE_NORMAL
- en: Consider that our predictions are [1, 1, 1] and the ground truth is [1, 0, 1].
    The equality would return a simple list of Boolean objects, such as `[True, False,
    True]`. When we sum a Boolean list in Python, it returns the number of `True`
    cases, giving us the exact number of times our model made correct predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Dividing this value by the total number of predictions made (or, equally, the
    number of test reviews) gives us our accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s write the previous two-line logic into a simple, lightweight function
    to calculate accuracy, as shown in the following snippet. This would prevent us
    from repeating the logic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Removing stop words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'By simply passing a flag to the `CountVectorizer` step, we can remove the most
    common stop words. We will specify the language in which the stop words we want
    to remove are written. In the following case, that''s `english`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, this is not very helpful in improving our accuracy. This would
    indicate that the noise added by stop words is being removed or neglected by the
    classifier itself.
  prefs: []
  type: TYPE_NORMAL
- en: Increasing ngram range
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s now try to improve the information available to the classifier by including
    bigrams and trigrams, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Multinomial Naive Bayes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s initialize the classifier in a manner identical to our logistic regression
    classifier, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous command will measure performance on the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Adding TF-IDF
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s try the preceding model with TF-IDF, as another step after bag-of-words
    (unigrams), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This is better than our previous value, but let's see what else we can do to
    improve this further.
  prefs: []
  type: TYPE_NORMAL
- en: Removing stop words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s now remove the stop words for English again, by simply passing `english`
    to the tokenizer as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This helps improve performance, but only marginally. We might be better off
    simply keeping in the stop words for other classifiers that we try.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a last manual experiment, let''s try adding bigrams and unigrams, as we
    did lfor ogistic regression, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This is significantly better than the previous Multinomial Naive Bayes performance,
    but not as good as the performance of our logistic regression classifier, which
    was close to achieving 88% accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now try something specific to Bayesian classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: Changing fit prior to false
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Increasing `ngram_range` did work for us, but changing `prior` from `uniform`
    to fitting it (by changing `fit_prior` to `False`) did not help at all, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We have now thought of each combination that might improve our performance.
    Note that this approach is tedious, and also error-prone because it relies too
    greatly on human intuition.
  prefs: []
  type: TYPE_NORMAL
- en: Support vector machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Support vector machines** (**SVM**) continue to remain a hugely popular machine
    learning technique, having made its way from the industry to classrooms and then
    back. In addition to several forms of regression, SVM is one of the techniques
    that forms the backbone of the multi-billion-dollar online ad targeting industry.'
  prefs: []
  type: TYPE_NORMAL
- en: In academia, work such as that by T Joachim ([https://www.cs.cornell.edu/people/tj/publications/joachims_98a.pdf](https://www.cs.cornell.edu/people/tj/publications/joachims_98a.pdf))
    recommends support vector classifiers for text classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s difficult to estimate whether it will be equally effective for us based
    on such literature, mainly due to a difference in the dataset and pre-processing
    steps. Let''s give it a shot nevertheless:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: While SVM works best with linearly separable data (as we can see, our text is
    not linearly separable), it's still worth giving it a try for completeness.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous example, SVM did not perform well, and it also took a really
    long time to train (~150x) when compared to other classifiers. We will not look
    at SVM for this particular dataset again.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Decision trees are simple, intuitive tools for classification and regression
    alike. They often resemble a flow chart of decisions when seen visually, hence
    the name decision tree. We will reuse our pipeline, simply using the `DecisionTreeClassifier`
    as our main classification technique, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Random forest classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s now try the first ensemble classifier. The forest in Random forest classifiers
    comes from the fact that each instance of this classifier consists of several
    decision trees. The Random in Random forests comes from the fact that each tree
    selects a finite number of features from all features at random, as shown in the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Although considered to be very powerful when used in most machine learning tasks,
    the Random Forest approach doesn't do particularly well in our case. This is partially
    because of our rather crude feature extraction.
  prefs: []
  type: TYPE_NORMAL
- en: Approaches such as decision trees, RFC, and Extra trees classifiers don't do
    well in high-dimensional spaces such as text.
  prefs: []
  type: TYPE_NORMAL
- en: Extra trees classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Extra in Extra Trees comes from the idea that it is extremely randomized.
    While the tree splits in a Random Forest classifier are effectively deterministic,
    they are randomized in the Extra Trees classifier. This changes the bias-variance
    trade-off in cases of high-dimensional data such as ours (where every word is
    effectively a dimension or classifier). The following snippet shows the classifier
    in action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, this change works in our favor here, but this is not universally
    true. Results will vary across datasets as well as feature extraction pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing our classifiers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's now focus on our best performing model, logistic regression, and see if
    we can push its performance a little higher. The best performance for our LR-based
    model is an accuracy of 0.88312, as seen earlier.
  prefs: []
  type: TYPE_NORMAL
- en: We are using the phrases parameter search and hyperparameter search interchangeably
    here. This is done to stay consistent with deep learning vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: We want to select the best performing configuration of our pipeline. Each configuration
    might be different in small ways, such as when we remove stop words, bigrams,
    and trigrams, or similar processes. The total number of such configurations can
    be fairly large, and can sometimes run into the thousands. In addition to manually
    selecting a few combinations to try, we can try all several thousand of these
    combinations and evaluate them.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, this process would be far too time-consuming for most small-scale
    experiments such as ours. In large experiments, possible space can run into the
    millions and take several days of computing, making it cost- and time-prohibitive.
  prefs: []
  type: TYPE_NORMAL
- en: We recommend reading a blog on hyperparameter tuning ([https://www.oreilly.com/ideas/evaluating-machine-learning-models/page/5/hyperparameter-tuning](https://www.oreilly.com/ideas/evaluating-machine-learning-models/page/5/hyperparameter-tuning))
    to become familiar with the vocabulary and ideas discussed here in greater detail.
  prefs: []
  type: TYPE_NORMAL
- en: Parameter tuning using RandomizedSearch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An alternative approach was proposed by Bergstra and Bengio ([http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf))
    in 2012\. They demonstrated that a random search across a large hyperparameter
    space is more effective than a manual approach, as we did for Multinomial Naive
    Bayes, and often as effective—or more so—than `GridSearch`.
  prefs: []
  type: TYPE_NORMAL
- en: How do we use it here?
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we will build on top of the results such as that of Bergstra and Bengio.
    We will break down our parameter search into the following two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Using `RandomizedSearch`, go through a wide parameter combination space in a
    limited number of iterations
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the results from step 1 to run `GridSearch` in a slightly narrow space
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can repeat the previous steps until we stop seeing improvements in our results,
    but we won''t do that here. We''ll leave that as an exercise for the reader. Our
    example is outlined in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the `param_grid` variable defines our search space. In our pipeline,
    we assign names to each estimator such as `vect`, `clf`, and so on. The convention
    of `clf` double underscore (also called dunder) signifies that this `C` is an
    attribute of the `clf` object. Similarly, for `vect` we specify whether stop words
    are to be removed or not. As an example, `english` means removing English stop
    words where the list of stop words is what `scikit-learn` internally uses. You
    can also replace this with a command from spaCy, NLTK, or one more closely customized
    to your tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code gives us a cross validation accuracy in the range of 0.87\.
    This might vary depending on how the randomized splits are created.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the preceding snippet, the classifier performance improves by more
    than 1% by simply changing a few parameters. This is amazing progress!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now take a look at what parameters we''re using. In order to compare
    this, you need to know the default values for all of the parameters. Alternatively,
    we can simply look at the parameters from `param_grid` that we wrote and note
    the selected parameter values. For everything not in the grid, the default values
    are chosen and remain unchanged, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we notice these things in the best classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: The chosen `C` value in `clf` is `100`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lowercase` is set to `False`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Removing stop words is a bad idea
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding bigrams and trigrams helps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Observations like the preceding are very specific to this dataset and classifier
    pipeline. In my experience, however, this can and does vary widely.
  prefs: []
  type: TYPE_NORMAL
- en: Let's also avoid assuming that the values are always the best we'll get when
    running `RandomizedSearch` for so few iterations. The rule of thumb in this case
    is to run it for at least 60 iterations, and to also use a much larger `param_grid`.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we used `RandomizedSearch` to understand the broad layout of parameters
    we want to try. We added the best values for some of those to our pipeline itself
    and we will continue to experiment with the values of other parameters.
  prefs: []
  type: TYPE_NORMAL
- en: We have not mentioned what the `C` parameter stands for or how it influences
    the classifier. This is definitely important when understanding and performing
    a manual parameter search. Changing `C` helps simply by trying out different values.
  prefs: []
  type: TYPE_NORMAL
- en: GridSearch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now run `GridSearch` for our selected parameters. Here, we are choosing
    to include bigrams and trigrams while running `GridSearch` over the `C` parameter
    of `LogisticRegression`.
  prefs: []
  type: TYPE_NORMAL
- en: Our intention here is to automate as much as possible. Instead of trying varying
    values in `C` during our `RandomizedSearch`, we are trading off human learning
    time (a few hours) with compute time (a few extra minutes). This mindset saves
    us both time and effort.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding lines of code, we have ran the classifier over our `lr_clf`
    using the new, simpler `param_grid`, which works only over the `C` parameter of
    `LogisticRegression`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see what the steps in our best estimator are, and in particular, what
    the value of `C` is, as shown in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s get the resulting performance directly from our object. Each of these
    objects has an attribute called `best_score_`. This attribute stores the best
    value of the metric we chose. In the following case, we have chosen accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: As you can see in the preceding code, that's almost a ~3% performance gain over
    the non-optimized model, despite the fact we tried very few parameters to optimize.
  prefs: []
  type: TYPE_NORMAL
- en: It is worth mentioning that we can and must repeat these steps (`RandomizedSearch`
    and `GridSearch`) to push the model's accuracy even higher.
  prefs: []
  type: TYPE_NORMAL
- en: Ensembling models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ensembling models is a very powerful technique for improving your model performance
    across a variety of machine learning tasks.
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we have quoted from the Kaggle Ensembling Guide ([https://mlwave.com/kaggle-ensembling-guide/](https://mlwave.com/kaggle-ensembling-guide/))
    written by MLWave.
  prefs: []
  type: TYPE_NORMAL
- en: We can explain why ensembling helps to reduce error or improve accuracy, as
    well as demonstrate the popular techniques on our chosen task and dataset. While
    each of these techniques might not result in a performance gain for us on our
    dataset specifically, they are still a powerful tool to have in your mental toolkit.
  prefs: []
  type: TYPE_NORMAL
- en: To ensure that you understand these techniques, we strongly urge you to try
    them on a few datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Voting ensembles – Simple majority (aka hard voting)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The simplest ensembling technique is perhaps to take a simple majority. This
    works on the intuition that a single model might make an error on a particular
    prediction but that several different models are unlikely to make identical errors.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ground truth: 11011001'
  prefs: []
  type: TYPE_NORMAL
- en: The numbers 1 and 0 represent a `True` and `False` prediction for an imagined
    binary classifier. Each digit is a single true or false prediction for different
    inputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s assume there are three models with only one error for this example;
    they are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Model A prediction: 10011001'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Model B prediction: 11011001'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Model C prediction: 11011001'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The majority votes gives us the correct answer as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Majority vote: 11011001'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the case of an even number of models, we can use a tie breaker. A tie breaker
    can be as simple as picking a random result, or more nuanced by picking the results
    with more confidence.
  prefs: []
  type: TYPE_NORMAL
- en: To try this on our dataset, we import `VotingClassifier` from scikit-learn.
    `VotingClassifier` does not use the pre-trained models as inputs. It will call
    fit on the models or classifier pipelines, and then use the predictions of all
    models to make the final prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'To counter the hype in favor of ensembles elsewhere, we can demonstrate that
    hard voting may hurt your accuracy performance. If someone claims that ensembling
    always helps, show them the following example for a more constructive discussion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We used only two classifiers for demonstration in the preceding example: Extra
    Trees and Random Forest. Individually, each of these classifiers has their performance
    capped at an accuracy of ~74%.'
  prefs: []
  type: TYPE_NORMAL
- en: In this particular example, the performance of the voting classifier is worse
    than both of them alone.
  prefs: []
  type: TYPE_NORMAL
- en: Voting ensembles – soft voting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Soft voting predicts the class label based on class probabilities. The sums
    of the predicted probabilities for each classifier areg calculated for each class
    (which is important in the case of multiple classes). The assigned class is then
    the class with the maximum probability sum or `argmax(p_sum)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is recommended for an ensemble of well-calibrated classifiers, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Well calibrated classifiers are probabilistic classifiers for which the output
    of the predict_proba method can be directly interpreted as a confidence level.
  prefs: []
  type: TYPE_NORMAL
- en: '- From the Calibration Docs on sklearn ([http://scikit-learn.org/stable/modules/calibration.html](http://scikit-learn.org/stable/modules/calibration.html))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our code flow is identical to our hard voting classifier except that the parameter
    `voting` is passed as `soft`*,* as shown in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Here, we can see that soft voting gives us an absolute accuracy gain of 1.62%.
  prefs: []
  type: TYPE_NORMAL
- en: Weighted classifiers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The only way for inferior models to overrule the best (expert) model is for
    them to collectively and confidently agree on an alternative.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid this scenario, we can use a weighted majority vote—but why weighting?
  prefs: []
  type: TYPE_NORMAL
- en: 'Usually, we want to give a better model more weight in a vote. The simplest,
    but computationally inefficient, way to do this is to repeat the classifier pipelines
    under different names, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Repeat the experiment with hard voting instead of soft voting. This will tell
    you how the voting strategy influences the accuracy of our ensembled classifier,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Here, we can see that weighted voting gives us an absolute accuracy gain of
    1.50%.
  prefs: []
  type: TYPE_NORMAL
- en: So, what have we learned so far?
  prefs: []
  type: TYPE_NORMAL
- en: A simple majority-based voting classifier can perform worse than individual
    models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Soft voting works better than hard voting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weighing classifiers by simply repeating classifiers can help
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So far, we have been selecting classifiers seemingly at random. This is less
    than ideal, especially when we are building for a commercial utility where every
    0.001% gain matters.
  prefs: []
  type: TYPE_NORMAL
- en: Removing correlated classifiers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at this in action by taking three simple models as an example.
    As you can see, the ground truth is all 1s:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'These models are highly correlated in their predictions. When we take a majority
    vote, we see no improvement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s compare that to the following three lower-performing but highly
    uncorrelated models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'When we ensemble this with a majority vote, we get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Here, we see a much higher rate of improvement than in any of our individual
    models. Low correlation between model predictions can lead to better performance.
    In practice, this is tricky to get right but is worth investigating nevertheless.
  prefs: []
  type: TYPE_NORMAL
- en: We will leave the following section as an exercise for you to try out.
  prefs: []
  type: TYPE_NORMAL
- en: As a quick hint, you will need to find the correlations among predictions of
    different models and select pairs that are less correlated to each other (ideally
    less than 0.5) and yet have a good enough performance as individual models.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: So, what result do we get when we use two classifiers from the same approach?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the preceding result is not very encouraging either, but remember,
    this is just a hint! We encourage you to go ahead and try this task on your own
    and with more classifiers, including ones we have not discussed here.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we looked at several new ideas regarding machine learning.
    The intention here was to demonstrate some of the most common classifiers. We
    looked at how to use them with one thematic idea: translating text to a numerical
    representation and then feeding that to a classifier.'
  prefs: []
  type: TYPE_NORMAL
- en: This chapter covered a fraction of the available possibilities. Remember, you
    can try anything from better feature extraction using Tfidf to tuning classifiers
    with `GridSearch` and `RandomizedSearch`, as well as ensembling several classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter was mostly focused on pre-deep learning methods for both feature
    extraction and classification.
  prefs: []
  type: TYPE_NORMAL
- en: Note that deep learning methods also allow us to use a single model where the
    feature extraction and classification are both learned from the underlying data
    distribution. While a lot has been written about deep learning in computer vision,
    we have offered only an introduction to deep learning in natural language processing.
  prefs: []
  type: TYPE_NORMAL
