- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Monitoring and Evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once there is something to test, even a trial version, be on top of the processes
    for evaluating the results. Unsurprisingly, the methods discussed (surveys, interviews,
    feedback) can be re-used to see how beta customers or early adopters perform.
  prefs: []
  type: TYPE_NORMAL
- en: Another Pet Peeve
  prefs: []
  type: TYPE_NORMAL
- en: The word *beta* sends the wrong message to a non-technical customer that the
    product is not ready for them. Consider other terms such as limited release or,
    my favorite, *access for early adopters*. This label might put them in a better
    frame of mind to handle issues and provide feedback.
  prefs: []
  type: TYPE_NORMAL
- en: Since **retrieval-augmented generation** (**RAG**) is fundamental to most enterprise
    solutions for sales and support, metrics around the quality of that approach are
    essential. A combination of data science, product managers, and the design team
    is required to improve results. A heuristic approach using design experts or trained
    individuals can evaluate RAG or other LLM outputs that provide results to customers.
  prefs: []
  type: TYPE_NORMAL
- en: 'This means a suite of additional methods are available to choose from, but
    honestly, all of these methods should be applied:'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluate using **retrieval-augmented generation** **assessment** (**RAGAs**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitor with usability measures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Refine with a heuristic evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let‚Äôs jump right into metrics to benchmark LLM solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluate using RAGAs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This book is about design, so product people are not expected to implement the
    **RAGAs**. RAGAs is a framework for evaluating the RAG pipeline. *Any approach
    that takes test data, is actually used, and can measure quality reliably is fine
    with me*. RAGAs is popular with the AI community, so it is worth covering. Call
    on product experts to evaluate results to validate findings. The goal is to understand
    the metrics and make decisions to deliver model improvements.
  prefs: []
  type: TYPE_NORMAL
- en: The RAGAs process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All good stories start at the beginning. An LLM product needs to be evaluated.
    Don‚Äôt wait for customers to complain; it comes too late, and customers disappear
    quickly if they are frustrated with quality. This is similar to phone support;
    when a customer has a horrible interaction, they tend to tell 20 people how bad
    it was, and this lack of goodwill hurts the company‚Äôs reputation. If backend systems
    or recommenders miss their mark, it will leave a foul taste in customers‚Äô mouths.
    By monitoring how the system is performing, there is a better chance for improvement.
    As Peter Drucker was quoted, ‚Äú*You can‚Äôt improve what you* *don‚Äôt measure.*‚Äù
  prefs: []
  type: TYPE_NORMAL
- en: 'A collection of metrics can be deployed. Let‚Äôs lay out the steps for RAGAs:'
  prefs: []
  type: TYPE_NORMAL
- en: Synthetically create a diverse dataset for testing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use these metrics to measure solution quality.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Care for the application. Use smaller and cheaper models to generate actionable
    insights.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feed these insights back to improve the overall experience.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This chapter will summarize RAGAs **metric-driven development** (**MDD**), a
    fancy name for what we have called care and feeding. Use data to drive actionable
    insights; too many failed projects occur because of this simple oversight.
  prefs: []
  type: TYPE_NORMAL
- en: RAGAs identify problems from the user‚Äôs perspective; this will be covered in
    this chapter. The associated Discord channels for the always-to-arise technical
    issues are active. There is an online collection of RAGA-related videos and tutorials.
    If needed, here is the in-depth documentation on RAGAs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Documentation: [Introduction to RAGAs](https://docs.ragas.io/en/latest/concepts/index.html)
    ([https://docs.ragas.io/en/latest/concepts/index.html](https://docs.ragas.io/en/latest/concepts/index.html))'
  prefs: []
  type: TYPE_NORMAL
- en: Testing data (for the developers in the room)
  prefs: []
  type: TYPE_NORMAL
- en: The more technically inclined readers can use the GitHub FAQ files. Install
    RAGAs with Python and get started. This is outside the book‚Äôs scope. The metrics
    covered in this chapter apply to a variety of similar approaches. Learn these
    standard metrics to evaluate LLM quality. Building RAGAs is not required to understand
    the metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tutorial: [Installing RAGAs](https://docs.ragas.io/en/stable/getstarted/install.html)
    ([https://docs.ragas.io/en/stable/getstarted/install.html](https://docs.ragas.io/en/stable/getstarted/install.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B21964_08.xhtml#_idTextAnchor172)*, Fine-Tuning*, covered synthesizing
    data. Because the models need to be monitored, synthesizing data needs to go to
    the next level to expand the variety.'
  prefs: []
  type: TYPE_NORMAL
- en: Synthesizing data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Tools can create a variety of test data. The challenge with using the same
    model to generate samples is that test data from fine-tuned or prompt examples
    can be too close to validation examples. This was found in the experiment from
    [*Chapter 8*](B21964_08.xhtml#_idTextAnchor172)*, Fine Tuning*. Since LLMs like
    to predict the next word, output can be similar from one generation to the next.
    Samples won‚Äôt be as varied as the customer‚Äôs phrasing. Focusing on writing characteristics
    such as reasoning, conditioning, and multi-context can give a more comprehensive
    range of outputs and, thus, more robust tests. These advanced instructions get
    the LLM to vary output more broadly:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reasoning**: Write questions that require reasoning to provide an answer:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How does photosynthesis work?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Rewritten: What **consequences** can occur'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: when you **disrupt the balance**
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: of photosynthesis in an **ecosystem**?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Conditioning**: Include conditional elements that add complexity:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What strategies can improve sales performance?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Rewritten: How do sales strategies differ based'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: on **factors such as target market demographics,**
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**product complexity, and competitive landscape?**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Multi-context**: Request diverse information to form an answer:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How does predictive maintenance
  prefs: []
  type: TYPE_NORMAL
- en: benefit manufacturing operations?
  prefs: []
  type: TYPE_NORMAL
- en: 'Rewritten: How does **data analysis, machine**'
  prefs: []
  type: TYPE_NORMAL
- en: '**learning and teamwork** improve'
  prefs: []
  type: TYPE_NORMAL
- en: predictive maintenance in manufacturing?
  prefs: []
  type: TYPE_NORMAL
- en: '**Conversational**: Convert portions of questions into the back-and-forth expected
    in a chat question-and-answer solution:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hi, I need help resetting my password.
  prefs: []
  type: TYPE_NORMAL
- en: Sure, I can help with that.
  prefs: []
  type: TYPE_NORMAL
- en: Have you tried the 'Forgot Password'
  prefs: []
  type: TYPE_NORMAL
- en: link on the sign-in page?
  prefs: []
  type: TYPE_NORMAL
- en: Yes, but I didn't get the reset email.
  prefs: []
  type: TYPE_NORMAL
- en: Check your spam folder. Sometimes
  prefs: []
  type: TYPE_NORMAL
- en: the email ends up there.
  prefs: []
  type: TYPE_NORMAL
- en: This is done because humans are diverse, random, sometimes off-topic, humorous,
    lack a sense of humor, make typos, can ramble, write in multiple languages, be
    terse, or be verbose. It would be best to have diversity in any dataset. Look
    for this and demand it in the enterprise data. Keep these synthesizing techniques
    available to expand the breadth of test cases when gathering realistic examples
    falls short. For now, focus on the numbers. What metrics can measure and evaluate
    the state of a solution?
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The center of all of this is the metrics. There are six to cover. The first
    four are around the model‚Äôs perspective, and the last two have a **user experience**
    (**UX**) feel:'
  prefs: []
  type: TYPE_NORMAL
- en: Faithfulness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Answer relevancy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Context precision
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Context recall
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Context entity recall
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summarization score
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RAGA lays out the metrics in *Figure 10**.1*. This chart is also in the OpenAI
    demo (at the 20-minute mark), which will be covered next. Each concept requires
    an in-depth explanation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_10_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 ‚Äì How to think about evaluating an LLM
  prefs: []
  type: TYPE_NORMAL
- en: 'These concepts are worth internalizing to communicate clearly with developers
    and data scientists. There is a lot to unpack here with scoring, and it will be
    trickier still to translate scores into actionable tasks. This could have been
    a very technical chapter. To build a RAGAs testbed, do it independently. The value
    we focus on is understanding the metrics and how they apply. Even without RAGAs,
    the concept of model metrics applies to many solutions. The documentation examples
    are used to facilitate communication so that you can refer back to it or as updates
    become available:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [Evaluating the performance of RAG solutions](https://docs.ragas.io/en/latest/concepts/metrics/index.html)
    ([https://docs.ragas.io/en/latest/concepts/metrics/index.html](https://docs.ragas.io/en/latest/concepts/metrics/index.html))'
  prefs: []
  type: TYPE_NORMAL
- en: Math is scary
  prefs: []
  type: TYPE_NORMAL
- en: Do not get caught up in the math. Focus on the value of the metric and get comfortable
    with the terms. Humans will not manually calculate these values. They are all
    done by the software. *If math is scary to you, ignore it. Learn* *the terms*.
  prefs: []
  type: TYPE_NORMAL
- en: Faithfulness (for generation)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is a measure of the generated answer‚Äôs factual accuracy. Can the answer
    be fact-checked (no fake news)?
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>F</mi><mi>a</mi><mi>i</mi><mi>t</mi><mi>h</mi><mi>f</mi><mi>u</mi><mi>l</mi><mi>n</mi><mi>e</mi><mi>s</mi><mi>s</mi><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mo>=</mo><mfrac><mtable
    columnwidth="auto" columnalign="center" rowspacing="1.0000ex" rowalign="baseline
    baseline"><mtr><mtd><mrow><mrow><mo>|</mo><mi>N</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>c</mi><mi>l</mi><mi>a</mi><mi>i</mi><mi>m</mi><mi>s</mi><mi>t</mi><mi>h</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>a</mi><mi>n</mi><mi>b</mi><mi>e</mi></mrow></mrow></mtd></mtr><mtr><mtd><mrow><mrow><mi>i</mi><mi>n</mi><mi>f</mi><mi>e</mi><mi>r</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>f</mi><mi>r</mi><mi>o</mi><mi>m</mi><mi>g</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>n</mi><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi><mo>|</mo></mrow></mrow></mtd></mtr></mtable><mtable
    columnwidth="auto" columnalign="center" rowspacing="1.0000ex" rowalign="baseline
    baseline"><mtr><mtd><mrow><mrow><mo>|</mo><mi>T</mi><mi>o</mi><mi>t</mi><mi>a</mi><mi>l</mi><mi>n</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>c</mi><mi>l</mi><mi>a</mi><mi>i</mi><mi>m</mi><mi>s</mi></mrow></mrow></mtd></mtr><mtr><mtd><mrow><mrow><mi>i</mi><mi>n</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>g</mi><mi>e</mi><mi>n</mi><mi>e</mi><mi>r</mi><mi>a</mi><mi>t</mi><mi>e</mi><mi>d</mi><mi>a</mi><mi>n</mi><mi>s</mi><mi>w</mi><mi>e</mi><mi>r</mi><mo>|</mo></mrow></mrow></mtd></mtr></mtable></mfrac></mrow></mrow></math>](img/5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This means not hallucinating to give a faithful answer. If asked for someone‚Äôs
    birthday, it better be right. The more individual elements that can be fact-checked,
    the larger the numerator and denominator. Consider the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the calculation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Break down the answer into individual statements and determine whether it can
    be inferred from the original context (take our word for this made-up store):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The store is on Melrose (True)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It is open 7 days a week (True)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It is open from 11 am to 9 pm (True)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Calculate faithfulness:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>F</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>f</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>1.0</mml:mn></mml:math>](img/6.png)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_IMG
- en: 'Let‚Äôs reuse the same example but with a slight change:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is a calculation for an example with errors:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Break down the answer again:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The store is on Melvin Place (False)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It is open 7 days a week (True)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It is open from 11 am to 9 pm (True)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Calculate faithfulness:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>F</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>f</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>0.67</mml:mn></mml:math>](img/7.png)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_IMG
- en: This judges the correctness of the original context. Knowing the right store
    is more relevant than the hours (store hours might overlap between a correct and
    wrong answer, so even if they are wrong, they might be right enough for the customer),
    but an address would be a total failure. Models are not smart. They don‚Äôt know
    the value of one of these elements (location) versus another (hours or days).
    If the faithfulness of an answer is low, it is hallucinating, and the answer should
    be rejected.
  prefs: []
  type: TYPE_NORMAL
- en: The trick is that although datasets with questions, context, and responses are
    publicly available, they lack enterprise content. A testing matrix based on answers
    with ground truth is necessary to monitor and judge changes.
  prefs: []
  type: TYPE_NORMAL
- en: There are parameters one can manipulate to improve this metric and the other
    answer-related factors (relevancy, similarity, and correctness). Data scientists
    can play with these depending on the tools used. The model can also impact how
    much effort it will take to get to a better experience by reducing hallucinations
    and improving consistency. Judge out-of-the-box performance by monitoring LLM
    leaderboards. At the time of this writing, the OpenAI models are at or near the
    top of the Hughes leaderboard for evaluating models for hallucinations when summarizing
    a document. This is one example of a leaderboard. Remember, different models can
    be used for different components, so don‚Äôt focus only on ChatGPT when looking
    at the boards.
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [Hughes Hallucination Evaluation Model (HHEM) Leaderboard](https://huggingface.co/spaces/vectara/leaderboard)
    ([https://huggingface.co/spaces/vectara/leaderboard](https://huggingface.co/spaces/vectara/leaderboard))'
  prefs: []
  type: TYPE_NORMAL
- en: These leaderboards are based on generic metrics against foundational models.
    Enterprise data-based LLM solutions will require their own analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Answer Relevancy (for generation)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: How relevant is the answer to the question? If parts of the answer are missing
    or include redundant results, the score will be lower. The higher the score, the
    better; the best scores should approach 1, typically between 0 and 1, but because
    of the math, they can range as low as -1.
  prefs: []
  type: TYPE_NORMAL
- en: 'The equation is explained in the RAGAs documentation as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>a</mi><mi>n</mi><mi>s</mi><mi>w</mi><mi>e</mi><mi>r</mi><mi>r</mi><mi>e</mi><mi>l</mi><mi>e</mi><mi>v</mi><mi>a</mi><mi>n</mi><mi>c</mi><mi>y</mi><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><mrow><munderover><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><mi
    mathvariant="normal">c</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">s</mi><mo>(</mo><msub><mi>E</mi><msub><mi>g</mi><mi>i</mi></msub></msub><mo>,</mo><msub><mi>E</mi><mi>o</mi></msub><mo>)</mo></mrow></mrow></mrow></mrow></math>](img/8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math>](img/9.png)
    is the embedding of the generated question ùëñ'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:math>](img/10.png)
    is the embedding of the original question'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ùëÅ is the number of generated questions, which is 3 by default
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The math is complex because it is based on Embedded vector values, the multidimensional
    space discussed in [*Chapter 6*](B21964_06_split_000.xhtml#_idTextAnchor134),
    *Gathering Data ‚Äì Content is King*. This metric is based on the question, the
    content, and the answer. However, it is calculated based on a cosine similarity
    from the original question to a collection of generated questions. The LLM reverse-engineers
    these based on the answer. It does not consider factuality and penalizes cases
    where the answer lacks completeness.
  prefs: []
  type: TYPE_NORMAL
- en: Because they use cosine similarity, the values can range from -1 to 1, while
    typically, they will be from 0 to 1\. Getting a feel for the data for models is
    challenging. ChatGPT works within a much smaller range of values, as discussed
    in the OpenAI community.
  prefs: []
  type: TYPE_NORMAL
- en: 'Discussion: [Text Embedding Issues](https://community.openai.com/t/some-questions-about-text-embedding-ada-002-s-embedding/35299/3)
    ([https://community.openai.com/t/some-questions-about-text-embedding-ada-002-s-embedding/35299/3](https://community.openai.com/t/some-questions-about-text-embedding-ada-002-s-embedding/35299/3))'
  prefs: []
  type: TYPE_NORMAL
- en: I suspect some of this thread is too technical. Even I tend to glaze over because
    we do not need to calculate these to learn how to value them. It just points to
    the magic that goes on behind the scenes. Understanding results can be challenging.
    Get comfortable with the data and metrics and work out improvements.
  prefs: []
  type: TYPE_NORMAL
- en: Pick one, any one
  prefs: []
  type: TYPE_NORMAL
- en: 'Another approach is a simple comparison of the number of statements made in
    the output and the relevance of each statement based on the input. This approach
    identifies low scores as problematic:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>a</mi><mi>n</mi><mi>s</mi><mi>w</mi><mi>e</mi><mi>r</mi><mi>r</mi><mi>e</mi><mi>l</mi><mi>e</mi><mi>v</mi><mi>a</mi><mi>n</mi><mi>c</mi><mi>y</mi><mo>=</mo><mfrac><mrow><mi>N</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>R</mi><mi>e</mi><mi>l</mi><mi>e</mi><mi>v</mi><mi>a</mi><mi>n</mi><mi>t</mi><mi>S</mi><mi>t</mi><mi>a</mi><mi>t</mi><mi>e</mi><mi>m</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>s</mi></mrow><mrow><mi>T</mi><mi>o</mi><mi>t</mi><mi>a</mi><mi>l</mi><mi>N</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>S</mi><mi>t</mi><mi>a</mi><mi>t</mi><mi>e</mi><mi>m</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>s</mi></mrow></mfrac></mrow></mrow></math>](img/11.png)'
  prefs: []
  type: TYPE_IMG
- en: I share this to stir the pot of complexity. This popular alternative approach
    comes from DeepEval, another LLM evaluation framework. They have all the same
    metrics (and more, with 14 at the last check), but as this shows, the calculation
    can vary. There are a dozen popular evaluation tools. I just picked the popular
    RAGAS to help product people understand the key metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Documentation: [DeepEval Metrics](https://docs.confident-ai.com/docs/metrics-introduction)
    ([https://docs.confident-ai.com/docs/metrics-introduction](https://docs.confident-ai.com/docs/metrics-introduction))'
  prefs: []
  type: TYPE_NORMAL
- en: 'This metric is different from answer correctness. It does not consider the
    facts but the need for more completeness or the inclusion of redundant details.
    The LLM generates questions for the answer multiple times (*N*). Then, the average
    value from the collection of cosine similarity scores for each question compared
    to the original question is calculated. A great answer makes it likely the original
    answer can be reconstructed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The LLM should generate questions from answers like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'ChatGPT 4o was given a simple prompt to generate these questions and returned
    junk:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Work with the engineers, gather data that makes sense, and learn. It can be
    frustrating. Solutions such as fine-tuning, a better knowledge base, and feedback
    loops helps relevance. However, there are more technical approaches as well. To
    go deeper, ask your intern, ChatGPT:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Context precision (for retrieval)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'How relevant is the context to the question? Let‚Äôs see:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>C</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi><mi>P</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>@</mo><mi>K</mi><mo>=</mo><mfrac><mrow><msubsup><mo>‚àë</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><mrow><mo>(</mo><mi>P</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>@</mo><mi>k</mi><mo>√ó</mo><msub><mi>v</mi><mi>k</mi></msub><mo>)</mo></mrow></mrow><mtable
    columnwidth="auto" columnalign="center" rowspacing="1.0000ex" rowalign="baseline
    baseline"><mtr><mtd><mrow><mi>T</mi><mi>o</mi><mi>t</mi><mi>a</mi><mi>l</mi><mi>n</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>r</mi><mi>e</mi><mi>l</mi><mi>e</mi><mi>v</mi><mi>a</mi><mi>n</mi><mi>t</mi></mrow></mtd></mtr><mtr><mtd><mrow><mi>i</mi><mi>t</mi><mi>e</mi><mi>m</mi><mi>s</mi><mi>i</mi><mi>n</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>t</mi><mi>o</mi><mi>p</mi><mi>K</mi><mi>r</mi><mi>e</mi><mi>s</mi><mi>u</mi><mi>l</mi><mi>t</mi><mi>s</mi></mrow></mtd></mtr></mtable></mfrac></mrow></mrow></math>](img/12.png)'
  prefs: []
  type: TYPE_IMG
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>@</mo><mi>k</mi><mo>=</mo><mfrac><mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi><mi>p</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>s</mi><mo>@</mo><mi>k</mi></mrow><mrow><mo>(</mo><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi><mi>p</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>s</mi><mo>@</mo><mi>k</mi><mo>+</mo><mi>f</mi><mi>a</mi><mi>l</mi><mi>s</mi><mi>e</mi><mi>p</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>s</mi><mo>@</mo><mi>k</mi><mo>)</mo></mrow></mfrac></mrow></mrow></math>](img/13.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *K* is the total number of chunks in context, and ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msub><mi>v</mi><mi>k</mi></msub><mo>‚àà</mo><mfenced
    open="{" close="}"><mn>0,1</mn></mfenced></mrow></mrow></math>](img/14.png) is
    the relevance indicator at rank *k*. Recall the two relevant chunks in the preceding
    ‚ÄúEngland‚Äù example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Are all of the ground truth items in the context and ranked high? The more
    relevant, the higher its rank. If chunks contain relevant details to support the
    ground truth, sum the precision for each chunk to arrive at the context precision.
    Using the two England example chunks from the previous example, calculate the
    precision for each chunk as it helps to answer our question about the capital
    of England and its location. There are no false positives in the context, such
    as telling me England is in France, so it is just based on the true positive of
    London being the capital in the second chunk, but it needs to provide the details
    about where England is located:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>@</mml:mo><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math>](img/15.png)'
  prefs: []
  type: TYPE_IMG
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>@</mml:mo><mml:mn>2</mml:mn><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:math>](img/16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Sum up the precision scores and arrive at the mean:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>C</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi><mi>P</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>=</mo><mfrac><mrow><mo>(</mo><mn>0</mn><mo>+</mo><mn>0.5</mn><mo>)</mo></mrow><mn>1</mn></mfrac><mo>=</mo><mn>0.5</mn></mrow></mrow></math>](img/17.png)'
  prefs: []
  type: TYPE_IMG
- en: The documentation could be more precise, but I did not find a reference that
    would make it more transparent. DeepEval, mentioned earlier, has some more context
    if you need their explanation. It is similar in value but slightly different in
    delivery. They don‚Äôt define the scope of a true positive, so it needs to be clarified
    how they arrive at their results when looking at complex statements with multiple
    elements. Defining a positive can be challenging, as one statement might contain
    many positives. Also, they don‚Äôt account for the relevance indicator in the calculation.
    The actual calculation is more accurate than the documentation. So, take it for
    what it is suggesting. The correct answers in the proper context are needed to
    answer a question. The higher those answers are ranked, the more likely a good
    result will be obtained. The model can be precise but needs to gather all contexts.
  prefs: []
  type: TYPE_NORMAL
- en: Context recall (for retriever)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Can the retriever retrieve all relevant context? The context is the material
    used as the source of information. The value is based on the ground truth (*GT*)
    and the retrieved context, with values only from 0 to 1\. Higher scores are better:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi><mo>=</mo><mfrac><mrow><mo>|</mo><mi>G</mi><mi>T</mi><mi>c</mi><mi>l</mi><mi>a</mi><mi>i</mi><mi>m</mi><mi>s</mi><mi>t</mi><mi>h</mi><mi>a</mi><mi>t</mi><mi>c</mi><mi>a</mi><mi>n</mi><mi>b</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>t</mi><mi>r</mi><mi>i</mi><mi>b</mi><mi>u</mi><mi>t</mi><mi>e</mi><mi>d</mi><mi>t</mi><mi>o</mi><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi><mo>|</mo></mrow><mrow><mo>|</mo><mi>N</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>c</mi><mi>l</mi><mi>a</mi><mi>i</mi><mi>m</mi><mi>s</mi><mi>i</mi><mi>n</mi><mi>G</mi><mi>T</mi><mo>|</mo></mrow></mfrac></mrow></mrow></math>](img/18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finding each statement in the retrieved context will give a 1.0 score for context
    recall. Let‚Äôs use our previous answer. These are the facts on which to base this.
    They are the ground truth:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Say the recalled context was the following two statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, both of the following statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Statement 1**: England is in Northern Europe'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Statement 2**: Its capital is London'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Can be evaluated against the context:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Statement 1**: False (Northern Europe is not in the context)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Statement 2**: True (London is defined as England‚Äôs capital)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This results in the context recall calculation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>=</mo><mn>0.5</mn></mrow></mrow></math>](img/19.png)'
  prefs: []
  type: TYPE_IMG
- en: This is important to understand because it might have the correct information,
    but if the solution doesn‚Äôt return the proper context, it might not provide all
    the expected parts of the answer. Each answer is weighted equally. It could give
    more weight to meaningful and relevant items if it were smarter. It has no idea.
  prefs: []
  type: TYPE_NORMAL
- en: A few more metrics are outside the four in the chart that started this section.
    One concerns entities.
  prefs: []
  type: TYPE_NORMAL
- en: Context entity recall
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Entity recall** is useful in solutions such as a help desk, where knowing
    the correct entities (a specific relevant piece of information, a value, or a
    label) is essential. Entities are necessary for filling out vacation requests
    (type of vacation, date, hours), filing an expense report (amount, attendees,
    date, category, purpose, type of payment), interacting with sales data (date,
    amount, contacts, address, product, quantity), or any form with many entities.
    This calculates the fraction of the union of similar entities from the context
    entities (*CE*) and the ground truth (*GE*) entities over the number of ground
    truth entities. Values range from zero to one; high values indicate better recall:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>t</mi><mi>y</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi><mo>=</mo><mfrac><mfenced
    open="|" close="|"><mrow><mi>C</mi><mi>E</mi><mo>‚à©</mo><mi>G</mi><mi>E</mi></mrow></mfenced><mrow><mo>|</mo><mi>G</mi><mi>E</mi><mo>|</mo></mrow></mfrac></mrow></mrow></math>](img/20.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This example shows the entities in the ground truth (*GE*):'
  prefs: []
  type: TYPE_NORMAL
- en: England, Northern Europe, London
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then, find the entities in the two context examples (*CE*):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Context Example 1**: England, Great Britain, United Kingdom'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Content Example 2**: England, London'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Calculate the scores based on the union of the entity matches:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>t</mi><mi>y</mi><mn>1</mn><mi>r</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi><mo>=</mo><mfrac><mfenced
    open="|" close="|"><mrow><mi>C</mi><mi>E</mi><mn>1</mn><mo>‚à©</mo><mi>G</mi><mi>E</mi></mrow></mfenced><mfenced
    open="|" close="|"><mrow><mi>G</mi><mi>E</mi></mrow></mfenced></mfrac><mo>=</mo><mfrac><mn>1</mn><mn>3</mn></mfrac><mo>=</mo><mn>0.33</mn></mrow></mrow></math>](img/21.png)'
  prefs: []
  type: TYPE_IMG
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>t</mi><mi>y</mi><mn>2</mn><mi>r</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi><mo>=</mo><mfrac><mfenced
    open="|" close="|"><mrow><mi>C</mi><mi>E</mi><mn>2</mn><mo>‚à©</mo><mi>G</mi><mi>E</mi></mrow></mfenced><mrow><mo>|</mo><mi>G</mi><mi>E</mi><mo>|</mo></mrow></mfrac><mo>=</mo><mfrac><mn>2</mn><mn>3</mn></mfrac><mo>=</mo><mn>0.67</mn></mrow></mrow></math>](img/22.png)'
  prefs: []
  type: TYPE_IMG
- en: From this, the conclusion is that the recall on Entity 2 is better for entity
    matching. Too many entities that don‚Äôt overlap are noise in this calculation.
  prefs: []
  type: TYPE_NORMAL
- en: Results are not deterministic; try, try, and try again
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some issues with calculations not being the same. This is just an
    example; every model‚Äôs values will be different. The first-time results might
    look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '**{''faithfulness'': 0.5624, ''answer_relevancy'': 0.7752, ''****answer_correctness'':
    0.5484}**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Rerunning the analysis then yields different results:'
  prefs: []
  type: TYPE_NORMAL
- en: '**{''faithfulness'': 0.6843, ''answer_relevancy'': 0.7193, ''****answer_correctness'':
    0.5246}**'
  prefs: []
  type: TYPE_NORMAL
- en: Don‚Äôt freak out. Why would they differ? The same models and data should give
    the same results. Reproducibility is not there. They suggest repeating runs three
    times and averaging results. This is the growing pain with metric quality. It‚Äôs
    not very deterministic, like the models themselves. It is a work in progress,
    but it should be suitable enough.
  prefs: []
  type: TYPE_NORMAL
- en: Online forums have grumblings about the quality of the metrics. Other vendors
    provide new and improved metrics, so be on the lookout. This isn‚Äôt a mature space.
    It draws on robust machine learning models, but these scoring methods are imperfect.
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [Possible bug in evaluation function in RAGAs](https://github.com/explodinggradients/ragas/issues/660)
    ([https://github.com/explodinggradients/ragas/issues/660](https://github.com/explodinggradients/ragas/issues/660))'
  prefs: []
  type: TYPE_NORMAL
- en: All of this is about the metrics from your experience. Just benchmark against
    the model data and use tools and techniques to improve. Comparing scores to other
    environments won‚Äôt be meaningful. Let‚Äôs finish with the UX metrics for this suite.
  prefs: []
  type: TYPE_NORMAL
- en: User experience metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although the previous metrics should be monitored and valuable to the whole
    team, it is good to see the following are considered UX metrics. Let‚Äôs dive right
    in.
  prefs: []
  type: TYPE_NORMAL
- en: Answer semantic similarity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This is based on the relationship between the ground truth and the similarity
    of the answer. It is based on the cosine similarity of the vectorized values of
    the statements. Look for highly correlated values. The range is from 0 to 1, and
    the higher the score, the better the matching between the generated answer and
    the ground truth:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ground truth**: The iPhone 15‚Äôs battery life is about 11 hours during typical
    web browsing, video watching, and social website use'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High similarity answer**: The iPhone‚Äôs all-day battery life can handle the
    robust media usage of a very active phone user'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Low similarity answer**: Newer phones have a longer battery life than the
    last generation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I can‚Äôt explain why the RAGAs document leaves out the scoring metric, but we
    can live without it since this is not calculated by hand.
  prefs: []
  type: TYPE_NORMAL
- en: 'I see how this is likely based on the work of Risch, M√∂ller, Gutsche, and Peitsch
    (2021), so to explore this article and read Isabelle Nguyen‚Äôs blog:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [Semantic Answer Similarity for Evaluating Question Answering Models](https://arxiv.org/pdf/2108.06130)
    by Risch et al. ([https://arxiv.org/pdf/2108.06130](https://arxiv.org/pdf/2108.06130))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [Semantic Answer Similarity: The Smarter Metric to Score Question
    Answering Predictions](https://www.deepset.ai/blog/semantic-answer-similarity-to-evaluate-qa)
    by Isabelle Nguyen ([https://www.deepset.ai/blog/semantic-answer-similarity-to-evaluate-qa](https://www.deepset.ai/blog/semantic-answer-similarity-to-evaluate-qa))'
  prefs: []
  type: TYPE_NORMAL
- en: Risch et al. provide good examples of how it evaluates answer quality. Use this
    to adjust prompts to tighten or loosen the LLM‚Äôs response. It also introduces
    the concept of the F1 score, which leads us to answer correctness.
  prefs: []
  type: TYPE_NORMAL
- en: Answer correctness
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This builds on the similarity score for the answer. It looks at the similarity
    of the generated answer and the ground truth and whether the facts are supported.
    So, it is important if it is accurate or leads us astray with **false positives**
    (**FPs**) or **false** **negatives** (**FNs**):'
  prefs: []
  type: TYPE_NORMAL
- en: '**True positive (TP)**: Facts or statements found in the ground truth and generated
    answer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**FP**: Statements or facts in the generated answer that are not found in the
    ground truth'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**FN**: Missing facts or statements found in the ground truth but missing from
    the generated answer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'I will use the example from the documentation to keep this straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ground truth**: Einstein was born in 1879 in Germany'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High answer correctness**: In 1879, Einstein was born in Germany'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Low answer correctness**: Einstein was born in Spain in 1879'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then evaluate the low answer correctness (evaluated against ‚ÄúEinstein was born
    in Spain in 1879‚Äù):'
  prefs: []
  type: TYPE_NORMAL
- en: '**TP**: Einstein was born in 1879'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**FP**: Einstein was born in Spain (incorrect statement)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**FN**: Einstein was born in Germany (Germany isn‚Äôt in the answer)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is the F1 score:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>F</mi><mn>1</mn><mi>S</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mo>=</mo><mfrac><mfenced
    open="|" close="|"><mrow><mi>T</mi><mi>P</mi></mrow></mfenced><mrow><mo>(</mo><mfenced
    open="|" close="|"><mrow><mi>T</mi><mi>P</mi></mrow></mfenced><mo>+</mo><mn>0.5</mn><mo>√ó</mo><mo>(</mo><mfenced
    open="|" close="|"><mrow><mi>F</mi><mi>P</mi></mrow></mfenced><mo>+</mo><mfenced
    open="|" close="|"><mrow><mi>F</mi><mi>N</mi></mrow></mfenced><mo>)</mo><mo>)</mo></mrow></mfrac></mrow></mrow></math>](img/23.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice how the false values are weighted. If there is no false information,
    then the F1 score would be the max of 1\. The more false information, the more
    the score trends to zero. If there are no true positives, the score will be zero.
  prefs: []
  type: TYPE_NORMAL
- en: Other metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Realize this is just a sampling of the available metrics; a few RAGAs framework
    items were skipped, and we mentioned more can be found from the other frameworks.
    Look for repeatability and reliability in metrics that interpret the quality of
    the interactions. Rajeep Biswas (2023) covers other metrics in his overview:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [Metrics for evaluating LLMs](https://www.linkedin.com/pulse/evaluating-large-language-models-llms-standard-set-metrics-biswas-ecjlc/)
    ([https://www.linkedin.com/pulse/evaluating-large-language-models-llms-standard-set-metrics-biswas-ecjlc/](https://www.linkedin.com/pulse/evaluating-large-language-models-llms-standard-set-metrics-biswas-ecjlc/))'
  prefs: []
  type: TYPE_NORMAL
- en: I don‚Äôt want everyone to get hung up on the math. Appreciating and valuing a
    metric should be based on trusting it to do what it says. But we have to put a
    stake in the ground. Apply the metrics and gauge the team‚Äôs level of trust in
    them. The more they are used and iterated, the easier it is to judge the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'RAGAs is an emerging field, and the metrics will change with it. For a different
    explanation of RAGAs metrics, try this article by Leonie Monigatti (2023):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [Evaluating RAG Applications with RAGAs](https://towardsdatascience.com/evaluating-rag-applications-with-ragas-81d67b0ee31a)
    ([https://towardsdatascience.com/evaluating-rag-applications-with-ragas-81d67b0ee31a](https://towardsdatascience.com/evaluating-rag-applications-with-ragas-81d67b0ee31a))'
  prefs: []
  type: TYPE_NORMAL
- en: Metrics give a high-level view of quality without addressing the necessary detailed
    changes to which the metric might allude. The biggest specific issue is hallucination
    errors. Monitoring and addressing these issues are critically important to building
    trust.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring and classifying the types of hallucination errors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Minimizing hallucinations is a recurring thread in this book. [*Chapter 3*](B21964_03.xhtml#_idTextAnchor058),
    *Identifying Optimal Use Cases for ChatGPT*, covered logging errors from chat
    logs. It is time to explore more refined ways of classifying these errors. Once
    errors are classified, help with the model, the data, or the training to address
    the problems. Two classification methods are worth exploring, starting with error
    types classified by Vectara.
  prefs: []
  type: TYPE_NORMAL
- en: Classifying by error types
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Vectara** is in the class of RAGAs search tools run as a service. Regardless
    of the tools available, it is essential to identify and fix hallucinations. The
    data should show a small percentage, such as one to three percent, but that level
    of quality requires some work. The Vectara classifications are helpful because
    they are orthogonal and roll up into their version of a quality score, which they
    call the **Factual Consistency Score**. I am a sucker for the word *consistency*
    in any metric. This is a way of monitoring ongoing progress and tracking quality,
    even when humans need help understanding why the values have changed. The more
    learning and testing, the better changes will improve the results. There is only
    guidance, no rules.'
  prefs: []
  type: TYPE_NORMAL
- en: 'I will quote the exact examples used as input but then include commentary on
    the issues with the output:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [Automating Hallucination Detection](https://vectara.com/blog/automating-hallucination-detection-introducing-vectara-factual-consistency-score/)
    ([https://vectara.com/blog/automating-hallucination-detection-introducing-vectara-factual-consistency-score/](https://vectara.com/blog/automating-hallucination-detection-introducing-vectara-factual-consistency-score/))'
  prefs: []
  type: TYPE_NORMAL
- en: '*"The first vaccine for Ebola was approved by the FDA in 2019 in the US, five
    years after the initial outbreak in 2014\. To produce the vaccine, scientists
    had to sequence the DNA of Ebola, then identify possible vaccines, and finally
    show successful clinical trials. Scientists say a vaccine for COVID-19 is unlikely
    to be ready this year, although clinical trials have* *already started."*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Output Type 1: Relation error:'
  prefs: []
  type: TYPE_NORMAL
- en: '*"The Ebola vaccine was rejected by the FDA* *in 2019."*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Analysis**: *The AI confused the relationship between Ebola and COVID-19\.*
    The Ebola vaccine was approved in 2019.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Commentary**: [*Chapter 6*](B21964_06_split_000.xhtml#_idTextAnchor134),
    *Gathering Data ‚Äì Content is King,* covered the common association error in which
    Hank Arron‚Äôs and Hank Greenberg‚Äôs religions were confused. Relationships in conversation,
    even between humans, are complex. ‚ÄúIt,‚Äù ‚Äúthey,‚Äù ‚Äúus,‚Äù ‚Äúwe,‚Äù ‚Äúthere,‚Äù ‚Äúyou,‚Äù ‚Äúyour,‚Äù
    ‚Äúthem,‚Äù and other phrases can be misconnected to the wrong subject, resulting
    in attribution errors. Don‚Äôt allow customers to think Product A does something
    that is only supported by Product B.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Output Type 2: Entity error:'
  prefs: []
  type: TYPE_NORMAL
- en: '*The COVID-19 vaccine was approved by the FDA* *in 2019.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Analysis**:*This error arises from the AI confusing details between Ebola
    and COVID-19\.* COVID-19 appeared on the world stage in January 2020 with an announcement
    from the Center for Disease Control in the US, and the first vaccine appeared
    in December 2020.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Commentary**: Entity recognition is complex. Understanding the sources is
    required to identify entities. Designers, writers, and PMs must know their products
    and build a team of experts on the business. In this case, it is easy; everyone
    experienced COVID and might realize the timing is wrong. With more technical materials,
    especially those that cover multiple products, it is easy to match the wrong product
    to an unrelated bug, specification, or feature. These are entity errors. One can
    look at editing the source documents or how the RAGAs tools segment or chunk the
    document. Some documentation could be more straightforward for ingestion by an
    LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Output Type 3: Coreference error:'
  prefs: []
  type: TYPE_NORMAL
- en: '*"The first vaccine for Ebola was approved by the FDA in 2019\. They say a
    vaccine for COVID-19 is unlikely to be ready* *this year.‚Äù*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Analysis**: *The confusion arises with the pronoun ‚Äúthey‚Äù; the summary refers
    to the FDA, but in the original article, ‚Äúthey‚Äù relates* *to scientists.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Commentary**: Similar to Type 1, this focuses on misconnected subjects. The
    context of ‚Äúthey‚Äù was correct when referring to scientists, but because the chunk
    analyzed contained the FDA reference, it got confused. If the source document
    was clear that ‚ÄúScientists say the vaccine‚Ä¶‚Äù this error would not happen. However,
    it would be annoying as a human to read articles that never use pronouns or determiners
    (words that refer to a noun more specifically, such as ‚Äúthe book‚Äù or ‚Äúher show‚Äù).
    Tools should get better at making these relationships, or content will be rewritten.
    At least recognize the issue and correct it. This repetitive use of words, like
    if we repeatedly used ‚ÄúScientists say,‚Äù is called **burstiness**. It will touch
    on this again later in this chapter. It could be that human-readable documentation
    will have to be distinct from optimized material for RAG. Getting context when
    something is at the top or in a document‚Äôs sidebar for a paragraph five pages
    later is hard.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Output Type 4: Discourse link error:'
  prefs: []
  type: TYPE_NORMAL
- en: '*‚ÄúTo produce the vaccine, scientists have to show successful human trials,
    then sequence the DNA of* *the virus.‚Äù*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Analysis**:*This misplacement stems from an incorrect order of events; the
    original article states that sequencing the virus‚Äôs DNA precedes demonstrating
    successful* *human trials.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Commentary**: This issue with order can take a lot of work to catch. Did
    you have to read it twice to see the problem? Steps and order are crucial in technical
    documentation. If there is a lot of step-by-step documentation, create a collection
    of test cases focused on order. Their label, ‚Äúdiscourse link error,‚Äù belies the
    simplicity of this example. I don‚Äôt know why they used that wording. Maybe they
    wanted a slightly different word than ‚Äúrelation.‚Äù This doesn‚Äôt appear to be a
    link issue. More detail was not provided. Expect consolidation in nomenclature
    and standardization in testing over the next few years. Wynter et al. (2023) probably
    would call this logical inconsistency.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [An evaluation on large language model outputs: Discourse and memorization](https://www.sciencedirect.com/science/article/pii/S2949719123000213)
    by Wynter et al., 2023 ([https://www.sciencedirect.com/science/article/pii/S2949719123000213](https://www.sciencedirect.com/science/article/pii/S2949719123000213))'
  prefs: []
  type: TYPE_NORMAL
- en: Let me share one more classification method.
  prefs: []
  type: TYPE_NORMAL
- en: Factual and faithful hallucinations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[*Chapter 4*](B21964_04.xhtml#_idTextAnchor085)*, Scoring Stories*, provides
    a manual classification approach. Take advantage of emerging definitions and classifications
    when they will help organize or resolve issues. As Deval Shah discusses, it could
    be helpful to distinguish between factuality and faithfulness hallucinations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [Taxonomy of Hallucinations in LLMs](https://www.lakera.ai/blog/guide-to-hallucinations-in-large-language-models)
    by Deval Shah ([https://www.lakera.ai/blog/guide-to-hallucinations-in-large-language-models](https://www.lakera.ai/blog/guide-to-hallucinations-in-large-language-models))'
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs start with fact-based issues in *Table 10.1*. Bold represents the hallucinations.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Type** | **User Input** | **Model Output** | **Explanation** |'
  prefs: []
  type: TYPE_TB
- en: '| **Factual** **Inconsistency** | Who was the famous Jewish home run hitter
    in MLB? | **Hank Arron** was the most famous Jewish hitter in MLB. | The LLM‚Äôs
    answer is wrong as Hank Greenberg was the top Jewish home run hitter in MLB. |'
  prefs: []
  type: TYPE_TB
- en: '| **Factual** **Fabrication** | Tell me about the origins of dragons. | **Dragons
    roamed the northern volcanoes of Old Eros where they were tamed by the** **royal
    family.** | The LLM‚Äôs answer is made up. It sounds like it came from Game of Thrones.
    |'
  prefs: []
  type: TYPE_TB
- en: Table 10.1 ‚Äì Examples of factual hallucinations
  prefs: []
  type: TYPE_NORMAL
- en: Next, he breaks down faithfulness, categorizing it into instruction, context,
    and logical inconsistencies. He has a wonderful section surveying the origins
    of hallucinations in LLMs. Do visit it for more insight.
  prefs: []
  type: TYPE_NORMAL
- en: Huang‚Äôs article covers how hallucinations come from data sources, training,
    and inference (as covered), how no single benchmark covers all the issues, and,
    critically, how to mitigate hallucination.
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [A Survey on Hallucination in Large Language Models: Principles, Taxonomy,
    Challenges, and Open Questions](https://arxiv.org/pdf/2311.05232) by Huang et
    al., 2023 ([https://arxiv.org/pdf/2311.05232](https://arxiv.org/pdf/2311.05232))'
  prefs: []
  type: TYPE_NORMAL
- en: The section on mitigation makes reading the whole article worth it. These strategies
    are covered next.
  prefs: []
  type: TYPE_NORMAL
- en: Overall approaches to reducing issues during monitoring
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Continuous improvement based on monitoring is an absolute necessity with this
    technology. This is repeated more than any other topic in this book. This care
    and feeding cycle must be done while customers learn to engage. They have a low
    tolerance for dumb experiences and will turn away from poor recommendations. It
    takes work to re-engage a lost customer. [*Chapter 11*](B21964_11.xhtml#_idTextAnchor236)*,
    Process*, focuses on process improvements. First, look at general methods to solve
    quality issues.
  prefs: []
  type: TYPE_NORMAL
- en: 'Chunking, data manipulation, and writing or editing for LLMs help. More approaches
    are out in the wild. Even ChatGPT knows about these. Some help augment the data
    to make the system more robust to user variety. Back translation, text summarization‚Äîespecially
    when using a different LLM to supplement the main LLM‚Äîand noise injection (including
    misspellings and grammar errors) help the model understand the imperfectness of
    human language. Try this prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: It should be straightforward to recognize and even respect why variety is essential.
    Chat GPT 4o will respond with dozens of techniques. People are not only different,
    but they are also not perfect. Generating examples with imperfections in the dataset
    and getting the model to overcome these and become more robust is all part of
    training. Seeing how customers ask questions, make mistakes, and retry when they
    need help getting the correct answer is excellent. But don‚Äôt take the LLM‚Äôs word
    for it. Let‚Äôs look at how the humans at OpenAI approached building a solution
    for an enterprise customer.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI‚Äôs case study on quality and how to measure it
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'OpenAI has some good developer day talks on how to think about evaluation.
    It is a clear explanation without going over people‚Äôs heads. Check it out to learn
    more about scoring. The most significant takeaways are:'
  prefs: []
  type: TYPE_NORMAL
- en: Not every suggestion resulted in improvements (items with a check worked)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It takes a team to address and refine an enterprise solution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solutions can dramatically improve with a methodological iterative approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure10.2* shows OpenAI‚Äôs methods to improve the case study from the video.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_10_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 ‚Äì Different techniques can succeed or fail to improve the experience
  prefs: []
  type: TYPE_NORMAL
- en: 'Video: Maximizing LLM performance [techniques](https://youtu.be/ahnGLM-RC1Y)
    ([https://youtu.be/ahnGLM-RC1Y](https://youtu.be/ahnGLM-RC1Y))'
  prefs: []
  type: TYPE_NORMAL
- en: The chart and video show successful (‚úî) and two failed (üö´) approaches. They
    worked through various solutions to find good fits. The video doesn‚Äôt show the
    details of the changes they made. It is not explained what changes were made to
    raise accuracy. However, it is still a good case study that shows dramatic improvements
    in quality. With every effort, a testing process must be implemented to evaluate
    systematically. There needs to be more than user perception, surveys, and feedback.
  prefs: []
  type: TYPE_NORMAL
- en: Systematic testing processes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To evaluate any system, a few things are needed:'
  prefs: []
  type: TYPE_NORMAL
- en: Understand and be able to apply real-world usage and understand edge cases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be statistically confident in the amount of testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be able to repeat or even automate the test with reliability and consistency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be able to make changes systematically to understand the results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With our human testing, methods are available to test with as few as five customers.
    Typically, human testers can go into the dozens, with only methods such as surveys
    intentionally hitting thousands. However, when it comes to LLMs, there are places
    where automation and scale are critical to success. OpenAI suggests in *Table
    10.2* that larger sample sizes of test cases are are needed to evaluate LLMs to
    improve quality.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Difference** **to detect** | **The sample size needed for** **95% confidence**
    |'
  prefs: []
  type: TYPE_TB
- en: '| **30%** | ~10 |'
  prefs: []
  type: TYPE_TB
- en: '| **10%** | ~100 |'
  prefs: []
  type: TYPE_TB
- en: '| **3%** | ~1,000 |'
  prefs: []
  type: TYPE_TB
- en: '| **1%** | ~10,000 |'
  prefs: []
  type: TYPE_TB
- en: Table 10.2 ‚Äì Relationship between detection difference percentages to the number
    of test cases
  prefs: []
  type: TYPE_NORMAL
- en: The chart in *Table 10.2* is from OpenAI‚Äôs testing strategy documentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Documentation: [OpenAI‚Äôs view on testing strategy](https://platform.openai.com/docs/guides/prompt-engineering/strategy-test-changes-systematically)
    ([https://platform.openai.com/docs/guides/prompt-engineering/strategy-test-changes-systematically](https://platform.openai.com/docs/guides/prompt-engineering/strategy-test-changes-systematically))'
  prefs: []
  type: TYPE_NORMAL
- en: Confidence increases as the number of test case samples increases. This is done
    with simple assumptions and straightforward explanations. This should answer any
    questions about the scale of testing compared to the reliability expected in practice.
    Don‚Äôt expect 10,000 examples of an answer to a single question; think broader.
    Test cases will be built into a collection over time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although this is easiest to understand for a conversational assistant, some
    thought has to be applied to other use cases, such as a recommender. Create test
    data and understand the variety of recommendations. Each focused recommendation
    will need a collection of test cases that understand the range of data elements.
    With five data elements in the recommendation, each with 3 to 30 possible values,
    a range of 30 to 150 combinations can result. Here is a recommendation for smart
    air filters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This recommendation for a large commercial building is based on air quality
    measures, the cost of energy to run the system, filter dirt capacity, filter type,
    and the filter‚Äôs current dirtiness. There are thousands of possible combinations
    and dozens of recommendations in that case. The system must deal with all these
    entities, variables, and recommendations (see bolded items). So, thousands of
    test cases are needed, and results need to be validated to ensure that good advice
    is provided.
  prefs: []
  type: TYPE_NORMAL
- en: Each area of expertise is also multiplied by the number of languages supported,
    and the test matrix will grow. These must be automated. Breaking down test cases
    by subject helps one understand the scale of the problem better. Recall the Alligiance
    example in [*Chapter 6*](B21964_06_split_000.xhtml#_idTextAnchor134)*, Gathering
    Data‚ÄîContent is King*. There were 400 FAQs and many ways of asking each question.
    Five test cases for each FAQ would be over 2000 test cases.
  prefs: []
  type: TYPE_NORMAL
- en: Test cases can come from humans or an LLM. An example is how to "expense dinner
    at Joe‚Äôs Eatery for a client dinner for $21.46"; when surveyed, 100 participants
    were asked how they would say it. They generated 244 different utterances, and
    87% were unique. Here are ten examples in *Table 10.3* and some analysis of these
    potential test cases.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Test case utterance ‚Äì** **human-generated** | **Test** **case considerations**
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **$21.46 dinner** **at Joe‚Äôs** | Doesn‚Äôt mention intent; what should an LLM
    do with this info? It might be confused with an appointment. The amount does help.
    |'
  prefs: []
  type: TYPE_TB
- en: '| **12/12/18,** **Dinner, 21.46** | Notice ambivalent date format (MM/DD or
    DD/MM?), no intent to be clear this is an expense and no currency. |'
  prefs: []
  type: TYPE_TB
- en: '| **Create an expense for $21.46 at Joe‚Äôs Eatery with my co-worker Lisa Jones
    and** **a client.** | Intent, amount, location, and details. A good example. |'
  prefs: []
  type: TYPE_TB
- en: '| **dinner - client visit - $21.46 Canadian** **Joe‚Äôs eatery** | Points out
    Canadian dollars. |'
  prefs: []
  type: TYPE_TB
- en: '| **Dinner - Joe‚Äôs Eatery - $****21.46** | Terse, but contains 3 of the 5 items
    needed. A good start. |'
  prefs: []
  type: TYPE_TB
- en: '| **Expense 21 dollars 46 cents Joe‚Äôs Eatery for a client visit on October**
    **3, 2018** | Notice amount format is spoken out. |'
  prefs: []
  type: TYPE_TB
- en: '| **Hey, I used my corporate Amex for dinner. Didn‚Äôt you** **see that?** |
    Expects the credit card integration to find his expenses and connect it for expensing.
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Reimburse me for $21.46 for a client visit with** **George Smith** | Reasonable,
    doesn‚Äôt mention dinner or a date. |'
  prefs: []
  type: TYPE_TB
- en: '| **Submit $21.46 for** **client visit** | Lacking a lot of information, but
    reasonable. |'
  prefs: []
  type: TYPE_TB
- en: '| **Take care of my dinner expense of 21.46 dollars at Joe‚Äôs for a client visit
    with the** **following people** | Conversational and expects to provide more details
    to follow. |'
  prefs: []
  type: TYPE_TB
- en: Table 10.3 ‚Äì Potential test cases written by human subjects
  prefs: []
  type: TYPE_NORMAL
- en: 'Humans generated these, but an LLM model can also generate test cases. Let‚Äôs
    compare what happens when prompting ChatGPT 3.5:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: ChatGPT 3.5‚Äôs results are shown in *Table 10.4*. There are differences.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Test case utterance ‚Äì** **OpenAI-generated** | **Testing considerations**
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **‚ÄúRecord a meal expense of $21.46 for a client lunch at Joe‚Äôs Eatery on
    August** **15th, 2024.‚Äù** | All elements are included, uses the awkward ‚ÄúRecord‚Äù.
    |'
  prefs: []
  type: TYPE_TB
- en: '| **‚ÄúExpense $21.46 for a meal with a client at Joe‚Äôs Eatery** **on 8/15/24.‚Äù**
    | All elements, used US date format. |'
  prefs: []
  type: TYPE_TB
- en: '| **‚ÄúAdd a meal expense of $21.46 for lunch with a client at Joe‚Äôs** **Eatery
    today.‚Äù** | All elements, used ‚Äútoday‚Äù. |'
  prefs: []
  type: TYPE_TB
- en: '| **‚ÄúExpense: Meal. Amount: $21.46\. Date: August 15th, 2024\. Location: Joe‚Äôs
    Eatery. Purpose:** **Client Lunch.‚Äù** | Not conversational. No human ever did
    it like this. |'
  prefs: []
  type: TYPE_TB
- en: '| **‚ÄúRegister a $21.46 expense for a meal at Joe‚Äôs Eatery on August 15, 2024,
    for a** **client lunch.‚Äù** | All elements, uses the awkward ‚ÄúRegister‚Äù. |'
  prefs: []
  type: TYPE_TB
- en: '| **‚ÄúDocument a meal expense of $21.46 for a client lunch at Joe‚Äôs Eatery**
    **on 8/15/24.‚Äù** | All elements. |'
  prefs: []
  type: TYPE_TB
- en: '| **‚ÄúAdd an expense for a meal at Joe‚Äôs Eatery, $21.46, for a client lunch
    on** **August 15th.‚Äù** | All elements. |'
  prefs: []
  type: TYPE_TB
- en: '| **‚ÄúExpense: Meal. Amount: $21.46\. Date: 15th August 2024\. Location: Joe‚Äôs
    Eatery. Purpose:** **Client Lunch.‚Äù** | Not conversational. No human ever did
    it like this. |'
  prefs: []
  type: TYPE_TB
- en: '| **‚ÄúRecord $21.46 spent on a meal with a client at Joe‚Äôs Eatery on August**
    **15, 2024.‚Äù** | All elements, almost identical to the preceding one. |'
  prefs: []
  type: TYPE_TB
- en: Table 10.4 ‚Äì Potential test cases written by ChatGPT 3.5
  prefs: []
  type: TYPE_NORMAL
- en: 'By comparing the results, a few conclusions can be made:'
  prefs: []
  type: TYPE_NORMAL
- en: Humans only included some of the information all of the time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Humans went off-topic and expected to follow up with additional information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ChatGPT ignored my request to vary what was included
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ChatGPT used words humans didn‚Äôt use (such as Record and Register)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ChatGPT didn‚Äôt vary the cases very much, the prompt can be improved
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is a simple example. Humans are unpredictable and *human* when communicating,
    so consider that when allocating resources to create test cases. Time can be spent
    improving the prompt and forcing ChatGPT to give better and more varied results.
    Or a new model can provide a much more significant jump in quality. *Table 10.5*
    shows results from GPT 4o. This is better with the same prompt and no prompt engineering.
  prefs: []
  type: TYPE_NORMAL
- en: '| **ChatGPT** **4o responses** | **Testing considerations** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Detailed:** Please expense a meal receipt for $21.46 US dollars dated 8/15/2024
    for a client lunch at Joe‚Äôs Eatery. | Robust and complete |'
  prefs: []
  type: TYPE_TB
- en: '| **Detailed with variation**: Create an expense report for a meal costing
    $21.46 USD from Joe‚Äôs Eatery on August 15, 2024, for a client lunch. | Varied
    the date format. |'
  prefs: []
  type: TYPE_TB
- en: '| **Medium detail**: Log a meal expense of $21.46 for a client meeting on 8/15/2024
    at Joe‚Äôs Eatery. | Log is a good choice. |'
  prefs: []
  type: TYPE_TB
- en: '| **Medium detail with different structure**: Expense a client lunch meal for
    $21.46 from Joe‚Äôs Eatery on 15th August 2024. | ‚Äúlunch meal‚Äù is not something
    from humans. |'
  prefs: []
  type: TYPE_TB
- en: '| **Minimal detail**: Start a meal expense for $21.46 at Joe‚Äôs Eatery. | ‚ÄúStart‚Äù
    is a good trigger, but it is generic. |'
  prefs: []
  type: TYPE_TB
- en: '| **Minimal detail with date**: Record a $21.46 meal expense for a client meeting
    on 8/15/2024. | ‚ÄúRecord‚Äù is still not great. |'
  prefs: []
  type: TYPE_TB
- en: '| **Basic**: Enter a meal expense for a client lunch costing $21.46. | Good
    and short. |'
  prefs: []
  type: TYPE_TB
- en: '| **Basic with location**: Log a client lunch expense for $21.46 at Joe‚Äôs Eatery.
    | A shorter version of the medium detail item. |'
  prefs: []
  type: TYPE_TB
- en: '| **Very minimal**: Expense $21.46 for a client meal. | So short, it need follow-up.
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Least robust**: Start an expense. | Excellent, and common from humans.
    |'
  prefs: []
  type: TYPE_TB
- en: Table 10.5 ‚Äì Potential test cases written by ChatGPT 4o
  prefs: []
  type: TYPE_NORMAL
- en: I have also included their labeling, showcasing the types of use cases they
    provide. Monitor actual user inputs, making judging the value derived from generated
    data easier. It is hard for one individual to think like *everyone*, so don‚Äôt
    try. Use monitoring resources and log analysis to get robust training data and
    test cases.
  prefs: []
  type: TYPE_NORMAL
- en: The problems get harder when the space gets more complex. Recall our graph from
    [*Chapter 1*](B21964_01.xhtml#_idTextAnchor016), *Recognizing the Power of Design
    in ChatGPT*, reshared as *Figure 10**.3*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_10_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 ‚Äì The chance of failure increases at each turn
  prefs: []
  type: TYPE_NORMAL
- en: When an LLM is right 60% of the time, failure is 95% likely within six interactions.
    So, to move that bar up to 97% or greater, a lot of testing and work is needed.
    Generating the correct set of test cases helps monitor for issues. Because of
    monitoring, moving to a new model can be evaluated by applying the test cases
    to the latest model. It‚Äôs okay when everything changes with a new model or version,
    as indicated by the differences between 3.5, 4o, and 4o-mini (it returned results
    similar to 4o). However, with mini being only 15% of the cost of 3.5, it would
    make sense to move if this was a real production system once verified against
    the test cases. There is no guarantee of backward compatibility with LLMs. The
    scale of testing efforts can easily reach 100‚Äôs of thousands of use cases. An
    example of a testing matrix will make this clear.
  prefs: []
  type: TYPE_NORMAL
- en: Testing matrix approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Because these projects impact human performance, the traditional **quality assurance**
    (**QA**) team must be set up to create an effective test matrix. They can develop
    automation and manage the process. Design owners can handle the examples and make
    sure the failures are documented.
  prefs: []
  type: TYPE_NORMAL
- en: With an LLM, monitoring will uncover conversations not initially covered by
    the test cases. It will happen. Just consider how to prioritize improvements for
    them, like any other issue. Once they are known, consider whether they are worth
    including in tests.
  prefs: []
  type: TYPE_NORMAL
- en: 'One can test each skill in isolation, and then when combining a collection
    of skills, some issues might arise. A **skill** is something the model can do.
    It could be connected to an inventory system, report production numbers, or schedule
    an appointment. These are skills. One approach is to build the base model only
    with content from one isolated area, thus allowing us to gauge its effectiveness
    in isolation. Do this for each area. Then, combine all (or sets of areas) retesting
    to understand better the overlap or complexities between the data ingested. The
    QA team will be busy setting up these test harnesses. Product people will be busy
    understanding the results. This example assumes a single model approach to handle
    all interactions, but we have provided multiple examples where a multi-modal approach
    is a better solution. Your choice. The concept of the types of tests still applies.
    It is valuable to have different kinds of test suites. We can review the types
    of tests for conversational interactions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**In-domain**: Questions the skill area should understand and be able to answer.
    They are the meat of the meal, the main course. Get these right:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Out-of-domain**: A collection of questions the skill would not be expected
    to understand based on lack of access to the necessary business data. Because
    they are not for *this* business. However, they are real questions and sound similar
    to the customer‚Äôs needs, just in the wrong context. The general model might want
    to answer these. They are distractors to the in-domain questions. Hence, each
    area of interest will have its collection of out-of-domain items. These examples
    might sound too similar to an LLM because the structure and words seem familiar,
    such as (stock) orders, download (statements), account (details), transfer (money),
    check (status), and (order) history:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Recognize that these examples sound close but are unrelated to Alligiance. They
    sound confusing in the context of this bank. They are close, and a customer might
    not even know they can‚Äôt ask this skill about these problems. Here is a little
    more about the concepts of out-of-domain understanding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Article: [Out of Domain Detection](https://www.elevait.de/blog/out-of-domain-detection)
    ([https://www.elevait.de/blog/out-of-domain-detection](https://www.elevait.de/blog/out-of-domain-detection))'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Random**: Garbage and unrelated items that should not result in a helpful
    response. It could be from a stuck keyboard, poor speech-to-text, someone‚Äôs phone
    in their pocket doing random stuff, or silly, irrelevant questions:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: These are the same random questions for each skill.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Neighbor**: The collection of in-domain test cases from all *other* skills
    used to break the area of interest. Does the question from one area overlap and
    cause a different and wrong response? So, is this an issue when all this expertise
    is available in one **user interface** (**UI**)? It should be addressed if the
    LLM can‚Äôt resolve this disambiguation issue. It might fix it like in this example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I need the weekly report
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The full sales report
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: If only one feature contained a weekly report, there would be clarity. However,
    once multiple features include reports, see how some test cases from one solution
    area might impact others. Think in vector space. All of these questions asking
    for reports can be very similar. Hence, they are neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: '**Language**: Consider test cases for specific languages. One starts by translating
    existing questions into other languages. Still, as mentioned in [*Chapter 5*](B21964_05_split_000.xhtml#_idTextAnchor108),
    *Defining the Desired Experience*, consider the unique needs of the language,
    cultures, and nuisances that would necessitate original content for that specific
    language. Assume at least 10% of test cases for a language will be unique.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building the Matrix
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So, what would a test matrix look like for each collection of use cases? A bank
    might have seven main business areas that require support. Based on the frequency
    of use, they will scale up tests for the big or complex areas. To test each area
    with the types of tests explained, it might look like *Table 10.6*.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Product Areas** | **In-domain** | **Out-of- domain** | **Random** | **Neighbors**
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Account Statements** | 303 | 1002 | 400 | 1,129 |'
  prefs: []
  type: TYPE_TB
- en: '| **Bank Transfers** | 78 | 423 | 400 | 1,354 |'
  prefs: []
  type: TYPE_TB
- en: '| **Account Setup** | 150 | 301 | 400 | 1,282 |'
  prefs: []
  type: TYPE_TB
- en: '| **Deposits** **and Withdrawals** | 201 | 400 | 400 | 1,231 |'
  prefs: []
  type: TYPE_TB
- en: '| **Training** | 50 | 375 | 400 | 1,382 |'
  prefs: []
  type: TYPE_TB
- en: '| **Trading Stocks** **and Bonds** | 605 | 1320 | 400 | 827 |'
  prefs: []
  type: TYPE_TB
- en: '| **Rewards** | 45 | 400 | 400 | 1,387 |'
  prefs: []
  type: TYPE_TB
- en: '| **Sub-Totals (17,045)** | 1,432 | 4,221 | 2,800 | 8,592 |'
  prefs: []
  type: TYPE_TB
- en: '| **Ten Language** **Test (*10)** | 14,320 | 42,210 | 28,000 | 85,920 |'
  prefs: []
  type: TYPE_TB
- en: '| **Unique Language** **Tests (10%)** | 1,432 | 4,221 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **All Test** | **193,148** |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: Table 10.6 ‚Äì Matrix for test cases to validate an extensive conversational AI
  prefs: []
  type: TYPE_NORMAL
- en: 'Trading Stocks and Bonds is the most critical area with the most test cases.
    The rewards program is the smallest. Notice the subtotaled tests per area. Then,
    assuming ten supported translations, the number of tests grows. The number of
    neighbor tests varies because it is the sub-total of all in-domain tests minus
    the number of in-domain tests for this category. The language tests are assuming
    cloning tests 1-to-1 per language for a deployment in ten foreign languages. There
    is an additional row for up to 10% of language tests that might be specific to
    the locality. As discussed earlier, it is ok to only go this far with testing
    in some languages regarding how much support to provide per language. This is
    why our English testing of 17,045 test cases grows to almost 200 K with language
    support. Here are some tips for scaling language tests:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Only translate currency indirectly**: ‚ÄúI took a 25$ Uber ride today,‚Äù translated
    into Japanese, might change the currency symbol to Yen (¬•). But 25¬• is not a meaningful
    value for a cab ride (about 25 cents in dollars or Euros), so training a system
    with too many wrong numbers might confuse the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use localized values**: Recall our examples of Uber not being universal.
    A direct translation of Uber would be Uber in Japanese, but a better example is
    to use Go, the Japanese taxi-hailing app, for the training example.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use idiomatic language**: Both test cases and sample data can benefit from
    how customers communicate. For example, localizing in the USA for dollars and
    training on bucks and cash but not necessarily using more esoteric slang such
    as moolah, coin, cheddar, dough, or greenbacks makes sense. Overtraining might
    cause unexpected consequences. To continue with our Japanese example, train on
    JPY (the code for the Yen), but the Japanese Yen doesn‚Äôt have slang terms. Hence,
    one-to-one translation of idiomatic language is not expected.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The scale of tests correlates with quality**: For example, test whether 20%
    is sufficient for an infrequently used language. LLMs work better in some languages
    because the base model has more training data. Don‚Äôt expect magic, especially
    when it comes to enterprise integrations. Translation steps are needed between
    APIs and responses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Don‚Äôt let the number of tests sound scary. For many enterprise applications,
    these numbers are low. I know one team now has about 500,000 tests, just in English.
    Automation and QA engineers will be busy maintaining and working with the data
    and product team to grow this set. Don‚Äôt make tests to make tests. Use the tests
    to find gaps in the LLM‚Äôs understanding. Recall that every change in the model
    or addition of new data or feature areas will change the quality of the solution.
    During this part of your LLM journey, **always be caring** (and feeding) (**ABC**,
    if a Glengarry Glen Ross reference is ok. Google it). The way to do this is to
    improve retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: Improving retrieval
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'I reviewed a short video for the book. The explanations of the concepts taken
    from the RAGAs documentation are used in this video. They start using the data
    results to improve the overall solution. Greg Loughnane, PhD, slows down when
    he gets the good stuff after Chris Alexiuk whips through setting up the environment
    in the first five minutes of the video. Here is my summary of UX-related elements:'
  prefs: []
  type: TYPE_NORMAL
- en: Improve one metric at a time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Focusing on retriever improvements helps with the generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try a different retriever to get better context (@24 minutes) ‚Äì expanding the
    capture of material before and after the matching context.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See how context relevancy goes down as the size of the context window goes up;
    chunk size matters (this makes sense since there is more unrelated context in
    the denominator of the score) (@31 minutes)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other tools, such as LangChain and LlamaIndex, provide evaluation metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Video: [Tutorial for improving retrieval](https://www.youtube.com/watch?v=mEv-2Xnb_Wk)
    ([https://www.youtube.com/watch?v=mEv-2Xnb_Wk](https://www.youtube.com/watch?v=mEv-2Xnb_Wk))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is all about improvements; data scientists can experiment with many options
    and variables to balance cost, performance, and quality. Learn to understand what
    they can change and how it impacts quality. Since this work is within the expertise
    of the data scientists, focus on understanding the quality of the results. There
    are other metrics of interest. Let‚Äôs give a little context on those.
  prefs: []
  type: TYPE_NORMAL
- en: The wide range of LLM evaluation metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: RAGAs was reviewed because it is popular, has a good set of metrics, and is
    consistent with what traditional deterministic models use. But it is not the only
    approach. We mentioned DeepEval, but there are many more. Some of these approaches
    have specific metrics that sound appealing. Each vendor can have its approach,
    so let me expose a few more metrics that can add value to enterprise solutions
    in *Table 10.7*.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Metric** | **Purpose** | **Applications** | **References** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Rouge** | To compute the effectiveness of auto-generated summaries | Books,
    technical Documentation, articles, marketing material, and so on | Article: [How
    to Use Rouge 2.0](https://kavita-ganesan.com/rouge2-usage-documentation/) ([https://kavita-ganesan.com/rouge2-usage-documentation/](https://kavita-ganesan.com/rouge2-usage-documentation/))
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Human Evaluation** | To ensure user interaction quality | Conversational
    interactions | See [*Chapter 3*](B21964_03.xhtml#_idTextAnchor058), *Identifying
    Optimal Use Cases* *for ChatGPT* |'
  prefs: []
  type: TYPE_TB
- en: '| **Age-specific** **Suitability** | To match reading or educational levels
    | Curriculum tutoring, coaching | Manual review by experts and content filtering
    tools |'
  prefs: []
  type: TYPE_TB
- en: '| **Toxicity Reduction** | To maintain style and tone for public-facing and
    public-sector solutions | All generative output including recommendations | Toxicity
    and bias detection software, sentiment analysis |'
  prefs: []
  type: TYPE_TB
- en: '| **Perplexity** | How probable a piece of generated text is based on its training
    | Content generation | Article: [Perplexity and Burstiness](https://guides.library.unlv.edu/c.php?g=1361336&p=10054021)
    ([https://guides.library.unlv.edu/c.php?g=1361336 &p=10054021](https://guides.library.unlv.edu/c.php?g=1361336&p=10054021))
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Burstiness** | The repetition of words or phrases in a document | Detect
    whether AI or a human wrote content | Product: [Originality AI](https://originality.ai/blog/chat-zero)
    ([https://originality.ai/blog/chat-zero](https://originality.ai/blog/chat-zero))
    |'
  prefs: []
  type: TYPE_TB
- en: Table 10.7 ‚Äì Other evaluation metrics
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of *Table 10.7* came from Aisera: [LLM Evaluation: Metrics and Benchmarking
    Performance](https://aisera.com/blog/llm-evaluation/#5-benchmarking-steps-for-a-better-evaluation-of-llm-performance)
    ([https://aisera.com/blog/llm-evaluation/#5-benchmarking-steps-for-a-better-evaluation-of-llm-performance](https://aisera.com/blog/llm-evaluation/#5-benchmarking-steps-for-a-better-evaluation-of-llm-performance)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'All metrics have a good reason for being collected and analyzed. Frameworks
    cost time and money to maintain, and some data-centric metrics have value because
    they impact costs. The number of LLM conversations per day or the number of tokens
    used helps with budgeting. Consider how to get value out of these metrics to aid
    in understanding customers‚Äô needs. Microsoft also has a few good articles about
    data-centric metrics. Here are some metrics with usability implications:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Concurrent users**: This can sometimes be correlated with performance (too
    many simultaneous users can slow some services, impacting service level and customer
    satisfaction). Recall that there is no such thing as a slow, *good* UI. In the
    case of chat, where human agents might be available for hand-offs, response time
    will be impacted if human agent availability doesn‚Äôt match concurrent user metrics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Token usage**: Token usage = cost. As mentioned in [*Chapter 6*](B21964_06_split_000.xhtml#_idTextAnchor134),
    *Gathering Data ‚Äì Content is King*, look for opportunities to use less expensive
    models while maintaining or improving quality. This means lower customer costs
    or the ability to offer free or less costly tiers to serve a wider audience.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Filtering interventions**: If the process has guardrails for handling quality
    bias and inappropriateness, monitor the rate of these interventions and review
    them to decide whether there is anything to do about them. We mentioned that with
    enterprise software, typically for authorized authenticated users, abhorrent behaviors
    are rarely problematic, unlike in social media. It can happen, and blocking it
    is excellent; however, if work is needed to avoid these conditions or to adjust
    triggers to poorly timed interventions (e.g., being too strict for something that
    might be a perfectly reasonable request), look at these articles.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Article**: [How to Evaluate LLMs: A Complete Metric Framework](https://www.microsoft.com/en-us/research/group/experimentation-platform-exp/articles/how-to-evaluate-llms-a-complete-metric-framework/)
    ([https://www.microsoft.com/en-us/research/group/experimentation-platform-exp/articles/how-to-evaluate-llms-a-complete-metric-framework/](https://www.microsoft.com/en-us/research/group/experimentation-platform-exp/articles/how-to-evaluate-llms-a-complete-metric-framework/))'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Article**: [Patterns of Trustworthy Experimentation: During-Experiment Stage](https://www.microsoft.com/en-us/research/group/experimentation-platform-exp/articles/patterns-of-trustworthy-experimentation-during-experiment-stage/)
    ([https://www.microsoft.com/en-us/research/group/experimentation-platform-exp/articles/patterns-of-trustworthy-experimentation-during-experiment-stage/](https://www.microsoft.com/en-us/research/group/experimentation-platform-exp/articles/patterns-of-trustworthy-experimentation-during-experiment-stage/))'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: These are the tools to understand a variety of data-driven metrics, but there
    is also the softer side, the human customer, in the loop. Although this data likely
    impacts our customers in various ways, it is helpful to understand a customer‚Äôs
    perception. Time to explore some usability metrics to show how the system works
    in the customer‚Äôs eyes.
  prefs: []
  type: TYPE_NORMAL
- en: Monitor with usability metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Earlier chapters explored ways to evaluate and find issues. This can be done
    by using a checklist, a particular set of rules expected for a UX, or a set of
    heuristics, a collection of guiding principles that, when applied correctly, helps
    expose issues quickly. The last chapter covered those methods, leaving a few more
    exciting metrics.
  prefs: []
  type: TYPE_NORMAL
- en: There are multiple ways to interpret how the system is doing. Since surveys,
    interviews, and other subjective metrics were covered, let‚Äôs address measuring
    quality changes over time. This means measuring the fidelity of the experience
    by asking customers to answer specific questions, resulting in a **net promoter
    score** (**NPS**), a single-question survey, the more robust and time-consuming
    ten-question **software usability scale** (**SUS**) metric, or other forms of
    **customer satisfaction** (**CSAT**) surveys.
  prefs: []
  type: TYPE_NORMAL
- en: First, realize why it is helpful to measure usability with a score. It will
    only give a broad sense of how the system is performing. It won‚Äôt uncover design
    flaws (unless there are open-ended follow-up questions). There are two good reasons
    to do this. First, it is easy to compare to other products and see if the solution
    meets expectations and exceeds what is found competitively, and second, to establish
    a baseline to redo these evaluations over time to measure progress. This means
    asking randomly for feedback, typically after interacting with the product. There
    will be variability because the same customer isn‚Äôt always asked for feedback,
    so more data is needed to estimate accurately. For simple questions such as NPS,
    this is an easy ask. It takes a little more effort to code and request an SUS
    score. These can be supplemented at any time with more expository and open-ended
    questions.
  prefs: []
  type: TYPE_NORMAL
- en: Net Promoter Score (NPS)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**NICE Satmetrix** is the co-developer and owner of the NPS. It is well known
    as a simple benchmark of brand quality. Because of its simplicity, it has also
    been adapted for use in product analysis. Nominally, a business‚Äôs customer is
    asked the following question:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This classic question is simple to ask and easy to calculate. It is a broad
    stroke, and it can be adapted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Adapt the wording to your product. The *Retently* website does an excellent
    job of explaining how to tweak this wording to make it work for your use case.
    I won‚Äôt repeat this material here. Read the article if you are ready to deploy
    NPS.
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [NPS and how to modify the survey](https://www.retently.com/blog/nps-survey-templates/)
    ([https://www.retently.com/blog/nps-survey-templates/](https://www.retently.com/blog/nps-survey-templates/))'
  prefs: []
  type: TYPE_NORMAL
- en: Deploy a version of this question based on the product or service. A typical
    pattern is to ask for feedback for every 50th customer with a simple dialog box
    prompt, a side panel, or even inline, depending on the design. It is optional
    to be answered. Some customers won‚Äôt participate. The same customer is typically
    not asked again; tag their account to avoid over-asking survey questions. Automate
    the aggregation of results (hopefully, this is a random sample and needs to be
    correlated with the release number) to generate the NPS. *Table 10.8* covers design
    patterns for gathering the NPS or SUS scores.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Use Case** | **Deployment Method** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Conversational** **chat assistant** | Inline, when a clear ending to the
    conversation is known |'
  prefs: []
  type: TYPE_TB
- en: '| **Conversational** **chat assistant** | As a dialog box when a chat is closed
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Conversational** **chat assistant** | After a feedback flow (such as from
    a Give Feedback icon or label) |'
  prefs: []
  type: TYPE_TB
- en: '| **Recommender** | At the end of a session |'
  prefs: []
  type: TYPE_TB
- en: '| **Recommender** | At the end of any feedback process for evaluating the recommendation
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Web or** **application UI** | At the end of a transaction or significant
    flow |'
  prefs: []
  type: TYPE_TB
- en: '| **Phone tree** | At the end of a transaction or significant flow (i.e., do
    have time for a one-question survey?) |'
  prefs: []
  type: TYPE_TB
- en: '| **SMS,** **Slack, Teams** | After a specific amount of usage |'
  prefs: []
  type: TYPE_TB
- en: '| **Backend or** **hidden AI** | After a specific amount of usage, or the end
    of a significant flow |'
  prefs: []
  type: TYPE_TB
- en: '| **Via Email** | Post-purchase, interaction, or support |'
  prefs: []
  type: TYPE_TB
- en: '| **On receipts, feedback cards at points of sales, or** **with service** |
    Via a QR code |'
  prefs: []
  type: TYPE_TB
- en: '| **Phone calls** | Via human or automated follow-up at the end of a call (You
    will be transferred to answer a brief one-question survey about your experience
    today) |'
  prefs: []
  type: TYPE_TB
- en: '| **In** **real-life interactions** | By asking the customer, and likely entering
    the score and any feedback manually |'
  prefs: []
  type: TYPE_TB
- en: Table 10.8 ‚Äì Approaches to deploying NPS
  prefs: []
  type: TYPE_NORMAL
- en: Recall our discussions about bias. It will be in the results. Sometimes, people
    are people pleasers, so they will not give good feedback when prompted in person.
    Their most recent interaction will color their input. If the interaction were
    a failure, it would impact the data. Ensure a good sample; for example, expect
    skewed results if feedback is gathered from one channel that only handles the
    closing of accounts.
  prefs: []
  type: TYPE_NORMAL
- en: Biases can be introduced in the way questions are asked. I have seen some examples
    that color code the number choices for a survey question, biasing the results.
    Use neutral colors for all options to reduce bias in survey questions. Some customers
    might hesitate to give a poor score because it is color-coded red, as shown in
    *Figure 10**.4*. Use a generic Likert scale, as discussed previously. However,
    once they select the score, asking an optional follow-up question is okay. This
    will give context to their reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_10_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.4 ‚Äì Example of an NPS question ‚Äì use neutral colors for the scale
  prefs: []
  type: TYPE_NORMAL
- en: 'The colors classify the results; don‚Äôt use the colors in a customer survey.
    Scores in the red are detractors, people who would not be advocates and would
    likely turn away from using the product. People scoring 7 or 8 are passive; they
    won‚Äôt get in the way but are not a big help. With little thought, they would switch
    channels or even products. That leaves 9s and 10s promoters, hence the name. Promoters
    who advocate for brands or products enthusiastically recommend the solution to
    others. The math is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mfrac><mrow><mi>N</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>P</mi><mi>r</mi><mi>o</mi><mi>m</mi><mi>o</mi><mi>t</mi><mi>e</mi><mi>r</mi><mi>s</mi><mo>‚àí</mo><mi>N</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>D</mi><mi>e</mi><mi>t</mi><mi>r</mi><mi>a</mi><mi>c</mi><mi>t</mi><mi>o</mi><mi>r</mi><mi>s</mi></mrow><mrow><mi>N</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>R</mi><mi>e</mi><mi>s</mi><mi>p</mi><mi>o</mi><mi>n</mi><mi>d</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>s</mi></mrow></mfrac><mi
    mathvariant="normal">*</mi><mn>100</mn></mrow></mrow></math>](img/24.png)'
  prefs: []
  type: TYPE_IMG
- en: The range is from -100 to 100\. It is fair to gauge your product against other
    brands. Simplestat reports that the average score in the enterprise space is 44\.
    *Table 10.9* gives examples of brands and some samples of NPS. Even well-loved
    brands are in the 60s and 70s. Brands will generally have significantly higher
    scores than services or products. Consider that when deciding how good a product
    is scoring. Comparing releases, channels, or competitive products gives context
    to scores.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_10_e.png)'
  prefs: []
  type: TYPE_IMG
- en: Table 10.9 ‚Äì Example NPSs for some products and services
  prefs: []
  type: TYPE_NORMAL
- en: 'The scores are gathered from these resources:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [NPS scores for the table, Nice Source](https://info.nice.com/rs/338-EJP-431/images/NICE-Satmetrix-infographic-2018-b2c-nps-benchmarks-050418.pdf)
    ([https://info.nice.com/rs/338-EJP-431/images/NICE-Satmetrix-infographic-2018-b2c-nps-benchmarks-050418.pdf](https://info.nice.com/rs/338-EJP-431/images/NICE-Satmetrix-infographic-2018-b2c-nps-benchmarks-050418.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [NPS scores for the table, CustomerGauge Source](https://customergauge.com/benchmarks/blog/nps-saas-net-promoter-score-benchmarks)
    ([https://customergauge.com/benchmarks/blog/nps-saas-net-promoter-score-benchmarks](https://customergauge.com/benchmarks/blog/nps-saas-net-promoter-score-benchmarks))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [Typical NPS for a product](https://www.simplesat.io/understanding-feedback/net-promoter-score-benchmarks/)
    ([https://www.simplesat.io/understanding-feedback/net-promoter-score-benchmarks/](https://www.simplesat.io/understanding-feedback/net-promoter-score-benchmarks/))'
  prefs: []
  type: TYPE_NORMAL
- en: When at Oracle, we tested and shared an NPS with our customers for our Expense
    Assistant, and in its first release, it scored 55\. This was much higher than
    previous solutions; it was considered a great win. Still, there was room for improvement,
    so various methods were used, including those in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Use these example scores or explore other online posts for scores. It is just
    a general benchmark, but it is quick and easy. It doesn‚Äôt guide where to go next
    and typically requires 100s of responses (roughly) to be valid. However, it also
    only takes a few seconds of a customer‚Äôs time. There are also detractors concerning
    the validity of this approach. Just keep all that in mind when attempting to gather
    an NPS. I think it is worth the time. If the application is coded to gather insights,
    use NPS as one method in addition to a more robust method for feedback. All from
    the same UX approach!
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [Net Promoter Score](https://en.wikipedia.org/wiki/Net_promoter_score)
    ([https://en.wikipedia.org/wiki/Net_promoter_score](https://en.wikipedia.org/wiki/Net_promoter_score))'
  prefs: []
  type: TYPE_NORMAL
- en: Consider the SUS for a robust metric that provides more insight.
  prefs: []
  type: TYPE_NORMAL
- en: SUS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Where NPS is a one-question form, the **Software Usability Scale** (**SUS**)
    is a 10-question survey focused on usability. It is a reasonable way to measure
    UX or conversational quality. It can be deployed for UIs with recommenders, but
    it is hard to tease out specific details about one element, like a recommendation,
    without additional questions. It is a well-understood 100-point scale, so it is
    easy to interpret the scores.
  prefs: []
  type: TYPE_NORMAL
- en: 'It uses the Likert scale from one to five, ranging from ‚ÄúStrongly Disagree‚Äù
    to ‚ÄúStrongly Agree.‚Äù Imagine how these questions would feel when asked of them
    after using the application. The ten questions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: I think that I would like to use this system frequently.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I found the system unnecessarily complex.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I thought the system was easy to use.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I think that I would need the support of a technical person to be able to use
    this system.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I found the various functions in this system were well integrated.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I thought there was too much inconsistency in this system.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I would imagine that most people would learn to use this system very quickly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I found the system very cumbersome to use.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I felt very confident using the system.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I needed to learn a lot of things before I could get going with this system.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The SUS questions follow one of the tricks explored in our survey discussion.
    Some questions are phrased in the positive (I feel very confident), and others
    are negative (I found the system very cumbersome) to avoid respondents answering
    on autopilot. Patrick Lawson points out in his blog post that these questions
    also have redundancy. This is common in surveys. The same question is asked slightly
    differently to create a more robust metric. Read more about the SUS in Patrick‚Äôs
    background article:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [How to SUS out usability scores](https://www.thinkcompany.com/blog/how-to-sus-out-usability-scores/)
    by Patrick Lawson ([https://www.thinkcompany.com/blog/how-to-sus-out-usability-scores/](https://www.thinkcompany.com/blog/how-to-sus-out-usability-scores/))'
  prefs: []
  type: TYPE_NORMAL
- en: The calculation to get the 1 to 100 score is specific to this model. Subtract
    one from each of the positively oriented items‚Äô scores, subtract the answers for
    the negatively oriented scores from five, and sum all these scores up. The sum
    multiplied by 2.5 gives the total. Compare the score to the outcomes in *Figure
    10**.5*, shared by Jeff Sauro from MeasuringU. He includes a comparison to NPS.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_10_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.5 ‚Äì How to interpret a SUS score
  prefs: []
  type: TYPE_NORMAL
- en: 'Jeff Sauro goes further into the technical scoring and implications of edits
    to the traditional wording. They point out that some of the work choices for the
    metric, which John Brooke developed in the 1980s, might seem old-fashioned. Read
    their post to learn more about the scoring and reliability of the metric. It is
    standing the test of time:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [Is SUS Antiquated](https://measuringu.com/is-the-sus-too-antiquated/)
    ([https://measuringu.com/is-the-sus-too-antiquated/](https://measuringu.com/is-the-sus-too-antiquated/))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Jeff breaks down the scores and details even further. I encourage exploring
    his posts, even if you have used the SUS for years:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [Interpreting a SUS Score](https://measuringu.com/interpret-sus-score/)
    ([https://measuringu.com/interpret-sus-score/](https://measuringu.com/interpret-sus-score/))'
  prefs: []
  type: TYPE_NORMAL
- en: Go to the government website for details on implementing the SUS.
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [Software Usability Scale](https://www.usability.gov/how-to-and-tools/methods/system-usability-scale.html)
    ([https://www.usability.gov/how-to-and-tools/methods/system-usability-scale.html](https://www.usability.gov/how-to-and-tools/methods/system-usability-scale.html))'
  prefs: []
  type: TYPE_NORMAL
- en: The SUS provides more details to drill into than the NPS but doesn‚Äôt provide
    granular information on what to fix. These methods can be followed up with open-ended
    questions to expose issues. This additional detail is valuable and actionable.
    The SUS score communicates quality over time and gives broad visibility to progress.
  prefs: []
  type: TYPE_NORMAL
- en: Refine with heuristic evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Heuristic evaluation was covered in [*Chapter 9*](B21964_09_split_000.xhtml#_idTextAnchor190),
    *Guidelines and Heuristics*. The same approach, along with classifying and user
    scoring, from [*Chapter 4*](B21964_04.xhtml#_idTextAnchor085), *Scoring Stories*,
    can be used at this stage of the development process. All of that applies to monitoring
    and evaluation results. We can classify issues, score them to prioritize improvements,
    then apply refinements to the data, prompts, and model improvements (or even test
    against a new model to merge into the solution), or work on a new integration.
    You‚Äôve got this! Apply what you know. This leads us to a discussion on handling
    the process of conversational AI within typical development organizations in [*Chapter*
    *11*](B21964_11.xhtml#_idTextAnchor236), *Process*.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Tools on both ends were covered: analytic tools, such as RAGAs with metrics
    (sorry for the math!), and usability tools to monitor ongoing improvements from
    the customer‚Äôs perspective. This and monitoring conversations will flow into refinements
    needed for RAG materials, instructions/prompts, fine-tuning, and swapping to newer
    models while improving integrations with services and APIs. To do this well, this
    needs to fit into a process that can handle the dynamic nature of LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter offers insights into implementing processes conducive to LLM
    development and integrating with engineering to improve LLM solutions continuously.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '| ![](img/B21964_10_QR.jpg) | The links, book recommendations, and GitHub files
    in this chapter are posted on the reference page.Web Page: [Chapter 10 References](https://uxdforai.com/references#C10)
    ([https://uxdforai.com/references#C10](https://uxdforai.com/references#C10)) |'
  prefs: []
  type: TYPE_TB
