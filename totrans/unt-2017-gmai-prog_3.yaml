- en: Implementing Sensors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we'll learn to implement AI behavior using the concept of a
    sensory system similar to what living entities have. As we discussed earlier,
    a character AI system needs to have awareness of its environment such as where
    the obstacles are, where the enemy it is looking for is, whether the enemy is
    visible in the player's sight, and so on. The quality of our NPC's AI completely
    depends on the information it can get from the environment. Nothing breaks the
    level of immersion in a game like an NPC getting stuck behind a wall. Based on
    the information the NPC can collect, the AI system can decide which logic to execute
    in response to that data. If the sensory systems do not provide enough data, or
    the AI system is unable to properly take action on that data, the agent can begin
    to glitch, or behave in a way contrary to what the developer, or more importantly
    the player, would expect. Some games have become infamous for their comically
    bad AI glitches, and it's worth a quick internet search to find some videos of
    AI glitches for a good laugh.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can detect all the environment parameters and check them against our predetermined
    values if we want. But using a proper design pattern will help us maintain code
    and thus will be easy to extend. This chapter will introduce a design pattern
    that we can use to implement sensory systems. We will be covering:'
  prefs: []
  type: TYPE_NORMAL
- en: What sensory systems are
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some of the different sensory systems that exist
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to set up a sample tank with sensing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basic sensory systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our agent's sensory systems should believably emulate real-world senses such
    as vision, sound, and so on, to build a model of its environment, much like we
    do as humans. Have you ever tried to navigate a room in the dark after shutting
    off the lights? It gets more and more difficult as you move from your initial
    position when you turned the lights off because your perspective shifts and you
    have to rely more and more on your fuzzy memory of the room's layout. While our
    senses rely on and take in a constant stream of data to navigate their environment,
    our agent's AI is a lot more forgiving, giving us the freedom to examine the environment
    at predetermined intervals. This allows us to build a more efficient system in
    which we can focus only on the parts of the environment that are relevant to the
    agent.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of a basic sensory system is that there will be two components, `Aspect`
    and `Sense`. Our AI characters will have senses, such as perception, smell, and
    touch. These senses will look out for specific aspects such as enemies and bandits.
    For example, you could have a patrol guard AI with a perception sense that's looking
    for other game objects with an enemy aspect, or it could be a zombie entity with
    a smell sense looking for other entities with an aspect defined as a brain.
  prefs: []
  type: TYPE_NORMAL
- en: For our demo, this is basically what we are going to implement—a base interface
    called `Sense` that will be implemented by other custom senses. In this chapter,
    we'll implement perspective and touch senses. Perspective is what animals use
    to see the world around them. If our AI character sees an enemy, we want to be
    notified so that we can take some action. Likewise with touch, when an enemy gets
    too close, we want to be able to sense that, almost as if our AI character can
    hear that the enemy is nearby. Then we'll write a minimal `Aspect` class that
    our senses will be looking for.
  prefs: []
  type: TYPE_NORMAL
- en: Cone of sight
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the example provided in [Chapter 2](9e338555-162c-4ed0-a519-035cfcea94ce.xhtml)*,*
    *Finite State Machines and You*, we set up our agent to detect the player tank
    using line of sight, which is literally a line in the form of a raycast. A **raycast**
    is a feature in Unity that allows you to determine which objects are intersected
    by a line cast from a point toward a given direction. While this is a fairly efficient
    way to handle visual detection in a simple way, it doesn''t accurately model the
    way vision works for most entities. An alternative to using line of sight is using
    a cone-shaped field of vision. As the following figure illustrates, the field
    of vision is literally modeled using a cone shape. This can be in 2D or 3D, as
    appropriate for your type of game:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/db1843c4-9f08-43a7-bb52-1b9718725497.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding figure illustrates the concept of a cone of sight. In this case,
    beginning with the source, that is, the agent's eyes, the cone grows, but becomes
    less accurate with distance, as represented by the fading color of the cone.
  prefs: []
  type: TYPE_NORMAL
- en: The actual implementation of the cone can vary from a basic overlap test to
    a more complex realistic model, mimicking eyesight. In a simple implementation,
    it is only necessary to test whether an object overlaps with the cone of sight,
    ignoring distance or periphery. A complex implementation mimics eyesight more
    closely; as the cone widens away from the source, the field of vision grows, but
    the chance of getting to see things toward the edges of the cone diminishes compared
    to those near the center of the source.
  prefs: []
  type: TYPE_NORMAL
- en: Hearing, feeling, and smelling using spheres
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One very simple yet effective way of modeling sounds, touch, and smell is via
    the use of spheres. For sounds, for example, we can imagine the center as being
    the source and the loudness dissipating the farther from the center the listener
    is. Inversely, the listener can be modeled instead of, or in addition to, the
    source of the sound. The listener's hearing is represented by a sphere, and the
    sounds closest to the listener are more likely to be "heard." We can modify the
    size and position of the sphere relative to our agent to accommodate feeling and
    smelling.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure represents our sphere and how our agent fits into the
    setup:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ec1eded7-23cd-4bfd-94ba-cc68c8fc5769.png)'
  prefs: []
  type: TYPE_IMG
- en: As with sight, the probability of an agent registering the sensory event can
    be modified, based on the distance from the sensor or as a simple overlap event,
    where the sensory event is always detected as long as the source overlaps the
    sphere.
  prefs: []
  type: TYPE_NORMAL
- en: Expanding AI through omniscience
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a nutshell, omniscience is really just a way to make your AI cheat. While
    your agent doesn't necessarily know everything, it simply means that they *can*
    know anything. In some ways, this can seem like the antithesis to realism, but
    often the simplest solution is the best solution. Allowing our agent access to
    seemingly hidden information about its surroundings or other entities in the game
    world can be a powerful tool to provide an extra layer of complexity.
  prefs: []
  type: TYPE_NORMAL
- en: In games, we tend to model abstract concepts using concrete values. For example,
    we may represent a player's health with a numeric value ranging from 0 to 100\.
    Giving our agent access to this type of information allows it to make realistic
    decisions, even though having access to that information is not realistic. You
    can also think of omniscience as your agent being able to *use the force* or sense
    events in your game world without having to *physically* experience them.
  prefs: []
  type: TYPE_NORMAL
- en: While omniscience is not necessarily a specific pattern or technique, it's another
    tool in your toolbox as a game developer to cheat a bit and make your game more
    interesting by, in essence, bending the rules of AI, and giving your agent data
    that they may not otherwise have had access to through *physical* means.
  prefs: []
  type: TYPE_NORMAL
- en: Getting creative with sensing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While cones, spheres, and lines are among the most basic ways an agent can see,
    hear, and perceive their environment, they are by no means the only ways to implement
    these senses. If your game calls for other types of sensing, feel free to combine
    these patterns. Want to use a cylinder or a sphere to represent a field of vision?
    Go for it. Want to use boxes to represent the sense of smell? Sniff away!
  prefs: []
  type: TYPE_NORMAL
- en: Using the tools at your disposal, come up with creative ways to model sensing
    in terms relative to your player. Combine different approaches to create unique
    gameplay mechanics for your games by mixing and matching these concepts. For example,
    a magic-sensitive but blind creature could completely ignore a character right
    in front of them until they cast or receive the effect of a magic spell. Maybe
    certain NPCs can track the player using smell, and walking through a collider
    marked *water* can clear the scent from the player so that the NPC can no longer
    track him. As you progress through the book, you'll be given all the tools to
    pull these and many other mechanics off—sensing, decision-making, pathfinding,
    and so on. As we cover some of these techniques, start thinking about creative
    twists for your game.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the scene
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to get started with implementing the sensing system, you can jump
    right into the example provided for this chapter, or set up the scene yourself,
    by following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Let's create a few barriers to block the line of sight from our AI character
    to the tank. These will be short but wide cubes grouped under an empty game object
    called `Obstacles`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a plane to be used as a floor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we add a directional light so that we can see what is going on in our
    scene.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As you can see in the example, there is a target 3D model, which we use for
    our player, and we represent our AI agent using a simple cube. We will also have
    a `Target` object to show us where the tank will move to in our scene.
  prefs: []
  type: TYPE_NORMAL
- en: 'For simplicity, our example provides a point light as a child of the `Target`
    so that we can easily see our target destination in the game view. Our scene hierarchy
    will look similar to the following screenshot after you''ve set everything up
    correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ec3ecb5f-098e-4579-ac8e-10722262f860.png)'
  prefs: []
  type: TYPE_IMG
- en: The scene hierarchy
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we will position the tank, the AI character, and walls randomly in our
    scene. Increase the size of the plane to something that looks good. Fortunately,
    in this demo, our objects float, so nothing will fall off the plane. Also, be
    sure to adjust the camera so that we can have a clear view of the following scene:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/58a5d229-ccbe-48d9-b22a-5d1f383dddf2.png)'
  prefs: []
  type: TYPE_IMG
- en: Our game scene
  prefs: []
  type: TYPE_NORMAL
- en: With the essential setup out of the way, we can begin tackling the code for
    driving the various systems.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the player tank and aspect
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our `Target` object is a simple sphere game object with the mesh render removed,
    so that we end up with only the **Sphere Collider**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Look at the following code in the `Target.cs` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: You'll notice we left in an empty `Start` method in the code. While there is
    a cost in having empty `Start`, `Update`, and other `MonoBehaviour` events that
    don't do anything, we can sometimes choose to leave the `Start` method in during
    development, so that the component shows an enable/disable toggle in the inspector.
  prefs: []
  type: TYPE_NORMAL
- en: Attach this script to our `Target` object, which is what we assigned in the
    inspector to the `targetMarker` variable. The script detects the mouse click event
    and then, using a raycast, it detects the mouse click point on the plane in the
    3D space. After that, it updates the `Target` object to that position in the world
    space in the scene.
  prefs: []
  type: TYPE_NORMAL
- en: A raycast is a feature of the Unity Physics API that shoots a virtual ray from
    a given origin towards a given direction, and returns data on any colliders hit
    along the way.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the player tank
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our player tank is the simple tank model we used in [Chapter 2](9e338555-162c-4ed0-a519-035cfcea94ce.xhtml),
    *Finite State Machines and You*, with a kinematic rigid body component attached.
    The rigid body component is needed in order to generate trigger events whenever
    we do collision detection with any AI characters. The first thing we need to do
    is to assign the tag `Player` to our tank.
  prefs: []
  type: TYPE_NORMAL
- en: The `isKinematic` flag in Unity's Rigidbody component makes it so that external
    forces are ignored, so that you can control the Rigidbody entirely from code or
    from an animation, while still having access to the Rigidbody API.
  prefs: []
  type: TYPE_NORMAL
- en: The tank is controlled by the `PlayerTank` script, which we will create in a
    moment. This script retrieves the target position on the map and updates its destination
    point and the direction accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code in the `PlayerTank.cs` file is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/9d470de6-cccd-4cb1-b95d-38034bd5bff7.png)'
  prefs: []
  type: TYPE_IMG
- en: Properties of our tank object
  prefs: []
  type: TYPE_NORMAL
- en: The preceding screenshot shows us a snapshot of our script in the inspector
    once applied to our tank.
  prefs: []
  type: TYPE_NORMAL
- en: This script queries the position of the `Target` object on the map and updates
    its destination point and the direction accordingly. After we assign this script
    to our tank, be sure to assign our `Target` object to the `targetTransform` variable.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the Aspect class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next, let's take a look at the `Aspect.cs` class. `Aspect` is a very simple
    class with just one public enum of type `AspectTypes` called `aspectType`. That's
    all of the variables we need in this component. Whenever our AI character senses
    something, we'll check the  `aspectType` to see whether it's the aspect that the
    AI has been looking for.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code in the `Aspect.cs` file looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Attach this aspect script to our player tank and set the `aspectType` to `PLAYER`,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4b549b44-5340-4e0b-8e0a-770184fa4f69.png)'
  prefs: []
  type: TYPE_IMG
- en: Setting the Aspect Type of the tank
  prefs: []
  type: TYPE_NORMAL
- en: Creating an AI character
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our NPC will be roaming around the scene in a random direction. It''ll have
    the following two senses:'
  prefs: []
  type: TYPE_NORMAL
- en: The perspective sense will check whether the tank aspect is within a set visible
    range and distance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The touch sense will detect if the enemy aspect has collided with its box collider,
    which we'll be adding to the tank in a later step
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because our player tank will have the `PLAYER` aspect type, the NPC will be
    looking for any `aspectType` not equal to its own.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code in the `Wander.cs` file is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The `Wander` script generates a new random position in a specified range whenever
    the AI character reaches its current destination point. The `Update` method will
    then rotate our enemy and move it toward this new destination. Attach this script
    to our AI character so that it can move around in the scene. The `Wander` script
    is rather simplistic, but we will cover more advanced locomotion approaches in
    later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Sense class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `Sense` class is the interface of our sensory system that the other custom
    senses can implement. It defines two virtual methods, `Initialize` and `UpdateSense`,
    which will be implemented in custom senses, and are executed from the `Start`
    and `Update` methods, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Virtual methods are methods that can be overridden using the `override` modifier
    in derived classes. Unlike `abstract` classes, virtual classes do not require
    that you override them.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code in the `Sense.cs` file looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The basic properties include its detection rate to execute the sensing operation,
    as well as the name of the aspect it should look for. This script will not be
    attached to any of our objects since we'll be deriving from it for our actual
    senses.
  prefs: []
  type: TYPE_NORMAL
- en: Giving a little perspective
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The perspective sense will detect whether a specific aspect is within its field
    of view and visible distance. If it sees anything, it will take the specified
    action, which in this case is to print a message to the console.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code in the `Perspective.cs` file looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We need to implement the `Initialize` and `UpdateSense` methods that will be
    called from the `Start` and `Update` methods of the parent `Sense` class, respectively.
    In the `DetectAspect` method, we first check the angle between the player and
    the AI's current direction. If it's in the field of view range, we shoot a ray
    in the direction that the player tank is located. The ray length is the value
    of the visible distance property.
  prefs: []
  type: TYPE_NORMAL
- en: The `Raycast` method will return when it first hits another object. This way,
    even if the player is in the visible range, the AI character will not be able
    to see if it's hidden behind the wall. We then check for an `Aspect` component,
    and it will return true only if the object that was hit has an `Aspect` component
    and its `aspectType` is different from its own.
  prefs: []
  type: TYPE_NORMAL
- en: The `OnDrawGizmos` method draws lines based on the perspective field of view
    angle and viewing distance so that we can see the AI character's line of sight
    in the editor window during play testing. Attach this script to our AI character
    and be sure that the aspect type is set to `ENEMY`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This method can be illustrated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Touching is believing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The next sense we'll be implementing is `Touch.cs`, which triggers when the
    player tank entity is within a certain area near the AI entity. Our AI character
    has a box collider component and its `IsTrigger` flag is on.
  prefs: []
  type: TYPE_NORMAL
- en: We need to implement the `OnTriggerEnter` event, which will be called whenever
    another collider enters the collision area of this game object's collider. Since
    our tank entity also has a collider and rigid body components, collision events
    will be raised as soon as the colliders of the AI character and player tank collide.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unity provides two other trigger events besides `OnTriggerEnter`: `OnTriggerExit`
    and `OnTriggerStay`. Use these to detect when a collider leaves a trigger, and
    to fire off every frame that a collider is inside the trigger, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code in the `Touch.cs` file is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Our sample NPC and tank have  `BoxCollider` components on them already. The
    NPC has its sensor collider set to `IsTrigger = true` . If you''re setting up
    the scene on your own, make sure you add the `BoxCollider` component yourself,
    and that it covers a wide enough area to trigger easily for testing purposes.
    Our trigger can be seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/79522f53-2057-45db-a581-acae504d9d2f.png)'
  prefs: []
  type: TYPE_IMG
- en: The collider around our player
  prefs: []
  type: TYPE_NORMAL
- en: 'The previous screenshot shows the box collider on our enemy AI that we''ll
    use to trigger the touch sense event. In the following screenshot, we can see
    how our AI character is set up:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dbb978b2-789e-49da-b789-ab409d25467b.png)'
  prefs: []
  type: TYPE_IMG
- en: The properties of our NPC
  prefs: []
  type: TYPE_NORMAL
- en: For demo purposes, we just print out that the enemy aspect has been detected
    by the touch sense, but in your own games, you can implement any events and logic
    that you want. This system ties in really nicely with other concepts covered in
    this book, such as states, which we learned about in [Chapter 2](9e338555-162c-4ed0-a519-035cfcea94ce.xhtml)*,
    Finite State Machines and You*.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Hit play in the Unity editor and move the player tank near the wandering AI
    NPC by clicking on the ground to direct the tank to move to the clicked location.
    You should see the Enemy touch detected message in the console log window whenever
    our AI character gets close to our player tank:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/888ce9b1-b216-4115-9ede-f83fbdcdf7a2.png)'
  prefs: []
  type: TYPE_IMG
- en: Our NPC and tank in action
  prefs: []
  type: TYPE_NORMAL
- en: The previous screenshot shows an AI agent with touch and perspective senses
    looking for another aspect. Move the player tank in front of the NPC, and you'll
    get the Enemy detected message. If you go to the editor view while running the
    game, you should see the debug lines being rendered. This is because of the `OnDrawGizmos`
    method implemented in the perspective `Sense` class.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter introduced the concept of using sensors and implemented two distinct
    senses—perspective and touch—for our AI character. The sensory system is one component
    of the whole decision-making system. We can use the sensory system in combination
    with a behavior system to execute certain behaviors for certain senses. For example,
    we can use an FSM to change to Chase and Attack states from the Patrol state once
    we have detected that there's an enemy within line of sight. We'll also cover
    how to apply behavior tree systems in [Chapter 6](8db41b31-be4b-432f-a68e-ef13e1f7e03b.xhtml),
    *Behavior Trees*.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll be looking at popular pathfinding algorithms. We'll
    learn how to make our AI agent navigate complex environments using the ever-popular
    A* pathfinding algorithm, and even Unity's own `NavMesh` system.
  prefs: []
  type: TYPE_NORMAL
