<html><head></head><body><div><div><div><div><div><h1 class="title"><a id="ch10" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Chapter 10. Current Trends in Neural Networks</h1></div></div></div><p class="calibre11">This final chapter shows the reader the most recent trends in neural networks. Although this book is introductory, it is always useful to be aware of the latest developments and where the science behind this theory is going to. Among the latest advancements is the so-called <strong class="calibre12">deep learning</strong>, a very popular research field for many data scientists; this type of network<a id="id557" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> is briefly covered in this chapter. Convolutional and cognitive architectures are also in this trend and gaining popularity for multimedia data recognition. Hybrid systems that combine different architectures are a very interesting strategy for solving more complex problems, as well as applications that involve analytics, data visualization, and so on. Being more theoretical, there is no actual implementation of the architectures, although an example of implementation for a hybrid system is provided. Topics covered in this chapter include:</p><div><ul class="itemizedlist"><li class="listitem">Deep learning</li><li class="listitem">Convolutional neural networks</li><li class="listitem">Long short term memory networks</li><li class="listitem">Hybrid systems</li><li class="listitem">Neuro-Fuzzy</li><li class="listitem">Neuro-Genetic</li><li class="listitem">Implementation of a hybrid neural network</li></ul></div><div><div><div><div><h1 class="title2"><a id="ch10lvl1sec59" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Deep learning</h1></div></div></div><p class="calibre11">One of the latest advancements in neural networks is the so-called deep learning. Nowadays it is nearly impossible to talk about neural networks without mentioning deep learning, because the recent research on feature extraction, data representation, and transformation has found that many layers of processing information are able to abstract and produce better representations of data for learning. Throughout this book we have seen that neural networks require input data in numerical form, no matter if the original data is categorical or binary, neural networks cannot process non-numerical data directly. But it turns out that in the real world most of the data is non-numerical or is even unstructured, such<a id="id558" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> as images, videos, audios, texts, and so on.</p><p class="calibre11">In this sense a deep network would have many layers that could act as data processing units to transform this data and provide it to the next layer for subsequent data processing. This is analogous to the process that happens in the brain, from the nerve endings to the cognitive core; in this long path the signals are processed by multiple layers before resulting in signals that control the human body. Currently, most of the research on deep learning has been on the processing of unstructured data, particularly image and sound recognition and natural language processing.</p><div><div><h3 class="title4"><a id="tip37" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Tip</h3><p class="calibre17">Deep learning is still under development and much has changed since 2012. Big companies such as Google and Microsoft have teams for research on this field and much is likely to change in the next couple of years.</p></div></div><p class="calibre11">A scheme of a deep learning architecture is shown in the following figure:</p><div><img src="img/B5964_10_01.jpg" alt="Deep learning" class="calibre220"/></div><p class="calibre11">On the other hand, deep neural networks have some problems that need to be overcome. The main problem is overfitting. The many layers that produce new representations of data are very sensitive to the training data, because the deeper the signals reach in the neural layers, the more specific the transformation will be for the input data. Regularization methods and pruning are often applied to prevent overfitting. Computation time is another common issue in training deep networks. The standard backpropagation algorithm can take a very long time to train a deep neural network, although<a id="id559" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> strategies such as selecting a smaller training dataset can speed up the training time. In addition, in order to train a deep neural network, it is often recommended to use a faster machine and parallelize the training as much as possible.</p></div></div></div>



  
<div><div><div><div><div><h1 class="title2"><a id="ch10lvl1sec60" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Deep architectures</h1></div></div></div><p class="calibre11">There is a great <a id="id560" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>variety of deep neural architectures with both feedforward and feedback flows, although they are typically feedforward. Main architectures are, without limitation to:</p><p class="calibre11">
<strong class="calibre12">Convolutional neural network</strong>
</p><p class="calibre11">In this <a id="id561" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>architecture, the layers may have<a id="id562" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> multidimensional organization. Inspired by the visual cortex of animals, the typical dimensionality applied to the layers is three-dimensional. In <strong class="calibre12">convolutional neural networks</strong> (<strong class="calibre12">CNNs</strong>), part of the signals of a preceding layer is fed into another part of neurons in the following layer. This architecture is feedforward and is well applied for image and sound recognition. The main feature that distinguishes this architecture from Multilayer Perceptrons is the partial connectivity between layers. Considering the fact that not all neurons are relevant for a certain neuron in the next layer, the connectivity is local and respects the correlation between neurons. This prevents both long time training and overfitting, provided that a fully connected MLP blows up the number of weight as the dimension of images grows, for example. In addition, neurons in layers are arranged in dimensions, typically three, thereby staked in an array in width, height, and depth.</p><div><img src="img/B5964_10_02.jpg" alt="Deep architectures" class="calibre221"/></div><p class="calibre11">In this architecture, the layers may have multidimensional organization. Inspired by the visual cortex of animals, the typical dimensionality applied to the layers is three-dimensional. In <strong class="calibre12">convolutional neural networks</strong> (<strong class="calibre12">CNNs</strong>), part of the signals of a preceding<a id="id563" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> layer is fed into another part<a id="id564" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> of neurons in the following layer. This architecture is feedforward and is well applied for image and sound recognition. The main feature that distinguishes this architecture from multilayer perceptrons is the partial connectivity between layers. Considering the fact that not all neurons are relevant for a certain neuron in the next layer, the connectivity is local and respects the correlation between neurons. This prevents both long time training and overfitting, provided that a fully connected MLP blows up the number of weight as the dimension of images grows, for example. In addition, neurons in layers are arranged in dimensions, typically three, thereby staked in an array in width, height, and depth.</p><p class="calibre11">
<strong class="calibre12">Long short-term memory</strong>: This is a recurrent type of neural network that takes into account <a id="id565" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>always the last value <a id="id566" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>of the hidden layer, exactly like a <strong class="calibre12">hidden Markov model</strong> (<strong class="calibre12">HMM</strong>). A <strong class="calibre12">Long Short Time Memory network</strong> (<strong class="calibre12">LSTM</strong>) has LSTM <a id="id567" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>units instead of traditional neurons, and these units implement operations such as store and forget a value to control the flow in a deep network. This architecture is well applied to natural language processing, due to the capacity of retaining information for a long time while receiving completely unstructured data such as audio or text files. One way to train this type of network is the backpropagation through time (BPTT) algorithm, but there are also other algorithms such as reinforcement learning or evolution strategies.</p><div><img src="img/B5964_10_03.jpg" alt="Deep architectures" class="calibre222"/></div><p class="calibre11">
<strong class="calibre12">Deep belief network</strong>: <strong class="calibre12">Deep belief networks</strong> (<strong class="calibre12">DBN's</strong>) are probabilistic models where layers are<a id="id568" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> classified into visible and hidden. This<a id="id569" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> is also a type of recurrent neural <a id="id570" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>network based on a <strong class="calibre12">restricted Boltzmann machine</strong> (<strong class="calibre12">RBM</strong>). It is typically used as a first step in the training of a <strong class="calibre12">deep neural network</strong> (<strong class="calibre12">DNN</strong>), which is further trained by other supervised algorithms such as backpropagation. In this architecture each layer acts like a feature detector, abstracting new representations of data. The visible layer acts both as an output and as<a id="id571" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> an input, and the deepest hidden layer represents the<a id="id572" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> highest level of abstraction. Applications of this architecture are typically the same as those of convolutional neural networks.</p><div><img src="img/B5964_10_04.jpg" alt="Deep architectures" class="calibre223"/></div><div><div><div><div><h2 class="title5"><a id="ch10lvl2sec121" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How to implement deep learning in Java</h2></div></div></div><p class="calibre11">Because <a id="id573" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>this book is introductory, we are not<a id="id574" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> diving into further details on deep learning in this chapter. However, some recommendations of code for a deep architecture are provided. An example on how a convolutional neural network would be implemented is provided here. One needs to implement a class called <code class="literal">ConvolutionalLayer</code> to represent a multidimensional layer, and a <code class="literal">CNN</code> class for the convolutional neural network itself:</p><div><pre class="programlisting">public class ConvolutionalLayer extends NeuralLayer{
  int height,width, depth;
//…
  ArrayList&lt;ArrayList&lt;ArrayList&lt;Neuron&gt;&gt;&gt; neurons;
  Map&lt;Neuron,Neuron&gt; connections;
  ConvolutionalLayer previousLayer;

  //the method call should take into account the mapping 
  // between neurons from different layers
  @Override
  public void calc(){
    ArrayList&lt;ArrayList&lt;ArrayList&lt;double&gt;&gt;&gt; inputs;
    foreach(Neuron n:neurons){
      foreach(Neuron m:connections.keySet()){ 
  // here we get only the inputs that are connected to the neuron

      }
    }
  }

}

public class CNN : NeuralNet{
  int depth;
  ArrayList&lt;ConvolutionalLayer&gt; layers;
//…
  @Override
  public void calc(){
    //here we perform the calculation for each layer, 
    //taking into account the connections between layers
  }
}</pre></div><p class="calibre11">In this class, <a id="id575" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the neurons are organized in dimensions and<a id="id576" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> methods for pruning are used to make the connections between the layers. Please see the files <code class="literal">ConvolutionalLayer.java</code> and <code class="literal">CNN.java</code> for further details.</p><p class="calibre11">Since the other architectures are recurrent and this book does not cover the recurrent neural networks (for simplicity purposes in an introductory book) they are provided only for the reader's information. We suggest the reader to take a look at the references provided to find out more on these architectures.</p><div><div><div><div><h3 class="title7"><a id="ch10lvl3sec03" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Hybrid systems</h3></div></div></div><p class="calibre11">In machine<a id="id577" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> learning, or even in the artificial intelligence field, there are many other algorithms and techniques other than neural networks. Each <a id="id578" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>technique has its strengths and drawbacks, and that inspires many researchers to combine them into a single structure. Neural networks are part of the connectionist approach for artificial intelligence, whereby operations are performed on numerical and continuous values; but there are other approaches that include cognitive (rule-based systems) and evolutionary computation.</p><div><table border="1" class="calibre20"><colgroup class="calibre21"><col class="calibre22"/><col class="calibre22"/><col class="calibre22"/></colgroup><thead class="calibre23"><tr class="calibre24"><th valign="bottom" class="calibre25">
<p class="calibre26">Connectionist</p>
</th><th valign="bottom" class="calibre25">
<p class="calibre26">Cognitive</p>
</th><th valign="bottom" class="calibre25">
<p class="calibre26">Evolutionary</p>
</th></tr></thead><tbody class="calibre27"><tr class="calibre31"><td class="calibre29">
<p class="calibre26">Numerical processing</p>
</td><td class="calibre29">
<p class="calibre26">Symbolic processing</p>
</td><td class="calibre29">
<p class="calibre26">Numerical and symbolic processing</p>
</td></tr><tr class="calibre34"><td class="calibre29">
<p class="calibre26">Large network structures</p>
</td><td class="calibre29">
<p class="calibre26">Large rule bases and premises</p>
</td><td class="calibre29">
<p class="calibre26">Large quantity of solutions</p>
</td></tr><tr class="calibre31"><td class="calibre29">
<p class="calibre26">Performance by statistics</p>
</td><td class="calibre29">
<p class="calibre26">Design by experts/statistics</p>
</td><td class="calibre29">
<p class="calibre26">Better solutions are produced every iteration</p>
</td></tr><tr class="calibre37"><td class="calibre29">
<p class="calibre26">Highly sensitive to data</p>
</td><td class="calibre29">
<p class="calibre26">Highly sensitive to theory</p>
</td><td class="calibre29">
<p class="calibre26">Local minima proof</p>
</td></tr></tbody></table></div><pre><a id="ch10lvl2sec122" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Neuro-fuzzy</h2></div></div></div><p class="calibre11">Fuzzy logic is <a id="id584" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>a type of rule-based processing, where every<a id="id585" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> variable is converted to a symbolic value according to a membership function, and then the combination of all variables is queried against an <em class="calibre16">IF-THEN</em> rule database.</pre><div><img src="img/B5964_10_05.jpg" alt="Neuro-fuzzy" class="calibre224"/></div><p class="calibre11">A membership function usually has a Gaussian bell shape, which tells us how much a given value is a <em class="calibre16">member</em> of that class. Let's take, for example, temperature, which may take on three<a id="id586" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> different classes (cold, normal, and warm). A membership <a id="id587" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>value will be higher the more the temperature is closer to the bell shape centers.</p><div><img src="img/B5964_10_06.jpg" alt="Neuro-fuzzy" class="calibre225"/></div><p class="calibre11">Furthermore, the fuzzy processing finds which rules are fired by every input record and which output values are produced. A neuro-fuzzy architecture treats each input differently, so the first hidden layer has a set of neurons for each input corresponding for each membership function:</p><div><img src="img/B5964_10_07.jpg" alt="Neuro-fuzzy" class="calibre226"/></div><div><div><h3 class="title6"><a id="tip39" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Tip</h3><p class="calibre17">In this architecture, the training finds optimal weights for the rule processing and weighted sum of consequent parameters only, the first hidden layer has no adjustable weights.</p></div></div><p class="calibre11">In fuzzy logic <a id="id588" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>architecture, the experts define a rule database that may become <a id="id589" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>huge as the number of variables increase. The neuro-fuzzy architecture releases the designer from defining the rules, and lets this task be performed by the neural network. The training of a neuro-fuzzy can be performed by gradient type algorithms such as backpropagation or matrix algebra such as least squares, both in the supervised mode. Neuro-fuzzy systems are suitable for control of dynamic systems and diagnostics.</p></div><div><div><div><div><h2 class="title5"><a id="ch10lvl2sec123" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Neuro-genetic</h2></div></div></div><p class="calibre11">In the <a id="id590" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>evolutionary artificial intelligence approach, one common<a id="id591" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> strategy is genetic algorithms. This name is inspired by natural evolution, which states that beings more adapted to the environment are able to produce new generations of better adapted beings. In the computing intelligence field, the <em class="calibre16">beings</em> or <em class="calibre16">individuals</em> are candidate solutions or hypotheses that can solve an optimization problem. Supervised neural networks are used for optimization, since there is an error measure that we want to minimize by adjusting the neural weights. While the training algorithms are able to find better weights by gradient methods, they<a id="id592" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> often fall in local minima. Although some mechanisms, <a id="id593" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>such as regularization and momentum, may improve the results, once the weights fall in a local minimum, it is very unlikely that a better weight will be found, and in this context genetic algorithms are very good at it.</p><p class="calibre11">Think of the neural weights as a genetic code (or DNA). If we could generate a finite number of random generated weight sets, and evaluate which produce the best results (smaller errors or other performance measurement), we would select a top N best weight, and then set and apply genetic operations on them, such as reproduction (interchange of weights) and mutation (random change of weights).</p><div><img src="img/B5964_10_08.jpg" alt="Neuro-genetic" class="calibre227"/></div><p class="calibre11">This process is repeated until some acceptable solution is found.</p><p class="calibre11">Another strategy is to use genetic operations on neural network parameters, such as number of neurons, learning rate, activation functions, and so on. Considering that, there is always a need to adjust parameters or train multiple times to ensure we've found a good solution. So, one may code all parameters in a genetic code (parameter set) and generate multiple neural networks for each parameter set.</p><p class="calibre11">The scheme of a genetic algorithm is shown in the following figure:</p><div><img src="img/B5964_10_09.jpg" alt="Neuro-genetic" class="calibre228"/></div><div><div><h3 class="title6"><a id="tip40" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Tip</h3><p class="calibre17">Genetic<a id="id594" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> algorithms are broadly used for many <a id="id595" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>optimization problems, but in this book we are sticking with these two classes of problems, weight and parameter optimization.</p></div></div></div></div></div>



  
<div><div><div><div><div><h1 class="title2"><a id="ch10lvl1sec61" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Implementing a hybrid neural network</h1></div></div></div><p class="calibre11">Now, let's <a id="id596" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>implement a simple code that can be used in the neuro-fuzzy and neuro-genetic networks. First, we need to define Gaussian functions for activation that will be the membership functions:</p><div><pre class="programlisting">public class Gaussian implements ActivationFunction{
  double A=1.0,B=0.0,C=1.0;
  public Gaussian(double A){ ///…
  }
  public double calc(double x){
    return this.A*Math.exp(-Math.pow(x-this.B,2.0) / 2*Math.pow(this.C,2.0));
  }
}</pre></div><p class="calibre11">The fuzzy sets and rules need to be represented in a way that a neural network can understand and drive the execution. This representation includes the quantity of sets per input, therefore having the information on how the neurons are connected; and the membership<a id="id597" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> functions for each set. A simple way to represent the quantity is an array. The array of sets just indicates how many sets there are for each variable; and the array of rules is a matrix, where each row represents a rule and each column represents a variable; each set can be assigned a numerical integer value for reference in the rule array. An example of three variables, each having three sets, is defined in the following snippet, along with the rules:</p><div><pre class="programlisting">int[] setsPerVariable = {3,3,3};
int[][] rules = {{0,0,0},{0,1,0},{1,0,1},{1,1,0},{2,0,2},{2,1,1}, {2,2,2}}; </pre></div><p class="calibre11">The membership functions can be referenced in a serialized array:</p><div><pre class="programlisting">ActivationFunction[] fuzzyMembership = {new Gausian(1.0),//… 
}};</pre></div><p class="calibre11">We need also to create classes for the layers of a neuro fuzzy architecture, such as <code class="literal">InputFuzzyLayer</code> and <code class="literal">RuleLayer</code>. They can be children of a <code class="literal">NeuroFuzzyLayer</code> superclass, which can inherit from <code class="literal">NeuralLayer</code>. These classes are necessary because they work differently from the already defined neural layer:</p><div><pre class="programlisting">public class NeuroFuzzyLayer extends NeuralLayer{
  double[] inputs;
  ArrayList&lt;Neuron&gt; neurons;
  Double[] outputs;
  NeuroFuzzyLayer previousLayer;
///…
}

public class InputFuzzyLayer extends NeuroFuzzyLayer{
 
  int[] setsPerVariable;
  ActivationFunction[] fuzzyMembership;
  //…
}

public class RuleLayer extends NeuroFuzzyLayer{
  int[][] rules;
//…
}</pre></div><p class="calibre11">A <code class="literal">NeuroFuzzy</code> class will inherit from <code class="literal">NeuralNet</code>, having references to the other fuzzy layer classes. The <code class="literal">calc()</code>methods of the <code class="literal">NeuroFuzzyLayer</code> will also be different, taking into account the membership functions centers:</p><div><pre class="programlisting">public class NeuroFuzzy extends NeuralNet{
  InputFuzzyLayer inputLayer;
  RuleLayer ruleLayer;
  NeuroFuzzyLayer outputLayer;
//…
}</pre></div><p class="calibre11">For more <a id="id598" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>details, see the files in the <code class="literal">edu.packt.neuralnet.neurofuzzy</code> package.</p><p class="calibre11">To code a neuro-genetic for weight sets, one needs to define the genetic operations. Let's create a class called <code class="literal">NeuroGenetic</code> to implement reproduction and mutation:</p><div><pre class="programlisting">public class NeuroGenetic{
  // each element ArrayList&lt;double&gt; is a solution, i.e. 
  // a set of weights
  ArrayList&lt;ArrayList&lt;double&gt;&gt; population;
  ArrayList&lt;double&gt; score;

  NeuralNet neuralNet;
  NeuralDataSet trainingDataSet;
  NeuralDataSet testDataSet;

  public ArrayList&lt;ArrayList&lt;double&gt;&gt; reproduction(ArrayList&lt;ArrayList&lt;double&gt;&gt; solutions){
    // a set of weights is passed as an argument
    // the weights are just swapped between them in groups of two
  }
 
  public ArrayList&lt;ArrayList&lt;double&gt;&gt; mutation(ArrayList&lt;ArrayList&lt;double&gt;&gt; solutions){
    // a random weight can suddenly change its value
  }

}</pre></div><p class="calibre11">The next step is to define the evaluation of each weight on each iteration:</p><div><pre class="programlisting">public double evaluation(ArrayList&lt;double&gt; solution){
  neuralNet.setAllWeights(solution);
  LearningAlgorithm la = new LearningAlgorithm(neuralNet,trainingDataSet);
  la.forward();
  return la.getOverallGeneralError();
}</pre></div><p class="calibre11">Finally, we can just call a neuro-genetic algorithm by using the following code:</p><div><pre class="programlisting">public void run{
  generatePopulation();
  int generation=0;
  while(generation&lt;MaxGenerations &amp;&amp; bestMSError&gt;MinMSError){ 
    //evaluate all
    foreach(ArrayList&lt;double&gt; solution:population){
      score.set(i,evaluation(solution));
    }
    //make a rank
    int[] rank = rankAll(score);
    //check the best MSE
    if(ArrayOperations.min(score)&lt;bestMSError){
      bestMSError = ArrayOperations.min(score);
      bestSolution = population.get(ArrayOperations.indexMin(score));
    }
    //perform a selection for reproduction
    ArrayList&lt;ArrayList&lt;double&gt;&gt; newSolutions = reproduction(
      selectionForReproduction(rank,score,population));
    //perform selection for mutation
    ArrayList&lt;ArrayList&lt;double&gt;&gt; mutated = mutation(selectionForMutation(rank,score,population));
    //perform selection for elimintation
    if(generation&gt;5)
      eliminateWorst(rank,score,population);
    //add the new elements
    population.append(newSolutions);
    population.append(mutated);
  }
  System.out.println("Best MSE found:"+bestMSError);

}</pre></div></div></div>



  
<div><div><div><div><div><h1 class="title2"><a id="ch10lvl1sec62" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Summary</h1></div></div></div><p class="calibre11">In this final chapter, we gave the reader a glimpse of what to do next in this field. Being more theoretical, this chapter has focused more on the functionality and information than on practical implementation, because this would be very heavy for an introductory book. In every case, a simple code is provided to give a hint on how to further implement deep neural networks. The reader is then encouraged to modify the codes of the previous chapters, adapting them to the hybrid neural networks and comparing the results. Being a very dynamic and novel field of research, at every moment new approaches and algorithms are under development, and we provide in the references a list of publications to stay up to date on this subject.</p></div></div>



  </body></html>