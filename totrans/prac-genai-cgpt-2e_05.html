<html><head></head><body>
<div><h1 class="chapterNumber">3</h1>
<h1 class="chapterTitle" id="_idParaDest-49">Understanding Prompt Engineering</h1>
<p class="normal">In the previous chapters, we<a id="_idIndexMarker138"/> mentioned the term <strong class="keyWord">prompt</strong> several times while referring to user input in ChatGPT and <strong class="keyWord">large language models</strong> (<strong class="keyWord">LLMs</strong>) in general.</p>
<p class="normal">Since prompts have a massive impact on LLMs’ performance, prompt engineering is a crucial activity to get the most out of your GenAI tool. In fact, there are several techniques that can be implemented not only to refine your LLMs’ responses but also to reduce risks associated with hallucinations and biases.</p>
<p class="normal">In this chapter, we are going to cover the emerging techniques in the field of prompt engineering, starting from basic approaches up to advanced frameworks. More specifically, we will go through the following topics:</p>
<ul>
<li class="bulletList">What is prompt engineering?</li>
<li class="bulletList">Exploring zero-, one-, and few-shot learning</li>
<li class="bulletList">Principles of prompt engineering</li>
<li class="bulletList">Looking at some advanced techniques</li>
<li class="bulletList">Ethical considerations to avoid bias</li>
</ul>
<p class="normal">By the end of this chapter, you will have the foundations to build functional and solid prompts to interact with ChatGPT and, more broadly, with GenAI applications.</p>
<h1 class="heading-1" id="_idParaDest-50">Technical requirements</h1>
<p class="normal">You’ll need an OpenAI account. You can use the free ChatGPT version to run this chapter’s examples.</p>
<h1 class="heading-1" id="_idParaDest-51">What is prompt engineering?</h1>
<p class="normal">Before explaining what prompt engineering is, let’s start by defining a prompt.</p>
<p class="normal">A <strong class="keyWord">prompt</strong> is <a id="_idIndexMarker139"/>text input that guides the behavior of an LLM to generate an output. For example, whenever we interact with ChatGPT, asking a question or giving an instruction, that input text is a prompt. In the context of LLMs and LLM-powered applications, we can distinguish two types of prompts:</p>
<ul>
<li class="bulletList">The <a id="_idIndexMarker140"/>first type is a prompt that the user writes and sends to the LLM. For example, a prompt might be “Give me the recipe for Lasagna Bolognese,” or “Generate a workout plan to run a marathon.”</li>
</ul>
<figure class="mediaobject"><img alt="A screenshot of a chat  Description automatically generated" src="img/B31559_03_01.png"/><img alt="" src="img/B31559_03_02.png"/></figure>
<p class="packt_figref">Figure 3.1: An example of a user’s prompt</p>
<p class="normal">You will hear this referred to simply as a <strong class="keyWord">prompt</strong>, a <strong class="keyWord">query</strong>, or <strong class="keyWord">user input</strong>.</p>
<ul>
<li class="bulletList">The <a id="_idIndexMarker141"/>second type is a prompt that instructs the model to behave in a certain way regardless of the user’s query. This refers to the set of instructions in natural language that the model is provided with so that it behaves in a certain way when interacting with end users. You can think about that as a sort of “backend” of your LLM, something that will be handled by the application developers rather than the final users.</li>
</ul>
<figure class="mediaobject"><img alt="A screenshot of a computer  Description automatically generated" src="img/B31559_03_03.png"/><img alt="" src="img/B31559_03_04.png"/></figure>
<p class="packt_figref">Figure 3.2: An example of a system message</p>
<p class="normal">We refer to this type of prompt as a <strong class="keyWord">system message</strong>.</p>
<p class="normal">Prompt engineering <a id="_idIndexMarker142"/>is the process of designing effective prompts that elicit high-quality and relevant outputs from LLMs. Prompt engineering requires creativity, an understanding of the LLM, and a clear understanding of the objective you want to achieve.</p>
<figure class="mediaobject"><img alt="" src="img/B31559_03_05.png"/></figure>
<p class="packt_figref">Figure 3.3: Example of prompt engineering to specialize LLMs</p>
<p class="normal">Over the last few years, prompt engineering has become a brand-new discipline in itself, and this is a demonstration of the fact that interacting with those models requires a new set of skills and capabilities that did not exist before. </p>
<p class="normal">The <em class="italic">art of prompting</em> has become a top skill when it comes to building GenAI applications in enterprise scenarios; however, it can also be extremely useful for individual users who use ChatGPT or similar AI assistants in daily tasks, as it dramatically improves the quality and accuracy of results.</p>
<p class="normal">In the next sections, we are going to see some examples of how to build efficient, robust prompts leveraging ChatGPT.</p>
<h1 class="heading-1" id="_idParaDest-52">Understanding zero-, one-, and few-shot learning</h1>
<p class="normal">In the previous chapters, we mentioned how LLMs typically come in a pre-trained format. They have been trained on a huge amount of data and have had their (billions of) parameters configured accordingly.</p>
<p class="normal">However, this doesn’t mean that those LLMs can’t learn anymore. In <em class="italic">Chapter 2</em>, we learned the concept of fine-tuning. In the <em class="italic">Appendix</em>, too, we will see that one way to customize an OpenAI model and make it more capable of addressing specific tasks is <a id="_idIndexMarker143"/>by <strong class="keyWord">fine-tuning</strong>.</p>
<p class="normal">Fine-tuning is a proper training process that requires a training dataset, compute power, and some training time (depending on the amount of data and compute instances).</p>
<p class="normal">That is why it is worth testing another method for our LLMs to become more skilled in specific <a id="_idIndexMarker144"/>tasks: <strong class="keyWord">shot learning.</strong></p>
<div><p class="normal"><strong class="keyWord">Definition</strong></p>
<p class="normal">In the context of LLMs, <strong class="keyWord">shot learning</strong> refers to<a id="_idIndexMarker145"/> the model’s ability to perform tasks with varying amounts of task-specific examples provided during inference. These shot-learning paradigms enable LLMs to adapt to new tasks with minimal to no additional training, enhancing their versatility and efficiency in natural language processing applications.</p>
</div>
<p class="normal">The idea is to let the model learn from simple examples rather than the entire dataset. Those examples are samples of the way we would like the model to respond so that the model not only learns the content but also the format, style, and taxonomy to use in its response.</p>
<p class="normal">Furthermore, shot learning occurs directly via the prompt (as we will see in the following scenarios), so the whole experience is less time-consuming and easier to perform.</p>
<p class="normal">The number of examples provided determines the level of shot learning we are referring to. In other words, we refer to zero-shot if no example is provided, one-shot if one example is provided, and few-shot if more than two examples are provided.</p>
<p class="normal">Let’s focus on each of those scenarios.</p>
<h2 class="heading-2" id="_idParaDest-53">Zero-shot learning</h2>
<p class="normal">In this <a id="_idIndexMarker146"/>type of learning, the model is asked to perform a task for which it <a id="_idIndexMarker147"/>has not seen any training examples. The model must rely on prior knowledge or general information about the task to complete it. For example, a zero-shot-learning approach could be that of asking the model to generate a description, as defined in my prompt:</p>
<figure class="mediaobject"><img alt="" src="img/B31559_03_06.png"/></figure>
<p class="packt_figref">Figure 3.4: Example of zero-shot learning</p>
<h2 class="heading-2" id="_idParaDest-54">One-shot learning</h2>
<p class="normal">In this type of<a id="_idIndexMarker148"/> learning, the model is given a single example of each new task it is asked to <a id="_idIndexMarker149"/>perform. The model must use its prior knowledge to generalize from this single example to perform the task. If we consider the preceding example, I could provide my model with a prompt-completion example before asking it to generate a new one:</p>
<figure class="mediaobject"><img alt="" src="img/B31559_03_07.png"/></figure>
<p class="packt_figref">Figure 3.5: Example of one-shot learning</p>
<p class="normal">As you can see<a id="_idIndexMarker150"/> from the previous screenshot, the model was able to generate an answer that mirrors<a id="_idIndexMarker151"/> the style and template of the example provided. The same reasoning applies when we provide multiple examples, as described in the next section.</p>
<h2 class="heading-2" id="_idParaDest-55">Few-shot learning</h2>
<p class="normal">In this type of learning, the<a id="_idIndexMarker152"/> model is given a small number of <a id="_idIndexMarker153"/>examples (typically between 2 and 5) of each new task it is asked to perform. The model must use its prior knowledge to generalize from these examples to perform the task. Let’s continue with our example and provide the model with further examples:</p>
<figure class="mediaobject"><img alt="" src="img/B31559_03_08.png"/></figure>
<p class="packt_figref">Figure 3.6: Example of few-shot learning with three examples</p>
<p class="normal">As mentioned <a id="_idIndexMarker154"/>previously, it is important to remember that these forms of learning are different from traditional supervised learning, as well as fine-tuning. In few-shot learning, the goal is to enable the model to learn from very few examples, and to generalize from those examples to new tasks. Plus, we are not modifying the architecture and knowledge of the model itself, meaning that the<a id="_idIndexMarker155"/> moment the user starts a new conversation, and the previous prompt is out of the context window, the model will “forget” about it.</p>
<div><p class="normal"><strong class="keyWord">Definition</strong></p>
<p class="normal">Supervised learning<a id="_idIndexMarker156"/> is a type of machine learning where a model is trained on a labeled dataset, meaning the input data is paired with corresponding correct outputs (labels). The goal is for the model to learn the relationship between inputs and outputs so it can accurately predict the output for new, unseen data.</p>
</div>
<p class="normal">Now that we’ve learned how to let OpenAI models learn from examples, let’s focus on how to properly define our prompt to make the model’s response as accurate as possible.</p>
<h1 class="heading-1" id="_idParaDest-56">Principles of prompt engineering</h1>
<p class="normal">Traditionally, in the context of <a id="_idIndexMarker157"/>computing and data processing, the expression <em class="italic">garbage in, garbage out </em>has been used, meaning that the quality of output is determined by the quality of the input. If incorrect or poor-quality input (garbage) is entered into a system, the output will also be flawed or nonsensical (garbage).</p>
<p class="normal">When it comes to prompting, the story is similar: if we want accurate and relevant results from our LLMs, we need to provide high-quality input. However, building good prompts is not just about the quality of the response. In fact, we can construct good prompts to:</p>
<ul>
<li class="bulletList">Maximize the relevancy of an LLM’s responses.</li>
<li class="bulletList">Specify the type formatting and style of responses.</li>
<li class="bulletList">Provide conversational context.</li>
<li class="bulletList">Reduce inner LLMs’ biases and improve fairness and inclusivity.</li>
<li class="bulletList">Reduce hallucination.<div><p class="normal"><strong class="keyWord">Definition</strong></p>
<p class="normal">In the context of LLMs, <strong class="keyWord">hallucination</strong> refers <a id="_idIndexMarker158"/>to the generation of text or responses that are factually incorrect, nonsensical, or not grounded in the training data. This occurs when an LLM produces confident-sounding but erroneous or fabricated information. For example, a user could ask an LLM: <em class="italic">“Who is the author of the book Invisible Cities?”</em> If the model responds with something like: <em class="italic">“Invisible Cities was written by Gabriel García Márquez.”,</em> this is a hallucination because the correct author is <em class="italic">Italo Calvino</em>. The model generated an answer that sounds plausible but is factually incorrect.</p>
</div>
</li>
</ul>
<p class="normal">Let’s see some basic prompt engineering techniques in the following sections to achieve these results.</p>
<h2 class="heading-2" id="_idParaDest-57">Clear instructions</h2>
<p class="normal">The principle of giving<a id="_idIndexMarker159"/> clear instructions is to provide the model with enough information and guidance to perform the task correctly and efficiently. Clear instructions should include the following elements:</p>
<ul>
<li class="bulletList">The goal or objective of the task, such as “write a poem” or “summarize an article.”</li>
<li class="bulletList">The format or structure of the expected output, such as “use four lines with rhyming words” or “use bullet points with no more than 10 words each.”</li>
<li class="bulletList">The constraints or limitations of the task, such as “do not use any profanity” or “do not copy any text from the source.”</li>
<li class="bulletList">The context or background of the task, such as “the poem is about autumn” or “the article is from a scientific journal.”</li>
</ul>
<p class="normal">Let’s say, for example, that we want our model to fetch any kind of instructions from text and return to us a tutorial in a bullet list. If there are no instructions in the provided text, the model should inform us about that. Let’s see an example in ChatGPT:</p>
<figure class="mediaobject"><img alt="" src="img/B31559_03_09.png"/></figure>
<p class="packt_figref">Figure 3.7: Example of clear instructions in ChatGPT</p>
<p class="normal">Note that, if we pass the <a id="_idIndexMarker160"/>model other text that does not contain any instructions, it will be able to respond as we instructed it:</p>
<figure class="mediaobject"><img alt="" src="img/B31559_03_10.png"/></figure>
<p class="packt_figref">Figure 3.8: Example of chat model following instructions</p>
<div><p class="normal"><strong class="keyWord">Note</strong></p>
<p class="normal">In the previous figure, we saw ChatGPT keeping in mind the instructions we prompted it with at the beginning of the conversation. This happens because ChatGPT has a so-called context window, which is equal to a single chat: everything we input in the chat session will be part of ChatGPT’s context and henceforth part of its knowledge; the moment we start a new session from scratch, ChatGPT will not remember any previous instructions.</p>
</div>
<p class="normal">By giving clear instructions, you <a id="_idIndexMarker161"/>can help the model understand what you want it to do and how you want it to do it. This can improve the quality and relevance of the model’s output, and reduce the need for further revisions or corrections.</p>
<p class="normal">However, sometimes there are scenarios where clarity is not enough. We might need to infer the way of thinking of our LLM to make it more robust with respect to its task. In the next subsection, we are going to examine a technique to do this – one that is very useful in cases of solving complex tasks.</p>
<h2 class="heading-2" id="_idParaDest-58">Split complex tasks into subtasks</h2>
<p class="normal">When we interact with <a id="_idIndexMarker162"/>LLMs to let them solve some tasks, sometimes those tasks are too complex or ambiguous for a single prompt to handle, and it is better to split them into simpler subtasks that can be solved by different prompts.</p>
<p class="normal">Here are some examples of splitting complex tasks into subtasks:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Text summarization</strong>: A <a id="_idIndexMarker163"/>complex task that involves generating a concise and accurate summary of a long text. This task can be split into subtasks such as:<ul>
<li class="bulletList">Extracting the main points or keywords from the text.</li>
<li class="bulletList">Rewriting the main points or keywords in a coherent way.</li>
<li class="bulletList">Trimming the<a id="_idIndexMarker164"/> summary to fit a desired length or format.</li>
</ul>
</li>
<li class="bulletList"><strong class="keyWord">Poem generation</strong>: A <a id="_idIndexMarker165"/>creative task that involves producing a poem that follows a certain style, theme, or mood. This<a id="_idIndexMarker166"/> task can be split into subtasks such as:<ul>
<li class="bulletList">Choosing a poetic form (such as sonnet, haiku, limerick, etc.) and a rhyme scheme (such as ABAB, AABB, ABCB, etc.) for the poem.</li>
<li class="bulletList">Generating a title and a topic for the poem based on the user’s input or preference.</li>
<li class="bulletList">Generating the lines or verses of the poem that match the chosen form, rhyme scheme, and topic.</li>
<li class="bulletList">Refining and polishing the poem to ensure coherence, fluency, and originality.</li>
</ul>
</li>
<li class="bulletList"><strong class="keyWord">Code generation</strong>: A technical<a id="_idIndexMarker167"/> task that involves producing working code for a video game. This task can be split into subtasks such as:<ul>
<li class="bulletList">Create basic movements and integrate their logic into the game engine’s loop.</li>
<li class="bulletList">Add advanced movement features like printing or jumping logic with gravity.</li>
<li class="bulletList">Ensure physics and collision handling are enabled.</li>
<li class="bulletList">Enable debugging and optimization by generating testing procedures.</li>
<li class="bulletList">Generate documentation for future reference.</li>
</ul>
</li>
</ul>
<p class="normal">Let’s consider the following example. We will provide the model with a short article and ask it to summarize it following these instructions:</p>
<ul>
<li class="bulletList">You are an AI assistant that summarizes articles.</li>
<li class="bulletList">To complete this task, do the following subtasks:<ul>
<li class="bulletList">Read the provided article context comprehensively and identify the main topic and key points.</li>
<li class="bulletList">Generate a paragraph summary of the current article context that captures the essential information and conveys the main idea.</li>
<li class="bulletList">Print each step of the process.</li>
</ul>
</li>
</ul>
<p class="normal">This is the short article we will provide:</p>
<pre class="programlisting code"><code class="hljs-code">Large Language Models (LLMs), a subset of artificial intelligence, have revolutionized the field of natural language processing by demonstrating an unprecedented ability to understand and generate human-like text. These models are trained on vast datasets comprising diverse linguistic inputs, enabling them to produce coherent and contextually relevant responses across a wide range of topics. By leveraging architectures such as transformers, LLMs like GPT-3 and its successors can complete text, answer questions, perform translations, and even engage in complex dialogue. Their applications span from automated customer support and content creation to advanced research and education tools. Despite their incredible capabilities, LLMs also pose challenges, including the potential for biases inherent in training data and the risk of generating misleading or false information. As the development of LLMs continues to advance, ongoing efforts in ethical AI research and deployment strategies are crucial to harness their benefits responsibly and effectively.
</code></pre>
<p class="normal">Let’s see <a id="_idIndexMarker168"/>how the model works:</p>
<figure class="mediaobject"><img alt="" src="img/B31559_03_11.png"/></figure>
<figure class="mediaobject">Figure 3.9: Example of OpenAI GPT-4o splitting a task into subtasks to generate a summary</figure>
<p class="normal">Splitting <a id="_idIndexMarker169"/>complex tasks into easier sub tasks is a powerful technique. Nevertheless, it does not address one of the main risks of LLM-generated content, that is, having an incorrect output. In the next two subsections, we are going to see some techniques that are mainly aimed at addressing this risk.</p>
<h2 class="heading-2" id="_idParaDest-59">Ask for justification</h2>
<p class="normal">In prompt <a id="_idIndexMarker170"/>engineering, requesting that a model provides justifications for its responses enhances transparency and reliability. This practice allows users to assess the reasoning behind the model’s answers, ensuring they are logical and grounded in relevant information (<a href="https://arxiv.org/abs/2303.08769">https://arxiv.org/abs/2303.08769</a>). By understanding the model’s thought process, users can identify potential biases or inaccuracies, leading to more informed decisions and effective utilization of AI systems.</p>
<p class="normal">For instance, when an AI model suggests a medical diagnosis, asking for its reasoning can reveal whether the suggestion is based on pertinent symptoms and medical history or if it’s influenced by irrelevant data. Similarly, in legal contexts, if an AI system provides case recommendations, understanding its justification helps ensure the advice is based on appropriate legal precedents. This level of insight is crucial for building trust in AI applications and for <a id="_idIndexMarker171"/>refining prompts to elicit more accurate and contextually appropriate responses.</p>
<p class="normal">Let’s consider the following example. We want our LLM to solve riddles and we prompt it with the following set of instructions:</p>
<figure class="mediaobject"><img alt="" src="img/B31559_03_12.png"/></figure>
<p class="packt_figref">Figure 3.10: Example of OpenAI’s GPT-4o providing justification after solving a riddle</p>
<p class="normal">With a similar<a id="_idIndexMarker172"/> approach, we could also intervene at different prompt levels to improve our LLM’s performance. For example, we might discover that the model is systematically tackling a mathematical problem in the wrong way, hence we might want to suggest the right approach directly at the meta-prompt level.</p>
<p class="normal">Another example might be that of asking the model to generate multiple outputs – along with their justifications – to evaluate different reasoning techniques and prompt the best one in the meta-prompt. We’ll focus on this in the next subsection.</p>
<h2 class="heading-2" id="_idParaDest-60">Generate many outputs, then use the model to pick the best one</h2>
<p class="normal">In prompt<a id="_idIndexMarker173"/> engineering, instructing a model to generate multiple responses to a single prompt is a technique known as self-consistency. This approach involves directing the model to produce several outputs for a given input, which are then evaluated to identify the most consistent or accurate response. By comparing these multiple outputs, users can discern common themes or solutions, enhancing the reliability of the LLM’s performance.</p>
<p class="normal">Let’s see an example, following up with the riddles examined in the previous section:</p>
<ul>
<li class="bulletList">You are an AI assistant specialized in solving riddles.</li>
<li class="bulletList">Given a riddle, you have to generate three answers to the riddle.</li>
<li class="bulletList">For each answer, be specific about the reasoning you made.</li>
<li class="bulletList">Then, among the three answers, select the one which is most plausible given the riddle.</li>
</ul>
<p class="normal">In this case, I’ve prompted the model to generate three answers to the riddle, then to give me the most likely, justifying why. Let’s see the result:</p>
<figure class="mediaobject"><img alt="" src="img/B31559_03_13.png"/></figure>
<p class="packt_figref">Figure 3.11: Example of GPT-4o generating three plausible answers and picking the most likely one, providing justification</p>
<p class="normal">As previously mentioned, forcing<a id="_idIndexMarker174"/> the model to tackle a problem with different approaches is a way to collect multiple samples of reasonings, which might serve as further instructions in the meta-prompt. For example, if we want the model to always propose something that is not the most straightforward solution to a problem – in other words, if we want it to “think differently” – we might force it to solve a problem in N ways and then use the most creative reasoning as the <a id="_idIndexMarker175"/>framework in the meta-prompt.</p>
<p class="normal">The last element we are going to examine is the overall structure we want to give to our meta-prompt.</p>
<h2 class="heading-2" id="_idParaDest-61">Use delimiters</h2>
<p class="normal">The <a id="_idIndexMarker176"/>last principle to be covered is related to the format we want to give to our meta prompt. This helps our LLM to better understand its intents as well as to make connections among sections and paragraphs.</p>
<p class="normal">To achieve this, we can use delimiters within our prompt. A delimiter can be any sequence of characters or symbols that is clearly mapping a schema rather than a concept. For example, we can consider the following sequence delimiters:</p>
<ul>
<li class="bulletList"><code class="inlineCode">&gt;&gt;&gt;&gt;</code></li>
<li class="bulletList"><code class="inlineCode">====</code></li>
<li class="bulletList"><code class="inlineCode">------</code></li>
<li class="bulletList"><code class="inlineCode">####</code></li>
<li class="bulletList"><code class="inlineCode">` ` ` ` `</code></li>
</ul>
<p class="normal">Let’s consider, for example, a meta-prompt that aims at instructing the model to translate a user’s tasks into Python code, also providing an example of doing so:</p>
<pre class="programlisting code"><code class="hljs-code">You are a Python expert that produces Python code as per the user's request.
===&gt;START EXAMPLE
---User Query---
Give me a function to print a string of text.
---User Output---
Below you can find the described function:
```def my_print(text):
     #returning the printed text
     return print(text)
```
&lt;===END EXAMPLE
</code></pre>
<p class="normal">Let’s see how it works:</p>
<figure class="mediaobject"><img alt="" src="img/B31559_03_14.png"/></figure>
<p class="packt_figref">Figure 3.12: Sample output of a model using delimiters in the system message</p>
<p class="normal">As you can see, it <a id="_idIndexMarker177"/>also printed the code in backticks as shown within the system message.</p>
<p class="normal">All the principles examined up to this point are general rules that can make your interaction with ChatGPT and, more broadly, GenAI tools more meaningful to your goal. In the next section, we are going to see some advanced techniques for prompt engineering that address the way the model reasons <a id="_idIndexMarker178"/>and thinks about the answer, before providing it to the final user.</p>
<h2 class="heading-2" id="_idParaDest-62">Meta-prompting</h2>
<p class="normal">In <a id="_idIndexMarker179"/>prompt engineering, instructing a model to refine its own prompts – also <a id="_idIndexMarker180"/>known as meta-prompting (<a href="https://arxiv.org/abs/2401.12954">https://arxiv.org/abs/2401.12954</a>) – is an effective technique to enhance prompt quality and, consequently, the relevance of generated outputs. By engaging the model in the iterative process of prompt refinement, users can leverage the model’s language understanding to identify ambiguities or areas for improvement within the initial prompt. This self-improvement loop leads to more precise and contextually appropriate prompts, which in turn elicit more accurate and useful responses from the model.</p>
<p class="normal">For instance, let’s say we want to generate an elevator pitch for our new sustainable brand of running shoes. How would you ask the LLM to do that? Well, you might leverage some of the above techniques, like clear instructions or splitting the task into sub tasks; alternatively (or additionally), you could ask the LLM itself to refine your prompt to make it more relevant to your goal.</p>
<p class="normal">To do that, we can initially instruct the model to refine the prompt as follows:</p>
<figure class="mediaobject"><img alt="" src="img/B31559_03_15.png"/></figure>
<p class="packt_figref">Figure 3.13: Example of a user asking ChatGPT to refine a prompt</p>
<p class="normal">Now, let’s send our prompt:</p>
<figure class="mediaobject"><img alt="" src="img/B31559_03_16.png"/></figure>
<p class="packt_figref">Figure 3.14: Example of ChatGPT refining the user’s prompt</p>
<p class="normal">As you can see, ChatGPT<a id="_idIndexMarker181"/> was able to refine our prompt and <a id="_idIndexMarker182"/>make it more tailored to our goal. Note that, in the above example, we only asked for one refinement; however, this can be an iterative process to not only enhance the clarity and precision of the prompt but also ensure that the model’s outputs are more aligned with the user’s specific requirements, making interactions more efficient and productive.</p>
<h1 class="heading-1" id="_idParaDest-63">Exploring some advanced techniques</h1>
<p class="normal">In previous sections, we covered some basic techniques of prompt engineering that can improve your LLM’s response regardless of the type of task you are trying to accomplish.</p>
<p class="normal">On the other hand, there are some advanced techniques that might be implemented for specific scenarios that we are going to cover in this section.</p>
<div><p class="normal"><strong class="keyWord">Note</strong></p>
<p class="normal">Some advanced prompt engineering techniques like <strong class="keyWord">chain-of-thought</strong> (<strong class="keyWord">CoT</strong>) prompting are integrated into modern models such as OpenAI’s o1 series. These models are designed to internally process complex reasoning tasks by generating step-by-step logical sequences before arriving at a final answer, enhancing their problem-solving capabilities. This internal reasoning process allows o1 models to handle intricate queries more effectively without requiring explicit CoT prompts from users. However, employing CoT prompting can still be beneficial in guiding the model’s reasoning process for specific tasks and, more broadly, is a good practice whenever we interact with models of previous versions that do not exhibit advanced reasoning capabilities.</p>
</div>
<h2 class="heading-2" id="_idParaDest-64">Chain of thought</h2>
<p class="normal">Introduced in the paper <em class="italic">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</em> by Wei et al., CoT is a <a id="_idIndexMarker183"/>technique that enables complex reasoning capabilities through intermediate reasoning steps. It also encourages the model to explain its reasoning, “forcing” it not to be too fast and risk giving the wrong response (as we saw in previous sections).</p>
<p class="normal">Let’s say that we want to prompt our LLM to solve first-degree equations. To do so, we are going to provide it with a generic reasoning list as a meta-prompt:</p>
<pre class="programlisting code"><code class="hljs-code">To solve a generic first-degree equation, follow these steps:
1. **Identify the Equation:** Start by identifying the equation you want to solve. It should be in the form of "ax + b = c," where 'a' is the coefficient of the variable, 'x' is the variable, 'b' is a constant, and 'c' is another constant.
2. **Isolate the Variable:** Your goal is to isolate the variable 'x' on one side of the equation. To do this, perform the following steps:
 
   a. **Add or Subtract Constants:** Add or subtract 'b' from both sides of the equation to move constants to one side.
 
   b. **Divide by the Coefficient:** Divide both sides by 'a' to isolate 'x'. If 'a' is zero, the equation may not have a unique solution.
3. **Simplify:** Simplify both sides of the equation as much as possible.
4. **Solve for 'x':** Once 'x' is isolated on one side, you have the solution. It will be in the form of 'x = value.'
5. **Check Your Solution:** Plug the found value of 'x' back into the original equation to ensure it satisfies the equation. If it does, you've found the correct solution.
6. **Express the Solution:** Write down the solution in a clear and concise form.
7. **Consider Special Cases:** Be aware of special cases where there may be no solution or infinitely many solutions, especially if 'a' equals zero.
Equation:
</code></pre>
<p class="normal">Let’s see how it<a id="_idIndexMarker184"/> works:</p>
<figure class="mediaobject"><img alt="" src="img/B31559_03_17.png"/><img alt="" src="img/B31559_03_18.png"/></figure>
<p class="packt_figref">Figure 3.15: Output of the model solving an equation with the CoT approach</p>
<p class="normal">This methodical approach<a id="_idIndexMarker185"/> mirrors human problem-solving by decomposing the task into manageable steps, enhancing clarity and reducing errors.</p>
<p class="normal">With CoT, we are prompting the model to generate intermediate reasoning steps. This is also a component of another reasoning technique that we are going to examine next.</p>
<h2 class="heading-2" id="_idParaDest-65">ReAct</h2>
<p class="normal">Introduced in the paper <em class="italic">ReAct: Synergizing Reasoning and Acting in Language Models </em>by Yao et al., <strong class="keyWord">Reason and Act</strong> (<strong class="keyWord">ReAct</strong>) is a <a id="_idIndexMarker186"/>general paradigm that combines reasoning and acting with LLMs. ReAct prompts the language model to generate verbal reasoning traces and actions for a task, and also receive observations from external sources such as web searches or databases. This allows the language model to perform dynamic reasoning and quickly adapt its action plan based on external information. For example, you can prompt the language model to answer a question by first reasoning about the question, then performing an action to send a query to the web, then receiving an observation from the search results, and then continuing with this thought, action, observation loop until it reaches a conclusion.</p>
<p class="normal">The difference between CoT and ReAct approaches is that CoT prompts the language model to generate intermediate reasoning steps for a task, while ReAct prompts the language model to generate intermediate reasoning steps, actions, and observations for a task.</p>
<p class="normal">Note that the “action” phase is generally related to the possibility of our LLM interacting with external tools, such as web search. However, in the following example, we won’t use tools but rather refer to the term “action” for any task we ask the model to do for us.</p>
<p class="normal">This is how the ReAct meta-prompt might look:</p>
<pre class="programlisting code"><code class="hljs-code">Answer the following questions as best you can.
Use the following format:
 ---------------
Question: the input question you must answer
Thought: you should always think about what to do
Action: the action to take
Action Input: the input to the action
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can repeat N times)
Thought: I now know the final answer
Final Answer: the final answer to the original input question
-----------------
This is my question: Who won the climbing Olympics in 2024?
</code></pre>
<p class="normal">Let’s see how it works with a simple user query:</p>
<figure class="mediaobject"><img alt="" src="img/B31559_03_19.png"/></figure>
<p class="packt_figref">Figure 3.16: Example of ReAct prompting</p>
<p class="normal">As you can see, in this scenario, the<a id="_idIndexMarker187"/> model leveraged the web tool at the action input.</p>
<p class="normal">This is a great example of how prompting a model to think step by step and explicitly detail each step of the reasoning makes it “wiser” and more cautious before answering. It is also a great technique to prevent hallucination.</p>
<p class="normal">Overall, prompt engineering is a powerful discipline, still in its emerging phase yet already widely adopted within LLM-powered applications. In the following chapters, we are going to see concrete applications of these techniques.</p>
<h1 class="heading-1" id="_idParaDest-66">Ethical considerations to avoid bias</h1>
<p class="normal">Whenever we deal with AI <a id="_idIndexMarker188"/>systems like LLMs, we must be aware of their associated risk of <strong class="keyWord">hidden bias</strong>, which derives directly from the knowledge base the model has been trained on.</p>
<div><p class="normal"><strong class="keyWord">Definition</strong></p>
<p class="normal">Hidden bias, also <a id="_idIndexMarker189"/>known as implicit or unconscious bias, refers to the subtle and unintentional attitudes, stereotypes, or associations that influence a person’s perceptions and actions without their conscious awareness. These biases can shape behaviors and decisions in ways that reflect societal stereotypes, often leading to unintended discrimination. For example, someone might unknowingly associate leadership roles with men over women, which could impact hiring or promotion choices. In the context of LLM, hidden bias manifests in the model’s outputs when it reproduces or amplifies biases present in its training data, potentially leading to skewed or unfair responses. Addressing hidden bias is essential to fostering fairness and reducing systemic inequities.</p>
</div>
<p class="normal">For example, concerning the main chunk of training <a id="_idIndexMarker190"/>data of GPT-3, known as the <strong class="keyWord">Common Crawl</strong>, a 2012 study (<a href="https://commoncrawl.org/blog/a-look-inside-common-crawls-210tb-2012-web-corpus">https://commoncrawl.org/blog/a-look-inside-common-crawls-210tb-2012-web-corpus</a>) revealed that over 55% of the corpus originated from .<em class="italic">com</em> domains, with twelve top-level domains each representing more than 1% of the data.</p>
<p class="normal">Given that <em class="italic">.com</em> domains are heavily utilized by Western entities, this concentration suggests a significant Western influence in the dataset. Additionally, the prevalence of English-language content within Common Crawl further indicates a Western-centric bias, as English is predominantly spoken in Western nations.</p>
<p class="normal">If this is the case, we are already facing a hidden bias of the model (more specifically, a racial and linguistic bias), which will inevitably mimic a limited and unrepresentative category of human beings.</p>
<p class="normal">In their paper <em class="italic">Language Models are Few-Shots Learners</em> (<a href="https://arxiv.org/abs/2005.14165">https://arxiv.org/abs/2005.14165</a>), OpenAI’s researchers Tom Brown et al. created an experimental setup to investigate racial bias in GPT-3. The model was prompted with phrases containing racial categories and 800 samples were generated for each category. The sentiment of the generated text was measured using Senti WordNet based on word co-occurrences on a scale ranging from -100 to 100 (with positive scores indicating positive words and vice versa).</p>
<p class="normal">The results showed that the sentiment associated with each racial category varied across different models, with <em class="italic">Asian </em>consistently having a high sentiment (meaning a lot of positive words) and <em class="italic">Black </em>consistently having a low sentiment (meaning a lot of negative words). The authors caution that the results reflect the experimental setup and that socio-historical factors may influence the sentiment associated with different demographics.</p>
<p class="normal">This hidden bias could generate harmful responses not in line with responsible AI principles.</p>
<p class="normal">However, it is worth noting how ChatGPT, as well as all OpenAI models, are subject to continuous improvements. This is also consistent with OpenAI’s AI alignment (<a href="https://openai.com/index/our-approach-to-alignment-research/">https://openai.com/index/our-approach-to-alignment-research/</a>), whose research focuses on training AI systems to <a id="_idIndexMarker191"/>be helpful, truthful, and safe.</p>
<p class="normal">For example, if we ask GPT-4o to formulate guesses based on people’s gender and age, it will not accommodate the exact request, but rather provide us with a hypothetical function as well as a huge disclaimer:</p>
<figure class="mediaobject"><img alt="" src="img/B31559_03_20.png"/></figure>
<p class="packt_figref">Figure 3.17: Example of GPT-4o improving over time since it gives an unbiased response</p>
<p class="normal">Overall, despite the continuous<a id="_idIndexMarker192"/> improvement in the domain of ethical principles, while using ChatGPT, we should always make sure that the output is in line with those principles. The concepts of bias and ethics within ChatGPT and OpenAI models have a wider connotation within the whole topic of responsible AI, which we are going to focus on in the last chapter of this book.</p>
<h1 class="heading-1" id="_idParaDest-67">Summary</h1>
<p class="normal">In this chapter, we have dived into the concept of prompt engineering since it’s a key component to control the output of ChatGPT and LLMs in general. We learned how to leverage different levels of shot learning to make LLMs more tailored to our objectives.</p>
<p class="normal">We started with an introduction to the concept of prompt engineering and why it is important, then moving toward the basic principles – including clear instructions, asking for justification, etc.</p>
<p class="normal">Then, we moved toward more advanced techniques, which are meant to shape the reasoning approach of our LLMs: few-shot learning, CoT, and ReAct.</p>
<p class="normal">Prompt engineering is an emerging discipline that is paving the way for a new category of applications, infused with LLMs.</p>
<p class="normal">Starting from the next chapter, we will explore different domains where ChatGPT can boost productivity and have a disruptive impact on the way we work today.</p>
<h1 class="heading-1" id="_idParaDest-68">References</h1>
<p class="normal">The following are the references for this chapter:</p>
<ul>
<li class="bulletList"><em class="italic">Language Models are Few-Shot Learners</em>: <a href="https://arxiv.org/abs/2005.14165">https://arxiv.org/abs/2005.14165</a></li>
<li class="bulletList"><em class="italic">On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?</em>: <a href="https://dl.acm.org/doi/10.1145/3442188.3445922">https://dl.acm.org/doi/10.1145/3442188.3445922</a></li>
<li class="bulletList">ReAct approach: <a href="https://arxiv.org/abs/2210.03629">https://arxiv.org/abs/2210.03629</a></li>
<li class="bulletList">Chain-of-thought approach: <a href="https://arxiv.org/abs/2201.11903">https://arxiv.org/abs/2201.11903</a></li>
<li class="bulletList"><em class="italic">What is prompt engineering?</em>: <a href="https://www.mckinsey.com/featured-insights/mckinsey-explainers/what-is-prompt-engineering">https://www.mckinsey.com/featured-insights/mckinsey-explainers/what-is-prompt-engineering</a></li>
<li class="bulletList">Prompt engineering principles: <a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/advanced-prompt-engineering?pivots=programming-language-chat-completions">https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/advanced-prompt-engineering?pivots=programming-language-chat-completions</a></li>
</ul>
</div>
</body></html>