<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div id="_idContainer025">
			<h1 id="_idParaDest-130" class="chapter-number"><a id="_idTextAnchor162"/>10</h1>
			<h1 id="_idParaDest-131"><a id="_idTextAnchor163"/>Checkpointing and Recovery</h1>
			<p><strong class="bold">Checkpointing</strong> and <strong class="bold">recovery</strong> refer to the process of saving the state of a system, application, or model at specific intervals (checkpointing) and restoring it from a saved state in case of failure (recovery). In machine learning, checkpointing involves periodically saving model parameters, optimizer states, and training progress so that training can resume from the last checkpoint instead of starting over. This is especially useful for long-running tasks, where interruptions due to system crashes, power failures, or preempted cloud instances can otherwise result in <span class="No-Break">significant losses.</span></p>
			<p>Checkpointing and recovery are crucial for ensuring <strong class="bold">fault tolerance</strong>, <strong class="bold">efficiency</strong>, and <strong class="bold">reproducibility</strong> in training large-scale models. Without checkpointing, an unexpected failure could waste hours or even days of computation. Additionally, it allows for <strong class="bold">experiment reproducibility</strong>, enabling researchers to revisit and fine-tune models from intermediate states, rather than redoing entire training runs. Efficient checkpointing strategies (e.g., saving at fixed intervals or when validation performance improves) help balance storage overhead while minimizing <span class="No-Break">retraining costs.</span></p>
			<p>In this chapter, we’ll explore strategies for determining optimal checkpoint frequency, efficient storage formats for large models, and techniques for recovering from various types of failures. You’ll also gain insights into checkpointing in distributed training scenarios and version control for <span class="No-Break">model checkpoints.</span></p>
			<p>In this chapter, we’ll be covering the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Why is <span class="No-Break">checkpointing important?</span></li>
				<li>Checkpoint frequency and <span class="No-Break">storage strategies</span></li>
				<li>Efficient <span class="No-Break">checkpoint formats</span></li>
				<li>Recovering <span class="No-Break">from failures</span></li>
				<li>Checkpointing in distributed <span class="No-Break">LLM training</span></li>
				<li>Version control for <span class="No-Break">LLM checkpoints</span></li>
				<li>Automated checkpointing and <span class="No-Break">recovery systems</span></li>
			</ul>
			<h1 id="_idParaDest-132"><a id="_idTextAnchor164"/>Why is checkpointing important?</h1>
			<p>Checkpointing is a common <a id="_idIndexMarker512"/>practice in LLM training due to the long duration and resource-intensive nature of the LLM <span class="No-Break">training process.</span></p>
			<p>Let’s implement a basic <span class="No-Break">checkpointing system:</span></p>
			<pre class="source-code">
import torch
from transformers import GPT2LMHeadModel, GPT2Config
import os
class LLMTrainer:
    def __init__(
        self, model, optimizer, checkpoint_dir='checkpoints'
    ):
        self.model = model
        self.optimizer = optimizer
        self.checkpoint_dir = checkpoint_dir
        os.makedirs(checkpoint_dir, exist_ok=True)
    def save_checkpoint(self, epoch, step, loss):
        checkpoint = {
            'epoch': epoch,
            'step': step,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'loss': loss
        }
        checkpoint_path = os.path.join(self.checkpoint_dir,
            f'checkpoint_epoch_{epoch}_step_{step}.pt')
        torch.save(checkpoint, checkpoint_path)
        print(f"Checkpoint saved: {checkpoint_path}")
    def load_checkpoint(self, checkpoint_path):
        checkpoint = torch.load(checkpoint_path)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(
            checkpoint['optimizer_state_dict'])
        return (
            checkpoint['epoch'], checkpoint['step'],
            checkpoint['loss']
        )
# Simulating training loop
for epoch in range(10):
    for step in range(1000):
        # ... training code ...
        if step % 100 == 0:
            trainer.save_checkpoint(epoch, step, loss.item())
# Loading a checkpoint
epoch, step, loss = trainer.load_checkpoint(
    'checkpoints/checkpoint_epoch_5_step_500.pt')
print(f"Resumed training from epoch {epoch}, step {step}, with loss {loss}")</pre>			<p>This implementation demonstrates the basic structure of a checkpointing system. The <strong class="source-inline">save_checkpoint</strong> method <a id="_idIndexMarker513"/>saves the model state, optimizer state, and training progress information. The <strong class="source-inline">load_checkpoint</strong> method allows you to resume training from a <span class="No-Break">saved checkpoint.</span></p>
			<h1 id="_idParaDest-133"><a id="_idTextAnchor165"/>Checkpoint frequency and storage strategies</h1>
			<p>Determining the optimal checkpoint frequency<a id="_idIndexMarker514"/> involves striking a balance between <em class="italic">safety</em> and <em class="italic">efficiency</em>. Let’s explore different strategies and <span class="No-Break">their implementation:</span></p>
			<pre class="source-code">
import time
import shutil
class AdvancedLLMTrainer(LLMTrainer):
    def __init__(
        self, model, optimizer, checkpoint_dir='checkpoints',
        max_checkpoints=5
    ):
        super().__init__(model, optimizer, checkpoint_dir)
        self.max_checkpoints = max_checkpoints
        self.checkpoints = []
    def save_checkpoint(self, epoch, step, loss):
        checkpoint_path = super().save_checkpoint(epoch, step, loss)
        self.checkpoints.append(checkpoint_path)
        if len(self.checkpoints) &gt; self.max_checkpoints:
            oldest_checkpoint = self.checkpoints.pop(0)
            os.remove(oldest_checkpoint)
            print(f"Removed old checkpoint: {oldest_checkpoint}")
    def save_checkpoint_by_time(
        self, epoch, step, loss, interval_minutes=60
    ):
        current_time = time.time()
        if (
            not hasattr(self, 'last_checkpoint_time') or
            current_time - self.last_checkpoint_time &gt;= 
            interval_minutes * 60
        ):
            self.save_checkpoint(epoch, step, loss)
            self.last_checkpoint_time = current_time
    def save_best_checkpoint(self, epoch, step, loss):
        if not hasattr(self, 'best_loss') or loss &lt; self.best_loss:
            self.best_loss = loss
            checkpoint_path = os.path.join(
                self.checkpoint_dir, 'best_model.pt')
            torch.save({
                'epoch': epoch,
                'step': step,
                'model_state_dict': self.model.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'loss': loss
            }, checkpoint_path)
            print(f"Best model saved: {checkpoint_path}")
# Usage example
trainer = AdvancedLLMTrainer(model, optimizer)
for epoch in range(10):
    for step in range(1000):
        # ... training code ...
        trainer.save_checkpoint_by_time(epoch, step, loss.item(),
            interval_minutes=30)
        trainer.save_best_checkpoint(epoch, step, loss.item())</pre>			<p>This implementation introduces several <a id="_idIndexMarker515"/><span class="No-Break">checkpointing strategies:</span></p>
			<ul>
				<li><strong class="bold">Regular checkpointing with a maximum number of checkpoints</strong>: This prevents excessive disk usage by removing old checkpoints when the limit <span class="No-Break">is reached</span></li>
				<li><strong class="bold">Time-based checkpointing</strong>: This saves checkpoints at regular time intervals, which can be useful for long-running <span class="No-Break">training processes</span></li>
				<li><strong class="bold">Best model checkpointing</strong>: This saves the model with the best performance (lowest loss in this case), which is useful for <span class="No-Break">model selection</span></li>
			</ul>
			<p>Here’s a trade-off analysis of the three <span class="No-Break">checkpointing strategies:</span></p>
			<ul>
				<li><strong class="bold">Regular checkpointing with a maximum number </strong><span class="No-Break"><strong class="bold">of checkpoints</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">Pros</strong>: Prevents excessive storage usage and ensures periodic snapshots of <span class="No-Break">training progress</span></li><li><strong class="bold">Cons</strong>: Might overwrite useful older checkpoints, potentially losing good models if <span class="No-Break">performance fluctuates</span></li><li><strong class="bold">Best use case</strong>: When storage is a constraint and periodic snapshots are needed <span class="No-Break">for resumption</span></li></ul></li>
				<li><span class="No-Break"><strong class="bold">Time-based checkpointing</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">Pros</strong>: Ensures checkpoints <a id="_idIndexMarker516"/>are spaced out over time, which is useful for monitoring long <span class="No-Break">training runs</span></li><li><strong class="bold">Cons</strong>: Can be inefficient if checkpoints are saved too frequently (wasting storage) or too infrequently (missing <span class="No-Break">critical states)</span></li><li><strong class="bold">Best use case</strong>: For long-running training processes where consistent snapshots are needed for debugging <span class="No-Break">or rollback</span></li></ul></li>
				<li><strong class="bold">Best </strong><span class="No-Break"><strong class="bold">model checkpointing</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">Pros</strong>: Retains the most promising model, which is useful for final <span class="No-Break">model selection.</span></li><li><strong class="bold">Cons</strong>: If loss is noisy, a single “best” checkpoint may not be truly representative. Can fail to capture intermediate <span class="No-Break">learning dynamics.</span></li><li><strong class="bold">Best use case</strong>: When selecting the most performant model is the priority over <span class="No-Break">periodic snapsh<a id="_idTextAnchor166"/>ots.</span></li></ul></li>
			</ul>
			<p>Here are some factors to consider when selecting the strategy you wish <span class="No-Break">to adopt:</span></p>
			<ul>
				<li><strong class="bold">Computational cost</strong>: Frequent checkpointing increases disk I/O and <span class="No-Break">CPU overhead</span></li>
				<li><strong class="bold">Failure recovery</strong>: Regular and time-based checkpointing help resume training after interruptions, whereas best-model checkpointing may not provide <span class="No-Break">recent progress</span></li>
				<li><strong class="bold">Storage constraints</strong>: Maintaining many checkpoints consumes storage; regular checkpointing with a limit is most efficient in <span class="No-Break">managing this</span></li>
				<li><strong class="bold">Rate of model improvement</strong>: If the model improves rapidly, frequent checkpoints may be useful; if the progress is slow, fewer but more strategic checkpoints <span class="No-Break">may suffice</span></li>
			</ul>
			<p>The recommended approach<a id="_idIndexMarker517"/> for checkpointing LLMs is to <span class="No-Break">combine strategies:</span></p>
			<ul>
				<li>Use regular checkpointing (e.g., every few hours) to ensure progress <span class="No-Break">is saved</span></li>
				<li>Use best model checkpointing to retain the <span class="No-Break">best-performing model</span></li>
				<li>Use a rolling window of recent checkpoints to balance storage efficiency and <span class="No-Break">recovery options</span></li>
			</ul>
			<h1 id="_idParaDest-134"><a id="_idTextAnchor167"/>Efficient checkpoint formats</h1>
			<p>For LLMs with billions of <a id="_idIndexMarker518"/>parameters, checkpoint size can become a significant concern. Let’s explore some strategies for efficient <span class="No-Break">checkpoint storage:</span></p>
			<ol>
				<li>Import the necessary libraries and <span class="No-Break">implement </span><span class="No-Break"><strong class="source-inline">EfficientLLMTrainer</strong></span><span class="No-Break">:</span><pre class="source-code">
import torch
import io
import zipfile
class EfficientLLMTrainer(AdvancedLLMTrainer):
    def save_checkpoint_efficient(self, epoch, step, loss):
        checkpoint = {
            'epoch': epoch,
            'step': step,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'loss': loss
        }
        checkpoint_path = os.path.join(
            self.checkpoint_dir,
                f'checkpoint_epoch_{epoch}_step_{step}.zip')
        with zipfile.ZipFile(checkpoint_path,
            'w', zipfile.ZIP_DEFLATED
        ) as zipf:
            for key, value in checkpoint.items():
                if isinstance(value, dict):  # For model and optimizer state_dicts
                    buffer = io.BytesIO()
                    torch.save(value, buffer)
                    zipf.writestr(f'{key}.pt',
                    buffer.getvalue())
                else:
                    zipf.writestr(f'{key}.txt', str(value))
        print(f"Efficient checkpoint saved: {checkpoint_path}")</pre><p class="list-inset">This code <a id="_idIndexMarker519"/>defines an <strong class="source-inline">EfficientLLMTrainer</strong> class that extends <strong class="source-inline">AdvancedLLMTrainer</strong> (presumably a pre-existing class for training LLMs). The key function implemented is <strong class="source-inline">save_checkpoint_efficient</strong>, which efficiently saves model checkpoints in a compressed <span class="No-Break">ZIP format.</span></p></li>				<li>Let’s define the function (<strong class="source-inline">load_checkpoint_efficient</strong>) to load the checkpoint in<a id="_idIndexMarker520"/> <span class="No-Break">ZIP format:</span><pre class="source-code">
    def load_checkpoint_efficient(self, checkpoint_path):
        checkpoint = {}
        with zipfile.ZipFile(checkpoint_path, 'r') as zipf:
            for filename in zipf.namelist():
                if filename.endswith('.pt'):
                    with zipf.open(filename) as f:
                        key = filename[:-3]  
                        # Remove .pt extension
                        checkpoint[key] = torch.load(
                            io.BytesIO(f.read()))
                else:
                    with zipf.open(filename) as f:
                        key = filename[:-4]  
                        # Remove .txt extension
                        value = f.read().decode('utf-8')
                        checkpoint[key] = (
                            int(value) if key in
                            ['epoch', 'step'] 
                            else float(value)
                        )
        self.model.load_state_dict(
            checkpoint['model_state_dict'])
        self.optimizer.load_state_
            dict(checkpoint['optimizer_state_dict'])
        return (
            checkpoint['epoch'], checkpoint['step'],
            checkpoint['loss']
        )</pre><p class="list-inset">This function, <strong class="source-inline">load_checkpoint_efficient</strong>, is responsible for loading a previously saved <a id="_idIndexMarker521"/>checkpoint from a ZIP file and restoring the model and optimizer states. See the following <span class="No-Break">example usage.</span></p></li>				<li><span class="No-Break">Example usage:</span><pre class="source-code">
trainer = EfficientLLMTrainer(model, optimizer)
trainer.save_checkpoint_efficient(epoch, step, loss.item())
epoch, step, loss = trainer.load_checkpoint_ efficient(
    'checkpoints/checkpoint_epoch_5_step_500.zip') </pre><p class="list-inset">This implementation uses ZIP compression to reduce the size of checkpoints. It also separates the model and optimizer state dictionaries from other metadata, allowing for more efficient storage <span class="No-Break">and loading.</span></p></li>			</ol>
			<p>Other strategies for efficient checkpoint storage include <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Quantization</strong>: Reducing the precision <a id="_idIndexMarker522"/>of model weights (e.g., from float32 to float16) can significantly reduce the checkpoint size (see more about this strategy in <a href="B31249_13.xhtml#_idTextAnchor209"><span class="No-Break"><em class="italic">Chapter 13</em></span></a><span class="No-Break">)</span></li>
				<li><strong class="bold">Incremental checkpointing</strong>: Only save the changes since the last checkpoint, rather than the entire <span class="No-Break">model state</span></li>
				<li><strong class="bold">Distributed storage</strong>: In multi-GPU or multi-node setups, distribute the checkpoint across multiple <span class="No-Break">storage devices</span></li>
				<li><strong class="bold">Cloud storage</strong>: Use cloud storage solutions that offer fast I/O and <span class="No-Break">automatic compression</span></li>
			</ul>
			<p>For very large models, you might also consider more advanced techniques, such as <strong class="bold">model sharding</strong>, where different parts <a id="_idIndexMarker523"/>of the model are saved separately and can be loaded <span class="No-Break">on demand.</span></p>
			<h1 id="_idParaDest-135"><a id="_idTextAnchor168"/>Recovering from failures</h1>
			<p>Robust recovery mechanisms are <a id="_idIndexMarker524"/>crucial for LLM training. Let’s implement <a id="_idIndexMarker525"/>a system that can handle various types <span class="No-Break">of failures:</span></p>
			<pre class="source-code">
import signal
import sys
class RobustLLMTrainer(EfficientLLMTrainer):
    def __init__(
        self, model, optimizer, checkpoint_dir='checkpoints',
        autosave_interval=15
    ):
        super().__init__(model, optimizer, checkpoint_dir)
        self.autosave_interval = autosave_interval
        self.setup_signal_handlers()
    def setup_signal_handlers(self):
        signal.signal(signal.SIGINT, self.handle_interrupt)
        signal.signal(signal.SIGTERM, self.handle_interrupt)
    def handle_interrupt(self, signum, frame):
        print("Interrupted! Saving checkpoint before exiting...")
        self.save_checkpoint_efficient(self.current_epoch,
            self.current_step, self.current_loss)
        sys.exit(0)
    def train(self, epochs, steps_per_epoch, train_fn):
        try:
            start_epoch, start_step = 0, 0
            latest_checkpoint = self.get_latest_checkpoint()
            if latest_checkpoint:
                start_epoch, start_step, _ = \
                self.load_checkpoint_efficient(latest_checkpoint)
                print(
                    f"Resuming from epoch {start_epoch}, "
                    f"step {start_step}"
                )
            for epoch in range(start_epoch, epochs):
                self.current_epoch = epoch
                for step in range(start_step, steps_per_epoch):
                    self.current_step = step
                    self.current_loss = train_fn(
                        self.model, epoch, step)
                    if step % self.autosave_interval == 0:
                        self.save_checkpoint_efficient(
                            epoch, step, self.current_loss)
                start_step = 0  # Reset step counter at the start of each epoch
        except Exception as e:
            print(f"Error occurred: {e}")
            print("Saving checkpoint before exiting...")
            self.save_checkpoint_efficient(self.current_epoch,
                self.current_step, self.current_loss)
            raise
    def get_latest_checkpoint(self):
        checkpoints = sorted(os.listdir(self.checkpoint_dir))
        return (
            os.path.join(self.checkpoint_dir, checkpoints[-1])
            if checkpoints
            else None
        )
# Usage
def train_step(model, epoch, step):
    # Simulated training step
    loss = 1 / (epoch + 1 + step + 1)  # Dummy loss that decreases over time
    return loss
trainer = RobustLLMTrainer(model, optimizer)
trainer.train(epochs=10, steps_per_epoch=1000, train_fn=train_step)</pre>			<p>The <strong class="source-inline">RobustLLMTrainer</strong> class extends <strong class="source-inline">EfficientLLMTrainer</strong> to add resilience by handling interruptions (such as <strong class="source-inline">SIGINT</strong> for <em class="italic">Ctrl</em> + <em class="italic">C</em> and <strong class="source-inline">SIGTERM</strong> for termination) and saving checkpoints to prevent data loss. It initializes with a model, optimizer, checkpoint directory, and auto-save interval, then sets up signal handlers to trigger a graceful shutdown by saving progress <span class="No-Break">before exiting.</span></p>
			<p>During training, it attempts to<a id="_idIndexMarker526"/> resume from the latest checkpoint if available. It loops through epochs and steps, running <strong class="source-inline">train_fn</strong> to compute loss and periodically saving checkpoints based on <strong class="source-inline">autosave_interval</strong>. If an exception occurs, it catches the error, saves the progress, and re-raises the exception to avoid <span class="No-Break">silent failures.</span></p>
			<p>The <strong class="source-inline">get_latest_checkpoint()</strong> method retrieves the most recent checkpoint by sorting files in the checkpoint directory (though <strong class="source-inline">os</strong> is missing and should be imported). The script concludes with an example usage where a dummy loss function is defined, and training is started with <strong class="source-inline">trainer.train(epochs=10, </strong><span class="No-Break"><strong class="source-inline">steps_per_epoch=1000, train_fn=train_step)</strong></span><span class="No-Break">.</span></p>
			<p>This implementation includes several <span class="No-Break">robustness features:</span></p>
			<ul>
				<li><strong class="bold">Signal handling</strong>: The trainer catches <a id="_idIndexMarker527"/>interrupt signals (<em class="italic">Ctrl</em> + <em class="italic">C</em>) and gracefully saves a checkpoint <span class="No-Break">before exiting</span></li>
				<li><strong class="bold">Automatic resumption</strong>: The trainer automatically finds and loads the latest checkpoint when <span class="No-Break">starting training</span></li>
				<li><strong class="bold">Regular auto-saves</strong>: Checkpoints are saved at regular intervals <span class="No-Break">during training</span></li>
				<li><strong class="bold">Exception handling</strong>: If an error occurs during training, a checkpoint is saved before the exception <span class="No-Break">is re-raised</span></li>
			</ul>
			<p>These features help recover from various types <span class="No-Break">of failures:</span></p>
			<ul>
				<li><strong class="bold">System crashes or power outages</strong>: Regular auto-saves ensure that not too much progress <span class="No-Break">is lost</span></li>
				<li><strong class="bold">User interruptions</strong>: Signal handling allows for graceful exits with the <span class="No-Break">state saved</span></li>
				<li><strong class="bold">Code errors</strong>: Exception handling <a id="_idIndexMarker528"/>ensures that progress is saved even if an unexpected <span class="No-Break">error occurs</span></li>
			</ul>
			<p>For even more robust recovery, consider implementing <span class="No-Break">the</span><span class="No-Break"><a id="_idIndexMarker529"/></span><span class="No-Break"> following:</span></p>
			<ul>
				<li><strong class="bold">Checkpoint validation</strong>: Verify the integrity of checkpoints before <span class="No-Break">loading them</span></li>
				<li><strong class="bold">Multiple backup checkpoints</strong>: Keep several recent checkpoints in case the latest one <span class="No-Break">is corrupted</span></li>
				<li><strong class="bold">Distributed checkpointing</strong>: In multi-node setups, ensure that checkpoints are consistent across <span class="No-Break">all nodes</span></li>
			</ul>
			<h1 id="_idParaDest-136"><a id="_idTextAnchor169"/>Checkpointing in distributed LLM training</h1>
			<p>Distributed training introduces <a id="_idIndexMarker530"/>additional complexity <span class="No-Break">to checkpointing.</span></p>
			<p>Let’s break down the implementation of a <a id="_idIndexMarker531"/>basic distributed checkpointing system and understand <span class="No-Break">each component:</span></p>
			<ol>
				<li>We first define the <strong class="source-inline">DistributedLLMTrainer</strong> class, which inherits from <strong class="source-inline">RobustLLMTrainer</strong>. The <strong class="source-inline">DistributedLLMTrainer</strong> class is designed for the distributed training of LLMs using PyTorch’s <strong class="source-inline">torch.distributed</strong> framework. It ensures that the model is trained across multiple devices (e.g., GPUs) or <span class="No-Break">nodes efficiently:</span><pre class="source-code">
import torch.distributed as dist
class DistributedLLMTrainer(RobustLLMTrainer):
    def __init__(
        self, model, optimizer, checkpoint_dir='checkpoints',
        autosave_interval=15
    ):
        super().__init__(model, optimizer, checkpoint_dir,
            autosave_interval)
        self.rank = dist.get_rank()
        self.world_size = dist.get_world_size()</pre><p class="list-inset">The initialization does <span class="No-Break">the </span><span class="No-Break"><a id="_idIndexMarker532"/></span><span class="No-Break">following:</span></p><ol><li class="upper-roman">Calls the parent <span class="No-Break">class initializer.</span></li><li class="upper-roman">Sets up distributed <span class="No-Break">training attributes:</span><ol><li class="lower-roman"><strong class="source-inline">self.rank</strong>: Identifies the <span class="No-Break">current process</span></li><li class="lower-roman"><strong class="source-inline">self.world_size</strong>: Indicates the total number <span class="No-Break">of processes</span></li></ol></li></ol></li>				<li>We then use the following methods<a id="_idIndexMarker533"/> to save and load checkpoints during <span class="No-Break">distributed training:</span><pre class="source-code">
def save_checkpoint_distributed(self, epoch, step, loss):
    if self.rank == 0:  # Only the main process saves checkpoints
        self.save_checkpoint_efficient(epoch, step, loss)
    dist.barrier()  # Synchronize all processes
def load_checkpoint_distributed(self, checkpoint_path):
    if self.rank == 0:
        epoch, step, loss = \
            self.load_checkpoint_efficient(checkpoint_path)
    else:
        epoch, step, loss = 0, 0, 0.0
    # Broadcast the loaded data to all processes
    epoch = torch.tensor(epoch).to(self.rank)
    step = torch.tensor(step).to(self.rank)
    loss = torch.tensor(loss).to(self.rank)
    dist.broadcast(epoch, 0)
    dist.broadcast(step, 0)
    dist.broadcast(loss, 0)
    # Make sure all processes have loaded the checkpoint
    dist.barrier()
    return epoch.item(), step.item(), loss.item()</pre><p class="list-inset">The following methods<a id="_idIndexMarker534"/> handle <span class="No-Break">distributed checkpointing:</span></p><ul><li><strong class="source-inline">save_checkpoint_distributed</strong>: Only the main process (rank <strong class="source-inline">0</strong>) saves the checkpoint to avoid redundant writes, reduce disk I/O, and ensure consistency across <a id="_idIndexMarker535"/>processes. If all ranks are saved independently, it could lead to storage inefficiencies and potential race conditions. After saving, <strong class="source-inline">dist.barrier()</strong> synchronizes all processes to ensure they wait for the checkpoint to be written. When loading, only rank <strong class="source-inline">0</strong> reads the checkpoint to prevent redundant disk access; then, it broadcasts the loaded values to all other ranks using <strong class="source-inline">dist.broadcast()</strong>, ensuring every process starts from the same state before <span class="No-Break">resuming training.</span></li><li><span class="No-Break"><strong class="source-inline">load_checkpoint_distributed</strong></span><span class="No-Break">:</span><ul><li>Only the main process loads <span class="No-Break">the checkpoint</span></li><li>It broadcasts the loaded values to all <span class="No-Break">other processes</span></li><li>It ensures all processes have the same <span class="No-Break">checkpoint data</span></li></ul></li></ul></li>				<li>Next, we implement<a id="_idIndexMarker536"/> <span class="No-Break">distributed </span><span class="No-Break"><a id="_idIndexMarker537"/></span><span class="No-Break">training:</span><pre class="source-code">
def train_distributed(self, epochs, steps_per_epoch, train_fn):
    try:
        start_epoch, start_step = 0, 0
        if self.rank == 0:
            latest_checkpoint = self.get_latest_checkpoint()
            if latest_checkpoint:
                start_epoch, start_step, _ = \
                    self.load_checkpoint_efficient(
                    latest_checkpoint)
        # Broadcast the starting epoch and step to all processes
        start_epoch = torch.tensor(start_epoch).to(self.rank)
        start_step = torch.tensor(start_step).to(self.rank)
        dist.broadcast(start_epoch, 0)
        dist.broadcast(start_step, 0)
        start_epoch = start_epoch.item()
        start_step = start_step.item()
        if self.rank == 0:
            print(
                f"Resuming from epoch {start_epoch}, "
                f"step {start_step}"
            )
        for epoch in range(start_epoch, epochs):
            self.current_epoch = epoch
            for step in range(start_step, steps_per_epoch):
                self.current_step = step
                self.current_loss = train_fn(
                    self.model, epoch, step)
                if step % self.autosave_interval == 0:
                    self.save_checkpoint_distributed(
                        epoch, step, self.current_loss)
            start_step = 0  # Reset step counter at the start of each epoch
    except Exception as e:
        print(f"Error occurred on rank {self.rank}: {e}")
        self.save_checkpoint_distributed(self.current_epoch,
            self.current_step, self.current_loss)
        dist.destroy_process_group()
        raise</pre><p class="list-inset">The <strong class="source-inline">train_distributed</strong> method <a id="_idIndexMarker538"/>does <span class="No-Break">the following:</span></p><ol><li class="upper-roman">Determines the starting point (epoch <span class="No-Break">and step)</span></li><li class="upper-roman">Broadcasts this information to <span class="No-Break">all processes</span></li><li class="upper-roman">Runs the training loop with <span class="No-Break">periodic checkpointing</span></li><li class="upper-roman">Handles exceptions by <a id="_idIndexMarker539"/>saving a final checkpoint and <span class="No-Break">cleaning up</span></li></ol></li>				<li>We then employ the following code to initialize distributed training with PyTorch, set up your model for parallel <a id="_idIndexMarker540"/>execution, and perform a simple distributed <span class="No-Break">training loop:</span><pre class="source-code">
def init_distributed():
    dist.init_process_group(backend='nccl')
    rank = dist.get_rank()
    torch.cuda.set_device(rank)
    return rank
def distributed_train_step(model, epoch, step):
    # Simulated distributed training step
    loss = 1 / (epoch + 1 + step + 1)  # Dummy loss that decreases over time
    return loss
def main():
    rank = init_distributed()
    model = GPT2LMHeadModel(GPT2Config()).to(rank)
    model = torch.nn.parallel.DistributedDataParallel(
        model, device_ids=[rank])
    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)
    trainer = DistributedLLMTrainer(model, optimizer)
    trainer.train_distributed(epochs=10, steps_per_epoch=1000,
        train_fn=distributed_train_step)
if __name__ == "__main__":
    main()</pre><p class="list-inset">This code includes <span class="No-Break">the</span><span class="No-Break"><a id="_idIndexMarker541"/></span><span class="No-Break"> following:</span></p><ul><li><strong class="source-inline">init_distributed</strong>: Initializes <a id="_idIndexMarker542"/>the distributed <span class="No-Break">training environment</span></li><li><strong class="source-inline">distributed_train_step</strong>: A dummy training function <span class="No-Break">for demonstration</span></li><li><strong class="source-inline">main</strong>: Shows how to use <strong class="source-inline">DistributedLLMTrainer</strong> <span class="No-Break">in practice</span></li></ul></li>			</ol>
			<p>Key considerations for distributed checkpointing include maintaining consistency by synchronizing all processes using barriers during checkpointing, ensuring that only the main process handles I/O operations to avoid conflicts, and effectively sharing important data by broadcasting it from the main process to the other processes. Additionally, the system incorporates robust error handling, allowing it to gracefully save checkpoints and clean up distributed resources in case <span class="No-Break">of failures.</span></p>
			<p>Next, let us focus on the version <span class="No-Break">control aspect.</span></p>
			<h1 id="_idParaDest-137"><a id="_idTextAnchor170"/>Version control for LLM checkpoints</h1>
			<p>Version control for LLM checkpoints <a id="_idIndexMarker543"/>can help with managing different versions of your model during the development process. Here’s a <span class="No-Break">simple implementation:</span></p>
			<pre class="source-code">
import os
import json
import shutil
class VersionControlledLLMTrainer(DistributedLLMTrainer):
    def __init__(
        self, model, optimizer, checkpoint_dir='checkpoints',
        version_file='versions.json'
    ):
        super().__init__(model, optimizer, checkpoint_dir)
        self.version_file = version_file
        self.versions = self.load_versions()
    def load_versions(self):
        if os.path.exists(self.version_file):
            with open(self.version_file, 'r') as f:
                return json.load(f)
        return {}
    def save_versions(self):
        with open(self.version_file, 'w') as f:
            json.dump(self.versions, f, indent=2)
    def save_checkpoint_versioned(
        self, epoch, step, loss, version_name
    ):
        checkpoint_path = self.save_checkpoint_efficient(
            epoch, step, loss)
        self.versions[version_name] = {
            'path': checkpoint_path,
            'epoch': epoch,
            'step': step,
            'loss': loss
        }
        self.save_versions()
        print(f"Saved version '{version_name}': {checkpoint_path}")
    def load_checkpoint_versioned(self, version_name):
        if version_name not in self.versions:
            raise ValueError(f"Version '{version_name}' not found")
        version_info = self.versions[version_name]
        return self.load_checkpoint_efficient(version_info['path'])
    def create_branch(self, base_version, new_version):
        if base_version not in self.versions:
            raise ValueError(
                f"Base version '{base_version}' not found")
        base_info = self.versions[base_version]
        new_path = f"{self.checkpoint_dir}/branch_{new_version}.pt"
        shutil.copy(base_info['path'], new_path)
        self.versions[new_version] = {
            'path': new_path,
            'epoch': base_info['epoch'],
            'step': base_info['step'],
            'loss': base_info['loss'],
            'branched_from': base_version
        }
        self.save_versions()
        print(f"Created branch '{new_version}' from '{base_version}'")
# Usage
trainer = VersionControlledLLMTrainer(model, optimizer)
trainer.save_checkpoint_versioned(epoch=10, step=500,
    loss=0.1, version_name="v1.0")
trainer.create_branch("v1.0", "experimental_branch")
epoch, step, loss = trainer.load_checkpoint_versioned(
    "experimental_branch")</pre>			<p>This implementation provides <a id="_idIndexMarker544"/>basic version <span class="No-Break">control features:</span></p>
			<ul>
				<li><strong class="bold">Version tracking</strong>: Each saved checkpoint can be associated with a <span class="No-Break">version name</span></li>
				<li><strong class="bold">Branching</strong>: You can create new branches from existing checkpoints, allowing <span class="No-Break">for experimentation</span></li>
				<li><strong class="bold">Version history</strong>: The version information is stored in a JSON file for easy inspection <span class="No-Break">and management</span></li>
			</ul>
			<p>The key benefits of version control for LLM checkpoints are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">Experimentation</strong>: You can easily try different training strategies or hyperparameters from a <a id="_idIndexMarker545"/>common <span class="No-Break">starting point</span></li>
				<li><strong class="bold">Collaboration</strong>: Team members can share and work on different versions of <span class="No-Break">the model</span></li>
				<li><strong class="bold">Reproducibility</strong>: Specific versions of the model can be referenced <span class="No-Break">and recreated</span></li>
			</ul>
			<h1 id="_idParaDest-138"><a id="_idTextAnchor171"/>Automated checkpointing and recovery systems</h1>
			<p>To make the checkpointing <a id="_idIndexMarker546"/>and recovery process more robust <a id="_idIndexMarker547"/>and hands-off, we<a id="_idTextAnchor172"/> can implement an <span class="No-Break">automated system:</span></p>
			<ol>
				<li>First, import the <span class="No-Break">required modules:</span><pre class="source-code">
import threading
import time</pre><p class="list-inset">We imported two key <span class="No-Break">modules here:</span></p><ul><li><strong class="source-inline">threading</strong>: Enables the creation of threads for running tasks (such as auto-save and health checks) concurrently with the main <span class="No-Break">training process</span></li><li><strong class="source-inline">time</strong>: Used to manage intervals between auto-saves and health checks, as wel<a id="_idTextAnchor173"/>l as timestamping <span class="No-Break">saved checkpoints</span></li></ul></li>				<li>Next, we define <a id="_idIndexMarker548"/>and initialize <span class="No-Break">the class:</span><pre class="source-code">
class AutomatedLLMTrainer(VersionControlledLLMTrainer):
    def __init__(
        self, model, optimizer, checkpoint_dir='checkpoints',
        autosave_interval=15, version_file='versions.json',
        health_check_interval=60
    ):
        super().__init__(model, optimizer, checkpoint_dir,
            version_file)
        self.autosave_interval = autosave_interval
        self.health_check_interval = health_check_interval
        self.training_active = False</pre><p class="list-inset">The <strong class="source-inline">AutomatedLLMTrainer</strong> class inherits from a base class, <strong class="source-inline">VersionControlledLLMTrainer</strong>, which handles basic checkpointing logic. This class introduces <a id="_idIndexMarker549"/>automation for checkpointing and system <span class="No-Break">health monitoring.</span></p><p class="list-inset">The following is a list of parameters that manage auto-saving, system health checks, and training <span class="No-Break">execution control:</span></p><ul><li><strong class="source-inline">autosave_interval</strong>: The time (in seconds) between <span class="No-Break">auto-save checkpoints.</span></li><li><strong class="source-inline">health_check_interval</strong>: The time between system <span class="No-Break">health checks.</span></li><li><strong class="source-inline">training_active</strong>: A flag to indicate whether the training is ongoing. It is used to control the <span class="No-Break">threads’ execution.</span></li></ul><p class="list-inset">The constructor calls <strong class="source-inline">super().__init__()</strong> to inherit functionality from the parent class and sets up <a id="_idIndexMarker550"/>interv<a id="_idTextAnchor174"/>als for auto-saving and <span class="No-Break">health checks.</span></p></li>				<li>Auto-save the <a id="_idIndexMarker551"/>thread <span class="No-Break">for checkpointing:</span><pre class="source-code">
def start_autosave_thread(self):
    def autosave_loop():
        while self.training_active:
            time.sleep(self.autosave_interval)
            if self.training_active:
                self.save_checkpoint_versioned(
                    self.current_epoch, self.current_step,
                    self.current_loss,
                    f"autosave_{time.time()}")
    self.autosave_thread = threading.Thread(
        target=autosave_loop)
    self.autosave_thread.start()</pre><p class="list-inset">This method starts a separate thread that periodically saves a checkpoint <span class="No-Break">during training.</span></p><p class="list-inset">The following is a list of components responsible for handling periodic auto-saving <span class="No-Break">during training:</span></p><ul><li><strong class="source-inline">autosave_loop</strong>: A function that continuously runs while <strong class="source-inline">training_active</strong> is <strong class="source-inline">True</strong>. Every <strong class="source-inline">autosave_interval</strong> seconds, it calls the <strong class="source-inline">save_checkpoint_versioned()</strong> method to save the <span class="No-Break">current state.</span></li><li><strong class="source-inline">threading.Thread</strong>: The thread runs <strong class="source-inline">autosave_loop</strong> in the background, ensuring that<a id="_idIndexMarker552"/> auto-saves happen<a id="_idTextAnchor175"/> concurrently with the <span class="No-Break">training process.</span></li></ul></li>				<li>Next, we implement a<a id="_idIndexMarker553"/> health check thread. This method starts a health check thread that monitors system performance at <span class="No-Break">regular intervals:</span><pre class="source-code">
def start_health_check_thread(self):
    def health_check_loop():
        while self.training_active:
            time.sleep(self.health_check_interval)
            if self.training_active:
                if not self.check_system_health():
                    print("System health check failed.
                        Initiating recovery...")
                    self.initiate_recovery()
    self.health_check_thread = threading.Thread(
        target=health_check_loop)
    self.health_check_thread.start()</pre><p class="list-inset">Here are the main elements of the <span class="No-Break">preceding snippet:</span></p><ul><li><strong class="source-inline">health_check_loop</strong>: A function that continuously runs during training. Every <strong class="source-inline">health_check_interval</strong> seconds, it checks the system health by calling <strong class="source-inline">check_system_health()</strong>. If a problem is detected, it triggers the <span class="No-Break">recovery process.</span></li><li><strong class="source-inline">check_system_health()</strong>: This method needs to be defined to check the system’s performance metrics (e.g., GPU memory or CPU usage). If the health c<a id="_idTextAnchor176"/>heck fails, it <span class="No-Break">calls </span><span class="No-Break"><strong class="source-inline">initiate_recovery()</strong></span><span class="No-Break">.</span></li></ul></li>				<li>We perform system health check and recovery. The following placeholder method is where the system<a id="_idIndexMarker554"/> health checks will be implemented, for example, checking GPU <a id="_idIndexMarker555"/>memory, CPU utilization, disk space, or any other resource critical to the training process. It returns <strong class="source-inline">True</strong> if everything is fine, and <strong class="source-inline">False</strong> if there’s <span class="No-Break">a problem:</span><pre class="source-code">
def check_system_health(self):
    # Implement system health checks here
    # For example, check GPU memory, CPU usage, disk space, etc.
    return<a id="_idTextAnchor177"/> True  # Return False if health check fails</pre><p class="list-inset">The following method will contain logic for what to do if the system health check fails. It could, for instance, reload the last checkpoint, reduce the batch size, or take other corrective actions depending on the <span class="No-Break">issue detected:</span></p><pre class="source-code">def initiate_recovery(self):
    # Implement recovery logic here
    # For example, reload from the last c<a id="_idTextAnchor178"/>heckpoint, reduce batch size, etc.
    pass</pre></li>				<li>Finally, we conduct automated training with checkpointing and health checks. This method manages the overall training process with automation. It activates the auto-save and health check threads and initiates distributed training via the parent class’s <span class="No-Break"><strong class="source-inline">train_distributed()</strong></span><span class="No-Break"> method:</span><pre class="source-code">
def train_with_automation(
    self, epochs, steps_per_epoch, train_fn):
    self.training_active = True
    self.start_autosave_thread()
    self.start_health_check_thread()
    try:
        super().train_distributed(epochs, steps_per_epoch,
            train_fn)
    finally:
        self.training_active = False
        self.autosave_thread.join()
        self.health_check_thread.join()</pre><p class="list-inset">Here’s a breakdown of the main <span class="No-Break">code elements:</span></p><ul><li><strong class="source-inline">self.training_active</strong>: Set to <strong class="source-inline">True</strong> to indicate that training <span class="No-Break">is running</span></li><li><strong class="source-inline">try-finally block</strong>: Ensures that no matter how the training ends (whether it completes or <a id="_idIndexMarker556"/>crashes), the <strong class="source-inline">training_active</strong> flag is set to <strong class="source-inline">False</strong> and both <a id="_idIndexMarker557"/>threads are <span class="No-Break">properly terminated</span></li></ul></li>			</ol>
			<p>This approach reduces manual intervention, enhances reliability, and offers flexibility in defining recover<a id="_idTextAnchor179"/>y logic based on the specific <span class="No-Break">training needs.</span></p>
			<h1 id="_idParaDest-139"><a id="_idTextAnchor180"/>Summary</h1>
			<p>Implementing robust checkpointing and recovery systems is common practice for successful LLM training. By incorporating these techniques, you can ensure that your long-running training processes are resilient to failures, easily manageable, and conducive to experimentation <span class="No-Break">and collaboration.</span></p>
			<p>To expand our discussion, <em class="italic">Table 10.1</em> lists checkpointing strategies, trade-offs, and <span class="No-Break">use cases:</span></p>
			<table id="table001-2" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Checkpointing </strong><span class="No-Break"><strong class="bold">Strategy</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Description</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Trade-Offs</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Use Cases</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Regular (with <span class="No-Break">max limit)</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Saves at intervals (steps/epochs); keeps a <span class="No-Break">maximum number.</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Pros: Saves storage; <span class="No-Break">periodic snapshots.</span></p>
							<p>Cons: Might overwrite <span class="No-Break">good checkpoints.</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Iterative model development; monitoring training progress; preventing complete data loss during long <span class="No-Break">training runs.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Time-based</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Saves at specified intervals (e.g., every <span class="No-Break">30 minutes).</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Pros: <span class="No-Break">Time-spaced snapshots.</span></p>
							<p>Cons: Inefficient if the interval is <span class="No-Break">too short/long.</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Long-running experiments where consistent, time-stamped checkpoints are crucial for debugging and analysis; ensuring recoverability in case of <span class="No-Break">system failures.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Best model</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Saves only when the model achieves the <span class="No-Break">best performance.</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Pros: Retains the <span class="No-Break">best model.</span></p>
							<p>Cons: May not be representative if loss is noisy; no <span class="No-Break">intermediate snapshots.</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Selecting the most <span class="No-Break">performant model.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Efficient (compression)</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Uses compression (e.g., ZIP) to <span class="No-Break">reduce size.</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Storage-constrained environments; handling large models where storage is a primary concern; archiving models for <span class="No-Break">long-term storage.</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Storage-constrained environments; archiving models for <span class="No-Break">long-term storage.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Efficient (quantization)</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Reduces precision of weights (e.g., float32 <span class="No-Break">to float16).</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Pros: <span class="No-Break">Reduces size.</span></p>
							<p>Cons: Potential <span class="No-Break">accuracy loss.</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Deploying models on resource-limited devices; reducing checkpoint size for faster transfer and storage; accelerating <span class="No-Break">model loading.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Efficient (incremental)</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Saves only changes since the <span class="No-Break">last checkpoint.</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Pros: Can significantly <span class="No-Break">reduce size.</span></p>
							<p>Cons: Complex; <span class="No-Break">potentially fragile.</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Training models with gradual parameter updates; large models where frequent full checkpoints are impractical; continuous <span class="No-Break">learning scenarios.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Distributed</span></p>
						</td>
						<td class="No-Table-Style">
							<p>In distributed training, only the main process (rank 0) saves; data is broadcast <span class="No-Break">to others.</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Pros: Avoids redundant writes; <span class="No-Break">ensures consistency.</span></p>
							<p>Cons: <span class="No-Break">Requires coordination.</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Large-scale distributed training jobs; ensuring consistent model states across multiple workers; minimizing <span class="No-Break">network overhead.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Version-controlled</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Associates checkpoints with versions; <span class="No-Break">supports branching.</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Pros: Experimentation; <span class="No-Break">reproducibility; rollback.</span></p>
							<p>Cons: <span class="No-Break">Adds complexity.</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Collaborative model development; tracking experimental variations; ensuring reproducibility for scientific research; managing <span class="No-Break">model evolution.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Automated (with <span class="No-Break">health checks)</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Auto-saves checkpoints; performs health checks; can <span class="No-Break">initiate recovery.</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Pros: Reduces manual work; <span class="No-Break">enhances reliability.</span></p>
							<p>Cons: Requires health check/recovery <span class="No-Break">implementation.</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Mission-critical training jobs; automated recovery from failures; long-running experiments requiring <span class="No-Break">high reliability.</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 10.1 – Checkpointing strategies, trade-offs, and use cases</p>
			<p>In the next chapter, we’ll explore effective techniques for adapting pre-trained language models to specific tasks <span class="No-Break">or domains.</span></p>
		</div>
	</div></div></body></html>