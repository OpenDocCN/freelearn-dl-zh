["```py\nimport torch\nfrom diffusers import StableDiffusionPipeline\ntext2img_pipe = StableDiffusionPipeline.from_pretrained(\n    \"stablediffusionapi/deliberate-v2\",\n    torch_dtype = torch.float16\n)\n```", "```py\ntext2img_pipe.to(\"cuda:0\")\nprompt =\"high resolution, a photograph of an astronaut riding a horse\"\ninput_image = text2img_pipe(\n    prompt = prompt,\n    generator = torch.Generator(\"cuda:0\").manual_seed(100),\n    height = 512,\n    width = 768\n).images[0]\ntext2img_pipe.to(\"cpu\")\ntorch.cuda.empty_cache()\ninput_image\n```", "```py\npip install -U transformer\n```", "```py\nfrom transformers import AutoProcessor, Blip2ForConditionalGeneration\nimport torch\nprocessor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n# by default `from_pretrained` loads the weights in float32\n# we load in float16 instead to save memory\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = Blip2ForConditionalGeneration.from_pretrained(\n    \"Salesforce/blip2-opt-2.7b\",\n    torch_dtype=torch.float16\n).to(device)\n```", "```py\nprompt = \"describe the content of the image:\"\ninputs = processor(\n    input_image,\n    text=prompt,\n    return_tensors=\"pt\"\n).to(device, torch.float16)\ngenerated_ids = model.generate(**inputs, max_new_tokens=768)\ngenerated_text = processor.batch_decode(\n    generated_ids,\n    skip_special_tokens=True\n)[0].strip()\nprint(generated_text)\n```", "```py\ngit clone https://github.com/haotian-liu/LLaVA.git\ncd LLaVA\n```", "```py\npip install -U .\n```", "```py\n# Make sure you have git-lfs installed (https://git-lfs.com)\ngit lfs install\ngit clone https://huggingface.co/liuhaotian/llava-v1.5-7b\n```", "```py\nfrom llava.constants import (\n    IMAGE_TOKEN_INDEX,\n    DEFAULT_IMAGE_TOKEN,\n    DEFAULT_IM_START_TOKEN,\n    DEFAULT_IM_END_TOKEN\n)\nfrom llava.conversation import (\n    conv_templates, SeparatorStyle\n)\nfrom llava.model.builder import load_pretrained_model\nfrom llava.mm_utils import (\n    process_images,\n    tokenizer_image_token,\n    get_model_name_from_path,\n    KeywordsStoppingCriteria\n)\n```", "```py\n# load up tokenizer, model, image_processor\nmodel_path = \"/path/to/llava-v1.5-7b\"\nmodel_name = get_model_name_from_path(model_path)\nconv_mode = \"llava_v1\"\ntokenizer, model, image_processor, _ = load_pretrained_model(\n    model_path = model_path,\n    model_base = None,\n    model_name = model_name,\n    load_4bit = True,\n    device = \"cuda\",\n    device_map = {'':torch.cuda.current_device()}\n)\n```", "```py\n    # start a new conversation\n    ```", "```py\n    user_input = \"\"\"Analyze the image in a comprehensive and detailed manner\"\"\"\n    ```", "```py\n    conv = conv_templates[conv_mode].copy()\n    ```", "```py\n    # process image to tensor\n    ```", "```py\n    image_tensor = process_images(\n    ```", "```py\n        [input_image],\n    ```", "```py\n        image_processor,\n    ```", "```py\n        {\"image_aspect_ratio\":\"pad\"}\n    ```", "```py\n    ).to(model.device, dtype=torch.float16)\n    ```", "```py\n    if model.config.mm_use_im_start_end:\n    ```", "```py\n        inp = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + \\\n    ```", "```py\n            DEFAULT_IM_END_TOKEN + '\\n' + user_input\n    ```", "```py\n    else:\n    ```", "```py\n        inp = DEFAULT_IMAGE_TOKEN + '\\n' + user_input\n    ```", "```py\n    conv.append_message(conv.roles[0], inp)\n    ```", "```py\n    # get the prompt for inference\n    ```", "```py\n    conv.append_message(conv.roles[1], None)\n    ```", "```py\n    prompt = conv.get_prompt()\n    ```", "```py\n    # convert prompt to token ids\n    ```", "```py\n    input_ids = tokenizer_image_token(\n    ```", "```py\n        prompt,\n    ```", "```py\n        tokenizer,\n    ```", "```py\n        IMAGE_TOKEN_INDEX,\n    ```", "```py\n        return_tensors='pt'\n    ```", "```py\n    ).unsqueeze(0).cuda()\n    ```", "```py\n    stop_str = conv.sep if conv.sep_style != \\\n    ```", "```py\n        SeparatorStyle.TWO else conv.sep2\n    ```", "```py\n    keywords = [stop_str]\n    ```", "```py\n    stopping_criteria = KeywordsStoppingCriteria(keywords,\n    ```", "```py\n        tokenizer, input_ids)\n    ```", "```py\n    # output the data\n    ```", "```py\n    with torch.inference_mode():\n    ```", "```py\n        output_ids = model.generate(\n    ```", "```py\n            input_ids,\n    ```", "```py\n            images =image_tensor,\n    ```", "```py\n            do_sample = True,\n    ```", "```py\n            temperature = 0.2,\n    ```", "```py\n            max_new_tokens = 1024,\n    ```", "```py\n            streamer = None,\n    ```", "```py\n            use_cache = True,\n    ```", "```py\n            stopping_criteria = [stopping_criteria]\n    ```", "```py\n        )\n    ```", "```py\n    outputs = tokenizer.decode(output_ids[0,\n    ```", "```py\n        input_ids.shape[1]:]).strip()\n    ```", "```py\n    # make sure the conv object holds all the output\n    ```", "```py\n    conv.messages[-1][-1] = outputs\n    ```", "```py\n    print(outputs)\n    ```", "```py\n    The image features a man dressed in a white space suit, riding a horse in a desert-like environment. The man appears to be a space traveler, possibly on a mission or exploring the area. The horse is galloping, and the man is skillfully riding it.\n    ```", "```py\n    In the background, there are two moons visible, adding to the sense of a space-themed setting. The combination of the man in a space suit, the horse, and the moons creates a captivating and imaginative scene.</s>\n    ```"]