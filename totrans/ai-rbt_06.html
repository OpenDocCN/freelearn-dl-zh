<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-98"><a id="_idTextAnchor205"/>6</h1>
<h1 id="_idParaDest-99"><a id="_idTextAnchor206"/>Teaching a Robot to Listen</h1>
<p>Teaching a robot to listen to spoken instructions is a whole discipline in itself. It is not sufficient for the robot to just recognize individual words or some canned phrase. We want the robot to respond to normal spoken commands with a normal variety of phrasing. We might say, “Pick up the toys,” or “Please pick up all the toys,” or “Clean this mess up,” any of which would be a valid command to instruct the robot to begin searching the room for toys to pick up and put away. We will be using a variety of techniques and processes for this chapter. We are going to be building on an open source verbal <a id="_idIndexMarker442"/>assistant called <strong class="bold">Mycroft</strong>, an AI-based speech recognition and <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>) engine that can be programmed and extended by us. We will <a id="_idIndexMarker443"/>be adding some additional capability to Mycroft – we will use a technique I call the “fill in the blank” method of command processing to extract the intent of the user’s voice instructions, so that the robot does what you want it to do, even if that is not exactly what you said. We will complete this chapter by teaching the robot to both tell and respond to a specific form of human communication – knock-knock jokes.</p>
<p>The following topics will be covered in the chapter:</p>
<ul>
<li>Exploring robot <a id="_idIndexMarker444"/>speech recognition with NLP – both <strong class="bold">speech to text</strong> (<strong class="bold">STT</strong>) and <strong class="bold">text to </strong><strong class="bold">speech</strong> (<strong class="bold">TTS</strong>)</li>
<li>Programming <a id="_idIndexMarker445"/>our robot</li>
</ul>
<h1 id="_idParaDest-100"><a id="_idTextAnchor207"/>Technical requirements</h1>
<p>This chapter uses the following tools:</p>
<ul>
<li>Mycroft Open <a id="_idIndexMarker446"/>Source Voice Assistant (<a href="http://mycroft.ai">http://mycroft.ai</a>) – I had to build it from source from the GitHub repository (<a href="https://github.com/MycroftAI">https://github.com/MycroftAI</a>), so expect to do the same to keep it compatible with <a id="_idIndexMarker447"/>the <strong class="bold">Robot Operating System</strong> (<strong class="bold">ROS</strong>) we run the robot with.</li>
<li>Python 3.2.</li>
<li>You will need a GitHub account at <a href="https://github.com/">https://github.com/</a>.</li>
<li>I used a miniature USB speaker and microphone for this project, which worked very well with the Jetson. They can be found at <a href="https://www.amazon.com/gp/product/B08R95XJW8">https://www.amazon.com/gp/product/B08R95XJW8</a></li>
</ul>
<p>The code used in this chapter can be found in the GitHub repository for this book at <a href="https://github.com/PacktPublishing/Artificial-Intelligence-for-Robotics-2e">https://github.com/PacktPublishing/Artificial-Intelligence-for-Robotics-2e</a>.</p>
<h1 id="_idParaDest-101"><a id="_idTextAnchor208"/>Exploring robot speech recognition with NLP</h1>
<p>This is going <a id="_idIndexMarker448"/>to be a rather involved chapter, but all of the concepts are fairly easy to understand. We will end up with a very strong <a id="_idIndexMarker449"/>framework to build voice recognition and commands upon. Not only will you get a voice-based command system for a robot, but also a full-featured digital assistant that tells jokes. Let’s first quickly introduce NLP.</p>
<h2 id="_idParaDest-102"><a id="_idTextAnchor209"/>Briefly introducing the NLP concept</h2>
<p>NLP is not just converting sound waves to written words (speech to text, or STT), but also understanding what <a id="_idIndexMarker450"/>those words mean. We don’t want to just have some rigid, pre-programmed spoken commands, but some ability for the robot to respond to human speech.</p>
<p>We will be using two different forms of STT processing:</p>
<ul>
<li><strong class="bold">Spectrum analysis</strong>: This type helps to detect when you say the robot’s name. This technique <a id="_idIndexMarker451"/>recognizes words or phrases by sampling which frequencies and amplitudes make up the word. This process has the <a id="_idIndexMarker452"/>advantage of not taking a lot of computer resources and it is good at recognizing just one word or phrase – our “wake word” that will cause the computer to switch to the second type of voice recognition. This is the reason other voice-operated assistants require you to use a specific word (e.g., Siri, or Alexa) to enable them to start listening.</li>
<li><strong class="bold">Phoneme recognition</strong>: This technique converts STT by recognizing the parts of sounds – phonemes – that make up words. This technique, which seeks to interpret all <a id="_idIndexMarker453"/>sounds as words, is much more difficult, so we use the wake word to trigger the change. We’ll cover this in more detail later in this chapter.</li>
</ul>
<p>Next, let’s explore our primary goals for implementing speech recognition.</p>
<h2 id="_idParaDest-103"><a id="_idTextAnchor210"/>Setting our goals</h2>
<p>We set several goals for our robot in <a href="B19846_02.xhtml#_idTextAnchor032"><em class="italic">Chapter 2</em></a>, which included being able to give voice commands to the robot since we may be using the robot without a base station. I also wanted the <a id="_idIndexMarker454"/>robot to be able to interact with my grandchildren, and specifically to be able to tell and respond to knock-knock jokes, a favorite activity of my grandson, William. For our robot, we do not want to use canned or memorized speech commands but rather have it be able to do some NLP to create a form of robot understanding of the spoken word. For example, if we want to have a command for picking up a toy, we humans could phrase that several ways: <em class="italic">grab a toy</em>, <em class="italic">grasp a toy</em>, <em class="italic">pick up that toy car</em>, or even <em class="italic">get that</em>. We want the robot to understand or at least respond to all of those utterances with the same action, to drive to the nearest toy and pick it up with the robot arm. STT systems are fairly commonplace today, but we would like to have some natural variations in the robot’s speech patterns to help create the illusion that the robot is smarter than it really is. We can break this process down into several steps, which we will be handling independently:</p>
<ul>
<li>Receive audio (sound) inputs. We need the robot to be able to hear or have the ability to convert sound into a digital form.</li>
<li>Those sounds need to be converted into text that the robot can process.</li>
<li>Use processing on those text words to understand the intent of the speaker. We need to not just recognize individual words but combine those words into sentences and from those sentences, infer the intent of the speaker to understand what the robot is to do.</li>
<li>Use that intent as a command to perform some task.</li>
<li>Provide verbal responses in the form of spoken words (text to speech, or TTS) back to the operator to confirm the robot heard and understood the command.</li>
<li>Create a custom verbal interface that both tells and responds to knock-knock jokes.</li>
</ul>
<p>We will start <a id="_idIndexMarker455"/>in the next section by introducing the process of STT, which is how the robot will receive voice input from you.</p>
<h2 id="_idParaDest-104"><a id="_idTextAnchor211"/>Understanding the STT process</h2>
<p>In the rest of this chapter, we will be implementing an AI-based voice recognition and response system <a id="_idIndexMarker456"/>in the robot and creating our own custom voice interface. We will be using Mycroft, an open source voice-activated digital assistant that is adept at understanding speech and is easily extended for new functions and custom interfaces.</p>
<p>We will discuss each of the steps involved in voice interaction in detail. There are two forms of STT involved in this process that greatly simplify matters for <a id="_idIndexMarker457"/>the robot: <strong class="bold">wake word recognition</strong> and <strong class="bold">STT recognition</strong>. Let’s explore wake word recognition first.</p>
<h3>Listening for the wake word</h3>
<p>In the first approach, the robot is listening continuously for only one sound – the wake word. This is a <a id="_idIndexMarker458"/>specific sound that just means one thing – get ready to process the next sound into a command. Why is this necessary? Since the robot has only a very small processor – the Jetson Nano – it really does not have the sort of onboard compute power to run a robust STT engine. But it can run a simple sound recognizer that can listen for the wake word. You are familiar with this from other voice command systems, such as Alexa or Siri, that also either use a special wake word or a button <a id="_idIndexMarker459"/>to have the interface pay attention (see <a href="https://www.howtogeek.com/427686/how-alexa-listens-for-wake-words/">https://www.howtogeek.com/427686/how-alexa-listens-for-wake-words/</a>).</p>
<p>Once the wake word is received, the Jetson Nano switches into record mode and records the next thing we say. It then transfers that information to an online system, the Google Cloud Speech to Text system (the same thing that runs the Google Assistant).</p>
<p>How does the robot recognize the wake word? The speech system we will be using, the open source system Mycroft, uses one of two methods:</p>
<ul>
<li>The first is a <strong class="bold">phoneme recognition system</strong> called Sphynx. What is a phoneme? You can <a id="_idIndexMarker460"/>understand that words are made up of individual sounds, which we roughly assign to letters of the alphabet. An example <a id="_idIndexMarker461"/>would be the <em class="italic">p</em> sound in the word <em class="italic">pet</em> or <em class="italic">pick</em> – this is an example of a phoneme. The word “Albert” has several phonemes – the <em class="italic">A</em> sound, (ah), the <em class="italic">L</em> sound, the <em class="italic">B</em>, the <em class="italic">ER</em> together, and finally, the <em class="italic">T</em>. The letters we associate with the sounds – for example, the <em class="italic">ch</em> in <em class="italic">cherry</em>, and the <em class="italic">er</em> in <em class="italic">Albert</em>, are called <strong class="bold">graphemes</strong>, as they graphically represent these sounds. We could say <a id="_idIndexMarker462"/>that the STT problem is one of mapping these phonemes to graphemes, but we know that this is too easy – English has all sorts of borrowed words and phrases where the pronunciation and the spelling are far apart.<p class="list-inset">The frontend of the Mycroft speech recognition process uses phonemes to recognize the wake word. You will find that it is quite sensitive. I had no problem getting the speech processor to receive the wake word from eight feet away. When we get to the setup, we will change the default Mycroft wake word from “Hey, Mycroft,” to “Hey, Albert.”</p></li>
<li>Mycroft can also <a id="_idIndexMarker463"/>use a trained <strong class="bold">neural network</strong> that has been taught to recognize entire words all at once by their <strong class="bold">spectral power graph</strong>. What is <a id="_idIndexMarker464"/>a spectral graph? The sound of your voice is not one frequency of sound energy – it is a complex congregation of different frequencies produced by our mouths and vocal cords. If we spoke in pure frequencies, we would sound like a flute – pure tones at mostly one frequency. We can use a <a id="_idIndexMarker465"/>process called a <strong class="bold">fast Fourier transform</strong> to convert a selection of speech into a graph that shows the amount of energy (volume) at each frequency. This is called a spectral plot or spectral graph. The low frequencies are on the left, and higher frequencies are on the right. Most human speech’s energy is concentrated in the frequencies between 300 Hz and 4,000 Hz. Each word has a unique distribution of sound energy amounts in these frequencies, and can be recognized by a neural network in this manner:</li>
</ul>
<div><div><img alt="Figure 6.1 – Analog audio waveform (top) and the spectral graph for the phrase “Hey, Albert” (bottom)﻿" src="img/B19846_06_1.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.1 – Analog audio waveform (top) and the spectral graph for the phrase “Hey, Albert” (bottom)</p>
<p class="list-inset">This preceding diagram shows both the audio waveform (top graph) in the time domain and the spectral plot in the frequency domain for the phrase “Hey, Albert.”</p>
<p>Both the phoneme <a id="_idIndexMarker466"/>method and the neural network method use spectral plots to recognize sounds as words, but the phoneme process divides words into individual sounds, and the neural network listens and recognizes the entire word all at once. Why does this make a big difference? The phoneme system can be developed to recognize any word in English without reprogramming or retraining, while the neural network has to be trained on each word individually, and hopefully by a lot of different speakers with a lot of different accents. We’ll be using the neural network method for Albert.</p>
<p class="callout-heading">Note</p>
<p class="callout">You can remember from <a href="B19846_04.xhtml#_idTextAnchor126"><em class="italic">Chapter 4</em></a> that we needed labeled data to train a neural network. You recall <a id="_idIndexMarker467"/>we had pictures in categories and trained on each category. Training <strong class="bold">artificial neural networks</strong> (<strong class="bold">ANNs</strong>) for sound is the same: we need sounds and the associated words. Can you think of a place to get samples of lots of different voices where you also have the exact written script to match? Have you ever listened to a book on tape?</p>
<h3>Converting STT</h3>
<p>Our next step after receiving the wake word is to record the next sounds that the robot hears. The Mycroft <a id="_idIndexMarker468"/>system then transfers that audio data over the internet to the Google online STT engine (<a href="https://cloud.google.com/speech-to-text/">https://cloud.google.com/speech-to-text/</a>). This is a quick way to resolve the problem of our little Jetson Nano not having enough processing power or storage to have a robust speech recognition capability.</p>
<p>What goes on in Google Cloud? The STT engine breaks the speech down into phonemes (sounds) and uses a neural network to assign the most probable graphemes (letters) to those sounds. The output would be spelled out more phonetically than we want to receive. For example, as per the <em class="italic">Carnegie Mellon University Pronouncing Dictionary</em>, the sentence “Pick up the toys, please?” comes out as  <code>P IH K . AH P . DH AH . T OY Z . P L IY Z</code>. Why is this the case? What happened? These are the phonemes that make up that sentence. The periods indicate spaces between words. Now the system has to convert this into the words we are expecting. The STT system uses word rules and dictionaries to come up with the most likely conversion into regular words. This includes both expert systems (word rules) and trained neural networks that predict output words based on phonemes.</p>
<p>We can call this step the <strong class="bold">language model</strong>. Our STT outputs the sentence “How many ounces in a gallon?” and sends it back to the robot, all in less than two seconds.</p>
<p>Now that we have the command in text, an English sentence, how does the robot recognize what your intent is?</p>
<h2 id="_idParaDest-105"><a id="_idTextAnchor212"/>Clarifying the intent</h2>
<p>The NLP we are <a id="_idIndexMarker469"/>doing has one aim, or goal. We are giving commands to our robot using a voice interface. Commands in English normally follow a sentence pattern, something like “You – do this.” Often the “you” subject of the sentence is implied or understood and left out. We are left with statements such as “Clean this room,” or “Pick up those toys.” The intent of these commands is to have the robot initiate a program that results in the robot picking up toys and putting them away. The robot and its processor have to divine or derive the intent of the user from the words that are spoken. What we want is for any reasonable sentence to have as its meaning: “You, robot, start your pick-up-toys process.”</p>
<p>Think of how many ways we can say that command to the robot. Here are some examples:</p>
<ul>
<li>Let’s clean up this room</li>
<li>Put away the toys</li>
<li>Pick up the toys</li>
<li>Pick up all the toys</li>
<li>Clean up this room</li>
<li>Put those away</li>
<li>Put this away</li>
<li>Time to clean up</li>
</ul>
<p>What do <a id="_idIndexMarker470"/>these phrases have in common? They all imply the subject who is doing the action is the robot. There are no words such as “You,” “robot,” or “Albert” to indicate to whom the command is intended for. The word “toys” appears a lot, as does “pick,” “clean,” and “put away.” It is possible that we can just pay attention to those keywords to understand this command. If we get rid of all of the common conjunction and pronoun words, what does the list look like?</p>
<ul>
<li>Clean room</li>
<li>Put toys</li>
<li>Pick toys</li>
<li>Pick toys</li>
<li>Clean room</li>
<li>Put away</li>
<li>Put away</li>
<li>Time clean</li>
</ul>
<p>An important concept for this chapter is to understand that we are not trying to understand all speech, but only that subset of speech that are commands that the robot can execute. A general solution to this voice recognition problem would be to have some ability to <a id="_idIndexMarker471"/>predict from the command given to the robot, the likelihood that the intent of the user points to one command more than any of the others. You can see that in the case of the word “clean,” none of our other commands (“drive around,” “move arm,” or “stop”) relate to “clean” at all. Thus, a sentence with “clean” in it is likely associated with the <em class="italic">pick up toys</em> command. This process of deciding intent will be used later in this chapter to send commands to the robot using Mycroft.</p>
<p>Now we are going to jump right into programming the Albert robot to listen and understand commands using Mycroft.</p>
<h1 id="_idParaDest-106"><a id="_idTextAnchor213"/>Programming our robot</h1>
<p>As discussed earlier in this chapter, Mycroft is a version of a digital assistant similar to Siri from Apple <a id="_idIndexMarker472"/>or Alexa from Amazon in that it can listen to voice commands in a mostly normal fashion and interface those commands to a computer. We are using it because it has an interface that runs on a Jetson Nano 3. In this section, we will be setting up our hardware and our software (i.e., Mycroft).</p>
<h2 id="_idParaDest-107"><a id="_idTextAnchor214"/>Setting up the hardware</h2>
<p>We will be installing Mycroft on Nvidia Jetson Nano (or whatever microprocessor you’re using). One of <a id="_idIndexMarker473"/>the few things that the Jetson Nano did not come with is <strong class="bold">audio capability</strong>. It has no speakers or microphones. I found that a quick and effective way to add that capability was to use an existing hardware kit that provided both a very high-quality speaker and an excellent set of stereo <a id="_idIndexMarker474"/>microphones in a robot-friendly form factor. Note that this works with pretty much any Linux <strong class="bold">single-board </strong><strong class="bold">computer</strong> (<strong class="bold">SBC</strong>).</p>
<p>The kit is a miniature USB audio board that plugs into the Jetson Nano. It has both speakers and a microphone.</p>
<p class="callout-heading"> Note</p>
<p class="callout">I used a USB audio board (the brand is not important as any of them will do) for the Jetson Nano, which has been working very well for me and fits in the very small space we have on the robot. Installation could not be simpler. Plug in the audio board. You will need to go to <strong class="bold">Settings</strong> in the upper-right corner of your screen to select the USB audio version. There will be several other options listed.</p>
<p>Turn on <a id="_idIndexMarker475"/>your Jetson Nano 3 with the new speaker and microphone. I ran a quick test with YouTube to make sure the audio worked, and you can test it directly in the <strong class="bold">Settings</strong> user interface. Now we can dive into the software.</p>
<h2 id="_idParaDest-108"><a id="_idTextAnchor215"/>Setting up the Mycroft software</h2>
<p>There are <a id="_idIndexMarker476"/>several ways to install Mycroft, as we <a id="_idIndexMarker477"/>have to put Mycroft on top of the other software we have already:</p>
<ol>
<li>Since Mycroft must get along with the ROS, and all of the AI libraries we installed, such as TensorFlow, Theano, and Keras, it is best that we use the <code>git clone</code> method to download the source code and build Mycroft on the Jetson Nano:<pre class="source-code">
<strong class="bold">git clone https://github.com/MycroftAI/mycroft-core.git cd Mycroft-core</strong>
<strong class="bold">bash dev_setup.sh</strong></pre><p class="list-inset">Mycroft will create a virtual environment it needs to run. It also isolates the Mycroft package from the rest of the packages on the Jetson Nano.</p></li> </ol>
<p class="callout-heading">Note</p>
<p class="callout">Please do not install Mycroft as the root user (or superuser). This will cause permissions problems with the configuration files.</p>
<ol>
<li value="2">In order to get the Mycroft system to work in this manner, I also had to do one more step. The Mycroft system kept failing when I first tried to get it to run. It would quit or get stuck when I tried to start the debugger. In order to correct this problem, I had to recompile the entire system using the following steps:<pre class="source-code">
<strong class="bold">sudo rm -R -/.virtualenvs/Mycroft</strong>
<strong class="bold">cd ~/mycroft-core</strong>
<code>mycroft-core</code> directory.</p></li> <li>You can <a id="_idIndexMarker478"/>start in debug mode:<pre class="source-code">
<code>Time</code> skill is totally self-contained inside the Jetson Nano. The robot should give you a voice response that is replicated on the debug console.</p></li>
<li>Next, you can ask Mycroft a more advanced skill, such as looking up information on the internet. Ask, “Hey, Mycroft, how many ounces in a gallon?” Mycroft will use the internet to look up the answer and reply.</li>
<li>Next, you can change the wake word on the Mycroft website to something more appropriate – we did not name this robot Mycroft. We have been calling this robot Albert, but you can choose to call the robot anything you want. You may find <a id="_idIndexMarker480"/>that a very short name <a id="_idIndexMarker481"/>such as Bob is too quick to be a good wake word, so pick a name with at least two syllables. To do this, navigate to the Mycroft web page (<a href="http://home.mycroft.ai">http://home.mycroft.ai</a>) and log in to your account, which we created back in <em class="italic">Step 4</em>. Click on your name in the upper right corner and select <strong class="bold">Settings</strong> from the menu. You can select several settings on this page, such as the type of voice you want, the units of measurement, and time and date formats. Select <strong class="bold">Advanced Settings</strong>, which will take you to the page where we can change the wake word.</li>
<li>We change the first field, the <strong class="bold">Wake word</strong> field, to <strong class="bold">Custom</strong>. We change the next line to put in our custom wake word – “Hey, Albert.”</li>
<li>We also need to look up the phonemes for this wake word. Go to <em class="italic">The CMU Pronouncing Dictionary</em> from Carnegie Mellon University (<a href="http://www.speech.cs.cmu.edu/cgi-bin/cmudict">http://www.speech.cs.cmu.edu/cgi-bin/cmudict</a>). Put in our phrase and you will get out the phoneme phrase. Copy and paste this phrase and go back to the Mycroft page to paste the phoneme phrase into the <strong class="bold">Phonemes</strong> field. You are done – don’t change any of the other settings.</li>
<li>Hit <strong class="bold">Save</strong> at the top of the page before you navigate away.</li>
</ol>
<p>You can test your new wake word back on the Jetson Nano. Start Mycroft up again in debug mode and wait for it to come up. Say your new wake phrase and enjoy the response. I have a standard test set of phrases to show Mycroft’s skill at being the voice of our robot. Try the following:</p>
<ul>
<li>Hey, Albert. What time is it?</li>
<li>Hey, Albert. What is the weather for tomorrow? Hey, Albert. How many ounces in a gallon?</li>
<li>Hey, Albert. Who is the king of England?</li>
</ul>
<p>You should get the appropriate answers to these questions.</p>
<p>Mycroft has <a id="_idIndexMarker482"/>many other skills that we <a id="_idIndexMarker483"/>can take advantage of, such as setting a timer, setting an alarm, listening to music on Pandora, or playing the news. What we will be doing next is adding to these skills by creating our own that are specific to our room-cleaning robot.</p>
<h2 id="_idParaDest-109"><a id="_idTextAnchor216"/>Adding skills</h2>
<p>The first skill <a id="_idIndexMarker484"/>we will create is a command to pick up toys. We are going to connect this command to the ROS to control the robot. Later on, we will add a skill to tell knock-knock jokes.</p>
<h3>Designing our dialogs</h3>
<p>Our first step is to design our dialog on how we will talk to the robot. Start by making a list of what <a id="_idIndexMarker485"/>ways you might tell the robot to pick up the toys in the playroom. Here is my list, which I generated using ChatGPT (version 3.5):</p>
<ul>
<li>Hey robot, could you please start picking up all the toys?</li>
<li>It’s time to tidy up. Can you gather all the toys for me?</li>
<li>Need your help, robot. Could you please pick up all the toys and put them in the toy bin?</li>
<li>Let’s clean up together. Begin by collecting all the toys, please.</li>
<li>I need a clean room. Can you please start by picking up the toys scattered around?</li>
<li>Dear robot, would you be so kind as to tidy up the room by gathering all the toys?</li>
<li>Let’s make <a id="_idIndexMarker486"/>this room spotless. Begin with collecting all the toys, please.</li>
<li>It’s clean-up time.</li>
<li>Can you pick up all the toys?</li>
<li>Pick up all the toys.</li>
<li>You could collect all the toys and place them in the toy box.</li>
<li>It’s time to showcase your cleaning skills. Start by picking up all the toys, please.</li>
<li>Could you please make the room tidy by picking up all the toys?</li>
</ul>
<p>You will note that there are several keywords that are specific to the command to clean up the room. We have the word “clean,” of course. We have the phrase “pick up,” and “away.” We also have the words “toys” or “toy”, and finally, “mess.” These keywords will cue the natural language processor and allow some variation in the exact words used.</p>
<p>Next, we write down what we want the robot to say back. We don’t want the same canned response each time; it would be good to have some natural variation in the robot’s responses. Here is my list of responses, created again by ChatGPT3, with a variety of robot attitudes represented:</p>
<ul>
<li>Command received – picking up toys.</li>
<li>Of course, I will start cleaning the room right away.</li>
<li>Understood, I will tidy up the room and ensure everything is in its proper place.</li>
<li>Cleaning mode activated! I will make sure to leave your room spotless and organized.</li>
<li>As you wish, I will take care of cleaning the room for you.</li>
<li>No problem, I will use my cleaning capabilities to efficiently tidy up the room and remove any clutter.</li>
<li>Consider it done! I will sweep, dust, and mop the room to make it pristine for you.</li>
<li>Cleaning in progress! I will diligently clean up the room and ensure it’s tidy and presentable.</li>
<li>I’m on it! I will use my cleaning tools and techniques to make your room look spick and span.</li>
<li>Acknowledged, I will clean up the room and ensure it’s organized and ready for your use.</li>
<li>Cleaning task accepted! I will work diligently to make your room clean and inviting, just the way you like it.</li>
</ul>
<p>You can use <a id="_idIndexMarker487"/>as many of these as you want. The Mycroft system will grab a random phrase from this list. This gives us some room for creativity and gives the illusion that the robot is more intelligent than it really is. This type of response system enables us to quickly develop our dialogs.</p>
<p>In the next section, we’ll look at how we can connect voice commands to the ROS interface.</p>
<h3>Creating skills</h3>
<p>In this section, we’ll build the framework in Mycroft that allows it to associate our spoken phrases with the commands to be sent to the robot. This will involve adding each skill that we want the robot to possess.</p>
<h4>Cleaning up the toys</h4>
<p>First, let’s <a id="_idIndexMarker488"/>add the most basic skill for Albert – cleaning the room by picking up toys. To do this, follow these steps:</p>
<ol>
<li>Use the <code>mycroft-msk create</code> command, which helps us put together our skills in the proper format:<pre class="source-code">
<code>cleanroomrobot-skill</code>.</li>
<li>Then it will ask for a class name and a repository name, for both of which I used <code>Cleanroomrobot</code>.</li>
<li>Enter a one-line description for your skill: <code>Pick up all of the toys in </code><code>the room</code>.</li>
<li>Enter a long description, such as <code>Command the robot to detect toys, move to grab a toy, pick it up, and put it into </code><code>the toybox</code>.</li>
<li>Enter <a id="_idIndexMarker489"/>some example phrases to trigger your skill:<ul><li><code>Hey robot, could you please start picking up all </code><code>the toys?</code></li><li><code>It's time to tidy up. Can you gather all the toys </code><code>for me?</code></li><li><code>Can you pick up all </code><code>the toys?</code></li></ul></li>
<li>Enter the following parameters:<ul><li><code>&lt;your </code><code>name here&gt;</code></li><li><code>Productivity</code></li><li><code>IoT</code></li></ul></li>
<li>Entering tags makes it easier to search for your skill (although this is optional): <code>robot</code>, <code>cleanup</code>, <code>pick up</code>, and <code>toys</code>.</li>
<li>We will end up with a directory structure in <code>/opt/Mycroft/skills/cleanroomrobot-skill</code> like the following:<pre class="source-code">
<strong class="bold">Cleanroomrobot-skill</strong>
<strong class="bold">   Git</strong>
<strong class="bold">   __pycache__</strong>
<strong class="bold">  Locale</strong>
<strong class="bold">    En-us</strong>
<strong class="bold">        cleanroomrobot.dialog</strong>
<strong class="bold">        cleanroomrobot.intent</strong>
<strong class="bold">__init__.py</strong>
<strong class="bold">LISCENSE.md</strong>
<strong class="bold">Manifest.yml</strong>
<strong class="bold">README.md</strong>
<code>init.py</code> file in the <code>skill_pickup_toys</code> directory that we copied from the template.</li>
<li>We are going to import the libraries we need from Mycroft (<code>IntentBuilder</code>, <code>MycroftSkill</code>, <code>getLogger</code>, and <code>intent_handler</code>). We also import <code>rclpy</code>, the ROS Python interface, and the ROS standard message <code>String</code>, which we use to send commands to the robot by publishing on the <code>syscommand</code> topic:<pre class="source-code">
from mycroft import MycroftSkill, intent_handler, intent_file_handler
import rclpy
from rclpy.node import Node
from std_msgs.msg import String, Int32MultiArray, Int32
from adapt.intent import IntentBuilder
from mycroft.util.log import getLogger</pre></li> </ol>
<p class="callout-heading">Note</p>
<p class="callout"><code>MycroftSkill</code> is a function that is called when one of its phrases is recognized by the Mycroft Intent Engine. As such, it has no body or main function, just a function definition for the <code>create_skill</code> function that instantiates a <code>MycroftSkill</code> object. The <code>init</code> function does most of the work of setting up the various dialogs, intent handlers, and vocabulary for the skill. This arrangement works very well in our limited environment of giving the robot commands or telling jokes.</p>
<ol>
<li value="12">The next line is the logger for Mycroft so that we can save our responses. Anything <a id="_idIndexMarker491"/>we put out to stdout, such as print statements, will end up in the log, or on the screen if you are in debug mode:<pre class="source-code">
<code>Cleanroomrobot</code> to match what we defined previously:<pre class="source-code">
class Cleanroomrobot(MycroftSkill):
    def __init__(self):
        MycroftSkill.__init__(self)
    def setRobotInterface(self,interfce):
        self.interface = interfce
    def initialize(self):
        pass  # just return for now</pre></li> <li>We set up the publisher for our <code>syscommand</code> topic in the ROS. This is how we send commands to the robot control program via the ROS publish/subscribe system. We will be publishing commands only, and the only message format we need is <code>String</code>:<pre class="source-code">
pub = rospy.Publisher('/syscommand', String, queue_size=1000)
# define our service for publishing commands to the robot control system # all our robot commands go out on the topic syscommand
def pubMessage(str):
pub.publish(str)</pre></li> <li>Our Mycroft <a id="_idIndexMarker492"/>skill is created as a child object of the <code>MycroftSkill</code> object. We rename our skill object class to <code>CleanRoomSkill</code>:<pre class="source-code">
class CleanRoomSkill(MycroftSkill):
def   init  (self):
super(CleanRoomSkill, self).  init  (name="PickupToys")</pre><p class="list-inset">According to the template, Mycroft requires both an <code>init</code> method and an <code>initialize</code> method. These commands set up the intent in the Intent Builder part of Mycroft and register our handler when any of our phrases are spoken.</p></li> <li>Next, we refer to the dialogs we built back in the <em class="italic">Creating skills</em> section with <code>require("CleanRoomKeyword")</code>, so be careful that all the spelling is correct:<pre class="source-code">
def initialize(self):
        clean_room_intent = IntentBuilder("cleanroomrobot").require("cleanroomrobot").build()
        self.register_intent(clean_room_intent, self.handle_cleanroomrobot)</pre></li> <li>This next section creates our handler for when the system has recognized one of our phrases, and we want to perform the action for this command. This is where we kick off the publish command to the robot’s control program via the ROS using the <code>pubMessage</code> function we defined earlier:<pre class="source-code">
    @intent_file_handler('cleanroomrobot.intent')
    ##@intent_handler('cleanroomrobot.intent')
    def handle_cleanroomrobot(self, message):
        self.speak_dialog('cleanroomrobot')
        self.interface.cmdPublisher("CleanRoom")</pre></li> <li>We also <a id="_idIndexMarker493"/>need a <code>stop</code> function, where we can command the robot to stop cleaning, if necessary, to prevent any sort of <em class="italic">Mickey Mouse – Sorcerer’s </em><em class="italic">Apprentice</em> accident:<pre class="source-code">
    def stop(self):
        self.interface.cmdPublisher("STOPCleanRoom")
        pass</pre></li> </ol>
<p class="callout-heading">Note</p>
<p class="callout">In the movie <em class="italic">Fantasia</em>, Mickey Mouse acts out the part of the Sorcerer’s Apprentice from a fairy tale. In the story, the Apprentice learns to animate a broom, which he teaches to fetch water from a well. The problem is the Apprentice never learned how to stop the enchantment, and soon the room is flooded.</p>
<ol>
<li value="19">We now need a block of code to create the skill in the program where we can associate the ROS interface to the robot into the skill. We will add a <code>create_skill</code> function to allow Mycroft to create the skill and to have a function pointer to enable the skill:<pre class="source-code">
def create_skill():
    newSkill = Cleanroomrobot()
    newSkill.setRobotInterface(rosInterface())
    return newSkill</pre></li> <li>Next, we have the ROS interface. All we need to do is send a command to the robot to publish mode commands on our <code>RobotCmd</code> topic:<pre class="source-code">
class rosInterface(Node):
    def __init__(self):
        super().__init__('mycroftROS') # node name
        self.cmdSubscribe = self.create_subscription(String, 'RobotCmd', self.cmdCallback,10)
        self.cmdPublisher = self.create_publisher(String, 'RobotCmd', 10)
    def cmdCallback(self,msg):
        robotCmd = msg.data</pre><p class="list-inset">We define <a id="_idIndexMarker494"/>our ROS interface and create a control node called <code>mycroftROS</code> to serve as our interface. Then we create a subscriber and publisher to the <code>RobotCmd</code> topic so we can send and receive commands from the ROS 2 interface.</p></li> <li>The rest of the program is just housekeeping. We need to start up our ROS node, start the Mycroft logger, and instantiate the ROS interface object and the <code>cleanSkill</code> objects for ROS and Mycroft, respectively. Then we point the <code>cleanSkill</code> object to the ROS interface so they can communicate. Finally, we start the ROS 2 interface with the <code>.spin</code> function. When the program is stopped, we fall out of <code>.spin</code> and shut down our program:<pre class="source-code">
## main ###
rclpy.init()
LOGGER = getLogger(__name__)
interface = rosInterface()
cleanSkill = Cleanroomrobot()
cleanSkill.setRobotInterface(interface)
rclpy.spin(interface)
rosInterface.destroy_node()
rclpy.shutdown()</pre></li> <li>In order <a id="_idIndexMarker495"/>for our skill to work, we need to copy our directory to <code>/opt/mycroft/skills</code>. From there, we can test it in debug mode. Remember that you have to source the ROS 2 directory (<code>source /opt/ros/foxy/local_setup.sh</code> and <code>source ~/ros2_ws/install/local_setup.sh</code>) or the program won’t be able to find all of the inclusion files or ROS nodes.</li>
</ol>
<p>Our next skill comes at the request of my grandson, William, who just adores knock-knock jokes. William is seven, so he is just the right age for this. Let’s look at how we can implement this.</p>
<h4>Telling jokes</h4>
<p>In this section, we will handle the case where the robot is telling the knock-knock joke. As you <a id="_idIndexMarker496"/>probably know, knock-knock jokes are pun-based jokes that always take the same form:</p>
<p class="uthor-quote">Person 1: Knock, knock</p>
<p class="uthor-quote">Person 2: Who’s there?</p>
<p class="uthor-quote">Person 1: Wooden</p>
<p class="uthor-quote">Person 2: Wooden Who?</p>
<p class="uthor-quote">Person 1: Wooden you like to know!</p>
<p>As you can see, the dialog is very simple. Several parts of it are standard, such as the first two lines – “Knock, knock” and “Who’s there?” We can create a generic knock-knock joke in the following form:</p>
<ol>
<li>Knock, knock.</li>
<li>Who’s there?</li>
<li><code>&lt;</code><code>word 1&gt;</code></li>
<li><code>&lt;word </code><code>1&gt;</code> who?</li>
<li><code>&lt;</code><code>punchline phrase&gt;</code></li>
</ol>
<p>In defining our joke, you can see we just have two variable elements – the word in <em class="italic">Step 3</em>, and the punchline phrase in <em class="italic">Step 5</em>. Our word is repeated in <em class="italic">Step 4</em>.</p>
<p>We begin <a id="_idIndexMarker497"/>by creating a joke database of one-line jokes, which we will put in a text file. Since we just have two elements, we can separate them with a slash (<code>/)</code>. Here is an example:</p>
<pre class="source-code">
tarzan / tarzan stripes forever
orange / orange you glad I can tell jokes?</pre> <p>I’m providing you with a database of about 10 jokes in the files section of the repository for this chapter. Please feel free to add all of your favorites, or send them to me and I’ll add them.</p>
<p>Now, let’s look at the steps involved in telling the joke:</p>
<ol>
<li>We will start, as with any skill, with the wake word, “Hey, Albert.”</li>
<li>Then we need a phrase to indicate we want to hear a joke, so we will use variations of “Tell me a knock-knock joke,” such as “I want to hear a knock-knock joke.”</li>
<li>This will trigger our skill program to look up a joke. We will create several intents, or response capabilities, to respond to the user (or child) talking to the robot. We will start with the “Who’s there?” dialog intent. That will let the robot know to proceed to the next part of the joke, which is to say our word.</li>
<li>Then we disable the “Who’s there?” dialog and enable a dialog for listening for <code>&lt;word&gt;</code> and the phrase “who.”</li>
<li>Then we can deliver the final part of the joke by reciting the punchline phrase, and we are done.</li>
</ol>
<p>How can we implement this? You can follow these steps:</p>
<ol>
<li>We start by creating our vocabulary files, of which we will need three. These are the things that the user will be saying to the robot. We have our first “tell me a knock, knock joke” phrase – so let’s create a file called <code>knockknock.voc</code> (you can use any text editor to create the file) and put the following in it:<pre class="source-code">
Tell me a knock-knock joke Can I have a knock-knock joke Give me a knock-knock joke Play me a knock-knock joke</pre><p class="list-inset">Please note that the Mycroft STT system interprets the phrase “knock, knock” as <code>knock-knock</code> with a hyphen, so it is important to put that into our script.</p></li> <li>Now our <a id="_idIndexMarker498"/>second vocabulary is just “Who’s there,” so we can create this as a second <code>.voc</code> file, <code>whosthere.voc</code>, which contains the line <code>Whos there</code>.</li>
<li>Our final line is a bit trickier. We really only care about the keyword “who” to trigger the punchline, so we can look only for that. Make a file called <code>who.voc</code> and put the one word <code>who</code> in it. Remember these all go in the <code>dialog/en-us</code> folder in our <code>skill</code> directory.</li>
<li>Now for our responses. We have one canned response, which is to reply to “tell me a knock-knock joke” with the phrase “knock, knock.” We don’t need any sophisticated dialog system, we just have the robot say the “knock, knock” phrase. To do this, we first import the libraries we need to call in this program, which are the <code>MycroftSkill</code> class and the <code>intent_file_handler</code> function:<pre class="source-code">
from mycroft import MycroftSkill, intent_file_handler</pre></li> <li>We define our skill as a child object of the <code>MycroftSkill</code> object – this is a standard object-oriented design. We are inheriting all of the functions and data of the <code>MycroftSkill</code> parent object and adding our own functionality to it. We create an initialize function and then call the <code>init</code> parent function to execute the code of the parent class as well. We are augmenting the functionality of the <code>init</code> parent function. Without this call, we would be replacing the <code>init</code> function with our own, and might have to duplicate a considerable amount of work:<pre class="source-code">
class Knockknock(MycroftSkill):
  def __init__(self):
     MycroftSkill.__init__(self)</pre></li> <li>The next step is to create our <code>knockknock.intent</code> file and place that file in the <code>voc</code> directory (which was <code>dialog/voc-en</code>):<pre class="source-code">
@intent_file_handler('knockknock.intent') 
def handle_knockknock(self, message):
name,punchline = self.pick_joke()</pre><p class="list-inset">Here, we get two parts from the joke database:</pre><ul><li>The name to say after “who’s there”</li><li>The punchline that ends the joke</li></ul></li> <li>We use the <code>get_response</code> function from <code>MycroftSkill</code> to have the robot make a statement and then wait for a reply, which will get turned into a text string and stored in the <code>response</code> variable:<pre class="source-code">
response=self.get_response(announcement="knock, knock") 
# response will always be "who's there" 
response=self.get_response(announcement=name)</pre></li> <li>Now we are at the part where the robot says the name in response. For example, the user asks “who’s there?” and the robot replies “Harold.” What we are expecting next is for the user to say “Harold (or whatever name) who?” We will check our response, and see whether the word “who” is included. If it is not, we can <a id="_idIndexMarker500"/>prompt the user to follow along with the joke. We will only do this one time to keep from getting stuck in a loop. If they are not playing along, the robot will just continue:<pre class="source-code">
# response will be "name who"
# if end of respose is not the word who, we can re-prompt 
if "who" not in response:
  prompt = "You are supposed to say "+name+" who" 
  response=self.get_response(announcement=prompt)</pre></li> <li>We have moved through the joke, so now we get to say the punchline, such as “Harold you like a hug?” (How would you like a hug?). The task is complete and we exit the routine; both the comedy routine and the program routine:<pre class="source-code">
self.speak(punchline)</pre></li> <li>We need a function to read the joke database we defined earlier. As described earlier, the database has one knock-knock joke per line, with a forward slash (<code>/</code>) between the name and the punchline. We read all of the jokes, put them in a list, and then choose one at random using the (wait for it) <code>random.choice</code> function. We return the name and the punchline separately. We should only call this routine once per instance of the joke:<pre class="source-code">
def pick_joke(): 
  jokeFile="knockknock.jokes" 
  jfile = open(jokeFile,"r") 
  jokes = []
  for jokeline in jfile:
    jokes.append(jokeline)
  joke = choice(jokes) 
  jokeParts = joke.split("/") 
  name = jokeParts[0] 
  punchline = jokeParts[1] 
  return name, punchline</pre></li> <li>We finish the program by defining our instance of the <code>Knockknock</code> class and returning that object to the calling program, Mycroft:<pre class="source-code">
def create_skill():
  return Knockknock()</pre></li> </ol>
<p>Next, we’ll discuss the other end of the knock-knock joke concept, which is to receive a joke – where the <a id="_idIndexMarker501"/>child wants to tell the robot a joke. If you know any seven-year-olds, then you know that this is a requirement also – the child will want to tell the robot a joke as well.</p>
<h4>Receiving jokes</h4>
<p>The receiving dialog is pretty simple as well. The user will say “knock, knock”, which is the cue <a id="_idIndexMarker502"/>to the robot to go into the <em class="italic">receive knock-knock joke</em> mode. The robot then has only one response – “who’s there.” We could also add “who is there?” if we want to keep to the common sci-fi concept that robots do not use contractions.</p>
<p class="callout-heading">Note</p>
<p class="callout">Data, the android from <em class="italic">Star Trek: The Next Generation</em>, stated many times he was not able to use contractions, although the writers slipped up from time to time.</p>
<p>In order to <a id="_idIndexMarker503"/>create our schema for our new Mycroft skill, we will be using the <code>pip3 install msk</code>. MSK provides a dialog-driven approach to building skills that will make a framework, including all of the subdirectories for dialog files and vocabulary. This saves a lot of time, so let’s try it out:</p>
<ol>
<li>The following is the command for creating the <em class="italic">receive knock-knock </em><em class="italic">joke</em> code:<pre class="source-code">
<strong class="bold">$ msk create</strong>
<strong class="bold">Enter a short unique skill name (ie. "siren alarm" or "pizza orderer"): receiveKnock</strong>
<strong class="bold">Class name: ReceiveKnockSkill</strong>
<strong class="bold">Repo name: receive-knock-skill</strong>
<strong class="bold">Looks good? (Y/n) y</strong>
<strong class="bold">Enter some example phrases to trigger your skill:</strong>
<strong class="bold">knock knock</strong>
<strong class="bold">-</strong>
<strong class="bold">Enter what your skill should say to respond:</strong>
<strong class="bold">who's there</strong>
<strong class="bold">Enter a one line description for your skill (ie. Orders fresh pizzas from the store): This skill receives a knock knock joke from the user</strong>
<strong class="bold">Enter a long description:</strong>
<strong class="bold">This is the other half of the Knock Knock joke continuum - we are giving the robot the ability to receive knock knock jokes. The user says knock knock, the robot responds whos there and so on</strong>
<strong class="bold">&gt;</strong>
<strong class="bold">Enter author: Francis Govers</strong>
<strong class="bold">Would you like to create a GitHub repo for it? (Y/n) Y</strong>
<strong class="bold">=== GitHub Credentials === Username: ********** Password:*********</strong>
<strong class="bold">Counting objects: 12, done.</strong>
<strong class="bold">Delta compression using up to 4 threads. Compressing objects: 100% (5/5), done.</strong>
<strong class="bold">Writing objects: 100% (12/12), 1.35 KiB | 0 bytes/s, done. Total 12 (delta 0), reused 0 (delta 0)</strong>
<strong class="bold">To https://github.com/FGovers/receive-knock-skill</strong>
<strong class="bold">* [new branch] master -&gt; master</strong>
<code>/opt/Mycroft/skills/receive-knock-skill</code>. The program is still the <code>init.py</code> file.</li>
<li>We start with our imports, which are <code>MycroftSkill</code> and <code>intent_file_handler</code>. We will also need the <code>time</code> library to do some pauses:<pre class="source-code">
from mycroft import MycroftSkill, intent_file_handler import time</pre></li> <li>Here is our class definition for our <code>ReceiveKnock</code> class, which is a child class of the <code>MycroftSkill</code> object we imported. We start the <code>init</code> function by passing an <code>init</code> command back up to the parent class (<code>MycroftSkill</code>) and have it do its initialization. Then we add our custom functionality on top of that:<pre class="source-code">
class ReceiveKnock(MycroftSkill):
  def __init__(self):
       MycroftSkill.__init__(self)</pre></li> <li>The next section is our intent handler for receiving a knock-knock joke. We use the <code>@decorator</code> to extend the intent handler, in this case, reading the parameters of the intent from a file called <code>knock.receive.intent</code>. The intent handler just has our two key words, the immortal phrase: <code>knock, knock</code>. We are fortunate that all jokes start exactly the same way, so we only have these two words.<p class="list-inset">After the <code>handle_knock_receive</code> function has been activated by the Intent Engine seeing the phrase “knock, knock,” we then get control passed to our handler. What is our next step? We reply with the single answer “Who is there?” You will remember we said robots do not use contractions. We use a different function to do this. We don’t want to use another intent handler, but fortunately, Mycroft provides a free-form interface called <code>get_response</code>. You need <a id="_idIndexMarker505"/>to look up the documentation for this versatile function, but it makes our joke routine a lot simpler. The <code>get_response</code> function both lets us speak our reply and then receive whatever the user says next and store it as a string in the <code>response</code> variable:</p><pre class="source-code">
@intent_file_handler('knock.receive.intent') 
def handle_knock_receive(self, message):
  response =self.get_response('who.is.there')</pre><p class="list-inset">Now that we have our response, we can just repeat it back with the robot’s voice, with the additional word “who?” So, if the child says, “Howard,” the robot responds “Howard who?”</p></li> <li>We use <code>get_response</code> again to have the robot speak and then record whatever the child or adult says next. We don’t need it, but we want to have the robot’s speech system listen to whatever is said next. We toss away the response, but insert our own comment to the joke from our dialog <code>veryfunny.dialog</code>, which is a file in the <code>dialog</code> directory. I created this file to hold responses to our jokes from the robot. I tried to make some responses that the grandchildren would find funny – I guess I can add “robot joke writer” to my resume, as I seem to have done this a lot in my career. After this, I added a sleep timer to allow everything to settle down before returning control. We include the standard <code>stop</code> function required of all <code>MycroftSkills</code>, and make our <code>create_skill</code> function make a <code>ReceiveCall</code> object and return it:<pre class="source-code">
response2= response + " who?"
response3 =self.get_response(announcement=response2)
self.speak_dialog('veryfunny') 
time.sleep(3)
def stop(self):
  pass
def create_skill():
  return ReceiveKnock()</pre><p class="list-inset">You can <a id="_idIndexMarker506"/>get as creative as you want with the responses, but here are my suggestions:</p><ul><li>That was very funny!</li><li>Ha ha ha.</li><li>Very good joke.</li><li>I like that one. Thank you!</li><li>Ho HO! Ho.</li><li>That was cute!</li><li>I do not have a sound for a groan thththththpppppp!</li></ul></li> </ol>
<p>Here is our directory structure and files for our receive knock-knock jokes skill:</p>
<pre class="source-code">
receive-knock-skill directory:
init .py README.md
settingsmeta.json
./dialog/en-us:
knock.receive.dialog veryfunny.dialog
./vocab/en-us:
knock.receive.intent</pre> <p>Remember the local version of the skill goes in the <code>/opt/mycroft/skills/receive-knock-skill</code> directory. Now test to your heart’s content – how many knock-knock jokes can you tell the robot?</p>
<h1 id="_idParaDest-110"><a id="_idTextAnchor217"/>Summary</h1>
<p>This chapter introduced NLP for robotics and concentrated on developing a natural language interface for the robot that accomplished three tasks: starting the <em class="italic">pick up toys</em> process, telling knock-knock jokes, and listening to knock-knock jokes.</p>
<p>The concepts introduced included recognizing words by phonemes, turning phonemes into graphemes and graphemes into words, parsing intent from sentences, and executing computer programs with a voice interface. We introduced the open source AI engine, Mycroft, which is an AI-based voice assistant program that runs on the Jetson Nano. We also wrote a joke database to entertain small children with some very simple dialog.</p>
<p>In the next chapter, we’ll be learning about <strong class="bold">robot navigation</strong> using landmarks, neural networks, obstacle avoidance, and machine learning.</p>
<h1 id="_idParaDest-111"><a id="_idTextAnchor218"/>Questions</h1>
<ol>
<li>Do some internet research on why the AI engine was named Mycroft. How many different stories did you find, and which one did you like?</li>
<li>In the discussion of intent, how would you design a neural network to predict command intent from natural language sentences?</li>
<li>Rewrite “Receive knock-knock jokes” to remember the jokes told to the robot by adding them to the joke database used by the “tell knock knock jokes” program. Is this machine learning?</li>
<li>Modify the “tell jokes” program to play sounds from a wave file, such as a music clip, as well as doing TTS.</li>
<li>The sentence structures used in this chapter are all based on English grammar. Other languages, such as French and Japanese, have different structures. How does that change the parsing of sentences? Would the program we wrote be able to understand Yoda?</li>
<li>Do you think that Mycroft’s Intent Engine is actually understanding intent, or just pulling out keywords?</li>
<li>Describe the voice commands necessary to instruct the robot to drive to an object and pick it up without the robot being able to recognize the object. How many commands do you need?</li>
<li>From <em class="italic">Question 7</em>, work to minimize the number of commands. How many can you eliminate or combine?</li>
<li>Also from <em class="italic">Question 7</em>, how many unique keywords are involved? How many non-unique keywords?</li>
</ol>
<h1 id="_idParaDest-112"><a id="_idTextAnchor219"/> Further reading</h1>
<ul>
<li><em class="italic">Python Natural Language Processing</em> by Jalaj Thanaki, Packt Publishing</li>
<li><em class="italic">Artificial Intelligence with Python</em> by Prateek Joshi, Packt Publishing</li>
<li>Mycroft tutorial for developing skills is located at <a href="https://mycroft.gitbook.io/mycroft-docs/developing_a_skill/introduction-developing-skills">https://mycroft.gitbook.io/mycroft-docs/developing_a_skill/introduction-developing-skills</a></li>
<li>Additional documentation for using Mycroft is located at <a href="https://media.readthedocs.org/pdf/mycroft-core/stable/mycroft-core.pdf">https://media.readthedocs.org/pdf/mycroft-core/stable/mycroft-core.pdf</a></li>
</ul>
</div>


<div><h1 id="_idParaDest-113" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor220"/>Part 3: Advanced Concepts – Navigation, Manipulation, Emotions, and More</h1>
<p>In the last part of the book, we tackle more advanced topics including AI-based navigation and obstacle avoidance. We learn about decision trees and classification algorithms for unsupervised learning and then start an exciting chapter on creating a simulation of a robot personality. While we can’t give a robot real emotions, we can create a simulation of emotion using state machines and Monte Carlo techniques. Finally, we end the book with a discussion of AI philosophy and a look at the future from the author’s perspective, and provide advice for people wanting to pursue robotics and autonomy as a career.</p>
<p>This part has the following chapters:</p>
<ul>
<li><a href="B19846_07.xhtml#_idTextAnchor221"><em class="italic">Chapter 7</em></a>, <em class="italic">Teaching the Robot to Navigate and Avoid Stairs</em></li>
<li><a href="B19846_08.xhtml#_idTextAnchor235"><em class="italic">Chapter 8</em></a>, <em class="italic">Putting Things Away</em></li>
<li><a href="B19846_09.xhtml#_idTextAnchor294"><em class="italic">Chapter 9</em></a>, <em class="italic">Giving the Robot an Artificial Personality</em></li>
<li><a href="B19846_10.xhtml#_idTextAnchor366"><em class="italic">Chapter 10</em></a>, <em class="italic">Conclusions and Reflections</em></li>
</ul>
</div>
<div><div></div>
</div>
</body></html>