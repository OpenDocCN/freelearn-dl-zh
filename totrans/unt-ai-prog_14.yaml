- en: '*Chapter 11*: Machine Learning in Unity'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第11章*：Unity中的机器学习'
- en: 'Machine learning is the hottest buzzword in **Artificial Intelligence** (**AI**).
    Nowadays, everything contains (or claims to contain) some machine learning-powered
    AI that is supposed to improve our life: calendars, to-do apps, photo management
    software, every smartphone, and much more. However, even if the phrase *machine
    learning* is just a marketing gimmick most of the time, it is without question
    that machine learning has improved significantly in recent years. Most importantly,
    though, there are now plenty of tools that allow everybody to implement a learning
    algorithm without any previous academic-level AI knowledge.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是**人工智能**（**AI**）中最热门的术语。如今，几乎所有东西都包含（或声称包含）一些由机器学习驱动的AI，旨在改善我们的生活：日历、待办事项应用、照片管理软件、每部智能手机等等。然而，尽管“机器学习”这个短语大多数时候只是一个营销噱头，但毫无疑问的是，机器学习在近年来已经取得了显著的进步。最重要的是，现在有大量的工具允许每个人在没有先前的学术级AI知识的情况下实现学习算法。
- en: At the moment, machine learning is not used in game development (except for
    applications for procedural content generation). There are many reasons for that.
    The main reason, though, is that a designer can't control the output of a machine
    learning agent, and in game design, uncontrollable outcomes often correlate to
    not-fun games. For this reason, game AI developers prefer more predictable and
    straightforward techniques, such as behavior trees.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，机器学习在游戏开发中并未得到广泛应用（除了用于程序内容生成的应用）。这有很多原因。但主要原因是设计师无法控制机器学习代理的输出，而在游戏设计中，不可控的结果通常与不有趣的游戏相关。因此，游戏AI开发者更倾向于使用更可预测和直接的技术，例如行为树。
- en: On the other hand, being able to use machine learning algorithms in Unity is
    useful for non-gaming purposes, such as simulations, AI research, and *some serious*
    gaming applications. Whatever the reason, Unity provides a complete toolkit for
    machine learning that spares us the complication of interfacing the game engine
    with an external machine learning framework.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，能够在Unity中使用机器学习算法对于非游戏用途非常有用，例如模拟、AI研究和*一些严肃的*游戏应用。无论原因如何，Unity提供了一个完整的机器学习工具包，使我们免于将游戏引擎与外部机器学习框架接口的复杂性。
- en: 'In this chapter, we will look at the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨以下主题：
- en: An introduction to the Unity Machine Learning Agents Toolkit
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Unity机器学习工具包简介
- en: Setting up the Unity Machine Learning Agents Toolkit
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置Unity机器学习代理工具包
- en: Seeing how to run a simple example
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 看看如何运行一个简单的示例
- en: Machine learning is an extensive topic; therefore, we do not expect to cover
    every single aspect of it. Instead, look at the toolkit documentation and the
    additional resources linked at the end of this chapter for further reference.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是一个广泛的话题；因此，我们并不期望涵盖它的每一个方面。相反，您可以查看工具包文档以及本章末尾链接的附加资源以获取进一步参考。
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'For this chapter, you need Unity3D 2022, Python 3.7, PyTorch, and the ML-Agents
    Toolkit installed on your system. Don''t worry if you don''t; we will go over
    the installation steps. You can find the example project described in this chapter
    in the `Chapter 11` folder in the book''s repository: [https://github.com/PacktPublishing/Unity-Artificial-Intelligence-Programming-Fifth-Edition/tree/main/Chapter11](https://github.com/PacktPublishing/Unity-Artificial-Intelligence-Programming-Fifth-Edition/tree/main/Chapter11)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，您需要在您的系统上安装Unity3D 2022、Python 3.7、PyTorch和ML-Agents工具包。如果您还没有安装，请不要担心；我们将介绍安装步骤。您可以在本书的仓库中的`第11章`文件夹中找到本章描述的示例项目：[https://github.com/PacktPublishing/Unity-Artificial-Intelligence-Programming-Fifth-Edition/tree/main/Chapter11](https://github.com/PacktPublishing/Unity-Artificial-Intelligence-Programming-Fifth-Edition/tree/main/Chapter11)
- en: The Unity Machine Learning Agents Toolkit
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Unity机器学习代理工具包
- en: The **Unity Machine Learning Agents Toolkit** (**ML-Agents Toolkit**) is a collection
    of software and plugins that help developers write autonomous game agents powered
    by machine learning algorithms. You can explore and download the source code at
    the GitHub repository at [https://github.com/Unity-Technologies/ml-agents](https://github.com/Unity-Technologies/ml-agents).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**Unity机器学习代理工具包**（**ML-Agents Toolkit**）是一组软件和插件，帮助开发者编写由机器学习算法驱动的自主游戏代理。您可以在GitHub仓库[https://github.com/Unity-Technologies/ml-agents](https://github.com/Unity-Technologies/ml-agents)中探索和下载源代码。'
- en: The ML-Agents Toolkit is based on the reinforcement learning algorithm. Simplistically,
    reinforcement learning is the algorithmic equivalent of training a dog. For example,
    if you want to teach a dog some trick, you give him a command, and then, when
    the dog does what you expect, you reward him. The reward tells your dog that it
    responded correctly to the command, and therefore, the next time it hears the
    same command, it will do the same thing to get a new reward.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ML-Agents工具包基于强化学习算法。简单来说，强化学习是训练狗的算法等价物。例如，如果您想教狗一些技巧，您给他一个命令，然后，当狗按照您的期望行事时，您就奖励它。奖励告诉您的狗它正确地响应了命令，因此，下次它听到相同的命令时，它会做同样的事情以获得新的奖励。
- en: Note
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In reinforcement learning, you can also punish your agent when doing the wrong
    things, but in the dog-training example, I can assure you that punishment is entirely
    unnecessary. Just give them rewards!
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，您也可以在智能体做错事情时惩罚它，但在狗训练的例子中，我可以向您保证惩罚是完全不必要的。只需给他们奖励即可！
- en: 'For an AI agent trained with reinforcement learning, we perform a similar cycle:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 对于使用强化学习训练的AI智能体，我们执行类似的循环：
- en: When an agent acts, the action influences the world (such as changing the Agent's
    position, moving an object around, collecting a coin, gaining score points, and
    so on).
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当智能体采取行动时，该行动会影响世界（例如改变智能体的位置、移动物体、收集硬币、获得分数点等等）。
- en: The algorithm then sends the new world state back to the Agent with a reward
    (or punishment).
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 算法随后将新的世界状态和奖励（或惩罚）发送回智能体。
- en: When the Agent decides its following action, it will choose the action that
    maximizes the expected reward (or minimizes the expected punishment).
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当智能体决定其后续行动时，它将选择最大化预期奖励（或最小化预期惩罚）的行动。
- en: For this reason, it is clear that training a reinforcement learning agent requires
    several simulations of the scenario in which the Agent acts, receives a reward,
    updates its decision-making values, performs another action, and so on. This work
    is offloaded from Unity to Torch via PyTorch. **Torch** is a popular open source
    machine learning library used, among others, by tech giants such as Facebook and
    IBM.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，很明显，训练一个强化学习智能体需要模拟智能体采取行动、接收奖励、更新其决策值、执行另一个行动等场景的多次模拟。这项工作通过PyTorch从Unity卸载。**Torch**
    是一个流行的开源机器学习库，Facebook和IBM等科技巨头也在使用它。
- en: Refer to the *Further reading* section at the end of the chapter for more information
    on reinforcement learning.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 有关强化学习的更多信息，请参阅章节末尾的*进一步阅读*部分。
- en: Let's now see how to install the toolkit.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何安装这个工具包。
- en: Installing the ML-Agents Toolkit
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装ML-Agents工具包
- en: 'As a first step, we need to download the toolkit. We can do this by cloning
    the repository with the following command:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一步，我们需要下载这个工具包。我们可以通过以下命令克隆仓库来完成：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This command creates an `ml-agents` folder in your current folder. The ML-Agents
    Toolkit is composed of two main components:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令会在您的当前文件夹中创建一个`ml-agents`文件夹。ML-Agents工具包由两个主要组件组成：
- en: A Python package containing the Python interface for Unity and PyTorch's trainers
    (stored in the `ml-agents` folder)
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含Unity和PyTorch训练器的Python接口的Python包（存储在`ml-agents`文件夹中）
- en: A Python package containing the interface with OpenAI Gym ([https://gym.openai.com/](https://gym.openai.com/)),
    a toolkit for training reinforcement learning agents (stored in the `gym-unity`
    folder).
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含与OpenAI Gym（[https://gym.openai.com/](https://gym.openai.com/)）接口的Python包，OpenAI
    Gym是一个用于训练强化学习智能体的工具包（存储在`gym-unity`文件夹中）。
- en: Information
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 信息
- en: '**Git** is the most famous version-control application in the world. It is
    used to store your source code, keep track of different versions, collaborate
    with other people, and much more. If you are not already using Git, you should
    really check it out. You can download it from [https://git-scm.com/](https://git-scm.com/).'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**Git** 是世界上最著名的版本控制应用程序。它用于存储您的源代码，跟踪不同版本，与他人协作，等等。如果您还没有使用Git，您真的应该试试看。您可以从[https://git-scm.com/](https://git-scm.com/)下载它。'
- en: Now, it is time to install the required dependencies.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候安装所需的依赖项了。
- en: Installing Python and PyTorch on Windows
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Windows上安装Python和PyTorch
- en: 'The suggested version of Python for the ML-Agents Toolkit is version 3.7\.
    You can install it in many ways, the faster of which is by searching in Microsoft
    Store for Python 3.7 (or follow this link: [https://www.microsoft.com/en-us/p/python-37/9nj46sx7x90p](https://www.microsoft.com/en-us/p/python-37/9nj46sx7x90p)).'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ML-Agents 工具包建议的 Python 版本是 3.7。你可以通过多种方式安装它，其中最快的方式是在 Microsoft Store 中搜索 Python
    3.7（或点击此链接：[https://www.microsoft.com/en-us/p/python-37/9nj46sx7x90p](https://www.microsoft.com/en-us/p/python-37/9nj46sx7x90p))。
- en: 'On Windows, you need to manually install PyTorch before installing the `mlagents`
    package. To do that, you can simply run this command in a terminal:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Windows 上，在安装 `mlagents` 包之前，你需要手动安装 PyTorch。为此，你可以在终端中简单地运行以下命令：
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Information
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: If you have any difficulties installing PyTorch, you can refer to the official
    installation guide at [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你安装 PyTorch 时遇到任何困难，可以参考官方安装指南：[https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)。
- en: After this step, you should be able to follow the same installation steps for
    macOS and Unix-like systems.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步之后，你应该能够遵循与 macOS 和 Unix-like 系统相同的安装步骤。
- en: Installing Python and PyTorch on macOS and Unix-like systems
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 macOS 和 Unix-like 系统上安装 Python 和 PyTorch
- en: To install the ML-Agents Toolkit on macOS or Linux, you need first to install
    Python 3.6 or Python 3.7 (at the moment, the ML-Agents Toolkit recommends only
    these two Python versions).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 macOS 或 Linux 上安装 ML-Agents 工具包，你首先需要安装 Python 3.6 或 Python 3.7（目前，ML-Agents
    工具包只推荐这两个 Python 版本）。
- en: 'Then, you can run the following command:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以运行以下命令：
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: On macOS and Linux, this command installs automatically the correct version
    of PyTorch.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在 macOS 和 Linux 上，此命令会自动安装正确的 PyTorch 版本。
- en: After the installation, if everything is correct, you should be able to run
    the `mlagents-learn --help` command without any errors from any place in the system.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完成后，如果一切正常，你应该能够在系统中的任何位置运行 `mlagents-learn --help` 命令而不会出现任何错误。
- en: Note
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Pip3 is automatically installed with any Python 3.x distribution. If, for some
    reason, you don''t have `pip3` installed, try following the official guide: [https://pip.pypa.io/en/latest/installing/](https://pip.pypa.io/en/latest/installing/).'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Pip3 会自动与任何 Python 3.x 分发一起安装。如果你没有安装 `pip3`，请尝试按照官方指南操作：[https://pip.pypa.io/en/latest/installing/](https://pip.pypa.io/en/latest/installing/)。
- en: Using the ML-Agents Toolkit – a basic example
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 ML-Agents 工具包 – 一个基本示例
- en: Now that everything is installed, we can start using the ML-Agents Toolkit.
    First, let's explain the basic architecture of an ML-Agents scene.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在一切都已经安装完毕，我们可以开始使用 ML-Agents 工具包。首先，让我们解释一下 ML-Agents 场景的基本架构。
- en: 'An ML-Agents scene is called a **learning environment**. The learning environment
    is a standard Unity scene and contains two main elements:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ML-Agents 场景被称为 **学习环境**。学习环境是一个标准的 Unity 场景，包含两个主要元素：
- en: '`Agent` class and write the behavior for the agent. For instance, if the Agent
    is a car, we need to write how the car is controlled by the input and how we can
    reward and penalize the car (for example, we can reward the vehicle for going
    above a certain speed and punish it when it goes off-road). A learning environment
    can have as many agents as you like.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写 `Agent` 类的行为。例如，如果 Agent 是一辆车，我们需要编写如何通过输入来控制车辆，以及我们如何奖励和惩罚车辆（例如，我们可以奖励车辆以超过一定速度行驶，当它驶离道路时进行惩罚）。一个学习环境可以包含你想要的任意数量的
    Agent。
- en: '`OnEpisodeBegin()` for each `Agent` in the scene.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对场景中的每个 `Agent` 调用 `OnEpisodeBegin()`。
- en: Invokes `CollectObservations(VectorSensor sensor)` for each `Agent` in the scene.
    This function is used to collect information on the environment so that each `Agent`
    can update its internal model.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对场景中的每个 `Agent` 调用 `CollectObservations(VectorSensor sensor)`。此函数用于收集环境信息，以便每个
    `Agent` 可以更新其内部模型。
- en: Invokes `OnActionReceived()` for every `Agent` in the scene. This function executes
    the action chosen by each `Agent` and collects the rewards (or penalty).
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对场景中的每个 `Agent` 调用 `OnActionReceived()`。此函数执行每个 `Agent` 选择的动作并收集奖励（或惩罚）。
- en: If an agent completes its episode, the academy calls `OnEpisodeBegin()` for
    the Agent. This function is responsible for resetting the Agent in the starting
    configuration.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一个 Agent 完成了其剧集，学院会为 Agent 调用 `OnEpisodeBegin()`。此函数负责将 Agent 重置到起始配置。
- en: 'To start using the ML-Agents Toolkit, we need to do the following:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用 ML-Agents 工具包，我们需要做以下几步：
- en: Open Unity and create an empty project.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开 Unity 并创建一个空项目。
- en: Go to **Windows** | **Package Manager**.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往 **Windows** | **软件包管理器**。
- en: 'In the top-left menu, select **Unity Registry**:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在右上角菜单中，选择 **Unity 注册表**：
- en: '![Figure 11.1 – The Unity Package Manager'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.1 – Unity 包管理器'
- en: '](img/B17981_11_1.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17981_11_1.jpg)'
- en: Figure 11.1 – The Unity Package Manager
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.1 – Unity 包管理器
- en: 'Look for the **ML Agents** package and install it:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查找 **ML Agents** 包并安装它：
- en: '![Figure 11.2 – The ML Agents package in Package Manager'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.2 – 包管理器中的 ML Agents 包'
- en: '](img/B17981_11_2.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17981_11_2.jpg)'
- en: Figure 11.2 – The ML Agents package in Package Manager
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.2 – 包管理器中的 ML Agents 包
- en: We need to make sure that we are using the correct runtime.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要确保我们使用的是正确的运行时。
- en: 'To do so, go to **Edit** | **Project Settings** | **Player**, and for each
    platform (PC, Mac, Android, and so on), go into **Other Settings** and make sure
    that **Api Compatibility Level** is set to **.NET Framework**. If not, adjust
    these settings to be as we need them and then save, as follows:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要这样做，请转到 **编辑** | **项目设置** | **玩家**，并为每个平台（PC、Mac、Android 等），进入 **其他设置** 并确保
    **API 兼容性级别** 设置为 **.NET Framework**。如果不是，调整这些设置以满足我们的需求，然后保存，如下所示：
- en: '![Figure 11.3 – The Project Settings window with the correct settings'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.3 – 包含正确设置的“项目设置”窗口'
- en: '](img/B17981_11_3.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17981_11_3.jpg)'
- en: Figure 11.3 – The Project Settings window with the correct settings
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.3 – 包含正确设置的“项目设置”窗口
- en: Creating the scene
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建场景
- en: 'Creating a learning environment is easy. Let''s create a simple 3D scene with
    a plane, a sphere, and a cube, as shown in the following screenshot:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 创建学习环境很简单。让我们创建一个简单的 3D 场景，包含一个平面、一个球体和一个立方体，如下截图所示：
- en: '![Figure 11.4 – The basic demo scene'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.4 – 基本演示场景'
- en: '](img/B17981_11_4.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17981_11_4.jpg)'
- en: Figure 11.4 – The basic demo scene
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.4 – 基本演示场景
- en: We put the cube at the plane's center and add a `Rigidbody` component to the
    sphere. This scene aims to train a rolling ball to reach the target (the cube)
    without falling from the plane.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将立方体放在平面的中心，并给球体添加一个 `Rigidbody` 组件。这个场景的目的是训练一个滚动的球体到达目标（立方体）而不从平面上掉落。
- en: Implementing the code
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现代码
- en: 'Now, we need to implement the code that will describe the Agent''s behavior
    and the ML-Agent''s academy. The agent''s behavior script describes how the Agents
    perform actions in the simulation, the reward the Agent receives, and how we reset
    it to start a new simulation:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要实现描述代理行为和 ML-Agent 学院的代码。代理的行为脚本描述了代理在模拟中如何执行动作、代理收到的奖励以及我们如何将其重置以开始新的模拟：
- en: 'Select the sphere. Let''s add to it a new script, called `SphereAgent`, with
    the following content:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择球体。让我们给它添加一个新的脚本，命名为 `SphereAgent`，内容如下：
- en: '[PRE3]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This is the base agent for our demo. `OnEpisodeBegin` is a function called by
    the system every time we want to reset the training scene. In our example, we
    check whether the sphere fell from the plane and bring it back to zero; otherwise,
    we move it to a random location.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的演示的基础代理。`OnEpisodeBegin` 是系统在每次我们想要重置训练场景时调用的函数。在我们的例子中，我们检查球体是否从平面上掉落，并将其带回零；否则，我们将其移动到随机位置。
- en: 'We need to add the `CollectObservations` method to `SphereAgent`. The agent
    uses this method to get information from the game world and then uses it to perform
    a decision:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要将 `CollectObservations` 方法添加到 `SphereAgent` 中。代理使用此方法从游戏世界中获取信息，然后使用它来做出决策：
- en: '[PRE4]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In this example, we are interested in the following:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们感兴趣的是以下内容：
- en: The relative position of the sphere agent from the cube (the target). We are
    only interested in the `x` and `z` values because the sphere only moves on the
    plane (note that we normalize the values by dividing by `5`, which is half the
    default plane size).
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 球体代理相对于立方体（目标）的相对位置。我们只对 `x` 和 `z` 值感兴趣，因为球体只在平面上移动（注意，我们通过除以 `5`（默认平面大小的一半）来归一化这些值）。
- en: The distance from the plane's edges.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平面边缘的距离。
- en: The sphere's velocity.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 球体的速度。
- en: We need to implement the `OnActionReceived` method. This method is called whenever
    the Agent needs to act. The method takes a single parameter of the `ActionBuffer`
    type. The `ActionBuffer` object contains a description of the control inputs for
    the sphere. In our case, we only need two continuous actions, corresponding to
    the force applied along the *x* and *z* axes of the game.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要实现 `OnActionReceived` 方法。每当代理需要采取行动时，都会调用此方法。该方法接受一个 `ActionBuffer` 类型的单个参数。`ActionBuffer`
    对象包含对球体控制输入的描述。在我们的例子中，我们只需要两个连续的动作，对应于沿游戏 *x* 和 *z* 轴施加的力。
- en: 'We also need to define the rewards. As said before, we reward the Agent with
    one point by calling `SetReward` when we reach the target. If the Agent falls
    off the plane, we end the episode with zero points by calling `EndEpisode`. The
    final version of the code is the following:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还需要定义奖励。如前所述，当我们达到目标时，通过调用`SetReward`，我们给代理奖励一分。如果代理从飞机上掉落，我们通过调用`EndEpisode`以零分结束这一轮。代码的最终版本如下：
- en: '[PRE5]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now, it is time to connect this `SphereAgent` script to our sphere.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候将这个`SphereAgent`脚本连接到我们的球体上了。
- en: Adding the final touches
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 添加最终细节
- en: 'Now, we need to connect all the pieces to make the demo work:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要连接所有部件，使演示工作：
- en: First, we attach the `SphereAgent` script to the sphere.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将`SphereAgent`脚本附加到球体上。
- en: We drag and drop the cube into the **Target** field of the **Sphere Agent**
    component.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将立方体拖放到**Sphere Agent**组件的**目标**字段中。
- en: We add a **Decision Requester** component by clicking on **Add Component**.
    We can leave the default settings.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过点击**添加组件**，我们添加一个**决策请求器**组件。我们可以保留默认设置。
- en: In the `MovingSphere`.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`MovingSphere`中。
- en: We set the `8`, corresponding to the number of observations we added in the
    `CollectObservations` method.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将`8`设置为对应我们在`CollectObservations`方法中添加的观察数量。
- en: 'Finally, we set the `2`. At this point, the sphere scripts should look like
    the following screenshot:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将`2`设置为。此时，球体脚本应该看起来像以下截图：
- en: '![Figure 11.5 – The Inspector view for the sphere agent'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '![图11.5 – 球体代理的检查器视图'
- en: '](img/B17981_11_5.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17981_11_5.jpg)'
- en: Figure 11.5 – The Inspector view for the sphere agent
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.5 – 球体代理的检查器视图
- en: It is now to test our environment.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是测试我们的环境的时候了。
- en: Testing the learning environment
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试学习环境
- en: Before we start learning, we want to test the environment by controlling the
    Agents with manual input. It is very useful to debug the learning environment
    without wasting hours of the training process.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始学习之前，我们希望通过手动输入控制代理来测试环境。在不浪费数小时训练过程的情况下调试学习环境非常有用。
- en: 'Fortunately, the ML-Agents Toolkit makes it very handy to control an agent
    with live input. We only need two steps:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，ML-Agents Toolkit使得使用实时输入控制代理变得非常方便。我们只需要两个步骤：
- en: 'We add the `Heuristic` method to the `SphereAgent` component. This function
    allows us to manually specify the values of the `ActionBuffer` objects. In our
    case, we want to add the two continuous actions to the input axes of the controller:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将`Heuristic`方法添加到`SphereAgent`组件中。这个函数允许我们手动指定`ActionBuffer`对象的值。在我们的情况下，我们想要将两个连续动作添加到控制器的输入轴上：
- en: '[PRE6]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, we go to the Inspector and set the **Behavior Type** parameter to **Heuristic
    Only**:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们转到检查器，并将**行为类型**参数设置为**仅启发式**：
- en: '![Figure 11.6 – The Behavior Type configuration'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '![图11.6 – 行为类型配置'
- en: '](img/B17981_11_6.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17981_11_6.jpg)'
- en: Figure 11.6 – The Behavior Type configuration
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.6 – 行为类型配置
- en: At this point, you can press **Play** in Unity, and you should be able to control
    the sphere using the arrow key (or a gaming controller stick). You can test the
    environment by checking the episode's behavior. If you reach the target cube,
    it should disappear and spawn in another random location. If you fall from the
    plane, you should respawn on the plane.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你可以按**播放**在Unity中，你应该能够使用箭头键（或游戏控制器摇杆）控制球体。你可以通过检查这一轮的行为来测试环境。如果你到达目标立方体，它应该消失并在另一个随机位置重新生成。如果你从飞机上掉落，你应该在飞机上重生。
- en: If everything looks fine, it is time to train the Agent automatically.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切看起来正常，那么是时候自动训练代理了。
- en: Training an agent
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练代理
- en: 'Before we can start training, we need to write a training configuration file.
    Open your terminal and go into any empty folder. Then, create a `sphere.yaml`
    file with the following code:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始训练之前，我们需要编写一个训练配置文件。打开您的终端并进入任何空文件夹。然后，创建一个`sphere.yaml`文件，包含以下代码：
- en: '[PRE7]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Then, we need to be sure to change the **Behavior Type** parameter in the sphere
    object to **Default**.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要确保将球体对象中的**行为类型**参数更改为**默认**。
- en: 'Now, from the same folder, we should be able to run the following command:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，从同一个文件夹中，我们应该能够运行以下命令：
- en: '[PRE8]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '`run-id` is a unique ID for your running session. If everything is going according
    to plan, you should see the **Start training by pressing the Play button in the
    Unity Editor** message on the terminal window at some point. Now, you can do as
    the message says and press **Play** in Unity.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '`run-id`是你运行会话的唯一ID。如果一切按计划进行，你应该在某个时候在终端窗口中看到**通过在Unity编辑器中按播放按钮开始训练**的消息。现在，你可以按照消息所说的，在Unity中按**播放**。'
- en: After the training is complete, you will find the trained model in the `results/myMovingSphere/MovingSphere.onnx`
    file (the `results` folder inside the folder, in which you run the `mlagents-learn`
    command).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，您将在`results/myMovingSphere/MovingSphere.onnx`文件中找到训练好的模型（在运行`mlagents-learn`命令的文件夹内的`results`文件夹中）。
- en: 'Copy this file inside your Unity project and then put this in the **Model**
    placeholder in the **Behavior Parameters** component of the sphere:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 将此文件复制到您的Unity项目中，然后将其放入球体组件的**模型**占位符中：
- en: '![Figure 11.7 – The trained MovingSphere model inside the behavior parameters'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '![图11.7 – 行为参数中的训练好的MovingSphere模型](img/B17981_11_7.jpg)'
- en: '](img/B17981_11_7.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17981_11_7.jpg)'
- en: Figure 11.7 – The trained MovingSphere model inside the behavior parameters
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.7 – 行为参数中的训练好的MovingSphere模型
- en: Now, if you press **Play**, the Sphere will move autonomously according to the
    training model. It is not something big and complex, but it is automated learning
    nevertheless.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果您按下**播放**，球体将根据训练模型自主移动。这并不是什么大而复杂的事情，但无论如何，它是自动化学习。
- en: Summary
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we barely scratched the surface of machine learning and how
    to use it for training Unity agents. We learned how to install Unity's official
    ML-Agents Toolkit, set up a learning environment, and trained the model. However,
    this is just a basic introduction to the ML-Agents Toolkit, and many unexplored
    directions are waiting for you. I encourage you to look at the ML-Agents official
    repository; it includes many interesting demo projects.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们只是刚刚触及了机器学习的表面以及如何用它来训练Unity代理。我们学习了如何安装Unity的官方ML-Agents工具包，设置学习环境，并训练了模型。然而，这仅仅是ML-Agents工具包的基本介绍，还有许多未探索的方向等待您去发现。我鼓励您查看ML-Agents官方仓库；它包括许多有趣的演示项目。
- en: In the next chapter, we will wrap everything up by developing an AI agent into
    a more complex game demo.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将通过将一个AI代理开发成一个更复杂的游戏演示来总结一切。
- en: Further reading
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: For more information, I encourage you to check the in-depth documentation for
    ML-Agents in the official repository ([https://github.com/Unity-Technologies/ml-agents/tree/master/docs](https://github.com/Unity-Technologies/ml-agents/tree/master/docs)).
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如需更多信息，我鼓励您查看官方仓库中ML-Agents的详细文档（[https://github.com/Unity-Technologies/ml-agents/tree/master/docs](https://github.com/Unity-Technologies/ml-agents/tree/master/docs)）。
- en: For a more in-depth (but still very accessible) introduction to reinforcement
    learning, there is a good article on freeCodeCamp ([https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419](https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419)).
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于一个更深入（但仍然非常易于理解）的强化学习介绍，freeCodeCamp上有一篇很好的文章（[https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419](https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419)）。
- en: If you are willing to go even deeper into reinforcement learning, a perfect
    next step is *Deep Reinforcement Learning Hands-On, Second Edition*, *Maxim* *Lapan*,
    *Packt Publishing*.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您愿意更深入地了解强化学习，一个完美的下一步是*《深度强化学习实战，第二版》，*Maxim* *Lapan*，*Packt Publishing*。
