- en: '*Chapter 11*: Machine Learning in Unity'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Machine learning is the hottest buzzword in **Artificial Intelligence** (**AI**).
    Nowadays, everything contains (or claims to contain) some machine learning-powered
    AI that is supposed to improve our life: calendars, to-do apps, photo management
    software, every smartphone, and much more. However, even if the phrase *machine
    learning* is just a marketing gimmick most of the time, it is without question
    that machine learning has improved significantly in recent years. Most importantly,
    though, there are now plenty of tools that allow everybody to implement a learning
    algorithm without any previous academic-level AI knowledge.'
  prefs: []
  type: TYPE_NORMAL
- en: At the moment, machine learning is not used in game development (except for
    applications for procedural content generation). There are many reasons for that.
    The main reason, though, is that a designer can't control the output of a machine
    learning agent, and in game design, uncontrollable outcomes often correlate to
    not-fun games. For this reason, game AI developers prefer more predictable and
    straightforward techniques, such as behavior trees.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, being able to use machine learning algorithms in Unity is
    useful for non-gaming purposes, such as simulations, AI research, and *some serious*
    gaming applications. Whatever the reason, Unity provides a complete toolkit for
    machine learning that spares us the complication of interfacing the game engine
    with an external machine learning framework.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will look at the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to the Unity Machine Learning Agents Toolkit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up the Unity Machine Learning Agents Toolkit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seeing how to run a simple example
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning is an extensive topic; therefore, we do not expect to cover
    every single aspect of it. Instead, look at the toolkit documentation and the
    additional resources linked at the end of this chapter for further reference.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this chapter, you need Unity3D 2022, Python 3.7, PyTorch, and the ML-Agents
    Toolkit installed on your system. Don''t worry if you don''t; we will go over
    the installation steps. You can find the example project described in this chapter
    in the `Chapter 11` folder in the book''s repository: [https://github.com/PacktPublishing/Unity-Artificial-Intelligence-Programming-Fifth-Edition/tree/main/Chapter11](https://github.com/PacktPublishing/Unity-Artificial-Intelligence-Programming-Fifth-Edition/tree/main/Chapter11)'
  prefs: []
  type: TYPE_NORMAL
- en: The Unity Machine Learning Agents Toolkit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Unity Machine Learning Agents Toolkit** (**ML-Agents Toolkit**) is a collection
    of software and plugins that help developers write autonomous game agents powered
    by machine learning algorithms. You can explore and download the source code at
    the GitHub repository at [https://github.com/Unity-Technologies/ml-agents](https://github.com/Unity-Technologies/ml-agents).
  prefs: []
  type: TYPE_NORMAL
- en: The ML-Agents Toolkit is based on the reinforcement learning algorithm. Simplistically,
    reinforcement learning is the algorithmic equivalent of training a dog. For example,
    if you want to teach a dog some trick, you give him a command, and then, when
    the dog does what you expect, you reward him. The reward tells your dog that it
    responded correctly to the command, and therefore, the next time it hears the
    same command, it will do the same thing to get a new reward.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In reinforcement learning, you can also punish your agent when doing the wrong
    things, but in the dog-training example, I can assure you that punishment is entirely
    unnecessary. Just give them rewards!
  prefs: []
  type: TYPE_NORMAL
- en: 'For an AI agent trained with reinforcement learning, we perform a similar cycle:'
  prefs: []
  type: TYPE_NORMAL
- en: When an agent acts, the action influences the world (such as changing the Agent's
    position, moving an object around, collecting a coin, gaining score points, and
    so on).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The algorithm then sends the new world state back to the Agent with a reward
    (or punishment).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When the Agent decides its following action, it will choose the action that
    maximizes the expected reward (or minimizes the expected punishment).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For this reason, it is clear that training a reinforcement learning agent requires
    several simulations of the scenario in which the Agent acts, receives a reward,
    updates its decision-making values, performs another action, and so on. This work
    is offloaded from Unity to Torch via PyTorch. **Torch** is a popular open source
    machine learning library used, among others, by tech giants such as Facebook and
    IBM.
  prefs: []
  type: TYPE_NORMAL
- en: Refer to the *Further reading* section at the end of the chapter for more information
    on reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now see how to install the toolkit.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the ML-Agents Toolkit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As a first step, we need to download the toolkit. We can do this by cloning
    the repository with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This command creates an `ml-agents` folder in your current folder. The ML-Agents
    Toolkit is composed of two main components:'
  prefs: []
  type: TYPE_NORMAL
- en: A Python package containing the Python interface for Unity and PyTorch's trainers
    (stored in the `ml-agents` folder)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Python package containing the interface with OpenAI Gym ([https://gym.openai.com/](https://gym.openai.com/)),
    a toolkit for training reinforcement learning agents (stored in the `gym-unity`
    folder).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Information
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Git** is the most famous version-control application in the world. It is
    used to store your source code, keep track of different versions, collaborate
    with other people, and much more. If you are not already using Git, you should
    really check it out. You can download it from [https://git-scm.com/](https://git-scm.com/).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, it is time to install the required dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Python and PyTorch on Windows
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The suggested version of Python for the ML-Agents Toolkit is version 3.7\.
    You can install it in many ways, the faster of which is by searching in Microsoft
    Store for Python 3.7 (or follow this link: [https://www.microsoft.com/en-us/p/python-37/9nj46sx7x90p](https://www.microsoft.com/en-us/p/python-37/9nj46sx7x90p)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'On Windows, you need to manually install PyTorch before installing the `mlagents`
    package. To do that, you can simply run this command in a terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Information
  prefs: []
  type: TYPE_NORMAL
- en: If you have any difficulties installing PyTorch, you can refer to the official
    installation guide at [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/).
  prefs: []
  type: TYPE_NORMAL
- en: After this step, you should be able to follow the same installation steps for
    macOS and Unix-like systems.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Python and PyTorch on macOS and Unix-like systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To install the ML-Agents Toolkit on macOS or Linux, you need first to install
    Python 3.6 or Python 3.7 (at the moment, the ML-Agents Toolkit recommends only
    these two Python versions).
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, you can run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: On macOS and Linux, this command installs automatically the correct version
    of PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: After the installation, if everything is correct, you should be able to run
    the `mlagents-learn --help` command without any errors from any place in the system.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Pip3 is automatically installed with any Python 3.x distribution. If, for some
    reason, you don''t have `pip3` installed, try following the official guide: [https://pip.pypa.io/en/latest/installing/](https://pip.pypa.io/en/latest/installing/).'
  prefs: []
  type: TYPE_NORMAL
- en: Using the ML-Agents Toolkit – a basic example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that everything is installed, we can start using the ML-Agents Toolkit.
    First, let's explain the basic architecture of an ML-Agents scene.
  prefs: []
  type: TYPE_NORMAL
- en: 'An ML-Agents scene is called a **learning environment**. The learning environment
    is a standard Unity scene and contains two main elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Agent` class and write the behavior for the agent. For instance, if the Agent
    is a car, we need to write how the car is controlled by the input and how we can
    reward and penalize the car (for example, we can reward the vehicle for going
    above a certain speed and punish it when it goes off-road). A learning environment
    can have as many agents as you like.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`OnEpisodeBegin()` for each `Agent` in the scene.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Invokes `CollectObservations(VectorSensor sensor)` for each `Agent` in the scene.
    This function is used to collect information on the environment so that each `Agent`
    can update its internal model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Invokes `OnActionReceived()` for every `Agent` in the scene. This function executes
    the action chosen by each `Agent` and collects the rewards (or penalty).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If an agent completes its episode, the academy calls `OnEpisodeBegin()` for
    the Agent. This function is responsible for resetting the Agent in the starting
    configuration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To start using the ML-Agents Toolkit, we need to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Open Unity and create an empty project.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to **Windows** | **Package Manager**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the top-left menu, select **Unity Registry**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.1 – The Unity Package Manager'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17981_11_1.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.1 – The Unity Package Manager
  prefs: []
  type: TYPE_NORMAL
- en: 'Look for the **ML Agents** package and install it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.2 – The ML Agents package in Package Manager'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17981_11_2.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.2 – The ML Agents package in Package Manager
  prefs: []
  type: TYPE_NORMAL
- en: We need to make sure that we are using the correct runtime.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do so, go to **Edit** | **Project Settings** | **Player**, and for each
    platform (PC, Mac, Android, and so on), go into **Other Settings** and make sure
    that **Api Compatibility Level** is set to **.NET Framework**. If not, adjust
    these settings to be as we need them and then save, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.3 – The Project Settings window with the correct settings'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17981_11_3.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.3 – The Project Settings window with the correct settings
  prefs: []
  type: TYPE_NORMAL
- en: Creating the scene
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Creating a learning environment is easy. Let''s create a simple 3D scene with
    a plane, a sphere, and a cube, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.4 – The basic demo scene'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17981_11_4.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.4 – The basic demo scene
  prefs: []
  type: TYPE_NORMAL
- en: We put the cube at the plane's center and add a `Rigidbody` component to the
    sphere. This scene aims to train a rolling ball to reach the target (the cube)
    without falling from the plane.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, we need to implement the code that will describe the Agent''s behavior
    and the ML-Agent''s academy. The agent''s behavior script describes how the Agents
    perform actions in the simulation, the reward the Agent receives, and how we reset
    it to start a new simulation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Select the sphere. Let''s add to it a new script, called `SphereAgent`, with
    the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This is the base agent for our demo. `OnEpisodeBegin` is a function called by
    the system every time we want to reset the training scene. In our example, we
    check whether the sphere fell from the plane and bring it back to zero; otherwise,
    we move it to a random location.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to add the `CollectObservations` method to `SphereAgent`. The agent
    uses this method to get information from the game world and then uses it to perform
    a decision:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this example, we are interested in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The relative position of the sphere agent from the cube (the target). We are
    only interested in the `x` and `z` values because the sphere only moves on the
    plane (note that we normalize the values by dividing by `5`, which is half the
    default plane size).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The distance from the plane's edges.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The sphere's velocity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We need to implement the `OnActionReceived` method. This method is called whenever
    the Agent needs to act. The method takes a single parameter of the `ActionBuffer`
    type. The `ActionBuffer` object contains a description of the control inputs for
    the sphere. In our case, we only need two continuous actions, corresponding to
    the force applied along the *x* and *z* axes of the game.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We also need to define the rewards. As said before, we reward the Agent with
    one point by calling `SetReward` when we reach the target. If the Agent falls
    off the plane, we end the episode with zero points by calling `EndEpisode`. The
    final version of the code is the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, it is time to connect this `SphereAgent` script to our sphere.
  prefs: []
  type: TYPE_NORMAL
- en: Adding the final touches
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, we need to connect all the pieces to make the demo work:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we attach the `SphereAgent` script to the sphere.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We drag and drop the cube into the **Target** field of the **Sphere Agent**
    component.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We add a **Decision Requester** component by clicking on **Add Component**.
    We can leave the default settings.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the `MovingSphere`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We set the `8`, corresponding to the number of observations we added in the
    `CollectObservations` method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, we set the `2`. At this point, the sphere scripts should look like
    the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.5 – The Inspector view for the sphere agent'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17981_11_5.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.5 – The Inspector view for the sphere agent
  prefs: []
  type: TYPE_NORMAL
- en: It is now to test our environment.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the learning environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we start learning, we want to test the environment by controlling the
    Agents with manual input. It is very useful to debug the learning environment
    without wasting hours of the training process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, the ML-Agents Toolkit makes it very handy to control an agent
    with live input. We only need two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We add the `Heuristic` method to the `SphereAgent` component. This function
    allows us to manually specify the values of the `ActionBuffer` objects. In our
    case, we want to add the two continuous actions to the input axes of the controller:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we go to the Inspector and set the **Behavior Type** parameter to **Heuristic
    Only**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.6 – The Behavior Type configuration'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17981_11_6.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.6 – The Behavior Type configuration
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you can press **Play** in Unity, and you should be able to control
    the sphere using the arrow key (or a gaming controller stick). You can test the
    environment by checking the episode's behavior. If you reach the target cube,
    it should disappear and spawn in another random location. If you fall from the
    plane, you should respawn on the plane.
  prefs: []
  type: TYPE_NORMAL
- en: If everything looks fine, it is time to train the Agent automatically.
  prefs: []
  type: TYPE_NORMAL
- en: Training an agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we can start training, we need to write a training configuration file.
    Open your terminal and go into any empty folder. Then, create a `sphere.yaml`
    file with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Then, we need to be sure to change the **Behavior Type** parameter in the sphere
    object to **Default**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, from the same folder, we should be able to run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '`run-id` is a unique ID for your running session. If everything is going according
    to plan, you should see the **Start training by pressing the Play button in the
    Unity Editor** message on the terminal window at some point. Now, you can do as
    the message says and press **Play** in Unity.'
  prefs: []
  type: TYPE_NORMAL
- en: After the training is complete, you will find the trained model in the `results/myMovingSphere/MovingSphere.onnx`
    file (the `results` folder inside the folder, in which you run the `mlagents-learn`
    command).
  prefs: []
  type: TYPE_NORMAL
- en: 'Copy this file inside your Unity project and then put this in the **Model**
    placeholder in the **Behavior Parameters** component of the sphere:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.7 – The trained MovingSphere model inside the behavior parameters'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17981_11_7.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.7 – The trained MovingSphere model inside the behavior parameters
  prefs: []
  type: TYPE_NORMAL
- en: Now, if you press **Play**, the Sphere will move autonomously according to the
    training model. It is not something big and complex, but it is automated learning
    nevertheless.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we barely scratched the surface of machine learning and how
    to use it for training Unity agents. We learned how to install Unity's official
    ML-Agents Toolkit, set up a learning environment, and trained the model. However,
    this is just a basic introduction to the ML-Agents Toolkit, and many unexplored
    directions are waiting for you. I encourage you to look at the ML-Agents official
    repository; it includes many interesting demo projects.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will wrap everything up by developing an AI agent into
    a more complex game demo.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For more information, I encourage you to check the in-depth documentation for
    ML-Agents in the official repository ([https://github.com/Unity-Technologies/ml-agents/tree/master/docs](https://github.com/Unity-Technologies/ml-agents/tree/master/docs)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a more in-depth (but still very accessible) introduction to reinforcement
    learning, there is a good article on freeCodeCamp ([https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419](https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are willing to go even deeper into reinforcement learning, a perfect
    next step is *Deep Reinforcement Learning Hands-On, Second Edition*, *Maxim* *Lapan*,
    *Packt Publishing*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
