<html><head></head><body>
		<div>
			<div id="_idContainer131" class="Content">
			</div>
		</div>
		<div id="_idContainer132" class="Content">
			<h1 id="_idParaDest-83"><a id="_idTextAnchor084"/>3. Topic Modeling and Theme Extraction</h1>
		</div>
		<div id="_idContainer182" class="Content">
			<p class="callout-heading">Overview</p>
			<p class="callout">This chapter describes the use of Topic Modeling to understand common themes in a document set by analyzing documents using Amazon Comprehend. You will learn the fundamentals of the algorithm used for Topic Modeling, <strong class="bold">Latent Dirichlet Allocation</strong> (<strong class="bold">LDA</strong>). Learning LDA will allow you to apply Topic Modeling to a multitude of unique business cases. You will then perform Topic Modeling on two documents with a known topic structure. By the end of this chapter, you will be able to extract and analyze common themes through Topic Modeling with Amazon Comprehend and describe the basics of Topic Modeling analysis. You will also be able to perform Topic Modeling on a set of documents and analyze the results.</p>
			<h1 id="_idParaDest-84"><a id="_idTextAnchor085"/>Introduction</h1>
			<p>Topic Modeling is an important capability for business systems to make sense of unstructured information, ranging from support tickets to customer feedback and complaints, to business documents. Topic Modeling helps process automation to route customer feedback and mail; it enables a business to categorize and then effectively respond to social media posts, reviews, and other user-generated content from the various channels. It enables businesses to respond faster to critical items by understanding the topics and themes on incoming omnichannel interactions as well as responding most effectively by routing the materials to the most appropriate teams. Another two areas where Topic Modeling helps are knowledge management and brand monitoring.</p>
			<h1 id="_idParaDest-85"><a id="_idTextAnchor086"/>Topic Modeling with Latent Dirichlet Allocation (LDA)</h1>
			<p>The subjects or <strong class="bold">common themes</strong> of a set of documents can be determined with Amazon Comprehend. For example, you have a movie review website with two message boards, and you want to determine which message board is discussing two newly released movies (one about sport and the other about a political topic). You can provide the message board text data to Amazon Comprehend to discover the most prominent topics discussed on each message board.</p>
			<p>The machine learning algorithm that Amazon Comprehend uses to perform Topic Modeling is called <strong class="bold">Latent Dirichlet Allocation</strong> (<strong class="bold">LDA</strong>). LDA is a learning-based model that's used to determine the most important topics in a collection of documents.</p>
			<p>How LDA works is that it considers every document to be a combination of topics, and each word in the document is associated with one of these topics.</p>
			<p>For example, if the first paragraph of a document consists of words such as <strong class="bold">eat</strong>, <strong class="bold">chicken</strong>, <strong class="bold">restaurant</strong>, and <strong class="bold">cook</strong>, then you conclude that the topic can be generalized to <strong class="bold">Food</strong>. Similarly, if the second paragraph of a document contains words such as <strong class="bold">ticket</strong>, <strong class="bold">train</strong>, <strong class="bold">kilometer</strong>, and <strong class="bold">vacation</strong>, then you can conclude that the topic is <strong class="bold">Travel</strong>.</p>
			<h2 id="_idParaDest-86"><a id="_idTextAnchor087"/>Basic LDA Example</h2>
			<p>LDA has lots of math behind it—concepts such as <strong class="bold">Expectation Maximization</strong>, Gibs sampling, priors, and a probability distribution over a "bag of words". If you want to understand the mathematical underpinnings, a good start is the Amazon documentation on SageMaker (<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/lda-how-it-works.html">https://docs.aws.amazon.com/sagemaker/latest/dg/lda-how-it-works.html</a>). Let's look at LDA more pragmatically and understand it empirically through an example.</p>
			<p>Say you have one document with six sentences, and you want to infer two common topics.</p>
			<p>The sentences are as follows:</p>
			<ul>
				<li>They loved each other greatly.</li>
				<li>Most people experience love without noticing that there is anything remarkable about it.</li>
				<li>It was partly the war; the revolution did the rest.</li>
				<li>The war was an artificial break in life, as if life could be put off for a time. What nonsense! </li>
				<li>I said life, but I mean life as you see it in a great picture, transformed by genius, creatively enriched. </li>
				<li>Only now have people decided to experience it not in books and pictures, but in themselves, not as an abstraction, but in practice.</li>
			</ul>
			<p>When you feed these sentences into an LDA algorithm, specifying the number of topics as two, it will discover the following:</p>
			<p><strong class="bold">Sentence-Topics</strong></p>
			<p>Sentence 1: Topic 0</p>
			<p>Sentence 2: Topic 0</p>
			<p>Sentence 3: Topic 1</p>
			<p>Sentence 4: Topic 1</p>
			<p>Sentence 5: Topic 0</p>
			<p>Sentence 6: Topic 0</p>
			<p><strong class="bold">Topic terms</strong></p>
			<p>Topic 0: life 12%, people 8%, experience 8%, love 5%, and so forth</p>
			<p>Topic 1: 62% revolution, 23% war, and the rest 15%</p>
			<p>Of course, knowing that the sentences are from the book <em class="italic">Dr. Zhivago</em> by the famous Russian author <em class="italic">Boris Pasternak</em>, the topics war and life/love seem reasonable.</p>
			<p>While this example is a simplistic depiction of a complex algorithm, it gives you an idea. As discussed in this chapter, in various business situations, an indication of what a document or an e-mail or a social media post is about is very valuable for downstream systems—and the ability to perform that classification automatically is priceless.</p>
			<h2 id="_idParaDest-87"><a id="_idTextAnchor088"/>Why Use LDA?</h2>
			<p>LDA is useful when you want to group a set of documents based on common topics, without thinking about the documents themselves. LDA can create subjects from inferring the general topics by analyzing the words in the documents. This is usually utilized in suggestion frameworks, report arrangement, and record synopsis. In conclusion, LDA has many uses. For example, you have 30,000 user emails and want to determine the most common topics to provide group-specific recommended content based on the most prevalent topics. Manually reading, or even outsourcing the manual reading, of 30,000 emails would take an excessive investment in terms of time and money, and the accuracy would be difficult to confirm. However, Amazon Comprehend can seamlessly provide the most common topics in 30,000 emails in a few steps with incredible accuracy. First, convert the emails to text files, upload them to an S3 bucket, and then imitate a Topic Modeling job with Amazon Comprehend. The output is two CSV files with the corresponding topics and terms.</p>
			<h1 id="_idParaDest-88"><a id="_idTextAnchor089"/>Amazon Comprehend—Topic Modeling Guidelines</h1>
			<p>The most accurate results are obtained if you provide Comprehend with the largest possible corpus. More specifically:</p>
			<ul>
				<li>You should use no fewer than 1,000 records in every subject. </li>
				<li>Each document ought to be something like three sentences in length.</li>
				<li>If a document comprises, for the most part, numeric information, you should expel it from the corpus.</li>
			</ul>
			<p>Currently, Topic Modeling is limited to two document languages: <strong class="bold">English</strong> and <strong class="bold">Spanish</strong>.</p>
			<p>A Topic Modeling job allows two format types for input data (refer to the following <em class="italic">Figure 3.1</em>). This allows users to process both collections of large documents (for example, newspaper articles or scientific journals), and short documents (for example, tweets or social media posts).</p>
			<p><strong class="bold">Input Format Options:</strong></p>
			<div>
				<div id="_idContainer133" class="IMG---Figure">
					<img src="image/B16061_03_01.jpg" alt="Figure 3.1: AWS Comprehend—Topic Modeling input format options&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.1: AWS Comprehend—Topic Modeling input format options</p>
			<p><strong class="bold">Output Format Options:</strong></p>
			<div>
				<div id="_idContainer134" class="IMG---Figure">
					<img src="image/B16061_03_02.jpg" alt="Figure 3.2: AWS Comprehend—Topic Modeling output files description&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.2: AWS Comprehend—Topic Modeling output files description</p>
			<p>After Amazon Comprehend processes your document collection, the modeling outputs two CSV files: <strong class="source-inline">topic-terms.csv</strong> (see <em class="italic">Figure 3.2</em>) and <strong class="source-inline">doc-topics.csv</strong>.</p>
			<p>The <strong class="source-inline">topic-terms.csv</strong> file provides a list of topics in the document collection with the terms, respective topics, and their weights. For example, if you gave Amazon Comprehend two hypothetical documents, <strong class="bold">learning to garden</strong> and <strong class="bold">investment strategies</strong>, it might return the following to describe the two topics in the collection:</p>
			<div>
				<div id="_idContainer135" class="IMG---Figure">
					<img src="image/B16061_03_03.jpg" alt="Figure 3.3: Sample Topic Modeling output (topic-terms.csv) for two documents' input&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.3: Sample Topic Modeling output (topic-terms.csv) for two documents' input</p>
			<p>The <strong class="source-inline">doc-topics.csv</strong> file provides a list of the documents provided for the Topic Modeling job, and the respective topics and their proportions in each document. Given two hypothetical documents, <strong class="source-inline">learning_to_garden.txt</strong> and <strong class="source-inline">investment_strategies.txt,</strong> you can expect the following output:</p>
			<div>
				<div id="_idContainer136" class="IMG---Figure">
					<img src="image/B16061_03_04.jpg" alt="Figure 3.4: Sample Topic Modeling output (doc-topics.csv) for two documents' input&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.4: Sample Topic Modeling output (doc-topics.csv) for two documents' input</p>
			<h2 id="_idParaDest-89"><a id="_idTextAnchor090"/>Exercise 3.01: Using Amazon Comprehend to Perform Topic Modeling on Two Documents with Known Topics</h2>
			<p>In this exercise, we will use two documents (<strong class="bold">Romeo and Juliet</strong> and <strong class="bold">War of the Worlds</strong>) to better understand LDA. We will use Amazon Comprehend to discover the main topics in the two documents. Before proceeding to the exercise, just look at an overview of the data pipeline architecture. The text files are stored in S3, and then we direct Comprehend to look for the files in the input bucket. Comprehend analyzes the documents and puts the results back in S3 in the output bucket:</p>
			<div>
				<div id="_idContainer137" class="IMG---Figure">
					<img src="image/B16061_03_05.jpg" alt="Figure 3.5: Data pipeline architecture overview&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.5: Data pipeline architecture overview</p>
			<p>Complete the Topic Modeling of a known topic structure:</p>
			<ol>
				<li>First, you need to get to the S3 console. Please refer to <em class="italic">Chapter 1</em>, <em class="italic">An Introduction to AWS</em>, for account setup instructions. Go to <a href="https://aws.amazon.com/">https://aws.amazon.com/</a> and click <strong class="source-inline">My Account</strong> followed by <strong class="source-inline">AWS Management Console</strong>. Click <strong class="source-inline">Services</strong>, and then search or select <strong class="source-inline">S3</strong> in a new browser tab. You will see the S3 console as shown in the following screenshot:<div id="_idContainer138" class="IMG---Figure"><img src="image/B16061_03_06.jpg" alt="Figure 3.6: Amazon S3 console&#13;&#10;"/></div><p class="figure-caption">Figure 3.6: Amazon S3 console</p></li>
				<li>We need an input and output S3 bucket. Let's create both. Now, click the <strong class="source-inline">Create bucket</strong> button to create a bucket:<div id="_idContainer139" class="IMG---Figure"><img src="image/B16061_03_07.jpg" alt="Figure 3.7: Creating a bucket&#13;&#10;"/></div><p class="figure-caption">Figure 3.7: Creating a bucket</p></li>
				<li>For the bucket name, enter a unique name that describes the function. Here, the name <strong class="source-inline">aws-ml-input-for-topic-modeling</strong> is used. Click the <strong class="source-inline">Create</strong> button:<p class="callout-heading">Note</p><p class="callout">The bucket names in AWS have to be unique. So, you might get an error saying, "Bucket name already exists." One easy way to get a unique name is to append the bucket name with today's date (plus time, if required); say, YYYYMMDDHHMM. While writing this chapter, we created a bucket, <strong class="source-inline">aws-ml-input-for-topic-modeling-20200301</strong>.</p><p class="callout">Clicking <strong class="source-inline">Create</strong> in the following window uses all the default settings for properties and permissions, while clicking <strong class="source-inline">Next</strong> allows you to adjust these settings according to your needs.</p><div id="_idContainer140" class="IMG---Figure"><img src="image/B16061_03_08.jpg" alt="Figure 3.8: Creating a bucket name input&#13;&#10;"/></div><p class="figure-caption">Figure 3.8: Creating a bucket name input</p></li>
				<li>Click <strong class="source-inline">Next</strong>, then <strong class="source-inline">Next</strong> again to go to <strong class="source-inline">Configure options</strong>, click <strong class="source-inline">Next</strong> once more to go to <strong class="source-inline">Set permissions</strong>, and finally click on <strong class="source-inline">Create Bucket</strong> in the <strong class="source-inline">Review</strong> tab:</li>
				<li>Now, click the bucket and then the <strong class="source-inline">Create folder</strong> button to create a folder:<div id="_idContainer141" class="IMG---Figure"><img src="image/B16061_03_09.jpg" alt="Figure 3.9: Creating a folder in S3 for Topic modeling input&#13;&#10;"/></div><p class="figure-caption">Figure 3.9: Creating a folder in S3 for Topic modeling input</p></li>
				<li>Now, type in <strong class="source-inline">known_structure</strong> as the folder name, and then click the <strong class="source-inline">Save</strong> button:<div id="_idContainer142" class="IMG---Figure"><img src="image/B16061_03_10.jpg" alt="Figure 3.10: Saving the known_structure folder name&#13;&#10;"/></div><p class="figure-caption">Figure 3.10: Saving the known_structure folder name</p></li>
				<li>After clicking the <strong class="source-inline">Save</strong> button, your folder will be generated. Now, click the <strong class="source-inline">known_structure</strong> folder:<div id="_idContainer143" class="IMG---Figure"><img src="image/B16061_03_11.jpg" alt="Figure 3.11: The input bucket screen&#13;&#10;"/></div><p class="figure-caption">Figure 3.11: The input bucket screen</p></li>
				<li>Now, click the <strong class="source-inline">Upload</strong> button:<div id="_idContainer144" class="IMG---Figure"><img src="image/B16061_03_12.jpg" alt="Figure 3.12: The Upload button&#13;&#10;"/></div><p class="figure-caption">Figure 3.12: The Upload button</p></li>
				<li>Now, you will be prompted to add files to the folder. Click <strong class="source-inline">Add files</strong>, or drag the files onto the screen:<div id="_idContainer145" class="IMG---Figure"><img src="image/B16061_03_13.jpg" alt="Figure 3.13: The Add files button&#13;&#10;"/></div><p class="figure-caption">Figure 3.13: The Add files button</p></li>
				<li>The files for this chapter are located in the <strong class="source-inline">Chapter03</strong> folder in the GitHub repository at <a href="https://packt.live/3eba6rM">https://packt.live/3eba6rM</a>. As we mentioned in <em class="italic">Chapter 1</em>, <em class="italic">An Introduction to AWS</em>, you should have downloaded the GitHub files to a local subdirectory.<p>By way of an example, we have downloaded the files to the <strong class="source-inline">Documents/aws-book/The-Applied-AI-and-Natural-Language-Processing-with-AWS</strong> directory. Navigate to <strong class="source-inline">Upload</strong> and select the following two text files from your local disk. As you may have guessed, the files for this exercise are located in the <strong class="source-inline">Exercise3.01</strong> subdirectory:</p><p>Once the files have been selected, click on the <strong class="source-inline">Open</strong> button to upload the files:</p><div id="_idContainer146" class="IMG---Figure"><img src="image/B16061_03_14.jpg" alt="Figure 3.14: Selecting files to upload from the local directory&#13;&#10;"/></div><p class="figure-caption">Figure 3.14: Selecting files to upload from the local directory</p><p>The following figure shows the uploading of text files:</p><div id="_idContainer147" class="IMG---Figure"><img src="image/B16061_03_15.jpg" alt="Figure 3.15: Uploading for the two known_structure text files&#13;&#10;"/></div><p class="figure-caption">Figure 3.15: Uploading for the two known_structure text files</p></li>
				<li>Click <strong class="source-inline">Next</strong> in the <strong class="source-inline">Set permissions</strong> and <strong class="source-inline">Set Properties</strong> tabs. Select <strong class="source-inline">Upload</strong> in the <strong class="source-inline">Review</strong> tab:<div id="_idContainer148" class="IMG---Figure"><img src="image/B16061_03_16.jpg" alt="Figure 3.16: Amazon S3 upload files&#13;&#10;"/></div><p class="figure-caption">Figure 3.16: Amazon S3 upload files</p></li>
				<li>Navigate to the <strong class="source-inline">Amazon S3</strong> home screen:<div id="_idContainer149" class="IMG---Figure"><img src="image/B16061_03_17.jpg" alt="Figure 3.17: Amazon S3&#13;&#10;"/></div><p class="figure-caption">Figure 3.17: Amazon S3</p></li>
				<li>Next, create an output S3 bucket. Use the same S3 bucket creation process. To do so, click the <strong class="source-inline">Create bucket</strong> button:<div id="_idContainer150" class="IMG---Figure"><img src="image/B16061_03_18.jpg" alt="Figure 3.18: Creating a bucket&#13;&#10;"/></div><p class="figure-caption">Figure 3.18: Creating a bucket</p></li>
				<li> Now, name the bucket and then click the <strong class="source-inline">Create</strong> button:<div id="_idContainer151" class="IMG---Figure"><img src="image/B16061_03_19.jpg" alt="Figure 3.19: Creating bucket output for Topic Modeling&#13;&#10;"/></div><p class="figure-caption">Figure 3.19: Creating bucket output for Topic Modeling</p></li>
				<li>Click <strong class="source-inline">Next</strong> under <strong class="source-inline">Configure Options</strong>, <strong class="source-inline">Next</strong> under <strong class="source-inline">Set permissions</strong>, and <strong class="source-inline">Create Bucket</strong> in the <strong class="source-inline">Review</strong> window.<p>Now you have two buckets, one for input with two text files, and an empty output bucket. Let's now proceed to Amazon Comprehend.</p></li>
				<li>Navigate to Amazon Comprehend: <a href="https://console.aws.amazon.com/comprehend/">https://console.aws.amazon.com/comprehend/</a>. If you are presented with the following screen, click <strong class="source-inline">Launch Amazon Comprehend</strong>:<div id="_idContainer152" class="IMG---Figure"><img src="image/B16061_03_20.jpg" alt="Figure 3.20: The Amazon Comprehend home screen&#13;&#10;"/></div><p class="figure-caption">Figure 3.20: The Amazon Comprehend home screen</p></li>
				<li>Now, click the first <strong class="source-inline">Analysis jobs</strong> option in the left-hand side toolbar (<strong class="bold">not</strong> the one under Amazon Comprehend Medical):<div id="_idContainer153" class="IMG---Figure"><img src="image/B16061_03_21.jpg" alt="Figure 3.21: The Amazon Comprehend organization screen&#13;&#10;"/></div><p class="figure-caption">Figure 3.21: The Amazon Comprehend organization screen</p></li>
				<li>Now, click the <strong class="source-inline">Create job</strong> button: <div id="_idContainer154" class="IMG---Figure"><img src="image/B16061_03_22.jpg" alt="Figure 3.22: The Amazon Comprehend Create job button&#13;&#10;"/></div><p class="figure-caption">Figure 3.22: The Amazon Comprehend Create job button</p></li>
				<li>Enter <strong class="source-inline">known_structure_topic_modeling_job</strong> in the <strong class="source-inline">Name</strong> field:<div id="_idContainer155" class="IMG---Figure"><img src="image/B16061_03_23.jpg" alt="Figure 3.23: Name of the Topic Modeling job&#13;&#10;"/></div><p class="figure-caption">Figure 3.23: Name of the Topic Modeling job</p></li>
				<li>Select <strong class="source-inline">Topic Modeling</strong> in the <strong class="source-inline">Analysis type</strong> drop-down box:<div id="_idContainer156" class="IMG---Figure"><img src="image/B16061_03_24.jpg" alt="Figure 3.24: Selecting analysis type (Topic Modeling)&#13;&#10;"/></div><p class="figure-caption">Figure 3.24: Selecting analysis type (Topic Modeling)</p></li>
				<li>Now, scroll down to the <strong class="source-inline">Input data</strong> tab and then click <strong class="source-inline">Browse S3</strong>:<div id="_idContainer157" class="IMG---Figure"><img src="image/B16061_03_25.jpg" alt="Figure 3.25: Clicking Search to locate the Topic Modeling input data source&#13;&#10;"/></div><p class="figure-caption">Figure 3.25: Clicking Search to locate the Topic Modeling input data source</p></li>
				<li>The list of S3 buckets will be displayed:<div id="_idContainer158" class="IMG---Figure"><img src="image/B16061_03_26.jpg" alt="Figure 3.26: Selecting the input bucket&#13;&#10;"/></div><p class="figure-caption">Figure 3.26: Selecting the input bucket</p></li>
				<li>Select the input bucket (in my case, it is <strong class="source-inline">aws-ml-input-for-topic-modeling-20200301</strong>) and click on the bucket. Then, the folder will be displayed:<div id="_idContainer159" class="IMG---Figure"><img src="image/B16061_03_27.jpg" alt="Figure 3.27: Selecting the input folder&#13;&#10;"/></div><p class="figure-caption">Figure 3.27: Selecting the input folder</p></li>
				<li>Click the radio button next to <strong class="source-inline">known_structure</strong> and then click the <strong class="source-inline">Choose</strong> button, which will direct you to the following screen:<div id="_idContainer160" class="IMG---Figure"><img src="image/B16061_03_28.jpg" alt="Figure 3.28: The Input data section with the S3 location filled in&#13;&#10;"/></div><p class="figure-caption">Figure 3.28: The Input data section with the S3 location filled in</p></li>
				<li>Now, from the drop-down menu, select <strong class="source-inline">One document per file</strong>:<div id="_idContainer161" class="IMG---Figure"><img src="image/B16061_03_29.jpg" alt="Figure 3.29: Selecting One document per file&#13;&#10;"/></div><p class="figure-caption">Figure 3.29: Selecting One document per file</p></li>
				<li>Now, enter <strong class="source-inline">2</strong> for the <strong class="source-inline">Number of Topics</strong> you need to have:<div id="_idContainer162" class="IMG---Figure"><img src="image/B16061_03_30.jpg" alt="Figure 3.30: Entering 2 for the number of topics to perform Topic Modeling&#13;&#10;"/></div><p class="figure-caption">Figure 3.30: Entering 2 for the number of topics to perform Topic Modeling</p></li>
				<li>Next, click <strong class="source-inline">Browse S3</strong> in the <strong class="source-inline">Output data</strong> tab:<div id="_idContainer163" class="IMG---Figure"><img src="image/B16061_03_31.jpg" alt="Figure 3.31: Output data tab and the Browse S3 button for the Topic Modeling S3 output location&#13;&#10;"/></div><p class="figure-caption">Figure 3.31: Output data tab and the Browse S3 button for the Topic Modeling S3 output location</p></li>
				<li>Select the output bucket (in our case, it is <strong class="source-inline">aws-ml-output-for-topic-modeling-20200301) </strong>and then click <strong class="source-inline">Choose</strong>:<div id="_idContainer164" class="IMG---Figure"><img src="image/B16061_03_32.jpg" alt="Figure 3.32: Selecting the output S3 bucket&#13;&#10;"/></div><p class="figure-caption">Figure 3.32: Selecting the output S3 bucket</p></li>
				<li>Make sure that the <strong class="source-inline">Output data</strong> tab looks similar to the following screenshot:<div id="_idContainer165" class="IMG---Figure"><img src="image/B16061_03_33.jpg" alt="Figure 3.33: Output data tab with the output bucket name&#13;&#10;"/></div><p class="figure-caption">Figure 3.33: Output data tab with the output bucket name</p></li>
				<li>Scroll down to the <strong class="source-inline">Access permissions</strong> tab, and then select the option <strong class="source-inline">Create an IAM role</strong>:<div id="_idContainer166" class="IMG---Figure"><img src="image/B16061_03_34.jpg" alt="Figure 3.34: Selecting Create an IAM role and providing permission to &#13;&#10;Input and Output S3 buckets&#13;&#10;"/></div><p class="figure-caption">Figure 3.34: Selecting Create an IAM role and providing permission to Input and Output S3 buckets</p><p>Check to make sure that <strong class="source-inline">Input and Output S3 buckets</strong> is listed under <strong class="source-inline">Permissions to access</strong>:</p></li>
				<li>Enter <strong class="source-inline">myTopicModelingRole</strong> in the <strong class="source-inline">Name suffix</strong> field and then click the <strong class="source-inline">Create job</strong> button:<div id="_idContainer167" class="IMG---Figure"><img src="image/B16061_03_35.jpg" alt="Figure 3.35: Clicking the Create job button&#13;&#10;"/></div><p class="figure-caption">Figure 3.35: Clicking the Create job button</p></li>
				<li>Creating the job may take a few minutes and you might see a message "Propagating IAM role, please remain on the page." Once the creation is complete, you will be redirected to the Comprehend home screen as follows:<div id="_idContainer168" class="IMG---Figure"><img src="image/B16061_03_36.jpg" alt="Figure 3.36: The Comprehend home screen&#13;&#10;"/></div><p class="figure-caption">Figure 3.36: The Comprehend home screen</p><p class="callout-heading">Note</p><p class="callout">Bear in mind that clicking <strong class="source-inline">Create job</strong> starts the job as well. There is no separate "start a job" button. Also, if you want to redo the job, you will have to use the <strong class="source-inline">Copy</strong> button.</p></li>
				<li>While the job is being processed, the status displayed will be <strong class="source-inline">In Progress</strong>:<div id="_idContainer169" class="IMG---Figure"><img src="image/B16061_03_37.jpg" alt="Figure 3.37: In progress status displayed&#13;&#10;"/></div><p class="figure-caption">Figure 3.37: In progress status displayed</p></li>
				<li>On our account, it took around 4 minutes to complete the job. When the status changes to <strong class="source-inline">Completed</strong>, click the Topic Modeling job name:<div id="_idContainer170" class="IMG---Figure"><img src="image/B16061_03_38.jpg" alt="Figure 3.38: Completed status displayed&#13;&#10;"/></div><p class="figure-caption">Figure 3.38: Completed status displayed</p></li>
				<li>Now, scroll down to the <strong class="source-inline">Output</strong> section:<div id="_idContainer171" class="IMG---Figure"><img src="image/B16061_03_39.jpg" alt="Figure 3.39: Topic Modeling output display home screen&#13;&#10;"/></div><p class="figure-caption">Figure 3.39: Topic Modeling output display home screen</p></li>
				<li>Click the hyperlink under <strong class="source-inline">Data location</strong>:<div id="_idContainer172" class="IMG---Figure"><img src="image/B16061_03_40.jpg" alt="Figure 3.40: Topic Modeling data output hyperlinked location&#13;&#10;"/></div><p class="figure-caption">Figure 3.40: Topic Modeling data output hyperlinked location</p><p>This will take you directly to the S3 bucket:</p><div id="_idContainer173" class="IMG---Figure"><img src="image/B16061_03_41.jpg" alt="Figure 3.41: Topic Modeling output file in S3&#13;&#10;"/></div><p class="figure-caption">Figure 3.41: Topic Modeling output file in S3</p></li>
				<li>Click <strong class="source-inline">Download</strong> and save the file in your local disk. Usually, the <strong class="source-inline">Downloads</strong> folder is an ideal location:<div id="_idContainer174" class="IMG---Figure"><img src="image/B16061_03_42.jpg" alt="Figure 3.42: Topic Modeling downloading the output file to the local disk&#13;&#10;"/></div><p class="figure-caption">Figure 3.42: Topic Modeling downloading the output file to the local disk</p></li>
				<li>Extract <strong class="source-inline">output.tar.gz</strong> and usually, it will show up in a directory output:<div id="_idContainer175" class="IMG---Figure"><img src="image/B16061_03_43.jpg" alt="Figure 3.43: Output files from Topic Modeling&#13;&#10;"/></div><p class="figure-caption">Figure 3.43: Output files from Topic Modeling</p></li>
				<li>Now examine the two files: <strong class="source-inline">topic-terms.xlsx</strong> and <strong class="source-inline">doc-topics.xlsx</strong>:<p class="callout-heading">Note</p><p class="callout">Your <strong class="source-inline">topic-terms.csv</strong> and <strong class="source-inline">doc-topics.csv</strong> results should be the same as the following results. If your results are NOT the same, use the output files for the remainder of the chapter, which are located at <em class="italic">Chapter03/Exercise3.01/topic-terms.csv</em> <a href="https://packt.live/3iHlH5y">https://packt.live/3iHlH5y</a> and <em class="italic">Chapter03/Exercise3.01/doc-topics.csv</em> <a href="https://packt.live/2ZMTaTw">https://packt.live/2ZMTaTw</a>.</p><p>The following is the output generated. As we had indicated that we want to have topics, Comprehend has segregated the relevant words into two groups/topics as well as the weights. It doesn't know what the topics are, but has inferred the similarity of the words to one of the two topics:</p><div id="_idContainer176" class="IMG---Figure"><img src="image/B16061_03_44.jpg" alt="Figure 3.44: topic-terms.csv result&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 3.44: topic-terms.csv result</p>
			<p>The <strong class="source-inline">doc-topics.csv</strong> shows the affinity of the documents to the topics. In this case, it is very deterministic, but if we have more topics, the proportion will show the strength of the topics in each of the documents:</p>
			<div>
				<div id="_idContainer177" class="IMG---Figure">
					<img src="image/B16061_03_45.jpg" alt="Figure 3.45: doc-topics.csv results&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.45: doc-topics.csv results</p>
			<p>In this exercise, we used Amazon Comprehend to infer topics embedded in a set of documents. While this is easier to do with two documents; Amazon Comprehend is very effective when we have hundreds of documents with multiple documents and we want to perform process automation.</p>
			<h2 id="_idParaDest-90"><a id="_idTextAnchor091"/>Exercise 3.02: Performing Known Structure Analysis Programmatically</h2>
			<p>While it is easy to look at one or two outputs, when we want to scale and analyze hundreds of documents with different topics, we need to use Comprehend programmatically. That is what we will do in this exercise.</p>
			<p>In this exercise, we will programmatically upload the CSV files (<strong class="source-inline">doc-topics.csv</strong> and <strong class="source-inline">Topic-terms.csv</strong>) to S3, merge the CSV files on the Topic column, and print the output to the console. The following are the steps for performing known structure analysis:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">For this step, you will be using Jupyter Notebook. You may either follow along with the exercise and type in the code or obtain it from the source code folder, <strong class="source-inline">local_csv_to_s3_for_analysis.ipynb</strong>, and paste it into the editor. The source code is available on GitHub in the following repository: <a href="https://packt.live/2BOqjWT">https://packt.live/2BOqjWT</a>. As explained in <em class="italic">Chapter 1</em>, <em class="italic">An Introduction to AWS</em>, you should have downloaded the repository to your local disk.</p>
			<ol>
				<li value="1">First, we will import <strong class="source-inline">boto3</strong> using the following command:<p class="source-code">import boto3</p></li>
				<li>Next, we will import <strong class="source-inline">pandas</strong> using the following command:<p class="source-code">import pandas as <strong class="bold">pd</strong></p></li>
				<li>Now, we will create the S3 client object using the following command:<p class="source-code"># Setup a region</p><p class="source-code">region = 'us-west-2'</p><p class="source-code"># Create an S3 client</p><p class="source-code">s3 = boto3.client('s3',region_name = region)</p></li>
				<li>Next, we will create a variable with a unique bucket name. Here, the selected bucket name is <strong class="source-inline">known-tm-analysis</strong>, but you will need to create a unique name:<p class="source-code"># Creates a variable with the bucket name</p><p class="source-code">#'&lt;insert a unique bucket name&gt;'</p><p class="source-code">bucket_name = '<strong class="bold">known-tm-analysis-20200302</strong>'</p></li>
				<li>Next, create a new bucket:<p class="source-code"># Create a location Constraint</p><p class="source-code">location = {'LocationConstraint': region}</p><p class="source-code"># Creates a new bucket </p><p class="source-code">s3.create_bucket(Bucket=bucket_name,\</p><p class="source-code">                 CreateBucketConfiguration=location)</p></li>
				<li>Create a list of the CSV filenames to import:<p class="source-code">filenames_list = ['<strong class="bold">doc-topics.csv'</strong>, '<strong class="bold">topic-terms.csv'</strong>]</p><p class="callout-heading">Note</p><p class="callout">Ensure that the two CSV files (highlighted) in the aforementioned step are stored in the same location where you're running the Jupyter Notebook code. An alternative is to specify the exact path as it exists on your local system. </p></li>
				<li>Now, iterate on each file to upload to S3 using the following line of code:<p class="source-code">for filename in filenames_list:</p><p class="source-code">    s3.upload_file(filename, bucket_name, filename)</p><p class="callout-heading">Note</p><p class="callout">Do not execute <em class="italic">steps 7</em> and <em class="italic">8</em> yet. We will show the code for the entire <strong class="source-inline">for</strong> block in <em class="italic">step 9</em>.</p></li>
				<li>Next, check whether the filename is <strong class="source-inline">doc-topics.csv</strong>: and get the <strong class="source-inline">doc-topics.csv</strong> file object and assign it to the <strong class="source-inline">obj</strong> variable.<p class="source-code">    if filename == 'doc-topics.csv':</p><p class="source-code">        obj = s3.get_object(Bucket=bucket_name, Key=filename)</p></li>
				<li>	Next, read the <strong class="source-inline">csv</strong> object and assign it to the <strong class="source-inline">doc_topics</strong> variable. You can see the entire code block, including steps <em class="italic">7</em> and <em class="italic">8</em> below:<p class="source-code">for filename in filenames_list:</p><p class="source-code">    # Uploads each CSV to the created bucket</p><p class="source-code">    s3.upload_file(filename, bucket_name, filename)</p><p class="source-code">    # checks if the filename is 'doc-topics.csv'</p><p class="source-code">    if filename == 'doc-topics.csv':</p><p class="source-code">        # gets the 'doc-topics.csv' file as an object</p><p class="source-code">        obj = s3.get_object(Bucket=bucket_name, Key=filename)</p><p class="source-code">        # reads the csv and assigns to doc_topics </p><p class="source-code">        doc_topics = pd.read_csv(obj['Body'])</p><p class="source-code">    else:</p><p class="source-code">        obj = s3.get_object(Bucket=bucket_name, Key=filename)</p><p class="source-code">        topic_terms = pd.read_csv(obj['Body'])</p></li>
				<li>Now, merge the files on the Topic column to obtain the most common terms per document using the following command:<p class="source-code">merged_df = pd.merge(doc_topics, topic_terms, on='topic')</p><p class="source-code"># print the merged_df to the console</p><p class="source-code">print(merged_df)</p></li>
				<li>Next, execute the notebook cells using the <em class="italic">Shift</em> + <em class="italic">Enter</em> keys:</li>
				<li>The console output is a merged DataFrame that provides the docnames with their respective terms and the term's weights (refer to the following):<div id="_idContainer178" class="IMG---Figure"><img src="image/B16061_03_46.jpg" alt="Figure 3.46: Output from the s3.create_bucket call&#13;&#10;"/></div><p class="figure-caption">Figure 3.46: Output from the s3.create_bucket call</p><div id="_idContainer179" class="IMG---Figure"><img src="image/B16061_03_47.jpg" alt="Figure 3.47: known_structure Topic Modeling merged results&#13;&#10;"/></div><p class="figure-caption">Figure 3.47: known_structure Topic Modeling merged results</p></li>
				<li>To verify the CSV files, navigate to S3 (reload the page if the new bucket does not appear), and the new bucket will have been created in S3. Click on the bucket to verify a successful import:<div id="_idContainer180" class="IMG---Figure"><img src="image/B16061_03_48.jpg" alt="Figure 3.48: known-tm-analysis S3 bucket&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 3.48: known-tm-analysis S3 bucket</p>
			<p>There will be two CSV files in the bucket – <strong class="source-inline">doc-topics.csv</strong> and <strong class="source-inline">topic-terms.csv</strong>:</p>
			<div>
				<div id="_idContainer181" class="IMG---Figure">
					<img src="image/B16061_03_49.jpg" alt="Figure 3.49: Topic Modeling results uploaded to S3&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.49: Topic Modeling results uploaded to S3</p>
			<p>In this exercise, we learned how to use Comprehend programmatically. We programmatically uploaded two CSV files to S3, merged them on a column, and printed the output to the console. </p>
			<h2 id="_idParaDest-91"><a id="_idTextAnchor092"/>Activity 3.01: Performing Topic Modeling on a Set of Documents with Unknown Topics</h2>
			<p>In this activity, we will perform Topic Modeling on a set of documents with unknown topics. Suppose your employer wants you to build a data pipeline to analyze negative movie reviews that are in individual text files with a unique ID filename. Thus, you need to perform Topic Modeling to determine which files represent the respective topics. Overall, negative reviews represent a loss to the company, so they are prioritizing negative reviews over positive reviews. The company's end goal is to incorporate the data into a feedback chatbot application. To ensure that this happens correctly, you need a file that contains negative comments. The expected outcome for this activity will be the Topic Modeling results from the negative movie review files.</p>
			<p><strong class="bold">Performing Topic Modeling:</strong></p>
			<ol>
				<li value="1">Navigate to the following link (or to your local directory where you have downloaded the GitHub files) to obtain the text data file that contains negative review comments: <a href="https://packt.live/38Nw4jT">https://packt.live/38Nw4jT</a>.</li>
				<li>Create a bucket for Topic Modeling with a unique name.</li>
				<li>Create a folder for Topic Modeling.</li>
				<li>Import the dependencies of the Python library, such as <strong class="source-inline">os</strong> and <strong class="source-inline">boto3</strong>.</li>
				<li>Mention your unique bucket name.</li>
				<li>Gather all of the working directories of the local path and make them into text files.</li>
				<li>Create a list for all of the text files.</li>
				<li>Iterate the files and upload them to S3.</li>
				<li>Create a job in Organization using Amazon Comprehend.</li>
				<li>As per requirements, choose the input data. This may be <strong class="bold">My document</strong> or <strong class="bold">Example document</strong>.</li>
				<li>Choose the file from the data source.</li>
				<li>Apply the input format.</li>
				<li>Provide the number of topics to perform the modeling.</li>
				<li>Choose an IAM role and create a job.</li>
				<li>Download the output file and extract the file.</li>
				<li>The generated output will include the two CSV files.<p><strong class="bold">Analysis of Unknown Topics:</strong></p></li>
				<li>Import dependences of the Python library, such as <strong class="source-inline">boto3</strong> and <strong class="source-inline">pandas</strong>.</li>
				<li>Create an S3 client.</li>
				<li>Create a new bucket with a unique name.</li>
				<li>Create a list of CSV filenames to import.</li>
				<li>Check the filename and assign it to the <strong class="bold">obj</strong> variable.</li>
				<li>Read the <strong class="bold">obj</strong> variable.</li>
				<li>Merge the files on the Topic column.</li>
				<li>Print the merged files to the console.</li>
			</ol>
			<p>This is a long activity. Yet, you were able to manage 1,000 files, upload them to S3, perform Topic Modeling using Amazon Comprehend, and then merge the results into a table that had more than 40,000 rows. In real-world situations, you will be handling thousands of documents, not just one or two. That is the reason we did this activity using Jupyter Notebook and Python.</p>
			<p>However, this is only the first step in a multi-step automation process — an important and essential step of inferencing on the unstructured documents. While Comprehend analyzed the documents and gave us a list of topics, it is still our job to figure out what to do with them.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution for this activity can be found on page 291.</p>
			<h1 id="_idParaDest-92"><a id="_idTextAnchor093"/>Summary</h1>
			<p>In this chapter, we learned about analyzing Topic Modeling results from AWS Comprehend. You are now able to incorporate S3 to store data and use it to perform analysis. Also, we learned how to analyze documents where we know the topics before performing Topic Modeling, as well as documents where the topic is unknown. We know that the latter requires additional analysis to determine the relevant topics.</p>
			<p>We did not build the downstream systems that analyze the topic lists and then route the document appropriately. For example, you might have a mapping of the topics to a SharePoint folder for knowledge management or a workflow to route the files via email to appropriate persons depending on the topics detected. While the broader topic of <strong class="bold">Robotic Process Automation</strong> (<strong class="bold">RPA</strong>) is beyond the scope of this book, you have learned how to use Amazon Comprehend to implement the Topic and Theme detection steps for process automation.</p>
			<p>Another application of what you learned in this chapter is document clustering for knowledge management. In this case, we would restrict the number of topics to 10 and then segregate the documents based on their major topics. For example, if these documents were news articles, this process would divide the articles into 10 subjects, which are easier to handle in downstream systems such as a new recommendation engine.</p>
			<p>As you can see, Topic Modeling can be applied in a variety of applications and systems. Now you have the skills required to perform Topic Modeling using Amazon Comprehend.</p>
			<p>In the next chapter, we will dive into the concept of chatbots and their use of natural language processing.</p>
		</div>
		<div>
			<div id="_idContainer183" class="Content">
			</div>
		</div>
	</body></html>