<html><head></head><body>
		<div><h1 id="_idParaDest-43" class="chapter-number"><a id="_idTextAnchor064"/>3</h1>
			<h1 id="_idParaDest-44"><a id="_idTextAnchor065"/>Generating Images Using Stable Diffusion</h1>
			<p>In this chapter, we will start using common Stable Diffusion functionalities by leveraging the Hugging Face Diffusers package (<a href="https://github.com/huggingface/diffusers">https://github.com/huggingface/diffusers</a>) and open-source packages. As we mentioned in <a href="B21263_01.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a>, <em class="italic">Introduction to Stable Diffusion</em>, Hugging Face Diffusers is currently the most widely used Python implementation of Stable Diffusion. As we explore image generation, we will walk through the common terminologies used.</p>
			<p>Assume you have all the packages and dependencies installed; if you see an error message saying no GPU is found or CUDA is required, refer to <a href="B21263_02.xhtml#_idTextAnchor037"><em class="italic">Chapter 2</em></a> to set up the environment to run Stable Diffusion.</p>
			<p>With this chapter, I aim to familiarize you with Stable Diffusion by using the Diffusers package from Hugging Face. We will dig into the internals of Stable Diffusion in the next chapter.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>How to log in to Hugging Face with Hugging Face tokens</li>
				<li>Generating an image using Stable Diffusion</li>
				<li>Using a generation seed to reproduce an image</li>
				<li>Using the Stable Diffusion scheduler</li>
				<li>Swapping or changing a Stable Diffusion model</li>
				<li>Using a guidance scale</li>
			</ul>
			<p>Let’s start.</p>
			<h1 id="_idParaDest-45"><a id="_idTextAnchor066"/>Logging in to Hugging Face</h1>
			<p>You may use the <code>login()</code> function<a id="_idIndexMarker086"/> in the <code>huggingface_hub</code> library like this:</p>
			<pre class="source-code">
from huggingface_hub import login
login()</pre>
			<p>In doing so, you are authenticating with the Hugging Face Hub. This allows you to download pre-trained diffusion models that are hosted on the Hub. Without logging in, you may not be able to download these models using the model ID, such as <code>runwayml/stable-diffusion-v1-5</code>.</p>
			<p>When you run the preceding code, you are providing your Hugging Face token. You may wonder about the steps to <em class="italic">access</em> the token, but don’t worry. The token input dialog will provide links and information to <em class="italic">access</em> the token.</p>
			<p>After you have logged in, you can download pre-trained diffusion models by using the <code>from_pretrained()</code> function in the Diffusers package. For example, the following code will download the <code>stable-diffusion-v1-5</code> model from the Hugging Face Hub:</p>
			<pre class="source-code">
import torch
from diffusers import StableDiffusionPipeline
text2img_pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype = torch.float16
).to("cuda:0")</pre>
			<p class="callout-heading">Note</p>
			<p class="callout">You may have noticed that I am using <code>to("cuda:0")</code> instead of <code>to("cuda")</code> because in the case of multiple-GPU scenarios, you can change the CUDA index to tell Diffusers to use a specified GPU. For instance, you can use <code>to("cuda:1")</code> to use the second CUDA-enabled GPU to generate Stable Diffusion images.</p>
			<p>After downloading the model, it is time to generate an image using Stable Diffusion.<a id="_idTextAnchor067"/></p>
			<h1 id="_idParaDest-46"><a id="_idTextAnchor068"/>Generating an image</h1>
			<p>Now that we have the Stable Diffusion <a id="_idIndexMarker087"/>model loaded up to the GPU, let’s generate an image. <code>text2img_pipe</code> holds the pipeline object; all we need to provide is a <code>prompt</code> string, using natural language to describe the image we want to generate, as shown in the following code:</p>
			<pre class="source-code">
# generate an image
prompt ="high resolution, a photograph of an astronaut riding a horse"
image = text2img_pipe(
    prompt = prompt
).images[0]
image</pre>
			<p>Feel free to change the prompt to anything else that comes to your mind when you are reading this, for example, <code>high resolution, a photograph of a cat running on the surface of Mars</code> or <code>4k, high quality image of a cat driving a plane</code>. It is amazing how Stable Diffusion can generate images according to a description in purely natural language.</p>
			<p>If you run the preceding code without changing it, you may see an image like this showing up:</p>
			<div><div><img src="img/B21263_03_01.jpg" alt="Figure 3.1: An image of an astronaut riding a horse"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.1: An image of an astronaut riding a horse</p>
			<p>I said <em class="italic">you may see an image like this</em> because <a id="_idIndexMarker088"/>there is a 99.99% chance you will not see the same image; instead, you will see an image with a similar look and feel. To make the generation consistent, we will need another parameter, called <code>generator<a id="_idTextAnchor069"/></code>.</p>
			<h1 id="_idParaDest-47"><a id="_idTextAnchor070"/>Generation seed</h1>
			<p>In Stable Diffusion, a seed is a random number that is used to initialize the generation process. The<a id="_idIndexMarker089"/> seed is used to create a noise tensor, which is then used by the diffusion model to generate an image. The same seed together with the same prompt and settings will generally produce the same image.</p>
			<p>The <a id="_idIndexMarker090"/>generation seed is needed for two reasons:</p>
			<ul>
				<li><strong class="bold">Reproducibility</strong>: By using the same seed, you can consistently generate the same image with identical settings and prompts.</li>
				<li><strong class="bold">Exploration</strong>: You can discover diverse image variations by altering the seed number. This often leads to the emergence of novel and intriguing images.</li>
			</ul>
			<p>When a seed number is not provided, the Diffusers package automatically generates a random number for each image creation process. However, you have the option to specify your preferred seed number, as demonstrated in the following Python code:</p>
			<pre class="source-code">
my_seed = 1234
generator = torch.Generator("cuda:0").manual_seed(my_seed)
prompt ="high resolution, a photograph of an astronaut riding a horse"
image = text2img_pipe(
    prompt = prompt,
    generator = generator
).images[0]
display(image)</pre>
			<p>In the preceding code, we use <code>torch</code> to create a <code>torch.Generator</code> object with a manual seed provided. We specifically use this generator for image generation. By doing this, we can reproduce the same image repeatedly.</p>
			<p>The generation seed is one method to control Stable Diffusion image generation. Next, let's explore the scheduler for further customizati<a id="_idTextAnchor071"/>on.</p>
			<h1 id="_idParaDest-48"><a id="_idTextAnchor072"/>Sampling scheduler</h1>
			<p>After discussing the generation seed, let’s now delve into another <a id="_idIndexMarker091"/>essential aspect of Stable Diffusion image generation: the sampling scheduler.</p>
			<p>The original Diffusion models have demonstrated impressive results in generating images. However, one drawback is the slow reverse-denoising process, which typically requires 1,000 steps to transform a random noise data space into a coherent image (specifically, latent data space, a concept we will explore further in <a href="B21263_04.xhtml#_idTextAnchor081"><em class="italic">Chapter 4</em></a>). This lengthy process can be burdensome.</p>
			<p>To shorten the image generation process, several solutions have been brought out by researchers. The idea is simple: instead of denoising 1,000 steps, what if we could take a sample and only perform the key steps on that sample? And this idea works. Samplers or schedulers enable the Diffusion model to generate an image in a mere 20 steps!</p>
			<p>In the Hugging Face Diffusers package, these helpful components<a id="_idIndexMarker092"/> are referred to as <strong class="bold">schedulers</strong>. However, you may also encounter the term <strong class="bold">sampler</strong> in other <a id="_idIndexMarker093"/>resources. You may take a look at the Diffusers <em class="italic">Schedulers</em> [2] page for the latest supported schedulers.</p>
			<p>By default, the Diffusers package uses <code>PNDMScheduler</code>. We can find it by running this line of code:</p>
			<pre class="source-code">
# Check out the current scheduler
text2img_pipe.scheduler</pre>
			<p>The code will return an object like this:</p>
			<pre class="source-code">
PNDMScheduler {
  "_class_name": "PNDMScheduler",
  "_diffusers_version": "0.17.1",
  "beta_end": 0.012,
  "beta_schedule": "scaled_linear",
  "beta_start": 0.00085,
  "clip_sample": false,
  "num_train_timesteps": 1000,
  "prediction_type": "epsilon",
  "set_alpha_to_one": false,
  "skip_prk_steps": true,
  "steps_offset": 1,
  "trained_betas": null
}</pre>
			<p>At first glance, the <code>PNDMScheduler</code> object’s fields might seem complex and unfamiliar. However, as you delve deeper into the internals of the Stable Diffusion model in <em class="italic">Chapters 4</em> and <em class="italic">5</em>, these fields will become more familiar and comprehensible. The learning journey ahead promises to unravel the intricacies of the Stable Diffusion model and shed light on the purpose and significance of each field within the <code>PNDMScheduler</code> object.</p>
			<p>Many list schedulers can generate<a id="_idIndexMarker094"/> images in as few as 20 to 50 steps. Based on my experience, the <code>Euler</code> scheduler is one of the top choices. Let’s apply the <code>Euler</code> scheduler to generate an image:</p>
			<pre class="source-code">
from diffusers import EulerDiscreteScheduler
text2img_pipe.scheduler = EulerDiscreteScheduler.from_config(
    text2img_pipe.scheduler.config)
generator = torch.Generator("cuda:0").manual_seed(1234)
prompt ="high resolution, a photograph of an astronaut riding a horse"
image = text2img_pipe(
    prompt = prompt,
    generator = generator
).images[0]
display(image)</pre>
			<p>You can customize the number of denoising steps by using the <code>num_inference_steps</code> parameter. A higher step count generally leads to better image quality. Here, we set the scheduling steps to <code>20</code> and compared the results of the default <code>PNDMScheduler</code> and <code>EulerDiscreteScheduler</code>:</p>
			<pre class="source-code">
# Euler scheduler with 20 steps
from diffusers import EulerDiscreteScheduler
text2img_pipe.scheduler = EulerDiscreteScheduler.from_config(
    text2img_pipe.scheduler.config)
generator = torch.Generator("cuda:0").manual_seed(1234)
prompt ="high resolution, a photograph of an astronaut riding a horse"
image = text2img_pipe(
    prompt = prompt,
    generator = generator,
    num_inference_steps = 20
).images[0]
display(image)</pre>
			<p>The following figure shows <a id="_idIndexMarker095"/>the difference between the two schedulers:</p>
			<div><div><img src="img/B21263_03_02.jpg" alt="Figure 3.2: Left: Euler scheduler with 20 steps; right: PNDMScheduler with 20 steps"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.2: Left: Euler scheduler with 20 steps; right: PNDMScheduler with 20 steps</p>
			<p>In this comparison, the Euler scheduler correctly <a id="_idIndexMarker096"/>generates an image with all four horse legs, while the PNDM scheduler provides more detail but misses one horse leg. These schedulers perform remarkably well, reducing the entire image generation process from 1,000 steps to just 20 steps, making it feasible to run Stable Diffusion on home computers.</p>
			<p>Note that each scheduler has advantages and disadvantages. You may need to try out the schedulers to find out which one fits the best.</p>
			<p>Next, let’s explore the process of replacing the original Stable Diffusion model with a community-contributed, fine-tuned alte<a id="_idTextAnchor073"/>rnative.</p>
			<h1 id="_idParaDest-49"><a id="_idTextAnchor074"/>Changing a model</h1>
			<p>At the time of writing<a id="_idIndexMarker097"/> this chapter, there are numerous models available, fine-tuned based on the V1.5 Stable Diffusion model, contributed by the thriving user community. If the model file is hosted on Hugging Face, you can easily switch to a different model by changing its identifier, as shown in the following code snippet:</p>
			<pre class="source-code">
# Change model to "stablediffusionapi/deliberate-v2"
from diffusers import StableDiffusionPipeline
text2img_pipe = StableDiffusionPipeline.from_pretrained(
    "stablediffusionapi/deliberate-v2",
    torch_dtype = torch.float16
).to("cuda:0")
prompt ="high resolution, a photograph of an astronaut riding a horse"
image = text2img_pipe(
    prompt = prompt
).images[0]
display(image)</pre>
			<p>Additionally, you can also use a <code>ckpt/safetensors</code> model downloaded<a id="_idIndexMarker098"/> from civitai.com (<a href="http://civitai.com">http://civitai.com</a>). Here, we demonstrate<a id="_idIndexMarker099"/> loading the <code>deliberate-v2</code> model using the following code:</p>
			<pre class="source-code">
from diffusers import StableDiffusionPipeline
text2img_pipe = StableDiffusionPipeline.from_single_file(
    "path/to/deliberate-v2.safetensors",
    torch_dtype = torch.float16
).to("cuda:0")
prompt ="high resolution, a photograph of an astronaut riding a horse"
image = text2img_pipe(
    prompt = prompt
).images[0]
display(image)</pre>
			<p>The primary difference when loading a model from a local file lies in the use of the <code>from_single_file</code> function instead of <code>from_pretrained</code>. A <code>ckpt</code> model file can be loaded up using the preceding code.</p>
			<p>In <a href="B21263_06.xhtml#_idTextAnchor117"><em class="italic">Chapter 6</em></a> of this book, we will focus exclusively on model loading, covering both Hugging Face and local storage methods. By experimenting with various models, you can discover improvements, unique artistic styles, or better compatibility for specific use cases.</p>
			<p>We have touched on the generation<a id="_idIndexMarker100"/> seed, scheduler, and model usage. Another parameter that plays a key role is <code>guidance_scale</code>. Let’s take a look<a id="_idTextAnchor075"/> at it next.</p>
			<h1 id="_idParaDest-50"><a id="_idTextAnchor076"/>Guidance scale</h1>
			<p>Guidance scale or <strong class="bold">Classifier-Free Guidance</strong> (<strong class="bold">CFG</strong>) is a parameter that controls<a id="_idIndexMarker101"/> the adherence<a id="_idIndexMarker102"/> of the generated image to the text prompt. A higher guidance scale will force the image to be more aligned with the prompt, while a lower guidance scale will give more space for Stable Diffusion to decide what to put into the image.</p>
			<p>Here is a sample of applying a different guidance scale while keeping other parameters the same:</p>
			<pre class="source-code">
import torch
generator = torch.Generator("cuda:0").manual_seed(123)
prompt = """high resolution, a photograph of an astronaut riding a horse on mars"""
image_3_gs = text2img_pipe(
    prompt = prompt,
    num_inference_steps = 30,
    guidance_scale = 3,
    generator = generator
).images[0]
image_7_gs = text2img_pipe(
    prompt = prompt,
    num_inference_steps = 30,
    guidance_scale = 7,
    generator = generator
).images[0]
image_10_gs = text2img_pipe(
    prompt = prompt,
    num_inference_steps = 30,
    guidance_scale = 10,
    generator = generator
).images[0]
from diffusers.utils import make_image_grid
images = [image_3_gs,image_7_gs,image_10_gs]
make_image_grid(images,rows=1,cols=3)</pre>
			<p><em class="italic">Figure 3</em><em class="italic">.3</em> provides<a id="_idIndexMarker103"/> a side-by-side<a id="_idIndexMarker104"/> comparison:</p>
			<div><div><img src="img/B21263_03_03.jpg" alt="Figure 3.3: Left: guidance_scale = 3; middle: guidance_scale = 7; right: guidance_scale = 10"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.3: Left: guidance_scale = 3; middle: guidance_scale = 7; right: guidance_scale = 10</p>
			<p>In practice, besides prompt adherence, we can notice that a high guidance scale setting has the following effects:</p>
			<ul>
				<li>Increases the color saturation</li>
				<li>Increases the contrast</li>
				<li>May lead to a blurred image if set too high</li>
			</ul>
			<p>The <code>guidance_scale</code> parameter<a id="_idIndexMarker105"/> is typically set<a id="_idIndexMarker106"/> between <code>7</code> and <code>8.5</code>. A value of <code>7.5</code> is a go<a id="_idTextAnchor077"/>od default value.</p>
			<h1 id="_idParaDest-51"><a id="_idTextAnchor078"/>Summary</h1>
			<p>In this chapter, we explored the essentials of using Stable Diffusion through the Hugging Face Diffusers package. We accomplished the following:</p>
			<ul>
				<li>Logged in to Hugging Face to enable automatic model downloads</li>
				<li>Generated images deterministically using the generator</li>
				<li>Utilized the scheduler for efficient image creation</li>
				<li>Adjusted the guidance scale for desired image qualities</li>
			</ul>
			<p>With just a few lines of code, we successfully created images, demonstrating the remarkable capabilities of the Diffusers package. This chapter did not cover every feature and option; keep in mind that the package is continually evolving, with new functions and enhancements regularly added.</p>
			<p>For those eager to unlock the full potential of the Diffusers package, I encourage you to explore its source code. Dive into the inner workings, uncover hidden gems, and build a Stable Diffusion pipeline from scratch. A rewarding journey awaits!</p>
			<pre class="console">
git clone https://github.com/huggingface/diffusers</pre>
			<p>In the next chapter, we will delve into the internals of the package and learn how to construct a custom Stable Diffusion pipeline tailored to your unique needs<a id="_idTextAnchor079"/> and preferences.</p>
			<h1 id="_idParaDest-52"><a id="_idTextAnchor080"/>References</h1>
			<ol>
				<li><em class="italic">High-Resolution Image Synthesis with Latent Diffusion </em><em class="italic">Models</em>: <a href="https://arxiv.org/abs/2112.10752">https://arxiv.org/abs/2112.10752</a></li>
				<li>Hugging Face Diffusers schedulers: <a href="https://huggingface.co/docs/diffusers/api/schedulers/overview">https://huggingface.co/docs/diffusers/api/schedulers/overview</a></li>
			</ol>
		</div>
	</body></html>