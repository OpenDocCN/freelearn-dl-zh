<html><head></head><body>
		<div id="_idContainer023">
			<h1 id="_idParaDest-43" class="chapter-number"><a id="_idTextAnchor064"/>3</h1>
			<h1 id="_idParaDest-44"><a id="_idTextAnchor065"/>Generating Images Using Stable Diffusion</h1>
			<p>In this chapter, we will start using common Stable Diffusion functionalities by leveraging the Hugging Face Diffusers package (<a href="https://github.com/huggingface/diffusers">https://github.com/huggingface/diffusers</a>) and open-source packages. As we mentioned in <a href="B21263_01.xhtml#_idTextAnchor015"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>, <em class="italic">Introduction to Stable Diffusion</em>, Hugging Face Diffusers is currently the most widely used Python implementation of Stable Diffusion. As we explore image generation, we will walk through the common <span class="No-Break">terminologies used.</span></p>
			<p>Assume you have all the packages and dependencies installed; if you see an error message saying no GPU is found or CUDA is required, refer to <a href="B21263_02.xhtml#_idTextAnchor037"><span class="No-Break"><em class="italic">Chapter 2</em></span></a> to set up the environment to run <span class="No-Break">Stable Diffusion.</span></p>
			<p>With this chapter, I aim to familiarize you with Stable Diffusion by using the Diffusers package from Hugging Face. We will dig into the internals of Stable Diffusion in the <span class="No-Break">next chapter.</span></p>
			<p>In this chapter, we will cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>How to log in to Hugging Face with Hugging <span class="No-Break">Face tokens</span></li>
				<li>Generating an image using <span class="No-Break">Stable Diffusion</span></li>
				<li>Using a generation seed to reproduce <span class="No-Break">an image</span></li>
				<li>Using the Stable <span class="No-Break">Diffusion scheduler</span></li>
				<li>Swapping or changing a Stable <span class="No-Break">Diffusion model</span></li>
				<li>Using a <span class="No-Break">guidance scale</span></li>
			</ul>
			<p><span class="No-Break">Let’s start.</span></p>
			<h1 id="_idParaDest-45"><a id="_idTextAnchor066"/>Logging in to Hugging Face</h1>
			<p>You may use the <strong class="source-inline">login()</strong> function<a id="_idIndexMarker086"/> in the <strong class="source-inline">huggingface_hub</strong> library <span class="No-Break">like this:</span></p>
			<pre class="source-code">
from huggingface_hub import login
login()</pre>
			<p>In doing so, you are authenticating with the Hugging Face Hub. This allows you to download pre-trained diffusion models that are hosted on the Hub. Without logging in, you may not be able to download these models using the model ID, such <span class="No-Break">as </span><span class="No-Break"><strong class="source-inline">runwayml/stable-diffusion-v1-5</strong></span><span class="No-Break">.</span></p>
			<p>When you run the preceding code, you are providing your Hugging Face token. You may wonder about the steps to <em class="italic">access</em> the token, but don’t worry. The token input dialog will provide links and information to <em class="italic">access</em> <span class="No-Break">the token.</span></p>
			<p>After you have logged in, you can download pre-trained diffusion models by using the <strong class="source-inline">from_pretrained()</strong> function in the Diffusers package. For example, the following code will download the <strong class="source-inline">stable-diffusion-v1-5</strong> model from the Hugging <span class="No-Break">Face Hub:</span></p>
			<pre class="source-code">
import torch
from diffusers import StableDiffusionPipeline
text2img_pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype = torch.float16
).to("cuda:0")</pre>
			<p class="callout-heading">Note</p>
			<p class="callout">You may have noticed that I am using <strong class="source-inline">to("cuda:0")</strong> instead of <strong class="source-inline">to("cuda")</strong> because in the case of multiple-GPU scenarios, you can change the CUDA index to tell Diffusers to use a specified GPU. For instance, you can use <strong class="source-inline">to("cuda:1")</strong> to use the second CUDA-enabled GPU to generate Stable <span class="No-Break">Diffusion images.</span></p>
			<p>After downloading the model, it is time to generate an image using <span class="No-Break">Stable Diffusion.</span><a id="_idTextAnchor067"/></p>
			<h1 id="_idParaDest-46"><a id="_idTextAnchor068"/>Generating an image</h1>
			<p>Now that we have the Stable Diffusion <a id="_idIndexMarker087"/>model loaded up to the GPU, let’s generate an image. <strong class="source-inline">text2img_pipe</strong> holds the pipeline object; all we need to provide is a <strong class="source-inline">prompt</strong> string, using natural language to describe the image we want to generate, as shown in the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
# generate an image
prompt ="high resolution, a photograph of an astronaut riding a horse"
image = text2img_pipe(
    prompt = prompt
).images[0]
image</pre>
			<p>Feel free to change the prompt to anything else that comes to your mind when you are reading this, for example, <strong class="source-inline">high resolution, a photograph of a cat running on the surface of Mars</strong> or <strong class="source-inline">4k, high quality image of a cat driving a plane</strong>. It is amazing how Stable Diffusion can generate images according to a description in purely <span class="No-Break">natural language.</span></p>
			<p>If you run the preceding code without changing it, you may see an image like this <span class="No-Break">showing up:</span></p>
			<div>
				<div id="_idContainer020" class="IMG---Figure">
					<img src="image/B21263_03_01.jpg" alt="Figure 3.1: An image of an astronaut riding a horse"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.1: An image of an astronaut riding a horse</p>
			<p>I said <em class="italic">you may see an image like this</em> because <a id="_idIndexMarker088"/>there is a 99.99% chance you will not see the same image; instead, you will see an image with a similar look and feel. To make the generation consistent, we will need another parameter, <span class="No-Break">called </span><span class="No-Break"><strong class="source-inline">generator<a id="_idTextAnchor069"/></strong></span><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-47"><a id="_idTextAnchor070"/>Generation seed</h1>
			<p>In Stable Diffusion, a seed is a random number that is used to initialize the generation process. The<a id="_idIndexMarker089"/> seed is used to create a noise tensor, which is then used by the diffusion model to generate an image. The same seed together with the same prompt and settings will generally produce the <span class="No-Break">same image.</span></p>
			<p>The <a id="_idIndexMarker090"/>generation seed is needed for <span class="No-Break">two reasons:</span></p>
			<ul>
				<li><strong class="bold">Reproducibility</strong>: By using the same seed, you can consistently generate the same image with identical settings <span class="No-Break">and prompts.</span></li>
				<li><strong class="bold">Exploration</strong>: You can discover diverse image variations by altering the seed number. This often leads to the emergence of novel and <span class="No-Break">intriguing images.</span></li>
			</ul>
			<p>When a seed number is not provided, the Diffusers package automatically generates a random number for each image creation process. However, you have the option to specify your preferred seed number, as demonstrated in the following <span class="No-Break">Python code:</span></p>
			<pre class="source-code">
my_seed = 1234
generator = torch.Generator("cuda:0").manual_seed(my_seed)
prompt ="high resolution, a photograph of an astronaut riding a horse"
image = text2img_pipe(
    prompt = prompt,
    generator = generator
).images[0]
display(image)</pre>
			<p>In the preceding code, we use <strong class="source-inline">torch</strong> to create a <strong class="source-inline">torch.Generator</strong> object with a manual seed provided. We specifically use this generator for image generation. By doing this, we can reproduce the same <span class="No-Break">image repeatedly.</span></p>
			<p>The generation seed is one method to control Stable Diffusion image generation. Next, let's explore the scheduler for <span class="No-Break">further customizati<a id="_idTextAnchor071"/>on.</span></p>
			<h1 id="_idParaDest-48"><a id="_idTextAnchor072"/>Sampling scheduler</h1>
			<p>After discussing the generation seed, let’s now delve into another <a id="_idIndexMarker091"/>essential aspect of Stable Diffusion image generation: the <span class="No-Break">sampling scheduler.</span></p>
			<p>The original Diffusion models have demonstrated impressive results in generating images. However, one drawback is the slow reverse-denoising process, which typically requires 1,000 steps to transform a random noise data space into a coherent image (specifically, latent data space, a concept we will explore further in <a href="B21263_04.xhtml#_idTextAnchor081"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>). This lengthy process can <span class="No-Break">be burdensome.</span></p>
			<p>To shorten the image generation process, several solutions have been brought out by researchers. The idea is simple: instead of denoising 1,000 steps, what if we could take a sample and only perform the key steps on that sample? And this idea works. Samplers or schedulers enable the Diffusion model to generate an image in a mere <span class="No-Break">20 steps!</span></p>
			<p>In the Hugging Face Diffusers package, these helpful components<a id="_idIndexMarker092"/> are referred to as <strong class="bold">schedulers</strong>. However, you may also encounter the term <strong class="bold">sampler</strong> in other <a id="_idIndexMarker093"/>resources. You may take a look at the Diffusers <em class="italic">Schedulers</em> [2] page for the latest <span class="No-Break">supported schedulers.</span></p>
			<p>By default, the Diffusers package uses <strong class="source-inline">PNDMScheduler</strong>. We can find it by running this line <span class="No-Break">of code:</span></p>
			<pre class="source-code">
# Check out the current scheduler
text2img_pipe.scheduler</pre>
			<p>The code will return an object <span class="No-Break">like this:</span></p>
			<pre class="source-code">
PNDMScheduler {
  "_class_name": "PNDMScheduler",
  "_diffusers_version": "0.17.1",
  "beta_end": 0.012,
  "beta_schedule": "scaled_linear",
  "beta_start": 0.00085,
  "clip_sample": false,
  "num_train_timesteps": 1000,
  "prediction_type": "epsilon",
  "set_alpha_to_one": false,
  "skip_prk_steps": true,
  "steps_offset": 1,
  "trained_betas": null
}</pre>
			<p>At first glance, the <strong class="source-inline">PNDMScheduler</strong> object’s fields might seem complex and unfamiliar. However, as you delve deeper into the internals of the Stable Diffusion model in <em class="italic">Chapters 4</em> and <em class="italic">5</em>, these fields will become more familiar and comprehensible. The learning journey ahead promises to unravel the intricacies of the Stable Diffusion model and shed light on the purpose and significance of each field within the <span class="No-Break"><strong class="source-inline">PNDMScheduler</strong></span><span class="No-Break"> object.</span></p>
			<p>Many list schedulers can generate<a id="_idIndexMarker094"/> images in as few as 20 to 50 steps. Based on my experience, the <strong class="source-inline">Euler</strong> scheduler is one of the top choices. Let’s apply the <strong class="source-inline">Euler</strong> scheduler to generate <span class="No-Break">an image:</span></p>
			<pre class="source-code">
from diffusers import EulerDiscreteScheduler
text2img_pipe.scheduler = EulerDiscreteScheduler.from_config(
    text2img_pipe.scheduler.config)
generator = torch.Generator("cuda:0").manual_seed(1234)
prompt ="high resolution, a photograph of an astronaut riding a horse"
image = text2img_pipe(
    prompt = prompt,
    generator = generator
).images[0]
display(image)</pre>
			<p>You can customize the number of denoising steps by using the <strong class="source-inline">num_inference_steps</strong> parameter. A higher step count generally leads to better image quality. Here, we set the scheduling steps to <strong class="source-inline">20</strong> and compared the results of the default <strong class="source-inline">PNDMScheduler</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">EulerDiscreteScheduler</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
# Euler scheduler with 20 steps
from diffusers import EulerDiscreteScheduler
text2img_pipe.scheduler = EulerDiscreteScheduler.from_config(
    text2img_pipe.scheduler.config)
generator = torch.Generator("cuda:0").manual_seed(1234)
prompt ="high resolution, a photograph of an astronaut riding a horse"
image = text2img_pipe(
    prompt = prompt,
    generator = generator,
    num_inference_steps = 20
).images[0]
display(image)</pre>
			<p>The following figure shows <a id="_idIndexMarker095"/>the difference between the <span class="No-Break">two schedulers:</span></p>
			<div>
				<div id="_idContainer021" class="IMG---Figure">
					<img src="image/B21263_03_02.jpg" alt="Figure 3.2: Left: Euler scheduler with 20 steps; right: PNDMScheduler with 20 steps"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.2: Left: Euler scheduler with 20 steps; right: PNDMScheduler with 20 steps</p>
			<p>In this comparison, the Euler scheduler correctly <a id="_idIndexMarker096"/>generates an image with all four horse legs, while the PNDM scheduler provides more detail but misses one horse leg. These schedulers perform remarkably well, reducing the entire image generation process from 1,000 steps to just 20 steps, making it feasible to run Stable Diffusion on <span class="No-Break">home computers.</span></p>
			<p>Note that each scheduler has advantages and disadvantages. You may need to try out the schedulers to find out which one fits <span class="No-Break">the best.</span></p>
			<p>Next, let’s explore the process of replacing the original Stable Diffusion model with a community-contributed, <span class="No-Break">fine-tuned alte<a id="_idTextAnchor073"/>rnative.</span></p>
			<h1 id="_idParaDest-49"><a id="_idTextAnchor074"/>Changing a model</h1>
			<p>At the time of writing<a id="_idIndexMarker097"/> this chapter, there are numerous models available, fine-tuned based on the V1.5 Stable Diffusion model, contributed by the thriving user community. If the model file is hosted on Hugging Face, you can easily switch to a different model by changing its identifier, as shown in the following <span class="No-Break">code snippet:</span></p>
			<pre class="source-code">
# Change model to "stablediffusionapi/deliberate-v2"
from diffusers import StableDiffusionPipeline
text2img_pipe = StableDiffusionPipeline.from_pretrained(
    "stablediffusionapi/deliberate-v2",
    torch_dtype = torch.float16
).to("cuda:0")
prompt ="high resolution, a photograph of an astronaut riding a horse"
image = text2img_pipe(
    prompt = prompt
).images[0]
display(image)</pre>
			<p>Additionally, you can also use a <strong class="source-inline">ckpt/safetensors</strong> model downloaded<a id="_idIndexMarker098"/> from civitai.com (<a href="http://civitai.com">http://civitai.com</a>). Here, we demonstrate<a id="_idIndexMarker099"/> loading the <strong class="source-inline">deliberate-v2</strong> model using the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
from diffusers import StableDiffusionPipeline
text2img_pipe = StableDiffusionPipeline.from_single_file(
    "path/to/deliberate-v2.safetensors",
    torch_dtype = torch.float16
).to("cuda:0")
prompt ="high resolution, a photograph of an astronaut riding a horse"
image = text2img_pipe(
    prompt = prompt
).images[0]
display(image)</pre>
			<p>The primary difference when loading a model from a local file lies in the use of the <strong class="source-inline">from_single_file</strong> function instead of <strong class="source-inline">from_pretrained</strong>. A <strong class="source-inline">ckpt</strong> model file can be loaded up using the <span class="No-Break">preceding code.</span></p>
			<p>In <a href="B21263_06.xhtml#_idTextAnchor117"><span class="No-Break"><em class="italic">Chapter 6</em></span></a> of this book, we will focus exclusively on model loading, covering both Hugging Face and local storage methods. By experimenting with various models, you can discover improvements, unique artistic styles, or better compatibility for specific <span class="No-Break">use cases.</span></p>
			<p>We have touched on the generation<a id="_idIndexMarker100"/> seed, scheduler, and model usage. Another parameter that plays a key role is <strong class="source-inline">guidance_scale</strong>. Let’s take a look<a id="_idTextAnchor075"/> at <span class="No-Break">it next.</span></p>
			<h1 id="_idParaDest-50"><a id="_idTextAnchor076"/>Guidance scale</h1>
			<p>Guidance scale or <strong class="bold">Classifier-Free Guidance</strong> (<strong class="bold">CFG</strong>) is a parameter that controls<a id="_idIndexMarker101"/> the adherence<a id="_idIndexMarker102"/> of the generated image to the text prompt. A higher guidance scale will force the image to be more aligned with the prompt, while a lower guidance scale will give more space for Stable Diffusion to decide what to put into <span class="No-Break">the image.</span></p>
			<p>Here is a sample of applying a different guidance scale while keeping other parameters <span class="No-Break">the same:</span></p>
			<pre class="source-code">
import torch
generator = torch.Generator("cuda:0").manual_seed(123)
prompt = """high resolution, a photograph of an astronaut riding a horse on mars"""
image_3_gs = text2img_pipe(
    prompt = prompt,
    num_inference_steps = 30,
    guidance_scale = 3,
    generator = generator
).images[0]
image_7_gs = text2img_pipe(
    prompt = prompt,
    num_inference_steps = 30,
    guidance_scale = 7,
    generator = generator
).images[0]
image_10_gs = text2img_pipe(
    prompt = prompt,
    num_inference_steps = 30,
    guidance_scale = 10,
    generator = generator
).images[0]
from diffusers.utils import make_image_grid
images = [image_3_gs,image_7_gs,image_10_gs]
make_image_grid(images,rows=1,cols=3)</pre>
			<p><span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.3</em> provides<a id="_idIndexMarker103"/> a <span class="No-Break">side-by-side</span><span class="No-Break"><a id="_idIndexMarker104"/></span><span class="No-Break"> comparison:</span></p>
			<div>
				<div id="_idContainer022" class="IMG---Figure">
					<img src="image/B21263_03_03.jpg" alt="Figure 3.3: Left: guidance_scale = 3; middle: guidance_scale = 7; right: guidance_scale = 10"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.3: Left: guidance_scale = 3; middle: guidance_scale = 7; right: guidance_scale = 10</p>
			<p>In practice, besides prompt adherence, we can notice that a high guidance scale setting has the <span class="No-Break">following effects:</span></p>
			<ul>
				<li>Increases the <span class="No-Break">color saturation</span></li>
				<li>Increases <span class="No-Break">the contrast</span></li>
				<li>May lead to a blurred image if set <span class="No-Break">too high</span></li>
			</ul>
			<p>The <strong class="source-inline">guidance_scale</strong> parameter<a id="_idIndexMarker105"/> is typically set<a id="_idIndexMarker106"/> between <strong class="source-inline">7</strong> and <strong class="source-inline">8.5</strong>. A value of <strong class="source-inline">7.5</strong> is a go<a id="_idTextAnchor077"/>od <span class="No-Break">default value.</span></p>
			<h1 id="_idParaDest-51"><a id="_idTextAnchor078"/>Summary</h1>
			<p>In this chapter, we explored the essentials of using Stable Diffusion through the Hugging Face Diffusers package. We accomplished <span class="No-Break">the following:</span></p>
			<ul>
				<li>Logged in to Hugging Face to enable automatic <span class="No-Break">model downloads</span></li>
				<li>Generated images deterministically using <span class="No-Break">the generator</span></li>
				<li>Utilized the scheduler for efficient <span class="No-Break">image creation</span></li>
				<li>Adjusted the guidance scale for desired <span class="No-Break">image qualities</span></li>
			</ul>
			<p>With just a few lines of code, we successfully created images, demonstrating the remarkable capabilities of the Diffusers package. This chapter did not cover every feature and option; keep in mind that the package is continually evolving, with new functions and enhancements <span class="No-Break">regularly added.</span></p>
			<p>For those eager to unlock the full potential of the Diffusers package, I encourage you to explore its source code. Dive into the inner workings, uncover hidden gems, and build a Stable Diffusion pipeline from scratch. A rewarding <span class="No-Break">journey awaits!</span></p>
			<pre class="console">
git clone https://github.com/huggingface/diffusers</pre>
			<p>In the next chapter, we will delve into the internals of the package and learn how to construct a custom Stable Diffusion pipeline tailored to your unique needs<a id="_idTextAnchor079"/> <span class="No-Break">and preferences.</span></p>
			<h1 id="_idParaDest-52"><a id="_idTextAnchor080"/>References</h1>
			<ol>
				<li><em class="italic">High-Resolution Image Synthesis with Latent Diffusion </em><span class="No-Break"><em class="italic">Models</em></span><span class="No-Break">: </span><a href="https://arxiv.org/abs/2112.10752"><span class="No-Break">https://arxiv.org/abs/2112.10752</span></a></li>
				<li>Hugging Face Diffusers <span class="No-Break">schedulers: </span><a href="https://huggingface.co/docs/diffusers/api/schedulers/overview"><span class="No-Break">https://huggingface.co/docs/diffusers/api/schedulers/overview</span></a></li>
			</ol>
		</div>
	</body></html>