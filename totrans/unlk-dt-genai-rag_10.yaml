- en: <st c="0">10</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="3">Key RAG Components in LangChain</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="35">This chapter takes an in-depth look at the key technical components
    that we have been talking about as they relate to</st> **<st c="154">LangChain</st>**
    <st c="163">and</st> **<st c="168">retrieval-augmented generation</st>** <st c="198">(</st>**<st
    c="200">RAG)</st>**<st c="204">. As a refresher, the key technical components
    of our RAG system, in order of how they are used, are</st> **<st c="305">vector
    stores</st>**<st c="318">,</st> **<st c="320">retrievers</st>**<st c="330">, and</st>
    **<st c="336">large language models</st>** <st c="357">(</st>**<st c="359">LLMs</st>**<st
    c="363">).</st> <st c="367">We will step through the latest version of our code,
    last seen in</st> [*<st c="433">Chapter 8</st>*](B22475_08.xhtml#_idTextAnchor152)<st
    c="442">,</st> *<st c="444">Code lab 8.3</st>*<st c="456">. We will focus on each
    of these core components, and we will show the various options for each component
    using LangChain in the code.</st> <st c="591">Naturally, a lot of this discussion
    will highlight differences among each option and discuss the different scenarios
    in which one option might be better</st> <st c="744">over another.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="757">We start with a code lab outlining options for your</st> <st c="810">vector
    store.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="823">Technical requirements</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="846">The code for this chapter is placed in the following GitHub</st>
    <st c="907">repository:</st> [<st c="919">https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_10</st>](https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_10)
  prefs: []
  type: TYPE_NORMAL
- en: <st c="1016">Code lab 10.1 – LangChain vector store</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="1055">The goal for all these code</st> <st c="1084">labs is to help you
    become more familiar with how the</st> <st c="1137">options for each key component
    offered within the LangChain platform can enhance your RAG system.</st> <st c="1236">We
    will dive deep into what each component does, available functions, parameters
    that make a difference, and ultimately, all of the options you can take advantage
    of for a better RAG implementation.</st> <st c="1435">Starting with</st> *<st
    c="1449">Code lab 8.3</st>*<st c="1461">, (skipping</st> [*<st c="1473">Chapter
    9</st>*](B22475_09.xhtml#_idTextAnchor184)<st c="1482">’s evaluation code), we
    will step through these elements in order of how they appear in code, starting
    with the vector stores.</st> <st c="1610">You can find this code in its entirety
    in the</st> [*<st c="1656">Chapter 10</st>*](B22475_10.xhtml#_idTextAnchor218)
    <st c="1666">code folder on GitHub also labeled</st> <st c="1702">as</st> `<st
    c="1705">10.1</st>`<st c="1709">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="1710">Vector stores, LangChain, and RAG</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**<st c="1744">Vector stores</st>** <st c="1758">play</st> <st c="1764">a crucial
    role in RAG systems by efficiently storing and</st> <st c="1821">indexing vector
    representations of the knowledge base documents.</st> <st c="1886">LangChain</st>
    <st c="1895">provides seamless</st> <st c="1914">integration with various vector
    store</st> <st c="1952">implementations, such as</st> **<st c="1977">Chroma</st>**<st
    c="1983">,</st> **<st c="1985">Weaviate</st>**<st c="1993">,</st> **<st c="1995">FAISS</st>**
    <st c="2000">(</st>**<st c="2002">Facebook AI Similarity Search</st>**<st c="2031">),</st>
    **<st c="2035">pgvector</st>**<st c="2043">, and</st> **<st c="2049">Pinecone</st>**<st
    c="2057">. For this</st> <st c="2068">code lab, we will show the code for adding</st>
    <st c="2110">your data to Chroma, Weaviate, and FAISS, laying the groundwork for
    you to be able to integrate any vector store among the many that LangChain offers.</st>
    <st c="2262">These vector stores offer high-performance similarity search capabilities,
    enabling fast retrieval of relevant documents based on the</st> <st c="2396">query
    vector.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2409">LangChain’s vector store class serves as a unified interface for
    interacting with different vector store backends.</st> <st c="2525">It provides
    methods for adding documents to the vector store, performing similarity searches,
    and retrieving the stored documents.</st> <st c="2656">This abstraction allows
    developers to easily switch between vector store implementations without modifying
    the core</st> <st c="2772">retrieval logic.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2788">When building a RAG system with LangChain, you can leverage the
    vector store class to efficiently store and retrieve document vectors.</st> <st
    c="2924">The choice of vector store depends on factors such as scalability, search
    performance, and deployment requirements.</st> <st c="3040">Pinecone, for example,
    offers a fully managed vector database with high scalability and real-time search
    capabilities, making it suitable for production-grade RAG systems.</st> <st c="3212">On
    the other hand, FAISS provides an open source library for efficient similarity
    search, which can be used for local development and experimentation.</st> <st
    c="3363">Chroma is a popular place for developers to start when building their
    first RAG pipelines due to its ease of use and its effective integration</st>
    <st c="3506">with LangChain.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3521">If you look at the code we discussed in previous chapters, we are
    already using Chroma.</st> <st c="3610">Here is a snippet of that code showing
    our use of Chroma, which you can find in the code for this code lab</st> <st c="3717">as
    well:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: <st c="3957">LangChain calls this an</st> **<st c="3982">integration</st>**
    <st c="3993">since it is integrating with a third party called Chroma.</st> <st
    c="4052">There are many other integrations available</st> <st c="4096">with LangChain.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="4111">On the LangChain</st> <st c="4128">website, as it currently stands,
    there is an</st> **<st c="4174">Integrations</st>** <st c="4186">link in</st>
    <st c="4195">the main website navigation at the top of the website page.</st>
    <st c="4255">If you click on that, you will see a menu down the left side that
    stretches out pretty far and has the main categories of</st> **<st c="4377">Providers</st>**
    <st c="4386">and</st> **<st c="4391">Components</st>**<st c="4401">. As you might
    have guessed, this gives you the ability to view all the integrations by either
    providers or components.</st> <st c="4521">If you click on</st> **<st c="4537">Providers</st>**<st
    c="4546">, you will first see</st> **<st c="4567">Partner Packages</st>** <st
    c="4583">and</st> **<st c="4588">Featured Community Providers</st>**<st c="4616">.
    Chroma is not currently in either of these lists, but if you want to find out
    more about Chroma as a provider, click on the link near the end of the page that
    says</st> **<st c="4782">Click here to see all providers</st>**<st c="4813">.
    The list is in alphabetical order.</st> <st c="4850">Scroll down to the Cs and
    find Chroma.</st> <st c="4889">This will show you useful LangChain documentation
    related to Chroma, particularly in creating both the vector store and</st> <st
    c="5009">the retriever.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="5023">Another useful approach is to click on</st> **<st c="5063">Vector
    stores</st>** <st c="5076">under</st> **<st c="5083">Components</st>**<st c="5093">.
    There are currently 49 vector store options!</st> <st c="5140">The current link
    is here for version 0.2.0, but keep an eye out for future versions</st> <st c="5224">as
    well:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[<st c="5232">https://python.langchain.com/v0.2/docs/integrations/vectorstores/</st>](https://python.langchain.com/v0.2/docs/integrations/vectorstores/
    )'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="5298">Another area we highly recommend you review is the LangChain vector
    store</st> <st c="5373">documentation here:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[<st c="5392">https://api.python.langchain.com/en/latest/core_api_reference.html#module-langchain_core.vectorstores</st>](https://api.python.langchain.com/en/latest/core_api_reference.html#module-langchain_core.vectorstores
    )'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="5494">We have already talked about our current vector store and Chroma
    in general in-depth in past chapters, but let’s review Chroma and discuss where
    it is</st> <st c="5646">most useful.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="5658">Chroma</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**<st c="5665">Chroma</st>** <st c="5672">is an open</st> <st c="5683">source
    AI-native vector database designed for developer productivity and</st> <st c="5756">ease
    of use.</st> <st c="5770">It offers fast search performance and seamless integration
    with LangChain through its Python SDK.</st> <st c="5868">Chroma supports various
    deployment modes, including in-memory, persistent storage, and containerized deployment</st>
    <st c="5980">using Docker.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="5993">One of the key advantages of Chroma is its simplicity and developer-friendly
    API.</st> <st c="6076">It provides straightforward methods for adding, updating,
    deleting, and querying documents in the vector store.</st> <st c="6188">Chroma
    also supports dynamic filtering of collections based on metadata, allowing for
    more targeted searches.</st> <st c="6298">Additionally, Chroma offers built-in
    functionality for document chunking and indexing, making it convenient to work
    with large</st> <st c="6425">text datasets.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="6439">When considering Chroma as a vector store for a RAG application,
    it’s important to evaluate its architecture and selection criteria.</st> <st c="6573">Chroma’s
    architecture consists of an indexing layer for fast vector retrieval, a storage
    layer for efficient data management, and a processing layer for real-time operations.</st>
    <st c="6748">Chroma integrates smoothly with LangChain, allowing developers to
    leverage its capabilities within the LangChain ecosystem.</st> <st c="6872">The
    Chroma client can be easily instantiated and passed to LangChain, enabling efficient
    storage and retrieval of document vectors.</st> <st c="7004">Chroma also supports
    advanced retrieval options, such</st> <st c="7058">as</st> **<st c="7061">maximum
    marginal relevance</st>** <st c="7087">(</st>**<st c="7089">MMR</st>**<st c="7092">)
    and metadata filtering to refine</st> <st c="7128">search results.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="7143">Overall, Chroma is</st> <st c="7162">a solid choice</st> <st c="7177">for
    developers seeking an open source, easy-to-use vector database that integrates
    well with LangChain.</st> <st c="7282">Its simplicity, fast search performance,
    and built-in document processing features make it an attractive option for building
    RAG applications.</st> <st c="7425">In fact, these are some of the reasons why
    we chose to feature Chroma in this book in several of the chapters.</st> <st c="7536">However,
    it’s important to assess your specific requirements and compare Chroma with other
    vector store alternatives to determine the best fit for your project.</st> <st
    c="7697">Let’s look at the code and discuss some of the other options available,
    starting</st> <st c="7778">with FAISS.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="7789">FAISS</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="7795">Let’s start with how our code</st> <st c="7826">would</st> <st
    c="7832">change if we wanted to use FAISS as our vector store.</st> <st c="7886">You
    would first need to</st> <st c="7910">install FAISS:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: <st c="7947">After you have restarted the kernel (since you installed a new
    package), run all the code down to the vector store-related cell, and replace
    the Chroma-related code with the FAISS vector</st> <st c="8135">store instantiation:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: <st c="8300">The</st> `<st c="8305">Chroma.from_documents()</st>` <st c="8328">method
    call has been replaced with</st> `<st c="8364">FAISS.from_documents()</st>`<st
    c="8386">. The</st> `<st c="8392">collection_name</st>` <st c="8407">and</st>
    `<st c="8412">client</st>` <st c="8418">parameters are not applicable to FAISS,
    so they have been removed from the method call.</st> <st c="8507">We repeat some
    of the code that we saw with the Chroma vector store, such as the document generation,
    which allows us to show the exact equivalent in code between the two vector store
    options.</st> <st c="8700">With these changes, the code can now use FAISS as the
    vector store instead</st> <st c="8775">of Chroma.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '**<st c="8785">FAISS</st>** <st c="8791">is an</st> <st c="8798">open source
    library developed by Facebook AI.</st> <st c="8844">FAISS offers high-performance
    search capabilities and can handle large datasets that may not fit entirely in
    memory.</st> <st c="8961">Much like other vector stores mentioned here, the architecture
    of FAISS consists of an indexing layer that organizes vectors for fast retrieval,
    a storage layer for efficient data management, and an optional processing layer
    for real-time operations.</st> <st c="9212">FAISS provides various indexing techniques,
    such as clustering and quantization, to optimize search performance and memory
    usage.</st> <st c="9342">It also supports GPU acceleration for even faster</st>
    <st c="9392">similarity search.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="9410">If you have GPUs available, you</st> <st c="9443">can install this
    package instead of the one we</st> <st c="9490">installed previously:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: <st c="9534">Using the GPU version of FAISS can significantly speed up the similarity
    search process, especially for large-scale datasets.</st> <st c="9661">GPUs can
    handle a large number of vector comparisons in parallel, enabling faster retrieval
    of relevant documents in a RAG application.</st> <st c="9797">If you are working
    in an environment that deals with massive amounts of data and requires a substantial
    performance boost compared to what we have already been working with (Chroma),
    you should definitely test FAISS GPU and see the impact it can have</st> <st c="10048">for
    you.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="10056">The FAISS LangChain documentation provides detailed examples and
    guides on how to use FAISS within the LangChain framework.</st> <st c="10181">It
    covers topics such as ingesting documents, querying the vector store, saving and
    loading indexes, and performing advanced operations, such as filtering and merging.</st>
    <st c="10349">The documentation also highlights FAISS-specific features, such
    as similarity search with scores and serialization/deserialization</st> <st c="10480">of
    indexes.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="10491">Overall, FAISS is</st> <st c="10510">a powerful and efficient
    vector store option for building RAG applications with LangChain.</st> <st c="10601">Its
    high-performance search capabilities, scalability, and seamless integration with
    LangChain make it a compelling choice for developers seeking a robust and customizable
    solution for storing and retrieving</st> <st c="10809">document</st> <st c="10818">vectors.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="10826">Those are two powerful options for your vector store needs.</st>
    <st c="10887">Next, we will show and discuss the Weaviate vector</st> <st c="10938">store
    option.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="10951">Weaviate</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="10960">There are multiple options</st> <st c="10987">for how you want
    to use and access Weaviate.</st> <st c="11033">We are going to show the</st> <st
    c="11057">embedding version, which runs a Weaviate instance from your application
    code rather than from a standalone Weaviate</st> <st c="11174">server installation.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="11194">When Embedded Weaviate starts for the first time, it creates a
    permanent datastore in the location set in</st> `<st c="11301">persistence_data_path</st>`<st
    c="11322">. When your client exits, the Embedded Weaviate instance also exits,
    but the data persists.</st> <st c="11414">The next time the client runs, it starts
    a new instance of Embedded Weaviate.</st> <st c="11492">New Embedded Weaviate
    instances use the data that is saved in</st> <st c="11554">the datastore.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="11568">If you are familiar with</st> **<st c="11594">GraphQL</st>**<st
    c="11601">, you may recognize the influence it has had on Weaviate when you start
    looking at the code.</st> <st c="11694">The query language and API are inspired
    by GraphQL, but</st> <st c="11750">Weaviate does not use GraphQL directly.</st>
    <st c="11790">Weaviate uses a RESTful API with a query language that resembles
    GraphQL in terms of its structure and functionality.</st> <st c="11908">Weaviate
    uses predefined data types for properties in the schema definition, similar to
    GraphQL’s scalar types.</st> <st c="12020">The available data types in Weaviate
    include string, int, number, Boolean, date,</st> <st c="12101">and more.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="12110">One strength of Weaviate is its support of batch operations for
    creating, updating, or deleting multiple data objects in a single request.</st>
    <st c="12250">This is similar to GraphQL’s mutation operations, where you can
    perform multiple changes in a single request.</st> <st c="12360">Weaviate uses
    the</st> `<st c="12378">client.batch</st>` <st c="12390">context manager to group
    multiple operations into a batch, which we will demonstrate in</st> <st c="12479">a
    moment.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="12488">Let’s start with how our code would change if we wanted to use
    Weaviate as our vector store.</st> <st c="12582">You will first need to</st> <st
    c="12605">install FAISS:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: <st c="12680">After you have restarted the kernel (since you installed a new
    package), you run all the code down to the vector store-related cell, and update
    the code with the FAISS vector</st> <st c="12856">store instantiation:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: <st c="13068">As you can see, there are many additional packages to import for
    Weaviate.</st> <st c="13144">We also install</st> `<st c="13160">tqdm</st>`<st
    c="13164">, which is not specific to</st> <st c="13191">Weaviate, but it is required,
    as Weaviate uses</st> `<st c="13238">tqdm</st>` <st c="13242">to show progress
    bars when</st> <st c="13270">it loads.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="13279">We must first declare</st> `<st c="13302">weaviate_client</st>`
    <st c="13317">as the</st> <st c="13325">Weaviate client:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: <st c="13412">The differences between our original</st> <st c="13449">Chroma
    vector store code and using Weaviate are more complicated than other approaches
    we have taken so far.</st> <st c="13559">With Weaviate, we initialized with the</st>
    `<st c="13598">WeaviateClient</st>` <st c="13612">client and the embedding options
    to enable embedded mode, as you</st> <st c="13678">saw previously.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="13693">Before we proceed, we need to make sure there is not already an
    instance of the Weaviate client in place, or our code</st> <st c="13812">will
    fail:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: <st c="13893">For</st> <st c="13898">Weaviate, you have to</st> <st c="13919">make
    sure you clear out any lingering schemas from past iterations since they can persist
    in</st> <st c="14013">the background.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="14028">We then use the</st> `<st c="14045">weaviate</st>` <st c="14053">client
    to establish our database using a GraphQL-like</st> <st c="14108">definition schema:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: <st c="14501">This provides a full schema class that you will later pass into
    the vector store definition as part of the</st> `<st c="14609">weviate_client</st>`
    <st c="14623">object.</st> <st c="14632">You need to define this schema for your
    collection using the</st> `<st c="14693">client.collections.create()</st>` <st
    c="14720">method.</st> <st c="14729">The schema definition includes specifying
    the class name, properties, and their data types.</st> <st c="14821">Properties
    can have different data types, such as string, integer, and Boolean.</st> <st
    c="14901">As you can see, Weaviate</st> <st c="14925">enforces a stricter schema
    validation compared to what we’ve used in previous labs</st> <st c="15009">with</st>
    <st c="15014">Chroma.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="15021">While this GraphQL-like schema adds some complexity to establishing
    your vector store, it also gives you more control of your database in helpful
    and powerful ways.</st> <st c="15187">In particular, you have more granular control
    over how to define</st> <st c="15252">your schema.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="15264">You may recognize the next code, as it looks a lot like the</st>
    `<st c="15325">dense_documents</st>` <st c="15340">and</st> `<st c="15345">sparse_documents</st>`
    <st c="15361">variables we have defined in the past, but if you look closely,
    there is a slight difference that is important</st> <st c="15473">to Weaviate:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: <st c="15745">There is a slight change to these definitions for Weaviate when
    we pre-process the documents with the metadata.</st> <st c="15858">We use</st>
    `<st c="15865">'doc_id'</st>` <st c="15873">rather than</st> `<st c="15886">'id'</st>`
    <st c="15890">for Weaviate.</st> <st c="15905">This is because</st> `<st c="15921">'id'</st>`
    <st c="15925">is used internally and is not available for our use.</st> <st c="15979">Later
    in the code, when you extract the ID from the metadata results, you will want
    to update that code to use</st> `<st c="16090">'doc_id'</st>` <st c="16098">as
    well.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="16107">Next, we define our vector store, similar to what we have done
    in the past with Chroma and FAISS, but with</st> <st c="16215">Weaviate-specific
    parameters:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: <st c="16416">For the vector store initialization, Chroma uses the</st> `<st
    c="16470">from_documents</st>` <st c="16484">method to create the vector store
    directly from the documents, whereas, for Weaviate, we create the vector store
    and then add the documents after.</st> <st c="16632">Weaviate also requires additional
    configuration, such as</st> `<st c="16689">text_key</st>`<st c="16697">,</st>
    `<st c="16699">attributes</st>`<st c="16709">, and</st> `<st c="16715">by_text</st>`<st
    c="16722">. One major difference is Weaviate’s use of</st> <st c="16766">a schema.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="16775">Lastly, we load up the</st> <st c="16798">Weaviate vector store
    instance with our actual content, which also applies</st> <st c="16874">the embedding
    function in</st> <st c="16900">the process:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: <st c="17319">In summary, Chroma offers a simpler and more flexible approach
    to data schema definition and focuses on embedding storage and retrieval.</st>
    <st c="17457">It can be easily embedded into your application.</st> <st c="17506">On
    the other hand, Weaviate</st> <st c="17533">provides a more structured and feature-rich
    vector database solution with explicit schema definition, multiple storage backends,
    and built-in support for various embedding models.</st> <st c="17714">It can be
    deployed as a standalone server or hosted in the cloud.</st> <st c="17780">The
    choice between Chroma, Weaviate, or any of the other vector stores depends on
    your specific requirements, such as the level of schema flexibility, deployment
    preferences, and the need for additional features beyond</st> <st c="17999">embedding</st>
    <st c="18009">storage.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="18017">Note that you can use any one of these vector stores and the remaining
    code will work with the data loaded to them.</st> <st c="18134">This is a strength
    of using LangChain, which allows you to swap components in and out.</st> <st c="18221">This
    is particularly necessary in the world of generative AI, where new and dramatically
    improved technologies are launching all the time.</st> <st c="18360">Using this
    approach, if you come across a newer and better vector store technology that makes
    a difference in your RAG pipeline, you can make this change relatively quickly
    and easily.</st> <st c="18545">Let’s talk next about another key component in
    the LangChain arsenal that is at the center of a RAG application:</st> <st c="18658">the
    retriever.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="18672">Code lab 10.2 – LangChain Retrievers</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '<st c="18709">In this code lab, we will cover a few examples of the most important
    component in the retrieval process: the</st> **<st c="18819">LangChain retriever</st>**<st
    c="18838">. Like the</st> <st c="18848">LangChain vector store, there are too</st>
    <st c="18887">many options for LangChain retrievers to list here.</st> <st c="18939">We
    will focus on a few popular choices that are particularly applicable to RAG applications,
    and we encourage you to look at all the others to see if there are better options
    for your specific situation.</st> <st c="19143">Just like we discussed with the
    vector stores, there is ample documentation on the LangChain website that will
    help you find your best</st> <st c="19278">solution:</st> [<st c="19288">https://python.langchain.com/v0.2/docs/integrations/retrievers/</st>](https://python.langchain.com/v0.2/docs/integrations/retrievers/
    )'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="19351">The documentation for the retriever package</st> <st c="19395">can
    be found</st> <st c="19409">here:</st> [<st c="19415">https://api.python.langchain.com/en/latest/core_api_reference.html#module-langchain_core.retrievers</st>](https://api.python.langchain.com/en/latest/core_api_reference.html#module-langchain_core.retrievers
    )
  prefs: []
  type: TYPE_NORMAL
- en: <st c="19514">Now, let’s get started with coding</st> <st c="19550">for retrievers!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="19565">Retrievers, LangChain, and RAG</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**<st c="19596">Retrievers</st>** <st c="19607">are</st> <st c="19611">responsible
    for querying the vector store and retrieving the most relevant documents based
    on the input query.</st> <st c="19723">LangChain offers a range of retriever implementations
    that can be used in conjunction with different vector stores and</st> <st c="19842">query</st>
    <st c="19847">encoders.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="19857">In our code so far, we have already seen three versions of the
    retriever; let’s review them first, as they relate to the original Chroma-based</st>
    <st c="20001">vector store.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="20014">Basic retriever (dense embeddings)</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="20049">We start with</st> <st c="20064">the</st> **<st c="20068">dense
    retriever</st>**<st c="20083">. This is the code we have used in several of our
    code labs up to</st> <st c="20149">this</st> <st c="20153">point:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: <st c="20229">The dense retriever is created using the</st> `<st c="20271">vectorstore.as_retriever</st>`
    <st c="20295">function, specifying the number of top results to retrieve (</st>`<st
    c="20356">k=10</st>`<st c="20361">).</st> <st c="20365">Under the hood of this
    retriever, Chroma uses dense vector representations of the documents and performs
    a similarity search using cosine distance or Euclidean distance to retrieve the
    most relevant documents based on the</st> <st c="20587">query embedding.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="20603">This is using the simplest type of retriever, the vector store
    retriever, which simply creates embeddings for each piece of text and uses those
    embeddings for retrieval.</st> <st c="20774">The retriever is essentially a wrapper
    around the vector store.</st> <st c="20838">Using this approach gives you access
    to the built-in retrieval/search functionality of the vector store, but in a way
    that integrates and interfaces in the LangChain ecosystem.</st> <st c="21015">It
    is a lightweight wrapper around the vector store class that gives you a consistent
    interface for all of the retriever options in LangChain.</st> <st c="21158">Because
    of this, once you construct a vector store, it’s very easy to construct a retriever.</st>
    <st c="21251">If you need to</st> <st c="21265">change your vector store or retriever,
    that is also very easy to do</st> <st c="21334">as</st> <st c="21337">well.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '<st c="21342">There are two primary search capabilities that come from these
    types of retrievers, stemming directly from the vector stores that it wraps: similarity
    search</st> <st c="21501">and MMR.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="21509">Similarity score threshold retrieval</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="21546">By default, retrievers use similarity</st> <st c="21584">search.</st>
    <st c="21593">If you want to set a threshold of similarity</st> <st c="21638">though,
    you simply need to set the search type to</st> `<st c="21688">similarity_score_threshold</st>`
    <st c="21714">and set that similarity score threshold within the</st> `<st c="21766">kwargs</st>`
    <st c="21772">function that you pass to the retriever object.</st> <st c="21821">The
    code looks</st> <st c="21836">like this:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: <st c="21973">This is a useful upgrade to the default similarity search that
    can be useful in many RAG applications.</st> <st c="22077">However, similarity
    search is not the only type of search these retrievers can support; there is</st>
    <st c="22174">also MMR.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="22183">MMR</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`<st c="22655">search_type="mmr"</st>` <st c="22672">as a parameter when you
    define the retriever,</st> <st c="22719">like this:</st>'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: <st c="22793">Adding this to any vector store-based retriever will cause it
    to utilize an MMR type</st> <st c="22879">of search.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="22889">Similarity search and</st> <st c="22911">MMR can be supported
    by any vector stores that also</st> <st c="22964">support those search techniques.</st>
    <st c="22997">Let’s next talk about the sparse search mechanism we introduced
    in</st> [*<st c="23064">Chapter 8</st>*](B22475_08.xhtml#_idTextAnchor152)<st
    c="23073">, the</st> <st c="23079">BM25 retriever.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="23094">BM25 retriever</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`<st c="23173">BM25Retriever</st>` <st c="23186">is the</st> <st c="23194">LangChain
    representation of BM25 that can be used for sparse text</st> <st c="23260">retrieval
    purposes.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="23279">You have seen this retriever as well, as we used it to turn our
    basic search into a hybrid search in</st> [*<st c="23381">Chapter 8</st>*](B22475_08.xhtml#_idTextAnchor152)<st
    c="23390">. We see this in our code with</st> <st c="23421">these settings:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: <st c="23509">The</st> `<st c="23514">BM25Retriever.from_documents()</st>` <st
    c="23544">method is called to create a sparse retriever from the sparse documents,
    specifying the number of top results to</st> <st c="23658">retrieve (</st>`<st
    c="23668">k=10</st>`<st c="23673">).</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="23676">BM25 works by calculating a relevance score for each document
    based on the query terms and the document’s</st> **<st c="23783">term frequencies
    and inverse document frequencies</st>** <st c="23832">(</st>**<st c="23834">TF-IDF</st>**<st
    c="23840">).</st> <st c="23844">It uses a</st> <st c="23854">probabilistic model
    to estimate the relevance of documents to a given query.</st> <st c="23931">The
    retriever returns the top-k documents with the highest</st> <st c="23990">BM25
    scores.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="24002">Ensemble retriever</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="24021">An</st> **<st c="24025">ensemble retriever</st>** <st c="24043">combines</st>
    <st c="24052">multiple retrieval methods and uses an additional</st> <st c="24103">algorithm
    to combine their results into one set.</st> <st c="24152">An ideal use of this
    type of retriever is when you want to combine dense and sparse retrievers to support
    a hybrid retriever approach like what we created in</st> [*<st c="24310">Chapter
    8</st>*](B22475_08.xhtml#_idTextAnchor152)<st c="24319">’s</st> *<st c="24323">Code</st>*
    *<st c="24328">lab 8.3</st>*<st c="24335">:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: <st c="24456">In our case, the ensemble retriever combines the Chroma dense
    retriever and the BM25 sparse retriever to achieve better retrieval performance.</st>
    <st c="24600">It is created using the</st> `<st c="24624">EnsembleRetriever</st>`
    <st c="24641">class, which takes the list of retrievers and their corresponding
    weights.</st> <st c="24717">In this case, the dense retriever and sparse retriever
    are passed with equal weights of</st> `<st c="24805">0.5</st>` <st c="24808">each.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="24814">The</st> `<st c="24819">c</st>` <st c="24820">parameter in the
    ensemble retriever is a reranking parameter that controls the balance between
    the original retrieval scores and the reranking scores.</st> <st c="24972">It
    is used to adjust the influence of the reranking step on the final retrieval results.</st>
    <st c="25061">In this case, the</st> `<st c="25079">c</st>` <st c="25080">parameter
    is set to</st> `<st c="25101">0</st>`<st c="25102">, which means no reranking
    is performed.</st> <st c="25143">When</st> `<st c="25148">c</st>` <st c="25149">is
    set to a non-zero value, the ensemble retriever performs an additional reranking
    step on the retrieved documents.</st> <st c="25267">The reranking step rescores
    the retrieved documents based on a separate reranking model or function.</st>
    <st c="25368">The reranking model can take into account additional features or
    criteria to assess the relevance of the documents to</st> <st c="25486">the query.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="25496">In RAG applications, the quality and relevance of the retrieved
    documents directly impact the generated output.</st> <st c="25609">By utilizing
    the</st> `<st c="25626">c</st>` <st c="25627">parameter and a suitable reranking
    model, you can enhance the retrieval results to better suit the specific requirements
    of your RAG application.</st> <st c="25774">For example, you can design a reranking
    model that takes into account factors such as document relevance, coherence with
    the query, or domain-specific criteria.</st> <st c="25935">By setting an appropriate
    value for</st> `<st c="25971">c</st>`<st c="25972">, you can strike a balance
    between the original retrieval scores and the reranking scores, giving more weight
    to the reranking model when needed.</st> <st c="26118">This can help prioritize
    documents that are more relevant and informative for the RAG task, leading to
    improved</st> <st c="26230">generated outputs.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="26248">When a query is passed to the ensemble retriever, it sends the
    query to both the dense and sparse retrievers.</st> <st c="26359">The ensemble
    retriever then combines the results from both retrievers based on their assigned
    weights and returns the top-k documents.</st> <st c="26494">Under the hood, the
    ensemble retriever leverages the strengths of both dense and sparse retrieval
    methods.</st> <st c="26601">Dense retrieval captures semantic similarity using
    dense vector representations, while sparse retrieval relies on keyword matching
    and term frequencies.</st> <st c="26754">By combining their results, the ensemble
    retriever aims to provide more accurate and comprehensive</st> <st c="26853">search
    results.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="26868">The specific classes and methods used in the code snippet may
    vary depending on the library or framework being used.</st> <st c="26986">However,
    the general concepts of dense retrieval using vector similarity search, sparse</st>
    <st c="27073">retrieval using BM25, and ensemble retrieval combining multiple
    retrievers</st> <st c="27149">remain</st> <st c="27156">the same.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27165">That covers the retrievers we have already seen in previous code,
    all drawn from the data we accessed and processed during the indexing stage.</st>
    <st c="27309">There are numerous other retriever types that work with data you
    extract from your documents that you can explore on the LangChain website to suit
    your needs.</st> <st c="27468">However, not all retrievers are designed to pull
    from documents you are processing.</st> <st c="27552">Next, we will review an
    example of a retriever built off a public data</st> <st c="27623">source, Wikipedia.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27641">Wikipedia retriever</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="27661">As described by the creators of the</st> <st c="27697">Wikipedia
    retriever</st> <st c="27717">on the LangChain</st> <st c="27734">website (</st>[<st
    c="27744">https://www.langchain.com/</st>](https://www.langchain.com/)<st c="27771">):</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27774">Wikipedia is the largest and most-read reference work in history,
    acting as a multilingual free online encyclopedia written and maintained by a
    community of volunteers.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27943">That sounds like a great resource to tap for useful knowledge
    in your RAG applications!</st> <st c="28032">We will add a new cell after our
    existing retriever cell where we will use this Wikipedia retriever to retrieve
    wiki pages from</st> `<st c="28160">wikipedia.org</st>` <st c="28173">into the</st>
    `<st c="28183">Document</st>` <st c="28191">format that is</st> <st c="28207">used
    downstream.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="28223">We first need to install a couple of</st> <st c="28261">new packages:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: <st c="28343">As always, when you install new packages, don’t forget to restart</st>
    <st c="28410">your kernel!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="28422">With</st> <st c="28427">the</st> `<st c="28432">WikipediaRetriever</st>`
    <st c="28450">retriever, we now have a mechanism</st> <st c="28485">that can fetch
    data from Wikipedia as it relates to the user query we pass to it, similar to
    the other retrievers we have used, but using the whole of Wikipedia data</st>
    <st c="28652">behind it:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: <st c="29245">In this code, we import the</st> `<st c="29274">WikipediaRetriever</st>`
    <st c="29292">class from the</st> `<st c="29308">langchain_community.retrievers</st>`
    <st c="29338">module.</st> `<st c="29347">WikipediaRetriever</st>` <st c="29365">is
    a retriever class specifically designed to retrieve relevant documents from Wikipedia
    based on a given query.</st> <st c="29479">We then instantiate an instance of
    this receiver using the</st> `<st c="29538">WikipediaRetriever</st>` <st c="29556">class
    and assign it to the variable retriever.</st> <st c="29604">The</st> `<st c="29608">load_max_docs</st>`
    <st c="29621">parameter is set to</st> `<st c="29642">10</st>`<st c="29644">,
    indicating that the retriever should load a maximum of 10 relevant documents.</st>
    <st c="29724">The user query here is</st> `<st c="29747">What defines the golden
    age of piracy in the Caribbean?</st>`<st c="29802">, and we can look at the response
    to see what Wikipedia articles are retrieved to help answer</st> <st c="29896">this
    question.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="29910">We</st> <st c="29913">call the</st> `<st c="29923">get_relevant_documents</st>`
    <st c="29945">method of the retriever</st> <st c="29969">object, passing in a
    query string as an argument, and receive this as the first document in</st> <st
    c="30062">that response:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: <st c="30474">You can see the matching content at</st> <st c="30511">this link:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[<st c="30521">https://en.wikipedia.org/wiki/Golden_Age_of_Piracy</st>](https://en.wikipedia.org/wiki/Golden_Age_of_Piracy
    )'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="30572">This link was provided as the source by</st> <st c="30613">the
    retriever.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="30627">In summary, this code demonstrates how to use the</st> `<st c="30678">WikipediaRetriever</st>`
    <st c="30696">class from the</st> `<st c="30712">langchain_community.retrievers</st>`
    <st c="30742">module to retrieve relevant documents from Wikipedia based on a
    given query.</st> <st c="30820">It then extracts and prints specific metadata
    information (title, summary, source) and the content of the first</st> <st c="30932">retrieved
    document.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="30951">WikipediaRetriever</st>` <st c="30970">internally handles the
    process of querying Wikipedia’s API or search functionality, retrieving the relevant
    documents, and returning them as a list of</st> `<st c="31122">Document</st>`
    <st c="31130">objects.</st> <st c="31140">Each</st> `<st c="31145">Document</st>`
    <st c="31153">object contains metadata and the actual page content, which can
    be accessed and utilized as needed.</st> <st c="31254">There are many other retrievers
    that can access public data sources similar to this but focused on specific domains.</st>
    <st c="31371">For scientific research, there is</st> `<st c="31405">PubMedRetriever</st>`<st
    c="31420">. For other fields of research, such as mathematics and computer science,
    there is</st> `<st c="31503">ArxivRetreiver</st>`<st c="31517">, which accesses
    data from the open-access archive of more than 2 million scholarly articles about
    these subjects.</st> <st c="31632">In the</st> <st c="31638">finance world, there
    is a retriever called</st> `<st c="31682">KayAiRetriever</st>` <st c="31696">that
    can access</st> **<st c="31713">Securities and Exchange Commission</st>** <st
    c="31747">(</st>**<st c="31749">SEC</st>**<st c="31752">) filings, which contain
    the financial statements that public companies are required to submit to the</st>
    <st c="31855">US SEC.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: '<st c="31862">For projects that deal with data that is not on a massive scale,
    we have one more retriever to highlight: the</st> <st c="31973">kNN retriever.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="31987">kNN retriever</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="32001">The</st> <st c="32005">nearest-neighbor algorithms we have</st>
    <st c="32041">been working with up to this point, the ones responsible for finding
    the most closely related content to the user query, have been based</st> <st c="32178">on</st>
    **<st c="32182">approximate nearest neighbor</st>** <st c="32210">(</st>**<st
    c="32212">ANN</st>**<st c="32215">).</st> <st c="32219">There is a more</st> *<st
    c="32235">traditional</st>* <st c="32246">and</st> *<st c="32251">older</st>*
    <st c="32256">algorithm that serves as an alternative to ANN though, and this
    is</st> <st c="32323">the</st> **<st c="32328">k-nearest neighbor</st>** <st c="32346">(</st>**<st
    c="32348">kNN</st>**<st c="32351">).</st> <st c="32355">But kNN is based on an
    algorithm that dates back to 1951; why would we use this when we have a more sophisticated
    and powerful algorithm like ANN available?</st> <st c="32512">Because kNN is</st>
    *<st c="32527">still better</st>* <st c="32539">than anything that came after
    it.</st> <st c="32574">That is not a misprint.</st> <st c="32598">kNN is still
    the</st> *<st c="32615">most effective</st>* <st c="32629">way to find the nearest
    neighbors.</st> <st c="32665">It is better than ANN, which is touted as</st> *<st
    c="32707">the</st>* <st c="32710">solution by all of the database, vector database,
    and information retrieval companies that operate in this field.</st> <st c="32825">ANN
    can come close, but kNN is still</st> <st c="32862">considered better.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="32880">Why is ANN touted as</st> *<st c="32902">the</st>* <st c="32905">solution
    then?</st> <st c="32921">Because kNN does not scale to the level you see in the
    large enterprises these vendors are targeting.</st> <st c="33023">But this is
    all relative.</st> <st c="33049">You may have a million data points, which sounds
    like a lot, with 1,536 dimension vectors, but that is still considered quite small
    on the global enterprise stage.</st> <st c="33213">kNN can handle that pretty
    easily!</st> <st c="33248">Many of the smaller projects that are using ANN in
    the field can probably benefit from using kNN instead.</st> <st c="33354">The
    theoretical limit of kNN is going to depend on many things, such as your development
    environment, your data, the dimensions of your data, internet connectivity if
    using APIs, and many more.</st> <st c="33548">So, we cannot give a specific number
    of data points.</st> <st c="33601">You will need to test this.</st> <st c="33629">But
    if it is smaller than the project I just described, 1 million data points with
    1,536 dimension vectors, in a relatively capable development environment, you
    should really consider kNN!</st> <st c="33818">At some point, you will notice
    a significant increase in processing time, and when the wait becomes too long
    for the usefulness of your application, switch to ANN.</st> <st c="33982">But
    in the meantime, be sure to take full advantage of the superior search capabilities</st>
    <st c="34070">of kNN.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="34077">Luckily, kNN is available in an easy-to-set-up retriever called</st>
    `<st c="34142">KNNRetriever</st>`<st c="34154">. This retriever will utilize the
    same dense embeddings we use with our other algorithms, and therefore, we will
    replace</st> `<st c="34275">dense_retriever</st>` <st c="34290">with the kNN-based</st>
    `<st c="34310">KNNRetriever</st>`<st c="34322">. Here is the code to implement
    that, fitting in nicely after we defined the previous version of our</st> `<st
    c="34423">dense_retriever</st>` <st c="34438">retriever object:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: <st c="34707">Run the</st> <st c="34715">remaining code in the code lab to see
    it take the place of our</st> <st c="34778">previous</st> `<st c="34788">dense_retriever</st>`
    <st c="34803">and perform in its place.</st> <st c="34830">In this particular
    situation, with a very limited dataset, it is difficult to evaluate if it is doing
    better than the ANN-based algorithm we were previously using.</st> <st c="34994">But,
    as your project scales, we highly recommend you take advantage of this approach
    until its scaling issues become too much of</st> <st c="35123">a burden.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="35132">This concludes our exploration of the retrievers that can support
    RAG.</st> <st c="35204">There are additional types of retrievers, as well as notable
    integrations with vector stores that support those retrievers that can be reviewed
    on the LangChain website.</st> <st c="35374">For example, there is a time-weighted
    vector store retriever that allows you to incorporate recency into the retrieval
    process.</st> <st c="35502">There is also a retriever called the Long-Context
    Reorder focused on improving results from long-context models that have difficulty
    paying attention to information in the middle of the retrieved documents.</st>
    <st c="35709">Be sure to take a look at what is available, as they have the potential
    to have a significant impact on your RAG application.</st> <st c="35835">We will
    now move on to talking about the</st> *<st c="35876">brains</st>* <st c="35882">of
    the operation and of the generation stage:</st> <st c="35929">the LLMs.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="35938">Code lab 10.3 – LangChain LLMs</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '<st c="35969">We now turn our attention</st> <st c="35996">to the last key
    component for RAG: the LLM.</st> <st c="36040">Just like the retriever in the
    retrieval stage, without the LLM for the generation stage, there is no RAG.</st>
    <st c="36147">The retrieval stage simply retrieves data from our data source,
    typically data the LLM does not know about.</st> <st c="36255">However, that does
    not mean that the LLM does not play a vital role in our RAG implementation.</st>
    <st c="36350">By providing the retrieved data to the LLM, we quickly catch that
    LLM up with what we want it to talk about, and this allows the LLM to do what
    it is really good at, providing a response based on that data to answer the original
    question posed by</st> <st c="36597">the user.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="36606">The synergy between LLMs and RAG systems stems from the complementary
    strengths of these two technologies.</st> <st c="36714">RAG systems enhance the
    capabilities of LLMs by incorporating external knowledge sources, enabling the
    generation of responses that are not only contextually relevant but also factually
    accurate and up to date.</st> <st c="36925">In turn, LLMs contribute to RAG by
    providing a sophisticated understanding of the query context, facilitating more
    effective retrieval of pertinent information from the knowledge base.</st> <st
    c="37110">This symbiotic relationship significantly improves the performance of
    AI systems in tasks that require both deep language understanding and access to
    a wide range of factual</st> <st c="37283">information, leveraging the strengths
    of each component to create a more powerful and</st> <st c="37370">versatile system.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '<st c="37387">In this code lab, we will cover a few examples of the most important
    component in the generation stage: the</st> <st c="37496">LangChain LLM.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="37510">LLMs, LangChain, and RAG</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="37535">As with the previous key components, we will first provide links
    to the LangChain documentation related to this major component, the</st> <st c="37669">LLMs:</st>
    [<st c="37675">https://python.langchain.com/v0.2/docs/integrations/llms/</st>](https://python.langchain.com/v0.2/docs/integrations/llms/
    )
  prefs: []
  type: TYPE_NORMAL
- en: <st c="37732">Here is a second helpful source for information combining LLMs
    with LangChain is the API</st> <st c="37822">documentation:</st> [<st c="37837">https://api.python.langchain.com/en/latest/community_api_reference.html#module-langchain_community.llms</st>](https://api.python.langchain.com/en/latest/community_api_reference.html#module-langchain_community.llms
    )
  prefs: []
  type: TYPE_NORMAL
- en: '<st c="37940">Let’s start with the API we have been using</st> <st c="37985">already:
    OpenAI.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="38001">OpenAI</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="38008">We already</st> <st c="38020">have this code in place, but</st>
    <st c="38048">let’s refresh the inner workings of this code by stepping through
    the key areas of our lab that make this</st> <st c="38155">component work:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="38170">First, we must install the</st> `<st c="38198">langchain-openai</st>`
    <st c="38214">package:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="38351">Next, we import the</st> `<st c="38372">openai</st>` <st c="38378">library,
    which is the official Python library for interacting with OpenAI’s API, and will
    be used in this code primarily to apply the API key to the model so that we can
    access the paid API.</st> <st c="38570">We then import the</st> `<st c="38589">ChatOpenAI</st>`
    <st c="38599">and</st> `<st c="38604">OpenAIEmbeddings</st>` <st c="38620">classes
    from the</st> `<st c="38638">langchain_openai</st>` <st c="38654">library:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="38859">In the next line, we load the environment variables from a file
    named</st> `<st c="38930">env.txt</st>` <st c="38937">using the</st> `<st c="38948">load_dotenv</st>`
    <st c="38959">function:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="39192">We then</st> <st c="39201">pass that API key into</st> <st c="39223">OpenAI
    using the</st> <st c="39241">following code:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="39681">Later in the code, we define the LLM we want</st> <st c="39727">to
    use:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="39792">This line creates an instance of the</st> `<st c="39830">ChatOpenAI</st>`
    <st c="39840">class, specifying the model name as</st> `<st c="39877">gpt-4o-mini</st>`
    <st c="39888">and setting the</st> `<st c="39905">temperature</st>` <st c="39916">variable
    to</st> `<st c="39929">0</st>`<st c="39930">. The temperature controls the randomness
    of the generated responses, with lower values producing more focused and deterministic
    outputs.</st> <st c="40068">Currently,</st> `<st c="40079">gpt-4o-mini</st>` <st
    c="40090">is the newest and most capable model while also being the most cost-effective
    of the GPT4 series.</st> <st c="40189">But even that model costs 10X more than</st>
    `<st c="40229">gpt-3.5-turbo</st>`<st c="40242">, which is actually a relatively</st>
    <st c="40275">capable model.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="40289">The most expensive of OpenAI’s models,</st> `<st c="40329">gpt-4-32k</st>`<st
    c="40338">, is not as fast or capable as</st> `<st c="40369">gpt-4o-mini</st>`
    <st c="40380">and has a context window 4X its size.</st> <st c="40419">There will
    likely be newer models soon, including</st> `<st c="40469">gpt-5</st>`<st c="40474">,
    that may be even lower cost and more capable.</st> <st c="40522">What you can
    take away from all of this is that you shouldn’t just assume the latest model
    is going to be the most expensive, and there are alternative versions that can
    be more capable and even more cost-effective coming out all the time.</st> <st
    c="40762">Stay diligent in following the latest releases of the models, and for
    each release, weigh</st> <st c="40852">the benefits of cost, LLM capability, and
    any other related</st> <st c="40912">attributes to decide if a change</st> <st
    c="40945">is warranted.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="40958">But in this effort, you do not need to limit yourself to just
    OpenAI.</st> <st c="41029">Using LangChain makes it easy to switch LLMs and broaden
    your search for the best solution to all options within the LangChain community.</st>
    <st c="41167">Let’s step through some other options you</st> <st c="41209">may
    consider.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="41222">Together AI</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**<st c="41234">Together AI</st>** <st c="41246">offers a</st> <st c="41255">developer-friendly
    set of services that give you access to numerous models.</st> <st c="41332">Their</st>
    <st c="41337">pricing for hosted LLMs is difficult to beat, and they often offer
    $5.00 in free credits to test out the</st> <st c="41443">different models.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="41460">If you are new to Together API, you can use this link to set up
    your API key and add it to your</st> `<st c="41557">env.txt</st>` <st c="41564">file
    just like we did in the past with the OpenAI API</st> <st c="41619">key:</st>
    [<st c="41624">https://api.together.ai/settings/api-keys</st>](https://api.together.ai/settings/api-keys
    )
  prefs: []
  type: TYPE_NORMAL
- en: <st c="41665">As you arrive at this web page, it currently offers you a $5.00
    credit that will be in place after you click on the</st> **<st c="41782">Get started</st>**
    <st c="41793">button.</st> <st c="41802">You do not have to provide a credit card
    to access this $</st><st c="41859">5.00 credit.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="41872">Be sure to add your new API key to your</st> `<st c="41913">env.txt</st>`
    <st c="41920">file</st> <st c="41926">as</st> `<st c="41929">TOGETHER_API_KEY</st>`<st
    c="41945">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="41946">Once you are logged in, you can see the current costs for each
    LLM</st> <st c="42014">here:</st> [<st c="42020">https://api.together.ai/models</st>](https://api.together.ai/models
    )
  prefs: []
  type: TYPE_NORMAL
- en: <st c="42050">For example, Meta Llama 3 70B instruct (Llama-3-70b-chat-hf) is
    currently listed to cost $0.90 per 1 million tokens.</st> <st c="42168">This is
    a model that has been shown to rival ChatGPT 4, but Together AI, will</st> <st
    c="42245">run with significantly lower inference costs than what OpenAI</st> <st
    c="42308">charges.</st> <st c="42317">Another highly capable model, the Mixtral
    mixture of experts model costs $1.20 per 1 million Follow these steps to set up
    and use</st> <st c="42447">Together AI:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="42459">We start with installing the package we need to use the</st> <st
    c="42516">Together API:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="42571">This prepares us to use the integration between the Together API</st>
    <st c="42637">and LangChain:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="42911">Just like we did in the past with the OpenAI API key, we are going
    to pull in</st> `<st c="42990">TOGETHER_API_KEY</st>` <st c="43006">so that it
    can access</st> <st c="43029">your account:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="43106">We are going to use the Llama 3 Chat model and Mistral’s Mixtral
    8X22B Instruct model, but you can choose from 50+ models</st> <st c="43229">here:</st>
    [<st c="43235">https://docs.together.ai/docs/inference-models</st>](https://docs.together.ai/docs/inference-models)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="43281">You may find a better model for your</st> <st c="43319">particular
    needs!</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="43336">Here, we are defining</st> <st c="43359">the models:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '***   <st c="43758">Here, we</st> <st c="43767">have</st> <st c="43773">updated
    the final code for using the Llama</st> <st c="43816">3 model:</st>'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="44280">This should look familiar, as it is the RAG chain we have used
    in the past, but running with the Llama</st> <st c="44383">3 LLM.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="44549">This is the final RAG chain we use, updated with the previous
    Llama 3-focused</st> <st c="44628">RAG chain.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="44638">Next, we want</st> <st c="44652">to run similar code to what we
    have run in the past that</st> <st c="44709">invokes and runs the RAG pipeline
    with the Llama 3 LLM replacing the</st> <st c="44779">ChatGPT-4o-mini model:</st>
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Google''s environmental initiatives include:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '1\. Empowering individuals to take action: Offering sustainability features
    in Google products, such as eco-friendly routing in Google Maps, energy efficiency
    features in Google Nest thermostats, and carbon emissions information in Google
    Flights…'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[TRUNCATED]'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '10\. Engagement with external targets and initiatives: Participating in industry-wide
    initiatives and partnerships to promote sustainability, such as the RE-Source
    Platform, iMasons Climate Accord, and World Business Council for Sustainable Development.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '***   <st c="45979">Let’s see what it looks like if we use the mixture of</st>
    <st c="46034">experts model:</st>'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '****<st c="46526">Again, this</st> <st c="46538">should look familiar, as it
    is the RAG chain we have used in the past, but</st> <st c="46613">this time running
    with the mixture of</st> <st c="46651">experts LLM.</st>****'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '****<st c="46839">Just as we did</st> <st c="46854">before, we update the final
    RAG pipeline with the previous</st> <st c="46914">mixture of experts-focused</st>
    <st c="46941">RAG chain.</st>****'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '****<st c="46951">This code will let us see the results of the mixture of experts
    replacing the</st> <st c="47030">ChatGPT-4o-mini model:</st>****'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE112]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE113]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE114]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE115]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE116]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE117]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE118]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE119]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE120]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE121]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Google''s environmental initiatives are organized around three key pillars:
    empowering individuals to take action, working together with partners and customers,
    and operating their business sustainably.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '1\. Empowering individuals: Google provides sustainability features like eco-friendly
    routing in Google Maps, energy efficiency features in Google Nest thermostats,
    and carbon emissions information in Google Flights. Their goal is to help individuals,
    cities, and other partners collectively reduce 1 gigaton of carbon equivalent
    emissions annually by 2030.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[TRUNCATED]'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Additionally, Google advocates for strong public policy action to create low-carbon
    economies, they work with the United Nations Framework Convention on Climate Change
    (UNFCCC) and support the Paris Agreement's goal to keep global temperature rise
    well below 2°C above pre-industrial levels. They also engage with coalitions and
    sustainability initiatives like the RE-Source Platform and the Google.org Impact
    Challenge on Climate Innovation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Google's environmental initiatives include empowering individuals to take action,
    working together with partners and customers, operating sustainably, achieving
    net-zero carbon emissions, focusing on water stewardship, engaging in a circular
    economy, and supporting sustainable consumption of public goods. They also engage
    with suppliers to reduce energy consumption and greenhouse gas emissions, report
    environmental data, and assess environmental criteria. Google is involved in various
    sustainability initiatives, such as the iMasons Climate Accord, ReFED, and supporting
    projects with The Nature Conservancy. They also work with coalitions like the
    RE-Source Platform and the World Business Council for Sustainable Development.
    Additionally, Google invests in breakthrough innovation and collaborates with
    startups to tackle sustainability challenges. They also focus on renewable energy
    and use data analytics tools to drive more intelligent supply chains.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE126]****'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '****<st c="49763">The new responses from Llama 3 and the mixture of experts
    models show expanded responses that seem to be similar, if not more robust, compared
    to the original responses we were able to achieve with OpenAI’s</st> `<st c="49971">gpt-4o-mini</st>`
    <st c="49982">model at considerably fewer costs than OpenAI’s more expensive but
    more</st> <st c="50055">capable models.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="50070">Extending the LLM capabilities</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="50101">There are aspects of these LLM</st> <st c="50132">objects that
    can be better utilized in your RAG application.</st> <st c="50194">As described
    in the LangChain LLM documentation (</st>[<st c="50243">https://python.langchain.com/v0.1/docs/modules/model_io/llms/streaming_llm/</st>](https://python.langchain.com/v0.1/docs/modules/model_io/llms/streaming_llm/)<st
    c="50319">):</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="50323">All LLMs implement the Runnable interface, which comes with default
    implementations of all methods, ie.</st> <st c="50427">ainvoke, batch, abatch,
    stream, astream.</st> <st c="50468">This gives all LLMs basic support for async,
    streaming and batch.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="50533">These are key features that can significantly speed up the processing
    of your RAG application, particularly if you are processing multiple LLM calls
    at once.</st> <st c="50692">In the following subsections, we will look at the
    key methods and how they can</st> <st c="50771">help you.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="50780">Async</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="50786">By default, async support</st> <st c="50812">runs the regular
    sync method in a separate thread.</st> <st c="50864">This allows other parts of
    your async program to keep running while the language model</st> <st c="50951">is
    working.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="50962">Stream</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '<st c="50969">Streaming support</st> <st c="50987">typically returns</st> `<st
    c="51006">Iterator</st>` <st c="51014">(or</st> `<st c="51019">AsyncIterator</st>`
    <st c="51032">for async streaming) with just one item: the final result from the
    language model.</st> <st c="51116">This doesn’t provide word-by-word streaming,
    but it ensures your code can</st> <st c="51189">work with any of the LangChain
    language model integrations that expect a stream</st> <st c="51270">of tokens.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="51280">Batch</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="51286">Batch support processes</st> <st c="51310">multiple inputs at
    the same time.</st> <st c="51345">For a sync batch, it uses multiple threads.</st>
    <st c="51389">For an async batch, it uses</st> `<st c="51417">asyncio.gather</st>`<st
    c="51431">. You can control how many tasks run at once using the</st> `<st c="51486">max_concurrency</st>`
    <st c="51501">setting</st> <st c="51510">in</st> `<st c="51513">RunnableConfig</st>`<st
    c="51527">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="51528">Not all LLMs support all of these functions natively though.</st>
    <st c="51590">For the two implementations we have discussed, as well as many more,
    LangChain provides an in-depth chart that can be found</st> <st c="51714">here:</st>
    [<st c="51720">https://python.langchain.com/v0.2/docs/integrations/llms/</st>](https://python.langchain.com/v0.2/docs/integrations/llms/)
  prefs: []
  type: TYPE_NORMAL
- en: <st c="51777">Summary</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '<st c="51785">This chapter explored the key technical components of RAG systems
    in the context of LangChain: vector stores, retrievers, and LLMs.</st> <st c="51918">It
    provided an in-depth look at the various options available for each component
    and discussed their strengths, weaknesses, and scenarios in which one option might
    be better</st> <st c="52092">than another.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="52105">The chapter started by examining vector stores, which play a crucial
    role in efficiently storing and indexing vector representations of knowledge base
    documents.</st> <st c="52268">LangChain integrates with various vector store implementations,
    such as Pinecone, Weaviate, FAISS, and PostgreSQL with vector extensions.</st>
    <st c="52406">The choice of vector store depends on factors such as scalability,
    search performance, and deployment requirements.</st> <st c="52522">The chapter
    then moved on to discuss retrievers, which are responsible for querying the vector
    store and retrieving the most relevant documents based on the input query.</st>
    <st c="52692">LangChain offers a range of retriever implementations, including
    dense retrievers, sparse retrievers (such as BM25), and ensemble retrievers that
    combine the results of</st> <st c="52861">multiple retrievers.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="52881">Finally, the chapter covered the role of LLMs in RAG systems.</st>
    <st c="52944">LLMs contribute to RAG by providing a sophisticated understanding
    of the query context and facilitating more effective retrieval of pertinent information
    from the knowledge base.</st> <st c="53123">The chapter showcased the integration
    of LangChain with various LLM providers, such as OpenAI and Together AI, and highlighted
    the capabilities and cost considerations of different models.</st> <st c="53312">It
    also discussed the extended capabilities of LLMs in LangChain, such as async,
    streaming, and batch support, and provided a comparison of the native implementations
    offered by different</st> <st c="53500">LLM integrations.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="53517">In the next chapter, we will continue to talk about how LangChain
    can be utilized to build a capable RAG application, focusing now on the smaller
    components that can be used in support of the key components we just discussed
    in</st> <st c="53746">this chapter.</st>****
  prefs: []
  type: TYPE_NORMAL
