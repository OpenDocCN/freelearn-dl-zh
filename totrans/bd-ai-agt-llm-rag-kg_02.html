<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-33"><a id="_idTextAnchor032"/>2</h1>
<h1 id="_idParaDest-34"><a id="_idTextAnchor033"/>The Transformer: The Model Behind the Modern AI Revolution</h1>
<p>In this chapter, we will discuss the limitations of the models we saw in the previous chapter, and how a new paradigm (first attention mechanisms and then the transformer) emerged to solve these limitations. This will enable us to understand how these models are trained and why they are so powerful. We will discuss why this paradigm has been successful and why it has made it possible to solve tasks in <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>) that<a id="_idIndexMarker089"/> were previously impossible. We will then see the capabilities of these models in practical application.</p>
<p>This chapter will clarify why contemporary LLMs are inherently based on the transformer architecture.</p>
<p>In this chapter, we’ll be covering the following topics:</p>
<ul>
<li>Exploring attention and self-attention</li>
<li>Introducing the transformer model</li>
<li>Training a transformer</li>
<li>Exploring masked language modeling</li>
<li>Visualizing internal mechanisms</li>
<li>Applying a transformer</li>
</ul>
<h1 id="_idParaDest-35"><a id="_idTextAnchor034"/>Technical requirements</h1>
<p>Most of this code can be run on a CPU, but some parts (fine-tuning and knowledge distillation) are preferable to be run on a GPU (one hour of training on a CPU versus less than five minutes on a GPU).</p>
<p>The code is written in PyTorch and uses standard libraries for the most part (PyTorch, Hugging Face Transformers, and so on), though some snippets come from Ecco, a specific library. The code can be found on GitHub: <a href="https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr2">https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr2</a></p>
<h1 id="_idParaDest-36"><a id="_idTextAnchor035"/>Exploring attention and self-attention</h1>
<p>In the 1950s, with<a id="_idIndexMarker090"/> the beginning <a id="_idIndexMarker091"/>of the computer revolution, governments began to become interested in the idea of machine translation, especially for military applications. These attempts failed miserably, for three main reasons: machine translation is more complex than it seems, there was not enough computational power, and there was not enough data. Governments concluded that it was a technically impossible challenge in the 1960s.</p>
<p>By the 1990s, two of the three limitations were beginning to be overcome: the internet finally allowed for abundant text, and the advent of GPUs finally allowed for computational power. The third requirement still had to be met: a model that could harness the newfound computational power to handle the complexity of natural language.</p>
<p>Machine translation captured the interest of researchers because it is a practical problem for which it is easy to evaluate the result (we can easily understand whether a translation is good or not). Moreover, we have an abundance of text in one language and a counterpart in another. So, researchers tried to adapt the previous models to the tasks (RNN, LSTM, and so on). The most commonly used system was the <strong class="bold">seq2seq model</strong>, where <a id="_idIndexMarker092"/>you have<a id="_idIndexMarker093"/> an <strong class="bold">encoder</strong> and a <strong class="bold">decoder</strong>. The <a id="_idIndexMarker094"/>encoder transforms the sequence into a new succinct representation that should preserve the relevant information (a sort of good summary). The decoder receives as input this context vector and uses this to transform (translate) this input into the output.</p>
<div><div><img alt="Figure 2.1 – A seq2seq model with an encoder and a decoder. In one case, we take the average of the hidden states (left); in the other case, we use attention to identify which hidden state is more relevant for the translation (right) " src="img/B21257_02_01.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.1 – A seq2seq model with an encoder and a decoder. In one case, we take the average of the hidden states (left); in the other case, we use attention to identify which hidden state is more relevant for the translation (right) </p>
<p>RNNs and derived models have some problems:</p>
<ul>
<li><strong class="bold">Alignment</strong>: The <a id="_idIndexMarker095"/>length of input and output can be different (for example, to translate English to French “<em class="italic">she doesn’t like potatoes</em>” into “<em class="italic">elle n’aime pas les pommes </em><em class="italic">de terre</em>”).</li>
<li><strong class="bold">Vanishing and exploding gradients</strong>: Problems that arise during training so that multiple layers cannot be managed effectively.</li>
<li><strong class="bold">Non-parallelizability</strong>: Training is computationally expensive and not parallelizable. RNNs forget after a few steps.</li>
</ul>
<div><div><img alt="Figure 2.2 – Example of issues with alignment: one to many (left) and spurious word (right) " src="img/B21257_02_02.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.2 – Example of issues with alignment: one to many (left) and spurious word (right) </p>
<p><strong class="bold">Attention mechanisms</strong> were initially <a id="_idIndexMarker096"/>described to solve the alignment problem, as well as to <a id="_idIndexMarker097"/>learn <a id="_idIndexMarker098"/>the relationships between the various parts of a text and the corresponding parts of the translated text.</p>
<p>The idea is that instead of passing the hidden state of RNNs, we pass contextual information that focuses only on the important parts of the sequence. During decoding (translation) for each token, we want to retrieve the corresponding and specific information in the other language. Attention determines which tokens in the input are important at that moment.</p>
<p>The first step is the alignment between the hidden state of the encoder (<em class="italic">h</em>) and the previous decoder output (<em class="italic">s</em>). The score function can be different: dot product or cosine similarity is most commonly used, but it can also be more complex functions such as the feedforward neural network layer. This step allows us to understand how relevant hidden state encoders are to the translation at that time. This step is conducted for all encoder steps.</p>
<p><math display="block"><mrow><mrow><mrow><mfenced close=")" open="("><mn>1</mn></mfenced><msub><mi>e</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>=</mo><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mo>(</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>h</mi><mi>j</mi></msub><mo>)</mo></mrow></mrow></mrow></math></p>
<p>Right now though, we have a scalar representing the similarity between two vectors (<em class="italic">h</em> and <em class="italic">s</em>). All these scores are passed into the softmax function that squeezes everything between 0 and 1. This step also serves to assign relative importance to each hidden state.</p>
<p><math display="block"><mrow><mrow><mfenced close=")" open="("><mn>2</mn></mfenced><msub><mi>a</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mfenced close=")" open="("><msub><mi>e</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></mfenced><mo>=</mo><mfrac><mrow><mi mathvariant="normal">e</mi><mi mathvariant="normal">x</mi><mi mathvariant="normal">p</mi><mo>(</mo><msub><mi>e</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>)</mo></mrow><mrow><msubsup><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi></msubsup><mrow><mi mathvariant="normal">e</mi><mi mathvariant="normal">x</mi><mi mathvariant="normal">p</mi><mo>(</mo><msub><mi>e</mi><mrow><mi>i</mi><mo>,</mo><mi>k</mi></mrow></msub><mo>)</mo></mrow></mrow></mfrac></mrow></mrow></math></p>
<p>Finally, we conduct<a id="_idIndexMarker099"/> a <a id="_idIndexMarker100"/>weighted sum of the various hidden states multiplied by the attention score. So, we have a fixed-length context vector capable of giving us information about the entire set of hidden states. In simple words, during translation, we have a context vector that is dynamically updated and tells us how much attention we should give to each part of the input sequence.</p>
<p><math display="block"><mrow><mrow><mfenced close=")" open="("><mn>3</mn></mfenced><msub><mi>c</mi><mi>t</mi></msub><mo>=</mo><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><mrow><msub><mi>a</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>∙</mo><msub><mi>h</mi><mi>j</mi></msub></mrow></mrow></mrow></mrow></math></p>
<p>As you can see from the original article, the model pays different attention to the various words in the input during translation.</p>
<div><div><img alt="Figure 2.3 – Example of alignment between sentences after training the model with attention. Each pixel shows the attention weight between the source word and the target word. (https://arxiv.org/pdf/1409.0473)" src="img/B21257_02_03.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.3 – Example of alignment between sentences after training the model with attention. Each pixel shows the attention weight between the source word and the target word. (<a href="https://arxiv.org/pdf/1409.0473">https://arxiv.org/pdf/1409.0473</a>)</p>
<p>In addition to solving alignment, the attention mechanism has <a id="_idIndexMarker101"/>other advantages:</p>
<ul>
<li>It reduces the vanishing gradient problem because it provides a shortcut to early states.</li>
<li>It eliminates the bottleneck problem; the encoder can directly go to the source in the translation.</li>
<li>It also provides interpretability because we know which words are used for alignment.</li>
<li>It definitely improves the performance of the model.</li>
</ul>
<p>Its success has given <a id="_idIndexMarker102"/>rise<a id="_idIndexMarker103"/> to several variants where the scoring function is different. One variant in particular, called <strong class="bold">self-attention</strong>, has the particular advantage that it extracts information directly from the input without necessarily needing to compare it with something else.</p>
<p>The insight behind self-attention is that if we want to look for a book for an essay on the French Revolution in a library (query), we don’t need to read all the books to find a book on the history of France (value), we just need to read the coasts of the books (key). Self-attention, in other words, is a method that allows us to search within context to find the representation we need.</p>
<div><div><img alt="Figure 2.4 – Self-attention mechanism. Matrix dimensions are included; numbers are arbitrary" src="img/B21257_02_04.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.4 – Self-attention mechanism. Matrix dimensions are included; numbers are arbitrary</p>
<p>Transacting this for a model, given an input we want to conduct a series of comparisons between the various components of the sequence (such as tokens) to obtain an output sequence (which we<a id="_idIndexMarker104"/> can<a id="_idIndexMarker105"/> then use for various models or tasks). The self-attention equation is as follows:</p>
<p><math display="block"><mrow><mrow><mi>A</mi><mi>t</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mfenced close=")" open="("><mrow><mi>Q</mi><mo>,</mo><mi>K</mi><mo>,</mo><mi>V</mi></mrow></mfenced><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mfenced close=")" open="("><mfrac><mrow><mi>Q</mi><mo>∙</mo><msup><mi>k</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac></mfenced><mo>∙</mo><mi>V</mi></mrow></mrow></math></p>
<p>You can immediately see that it is derived from the original attention formula. We have the dot product to conduct comparisons, and we then exploit the <code>softmax</code> function to calculate the relative importance and normalize the values between 0 and 1. <em class="italic">D</em> is the size of the sequence; in other words, self-attention is also normalized as a function of the length of our sequence.</p>
<p>The next step is using <code>softmax</code>. Here’s a little refresher on the function (how you calculate and how it is implemented more efficiently in Python):</p>
<p><math display="block"><mrow><mrow><mi>y</mi><mo>=</mo><mfrac><msup><mi>e</mi><msub><mi>x</mi><mi>i</mi></msub></msup><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>i</mi><mo>=</mo><mi>n</mi></mrow></msubsup><msup><mi>e</mi><msub><mi>x</mi><mi>i</mi></msub></msup></mrow></mfrac></mrow></mrow></math></p>
<p><math display="block"><mrow><mrow><mi>y</mi><mo>=</mo><mfenced close="]" open="["><mtable columnalign="center" columnwidth="auto" rowalign="baseline baseline baseline" rowspacing="1.0000ex 1.0000ex"><mtr><mtd><msub><mi>y</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd><msub><mi>y</mi><mn>2</mn></msub></mtd></mtr><mtr><mtd><msub><mi>y</mi><mn>3</mn></msub></mtd></mtr></mtable></mfenced><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mfenced close=")" open="("><mi>x</mi></mfenced><mo>=</mo><mi>f</mi><mfenced close=")" open="("><mfenced close="]" open="["><mtable columnalign="center" columnwidth="auto" rowalign="baseline baseline baseline" rowspacing="1.0000ex 1.0000ex"><mtr><mtd><msub><mi>x</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd><msub><mi>x</mi><mn>2</mn></msub></mtd></mtr><mtr><mtd><msub><mi>x</mi><mn>3</mn></msub></mtd></mtr></mtable></mfenced></mfenced><mo>=</mo><mfenced close="]" open="["><mtable columnalign="center" columnwidth="auto" rowalign="baseline baseline baseline" rowspacing="1.0000ex 1.0000ex"><mtr><mtd><mfrac><msup><mi>e</mi><msub><mi>x</mi><mn>1</mn></msub></msup><mrow><msup><mi>e</mi><msub><mi>x</mi><mn>1</mn></msub></msup><mo>+</mo><msup><mi>e</mi><msub><mi>x</mi><mn>2</mn></msub></msup><mo>+</mo><msup><mi>e</mi><msub><mi>x</mi><mn>3</mn></msub></msup></mrow></mfrac></mtd></mtr><mtr><mtd><mfrac><msup><mi>e</mi><msub><mi>x</mi><mn>2</mn></msub></msup><mrow><msup><mi>e</mi><msub><mi>x</mi><mn>1</mn></msub></msup><mo>+</mo><msup><mi>e</mi><msub><mi>x</mi><mn>2</mn></msub></msup><mo>+</mo><msup><mi>e</mi><msub><mi>x</mi><mn>3</mn></msub></msup></mrow></mfrac></mtd></mtr><mtr><mtd><mfrac><msup><mi>e</mi><msub><mi>x</mi><mn>3</mn></msub></msup><mrow><msup><mi>e</mi><msub><mi>x</mi><mn>1</mn></msub></msup><mo>+</mo><msup><mi>e</mi><msub><mi>x</mi><mn>2</mn></msub></msup><mo>+</mo><msup><mi>e</mi><msub><mi>x</mi><mn>3</mn></msub></msup></mrow></mfrac></mtd></mtr></mtable></mfenced></mrow></mrow></math></p>
<p><math display="block"><mrow><mrow><mi>p</mi><mi>y</mi><mi>t</mi><mi>h</mi><mi>o</mi><mi>n</mi><mo>:</mo><mi>y</mi><mo>=</mo><mfenced close="]" open="["><mtable columnalign="center" columnwidth="auto" rowalign="baseline baseline baseline" rowspacing="1.0000ex 1.0000ex"><mtr><mtd><msub><mi>y</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd><msub><mi>y</mi><mn>2</mn></msub></mtd></mtr><mtr><mtd><msub><mi>y</mi><mn>3</mn></msub></mtd></mtr></mtable></mfenced><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mfenced close=")" open="("><mi>x</mi></mfenced><mo>=</mo><mi>f</mi><mfenced close=")" open="("><mfenced close="]" open="["><mtable columnalign="center" columnwidth="auto" rowalign="baseline baseline baseline" rowspacing="1.0000ex 1.0000ex"><mtr><mtd><msub><mi>x</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd><msub><mi>x</mi><mn>2</mn></msub></mtd></mtr><mtr><mtd><msub><mi>x</mi><mn>3</mn></msub></mtd></mtr></mtable></mfenced></mfenced><mo>=</mo><mfrac><mfenced close="]" open="["><mtable columnalign="center center center" columnspacing="0.8000em 0.8000em" columnwidth="auto auto auto" rowalign="baseline"><mtr><mtd><msub><mi>x</mi><mn>1</mn></msub></mtd><mtd><msub><mi>x</mi><mn>2</mn></msub></mtd><mtd><msub><mi>x</mi><mn>3</mn></msub></mtd></mtr></mtable></mfenced><mrow><msup><mi>e</mi><msub><mi>x</mi><mn>1</mn></msub></msup><mo>+</mo><msup><mi>e</mi><msub><mi>x</mi><mn>2</mn></msub></msup><mo>+</mo><msup><mi>e</mi><msub><mi>x</mi><mn>3</mn></msub></msup></mrow></mfrac><mo>=</mo><mfrac><msup><mi>e</mi><mi>x</mi></msup><mrow><mi>s</mi><mi>u</mi><mi>m</mi><mo>(</mo><msup><mi>e</mi><mi>x</mi></msup><mo>)</mo></mrow></mfrac></mrow></mrow></math></p>
<p>As we saw in the previous chapter, the dot product can become quite wide as the length of the vectors increases. This can lead to inputs that are too large in the <code>softmax</code> function (this shifts<a id="_idIndexMarker106"/> the<a id="_idIndexMarker107"/> probability mass in <code>softmax</code> to a few elements and thus leads to small gradients). In the original article, they solved this by normalizing by the square root of <em class="italic">D</em>.</p>
<div><div><img alt="Figure 2.5 – Self-attention unrolled" src="img/B21257_02_05.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.5 – Self-attention unrolled</p>
<p>The real difference is that we use three matrices of weights <strong class="bold">Query</strong> (<strong class="bold">Q</strong>), <strong class="bold">Key</strong> (<strong class="bold">K</strong>), and <strong class="bold">Value</strong> (<strong class="bold">V</strong>) that are initially randomly initialized. <em class="italic">Q</em> is the current focus of attention, while <em class="italic">K</em> informs the model about previous inputs, and <em class="italic">V</em> serves to extract the final input information. So, the first step is the multiplication of these three matrices with our input <em class="italic">X</em> (an array of vectors, of which each represents a token).</p>
<p><math display="block"><mrow><mrow><mi mathvariant="bold-italic">Q</mi><mo>=</mo><mi mathvariant="bold-italic">X</mi><mo>∙</mo><msup><mi mathvariant="bold-italic">W</mi><mi>Q</mi></msup><mo>,</mo><mi mathvariant="bold-italic">K</mi><mo>=</mo><mi mathvariant="bold-italic">X</mi><mo>∙</mo><msup><mi mathvariant="bold-italic">W</mi><mi>V</mi></msup><mo>,</mo><mi mathvariant="bold-italic">V</mi><mo>=</mo><mi mathvariant="bold-italic">X</mi><mo>∙</mo><msup><mi mathvariant="bold-italic">W</mi><mi>V</mi></msup></mrow></mrow></math></p>
<p>The beauty of this system is that we can use it to extract more than one representation from the same input (after all, we can have multiple questions in a textbook). Therefore, since the operations are parallelizable, we can have multi-head attention. <strong class="bold">Multi-head self-attention</strong> enables <a id="_idIndexMarker108"/>the model to simultaneously capture multiple types of relationships within the input sequence. This is crucial because a single word in a sentence can be contextually related to several other words. During training, the <em class="italic">K </em>and <em class="italic">Q</em> matrices in each head specialize in modeling different kinds of relationships. Each attention head produces an output based on its specific perspective, resulting in <em class="italic">n</em> outputs for <em class="italic">n</em> heads. These outputs are then concatenated and passed through a final linear projection layer to restore the dimensionality backt<a id="_idIndexMarker109"/> to<a id="_idIndexMarker110"/> the original input size.</p>
<div><div><img alt="Figure 2.6 – Multi-head self-attention" src="img/B21257_02_06.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.6 – Multi-head self-attention</p>
<p>Self-attention has several advantages:</p>
<ul>
<li>We can extract <a id="_idIndexMarker111"/>different representations for each input.</li>
<li>We can conduct all these computations in parallel and thus with a GPU. Each head can be computed independently.</li>
<li>We can use it in models that do not necessarily consist of an encoder and a decoder.</li>
<li>We do not have to wait for different time steps to see the relationship between distant word pairs (as in RNN).</li>
<li>However, it has a quadratic cost in function of the number of tokens <em class="italic">N</em>, and it has no inherent <a id="_idIndexMarker112"/>notion of order.</li>
</ul>
<p>Self-attention is computationally expensive. It can be shown that, considering a sequence <em class="italic">T</em> and sequence length <em class="italic">d</em>, the computation cost and space is quadratic:</p>
<p><math display="block"><mrow><mrow><mrow><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mo>=</mo><mi mathvariant="script">O</mi><mfenced close=")" open="("><mrow><msup><mi>T</mi><mn>2</mn></msup><mo>+</mo><mi>d</mi></mrow></mfenced><mi>s</mi><mi>p</mi><mi>a</mi><mi>c</mi><mi>e</mi><mo>=</mo><mi mathvariant="script">O</mi><mo>(</mo><msup><mi>T</mi><mn>2</mn></msup><mo>+</mo><mi>T</mi><mi>d</mi><mo>)</mo></mrow></mrow></mrow></math></p>
<p>They identified the dot product as the culprit. This computational cost is one of the problems of scalability (taking into account that multi-head attention is calculated in each block). For this reason, many variations of self-attention have been proposed to reduce the computational cost.</p>
<p>Despite the computational cost, self-attention has shown its capability, especially when several layers are<a id="_idIndexMarker113"/> stacked<a id="_idIndexMarker114"/> on top of each other. In the next section, we will discuss how this makes the model extremely powerful despite its computational cost.</p>
<h1 id="_idParaDest-37"><a id="_idTextAnchor036"/>Introducing the transformer model</h1>
<p>Despite this decisive advance though, several problems remain in<a id="_idIndexMarker115"/> machine translation:</p>
<ul>
<li>The model fails to capture the meaning of the sentence and is still error-prone</li>
<li>In addition, we have problems with words that are not in the initial vocabulary</li>
<li>Errors in pronouns and other grammatical forms</li>
<li>The model fails to maintain context for long texts</li>
<li>It is not adaptable if the domain in the training set and test data is different (for example, if it is trained on literary texts and the test set is finance texts)</li>
<li>RNNs are not parallelizable, and you have to compute sequentially</li>
</ul>
<p>Considering these points, Google researchers in 2016 came up with the idea of eliminating RNNs altogether rather than improving them. According to the authors of the <em class="italic">Attention is All You Need</em> seminal article; all you need is a model that is based on multi-head self-attention. Before going into detail, the transformer consists entirely of stacked layers of multi-head self-attention. In this way, the model learns a hierarchical and increasingly sophisticated representation of the text.</p>
<p>The first step in the process is the transformation of text into numerical vectors (tokenization). After that, we have an embedding step to obtain vectors for each token. A special feature of the transformer is the introduction of a function to record the position of each token in the sequence (self-attention is not position-aware). This process is <a id="_idIndexMarker116"/>called <strong class="bold">positional encoding</strong>. The authors in the article use sin and cos alternately <a id="_idIndexMarker117"/>with position. This allows the model to know the relative position of each token.</p>
<p><mml:math display="block"><mml:msub><mml:mrow><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mo>/</mml:mo><mml:msup><mml:mrow><mml:mn>1000</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>i</mml:mi><mml:mo>/</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:math></p>
<p><mml:math display="block"><mml:msub><mml:mrow><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mo>/</mml:mo><mml:msup><mml:mrow><mml:mn>1000</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>i</mml:mi><mml:mo>/</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:math></p>
<p>In the first step, the embedding vectors are summed with the result of these functions. This is because self-attention is not aware of word order, but word order in a period is important. Thus, the order is directly encoded in the vectors it awaits. Note, though, that there are no learnable parameters in this function and that for long sequences, it will have to be modified (we will discuss this in the next chapter).</p>
<div><div><img alt="Figure 2.7 – Positional encoding" src="img/B21257_02_07.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.7 – Positional encoding</p>
<p>After that, we have a<a id="_idIndexMarker118"/> series of transformer blocks in sequence. The <strong class="bold">transformer block</strong> consists <a id="_idIndexMarker119"/>of four elements: multi-head self-attention, feedforward layer, residual connections, and layer normalization.</p>
<div><div><img alt="Figure 2.8 – Flow diagram of the transformer block" src="img/B21257_02_08.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.8 – Flow diagram of the transformer block</p>
<p>The feedforward layer consists of two linear layers. This layer is used to obtain a linear projection of the multi-head self-attention. The weights are identifiable for each position and are <a id="_idIndexMarker120"/>separated. It can be seen as two linear transformations with one ReLU activation in between.</p>
<p><mml:math display="block"><mml:mi>F</mml:mi><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mo>⁡</mml:mo><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></p>
<p>This adds a step of non-linearity to self-attention. The FFN layer is chosen because it is an easily parallelized operation.</p>
<p>Residual connections are connections that pass information between two layers without going through the intermediate layer transformation. Initially developed in convolutional networks, they allow a shortcut between layers and help the gradient pass down to the lower layers. In the transformer, blocks are present for both the attention layer and feedforward, where the input is summed with the output. Residual connections also have the advantage of making the loss surface smoother (this helps the model find a better minimum and not get stuck in a local loss). This powerful effect can be seen clearly in <em class="italic">Figure 2</em><em class="italic">.9</em>:</p>
<div><div><img alt="Figure 2.9 – Effect of the residual connections on the loss" src="img/B21257_02_09.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.9 – Effect of the residual connections on the loss</p>
<p class="callout-heading">Note</p>
<p class="callout"><em class="italic">Figure 2</em><em class="italic">.9</em> is originally from <em class="italic">Visualizing the Loss Landscape of Neural Nets</em> by Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein (<a href="https://github.com/tomgoldstein/loss-landscape/tree/master">https://github.com/tomgoldstein/loss-landscape/tree/master</a>).</p>
<p>The residual connection <a id="_idIndexMarker121"/>makes the loss surface smoother, which allows the model to be trained more efficiently and quickly.</p>
<p>Layer normalization<a id="_idIndexMarker122"/> is a form of normalization that helps training because it keeps the hidden layer values in a certain range (it is an alternative to batch normalization). Having taken a single vector, it is normalized in a process that takes advantage of the mean and standard deviation. Having calculated the mean and standard deviation, the vector is scaled:</p>
<p><math display="block"><mrow><mrow><mi>μ</mi><mo>=</mo><mfrac><mn>1</mn><mi>d</mi></mfrac><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><msub><mi>x</mi><mi>i</mi></msub></mrow><mi>σ</mi><mo>=</mo><msqrt><mrow><mfrac><mn>1</mn><mi>d</mi></mfrac><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><msup><mrow><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><mi>μ</mi><mo>)</mo></mrow><mn>2</mn></msup></mrow></mrow></msqrt></mrow></mrow></math></p>
<p><math display="block"><mrow><mrow><mover><mi>x</mi><mo stretchy="true">ˆ</mo></mover><mo>=</mo><mfrac><mrow><mo>(</mo><mi>x</mi><mo>−</mo><mi>μ</mi><mo>)</mo></mrow><mi>σ</mi></mfrac></mrow></mrow></math></p>
<p>In the final transformation, we exploit two parameters that are learned during training.</p>
<p><math display="block"><mrow><mrow><mi>L</mi><mi>a</mi><mi>y</mi><mi>e</mi><mi>r</mi><mi>N</mi><mi>o</mi><mi>r</mi><mi>m</mi><mi>a</mi><mi>l</mi><mi>i</mi><mi>z</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>=</mo><mi>γ</mi><mover><mi>x</mi><mo stretchy="true">ˆ</mo></mover><mo>+</mo><mi>β</mi></mrow></mrow></math></p>
<p>There is a lot of variability during the training, and this can hurt the learning of the training. To reduce uninformative variability, we add this normalization step, thus normalizing the gradient as well.</p>
<p>At this point, we can assemble everything into a single block. Consider that after embedding, we have as input <em class="italic">X</em> a matrix of dimension <em class="italic">n x d</em> (with <em class="italic">n</em> being the number of tokens, and <em class="italic">d</em> the dimensions of the embedding). This input <em class="italic">X</em> goes into a transformer block and comes out with the same dimensions. This process is repeated for all transformer blocks:</p>
<p><mml:math display="block"><mml:mi mathvariant="bold-italic">H</mml:mi><mml:mo>=</mml:mo><mml:mi>L</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>N</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>+</mml:mo><mml:mi>M</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>H</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>S</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>f</mml:mi><mml:mi>A</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math></p>
<p><mml:math display="block"><mml:mi mathvariant="bold-italic">H</mml:mi><mml:mo>=</mml:mo><mml:mi>L</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>N</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">H</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">H</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math></p>
<p>Some notes on this process are as follows:</p>
<ul>
<li>In some architectures, <em class="italic">LayerNorm</em> can be after the <em class="italic">FFN</em> block instead of before (whether it is better or not is still debated).</li>
<li>Modern models have up to 96 transformer blocks in series, but the structure is virtually identical. The idea is that the model learns an increasingly complex representation of the language.</li>
<li>Starting with the embedding of an input, self-attention allows this representation to be enriched by incorporating an increasingly complex context. In addition, the model also has information about the location of each token.</li>
<li>Absolute positional encoding has the defect of overrepresenting words at the beginning of the sequence. Today, there are variants that consider the relative position.</li>
</ul>
<p>Once we have “the bricks,” we <a id="_idIndexMarker123"/>can assemble them into a functional structure. In the original description, the model was structured for machine translation and composed of two parts: an encoder (which takes the text to be translated) and a decoder (which will produce the translation).</p>
<p>The original transformer is composed of different blocks of transformer blocks and structures in an encoder and decoder, as you can see in <em class="italic">Figure 2</em><em class="italic">.10</em>.</p>
<div><div><img alt="Figure 2.10 – Encoder-decoder structure" src="img/B21257_02_10.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.10 – Encoder-decoder structure</p>
<p>The decoder, like the encoder, is composed of an embedding, a positional encoder, and a series of transformer blocks. One note is that in the decoder, instead of self-attention, we have <strong class="bold">cross-attention</strong>. Cross-attention<a id="_idIndexMarker124"/> is exactly the same, only we take both elements from the encoder and the decoder (because we want to condition the generation of the<a id="_idIndexMarker125"/> decoder based on the encoder input). In this case, the queries come from the encoder and the rest from the decoder. As you can see from <em class="italic">Figure 2</em><em class="italic">.11</em>, the decoder sequence can be of different sizes, but the result is the same:</p>
<div><div><img alt="Figure 2.11 – Cross-attention" src="img/B21257_02_11.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.11 – Cross-attention</p>
<p>Input <em class="italic">N</em> comes from the encoder, while input <em class="italic">M</em> is from the decoder. In the figure, cross-attention is mixing information from the encoder and decoder, allowing the decoder to learn from the encoder.</p>
<p>Another note on the structure: in the decoder, the first self-attention has an additional mask to prevent the model from seeing the future.</p>
<p>This is especially true in the case of QT. In fact, if one wants to predict the next word and the model already knows what it is, we have data leakage. To compensate for this, we add a mask in which the upper-triangular portion is replaced with negative infinity: - ∞.</p>
<div><div><img alt="Figure 2.12 – Masked attention" src="img/B21257_02_12.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.12 – Masked attention</p>
<p>The first transformer <a id="_idIndexMarker126"/>consisted of an encoder and decoder, but today there are also models that are either encoder-only or decoder-only. Today, for generative AI, they are practically all decoder-only. We have our model; now, how can you train a system that seems so complex? In the next section, we will see how to succeed at training.</p>
<h1 id="_idParaDest-38"><a id="_idTextAnchor037"/>Training a transformer</h1>
<p>How do you train<a id="_idIndexMarker127"/> such a complex model? The answer to this question is simpler than you might think. The fact that the model can learn through multi-head self-attention complex and diverse relationships allows the model to be able to be flexible and able to learn complex patterns. It would be too expensive to build examples (or find them) to teach these complex relationships to the model. So, we want a system that allows the model to learn these relationships on its own. The advantage is that if we have a large amount of text available, the model can learn without the need for us to curate the training corpus. Thanks to the advent of the internet, we have the availability of huge corpora that allow models to see text examples of different topics, languages, styles, and more.</p>
<p>Although the original model was a <code>seq2seq</code> model, later transformers (such as LLMs) were trained as language models, especially <a id="_idIndexMarker128"/>in a <strong class="bold">self-supervised manner</strong>. In language modeling, we consider a sequence of word <em class="italic">s</em>, and the probability of the next word in the sequence <em class="italic">x</em> is <mml:math><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>w</mml:mi><mml:mo>|</mml:mo><mml:mi>h</mml:mi><mml:mo>)</mml:mo></mml:math>. This probability depends on the words up to that point. By the chain rule of the probability, we can decompose this probability:</p>
<p><mml:math display="block"><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo stretchy="false">∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></p>
<p>This allows us to calculate the conditional probability of a word from a sequence of previous words. The idea is that when we have enough text we can take a sequence such as <strong class="bold">“to be or not to”</strong> as input and have the model estimate the probability for the next word to be <strong class="bold">“be,”</strong> <math><mrow><mrow><mrow><mi>P</mi><mo>(</mo><mi>b</mi><mi>e</mi><mo>|</mo><mi>t</mi><mi>o</mi><mi>b</mi><mi>e</mi><mi>o</mi><mi>r</mi><mi>n</mi><mi>o</mi><mi>t</mi><mi>t</mi><mi>o</mi><mo>)</mo></mrow></mrow></mrow></math>. Then after the transformer block sequence, we have a layer that conducts a linear projection and<a id="_idIndexMarker129"/> a <strong class="bold">softmax layer</strong> that generates the output. The previous sequence is called context; the context length of the first transformers<a id="_idIndexMarker130"/> was 512 tokens. The model generates an output, which is a probability vector of dimension <em class="italic">V</em> (the model vocabulary), also <a id="_idIndexMarker131"/>called a <strong class="bold">logit vector</strong>. The projection layer is called an <strong class="bold">unembedder</strong> (it does <a id="_idIndexMarker132"/>reverse mapping) because we have to go from a dimension <em class="italic">N</em> tokens x <em class="italic">D</em> embedding to 1 x <em class="italic">V</em>. Since the input and output of each transformer block are the same, we could theoretically eliminate blocks and attach an unembedder and softmax to any intermediate block. This allows us to better interpret the function of each block and its internal representation.</p>
<p>Once we have this probability vector, we can use self-supervision for training. We take a corpus of text (unannotated) and train the model to minimize the difference between the probability of the true word in the sequence and the predicted probability. To do this, we use <strong class="bold">cross-entropy loss</strong> (the<a id="_idIndexMarker133"/> difference between the predicted and true probability distribution). The predicted probability distribution is the logit vector, while the true one is a one-hot encoder vector where it is 1 for the next word in the sequence and 0 elsewhere.</p>
<p><mml:math display="block"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mrow><mml:munder><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>w</mml:mi><mml:mo>∈</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:mfenced><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>[</mml:mo><mml:mi>w</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></p>
<p>In practice, it is simplified during training simply between the probability of the actual predicted word and 1. The process is iterative for each word in the word sequence (and is called <a id="_idIndexMarker134"/>teacher forcing). The final loss is the average over the entire sequence.</p>
<div><div><img alt="Figure 2.13 – Training of the transformer; the loss is the average of the loss of all the time steps" src="img/B21257_02_13.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.13 – Training of the transformer; the loss is the average of the loss of all the time steps</p>
<p>Since all calculations <a id="_idIndexMarker135"/>can be done in parallel in the transformer, we do not have to calculate word by word, but we fed the model with the whole sequence.</p>
<p>Once we have obtained a probability vector, we can choose the probability most (<strong class="bold">greedy decoding</strong>). Greedy<a id="_idIndexMarker136"/> decoding is formally defined as choosing the token with the highest probability at each time step:</p>
<p><mml:math display="block"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mo>∈</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>w</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></p>
<p>In fact, it is no longer used because the result is too predictable, generic, and repetitive. So, more sophisticated and less deterministic sampling methods are used. This sampling process is called decoding (or autoregressive generation or causal language modeling, since it is derived from previous word choice). This system, in the simplest version, is based either on generating the text of at most a predetermined sequence length, or as long as an end-of-sentence token (<code>&lt;EOS&gt;</code>) is selected.</p>
<p>We need to find a way to be able to select tokens while balancing both quality and diversity. A model that always chooses the same words will certainly have higher quality but will also be repetitive. There are different methods of doing the sampling:</p>
<ul>
<li><strong class="bold">Random sampling</strong>: The <a id="_idIndexMarker137"/>model chooses the next token randomly. The sentences<a id="_idIndexMarker138"/> are strange because the model chooses rare or singular words.</li>
<li><strong class="bold">Top-k sampling</strong>: At each step, we sort the probabilities and choose the top <em class="italic">k</em> most likely<a id="_idIndexMarker139"/> words. We renormalize the probability and choose one at random.</li>
<li><strong class="bold">Top-p sampling</strong>: This is an alternative in which we keep only a percentage of the most likely<a id="_idIndexMarker140"/> words.</li>
<li><code>softmax</code>, we <a id="_idIndexMarker141"/>divide by a temperature parameter (between 0 and 1). The closer <em class="italic">t</em> is to 0, the closer the probability of the most likely words is to 1 (close to greedy sampling). In some cases, we can also have <em class="italic">t</em> greater than 1 when we want a less greedy approach.</li>
</ul>
<p>So far, we have <a id="_idIndexMarker142"/>considered the fixed vocabulary and assumed that each token was a word. In general, once the model is trained, there might be some words that the model does not know to which a special token, <code>&lt;UNK&gt;</code>, is assigned. In transformers and LLMs afterward, a way was sought to solve the unknown word problem. For example, in the training set, we might have words such as <em class="italic">big</em>, <em class="italic">bigger</em>, and <em class="italic">small</em> but not <em class="italic">smaller</em>. <em class="italic">Smaller</em> would not be known by the model and would result in <code>&lt;UNK&gt;</code>. Depending on the training set, the model might have incomplete or outdated knowledge. In English, as in other languages, there are definite morphemes and grammatical rules, and we would like the tokenizer to be aware. To avoid too many <code>&lt;UNK&gt;</code> one solution is to think in terms of sub-words (tokens).</p>
<p>One of the most widely<a id="_idIndexMarker143"/> used is <strong class="bold">Byte-Pair Encoding</strong> (<strong class="bold">BPE</strong>). The process starts with a list of individual characters. The algorithm then scans the entire corpus and begins to merge the symbols that are most frequently found together. For example, we have <strong class="bold">E</strong> and <strong class="bold">R</strong>, and after the first scan, we add a new <strong class="bold">ER</strong> symbol to the vocabulary. The process continues iteratively to merge and create new symbols (longer and longer character strings). Typically, the algorithm stops when it has created <em class="italic">N</em> tokens (with <em class="italic">N</em> being a predetermined number at the beginning). In addition, there is a special end-of-word symbol to differentiate whether the token is inside or at the end of a word. Once the algorithm arrives at creating a vocabulary, we can segment the corpus with the tokenizer and for each subword, we assign an index corresponding to the index in the vocabulary.</p>
<div><div><img alt="Figure 2.14 – Example of the results of tokenization" src="img/B21257_02_14.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.14 – Example of the results of tokenization</p>
<p>This approach generally causes common words to be present in the model vocabulary while rare words are split into subwords. In addition, the model also learns suffixes and prefixes, and <a id="_idIndexMarker144"/>considers the difference between <em class="italic">app</em> and the <em class="italic">app#</em> subword, representing a complete word and a subword (<em class="italic">app#</em> as a subword of <em class="italic">application</em>).</p>
<h1 id="_idParaDest-39"><a id="_idTextAnchor038"/>Exploring masked language modeling</h1>
<p>Although the <a id="_idIndexMarker145"/>transformer was revolutionary, the popularization of the transformer in the scientific community is also due<a id="_idIndexMarker146"/> to the <strong class="bold">Bidirectional Encoder Representations from Transformers</strong> (<strong class="bold">BERT</strong>) model. This is because BERT was a revolutionary variant of the transformer that showed the capabilities of this type of model. BERT was revolutionary because it was already prospectively designed specifically for future applications (such as question answering, summarization, and machine translation). In fact, the original transformer analyzes the left-to-right sequence, so when the model encounters an entity, it cannot relate it to what is on the right of the entity. In these applications, it is important to have context from both directions.</p>
<div><div><img alt="Figure 2.15 – Difference between a causal and bidirectional language model" src="img/B21257_02_15.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.15 – Difference between a causal and bidirectional language model</p>
<p><strong class="bold">Bidirectional encoders</strong> resolve this <a id="_idIndexMarker147"/>limitation by allowing the model to find relationships over the entire sequence. Obviously, we can no longer use a language model to train it (it will be too easy to identify the next word in the sequence when you already know the answer) but we have to find a way to be able to train a bidirectional model. For clarification, the model reads the entire sequence at once and, in this case, consists of the encoder only.</p>
<p>To try to minimize <a id="_idIndexMarker148"/>changes to the structure we use what is called the <code>&lt;MASK&gt;</code>). In the original training, they masked 15 % of the tokens randomly. Notice that, in this case, we do not mask the future because we want the model to be aware of the whole context. Also, to better separate the different sentences, we have a special token, <code>[CLS]</code>, that signals the beginning of an input, and <code>[SEP]</code> to separate sentences in the input (for example, if we have a question and an answer). Otherwise, the structure is the same: we have an embedder, a position encoder, different transformer blocks, a linear projection, and a softmax. The loss is calculated in the same way; instead of using the next token, we use the masked token. The original article introduced two versions of BERT: BERT-BASE (12 layers, hidden size with d=768, 12 attention heads, and 110M total parameters) and BERT-LARGE (24 layers, hidden size with d=1024, 24 attention heads, and 340M total parameters).</p>
<p>MLM is a flexible approach because the idea is to corrupt the input and ask the model to rebuild. We can mask, but we can also reorder or conduct other transformations. The disadvantage of this method is that only 15 percent of the tokens are actually used to learn, so the model is highly inefficient.</p>
<p>The training is also highly flexible. For example, the model can be extended to <code>[SEP]</code> token between sentences). The last layer is a softmax for sentence classification; we consider the loss over the categories. This shows how the system is flexible and can be adapted to different tasks.</p>
<p>One final clarification. Until 2024, it was always assumed that these models were not capable of generating text. In 2024, two studies showed that by adapting the model, you can generate text even with a BERT-like model. For example, in this study, they show that one can generate text by exploiting a sequence of [MASK] tokens.</p>
<div><div><img alt="Figure 2.16 – Text generation with MLM (https://arxiv.org/pdf/2406.04823)" src="img/B21257_02_16.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.16 – Text generation with MLM (<a href="https://arxiv.org/pdf/2406.04823">https://arxiv.org/pdf/2406.04823</a>)</p>
<p>Now that we have seen the two main types of training for a transformer, we can better explore what happens inside these models.</p>
<h1 id="_idParaDest-40"><a id="_idTextAnchor039"/>Visualizing internal mechanisms</h1>
<p>We have seen the<a id="_idIndexMarker150"/> inner workings of the transformer, how it can be trained, and the main types of models. The beauty of attention is that we can visualize these relationships, and in this section, we will see how to do that. We can then visualize the relationships within the BERT attention head. As mentioned, in each layer, there are several attention heads and each of them learns a different representation of the input data. The color intensity indicates a greater weight in the attention weights (darker colors indicate weights that are close to 1).</p>
<p>We can do this<a id="_idIndexMarker151"/> using the BERTviz package:</p>
<pre class="source-code">
head_view(attention, tokens, sentence_b_start)</pre> <p class="callout-heading">Important note</p>
<p class="callout">The visualization is interactive. The code is in the repository. Try running it using different phrases and exploring different relationships between different words in the phrases. The visualization allows you to explore the different layers in the model by taking advantage of the drop-down model. Hovering over the various words allows you to see the individual weights of the various heads.</p>
<p>This is the corresponding visualization:</p>
<div><div><img alt="Figure 2.17 – Visualization of attention between all words in the input" src="img/B21257_02_17.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.17 – Visualization of attention between all words in the input</p>
<p>We can also view the various heads of the model at the same time. This allows us to see how the various heads model different relationships. This model has 12 heads for 12 layers, so the model has 144 attention heads and can therefore see more than 100 representations for the same sentences (this explains the capacity of a model). Moreover, these<a id="_idIndexMarker152"/> representations are not completely independent; information learned from earlier layers can be used by later layers:</p>
<pre class="source-code">
model_view(attention, tokens, sentence_b_start)</pre> <p class="callout-heading">Important note</p>
<p class="callout">The visualization is interactive. The code is in the repository. Try running it using different phrases and exploring different relationships. Here, we have the ensemble representation of the various attention heads. Observe how each head has a different function and how it models a different representation of the same inputs.</p>
<p> This is the corresponding visualization:</p>
<div><div><img alt="Figure 2.18 – Model view of the first two layers" src="img/B21257_02_18.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.18 – Model view of the first two layers</p>
<p>Another model that has been fundamental to the current development of today’s models is <strong class="bold">Generative Pre-Trained Transformer 2</strong> (<strong class="bold">GPT-2</strong>). GPT-2 is a causal (unidirectional) transformer<a id="_idIndexMarker153"/> pre-trained using language modeling on a very large corpus of ~40 GB of text <a id="_idIndexMarker154"/>data. GPT-2 was specifically trailed to predict the next token and to generate text with an input (it generates a token at a time; this token is then added to the input sequence to generate the next in an autoregressive process). In addition, this is perhaps the first model that has been trained with a massive amount of text. In addition, this model consists only of the decoder. GPT-2 is a family of models ranging from 12 layers of GPT-2 small to 48 layers of GPT-2 XL. Each layer consists of masked self-attention and a feed-forward neural network.</p>
<p>GPT-2 is generative and trained as a language model so we can give it an input judgment and observe the probability for the next token. For example, using “To be or not to” as input, the token with the highest probability is “be.”</p>
<div><div><img alt="Figure 2.19 – Probabilities associated with the next token for the GPT-2 model when probed with the “To be or not to” input sequence" src="img/B21257_02_19.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.19 – Probabilities associated with the next token for the GPT-2 model when probed with the “To be or not to” input sequence</p>
<p>Sometimes, it may be necessary to understand which tokens are most important to the model to generate the next token. <strong class="bold">Gradient X input</strong> is a<a id="_idIndexMarker155"/> technique originally developed for convolutional networks; at a given time step, we take the output probabilities for each token, select the tokens with the highest probability, and compute the gradient with <a id="_idIndexMarker156"/>respect to the input up to the input tokens. This gives us the importance of each token to generate the next token in the sequence (the rationale is that small changes in the input tokens with the highest importance carry the largest changes in the output). In the figure, we can see the most important tokens for the next token in the sequence:</p>
<div><div><img alt="Figure 2.20 – Gradient X input for the next token in the sequence" src="img/B21257_02_20.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.20 – Gradient X input for the next token in the sequence</p>
<p>As mentioned before, there is not only self-attention but also feedforward neural network, which plays an important role (it provides a significant portion of the parameters in the transformer block, about 66%). Therefore, several works have focused on examining the firings of neurons in layers (this technique was also originally developed for computer vision).</p>
<p>We can follow this activation after each layer, and for each of the tokens, we can monitor what their rank (by probability) is after each layer. As we can see, the model understands from<a id="_idIndexMarker157"/> the first layers which token is the most likely to continue a sequence:</p>
<div><div><img alt="Figure 2.21 – Heatmap of the rank for the top five most likely tokens after each layer" src="img/B21257_02_21.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.21 – Heatmap of the rank for the top five most likely tokens after each layer</p>
<p>Since there are a considerable<a id="_idIndexMarker158"/> number of neurons, it is complex to be able to observe them directly. Therefore, one way to investigate these activations is to first reduce dimensionality. To avoid negative activations, it is preferred to use <strong class="bold">Non-Negative Matrix Factorization</strong> (<strong class="bold">NMF</strong>) instead of <strong class="bold">Principal Component Analysis</strong> (<strong class="bold">PCA</strong>). The process<a id="_idIndexMarker159"/> first<a id="_idIndexMarker160"/> captures the activation of neurons in the FFNN layers of the model and is then decomposed into some factors (user-chosen parameters). Next, we can interactively observe the factors with the highest activation when a token has been generated. What we see in the graph is the factor excitation for each of the generated tokens:</p>
<div><div><img alt="Figure 2.22 – NMF for the activations of the model in generating a sequence" src="img/B21257_02_22.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.22 – NMF for the activations of the model in generating a sequence</p>
<p>We can also conduct this analysis for a single layer. This allows us to analyze interesting behaviors within the neurons of a layer (in the image layer 0 of the model). In this case, there are certain factors that focus on specific portions of the text (beginning, middle, and end). As we mentioned earlier, the model keeps track of word order in a sequence due to positional encoding, and this is reflected in activation. Other neurons, however, are activated by grammatical structures (such as conjunctions, articles, and so on). This indicates to us a specialization of what individual neurons in a pattern track and is one of the strength<a id="_idIndexMarker161"/> components of the transformer. By increasing the number of facts, we can increase the resolution and better understand what grammatical and semantic structures the pattern encodes in its activations. Moving forward in the structure of the model, we can see that layers learn a different representation.</p>
<div><div><img alt="Figure 2.23 – NMF for the activations of the model in generating a sequence" src="img/B21257_02_23.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.23 – NMF for the activations of the model in generating a sequence</p>
<p>We have seen how to build a transformer and how it works. Now that we know the anatomy of a transformer, it is time to see it at work.</p>
<h1 id="_idParaDest-41"><a id="_idTextAnchor040"/>Applying a transformer</h1>
<p>The power of a transformer <a id="_idIndexMarker162"/>lies in its ability to be able to learn from an enormous amount of text. During this phase of training (called <strong class="bold">pre-training</strong>), the <a id="_idIndexMarker163"/>model learns general rules about the structure of a language. This general representation can then be exploited for a myriad of applications. One of the most important concepts in deep learning<a id="_idIndexMarker164"/> is <strong class="bold">transfer learning</strong>, in which we exploit the ability of a model trained on a large amount of data for a task different from the one it was originally trained for. A special case of transfer learning<a id="_idIndexMarker165"/> is <strong class="bold">fine-tuning</strong>. Fine-tuning allows us to adapt the general knowledge of a model to a particular case. One way to do this is to add a set of parameters to a model (at the top of it) and then train these parameters by gradient descent for a specific task.</p>
<p>The transformer has been<a id="_idIndexMarker166"/> trained with large amounts of text and has learned semantic rules that are useful in understanding a text. We want to exploit this knowledge for a specific application such as sentiment classification. Instead of training a model from scratch, we can adapt a pre-trained transformer to classify our sentences. In this case, we do not want to destroy the internal representation of the model but preserve it. That is why, during fine-tuning, most of the layers are frozen (there is no update on the weights). Instead, we just train those one or two layers that we add to the top of the model. The idea is to preserve the representation and then learn how to use it for our specific task. Those two added layers learn precisely how to use the internal representation of the model. To give a simple example, let’s imagine we want to learn how to write scientific papers. To do that, we don’t have to learn how to write in English again, just to adapt our knowledge to this new task.</p>
<p>In BERT, as we mentioned, we add a particular token to the beginning of each sequence: a <code>[CLS]</code> token. During training or even inference in a bidirectional transformer, this token waits for all others in the sequence (if you remember, all tokens are connected). This means that the final vector (the one in the last layer) is contextualized for each element in the sequence. We can then exploit this vector for a classification task. If we have three classes (for example, positive, neutral, and negative) we can take the vector for a sequence and use softmax to classify.</p>
<p><mml:math display="block"><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>W</mml:mi><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>L</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></p>
<p>The model was not originally trained for sequence classification, so we'd like to introduce a learnable matrix to enable class separation. This matrix represents a linear transformation and can alternatively be implemented using one or more linear layers. We then apply a cross-entropy loss function to optimize these weights. This setup follows the standard supervised learning paradigm, where labeled data<a id="_idIndexMarker167"/> is used to adapt the transformer to a specific task.</p>
<p>In this process, we have so far assumed that the remainder of the transformer's weights remain frozen. However, as observed in convolutional neural networks, even minimal fine-tuning of model parameters can enhance performance. Such updates are typically carried out with a very low learning rate.</p>
<p>We can adapt a pre-trained transformer for new tasks through supervised fine-tuning.</p>
<div><div><img alt="Figure 2.24 – Fine-tuning a transformer" src="img/B21257_02_24.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.24 – Fine-tuning a transformer</p>
<p>In the first step, we are removing the final layer (this is specific to the original task). In the second step, we add a random initialized<a id="_idIndexMarker168"/> layer and gather training examples for the new task. During the fine-tuning, we are presenting the model with new examples (in this case, positive and negative reviews). While keeping the model frozen (each example is processed by the whole model in the forward pass), we update the weight only in the new layer (through backpropagation). The model has now learned the new task.</p>
<p>Conducting finetuning with Hugging Face is a straightforward process. We can use a model such as distill-BERT (a distilled version of BERT) with a few lines of code and the dataset we used in the previous chapter. We need to prepare the dataset and tokenize it (so that it can be used with a transformer). Hugging Face then allows with a simple wrapper that we can train the model. The arguments for training are stored in <code>TrainingArguments</code>:</p>
<pre class="source-code">
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=16
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
    evaluation_strategy="epoch"
)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset
)
trainer.train()</pre> <p class="callout-heading">Important note</p>
<p class="callout">Notice that the process is very similar to training a neural network. In fact, the transformer is a deep learning model; for the training, we are using similar hyperparameters.</p>
<p>In this case, we used only a small <a id="_idIndexMarker169"/>fraction of the reviews. The beauty of fine-tuning is that we need only a few examples to have a similar (if not better) performance than a model trained from scratch.</p>
<div><div><img alt="Figure 2.25 – Confusion matrix after fine-tuning" src="img/B21257_02_25.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.25 – Confusion matrix after fine-tuning</p>
<p>BERT’s training was <a id="_idIndexMarker170"/>done on 64 TPUs (special hardware for tensor operations) for four days; this is beyond the reach of most users. In contrast, fine-tuning is possible either on a single GPU or on a CPU. As a result, BERT achieved state-of-the-art performance upon its release across a wide array of tasks, including paraphrase detection, question answering, and sentiment analysis. Hence, several variants such <a id="_idIndexMarker171"/>as <strong class="bold">RoBERTa</strong> and <strong class="bold">SpanBERT</strong> (in this case, we mask an entire span instead<a id="_idIndexMarker172"/> of a single token with better results) or adapted to specific domains such as <strong class="bold">SciBERT</strong> were <a id="_idIndexMarker173"/>born. However, encoders are not optimal for generative tasks (because of mask training) while decoders are.</p>
<p>To conduct machine translation, the original transformer consisted of an encoder and a decoder. A model such as GPT-2 only has a decoder. We can conduct fine-tuning in the same way as seen before, we just need to construct the dataset in an optimal way for a model that is constituted by the decoder alone. For example, we can take a dataset in which<a id="_idIndexMarker174"/> we have English and French sentences and build a dataset for finetuning as follows: <code>&lt;sentence in English&gt;</code> followed by a special <code>&lt;to-fr&gt;</code> token and then the <code>&lt;sentence in French&gt;</code>. The same approach can be used to teach summarization to a model, where we insert a special token meaning summarization. The model is fine-tuned by conducting the next token prediction (language modeling).</p>
<div><div><img alt="Figure 2.26 – Fine-tuning of a decoder-only model" src="img/B21257_02_26.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.26 – Fine-tuning of a decoder-only model</p>
<p>Another way to exploit the learned knowledge of a model <a id="_idIndexMarker175"/>is to use <strong class="bold">knowledge distillation</strong>. In the previous section, we used distillGPT-2 which is a distilled version of GPT-2. A distilled model captures knowledge from a much larger model without losing significant performance but is much more manageable. Models that are trained with a large amount of text learn a huge body of knowledge. All this knowledge and skill is often redundant when we need a model for some specific task. We are interested in having a model that is <a id="_idIndexMarker176"/>very capable for a task, but without wanting to deal with a model of billions of parameters. In addition, sometimes we do not have enough examples for a model to learn the task from scratch. In this case, we can extract knowledge from the larger model.</p>
<div><div><img alt="Figure 2.27 – Generic teacher-student framework for knowledge distillation (https://arxiv.org/pdf/2006.05525)" src="img/B21257_02_27.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.27 – Generic teacher-student framework for knowledge distillation (<a href="https://arxiv.org/pdf/2006.05525">https://arxiv.org/pdf/2006.05525</a>)</p>
<p>Knowledge distillation can be seen as a form of compression, in which we try to transfer knowledge from a trained “teacher” model with many parameters to a “student” model with fewer parameters. The student model tries to mimic the teacher model and achieve the same performances as the teacher model in a task. In such a framework, we have three components: the models, knowledge, and algorithm. The algorithm can exploit either the teacher’s logits or intermediate activations. In the case of the logits, the student tries to mimic the predictions of the teacher model, so we try to minimize the difference between the logits produced by the teacher and the student. To do this, we use a distillation<a id="_idIndexMarker177"/> loss that allows us to train the student model.</p>
<div><div><img alt="Figure 2.28 – Teacher-student framework for knowledge distillation training (https://arxiv.org/pdf/2006.05525)" src="img/B21257_02_28.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.28 – Teacher-student framework for knowledge distillation training (<a href="https://arxiv.org/pdf/2006.05525">https://arxiv.org/pdf/2006.05525</a>)</p>
<p>For knowledge distillation, the steps are also similar. The first step is data preprocessing. For each model, you must remember to choose the model-specific tokenizer (although the one from GPT-2 is the most widely used many models have different tokenizers). We must then conduct fine-tuning of a model on our task (there is no model that is specific to classify reviews). This model will be our teacher. The next step is to train a student model. We can also use a pre-trained model that is smaller than the teacher (this allows us to be able to use a few examples to train it).</p>
<p>One important difference is that we now have a specific loss for knowledge distillation. This distillation loss calculates the loss between the teacher’s logits and the student’s logits. This function typically uses the Kullback-Leibler divergence loss to calculate the difference between the two probability distributions (Kullback-Leibler divergence is really a measure of the difference between two probability distributions). We can define it as follows:</p>
<pre class="source-code">
def distillation_loss(outputs_student, outputs_teacher,
                      temperature=2.0):
    log_prob_student = F.log_softmax(
        outputs_student / temperature, dim=-1)
    prob_teacher = F.softmax(
        outputs_teacher / temperature, dim=-1)
    loss = KLDivLoss(reduction='batchmean')(
        log_prob_student, prob_teacher)
    return loss</pre> <p>At this point, we just have to have a way to train our system. In this case, the teacher will be used only in<a id="_idIndexMarker178"/> inference while the student model will be trained. We will use the teacher’s logits to calculate the loss:</p>
<pre class="source-code">
def train_epoch(model, dataloader, optimizer, device,
                teacher_model, temperature=2.0):
    model.train()
    total_loss = 0
    for batch in tqdm(dataloader, desc="Training"):
        inputs = {k: v.to(device)
                  for k, v in batch.items()
                  if k in ['input_ids', 'attention_mask']}
        with torch.no_grad():
            outputs_teacher = teacher_model(**inputs).logits
        outputs_student = model(**inputs).logits
        loss = distillation_loss(
            outputs_student, outputs_teacher, temperature)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    return total_loss / len(dataloader)</pre> <p>As can be seen in the <a id="_idIndexMarker179"/>following figure, the performance of the student model is similar to the teacher model:</p>
<div><div><img alt="Figure 2.29 – Confusion matrix for the teacher and student model" src="img/B21257_02_29.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.29 – Confusion matrix for the teacher and student model</p>
<p>Fine-tuning and knowledge distillation allow us to be able to use a transformer for any supervised task. Fine-tuning allows us to work with datasets that are small (and where there are often too few examples to train a model from scratch). Knowledge distillation, on the other hand, allows <a id="_idIndexMarker180"/>us to get a smaller model (but performs as well as a much larger one) when the computational cost is the limit. By taking advantage of these techniques, we can tackle any task.</p>
<h1 id="_idParaDest-42"><a id="_idTextAnchor041"/>Summary</h1>
<p>In this chapter, we discussed the transformer, the model that revolutionized NLP and artificial intelligence. Today, all models that have commercial applications are derivatives of the transformer, as we learned in this chapter. Understanding how it works on a mechanistic level, and how the various parts (self-attention, embedding, tokenization, and so on) work together, allows us to understand the limitations of modern models. We saw how it works internally in a visual way, thus exploring the motive of modern artificial intelligence from multiple perspectives. Finally, we saw how we can adapt a transformer to our needs using techniques that leverage prior knowledge of the model. Now we can repurpose this process with virtually any dataset and any task.</p>
<p>Learning how to train a transformer will allow us to understand what happens when we take this process to scale. An LLM is a transformer with more parameters and that has been trained with more text. This leads to emergent properties that have made it so successful, but both its merits and shortcomings lie in the elements we have seen.</p>
<p>In <a href="B21257_03.xhtml#_idTextAnchor042"><em class="italic">Chapter 3</em></a>, we will see precisely how to obtain an LLM from a transformer. What we have learned in this chapter will allow us to see how this step comes naturally.</p>
</div>
</body></html>