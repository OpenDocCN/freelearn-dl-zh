<html><head></head><body>
<div id="_idContainer070">
<h1 class="chapter-number" id="_idParaDest-33"><a id="_idTextAnchor032"/><span class="koboSpan" id="kobo.1.1">2</span></h1>
<h1 id="_idParaDest-34"><a id="_idTextAnchor033"/><span class="koboSpan" id="kobo.2.1">The Transformer: The Model Behind the Modern AI Revolution</span></h1>
<p><span class="koboSpan" id="kobo.3.1">In this chapter, we will discuss the limitations of the models we saw in the previous chapter, and how a new paradigm (first attention mechanisms and then the transformer) emerged to solve these limitations. </span><span class="koboSpan" id="kobo.3.2">This will enable us to understand how these models are trained and why they are so powerful. </span><span class="koboSpan" id="kobo.3.3">We will discuss why this paradigm has been successful and why it has made it possible to solve tasks in </span><strong class="bold"><span class="koboSpan" id="kobo.4.1">natural language processing</span></strong><span class="koboSpan" id="kobo.5.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.6.1">NLP</span></strong><span class="koboSpan" id="kobo.7.1">) that</span><a id="_idIndexMarker089"/><span class="koboSpan" id="kobo.8.1"> were previously impossible. </span><span class="koboSpan" id="kobo.8.2">We will then see the capabilities of these models in </span><span class="No-Break"><span class="koboSpan" id="kobo.9.1">practical application.</span></span></p>
<p><span class="koboSpan" id="kobo.10.1">This chapter will clarify why contemporary LLMs are inherently based on the </span><span class="No-Break"><span class="koboSpan" id="kobo.11.1">transformer architecture.</span></span></p>
<p><span class="koboSpan" id="kobo.12.1">In this chapter, we’ll be covering the </span><span class="No-Break"><span class="koboSpan" id="kobo.13.1">following topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.14.1">Exploring attention </span><span class="No-Break"><span class="koboSpan" id="kobo.15.1">and self-attention</span></span></li>
<li><span class="koboSpan" id="kobo.16.1">Introducing the </span><span class="No-Break"><span class="koboSpan" id="kobo.17.1">transformer model</span></span></li>
<li><span class="koboSpan" id="kobo.18.1">Training </span><span class="No-Break"><span class="koboSpan" id="kobo.19.1">a transformer</span></span></li>
<li><span class="koboSpan" id="kobo.20.1">Exploring masked </span><span class="No-Break"><span class="koboSpan" id="kobo.21.1">language modeling</span></span></li>
<li><span class="koboSpan" id="kobo.22.1">Visualizing </span><span class="No-Break"><span class="koboSpan" id="kobo.23.1">internal mechanisms</span></span></li>
<li><span class="koboSpan" id="kobo.24.1">Applying </span><span class="No-Break"><span class="koboSpan" id="kobo.25.1">a transformer</span></span></li>
</ul>
<h1 id="_idParaDest-35"><a id="_idTextAnchor034"/><span class="koboSpan" id="kobo.26.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.27.1">Most of this code can be run on a CPU, but some parts (fine-tuning and knowledge distillation) are preferable to be run on a GPU (one hour of training on a CPU versus less than five minutes on </span><span class="No-Break"><span class="koboSpan" id="kobo.28.1">a GPU).</span></span></p>
<p><span class="koboSpan" id="kobo.29.1">The code is written in PyTorch and uses standard libraries for the most part (PyTorch, Hugging Face Transformers, and so on), though some snippets come from Ecco, a specific library. </span><span class="koboSpan" id="kobo.29.2">The code can be found on </span><span class="No-Break"><span class="koboSpan" id="kobo.30.1">GitHub: </span></span><a href="https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr2"><span class="No-Break"><span class="koboSpan" id="kobo.31.1">https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr2</span></span></a></p>
<h1 id="_idParaDest-36"><a id="_idTextAnchor035"/><span class="koboSpan" id="kobo.32.1">Exploring attention and self-attention</span></h1>
<p><span class="koboSpan" id="kobo.33.1">In the 1950s, with</span><a id="_idIndexMarker090"/><span class="koboSpan" id="kobo.34.1"> the beginning </span><a id="_idIndexMarker091"/><span class="koboSpan" id="kobo.35.1">of the computer revolution, governments began to become interested in the idea of machine translation, especially for military applications. </span><span class="koboSpan" id="kobo.35.2">These attempts failed miserably, for three main reasons: machine translation is more complex than it seems, there was not enough computational power, and there was not enough data. </span><span class="koboSpan" id="kobo.35.3">Governments concluded that it was a technically impossible challenge in </span><span class="No-Break"><span class="koboSpan" id="kobo.36.1">the 1960s.</span></span></p>
<p><span class="koboSpan" id="kobo.37.1">By the 1990s, two of the three limitations were beginning to be overcome: the internet finally allowed for abundant text, and the advent of GPUs finally allowed for computational power. </span><span class="koboSpan" id="kobo.37.2">The third requirement still had to be met: a model that could harness the newfound computational power to handle the complexity of </span><span class="No-Break"><span class="koboSpan" id="kobo.38.1">natural language.</span></span></p>
<p><span class="koboSpan" id="kobo.39.1">Machine translation captured the interest of researchers because it is a practical problem for which it is easy to evaluate the result (we can easily understand whether a translation is good or not). </span><span class="koboSpan" id="kobo.39.2">Moreover, we have an abundance of text in one language and a counterpart in another. </span><span class="koboSpan" id="kobo.39.3">So, researchers tried to adapt the previous models to the tasks (RNN, LSTM, and so on). </span><span class="koboSpan" id="kobo.39.4">The most commonly used system was the </span><strong class="bold"><span class="koboSpan" id="kobo.40.1">seq2seq model</span></strong><span class="koboSpan" id="kobo.41.1">, where </span><a id="_idIndexMarker092"/><span class="koboSpan" id="kobo.42.1">you have</span><a id="_idIndexMarker093"/><span class="koboSpan" id="kobo.43.1"> an </span><strong class="bold"><span class="koboSpan" id="kobo.44.1">encoder</span></strong><span class="koboSpan" id="kobo.45.1"> and a </span><strong class="bold"><span class="koboSpan" id="kobo.46.1">decoder</span></strong><span class="koboSpan" id="kobo.47.1">. </span><span class="koboSpan" id="kobo.47.2">The </span><a id="_idIndexMarker094"/><span class="koboSpan" id="kobo.48.1">encoder transforms the sequence into a new succinct representation that should preserve the relevant information (a sort of good summary). </span><span class="koboSpan" id="kobo.48.2">The decoder receives as input this context vector and uses this to transform (translate) this input into </span><span class="No-Break"><span class="koboSpan" id="kobo.49.1">the output.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer041">
<span class="koboSpan" id="kobo.50.1"><img alt="Figure 2.1 – A seq2seq model with an encoder and a decoder. In one case, we take the average of the hidden states (left); in the other case, we use attention to identify which hidden state is more relevant for the translation (right) " src="image/B21257_02_01.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.51.1">Figure 2.1 – A seq2seq model with an encoder and a decoder. </span><span class="koboSpan" id="kobo.51.2">In one case, we take the average of the hidden states (left); in the other case, we use attention to identify which hidden state is more relevant for the translation (right) </span></p>
<p><span class="koboSpan" id="kobo.52.1">RNNs and derived models have </span><span class="No-Break"><span class="koboSpan" id="kobo.53.1">some problems:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.54.1">Alignment</span></strong><span class="koboSpan" id="kobo.55.1">: The </span><a id="_idIndexMarker095"/><span class="koboSpan" id="kobo.56.1">length of input and output can be different (for example, to translate English to French “</span><em class="italic"><span class="koboSpan" id="kobo.57.1">she doesn’t like potatoes</span></em><span class="koboSpan" id="kobo.58.1">” into “</span><em class="italic"><span class="koboSpan" id="kobo.59.1">elle n’aime pas les pommes </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.60.1">de terre</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.61.1">”).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.62.1">Vanishing and exploding gradients</span></strong><span class="koboSpan" id="kobo.63.1">: Problems that arise during training so that multiple layers cannot be </span><span class="No-Break"><span class="koboSpan" id="kobo.64.1">managed effectively.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.65.1">Non-parallelizability</span></strong><span class="koboSpan" id="kobo.66.1">: Training is computationally expensive and not parallelizable. </span><span class="koboSpan" id="kobo.66.2">RNNs forget after a </span><span class="No-Break"><span class="koboSpan" id="kobo.67.1">few steps.</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer042">
<span class="koboSpan" id="kobo.68.1"><img alt="Figure 2.2 – Example of issues with alignment: one to many (left) and spurious word (right) " src="image/B21257_02_02.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.69.1">Figure 2.2 – Example of issues with alignment: one to many (left) and spurious word (right) </span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.70.1">Attention mechanisms</span></strong><span class="koboSpan" id="kobo.71.1"> were initially </span><a id="_idIndexMarker096"/><span class="koboSpan" id="kobo.72.1">described to solve the alignment problem, as well as to </span><a id="_idIndexMarker097"/><span class="koboSpan" id="kobo.73.1">learn </span><a id="_idIndexMarker098"/><span class="koboSpan" id="kobo.74.1">the relationships between the various parts of a text and the corresponding parts of the </span><span class="No-Break"><span class="koboSpan" id="kobo.75.1">translated text.</span></span></p>
<p><span class="koboSpan" id="kobo.76.1">The idea is that instead of passing the hidden state of RNNs, we pass contextual information that focuses only on the important parts of the sequence. </span><span class="koboSpan" id="kobo.76.2">During decoding (translation) for each token, we want to retrieve the corresponding and specific information in the other language. </span><span class="koboSpan" id="kobo.76.3">Attention determines which tokens in the input are important at </span><span class="No-Break"><span class="koboSpan" id="kobo.77.1">that moment.</span></span></p>
<p><span class="koboSpan" id="kobo.78.1">The first step is the alignment between the hidden state of the encoder (</span><em class="italic"><span class="koboSpan" id="kobo.79.1">h</span></em><span class="koboSpan" id="kobo.80.1">) and the previous decoder output (</span><em class="italic"><span class="koboSpan" id="kobo.81.1">s</span></em><span class="koboSpan" id="kobo.82.1">). </span><span class="koboSpan" id="kobo.82.2">The score function can be different: dot product or cosine similarity is most commonly used, but it can also be more complex functions such as the feedforward neural network layer. </span><span class="koboSpan" id="kobo.82.3">This step allows us to understand how relevant hidden state encoders are to the translation at that time. </span><span class="koboSpan" id="kobo.82.4">This step is conducted for all </span><span class="No-Break"><span class="koboSpan" id="kobo.83.1">encoder steps.</span></span></p>
<p><span class="_-----MathTools-_Math_Base"><math display="block"><mrow><mrow><mrow><mfenced close=")" open="("><mn>1</mn></mfenced><msub><mi>e</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>=</mo><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mo>(</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>h</mi><mi>j</mi></msub><mo>)</mo></mrow></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.84.1">Right now though, we have a scalar representing the similarity between two vectors (</span><em class="italic"><span class="koboSpan" id="kobo.85.1">h</span></em><span class="koboSpan" id="kobo.86.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.87.1">s</span></em><span class="koboSpan" id="kobo.88.1">). </span><span class="koboSpan" id="kobo.88.2">All these scores are passed into the softmax function that squeezes everything between 0 and 1. </span><span class="koboSpan" id="kobo.88.3">This step also serves to assign relative importance to each </span><span class="No-Break"><span class="koboSpan" id="kobo.89.1">hidden state.</span></span></p>
<p><span class="_-----MathTools-_Math_Base"><math display="block"><mrow><mrow><mfenced close=")" open="("><mn>2</mn></mfenced><msub><mi>a</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mfenced close=")" open="("><msub><mi>e</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></mfenced><mo>=</mo><mfrac><mrow><mi mathvariant="normal">e</mi><mi mathvariant="normal">x</mi><mi mathvariant="normal">p</mi><mo>(</mo><msub><mi>e</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>)</mo></mrow><mrow><msubsup><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi></msubsup><mrow><mi mathvariant="normal">e</mi><mi mathvariant="normal">x</mi><mi mathvariant="normal">p</mi><mo>(</mo><msub><mi>e</mi><mrow><mi>i</mi><mo>,</mo><mi>k</mi></mrow></msub><mo>)</mo></mrow></mrow></mfrac></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.90.1">Finally, we conduct</span><a id="_idIndexMarker099"/><span class="koboSpan" id="kobo.91.1"> a </span><a id="_idIndexMarker100"/><span class="koboSpan" id="kobo.92.1">weighted sum of the various hidden states multiplied by the attention score. </span><span class="koboSpan" id="kobo.92.2">So, we have a fixed-length context vector capable of giving us information about the entire set of hidden states. </span><span class="koboSpan" id="kobo.92.3">In simple words, during translation, we have a context vector that is dynamically updated and tells us how much attention we should give to each part of the </span><span class="No-Break"><span class="koboSpan" id="kobo.93.1">input sequence.</span></span></p>
<p><span class="_-----MathTools-_Math_Base"><math display="block"><mrow><mrow><mfenced close=")" open="("><mn>3</mn></mfenced><msub><mi>c</mi><mi>t</mi></msub><mo>=</mo><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><mrow><msub><mi>a</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>∙</mo><msub><mi>h</mi><mi>j</mi></msub></mrow></mrow></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.94.1">As you can see from the original article, the model pays different attention to the various words in the input </span><span class="No-Break"><span class="koboSpan" id="kobo.95.1">during translation.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer043">
<span class="koboSpan" id="kobo.96.1"><img alt="Figure 2.3 – Example of alignment between sentences after training the model with attention. Each pixel shows the attention weight between the source word and the target word. (https://arxiv.org/pdf/1409.0473)" src="image/B21257_02_03.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.97.1">Figure 2.3 – Example of alignment between sentences after training the model with attention. </span><span class="koboSpan" id="kobo.97.2">Each pixel shows the attention weight between the source word and the target word. </span><span class="koboSpan" id="kobo.97.3">(</span><a href="https://arxiv.org/pdf/1409.0473"><span class="koboSpan" id="kobo.98.1">https://arxiv.org/pdf/1409.0473</span></a><span class="koboSpan" id="kobo.99.1">)</span></p>
<p><span class="koboSpan" id="kobo.100.1">In addition to solving alignment, the attention mechanism has </span><a id="_idIndexMarker101"/><span class="No-Break"><span class="koboSpan" id="kobo.101.1">other advantages:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.102.1">It reduces the vanishing gradient problem because it provides a shortcut to </span><span class="No-Break"><span class="koboSpan" id="kobo.103.1">early states.</span></span></li>
<li><span class="koboSpan" id="kobo.104.1">It eliminates the bottleneck problem; the encoder can directly go to the source in </span><span class="No-Break"><span class="koboSpan" id="kobo.105.1">the translation.</span></span></li>
<li><span class="koboSpan" id="kobo.106.1">It also provides interpretability because we know which words are used </span><span class="No-Break"><span class="koboSpan" id="kobo.107.1">for alignment.</span></span></li>
<li><span class="koboSpan" id="kobo.108.1">It definitely improves the performance of </span><span class="No-Break"><span class="koboSpan" id="kobo.109.1">the model.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.110.1">Its success has given </span><a id="_idIndexMarker102"/><span class="koboSpan" id="kobo.111.1">rise</span><a id="_idIndexMarker103"/><span class="koboSpan" id="kobo.112.1"> to several variants where the scoring function is different. </span><span class="koboSpan" id="kobo.112.2">One variant in particular, called </span><strong class="bold"><span class="koboSpan" id="kobo.113.1">self-attention</span></strong><span class="koboSpan" id="kobo.114.1">, has the particular advantage that it extracts information directly from the input without necessarily needing to compare it with </span><span class="No-Break"><span class="koboSpan" id="kobo.115.1">something else.</span></span></p>
<p><span class="koboSpan" id="kobo.116.1">The insight behind self-attention is that if we want to look for a book for an essay on the French Revolution in a library (query), we don’t need to read all the books to find a book on the history of France (value), we just need to read the coasts of the books (key). </span><span class="koboSpan" id="kobo.116.2">Self-attention, in other words, is a method that allows us to search within context to find the representation </span><span class="No-Break"><span class="koboSpan" id="kobo.117.1">we need.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer044">
<span class="koboSpan" id="kobo.118.1"><img alt="Figure 2.4 – Self-attention mechanism. Matrix dimensions are included; numbers are arbitrary" src="image/B21257_02_04.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.119.1">Figure 2.4 – Self-attention mechanism. </span><span class="koboSpan" id="kobo.119.2">Matrix dimensions are included; numbers are arbitrary</span></p>
<p><span class="koboSpan" id="kobo.120.1">Transacting this for a model, given an input we want to conduct a series of comparisons between the various components of the sequence (such as tokens) to obtain an output sequence (which we</span><a id="_idIndexMarker104"/><span class="koboSpan" id="kobo.121.1"> can</span><a id="_idIndexMarker105"/><span class="koboSpan" id="kobo.122.1"> then use for various models or tasks). </span><span class="koboSpan" id="kobo.122.2">The self-attention equation is </span><span class="No-Break"><span class="koboSpan" id="kobo.123.1">as follows:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><math display="block"><mrow><mrow><mi>A</mi><mi>t</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mfenced close=")" open="("><mrow><mi>Q</mi><mo>,</mo><mi>K</mi><mo>,</mo><mi>V</mi></mrow></mfenced><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mfenced close=")" open="("><mfrac><mrow><mi>Q</mi><mo>∙</mo><msup><mi>k</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac></mfenced><mo>∙</mo><mi>V</mi></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.124.1">You can immediately see that it is derived from the original attention formula. </span><span class="koboSpan" id="kobo.124.2">We have the dot product to conduct comparisons, and we then exploit the </span><strong class="source-inline"><span class="koboSpan" id="kobo.125.1">softmax</span></strong><span class="koboSpan" id="kobo.126.1"> function to calculate the relative importance and normalize the values between 0 and 1. </span><em class="italic"><span class="koboSpan" id="kobo.127.1">D</span></em><span class="koboSpan" id="kobo.128.1"> is the size of the sequence; in other words, self-attention is also normalized as a function of the length of </span><span class="No-Break"><span class="koboSpan" id="kobo.129.1">our sequence.</span></span></p>
<p><span class="koboSpan" id="kobo.130.1">The next step is using </span><strong class="source-inline"><span class="koboSpan" id="kobo.131.1">softmax</span></strong><span class="koboSpan" id="kobo.132.1">. </span><span class="koboSpan" id="kobo.132.2">Here’s a little refresher on the function (how you calculate and how it is implemented more efficiently </span><span class="No-Break"><span class="koboSpan" id="kobo.133.1">in Python):</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><math display="block"><mrow><mrow><mi>y</mi><mo>=</mo><mfrac><msup><mi>e</mi><msub><mi>x</mi><mi>i</mi></msub></msup><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>i</mi><mo>=</mo><mi>n</mi></mrow></msubsup><msup><mi>e</mi><msub><mi>x</mi><mi>i</mi></msub></msup></mrow></mfrac></mrow></mrow></math></span></p>
<p><span class="_-----MathTools-_Math_Variable"><math display="block"><mrow><mrow><mi>y</mi><mo>=</mo><mfenced close="]" open="["><mtable columnalign="center" columnwidth="auto" rowalign="baseline baseline baseline" rowspacing="1.0000ex 1.0000ex"><mtr><mtd><msub><mi>y</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd><msub><mi>y</mi><mn>2</mn></msub></mtd></mtr><mtr><mtd><msub><mi>y</mi><mn>3</mn></msub></mtd></mtr></mtable></mfenced><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mfenced close=")" open="("><mi>x</mi></mfenced><mo>=</mo><mi>f</mi><mfenced close=")" open="("><mfenced close="]" open="["><mtable columnalign="center" columnwidth="auto" rowalign="baseline baseline baseline" rowspacing="1.0000ex 1.0000ex"><mtr><mtd><msub><mi>x</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd><msub><mi>x</mi><mn>2</mn></msub></mtd></mtr><mtr><mtd><msub><mi>x</mi><mn>3</mn></msub></mtd></mtr></mtable></mfenced></mfenced><mo>=</mo><mfenced close="]" open="["><mtable columnalign="center" columnwidth="auto" rowalign="baseline baseline baseline" rowspacing="1.0000ex 1.0000ex"><mtr><mtd><mfrac><msup><mi>e</mi><msub><mi>x</mi><mn>1</mn></msub></msup><mrow><msup><mi>e</mi><msub><mi>x</mi><mn>1</mn></msub></msup><mo>+</mo><msup><mi>e</mi><msub><mi>x</mi><mn>2</mn></msub></msup><mo>+</mo><msup><mi>e</mi><msub><mi>x</mi><mn>3</mn></msub></msup></mrow></mfrac></mtd></mtr><mtr><mtd><mfrac><msup><mi>e</mi><msub><mi>x</mi><mn>2</mn></msub></msup><mrow><msup><mi>e</mi><msub><mi>x</mi><mn>1</mn></msub></msup><mo>+</mo><msup><mi>e</mi><msub><mi>x</mi><mn>2</mn></msub></msup><mo>+</mo><msup><mi>e</mi><msub><mi>x</mi><mn>3</mn></msub></msup></mrow></mfrac></mtd></mtr><mtr><mtd><mfrac><msup><mi>e</mi><msub><mi>x</mi><mn>3</mn></msub></msup><mrow><msup><mi>e</mi><msub><mi>x</mi><mn>1</mn></msub></msup><mo>+</mo><msup><mi>e</mi><msub><mi>x</mi><mn>2</mn></msub></msup><mo>+</mo><msup><mi>e</mi><msub><mi>x</mi><mn>3</mn></msub></msup></mrow></mfrac></mtd></mtr></mtable></mfenced></mrow></mrow></math></span></p>
<p><span class="_-----MathTools-_Math_Space"><math display="block"><mrow><mrow><mi>p</mi><mi>y</mi><mi>t</mi><mi>h</mi><mi>o</mi><mi>n</mi><mo>:</mo><mi>y</mi><mo>=</mo><mfenced close="]" open="["><mtable columnalign="center" columnwidth="auto" rowalign="baseline baseline baseline" rowspacing="1.0000ex 1.0000ex"><mtr><mtd><msub><mi>y</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd><msub><mi>y</mi><mn>2</mn></msub></mtd></mtr><mtr><mtd><msub><mi>y</mi><mn>3</mn></msub></mtd></mtr></mtable></mfenced><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mfenced close=")" open="("><mi>x</mi></mfenced><mo>=</mo><mi>f</mi><mfenced close=")" open="("><mfenced close="]" open="["><mtable columnalign="center" columnwidth="auto" rowalign="baseline baseline baseline" rowspacing="1.0000ex 1.0000ex"><mtr><mtd><msub><mi>x</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd><msub><mi>x</mi><mn>2</mn></msub></mtd></mtr><mtr><mtd><msub><mi>x</mi><mn>3</mn></msub></mtd></mtr></mtable></mfenced></mfenced><mo>=</mo><mfrac><mfenced close="]" open="["><mtable columnalign="center center center" columnspacing="0.8000em 0.8000em" columnwidth="auto auto auto" rowalign="baseline"><mtr><mtd><msub><mi>x</mi><mn>1</mn></msub></mtd><mtd><msub><mi>x</mi><mn>2</mn></msub></mtd><mtd><msub><mi>x</mi><mn>3</mn></msub></mtd></mtr></mtable></mfenced><mrow><msup><mi>e</mi><msub><mi>x</mi><mn>1</mn></msub></msup><mo>+</mo><msup><mi>e</mi><msub><mi>x</mi><mn>2</mn></msub></msup><mo>+</mo><msup><mi>e</mi><msub><mi>x</mi><mn>3</mn></msub></msup></mrow></mfrac><mo>=</mo><mfrac><msup><mi>e</mi><mi>x</mi></msup><mrow><mi>s</mi><mi>u</mi><mi>m</mi><mo>(</mo><msup><mi>e</mi><mi>x</mi></msup><mo>)</mo></mrow></mfrac></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.134.1">As we saw in the previous chapter, the dot product can become quite wide as the length of the vectors increases. </span><span class="koboSpan" id="kobo.134.2">This can lead to inputs that are too large in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.135.1">softmax</span></strong><span class="koboSpan" id="kobo.136.1"> function (this shifts</span><a id="_idIndexMarker106"/><span class="koboSpan" id="kobo.137.1"> the</span><a id="_idIndexMarker107"/><span class="koboSpan" id="kobo.138.1"> probability mass in </span><strong class="source-inline"><span class="koboSpan" id="kobo.139.1">softmax</span></strong><span class="koboSpan" id="kobo.140.1"> to a few elements and thus leads to small gradients). </span><span class="koboSpan" id="kobo.140.2">In the original article, they solved this by normalizing by the square root </span><span class="No-Break"><span class="koboSpan" id="kobo.141.1">of </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.142.1">D</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.143.1">.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer045">
<span class="koboSpan" id="kobo.144.1"><img alt="Figure 2.5 – Self-attention unrolled" src="image/B21257_02_05.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.145.1">Figure 2.5 – Self-attention unrolled</span></p>
<p><span class="koboSpan" id="kobo.146.1">The real difference is that we use three matrices of weights </span><strong class="bold"><span class="koboSpan" id="kobo.147.1">Query</span></strong><span class="koboSpan" id="kobo.148.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.149.1">Q</span></strong><span class="koboSpan" id="kobo.150.1">), </span><strong class="bold"><span class="koboSpan" id="kobo.151.1">Key</span></strong><span class="koboSpan" id="kobo.152.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.153.1">K</span></strong><span class="koboSpan" id="kobo.154.1">), and </span><strong class="bold"><span class="koboSpan" id="kobo.155.1">Value</span></strong><span class="koboSpan" id="kobo.156.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.157.1">V</span></strong><span class="koboSpan" id="kobo.158.1">) that are initially randomly initialized. </span><em class="italic"><span class="koboSpan" id="kobo.159.1">Q</span></em><span class="koboSpan" id="kobo.160.1"> is the current focus of attention, while </span><em class="italic"><span class="koboSpan" id="kobo.161.1">K</span></em><span class="koboSpan" id="kobo.162.1"> informs the model about previous inputs, and </span><em class="italic"><span class="koboSpan" id="kobo.163.1">V</span></em><span class="koboSpan" id="kobo.164.1"> serves to extract the final input information. </span><span class="koboSpan" id="kobo.164.2">So, the first step is the multiplication of these three matrices with our input </span><em class="italic"><span class="koboSpan" id="kobo.165.1">X</span></em><span class="koboSpan" id="kobo.166.1"> (an array of vectors, of which each represents </span><span class="No-Break"><span class="koboSpan" id="kobo.167.1">a token).</span></span></p>
<p><span class="_-----MathTools-_Math_Variable_v-bold-italic"><math display="block"><mrow><mrow><mi mathvariant="bold-italic">Q</mi><mo>=</mo><mi mathvariant="bold-italic">X</mi><mo>∙</mo><msup><mi mathvariant="bold-italic">W</mi><mi>Q</mi></msup><mo>,</mo><mi mathvariant="bold-italic">K</mi><mo>=</mo><mi mathvariant="bold-italic">X</mi><mo>∙</mo><msup><mi mathvariant="bold-italic">W</mi><mi>V</mi></msup><mo>,</mo><mi mathvariant="bold-italic">V</mi><mo>=</mo><mi mathvariant="bold-italic">X</mi><mo>∙</mo><msup><mi mathvariant="bold-italic">W</mi><mi>V</mi></msup></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.168.1">The beauty of this system is that we can use it to extract more than one representation from the same input (after all, we can have multiple questions in a textbook). </span><span class="koboSpan" id="kobo.168.2">Therefore, since the operations are parallelizable, we can have multi-head attention. </span><strong class="bold"><span class="koboSpan" id="kobo.169.1">Multi-head self-attention</span></strong><span class="koboSpan" id="kobo.170.1"> enables </span><a id="_idIndexMarker108"/><span class="koboSpan" id="kobo.171.1">the model to simultaneously capture multiple types of relationships within the input sequence. </span><span class="koboSpan" id="kobo.171.2">This is crucial because a single word in a sentence can be contextually related to several other words. </span><span class="koboSpan" id="kobo.171.3">During training, the </span><em class="italic"><span class="koboSpan" id="kobo.172.1">K </span></em><span class="koboSpan" id="kobo.173.1">and </span><em class="italic"><span class="koboSpan" id="kobo.174.1">Q</span></em><span class="koboSpan" id="kobo.175.1"> matrices in each head specialize in modeling different kinds of relationships. </span><span class="koboSpan" id="kobo.175.2">Each attention head produces an output based on its specific perspective, resulting in </span><em class="italic"><span class="koboSpan" id="kobo.176.1">n</span></em><span class="koboSpan" id="kobo.177.1"> outputs for </span><em class="italic"><span class="koboSpan" id="kobo.178.1">n</span></em><span class="koboSpan" id="kobo.179.1"> heads. </span><span class="koboSpan" id="kobo.179.2">These outputs are then concatenated and passed through a final linear projection layer to restore the dimensionality backt</span><a id="_idIndexMarker109"/><span class="koboSpan" id="kobo.180.1"> to</span><a id="_idIndexMarker110"/><span class="koboSpan" id="kobo.181.1"> the original </span><span class="No-Break"><span class="koboSpan" id="kobo.182.1">input size.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer046">
<span class="koboSpan" id="kobo.183.1"><img alt="Figure 2.6 – Multi-head self-attention" src="image/B21257_02_06.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.184.1">Figure 2.6 – Multi-head self-attention</span></p>
<p><span class="koboSpan" id="kobo.185.1">Self-attention has </span><span class="No-Break"><span class="koboSpan" id="kobo.186.1">several advantages:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.187.1">We can extract </span><a id="_idIndexMarker111"/><span class="koboSpan" id="kobo.188.1">different representations for </span><span class="No-Break"><span class="koboSpan" id="kobo.189.1">each input.</span></span></li>
<li><span class="koboSpan" id="kobo.190.1">We can conduct all these computations in parallel and thus with a GPU. </span><span class="koboSpan" id="kobo.190.2">Each head can be </span><span class="No-Break"><span class="koboSpan" id="kobo.191.1">computed independently.</span></span></li>
<li><span class="koboSpan" id="kobo.192.1">We can use it in models that do not necessarily consist of an encoder and </span><span class="No-Break"><span class="koboSpan" id="kobo.193.1">a decoder.</span></span></li>
<li><span class="koboSpan" id="kobo.194.1">We do not have to wait for different time steps to see the relationship between distant word pairs (as </span><span class="No-Break"><span class="koboSpan" id="kobo.195.1">in RNN).</span></span></li>
<li><span class="koboSpan" id="kobo.196.1">However, it has a quadratic cost in function of the number of tokens </span><em class="italic"><span class="koboSpan" id="kobo.197.1">N</span></em><span class="koboSpan" id="kobo.198.1">, and it has no inherent </span><a id="_idIndexMarker112"/><span class="koboSpan" id="kobo.199.1">notion </span><span class="No-Break"><span class="koboSpan" id="kobo.200.1">of order.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.201.1">Self-attention is computationally expensive. </span><span class="koboSpan" id="kobo.201.2">It can be shown that, considering a sequence </span><em class="italic"><span class="koboSpan" id="kobo.202.1">T</span></em><span class="koboSpan" id="kobo.203.1"> and sequence length </span><em class="italic"><span class="koboSpan" id="kobo.204.1">d</span></em><span class="koboSpan" id="kobo.205.1">, the computation cost and space </span><span class="No-Break"><span class="koboSpan" id="kobo.206.1">is quadratic:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><math display="block"><mrow><mrow><mrow><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mo>=</mo><mi mathvariant="script">O</mi><mfenced close=")" open="("><mrow><msup><mi>T</mi><mn>2</mn></msup><mo>+</mo><mi>d</mi></mrow></mfenced><mi>s</mi><mi>p</mi><mi>a</mi><mi>c</mi><mi>e</mi><mo>=</mo><mi mathvariant="script">O</mi><mo>(</mo><msup><mi>T</mi><mn>2</mn></msup><mo>+</mo><mi>T</mi><mi>d</mi><mo>)</mo></mrow></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.207.1">They identified the dot product as the culprit. </span><span class="koboSpan" id="kobo.207.2">This computational cost is one of the problems of scalability (taking into account that multi-head attention is calculated in each block). </span><span class="koboSpan" id="kobo.207.3">For this reason, many variations of self-attention have been proposed to reduce the </span><span class="No-Break"><span class="koboSpan" id="kobo.208.1">computational cost.</span></span></p>
<p><span class="koboSpan" id="kobo.209.1">Despite the computational cost, self-attention has shown its capability, especially when several layers are</span><a id="_idIndexMarker113"/><span class="koboSpan" id="kobo.210.1"> stacked</span><a id="_idIndexMarker114"/><span class="koboSpan" id="kobo.211.1"> on top of each other. </span><span class="koboSpan" id="kobo.211.2">In the next section, we will discuss how this makes the model extremely powerful despite its </span><span class="No-Break"><span class="koboSpan" id="kobo.212.1">computational cost.</span></span></p>
<h1 id="_idParaDest-37"><a id="_idTextAnchor036"/><span class="koboSpan" id="kobo.213.1">Introducing the transformer model</span></h1>
<p><span class="koboSpan" id="kobo.214.1">Despite this decisive advance though, several problems remain in</span><a id="_idIndexMarker115"/> <span class="No-Break"><span class="koboSpan" id="kobo.215.1">machine translation:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.216.1">The model fails to capture the meaning of the sentence and is </span><span class="No-Break"><span class="koboSpan" id="kobo.217.1">still error-prone</span></span></li>
<li><span class="koboSpan" id="kobo.218.1">In addition, we have problems with words that are not in the </span><span class="No-Break"><span class="koboSpan" id="kobo.219.1">initial vocabulary</span></span></li>
<li><span class="koboSpan" id="kobo.220.1">Errors in pronouns and other </span><span class="No-Break"><span class="koboSpan" id="kobo.221.1">grammatical forms</span></span></li>
<li><span class="koboSpan" id="kobo.222.1">The model fails to maintain context for </span><span class="No-Break"><span class="koboSpan" id="kobo.223.1">long texts</span></span></li>
<li><span class="koboSpan" id="kobo.224.1">It is not adaptable if the domain in the training set and test data is different (for example, if it is trained on literary texts and the test set is </span><span class="No-Break"><span class="koboSpan" id="kobo.225.1">finance texts)</span></span></li>
<li><span class="koboSpan" id="kobo.226.1">RNNs are not parallelizable, and you have to </span><span class="No-Break"><span class="koboSpan" id="kobo.227.1">compute sequentially</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.228.1">Considering these points, Google researchers in 2016 came up with the idea of eliminating RNNs altogether rather than improving them. </span><span class="koboSpan" id="kobo.228.2">According to the authors of the </span><em class="italic"><span class="koboSpan" id="kobo.229.1">Attention is All You Need</span></em><span class="koboSpan" id="kobo.230.1"> seminal article; all you need is a model that is based on multi-head self-attention. </span><span class="koboSpan" id="kobo.230.2">Before going into detail, the transformer consists entirely of stacked layers of multi-head self-attention. </span><span class="koboSpan" id="kobo.230.3">In this way, the model learns a hierarchical and increasingly sophisticated representation of </span><span class="No-Break"><span class="koboSpan" id="kobo.231.1">the text.</span></span></p>
<p><span class="koboSpan" id="kobo.232.1">The first step in the process is the transformation of text into numerical vectors (tokenization). </span><span class="koboSpan" id="kobo.232.2">After that, we have an embedding step to obtain vectors for each token. </span><span class="koboSpan" id="kobo.232.3">A special feature of the transformer is the introduction of a function to record the position of each token in the sequence (self-attention is not position-aware). </span><span class="koboSpan" id="kobo.232.4">This process is </span><a id="_idIndexMarker116"/><span class="koboSpan" id="kobo.233.1">called </span><strong class="bold"><span class="koboSpan" id="kobo.234.1">positional encoding</span></strong><span class="koboSpan" id="kobo.235.1">. </span><span class="koboSpan" id="kobo.235.2">The authors in the article use sin and cos alternately </span><a id="_idIndexMarker117"/><span class="koboSpan" id="kobo.236.1">with position. </span><span class="koboSpan" id="kobo.236.2">This allows the model to know the relative position of </span><span class="No-Break"><span class="koboSpan" id="kobo.237.1">each token.</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><mml:math display="block"><mml:msub><mml:mrow><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mo>/</mml:mo><mml:msup><mml:mrow><mml:mn>1000</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>i</mml:mi><mml:mo>/</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:math></span></p>
<p><span class="_-----MathTools-_Math_Variable"><mml:math display="block"><mml:msub><mml:mrow><mml:mi>P</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mo>/</mml:mo><mml:msup><mml:mrow><mml:mn>1000</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>i</mml:mi><mml:mo>/</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:math></span></p>
<p><span class="koboSpan" id="kobo.238.1">In the first step, the embedding vectors are summed with the result of these functions. </span><span class="koboSpan" id="kobo.238.2">This is because self-attention is not aware of word order, but word order in a period is important. </span><span class="koboSpan" id="kobo.238.3">Thus, the order is directly encoded in the vectors it awaits. </span><span class="koboSpan" id="kobo.238.4">Note, though, that there are no learnable parameters in this function and that for long sequences, it will have to be modified (we will discuss this in the </span><span class="No-Break"><span class="koboSpan" id="kobo.239.1">next chapter).</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer047">
<span class="koboSpan" id="kobo.240.1"><img alt="Figure 2.7 – Positional encoding" src="image/B21257_02_07.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.241.1">Figure 2.7 – Positional encoding</span></p>
<p><span class="koboSpan" id="kobo.242.1">After that, we have a</span><a id="_idIndexMarker118"/><span class="koboSpan" id="kobo.243.1"> series of transformer blocks in sequence. </span><span class="koboSpan" id="kobo.243.2">The </span><strong class="bold"><span class="koboSpan" id="kobo.244.1">transformer block</span></strong><span class="koboSpan" id="kobo.245.1"> consists </span><a id="_idIndexMarker119"/><span class="koboSpan" id="kobo.246.1">of four elements: multi-head self-attention, feedforward layer, residual connections, and </span><span class="No-Break"><span class="koboSpan" id="kobo.247.1">layer normalization.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer048">
<span class="koboSpan" id="kobo.248.1"><img alt="Figure 2.8 – Flow diagram of the transformer block" src="image/B21257_02_08.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.249.1">Figure 2.8 – Flow diagram of the transformer block</span></p>
<p><span class="koboSpan" id="kobo.250.1">The feedforward layer consists of two linear layers. </span><span class="koboSpan" id="kobo.250.2">This layer is used to obtain a linear projection of the multi-head self-attention. </span><span class="koboSpan" id="kobo.250.3">The weights are identifiable for each position and are </span><a id="_idIndexMarker120"/><span class="koboSpan" id="kobo.251.1">separated. </span><span class="koboSpan" id="kobo.251.2">It can be seen as two linear transformations with one ReLU activation </span><span class="No-Break"><span class="koboSpan" id="kobo.252.1">in between.</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><mml:math display="block"><mml:mi>F</mml:mi><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mo>⁡</mml:mo><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></span></p>
<p><span class="koboSpan" id="kobo.253.1">This adds a step of non-linearity to self-attention. </span><span class="koboSpan" id="kobo.253.2">The FFN layer is chosen because it is an easily </span><span class="No-Break"><span class="koboSpan" id="kobo.254.1">parallelized operation.</span></span></p>
<p><span class="koboSpan" id="kobo.255.1">Residual connections are connections that pass information between two layers without going through the intermediate layer transformation. </span><span class="koboSpan" id="kobo.255.2">Initially developed in convolutional networks, they allow a shortcut between layers and help the gradient pass down to the lower layers. </span><span class="koboSpan" id="kobo.255.3">In the transformer, blocks are present for both the attention layer and feedforward, where the input is summed with the output. </span><span class="koboSpan" id="kobo.255.4">Residual connections also have the advantage of making the loss surface smoother (this helps the model find a better minimum and not get stuck in a local loss). </span><span class="koboSpan" id="kobo.255.5">This powerful effect can be seen clearly in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.256.1">Figure 2</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.257.1">.9</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.258.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer049">
<span class="koboSpan" id="kobo.259.1"><img alt="Figure 2.9 – Effect of the residual connections on the loss" src="image/B21257_02_09.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.260.1">Figure 2.9 – Effect of the residual connections on the loss</span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.261.1">Note</span></p>
<p class="callout"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.262.1">Figure 2</span></em></span><em class="italic"><span class="koboSpan" id="kobo.263.1">.9</span></em><span class="koboSpan" id="kobo.264.1"> is originally from </span><em class="italic"><span class="koboSpan" id="kobo.265.1">Visualizing the Loss Landscape of Neural Nets</span></em><span class="koboSpan" id="kobo.266.1"> by Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom </span><span class="No-Break"><span class="koboSpan" id="kobo.267.1">Goldstein (</span></span><a href="https://github.com/tomgoldstein/loss-landscape/tree/master"><span class="No-Break"><span class="koboSpan" id="kobo.268.1">https://github.com/tomgoldstein/loss-landscape/tree/master</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.269.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.270.1">The residual connection </span><a id="_idIndexMarker121"/><span class="koboSpan" id="kobo.271.1">makes the loss surface smoother, which allows the model to be trained more efficiently </span><span class="No-Break"><span class="koboSpan" id="kobo.272.1">and quickly.</span></span></p>
<p><span class="koboSpan" id="kobo.273.1">Layer normalization</span><a id="_idIndexMarker122"/><span class="koboSpan" id="kobo.274.1"> is a form of normalization that helps training because it keeps the hidden layer values in a certain range (it is an alternative to batch normalization). </span><span class="koboSpan" id="kobo.274.2">Having taken a single vector, it is normalized in a process that takes advantage of the mean and standard deviation. </span><span class="koboSpan" id="kobo.274.3">Having calculated the mean and standard deviation, the vector </span><span class="No-Break"><span class="koboSpan" id="kobo.275.1">is scaled:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><math display="block"><mrow><mrow><mi>μ</mi><mo>=</mo><mfrac><mn>1</mn><mi>d</mi></mfrac><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><msub><mi>x</mi><mi>i</mi></msub></mrow><mi>σ</mi><mo>=</mo><msqrt><mrow><mfrac><mn>1</mn><mi>d</mi></mfrac><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><msup><mrow><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><mi>μ</mi><mo>)</mo></mrow><mn>2</mn></msup></mrow></mrow></msqrt></mrow></mrow></math></span></p>
<p><span class="_-----MathTools-_Math_Base"><math display="block"><mrow><mrow><mover><mi>x</mi><mo stretchy="true">ˆ</mo></mover><mo>=</mo><mfrac><mrow><mo>(</mo><mi>x</mi><mo>−</mo><mi>μ</mi><mo>)</mo></mrow><mi>σ</mi></mfrac></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.276.1">In the final transformation, we exploit two parameters that are learned </span><span class="No-Break"><span class="koboSpan" id="kobo.277.1">during training.</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><math display="block"><mrow><mrow><mi>L</mi><mi>a</mi><mi>y</mi><mi>e</mi><mi>r</mi><mi>N</mi><mi>o</mi><mi>r</mi><mi>m</mi><mi>a</mi><mi>l</mi><mi>i</mi><mi>z</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>=</mo><mi>γ</mi><mover><mi>x</mi><mo stretchy="true">ˆ</mo></mover><mo>+</mo><mi>β</mi></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.278.1">There is a lot of variability during the training, and this can hurt the learning of the training. </span><span class="koboSpan" id="kobo.278.2">To reduce uninformative variability, we add this normalization step, thus normalizing the gradient </span><span class="No-Break"><span class="koboSpan" id="kobo.279.1">as well.</span></span></p>
<p><span class="koboSpan" id="kobo.280.1">At this point, we can assemble everything into a single block. </span><span class="koboSpan" id="kobo.280.2">Consider that after embedding, we have as input </span><em class="italic"><span class="koboSpan" id="kobo.281.1">X</span></em><span class="koboSpan" id="kobo.282.1"> a matrix of dimension </span><em class="italic"><span class="koboSpan" id="kobo.283.1">n x d</span></em><span class="koboSpan" id="kobo.284.1"> (with </span><em class="italic"><span class="koboSpan" id="kobo.285.1">n</span></em><span class="koboSpan" id="kobo.286.1"> being the number of tokens, and </span><em class="italic"><span class="koboSpan" id="kobo.287.1">d</span></em><span class="koboSpan" id="kobo.288.1"> the dimensions of the embedding). </span><span class="koboSpan" id="kobo.288.2">This input </span><em class="italic"><span class="koboSpan" id="kobo.289.1">X</span></em><span class="koboSpan" id="kobo.290.1"> goes into a transformer block and comes out with the same dimensions. </span><span class="koboSpan" id="kobo.290.2">This process is repeated for all </span><span class="No-Break"><span class="koboSpan" id="kobo.291.1">transformer blocks:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable_v-bold-italic"><mml:math display="block"><mml:mi mathvariant="bold-italic">H</mml:mi><mml:mo>=</mml:mo><mml:mi>L</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>N</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>+</mml:mo><mml:mi>M</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>H</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>S</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>f</mml:mi><mml:mi>A</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math></span></p>
<p><span class="_-----MathTools-_Math_Variable_v-bold-italic"><mml:math display="block"><mml:mi mathvariant="bold-italic">H</mml:mi><mml:mo>=</mml:mo><mml:mi>L</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>N</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">H</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">H</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math></span></p>
<p><span class="koboSpan" id="kobo.292.1">Some notes on this process are </span><span class="No-Break"><span class="koboSpan" id="kobo.293.1">as follows:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.294.1">In some architectures, </span><em class="italic"><span class="koboSpan" id="kobo.295.1">LayerNorm</span></em><span class="koboSpan" id="kobo.296.1"> can be after the </span><em class="italic"><span class="koboSpan" id="kobo.297.1">FFN</span></em><span class="koboSpan" id="kobo.298.1"> block instead of before (whether it is better or not is </span><span class="No-Break"><span class="koboSpan" id="kobo.299.1">still debated).</span></span></li>
<li><span class="koboSpan" id="kobo.300.1">Modern models have up to 96 transformer blocks in series, but the structure is virtually identical. </span><span class="koboSpan" id="kobo.300.2">The idea is that the model learns an increasingly complex representation of </span><span class="No-Break"><span class="koboSpan" id="kobo.301.1">the language.</span></span></li>
<li><span class="koboSpan" id="kobo.302.1">Starting with the embedding of an input, self-attention allows this representation to be enriched by incorporating an increasingly complex context. </span><span class="koboSpan" id="kobo.302.2">In addition, the model also has information about the location of </span><span class="No-Break"><span class="koboSpan" id="kobo.303.1">each token.</span></span></li>
<li><span class="koboSpan" id="kobo.304.1">Absolute positional encoding has the defect of overrepresenting words at the beginning of the sequence. </span><span class="koboSpan" id="kobo.304.2">Today, there are variants that consider the </span><span class="No-Break"><span class="koboSpan" id="kobo.305.1">relative position.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.306.1">Once we have “the bricks,” we </span><a id="_idIndexMarker123"/><span class="koboSpan" id="kobo.307.1">can assemble them into a functional structure. </span><span class="koboSpan" id="kobo.307.2">In the original description, the model was structured for machine translation and composed of two parts: an encoder (which takes the text to be translated) and a decoder (which will produce </span><span class="No-Break"><span class="koboSpan" id="kobo.308.1">the translation).</span></span></p>
<p><span class="koboSpan" id="kobo.309.1">The original transformer is composed of different blocks of transformer blocks and structures in an encoder and decoder, as you can see in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.310.1">Figure 2</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.311.1">.10</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.312.1">.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer050">
<span class="koboSpan" id="kobo.313.1"><img alt="Figure 2.10 – Encoder-decoder structure" src="image/B21257_02_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.314.1">Figure 2.10 – Encoder-decoder structure</span></p>
<p><span class="koboSpan" id="kobo.315.1">The decoder, like the encoder, is composed of an embedding, a positional encoder, and a series of transformer blocks. </span><span class="koboSpan" id="kobo.315.2">One note is that in the decoder, instead of self-attention, we have </span><strong class="bold"><span class="koboSpan" id="kobo.316.1">cross-attention</span></strong><span class="koboSpan" id="kobo.317.1">. </span><span class="koboSpan" id="kobo.317.2">Cross-attention</span><a id="_idIndexMarker124"/><span class="koboSpan" id="kobo.318.1"> is exactly the same, only we take both elements from the encoder and the decoder (because we want to condition the generation of the</span><a id="_idIndexMarker125"/><span class="koboSpan" id="kobo.319.1"> decoder based on the encoder input). </span><span class="koboSpan" id="kobo.319.2">In this case, the queries come from the encoder and the rest from the decoder. </span><span class="koboSpan" id="kobo.319.3">As you can see from </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.320.1">Figure 2</span></em></span><em class="italic"><span class="koboSpan" id="kobo.321.1">.11</span></em><span class="koboSpan" id="kobo.322.1">, the decoder sequence can be of different sizes, but the result is </span><span class="No-Break"><span class="koboSpan" id="kobo.323.1">the same:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer051">
<span class="koboSpan" id="kobo.324.1"><img alt="Figure 2.11 – Cross-attention" src="image/B21257_02_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.325.1">Figure 2.11 – Cross-attention</span></p>
<p><span class="koboSpan" id="kobo.326.1">Input </span><em class="italic"><span class="koboSpan" id="kobo.327.1">N</span></em><span class="koboSpan" id="kobo.328.1"> comes from the encoder, while input </span><em class="italic"><span class="koboSpan" id="kobo.329.1">M</span></em><span class="koboSpan" id="kobo.330.1"> is from the decoder. </span><span class="koboSpan" id="kobo.330.2">In the figure, cross-attention is mixing information from the encoder and decoder, allowing the decoder to learn from </span><span class="No-Break"><span class="koboSpan" id="kobo.331.1">the encoder.</span></span></p>
<p><span class="koboSpan" id="kobo.332.1">Another note on the structure: in the decoder, the first self-attention has an additional mask to prevent the model from seeing </span><span class="No-Break"><span class="koboSpan" id="kobo.333.1">the future.</span></span></p>
<p><span class="koboSpan" id="kobo.334.1">This is especially true in the case of QT. </span><span class="koboSpan" id="kobo.334.2">In fact, if one wants to predict the next word and the model already knows what it is, we have data leakage. </span><span class="koboSpan" id="kobo.334.3">To compensate for this, we add a mask in which the upper-triangular portion is replaced with negative infinity: - ∞.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer052">
<span class="koboSpan" id="kobo.335.1"><img alt="Figure 2.12 – Masked attention" src="image/B21257_02_12.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.336.1">Figure 2.12 – Masked attention</span></p>
<p><span class="koboSpan" id="kobo.337.1">The first transformer </span><a id="_idIndexMarker126"/><span class="koboSpan" id="kobo.338.1">consisted of an encoder and decoder, but today there are also models that are either encoder-only or decoder-only. </span><span class="koboSpan" id="kobo.338.2">Today, for generative AI, they are practically all decoder-only. </span><span class="koboSpan" id="kobo.338.3">We have our model; now, how can you train a system that seems so complex? </span><span class="koboSpan" id="kobo.338.4">In the next section, we will see how to succeed </span><span class="No-Break"><span class="koboSpan" id="kobo.339.1">at training.</span></span></p>
<h1 id="_idParaDest-38"><a id="_idTextAnchor037"/><span class="koboSpan" id="kobo.340.1">Training a transformer</span></h1>
<p><span class="koboSpan" id="kobo.341.1">How do you train</span><a id="_idIndexMarker127"/><span class="koboSpan" id="kobo.342.1"> such a complex model? </span><span class="koboSpan" id="kobo.342.2">The answer to this question is simpler than you might think. </span><span class="koboSpan" id="kobo.342.3">The fact that the model can learn through multi-head self-attention complex and diverse relationships allows the model to be able to be flexible and able to learn complex patterns. </span><span class="koboSpan" id="kobo.342.4">It would be too expensive to build examples (or find them) to teach these complex relationships to the model. </span><span class="koboSpan" id="kobo.342.5">So, we want a system that allows the model to learn these relationships on its own. </span><span class="koboSpan" id="kobo.342.6">The advantage is that if we have a large amount of text available, the model can learn without the need for us to curate the training corpus. </span><span class="koboSpan" id="kobo.342.7">Thanks to the advent of the internet, we have the availability of huge corpora that allow models to see text examples of different topics, languages, styles, </span><span class="No-Break"><span class="koboSpan" id="kobo.343.1">and more.</span></span></p>
<p><span class="koboSpan" id="kobo.344.1">Although the original model was a </span><strong class="source-inline"><span class="koboSpan" id="kobo.345.1">seq2seq</span></strong><span class="koboSpan" id="kobo.346.1"> model, later transformers (such as LLMs) were trained as language models, especially </span><a id="_idIndexMarker128"/><span class="koboSpan" id="kobo.347.1">in a </span><strong class="bold"><span class="koboSpan" id="kobo.348.1">self-supervised manner</span></strong><span class="koboSpan" id="kobo.349.1">. </span><span class="koboSpan" id="kobo.349.2">In language modeling, we consider a sequence of word </span><em class="italic"><span class="koboSpan" id="kobo.350.1">s</span></em><span class="koboSpan" id="kobo.351.1">, and the probability of the next word in the sequence </span><em class="italic"><span class="koboSpan" id="kobo.352.1">x</span></em><span class="koboSpan" id="kobo.353.1"> is </span><span class="_-----MathTools-_Math_Variable"><mml:math><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>w</mml:mi><mml:mo>|</mml:mo><mml:mi>h</mml:mi><mml:mo>)</mml:mo></mml:math></span><span class="koboSpan" id="kobo.354.1">. </span><span class="koboSpan" id="kobo.354.2">This probability depends on the words up to that point. </span><span class="koboSpan" id="kobo.354.3">By the chain rule of the probability, we can decompose </span><span class="No-Break"><span class="koboSpan" id="kobo.355.1">this probability:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><mml:math display="block"><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo stretchy="false">∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></span></p>
<p><span class="koboSpan" id="kobo.356.1">This allows us to calculate the conditional probability of a word from a sequence of previous words. </span><span class="koboSpan" id="kobo.356.2">The idea is that when we have enough text we can take a sequence such as </span><strong class="bold"><span class="koboSpan" id="kobo.357.1">“to be or not to”</span></strong><span class="koboSpan" id="kobo.358.1"> as input and have the model estimate the probability for the next word to be </span><strong class="bold"><span class="koboSpan" id="kobo.359.1">“be,”</span></strong> <span class="_-----MathTools-_Math_Variable"><math><mrow><mrow><mrow><mi>P</mi><mo>(</mo><mi>b</mi><mi>e</mi><mo>|</mo><mi>t</mi><mi>o</mi><mi>b</mi><mi>e</mi><mi>o</mi><mi>r</mi><mi>n</mi><mi>o</mi><mi>t</mi><mi>t</mi><mi>o</mi><mo>)</mo></mrow></mrow></mrow></math></span><span class="koboSpan" id="kobo.360.1">. </span><span class="koboSpan" id="kobo.360.2">Then after the transformer block sequence, we have a layer that conducts a linear projection and</span><a id="_idIndexMarker129"/><span class="koboSpan" id="kobo.361.1"> a </span><strong class="bold"><span class="koboSpan" id="kobo.362.1">softmax layer</span></strong><span class="koboSpan" id="kobo.363.1"> that generates the output. </span><span class="koboSpan" id="kobo.363.2">The previous sequence is called context; the context length of the first transformers</span><a id="_idIndexMarker130"/><span class="koboSpan" id="kobo.364.1"> was 512 tokens. </span><span class="koboSpan" id="kobo.364.2">The model generates an output, which is a probability vector of dimension </span><em class="italic"><span class="koboSpan" id="kobo.365.1">V</span></em><span class="koboSpan" id="kobo.366.1"> (the model vocabulary), also </span><a id="_idIndexMarker131"/><span class="koboSpan" id="kobo.367.1">called a </span><strong class="bold"><span class="koboSpan" id="kobo.368.1">logit vector</span></strong><span class="koboSpan" id="kobo.369.1">. </span><span class="koboSpan" id="kobo.369.2">The projection layer is called an </span><strong class="bold"><span class="koboSpan" id="kobo.370.1">unembedder</span></strong><span class="koboSpan" id="kobo.371.1"> (it does </span><a id="_idIndexMarker132"/><span class="koboSpan" id="kobo.372.1">reverse mapping) because we have to go from a dimension </span><em class="italic"><span class="koboSpan" id="kobo.373.1">N</span></em><span class="koboSpan" id="kobo.374.1"> tokens x </span><em class="italic"><span class="koboSpan" id="kobo.375.1">D</span></em><span class="koboSpan" id="kobo.376.1"> embedding to 1 x </span><em class="italic"><span class="koboSpan" id="kobo.377.1">V</span></em><span class="koboSpan" id="kobo.378.1">. </span><span class="koboSpan" id="kobo.378.2">Since the input and output of each transformer block are the same, we could theoretically eliminate blocks and attach an unembedder and softmax to any intermediate block. </span><span class="koboSpan" id="kobo.378.3">This allows us to better interpret the function of each block and its </span><span class="No-Break"><span class="koboSpan" id="kobo.379.1">internal representation.</span></span></p>
<p><span class="koboSpan" id="kobo.380.1">Once we have this probability vector, we can use self-supervision for training. </span><span class="koboSpan" id="kobo.380.2">We take a corpus of text (unannotated) and train the model to minimize the difference between the probability of the true word in the sequence and the predicted probability. </span><span class="koboSpan" id="kobo.380.3">To do this, we use </span><strong class="bold"><span class="koboSpan" id="kobo.381.1">cross-entropy loss</span></strong><span class="koboSpan" id="kobo.382.1"> (the</span><a id="_idIndexMarker133"/><span class="koboSpan" id="kobo.383.1"> difference between the predicted and true probability distribution). </span><span class="koboSpan" id="kobo.383.2">The predicted probability distribution is the logit vector, while the true one is a one-hot encoder vector where it is 1 for the next word in the sequence and </span><span class="No-Break"><span class="koboSpan" id="kobo.384.1">0 elsewhere.</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><mml:math display="block"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mrow><mml:munder><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>w</mml:mi><mml:mo>∈</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:mfenced><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>[</mml:mo><mml:mi>w</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></span></p>
<p><span class="koboSpan" id="kobo.385.1">In practice, it is simplified during training simply between the probability of the actual predicted word and 1. </span><span class="koboSpan" id="kobo.385.2">The process is iterative for each word in the word sequence (and is called </span><a id="_idIndexMarker134"/><span class="koboSpan" id="kobo.386.1">teacher forcing). </span><span class="koboSpan" id="kobo.386.2">The final loss is the average over the </span><span class="No-Break"><span class="koboSpan" id="kobo.387.1">entire sequence.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer053">
<span class="koboSpan" id="kobo.388.1"><img alt="Figure 2.13 – Training of the transformer; the loss is the average of the loss of all the time steps" src="image/B21257_02_13.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.389.1">Figure 2.13 – Training of the transformer; the loss is the average of the loss of all the time steps</span></p>
<p><span class="koboSpan" id="kobo.390.1">Since all calculations </span><a id="_idIndexMarker135"/><span class="koboSpan" id="kobo.391.1">can be done in parallel in the transformer, we do not have to calculate word by word, but we fed the model with the </span><span class="No-Break"><span class="koboSpan" id="kobo.392.1">whole sequence.</span></span></p>
<p><span class="koboSpan" id="kobo.393.1">Once we have obtained a probability vector, we can choose the probability most (</span><strong class="bold"><span class="koboSpan" id="kobo.394.1">greedy decoding</span></strong><span class="koboSpan" id="kobo.395.1">). </span><span class="koboSpan" id="kobo.395.2">Greedy</span><a id="_idIndexMarker136"/><span class="koboSpan" id="kobo.396.1"> decoding is formally defined as choosing the token with the highest probability at each </span><span class="No-Break"><span class="koboSpan" id="kobo.397.1">time step:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><mml:math display="block"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mo>∈</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>w</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></span></p>
<p><span class="koboSpan" id="kobo.398.1">In fact, it is no longer used because the result is too predictable, generic, and repetitive. </span><span class="koboSpan" id="kobo.398.2">So, more sophisticated and less deterministic sampling methods are used. </span><span class="koboSpan" id="kobo.398.3">This sampling process is called decoding (or autoregressive generation or causal language modeling, since it is derived from previous word choice). </span><span class="koboSpan" id="kobo.398.4">This system, in the simplest version, is based either on generating the text of at most a predetermined sequence length, or as long as an end-of-sentence token (</span><strong class="source-inline"><span class="koboSpan" id="kobo.399.1">&lt;EOS&gt;</span></strong><span class="koboSpan" id="kobo.400.1">) </span><span class="No-Break"><span class="koboSpan" id="kobo.401.1">is selected.</span></span></p>
<p><span class="koboSpan" id="kobo.402.1">We need to find a way to be able to select tokens while balancing both quality and diversity. </span><span class="koboSpan" id="kobo.402.2">A model that always chooses the same words will certainly have higher quality but will also be repetitive. </span><span class="koboSpan" id="kobo.402.3">There are different methods of doing </span><span class="No-Break"><span class="koboSpan" id="kobo.403.1">the sampling:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.404.1">Random sampling</span></strong><span class="koboSpan" id="kobo.405.1">: The </span><a id="_idIndexMarker137"/><span class="koboSpan" id="kobo.406.1">model chooses the next token randomly. </span><span class="koboSpan" id="kobo.406.2">The sentences</span><a id="_idIndexMarker138"/><span class="koboSpan" id="kobo.407.1"> are strange because the model chooses rare or </span><span class="No-Break"><span class="koboSpan" id="kobo.408.1">singular words.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.409.1">Top-k sampling</span></strong><span class="koboSpan" id="kobo.410.1">: At each step, we sort the probabilities and choose the top </span><em class="italic"><span class="koboSpan" id="kobo.411.1">k</span></em><span class="koboSpan" id="kobo.412.1"> most likely</span><a id="_idIndexMarker139"/><span class="koboSpan" id="kobo.413.1"> words. </span><span class="koboSpan" id="kobo.413.2">We renormalize the probability and choose one </span><span class="No-Break"><span class="koboSpan" id="kobo.414.1">at random.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.415.1">Top-p sampling</span></strong><span class="koboSpan" id="kobo.416.1">: This is an alternative in which we keep only a percentage of the most </span><span class="No-Break"><span class="koboSpan" id="kobo.417.1">likely</span></span><span class="No-Break"><a id="_idIndexMarker140"/></span><span class="No-Break"><span class="koboSpan" id="kobo.418.1"> words.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.419.1">Temperature sampling</span></strong><span class="koboSpan" id="kobo.420.1">: Before </span><strong class="source-inline"><span class="koboSpan" id="kobo.421.1">softmax</span></strong><span class="koboSpan" id="kobo.422.1">, we </span><a id="_idIndexMarker141"/><span class="koboSpan" id="kobo.423.1">divide by a temperature parameter (between 0 and 1). </span><span class="koboSpan" id="kobo.423.2">The closer </span><em class="italic"><span class="koboSpan" id="kobo.424.1">t</span></em><span class="koboSpan" id="kobo.425.1"> is to 0, the closer the probability of the most likely words is to 1 (close to greedy sampling). </span><span class="koboSpan" id="kobo.425.2">In some cases, we can also have </span><em class="italic"><span class="koboSpan" id="kobo.426.1">t</span></em><span class="koboSpan" id="kobo.427.1"> greater than 1 when we want a less </span><span class="No-Break"><span class="koboSpan" id="kobo.428.1">greedy approach.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.429.1">So far, we have </span><a id="_idIndexMarker142"/><span class="koboSpan" id="kobo.430.1">considered the fixed vocabulary and assumed that each token was a word. </span><span class="koboSpan" id="kobo.430.2">In general, once the model is trained, there might be some words that the model does not know to which a special token, </span><strong class="source-inline"><span class="koboSpan" id="kobo.431.1">&lt;UNK&gt;</span></strong><span class="koboSpan" id="kobo.432.1">, is assigned. </span><span class="koboSpan" id="kobo.432.2">In transformers and LLMs afterward, a way was sought to solve the unknown word problem. </span><span class="koboSpan" id="kobo.432.3">For example, in the training set, we might have words such as </span><em class="italic"><span class="koboSpan" id="kobo.433.1">big</span></em><span class="koboSpan" id="kobo.434.1">, </span><em class="italic"><span class="koboSpan" id="kobo.435.1">bigger</span></em><span class="koboSpan" id="kobo.436.1">, and </span><em class="italic"><span class="koboSpan" id="kobo.437.1">small</span></em><span class="koboSpan" id="kobo.438.1"> but not </span><em class="italic"><span class="koboSpan" id="kobo.439.1">smaller</span></em><span class="koboSpan" id="kobo.440.1">. </span><em class="italic"><span class="koboSpan" id="kobo.441.1">Smaller</span></em><span class="koboSpan" id="kobo.442.1"> would not be known by the model and would result in </span><strong class="source-inline"><span class="koboSpan" id="kobo.443.1">&lt;UNK&gt;</span></strong><span class="koboSpan" id="kobo.444.1">. </span><span class="koboSpan" id="kobo.444.2">Depending on the training set, the model might have incomplete or outdated knowledge. </span><span class="koboSpan" id="kobo.444.3">In English, as in other languages, there are definite morphemes and grammatical rules, and we would like the tokenizer to be aware. </span><span class="koboSpan" id="kobo.444.4">To avoid too many </span><strong class="source-inline"><span class="koboSpan" id="kobo.445.1">&lt;UNK&gt;</span></strong><span class="koboSpan" id="kobo.446.1"> one solution is to think in terms of </span><span class="No-Break"><span class="koboSpan" id="kobo.447.1">sub-words (tokens).</span></span></p>
<p><span class="koboSpan" id="kobo.448.1">One of the most widely</span><a id="_idIndexMarker143"/><span class="koboSpan" id="kobo.449.1"> used is </span><strong class="bold"><span class="koboSpan" id="kobo.450.1">Byte-Pair Encoding</span></strong><span class="koboSpan" id="kobo.451.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.452.1">BPE</span></strong><span class="koboSpan" id="kobo.453.1">). </span><span class="koboSpan" id="kobo.453.2">The process starts with a list of individual characters. </span><span class="koboSpan" id="kobo.453.3">The algorithm then scans the entire corpus and begins to merge the symbols that are most frequently found together. </span><span class="koboSpan" id="kobo.453.4">For example, we have </span><strong class="bold"><span class="koboSpan" id="kobo.454.1">E</span></strong><span class="koboSpan" id="kobo.455.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.456.1">R</span></strong><span class="koboSpan" id="kobo.457.1">, and after the first scan, we add a new </span><strong class="bold"><span class="koboSpan" id="kobo.458.1">ER</span></strong><span class="koboSpan" id="kobo.459.1"> symbol to the vocabulary. </span><span class="koboSpan" id="kobo.459.2">The process continues iteratively to merge and create new symbols (longer and longer character strings). </span><span class="koboSpan" id="kobo.459.3">Typically, the algorithm stops when it has created </span><em class="italic"><span class="koboSpan" id="kobo.460.1">N</span></em><span class="koboSpan" id="kobo.461.1"> tokens (with </span><em class="italic"><span class="koboSpan" id="kobo.462.1">N</span></em><span class="koboSpan" id="kobo.463.1"> being a predetermined number at the beginning). </span><span class="koboSpan" id="kobo.463.2">In addition, there is a special end-of-word symbol to differentiate whether the token is inside or at the end of a word. </span><span class="koboSpan" id="kobo.463.3">Once the algorithm arrives at creating a vocabulary, we can segment the corpus with the tokenizer and for each subword, we assign an index corresponding to the index in </span><span class="No-Break"><span class="koboSpan" id="kobo.464.1">the vocabulary.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer054">
<span class="koboSpan" id="kobo.465.1"><img alt="Figure 2.14 – Example of the results of tokenization" src="image/B21257_02_14.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.466.1">Figure 2.14 – Example of the results of tokenization</span></p>
<p><span class="koboSpan" id="kobo.467.1">This approach generally causes common words to be present in the model vocabulary while rare words are split into subwords. </span><span class="koboSpan" id="kobo.467.2">In addition, the model also learns suffixes and prefixes, and </span><a id="_idIndexMarker144"/><span class="koboSpan" id="kobo.468.1">considers the difference between </span><em class="italic"><span class="koboSpan" id="kobo.469.1">app</span></em><span class="koboSpan" id="kobo.470.1"> and the </span><em class="italic"><span class="koboSpan" id="kobo.471.1">app#</span></em><span class="koboSpan" id="kobo.472.1"> subword, representing a complete word and a subword (</span><em class="italic"><span class="koboSpan" id="kobo.473.1">app#</span></em><span class="koboSpan" id="kobo.474.1"> as a subword </span><span class="No-Break"><span class="koboSpan" id="kobo.475.1">of </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.476.1">application</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.477.1">).</span></span></p>
<h1 id="_idParaDest-39"><a id="_idTextAnchor038"/><span class="koboSpan" id="kobo.478.1">Exploring masked language modeling</span></h1>
<p><span class="koboSpan" id="kobo.479.1">Although the </span><a id="_idIndexMarker145"/><span class="koboSpan" id="kobo.480.1">transformer was revolutionary, the popularization of the transformer in the scientific community is also due</span><a id="_idIndexMarker146"/><span class="koboSpan" id="kobo.481.1"> to the </span><strong class="bold"><span class="koboSpan" id="kobo.482.1">Bidirectional Encoder Representations from Transformers</span></strong><span class="koboSpan" id="kobo.483.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.484.1">BERT</span></strong><span class="koboSpan" id="kobo.485.1">) model. </span><span class="koboSpan" id="kobo.485.2">This is because BERT was a revolutionary variant of the transformer that showed the capabilities of this type of model. </span><span class="koboSpan" id="kobo.485.3">BERT was revolutionary because it was already prospectively designed specifically for future applications (such as question answering, summarization, and machine translation). </span><span class="koboSpan" id="kobo.485.4">In fact, the original transformer analyzes the left-to-right sequence, so when the model encounters an entity, it cannot relate it to what is on the right of the entity. </span><span class="koboSpan" id="kobo.485.5">In these applications, it is important to have context from </span><span class="No-Break"><span class="koboSpan" id="kobo.486.1">both directions.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer055">
<span class="koboSpan" id="kobo.487.1"><img alt="Figure 2.15 – Difference between a causal and bidirectional language model" src="image/B21257_02_15.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.488.1">Figure 2.15 – Difference between a causal and bidirectional language model</span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.489.1">Bidirectional encoders</span></strong><span class="koboSpan" id="kobo.490.1"> resolve this </span><a id="_idIndexMarker147"/><span class="koboSpan" id="kobo.491.1">limitation by allowing the model to find relationships over the entire sequence. </span><span class="koboSpan" id="kobo.491.2">Obviously, we can no longer use a language model to train it (it will be too easy to identify the next word in the sequence when you already know the answer) but we have to find a way to be able to train a bidirectional model. </span><span class="koboSpan" id="kobo.491.3">For clarification, the model reads the entire sequence at once and, in this case, consists of the </span><span class="No-Break"><span class="koboSpan" id="kobo.492.1">encoder only.</span></span></p>
<p><span class="koboSpan" id="kobo.493.1">To try to minimize </span><a id="_idIndexMarker148"/><span class="koboSpan" id="kobo.494.1">changes to the structure we use what is called the </span><strong class="bold"><span class="koboSpan" id="kobo.495.1">Masked Language Model</span></strong><span class="koboSpan" id="kobo.496.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.497.1">MLM</span></strong><span class="koboSpan" id="kobo.498.1">) </span><strong class="bold"><span class="koboSpan" id="kobo.499.1">objective</span></strong><span class="koboSpan" id="kobo.500.1">. </span><span class="koboSpan" id="kobo.500.2">Instead of predicting the next word in the sequence, we mask some tokens and the model has to use the rest of the sequence to predict them. </span><span class="koboSpan" id="kobo.500.3">Given the entire context (the model now has access to the entire sequence), BERT must predict the token that has been masked with a special token (usually called </span><strong class="source-inline"><span class="koboSpan" id="kobo.501.1">&lt;MASK&gt;</span></strong><span class="koboSpan" id="kobo.502.1">). </span><span class="koboSpan" id="kobo.502.2">In the original training, they masked 15 % of the tokens randomly. </span><span class="koboSpan" id="kobo.502.3">Notice that, in this case, we do not mask the future because we want the model to be aware of the whole context. </span><span class="koboSpan" id="kobo.502.4">Also, to better separate the different sentences, we have a special token, </span><strong class="source-inline"><span class="koboSpan" id="kobo.503.1">[CLS]</span></strong><span class="koboSpan" id="kobo.504.1">, that signals the beginning of an input, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.505.1">[SEP]</span></strong><span class="koboSpan" id="kobo.506.1"> to separate sentences in the input (for example, if we have a question and an answer). </span><span class="koboSpan" id="kobo.506.2">Otherwise, the structure is the same: we have an embedder, a position encoder, different transformer blocks, a linear projection, and a softmax. </span><span class="koboSpan" id="kobo.506.3">The loss is calculated in the same way; instead of using the next token, we use the masked token. </span><span class="koboSpan" id="kobo.506.4">The original article introduced two versions of BERT: BERT-BASE (12 layers, hidden size with d=768, 12 attention heads, and 110M total parameters) and BERT-LARGE (24 layers, hidden size with d=1024, 24 attention heads, and 340M </span><span class="No-Break"><span class="koboSpan" id="kobo.507.1">total parameters).</span></span></p>
<p><span class="koboSpan" id="kobo.508.1">MLM is a flexible approach because the idea is to corrupt the input and ask the model to rebuild. </span><span class="koboSpan" id="kobo.508.2">We can mask, but we can also reorder or conduct other transformations. </span><span class="koboSpan" id="kobo.508.3">The disadvantage of this method is that only 15 percent of the tokens are actually used to learn, so the model is </span><span class="No-Break"><span class="koboSpan" id="kobo.509.1">highly inefficient.</span></span></p>
<p><span class="koboSpan" id="kobo.510.1">The training is also highly flexible. </span><span class="koboSpan" id="kobo.510.2">For example, the model can be extended to </span><strong class="bold"><span class="koboSpan" id="kobo.511.1">next sentence prediction</span></strong><span class="koboSpan" id="kobo.512.1">, where the</span><a id="_idIndexMarker149"/><span class="koboSpan" id="kobo.513.1"> task is to predict whether two pairs of sentences are related (paraphrase, coherence, and entailment). </span><span class="koboSpan" id="kobo.513.2">In this case, BERT was trained with training pairs of sentences positively related and unrelated sentences (we exploit the </span><strong class="source-inline"><span class="koboSpan" id="kobo.514.1">[SEP]</span></strong><span class="koboSpan" id="kobo.515.1"> token between sentences). </span><span class="koboSpan" id="kobo.515.2">The last layer is a softmax for sentence classification; we consider the loss over the categories. </span><span class="koboSpan" id="kobo.515.3">This shows how the system is flexible and can be adapted to </span><span class="No-Break"><span class="koboSpan" id="kobo.516.1">different tasks.</span></span></p>
<p><span class="koboSpan" id="kobo.517.1">One final clarification. </span><span class="koboSpan" id="kobo.517.2">Until 2024, it was always assumed that these models were not capable of generating text. </span><span class="koboSpan" id="kobo.517.3">In 2024, two studies showed that by adapting the model, you can generate text even with a BERT-like model. </span><span class="koboSpan" id="kobo.517.4">For example, in this study, they show that one can generate text by exploiting a sequence of [</span><span class="No-Break"><span class="koboSpan" id="kobo.518.1">MASK] tokens.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer056">
<span class="koboSpan" id="kobo.519.1"><img alt="Figure 2.16 – Text generation with MLM (https://arxiv.org/pdf/2406.04823)" src="image/B21257_02_16.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.520.1">Figure 2.16 – Text generation with MLM (</span><a href="https://arxiv.org/pdf/2406.04823"><span class="koboSpan" id="kobo.521.1">https://arxiv.org/pdf/2406.04823</span></a><span class="koboSpan" id="kobo.522.1">)</span></p>
<p><span class="koboSpan" id="kobo.523.1">Now that we have seen the two main types of training for a transformer, we can better explore what happens inside </span><span class="No-Break"><span class="koboSpan" id="kobo.524.1">these models.</span></span></p>
<h1 id="_idParaDest-40"><a id="_idTextAnchor039"/><span class="koboSpan" id="kobo.525.1">Visualizing internal mechanisms</span></h1>
<p><span class="koboSpan" id="kobo.526.1">We have seen the</span><a id="_idIndexMarker150"/><span class="koboSpan" id="kobo.527.1"> inner workings of the transformer, how it can be trained, and the main types of models. </span><span class="koboSpan" id="kobo.527.2">The beauty of attention is that we can visualize these relationships, and in this section, we will see how to do that. </span><span class="koboSpan" id="kobo.527.3">We can then visualize the relationships within the BERT attention head. </span><span class="koboSpan" id="kobo.527.4">As mentioned, in each layer, there are several attention heads and each of them learns a different representation of the input data. </span><span class="koboSpan" id="kobo.527.5">The color intensity indicates a greater weight in the attention weights (darker colors indicate weights that are close </span><span class="No-Break"><span class="koboSpan" id="kobo.528.1">to 1).</span></span></p>
<p><span class="koboSpan" id="kobo.529.1">We can do this</span><a id="_idIndexMarker151"/><span class="koboSpan" id="kobo.530.1"> using the </span><span class="No-Break"><span class="koboSpan" id="kobo.531.1">BERTviz package:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.532.1">
head_view(attention, tokens, sentence_b_start)</span></pre> <p class="callout-heading"><span class="koboSpan" id="kobo.533.1">Important note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.534.1">The visualization is interactive. </span><span class="koboSpan" id="kobo.534.2">The code is in the repository. </span><span class="koboSpan" id="kobo.534.3">Try running it using different phrases and exploring different relationships between different words in the phrases. </span><span class="koboSpan" id="kobo.534.4">The visualization allows you to explore the different layers in the model by taking advantage of the drop-down model. </span><span class="koboSpan" id="kobo.534.5">Hovering over the various words allows you to see the individual weights of the </span><span class="No-Break"><span class="koboSpan" id="kobo.535.1">various heads.</span></span></p>
<p><span class="koboSpan" id="kobo.536.1">This is the </span><span class="No-Break"><span class="koboSpan" id="kobo.537.1">corresponding visualization:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer057">
<span class="koboSpan" id="kobo.538.1"><img alt="Figure 2.17 – Visualization of attention between all words in the input" src="image/B21257_02_17.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.539.1">Figure 2.17 – Visualization of attention between all words in the input</span></p>
<p><span class="koboSpan" id="kobo.540.1">We can also view the various heads of the model at the same time. </span><span class="koboSpan" id="kobo.540.2">This allows us to see how the various heads model different relationships. </span><span class="koboSpan" id="kobo.540.3">This model has 12 heads for 12 layers, so the model has 144 attention heads and can therefore see more than 100 representations for the same sentences (this explains the capacity of a model). </span><span class="koboSpan" id="kobo.540.4">Moreover, these</span><a id="_idIndexMarker152"/><span class="koboSpan" id="kobo.541.1"> representations are not completely independent; information learned from earlier layers can be used by </span><span class="No-Break"><span class="koboSpan" id="kobo.542.1">later layers:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.543.1">
model_view(attention, tokens, sentence_b_start)</span></pre> <p class="callout-heading"><span class="koboSpan" id="kobo.544.1">Important note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.545.1">The visualization is interactive. </span><span class="koboSpan" id="kobo.545.2">The code is in the repository. </span><span class="koboSpan" id="kobo.545.3">Try running it using different phrases and exploring different relationships. </span><span class="koboSpan" id="kobo.545.4">Here, we have the ensemble representation of the various attention heads. </span><span class="koboSpan" id="kobo.545.5">Observe how each head has a different function and how it models a different representation of the </span><span class="No-Break"><span class="koboSpan" id="kobo.546.1">same inputs.</span></span></p>
<p><span class="koboSpan" id="kobo.547.1"> This is the </span><span class="No-Break"><span class="koboSpan" id="kobo.548.1">corresponding visualization:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer058">
<span class="koboSpan" id="kobo.549.1"><img alt="Figure 2.18 – Model view of the first two layers" src="image/B21257_02_18.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.550.1">Figure 2.18 – Model view of the first two layers</span></p>
<p><span class="koboSpan" id="kobo.551.1">Another model that has been fundamental to the current development of today’s models is </span><strong class="bold"><span class="koboSpan" id="kobo.552.1">Generative Pre-Trained Transformer 2</span></strong><span class="koboSpan" id="kobo.553.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.554.1">GPT-2</span></strong><span class="koboSpan" id="kobo.555.1">). </span><span class="koboSpan" id="kobo.555.2">GPT-2 is a causal (unidirectional) transformer</span><a id="_idIndexMarker153"/><span class="koboSpan" id="kobo.556.1"> pre-trained using language modeling on a very large corpus of ~40 GB of text </span><a id="_idIndexMarker154"/><span class="koboSpan" id="kobo.557.1">data. </span><span class="koboSpan" id="kobo.557.2">GPT-2 was specifically trailed to predict the next token and to generate text with an input (it generates a token at a time; this token is then added to the input sequence to generate the next in an autoregressive process). </span><span class="koboSpan" id="kobo.557.3">In addition, this is perhaps the first model that has been trained with a massive amount of text. </span><span class="koboSpan" id="kobo.557.4">In addition, this model consists only of the decoder. </span><span class="koboSpan" id="kobo.557.5">GPT-2 is a family of models ranging from 12 layers of GPT-2 small to 48 layers of GPT-2 XL. </span><span class="koboSpan" id="kobo.557.6">Each layer consists of masked self-attention and a feed-forward </span><span class="No-Break"><span class="koboSpan" id="kobo.558.1">neural network.</span></span></p>
<p><span class="koboSpan" id="kobo.559.1">GPT-2 is generative and trained as a language model so we can give it an input judgment and observe the probability for the next token. </span><span class="koboSpan" id="kobo.559.2">For example, using “To be or not to” as input, the token with the highest probability </span><span class="No-Break"><span class="koboSpan" id="kobo.560.1">is “be.”</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer059">
<span class="koboSpan" id="kobo.561.1"><img alt="Figure 2.19 – Probabilities associated with the next token for the GPT-2 model when probed with the “To be or not to” input sequence" src="image/B21257_02_19.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.562.1">Figure 2.19 – Probabilities associated with the next token for the GPT-2 model when probed with the “To be or not to” input sequence</span></p>
<p><span class="koboSpan" id="kobo.563.1">Sometimes, it may be necessary to understand which tokens are most important to the model to generate the next token. </span><strong class="bold"><span class="koboSpan" id="kobo.564.1">Gradient X input</span></strong><span class="koboSpan" id="kobo.565.1"> is a</span><a id="_idIndexMarker155"/><span class="koboSpan" id="kobo.566.1"> technique originally developed for convolutional networks; at a given time step, we take the output probabilities for each token, select the tokens with the highest probability, and compute the gradient with </span><a id="_idIndexMarker156"/><span class="koboSpan" id="kobo.567.1">respect to the input up to the input tokens. </span><span class="koboSpan" id="kobo.567.2">This gives us the importance of each token to generate the next token in the sequence (the rationale is that small changes in the input tokens with the highest importance carry the largest changes in the output). </span><span class="koboSpan" id="kobo.567.3">In the figure, we can see the most important tokens for the next token in </span><span class="No-Break"><span class="koboSpan" id="kobo.568.1">the sequence:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer060">
<span class="koboSpan" id="kobo.569.1"><img alt="Figure 2.20 – Gradient X input for the next token in the sequence" src="image/B21257_02_20.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.570.1">Figure 2.20 – Gradient X input for the next token in the sequence</span></p>
<p><span class="koboSpan" id="kobo.571.1">As mentioned before, there is not only self-attention but also feedforward neural network, which plays an important role (it provides a significant portion of the parameters in the transformer block, about 66%). </span><span class="koboSpan" id="kobo.571.2">Therefore, several works have focused on examining the firings of neurons in layers (this technique was also originally developed for </span><span class="No-Break"><span class="koboSpan" id="kobo.572.1">computer vision).</span></span></p>
<p><span class="koboSpan" id="kobo.573.1">We can follow this activation after each layer, and for each of the tokens, we can monitor what their rank (by probability) is after each layer. </span><span class="koboSpan" id="kobo.573.2">As we can see, the model understands from</span><a id="_idIndexMarker157"/><span class="koboSpan" id="kobo.574.1"> the first layers which token is the most likely to continue </span><span class="No-Break"><span class="koboSpan" id="kobo.575.1">a sequence:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer061">
<span class="koboSpan" id="kobo.576.1"><img alt="Figure 2.21 – Heatmap of the rank for the top five most likely tokens after each layer" src="image/B21257_02_21.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.577.1">Figure 2.21 – Heatmap of the rank for the top five most likely tokens after each layer</span></p>
<p><span class="koboSpan" id="kobo.578.1">Since there are a considerable</span><a id="_idIndexMarker158"/><span class="koboSpan" id="kobo.579.1"> number of neurons, it is complex to be able to observe them directly. </span><span class="koboSpan" id="kobo.579.2">Therefore, one way to investigate these activations is to first reduce dimensionality. </span><span class="koboSpan" id="kobo.579.3">To avoid negative activations, it is preferred to use </span><strong class="bold"><span class="koboSpan" id="kobo.580.1">Non-Negative Matrix Factorization</span></strong><span class="koboSpan" id="kobo.581.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.582.1">NMF</span></strong><span class="koboSpan" id="kobo.583.1">) instead of </span><strong class="bold"><span class="koboSpan" id="kobo.584.1">Principal Component Analysis</span></strong><span class="koboSpan" id="kobo.585.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.586.1">PCA</span></strong><span class="koboSpan" id="kobo.587.1">). </span><span class="koboSpan" id="kobo.587.2">The process</span><a id="_idIndexMarker159"/><span class="koboSpan" id="kobo.588.1"> first</span><a id="_idIndexMarker160"/><span class="koboSpan" id="kobo.589.1"> captures the activation of neurons in the FFNN layers of the model and is then decomposed into some factors (user-chosen parameters). </span><span class="koboSpan" id="kobo.589.2">Next, we can interactively observe the factors with the highest activation when a token has been generated. </span><span class="koboSpan" id="kobo.589.3">What we see in the graph is the factor excitation for each of the </span><span class="No-Break"><span class="koboSpan" id="kobo.590.1">generated tokens:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer062">
<span class="koboSpan" id="kobo.591.1"><img alt="Figure 2.22 – NMF for the activations of the model in generating a sequence" src="image/B21257_02_22.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.592.1">Figure 2.22 – NMF for the activations of the model in generating a sequence</span></p>
<p><span class="koboSpan" id="kobo.593.1">We can also conduct this analysis for a single layer. </span><span class="koboSpan" id="kobo.593.2">This allows us to analyze interesting behaviors within the neurons of a layer (in the image layer 0 of the model). </span><span class="koboSpan" id="kobo.593.3">In this case, there are certain factors that focus on specific portions of the text (beginning, middle, and end). </span><span class="koboSpan" id="kobo.593.4">As we mentioned earlier, the model keeps track of word order in a sequence due to positional encoding, and this is reflected in activation. </span><span class="koboSpan" id="kobo.593.5">Other neurons, however, are activated by grammatical structures (such as conjunctions, articles, and so on). </span><span class="koboSpan" id="kobo.593.6">This indicates to us a specialization of what individual neurons in a pattern track and is one of the strength</span><a id="_idIndexMarker161"/><span class="koboSpan" id="kobo.594.1"> components of the transformer. </span><span class="koboSpan" id="kobo.594.2">By increasing the number of facts, we can increase the resolution and better understand what grammatical and semantic structures the pattern encodes in its activations. </span><span class="koboSpan" id="kobo.594.3">Moving forward in the structure of the model, we can see that layers learn a </span><span class="No-Break"><span class="koboSpan" id="kobo.595.1">different representation.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer063">
<span class="koboSpan" id="kobo.596.1"><img alt="Figure 2.23 – NMF for the activations of the model in generating a sequence" src="image/B21257_02_23.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.597.1">Figure 2.23 – NMF for the activations of the model in generating a sequence</span></p>
<p><span class="koboSpan" id="kobo.598.1">We have seen how to build a transformer and how it works. </span><span class="koboSpan" id="kobo.598.2">Now that we know the anatomy of a transformer, it is time to see it </span><span class="No-Break"><span class="koboSpan" id="kobo.599.1">at work.</span></span></p>
<h1 id="_idParaDest-41"><a id="_idTextAnchor040"/><span class="koboSpan" id="kobo.600.1">Applying a transformer</span></h1>
<p><span class="koboSpan" id="kobo.601.1">The power of a transformer </span><a id="_idIndexMarker162"/><span class="koboSpan" id="kobo.602.1">lies in its ability to be able to learn from an enormous amount of text. </span><span class="koboSpan" id="kobo.602.2">During this phase of training (called </span><strong class="bold"><span class="koboSpan" id="kobo.603.1">pre-training</span></strong><span class="koboSpan" id="kobo.604.1">), the </span><a id="_idIndexMarker163"/><span class="koboSpan" id="kobo.605.1">model learns general rules about the structure of a language. </span><span class="koboSpan" id="kobo.605.2">This general representation can then be exploited for a myriad of applications. </span><span class="koboSpan" id="kobo.605.3">One of the most important concepts in deep learning</span><a id="_idIndexMarker164"/><span class="koboSpan" id="kobo.606.1"> is </span><strong class="bold"><span class="koboSpan" id="kobo.607.1">transfer learning</span></strong><span class="koboSpan" id="kobo.608.1">, in which we exploit the ability of a model trained on a large amount of data for a task different from the one it was originally trained for. </span><span class="koboSpan" id="kobo.608.2">A special case of transfer learning</span><a id="_idIndexMarker165"/><span class="koboSpan" id="kobo.609.1"> is </span><strong class="bold"><span class="koboSpan" id="kobo.610.1">fine-tuning</span></strong><span class="koboSpan" id="kobo.611.1">. </span><span class="koboSpan" id="kobo.611.2">Fine-tuning allows us to adapt the general knowledge of a model to a particular case. </span><span class="koboSpan" id="kobo.611.3">One way to do this is to add a set of parameters to a model (at the top of it) and then train these parameters by gradient descent for a </span><span class="No-Break"><span class="koboSpan" id="kobo.612.1">specific task.</span></span></p>
<p><span class="koboSpan" id="kobo.613.1">The transformer has been</span><a id="_idIndexMarker166"/><span class="koboSpan" id="kobo.614.1"> trained with large amounts of text and has learned semantic rules that are useful in understanding a text. </span><span class="koboSpan" id="kobo.614.2">We want to exploit this knowledge for a specific application such as sentiment classification. </span><span class="koboSpan" id="kobo.614.3">Instead of training a model from scratch, we can adapt a pre-trained transformer to classify our sentences. </span><span class="koboSpan" id="kobo.614.4">In this case, we do not want to destroy the internal representation of the model but preserve it. </span><span class="koboSpan" id="kobo.614.5">That is why, during fine-tuning, most of the layers are frozen (there is no update on the weights). </span><span class="koboSpan" id="kobo.614.6">Instead, we just train those one or two layers that we add to the top of the model. </span><span class="koboSpan" id="kobo.614.7">The idea is to preserve the representation and then learn how to use it for our specific task. </span><span class="koboSpan" id="kobo.614.8">Those two added layers learn precisely how to use the internal representation of the model. </span><span class="koboSpan" id="kobo.614.9">To give a simple example, let’s imagine we want to learn how to write scientific papers. </span><span class="koboSpan" id="kobo.614.10">To do that, we don’t have to learn how to write in English again, just to adapt our knowledge to this </span><span class="No-Break"><span class="koboSpan" id="kobo.615.1">new task.</span></span></p>
<p><span class="koboSpan" id="kobo.616.1">In BERT, as we mentioned, we add a particular token to the beginning of each sequence: a </span><strong class="source-inline"><span class="koboSpan" id="kobo.617.1">[CLS]</span></strong><span class="koboSpan" id="kobo.618.1"> token. </span><span class="koboSpan" id="kobo.618.2">During training or even inference in a bidirectional transformer, this token waits for all others in the sequence (if you remember, all tokens are connected). </span><span class="koboSpan" id="kobo.618.3">This means that the final vector (the one in the last layer) is contextualized for each element in the sequence. </span><span class="koboSpan" id="kobo.618.4">We can then exploit this vector for a classification task. </span><span class="koboSpan" id="kobo.618.5">If we have three classes (for example, positive, neutral, and negative) we can take the vector for a sequence and use softmax </span><span class="No-Break"><span class="koboSpan" id="kobo.619.1">to classify.</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><mml:math display="block"><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>W</mml:mi><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>L</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></span></p>
<p><span class="koboSpan" id="kobo.620.1">The model was not originally trained for sequence classification, so we'd like to introduce a learnable matrix to enable class separation. </span><span class="koboSpan" id="kobo.620.2">This matrix represents a linear transformation and can alternatively be implemented using one or more linear layers. </span><span class="koboSpan" id="kobo.620.3">We then apply a cross-entropy loss function to optimize these weights. </span><span class="koboSpan" id="kobo.620.4">This setup follows the standard supervised learning paradigm, where labeled data</span><a id="_idIndexMarker167"/><span class="koboSpan" id="kobo.621.1"> is used to adapt the transformer to a </span><span class="No-Break"><span class="koboSpan" id="kobo.622.1">specific task.</span></span></p>
<p><span class="koboSpan" id="kobo.623.1">In this process, we have so far assumed that the remainder of the transformer's weights remain frozen. </span><span class="koboSpan" id="kobo.623.2">However, as observed in convolutional neural networks, even minimal fine-tuning of model parameters can enhance performance. </span><span class="koboSpan" id="kobo.623.3">Such updates are typically carried out with a very low </span><span class="No-Break"><span class="koboSpan" id="kobo.624.1">learning rate.</span></span></p>
<p><span class="koboSpan" id="kobo.625.1">We can adapt a pre-trained transformer for new tasks through </span><span class="No-Break"><span class="koboSpan" id="kobo.626.1">supervised fine-tuning.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer064">
<span class="koboSpan" id="kobo.627.1"><img alt="Figure 2.24 – Fine-tuning a transformer" src="image/B21257_02_24.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.628.1">Figure 2.24 – Fine-tuning a transformer</span></p>
<p><span class="koboSpan" id="kobo.629.1">In the first step, we are removing the final layer (this is specific to the original task). </span><span class="koboSpan" id="kobo.629.2">In the second step, we add a random initialized</span><a id="_idIndexMarker168"/><span class="koboSpan" id="kobo.630.1"> layer and gather training examples for the new task. </span><span class="koboSpan" id="kobo.630.2">During the fine-tuning, we are presenting the model with new examples (in this case, positive and negative reviews). </span><span class="koboSpan" id="kobo.630.3">While keeping the model frozen (each example is processed by the whole model in the forward pass), we update the weight only in the new layer (through backpropagation). </span><span class="koboSpan" id="kobo.630.4">The model has now learned the </span><span class="No-Break"><span class="koboSpan" id="kobo.631.1">new task.</span></span></p>
<p><span class="koboSpan" id="kobo.632.1">Conducting finetuning with Hugging Face is a straightforward process. </span><span class="koboSpan" id="kobo.632.2">We can use a model such as distill-BERT (a distilled version of BERT) with a few lines of code and the dataset we used in the previous chapter. </span><span class="koboSpan" id="kobo.632.3">We need to prepare the dataset and tokenize it (so that it can be used with a transformer). </span><span class="koboSpan" id="kobo.632.4">Hugging Face then allows with a simple wrapper that we can train the model. </span><span class="koboSpan" id="kobo.632.5">The arguments for training are stored </span><span class="No-Break"><span class="koboSpan" id="kobo.633.1">in </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.634.1">TrainingArguments</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.635.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.636.1">
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=16
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
    evaluation_strategy="epoch"
)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset
)
trainer.train()</span></pre> <p class="callout-heading"><span class="koboSpan" id="kobo.637.1">Important note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.638.1">Notice that the process is very similar to training a neural network. </span><span class="koboSpan" id="kobo.638.2">In fact, the transformer is a deep learning model; for the training, we are using </span><span class="No-Break"><span class="koboSpan" id="kobo.639.1">similar hyperparameters.</span></span></p>
<p><span class="koboSpan" id="kobo.640.1">In this case, we used only a small </span><a id="_idIndexMarker169"/><span class="koboSpan" id="kobo.641.1">fraction of the reviews. </span><span class="koboSpan" id="kobo.641.2">The beauty of fine-tuning is that we need only a few examples to have a similar (if not better) performance than a model trained </span><span class="No-Break"><span class="koboSpan" id="kobo.642.1">from scratch.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer065">
<span class="koboSpan" id="kobo.643.1"><img alt="Figure 2.25 – Confusion matrix after fine-tuning" src="image/B21257_02_25.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.644.1">Figure 2.25 – Confusion matrix after fine-tuning</span></p>
<p><span class="koboSpan" id="kobo.645.1">BERT’s training was </span><a id="_idIndexMarker170"/><span class="koboSpan" id="kobo.646.1">done on 64 TPUs (special hardware for tensor operations) for four days; this is beyond the reach of most users. </span><span class="koboSpan" id="kobo.646.2">In contrast, fine-tuning is possible either on a single GPU or on a CPU. </span><span class="koboSpan" id="kobo.646.3">As a result, BERT achieved state-of-the-art performance upon its release across a wide array of tasks, including paraphrase detection, question answering, and sentiment analysis. </span><span class="koboSpan" id="kobo.646.4">Hence, several variants such </span><a id="_idIndexMarker171"/><span class="koboSpan" id="kobo.647.1">as </span><strong class="bold"><span class="koboSpan" id="kobo.648.1">RoBERTa</span></strong><span class="koboSpan" id="kobo.649.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.650.1">SpanBERT</span></strong><span class="koboSpan" id="kobo.651.1"> (in this case, we mask an entire span instead</span><a id="_idIndexMarker172"/><span class="koboSpan" id="kobo.652.1"> of a single token with better results) or adapted to specific domains such as </span><strong class="bold"><span class="koboSpan" id="kobo.653.1">SciBERT</span></strong><span class="koboSpan" id="kobo.654.1"> were </span><a id="_idIndexMarker173"/><span class="koboSpan" id="kobo.655.1">born. </span><span class="koboSpan" id="kobo.655.2">However, encoders are not optimal for generative tasks (because of mask training) while </span><span class="No-Break"><span class="koboSpan" id="kobo.656.1">decoders are.</span></span></p>
<p><span class="koboSpan" id="kobo.657.1">To conduct machine translation, the original transformer consisted of an encoder and a decoder. </span><span class="koboSpan" id="kobo.657.2">A model such as GPT-2 only has a decoder. </span><span class="koboSpan" id="kobo.657.3">We can conduct fine-tuning in the same way as seen before, we just need to construct the dataset in an optimal way for a model that is constituted by the decoder alone. </span><span class="koboSpan" id="kobo.657.4">For example, we can take a dataset in which</span><a id="_idIndexMarker174"/><span class="koboSpan" id="kobo.658.1"> we have English and French sentences and build a dataset for finetuning as follows: </span><strong class="source-inline"><span class="koboSpan" id="kobo.659.1">&lt;sentence in English&gt;</span></strong><span class="koboSpan" id="kobo.660.1"> followed by a special </span><strong class="source-inline"><span class="koboSpan" id="kobo.661.1">&lt;to-fr&gt;</span></strong><span class="koboSpan" id="kobo.662.1"> token and then the </span><strong class="source-inline"><span class="koboSpan" id="kobo.663.1">&lt;sentence in French&gt;</span></strong><span class="koboSpan" id="kobo.664.1">. </span><span class="koboSpan" id="kobo.664.2">The same approach can be used to teach summarization to a model, where we insert a special token meaning summarization. </span><span class="koboSpan" id="kobo.664.3">The model is fine-tuned by conducting the next token prediction (</span><span class="No-Break"><span class="koboSpan" id="kobo.665.1">language modeling).</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer066">
<span class="koboSpan" id="kobo.666.1"><img alt="Figure 2.26 – Fine-tuning of a decoder-only model" src="image/B21257_02_26.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.667.1">Figure 2.26 – Fine-tuning of a decoder-only model</span></p>
<p><span class="koboSpan" id="kobo.668.1">Another way to exploit the learned knowledge of a model </span><a id="_idIndexMarker175"/><span class="koboSpan" id="kobo.669.1">is to use </span><strong class="bold"><span class="koboSpan" id="kobo.670.1">knowledge distillation</span></strong><span class="koboSpan" id="kobo.671.1">. </span><span class="koboSpan" id="kobo.671.2">In the previous section, we used distillGPT-2 which is a distilled version of GPT-2. </span><span class="koboSpan" id="kobo.671.3">A distilled model captures knowledge from a much larger model without losing significant performance but is much more manageable. </span><span class="koboSpan" id="kobo.671.4">Models that are trained with a large amount of text learn a huge body of knowledge. </span><span class="koboSpan" id="kobo.671.5">All this knowledge and skill is often redundant when we need a model for some specific task. </span><span class="koboSpan" id="kobo.671.6">We are interested in having a model that is </span><a id="_idIndexMarker176"/><span class="koboSpan" id="kobo.672.1">very capable for a task, but without wanting to deal with a model of billions of parameters. </span><span class="koboSpan" id="kobo.672.2">In addition, sometimes we do not have enough examples for a model to learn the task from scratch. </span><span class="koboSpan" id="kobo.672.3">In this case, we can extract knowledge from the </span><span class="No-Break"><span class="koboSpan" id="kobo.673.1">larger model.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer067">
<span class="koboSpan" id="kobo.674.1"><img alt="Figure 2.27 – Generic teacher-student framework for knowledge distillation (https://arxiv.org/pdf/2006.05525)" src="image/B21257_02_27.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.675.1">Figure 2.27 – Generic teacher-student framework for knowledge distillation (</span><a href="https://arxiv.org/pdf/2006.05525"><span class="koboSpan" id="kobo.676.1">https://arxiv.org/pdf/2006.05525</span></a><span class="koboSpan" id="kobo.677.1">)</span></p>
<p><span class="koboSpan" id="kobo.678.1">Knowledge distillation can be seen as a form of compression, in which we try to transfer knowledge from a trained “teacher” model with many parameters to a “student” model with fewer parameters. </span><span class="koboSpan" id="kobo.678.2">The student model tries to mimic the teacher model and achieve the same performances as the teacher model in a task. </span><span class="koboSpan" id="kobo.678.3">In such a framework, we have three components: the models, knowledge, and algorithm. </span><span class="koboSpan" id="kobo.678.4">The algorithm can exploit either the teacher’s logits or intermediate activations. </span><span class="koboSpan" id="kobo.678.5">In the case of the logits, the student tries to mimic the predictions of the teacher model, so we try to minimize the difference between the logits produced by the teacher and the student. </span><span class="koboSpan" id="kobo.678.6">To do this, we use a distillation</span><a id="_idIndexMarker177"/><span class="koboSpan" id="kobo.679.1"> loss that allows us to train the </span><span class="No-Break"><span class="koboSpan" id="kobo.680.1">student model.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer068">
<span class="koboSpan" id="kobo.681.1"><img alt="Figure 2.28 – Teacher-student framework for knowledge distillation training (https://arxiv.org/pdf/2006.05525)" src="image/B21257_02_28.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.682.1">Figure 2.28 – Teacher-student framework for knowledge distillation training (</span><a href="https://arxiv.org/pdf/2006.05525"><span class="koboSpan" id="kobo.683.1">https://arxiv.org/pdf/2006.05525</span></a><span class="koboSpan" id="kobo.684.1">)</span></p>
<p><span class="koboSpan" id="kobo.685.1">For knowledge distillation, the steps are also similar. </span><span class="koboSpan" id="kobo.685.2">The first step is data preprocessing. </span><span class="koboSpan" id="kobo.685.3">For each model, you must remember to choose the model-specific tokenizer (although the one from GPT-2 is the most widely used many models have different tokenizers). </span><span class="koboSpan" id="kobo.685.4">We must then conduct fine-tuning of a model on our task (there is no model that is specific to classify reviews). </span><span class="koboSpan" id="kobo.685.5">This model will be our teacher. </span><span class="koboSpan" id="kobo.685.6">The next step is to train a student model. </span><span class="koboSpan" id="kobo.685.7">We can also use a pre-trained model that is smaller than the teacher (this allows us to be able to use a few examples to </span><span class="No-Break"><span class="koboSpan" id="kobo.686.1">train it).</span></span></p>
<p><span class="koboSpan" id="kobo.687.1">One important difference is that we now have a specific loss for knowledge distillation. </span><span class="koboSpan" id="kobo.687.2">This distillation loss calculates the loss between the teacher’s logits and the student’s logits. </span><span class="koboSpan" id="kobo.687.3">This function typically uses the Kullback-Leibler divergence loss to calculate the difference between the two probability distributions (Kullback-Leibler divergence is really a measure of the difference between two probability distributions). </span><span class="koboSpan" id="kobo.687.4">We can define it </span><span class="No-Break"><span class="koboSpan" id="kobo.688.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.689.1">
def distillation_loss(outputs_student, outputs_teacher,
                      temperature=2.0):
    log_prob_student = F.log_softmax(
        outputs_student / temperature, dim=-1)
    prob_teacher = F.softmax(
        outputs_teacher / temperature, dim=-1)
    loss = KLDivLoss(reduction='batchmean')(
        log_prob_student, prob_teacher)
    return loss</span></pre> <p><span class="koboSpan" id="kobo.690.1">At this point, we just have to have a way to train our system. </span><span class="koboSpan" id="kobo.690.2">In this case, the teacher will be used only in</span><a id="_idIndexMarker178"/><span class="koboSpan" id="kobo.691.1"> inference while the student model will be trained. </span><span class="koboSpan" id="kobo.691.2">We will use the teacher’s logits to calculate </span><span class="No-Break"><span class="koboSpan" id="kobo.692.1">the loss:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.693.1">
def train_epoch(model, dataloader, optimizer, device,
                teacher_model, temperature=2.0):
    model.train()
    total_loss = 0
    for batch in tqdm(dataloader, desc="Training"):
        inputs = {k: v.to(device)
                  for k, v in batch.items()
                  if k in ['input_ids', 'attention_mask']}
        with torch.no_grad():
            outputs_teacher = teacher_model(**inputs).logits
        outputs_student = model(**inputs).logits
        loss = distillation_loss(
            outputs_student, outputs_teacher, temperature)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    return total_loss / len(dataloader)</span></pre> <p><span class="koboSpan" id="kobo.694.1">As can be seen in the </span><a id="_idIndexMarker179"/><span class="koboSpan" id="kobo.695.1">following figure, the performance of the student model is similar to the </span><span class="No-Break"><span class="koboSpan" id="kobo.696.1">teacher model:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer069">
<span class="koboSpan" id="kobo.697.1"><img alt="Figure 2.29 – Confusion matrix for the teacher and student model" src="image/B21257_02_29.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.698.1">Figure 2.29 – Confusion matrix for the teacher and student model</span></p>
<p><span class="koboSpan" id="kobo.699.1">Fine-tuning and knowledge distillation allow us to be able to use a transformer for any supervised task. </span><span class="koboSpan" id="kobo.699.2">Fine-tuning allows us to work with datasets that are small (and where there are often too few examples to train a model from scratch). </span><span class="koboSpan" id="kobo.699.3">Knowledge distillation, on the other hand, allows </span><a id="_idIndexMarker180"/><span class="koboSpan" id="kobo.700.1">us to get a smaller model (but performs as well as a much larger one) when the computational cost is the limit. </span><span class="koboSpan" id="kobo.700.2">By taking advantage of these techniques, we can tackle </span><span class="No-Break"><span class="koboSpan" id="kobo.701.1">any task.</span></span></p>
<h1 id="_idParaDest-42"><a id="_idTextAnchor041"/><span class="koboSpan" id="kobo.702.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.703.1">In this chapter, we discussed the transformer, the model that revolutionized NLP and artificial intelligence. </span><span class="koboSpan" id="kobo.703.2">Today, all models that have commercial applications are derivatives of the transformer, as we learned in this chapter. </span><span class="koboSpan" id="kobo.703.3">Understanding how it works on a mechanistic level, and how the various parts (self-attention, embedding, tokenization, and so on) work together, allows us to understand the limitations of modern models. </span><span class="koboSpan" id="kobo.703.4">We saw how it works internally in a visual way, thus exploring the motive of modern artificial intelligence from multiple perspectives. </span><span class="koboSpan" id="kobo.703.5">Finally, we saw how we can adapt a transformer to our needs using techniques that leverage prior knowledge of the model. </span><span class="koboSpan" id="kobo.703.6">Now we can repurpose this process with virtually any dataset and </span><span class="No-Break"><span class="koboSpan" id="kobo.704.1">any task.</span></span></p>
<p><span class="koboSpan" id="kobo.705.1">Learning how to train a transformer will allow us to understand what happens when we take this process to scale. </span><span class="koboSpan" id="kobo.705.2">An LLM is a transformer with more parameters and that has been trained with more text. </span><span class="koboSpan" id="kobo.705.3">This leads to emergent properties that have made it so successful, but both its merits and shortcomings lie in the elements we </span><span class="No-Break"><span class="koboSpan" id="kobo.706.1">have seen.</span></span></p>
<p><span class="koboSpan" id="kobo.707.1">In </span><a href="B21257_03.xhtml#_idTextAnchor042"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.708.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.709.1">, we will see precisely how to obtain an LLM from a transformer. </span><span class="koboSpan" id="kobo.709.2">What we have learned in this chapter will allow us to see how this step </span><span class="No-Break"><span class="koboSpan" id="kobo.710.1">comes naturally.</span></span></p>
</div>
</body></html>