- en: '12'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scheduled Prompt Parsing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 10*](B21263_10.xhtml#_idTextAnchor197), we discussed how to unlock
    the 77-token prompt limitation and a solution to enable prompt weighting, which
    paved the way for this chapter. With the knowledge from [*Chapter 10*](B21263_10.xhtml#_idTextAnchor197),
    we can generate various kinds of images by leveraging the power of natural language
    and weighting formats. However, there are some limitations inherent in the out-of-the-box
    code from the Hugging Face Diffusers package.
  prefs: []
  type: TYPE_NORMAL
- en: For example, we cannot write a prompt to ask Stable Diffusion to generate a
    cat in the first five steps and then a dog in the next five steps. Similarly,
    we cannot write a prompt to ask Stable Diffusion to blend two concepts by alternately
    denoising the two concepts.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will explore the two solutions in the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Using the Compel package
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a custom scheduled prompt pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To get started with the code in this chapter, you will need to install the necessary
    packages for running Stable Diffusion. For detailed instructions on how to set
    up these packages, refer to [*Chapter 2*](B21263_02.xhtml#_idTextAnchor037).
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the packages required by Stable Diffusion, you will also need
    to install the `Compel` package for the *Using the Compel package* section, and
    the `lark` package for the *Building a custom scheduled prompt* *pipeline* section.
  prefs: []
  type: TYPE_NORMAL
- en: I will provide step-by-step instructions for installing and using these packages
    in each section.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Compel package
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Compel [1] is an open source text prompt-weighting and blending library developed
    and maintained by Damian Stewart. It is one of the easiest ways to enable blending
    prompts in Diffusers. This package also has the capability to apply weighting
    to prompts, similar to the solution we implemented in [*Chapter 10*](B21263_10.xhtml#_idTextAnchor197),
    but with a different weighting syntax. In this chapter, I will introduce the blending
    feature that can help us write a prompt to generate an image with two or more
    concepts blended.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine that we want to create a photo that is half cat and half dog. How would
    we do it with prompts? Let’s say we simply give Stable Diffusion the following
    prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are the lines of Python code (without using Compel):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see the result shown in *Figure 12**.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.1: Result of the A photo with half cat and half dog prompt](img/B21263_12_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.1: Result of the A photo with half cat and half dog prompt'
  prefs: []
  type: TYPE_NORMAL
- en: The word `half` should be applied to the photo itself, rather than the image.
    In this case, we can use Compel to help generate a text embedding that blends
    cat and dog.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before importing the `Compel` package, we will need to install the package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the reason the `Compel` package works with Diffusers is that the
    package produces text embedding using `tokenizer` (type: `transformers.models.clip.tokenization_clip.CLIPTokenizer`)
    and `text_encoder` (type: `transformers.models.clip.modeling_clip.CLIPTextModel`)
    from the Stable Diffusion model file. We should also be aware of this during the
    initialization of the `Compel` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The `pipeline` (type: `StableDiffusionPipeline`) is the Stable Diffusion pipeline
    we just created. Next, create a blend prompt using the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, feed the text embedding into the Stable Diffusion pipeline through the
    `prompt_embeds` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We will see a pet that looks like a cat and also a dog, as shown in *Figure
    12**.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.2: A blended photo – half cat and half dog – using Compel](img/B21263_12_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.2: A blended photo – half cat and half dog – using Compel'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can change the proposition number of the blend to have more `cat` or more
    `dog`. Let’s change the prompt to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We will get a photo slightly more like a cat, as shown in *Figure 12**.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.3: A blended photo of a cat and dog using Compel – 70% cat, 30%
    dog](img/B21263_12_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.3: A blended photo of a cat and dog using Compel – 70% cat, 30% dog'
  prefs: []
  type: TYPE_NORMAL
- en: Compel can do more than just prompt blending; it can also provide `weighted`
    and `and` conjunction prompts. You can explore more usage examples and features
    in its Syntax Features [2] documentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'While it is easy to use Compel to blend prompts, as we have seen in the previous
    example, a blending prompt like the one that follows is strange and unintuitive
    for day-to-day use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Upon my initial review of the sample code from the Compel repository, I was
    intrigued by the following line: `("A photo of cat", "A photo of dog").blend(0.7,
    0.3)`. This string prompts several questions, such as how can the `blend()` function
    be invoked? However, it becomes clear that `blend()` is part of the prompt string
    and not a function that can be invoked within the Python code.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In contrast, the prompt blending or scheduling feature of Stable Diffusion
    WebUI [3] is relatively more user-friendly. The syntax allows us to achieve the
    same blending effect with a prompt syntax like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This scheduled prompt in Stable Diffusion WebUI will render a photo of a cat
    during the first 50% of the steps and a photo of a dog during the last 50% of
    the steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, you can use the `|` operator to alternate the prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The preceding scheduled prompt will alternate between rendering photos of a
    cat and a dog. In other words, it will render a photo of a cat in one step and
    a photo of a dog in the next step, continuing this pattern until the end of the
    entire rendering process.
  prefs: []
  type: TYPE_NORMAL
- en: These two scheduling features can also be achieved by Diffusers. In the following
    section, we will explore how to implement both of these advanced prompt scheduling
    features for Diffusers.
  prefs: []
  type: TYPE_NORMAL
- en: Building a custom scheduled prompt pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we discussed in [*Chapter 5*](B21263_05.xhtml#_idTextAnchor097), the generation
    process utilizes input prompt embedding to denoise an image at each step. By default,
    every denoising step employs the exact same embedding. However, to gain more precise
    control over the generation, we can modify the pipeline code to supply unique
    embeddings for each denoising step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take, for instance, the following prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'During a total of 10 denoising steps, we hope the pipeline can remove noise
    in the first 5 steps to reveal `A photo of cat`, and the following 5 steps to
    reveal `A photo of dog`. To make this happen, we will need to implement the following
    components:'
  prefs: []
  type: TYPE_NORMAL
- en: A prompt parser capable of extracting the scheduling number from the prompt
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A method to embed the prompts and create a list of prompt embeddings that matches
    the number of steps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A new `pipeline` class derived from the Diffusers pipeline, enabling us to incorporate
    our new functionality into the pipeline while preserving all existing features
    of the Diffusers pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, let’s implement the formatted prompt parser.
  prefs: []
  type: TYPE_NORMAL
- en: A scheduled prompt parser
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The open sourced Stable Diffusion WebUI project’s source code reveals that we
    can use `lark` [4] – a parsing toolkit for Python. We will also use the `lark`
    package to parse the scheduled prompt for our own prompt parser.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install `lark`, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The Stable Diffusion WebUI compatible prompt is defined in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: If you decide to get up to your neck in the syntax swamp to fully understand
    every line of the definition, the `lark` document [5] is the place to go.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll use the Python function from the SD WebUI code repository. This
    function utilizes the Lark `schedule_parser` syntax definition to parse the input
    prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the total denoising steps to 10, and give a shorter name, `g`, for this
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s throw some prompts to the function to see the parsing results:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Test #1: `cat`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code will parse the `cat` input text as the following string:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The result indicates that all 10 steps will use `cat` to generate the image.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Test #2: `[cat:dog:0.5]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Change the prompt to `[cat:dog:0.5]`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The function will generate the following result:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The result means using `cat` for the first 5 steps, and `dog` for the last 5
    steps.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Test #3: `[cat|dog]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The function also supports alternative scheduling. Change the prompt to `[cat
    | dog]`, with an “or” `|` operator in the middle of the two names:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The prompt parser will generate the following result:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In other words, it alternates the prompt for each denoising step.
  prefs: []
  type: TYPE_NORMAL
- en: So far, it works well in terms of parsing. However, before feeding it to the
    pipeline, additional work needs to be done.
  prefs: []
  type: TYPE_NORMAL
- en: Filling in the missing steps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In *Test #2*, the generated result includes only two elements. We need to expand
    the result list to cover each step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'This Python function, `parse_scheduled_prompts`, takes two arguments: `text`
    and `steps` (with a default value of 10). The function processes the given text
    to generate a list of prompts based on a learned conditioning schedule.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a step-by-step explanation of what the function does:'
  prefs: []
  type: TYPE_NORMAL
- en: Use a `try-except` block to call the `get_learned_conditioning_prompt_schedules`
    function with the processed text and the specified number of steps. The result
    is stored in `parse_result`. If there’s an exception – say, a syntax error, it
    will be caught and printed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the length of `parse_result` is 1, return `parse_result` as the final output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Loop through the range of steps and perform the following actions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get the current prompt step and content from `parse_result`.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Increment the loop counter `i` by `1` and store it in the variable step.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If `step` is less than the current prompt step, append the current prompt content
    to `prompts_list` and continue to the next iteration.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If `step` is equal to the current prompt step, append the current prompt content
    to `prompts_list` and remove the first element from `parse_result`.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Return the `prompts_list` as the final output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The function essentially generates a list of prompts based on the learned conditioning
    schedule, with each prompt being added to the list according to the specified
    number of steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s call this function to test it out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We will get a prompt list as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Five prompts for `cat`, and five prompts for `dog` – each denoising step will
    take one of the prompts.
  prefs: []
  type: TYPE_NORMAL
- en: A Stable Diffusion pipeline supporting scheduled prompts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Until now, all prompts are still in plain text form. We will need to use custom
    embedding code to encode unlimited and weighted prompts into embeddings, or we
    can simply use the default encoder from Diffusers to generate embeddings but with
    a 77-token limitation.
  prefs: []
  type: TYPE_NORMAL
- en: To make the logic easier and more concise to follow, we will use the default
    text encoder in this section. Once we figure out how it works, it will be easy
    to swap the encoder with the more powerful one we built in [*Chapter 10*](B21263_10.xhtml#_idTextAnchor197).
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we will perform a minor surgical operation on the original Diffusers
    Stable Diffusion pipeline to support this embedding list, the operation includes
    creating a new pipeline class inherited from the Diffusers pipeline. We can directly
    reuse the tokenizer and text encoder from the initialized pipeline by using the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'I will further explain the preceding code next. We will implement the whole
    logic in the `scheduler_call` function (similar to the `__call__` function of
    `StableDiffusionPipeline`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: This Python function, `scheduler_call`, is a method of the `StableDiffusionPipeline_EXT`
    class, which is a subclass of `StableDiffusionPipeline`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the steps to implement the whole logic:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Set the default scheduler to `EulerDiscreteScheduler` for a better generation
    result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Prepare the `device` and `do_classifier_free_guidance` parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Call the `parse_scheduled_prompts` function to have the `prompt_list` prompt
    list. This is the function we built in the previous section of this chapter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If no scheduled prompt is found, use the normal single-prompt logic:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In *step 4*, the function processes the input prompt(s) to generate the prompt
    embeddings. The input prompt can be a single string or a list of strings. The
    function first parses the input prompt(s) into a list called `prompt_list`. If
    there is only one prompt in the list, the function directly encodes the prompt
    using the `_encode_prompt` method and stores the result in `prompt_embeds`. If
    there are multiple prompts, the function iterates through `prompt_list` and encodes
    each prompt separately using the `_encode_prompt` method. The resulting embeddings
    are stored in `embedding_list`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Prepare timesteps for the diffusion process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Prepare latent variables to initialize the `latents` tensor (this is a PyTorch
    tensor):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Implement the denoising loop:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE111]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE112]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE113]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE114]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE115]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE116]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE117]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE118]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE119]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In *step 7*, the denoising loop iterates through the timesteps of the diffusion
    process. If prompt scheduling is enabled (i.e., there are multiple prompt embeddings
    in `embedding_list`), the function selects the appropriate prompt embedding for
    the current timestep. The length of `embedding_list` is stored in `prompt_embeds_l_len`.
    If `prompt_embeds_l_len` is greater than `0`, it means prompt scheduling is enabled.
    The function calculates the `pe_index` index for the current timestep, `i`, using
    the modulo operator (`%`). This ensures that the index wraps around the length
    of `embedding_list` and selects the appropriate prompt embedding for the current
    timestep. The selected prompt embedding is then assigned to `prompt_embeds`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The last step is denoising post-processing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE121]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE122]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE123]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the last step, we convert the image data from latent space to pixel space
    by calling the `decode_latents()` function. The `StableDiffusionPipelineOutput`
    class is used here for a consistent structure to be maintained when returning
    outputs from the pipeline. We use it here to make our pipeline compatible with
    the Diffusers pipeline. You can also find the complete code in the code files
    associated with this chapter.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Congratulations to you if you are still here! Let’s execute it to witness the
    result:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE125]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE126]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE127]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE128]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE129]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE130]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE131]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE132]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE133]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE134]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE135]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE136]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We should see an image like the one shown in *Figure 12**.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.4: A blended photo with 60% cat and 40% dog, using a custom scheduled
    prompt pipeline](img/B21263_12_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.4: A blended photo with 60% cat and 40% dog, using a custom scheduled
    prompt pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s another example, using an alternative prompt `[cat|dog]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE137]'
  prefs: []
  type: TYPE_PRE
- en: 'Our alternative prompt gives an image similar to *Figure 12**.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.5: A photo of a blended cat and dog, using the alternative prompt
    scheduling from our custom-scheduled prompt pipeline](img/B21263_12_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.5: A photo of a blended cat and dog, using the alternative prompt
    scheduling from our custom-scheduled prompt pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: If you see half-cat, half-dog images generated, as shown in *Figure 12**.4*
    and *Figure 12**.5*, you have successfully built your custom prompt scheduler.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced two solutions for conducting scheduled prompt
    image generation. The first solution, the `Compel` package, is the easiest one
    to use. Simply install the package, and you can use its prompt blend feature to
    blend two or more concepts in one prompt.
  prefs: []
  type: TYPE_NORMAL
- en: The second solution is a customized pipeline that first parses the prompt string
    and prepares a prompt list for each denoising step. The custom pipeline loops
    through the prompt list to create an embedding list. Finally, a `scheduler_call`
    function uses the prompt embedding from the embedding list to generate images
    with precise control.
  prefs: []
  type: TYPE_NORMAL
- en: If you successfully implement the custom scheduled pipeline, you can control
    generation in a more precise way. Speaking of controlling, in [*Chapter 13*](B21263_13.xhtml#_idTextAnchor257),
    we are going to explore another way to control image generation – ControlNet.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Compel: [https://github.com/damian0815/compel](https://github.com/damian0815/compel)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compel Syntax Features: [https://github.com/damian0815/compel/blob/main/doc/syntax.md](https://github.com/damian0815/compel/blob/main/doc/syntax.md)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Stable Diffusion WebUI Prompt Editing: [https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features#prompt-editing](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features#prompt-editing'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: 'Lark – a parsing toolkit for Python: [https://github.com/lark-parser/lark](https://github.com/lark-parser/lark)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Lark usage document: [https://lark-parser.readthedocs.io/en/stable/](https://lark-parser.readthedocs.io/en/stable/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Part 3 – Advanced Topics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Parts 1 and 2, we established a solid foundation for Stable Diffusion, covering
    its fundamentals, customization options, and optimization techniques. Now, it’s
    time to venture into more advanced territories, where we’ll explore cutting-edge
    applications, innovative models, and expert-level strategies to generate remarkable
    visual content.
  prefs: []
  type: TYPE_NORMAL
- en: The chapters in this part will take you on a thrilling journey through the latest
    developments in Stable Diffusion. You’ll learn how to generate images with unprecedented
    control using ControlNet, craft captivating videos with AnimateDiff, and extract
    insightful descriptions from images using powerful vision-language models such
    as BLIP-2 and LLaVA. Additionally, you’ll get acquainted with Stable Diffusion
    XL, a newer and more capable iteration of the Stable Diffusion model.
  prefs: []
  type: TYPE_NORMAL
- en: To top it off, we’ll delve into the art of crafting optimized prompts for Stable
    Diffusion, including techniques to write effective prompts and leverage large
    language models to automate the process. By the end of this part, you’ll possess
    the expertise to tackle complex projects, push the boundaries of Stable Diffusion,
    and unlock new creative possibilities. Get ready to unleash your full potential
    and produce breathtaking results!
  prefs: []
  type: TYPE_NORMAL
- en: 'This part contains the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 13*](B21263_13.xhtml#_idTextAnchor257)*, Generating Images with ControlNet*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 14*](B21263_14.xhtml#_idTextAnchor277)*, Generating Video Using Stable
    Diffusion*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 15*](B21263_15.xhtml#_idTextAnchor289)*, Generating Image Descriptions
    Using BLIP-2 and LLaVA*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 16*](B21263_16.xhtml#_idTextAnchor309)*, Exploring Stable Diffusion
    XL*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 17*](B21263_17.xhtml#_idTextAnchor335)*, Building Optimized Prompts
    for Stable Diffusion*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
