<html><head></head><body>

		<p>&#13;
			<h1 class="chapter-number" id="_idParaDest-41"><a id="_idTextAnchor041"/><a id="_idTextAnchor042"/>3</h1>&#13;
			<h1 id="_idParaDest-42"><a id="_idTextAnchor043"/>Large Language Models</h1>&#13;
			<p>Language models are computational algorithms designed to process, understand, and generate natural language. The study, research, and development of these algorithms is known as <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>). NLP predates the field of <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) and can be traced back to the 1950s and the development of the first computers. While the first language models relied heavily on rule-based approaches, NLP shifted in the 1980s toward statistical methods and began to converge with ML. The increase in computational power and text corpora led to the development of deep learning and neural network-based language models in the early 21st century, which have seen significant progress over the last decade.</p>&#13;
			<p>Language models have a variety of applications in NLP for understanding and generating natural languages as well as more formal languages, such as programming and database query languages. Their use cases include tasks such as text labeling and sentiment analysis, translation, summarization, information extraction, and question answering. With the advent of <strong class="bold">large language models</strong> (<strong class="bold">LLMs</strong>), applications have further expanded to develop conversational chat systems and personal assistants, software development agents, and general problem-solvers. In this chapter, you’ll deep dive into the essential concepts and implementation of LLMs.</p>&#13;
			<p>This chapter will cover the following topics:</p>&#13;
			<ul>&#13;
				<li>Language modeling with n-gram models to provide a probabilistic viewpoint</li>&#13;
				<li><strong class="bold">Artificial neural networks</strong> (<strong class="bold">ANNs</strong>), their architecture, and training paradigm</li>&#13;
				<li>The application of ANNs to the language modeling domain</li>&#13;
				<li>The Transformer architecture</li>&#13;
				<li>LLMs in practice</li>&#13;
			</ul>&#13;
			<h1 id="_idParaDest-43"><a id="_idTextAnchor044"/>Technical requirements</h1>&#13;
			<p>This chapter is largely theoretical, with a short code snippet in Python to illustrate the <code>tiktoken</code> tokenizer library. To follow along, you will need access to a computer with Python version 3.8 or later.</p>&#13;
			<p>To make the most of this chapter, you will need proficiency with Python and the <code>pip</code> package manager. You will also need a basic knowledge of probabilities, calculus, and software development concepts such as APIs.</p>&#13;
			<h1 id="_idParaDest-44"><a id="_idTextAnchor045"/>Probabilistic framework</h1>&#13;
			<p>When building AI-intensive applications that interact with LLMs, you will likely come across API parameters relating to probabilities of tokens. To understand how LLMs relate to the concept of probabilities, this section introduces the probabilistic framework underpinning language models.</p>&#13;
			<p>Language modeling is typically done with a probabilistic view in mind, rather than in absolute and deterministic terms. This allows the algorithms to deal with the uncertainty and ambiguity often found in natural language.</p>&#13;
			<p>To build an intuitive understanding of probabilistic language modeling, consider the following start of a sentence, for which you want to predict the next word:</p>&#13;
			<pre class="source-code">&#13;
The</pre>			<p>This is obviously an ambiguous task with many possible answers. The article <em class="italic">the</em> is a very common and generic word in the English language, and the possibilities are endless. Any noun, such as <em class="italic">house</em>, <em class="italic">dog</em>, <em class="italic">spoon</em>, etc. could be a valid possible continuation of the sentence. Even adjectives such as <em class="italic">big</em>, <em class="italic">green</em>, and <em class="italic">lazy</em> are likely candidates. Conversely, there are words rarely seen after an article, including verbs, such as <em class="italic">eat</em>, <em class="italic">see</em>, and <em class="italic">learn</em>.</p>&#13;
			<p>To deal with this kind of uncertainty, consider instead a slightly different question: “What is the probability of each word to come next?”</p>&#13;
			<p>The answer to <em class="italic">this</em> question is no longer a single word, but instead a large lookup table, assigning each word in the vocabulary a number, which represents the probability of this word following <em class="italic">the</em>. If this lookup table is representative of the English language, one would expect nouns and adjectives to have a higher probability than verbs. <em class="italic">Table 3.1</em> shows what such a table could look like, using made-up values for the <em class="italic">Probability</em> column. You will see shortly how these probabilities can be calculated from a text corpus:</p>&#13;
			<table class="No-Table-Style _idGenTablePara-1" id="table001-1">&#13;
				<colgroup>&#13;
					<col/>&#13;
					<col/>&#13;
					<col/>&#13;
				</colgroup>&#13;
				<tbody>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p><strong class="bold">Previous word</strong></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p><strong class="bold">Next word</strong></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p><strong class="bold">Probability</strong></p>&#13;
						</td>&#13;
					</tr>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p>…</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>…</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>…</p>&#13;
						</td>&#13;
					</tr>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p>the</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>house</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>0.012%</p>&#13;
						</td>&#13;
					</tr>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p>the</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>dog</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>0.013%</p>&#13;
						</td>&#13;
					</tr>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p>the</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>spoon</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>0.007%</p>&#13;
						</td>&#13;
					</tr>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p>…</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>…</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>…</p>&#13;
						</td>&#13;
					</tr>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p>the</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>big</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>0.002%</p>&#13;
						</td>&#13;
					</tr>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p>the</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>green</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>0.001%</p>&#13;
						</td>&#13;
					</tr>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p>the</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>lazy</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>0.001%</p>&#13;
						</td>&#13;
					</tr>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p>…</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>…</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>…</p>&#13;
						</td>&#13;
					</tr>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p>the</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>eat</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>0.000%</p>&#13;
						</td>&#13;
					</tr>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p>the</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>see</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>0.000%</p>&#13;
						</td>&#13;
					</tr>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p>the</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>learn</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>0.000%</p>&#13;
						</td>&#13;
					</tr>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p>….</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>..</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>…</p>&#13;
						</td>&#13;
					</tr>&#13;
				</tbody>&#13;
			</table>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 3.1: A partial lookup table for words following the word <em class="italic">the</em></p>&#13;
			<p>In this simple example, one (but not the only) way to decide which word comes next is to scan through this lookup table and find the word with the highest probability. This method, known as <strong class="bold">greedy selection</strong>, would suggest that the word <em class="italic">dog</em> is the most probable continuation of the sentence. However, it’s important to note that there are many possibilities, each with a different probability. For instance, the word <em class="italic">house</em> is also a close second in terms of probabilities, indicating that it could also be a likely continuation of the sentence.</p>&#13;
			<p>To capture the flexibility and expressiveness of natural language, language models operate in terms of probabilities, and the process of training a language model means assigning probabilities for each word continuing the sentence thus far.</p>&#13;
			<p>Assume you have gone through the process of selecting the next word several times, and find yourself further along in the sentence:</p>&#13;
			<pre class="source-code">&#13;
The quick brown fox jumps over the</pre>			<p>How does this sentence continue? What does the probability distribution look like now?</p>&#13;
			<p>If you are familiar with this sentence<a class="_idFootnoteLink _idGenColorInherit" href="B22495_03.xhtml#footnote-000">1</a>, you’ll agree that at this point, the probability for the word <em class="italic">lazy</em> will stand out above all others. Your internal language model can’t help but autocomplete the entire sentence, and the words <em class="italic">lazy dog</em> will just pop into your head.</p>&#13;
			<div>&#13;
				<p class="Footnote"><a class="_idFootnoteAnchor _idGenColorInherit" href="B22495_03.xhtml#footnote-000-backlink">1</a>	This sentence is a pangram. A pangram contains every letter of the alphabet at least once. The sentence has been used in various contexts, such as typing practice and testing the display of text in computers.</p>&#13;
			</p>&#13;
			<p>But why is that? Aren’t you in the same situation as before, asking what follows next after <em class="italic">the</em>? The key difference here is that you have more context; you see more of the sentence, which demonstrates that considering only the preceding word is not sufficient to build a good predictor of the next word. Yet this basic concept marks the very beginning of language models and can be viewed as a distant ancestor of the likes of ChatGPT and other modern LLMs.</p>&#13;
			<h2 id="_idParaDest-45"><a id="_idTextAnchor046"/>n-gram language models</h2>&#13;
			<p>One of the first formalisms of a language model is an <strong class="bold">n-gram model</strong>, a simple statistical language model, first published in 1948 in Claude Shannon’s famous paper <em class="italic">A Mathematical Theory of </em><em class="italic">Communication</em> (<a href="https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf">https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf</a>).</p>&#13;
			<p>An n-gram language model can be described as a giant lookup table, where the model considers the last <code>n-1</code> words to predict the next. For <code>n=2</code>, you get a so-called bigram model, looking back only one word, as shown in <em class="italic">Table 3.1</em>.</p>&#13;
			<p>As the sentence in the previous example illustrated, such a simple bigram model is limited and fails to capture the nuances of natural language. However, before exploring what happens when <code>n</code> is scaled up to larger values, let’s briefly discuss how you would train a bigram model, which is to say, how to calculate the probabilities for each pair of words in the table:</p>&#13;
			<ol>&#13;
				<li>Take a large corpus of text, such as the collection of all Wikipedia pages in English.</li>&#13;
				<li>Scan through this text and count the occurrences of single words as well as observed pairs of words.</li>&#13;
				<li>Record all counts in a lookup table.</li>&#13;
				<li>Calculate the probability of word <img alt="" class="formula-image1" role="presentation" src="img/B22495_03_Equations.png"/> following word <img alt="" class="formula-image1" role="presentation" src="img/B22495_03_Equations1.png"/> as follows: divide the count for the word pair <img alt="" class="formula-image4" role="presentation" src="img/B22495_03_Equations2.png"/> by the count of the single word <img alt="" class="formula-image1" role="presentation" src="img/B22495_03_Equations1.png"/>.</li>&#13;
			</ol>&#13;
			<p>For example, to calculate the probability of seeing the word <em class="italic">dog</em> following the word <em class="italic">the</em>, divide the pair count by the single word count in the following way: </p>&#13;
			<p><img alt="" class="formula-image2" role="presentation" src="img/B22495_03_Equations3.png"/></p>&#13;
			<p>Here, the term <img alt="" class="formula-image4" role="presentation" src="img/B22495_03_Equations4.png"/> is pronounced as “probability of x given y.” In other words, the probability of seeing the word <em class="italic">dog</em> given we’ve just seen the word <em class="italic">the</em> is the count of seeing the words in combination (the numerator) divided by all counts of seeing <em class="italic">the</em> by itself (the denominator).</p>&#13;
			<p>Thus, the training process of an n-gram language model only requires a single pass over the text, counting all occurring n-grams and (n-1)-grams, and storing the numbers in a table.</p>&#13;
			<p>In practice, several tricks improve the quality of n-gram models, such as including special <code>&lt;start&gt;</code> and <code>&lt;end&gt;</code> markers at the beginning and end of each sentence, splitting words into smaller sub-words, such as <em class="italic">playing</em> into <em class="italic">play</em> and <em class="italic">-ing</em>, and many other improvements. You will review some of these techniques later in the <em class="italic">Tokenization</em> section, and they apply to modern LLMs as well.</p>&#13;
			<p>Let’s now revisit the choice of <code>n</code>. As you have seen, a low value, such as <code>n=2</code>, doesn’t yield a very good language model. Is it just a matter of scaling up <code>n</code> until you reach the level of desired quality?</p>&#13;
			<p>A larger <code>n</code> value can capture more context and leads to a more predictive model. For <code>n=8</code>, the model can look back at the last seven words. The lookup table, as shown in <em class="italic">Table 3.2</em>, would contain a row that captures the example sentence:</p>&#13;
			<table class="No-Table-Style _idGenTablePara-1" id="table002">&#13;
				<colgroup>&#13;
					<col/>&#13;
					<col/>&#13;
					<col/>&#13;
				</colgroup>&#13;
				<tbody>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p><strong class="bold">Previous </strong><strong class="bold">7 words</strong></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p><strong class="bold">Next word</strong></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p><strong class="bold">Probability</strong></p>&#13;
						</td>&#13;
					</tr>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p>…</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>…</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>…</p>&#13;
						</td>&#13;
					</tr>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p>the quick brown fox jumps over the</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>lazy</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>99.381%</p>&#13;
						</td>&#13;
					</tr>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p>….</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>..</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>…</p>&#13;
						</td>&#13;
					</tr>&#13;
				</tbody>&#13;
			</table>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 3.2: A possible entry in the lookup table for an 8-gram</p>&#13;
			<p>However, increasing <code>n</code> to large values has several challenges, which make this approach infeasible in practice.</p>&#13;
			<p>The size of the lookup table grows exponentially with a larger <code>n</code>. The <em class="italic">Oxford English Dictionary</em> contains approximately 273,000 English words (<a href="https://en.wikipedia.org/wiki/List_of_dictionaries_by_number_of_words">https://en.wikipedia.org/wiki/List_of_dictionaries_by_number_of_words</a>), which allows for <img alt="" class="formula-image" role="presentation" src="img/B22495_03_Equations5.png"/> 74.5 billion possible combinations of two words (though many of these combinations would never be seen in a text). Increasing the n-gram model to <code>n=8</code>, the possible combinations of eight words grows to the astronomical number of <img alt="" class="formula-image2" role="presentation" src="img/B22495_03_Equations6.png"/>. Storing an entry in the table for each combination would be impossible as this number far exceeds all available hard drive storage space in the world, especially since the world’s collective data is estimated to reach 175 zettabytes = <img alt="" class="formula-image" role="presentation" src="img/B22495_03_Equations7.png"/> bytes by 2025 (<a href="https://www.networkworld.com/article/966746/idc-expect-175-zettabytes-of-data-worldwide-by-2025.html">https://www.networkworld.com/article/966746/idc-expect-175-zettabytes-of-data-worldwide-by-2025.html</a>). Of course, most of these word combinations would never be encountered, and you could choose to omit unseen n-grams in the table.</p>&#13;
			<p>This challenge, known as the <code>n</code>, the probability of encountering any one n-gram shrinks exponentially. Most combinations of <code>n</code> words would never be encountered for any realistic size of training dataset. When processing text that is not part of the training corpus, the model would assign zero probability for unseen n-grams. The model would not be able to make meaningful predictions in this case, and this problem would be exacerbated the larger <code>n</code> became.</p>&#13;
			<p>In summary, while n-grams have their uses for certain narrow applications and educational purposes, the language models of today have evolved beyond purely statistical approaches. LLMs use machine learning techniques to deal with some of the issues pointed out above, which you’ll learn about in the next section.</p>&#13;
			<h1 id="_idParaDest-46"><a id="_idTextAnchor047"/>Machine learning for language modelling</h1>&#13;
			<p>Before diving into language modeling approaches using ML, this section first introduces some general ML concepts and gives a high-level overview of different neural network architectures.</p>&#13;
			<p>At its core, ML is a field concerned with developing and studying algorithms that learn from data. Rather than executing hardcoded rules, the system is expected to <em class="italic">learn by example</em>, looking at provided inputs and desired outcomes (often referred to as <strong class="bold">targets</strong> in ML literature) and adjusting its behavior during the training process to change its outputs to closely resemble the user-provided targets.</p>&#13;
			<p>ML algorithms are roughly differentiated into three groups:</p>&#13;
			<ul>&#13;
				<li>Supervised learning</li>&#13;
				<li>Unsupervised learning</li>&#13;
				<li>Reinforcement learning</li>&#13;
			</ul>&#13;
			<p>Each of these groups has different learning objectives and problem formulations. For language modeling, you can mainly consider supervised (and related self-supervised) algorithms.</p>&#13;
			<h2 id="_idParaDest-47"><a id="_idTextAnchor048"/>Artificial neural networks</h2>&#13;
			<p>One class of supervised learning algorithms is <strong class="bold">artificial neural networks</strong> (<strong class="bold">ANNs</strong>). All modern LLMs are variations of the basic ANN architecture. When you make an API call to a model such as GPT-4, your question flows through an ANN to produce the answer. These models have evolved in size and complexity over decades, but the core principles and building blocks remain the same.</p>&#13;
			<p>The neural architectures found in human brains may have inspired the original design of ANNs, but ANNs are significantly different from their biological counterparts.</p>&#13;
			<p>ANNs are made of many smaller units called neurons, which are interconnected with each other in various patterns, depending on the network architecture. Each neuron is a small processing unit, receiving numeric signals from other neurons and passing a (modified) signal to its successor neurons, analogous to biological neurons. ANNs have tunable parameters, referred to as <strong class="bold">weights</strong>, which sit on the connections between two neurons and can influence the signal passing between them.</p>&#13;
			<p>One of the most basic ANN architectures is the so-called <strong class="bold">feed-forward network</strong> (<strong class="bold">FFN</strong>), depicted in <em class="italic">Figure 3</em><em class="italic">.1</em>. In this architecture, neurons are arranged in layers, starting with an input layer, followed by one or more hidden layers, and finally an output layer. The layer size, which refers to the number of neurons per layer, can vary. Input and output layer sizes are determined by the specific problem domain. For example, you may want to learn a mapping from a two-dimensional input (say, the body mass index and age of a person) to a one-dimensional output (say, the daily resting calories burnt). The size of hidden layers is often chosen arbitrarily through experimentation in a process called <strong class="bold">hyper-parameter tuning</strong>.</p>&#13;
			<p>In FFNs, every neuron in one layer connects to all neurons in the following layer, leading to a many-to-many relationship between two consecutive layers. <em class="italic">Figure 3</em><em class="italic">.1</em> shows an FFN architecture with one input layer (Layer 1), two hidden layers (Layers 2 and 3), and one output layer (Layer 4):</p>&#13;
			<p class="IMG---Figure"><img alt="A diagram of a network&#10;&#10;Description automatically generated" src="img/B22495_03_01.png"/></p>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.1: A feed-forward neural network architecture</p>&#13;
			<p>Zooming in on the functioning of a single neuron, <em class="italic">Figure 3</em><em class="italic">.2</em> shows a neuron with inputs from two other neurons (denoted <img alt="" class="formula-image1" role="presentation" src="img/B22495_03_Equations8.png"/> and <img alt="" class="formula-image1" role="presentation" src="img/B22495_03_Equations9.png"/>). The connections to the neuron contain the weights (denoted <img alt="" class="formula-image1" role="presentation" src="img/B22495_03_Equations10.png"/> and <img alt="" class="formula-image1" role="presentation" src="img/B22495_03_Equations.png"/>). The inputs are first multiplied with their corresponding weight and then summed up. The resulting sum is passed through a non-linear activation function <img alt="" class="formula-image1" role="presentation" src="img/B22495_03_Equations11.png"/> and the result forms the output of the neuron (shown as <img alt="" class="formula-image1" role="presentation" src="img/B22495_03_Equations12.png"/>). In mathematical terms, this is expressed as follows: <img alt="" class="formula-image2" role="presentation" src="img/B22495_03_Equations13.png"/></p>&#13;
			<p>While the specifics of activation functions are out of scope for this chapter, suffice it to say that non-linearity is important for the network to be able to learn complex patterns in the data.</p>&#13;
			<p>&#13;
				<div>&#13;
					<img alt="" role="presentation" src="img/B22495_03_02.jpg"/>&#13;
				</p>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.2: Activation of a single neuron with two inputs</p>&#13;
			<p>During a forward pass through the network, you present the input data (for example, the BMI and the age of a person) at the input layer, calculate the activations of all neurons of the layer, pass these activations to the next layer, and so forth, until the output layer produces an outcome (which, in this example, you can interpret as the model’s prediction for calories burnt by a person).</p>&#13;
			<p>It may seem surprising that the simple activation functions governing individual neurons in a neural network can lead to complex pattern recognition capabilities. This phenomenon is rooted in the <strong class="bold">universal approximation theorem</strong>, which proves that a neural network with enough hidden layers and neurons can approximate any continuous function to any desired degree of accuracy.</p>&#13;
			<p>You now know how data flows forward in an ANN from input to output layer. For an untrained model, this is only the first of three phases. In the next section, you’ll learn about the other two phases required to train an ANN: loss calculation and a backward pass.</p>&#13;
			<h2 id="_idParaDest-48"><a id="_idTextAnchor049"/>Training an artificial neural network</h2>&#13;
			<p>So far, this chapter has described the forward pass of a network, that is, how a response for a given input is calculated. Since the initial weights of an ANN are chosen randomly, the output of an untrained network is also random and nonsensical. The weights need to be adjusted during the training process.</p>&#13;
			<p>The goal of training a neural network is to make its outputs match the provided targets for any given input. Thus, for supervised learning, a training dataset consists of input/target pairs of known correct responses. In the example of predicting the calories burnt given a person’s BMI and age, the training dataset would consist of many measurements of people’s BMI and age (the inputs) and their measured calories burnt (the targets). The more measurements the dataset contains, the better the model can learn patterns from the relationship between inputs and targets.</p>&#13;
			<p>The training process for an ANN can be broken down into three phases, as illustrated in <em class="italic">Figure 3</em><em class="italic">.3</em>:</p>&#13;
			<ol>&#13;
				<li><strong class="bold">Forward pass</strong>: Calculating the outputs from the inputs.</li>&#13;
				<li><strong class="bold">Loss calculation</strong>: Calculating an error signal between the outputs and the desired targets.</li>&#13;
				<li><strong class="bold">Backward pass and weight adjustment</strong>: Propagating the error back through the model and adjusting each of the weights.</li>&#13;
			</ol>&#13;
			<p>&#13;
				<div>&#13;
					<img alt="" role="presentation" src="img/B22495_03_03.jpg"/>&#13;
				</p>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.3: The three phases of training an ANN</p>&#13;
			<p>This process is repeated over multiple passes of the dataset, until the weight parameters no longer meaningfully change. At this point, the model is said to have converged and is ready for inference.</p>&#13;
			<p>Training starts with a <strong class="bold">forward pass</strong> of the data, passing in the inputs and recording the network’s output. As this output may differ from the correct target (especially for an untrained network with random weights), it is possible to calculate a metric called <strong class="bold">loss</strong>, which is a scalar number that reflects the difference between actual and desired output.</p>&#13;
			<p>The loss is required to execute the <strong class="bold">backward pass</strong>. This step will adjust all weights of the network in such a way that the network will produce an output closer to the target for the given input. The activation for each neuron is a well-formed differentiable expression with sums, products, and a differentiable activation function. This means that the derivative of a weight with respect to the loss can be calculated by the rules of calculus to determine how each weight parameter needs to be adjusted to minimize the loss.</p>&#13;
			<p>This gradient calculation is then propagated backward to previous layers using the chain rule of calculus, all the way to the input layer. Having calculated the gradients for each weight in this way, the weights can then be updated. Controlled by a parameter called the <strong class="bold">learning rate</strong>, the weights can be moved a small step toward the direction of minimizing the loss.</p>&#13;
			<p>While it’s possible to execute this loop of forward and backward passes for every single entry in the training set one by one, in practice the training set is split into small batches. A <strong class="bold">batch</strong> may contain tens, hundreds, or even thousands of data points. The <strong class="bold">batch size</strong> is another hyper-parameter chosen experimentally through hyper-parameter tuning before the actual training process. Batching up the data in such a manner serves these purposes:</p>&#13;
			<ul>&#13;
				<li>It leads to higher efficiency as batches can be processed in parallel, especially on specialized hardware, such as <strong class="bold">graphical processing </strong><strong class="bold">units</strong> (<strong class="bold">GPUs</strong>).</li>&#13;
				<li>The error gradients backpropagated through the network are averaged across each batch. This leads to more stable training as single outliers in the data have less impact on the weight changes.</li>&#13;
			</ul>&#13;
			<p>Training continues until the model no longer improves on unseen validation data.</p>&#13;
			<p>After training, the trained model can then be applied to previously unseen inputs. For example, the model can be integrated into a fitness tracking app, where it predicts burnt calories based on a person’s BMI and age with the expectation that it will not only work for measurements in the training data but also generalize to new data points as well. This application of a trained model to new data is known as <strong class="bold">inference</strong>.</p>&#13;
			<p>This training procedure is at the core of every neural network, including LLMs. As neural networks operate on numeric data, the next section will show how language can be represented numerically to make it compatible with the use of ANNs.</p>&#13;
			<h1 id="_idParaDest-49"><a id="_idTextAnchor050"/>ANNs for natural language processing</h1>&#13;
			<p>The previous section showed how ANNs can learn mappings of numerical inputs to numerical outputs. Language, however, is inherently non-numeric: a sentence is a sequence of discrete words from a large vocabulary. Building a neural network-based word predictor poses the following challenges:</p>&#13;
			<ul>&#13;
				<li>The inputs to the model are discrete words. Since ANNs operate on numeric inputs and outputs, a suitable mapping from words to numbers and vice versa is required.</li>&#13;
				<li>The inputs are further sequential. Unlike bigrams, the model should be able to take more than one word into account when predicting the next word.</li>&#13;
				<li>The output of the language model needs to be a probability distribution over all possible next words. To form a proper distribution, the outputs need to be normalized to be non-negative and sum up to one.</li>&#13;
			</ul>&#13;
			<p>The following sections will explain these challenges and review how they are addressed in modern language models.</p>&#13;
			<h2 id="_idParaDest-50"><a id="_idTextAnchor051"/>Tokenization</h2>&#13;
			<p>The first processing step to convert text to numeric inputs is called <strong class="bold">tokenization</strong>. During this phase, words are split into common sub-words, characters, and punctuation marks, making up the vocabulary of tokens. Each token is then assigned a unique integer ID.</p>&#13;
			<p>When interacting with LLMs, especially when dealing with self-hosted open-source models, the choice of tokenizer is important and must match exactly the one used during the training of the model. Luckily, many common open-source tokenizers exist. Even commercial LLM providers, such as OpenAI, have open-sourced their tokenizer libraries to make it easier to interact with their models. Bindings of OpenAI’s <code>tiktoken</code> library are available for many popular programming languages, including Python, C#, Java, Go, and Rust.</p>&#13;
			<p>The following code example demonstrates the use of the <code>tiktoken</code> Python library. After installing the package with <code>pip install tiktoken</code>, you can create an <code>encoder</code> object and encode any text, which will return a list of token IDs. The following code snippet tokenizes the sentence <em class="italic">tiktoken is a popular tokenizer!</em> and decodes each token ID back into its byte string:</p>&#13;
			<pre class="source-code">&#13;
import tiktoken&#13;
# use the gpt-4 tokenizer 'cl100k_base'encoder = tiktoken.get_encoding("cl100k_base")&#13;
token_ids = encoder.encode("tiktoken is a popular tokenizer!")&#13;
print("Token IDs", token_ids)&#13;
tokens = [encoder.decode_single_token_bytes(t) for t in token_ids]print("Tokens", tokens)</pre>			<p>Running this code produces the following output:</p>&#13;
			<pre class="source-code">&#13;
Token IDs [83, 1609, 5963, 374, 264, 5526, 47058, 0]&#13;
Tokens [b't', b'ik', b'token', b' is', b' a', b' popular', b' tokenizer', b'!']</pre>			<p>You can see that the word <em class="italic">tiktoken</em> was split into three tokens, <em class="italic">t</em>, <em class="italic">ik</em>, and <em class="italic">token</em>, likely because the word itself is not common enough to warrant its own token in the vocabulary. Also of note is that whitespace is often encoded as part of a token, at the beginning, such as in “ <em class="italic">is</em>.”</p>&#13;
			<p>When interacting with proprietary models via APIs, tokenization typically happens automatically and server-side. This means that you can submit prompts in text form without having to tokenize the inputs yourself. However, <code>tiktoken</code> and similar libraries are still useful tools when building AI-powered applications. For example, you can use them to calculate the number of tokens of a request, as API calls are usually charged by the number of submitted and returned tokens. Additionally, language models have an upper token limit for their inputs, known as their <strong class="bold">context size</strong>. Requests that are too large may fail or get truncated, which impacts the model’s response.</p>&#13;
			<p>For the purposes of developing applications with LLMs, it is sufficient to know about tokenization when it comes to the preprocessing of text. However, this is only the first step in making neural networks understand textual inputs. Even though the token IDs are numeric, the assignment from the token to its ID happens arbitrarily. Neural networks interpret their inputs geometrically and are not well suited to processing large integer numbers. In the second step, called embedding, these integers are converted into high-dimensional floating-point vectors, also known as <strong class="bold">embedding vectors</strong> or simply <strong class="bold">embeddings</strong>.</p>&#13;
			<h2 id="_idParaDest-51"><a id="_idTextAnchor052"/>Embedding</h2>&#13;
			<p><strong class="bold">Embedding</strong> is the process of mapping data into a high-dimensional vector space. This concept is not just relevant to the training of language models but will also play an important role for vector databases to retrieve semantically similar items, which we’ll discuss later in <a href="B22495_05.xhtml#_idTextAnchor115"><em class="italic">Chapter 5</em></a>, <em class="italic">Vector Databases</em>. Embeddings can be created for arbitrary data entities: words, sentences, entire documents, images, or even more abstract concepts, such as users or products in the context of building recommender systems.</p>&#13;
			<p>The purpose of embeddings is twofold:</p>&#13;
			<ul>&#13;
				<li>They are fixed-length floating-point representations of their corresponding entities, ideally suited to be processed by neural networks.</li>&#13;
				<li>Embeddings are coordinates in a vector space. With the right choice (or, rather, training), embeddings can represent semantic similarities of data entities through their geometric proximity. This enables the use of geometric algorithms, such as clustering or nearest neighbor search, to operate on the semantic meaning of the embedded data.</li>&#13;
			</ul>&#13;
			<p>Embeddings are a fundamental concept at the core of language models and vector search. To understand how tokens can be embedded, let’s assume a small vector space with only three dimensions, as illustrated in <em class="italic">Figure 3</em><em class="italic">.4</em>. To map a token into this space, a random point in this space is assigned to each token. Here, the token is represented by its integer ID, and the random point in this space is indicated by its x, y, and z coordinates. The mapping is done with the help of an embedding matrix consisting of <code>n</code> rows and <code>d</code> columns, initialized with random floating-point numbers. Here, <code>n</code> is the size of the vocabulary and <code>d</code> is the embedding dimensionality (in this example, <code>d</code> equals <code>3</code>). To retrieve the coordinates for a token, the token ID is used as a row index into the embedding matrix, returning a d-dimensional vector. For example, the token <em class="italic">fox</em> may be assigned the following coordinates: <code>[-0.241, </code><code>1.356, -0.7882]</code>.</p>&#13;
			<p class="IMG---Figure"><img alt="A diagram of a fox&#10;&#10;Description automatically generated" src="img/B22495_03_04.png"/></p>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.4: Visual representation of tokens embedded in a three-dimensional vector space</p>&#13;
			<p>Just like the weights of a neural network are assigned randomly before training, the values of the embedding matrix are also chosen randomly. Furthermore, and this is a crucial step in the training of LLMs, the embedding matrix values are treated as additional learnable parameters of the neural network. By allowing the gradients to flow all the way back into the embedding layer, the model can update the positions of the token coordinates during training in such a way that it helps the prediction task.</p>&#13;
			<p>Studies on fully trained embedding layers of LLMs reveal that the model moves semantically similar tokens close together. In the earlier example, you might find a cluster of nouns (<em class="italic">fox</em>, <em class="italic">dog</em>) or a cluster of adjectives (<em class="italic">quick</em>, <em class="italic">lazy</em>, <em class="italic">brown</em>). However, with only three dimensions, similarity is limited to only three attributes by which tokens can be compared. LLMs use vector spaces with much larger dimensionality, often in the order of hundreds or even thousands of dimensions. In such a high-dimensional space, tokens can relate to each other (and be close to each other geometrically) in many ways. Some of the dimensions may have interpretable meanings, such as the sentiment of a word. However, most of them are likely to make sense only to the model internally. </p>&#13;
			<p>In this section, you have seen how text is prepared for neural network training by splitting it into tokens and assigning token IDs, which can be used as an index to find the corresponding embedding vector in the embedding matrix. These vectors have geometric meaning and can be updated as part of the training phase. Next, you’ll learn how the outputs of the neural network can be interpreted as probabilities of choosing the next token.</p>&#13;
			<h2 id="_idParaDest-52"><a id="_idTextAnchor053"/>Predicting probability distributions</h2>&#13;
			<p>As you have seen in the <em class="italic">n-gram language models</em> section, the model needs to output a probability distribution over the next tokens, that is, one numeric value for each token in the vocabulary. By choosing an output layer size matching the vocabulary size, the neural network will give you the right output <em class="italic">shape</em>, but these numbers can theoretically be any real number, including negative or very large positive numbers.</p>&#13;
			<p>To form a proper probability distribution, the outputs must meet two additional conditions:</p>&#13;
			<ul>&#13;
				<li>The outputs must be non-negative.</li>&#13;
				<li>The outputs must sum up to 1.0.</li>&#13;
			</ul>&#13;
			<p>A special activation function called <strong class="bold">softmax</strong> has been designed for this exact purpose and is used for the output layer when probabilities are expected.</p>&#13;
			<p>The mathematical formulation of the softmax function is as follows: <img alt="" class="formula-image2" role="presentation" src="img/B22495_03_Equations14.png"/></p>&#13;
			<p>Intuitively, the application of the exponential function in the numerator maps the range from negative to positive infinity to that of non-negative numbers (<img alt="" class="formula-image4" role="presentation" src="img/B22495_03_Equations15.png"/> for all x). By dividing by the sum of all exponents, you normalize the values to ensure that the sum of outputs exactly adds up to 1.</p>&#13;
			<p>The targets for training the model also need to contain vectors of the same length (one value per token). Since the next word at each step in the token sequence is known, you can encode the correct token with <strong class="bold">one-hot encoding</strong>. You can assign a value of 1.0 to the correct token’s position in the vector and 0.0 to all other positions, as shown in <em class="italic">Figure 3</em><em class="italic">.5</em>. This ensures that during the backward pass, the probability of seeing the correct next token is increased while all other probabilities are decreased.</p>&#13;
			<p>&#13;
				<div>&#13;
					<img alt="" role="presentation" src="img/B22495_03_05.jpg"/>&#13;
				</p>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.5: Example output probabilities as predicted by the model and targets for the token <em class="italic">fox</em></p>&#13;
			<p>With tokenization, embedding, and softmax activation, you can convert language into a numeric format that an ANN can understand. Further, the ANN can interpret the numeric outputs of the model as a discrete probability distribution over the next token. The final missing piece to model language with ANNs is the processing of sequences, which are discussed next.</p>&#13;
			<h1 id="_idParaDest-53"><a id="_idTextAnchor054"/>Dealing with sequential data</h1>&#13;
			<p>To produce good next-token predictions, a language model needs to be able to consider a sizeable context, reaching back many words or even sentences.</p>&#13;
			<p>To demonstrate this, consider the following text:</p>&#13;
			<p><em class="italic">A solitary tiger stealthily stalks its prey in the dense jungle. The underbrush whispers as </em><strong class="bold">it</strong><em class="italic"> attacks, concealing </em><strong class="bold">its</strong><em class="italic"> advance toward an </em><em class="italic">unsuspecting fawn.</em></p>&#13;
			<p>The second sentence in this example contains two pronouns, <em class="italic">it</em> and <em class="italic">its</em> (shown in bold above), both referring to the <em class="italic">tiger</em> from the previous sentence, many words apart. But without seeing the first sentence, you’d likely assume that <em class="italic">it</em> refers to the underbrush instead, which would have led to a very different sentence ending, such as this one:</p>&#13;
			<p><em class="italic">The underbrush whispers as </em><strong class="bold">it</strong><em class="italic"> sways gently in the </em><em class="italic">soft breeze.</em></p>&#13;
			<p>This shows long-range context matters for language modeling and next-token prediction. You can construct examples of arbitrary length where the pronoun resolution relies on the context provided many sentences earlier. These temporal dependencies and ambiguities are inherent to natural language, so a good language model needs to process long sequences of words.</p>&#13;
			<p>However, the FFN architecture introduced earlier is stateless and does not possess any memory of previously seen inputs. It is not suitable for sequential tasks, where future tokens depend on and refer to previous tokens.</p>&#13;
			<p>Sequence learning is a fundamental problem in ML, not just for NLP but many other areas, such as time series prediction, speech recognition, video understanding, robot control, etc. In some cases, the inputs are sequential, in others the outputs are sequential, or even both. Different modifications to the FFN architecture have been proposed to tackle this problem.</p>&#13;
			<h2 id="_idParaDest-54"><a id="_idTextAnchor055"/>Recurrent neural networks</h2>&#13;
			<p>One class of ANNs that deals with sequential data is called <strong class="bold">recurrent neural networks</strong> (<strong class="bold">RNNs</strong>). Unlike FFNs, RNNs include connections from a neuron to itself and its neighboring neurons within the same layer. These recurrent connections give the model an internal state, where previous activations can flow in a circular fashion and remain in the network when processing the next input, as depicted in <em class="italic">Figure 3</em><em class="italic">.6</em>:</p>&#13;
			<p>&#13;
				<div>&#13;
					<img alt="" role="presentation" src="img/B22495_03_06.jpg"/>&#13;
				</p>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.6: Recurrent connections give RNNs an internal state</p>&#13;
			<p>The training of RNNs remains like that of FFNs, where the RNN can be <em class="italic">unrolled</em> across time steps, and conceptually transformed into an FFN (albeit with many more layers and additional inputs corresponding to the internal states).</p>&#13;
			<p>However, one limitation of RNNs is that the gradients quickly diminish with each iteration through a recurrent connection. The network tends to <em class="italic">forget</em> activations that go back more than a few time steps, an issue known as the <strong class="bold">vanishing </strong><strong class="bold">gradient problem</strong>.</p>&#13;
			<p>To address this problem, further architectural changes have been proposed, including <strong class="bold">long short-term memory</strong> (<strong class="bold">LSTM</strong>) and <strong class="bold">gated recurrent unit</strong> (<strong class="bold">GRU</strong>) networks. In these models, cells consisting of multiple neurons are introduced, which can trap the gradient signal inside over thousands of time steps, thus alleviating the vanishing gradient problem.</p>&#13;
			<p>LSTMs have been applied successfully to many sequence problem domains, including robotics, speech and handwriting recognition, language translation, and playing video games.</p>&#13;
			<p>However, the training of recurrent networks happens sequentially along the time dimension, meaning that each time step requires a separate forward and backward pass through the network. This slows down training significantly, particularly for long sequences.</p>&#13;
			<p>RNNs have another limitation. Though the network can, in principle, remember previous activations due to its recurrent connections, this internal state needs to be carried forward for each time step. The model does not have direct access to the global context and its previous inputs explicitly.</p>&#13;
			<p>Both limitations were addressed by a breakthrough discovery in 2017, which is discussed in the next section.</p>&#13;
			<h2 id="_idParaDest-55"><a id="_idTextAnchor056"/>Transformer architecture</h2>&#13;
			<p>In 2017, Google published a new network architecture aimed to address some of the drawbacks of recurrent networks. This now famous paper, titled <em class="italic">Attention Is All You Need</em> (<a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a>), introduced the <strong class="bold">Transformer</strong> architecture, which departed from the idea of recurrent connections and instead relied on an attention mechanism to consider previous tokens in an otherwise stateless neural network. This publication marked a significant shift in the field of ML and NLP and paved the way for almost all modern LLMs as variations of the original transformer.</p>&#13;
			<p>Their advantages over recurrent networks—including the ability to process sequences in parallel, reduced computational complexity for long sequences, and superior handling of long-range dependencies—are key reasons why transformer architectures have become ubiquitous in the domain of NLP and beyond.</p>&#13;
			<p>At a high level, the original transformer model consists of two components: an encoder and a decoder. This architecture was proposed for the purpose of language translation, a sequence-to-sequence learning task with an input sequence of tokens in the source language processed by <strong class="bold">the encoder</strong>, and an output sequence of tokens in the target language processed by <strong class="bold">the decoder</strong>.</p>&#13;
			<p>While some LLMs still use this encoder/decoder structure, other families of models nowadays use simplified architectures building only on the encoder (such as BERT language models and variants) or the decoder (the GPT family). Generative models, including OpenAI’s GPT series, Meta’s Llama, Anthropic’s Claude, and Google’s PaLM models, all frame language modeling as next-token prediction, where the learning task is sequence-to-<em class="italic">single token</em>, as compared to sequence-to-sequence in the encoder/decoder structure. This allows for a simpler architecture, doing away with the encoder and only using the decoder part of a transformer.</p>&#13;
			<p>Both the encoder and decoder of a transformer consist of many layers of so-called <strong class="bold">transformer blocks</strong>. Unlike FFNs, where each layer is simply a fully connected layer of neurons with the next, a transformer block has an additional attention layer preceding the fully connected layer.</p>&#13;
			<p>The attention layer’s purpose is to learn which tokens in the sequence seen so far are most relevant when processing the current token. It assigns high attention weights to words that are highly relevant in the current context, and low attention weights to generic or irrelevant words, as you can see in <em class="italic">Figure 3</em><em class="italic">.7</em>:</p>&#13;
			<p class="IMG---Figure"><img alt="A diagram of food and drinks&#10;&#10;Description automatically generated with medium confidence" src="img/B22495_03_07.png"/></p>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.7: Attention maps for two sentence variations ending in <em class="italic">hungry</em> versus <em class="italic">tasty</em></p>&#13;
			<p><em class="italic">Figure 3</em><em class="italic">.7</em> shows attention maps for two sentences where only the last word differs. Darker color shades indicate higher attention weights. A transformer model would learn to pay more attention to tokens related to <em class="italic">hungry</em>, such as <em class="italic">cat</em>, in the first example, and to tokens related to <em class="italic">tasty</em>, such as <em class="italic">food</em>, in the second example.</p>&#13;
			<p>This attention mechanism is key to transformers. The landmark paper on Transformer architecture demonstrated that this mechanism alone could solve the sequential data problem without introducing recurrent connections into the architecture.</p>&#13;
			<h1 id="_idParaDest-56"><a id="_idTextAnchor057"/>LLMs in practice</h1>&#13;
			<p>So far, this chapter has mainly discussed the theoretical foundations of LLMs. Let’s close this chapter with an overview of the LLM landscape as it stands today, discussing some considerations for choosing an appropriate LLM as well as different techniques to tailor the model’s responses to your needs.</p>&#13;
			<h2 id="_idParaDest-57"><a id="_idTextAnchor058"/>The evolving field of LLMs</h2>&#13;
			<p>Generative AI and LLMs are a rapidly changing field, with new models, frameworks, and research papers on the topic released frequently. Most of the know-how to train an LLM is publicly available, yet at the time of writing, the cost of training a state-of-the-art LLM from scratch is still in the order of tens to hundreds of millions of US dollars, due to the large amount of GPU compute resources needed. This cost puts training your own model out of reach of individuals and most smaller companies, who will have to rely on pre-trained LLMs.</p>&#13;
			<p>The most competent models as of the time of writing, namely OpenAI’s GPT-4o (<a href="https://openai.com/">https://openai.com/</a>) and Anthropic’s Claude 3.5 Sonnet (<a href="https://www.anthropic.com/">https://www.anthropic.com/</a>), remain closed source but can be accessed via APIs on a per-token cost model. Open-source models, such as Meta’s Llama 3 (<a href="https://llama.meta.com/">https://llama.meta.com/</a>), are still behind on common benchmarks, but the gap is closing quickly. Depending on your use case and throughput requirements, it may be more cost-effective to self-host an open-source model or choose one of the many providers that offer model-hosting services.</p>&#13;
			<p>Other considerations when choosing between open and closed models include security and compliance, technical support, and vendor lock-in. Commercial LLM offerings often come with technical support and moderation endpoints to filter illegal requests and harmful or objectionable content and provide detailed documentation for their APIs and models. Open models, in contrast, provide more flexibility and customization, as well as transparency and interoperability with other models, and avoid potential vendor lock-in.</p>&#13;
			<h2 id="_idParaDest-58"><a id="_idTextAnchor059"/>Prompting, fine-tuning, and RAG</h2>&#13;
			<p>LLMs accept inputs in the form of text prompts (or simply prompts), which can be questions, statements, or requests that guide the model’s response. While the best LLMs are very capable and efficient in answering a wide range of different requests, chances are that a simple prompt may not lead to acceptable results for your application. Your use case may require special domain knowledge or responses in an uncommon (natural or programming) language that is under-represented in the original training dataset, or you may work with proprietary non-public data. This will not prevent you from integrating LLMs into your applications. There are several strategies available to deal with this scenario:</p>&#13;
			<ul>&#13;
				<li>Different prompting strategies</li>&#13;
				<li>Fine-tuning an LLM on custom data</li>&#13;
				<li>Retrieval-augmented generation (RAG)</li>&#13;
			</ul>&#13;
			<p>Prompting an LLM is more of an art than a hard science, which has led to an entirely new “prompt engineer” role in software development. Common techniques include zero- and few-shot prompting and chain-of-thought prompting. For more advanced prompting techniques, you can refer to the <em class="italic">Prompt Engineering Guide</em> at <a href="https://www.promptingguide.ai/">https://www.promptingguide.ai/</a>. You’ll learn more about different prompting strategies in <a href="B22495_09.xhtml#_idTextAnchor193"><em class="italic">Chapter 9</em></a>, <em class="italic">LLM </em><em class="italic">Output Evaluation</em>.</p>&#13;
			<p>For an even more custom-tailored response, pre-trained LLMs can be further trained on your own specific data through a process known as <strong class="bold">fine-tuning</strong>. Fine-tuning allows for adjustment of the language and style of the response, as well as injecting domain knowledge into the LLM. However, the process can be expensive depending on the dataset size. Fine-tuned models need to be evaluated carefully, as adjusting the weights may lead to overfitting, which can impact the model responses on previous tasks.</p>&#13;
			<p><strong class="bold">Retrieval-augmented generation</strong> (<strong class="bold">RAG</strong>) is another strategy to inject outside knowledge of proprietary data into an LLM. Here, an external knowledge base (for example, a vector database, which you will learn about in <a href="B22495_05.xhtml#_idTextAnchor115"><em class="italic">Chapter 5</em></a>, <em class="italic">Vector Databases</em>) is first queried with each request, and relevant information from the external data source is then included in the LLM prompt. While this alleviates some of the downsides of fine-tuning, one limiting factor is the length of the prompt (the context size) that the LLM can process in a single request. It is thus important to filter out irrelevant information to keep the prompt size manageable.</p>&#13;
			<h1 id="_idParaDest-59"><a id="_idTextAnchor060"/>Summary</h1>&#13;
			<p>This chapter covered the main components of a modern transformer-based LLM and a quick overview of the LLM landscape as it stands today.</p>&#13;
			<p>It detailed how text can be transformed into numeric data to be processed by ANNs. To summarize, sentences of a large text corpus are tokenized and assigned integer token IDs. Token IDs index into an embedding matrix, turning the integers into real-valued embedding vectors of fixed length. To create the targets for supervised training, the inputs are shifted by one token to the right, so that the target at each position becomes the token that follows in the sequence.</p>&#13;
			<p>Sequential data can be learned with recurrent neural networks, but these have been superseded by transformers, which use an attention mechanism to learn which previous tokens are most relevant to predict the next. At every step in the sequence, the model predicts probabilities for each token in the vocabulary, which can be used to generate the next token.</p>&#13;
			<p>The training dataset, consisting of inputs and targets, is split into smaller batches. With repeated forward and backward passes through the network, gradient calculation, and weight adjustments, the network learns to adjust the probabilities for each token given the context of previous tokens. You learned how these mechanisms have been put into practice by modern-day LLMs. You also got a brief introduction to some methods that can help you make the most of your language model. </p>&#13;
			<p>In the next chapter, you will take this knowledge forward with an understanding of embedding models and their crucial role in machine learning.</p>&#13;
		</div>&#13;
	</body></html>