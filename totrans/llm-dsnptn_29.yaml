- en: '29'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Evaluating RAG Systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RAG systems strive to produce more accurate, relevant, and factually grounded
    responses. However, evaluating the performance of these systems presents unique
    challenges. Unlike traditional information retrieval or **question-answering**
    (**QA**) systems, RAG evaluation must consider both the quality of the retrieved
    information and the effectiveness of the LLM in utilizing that information to
    generate a high-quality response.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll explore the intricacies of evaluating RAG systems. We’ll
    examine the challenges inherent in this task, dissect the key metrics used to
    assess retrieval quality and generation performance, and discuss various strategies
    for conducting comprehensive evaluations.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter aims to provide you with a thorough understanding of the principles
    and practices of RAG evaluation, equipping you with the knowledge you’ll need
    to assess and improve these powerful systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will be covering the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Challenges in evaluating RAG systems for LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metrics for assessing retrieval quality in LLM-based RAG
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Considerations for retrieval metrics in RAG
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating the relevance of retrieved information for LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measuring the impact of retrieval on LLM generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: End-to-end evaluation of RAG systems in LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Human evaluation techniques for LLM-based RAG
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Benchmarks and datasets for RAG evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Challenges in evaluating RAG systems for LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Evaluating RAG systems presents a unique set of challenges that distinguish
    it from evaluating traditional information retrieval or QA systems. These challenges
    stem from the interplay between the retrieval and generation components and the
    need to assess both the factual accuracy and the quality of the generated text.
  prefs: []
  type: TYPE_NORMAL
- en: The following sections will detail the specific challenges that are encountered
    when evaluating RAG systems for LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: The interplay between retrieval and generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The performance of a RAG system is a product of both its retrieval component
    and its generation component. Strong retrieval can provide the LLM with relevant
    and accurate information, leading to a better-generated response. Conversely,
    poor retrieval can mislead the LLM, resulting in an inaccurate or irrelevant answer,
    even if the generator itself is highly capable. Therefore, evaluating a RAG system
    requires assessing not only the quality of the retrieved information but also
    how effectively the LLM utilizes that information in its generation process.
  prefs: []
  type: TYPE_NORMAL
- en: Context-sensitive evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unlike traditional information retrieval, where relevance is often assessed
    based on the query alone, RAG evaluation must consider the context in which the
    retrieved information is used. A document might be relevant to the query in isolation
    but not provide the specific information needed to answer the question accurately
    in the context of the generated response. This necessitates context-sensitive
    evaluation metrics that consider both the query and the generated text when assessing
    the relevance of retrieved documents.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond factual accuracy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While factual accuracy is a primary concern in RAG evaluation, it is not the
    only factor that determines the quality of a generated response. The response
    must also be fluent, coherent, and relevant to the user’s query. These aspects
    of text quality are typically assessed through human evaluation, which can be
    expensive and time-consuming. Developing automated metrics that correlate well
    with human judgments of these qualitative aspects remains an open research challenge.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations of automated metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Automated metrics, such as those borrowed from information retrieval (e.g.,
    precision, recall) or machine translation (e.g., BLEU, ROUGE), can provide useful
    insights into RAG system performance. However, they often fall short of capturing
    the full picture. Retrieval metrics might not fully reflect the usefulness of
    a document for generation, while generation metrics might not adequately assess
    the factual grounding of the generated text in the retrieved context.
  prefs: []
  type: TYPE_NORMAL
- en: Difficulty in error analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When a RAG system produces an incorrect or low-quality response, it can be challenging
    to pinpoint the root cause. Was the retrieval component unable to find relevant
    documents? Did the LLM fail to utilize the retrieved information properly? Did
    the LLM hallucinate or generate a response that is not grounded in the provided
    context? Disentangling these factors requires careful error analysis and potentially
    the development of new diagnostic tools.
  prefs: []
  type: TYPE_NORMAL
- en: The need for diverse evaluation scenarios
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: RAG systems can be deployed in a wide range of applications, from open-domain
    QA to domain-specific chatbots. The specific challenges and evaluation criteria
    may vary, depending on the use case. Evaluating a RAG system’s performance across
    diverse scenarios and domains is crucial for understanding its strengths and weaknesses.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic knowledge and evolving information
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In many real-world applications, the underlying knowledge base is constantly
    evolving. New information is added, and existing information is updated or becomes
    outdated. Evaluating how well a RAG system adapts to these changes and maintains
    the accuracy of its responses over time is a significant challenge.
  prefs: []
  type: TYPE_NORMAL
- en: Computational cost
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Evaluating RAG systems, especially those that use bigger LLMs, can be computationally
    expensive. Running inference with large models and performing human evaluations
    on a large scale can require significant resources. Finding ways to balance evaluation
    thoroughness with computational cost is an important consideration.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a look at some key metrics for evaluating the retrieval component
    in LLM-based RAG systems while focusing on relevance and utility for response
    generation.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics for assessing retrieval quality in LLM-based RAG
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The retrieval component plays a crucial role in the overall performance of a
    RAG system. It is responsible for providing the LLM with relevant and accurate
    information that serves as the basis for generating a response. Therefore, assessing
    the quality of the retrieval component is a vital aspect of RAG evaluation. We
    can adapt traditional information retrieval metrics to the RAG setting, focusing
    on the ability of the retriever to find documents that are not only relevant to
    the query but also useful for the LLM to generate a high-quality answer.
  prefs: []
  type: TYPE_NORMAL
- en: Recall@k
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Recall@k measures the proportion of relevant documents that are successfully
    retrieved within the top *k* results. In the context of RAG, we can define a relevant
    document as one that contains the necessary information to answer the query accurately:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Formula**: *Recall@k = (Number of relevant documents retrieved in top k)
    / (Total number of* *relevant documents)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interpretation**: A higher Recall@k indicates that the retrieval component
    can find a larger proportion of the relevant documents'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Example**: If five documents in the entire corpus contain information needed
    to answer a specific query, and the RAG system retrieves three of them within
    the top 10 results, then Recall@10 for that query would be 3/5 = 0.6'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Precision@k
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Precision@k measures the proportion of retrieved documents within the top *k*
    results that are relevant:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Formula**: *Precision@k = (Number of relevant documents retrieved in top
    k) / (**k)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interpretation**: A higher Precision@k indicates that a larger proportion
    of the retrieved documents are relevant'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Example**: If a RAG system retrieves 10 documents for a query, and four of
    them are relevant, then Precision@10 would be 4/10 = 0.4'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mean Reciprocal Rank (MRR)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'MRR considers the rank of the first relevant document retrieved. It emphasizes
    the importance of retrieving relevant documents early in the ranking:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Formula**: *MRR = (1 / |Q|) * Σ (1 / rank_i) for i = 1 to |Q|*, where *|Q|*
    is the number of queries and *rank_i* is the rank of the first relevant document
    for query *i*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interpretation**: A higher MRR indicates that relevant documents are retrieved
    at higher ranks (closer to the top).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Example**: If the first relevant document for a query is retrieved at rank
    three, the reciprocal rank is 1/3\. MRR averages these reciprocal ranks across
    multiple queries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalized Discounted Cumulative Gain (NDCG@k)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'NDCG@k is a more sophisticated metric that considers both the relevance of
    retrieved documents and their position in the ranking. It uses a graded relevance
    scale (e.g., 0, 1, 2, where 2 is highly relevant) and assigns higher scores to
    relevant documents retrieved at higher ranks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Formula**: NDCG@k involves calculating the **Discounted Cumulative Gain**
    (**DCG**) of the retrieved list and normalizing it by the **Ideal Discounted Cumulative
    Gain** (**IDCG**), which is the DCG of the perfectly ranked list. The formula
    is complex but can be easily computed using libraries such as sklearn.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interpretation**: A higher NDCG@k indicates that highly relevant documents
    are retrieved at higher ranks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, let’s discuss how to decide which retrieval metrics to use.
  prefs: []
  type: TYPE_NORMAL
- en: Considerations for retrieval metrics in RAG
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the context of RAG, we need to define relevance carefully. A document might
    be relevant to the query but not contain the specific information needed to answer
    it accurately. We might need to use a stricter definition of relevance, such as
    “contains the answer to the query.” As mentioned earlier, relevance in RAG is
    often context-sensitive. A document might be relevant to the query in isolation
    but not be the most helpful document for generating a specific answer given the
    other retrieved documents.
  prefs: []
  type: TYPE_NORMAL
- en: While metrics such as Recall@k and Precision@k focus on the top *k* retrieved
    documents, it’s also important to consider the overall quality of the retrieval
    across a wider range of results. Metrics such as **Average Precision** (**AP**)
    can provide a more holistic view.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s illustrate how to calculate Recall@k, Precision@k, MRR, and NDCG@k in
    Python using the sklearn library:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first import the necessary libraries and define sample data representing
    a set of queries, the ground truth relevant documents for each query, and the
    documents that are retrieved by a RAG system for each query:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then define a function, `calculate_recall_at_k`, to calculate Recall@k for
    a given set of queries, ground truth relevant documents, and retrieved document
    lists:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We next define a function, `calculate_precision_at_k`, to calculate Precision@k
    for a given set of queries, ground truth, and retrieved lists:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We define a function, `calculate_mrr`, to calculate the MRR for a given set
    of queries, ground truth, and retrieved lists. A higher MRR indicates that the
    system consistently retrieves relevant documents at higher ranks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We also define a function, `calculate_ndcg_at_k`, to calculate NDCG@k. We’ll
    use a simplified version here where relevance scores are binary (0 or 1):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we calculate and print the retrieval metrics for different values
    of *k*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Evaluating the relevance of retrieved information for LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While the retrieval metrics discussed in the previous section provide a general
    assessment of retrieval quality, they do not fully capture the nuances of relevance
    in the context of RAG. In RAG, the retrieved information is not the end product
    but rather an intermediate step that serves as input to an LLM. Therefore, we
    need to evaluate the relevance of the retrieved information not just to the query
    but also to the specific task of generating a high-quality response via the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 'Traditional information retrieval often focuses on finding documents that are
    topically relevant to the query. However, in RAG, we need a more nuanced notion
    of relevance that considers the following aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Answerability**: Does the retrieved information contain the specific information
    needed to answer the query accurately? A document might be generally relevant
    to the query but not contain the precise answer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Contextual utility**: Is the retrieved information useful in the context
    of the other retrieved documents? A document might be relevant in isolation but
    redundant or even contradictory when combined with other retrieved information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LLM compatibility**: Is the retrieved information in a format that the LLM
    can easily understand and utilize? For example, a long and complex document might
    be relevant but difficult for the LLM to process effectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Faithfulness support**: Does the retrieved information provide sufficient
    evidence to support the claims made in the generated answer? This is crucial for
    ensuring that the LLM’s response is grounded in the retrieved context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Methods for evaluating the relevance of retrieved information
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are some methods for evaluating the relevance of retrieved information
    that involve moving beyond traditional query relevance:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Human evaluation**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Direct assessment**: Human annotators can directly assess the relevance of
    retrieved documents to the query and the generated response. They can be asked
    to rate the relevance on a Likert scale (e.g., 1 to 5) or to provide binary judgments
    (relevant/not relevant).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Comparative evaluation**: Annotators can be presented with multiple sets
    of retrieved documents and asked to rank them based on their usefulness for answering
    the query or to choose the best set.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Task-based evaluation**: Annotators can be asked to use the retrieved documents
    to answer the query themselves. The accuracy and quality of their answers can
    serve as an indirect measure of the relevance and utility of the retrieved information.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automated metrics**: Let’s consider some commonly used automated metrics.
    Keep in mind that while automated metrics provide a quantitative measure of performance,
    human evaluation offers valuable qualitative insights into the relevance, coherence,
    and usefulness of the generated responses:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Answer overlap**: We can automatically measure the overlap between the generated
    answer and the retrieved documents using metrics such as ROUGE or BLEU. Higher
    overlap suggests that the LLM is utilizing the retrieved information.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**QA metrics**: If we have ground truth answers, we can treat the retrieved
    context as the input to a QA system and evaluate its performance using standard
    QA metrics such as **Exact Match** (**EM**) and F1 score.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Faithfulness metrics**: We can use techniques such as **Natural Language
    Inference** (**NLI**) to assess whether the generated answer is entailed by the
    retrieved context. We’ll discuss NLI models in detail in later sections of this
    chapter.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Perplexity**: We can measure the perplexity of the LLM when it’s conditioned
    on the retrieved context. Lower perplexity suggests that the LLM finds the context
    informative and useful for generation.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As an example, let’s illustrate how to implement simple answer overlap metrics
    using the `rouge-score` library in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we run the following command to install the `rouge-score` library, which
    provides implementations of ROUGE metrics, and import the necessary modules:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we define sample data representing a query, a generated answer, and a
    list of retrieved documents:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We next define a function, `calculate_rouge_scores`, to calculate ROUGE scores
    between the generated answer and each retrieved document:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then calculate and print the ROUGE scores for each document:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we calculate and print the average ROUGE scores across all documents:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Challenges in evaluating RAG-specific relevance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Having explored several methods for evaluating the relevance of retrieved information,
    we now turn to outlining some of the key challenges involved in this evaluation
    process:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Subjectivity**: Relevance judgments can be subjective, especially when considering
    factors such as contextual utility and LLM compatibility'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Annotation cost**: Human evaluation can be expensive and time-consuming,
    especially for large-scale evaluations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Metric limitations**: Automated metrics might not fully capture the nuances
    of RAG-specific relevance and might not always correlate well with human judgments'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dynamic contexts**: The relevance of a document can change depending on the
    other documents that are retrieved and the specific generation strategy used by
    the LLM'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, let’s learn how to measure the impact of retrieval on LLM generation.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring the impact of retrieval on LLM generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In RAG systems, the quality of the generated response is heavily influenced
    by the information that’s retrieved. Good retrieval provides the necessary context
    and facts, while poor retrieval can lead to irrelevant or incorrect responses.
    Enhancing retrieval through better models and filtering improves overall performance,
    which is measured by precision, faithfulness, and user satisfaction.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, a crucial aspect of evaluation involves measuring the impact of retrieval
    on LLM generation. Let’s check out some of the key metrics and techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Key metrics for evaluating retrieval impact
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As mentioned, the quality of a response generated by an LLM is closely tied
    to the information it retrieves. Therefore, evaluating the impact of retrieval
    on the final response is crucial. This involves assessing how effectively the
    LLM leverages the retrieved context to generate answers that are accurate, relevant,
    and well-grounded. Let’s now examine some of the key metrics used in this evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Groundedness/faithfulness**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Groundedness, also known as faithfulness, measures the extent to which the generated
    response is factually supported by the retrieved context. A grounded response
    should only contain information that can be inferred from the provided documents.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Some of the techniques for evaluating this metric are as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Human evaluation**: Human annotators can directly assess the groundedness
    of each statement in the generated response by verifying whether it is supported
    by the retrieved context. This can involve binary judgments (grounded/not grounded)
    or more fine-grained ratings.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automated metrics**:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NLI**: NLI models can be used to determine whether each sentence in the generated
    response is entailed by the retrieved context. We treat the concatenation of retrieved
    documents as the premise and each sentence in the response as the hypothesis.
    A high entailment score suggests that the sentence is grounded in the context.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**QA-based**: We can formulate questions based on the generated response and
    check whether a QA model can answer them correctly using the retrieved context
    as the source of information. A high answerability score indicates that the response
    is grounded.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fact verification models**: These models can be used to check whether each
    fact stated in the generated response is supported by the retrieved documents
    or external knowledge sources.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Answer relevance**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Answer relevance measures how well the generated response addresses the user’s
    query, given the retrieved context. Even if the retrieved context is imperfect,
    a good RAG system should still strive to provide a relevant and helpful answer.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Some of the techniques for evaluating this metric are as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Human evaluation**: Human judges can assess the relevance of the generated
    response to the query while taking into account the limitations of the retrieved
    context. They can rate relevance on a Likert scale or provide comparative judgments
    (e.g., ranking multiple responses).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automated metrics**:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Query-answer similarity**: We can measure the semantic similarity between
    the query and the generated response using embedding-based techniques (e.g., cosine
    similarity) or other similarity metrics.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Task-specific metrics**: Depending on the specific application, we can use
    task-specific metrics. For example, in a QA scenario, we can measure the overlap
    between the generated answer and a gold-standard answer using metrics such as
    EM or F1 score.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Information retrieval metrics**: We can treat the generated response as a
    retrieved document and evaluate its relevance to the query using traditional IR
    metrics such as precision, recall, or NDCG, assuming we have relevance judgments
    for query-answer pairs.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Context utilization**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This aspect focuses on how effectively the LLM utilizes the retrieved context
    when generating the response. It goes beyond just measuring groundedness and assesses
    whether the LLM is integrating and synthesizing information from the context appropriately.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Some of the techniques for evaluating this metric are as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Human evaluation**: Human annotators can assess the extent to which the LLM
    is using the retrieved context, identifying instances where the model is underutilizing
    or over-relying on the context'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automated metrics**:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Attribution analysis**: We can use techniques such as attention visualization
    or gradient-based attribution to identify which parts of the retrieved context
    the LLM is paying the most attention to during generation.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Context ablation**: We can measure the change in the generated response when
    portions of the context are removed or modified. This can help with determining
    which parts of the context are most influential.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As an example, let’s carry out a groundedness evaluation using an NLI model.
    For this example, we’ll use the Transformers library:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first run the following command, which installs the `transformers` library.
    This library provides tools for working with pre-trained transformer models such
    as NLI. We also import the necessary modules:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then define sample data representing a query, a generated answer, and the
    retrieved context:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We load a pre-trained NLI model and its corresponding tokenizer. Here, we’re
    using the `roberta-large-mnli` model, which is a RoBERTa model that’s been fine-tuned
    on the MultiNLI dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then define a function, `calculate_claim_groundedness`, that calculates
    the entailment score for a single claim (a sentence from the generated answer)
    given the context:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We also define a function, `calculate_groundedness`, to calculate the overall
    groundedness score for the entire generated answer. It splits the answer into
    sentences, calculates the entailment score for each sentence, and then averages
    the scores:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we calculate and print the overall groundedness score for the sample
    data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Challenges in measuring the impact of retrieval
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we’ve seen an example, let’s take a look at some of the key challenges
    that are encountered in the evaluation process of RAG systems:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Defining the ground truth**: Determining the ground truth for groundedness
    and answer relevance can be challenging and subjective, especially when dealing
    with complex or nuanced queries'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Attributing errors**: It can be difficult to determine whether an error in
    the generated response is due to poor retrieval, limitations of the LLM, or a
    combination of both'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Computational cost**: Evaluating the impact of retrieval on generation can
    be computationally expensive, especially when using bigger LLMs or performing
    human evaluations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inter-annotator agreement**: When using human evaluation, ensuring high inter-annotator
    agreement on subjective judgments such as groundedness and relevance can be difficult'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While evaluating the individual components of a RAG system (retrieval and generation)
    is important, it is also crucial to assess the system’s overall performance in
    an end-to-end manner. Let’s see that next.
  prefs: []
  type: TYPE_NORMAL
- en: End-to-end evaluation of RAG systems in LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While evaluating the individual components of a RAG system (retrieval and generation)
    is important, it is also crucial to assess the system’s overall performance in
    an end-to-end manner. End-to-end evaluation considers the entire RAG pipeline,
    from the initial user query to the final generated response, providing a holistic
    view of the system’s effectiveness.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at some holistic metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Task success**: For task-oriented RAG systems (e.g., QA, dialogue), we can
    measure the overall task success rate. This involves determining whether the generated
    response completes the intended task successfully.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are some techniques for evaluating this metric:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Automated evaluation**: For some tasks, we can automatically evaluate task
    success. For example, in QA, we can check whether the generated answer matches
    the gold-standard answer.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Human evaluation**: For more complex tasks, human evaluation might be necessary
    to judge whether the RAG system successfully achieved the task’s goal.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Answer quality**: This metric assesses the overall quality of the generated
    response while considering factors such as accuracy, relevance, fluency, coherence,
    and groundedness.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are some techniques for evaluating this metric:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Human evaluation**: Human judges can rate the overall quality of the generated
    response on a Likert scale or by using more detailed rubrics that consider multiple
    quality dimensions'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automated metrics**: While answer quality can be challenging to fully automate,
    some aspects of answer quality can be approximated using metrics such as the following:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ROUGE/BLEU**: Measures the overlap between the generated response and a reference
    answer (if available)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Perplexity**: Measures how well the LLM predicts the generated response (lower
    perplexity is generally better)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Groundedness metrics (NLI, QA-based)**: Assesses the factual consistency
    of the response with the retrieved context'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Relevance metrics**: Measures the similarity between the query and the generated
    response'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let’s look at some ways we can evaluate RAG systems.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation strategies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Evaluation strategies for RAG systems can be broadly categorized into black-box
    evaluation, glass-box evaluation, component-wise evaluation, and ablation studies,
    each offering distinct insights into system performance.
  prefs: []
  type: TYPE_NORMAL
- en: In black-box evaluation, the entire RAG system is treated as a single unit.
    Evaluators provide input queries and assess only the final generated responses
    without analyzing the intermediate retrieval or generation steps. This approach
    is particularly useful for measuring overall system performance and comparing
    different RAG implementations without having to delve into their internal mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: Glass-box evaluation, in contrast, involves a detailed examination of the internal
    workings of the RAG system. This method analyzes the retrieved context, the LLM’s
    attention patterns, and the intermediate generation steps. By dissecting these
    elements, glass-box evaluation helps identify system strengths and weaknesses,
    pinpoint error sources, and provide insights for targeted improvements.
  prefs: []
  type: TYPE_NORMAL
- en: A more granular approach is component-wise evaluation, which assesses the retrieval
    and generation components separately. Retrieval performance is typically measured
    using metrics such as Recall@k and NDCG, while the quality of the generated text
    is evaluated using metrics such as BLEU and ROUGE or through human judgment based
    on a fixed set of retrieved documents. This method is particularly effective in
    isolating and diagnosing performance issues within individual components.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, ablation studies offer a systematic way to measure the impact of different
    components on overall system effectiveness. By removing or modifying specific
    parts of the RAG system – such as testing performance with and without retrieval
    or swapping different retrieval and generation models – researchers can better
    understand how each component contributes to the system’s functionality and overall
    success.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges in end-to-end evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Evaluating RAG systems holistically presents several challenges, particularly
    when assessing complex interactions between retrieval and generation components.
    Some of these challenges are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Defining the ground truth**: For open-ended tasks or tasks that involve generating
    complex responses, defining the ground truth can be difficult or even impossible'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Attributing errors**: When the system generates an incorrect or low-quality
    response, it can be challenging to determine whether the error originated in the
    retrieval or generation component'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Computational cost**: End-to-end evaluation can be computationally expensive,
    especially when using bigger LLMs or performing human evaluations on a large scale'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reproducibility**: Ensuring reproducibility can be difficult due to the complex
    interactions between the retrieval and generation components and the potential
    use of non-deterministic retrieval mechanisms or stochastic decoding strategies
    during generation, which may lead to variations in outputs across runs even with
    the same inputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, let’s shift focus to the role of human evaluation in assessing LLM-based
    RAG systems, which complements automated metrics by capturing nuanced aspects
    such as relevance, coherence, and factual accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Human evaluation techniques for LLM-based RAG
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While automated metrics provide valuable insights, human evaluation remains
    the gold standard for assessing the overall quality and effectiveness of RAG systems.
    Human judgment is particularly crucial for evaluating aspects that are difficult
    to capture with automated metrics, such as the nuanced relevance of retrieved
    information, the coherence and fluency of generated text, and the overall helpfulness
    of the response in addressing the user’s need.
  prefs: []
  type: TYPE_NORMAL
- en: 'Human evaluators can assess various aspects of RAG system performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Relevance**: How relevant is the generated response to the user’s query?
    Does it address the specific information needed expressed in the query?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Groundedness/faithfulness**: Is the generated response factually supported
    by the retrieved context? Does it avoid hallucinating or contradicting the provided
    information?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Coherence and fluency**: Is the generated response well-structured, easy
    to understand, and written in grammatically correct and natural-sounding language?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Helpfulness**: Does the response provide a useful and satisfactory answer
    to the user’s query, considering the limitations of the retrieved context?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Context utilization**: How effectively does the system utilize the retrieved
    context in generating the response? Does it integrate and synthesize information
    from multiple sources appropriately?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Attribution**: Does the system provide clear citations or links to the sources
    in the retrieved context that support the generated claims?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Several methods can be used for the human evaluation of RAG systems:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Rating scales (Likert scales)**: Annotators rate different aspects of the
    generated response (e.g., relevance, groundedness, fluency) on a numerical scale,
    such as 1 to 5, where 1 represents poor quality and 5 represents excellent quality:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Advantages**: Simple to implement and easy to collect and aggregate data'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Disadvantages**: Can be subjective, susceptible to annotator bias, and may
    not capture nuanced differences'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Comparative evaluation (ranking/best–worst scaling)**: Annotators are presented
    with multiple RAG system outputs for the same query and asked to rank them based
    on their overall quality or specific criteria.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Best–worst scaling**: A specific form of comparative evaluation where annotators
    choose the best and worst options from a set of outputs:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Advantages**: More reliable than absolute ratings and captures relative differences
    between systems effectively'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Disadvantages**: More complex to implement than rating scales and requires
    more effort from annotators'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Task-based evaluation**: Annotators are asked to complete a specific task
    using the RAG system, such as finding an answer to a question, writing a summary,
    or engaging in a conversation. The quality of the RAG system is assessed based
    on the annotators’ ability to complete the task successfully and their satisfaction
    with the system’s performance:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Advantages**: More realistic and user-centered and provides a direct measure
    of the system’s utility'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Disadvantages**: More complex to design and implement and can be time-consuming
    and expensive'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Free-form feedback**: Annotators provide open-ended feedback on the strengths
    and weaknesses of the RAG system’s output:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Advantages**: Captures detailed insights and suggestions for improvement
    and can uncover unexpected issues'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Disadvantages**: More difficult to analyze and quantify and can be subjective
    and inconsistent'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Best practices for human evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To ensure the reliability and fairness of human evaluation, consider the following
    best practices:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Clear guidelines**: Provide annotators with clear and detailed guidelines
    that define the evaluation criteria and annotation procedures'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training and calibration**: Train annotators on the task and calibrate their
    judgments using example annotations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inter-annotator agreement**: Measure inter-annotator agreement (e.g., using
    Cohen’s Kappa or Fleiss’ Kappa) to ensure the reliability of the annotations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pilot studies**: Conduct pilot studies to refine the evaluation protocol
    and identify potential issues before launching a large-scale evaluation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multiple annotators**: Use multiple annotators for each item to mitigate
    individual biases and improve the robustness of the evaluation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Diverse annotator pool**: Recruit a diverse pool of annotators to capture
    a wider range of perspectives and reduce potential biases'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Quality control**: Implement mechanisms for identifying and correcting errors
    or inconsistencies in the annotations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Challenges in human evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Evaluating the performance of RAG systems built on LLMs poses a unique set
    of challenges. Here, we outline the key obstacles encountered in conducting reliable,
    consistent, and meaningful human evaluations of such systems:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cost and time**: Human evaluation can be expensive and time-consuming, especially
    for large-scale evaluations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Subjectivity**: Human judgments can be subjective and influenced by individual
    preferences and biases'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Annotator training and expertise**: Ensuring that annotators are properly
    trained and have the necessary expertise to assess RAG system performance can
    be challenging'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reproducibility**: Replicating human evaluations can be difficult due to
    the inherent variability in human judgments'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we’ll explore the role of standardized benchmarks and datasets
    in evaluating RAG systems, highlighting key benchmarks, evaluation criteria, and
    challenges.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarks and datasets for RAG evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Standardized benchmarks and datasets play a crucial role in driving progress
    in RAG research and development. They provide a common ground for evaluating and
    comparing different RAG systems, facilitating the process of identifying best
    practices and tracking advancements over time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at some key benchmarks and datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Knowledge Intensive Language Tasks (KILT)**: A comprehensive benchmark for
    evaluating knowledge-intensive language tasks, including QA, fact-checking, dialogue,
    and entity linking:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data source**: Based on Wikipedia, with a unified format for all tasks'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Strengths**: Provides a diverse set of tasks, allows both retrieval and generation
    to be evaluated, and includes a standardized evaluation framework'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Limitations**: Primarily based on Wikipedia, which might not reflect the
    diversity of real-world knowledge sources'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Natural Questions (NQ)**: A large-scale QA dataset collected from real user
    queries that is sent to the Google search engine:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data source**: Contains pairs of questions and Wikipedia pages that contain
    the answer'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Strengths**: Realistic queries, large scale, and includes both short and
    long answer annotations'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Limitations**: Since it primarily focuses on factoid questions, it might
    not be suitable for evaluating more complex reasoning or generation tasks'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TriviaQA**: A challenging QA dataset containing question-answer-evidence
    triples:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data source**: Collected from trivia enthusiasts, it includes both web and
    Wikipedia evidence documents'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Strengths**: More difficult than NQ; it requires reading and understanding
    multiple evidence documents'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Limitations**: Primarily focused on factoid questions, the writing style
    of trivia questions might not be representative of real-world user queries'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Explain Like I’m Five (ELI5)**: A dataset of questions and answers from the
    Reddit forum *Explain Like I’m Five*, where users ask for simplified explanations
    of complex topics:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data source**: Collected from Reddit, it includes questions and answers on
    a wide range of topics'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Strengths**: Focuses on long-form, explanatory answers, making it suitable
    for evaluating the generation capabilities of RAG systems'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Limitations**: The quality and accuracy of answers can vary and might require
    careful filtering or annotation'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ASQA**: The first long-form QA dataset that unifies ambiguous questions:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data source**: The dataset is built from scratch by combining multiple ambiguous
    questions'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Strengths**: Can help evaluate long-form QA tasks'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Limitations**: Building a high-quality dataset from scratch can be challenging'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Microsoft Machine Reading Comprehension (MS MARCO)**: A large-scale dataset
    for machine reading comprehension and QA:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data source**: Contains real anonymized user queries that are sent to the
    Bing search engine, along with human-generated answers and relevant passages.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Strengths**: It provides a large-scale, diverse set of queries and answers
    that includes both passage-level and full-document annotations'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Limitations**: Primarily focused on extractive QA, it might not be ideal
    for evaluating the generation capabilities of RAG systems'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stanford Question Answering Dataset (SQuAD)**: A widely used dataset for
    reading comprehension consisting of questions posed by crowdworkers on a set of
    Wikipedia articles:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data source**: Contains question-paragraph-answer triples, where the answer
    is a span of text in the paragraph'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Strengths**: A large-scale, well-established benchmark for reading comprehension'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Limitations**: Primarily focused on extractive QA, it might not be suitable
    for evaluating the generation capabilities of RAG systems'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As an example, let’s illustrate how to use the KILT dataset to evaluate a RAG
    system. We’ll use the KILT library in Python to do so:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following code to install the kilt library and import the necessary
    modules:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, download a specific KILT task, such as the Wizard of Wikipedia (WoW)
    dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, load the downloaded dataset into memory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a dummy RAG function that simulates the behavior of a RAG retrieval
    component. For demonstration purposes, it simply returns a fixed set of Wikipedia
    pages for each query. In a real-world scenario, you would replace this with your
    actual RAG retrieval implementation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a dummy RAG generation function that simulates the behavior of a RAG
    generation component. For demonstration purposes, it simply returns a fixed answer
    for each query. In a real-world scenario, you would replace this with your actual
    LLM-based generation implementation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the dummy RAG pipeline on the dataset, using the dummy retrieval and generation
    functions, and collect the generated predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, evaluate the generated predictions using the KILT evaluation functions.
    Both the retrieval performance (using `provenance_evaluation`) and the answer
    quality (using `answer_evaluation`) are assessed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This code provides a basic example of how to use the KILT framework to evaluate
    a RAG system. In a real-world scenario, you would replace the dummy retrieval
    and generation functions with your actual RAG implementation and use a larger
    portion of the dataset for evaluation. You can adapt this example to other KILT
    tasks by downloading and loading the corresponding datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few things to consider when choosing benchmarks and datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Task alignment**: Select benchmarks and datasets that align with the specific
    task you are evaluating (e.g., QA, dialogue, summarization)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Knowledge domain**: Consider the knowledge domain covered by the benchmark.
    Some benchmarks are based on general knowledge (e.g., Wikipedia), while others
    focus on specific domains (e.g., scientific literature, medical records)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Retrieval setting**: Choose benchmarks that are appropriate for the retrieval
    setting you are using (e.g., open-domain retrieval, closed-domain retrieval, passage
    retrieval, document retrieval)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generation requirements**: Consider the type of generation required by the
    task (e.g., extractive versus abstractive, short versus long answers)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dataset size and quality**: Ensure that the dataset is large enough to provide
    statistically significant results and that the data is of high quality (e.g.,
    accurate annotations and well-formed questions)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evaluation metrics**: Check what evaluation metrics are used by the benchmark
    and whether they are appropriate for your specific evaluation goals'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed a wide range of metrics for evaluating both retrieval
    quality and generation performance, including traditional information retrieval
    metrics such as Recall@k, Precision@k, MRR, and NDCG, as well as more RAG-specific
    metrics such as groundedness, faithfulness, and answer relevance. We explored
    various techniques for measuring these metrics, including automated methods based
    on NLI and QA models, and human evaluation approaches using rating scales, comparative
    judgments, and task-based assessments.
  prefs: []
  type: TYPE_NORMAL
- en: We emphasized the crucial role of human evaluation in capturing the nuanced
    aspects of RAG performance that are difficult to assess with automated metrics
    alone. We also discussed best practices for designing and conducting human evaluations,
    such as providing clear guidelines, training annotators, measuring inter-annotator
    agreement, and conducting pilot studies. We need to keep in mind that tradeoffs
    between automated and human evaluation will be important in real-world deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, we explored widely used benchmarks and datasets for RAG evaluation,
    including KILT, NQ, TriviaQA, ELI5, ASQA, MS MARCO, and SQuAD, highlighting their
    strengths and limitations and providing guidance on selecting appropriate benchmarks
    for different tasks and domains.
  prefs: []
  type: TYPE_NORMAL
- en: As we conclude, it is clear that evaluating RAG systems is a complex and evolving
    field. The development of more sophisticated evaluation metrics, the creation
    of more diverse and challenging benchmarks, and the refinement of human evaluation
    methodologies will continue to be crucial for driving progress in RAG research
    and development.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll explore agentic patterns in LLMs, focusing on how
    LLMs can perform tasks involving reasoning, planning, and decision-making autonomously
    using advanced retrieval and generation techniques.
  prefs: []
  type: TYPE_NORMAL
