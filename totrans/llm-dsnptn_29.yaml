- en: '29'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '29'
- en: Evaluating RAG Systems
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估RAG系统
- en: RAG systems strive to produce more accurate, relevant, and factually grounded
    responses. However, evaluating the performance of these systems presents unique
    challenges. Unlike traditional information retrieval or **question-answering**
    (**QA**) systems, RAG evaluation must consider both the quality of the retrieved
    information and the effectiveness of the LLM in utilizing that information to
    generate a high-quality response.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: RAG系统力求产生更准确、相关和事实依据的响应。然而，评估这些系统的性能提出了独特的挑战。与传统的信息检索或**问答**（**QA**）系统不同，RAG评估必须考虑检索信息的质量以及LLM利用这些信息生成高质量响应的有效性。
- en: In this chapter, we’ll explore the intricacies of evaluating RAG systems. We’ll
    examine the challenges inherent in this task, dissect the key metrics used to
    assess retrieval quality and generation performance, and discuss various strategies
    for conducting comprehensive evaluations.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨评估RAG系统的复杂性。我们将检查这项任务固有的挑战，分析用于评估检索质量和生成性能的关键指标，并讨论进行全面评估的各种策略。
- en: This chapter aims to provide you with a thorough understanding of the principles
    and practices of RAG evaluation, equipping you with the knowledge you’ll need
    to assess and improve these powerful systems.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章旨在为您提供对RAG评估原则和实践的全面理解，使您具备评估和改进这些强大系统所需的知识。
- en: 'In this chapter, we will be covering the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Challenges in evaluating RAG systems for LLMs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估LLM的RAG系统面临的挑战
- en: Metrics for assessing retrieval quality in LLM-based RAG
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估基于LLM的RAG检索质量的指标
- en: Considerations for retrieval metrics in RAG
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAG检索指标考虑因素
- en: Evaluating the relevance of retrieved information for LLMs
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估检索信息的相关性
- en: Measuring the impact of retrieval on LLM generation
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测量检索对LLM生成的影响
- en: End-to-end evaluation of RAG systems in LLMs
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在LLM中端到端评估RAG系统
- en: Human evaluation techniques for LLM-based RAG
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于LLM的RAG的人评技术
- en: Benchmarks and datasets for RAG evaluation
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAG评估的基准和数据集
- en: Challenges in evaluating RAG systems for LLMs
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估LLM的RAG系统面临的挑战
- en: Evaluating RAG systems presents a unique set of challenges that distinguish
    it from evaluating traditional information retrieval or QA systems. These challenges
    stem from the interplay between the retrieval and generation components and the
    need to assess both the factual accuracy and the quality of the generated text.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 评估RAG系统提出了一系列独特的挑战，这些挑战使其与传统信息检索或QA系统评估区分开来。这些挑战源于检索和生成组件之间的相互作用以及评估事实准确性和生成文本质量的需求。
- en: The following sections will detail the specific challenges that are encountered
    when evaluating RAG systems for LLMs.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 以下各节将详细说明在评估LLM的RAG系统时遇到的特定挑战。
- en: The interplay between retrieval and generation
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检索和生成之间的相互作用
- en: The performance of a RAG system is a product of both its retrieval component
    and its generation component. Strong retrieval can provide the LLM with relevant
    and accurate information, leading to a better-generated response. Conversely,
    poor retrieval can mislead the LLM, resulting in an inaccurate or irrelevant answer,
    even if the generator itself is highly capable. Therefore, evaluating a RAG system
    requires assessing not only the quality of the retrieved information but also
    how effectively the LLM utilizes that information in its generation process.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: RAG系统的性能是其检索组件和生成组件共同作用的结果。强大的检索可以为LLM提供相关且准确的信息，从而产生更好的响应。相反，较差的检索可能会误导LLM，导致即使生成器本身能力很强，也会产生不准确或不相关的答案。因此，评估RAG系统需要评估检索信息的质量以及LLM在生成过程中有效利用这些信息的能力。
- en: Context-sensitive evaluation
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 上下文敏感评估
- en: Unlike traditional information retrieval, where relevance is often assessed
    based on the query alone, RAG evaluation must consider the context in which the
    retrieved information is used. A document might be relevant to the query in isolation
    but not provide the specific information needed to answer the question accurately
    in the context of the generated response. This necessitates context-sensitive
    evaluation metrics that consider both the query and the generated text when assessing
    the relevance of retrieved documents.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统信息检索不同，传统信息检索通常仅基于查询来评估相关性，RAG评估必须考虑检索信息使用的上下文。一个文档可能在孤立的情况下与查询相关，但可能不提供在生成响应上下文中准确回答问题的具体信息。这需要上下文敏感的评估指标，在评估检索文档的相关性时，既要考虑查询也要考虑生成的文本。
- en: Beyond factual accuracy
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超越事实准确性
- en: While factual accuracy is a primary concern in RAG evaluation, it is not the
    only factor that determines the quality of a generated response. The response
    must also be fluent, coherent, and relevant to the user’s query. These aspects
    of text quality are typically assessed through human evaluation, which can be
    expensive and time-consuming. Developing automated metrics that correlate well
    with human judgments of these qualitative aspects remains an open research challenge.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然事实准确性是RAG评估的主要关注点，但它并不是决定生成响应质量的决定性因素。响应还必须流畅、连贯，并且与用户的查询相关。这些文本质量方面通常通过人工评估来评估，这可能既昂贵又耗时。开发与人类对这些定性方面判断相关联的自动化指标仍然是一个开放的研究挑战。
- en: Limitations of automated metrics
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动化指标的局限性
- en: Automated metrics, such as those borrowed from information retrieval (e.g.,
    precision, recall) or machine translation (e.g., BLEU, ROUGE), can provide useful
    insights into RAG system performance. However, they often fall short of capturing
    the full picture. Retrieval metrics might not fully reflect the usefulness of
    a document for generation, while generation metrics might not adequately assess
    the factual grounding of the generated text in the retrieved context.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化指标，如从信息检索（例如，精确度、召回率）或机器翻译（例如，BLEU、ROUGE）借用，可以为RAG系统性能提供有用的见解。然而，它们通常无法全面反映整个情况。检索指标可能无法完全反映文档对生成的有用性，而生成指标可能无法充分评估生成文本在检索上下文中的事实基础。
- en: Difficulty in error analysis
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 错误分析困难
- en: When a RAG system produces an incorrect or low-quality response, it can be challenging
    to pinpoint the root cause. Was the retrieval component unable to find relevant
    documents? Did the LLM fail to utilize the retrieved information properly? Did
    the LLM hallucinate or generate a response that is not grounded in the provided
    context? Disentangling these factors requires careful error analysis and potentially
    the development of new diagnostic tools.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个RAG系统产生错误或低质量的响应时，确定根本原因可能具有挑战性。检索组件是否无法找到相关文档？LLM是否未能正确利用检索到的信息？LLM是否产生了不基于提供上下文的响应？解开这些因素需要仔细的错误分析和可能的新诊断工具的开发。
- en: The need for diverse evaluation scenarios
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 需要多样化的评估场景
- en: RAG systems can be deployed in a wide range of applications, from open-domain
    QA to domain-specific chatbots. The specific challenges and evaluation criteria
    may vary, depending on the use case. Evaluating a RAG system’s performance across
    diverse scenarios and domains is crucial for understanding its strengths and weaknesses.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: RAG系统可以部署在广泛的领域中，从开放域问答到特定领域的聊天机器人。具体的挑战和评估标准可能因用例而异。评估RAG系统在多样化和不同领域中的性能对于理解其优势和劣势至关重要。
- en: Dynamic knowledge and evolving information
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动态知识和演变信息
- en: In many real-world applications, the underlying knowledge base is constantly
    evolving. New information is added, and existing information is updated or becomes
    outdated. Evaluating how well a RAG system adapts to these changes and maintains
    the accuracy of its responses over time is a significant challenge.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多实际应用中，底层知识库是不断演变的。新信息被添加，现有信息被更新或变得过时。评估一个RAG系统如何适应这些变化并保持其响应的准确性是一个重大挑战。
- en: Computational cost
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算成本
- en: Evaluating RAG systems, especially those that use bigger LLMs, can be computationally
    expensive. Running inference with large models and performing human evaluations
    on a large scale can require significant resources. Finding ways to balance evaluation
    thoroughness with computational cost is an important consideration.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a look at some key metrics for evaluating the retrieval component
    in LLM-based RAG systems while focusing on relevance and utility for response
    generation.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Metrics for assessing retrieval quality in LLM-based RAG
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The retrieval component plays a crucial role in the overall performance of a
    RAG system. It is responsible for providing the LLM with relevant and accurate
    information that serves as the basis for generating a response. Therefore, assessing
    the quality of the retrieval component is a vital aspect of RAG evaluation. We
    can adapt traditional information retrieval metrics to the RAG setting, focusing
    on the ability of the retriever to find documents that are not only relevant to
    the query but also useful for the LLM to generate a high-quality answer.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Recall@k
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Recall@k measures the proportion of relevant documents that are successfully
    retrieved within the top *k* results. In the context of RAG, we can define a relevant
    document as one that contains the necessary information to answer the query accurately:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '**Formula**: *Recall@k = (Number of relevant documents retrieved in top k)
    / (Total number of* *relevant documents)*'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interpretation**: A higher Recall@k indicates that the retrieval component
    can find a larger proportion of the relevant documents'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Example**: If five documents in the entire corpus contain information needed
    to answer a specific query, and the RAG system retrieves three of them within
    the top 10 results, then Recall@10 for that query would be 3/5 = 0.6'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Precision@k
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Precision@k measures the proportion of retrieved documents within the top *k*
    results that are relevant:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '**Formula**: *Precision@k = (Number of relevant documents retrieved in top
    k) / (**k)*'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interpretation**: A higher Precision@k indicates that a larger proportion
    of the retrieved documents are relevant'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Example**: If a RAG system retrieves 10 documents for a query, and four of
    them are relevant, then Precision@10 would be 4/10 = 0.4'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mean Reciprocal Rank (MRR)
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'MRR considers the rank of the first relevant document retrieved. It emphasizes
    the importance of retrieving relevant documents early in the ranking:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '**Formula**: *MRR = (1 / |Q|) * Σ (1 / rank_i) for i = 1 to |Q|*, where *|Q|*
    is the number of queries and *rank_i* is the rank of the first relevant document
    for query *i*.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interpretation**: A higher MRR indicates that relevant documents are retrieved
    at higher ranks (closer to the top).'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Example**: If the first relevant document for a query is retrieved at rank
    three, the reciprocal rank is 1/3\. MRR averages these reciprocal ranks across
    multiple queries.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalized Discounted Cumulative Gain (NDCG@k)
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'NDCG@k is a more sophisticated metric that considers both the relevance of
    retrieved documents and their position in the ranking. It uses a graded relevance
    scale (e.g., 0, 1, 2, where 2 is highly relevant) and assigns higher scores to
    relevant documents retrieved at higher ranks:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: NDCG@k是一个更复杂的指标，它考虑了检索文档的相关性和它们在排名中的位置。它使用一个分级的相关性量表（例如，0、1、2，其中2是非常相关）并给在更高排名检索到的相关文档分配更高的分数：
- en: '**Formula**: NDCG@k involves calculating the **Discounted Cumulative Gain**
    (**DCG**) of the retrieved list and normalizing it by the **Ideal Discounted Cumulative
    Gain** (**IDCG**), which is the DCG of the perfectly ranked list. The formula
    is complex but can be easily computed using libraries such as sklearn.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**公式**：NDCG@k涉及计算检索列表的**折算累积收益**（**DCG**）并将其通过**理想折算累积收益**（**IDCG**）进行归一化，后者是完美排名列表的DCG。公式很复杂，但可以使用sklearn等库轻松计算。'
- en: '**Interpretation**: A higher NDCG@k indicates that highly relevant documents
    are retrieved at higher ranks.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解释**：更高的NDCG@k表明，高度相关的文档在更高的排名中被检索到。'
- en: Next, let’s discuss how to decide which retrieval metrics to use.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论如何决定使用哪些检索指标。
- en: Considerations for retrieval metrics in RAG
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RAG中检索指标的考虑因素
- en: In the context of RAG, we need to define relevance carefully. A document might
    be relevant to the query but not contain the specific information needed to answer
    it accurately. We might need to use a stricter definition of relevance, such as
    “contains the answer to the query.” As mentioned earlier, relevance in RAG is
    often context-sensitive. A document might be relevant to the query in isolation
    but not be the most helpful document for generating a specific answer given the
    other retrieved documents.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在RAG的背景下，我们需要仔细定义相关性。一个文档可能对查询相关，但不包含回答查询所需的特定信息。我们可能需要使用更严格的定义，例如“包含查询的答案。”如前所述，RAG中的相关性通常是上下文相关的。一个文档可能在孤立的情况下与查询相关，但不是在给定其他检索文档的情况下生成特定答案的最有帮助的文档。
- en: While metrics such as Recall@k and Precision@k focus on the top *k* retrieved
    documents, it’s also important to consider the overall quality of the retrieval
    across a wider range of results. Metrics such as **Average Precision** (**AP**)
    can provide a more holistic view.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Recall@k和Precision@k等指标关注的是前*k*个检索到的文档，但考虑更广泛结果的整体检索质量也很重要。例如，**平均精度**（**AP**）可以提供一个更全面的视角。
- en: 'Let’s illustrate how to calculate Recall@k, Precision@k, MRR, and NDCG@k in
    Python using the sklearn library:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用Python和sklearn库来演示如何计算Recall@k、Precision@k、MRR和NDCG@k：
- en: 'We first import the necessary libraries and define sample data representing
    a set of queries, the ground truth relevant documents for each query, and the
    documents that are retrieved by a RAG system for each query:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先导入必要的库，并定义代表一组查询、每个查询的真实相关文档以及每个查询由RAG系统检索到的文档的样本数据：
- en: '[PRE0]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We then define a function, `calculate_recall_at_k`, to calculate Recall@k for
    a given set of queries, ground truth relevant documents, and retrieved document
    lists:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们定义一个函数，`calculate_recall_at_k`，用于计算给定查询集、真实相关文档和检索文档列表的Recall@k：
- en: '[PRE1]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We next define a function, `calculate_precision_at_k`, to calculate Precision@k
    for a given set of queries, ground truth, and retrieved lists:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们接下来定义一个函数，`calculate_precision_at_k`，用于计算给定查询集、真实值和检索列表的Precision@k：
- en: '[PRE2]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We define a function, `calculate_mrr`, to calculate the MRR for a given set
    of queries, ground truth, and retrieved lists. A higher MRR indicates that the
    system consistently retrieves relevant documents at higher ranks:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义一个函数，`calculate_mrr`，用于计算给定查询集、真实值和检索列表的MRR。更高的MRR表明系统在更高的排名中一致地检索到相关文档：
- en: '[PRE3]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We also define a function, `calculate_ndcg_at_k`, to calculate NDCG@k. We’ll
    use a simplified version here where relevance scores are binary (0 or 1):'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还定义了一个函数，`calculate_ndcg_at_k`，用于计算NDCG@k。在这里，我们将使用一个简化版本，其中相关性分数是二进制的（0或1）：
- en: '[PRE4]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Finally, we calculate and print the retrieval metrics for different values
    of *k*:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们计算并打印不同*k*值的检索指标：
- en: '[PRE5]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Evaluating the relevance of retrieved information for LLMs
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估LLMs检索信息的相关性
- en: While the retrieval metrics discussed in the previous section provide a general
    assessment of retrieval quality, they do not fully capture the nuances of relevance
    in the context of RAG. In RAG, the retrieved information is not the end product
    but rather an intermediate step that serves as input to an LLM. Therefore, we
    need to evaluate the relevance of the retrieved information not just to the query
    but also to the specific task of generating a high-quality response via the LLM.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然上一节中讨论的检索指标提供了检索质量的总体评估，但它们并没有完全捕捉到RAG中相关性的细微差别。在RAG中，检索到的信息不是最终产品，而是一个中间步骤，作为LLM的输入。因此，我们需要评估检索信息不仅与查询相关，而且与通过LLM生成高质量响应的具体任务相关。
- en: 'Traditional information retrieval often focuses on finding documents that are
    topically relevant to the query. However, in RAG, we need a more nuanced notion
    of relevance that considers the following aspects:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的信息检索通常侧重于找到与查询主题相关的文档。然而，在RAG中，我们需要一个更细致的相关性概念，它考虑以下方面：
- en: '**Answerability**: Does the retrieved information contain the specific information
    needed to answer the query accurately? A document might be generally relevant
    to the query but not contain the precise answer.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可回答性**：检索到的信息是否包含回答查询所需的特定信息？一个文档可能一般与查询相关，但不包含精确的答案。'
- en: '**Contextual utility**: Is the retrieved information useful in the context
    of the other retrieved documents? A document might be relevant in isolation but
    redundant or even contradictory when combined with other retrieved information.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文效用**：检索到的信息在其他检索文档的上下文中是否有用？一个文档在孤立的情况下可能相关，但与其他检索信息结合时可能冗余甚至矛盾。'
- en: '**LLM compatibility**: Is the retrieved information in a format that the LLM
    can easily understand and utilize? For example, a long and complex document might
    be relevant but difficult for the LLM to process effectively.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LLM兼容性**：检索到的信息是否以LLM可以轻松理解和利用的格式存在？例如，一个长而复杂的文档可能相关，但对于LLM来说可能难以有效处理。'
- en: '**Faithfulness support**: Does the retrieved information provide sufficient
    evidence to support the claims made in the generated answer? This is crucial for
    ensuring that the LLM’s response is grounded in the retrieved context.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**忠实度支持**：检索到的信息是否提供了足够的证据来支持生成答案中的主张？这对于确保LLM的响应基于检索到的上下文至关重要。'
- en: Methods for evaluating the relevance of retrieved information
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检索信息相关性的评估方法
- en: 'Here are some methods for evaluating the relevance of retrieved information
    that involve moving beyond traditional query relevance:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些评估检索信息相关性的方法，这些方法超越了传统的查询相关性：
- en: '**Human evaluation**:'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人工评估**：'
- en: '**Direct assessment**: Human annotators can directly assess the relevance of
    retrieved documents to the query and the generated response. They can be asked
    to rate the relevance on a Likert scale (e.g., 1 to 5) or to provide binary judgments
    (relevant/not relevant).'
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**直接评估**：人工标注者可以直接评估检索文档与查询和生成响应的相关性。他们可以要求在李克特量表（例如，1到5）上评分，或提供二元判断（相关/不相关）。'
- en: '**Comparative evaluation**: Annotators can be presented with multiple sets
    of retrieved documents and asked to rank them based on their usefulness for answering
    the query or to choose the best set.'
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**比较评估**：标注者可以展示多组检索到的文档，并要求他们根据其回答查询的有用性进行排名，或选择最佳集合。'
- en: '**Task-based evaluation**: Annotators can be asked to use the retrieved documents
    to answer the query themselves. The accuracy and quality of their answers can
    serve as an indirect measure of the relevance and utility of the retrieved information.'
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于任务的评估**：标注者可以要求使用检索到的文档自行回答查询。他们回答的准确性和质量可以作为检索信息相关性和有用性的间接衡量标准。'
- en: '**Automated metrics**: Let’s consider some commonly used automated metrics.
    Keep in mind that while automated metrics provide a quantitative measure of performance,
    human evaluation offers valuable qualitative insights into the relevance, coherence,
    and usefulness of the generated responses:'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动化指标**：让我们考虑一些常用的自动化指标。请记住，虽然自动化指标提供了性能的定量衡量，但人工评估为生成响应的相关性、连贯性和有用性提供了有价值的定性见解：'
- en: '**Answer overlap**: We can automatically measure the overlap between the generated
    answer and the retrieved documents using metrics such as ROUGE or BLEU. Higher
    overlap suggests that the LLM is utilizing the retrieved information.'
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**答案重叠**：我们可以使用ROUGE或BLEU等指标自动测量生成的答案和检索到的文档之间的重叠。更高的重叠表明LLM正在利用检索到的信息。'
- en: '**QA metrics**: If we have ground truth answers, we can treat the retrieved
    context as the input to a QA system and evaluate its performance using standard
    QA metrics such as **Exact Match** (**EM**) and F1 score.'
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问答指标**：如果我们有真实答案，我们可以将检索到的上下文视为问答系统的输入，并使用标准问答指标（如**精确匹配**（EM）和F1分数）来评估其性能。'
- en: '**Faithfulness metrics**: We can use techniques such as **Natural Language
    Inference** (**NLI**) to assess whether the generated answer is entailed by the
    retrieved context. We’ll discuss NLI models in detail in later sections of this
    chapter.'
  id: totrans-88
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**忠实度指标**：我们可以使用自然语言推理（NLI）等技术来评估生成的答案是否由检索到的上下文所蕴含。我们将在本章后面的部分详细讨论NLI模型。'
- en: '**Perplexity**: We can measure the perplexity of the LLM when it’s conditioned
    on the retrieved context. Lower perplexity suggests that the LLM finds the context
    informative and useful for generation.'
  id: totrans-89
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**困惑度**：当我们对检索到的上下文进行条件化时，我们可以测量LLM的困惑度。较低的困惑度表明LLM认为上下文是有信息量和对生成有用的。'
- en: 'As an example, let’s illustrate how to implement simple answer overlap metrics
    using the `rouge-score` library in Python:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们用Python中的`rouge-score`库来说明如何实现简单的答案重叠指标：
- en: 'First, we run the following command to install the `rouge-score` library, which
    provides implementations of ROUGE metrics, and import the necessary modules:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们运行以下命令来安装`rouge-score`库，该库提供了ROUGE指标的实现，并导入必要的模块：
- en: '[PRE6]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Then, we define sample data representing a query, a generated answer, and a
    list of retrieved documents:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们定义表示查询、生成的答案和检索到的文档列表的样本数据：
- en: '[PRE7]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We next define a function, `calculate_rouge_scores`, to calculate ROUGE scores
    between the generated answer and each retrieved document:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个函数`calculate_rouge_scores`来计算生成的答案和每个检索到的文档之间的ROUGE分数：
- en: '[PRE8]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We then calculate and print the ROUGE scores for each document:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们计算并打印每个文档的ROUGE分数：
- en: '[PRE9]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Finally, we calculate and print the average ROUGE scores across all documents:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们计算并打印所有文档的平均ROUGE分数：
- en: '[PRE10]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Challenges in evaluating RAG-specific relevance
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估RAG特定相关性的挑战
- en: 'Having explored several methods for evaluating the relevance of retrieved information,
    we now turn to outlining some of the key challenges involved in this evaluation
    process:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索了评估检索信息相关性的几种方法之后，我们现在转向概述涉及此评估过程的一些关键挑战：
- en: '**Subjectivity**: Relevance judgments can be subjective, especially when considering
    factors such as contextual utility and LLM compatibility'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主观性**：相关性判断可能是主观的，尤其是在考虑上下文效用和LLM兼容性等因素时。'
- en: '**Annotation cost**: Human evaluation can be expensive and time-consuming,
    especially for large-scale evaluations'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标注成本**：人工评估可能既昂贵又耗时，尤其是在大规模评估中。'
- en: '**Metric limitations**: Automated metrics might not fully capture the nuances
    of RAG-specific relevance and might not always correlate well with human judgments'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**指标限制**：自动指标可能无法完全捕捉RAG特定相关性的细微差别，并且可能并不总是与人类判断很好地相关。'
- en: '**Dynamic contexts**: The relevance of a document can change depending on the
    other documents that are retrieved and the specific generation strategy used by
    the LLM'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动态上下文**：文档的相关性可能根据检索到的其他文档和LLM使用的特定生成策略而变化。'
- en: Next, let’s learn how to measure the impact of retrieval on LLM generation.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们学习如何测量检索对LLM生成的影响。
- en: Measuring the impact of retrieval on LLM generation
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测量检索对LLM生成的影响
- en: In RAG systems, the quality of the generated response is heavily influenced
    by the information that’s retrieved. Good retrieval provides the necessary context
    and facts, while poor retrieval can lead to irrelevant or incorrect responses.
    Enhancing retrieval through better models and filtering improves overall performance,
    which is measured by precision, faithfulness, and user satisfaction.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在RAG系统中，生成的响应质量很大程度上受到检索到的信息的影响。良好的检索提供了必要的信息和事实，而差的检索可能导致不相关或不正确的响应。通过更好的模型和过滤来增强检索可以提高整体性能，这通过精确度、忠实度和用户满意度来衡量。
- en: Therefore, a crucial aspect of evaluation involves measuring the impact of retrieval
    on LLM generation. Let’s check out some of the key metrics and techniques.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，评估的一个关键方面是衡量检索对LLM生成的影响。让我们来看看一些关键指标和技术。
- en: Key metrics for evaluating retrieval impact
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估检索影响的关键指标
- en: 'As mentioned, the quality of a response generated by an LLM is closely tied
    to the information it retrieves. Therefore, evaluating the impact of retrieval
    on the final response is crucial. This involves assessing how effectively the
    LLM leverages the retrieved context to generate answers that are accurate, relevant,
    and well-grounded. Let’s now examine some of the key metrics used in this evaluation:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，由大型语言模型（LLM）生成的响应质量与其检索到的信息紧密相关。因此，评估检索对最终响应的影响至关重要。这包括评估LLM如何有效地利用检索到的上下文来生成准确、相关且扎根的答案。现在让我们考察一下在这次评估中使用的某些关键指标：
- en: '**Groundedness/faithfulness**:'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**扎根性/忠实度**：'
- en: Groundedness, also known as faithfulness, measures the extent to which the generated
    response is factually supported by the retrieved context. A grounded response
    should only contain information that can be inferred from the provided documents.
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**扎根性**，也称为忠实度，衡量的是生成的响应在多大程度上由检索到的上下文提供事实支持。一个扎根的响应应仅包含可以从提供的文档中推断出的信息。'
- en: 'Some of the techniques for evaluating this metric are as follows:'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 评估此指标的一些技术如下：
- en: '**Human evaluation**: Human annotators can directly assess the groundedness
    of each statement in the generated response by verifying whether it is supported
    by the retrieved context. This can involve binary judgments (grounded/not grounded)
    or more fine-grained ratings.'
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人工评估**：人工标注者可以直接通过验证每个陈述是否由检索到的上下文支持来评估生成响应中每个陈述的扎根性。这可能涉及二元判断（扎根/非扎根）或更细致的评分。'
- en: '**Automated metrics**:'
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动化指标**：'
- en: '**NLI**: NLI models can be used to determine whether each sentence in the generated
    response is entailed by the retrieved context. We treat the concatenation of retrieved
    documents as the premise and each sentence in the response as the hypothesis.
    A high entailment score suggests that the sentence is grounded in the context.'
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自然语言推理（NLI）**：NLI模型可以用来确定生成的响应中的每个句子是否由检索到的上下文所蕴涵。我们将检索到的文档的拼接视为前提，将响应中的每个句子视为假设。高蕴涵分数表明句子在上下文中是扎根的。'
- en: '**QA-based**: We can formulate questions based on the generated response and
    check whether a QA model can answer them correctly using the retrieved context
    as the source of information. A high answerability score indicates that the response
    is grounded.'
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于问答的评估**：我们可以基于生成的响应制定问题，并检查问答模型是否可以使用检索到的上下文作为信息来源正确回答这些问题。高回答能力分数表明响应是扎根的。'
- en: '**Fact verification models**: These models can be used to check whether each
    fact stated in the generated response is supported by the retrieved documents
    or external knowledge sources.'
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**事实验证模型**：这些模型可以用来检查生成的响应中陈述的每个事实是否由检索到的文档或外部知识来源支持。'
- en: '**Answer relevance**:'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**答案相关性**：'
- en: Answer relevance measures how well the generated response addresses the user’s
    query, given the retrieved context. Even if the retrieved context is imperfect,
    a good RAG system should still strive to provide a relevant and helpful answer.
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案相关性衡量的是在检索到的上下文的背景下，生成的响应如何有效地解决用户的查询。即使检索到的上下文不完美，一个好的RAG系统也应努力提供相关且有帮助的答案。
- en: 'Some of the techniques for evaluating this metric are as follows:'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 评估此指标的一些技术如下：
- en: '**Human evaluation**: Human judges can assess the relevance of the generated
    response to the query while taking into account the limitations of the retrieved
    context. They can rate relevance on a Likert scale or provide comparative judgments
    (e.g., ranking multiple responses).'
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人工评估**：人类评判者可以在考虑检索上下文的限制的同时评估生成响应与查询的相关性。他们可以在李克特量表上评分或提供比较性判断（例如，对多个响应进行排名）。'
- en: '**Automated metrics**:'
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动化指标**：'
- en: '**Query-answer similarity**: We can measure the semantic similarity between
    the query and the generated response using embedding-based techniques (e.g., cosine
    similarity) or other similarity metrics.'
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**查询-答案相似度**：我们可以使用基于嵌入的技术（例如，余弦相似度）或其他相似度指标来衡量查询和生成的响应之间的语义相似度。'
- en: '**Task-specific metrics**: Depending on the specific application, we can use
    task-specific metrics. For example, in a QA scenario, we can measure the overlap
    between the generated answer and a gold-standard answer using metrics such as
    EM or F1 score.'
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特定任务的指标**：根据具体的应用，我们可以使用特定任务的指标。例如，在问答场景中，我们可以使用EM或F1分数等指标来衡量生成的答案与标准答案之间的重叠。'
- en: '**Information retrieval metrics**: We can treat the generated response as a
    retrieved document and evaluate its relevance to the query using traditional IR
    metrics such as precision, recall, or NDCG, assuming we have relevance judgments
    for query-answer pairs.'
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**信息检索指标**：我们可以将生成的响应视为检索到的文档，并使用传统的信息检索指标（如精确度、召回率或NDCG）来评估其与查询的相关性，前提是我们有查询-答案对的相关性判断。'
- en: '**Context utilization**:'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文利用**：'
- en: This aspect focuses on how effectively the LLM utilizes the retrieved context
    when generating the response. It goes beyond just measuring groundedness and assesses
    whether the LLM is integrating and synthesizing information from the context appropriately.
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个方面关注的是LLM在生成响应时有效利用检索上下文的能力。它不仅测量可信度，还评估LLM是否适当地整合和综合上下文中的信息。
- en: 'Some of the techniques for evaluating this metric are as follows:'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 评估此指标的一些技术如下：
- en: '**Human evaluation**: Human annotators can assess the extent to which the LLM
    is using the retrieved context, identifying instances where the model is underutilizing
    or over-relying on the context'
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人工评估**：人工标注者可以评估LLM使用检索上下文的程度，识别出模型在上下文利用不足或过度依赖上下文的实例。'
- en: '**Automated metrics**:'
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动指标**：'
- en: '**Attribution analysis**: We can use techniques such as attention visualization
    or gradient-based attribution to identify which parts of the retrieved context
    the LLM is paying the most attention to during generation.'
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**归因分析**：我们可以使用注意力可视化或基于梯度的归因等技术来识别LLM在生成过程中最关注检索上下文的哪些部分。'
- en: '**Context ablation**: We can measure the change in the generated response when
    portions of the context are removed or modified. This can help with determining
    which parts of the context are most influential.'
  id: totrans-135
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文消除**：我们可以测量当上下文的部分被移除或修改时，生成的响应的变化。这有助于确定上下文中最具影响力的部分。'
- en: 'As an example, let’s carry out a groundedness evaluation using an NLI model.
    For this example, we’ll use the Transformers library:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 作为例子，让我们使用NLI模型进行可信度评估。为此，我们将使用Transformers库：
- en: 'We first run the following command, which installs the `transformers` library.
    This library provides tools for working with pre-trained transformer models such
    as NLI. We also import the necessary modules:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先运行以下命令，安装`transformers`库。这个库提供了用于处理预训练的transformer模型（如NLI）的工具。我们还导入了必要的模块：
- en: '[PRE11]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We then define sample data representing a query, a generated answer, and the
    retrieved context:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们定义了代表查询、生成答案和检索上下文的样本数据：
- en: '[PRE12]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We load a pre-trained NLI model and its corresponding tokenizer. Here, we’re
    using the `roberta-large-mnli` model, which is a RoBERTa model that’s been fine-tuned
    on the MultiNLI dataset:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们加载了一个预训练的NLI模型及其相应的分词器。在这里，我们使用的是`roberta-large-mnli`模型，这是一个在MultiNLI数据集上微调过的RoBERTa模型：
- en: '[PRE13]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We then define a function, `calculate_claim_groundedness`, that calculates
    the entailment score for a single claim (a sentence from the generated answer)
    given the context:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们定义了一个函数`calculate_claim_groundedness`，它根据上下文计算单个断言（来自生成答案的句子）的蕴涵分数：
- en: '[PRE14]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We also define a function, `calculate_groundedness`, to calculate the overall
    groundedness score for the entire generated answer. It splits the answer into
    sentences, calculates the entailment score for each sentence, and then averages
    the scores:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还定义了一个函数`calculate_groundedness`，用于计算整个生成答案的整体可信度分数。它将答案拆分为句子，计算每个句子的蕴涵分数，然后平均这些分数：
- en: '[PRE15]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Finally, we calculate and print the overall groundedness score for the sample
    data:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们计算并打印样本数据的整体可信度分数：
- en: '[PRE16]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Challenges in measuring the impact of retrieval
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测量检索影响的挑战
- en: 'Now that we’ve seen an example, let’s take a look at some of the key challenges
    that are encountered in the evaluation process of RAG systems:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了一个示例，让我们来看看在RAG系统的评估过程中遇到的一些关键挑战：
- en: '**Defining the ground truth**: Determining the ground truth for groundedness
    and answer relevance can be challenging and subjective, especially when dealing
    with complex or nuanced queries'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定义真实情况**：确定基于事实和答案相关性的真实情况可能很困难且具有主观性，尤其是在处理复杂或细微的查询时。'
- en: '**Attributing errors**: It can be difficult to determine whether an error in
    the generated response is due to poor retrieval, limitations of the LLM, or a
    combination of both'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**归因错误**：确定生成的响应中的错误是由于检索不良、LLM的限制，还是两者的结合，可能很困难。'
- en: '**Computational cost**: Evaluating the impact of retrieval on generation can
    be computationally expensive, especially when using bigger LLMs or performing
    human evaluations'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算成本**：评估检索对生成的影响可能计算成本高昂，尤其是在使用更大的LLM或进行人工评估时。'
- en: '**Inter-annotator agreement**: When using human evaluation, ensuring high inter-annotator
    agreement on subjective judgments such as groundedness and relevance can be difficult'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标注者间一致性**：在使用人工评估时，确保在主观判断（如基于事实和相关性）方面有高标注者间一致性可能很困难。'
- en: While evaluating the individual components of a RAG system (retrieval and generation)
    is important, it is also crucial to assess the system’s overall performance in
    an end-to-end manner. Let’s see that next.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然评估RAG系统的各个组件（检索和生成）很重要，但评估系统在端到端方式下的整体性能也同样关键。让我们看看下一个方面。
- en: End-to-end evaluation of RAG systems in LLMs
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM中RAG系统的端到端评估
- en: While evaluating the individual components of a RAG system (retrieval and generation)
    is important, it is also crucial to assess the system’s overall performance in
    an end-to-end manner. End-to-end evaluation considers the entire RAG pipeline,
    from the initial user query to the final generated response, providing a holistic
    view of the system’s effectiveness.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然评估RAG系统的各个组件（检索和生成）很重要，但评估系统在端到端方式下的整体性能也同样关键。端到端评估考虑了整个RAG管道，从初始用户查询到最终生成的响应，提供了对系统有效性的整体看法。
- en: 'Let’s take a look at some holistic metrics:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些整体指标：
- en: '**Task success**: For task-oriented RAG systems (e.g., QA, dialogue), we can
    measure the overall task success rate. This involves determining whether the generated
    response completes the intended task successfully.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任务成功**：对于面向任务的RAG系统（例如，问答、对话），我们可以衡量整体任务成功率。这涉及到确定生成的响应是否成功完成了预期的任务。'
- en: 'Here are some techniques for evaluating this metric:'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里有一些评估此指标的技术：
- en: '**Automated evaluation**: For some tasks, we can automatically evaluate task
    success. For example, in QA, we can check whether the generated answer matches
    the gold-standard answer.'
  id: totrans-161
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动评估**：对于某些任务，我们可以自动评估任务的成功率。例如，在问答中，我们可以检查生成的答案是否与黄金标准答案匹配。'
- en: '**Human evaluation**: For more complex tasks, human evaluation might be necessary
    to judge whether the RAG system successfully achieved the task’s goal.'
  id: totrans-162
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人工评估**：对于更复杂的任务，可能需要人工评估来判断RAG系统是否成功实现了任务目标。'
- en: '**Answer quality**: This metric assesses the overall quality of the generated
    response while considering factors such as accuracy, relevance, fluency, coherence,
    and groundedness.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**答案质量**：此指标在考虑准确性、相关性、流畅性、连贯性和基于事实性等因素的同时，评估生成响应的整体质量。'
- en: 'Here are some techniques for evaluating this metric:'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里有一些评估此指标的技术：
- en: '**Human evaluation**: Human judges can rate the overall quality of the generated
    response on a Likert scale or by using more detailed rubrics that consider multiple
    quality dimensions'
  id: totrans-165
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人工评估**：人类评判员可以使用李克特量表或使用考虑多个质量维度的更详细的标准来评估生成的响应的整体质量。'
- en: '**Automated metrics**: While answer quality can be challenging to fully automate,
    some aspects of answer quality can be approximated using metrics such as the following:'
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动指标**：虽然回答质量可能难以完全自动化，但可以使用以下指标等来近似回答质量的一些方面：'
- en: '**ROUGE/BLEU**: Measures the overlap between the generated response and a reference
    answer (if available)'
  id: totrans-167
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ROUGE/BLEU**：衡量生成的响应与参考答案（如果有的话）之间的重叠程度。'
- en: '**Perplexity**: Measures how well the LLM predicts the generated response (lower
    perplexity is generally better)'
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**困惑度**：衡量LLM预测生成响应的效果（通常困惑度越低越好）。'
- en: '**Groundedness metrics (NLI, QA-based)**: Assesses the factual consistency
    of the response with the retrieved context'
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于事实的一致性指标（NLI，基于QA的）**：评估响应与检索到的上下文的事实一致性。'
- en: '**Relevance metrics**: Measures the similarity between the query and the generated
    response'
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**相关性指标**：衡量查询与生成响应之间的相似度。'
- en: Now, let’s look at some ways we can evaluate RAG systems.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看我们可以如何评估RAG系统。
- en: Evaluation strategies
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估策略
- en: Evaluation strategies for RAG systems can be broadly categorized into black-box
    evaluation, glass-box evaluation, component-wise evaluation, and ablation studies,
    each offering distinct insights into system performance.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: RAG系统的评估策略可以广泛分为黑盒评估、玻璃盒评估、组件评估和消融研究，每种方法都提供了对系统性能的独特见解。
- en: In black-box evaluation, the entire RAG system is treated as a single unit.
    Evaluators provide input queries and assess only the final generated responses
    without analyzing the intermediate retrieval or generation steps. This approach
    is particularly useful for measuring overall system performance and comparing
    different RAG implementations without having to delve into their internal mechanisms.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在黑盒评估中，整个RAG系统被视为一个单一单元。评估者提供输入查询，并仅评估最终生成的响应，而不分析中间的检索或生成步骤。这种方法特别适用于衡量整体系统性能和比较不同的RAG实现，而无需深入了解其内部机制。
- en: Glass-box evaluation, in contrast, involves a detailed examination of the internal
    workings of the RAG system. This method analyzes the retrieved context, the LLM’s
    attention patterns, and the intermediate generation steps. By dissecting these
    elements, glass-box evaluation helps identify system strengths and weaknesses,
    pinpoint error sources, and provide insights for targeted improvements.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 与之相反，玻璃盒评估涉及对RAG系统内部工作方式的详细检查。这种方法分析检索到的上下文、LLM的注意力模式以及中间生成步骤。通过剖析这些元素，玻璃盒评估有助于识别系统的优势和劣势，定位错误来源，并为有针对性的改进提供见解。
- en: A more granular approach is component-wise evaluation, which assesses the retrieval
    and generation components separately. Retrieval performance is typically measured
    using metrics such as Recall@k and NDCG, while the quality of the generated text
    is evaluated using metrics such as BLEU and ROUGE or through human judgment based
    on a fixed set of retrieved documents. This method is particularly effective in
    isolating and diagnosing performance issues within individual components.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 更细粒度的方法是组件评估，它分别评估检索和生成组件。检索性能通常使用Recall@k和NDCG等指标来衡量，而生成文本的质量则使用BLEU和ROUGE等指标来评估，或者基于一组固定的检索文档进行人工判断。这种方法特别有效于隔离和诊断单个组件中的性能问题。
- en: Finally, ablation studies offer a systematic way to measure the impact of different
    components on overall system effectiveness. By removing or modifying specific
    parts of the RAG system – such as testing performance with and without retrieval
    or swapping different retrieval and generation models – researchers can better
    understand how each component contributes to the system’s functionality and overall
    success.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，消融研究提供了一种系统的方法来衡量不同组件对整体系统有效性的影响。通过移除或修改RAG系统的特定部分（例如，测试带有和不带有检索的性能或交换不同的检索和生成模型）——研究人员可以更好地理解每个组件如何贡献于系统的功能性和整体成功。
- en: Challenges in end-to-end evaluation
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 端到端评估的挑战
- en: 'Evaluating RAG systems holistically presents several challenges, particularly
    when assessing complex interactions between retrieval and generation components.
    Some of these challenges are as follows:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 全面评估RAG系统存在几个挑战，尤其是在评估检索和生成组件之间的复杂交互时。以下是一些挑战：
- en: '**Defining the ground truth**: For open-ended tasks or tasks that involve generating
    complex responses, defining the ground truth can be difficult or even impossible'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定义真实情况**：对于开放性任务或涉及生成复杂响应的任务，定义真实情况可能很困难，甚至不可能'
- en: '**Attributing errors**: When the system generates an incorrect or low-quality
    response, it can be challenging to determine whether the error originated in the
    retrieval or generation component'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**归因错误**：当系统生成错误或不高质量的响应时，可能很难确定错误是否起源于检索或生成组件'
- en: '**Computational cost**: End-to-end evaluation can be computationally expensive,
    especially when using bigger LLMs or performing human evaluations on a large scale'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算成本**：端到端评估可能计算成本高昂，尤其是在使用更大的LLM或在大规模上进行人工评估时'
- en: '**Reproducibility**: Ensuring reproducibility can be difficult due to the complex
    interactions between the retrieval and generation components and the potential
    use of non-deterministic retrieval mechanisms or stochastic decoding strategies
    during generation, which may lead to variations in outputs across runs even with
    the same inputs.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可重现性**：由于检索和生成组件之间的复杂交互以及生成过程中可能使用的非确定性检索机制或随机解码策略，确保可重现性可能很困难，这可能导致即使在相同输入的情况下，输出也会因运行而异。'
- en: Next, let’s shift focus to the role of human evaluation in assessing LLM-based
    RAG systems, which complements automated metrics by capturing nuanced aspects
    such as relevance, coherence, and factual accuracy.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们将重点转向人类评估在评估基于LLM的RAG系统中的作用，它通过捕捉诸如相关性、连贯性和事实准确性等细微方面来补充自动化指标。
- en: Human evaluation techniques for LLM-based RAG
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于LLM的RAG的人类评估技术
- en: While automated metrics provide valuable insights, human evaluation remains
    the gold standard for assessing the overall quality and effectiveness of RAG systems.
    Human judgment is particularly crucial for evaluating aspects that are difficult
    to capture with automated metrics, such as the nuanced relevance of retrieved
    information, the coherence and fluency of generated text, and the overall helpfulness
    of the response in addressing the user’s need.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然自动化指标提供了有价值的见解，但人类评估仍然是评估RAG系统整体质量和有效性的黄金标准。人类判断对于评估难以用自动化指标捕捉的方面尤其重要，例如检索信息的细微相关性、生成文本的连贯性和流畅性，以及响应在解决用户需求方面的整体有用性。
- en: 'Human evaluators can assess various aspects of RAG system performance:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 人类评估者可以评估RAG系统性能的各个方面：
- en: '**Relevance**: How relevant is the generated response to the user’s query?
    Does it address the specific information needed expressed in the query?'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**相关性**：生成的响应与用户查询的相关性如何？它是否解决了查询中表达的具体信息需求？'
- en: '**Groundedness/faithfulness**: Is the generated response factually supported
    by the retrieved context? Does it avoid hallucinating or contradicting the provided
    information?'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**扎根性/忠实性**：生成的响应是否由检索到的上下文在事实上得到支持？它是否避免了虚构或与提供的信息相矛盾？'
- en: '**Coherence and fluency**: Is the generated response well-structured, easy
    to understand, and written in grammatically correct and natural-sounding language?'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**连贯性和流畅性**：生成的响应是否结构良好，易于理解，并且用语法正确且自然的声音书写？'
- en: '**Helpfulness**: Does the response provide a useful and satisfactory answer
    to the user’s query, considering the limitations of the retrieved context?'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有用性**：考虑到检索到的上下文的限制，响应是否提供了有用且令人满意的答案，以解决用户的查询？'
- en: '**Context utilization**: How effectively does the system utilize the retrieved
    context in generating the response? Does it integrate and synthesize information
    from multiple sources appropriately?'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文利用**：系统在生成响应时如何有效地利用检索到的上下文？它是否适当地整合和综合来自多个来源的信息？'
- en: '**Attribution**: Does the system provide clear citations or links to the sources
    in the retrieved context that support the generated claims?'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**归属**：系统是否提供了对检索到的上下文中支持生成声明的来源的明确引用或链接？'
- en: 'Several methods can be used for the human evaluation of RAG systems:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用几种方法对RAG系统进行人类评估：
- en: '**Rating scales (Likert scales)**: Annotators rate different aspects of the
    generated response (e.g., relevance, groundedness, fluency) on a numerical scale,
    such as 1 to 5, where 1 represents poor quality and 5 represents excellent quality:'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评分量表（李克特量表）**：标注者根据数值量表（例如1到5）对生成的响应的不同方面（例如相关性、扎根性、流畅性）进行评分，其中1代表质量差，5代表质量优秀：'
- en: '**Advantages**: Simple to implement and easy to collect and aggregate data'
  id: totrans-196
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优点**：易于实施，易于收集和汇总数据'
- en: '**Disadvantages**: Can be subjective, susceptible to annotator bias, and may
    not capture nuanced differences'
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺点**：可能具有主观性，容易受到标注者偏差的影响，并且可能无法捕捉细微的差异'
- en: '**Comparative evaluation (ranking/best–worst scaling)**: Annotators are presented
    with multiple RAG system outputs for the same query and asked to rank them based
    on their overall quality or specific criteria.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**比较评估（排名/最好最差尺度）**：标注者被展示多个针对同一查询的RAG系统输出，并要求根据其整体质量或特定标准进行排名。'
- en: '**Best–worst scaling**: A specific form of comparative evaluation where annotators
    choose the best and worst options from a set of outputs:'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最好最差尺度**：一种特定的比较评估形式，其中标注者从一组输出中选择最佳和最差选项：'
- en: '**Advantages**: More reliable than absolute ratings and captures relative differences
    between systems effectively'
  id: totrans-200
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优点**：比绝对评分更可靠，并能有效捕捉系统之间的相对差异'
- en: '**Disadvantages**: More complex to implement than rating scales and requires
    more effort from annotators'
  id: totrans-201
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺点**：比评分量表更难实现，需要标注者付出更多努力'
- en: '**Task-based evaluation**: Annotators are asked to complete a specific task
    using the RAG system, such as finding an answer to a question, writing a summary,
    or engaging in a conversation. The quality of the RAG system is assessed based
    on the annotators’ ability to complete the task successfully and their satisfaction
    with the system’s performance:'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于任务的评估**：要求标注者使用RAG系统完成特定任务，例如回答问题、撰写摘要或进行对话。RAG系统的质量基于标注者成功完成任务的能力以及他们对系统性能的满意度：'
- en: '**Advantages**: More realistic and user-centered and provides a direct measure
    of the system’s utility'
  id: totrans-203
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优点**：更真实、以用户为中心，并提供系统实用性的直接度量'
- en: '**Disadvantages**: More complex to design and implement and can be time-consuming
    and expensive'
  id: totrans-204
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺点**：设计和实现更复杂，可能耗时且昂贵'
- en: '**Free-form feedback**: Annotators provide open-ended feedback on the strengths
    and weaknesses of the RAG system’s output:'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自由形式反馈**：标注者对RAG系统输出的优点和缺点提供开放式反馈：'
- en: '**Advantages**: Captures detailed insights and suggestions for improvement
    and can uncover unexpected issues'
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优点**：捕捉详细的见解和建议，并能揭示意外问题'
- en: '**Disadvantages**: More difficult to analyze and quantify and can be subjective
    and inconsistent'
  id: totrans-207
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺点**：更难分析和量化，可能主观且不一致'
- en: Best practices for human evaluation
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人类评估的最佳实践
- en: 'To ensure the reliability and fairness of human evaluation, consider the following
    best practices:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保人类评估的可靠性和公平性，请考虑以下最佳实践：
- en: '**Clear guidelines**: Provide annotators with clear and detailed guidelines
    that define the evaluation criteria and annotation procedures'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**清晰的指南**：为标注者提供清晰和详细的指南，定义评估标准和标注程序'
- en: '**Training and calibration**: Train annotators on the task and calibrate their
    judgments using example annotations'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**培训和校准**：对标注者进行任务培训并使用示例标注校准他们的判断'
- en: '**Inter-annotator agreement**: Measure inter-annotator agreement (e.g., using
    Cohen’s Kappa or Fleiss’ Kappa) to ensure the reliability of the annotations'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标注者间一致性**：测量标注者间一致性（例如，使用Cohen的Kappa或Fleiss的Kappa）以确保标注的可靠性'
- en: '**Pilot studies**: Conduct pilot studies to refine the evaluation protocol
    and identify potential issues before launching a large-scale evaluation'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**试点研究**：进行试点研究以完善评估方案并在大规模评估启动前识别潜在问题'
- en: '**Multiple annotators**: Use multiple annotators for each item to mitigate
    individual biases and improve the robustness of the evaluation'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多个标注者**：为每个项目使用多个标注者以减轻个人偏见并提高评估的稳健性'
- en: '**Diverse annotator pool**: Recruit a diverse pool of annotators to capture
    a wider range of perspectives and reduce potential biases'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多样化的标注者群体**：招募多样化的标注者群体以捕捉更广泛的视角并减少潜在的偏见'
- en: '**Quality control**: Implement mechanisms for identifying and correcting errors
    or inconsistencies in the annotations'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**质量控制**：实施机制以识别和纠正标注中的错误或不一致'
- en: Challenges in human evaluation
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人类评估的挑战
- en: 'Evaluating the performance of RAG systems built on LLMs poses a unique set
    of challenges. Here, we outline the key obstacles encountered in conducting reliable,
    consistent, and meaningful human evaluations of such systems:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 评估基于LLM构建的RAG系统的性能面临一系列独特的挑战。在此，我们概述了在执行可靠、一致和有意义的系统人类评估过程中遇到的关键障碍：
- en: '**Cost and time**: Human evaluation can be expensive and time-consuming, especially
    for large-scale evaluations'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本和时间**：人类评估可能很昂贵且耗时，尤其是对于大规模评估'
- en: '**Subjectivity**: Human judgments can be subjective and influenced by individual
    preferences and biases'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主观性**：人类判断可能具有主观性，并受个人偏好和偏见的影响'
- en: '**Annotator training and expertise**: Ensuring that annotators are properly
    trained and have the necessary expertise to assess RAG system performance can
    be challenging'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标注者培训和专业知识**：确保标注者得到适当的培训并具备评估RAG系统性能所需的必要专业知识可能具有挑战性'
- en: '**Reproducibility**: Replicating human evaluations can be difficult due to
    the inherent variability in human judgments'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可重复性**：由于人类判断固有的可变性，复制人类评估可能很困难'
- en: In the next section, we’ll explore the role of standardized benchmarks and datasets
    in evaluating RAG systems, highlighting key benchmarks, evaluation criteria, and
    challenges.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarks and datasets for RAG evaluation
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Standardized benchmarks and datasets play a crucial role in driving progress
    in RAG research and development. They provide a common ground for evaluating and
    comparing different RAG systems, facilitating the process of identifying best
    practices and tracking advancements over time.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at some key benchmarks and datasets:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '**Knowledge Intensive Language Tasks (KILT)**: A comprehensive benchmark for
    evaluating knowledge-intensive language tasks, including QA, fact-checking, dialogue,
    and entity linking:'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data source**: Based on Wikipedia, with a unified format for all tasks'
  id: totrans-228
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Strengths**: Provides a diverse set of tasks, allows both retrieval and generation
    to be evaluated, and includes a standardized evaluation framework'
  id: totrans-229
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Limitations**: Primarily based on Wikipedia, which might not reflect the
    diversity of real-world knowledge sources'
  id: totrans-230
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Natural Questions (NQ)**: A large-scale QA dataset collected from real user
    queries that is sent to the Google search engine:'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data source**: Contains pairs of questions and Wikipedia pages that contain
    the answer'
  id: totrans-232
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Strengths**: Realistic queries, large scale, and includes both short and
    long answer annotations'
  id: totrans-233
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Limitations**: Since it primarily focuses on factoid questions, it might
    not be suitable for evaluating more complex reasoning or generation tasks'
  id: totrans-234
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TriviaQA**: A challenging QA dataset containing question-answer-evidence
    triples:'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data source**: Collected from trivia enthusiasts, it includes both web and
    Wikipedia evidence documents'
  id: totrans-236
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Strengths**: More difficult than NQ; it requires reading and understanding
    multiple evidence documents'
  id: totrans-237
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Limitations**: Primarily focused on factoid questions, the writing style
    of trivia questions might not be representative of real-world user queries'
  id: totrans-238
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Explain Like I’m Five (ELI5)**: A dataset of questions and answers from the
    Reddit forum *Explain Like I’m Five*, where users ask for simplified explanations
    of complex topics:'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data source**: Collected from Reddit, it includes questions and answers on
    a wide range of topics'
  id: totrans-240
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Strengths**: Focuses on long-form, explanatory answers, making it suitable
    for evaluating the generation capabilities of RAG systems'
  id: totrans-241
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Limitations**: The quality and accuracy of answers can vary and might require
    careful filtering or annotation'
  id: totrans-242
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ASQA**: The first long-form QA dataset that unifies ambiguous questions:'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data source**: The dataset is built from scratch by combining multiple ambiguous
    questions'
  id: totrans-244
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Strengths**: Can help evaluate long-form QA tasks'
  id: totrans-245
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Limitations**: Building a high-quality dataset from scratch can be challenging'
  id: totrans-246
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Microsoft Machine Reading Comprehension (MS MARCO)**: A large-scale dataset
    for machine reading comprehension and QA:'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data source**: Contains real anonymized user queries that are sent to the
    Bing search engine, along with human-generated answers and relevant passages.'
  id: totrans-248
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Strengths**: It provides a large-scale, diverse set of queries and answers
    that includes both passage-level and full-document annotations'
  id: totrans-249
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Limitations**: Primarily focused on extractive QA, it might not be ideal
    for evaluating the generation capabilities of RAG systems'
  id: totrans-250
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stanford Question Answering Dataset (SQuAD)**: A widely used dataset for
    reading comprehension consisting of questions posed by crowdworkers on a set of
    Wikipedia articles:'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data source**: Contains question-paragraph-answer triples, where the answer
    is a span of text in the paragraph'
  id: totrans-252
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Strengths**: A large-scale, well-established benchmark for reading comprehension'
  id: totrans-253
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Limitations**: Primarily focused on extractive QA, it might not be suitable
    for evaluating the generation capabilities of RAG systems'
  id: totrans-254
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As an example, let’s illustrate how to use the KILT dataset to evaluate a RAG
    system. We’ll use the KILT library in Python to do so:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following code to install the kilt library and import the necessary
    modules:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Next, download a specific KILT task, such as the Wizard of Wikipedia (WoW)
    dataset:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Then, load the downloaded dataset into memory:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Define a dummy RAG function that simulates the behavior of a RAG retrieval
    component. For demonstration purposes, it simply returns a fixed set of Wikipedia
    pages for each query. In a real-world scenario, you would replace this with your
    actual RAG retrieval implementation:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Define a dummy RAG generation function that simulates the behavior of a RAG
    generation component. For demonstration purposes, it simply returns a fixed answer
    for each query. In a real-world scenario, you would replace this with your actual
    LLM-based generation implementation:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Run the dummy RAG pipeline on the dataset, using the dummy retrieval and generation
    functions, and collect the generated predictions:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Finally, evaluate the generated predictions using the KILT evaluation functions.
    Both the retrieval performance (using `provenance_evaluation`) and the answer
    quality (using `answer_evaluation`) are assessed:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This code provides a basic example of how to use the KILT framework to evaluate
    a RAG system. In a real-world scenario, you would replace the dummy retrieval
    and generation functions with your actual RAG implementation and use a larger
    portion of the dataset for evaluation. You can adapt this example to other KILT
    tasks by downloading and loading the corresponding datasets.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few things to consider when choosing benchmarks and datasets:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '**Task alignment**: Select benchmarks and datasets that align with the specific
    task you are evaluating (e.g., QA, dialogue, summarization)'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Knowledge domain**: Consider the knowledge domain covered by the benchmark.
    Some benchmarks are based on general knowledge (e.g., Wikipedia), while others
    focus on specific domains (e.g., scientific literature, medical records)'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Retrieval setting**: Choose benchmarks that are appropriate for the retrieval
    setting you are using (e.g., open-domain retrieval, closed-domain retrieval, passage
    retrieval, document retrieval)'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**检索设置**：选择适合您所使用的检索设置的基准（例如，开放域检索、封闭域检索、段落检索、文档检索）'
- en: '**Generation requirements**: Consider the type of generation required by the
    task (e.g., extractive versus abstractive, short versus long answers)'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成要求**：考虑任务所需的生成类型（例如，抽取式与抽象式、短答案与长答案）'
- en: '**Dataset size and quality**: Ensure that the dataset is large enough to provide
    statistically significant results and that the data is of high quality (e.g.,
    accurate annotations and well-formed questions)'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集大小和质量**：确保数据集足够大，以提供具有统计意义的成果，并且数据质量要高（例如，准确的注释和良好的问题格式）'
- en: '**Evaluation metrics**: Check what evaluation metrics are used by the benchmark
    and whether they are appropriate for your specific evaluation goals'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评估指标**：检查基准使用的评估指标，并确定它们是否适合您的特定评估目标'
- en: Summary
  id: totrans-278
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed a wide range of metrics for evaluating both retrieval
    quality and generation performance, including traditional information retrieval
    metrics such as Recall@k, Precision@k, MRR, and NDCG, as well as more RAG-specific
    metrics such as groundedness, faithfulness, and answer relevance. We explored
    various techniques for measuring these metrics, including automated methods based
    on NLI and QA models, and human evaluation approaches using rating scales, comparative
    judgments, and task-based assessments.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了评估检索质量和生成性能的广泛指标，包括传统的信息检索指标，如Recall@k、Precision@k、MRR和NDCG，以及更RAG特定的指标，如groundedness、faithfulness和answer
    relevance。我们探讨了测量这些指标的各种技术，包括基于NLI和QA模型的自动化方法，以及使用评分尺度、比较判断和基于任务的评估方法进行的人评方法。
- en: We emphasized the crucial role of human evaluation in capturing the nuanced
    aspects of RAG performance that are difficult to assess with automated metrics
    alone. We also discussed best practices for designing and conducting human evaluations,
    such as providing clear guidelines, training annotators, measuring inter-annotator
    agreement, and conducting pilot studies. We need to keep in mind that tradeoffs
    between automated and human evaluation will be important in real-world deployments.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 我们强调了人评在捕捉RAG性能细微差别方面的重要作用，这些差别仅用自动化指标难以评估。我们还讨论了设计和进行人评的最佳实践，例如提供明确的指南、培训注释员、测量注释员间的一致性以及进行试点研究。我们需要记住，在现实世界的部署中，自动化与人评之间的权衡将非常重要。
- en: Furthermore, we explored widely used benchmarks and datasets for RAG evaluation,
    including KILT, NQ, TriviaQA, ELI5, ASQA, MS MARCO, and SQuAD, highlighting their
    strengths and limitations and providing guidance on selecting appropriate benchmarks
    for different tasks and domains.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们探讨了广泛使用的RAG评估基准和数据集，包括KILT、NQ、TriviaQA、ELI5、ASQA、MS MARCO和SQuAD，突出了它们的优点和局限性，并提供了针对不同任务和领域的基准选择指南。
- en: As we conclude, it is clear that evaluating RAG systems is a complex and evolving
    field. The development of more sophisticated evaluation metrics, the creation
    of more diverse and challenging benchmarks, and the refinement of human evaluation
    methodologies will continue to be crucial for driving progress in RAG research
    and development.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，很明显，评估RAG系统是一个复杂且不断发展的领域。更复杂的评估指标的开发、更多样化和更具挑战性的基准的创建以及人评方法的改进将继续对推动RAG研究和开发进步至关重要。
- en: In the next chapter, we’ll explore agentic patterns in LLMs, focusing on how
    LLMs can perform tasks involving reasoning, planning, and decision-making autonomously
    using advanced retrieval and generation techniques.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨LLMs中的代理模式，重点关注LLMs如何使用高级检索和生成技术自主执行涉及推理、规划和决策的任务。
