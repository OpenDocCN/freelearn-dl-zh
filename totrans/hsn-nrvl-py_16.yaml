- en: Concluding Remarks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will summarize everything we have learned in this book and
    will provide further information so that you can continue your self-education. This
    chapter will help us revise the topics we have covered in a chapter-wise format
    and then provide a roadmap by sharing some details on Uber AI Labs, alife.org,
    and open-ended evolution at Reddit. We will also have a quick overview of the NEAT
    Software Catalog and the NEAT Algorithm Paper.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What we learned in this book
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Where to go from here
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What we learned in this book
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have finished with the experiments, I hope that you have gained
    a solid understanding of the neuroevolution method of training artificial neural
    networks. We used neuroevolution to find solutions to a variety of experiments,
    from classic computer science problems to the creation of agents that are capable
    of playing Atari games. We also examined tasks related to computer vision and
    visual discrimination.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will summarize what we learned in each chapter of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of the neuroevolution methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about the core concepts of genetic algorithms, such
    as genetic operators and genome encoding schemes.
  prefs: []
  type: TYPE_NORMAL
- en: 'We discussed two major genetic operators that allow us to maintain the evolutionary
    process:'
  prefs: []
  type: TYPE_NORMAL
- en: The mutation operator implements random mutations of the offspring, which introduces
    genetic diversity into the population.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The crossover operator generates offspring by sampling genes from each parent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After that, we continued with a discussion about the importance of choosing
    the right genome encoding schema. We considered two major encoding formats that
    exist: direct and indirect genome encoding. The former introduces a one-to-one
    relationship between the genome and the encoded phenotype ANN. Usually, direct
    encoding is applied to encode small ANNs, which has a limited number of connected
    nodes. The more advanced indirect encoding scheme allows us to encode the evolving
    ANN topology of large networks, often with millions of connections. Indirect encoding
    allows us to reuse repeating encoding blocks, thus significantly reducing the
    size of a genome.'
  prefs: []
  type: TYPE_NORMAL
- en: Once we were familiar with existing genome encoding schemes, we proceeded to
    discuss the neuroevolution method, which uses different encoding schemes. We started
    with an introduction to the NEAT algorithm, which uses the direct genome encoding
    scheme and enhances it with the concept of the innovation number. The innovation
    number associated with each gene of the genotype provides a means to precisely
    track when a particular mutation was introduced. This feature makes crossover
    operations between two parents straightforward and easy to implement. The NEAT
    method emphasizes the importance of starting from a very basic genome that gradually
    becomes more complex during evolution. In this manner, the evolutionary process
    has an excellent chance of finding the optimal solution.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the concept of speciation was introduced, which keeps useful mutations
    by isolating them in particular species (niches). The species within one niche
    are only allowed to cross over with each other. Speciation is the great moving
    force behind natural evolution, and it was shown to have a high impact on neuroevolution
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: Having discussed the basic NEAT algorithm, we proceeded to a discussion of its
    derivatives to address the limitations of the original algorithm. One of the significant
    drawbacks of the NEAT algorithm is caused by using a direct genome encoding scheme.
    This scheme, while easy to visualize and implement, only encodes small topologies
    of the phenotype ANNs. With an increase in the phenotype ANN size, the size of
    the genome increases in linear proportion. This linear increase in genome size
    eventually makes it hard to maintain. Thus, to address these drawbacks, a series
    of extensions based on the indirect genome encoding schemes, such as HyperNEAT
    and ES-HyperNEAT, were introduced.
  prefs: []
  type: TYPE_NORMAL
- en: The HyperNEAT method uses an advanced format to represent connections between
    nodes of the phenotype ANN in the form of four-dimensional points in the hypercube.
    The chosen hypercube's dimensionality is based on the fact that connections between
    two nodes within an ANN can be encoded by the coordinates of connection endpoints
    in a medium called the substrate. The substrate topology provides a framework
    that draws connections between the nodes of the phenotype ANN. The strength of
    a connection that's drawn between two particular nodes in a substrate is estimated
    by the auxiliary neural network known as the **Compositional Pattern Producing
    Network** (**CPPN**). The CPPN receives the coordinates of the hyper-point (the
    coordinates of the connection endpoints) as input and calculates the strength
    of the connection. Also, it computes the value of the flag, which indicates whether
    a connection should be expressed or not. The experimenter defines the substrate
    configuration in advance. It is defined by the geometric properties of the problem
    to be solved. At the same time, the topology of the CPPN is evolved during the
    neuroevolution process using the NEAT algorithm. Thus, we have the best of both
    worlds. The power of the NEAT algorithm allows us to evolve the optimal CPPN configurations.
    At the same time, the indirect encoding scheme is maintained by the CPPN and allows
    us to represent large phenotype ANNs.
  prefs: []
  type: TYPE_NORMAL
- en: The ES-HyperNEAT method introduces a further enhancement to the original NEAT
    and HyperNEAT methods by proposing an advanced method of substrate evolution that's
    on par with the evolution of the connectivity CPPN. The substrate evolution is
    built around the notion of information density, which allows more dense node placement
    in the areas with higher information variability. This approach allows the neuroevolution
    process to discover substrate configurations that precisely follow the geometrical
    regularities that are exposed by the problem to be solved.
  prefs: []
  type: TYPE_NORMAL
- en: We finished the first chapter with a discussion about the fascinating search
    optimization method known as **Novelty Search** (**NS**). This method is based
    on the concept of guiding the evolutionary search by criteria that have been estimated
    using the novelty of the solutions found. Conventionally, search optimization
    is based on goal-oriented fitness criteria, which measure how close we are to
    the goal. But there is a whole area of real-world problems that have deceptive
    fitness function landscapes, which introduce strong local optima traps. The goal-oriented
    search has a good chance of getting stuck in one of these traps and failing to
    find the ultimate solution. At the same time, the search optimization method,
    which rewards the novelty of the solution found, allows us to avoid these traps
    by completely ignoring the proximity to the final goal. The NS method was shown
    to be effective in tasks of autonomous navigation through deceptive maze environments;
    it outperformed the objective-based search methods.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter of this book, we discussed how to set up a working environment
    correctly and what Python libraries can be used to experiment with neuroevolution.
  prefs: []
  type: TYPE_NORMAL
- en: Python libraries and environment setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we started by discussing the practical aspects of the neuroevolution
    methods. We discussed the pros and cons of popular Python libraries that provide
    implementations of the NEAT algorithm and its extensions.
  prefs: []
  type: TYPE_NORMAL
- en: Along with the highlights of each Python library, we also provided small code
    snippets, giving you a feel of how to use each specific library in your experiments.
  prefs: []
  type: TYPE_NORMAL
- en: After that, we proceeded to discuss how to correctly set up the working environment.
    The working environment must have the necessary dependencies installed to allow
    the usage of the mentioned Python libraries. The installation can be done using
    several methods. We considered the two most common ones – the standard **package
    installer for Python** (**PIP**) utility and the Anaconda Distribution. Another
    critical aspect of the working environment's preparation is the creation of isolated
    virtual Python environments for each specific experiment. The virtual environments
    provide the benefits of having different dependency configurations for varying
    combinations of experiments and the NEAT Python libraries that are used in them.
  prefs: []
  type: TYPE_NORMAL
- en: Having dependencies isolated in a virtual environment also allows easy management
    of all the installed dependencies as a whole. The environment can be quickly deleted
    from your PC with everything installed into it, thus freeing disk space. You can
    also reuse a specific virtual environment for different experiments, which depends
    on the same NEAT implementation library.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter should have got you familiar with every tool you need in order
    to start with neuroevolution experiments. In the next chapter, we proceeded to
    discuss the XOR solver experiment using the basic NEAT algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Using NEAT for XOR solver optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This was the first chapter in which we started experimenting with the NEAT algorithm.
    We did this by implementing a solver for one of the classic computer science problems.
    We started by building a solver for the XOR problem. The XOR problem solver is
    a computer science experiment in the field of reinforcement learning. The XOR
    problem cannot be linearly separated and thus requires a solver to find the non-linear
    execution path. However, we can find the non-linear execution path by introducing
    hidden layers into the ANN structure.
  prefs: []
  type: TYPE_NORMAL
- en: We discussed how the NEAT algorithm perfectly fits this requirement due to its
    inherent ability to evolve ANNs from a very simple or a complex topology by gradual
    complexification. In the XOR experiment, we started with an initial ANN topology
    that consisted of the two input nodes and a single output node. During the experiment,
    the relevant topology of the solver ANN was discovered, and it introduced an additional
    hidden node representing the non-linearity, as we expected.
  prefs: []
  type: TYPE_NORMAL
- en: Also, we explained how to define an appropriate fitness function to guide the
    evolutionary search and to understand how to implement it in the Python script.
    We put great attention into describing the hyperparameters that fine-tune the
    performance of the NEAT-Python library for the XOR experiment.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we acquired the skills that are necessary in order to implement
    basic solvers for essential computer science experiments and were ready to move
    on to more advanced experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Pole-balancing experiments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we continued with experiments related to the classic problems
    of computer science in the field of reinforcement learning. We started with a
    discussion of how to implement an avoidance control optimization method using
    the NEAT algorithm, allowing us to balance a cart-pole apparatus (or an inverted
    pendulum). We began with a single pole-balancing system and provided all the necessary
    equations of motion that allow us to numerically approximate real-world physical
    apparatus.
  prefs: []
  type: TYPE_NORMAL
- en: We learned how specific control actions could be applied to the cart-pole apparatus
    in the form of the bang-bang controller. The bang-bang controller is a unique
    form of control system that is designed to apply a series of actions with equal
    force but in different directions continuously. To manage a bang-bang controller,
    the control's ANN needs to continuously receive and analyze the state of the cart-pole
    apparatus and produce the relevant control signals. The input signals of the system
    are defined by the horizontal position of the cart on the track, its linear speed,
    the current angle of the pole, and the angular speed of the pole. The output of
    the system is a binary signal indicating the direction of a control action that
    needs to be applied.
  prefs: []
  type: TYPE_NORMAL
- en: The neuroevolution process uses the cart-pole apparatus' simulation for the
    trial and error process characteristic of every RL-style training algorithm. It
    maintains the population of the genomes that evolve from generation to generation
    until a successful solver is found. During their evolution, each organism in the
    population is tested against a simulation of the cart-pole apparatus. At the end
    of the simulation, it receives a reward signal in the form of the number of time
    steps during which it was able to keep the apparatus balanced within the track's
    bounds. The received reward signal defines the fitness of the organism and determines
    its fate during the neuroevolution process.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we discussed how the objective function could be defined using the mentioned
    reward signal. After that, you learned how to implement an objective function
    using Python.
  prefs: []
  type: TYPE_NORMAL
- en: Having finished with a single pole-balancing experiment, we looked at a modified
    version of this experiment. The modified version comprised two poles with different
    lengths connected to the moving cart that needed to be balanced. This experiment
    had more complicated physics and required the discovery of a much more sophisticated
    controller during the experiment.
  prefs: []
  type: TYPE_NORMAL
- en: Both experiments that were presented in this chapter highlighted the importance
    of keeping a well-balanced population of solvers with a moderate number of species.
    Too many species in the population may hinder the neuroevolution process by reducing
    the chance of reproduction between the two organisms belonging to different species.
    Furthermore, taking into account that the population size is fixed, the more species
    you have within the population, the less populated they become. Sparsely populated
    species reduce the chance of discovering useful mutations. On the other hand,
    separate species allow us to maintain useful mutations within each speciation
    niche and exploit each mutation further in the next generations. Thus, too few
    species are also harmful to evolution. At the end of the pole-balancing experiment,
    you gained some practical skills that relate to keeping the number of species
    balanced by tweaking the corresponding hyperparameters of the NEAT algorithm (such
    as the compatibility threshold).
  prefs: []
  type: TYPE_NORMAL
- en: Another essential feature of the neuroevolution process that was highlighted
    in the pole-balancing experiment is related to the selection of the right initial
    condition of the stochastic process that guides the evolutionary process. The
    neuroevolution method's implementation is built around a pseudo-random number
    generator, which provides the likelihood of genome mutations and crossover rates.
    In the pseudo-random number generator, the sequence of numbers that will be produced
    is solely determined by the initial seed value that is supplied to the generator
    at the beginning. By using the same seed value, it is possible to produce the
    same random number sequences using the pseudo-random generator.
  prefs: []
  type: TYPE_NORMAL
- en: As a result of the experiment with the evolving controller's ANN for the cart-pole
    balancers, we discovered that the probability of finding a successful solution
    strongly depends on the value of the random number generator seed.
  prefs: []
  type: TYPE_NORMAL
- en: Mastering the pole-balancing experiments allowed you to be prepared to solve
    more complex problems associated with autonomous navigation, which were discussed
    in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Autonomous maze navigation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we continued our experiments with neuroevolution as an attempt
    to create a solver that can find an exit from a maze. Maze solving is a fascinating
    problem as it allows us to study a new search optimization method called Novelty
    Search. In [Chapter 5](22365f85-3003-4b67-8e1e-cc89fa5e259b.xhtml), *Autonomous
    Maze Navigation*, and [Chapter 6](62301923-b398-43da-b773-c8b1fe383f1d.xhtml),
    *Novelty Search Optimization Method*, we explored a series of the maze navigation
    experiments using the goal-oriented search optimization and the Novelty Search
    optimization method.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you learned how to implement a simulation of a robot that has
    an array of sensors that detect obstacles and monitor its position within the
    maze. Also, we discussed how to implement a goal-oriented objective function to
    guide the evolutionary process. The mentioned objective-function implementation
    is calculated as Euclidean distance between the robot's final position and the
    maze's exit.
  prefs: []
  type: TYPE_NORMAL
- en: Using the maze navigation simulator and the defined objective function, we conducted
    two experiments with simple and hard maze configurations. The results of the experiments
    give us insights into the impact of the deceptive fitness function landscape on
    the performance of the evolutionary process. In local optima areas, neuroevolution
    tends to produce fewer species, which hinders its ability to explore novel solutions.
    In extreme cases, this leads to the degeneration of the evolutionary process.
    This can result in having only a single species in the entire population.
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, you learned how to avoid such misfortunes by adjusting NEAT
    hyperparameters such as the compatibility disjoint coefficient. This parameter
    controls how strong topological differences in the compared genomes affect the
    compatibility factor, which is used to determine whether genomes belong to the
    same species. As a result, we were able to boost speciation and increase population
    diversity. This change had a positive impact on the search for a successful maze
    solver, and we were able to find it for a simple maze configuration. However,
    a hard maze configuration with more extreme local optima areas resisted all our
    attempts to find a successful maze solver using the goal-oriented objective function.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we were ready to learn about the Novelty Search optimization method, which
    was devised to overcome the limitations of the goal-oriented search.
  prefs: []
  type: TYPE_NORMAL
- en: Novelty Search optimization method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In all the experiments preceding this chapter, we defined an objective function
    as a derivative based on its proximity to the final goal of the problem. However,
    the maze-solving problem posed challenges that could not be solved by a goal-oriented
    objective function. Specific maze configurations can introduce strong local optima
    in which a goal-oriented objective search may become stuck. In many cases, a deceptive
    fitness function landscape such as this effectively blocks a goal-oriented objective
    search from finding a successful solution.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, using the practical experience we gained during the creation of a maze
    solver in the previous chapter, we embarked on the path of creating a more advanced
    solver. Our brand new solver used the Novelty Search optimization method to guide
    the evolutionary process. However, first of all, we needed to define the appropriate
    metric to estimate the novelty score of each solution in each generation. The
    novelty score that was produced by this metric was going to be used as a fitness
    value that would be assigned to the genomes in the population of solvers. Thus,
    the novelty is integrated into the standard neuroevolution process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The novelty metric should measure how novel each solution is compared to the
    solutions we found in the past and all the solutions from the current generation.
    There are two ways to measure solution novelty:'
  prefs: []
  type: TYPE_NORMAL
- en: The genotypic novelty is the novelty score and shows how the genotype of the
    current solution differs from the genotypes of all the other found solutions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The behavioral novelty demonstrates how the behavior of the current solution
    differs within the problem space compared to all the other solutions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the problem of solving a maze, a good choice is to use a behavioral novelty
    score because, in the end, we are interested in reaching the maze exit, which
    can be facilitated by exposing a certain behavior. Furthermore, the behavioral
    novelty score is much easier to calculate than the genotypic novelty score.
  prefs: []
  type: TYPE_NORMAL
- en: The trajectory of a particular solver through the maze defines its behavioral
    space. Thus, we can estimate the novelty score by comparing the trajectory vectors
    of the solvers. Numerically, the novelty score can be estimated by calculating
    the Euclidean distance between trajectory vectors. To further simplify this task,
    we can use only the coordinates of the last point of the solver trajectory to
    estimate the novelty score.
  prefs: []
  type: TYPE_NORMAL
- en: Having defined the novelty metric, you learned how to implement it in the source
    code using Python and integrate it into the maze simulator you created in [Chapter
    5](22365f85-3003-4b67-8e1e-cc89fa5e259b.xhtml), *Autonomous Maze Navigation*.
    After that, you were ready to repeat the experiments from the previous chapter
    and compare the results.
  prefs: []
  type: TYPE_NORMAL
- en: The experiment with a simple maze solver demonstrated an improvement in the
    topology of the produced control ANN. The topology became optimal and less complicated.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, the experiment with hard maze configuration also failed to produce
    a successful solver, the same as it did in [Chapter 5](22365f85-3003-4b67-8e1e-cc89fa5e259b.xhtml), *Autonomous
    Maze Navigation*. The failure seems to be caused by the inefficiency of a particular implementation of
    the NEAT algorithm used in the experiment. I have implemented the NEAT algorithm
    in Go so that it solves the hard maze configuration with ease using the Novelty
    Search optimization. You can find it on GitHub at [https://github.com/yaricom/goNEAT_NS](https://github.com/yaricom/goNEAT_NS).
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 6](62301923-b398-43da-b773-c8b1fe383f1d.xhtml), *Novelty Search
    Optimization Method*, you learned that the Novelty Search optimization method
    allows you to find a solution, even when the fitness function has a deceptive
    landscape with many local optima traps scattered inside. You have learned that
    the stepping stones forming the way to the solution are not always obvious. Sometimes,
    you need to step back to find the correct way. That is the main idea behind the
    Novelty Search method. It tries to find a solution by completely ignoring the
    proximity to the final goal and rewarding the novelty of each intermediate solution
    that is found on the way.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we got acquainted with the standard NEAT algorithm, and we
    were ready to begin experimenting with its more advanced extensions.
  prefs: []
  type: TYPE_NORMAL
- en: Hypercube-based NEAT for visual discrimination
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter was the first of four chapters in which we discussed advanced neuroevolution
    methods. In this chapter, you learned about the indirect genome encoding scheme,
    which uses the **Compositional Pattern Producing Network** (**CPPN**) to aid with
    the encoding of large phenotype ANN topologies. The CPPN encoding scheme introduced
    by the NEAT extension is named **HyperNEAT**. This extension is built around the
    concept of the connectivity substrate that represents the phenotype ANN topology.
    At the same time, connections between nodes in the substrate are expressed as
    four-dimensional points within the hypercube. In the HyperNEAT method, the topology
    of the CPPN is the part that is evolving and guided by the NEAT algorithm. We
    had already discussed the particulars of HyperNEAT, so we skipped the rest of
    the details of HyperNEAT for brevity.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we presented you with the interesting task of visual discrimination,
    which highlights the ability of the HyperNEAT algorithm to distinguish patterns
    in the visual field. You learned that the HyperNEAT method could find a successful
    visual pattern discriminator due to its inherent ability to reuse the successful
    connectivity patterns it found multiple times in the substrate that encodes the
    phenotype ANN of the solver. This was possible because of the power of the CPPN,
    which can discover the right strategy by passing signals from the input nodes
    (the perceiving image) to the output nodes (representing results).
  prefs: []
  type: TYPE_NORMAL
- en: You learned how to choose the correct geometry of a substrate to effectively
    employ the capabilities of the CPPN to find the geometric regularities. After
    that, you had a chance to apply your acquired knowledge in practice by implementing
    the visual discriminator that was trained using the HyperNEAT algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Also, having completed the visual discriminator experiment, you were able to
    verify the effectiveness of the indirect encoding scheme. We did this by comparing
    the topology of the produced CPPN with the maximum possible number of connections
    in the discriminator ANN substrate. The results of the visual discriminator experiment
    were pretty impressive. We were able to achieve an information compression ratio
    of 0.11% by encoding the connectivity pattern among 14,641 possible connections
    of the substrate, with only 16 connections between 10 nodes of the CPPN.
  prefs: []
  type: TYPE_NORMAL
- en: Visual tasks expose a high demand for the discriminator ANN architecture due
    to the high dimensionality of the input signal. Thus, in [Chapter 8](9f3dce4d-2cc7-4307-a704-bfcfe4ad56b4.xhtml),
    *ES-HyperNEAT and the Retina Problem*, we proceeded with a review of another class
    of visual recognition problems.
  prefs: []
  type: TYPE_NORMAL
- en: ES-HyperNEAT and the retina problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to select the substrate configuration that
    is best suited for a specific problem space. However, it is not always obvious
    what configuration to choose. If you select the wrong configuration, you can significantly
    impact the performance of the training process. As a result, the neuroevolution
    process can fail to produce a successful solution. Also, particular substrate
    configuration details can only be discovered during the training process, and
    cannot be known in advance.
  prefs: []
  type: TYPE_NORMAL
- en: The problem with finding an appropriate substrate configuration was solved using
    the ES-HyperNEAT method. In this chapter, you learned how the neuroevolution process
    could automatically handle the evolution of the substrate configuration among
    the evolution of connectivity CPPNs. We introduced you to the concept of the quadtree
    data structure, which allows effective traversal through the substrate topology
    and the detection of areas with high information density. We learned that it is
    beneficial to automatically place new nodes into these areas to create more subtle
    connectivity patterns, which describe hidden regularities that can be found in
    the real world.
  prefs: []
  type: TYPE_NORMAL
- en: After you became familiar with the details of the ES-HyperNEAT algorithm, you
    learned how to apply it to solve the visual recognition task known as the retina
    problem. In this task, the neuroevolution process needs to discover a solver that
    can recognize valid patterns simultaneously in two separate visual fields. That
    is, the detector ANN must decide if patterns presented in the right and left visual
    fields are valid for each field. The solution of this task can be found by introducing
    the modular architecture to the topology of the detector ANN. In such a configuration,
    each ANN module is responsible only for pattern recognition in the related side
    of the retina.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we implemented a successful retina problem solver using the
    ES-HyperNEAT method. We were able to visually confirm that the produced topology
    of the detector ANN included the modular structures. Furthermore, from the experiment's
    results, you learned that the resulting detector ANN structure has near-optimal
    complexity. Once again, this experiment demonstrated the potential of neuroevolution-based
    methods to discover efficient solutions by method of gradual complexification.
  prefs: []
  type: TYPE_NORMAL
- en: All the experiments, including the one described in this chapter, used a particular
    form of the fitness function that is defined in advance before the experiments
    started. However, it would be interesting to explore how the performance of the
    neuroevolution algorithm changes if the fitness function is allowed to co-evolve
    along with the solution it tries to optimize.
  prefs: []
  type: TYPE_NORMAL
- en: Co-evolution and the SAFE method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we discussed how the co-evolution strategy is widely found
    in nature and can be transferred into the realm of neuroevolution. You learned
    about the most common co-evolutionary strategies that can be found in nature:
    mutualism, competition (predation or parasitism), and commensalism. In our experiment,
    we explored the commensalistic type of evolution, which can be defined in commensalistic
    relationships as follows: the members of one species gain benefits without causing
    harm or giving benefits to other participating species.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Having learned about evolution strategies in the natural world, you were ready
    to understand the concepts behind the SAFE method. The abbreviation **SAFE** means
    **Solution And Fitness Evolution**, which suggests that we have two co-evolving
    populations: the population of potential solutions and the population of the fitness
    function candidates. At each generation of evolution, we evaluate each potential
    solution against all the objective function candidates and choose the best fitness
    score, which is observed as the fitness of the genome encoding solution. At the
    same time, we evolve the commensalistic population of the fitness function candidates
    using the Novelty Search method. Novelty Search uses the genomic novelty of each
    genome in the population as a novelty metric to estimate the individual''s fitness
    score.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you learned how to implement a modified maze solving experiment
    based on the SAFE method to evaluate the performance of the co-evolution strategy.
    Also, you learned how to define the objective function to guide the evolution
    of the population of potential solutions. This objective function includes two
    fitness metrics: the first is the distance from the maze exit, while the second
    is the behavioral novelty of the solution that was found. These metrics are combined
    using the coefficients that are produced by a population of the fitness function
    candidates.'
  prefs: []
  type: TYPE_NORMAL
- en: As in all the previous chapters, you continued to improve your Python skills
    by implementing the SAFE method using the MultiNEAT Python library. In the next
    chapter, you continued by studying even more advanced methods, thereby allowing
    you to use neuroevolution to train Atari game solvers.
  prefs: []
  type: TYPE_NORMAL
- en: Deep Neuroevolution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we presented you with the concept of deep neuroevolution, which
    can be used to train **Deep Artificial Neural Networks** (**DNNs**). You learned
    how deep neuroevolution can be used to train Atari game-playing agents using the
    deep reinforcement learning algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: We started with a discussion of the basic concepts behind reinforcement learning.
    We paid special attention to the popular Q-learning algorithm, which is one of
    the classic implementations of reinforcement learning. After that, you learned
    how a DNN could be used to approximate the Q-value function for complex tasks
    that cannot be approximated by a simple action-state table with Q-values. Next,
    we discussed how the neuroevolution-based method could be used to find the trainable
    parameters of the DNN. You learned that neuroevolution evolves a DNN for Q-value
    function approximation. As a result, we can train the appropriate DNN without
    using any form of error backpropagation that is common in conventional methods
    of DNN training.
  prefs: []
  type: TYPE_NORMAL
- en: Having learned about deep reinforcement learning, you were ready to apply your
    knowledge in practice by implementing the Atari game solver agent. To train an
    agent to play the Atari game, it needs to read the pixels of the game screen and
    derive the current state of the game. After that, using the extracted game state,
    the agent needs to select an appropriate action to be executed in the game environment.
    The ultimate goal of the agent is to maximize the final reward that will be received
    after completion of a particular game episode. Thus, we have classic trial and
    error learning, which is the essence of reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned, the game-playing agent needs to parse game screen pixels. The
    best way to do this is to use a **Convolutional Neural Network** (**CNN**) to
    process the inputs that are received from the game screen. In this chapter, we
    discussed the essentials of the CNN architecture and how it can be integrated
    into the game-playing agent. You learned how to implement CNN in Python using
    a popular TensorFlow framework.
  prefs: []
  type: TYPE_NORMAL
- en: Also, you learned about a unique genome encoding scheme that was designed specifically
    for tasks related to deep neuroevolution. This scheme allows us to encode the
    phenotype ANNs with millions of trainable parameters. The proposed scheme employs
    the seeds of the pseudorandom number generator to encode the connection weights
    of the phenotype ANN. In this encoding scheme, the genome was represented as a
    list of the random generator seeds. Each seed is used consequentially to generate
    all the connection weights from a source of pseudorandom numbers.
  prefs: []
  type: TYPE_NORMAL
- en: After learning about the details of genome encoding, you were ready to start
    an experiment that aimed to create an agent that was able to play the Frostbite
    Atari game. Furthermore, you learned how to employ a modern GPU to accelerate
    the computations involved in the training process. At the end of this chapter,
    we also presented an advanced visualization tool (VINE) that allows us to study
    the results of the neuroevolution experiments.
  prefs: []
  type: TYPE_NORMAL
- en: With this chapter, we finished our brief acquaintance with the most popular
    neuroevolution methods that exist at the time of writing this book. However, there
    are still many things that you can learn in the fast-growing field of applied
    artificial intelligence and neuroevolution methods.
  prefs: []
  type: TYPE_NORMAL
- en: Where to go from here
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We hope that your journey through the neuroevolution methods that were presented
    in this book was pleasant and insightful. We have done our best to present you
    with the most recent achievements in the field of neuroevolution. However, this
    field of applied computer science is developing rapidly, and new achievements
    are announced almost every month. There are many laboratories in universities,
    as well as in corporations around the globe, working on applying neuroevolution
    methods to solve tasks that are beyond the strength of mainstream deep learning
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: We hope that you have become fond of the neuroevolution methods we discussed
    and are eager to apply them in your work and experiments. However, you need to
    continue your self-education to keep pace with the next achievements in the area.
    In this section, we will present some places where you can continue your education.
  prefs: []
  type: TYPE_NORMAL
- en: Uber AI Labs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The core of Uber AI Labs is built around the Geometric Intelligence startup
    that was co-founded by Kenneth O. Stanley – one of the prominent pioneers in the
    field of neuroevolution. He is the author of the NEAT algorithm, which we have used often
    in this book. You can follow the works of Uber AI Labs at [https://eng.uber.com/category/articles/ai/](https://eng.uber.com/category/articles/ai/).
  prefs: []
  type: TYPE_NORMAL
- en: alife.org
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **International Society for Artificial Life** (**ISAL**) is a well-established
    community of researchers and enthusiasts from all around the world who are interested
    in scientific research activities related to artificial life. Genetic algorithms
    and neuroevolution, in particular, are among the areas of interest of this society.
    ISAL publishes the Artificial Life journal and sponsors a variety of conferences.
    You can find out more about ISAL activities at [http://alife.org](http://alife.org/).
  prefs: []
  type: TYPE_NORMAL
- en: Open-ended evolution at Reddit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The concept of open-ended evolution is directly related to genetic algorithms
    and neuroevolution in particular. Open-ended evolution assumes the creation of
    an evolutionary process that is not bound by any particular goal. It is inspired
    by the natural evolution of biological organisms, which produced us, humans. There
    is a dedicated subreddit where all of those who are interested discuss the research.
    You can find it at [https://www.reddit.com/r/oee/](https://www.reddit.com/r/oee/).
  prefs: []
  type: TYPE_NORMAL
- en: The NEAT Software Catalog
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The University of Central Florida maintains the list of software libraries that
    implement the NEAT algorithm and its extensions. The software is moderated by
    Kenneth O. Stanley, the author of the NEAT algorithm. My implementation of the
    NEAT and Novelty Search in Go language is also present in this catalog. You can
    find it at [http://eplex.cs.ucf.edu/neat_software/](http://eplex.cs.ucf.edu/neat_software/).
  prefs: []
  type: TYPE_NORMAL
- en: arXiv.org
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[arXiv.org](http://arxiv.org/) is a well-known service that publishes preprints
    of papers in many areas of science. It is generally an excellent source of cutting-edge
    information in the area of computer science. You can search through it for neuroevolution-related
    papers using the following search query: [http://search.arxiv.org:8081/?query=neuroevolution&in=grp_cs](http://search.arxiv.org:8081/?query=neuroevolution&in=grp_cs).'
  prefs: []
  type: TYPE_NORMAL
- en: The NEAT algorithm paper
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The original dissertation written by Kenneth O. Stanley describing the NEAT
    algorithm is a very enlightening read and is recommended for everyone interested
    in neuroevolution. It is available at [http://nn.cs.utexas.edu/downloads/papers/stanley.phd04.pdf](http://nn.cs.utexas.edu/downloads/papers/stanley.phd04.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we briefly summarized what we learned in this book. You also
    learned about the places where you can search for further insights and continue
    your self-education.
  prefs: []
  type: TYPE_NORMAL
- en: We are happy to live in an era where the future becomes a reality at such a
    pace that we completely fail to notice the tremendous changes that happen in our
    life. Humanity is rapidly moving on a path to mastering the marvels of gene editing
    and synthetic biology. We continue to conquer the deep mysteries of the human
    brain, which opens the way for an ultimate understanding of our consciousness.
    Our advanced experiments in cosmology allow us to zoom closer and closer to the
    very first moments of the Universe.
  prefs: []
  type: TYPE_NORMAL
- en: We have built an advanced piece of mathematical apparatus that allows us to
    describe such mysteries as a neutrino that, on its path, can become an electron
    and after that, a neutrino again. Our technological achievements can't be easily
    distinguished from magic, as Arthur C. Clark stated.
  prefs: []
  type: TYPE_NORMAL
- en: Life is about feeling its beauty. Keep your mind sharp, and always be curious.
    We are standing on the edge, where sparks from the research of synthetic consciousness
    will ignite the evolution of novel life forms. And who knows – maybe you will
    be the one who will start this.
  prefs: []
  type: TYPE_NORMAL
- en: Thank you, my dear reader, for your time and effort. I look forward to seeing
    what you will create using the knowledge you've gained from this book.
  prefs: []
  type: TYPE_NORMAL
