<html><head></head><body>
		<div id="_idContainer090">
			<h1 class="chapter-number" id="_idParaDest-154"><a id="_idTextAnchor193"/><a id="_idTextAnchor194"/>9</h1>
			<h1 id="_idParaDest-155"><a id="_idTextAnchor195"/>LLM Output Evaluation</h1>
			<p>Regardless of the form factor of your intelligent application, you must evaluate your use of <strong class="bold">large language models</strong> (<strong class="bold">LLMs</strong>). The <strong class="bold">evaluation</strong> of a computational system determines the system’s performance, gauges its reliability, and analyzes its security <span class="No-Break">and privacy.</span></p>
			<p>AI systems are <strong class="bold">non-deterministic</strong>. You cannot be certain what an AI system will output until you run an input through it. This means that you must evaluate how the AI system performs on a variety of inputs to have confidence that it performs in line with your requirements. To be able to change the AI system without introducing any unexpected regressions, you also need to have robust evaluations. Evaluations can help catch these regressions before releasing the AI system <span class="No-Break">to customers.</span></p>
			<p>In LLM-powered intelligent applications, evaluations measure the effect of components such as the model chosen and any hyperparameters used with the model, such as temperature, prompting, and <strong class="bold">retrieval-augmented generation</strong> (<strong class="bold">RAG</strong>) pipelines. Since the age of LLMs is still new as of writing in mid-2024, there is still an ongoing debate about when and how to best evaluate these LLM-powered intelligent applications. However, there are emerging best practices that you can use to direct <span class="No-Break">your evaluations.</span></p>
			<p>In this chapter, you will learn about how and why you should evaluate the use of LLMs in your intelligent application. You will be able to use the concepts and metrics discussed to evaluate current classes of intelligent applications, such as chatbots, and emerging ones, such as AI agents. The concepts learned here will be applicable for years to come, regardless of the form factors of future generations of <span class="No-Break">intelligent applications.</span></p>
			<p>This chapter will cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Understanding <span class="No-Break">LLM evaluation</span></li>
				<li><span class="No-Break">Model benchmarking</span></li>
				<li><span class="No-Break">Evaluation datasets</span></li>
				<li>Key metrics for <span class="No-Break">LLM evaluation</span></li>
				<li>The role of human review in <span class="No-Break">LLM evaluation</span></li>
				<li>Using evaluations as guardrails for <span class="No-Break">your application</span></li>
			</ul>
			<h1 id="_idParaDest-156"><a id="_idTextAnchor196"/>Technical requirements</h1>
			<p>You will need the following technical requirements to run the code in <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>A programming environment with Python <span class="No-Break">3.x installed.</span></li>
				<li>An OpenAI API key. To create an API key, refer to the OpenAI documentation <span class="No-Break">at </span><a href="https://platform.openai.com/docs/quickstart/step-2-set-up-your-api-key"><span class="No-Break"><span class="P---URL">https://platform.openai.com/docs/quickstart/step-2-set-up-your-api-key</span></span></a><span class="No-Break">.</span></li>
			</ul>
			<h1 id="_idParaDest-157"><a id="_idTextAnchor197"/>What is LLM evaluation?</h1>
			<p><strong class="bold">LLM evaluation</strong>, or <strong class="bold">LLM evals</strong>, is the systematic process of assessing LLMs and the intelligent applications that use them. This involves profiling their performance on specific tasks, reliability under certain conditions, effectiveness in particular use cases, and other criteria to understand a model’s overall capabilities. You want to make sure that your intelligent application meets certain standards as measured by <span class="No-Break">your evaluations.</span></p>
			<p>You also should be able to measure how the AI system’s performance evolves as you change components of the application or data used in the application. For example, if you want to change the LLM used in your application or a prompt, you should be able to measure the impact of these changes <span class="No-Break">with evaluations.</span></p>
			<p>Being able to measure the impact of changes is particularly important as the quality of an application improves. Once an intelligent application is “pretty good,” it can be quite challenging for human reviewers to assess whether and how a system has improved or regressed based on a change. For instance, if you have a travel assistant chatbot that successfully meets users’ expectations 90% of the time, it can be challenging and time-intensive for human reviewers to assess the impact of a small change that would raise the success rate <span class="No-Break">to 90.5%.</span></p>
			<p>When designing an evaluation suite for your LLM-powered intelligent application, you should consider the <span class="No-Break">following aspects:</span></p>
			<ul>
				<li><strong class="bold">Security</strong>: The AI system should not reveal any private or confidential information that it has access to. This can include both information in the LLM’s weights and information retrieved by <span class="No-Break">the application.</span></li>
				<li><strong class="bold">Reputation</strong>: The AI system should not generate output that could harm your business. For example, you would not want your chatbot to recommend your competitor’s services over your own under <span class="No-Break">any circumstances.</span></li>
				<li><strong class="bold">Correctness</strong>: The AI system should respond with correct output that does not include mistakes <span class="No-Break">or hallucinations.</span></li>
				<li><strong class="bold">Style</strong>: The AI system should respond according to the tone and style guidelines you specify. For example, if you are developing a legal chatbot, you may want the chatbot to maintain a formal tone and use appropriate <span class="No-Break">legal terminology.</span></li>
				<li><strong class="bold">Consistency</strong>: The AI system should generate output that is consistent with expectations. Given the same input, you should expect the system to perform in a predetermined manner. The response can differ, but any difference should be consistent. For example, if you are building a system that creates playlists based on a song, you would probably want it to generate similar playlists given an input song, even if there are different songs or different song orders on the <span class="No-Break">output playlist.</span></li>
				<li><strong class="bold">Ethics</strong>: The AI system should respond in line with a set of ethical principles. By defining expected behavior in an evaluation dataset, you can also help define what the ethical standards of the system should be. For example, an AI system should never generate biased or discriminatory content, and it should handle sensitive topics with care <span class="No-Break">and respect.</span></li>
			</ul>
			<p>In the next section, you will learn which points in your application you should evaluate. You will also review an example intelligent application that is used throughout this chapter in code examples to demonstrate <span class="No-Break">the concepts.</span></p>
			<h2 id="_idParaDest-158"><a id="_idTextAnchor198"/>Component and end-to-end evaluations</h2>
			<p>You must consider <em class="italic">where</em> in your application you want to perform the evaluations. Generally, you should evaluate all LLM components of a system and the <span class="No-Break">end-to-end system.</span></p>
			<p>To illustrate this idea about where to think about evaluations in your intelligent application, this chapter uses the example of a travel assistant chatbot. The chatbot uses RAG to make travel recommendations and answers questions based on a dataset of documents of popular tourist destinations and activities. Since this chapter is about evaluation, it will not go into detail about how the components of the application are built. Later on in the chapter, you will look at implementations of how you can evaluate this application’s <span class="No-Break">LLM usage.</span></p>
			<p>The travel assistant chatbot has the <span class="No-Break">following components:</span></p>
			<ul>
				<li><strong class="bold">Retriever</strong>: Finds the relevant documents to help inform answers in response to user messages. The retriever uses vector search to find the relevant documents. It also uses LLMs for <span class="No-Break">the following:</span><ul><li><strong class="bold">Metadata extractor</strong>: Extract any place name from the user query. This can be used to pre-filter the search results to include documents only about the <span class="No-Break">relevant place.</span></li><li><strong class="bold">Query pre-processor</strong>: Convert user messages into better <span class="No-Break">search queries.</span></li><li><strong class="bold">Retrieved documents post-processor</strong>: Mutate retrieved documents to create a list of <span class="No-Break">relevant facts.</span></li></ul></li>
				<li><strong class="bold">Relevancy guardrail</strong>: LLM call that makes sure that the user is only talking to the chatbot about travel-related topics. If the relevancy guardrail determines that the user message is irrelevant, the chatbot does not answer the user’s irrelevant question and prompts the user to ask something <span class="No-Break">more relevant.</span></li>
				<li><strong class="bold">Responder</strong>: Uses an LLM to respond to the user message based on the <span class="No-Break">retrieved content.</span></li>
			</ul>
			<p><span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.1</em> illustrates how these components <span class="No-Break">work together.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer088">
					<img alt="" role="presentation" src="image/B22495_09_01.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.1: Components of the travel assistant example chatbot</p>
			<h3><a id="_idTextAnchor199"/>Component evaluation</h3>
			<p>Every subsystem of your intelligent application that calls an LLM can be considered a <strong class="bold">component</strong>. You should evaluate all components, as each component contributes to the system’s overall performance. By evaluating each component, you can ensure that every part meets the required quality standards and performs reliably. This also lets you change components with more confidence since you can have clarity on how the changes are affecting all parts of <span class="No-Break">the system.</span></p>
			<p>One component can also contain subcomponents. You should evaluate the parent component and the child components with separate evaluations. For example, in the travel assistant chatbot, you should evaluate all individual components that use an LLM, such as the query pre-processor and response generator. You should also evaluate the retriever, considering its three LLM subcomponents as a <span class="No-Break">single component.</span></p>
			<p>By evaluating all logical LLM components, you can get a better understanding of the entire system’s behavior. This understanding lets you make changes to individual components while knowing the effect that those changes will have on other <span class="No-Break">related components.</span></p>
			<h3><a id="_idTextAnchor200"/>End-to-end evaluation</h3>
			<p><strong class="bold">End-to-end evaluation</strong> examines the performance of the entire integrated system. These evaluations capture aspects such as real-world applicability, user experience, and system reliability. They help identify potential bottlenecks or weaknesses in the overall architecture that may not be apparent when evaluating the <span class="No-Break">LLM alone.</span></p>
			<p>For RAG systems, this involves evaluating not only the language model’s output but also the efficiency and accuracy of the retrieval mechanism, the relevance of retrieved information, and how well the system combines external knowledge with the LLM’s <span class="No-Break">inherent capabilities.</span></p>
			<p>In the case of the travel assistant chatbot, an end-to-end evaluation would examine how the chatbot responds to user input. This evaluation considers all the intermediate LLM components and retrieval. You can evaluate qualitative aspects of the system, such as how relevant the answer is to the user question and whether there are <span class="No-Break">any hallucinations.</span></p>
			<p>In a later section, <em class="italic">Evaluation metrics</em>, you will learn more about ways to evaluate end-to-end systems. Before you learn how to apply these evaluation metrics to your LLM-powered intelligent application, you will learn how to assess which LLMs are most suitable for your application with model benchmarks in the <span class="No-Break">next section.</span></p>
			<h1 id="_idParaDest-159"><a id="_idTextAnchor201"/>Model benchmarking</h1>
			<p>The LLM itself is a fundamental component of any intelligent application. Given that there are many LLMs that may be suitable for your application, it is helpful to compare them to each other to see which will best serve your application. To compare multiple models, you can assess them all against a standard set of evaluations. This process of comparing models across a uniform set of evaluations is called <strong class="bold">model benchmarking</strong>. Benchmarking can help you understand the model’s capabilities <span class="No-Break">and limitations.</span></p>
			<p>Often, the LLMs that perform best on benchmarks are the largest models, such as GPT-4 and Claude 3 Opus. However, these larger models also tend to be more expensive to run and slow to generate, compared to smaller models, such as GPT-4o mini and Claude <span class="No-Break">3 Haiku.</span></p>
			<p>Even if the larger models are prohibitively expensive, it can still be helpful to use them when developing your application since they set a baseline of ideal system performance. You can design your evaluations around your system using these models, substitute the smaller models, and then work on optimizing the system to try to meet the standard of the system using the <span class="No-Break">larger model.</span></p>
			<p>When new LLMs are released, they are typically evaluated against a standard set of benchmarks. These standard benchmarks help developers understand how the <span class="No-Break">models compare.</span></p>
			<p>Here are a few popular LLM benchmarks that many models are <span class="No-Break">evaluated against:</span></p>
			<ul>
				<li><strong class="bold">Massive Multi-Task Language Understanding</strong> (<strong class="bold">MMLU</strong>): This benchmark measures a model’s knowledge acquisition using college-level multiple-choice questions. It evaluates whether the model selects the <span class="No-Break">correct answer.</span><p class="list-inset">You can learn more about this benchmark <span class="No-Break">at </span><a href="https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu"><span class="No-Break"><span class="P---URL">https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu</span></span></a><span class="No-Break">.</span></p></li>
				<li><strong class="bold">HellaSwag</strong>: This benchmark measures a model’s common-sense reasoning ability using multiple-choice text completion. It evaluates whether the model selects the correct <span class="No-Break">sentence completion.</span><p class="list-inset">You can learn more about this benchmark <span class="No-Break">at </span><a href="https://paperswithcode.com/sota/sentence-completion-on-hellaswag"><span class="No-Break"><span class="P---URL">https://paperswithcode.com/sota/sentence-completion-on-hellaswag</span></span></a><span class="No-Break">.</span></p></li>
				<li><strong class="bold">HumanEval</strong>: This benchmark measures a model’s programming ability in Python. It prompts a model to create a Python function to solve a task. It then evaluates whether the function that the model outputs is correct using preconstructed <span class="No-Break">unit tests.</span><p class="list-inset">You can learn more about this benchmark <span class="No-Break">at </span><a href="https://paperswithcode.com/sota/code-generation-on-humaneval"><span class="No-Break"><span class="P---URL">https://paperswithcode.com/sota/code-generation-on-humaneval</span></span></a><span class="No-Break">.</span></p></li>
				<li><strong class="bold">MATH</strong>: This benchmark measures a model’s ability to solve math word problems. It evaluates whether the model reaches the <span class="No-Break">correct solution.</span><p class="list-inset">You can learn more about this benchmark <span class="No-Break">at </span><a href="https://paperswithcode.com/dataset/math"><span class="No-Break"><span class="P---URL">https://paperswithcode.com/dataset/math</span></span></a><span class="No-Break">.</span></p></li>
			</ul>
			<p>You can assess the performance of LLMs based on these benchmarks to choose models that are most suitable for your application. For example, in the case of the travel assistant chatbot, a high score on MMLU is probably a good indication that the model is well suited for answering travel questions, as it would be helpful for the model to have world knowledge to inform its answers. In contrast, high scores on the HumanEval Python coding benchmark would likely have little bearing on the quality of its <span class="No-Break">travel recommendations.</span></p>
			<p>You can also create your own benchmarks to assess the LLM’s performance on a domain relevant to your application. You can even style these benchmarks after existing benchmarks. For the travel assistant chatbot, you could make a benchmark of multiple-choice questions about popular travel destinations styled after MMLU. This travel benchmark would help determine which models possess the best background information about travel. By choosing a model with more travel-related knowledge, you could improve the quality of <span class="No-Break">your responses.</span></p>
			<p>These benchmarks can also reveal which models are best suited for different components of your application. For instance, for the travel assistant chatbot, perhaps you need to use a large, expensive model that possesses significant knowledge of vacation destinations in the main responder, but can use a faster, cheaper model in other LLM components, such as the input <span class="No-Break">relevance guardrail.</span></p>
			<p>Once you have an idea of which models are appropriate for your AI components, you can start building those systems. To understand and measure how well these AI systems use the LLMs, you must create evaluation datasets and run evaluation metrics over them. In the next two sections, you will learn about creating these evaluation datasets <span class="No-Break">and metrics.</span></p>
			<h2 id="_idParaDest-160"><a id="_idTextAnchor202"/>Evaluation datasets</h2>
			<p>You must create <strong class="bold">evaluation datasets</strong> to measure AI system performance. An evaluation dataset is the data that you input into an AI system to produce an output that measures how well the AI system performs. Evaluation datasets often include some criteria that an <strong class="bold">evaluation metric</strong> can use to determine the score of the evaluation. An evaluation metric takes the input and the output of an AI system and returns a score measuring how the AI system performed for the case. You will learn more about evaluation metrics in the <em class="italic">Evaluation metrics</em> section of <span class="No-Break">this chapter.</span></p>
			<p>An evaluation dataset is a set of distinct evaluation cases. Each evaluation case typically includes the <span class="No-Break">following information:</span></p>
			<ul>
				<li><strong class="bold">Input</strong>: The data inputted into the <span class="No-Break">AI system.</span></li>
				<li><strong class="bold">Reference</strong>: Criteria that the evaluation metric uses to evaluate whether the AI system output is correct. The reference is often an ideal output for the system given the input. This ideal output is often called the <strong class="bold">golden answer</strong> or <strong class="bold">reference answer</strong>. This could also be a rubric of criteria that the AI system output should meet. Sometimes, evaluation datasets do not include references because the evaluation metric used on the dataset doesn’t need reference criteria to evaluate the input. When an evaluation does not require an output reference, it is called a <span class="No-Break"><strong class="bold">reference-free evaluation</strong></span><span class="No-Break">.</span></li>
				<li><strong class="bold">Metadata</strong>: An evaluation usually also includes metadata with each evaluation case. This can be a unique name, an ID, or <span class="No-Break">a tag.</span></li>
			</ul>
			<p>Evaluation datasets tend to conform to tabular or document-based data structures. Therefore, they are often stored in formats such as CSV, JSON, <span class="No-Break">or Parquet.</span></p>
			<p>Here is a small example evaluation dataset of user messages and model answers for the travel <span class="No-Break">assistant chatbot:</span></p>
			<table class="No-Table-Style _idGenTablePara-1" id="table001-5">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Input</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Golden answer</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Tags</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="source-inline">What should I do in New York City </strong><span class="No-Break"><strong class="source-inline">in July?</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>Check out Times Square, go to an outdoor concert, and visit the Statue <span class="No-Break">of Liberty.</span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">["todo", "</strong><span class="No-Break"><strong class="source-inline">nyc", "usa"]</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="source-inline">Can you help me with my </strong><span class="No-Break"><strong class="source-inline">math homework?</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>I’m sorry, I cannot help you with your math homework since I am a travel assistant. Do you have any <span class="No-Break">travel-related questions?</span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">["</strong><span class="No-Break"><strong class="source-inline">security"]</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="source-inline">What's the capital </strong><span class="No-Break"><strong class="source-inline">of France?</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>Paris is the capital <span class="No-Break">of France.</span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">["</strong><span class="No-Break"><strong class="source-inline">europe", "france"]</strong></span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 9.1: Evaluation dataset for the example chatbot</p>
			<p>The remainder of this chapter uses this dataset in <span class="No-Break">its evaluations.</span></p>
			<p>What exactly you include in an evaluation dataset depends on what functionality you want to evaluate and the evaluation metrics you are using. In the upcoming <em class="italic">Evaluation metrics</em> section, you will learn more about what exact information you need to include in your evaluation datasets for different <span class="No-Break">evaluation metrics.</span></p>
			<p>Regardless of what exact evaluation metrics you use, it is important to have a representative evaluation dataset. The dataset should be representative of the types of inputs that you expect your AI system to receive in addition to edge cases that you want to optimize the <span class="No-Break">system around.</span></p>
			<p>There is no precise number of evaluation cases that you should have or formula for determining what that number should be for a given scenario. Nevertheless, you can use the following very rough heuristics for building <span class="No-Break">evaluation datasets:</span></p>
			<ul>
				<li>Always have at least 10 evaluation cases for a <span class="No-Break">given metric</span></li>
				<li>Have at least 100-200 representative evaluation cases to get an idea of end-to-end <span class="No-Break">system performance</span></li>
			</ul>
			<p>Next, you will learn about a few strategies to help you create representative <span class="No-Break">evaluation datasets.</span></p>
			<h2 id="_idParaDest-161"><a id="_idTextAnchor203"/>Defining a baseline</h2>
			<p>To bootstrap your evaluation dataset, you must create a set of evaluation cases that cover the general expected behaviors and edge cases around which you want to optimize for in <span class="No-Break">your application.</span></p>
			<p>To define the common expectations of this baseline, it can be useful to collaborate with any stakeholders of the AI system to create evaluation cases for the <span class="No-Break">following areas:</span></p>
			<ul>
				<li><strong class="bold">A diverse sample of expected common inputs</strong>: You may be able to leverage existing data to help inform these evaluation cases. For example, in the travel assistant chatbot, you could derive evaluation cases from top Google search queries about travel. This follows the logic that whatever people are searching for on Google, they are likely to ask your chatbot about <span class="No-Break">as well.</span></li>
				<li><strong class="bold">Edge cases around which you want to optimize your system</strong>: Edge cases can include inputs that test the security and ethical guardrails of the system. If you red team your AI system, as discussed further in <a href="B22495_12.xhtml#_idTextAnchor253"><span class="No-Break"><em class="italic">Chapter 12</em></span></a>, <em class="italic">Correcting and Optimizing Your Generative AI Application</em>, you can likely find some good edge cases from the red <span class="No-Break">teaming results.</span></li>
			</ul>
			<p>This baseline of evaluation cases is often enough to release the AI system to a user-facing environment. Once the AI system is in use, you can validate the efficacy of your baseline evaluation cases and create additional evaluation cases, as discussed in the <span class="No-Break">next section.</span></p>
			<h2 id="_idParaDest-162"><a id="_idTextAnchor204"/>User feedback</h2>
			<p>After you release your AI system, you can source evaluation cases from user data to continuously refine and improve the system’s performance. If your application has any user feedback mechanisms, such as ratings or comments, you can use these to identify cases where the system succeeds <span class="No-Break">or fails.</span></p>
			<p>Generally, you should manually review any application data before adding it to an evaluation dataset. You want to ensure that the case is suitable for your evaluation dataset and does not contain any sensitive information. You can also add metadata, such as tags or an evaluation <span class="No-Break">case name.</span></p>
			<p>Even if the application data is not suitable for an evaluation case, perhaps because it is improperly formatted or contains personally identifiable information, you can modify it to create a suitable <span class="No-Break">evaluation case.</span></p>
			<p>It is possible to create a pipeline that uses LLMs to fully automate the process of creating evaluation cases from user feedback. However, you should strongly consider maintaining a human in the loop for the <span class="No-Break">following reasons:</span></p>
			<ul>
				<li>You want the quality of the evaluation dataset to be very high, which you can more easily ensure with human reviewers than an <span class="No-Break">LLM-based system.</span></li>
				<li>It is beneficial for the people involved in the AI system development to be aware of the cases in their evaluation dataset. This awareness helps give them context into the <span class="No-Break">system capabilities.</span></li>
				<li>Given that evaluation datasets typically do not need to be particularly large to be effective (a few hundred evaluation cases is often sufficient), creating an LLM-based system to create evaluation cases may be excessive for the requirements of <span class="No-Break">the task.</span></li>
			</ul>
			<p>Building your evaluation dataset from user feedback is an effective way to ground your evaluations in the types of inputs that users <span class="No-Break">are providing.</span></p>
			<h2 id="_idParaDest-163"><a id="_idTextAnchor205"/>Synthetic data</h2>
			<p>LLMs are capable tools for generating evaluation datasets. When you use an LLM to generate data, it is called <strong class="bold">synthetic data</strong>. You might want to use synthetic data because it is quite time consuming and tedious for humans to create evaluation cases. LLMs can help make the process of creating evaluation data faster <span class="No-Break">and easier.</span></p>
			<p>There are various strategies to create synthetic evaluation data. As of writing in mid-2024, there is no structured set of best practices for creating synthetic evaluation data. However, the following are some principles that you can keep in mind when creating synthetic <span class="No-Break">evaluation cases:</span></p>
			<ul>
				<li>Have a human in the loop. A human should review all synthetic data cases and edit or remove them as needed. This provides quality control on the <span class="No-Break">synthetic data.</span></li>
				<li>LLMs are very effective at creating <strong class="bold">perturbations</strong> on existing evaluation cases. Perturbations are slight variations on existing data, such as the rephrasing of a sentence. You can use perturbations to see whether the AI system performs differently based on slight changes. Ideally, a system should behave consistently <span class="No-Break">across perturbations.</span></li>
				<li>Often, an LLM-based chatbot, such as ChatGPT, Claude, or Gemini, can be sufficient to help create synthetic data. The back-and-forth of the chatbot interface can also help you refine and iterate on your synthetic <span class="No-Break">data creation.</span></li>
			</ul>
			<p>Using synthetic data in combination with a baseline and data from user feedback, you can create datasets to effectively evaluate the performance of your AI systems. You must pair these datasets with metrics to run evaluations. In the following section, you will learn more about <span class="No-Break">evaluation metrics.</span></p>
			<h1 id="_idParaDest-164"><a id="_idTextAnchor206"/>Evaluation metrics</h1>
			<p>To perform evaluations on your AI system, you must combine your evaluation data with an <strong class="bold">evaluation metric</strong>. An evaluation metric takes the input and the output of an AI system and returns a score measuring how the AI system performed for <span class="No-Break">the case.</span></p>
			<p>Evaluation metrics typically return scores between 0 and 1. The metric is called a <strong class="bold">binary metric</strong> if it returns only the scores of 0 <em class="italic">or</em> 1. The metric is called a <strong class="bold">normalized metric</strong> if it returns a score <em class="italic">between</em> 0 and 1, inclusive. Binary metrics clearly determine if the case passes or fails, 0 being fail and 1 being pass. Normalized metrics present a more nuanced view of how the AI system performs, but that nuance can lack interpretability. To add clarity to normalized metrics, you can set a minimum <strong class="bold">threshold</strong> score that the metric must return to be considered a pass. For example, say the metric <strong class="source-inline">Foo</strong> returns a score of <strong class="source-inline">0.6</strong> for an evaluation case and <strong class="source-inline">0.7</strong> for another. If you have a threshold of 0.65, then the <strong class="source-inline">0.6</strong> score is considered a fail and the <strong class="source-inline">0.7</strong> score <span class="No-Break">a pass.</span></p>
			<p>Evaluation metrics for LLM systems broadly fall into the <span class="No-Break">following categories:</span></p>
			<ul>
				<li><strong class="bold">Assertion-based metrics</strong>: Metrics that evaluate if an AI system output matches an in-code assertion, such as equality or regular <span class="No-Break">expression match.</span></li>
				<li><strong class="bold">Statistical metrics</strong>: Metrics that use a statistical algorithm to evaluate the output of an <span class="No-Break">AI system.</span></li>
				<li><strong class="bold">LLM-as-a-judge metrics</strong>: Metrics that use an LLM to evaluate if the output of an AI system meets <span class="No-Break">qualitative criteria.</span></li>
				<li><strong class="bold">RAG metrics</strong>: Metrics that evaluate RAG systems. Generally, RAG metrics use LLMs as judges. This chapter treats RAG metrics as their own category because of their <span class="No-Break">unique properties.</span></li>
			</ul>
			<p>Given the novelty of the LLM engineering space, the exact metrics you use might change, but the general categories discussed here will likely be useful. In the remainder of this section, you will learn more about these categories and the specific evaluation metrics <span class="No-Break">in them.</span></p>
			<h2 id="_idParaDest-165"><a id="_idTextAnchor207"/>Assertion-based metrics</h2>
			<p><strong class="bold">Assertion-based metrics</strong> are quantitative metrics that evaluate whether an AI system output meets certain criteria as defined in code. Assertion-based metrics resemble unit tests in traditional software engineering, where you compare whether a module output matches <span class="No-Break">an expectation.</span></p>
			<p>You can even wrap assertion-based evaluations in a unit-testing suite. Given that your intelligent application likely already has a test suite, you can start adding evaluations to your application by including assertion-based metrics in the test suite. This is a great way to start evaluating your AI components without adding additional technical overhead to your application. However, as your application matures, you will likely want to create a separate <span class="No-Break">evaluation suite.</span></p>
			<p>Some assertion-based metrics you can use are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">Equality</strong>: Evaluate whether the actual output is equal to (<strong class="source-inline">==</strong>) or not equal to (<strong class="source-inline">!=</strong>) an <span class="No-Break">expected value.</span></li>
				<li><strong class="bold">Comparison operators</strong>: Evaluate whether the actual output matches comparison criteria with one of the comparison operators: greater than (<strong class="source-inline">&gt;</strong>), greater than or equal to (<strong class="source-inline">&gt;=</strong>), less than (<strong class="source-inline">&lt;</strong>), or less than or equal to (<strong class="source-inline">&lt;=</strong>). These comparison operators are useful for evaluating <span class="No-Break">numeric outputs.</span></li>
				<li><strong class="bold">Sub-string match</strong>: Evaluate whether a string output includes an <span class="No-Break">expected sub-string.</span></li>
				<li><strong class="bold">Regular expression match</strong>: Evaluate whether a string output matches a <span class="No-Break">regular expression.</span></li>
			</ul>
			<p>In the following code example, you have a dataset of evaluation cases for the travel assistant chatbot application. This evaluation focuses on the input relevancy guardrail. The cases include the evaluation inputs, the expected output of the relevancy guardrail, and the actual output of running the inputs through the relevancy guardrail. The evaluation metric assesses whether the actual output is equal to the <span class="No-Break">expected output.</span></p>
			<p>First, install the <strong class="source-inline">prettytable</strong> Python package, which you will use to output results in a readable format. Install the package in <span class="No-Break">your terminal:</span></p>
			<pre class="console">
pip3 install prettytable==3.10.2</pre>			<p>Then, execute the following <span class="No-Break">Python code:</span></p>
			<pre class="console">
from prettytable import PrettyTable
input_relevance_guardrail_data = [
    {
        "input": "What should I do in New York City in July?",
        "output": True,
        "expected": True
    },
{
        "input": "Can you help me with my math homework?",
        "output": False,
        "expected": False
    },
    {
        "input": "What's the capital of France?",
        "output": False,
        "expected": True
    },
]
# assertion-based evaluation
def evaluate_correctness(output, expected):
    return 1 if output == expected else 0
def calculate_average(scores):
    return sum(scores) / len(scores)
def create_table(data):
    table = PrettyTable()
    table.field_names = ["Input", "Output", "Expected", "Score"]
    scores = [evaluate_correctness(case["output"], case["expected"]) for case in data]
    for case, score in zip(data, scores):
        table.add_row([case["input"], case["output"], case["expected"], score])
# Add a blank row for visual separation
table.add_row(["", "", "", ""])
    # Add average score to bottom of the table
    average_score = calculate_average(scores)
    table.add_row(["Average", "", "", f"{average_score:.4f}"])
    return table
# Create and print the table
result_table = create_table(input_relevance_guardrail_data)
print(result_table)</pre>			<p>This code outputs the following evaluation results to <span class="No-Break">the terminal:</span></p>
			<pre class="console">
+--------------------------------------------+--------+----------+--------+
|                   Input                    | Output | Expected | Score  |
+--------------------------------------------+--------+----------+--------+
| What should I do in New York City in July? |  True  |   True   |   1    |
|   Can you help me with my math homework?   | False  |  False   |   1    |
|       What's the capital of France?        | False  |   True   |   0    |
|                                            |        |          |        |
|                  Average                   |        |          | 0.6667 |
+--------------------------------------------+--------+----------+--------+</pre>			<p>The preceding code example shows how you can use assertion-based evaluation metrics to evaluate the LLM components of an <span class="No-Break">intelligent application.</span></p>
			<h2 id="_idParaDest-166"><a id="_idTextAnchor208"/>Statistical metrics</h2>
			<p>Statistical metrics use algorithms to determine a score. If you have a background in traditional <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>), you may already be familiar with the statistical metrics for evaluating LLMs’ system outputs. Statistical metrics are most useful when you are using LLM systems for tasks that would use other NLP models, such as classification, summarization, <span class="No-Break">and translation.</span></p>
			<p>The following are some popular NLP metrics that you can use to evaluate LLM <span class="No-Break">system outputs:</span></p>
			<ul>
				<li><strong class="bold">Bilingual Evaluation Understudy</strong> (<strong class="bold">BLEU</strong>): BLEU measures the precision of a model’s output against one or more reference texts. You can use the BLEU score to calculate how similar a model output is to a reference answer. BLEU was originally developed to measure the quality of machine-translated text compared to a <span class="No-Break">reference translation.</span><p class="list-inset">You can learn more about BLEU <span class="No-Break">at </span><a href="https://en.wikipedia.org/wiki/BLEU"><span class="No-Break"><span class="P---URL">https://en.wikipedia.org/wiki/BLEU</span></span></a><span class="No-Break">.</span></p></li>
				<li><strong class="bold">Recall-Oriented Understudy for Gisting Evaluation</strong> (<strong class="bold">ROUGE</strong>): ROUGE measures the quality of machine-generated text against one or more reference texts. In LLM systems, ROUGE is often used to assess how effectively an LLM summarizes reference texts. ROUGE is particularly useful for RAG systems, where the LLM summarizes the content in retrieved documents. It can also be used to measure the quality of a translation against <span class="No-Break">a reference.</span><p class="list-inset">You can learn more about ROUGE <span class="No-Break">at </span><a href="https://en.wikipedia.org/wiki/ROUGE_(metric)"><span class="No-Break"><span class="P---URL">https://en.wikipedia.org/wiki/ROUGE_(metric)</span></span></a><span class="No-Break">.</span></p></li>
			</ul>
			<p>In the following code example, you have a dataset of evaluation cases for the travel assistant chatbot application. This evaluation focuses on the response generator LLM. It calculates the BLEU score for how well the actual output measures against a reference output. It also calculates the ROUGE score for how the answer summarizes the retrieved <span class="No-Break">context information.</span></p>
			<p>First, you must install a few Python packages. The <strong class="source-inline">prettytable</strong> package output results in a readable format, the <strong class="source-inline">sacrebleu</strong> package calculates the BLEU score, and the <strong class="source-inline">rouge-score</strong> package calculates the ROUGE score. Install the packages in <span class="No-Break">the terminal:</span></p>
			<pre class="console">
pip3 install prettytable==3.10.2 sacrebleu==2.4.2 rouge-score==0.1.2</pre>			<p>Then, execute the following <span class="No-Break">Python code:</span></p>
			<pre class="console">
from prettytable import PrettyTable
import sacrebleu
from rouge_score import rouge_scorer
evaluation_data = [
    {
        "input": "What should I do in New York City in July?",
        "output": "Check out Times Square, go to an outdoor concert, and visit the Statue of Liberty.",
        "golden_answer": "Explore Central Park, attend outdoor concerts, and visit rooftop bars.",
        "contexts": [
            "Times Square is known for its Broadway theaters, bright lights, and bustling atmosphere.",
            "Outdoor concerts in Central Park are popular summer events attracting many visitors.",
            "The Statue of Liberty is a symbol of freedom and a must-see landmark in NYC."
        ]
    },
    {
        "input": "Can you help me with my math homework?",
        "output": "I'm designed to assist with travel queries. For math help, try using online resources like Khan Academy or Mathway.",
        "golden_answer": "I am a travel assistant chatbot, so I cannot help you with your math homework.",
        "contexts": []
    },
    {
        "input": "What's the capital of France?",
        "output": "The capital of France is Paris.",
        "golden_answer": "Paris is the capital of France.",
        "contexts": [
            "Paris, known as the City of Light, is the most populous city of France.",
            "European capitals: Paris, France; Berlin, Germany; Madrid, Spain",
        ]
    }
]
# Statistical evaluators
def evaluate_bleu(output, golden_answer):
    bleu = sacrebleu.corpus_bleu([output], [[golden_answer]])
    return bleu.score / 100  # Normalize BLEU score to be between 0 and 1
def evaluate_rouge(output, contexts):
    context_text = ("\n").join(contexts)
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    scores = scorer.score(context_text, output)
    return scores['rougeL'].fmeasure
def calculate_average(scores):
    return sum(scores) / len(scores)
# truncate strings for easier printing in table
def truncate_string(s, max_length=10):
    return (s[:max_length] + '...') if len(s) &gt; max_length else s
def create_table(data):
    table = PrettyTable()
    table.field_names = ["Input", "Output", "Golden Answer", "# Contexts", "BLEU", "ROUGE"]
    bleu_scores = [evaluate_bleu(case["output"], case["golden_answer"]) for case in data]
    rouge_scores = [evaluate_rouge(case["output"], case["contexts"]) for case in data]
    for case, bleu, rouge in zip(data, bleu_scores, rouge_scores):
        table.add_row([
            truncate_string(case["input"]),
            truncate_string(case["output"]),
            truncate_string(case["golden_answer"]),
            len(case["contexts"]),
            f"{bleu:.4f}",
            f"{rouge:.4f}"])
    # Add a blank row for visual separation
    table.add_row(["", "", "", "", "", ""])
    # Add the average score to bottom of the table
    average_bleu = calculate_average(bleu_scores)
    average_rouge = calculate_average(rouge_scores)
    table.add_row(["Average", "", "", "", f"{average_bleu:.4f}", f"{average_rouge:.4f}"])
    return table
# Create and print the table
result_table = create_table(evaluation_data)
print(result_table)</pre>			<p>This code outputs the following to <span class="No-Break">the terminal:</span></p>
			<pre class="console">
+---------------+---------------+---------------+------------+--------+--------+
|     Input     |     Output    | Golden Answer | # Contexts |  BLEU  | ROUGE  |
+---------------+---------------+---------------+------------+--------+--------+
| What shoul... | Check out ... | Explore Ce... |     3      | 0.0951 | 0.2857 |
| Can you he... | I'm design... | I am a tra... |     0      | 0.0270 | 0.0000 |
| What's the... | The capita... | Paris is t... |     2      | 0.2907 | 0.2857 |
|               |               |               |            |        |        |
|    Average    |               |               |            | 0.1376 | 0.1905 |
+---------------+---------------+---------------+------------+--------+--------+</pre>			<p>The preceding example demonstrates how you can use BLEU and ROUGE scores as evaluation metrics to measure the outputs of the travel assistant chatbot. For instance, in the preceding example, the fact that the BLEU and ROUGE scores are so different in the first <strong class="source-inline">New York City</strong> test case indicates that the model answer deviates significantly from the golden answer but has relatively high adherence to the context information. This difference implies that you could optimize the retriever to get more relevant context information to better satisfy the <span class="No-Break">golden answer.</span></p>
			<p>These statistical metrics are most useful for assessing the quality of LLM outputs when the LLMs are used for more traditional NLP tasks, such as translation and summarization. They can also provide a useful directional metric when comparing different versions of the same AI system on the same <span class="No-Break">evaluation dataset.</span></p>
			<p>While these <strong class="bold">quantitative metrics</strong> can provide valuable insights into LLM performance, they are usually not sufficient for evaluating an LLM-powered intelligent application. These metrics often fail to capture the nuanced aspects of language generation, such as coherence, creativity, factual correctness, and contextual appropriateness. Therefore, you need to also create <strong class="bold">qualitative evaluations</strong> to understand how well the LLM system performs on these metrics. In the following sections, you will learn about using LLMs as judges and RAG-specific metrics to evaluate <span class="No-Break">LLM output.</span></p>
			<h2 id="_idParaDest-167"><a id="_idTextAnchor209"/>LLM-as-a-judge evaluations</h2>
			<p>You can use an LLM to evaluate the outputs of an LLM system along qualitative criteria. Many LLM systems perform broad open-domain tasks, such as a chatbot carrying on extended conversations. Quantitative metrics, such as the ones discussed previously, cannot necessarily capture whether the LLM system performs these tasks effectively. For instance, a ROUGE score may be able to indicate how closely a summary tracks source documents, but it cannot tell you if the summary includes a hallucination. You will learn more about hallucinations in <a href="B22495_11.xhtml#_idTextAnchor232"><span class="No-Break"><em class="italic">Chapter 11</em></span></a>, <em class="italic">Common Failures of </em><span class="No-Break"><em class="italic">Generative AI</em></span><span class="No-Break">.</span></p>
			<p>Before the rise of LLMs, it was challenging to systematically evaluate qualitative aspects of natural language generation. Now you can use LLMs to evaluate the outputs of LLM-powered systems. Using LLMs to perform evaluations is called <strong class="bold">LLM-as-a-judge</strong>. Evaluating LLM output with another judge LLM is never a perfect solution. The judge LLM is subject to all the limitations of LLMs that require you to evaluate the LLM system in the first place. However, as of writing in mid-2024, LLM-as-a-judge seems to be the best approach to systematically perform qualitative evaluation of <span class="No-Break">LLM output.</span></p>
			<p>A few areas where you can use LLM-as-a-judge qualitative metrics include <span class="No-Break">the following:</span></p>
			<ul>
				<li>Tone and style of <span class="No-Break">the response</span></li>
				<li>Whether the response is personalized to the user based on <span class="No-Break">input information</span></li>
				<li>Whether the response contains sensitive information, such as personally identifiable information, that it should <span class="No-Break">not share</span></li>
				<li>Whether the response complies with a certain law <span class="No-Break">or regulation</span></li>
			</ul>
			<p>When creating LLM-as-a-judge evaluation metrics, it is useful to keep the following key points <span class="No-Break">in mind:</span></p>
			<ul>
				<li>Always set the LLM <strong class="bold">temperature</strong> to 0 for consistent outputs. Temperature is a hyperparameter for LLMs that controls the randomness of their predictions. A temperature of 0 produces deterministic outputs. A higher temperature produces more diverse and less consistent outputs, which can be preferable if the LLM is performing creative work. However, you want the evaluations to be as consistent <span class="No-Break">as possible.</span></li>
				<li>Better LLMs tend to be better evaluators. LLMs that rank higher on benchmarks tend to produce evaluation results that are more consistent <span class="No-Break">with expectations.</span></li>
				<li><strong class="bold">Multi-shot prompting</strong> often improves evaluator accuracy. To perform multi-shot prompting, include examples of inputs and the outputs the model should provide, in addition to including the evaluation criteria in the model prompt. These examples often help the model perform better evaluations. Generally, you should include at least five examples that represent a diverse set of <span class="No-Break">evaluation scenarios.</span></li>
				<li><strong class="bold">Chain-of-thought prompting</strong> often further improves LLM-as-a-judge evaluator performance. In a chain-of-thought prompt, you ask the model to explain its thought process before producing a <span class="No-Break">final answer.</span></li>
				<li>Every LLM-as-a-judge evaluation metric should only evaluate a single qualitative aspect. Focusing on a single aspect makes the evaluation task easier for the LLM to interpret. If you need to assess multiple aspects, create multiple LLM-as-a-judge <span class="No-Break">evaluation metrics.</span></li>
				<li>The LLM you use matters. Different LLMs can produce different outcomes on the same evaluation task. Be consistent in using the same LLM for all evaluations with a metric. If you change the LLM used by a metric, you cannot reliably compare the results produced with <span class="No-Break">different LLMs.</span></li>
				<li>Produce structured evaluation output. The judge LLM should produce structured outputs, such as pass or fail, or a score of integers 0-5. You can then normalize these scores. For instance, if the judge LLM outputs <strong class="source-inline">pass</strong> or <strong class="source-inline">fail</strong>, then <strong class="source-inline">pass</strong> is normalized as 1 and <strong class="source-inline">fail</strong> as 0. If the judge LLM outputs integers <strong class="source-inline">0</strong>-<strong class="source-inline">5</strong>, <strong class="source-inline">0</strong> is normalized as 0, <strong class="source-inline">1</strong> as 0.2, <strong class="source-inline">2</strong> as 0.4... and <strong class="source-inline">5</strong> <span class="No-Break">as 1.</span></li>
			</ul>
			<p>The following code example uses an LLM as a judge to evaluate whether the travel assistant chatbot provides a suggestion to the user in its response. The LLM evaluator also includes few-shot examples to improve the judge model’s understanding of <span class="No-Break">the task.</span></p>
			<p>The code example runs the evaluation over a dataset of inputs and outputs. Note that this is a reference-free evaluation, as the LLM-as-a-judge does not need a reference answer to determine whether the chatbot provides <span class="No-Break">irrelevant answers.</span></p>
			<p>First, you must install a few Python packages. The <strong class="source-inline">prettytable</strong> package output results in a readable format and the <strong class="source-inline">openai</strong> package calls the OpenAI API to use the GPT-4o LLM. Install the packages in <span class="No-Break">your terminal:</span></p>
			<pre class="console">
pip3 install prettytable==3.10.2 openai==1.39.0</pre>			<p>Then, execute <span class="No-Break">the code:</span></p>
			<pre class="console">
import json
from prettytable import PrettyTable
import openai
import os
# Add your OpenAI API key to call the model
openai.api_key = os.getenv("OPENAI_API_KEY")
# Data to evaluate
evaluation_data = [
    {
        "input": "What should I do in New York City in July?",
        "output": "Check out Times Square, go to an outdoor concert, and visit the Statue of Liberty.",
    },
    {
        "input": "Can you help me with my math homework?",
        "output": "I'm designed to assist with travel queries. For math help, try using online resources like Khan Academy or Mathway.",
    },
    {
        "input": "What's the capital of France?",
        "output": "The capital of France is Paris.",
    }
]
# LLM-as-a-Judge Evaluation metric
# that assesses if the output includes a recommendation.
def evaluate_includes_recommendation(input, output):
    # Few-shot examples to help the model produce better answers.
    few_shot_examples = [
        {
            "input": "What are some good restaurants in Paris?",
            "output": "Try Le Jules Verne for an upscale dining experience, or visit Le Relais de l'Entrecôte for a classic steak frites.",
            "recommendation": True
        },
        {
            "input": "Where should I stay in London?",
            "output": "Consider staying at The Ritz for luxury or the Hoxton for a more budget-friendly option.",
            "recommendation": True
        },
        {
            "input": "What's the weather like in Tokyo in winter?",
            "output": "In winter, Tokyo is generally cool with temperatures ranging from 2°C to 12°C. While you're there, consider visiting the hot springs (onsen) for a warm and relaxing experience.",
            "recommendation": True
        },
        {
            "input": "What's the population of Berlin?",
            "output": "The population of Berlin is approximately 3.6 million.",
            "recommendation": False
        },
        {
            "input": "What's the currency used in Japan?",
            "output": "The currency used in Japan is the Japanese Yen (JPY).",
            "recommendation": False
        }
    ]
    # Constructing the prompt
    prompt = """Determine whether the following output includes a recommendation based on the input.
Format response as a JSON object with the shape { "recommendation": boolean }.
Examples:
"""
    # Append few-shot examples to the prompt.
    for example in few_shot_examples:
        prompt += f"""Input: {example['input']}
Output: {example['output']}
Recommendation: {{ "recommendation": {str(example['recommendation']).lower()} }}
"""
    prompt += f"""Input: {input}
Output: {output}
Recommendation:"""
    # Call the OpenAI API
    response = openai.chat.completions.create(
        # Use strong evaluator LLM
        model="gpt-4o",
        ## Format response as JSON, so it is easier to parse
        response_format={ "type": "json_object" },
        messages=[{ "role": "user", "content": prompt }],
        # Make sure temperature=0 for consistent outputs
        temperature=0
    )
    recommendation = json.loads(response.choices[0].message.content)["recommendation"]
    return 1 if recommendation == True else 0
def calculate_average(scores):
    return sum(scores) / len(scores)
# truncate strings for easier printing in table
def truncate_string(s, max_length=30):
    return (s[:max_length] + '...') if len(s) &gt; max_length else s
def create_table(data):
    table = PrettyTable()
    table.field_names = ["Input", "Output", "Score"]
    scores = [evaluate_includes_recommendation(case["input"], case["output"]) for case in data]
    for case, score in zip(data, scores):
        table.add_row([
            truncate_string(case["input"]),
            truncate_string(case["output"]),
            score])
    # Add a blank row for visual separation
    table.add_row(["", "", ""])
    # Add the average score to bottom of the table
    average = calculate_average(scores)
    table.add_row(["Average", "", f"{average:.4f}"])
    return table
# Create and print the table
result_table = create_table(evaluation_data)
print(result_table)</pre>			<p>This code outputs the following to <span class="No-Break">the terminal:</span></p>
			<pre class="console">
+-----------------------------------+-----------------------------------+--------+
|               Input               |               Output              | Score  |
+-----------------------------------+-----------------------------------+--------+
| What should I do in New York C... | Check out Times Square, go to ... |   1    |
| Can you help me with my math h... | I'm designed to assist with tr... |   1    |
|   What's the capital of France?   | The capital of France is Paris... |   0    |
|                                   |                                   |        |
|              Average              |                                   | 0.6667 |
+-----------------------------------+-----------------------------------+--------+</pre>			<p>The preceding example demonstrates how to create a simple LLM-as-a-judge metric to evaluate whether a response includes a recommendation. You can extend the techniques to create additional LLM-as-a-judge metrics to look at various aspects of your LLM system. In the next section, you will learn about some more complex LLM-as-a-judge metrics for evaluating <span class="No-Break">RAG systems.</span></p>
			<h2 id="_idParaDest-168"><a id="_idTextAnchor210"/>RAG metrics</h2>
			<p>RAG is currently one of the most popular ways to use LLMs. A distinct set of metrics has emerged to measure the efficacy of a RAG system. These metrics all use an LLM as <span class="No-Break">a judge.</span></p>
			<p>These metrics focus on the two core components of any RAG system, retrieval <span class="No-Break">and generation:</span></p>
			<ul>
				<li><strong class="bold">Retrieval</strong>: This component fetches relevant information from external sources. It often combines vector search with LLM-based pre- <span class="No-Break">and post-processing.</span></li>
				<li><strong class="bold">Generation</strong>: This component uses an LLM to produce <span class="No-Break">text outputs.</span></li>
			</ul>
			<p>The following LLM-as-a-judge metrics are often used to evaluate <span class="No-Break">RAG systems:</span></p>
			<ul>
				<li><strong class="bold">Answer faithfulness</strong>: Measures how grounded the generated response is to the retrieved <span class="No-Break">context information</span></li>
				<li><strong class="bold">Answer relevance</strong>: Measures how relevant the generated response is to the <span class="No-Break">provided input</span></li>
			</ul>
			<p><strong class="bold">Ragas</strong> is a popular Python library that includes modules implementing these metrics along with others for RAG evaluation. In the remainder of this section, you will learn how Ragas implements these metrics. To learn more about Ragas and its available metrics, refer to its documentation (<a href="https://docs.ragas.io/en/stable/index.html"><span class="No-Break"><span class="P---URL">https://docs.ragas.io/en/stable/index.html</span></span></a><span class="No-Break">).</span></p>
			<h3>Answer faithfulness</h3>
			<p>Answer faithfulness is an evaluation metric for the generation component of RAG systems. It measures the extent to which the information in the generated response aligns with the retrieved <span class="No-Break">context information.</span></p>
			<p>By identifying factual discrepancies between the generated answer and the retrieved context, the answer faithfulness metric can help identify if there are any hallucinations in <span class="No-Break">the answer.</span></p>
			<p>Ragas includes a module to measure faithfulness. It calculates faithfulness with <span class="No-Break">this formula:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer089">
					<img alt="" role="presentation" src="image/B22495_09_Equation.jpg"/>
				</div>
			</div>
			<p>The data to input into the faithfulness formula is derived with <span class="No-Break">these steps:</span></p>
			<ol>
				<li>Extract all claims from the generated response with <span class="No-Break">an LLM.</span></li>
				<li>Locate each claim in the reference material with <span class="No-Break">an LLM.</span></li>
				<li>Calculate the proportion of claims that can be inferred from the <span class="No-Break">context information.</span></li>
			</ol>
			<p>The following code example uses the Ragas faithfulness metric on an example set of input, contexts, and RAG <span class="No-Break">system outputs.</span></p>
			<p>First, you must install a few Python packages. The <strong class="source-inline">ragas</strong> package includes the response faithfulness metric and a reporting module. The <strong class="source-inline">langchain-openai</strong> package lets you pass an OpenAI model to Ragas. This example uses the GPT-4o mini model. Ragas also depends on the <strong class="source-inline">datasets</strong> package to format inputs. Install the packages in <span class="No-Break">your terminal:</span></p>
			<pre class="console">
pip3 install ragas==0.1.13 langchain-openai==0.1.20 datasets==2.20.0</pre>			<p>Then, run the following code to perform <span class="No-Break">the evaluation:</span></p>
			<pre class="console">
from ragas.metrics import faithfulness
from ragas import evaluate
from datasets import Dataset
from langchain_openai.chat_models import ChatOpenAI
import os
openai_api_key = os.getenv("OPENAI_API_KEY")
evaluation_data = [
    {
        "input": "What should I do in New York City in July?",
        "output": "Check out Times Square, go to an outdoor concert, and visit the Statue of Liberty.",
        "contexts": [
            "Times Square is known for its Broadway theaters, bright lights, and bustling atmosphere.",
            "Outdoor concerts in Central Park are popular summer events attracting many visitors.",
            "The Statue of Liberty is a symbol of freedom and a must-see landmark in NYC."
        ]
    },
    {
        "input": "Can you help me with my math homework?",
        "output": "I'm designed to assist with travel queries. For math help, try using online resources like Khan Academy or Mathway.",
        "contexts": []
    },
    {
        "input": "What's the capital of France?",
        "output": "The capital of France is Paris.",
        "contexts": [
            "Paris, known as the City of Light, is the most populous city of France.",
            "European capitals: Paris, France; Berlin, Germany; Madrid, Spain",
        ]
    }
]
# Format our dataset for Ragas data structure
def prepare_data_for_ragas(data_list):
    data_table = {
        'question': [],
        'answer': [],
        'contexts': []
    }
    for data_item in data_list:
        data_table["question"].append(data_item["input"])
        data_table["answer"].append(data_item["output"])
        data_table["contexts"].append(data_item["contexts"])
    return data_table
def create_report(data):
    ragas_dict = prepare_data_for_ragas(data)
    dataset = Dataset.from_dict(prepare_data_for_ragas(data))
    langchain_llm = ChatOpenAI(
        model_name="gpt-4o-mini",
        api_key=openai_api_key)
    score = evaluate(dataset, metrics=[faithfulness], llm=langchain_llm)
    return score
# Create and print the table
results = create_report(evaluation_data)
print(results.to_pandas())
print(results)</pre>			<p>Executing this code outputs results resembling the following to <span class="No-Break">the terminal:</span></p>
			<pre class="console">
Evaluating: 100%
 3/3 [00:05&lt;00:00,  1.72s/it]
                                    question  \
0  What should I do in New York City in July?
1      Can you help me with my math homework?
2               What's the capital of France?
                                              answer  \
0  Check out Times Square, go to an outdoor conce...
1  I'm designed to assist with travel queries. Fo...
2                    The capital of France is Paris.
                                            contexts  faithfulness
0  [Times Square is known for its Broadway theate...           1.0
1                                                 []           0.0
2  [Paris, known as the City of Light, is the mos...           1.0
{'faithfulness': 0.6667}</pre>			<p>You can see from the results that the Ragas evaluator deemed the first and third examples faithful, and not the <span class="No-Break">second one.</span></p>
			<p>In the following section, you will learn how to use another RAG evaluation metric: <span class="No-Break">answer relevance.</span></p>
			<h3>Answer relevance</h3>
			<p>Answer relevance measures how relevant the output of a RAG system is to the input. This metric is useful because it determines how well a RAG system responds to the <span class="No-Break">provided input.</span></p>
			<p>Ragas uses the input, generated output, and context information retrieved to generate that output in its answer relevance metric. It calculates the answer relevance evaluation metric score with the <span class="No-Break">following steps:</span></p>
			<ol>
				<li>Use an LLM to generate a list of questions from the <span class="No-Break">generated response.</span></li>
				<li>Create a vector embedding for each LLM-generated question from the previous step. Also, create a vector embedding for the initial <span class="No-Break">input query.</span></li>
				<li>Calculate the cosine similarity between the original question embedding and each generated <span class="No-Break">question embedding.</span></li>
				<li>The answer relevance score is the mean of the cosine similarities between the original question and each <span class="No-Break">generated question.</span></li>
			</ol>
			<p>Ragas assumes that if the generated answer is highly relevant to the original question, then the questions that can be derived from this answer should be semantically similar to the original question. This assumption is based on the idea that a relevant answer contains information that directly addresses the query. Therefore, the judge LLM should be able to <em class="italic">reverse-engineer</em> questions that closely align with the <span class="No-Break">original input.</span></p>
			<p>The following code example uses the Ragas answer relevance metric on an example set of input, contexts, and RAG <span class="No-Break">system outputs.</span></p>
			<p>First, you must install a few Python packages. Note that these are the same dependencies as for the Ragas faithfulness evaluation example in the previous section. The <strong class="source-inline">ragas</strong> package includes the response answer relevance metric and a reporting module. The <strong class="source-inline">langchain-openai</strong> package lets you pass an OpenAI model to Ragas. This example uses the GPT-4o mini model. Ragas also depends on the <strong class="source-inline">datasets</strong> package to format inputs. Install the packages in <span class="No-Break">your terminal:</span></p>
			<pre class="console">
pip3 install ragas==0.1.13 langchain-openai==0.1.20 datasets==2.20.0</pre>			<p>Then, run the following code to perform <span class="No-Break">the evaluation:</span></p>
			<pre class="console">
from ragas.metrics import answer_relevancy
from ragas import evaluate
from datasets import Dataset
from langchain_openai.chat_models import ChatOpenAI
from langchain_openai.embeddings import OpenAIEmbeddings
import os
openai_api_key = os.getenv("OPENAI_API_KEY")
evaluation_data = [
    {
        "input": "What should I do in New York City in July?",
        "output": "Check out Times Square, go to an outdoor concert, and visit the Statue of Liberty.",
        "contexts": [
            "Times Square is known for its Broadway theaters, bright lights, and bustling atmosphere.",
            "Outdoor concerts in Central Park are popular summer events attracting many visitors.",
            "The Statue of Liberty is a symbol of freedom and a must-see landmark in NYC."
        ]
    },
    {
        "input": "Can you help me with my math homework?",
        "output": "I'm designed to assist with travel queries. For math help, try using online resources like Khan Academy or Mathway.",
        "contexts": []
    },
    {
        "input": "What's the capital of France?",
        "output": "The capital of France is Paris.",
        "contexts": [
            "Paris, known as the City of Light, is the most populous city of France.",
            "European capitals: Paris, France; Berlin, Germany; Madrid, Spain",
        ]
    }
]
# Format our dataset for Ragas data structure
def prepare_data_for_ragas(data_list):
    data_table = {
        'question': [],
        'answer': [],
        'contexts': []
    }
    for data_item in data_list:
        data_table["question"].append(data_item["input"])
        data_table["answer"].append(data_item["output"])
        data_table["contexts"].append(data_item["contexts"])
    return data_table
def create_report(data):
    ragas_dict = prepare_data_for_ragas(data)
    dataset = Dataset.from_dict(prepare_data_for_ragas(data))
    langchain_llm = ChatOpenAI(
        model_name="gpt-4o-mini",
  api_key=openai_api_key)
    langchain_embeddings = OpenAIEmbeddings(
        model="text-embedding-3-large",
        api_key=openai_api_key
    )
    score = evaluate(dataset,
                     metrics=[answer_relevancy],
                     llm=langchain_llm,
                     embeddings=langchain_embeddings
                    )
    return score
# Create and print the table
results = create_report(evaluation_data)
print(results.to_pandas())
print(results)</pre>			<p>Executing this code outputs the following results to <span class="No-Break">the terminal:</span></p>
			<pre class="console">
Evaluating: 100%
 3/3 [00:04&lt;00:00,  4.85s/it]
                                    question  \
0  What should I do in New York City in July?
1      Can you help me with my math homework?
2               What's the capital of France?
                                              answer  \
0  Check out Times Square, go to an outdoor conce...
1  I'm designed to assist with travel queries. Fo...
2                    The capital of France is Paris.
                                            contexts  answer_relevancy
0  [Times Square is known for its Broadway theate...          0.630561
1                                                 []          0.000000
2  [Paris, known as the City of Light, is the mos...          0.873249
{'answer_relevancy': 0.5013}</pre>			<p>You can see from the results that the first and third cases were relevant, while the second was not. This makes sense because the first and third had quite relevant contexts, whereas the second had no context information <span class="No-Break">at all.</span></p>
			<p>The Ragas answer relevance metric has noteworthy limitations. The quality of the underlying language model significantly impacts the metric’s effectiveness, as it relies heavily on the LLM’s capacity to generate appropriate questions from the given answer. The metric may also struggle with handling complex or multi-faceted queries, particularly when the answer doesn’t comprehensively address all aspects of the original question, potentially resulting in an incomplete assessment of relevance for more <span class="No-Break">intricate topics.</span></p>
			<p>There are other approaches that you can take to evaluate answer relevance. For instance, the <strong class="bold">DeepEval</strong> evaluation framework calculates answer relevancy with the <span class="No-Break">following strategy:</span></p>
			<ol>
				<li>Extract all statements in an output with <span class="No-Break">an LLM.</span></li>
				<li>Use the same LLM to determine which statements are relevant to <span class="No-Break">the input.</span></li>
				<li>Calculate answer relevance as the number of relevant statements divided by the total number <span class="No-Break">of statements.</span></li>
			</ol>
			<p>The difference between the Ragas and DeepEval strategies to calculating the answer relevancy metric demonstrates that the AI engineering field is still converging on how to calculate these metrics, even if it is becoming standard to evaluate based on some form of <span class="No-Break">these metrics.</span></p>
			<p>Using the RAG evaluation metrics discussed in this section, you can measure how well your RAG system is performing and measure improvement in the system over time. You can also experiment with other RAG metrics in frameworks such as Ragas <span class="No-Break">or DeepEval.</span></p>
			<p>In the next section, you will learn how you can perform a manual human review of your data to augment the automated evaluation metrics discussed in <span class="No-Break">this section.</span></p>
			<h2 id="_idParaDest-169"><a id="_idTextAnchor211"/>Human review</h2>
			<p>While LLMs can be effective tools for qualitative evaluation, they are often inferior to the original form of non-artificial intelligence: humans. <strong class="bold">Human review</strong> is considered the gold standard of <span class="No-Break">qualitative review.</span></p>
			<p>When using human review, you should take into account that humans likely prefer simpler rating metrics that do not require them to do complex multi-step calculations, such as the answer relevance metric described earlier in this chapter. Instead, give human reviewers simple rating systems. Pass/fail criteria are the simplest and can be normalized to 0 or 1. You can also use a rating system such as 0-5, which can be normalized to 0, 0.2, and so on <span class="No-Break">until 1.</span></p>
			<p>Human reviewer free-form feedback can be particularly valuable, as this open-ended feedback can provide insight that would not be captured by the rating <span class="No-Break">metric alone.</span></p>
			<p>It is also useful to capture who the human reviewer is for an evaluation. You can use this to follow up with the person if need be or to track how some individuals perform ratings as compared <span class="No-Break">to others.</span></p>
			<p>Despite the qualitative advantage of human review, it also comes with its own set <span class="No-Break">of limitations:</span></p>
			<ul>
				<li><strong class="bold">Cost</strong>: Human reviewers tend to be more expensive than using an LLM as <span class="No-Break">a judge.</span></li>
				<li><strong class="bold">Time</strong>: Human reviewers usually take much longer than using an LLM as a judge. You also cannot parallelize a single human like you can an <span class="No-Break">AI model.</span></li>
				<li><strong class="bold">Tedium</strong>: Evaluating the output of LLMs can be an incredibly tedious task for human reviewers. Many people do not want to perform evaluations, so it can be difficult to find people to consistently perform <span class="No-Break">the evaluations.</span></li>
				<li><strong class="bold">Elasticity</strong>: Often, you need to run large numbers of evaluations as part of your software development process or at regular intervals. It can be hard to find human reviewers to perform an evaluation exactly when you need <span class="No-Break">them to.</span></li>
				<li><strong class="bold">Inconsistency</strong>: Human reviewers can be inconsistent in their evaluation. Different people might evaluate the same case in different ways. The same person could even evaluate the same case differently at a different moment, depending on factors such as tiredness, mood, <span class="No-Break">and environment.</span></li>
			</ul>
			<p>Given the strengths and weaknesses of using humans as reviewers, you must carefully consider when to use human review. Human review is probably the most useful for conducting initial qualitative evaluation. Human reviewers can set a baseline for application performance that you can measure against with a reasonably high degree <span class="No-Break">of confidence.</span></p>
			<p>You can also use human reviews as a baseline to measure LLM-as-a-judge metrics against. You can try to get the LLM-as-a-judge metric to conform as closely as possible to the human <span class="No-Break">review results.</span></p>
			<p>Additionally, your LLM-as-a-judge metric can use examples from the human review in its prompt to demonstrate to the LLM what the classification should look like as a form of multi-shot prompting.  Multi-shot prompting has been shown to increase model <span class="No-Break">performance meaningfully.</span></p>
			<p>Human review is one of the most effective means of qualitative evaluation, if also a slow and <span class="No-Break">expensive one.</span></p>
			<h2 id="_idParaDest-170"><a id="_idTextAnchor212"/>Evaluations as guardrails</h2>
			<p>A <strong class="bold">guardrail</strong> is a mechanism that prevents the AI from producing an undesirable or incorrect output. Guardrails ensure that generated responses are within acceptable boundaries and align with your application’s quality, ethical, and <span class="No-Break">relevance standards.</span></p>
			<p>Previously in this chapter, you learned about <strong class="bold">reference-free evaluations</strong>. These are evaluations that only require an input without a reference output or golden answer. You can also use reference-free evaluations as guardrails to help ensure the AI system performs correctly. For example, in the <em class="italic">RAG metrics</em> section, you looked at the answer relevance metric. You could use this as a guardrail in the travel assistant chatbot to ensure that the chatbot only responds with answers that meet a certain relevancy threshold. If the answer doesn’t meet this threshold, you could perform some additional application logic before responding to <span class="No-Break">the user.</span></p>
			<p>Throughout this chapter, you have learned how to use evaluations to improve the quality of your intelligent application. Using reference-free evaluations as guardrails lets you extend the utility of your evaluations to a component of the <span class="No-Break">application itself.</span></p>
			<h1 id="_idParaDest-171"><a id="_idTextAnchor213"/>Summary</h1>
			<p>In this chapter, you explored methods for evaluating LLM outputs in your intelligent application. You learned what LLM evaluation is and why it’s important for your intelligent application. Model benchmarking is a form of evaluation that can help you determine which LLMs to use in <span class="No-Break">your application.</span></p>
			<p>Once your application has functional AI modules, you can make evaluation datasets and run metrics on them to measure performance and change over time. In addition to the automated evaluations, you can perform manual human review to further measure application quality. Finally, you can use reference-free metrics as guardrails within <span class="No-Break">your application.</span></p>
			<p>In the next chapter, you will learn how to optimize the semantic data model to enhance retrieval accuracy and <span class="No-Break">overall performance.</span></p>
		</div>
	</body></html>