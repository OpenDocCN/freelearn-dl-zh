- en: Novelty Search Optimization Method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you will learn about an advanced solution search optimization
    method that can be used to create autonomous navigator agents. This method is
    called **Novelty Search** (**NS**). The main idea of this method is that an objective
    function can be defined using the novelty of the behavior exposed by the solver
    agent, rather than the distance to a goal in the solution search space.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will learn how to use NS-based search optimization methods
    with the neuroevolution algorithm to train successful maze navigation agents.
    By conducting the experiments presented in this chapter, you will also see that
    the NS method is superior to the conventional goal-oriented search optimization
    method for specific tasks. By the end of this chapter, you will have learned the
    basics of the NS optimization method. You will be able to define the fitness function
    using the novelty score and apply it to solve practical tasks related to your
    work or experiments.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: The NS optimization method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NS implementation basics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fitness function with the novelty score
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Experimenting with a simple maze configuration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Experimenting with a hard-to-solve maze configuration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following technical requirements should be met in order to carry out the
    experiments described in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Windows 8/10, macOS 10.13 or newer, or modern Linux
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anaconda Distribution version 2019.03 or newer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code for this chapter can be found at [https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/tree/master/Chapter6](https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/tree/master/Chapter6)
  prefs: []
  type: TYPE_NORMAL
- en: The NS optimization method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main idea behind NS is to reward the novelty of the produced solution rather
    than its progress to the final goal. This idea is inspired by natural evolution.
    When looking for a successful solution, it is not always obvious the exact steps
    that should be taken. Natural evolution continuously produces novel forms, with
    different phenotypes trying to exploit the surrounding environment and adapt to
    the changes. This has allowed an explosion of life forms on Earth and ignited
    qualitative leaps forward in the evolution of life. The same process allowed life
    forms to leave the sea and conquer the land. The extraordinary genesis of eukaryotes
    became the source of all higher life forms on the planet. All these are examples
    of rewarding novelty during evolution. At the same time, there is no clear objective
    or final goal in natural evolution.
  prefs: []
  type: TYPE_NORMAL
- en: As you learned in the previous chapter, conventional goal-oriented fitness functions
    are susceptible to local optima traps. This pathology produces pressure on the
    evolutionary process to converge to a single solution that often gets stuck in
    dead ends in a search space, with no local steps available that can improve the
    performance of the fitness function any further. Thus, as a result, the successful
    solution is left unexplored.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, NS drives evolution toward diversity. This drive helps the
    neuroevolution process to produce successful solver agents, even for tasks with
    a deceptive landscape of the fitness function values, such as maze navigation
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: A real-life example of such a deceptive problem is the task of navigating around
    an unknown city. If you visit old cities with irregular road maps, you need to
    use a different strategy to get from point A to point B than in modern cities
    with regular grid patterns of roads. In modern cities, traveling along roads that
    point in the direction of your destination is sufficient, but navigation in old
    cities is much more tricky. Heading toward the destination often leads you to
    dead ends (deceptive local optima). You need to employ a more explorative approach,
    trying novel and often counterintuitive directions that seemingly lead you away
    from your destination. So, finally, after another twist in the road, you reach
    your destination. However, note that from the start it was not obvious which turns
    to take based only on the distance to the final destination (that is, the goal-oriented
    fitness score). The stepping stones leading to the ultimate solution are often
    placed in counterintuitive places that seem to lead you away, but ultimately help
    you to succeed.
  prefs: []
  type: TYPE_NORMAL
- en: Please refer to [Chapter 1](f59c6396-55e5-4495-95c0-7af9a42c2f20.xhtml), *Overview
    of Neuroevolution Methods*, for more details about NS optimization.
  prefs: []
  type: TYPE_NORMAL
- en: NS implementation basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'NS implementation should include data structure to hold information about the
    explored novel item and the structure to maintain and manage a list of novel items.
    In our implementation, this functionality is encapsulated in three Python classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`NoveltyItem`: The structure that holds all relevant information about the
    novelty score of the individual that was evaluated during the evolution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NoveltyArchive`: The class that maintains a list of the relevant `NoveltyItem`
    instances. It provides methods to evaluate the novelty scores of individual genomes
    compared to the already collected `NoveltyItem` instances and the current population.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ItemsDistance`: The auxiliary structure that holds the distance (novelty)
    metric value between the two `NoveltyItem` instances. It is used in calculations
    of the average k-nearest neighbor distance, which is used as a novelty score value
    in our experiment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For implementation details, refer to the `novelty_archive.py` file at [https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter6/novelty_archive.py](https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter6/novelty_archive.py).
  prefs: []
  type: TYPE_NORMAL
- en: NoveltyItem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This class is the main structure that holds information about the novelty score
    of each individual evaluated during the evolution. It has several fields that
    store relevant information, as we can see in the source code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The `generation` field holds the ID of the generation at which this item was
    created. Basically, `genomeId` is the ID of the genome that was evaluated, and `fitness`
    is a goal-oriented fitness score of the evaluated genome (the proximity to the
    maze exit). Furthermore, `novelty` is the novelty score given to the evaluated
    genome, as we discuss in the next section, and `data` is a list of data points
    representing the coordinates of specific maze positions that the maze solver agent
    visited during a simulation. This data list is used to estimate the distance between
    the current and other novelty items. The calculated distance after that can be
    used to estimate the novelty score associated with the specific novelty item.
  prefs: []
  type: TYPE_NORMAL
- en: NoveltyArchive
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This class maintains a list of relevant novelty items and provides methods
    to evaluate the novelty scores of individual genomes as well as of the entire
    population of genomes as a whole. It has the following fields defined in the constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Note that `novelty_metric` is a reference to the function that can be used to
    estimate the novelty metric or distance between two novelty items.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, `novelty_threshold` defines the current minimal novelty score value
    of `NoveltyItem` to be eligible for adding to this archive. This value is dynamic
    and is changed during execution to maintain the size of the archive within particular
    limits; `novelty_floor` is the minimal possible value of `novelty_threshold`.
    The `items_added_in_generation` and `time_out` fields are used to schedule the
    dynamics of the change of the `novelty_threshold` values. The `neighbors` field is
    a default number of *k-nearest neighbors* to use for a novelty score estimation.
    The generation is the current evolutionary generation. Basically, `novel_items` is
    a list of all the relevant `NoveltyItem` instances collected so far, and `fittest_items` is
    the list of the novel items having the maximal goal-oriented fitness score among
    all.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dynamics of the `novelty_threshold` field are determined by the following
    source code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The preceding function is invoked at the end of each evolutionary generation
    to adjust the `novelty_threshold` field value for the next generation. As already
    mentioned, this value determines how many novelty items should be added to the
    archive in the next generation. The dynamic adjustment of this property is necessary
    to match the difficulty of finding novel solutions using the NS method over time.
    At the beginning of the evolution, there were immense opportunities to find novel
    solutions with high novelty scores, since only a few paths were explored in the
    maze. However, toward the end of the evolution, it becomes harder because fewer
    unexplored paths remain. To compensate for this, if a novel path is not found
    in the last 2,500 evaluations (`10` generations), the `novelty_threshold` value
    is lowered by 5%. On the other hand, to decrease the speed of adding a new `NoveltyItem`
    to the archive in the early stages of evolution, the `novelty_threshold` value
    is raised by 20%, if over four items were added in the last generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following source code shows how the `novelty_threshold` value is used to
    determine which `NoveltyItem` to add:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code uses a function to evaluate the novelty score, which we will
    describe in the next section to estimate the novelty of the provided genome. If
    this function is invoked in the update archive mode (`only_fitness = False`),
    then the obtained novelty score (`result`) is compared with the current value
    of the `novelty_threshold` field. Based on the results of the comparison, the
    `NoveltyItem` object is added to the `NoveltyArchive` object or not. Furthermore,
    the `ArchiveSeedAmount` constant is introduced to do initial seeding of the archive
    with the `NoveltyItem` instances at the beginnings of the evolution when the archive
    is still empty.
  prefs: []
  type: TYPE_NORMAL
- en: The fitness function with the novelty score
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we have defined the basic principles behind the NS method, we need to find
    a way to integrate it into the definition of the fitness function that will be
    used to guide the neuroevolution process. In other words, we need to define the
    novelty metric that can capture the amount of novelty that is introduced by a
    particular solver agent during the evolutionary process. There are several characteristics
    that can be used as novelty metrics for a solver agent:'
  prefs: []
  type: TYPE_NORMAL
- en: The novelty of the solver genotype structure—the *structural* novelty
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The stepping stones found in the search space of the solution—the *behavioral*
    novelty
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our primary interest in this chapter is to create a successful maze navigator
    agent. To successfully navigate through the maze, the agent must pay equal attention
    to most places in the maze. Such behavior can be achieved by rewarding agents
    who choose a unique exploration path compared to already known paths from the
    previously tested agents. In terms of the types of the previously mentioned novelty
    metrics, this means that we need to define a fitness function using a metric built
    around *behavioral* novelty.
  prefs: []
  type: TYPE_NORMAL
- en: The novelty score
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The behavioral space of the maze solver agent is defined by its trajectory through
    the maze while running the maze-solving simulation. An effective novelty score
    implementation needs to compute the sparseness at any point in such a behavioral
    space. Thus, any area with a denser cluster of visited points of behavior space
    is less novel, giving fewer rewards to the solver agent.
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned in [Chapter 1](f59c6396-55e5-4495-95c0-7af9a42c2f20.xhtml), *Overview
    of Neuroevolution Methods*, the most straightforward measure of sparseness at
    a point is the average distance from it to the *k-nearest neighbors*. The sparse
    areas have higher distance values, and the denser areas have lower distance values,
    correspondingly. The following formula gives the sparseness at point ![](img/7b7415d4-4282-432b-90c8-72d43185f310.png) of
    the behavioral space:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d7a917c6-ca46-4d78-ad0d-26539f91feb6.png)'
  prefs: []
  type: TYPE_IMG
- en: Note ![](img/35180e98-48b5-40c9-9d97-e521f6cd8278.png) is the ![](img/ef6e7309-eb38-4d1b-a9a2-b50ad4650c8f.png)
    nearest neighbor of ![](img/2b96ddaa-406a-4b2c-8add-7650fdd462d2.png) as calculated
    by the distance (novelty) metric, ![](img/ab46e610-e35c-413f-90a8-35554b3386a8.png).
  prefs: []
  type: TYPE_NORMAL
- en: The calculated by the above formula sparseness at the particular point in the
    behavioral space is a novelty score that can be used by the fitness function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Python code to find a novelty score is defined in the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding function has the following major implementation parts:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we check whether the `_novelty_avg_knn` function provided with the argument
    holds a list of all the genomes in the current population. In that case, we start
    by populating the list of distances between behavioral characteristics of all
    genomes in the population, including all the `NoveltyItem` objects from `NoveltyArchive`.
    Otherwise, we use the provided novelty item (`item`) to find distances between
    it and all `NoveltyItem` objects from `NoveltyArchive`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, we sort a list of distances in ascending order to have the smallest
    distances first because we are interested in the points that are closest to the
    provided novel item in the behavioral space:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we initialize all the intermediate variables necessary for the k-nearest
    neighbors scores calculation, and test whether the number of distance values collected
    in the previous step is higher than the `ArchiveSeedAmount` constant value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can check whether the length of the found distances list is less than
    the number of neighbors that we are asked to test against (`neighbors`). If so,
    we update the value of the related variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'After all the local variables are set to the correct values, we can start the
    cycle that collects the sum of all distances and weights for each connection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'When the preceding cycle exits because of the calculated weight value exceeding
    the specified number of neighbors, or if we already iterated over all distance
    values in the `distances` list, we are ready to calculate the novelty score for
    a given item as an average distance to the k-nearest neighbors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The function then returns the estimated novelty score value.
  prefs: []
  type: TYPE_NORMAL
- en: For more implementation details, see the `novelty_archive.py` file at [https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter6/novelty_archive.py](https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter6/novelty_archive.py).
  prefs: []
  type: TYPE_NORMAL
- en: The novelty metric
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The novelty metric is a measure of how different the current solution is from
    the already known ones. It is used to calculate the novelty score when estimating
    the distance from the current point in the behavioral space to its *k-nearest
    neighbors*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our experiment, the novelty metric measuring the difference in the behavior
    of the two agents is determined by the *item-wise distance* between the two trajectory
    vectors (one vector per agent). The trajectory vector contains the coordinates
    of the positions that were visited by the maze navigator agent during a simulation.
    The following formula gives the definition of the metric:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a61b41a8-e092-4cca-8df7-32742826e827.png)'
  prefs: []
  type: TYPE_IMG
- en: Note ![](img/cbb7ac77-245c-4bb4-9e7d-415935eedb59.png) is the size of the trajectory
    vector, and ![](img/ead12303-fc2d-4a5d-9a5c-10b6fa003efc.png) and ![](img/e8429ede-3355-4312-9e8c-8485763b8fda.png)
    are the values at position ![](img/15e07257-bf42-4ea2-ab1a-83055f6410b7.png) of
    the compared trajectory vectors, ![](img/dacf8b1f-2342-42a5-903b-1a8168bcd0a6.png)
    and ![](img/a2692ece-78af-4644-a161-903af7653c63.png).
  prefs: []
  type: TYPE_NORMAL
- en: In a maze navigation experiment, we are mostly interested in the final position
    of the solver agent. Thus, the trajectory vector may only contain the final coordinates
    of the agent after completing all the necessary steps in the maze navigation simulation or
    when the maze exit is found.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Python code for the novelty metric value estimation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code takes two novelty items and finds the *item-wise* distance
    between the two trajectory vectors holding the positions of a corresponding solver
    agent during the simulation of a maze navigation.
  prefs: []
  type: TYPE_NORMAL
- en: Fitness function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The fitness function used in the experiments described in this chapter directly
    applies the novelty score defined previously as the fitness value of the genome.
    As a result, the neuroevolution process tries to maximize the novelty of the produced
    individuals by using such a fitness function.
  prefs: []
  type: TYPE_NORMAL
- en: 'For different tasks in this experiment, we use various fitness factors:'
  prefs: []
  type: TYPE_NORMAL
- en: The novelty score is used to guide the neuroevolution process (solution search
    optimization). It is assigned as a fitness value to each genome and used for genome
    evaluation during generations of evolution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The goal-oriented fitness score (the distance to the maze exit) obtained from
    the maze simulator is used to test if the ultimate goal has been achieved (that
    is, the maze exit has been found)—also, this value is recorded for performance
    evaluation of each solver agent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The source code of the fitness values evaluation is presented in two functions:'
  prefs: []
  type: TYPE_NORMAL
- en: The callback function to evaluate the fitness scores of the entire population
    (`eval_genomes`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The function to evaluate individual genomes through the maze solving simulation
    ( `eval_individual`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The population fitness evaluation function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The fitness evaluation function is a callback function that is registered with
    the NEAT-Python library, allowing this library to run an evaluation of population
    genomes against specific conditions of a particular task that needs to be solved.
    We implement this function to evaluate each genome in the current population using
    the maze-solving task, and to use the obtained novelty score as a genome fitness
    value.
  prefs: []
  type: TYPE_NORMAL
- en: The NEAT-Python library doesn't allow us to send any signals about task completion
    from the callback function other than by specifying the specific fitness score
    value of the winner genome. This fitness value must be higher than the fitness
    threshold in the NEAT-Python hyperparameter configuration. However, with the NS
    algorithm, it is not possible to accurately estimate the upper limit of the novelty
    score that can be achieved by the winner genome. Furthermore, the winner genome
    can have the novelty score value that is below the values obtained by genomes
    earlier in the evolution process, when the solution search space was not so thoroughly
    explored.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, given that the novelty score is assigned to genomes as their fitness values,
    we need to come up with a workaround that allows us to use the standard termination
    criteria defined by the NEAT-Python library. We do this by using a specific indicative
    novelty score value that is big enough to be encountered during normal algorithm
    execution. This value determines the termination criterion that is provided through
    the NEAT-Python hyperparameter configuration. We use `800000` as an indicative
    measure of the novelty score and its natural logarithm (about `13.59`) as the
    appropriate fitness threshold.
  prefs: []
  type: TYPE_NORMAL
- en: 'The full source code of the function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The significant parts of the implementation of the function are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create the dictionary to store evaluated novelty items (`n_items_map`)
    for each genome in the population, and cycle through all genomes in the population,
    evaluating their maze-solving performance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, we cycle through all genomes in the population one more time to
    assign fitness scores to the genomes using estimated novelty scores. The process
    of novelty score estimation uses the `NoveltyItem` objects collected in `n_items_map` in
    the first cycle (described earlier) during the maze-solving simulation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, if a successful solver genome is found in the first cycle, we assign
    it with a fitness value equal to the indicative fitness score described earlier
    (`~13.59`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Please note that we apply the natural logarithm to the obtained novelty score
    values and to the indicative novelty score to keep them in numerical proximity.
    As a result, we can properly render performance plots using statistics collected
    during the experiment.
  prefs: []
  type: TYPE_NORMAL
- en: The individual fitness evaluation function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This function is an essential part of the population fitness evaluation, and
    it is invoked from the `eval_genomes` function, discussed earlier, to evaluate
    the maze-solving performance of each genome in the population.
  prefs: []
  type: TYPE_NORMAL
- en: 'The evaluation of the individual genome as a maze-solving agent through the
    maze navigation simulation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s delve into the meaning of all the central parts of the implementation
    of the `eval_individual` function:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create the `NoveltyItem` object to hold information about the novelty
    score associated with a particular genome and save it under the `genome_id` key
    in the `n_items_map` dictionary:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, we create a deep copy of the original maze environment to avoid
    side effects during the simulation, and create the control ANN from the provided
    genome:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, using a copy of the maze environment and the created control ANN, we execute
    the maze-solving simulation for a given number of simulation steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'After the simulation is finished, the returned goal-based fitness score (proximity
    to the maze exit) and other simulation and genome parameters are stored in `AgentRecord`,
    which is then added to the record store:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we estimate the novelty score of the given genome if it is not a winner,
    and update the list of the fittest genomes in `NoveltyArchive` with `NoveltyItem`
    of the current genome, if appropriate:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: In this experiment, the fitness score of the genome is defined as two separate
    values, each serving a different purpose. The goal-oriented fitness score helps
    to test whether a solution has been found and collects useful performance statistics.
    The novelty-based fitness score guides the neuroevolution process in the direction
    of the maximal diversity of solver behavior, which means that the gradient of
    the solution search is directed toward exploring different behaviors, without
    any explicit objective.
  prefs: []
  type: TYPE_NORMAL
- en: For more details about the implementation, please refer to the `maze_experiment.py`
    file at [https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter6/maze_experiment.py.](https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter6/maze_experiment.py)
  prefs: []
  type: TYPE_NORMAL
- en: Experimenting with a simple maze configuration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We start our experiments using a simple maze configuration similar to the one
    described in the previous chapter. However, instead of the goal-oriented objective
    function, we use the NS optimization method to guide the neuroevolution process.
    We hope that with Novelty Search method it will be possible to find a successful
    maze solver with fewer epochs of evolution.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the schema of the simple maze in the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f1a00fe6-4220-42ac-a268-9e614a672c5e.png)'
  prefs: []
  type: TYPE_IMG
- en: The simple maze configuration
  prefs: []
  type: TYPE_NORMAL
- en: The maze configuration is the same as in the previous chapter. However, we need
    to adjust the corresponding NEAT hyperparameters to meet the specifications of
    the NS optimization method.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The objective function used in the experiments described in this chapter is
    based on a novelty metric that has no clear upper-boundary value. As a result,
    the fitness threshold value cannot be estimated precisely. Thus, to signal that
    the winning solution was found, we use an indicative value that is big enough
    to not be encountered during the normal algorithm execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'We selected `800000` as the indicative novelty score value. However, to maintain
    the visual presentation of the fitness scores when plotting the results of an
    experiment, we scaled down the obtained novelty scores of the solver agents using
    the natural logarithm. Thus, the fitness threshold value used in the configuration
    file becomes `13.5`, which is a bit less than the maximum possible fitness score
    (`13.59`) to avoid issues with rounding float numbers. Also, we increase the population
    size from the value described in the previous chapter (`250`) to make the solution
    search space deeper because we need to examine the maximum number of unique places
    in the maze:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We run more generations in each trial than we did in the experiment in the
    previous chapter. Therefore, we have increased the stagnation value to keep species
    around for longer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: All other NEAT hyperparameters have similar values to the ones presented in
    the previous chapter. Please refer to the previous chapter for the rationales
    for selecting the specific hyperparameter values.
  prefs: []
  type: TYPE_NORMAL
- en: The complete list of hyperparameters used in the experiment can be found in
    the `maze_config.ini` file at [https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter6/maze_config.ini](https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter6/maze_config.ini).
  prefs: []
  type: TYPE_NORMAL
- en: Working environment setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The working environment for the experiment should include all dependencies
    and can be created using Anaconda with the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: These commands create and activate a `maze_ns_neat` virtual environment with
    Python 3.5\. After that, the NEAT-Python library with version 0.92 is installed,
    along with other dependencies used by our visualization utilities.
  prefs: []
  type: TYPE_NORMAL
- en: The experiment runner implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The experiment runner implementation used in this chapter is similar for the
    most part to the one used in the previous chapter but has significant differences,
    which we will discuss in this section.
  prefs: []
  type: TYPE_NORMAL
- en: The trials cycle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduce an upgrade to the experiment runner implementation.
    We implement support to run multiple trials sequentially until the solution is
    found. Such an upgrade dramatically simplifies working with the multiple experiment
    trials sequentially, especially taking into account that each trial can take a
    long time to execute.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main cycle of the experiment runner now looks like this (see `__main__` in
    the `maze_experiment.py` script):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The cycle runs the `args.trials` number of experiment trials, where `args.trials`
    is provided by the user from the command-line.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first lines of the cycle create the `NoveltyArchive` object, which is a
    part of the Novelty Search algorithm. Later, during a specific trial, this object
    will be used to store all the relevant `NoveltyItems`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Note that `maze.maze_novelty_metric` is a reference to the function that is
    used to evaluate the novelty score of each solver agent.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the source code for this chapter, we provide implementations of two novelty
    metric functions:'
  prefs: []
  type: TYPE_NORMAL
- en: The item-wise distance novelty metric (`maze.maze_novelty_metric`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Euclidean distance novelty metric (`maze.maze_novelty_metric_euclidean`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, in our experiments, we use the first implementation. The second implementation
    is intended for you to run additional experiments.
  prefs: []
  type: TYPE_NORMAL
- en: The experiment runner function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The runner function has many similarities to the runner function introduced
    in the previous chapter, but, at the same time, it has unique features that are
    specific to the NS optimization algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we consider the most significant parts of the implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'It starts with selecting a specific seed value for a random number generator,
    based on the current system time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, it loads the NEAT algorithm configuration and creates an initial
    population of genomes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: To hold the intermediate results after each generation evaluation, we initialize
    a `trial_sim` global variable with the `MazeSimulationTrial` object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We use a global variable so it can be accessed by the fitness evaluation callback
    function (`eval_genomes(genomes, config)`) that is passed to the NEAT-Python framework:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, traditionally, we register with the `Population` object the number of
    reporters to output algorithm results and to collect statistics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we are ready to run the NEAT algorithm over a specified number of generations
    and evaluate the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, the collected statistics and novelty archive records can be visualized
    and saved to the filesystem:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we perform additional visualization routines introduced in this chapter
    that visualize the path of the maze-solver agents through the maze.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We do this by running a simulation of maze navigation against the controller
    ANN of the best solver agent found during the evolution. During this simulation
    run, all the path points visited by a solver agent are collected to be rendered
    later by the `draw_agent_path` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: In the end, the `run_experiment` function returns a Boolean value indicating
    whether a successful maze solver agent was found during the trial or not.
  prefs: []
  type: TYPE_NORMAL
- en: Please refer to the `run_experiment(config_file, maze_env, novelty_archive,
    trial_out_dir, args=None, n_generations=100, save_results=False, silent=False)`
    function in the `maze_experiment.py` file at [https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter6/maze_experiment.py](https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter6/maze_experiment.py).
  prefs: []
  type: TYPE_NORMAL
- en: Running the simple maze navigation experiment with NS optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Make sure you copy all related Python scripts and configuration files (`maze_config.ini`
    and `medium_maze.txt)` into the local directory from the online repository that
    can be found at: [https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter6/](https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter6/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now enter this directory, and execute the following command in the Terminal
    application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Do not forget to activate the appropriate virtual environment with the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '`conda activate maze_ns_neat`'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding command runs 10 trials of the maze navigation experiment with
    the simple maze configuration loaded from the `medium_maze.txt` file. The neuroevolution
    algorithm evaluates `500` generations of maze solvers in each trial, using the
    NEAT configuration data loaded from the `maze_config.ini` file. The width and height parameters
    specify the dimensions of the maze records subplot (see the `visualize.draw_maze_records` function
    implementation for more details).
  prefs: []
  type: TYPE_NORMAL
- en: 'After `99` generations of the evolution, the successful maze solver agent is
    found in generation `100`. There are general statistics about the population of
    genomes in the last generation of evolution. In the console output of the completed
    Python program, you will see the following for the last generation of evolution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, we display the configuration of the winner genome and general statistics
    about the trial:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The console output shows us that the winner genome that encodes the control
    ANN of the successful maze solver has only two node genes and eight connection
    genes. These genes correspond to the two output nodes in the controller ANN, with
    the eight connections used to establish links with the inputs. The resulting configuration
    of the controller ANN is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b05eed53-88f7-422e-b9c1-a227acc31092.png)'
  prefs: []
  type: TYPE_IMG
- en: The configuration of the successful controller ANN
  prefs: []
  type: TYPE_NORMAL
- en: The configuration of the successful controller ANN is better than the configuration
    described in the previous chapter, which was found using the *goal-oriented* search
    optimization method. In this experiment, the ANN configuration omits the hidden
    nodes completely, and the evolutionary process takes fewer generations finding
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we can assume that the Novelty Search optimization method is at least
    as effective as the goal-oriented method. This is even though the search optimization
    method is not based on the proximity to the final goal, but on rewarding novel
    behavior. The neuroevolution process produced a successful maze solver agent without
    any hints about the proximity to the final goal (maze exit), and that is just
    amazing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, it is interesting to look at the speciation graph during the evolution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/426942dd-7951-4b6b-b8bd-8852762afaf5.png)'
  prefs: []
  type: TYPE_IMG
- en: The speciation graph
  prefs: []
  type: TYPE_NORMAL
- en: In the speciation graph, we can see that the total number of species during
    the evolutionary process does not exceed nine. Furthermore, most of them are present
    from the very first generations of the evolution until a successful maze solver
    is found.
  prefs: []
  type: TYPE_NORMAL
- en: Agent record visualization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We used the method of visualizing agent records that was introduced in the previous
    chapter, and we introduced a new visualization method to visualize the path of
    the solver agent through the maze.
  prefs: []
  type: TYPE_NORMAL
- en: The visualization of agents records saved automatically for each completed trial
    as an `obj_medium_maze_records.svg` SVG file in the output directory of the corresponding
    experiment.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following image, you can look at the visualization of agents records for
    the experiment described in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c57f068f-d251-48d0-9763-d40b3c64bc10.png)'
  prefs: []
  type: TYPE_IMG
- en: The visualization of agents records
  prefs: []
  type: TYPE_NORMAL
- en: The top subplot of the plot shows the final positions of the agents belonging
    to the fittest species that have a goal-oriented fitness score value above **0.8**.
    We were able to find eight species that explored almost all areas of the maze
    and were finally able to find the maze exit. At the same time, even the evolutionary
    losers (the bottom plot) demonstrated highly explorative behavior, evenly filling
    the first half of the maze area (compare this with the similar plot in the previous
    chapter).
  prefs: []
  type: TYPE_NORMAL
- en: Also, it is important to note that eight of the total nine species created during
    the evolutionary process demonstrate the highest goal-oriented fitness scores;
    that is, they were almost able to reach the maze exit (and one of them ultimately
    reached it). This achievement is in stark contrast with the experiment in the
    previous chapter, where only half of all species (six from twelve) achieved the
    same results.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the most exciting visualization allows us to look at the path of the
    successful maze solver agent that was able to find the maze exit:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fa1bb586-03f5-4f99-8cdf-fc088c4f51f0.png)'
  prefs: []
  type: TYPE_IMG
- en: The path through the maze of the successful maze solver
  prefs: []
  type: TYPE_NORMAL
- en: The visualization can be found in the `output` directory of the experiment in
    the `best_solver_path.svg` file.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, a successful maze solver agent was able to find an almost optimal
    path through the maze, even though it does appear to get a little confused at
    the beginning.
  prefs: []
  type: TYPE_NORMAL
- en: It's just amazing that such a convoluted path through the maze can be found
    without any reference to the location of the maze exit but only by rewarding the
    novelty of each intermediate solution that is found.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Set the population size (`pop_size`) parameter in the `maze_config.ini` file
    to `250`. See if the maze solver can be found in this case.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Change the value of the parameter specifying the probability of adding a new
    node (`node_add_prob`). Was the neuroevolution process able to find a solution,
    and is it optimal from a topological point of view?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Change the initial genome configuration to have zero hidden nodes at the beginning
    (`num_hidden`). How does this affect the algorithm's performance?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try to use another novelty metric that is provided with the source code (`maze.maze_novelty_metric_euclidean`)
    and see what happens.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Change the `location_sample_rate` command-line parameter from its default value
    (`4000`), which allows you to include only the final position of the maze solver
    into its behavioral vector. Try the values that are less than `400` (the number
    of maze simulation steps). For example, if we set this parameter to `100`, then
    the behavioral vector will include coordinates a maximum of four trajectory points for
    each solver agent. See how this parameter can influence algorithm performance.
    You can provide a value for this parameter by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command runs the simple maze experiment with `location_sample_rate`
    set to `100`.
  prefs: []
  type: TYPE_NORMAL
- en: Experimenting with a hard-to-solve maze configuration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the next experiment, we evaluate the effectiveness of the NS optimization
    method in a more complex task. In this task, we try to evolve a maze solving agent
    that can find a path through a maze with a complex configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this experiment, we use the hard-to-solve maze configuration introduced
    in the previous chapter. Such an approach allows us to compare results obtained
    with the NS optimization method against the results obtained with the *goal-oriented*
    optimization method used in the previous chapter. The maze configuration is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ba8f0918-f64e-4432-aacc-6723715819de.png)'
  prefs: []
  type: TYPE_IMG
- en: The hard-to-solve maze configuration
  prefs: []
  type: TYPE_NORMAL
- en: This maze configuration is identical to the one described in the previous chapter.
    Thus, you can refer to [Chapter 5](22365f85-3003-4b67-8e1e-cc89fa5e259b.xhtml),
    *Autonomous Maze Navigation*, for a detailed description.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter selection and working environment setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The hyperparameters for this experiment are the same that we used for a simple
    maze experiment earlier in this chapter. We decided to leave the hyperparameters
    unchanged to test how well the algorithm generalizes by trying to find a solution
    to a task within the same domain, but with a different configuration.
  prefs: []
  type: TYPE_NORMAL
- en: The working environment for this experiment is fully compatible with the environment
    already created for the simple maze experiment. Thus, we can use it as well.
  prefs: []
  type: TYPE_NORMAL
- en: Running the hard-to-solve maze navigation experiment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To run this experiment, we can use the same experiment runner that we developed
    for the simple maze experiment, with the only difference being that different
    command-line parameters should be provided at the start. You can start the hard
    maze experiment with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: This command starts the hard-to-solve maze experiment for `10` trials with `500`
    generations each. The width and height parameters determine the dimensions of
    the subplot to draw the maze records collected during the experiment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the NEAT-Python library for the hard maze experiment, we were unable
    to find a successful maze solver within 10 trials, even with the NS optimization
    method. Nevertheless, the results obtained with the NS method are more promising
    than with the goal-oriented optimization method from the previous chapter. You
    can see this in the following plot, which depicts the final positions of the solver
    agents during the maze navigation simulation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/80175455-60ad-457a-9aeb-74a46b29a230.png)'
  prefs: []
  type: TYPE_IMG
- en: The agents records visualization
  prefs: []
  type: TYPE_NORMAL
- en: The plot that visualizes the final positions of all evaluated agents demonstrates
    that, during this experiment, more areas of the maze were explored with the NS
    optimization method than with the goal-oriented method. Also, you can see that
    some species were almost at the finish line, only a few steps away from reaching
    the maze exit.
  prefs: []
  type: TYPE_NORMAL
- en: 'The path of the most successful maze-solver agent is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d697228a-ba57-4289-8c41-672d3bd5b1f2.png)'
  prefs: []
  type: TYPE_IMG
- en: The path through the maze of the most successful maze solver agent
  prefs: []
  type: TYPE_NORMAL
- en: The path through the maze taken by the most successful solver agent demonstrates
    that the agent was able to discover the crucial relations between sensor inputs
    and the maneuvers to perform. However, it still lacks precision in applying the
    control signal. Due to this flaw, some control actions lead to ineffective trajectory
    loops, consuming precious time steps allotted to solve the maze.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, it is interesting to take a look at the topology of the control ANN
    of the most successful maze solver:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e53507b6-a2b8-4baa-ac42-057c119551a0.png)'
  prefs: []
  type: TYPE_IMG
- en: The topology of the control ANN
  prefs: []
  type: TYPE_NORMAL
- en: You can see that all sensor inputs were involved in the decision-making, in
    contrast with the topology of the control ANN devised in the previous experiment
    in this chapter. Furthermore, the network topology includes two hidden nodes,
    which allows the agent to implement a complex control strategy to navigate through
    the hard-to-solve maze environment.
  prefs: []
  type: TYPE_NORMAL
- en: Despite our failure to evolve a successful maze solver agent with the Novelty
    Search optimization method in this experiment using the NEAT-Python library, it
    is rather an issue of ineffective NEAT implementation by the library than a failure
    of the Novelty Search method.
  prefs: []
  type: TYPE_NORMAL
- en: I have made an implementation of the NEAT algorithm in the GO programming language
    that solves a hard maze navigation task with high efficiency. You can check it
    out on GitHub at [https://github.com/yaricom/goNEAT_NS](https://github.com/yaricom/goNEAT_NS).
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the source code for this chapter, we also provide the experiment runner implementation
    based on the MultiNEAT Python library that we introduced in [Chapter 2](c673e180-4440-4eea-98f8-8800c77162c8.xhtml),
    *Python Libraries and Environment Setup*.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can try to use it to solve the hard maze task as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Update the current Anaconda environment by installing the MultiNEAT Python
    library with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the experiment runner implementation based on the MultiNEAT library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: These commands install the MultiNEAT library in the current Anaconda environment
    and start 10 trials (with `500` generations each) of the hard maze experiment
    using an appropriate experiment runner.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about the *Novelty Search* optimization method
    and how it can be used to guide the neuroevolution process in deceptive problem
    space environments, such as maze navigation. We conducted the same maze navigation
    experiments as in the previous chapter. After that, we compared the results we
    obtained to determine if the NS method has advantages over the goal-oriented optimization
    method introduced in the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: You got the practical experience of writing source code using Python and experimented
    with tuning the important hyperparameters of the NEAT algorithm. Also, we introduced
    a new visualization method, allowing you to see the path of the agent through
    the maze. With this method, you can easily compare how different agents are trying
    to solve the maze navigation problem and whether the path through the maze that
    was found is optimal or not.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter introduces more advanced applications of the NEAT algorithm.
    We start with the task of visual discrimination and introduce you to the HyperNEAT
    extension of the NEAT algorithm. The HyperNEAT method allows you to work with
    large-scale ANNs operating over thousands or millions of parameters. This scale
    of operations is impossible with the classic NEAT algorithm.
  prefs: []
  type: TYPE_NORMAL
