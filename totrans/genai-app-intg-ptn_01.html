<html><head></head><body>
<div><div><h1 class="chapterNumber">1</h1>
<h1 class="chapterTitle" id="_idParaDest-15">Introduction to Generative AI Patterns</h1>
<p class="normal">This chapter provides an overview of key concepts, techniques, and integration patterns related to generative AI that will empower you to harness these capabilities in real-world applications.</p>
<p class="normal">We will provide an overview of generative AI architectures, such as transformers and diffusion models, which are the basis for these generative models to produce text, images, audio, and more. You’ll get a brief introduction to specialized training techniques, like pre-training and prompt engineering, that upgrade basic language models into creative powerhouses.</p>
<p class="normal">Understanding the relentless pace of innovation in this space is critical due to new models and ethical considerations emerging constantly. We’ll introduce strategies for experimenting rapidly while ensuring responsible, transparent development.</p>
<p class="normal">The chapter also introduces common integration patterns for connecting generative AI into practical workflows. Whether crafting chatbots that leverage models in real time or performing batch enrichment of data, we will introduce prototyping blueprints to jumpstart building AI-powered systems.</p>
<p class="normal">By the end, you will have a one-thousand-foot view of which generative AI models are available, why experimentation is important, and how these integration patterns can help create value for your organization leveraging generative AI.</p>
<p class="normal">In a nutshell, the following main topics will be covered:</p>
<ul>
<li class="bulletList">Interacting with AI</li>
<li class="bulletList">Predictive AI vs generative AI use case ideation</li>
<li class="bulletList">A change in the paradigm</li>
<li class="bulletList">General generative AI concepts</li>
<li class="bulletList">Introduction to generative AI integration patterns</li>
</ul>
<h1 class="heading-1" id="_idParaDest-16">From AI predictions to generative AI</h1>
<p class="normal">The intent of this section is to provide a brief overview of artificial intelligence, highlighting our initial experiences with it. In the early 2000s, AI <a id="_idIndexMarker000"/>started to become more tangible for consumers. For example, in 2001, Google introduced the “Did you mean?” feature (<a href="https://blog.google/intl/en-mena/product-updates/explore-get-answers/25-biggest-moments-in-search-from-helpful-images-to-ai/">https://blog.google/intl/en-mena/product-updates/explore-get-answers/25-biggest-moments-in-search-from-helpful-images-to-ai/</a>), which suggests spelling corrections. This was one of Google’s first applications of machine learning and one of the early AI features that the general public got to experience on a large scale. </p>
<p class="normal">Over the following years, AI systems became more sophisticated, especially in areas like computer vision, speech-to-text conversion, and text-to-speech synthesis. Working in the telecom industry helped me witness the innovation driven by speech-to-text in particular. Integrating<a id="_idIndexMarker001"/> speech-to-text capabilities into <strong class="keyWord">interactive voice response</strong> (<strong class="keyWord">IVR</strong>) systems led to better user experiences by allowing people to speak their requests rather than punch numbers into a keypad. For example, you could be calling a bank where you would be welcomed by a message asking you to say “balance” to check your balance, “open account” in order to open an account, etc. Nowadays we are seeing more and more implementations of AI, simplifying more complex and time-consuming tasks.</p>
<p class="normal">The exponential increase in available computing power, paired with the massive datasets needed to train machine learning models, unleashed new AI capabilities. In the 2010s, AI started matching and even surpassing human performance on certain tightly defined tasks like image classification. </p>
<p class="normal">The advent of generative AI<a id="_idIndexMarker002"/> has reignited interest and innovation in the AI field, introducing new approaches for exploring use cases and system integration. Models like Gemini, PaLM, Claude, DALL-E, OpenAI GPT, and Stable Diffusion showcase the ability of AI systems to generate synthetic text, images, and other media. The outputs exhibit creativity and imagination that capture the public’s attention. However, the powerful capabilities of generative models also highlight new challenges around system design and responsible deployment. There is a need to rethink integration patterns and architecture to support safe, robust, and cost-effective implementations. Specifically, issues around security, bias, toxicity, and misinformation must be addressed through techniques like dataset filtering, human-in-the-loop systems, enhanced monitoring, and immediate remediation.</p>
<p class="normal">As generative AI <a id="_idIndexMarker003"/>continues maturing, best practices and governance frameworks must evolve in tandem. Industry leaders have formed partnerships like the Content Authenticity Initiative to develop technical standards and policy guidance around the responsible development of the next iteration of AI. This technology’s incredible potential, from accelerating drug discovery to envisioning new products, can only be realized through a commitment to transparency, ethics, and human rights. Constructive collaboration that balances innovation with caution is imperative.</p>
<p class="normal">Generative AI marks an inflection point for the field. The ripples from this groundswell of creative possibility are just beginning to reach organizations and communities. Maintaining an open, evidence-driven dialogue around not just capabilities but also challenges lays a foundation for AI deployment that empowers people, unlocks new utility, and earns widespread trust.</p>
<p class="normal">We are witnessing an unprecedented democratization of generative AI capabilities through publicly accessible APIs from established companies like Google, Meta, and Amazon, and startups such as Anthropic, Mistral AI, Stability AI, and OpenAI. The table below summarizes several leading models that provide versatile foundations for natural language and image generation.</p>
<p class="normal">Just a few years ago, developing with generative AI required specialized expertise in deep learning and access to vast computational resources. Now, models like Gemini, Claude, GPT-4, DALL-E, and Stable Diffusion can be accessed via simple API calls at near-zero cost. The bar for experimentation has never been lower.</p>
<p class="normal">This commoditization has sparked an explosion of new applications leveraging these pre-trained models – from creative tools for content generation to process automation solutions infused with AI. Expect integrations with generative foundations across all industries in the coming months and years.</p>
<p class="normal">Models are becoming more knowledgeable, with broader capabilities and reasoning that will reduce hallucinations and increase accuracy across model responses. Multimodality is also gaining traction, with models able to ingest and generate content across text, images, audio, video, and 3D scenes. In terms of scalability, model size and context windows continue expanding exponentially; for example, Google’s Gemini 1.5 now supports a context window of 1 million tokens.</p>
<p class="normal">Overall, the outlook points to a future where generative AI will become deeply integrated into most technologies. These models introduce new efficiencies and automation potential and inspire creativity across nearly every industry imaginable.</p>
<p class="normal">The table below highlights some of the most popular LLMs and their providers. The purpose of the table is to highlight the vast number of options available on the market at the time of writing this book. We expect this table to quickly become outdated by the time of publication and highly <a id="_idIndexMarker004"/>encourage readers to dive deep into the model providers’ websites to stay up to date with any new launches.</p>
<table class="table-container" id="table001">
<tbody>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Model</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Provider</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Landing Page</strong></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">Gemini</p>
</td>
<td class="table-cell">
<p class="normal">Google</p>
</td>
<td class="table-cell">
<p class="normal"><a href="https://deepmind.google/technologies/gemini">https://deepmind.google/technologies/gemini</a></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">Claude</p>
</td>
<td class="table-cell">
<p class="normal">Anthropic</p>
</td>
<td class="table-cell">
<p class="normal"><a href="https://claude.ai/">https://claude.ai/</a></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">ChatGPT</p>
</td>
<td class="table-cell">
<p class="normal">OpenAI</p>
</td>
<td class="table-cell">
<p class="normal"><a href="https://openai.com/blog/chatgpt">https://openai.com/blog/chatgpt</a></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">Stable Diffusion</p>
</td>
<td class="table-cell">
<p class="normal">Stability AI</p>
</td>
<td class="table-cell">
<p class="normal"><a href="https://stability.ai/">https://stability.ai/</a></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">Mistral</p>
</td>
<td class="table-cell">
<p class="normal">Mistral AI</p>
</td>
<td class="table-cell">
<p class="normal"><a href="https://mistral.ai/">https://mistral.ai/</a></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">LLaMA</p>
</td>
<td class="table-cell">
<p class="normal">Meta</p>
</td>
<td class="table-cell">
<p class="normal"><a href="https://llama.meta.com/">https://llama.meta.com/</a></p>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref">Table 1.1: Overview of popular LLMs and their providers</p>
<h1 class="heading-1" id="_idParaDest-17">Predictive AI vs generative AI use case ideation</h1>
<p class="normal">Predictive AI refers to systems that analyze data<a id="_idIndexMarker005"/> to identify patterns and make forecasts or classifications about future events. In contrast, generative AI models <a id="_idIndexMarker006"/>create new synthetic content like images, text, or code based on the patterns gleaned from their training data. For example, with predictive AI, you can confidently identify if an image contains a cat or not, whereas with generative AI you can create an image of a cat from a text prompt, modify an existing image to include a cat where there was none, or generate a creative text blurb about a cat.</p>
<p class="normal">Product innovation focused on AI involves various phases of the product development lifecycle. With the emergence of generative AI, the paradigm has shifted away from initially needing to compile training data to train traditional ML models and toward leveraging flexible pre-trained models.</p>
<p class="normal">Foundational models like <a id="_idIndexMarker007"/>Google’s PaLM 2<a id="_idIndexMarker008"/> and Gemini, OpenAI’s GPT<a id="_idIndexMarker009"/> and DALL-E, and Stable Diffusion<a id="_idIndexMarker010"/> provide broad foundations enabling rapid prototype <a id="_idIndexMarker011"/>development. Their versatile capabilities lower the barrier for experimenting with novel AI applications. </p>
<p class="normal">Where previously data curation and model training from scratch could take months before assessing viability, now proof-of-concept generation is possible within days without the need to fine-tune a foundation model.</p>
<p class="normal">This generative approach<a id="_idIndexMarker012"/> facilitates more iterative concept validation. After quickly building an initial prototype powered by the baseline model, developers can then collect niche training data and perform knowledge transfer via techniques like distillation to customize later versions; we will deep dive into the concept of distillation later in the book. The model’s primary foundation contains already encoded patterns useful for kickstarting and for iterations of new models.</p>
<p class="normal">In contrast, the predictive modeling approach requires upfront data gathering and training before any application testing. This more linear progression limits early-stage flexibility. However, predictive systems can efficiently learn specialized correlations and achieve a high level of confidence inference metrics once substantial data exists.</p>
<p class="normal">Leveraging versatile generative foundations supports rapid prototyping and use case exploration. But, later, custom predictive modeling boosts performance on narrow tasks with sufficient data. Blending these AI approaches capitalizes on their complementary strengths throughout the model deployment lifecycle. </p>
<p class="normal">Beyond the basic use – prompt engineering – of a foundational model, several auxiliary, more complex techniques can enhance its capabilities. Examples<a id="_idIndexMarker013"/> include <strong class="keyWord">Chain-of-Thought</strong> (<strong class="keyWord">CoT</strong>) and <strong class="keyWord">ReAct</strong>, which empower the model to not only reason about a situation but also define and evaluate a course of action. </p>
<p class="normal">ReAct, presented in the<a id="_idIndexMarker014"/> paper <em class="italic">ReAct: Synergizing Reasoning and Acting in Language Models</em> (<a href="https://arxiv.org/abs/2210.03629">https://arxiv.org/abs/2210.03629</a>), addresses the current disconnect between LLMs’ language understanding and their ability to make decisions. While LLMs excel at tasks like comprehension and question answering, their reasoning and action-taking skills (for example, generating action plans or adapting to unforeseen situations) are often treated separately.</p>
<p class="normal">ReAct bridges this gap by prompting LLMs to generate both “reasoning traces,” detailing the model’s thought process, and task-specific actions in an interleaved manner. This tight coupling allows the model to leverage reasoning for planning, execution monitoring, and error handling, while simultaneously using actions to gather additional information from external sources like knowledge bases or environments. This integrated approach demonstrably improves LLM performance in both language and decision-making tasks. </p>
<p class="normal">For example, in question-answering and fact-verification tasks, ReAct combats common issues like hallucination and error propagation by utilizing a simple Wikipedia API. This interaction allows the model to generate more transparent and trustworthy solutions compared to methods lacking reasoning or action components. LLM hallucinations are defined as content generated that seems plausible yet factually unsupported. There are various papers that aim to address this phenomenon. For example, <em class="italic">A survey of Hallucination in Large Language Models – Principles, Taxonomy, Challenges, and Open Questions</em> deep dives into an approach to not only identify but also mitigate hallucinations. Another good example of a mitigation technique is covered in the paper <em class="italic">Chain-of-Verification Reduces Hallucination in Large Language Models</em> (<a href="https://arxiv.org/pdf/2309.11495.pdf">https://arxiv.org/pdf/2309.11495.pdf</a>). At the time of writing this book, hallucinations are a very rapidly changing field.</p>
<p class="normal">Both CoT and ReAct rely on prompting: feeding the LLM with carefully crafted instructions that guide its thought process. CoT, as presented<a id="_idIndexMarker015"/> in the paper <em class="italic">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</em> (<a href="https://arxiv.org/abs/2201.11903">https://arxiv.org/abs/2201.11903</a>), focuses on building a chain of reasoning steps, mimicking human thinking. Imagine prompting the model with: “I want to bake a cake. First, I need flour. Where can I find some?” The model responds with a potential source, like your pantry. This back-and-forth continues, building a logical chain of actions and decisions.</p>
<p class="normal">ReAct takes things<a id="_idIndexMarker016"/> a step further, integrating action into the reasoning loop. Think of it as a dynamic dance between thought and action. The LLM not only reasons about the situation but also interacts with the world, fetching information or taking concrete steps, and then updates its reasoning based on the results. It’s like the model simultaneously planning a trip and checking maps to adjust the route if it hits a roadblock.</p>
<p class="normal">This powerful synergy between reasoning and action unlocks a new realm of possibilities for LLMs. CoT and ReAct tackle challenges<a id="_idIndexMarker017"/> like error propagation (jumping to the wrong conclusions based on faulty assumptions) by allowing the model to trace its logic and correct course. They also improve transparency, making the LLM’s thought process clear and understandable.</p>
<p class="normal">In other words, <strong class="keyWord">large language models</strong> (<strong class="keyWord">LLMs</strong>) are like brilliant linguists, adept<a id="_idIndexMarker018"/> at understanding and generating text. But when it comes to real-world tasks demanding reasoning and action, they often stumble. Here’s where techniques like CoT and ReAct enter the scene, transforming LLMs into reasoning powerhouses.</p>
<p class="normal">Imagine an LLM helping diagnose a complex disease. CoT could guide it through a logical chain of symptoms and examinations, while ReAct could prompt it to consult medical databases or run simulations. This not only leads to more accurate diagnoses but also enables doctors to understand the LLM’s reasoning, fostering trust and collaboration.</p>
<p class="normal">These futuristic applications are what drive us to keep building and investing in this technology, which is very exciting. Before we dive deep into the patterns that are needed to leverage generative AI technology to generate business value, let’s take a step back and look at some initial concepts.</p>
<h1 class="heading-1" id="_idParaDest-18">A change in the paradigm</h1>
<p class="normal">It feels like eons ago in tech years, but let’s rewind just a couple of years, back when if you were embarking on solving an AI problem, you couldn’t default to utilizing a pre-trained model through the web or a managed endpoint. The process was meticulous – you’d have to first clearly define the specific use case, identify what data you had available and could collect to train a custom model, select the appropriate algorithm and model architecture, train the model using specialized hardware and software, and validate if the outputs would actually help solve the task at hand. If all went well, you would have a model that would take a predefined input and also provide a predefined output.</p>
<p class="normal">The paradigm profoundly shifted with the advent of LLMs and <a id="_idIndexMarker019"/>large multimodal models. Suddenly, you could access a pre-trained model with billions of <a id="_idIndexMarker020"/>parameters and start experimenting right off the bat with these versatile foundational models where the inputs and outputs are dynamic in nature. After tinkering around, you’d then evaluate if any fine-tuning is necessary to adapt the model to your needs, rather than pre-training an entire model from scratch. And spoiler alert – in most cases, chances are you won’t even need to fine-tune a foundational model.</p>
<p class="normal">Another key shift relates to the early belief that one model would outperform all others and solve all tasks. However, the model itself is just the engine; you still need an entire ecosystem packaged together to provide a complete solution. Foundational models have certainly demonstrated some incredible capabilities beyond initial expectations. But we also observe that certain models are better suited for certain tasks. And running the same prompt through other models can produce very different outputs depending on the underlying model’s training datasets and architecture.</p>
<p class="normal">So, the new experimental path often focuses first on prompt engineering, response evaluation, and then fine-tuning the foundational model<a id="_idIndexMarker021"/> if gaps exist. This contrasts sharply with the previous flow of data prep, training, and experimentation before you could get your hands dirty. The bar to start creating with AI has never been lower.</p>
<p class="normal">In the following sections, we will explore the difference between the development lifecycle of predictive AI and generative AI use cases. In each section, we have provided a high-level visual representation of a simplified development lifecycle and an explanation of the thought process behind each approach.</p>
<h2 class="heading-2" id="_idParaDest-19">Predictive AI use case development – simplified lifecycle</h2>
<figure class="mediaobject"><img alt="" height="242" src="img/B22175_01_01.png" width="825"/></figure>
<p class="packt_figref">Figure 1.1: Predictive AI use case development simplified lifecycle</p>
<p class="normal">Let’s dive into the process<a id="_idIndexMarker022"/> of developing a predictive AI model first. Everything starts with a good use case, and ROI (return on investment) is top of mind when evaluating AI use cases. Think about pain points in your business or industry that could be solved by predicting an outcome. It is very important to always keep an eye on feasibility – whether you can procure the data you need, etc.</p>
<p class="normal">Once you’ve landed on a compelling value-driven use case, next up is picking algorithms. You’ve got endless options here – decision trees, neural nets, regressions, random forests, and on and on. It is very important not to be swayed by the bias for the latest and greatest and to focus on the core requirements of your data and use case to narrow the options down. You can always switch it up or add additional experiments as you iterate through your testing.</p>
<p class="normal">With a plan in place, now it is time to get your hands dirty with the data. Identifying sources, cleaning things up, and carrying out feature engineering is an art and, more often than not, the key to improving your model’s results. There is no shortcut for rigor here, unfortunately! Garbage in, garbage out, as they say. But once you’ve wrangled datasets you can rely on, then comes the fun part.</p>
<p class="normal">It’s time to work with your model. Define your evaluation process upfront, split data wisely, and start training various configurations. Don’t forget to monitor and tune based on validation performance. Then, once you’ve got your golden model, implement robust serving infrastructure so it scales without a hitch.</p>
<p class="normal">But wait, not so fast! Testing<a id="_idIndexMarker023"/> doesn’t end when models are in production. Collect performance data continuously, monitor for concept drifts, and retrain when needed. A solid predictive model requires ongoing feedback mechanisms, as shown via the arrow connecting <strong class="keyWord">Model Enhancement </strong>to <strong class="keyWord">Testing</strong> in <em class="italic">Figure 1.1</em>. There is no such thing as set and forget in this space.</p>
<h2 class="heading-2" id="_idParaDest-20">Generative AI use case development – simplified lifecycle</h2>
<figure class="mediaobject"><img alt="" height="256" src="img/B22175_01_02.png" width="825"/></figure>
<p class="packt_figref">Figure 1.2: Generative AI use case development simplified lifecycle</p>
<p class="normal">The <a id="_idIndexMarker024"/>process of generative AI use case development is similar but not the same as in predictive AI; there are some common steps, but the order of tasks is different.</p>
<p class="normal">The first step is the ideation of potential use cases. This selection needs to be balanced with business needs as satisfying them is our main objective.</p>
<p class="normal">With a clear problem definition in place, extensive analysis of published model benchmarks often informs the selection of a robust foundational model best suited for the task. In this step, it is worth asking ourselves the question is this use case better suited for a predictive model?</p>
<p class="normal">As foundational models provide capabilities out of the box, initial testing comes as a step early in the process. A structured testing methodology helps reveal innate strengths, weaknesses, and quirks of a specific model. Both quantitative metrics and qualitative human evaluations fuel iterative improvement throughout the full development lifecycle.</p>
<p class="normal">The next step is to move to the art of<a id="_idIndexMarker025"/> prompt engineering. Prompting is the mechanism used to interact with LLMs. Techniques like chain-of-thought prompting, skeleton prompts, and retrieval augmentation build guardrails enabling more consistent, logical outputs.</p>
<p class="normal">If gaps remain after prompt<a id="_idIndexMarker026"/> optimization, model enhancement via fine-tuning and distillation offers a precision tool to adapt models closer to the target task. </p>
<p class="normal">In rare cases, pretraining a fully custom model from scratch is warranted when no existing model can viably serve the use case. However, it is important to keep in mind that due to the massive data requirements posed by model retraining, this task won’t be suitable for most use cases and teams; retraining a foundational model requires an extensive amount of data and processing power that makes the process unpractical from a financial and technical perspective.</p>
<p class="normal">Above all, the interplay between evaluation and model improvement underscores the deeply empirical nature of advancing generative AI responsibly. Testing often reveals that better solutions come from creativity in problem framing rather than pure technological advances alone.</p>
<figure class="mediaobject"><img alt="" height="517" src="img/B22175_01_03.png" width="825"/></figure>
<p class="packt_figref">Figure 1.3: Predictive and generative AI development lifecycle side-by-side comparison</p>
<p class="normal">As we can see from the<a id="_idIndexMarker027"/> preceding figure, the development lifecycle<a id="_idIndexMarker028"/> is an iterative process that enables us to realize value from a given use case and technology type. Across the rest of this chapter and this book, we are going to focus on generative AI general concepts, some that are going to be familiar if you are experienced in predictive AI and others that are specific to this new field in AI.</p>
<h1 class="heading-1" id="_idParaDest-21">General generative AI concepts</h1>
<p class="normal">When integrating generative AI<a id="_idIndexMarker029"/> into practical applications, it is important to have an understanding of concepts such as model architecture and training. In this section, we cover an overview of prominent concepts, including transformers, diffusion models, pre-training, and prompt engineering, that enable systems to generate impressively accurate text, images, audio, and more.</p>
<p class="normal">Understanding these core concepts<a id="_idIndexMarker030"/> will equip you to make informed decisions when selecting foundations for your use cases. However, putting models into production requires further architectural considerations. We will be highlighting these decision points in the rest of the chapters in the book and in practical examples.</p>
<h2 class="heading-2" id="_idParaDest-22">Generative AI model architectures</h2>
<p class="normal">Generative AI models are based on specialized neural network <a id="_idIndexMarker031"/>architectures optimized for generative tasks. The two more widely known models are transformers and diffusion models.</p>
<p class="normal"><strong class="keyWord">Transformer models</strong> are not a new concept. They were first<a id="_idIndexMarker032"/> introduced by Google in a 2017 paper called <em class="italic">Attention Is All You Need</em> (<a href="https://arxiv.org/pdf/1706.03762.pdf">https://arxiv.org/pdf/1706.03762.pdf</a>). The paper explains the <strong class="keyWord">Transformer neural network architecture</strong>, which is entirely based on attention<a id="_idIndexMarker033"/> mechanisms using the encoder and decoder concepts. This architecture enables models to identify relationships across an input text. By having these relationships, the model predicts the next token, leveraging its previous prediction as an input, creating this recursive loop to generate new content.</p>
<p class="normal"><strong class="keyWord">Diffusion models</strong> have drawn considerable interest<a id="_idIndexMarker034"/> as generative models due to their foundation in the physical processes of non-equilibrium thermodynamics. In physics, diffusion refers to the motion<a id="_idIndexMarker035"/> of particles from areas of high concentration to low concentration over time. Diffusion models try to mimic this concept in their training process. These models are trained through two phases: the <strong class="keyWord">forward diffusion</strong> process adds “noise” to the<a id="_idIndexMarker036"/> original training data, followed by a <strong class="keyWord">reverse conditioning</strong> process, which then learns<a id="_idIndexMarker037"/> how to remove noise in the reverse diffusion process. By learning this process, these models can produce samples by starting from pure noise and letting the reverse diffusion model clear away unnecessary “noise” and preserving the desired “generated” content.</p>
<p class="normal">Other types of deep<a id="_idIndexMarker038"/> learning architectures, such as <strong class="keyWord">Generative Adversarial Networks</strong> (<strong class="keyWord">GANs</strong>), allow you to generate synthetic data based on existing data. GANs<a id="_idIndexMarker039"/> are useful because they leverage two models: one to generate a synthetic output and another one that tries to predict if this output is real or fake. </p>
<p class="normal">Through this iterative process, we can generate data that is indistinguishable from the real data but different enough to be used to enhance our training datasets. Another example<a id="_idIndexMarker040"/> of data generation architectures is <strong class="keyWord">Variational Autoencoders</strong> (<strong class="keyWord">VAEs</strong>), which use an encoder-decoder approach to generate new data <a id="_idIndexMarker041"/>samples resembling their training datasets.</p>
<h2 class="heading-2" id="_idParaDest-23">Techniques available to optimize foundational models</h2>
<p class="normal">There are several techniques used to develop and optimize foundational models that have driven significant gains in AI capabilities, some of which are more complex than others from a technical and monetary perspective:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Pre-training</strong> refers to<a id="_idIndexMarker042"/> fully training a model<a id="_idIndexMarker043"/> on a large dataset. It allows models to learn very broad representations from billions of data points, which help the model adapt to other closely related tasks. Popular methods include contrastive self-supervised pre-training on unlabeled data and pre-training on vast supervised data like the internet. </li>
<li class="bulletList"><strong class="keyWord">Fine-tuning</strong> adapts a<a id="_idIndexMarker044"/> pre-trained model’s already<a id="_idIndexMarker045"/> learned feature representations to perform a specific task. This only tunes some higher-level model layers rather than training from scratch. On the other hand, adapter tuning equips models with small, lightweight adapters that can rapidly tune to new tasks without interfering with existing capabilities. These pluggable adapters give a parameter-efficient way of accumulating knowledge across multiple tasks by learning task-specific behaviors while reusing the bulk of model weights. They help mitigate forgetting previous tasks and simplify personalization. For example, models may first be pre-trained on billions of text webpages to acquire general linguistic knowledge, before being fine-tuned on more domain-specific datasets for question answering, classification, etc.</li>
<li class="bulletList"><strong class="keyWord">Distillation</strong> uses a “teacher” model to train<a id="_idIndexMarker046"/> a smaller “student” model to reproduce the performance of the larger pre-trained model at a lower cost and latency. Quantizing and compressing large models into efficient forms <a id="_idIndexMarker047"/>for deployments also helps optimize performance<a id="_idIndexMarker048"/> and cost.</li>
</ul>
<p class="normal">The combination of comprehensive pre-training followed by specialized fine-tuning, adapter tuning, and portable distillation has enabled unprecedented versatility of deep learning across domains. Each approach smartly reuses and transfers available knowledge, enabling the customization and scaling of generative AI.</p>
<h2 class="heading-2" id="_idParaDest-24">Techniques to augment your foundational model responses</h2>
<p class="normal">In addition to architecture and training advances, progress in generative AI has been fueled by innovations in how these models are augmented by external data at inference time.</p>
<p class="normal"><strong class="keyWord">Prompt engineering</strong> tunes the<a id="_idIndexMarker049"/> text prompts provided<a id="_idIndexMarker050"/> to models to steer their generation quality, capabilities, and properties. Well-designed prompts guide the model to produce the desired output format, reduce ambiguity, and provide helpful contextual constraints. This allows simpler model architectures to solve complex problems by encoding human knowledge into the prompts. </p>
<p class="normal"><strong class="keyWord">Retrieval augmented generation</strong>, also <a id="_idIndexMarker051"/>known as <strong class="keyWord">RAG</strong>, enhances text generation<a id="_idIndexMarker052"/> through efficient retrieval of relevant knowledge from external stores. Models receive contextual pieces of information as “context” to be considered as additional input before generating its output. <strong class="keyWord">Grounding</strong> LLMs (large language models) refers to providing model-specific <a id="_idIndexMarker053"/>factual knowledge rather than just model parameters, enabling more accurate, knowledgeable, and specific language generation.</p>
<p class="normal">Together, these approaches augment basic predictive language models to become far more versatile, robust, and scalable. They reduce brittleness via tight integration of human knowledge and grounded learning rather than just statistical patterns. RAG handles the breadth and real-time retrieval of information, prompts provide depth and rules to the desired outputs, and grounding binds them to reality. We would highly encourage readers to get familiar with this topic, as it is an industry best practice to perform RAG and to<a id="_idIndexMarker054"/> ground your model to prevent it from hallucinating. A good start is the following paper: <em class="italic">Retrieval-Augmented Generation for Large Language Models: A Survey</em> (<a href="https://arxiv.org/pdf/2312.10997">https://arxiv.org/pdf/2312.10997</a>).</p>
<h1 class="heading-1" id="_idParaDest-25">Constant evolution across the generative AI space</h1>
<p class="normal">The generative AI space<a id="_idIndexMarker055"/> is characterized by relentless innovation and rapid advancement across model architectures, applications, and ethical considerations. As soon as one method or architecture shows promising results, hundreds of competing and complementary approaches emerge to push capabilities even further. Transformers gave way to <a id="_idIndexMarker056"/>BERT, which was outpaced by GPT-3, soon <a id="_idIndexMarker057"/>rivaled by image synthesizers like <a id="_idIndexMarker058"/>DALL-E, and now GPT-4<a id="_idIndexMarker059"/> and Gemini <a id="_idIndexMarker060"/>are competing for the top spot. All of which happened in the past few years.</p>
<p class="normal">Meanwhile, we are seeing new modalities like audio, video, and 3D scene generation gaining vast popularity and usability. On the business front, new services are launched monthly, targeting media and entertainment, finance, healthcare, art, code, music, and more. However, considerations around ethics, access control, and legalities are key in order to maintain public trust.</p>
<p class="normal">One breakthrough enables several more, and each unlocks added potential. This self-fueling cycle arises from the very nature of AI – its ability to recursively assist innovation. The only certainty is that the field will look very different within months, not years. Maintaining awareness, responsiveness, and responsibility is critical amid this constant evolution.</p>
<h1 class="heading-1" id="_idParaDest-26">Introducing generative AI integration patterns</h1>
<p class="normal">Let’s now assume you already<a id="_idIndexMarker061"/> have a promising use case in mind. As I’m sure you would agree, clearly defining the use case is critical before proceeding further. You’ve already identified which foundational model provides acceptable performance for your needs. So now you’re starting to consider how GenAI fits into the application development process.</p>
<p class="normal">At a high level, there are<a id="_idIndexMarker062"/> two main workflows for integrating applications with GenAI. One is <strong class="keyWord">real time</strong>, where you’ll typically interact<a id="_idIndexMarker063"/> with an end user or AI agent, providing responses as prompts come in. The second is <strong class="keyWord">batch processing</strong>, where requirements are<a id="_idIndexMarker064"/> bundled up and <a id="_idIndexMarker065"/>processed in groups (batches).</p>
<p class="normal">A prime example of a real-time workflow would be a chatbot. Here, prompts from the user are processed and then sent to the model and the responses are returned immediately, as you need to consume the outputs without delay. On the other hand, consider a data enrichment use case for batch processing. You could collect multiple data points over time for later consumption after being enriched by the model in batches.</p>
<p class="normal">In this book, we will explore these integration patterns through practical examples. This will help you to obtain hands-on experience with GenAI-driven applications and allow you to integrate these patterns in your own use cases.</p>
<p class="normal">By “integration pattern,” we refer to a standardized<a id="_idIndexMarker066"/> architectural approach for incorporating a technology into your application or system. In this context, integration patterns provide proven methods for connecting generative AI models to real-world software.</p>
<p class="normal">There are a few key reasons<a id="_idIndexMarker067"/> why we need integration patterns when working with generative AI:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Time savings</strong>: Following established patterns allows developers to avoid reinventing the wheel for common integration challenges. This accelerates time to value.</li>
<li class="bulletList"><strong class="keyWord">Improving quality</strong>: Leveraging best practices encoded in integration patterns leads to more robust, production-grade integrations. Things like scalability, security, and reliability are top of mind.</li>
<li class="bulletList"><strong class="keyWord">Reducing risk</strong>: Well-defined integration patterns enable developers to mitigate risks around performance, costs, and other pitfalls that can emerge when integrating new technologies.</li>
</ul>
<p class="normal">Overall, integration patterns deliver templates and guardrails, so developers don’t have to start integration efforts from scratch. By relying on proven blueprints, readers can integrate generative AI more efficiently<a id="_idIndexMarker068"/> while avoiding common mistakes. This speeds up development cycles significantly and sets integrations up for long-term success. </p>
<h1 class="heading-1" id="_idParaDest-27">Summary</h1>
<p class="normal">In this chapter, we covered an overview of key concepts, techniques, and integration patterns related to generative AI. You now have a high-level background on prominent generative AI model architectures like transformers and diffusion models, as well as various methods for developing and enhancing these models, covering pre-training, fine-tuning, adapter tuning, distillation, prompt engineering, retrieval augmented generation, and grounding.</p>
<p class="normal">We discussed how rapid innovation in generative AI leads to constant evolution, with new models and capabilities emerging at a fast pace. It emphasizes the need to keep pace with progress while ensuring ethical, responsible development.</p>
<p class="normal">Finally, we introduced common integration patterns for connecting generative AI to real-world applications, considering real-time use cases like chatbots as well as batch processing for data enrichment. Real examples were provided to demonstrate workflows for integrating generative models into production systems.</p>
<p class="normal">Innovation in AI has a very fast pace, demanding constant awareness, swift experimentation, and a responsible approach to harnessing the latest advances. This is particularly evident in the field of generative AI, where we’re witnessing a paradigm shift in AI-powered applications that allows for faster experimentation and development.</p>
<p class="normal">A wide array of techniques has emerged to enhance models’ capabilities and efficiency. These include pre-training, adapter tuning, distillation, and prompt engineering, each offering unique advantages in different scenarios. When it comes to integrating these AI models into practical applications, key patterns have emerged for both real-time workflows, such as chatbots, and batch processing tasks like data enrichment.</p>
<p class="normal">The art of crafting well-designed prompts has become crucial in constraining and steering model outputs effectively. Additionally, techniques like retrieval augmentation and grounding have proven invaluable in improving the accuracy of AI-generated content. The potential in blending predictive and generative approaches is a very interesting space. This combination leverages the strengths of both methodologies, allowing for custom modeling where sufficient data exists while utilizing generative foundations to enable rapid prototyping and innovation.</p>
<p class="normal">These core concepts empower informed decision-making when architecting generative AI systems. The integration patterns offer blueprints for connecting models to practical applications across diverse domains. </p>
<p class="normal">Harnessing the power of LLMs begins with identifying the right use cases where they can drive value for your business. In the next chapter, we will present a framework and examples for categorizing LLM use cases based on projected business value.</p>
<p class="normal">In the next chapter, we will explore identifying use cases that can be solved with Generative AI.</p>
<h1 class="heading-1">Join our community on Discord</h1>
<p class="normal">Join our community’s Discord space for discussions with the authors and other readers:</p>
<p class="normal"><a href="https://packt.link/genpat">https://packt.link/genpat</a></p>
<p class="normal"><img alt="" height="177" src="img/QR_Code134841911667913109.png" width="177"/></p>
</div>
</div></body></html>