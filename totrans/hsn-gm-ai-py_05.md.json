["```py\nimport numpy as np\nfrom tqdm import tqdm\nimport random\n\ngamma = 0.5 \nrewardSize = -1\ngridSize = 4\nalpha = 0.5 \nterminations = [[0,0], [gridSize-1, gridSize-1]]\nactions = [[-1, 0], [1, 0], [0, 1], [0, -1]]\nepisodes = 10000\n\nV = np.zeros((gridSize, gridSize))\nreturns = {(i, j):list() for i in range(gridSize) for j in range(gridSize)}\ndeltas = {(i, j):list() for i in range(gridSize) for j in range(gridSize)}\nstates = [[i, j] for i in range(gridSize) for j in range(gridSize)]\n\ndef generateInitialState():\n    initState = random.choice(states[1:-1])\n    return initState\n\ndef generateNextAction():\n    return random.choice(actions)\n\ndef takeAction(state, action):\n  if list(state) in terminations:\n    return 0, None\n  finalState = np.array(state)+np.array(action)\n\n  if -1 in list(finalState) or gridSize in list(finalState):\n    finalState = state\n  return rewardSize, list(finalState)\n\nfor it in tqdm(range(episodes)):    \n  state = generateInitialState()\n  while True:\n    action = generateNextAction()\n    reward, finalState = takeAction(state, action) \n    if finalState is None:\n      break\n\n  before =  V[state[0], state[1]]\n  V[state[0], state[1]] += alpha*(reward + gamma*V[finalState[0], finalState[1]] - V[state[0], state[1]])\n  deltas[state[0], state[1]].append(float(np.abs(before-V[state[0], state[1]])))        \n  state = finalState\n\nprint(V)\n```", "```py\ndef takeAction(state, action):\n if list(state) in terminations:\n   return 0, None\n finalState = np.array(state)+np.array(action)\n if -1 in list(finalState) or gridSize in list(finalState):\n   finalState = state\n return rewardSize, list(finalState)\n```", "```py\nV[state[0], state[1]] += alpha*(reward + gamma*V[finalState[0], finalState[1]] - V[state[0], state[1]])\n```", "```py\npip install matplotlib\n```", "```py\ngamma = 0.1 \nrewardSize = -1\ngridSize = 4\nalpha = 0.1 \n```", "```py\nplt.figure(figsize=(20,10))\nall_series = [list(x)[:50] for x in deltas.values()]\nfor series in all_series:\n    plt.plot(series)\n\nplt.show()\n\nprint(V)\n```", "```py\nfrom os import system, name\nfrom time import sleep\nimport numpy as np\nimport gym\nimport random\nfrom tqdm import tqdm\n```", "```py\nenv = gym.make(\"FrozenLake-v0\")\nenv.render()\naction_size = env.action_space.n\nprint(\"Action size \", action_size)\nstate_size = env.observation_space.n\nprint(\"State size \", state_size)\n```", "```py\nqtable = np.ones((state_size, action_size))/action_size \nprint(qtable)\n```", "```py\ntotal_episodes = 50000 \ntotal_test_episodes = 100\nplay_game_test_episode = 1000\nmax_steps = 99 \nlearning_rate = 0.7 \ngamma = 0.618 \n```", "```py\nepsilon = 1.0 \nmax_epsilon = 1.0\nmin_epsilon = 0.01\ndecay_rate = 0.01 \n```", "```py\nenv = gym.make(\"Taxi-v2\")\n```", "```py\ndef play_game(render_game):\n  state = env.reset()\n  step = 0\n  done = False\n  total_rewards = 0 \n  for step in range(max_steps):\n    if render_game:\n      env.render()\n      print(\"**...*****************\")\n      print(\"EPISODE \", episode)\n      sleep(.5)\n      clear()\n   action = np.argmax(qtable[state,:])\n   new_state, reward, done, info = env.step(action)\n   total_rewards += reward\n   if done:\n     rewards.append(total_rewards)\n     if render_game:\n       print (\"Score\", total_rewards)\n     break\n   state = new_state\n return done, state, step, total_rewards\n```", "```py\naction = np.argmax(qtable[state,:])\n```", "```py\nfor episode in tqdm(range(total_episodes)):\n  state = env.reset()\n  step = 0\n  done = False\n  if episode % play_game_test_episode == 0:\n    play_game(True)\n  for step in range(max_steps):\n    exp_exp_tradeoff = random.uniform(0,1)\n    if exp_exp_tradeoff > epsilon:\n      action = np.argmax(qtable[state,:])\n    else:\n      action = env.action_space.sample()\n   new_state, reward, done, info = env.step(action)\n   qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n   state = new_state\n   if done == True:\n     break\n  epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n```", "```py\nexp_exp_tradeoff = random.uniform(0,1)\nif exp_exp_tradeoff > epsilon:\n  action = np.argmax(qtable[state,:])\nelse:\n  action = env.action_space.sample()\n```", "```py\nqtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n```", "```py\nepsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n```", "```py\nenv.reset()\nprint(qtable)\n\nfor episode in range(total_test_episodes):\n  done, state, step, total_rewards = play_game(False)\n\nenv.close()\nprint (\"Score over time: \" + str(sum(rewards)/total_test_episodes))\n```"]