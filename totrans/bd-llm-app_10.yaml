- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building Multimodal Applications with LLMs
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we are going beyond LLMs, to introduce the concept of multimodality
    while building agents. We will see the logic behind the combination of foundation
    models in different AI domains – language, images, and audio – into one single
    agent that can adapt to a variety of tasks. By the end of this chapter, you will
    be able to build your own multimodal agent, providing it with the tools and LLMs
    needed to perform various AI tasks.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'Throughout this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to multimodality and **large multimodal models** (**LMMs**)
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples of emerging LMMs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to build a multimodal agent with single-modal LLMs using LangChain
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To complete the tasks in this chapter, you will need the following:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: A Hugging Face account and user access token.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An OpenAI account and user access token.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python 3.7.1 or later version.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Python packages. Make sure to have the following Python packages installed:
    `langchain`, `python-dotenv`, `huggingface_hub`, `streamlit`, `pytube`, `openai`,
    and `youtube_search`. Those can be easily installed via `pip install` in your
    terminal.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can find all the code and examples in the book’s GitHub repository at [https://github.com/PacktPublishing/Building-LLM-Powered-Applications](Chapter_10.xhtml).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Why multimodality?
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the context of Generative AI, multimodality refers to a model’s capability
    of processing data in various formats. For example, a multimodal model can communicate
    with humans via text, speech, images, or even videos, making the interaction extremely
    smooth and “human-like.”
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Chapter 1*, we defined **large foundation models** (**LFMs**) as a type
    of pre-trained generative AI model that offers immense versatility by being adaptable
    for various specific tasks. LLMs, on the other hand, are a subset of foundation
    models that are able to process one type of data: natural language. Even though
    LLMs have proven to be not only excellent text understanders and generators but
    also reasoning engines to power applications and copilots, it soon became clear
    that we could aim at even more powerful applications.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: The dream is to have intelligent systems that are capable of handling multiple
    data formats – text, images, audio, video, etc – always powered by the reasoning
    engine, which makes them able to plan and execute actions with an agentic approach.
    Such an AI system would be a further milestone toward the reaching of **artificial
    general intelligence** (**AGI**).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '**Definition**'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: AGI is a hypothetical type of **artificial intelligence** (**AI**) that can
    perform any intellectual task that a human can. AGI would have a general cognitive
    ability, similar to human intelligence, and be able to learn from experience,
    reason, plan, communicate, and solve problems across different domains. An AGI
    system would also be able to “perceive” the world as we do, meaning that it could
    process data in different formats, from text to images to sounds. Hence, AGI implies
    multimodality.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Creating AGI is a primary goal of some AI research and a common topic in science
    fiction. However, there is no consensus on how to achieve AGI, what criteria to
    use to measure it, or when it might be possible. Some researchers argue that AGI
    could be achieved in years or decades, while others maintain that it might take
    a century or longer, or that it might never be achieved.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: However, AGI is not seen as the ultimate milestone in AI development. In fact,
    in recent months another definition has emerged in the context of AI – that is,
    Strong AI or Super AI, referring to an AI system that is more capable than a human.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing this book (February 2024), LMMs such as GPT-4 Turbo with
    Vision are a reality. However, those are not the only ways to reach multimodality.
    In this chapter, we are going to examine how to merge multiple AI systems to reach
    a multimodal AI assistant. The idea is that if we combine single-modal models,
    one for each data format we want to process, and then use an LLM as the brain
    of our agent to let it interact in dynamic ways with those models (that will be
    its tools), we can still achieve this goal. The following diagram shows the structure
    of a multimodal application that integrates various single-modal tools to perform
    a task – in this case, describing a picture aloud. The application uses image
    analysis to examine the picture, text generation to create some text that describes
    what it observes in the picture, and text-to-speech to convey this text to the
    user through speech.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: The LLM acts as the “reasoning engine” of the application, invoking the proper
    tools needed to accomplish the user’s query.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '![A person talking to a speech bubble  Description automatically generated](img/B21714_10_01.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.1: Illustration of multimodal application with single-modal tools'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: In the upcoming section, we are going to explore various approaches to building
    multimodal applications, all based on the idea of combining existing single-modal
    tools or models.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Building a multimodal agent with LangChain
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we’ve covered the main aspects of multimodality and how to achieve it
    with modern LFMs. As we saw throughout Part 2 of this book, LangChain offers a
    variety of components that we leveraged massively, such as chains, agents, tools,
    and so on. As a result, we already have all the ingredients we need to start building
    our multimodal agent.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: 'However, in this chapter, we will adopt three approaches to tackle the problem:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '**The agentic, out-of-the-box approach**: Here we will leverage the Azure Cognitive
    Services toolkit, which offers native integrations toward a set of AI models that
    can be consumed via API, and that covers various domains such as image, audio,
    OCR, etc.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The agentic, custom approach**: Here, we are going to select single models
    and tools (including defining custom tools) and concatenate them into a single
    agent that can leverage all of them.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The hard-coded approach**: Here, we are going to build separate chains and
    combine them into a sequential chain.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the upcoming sections, we will cover all these approaches with concrete examples.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: 'Option 1: Using an out-of-the-box toolkit for Azure AI Services'
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Formerly known as Azure Cognitive Services, Azure AI Services are a set of cloud-based
    APIs and AI services developed by Microsoft that enable developers and data scientists
    to add cognitive capabilities to their apps. AI Services are meant to provide
    every developer with AI models to be integrated with programming languages such
    as Python, C#, or JavaScript.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: 'Azure AI Services cover various domains of AI, including speech, natural language,
    vision, and decision-making. All those services come with models that can be consumed
    via API, and you can decide to:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Leverage powerful pre-built models available as they are and ready to use.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customize those pre-built models with custom data so that they are tailored
    to your use case.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hence, considered all together, Azure AI Services can achieve the goal of multimodality,
    if properly orchestrated by an LLM as a reasoning engine, which is exactly the
    framework LangChain built.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Getting Started with AzureCognitiveServicesToolkit
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In fact, LangChain has a native integration with Azure AI Services called **AzureCognitiveServicesToolkit**,
    which can be passed as a parameter to an agent and leverage the multimodal capabilities
    of those models.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: The toolkit makes it easier to incorporate Azure AI services’ capabilities –
    such as image analysis, form recognition, speech-to-text, and text-to-speech –
    within your application. It can be used within an agent, which is then empowered
    to use the AI services to enhance its functionality and provide richer responses.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: 'Currently, the integration supports the following tools:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '**AzureCogsImageAnalysisTool**: Used to analyze and extract metadata from images.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AzureCogsSpeech2TextTool**: Used to convert speech to text.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AzureCogsText2SpeechTool**: Used to synthetize text to speech with neural
    voices.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AzureCogsFormRecognizerTool**: Used to perform **optical character recognition**
    (**OCR**).'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Definition**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: OCR is a technology that converts different types of documents, such as scanned
    paper documents, PDFs, or images captured by a digital camera, into editable and
    searchable data. OCR can save time, cost, and resources by automating data entry
    and storage processes. It can also enable access to and editing of the original
    content of historical, legal, or other types of documents.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: For example, if you ask an agent what you can make with some ingredients, and
    provide an image of eggs and flour, the agent can use the Azure AI Services Image
    Analysis tool to extract the caption, objects, and tags from the image, and then
    use the provided LLM to suggest some recipes based on the ingredients. To implement
    this, let’s first set up our toolkit.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the toolkit
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To get started with the toolkit, you can follow these steps:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: You first need to create a multi-service instance of Azure AI Services in Azure
    following the instructions at [https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?tabs=windows&pivots=azportal](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?tabs=windows&pivots=azportal).
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A multi-service resource allows you to access multiple AI services with a single
    key and endpoint to be passed to LangChain as environmental variables. You can
    find your keys and endpoint under the **Keys and Endpoint** tab in your resource
    panel:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B21714_10_02.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.2: Screenshot of a multi-service instance of Azure AI Services'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the resource is set, we can start building our LegalAgent. To do so, the
    first thing we need to do is set the AI services environmental variables in order
    to configure the toolkit. To do so, I’ve saved the following variables in my `.env`
    file:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then, you can load them as always alongside the other environmental variables:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, we can configure our toolkit and also see which tools we have, alongside
    their description:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The following is the corresponding output:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, it’s time to initialize our agent. For this purpose, we will use a `STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION`
    agent that, as we saw in previous chapters, also allows for multi-tools input,
    since we will also add further tools in the *Leveraging multiple tools* section:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now we have all the ingredients to start testing our agent.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging a single tool
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To start easy, let’s simply ask the agent to describe the following picture,
    which will only require the `image_analysis` tool to be accomplished:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '![A person holding a slingshot  Description automatically generated](img/B21714_10_03.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.3: Sample picture of a slingshot (source: [https://www.stylo24.it/wp-content/uploads/2020/03/fionda.jpg](https://www.stylo24.it/wp-content/uploads/2020/03/fionda.jpg))'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s pass the URL of this image as input to our model, as per the description
    of the `azure_cognitive_services_image_analysis` tool:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We then get the following output:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '{'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '"action": "azure_cognitive_services_image_analysis",'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '"action_input": "https://www.stylo24.it/wp-content/uploads/2020/03/fionda.jpg"'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '{'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '"action": "Final Answer",'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '"action_input": "The image is of a person holding a slingshot."'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As you can see, the agent was able to retrieve the proper tool to address the
    user’s question. In this case, the question was very simple, so I want to challenge
    the same tool with a trickier question.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal is to replicate the GPT-4 capabilities in its common-sense reasoning
    while working with images, as the following illustration from GPT-4’s earliest
    experiments shows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '![A close up of a cell phone  Description automatically generated](img/B21714_10_04.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.4: Example of visual capabilities and common sense reasoning of GPT-4
    (source: [https://openai.com/research/gpt-4](https://openai.com/research/gpt-4))'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: 'So let’s ask our model something more challenging. Let’s ask it to reason about
    the consequences of letting the slingshot go:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We then obtain the following output:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '{'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '"action": "azure_cognitive_services_image_analysis",'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '"action_input": "https://www.stylo24.it/wp-content/uploads/2020/03/fionda.jpg"'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '{'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '"action": "Final Answer",'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '"action_input": "If the person lets the slingshot go, it will fly through the
    air."'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'It might seem an easy question, but the agent’s answers imply an extremely
    refined common sense reasoning: thanks to the metadata extracted from the image
    leveraging the `image_analysis` tool, the LLM was able to set up reasoning in
    terms of consequences given an action (the person letting the slingshot go).'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the agent only leveraged one tool. But what happens if we ask
    it something that requires at least two tools?
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging multiple tools
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s say, for example, that we want the model to read a story aloud to us based
    on a picture.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '![Meet Côtelette, the Kyrgyz stray dog that followed cyclists all season ...](img/B21714_10_05.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.5: Example of an input image for a story-telling agent (source: [https://i.redd.it/diawvlriobq11.jpg](https://i.redd.it/diawvlriobq11.jpg))'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s ask the agent to do so:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We obtain the following output:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '{'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '"action": "azure_cognitive_services_image_analysis",'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '"action_input": {'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '"query": "https://i.redd.it/diawvlriobq11.jpg"'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '{'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '"action": "azure_cognitive_services_text2speech",'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '"action_input": {'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '"query": "Once upon a time, in a snowy winter wonderland, there lived a brave
    little dog named Snowy. Snowy loved to explore the vast snowy hills and mountains
    surrounding his home. One day, Snowy stumbled upon a group of lost animals who
    were stranded in the snow. They were cold, hungry, and scared. Without hesitation,
    Snowy stepped up to help. He led the group to a nearby shelter and found food
    and warmth for everyone. Snowy''s bravery and kindness earned him a reputation
    as a hero in the winter wonderland. From then on, he became known as Snowy the
    Savior of the Snow."'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '{'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '"action": "Final Answer",'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '"action_input": "I hope you enjoyed the story of Snowy the Savior of the Snow,
    the brave little dog who helped rescue a group of lost animals in the snowy winter
    wonderland. Thank you for the opportunity to share this tale with you."'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'As you can see, the agent was able to invoke two tools to accomplish the request:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: It first started with the `image_analysis` tool to generate the image caption
    used to produce the story.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, it invoked the `text2speech` tool to read it aloud to the user.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The agent saved the audio file in a temporary file, and you can listen to it
    directly by clicking on the URL. Alternatively, you can save the output as a Python
    variable and execute it as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Finally, we can also modify the default prompt that comes with the agent type,
    to make it more customized with respect to our specific use case. To do so, we
    first need to inspect the template and then decide which part we can modify. To
    inspect the template, you can run the following command:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Here is our output:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '{{'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '"action": $TOOL_NAME,'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '"action_input": $INPUT'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '}}'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: $JSON_BLOB
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '...'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]$JSON_BLOB[PRE23]'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s modify the prefix of the prompt and pass it as `kwargs` to our agent:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: As you can see, now the agent acts more similar to a storyteller with a specific
    style. You can customize your prompt as you wish, always keeping in mind that
    each pre-built agent has its own prompt template, hence it is always recommended
    to first inspect it before customizing it.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have explored the out-of-the-box capabilities of the toolkit, let’s
    build an end-to-end application.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Building an end-to-end application for invoice analysis
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Analyzing invoices might require a lot of manual work if not assisted by digital
    processes. To address this, we will build an AI assistant that is able to analyze
    invoices for us and tell us any relevant information aloud. We will call this
    application **CoPenny**.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: With CoPenny, individuals and enterprises could reduce the time of invoice analysis,
    as well as build toward document process automation and, more generally, digital
    process automation.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '**Definition**'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Document process automation is a strategy that uses technology to streamline
    and automate various document-related tasks and processes within an organization.
    It involves the use of software tools, including document capture, data extraction,
    workflow automation, and integration with other systems. For example, document
    process automation can help you extract, validate, and analyze data from invoices,
    receipts, forms, and other types of documents. Document process automation can
    save you time and money, improve accuracy and efficiency, and provide valuable
    insights and reports from your document data.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '**Digital process automation** (**DPA**) is a broader term that refers to automating
    any business process with digital technology. DPA can help you connect your apps,
    data, and services and boost your team’s productivity with cloud flows. DPA can
    also help you create more sophisticated and intuitive customer experiences, collaborate
    across your organization, and innovate with AI and ML.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 'To start building our application, we can follow these steps:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 'Using `AzureCognitiveServicesToolkit`, we will leverage the `azure_cognitive_services_form_recognizer`
    and `azure_cognitive_services_text2speech` tools, so we can limit the agent’s
    “powers” only to those two:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The following is the corresponding output:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Let’s now initialize the agent with the default prompt and see the results.
    For this purpose, we will use a sample invoice as a template with which to query
    the agent:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![A close-up of a receipt  Description automatically generated](img/B21714_10_06.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.6: Sample template of a generic invoice (source: https://www.whiteelysee.fr/design/wp-content/uploads/2022/01/custom-t-shirt-order-form-template-free.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by asking the model to tell us all the men’s **stock-keeping units**
    (**SKUs**) on the invoice:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We then get the following output (showing a truncated output; you can find
    the whole output in the book’s GitHub repository):'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '{'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '"action": "azure_cognitive_services_form_recognizer",'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '"action_input": {'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '"query": "https://www.whiteelysee.fr/design/wp-content/uploads/2022/01/custom-t-shirt-order-form-template-free.jpg"'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We can also ask for multiple information (women’s SKUs, shipping address, and
    delivery dates) as follows (note that the delivery date is not specified, as we
    want our agent not to hallucinate):'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This gives us the following output:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Finally, let’s also leverage the text2speech tool to produce the audio of the
    response:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: As per the previous example, you can listen to the audio by clicking on the
    URL in the chain, or using Python’s `Display` function if you save it as a variable.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we want our agent to be better tailored toward our goal. To do so, let’s
    customize the prompt giving specific instructions. In particular, we want the
    agent to produce the audio output without the user explicitly asking for it:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Let’s run the agent:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'This yields the following output:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '{'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '"action": "azure_cognitive_services_form_recognizer",'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '"action_input": {'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '"query": "https://www.whiteelysee.fr/design/wp-content/uploads/2022/01/custom-t-shirt-order-form-template-free.jpg"'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: As you can see, now the agent saved the output into an audio file, even when
    the user didn’t ask explicitly for it.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '`AzureCognitiveServicesToolkit` is a powerful integration that allows for native
    consumption of Azure AI Services. However, there are some pitfalls of this approach,
    including the limited number of AI services. In the next section, we are going
    to explore yet another option to achieve multimodality, with a more flexible approach
    while still keeping an agentic strategy.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 'Option 2: Combining single tools into one agent'
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this leg of our journey toward multimodality, we will leverage different
    tools as plug-ins to our `STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION` agent.
    Our goal is to build a copilot agent that will help us generate reviews about
    YouTube videos, as well as post those reviews on our social media with a nice
    description and related picture. In all of that, we want to make little or no
    effort, so we need our agent to perform the following steps:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Search and transcribe a YouTube video based on our input.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Based on the transcription, generate a review with a length and style defined
    by the user query.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate an image related to the video and the review.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will call our copilot **GPTuber**. In the following subsections, we will
    examine each tool and then put them all together.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: YouTube tools and Whisper
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first step of our agent will be to search and transcribe the YouTube video
    based on our input. To do so, there are two tools we need to leverage:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '**YouTubeSearchTool**: An out-of-the-box tool offered by LangChain and adapted
    from [https://github.com/venuv/langchain_yt_tools](https://github.com/venuv/langchain_yt_tools).
    You can import and try the tool by running the following code, specifying the
    topic of the video and the number of videos you want the tool to return:'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Here is the output:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The tool returns the URL of the video. To watch it, you can add it to [https://youtube.com
    domain](https://youtube.com).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '**CustomYTTranscribeTool**: This is a custom tool that I’ve adapted from [https://github.com/venuv/langchain_yt_tools](https://github.com/venuv/langchain_yt_tools).
    It consists of transcribing the audio file retrieved from the previous tool using
    a speech-to-text model. In our case, we will be leveraging OpenAI’s **Whisper**.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Whisper is a transformer-based model introduced by OpenAI in September 2022\.
    It works as follows:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: It splits the input audio into 30-second chunks, converting them into spectrograms
    (visual representations of sound frequencies).
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It then passes them to an encoder.
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The encoder then produces a sequence of hidden states that capture the information
    in the audio.
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A decoder then predicts the corresponding text caption, using special tokens
    to indicate the task (such as language identification, speech transcription, or
    speech translation) and the output language.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The decoder can also generate timestamps for each word or phrase in the caption.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unlike most OpenAI models, Whisper is open-source.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 'Since this model takes as input only files and not URLs, within the custom
    tool, there is a function defined as `yt_get` (you can find it in the GitHub repository)
    that, starting from the video URL, downloads it into a `.mp4` file. Once downloaded,
    you can try Whisper with the following lines of code:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Here is the corresponding output:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: By embedding Whisper in this custom tool, we can transcribe the output of the
    first tool into a transcript that will serve as input to the next tool. You can
    see the code and logic behind this embedding and the whole tool in this book’s
    GitHub repository at [https://github.com/PacktPublishing/Building-LLM-Powered-Applications](Chapter_10.xhtml),
    which is a modified version from [https://github.com/venuv/langchain_yt_tools](https://github.com/venuv/langchain_yt_tools).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we already have two tools, we can start building our tools list and initializing
    our agent, using the following code:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The following is the corresponding output:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Great! We were able to generate the transcription of this video. The next step
    will be to generate a review alongside a picture. While the review can be written
    directly from the LLM and passed as a parameter to the model (so we don’t need
    another tool), the image generation will need an additional tool. For this purpose,
    we are going to use OpenAI’s DALL·E.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: DALL·E and text generation
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Introduced by OpenAI in January 2021, DALL·E is a transformer-based model that
    can create images from text descriptions. It is based on GPT-3, which is also
    used for natural language processing tasks. It is trained on a large dataset of
    text-image pairs from the web and uses a vocabulary of tokens for both text and
    image concepts. DALL·E can produce multiple images for the same text, showing
    different interpretations and variations.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: 'LangChain offers native integration with DALL·E, which you can use as a tool
    by running the following code (always setting the environmental variable of your
    `OPENAI_API_KEY` from the `.env` file):'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Here is the corresponding output:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The following is the image that was generated, as requested:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '![A house with bats flying in the sky  Description automatically generated](img/B21714_10_07.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.7: Image generated by DALL·E upon the user’s input'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: 'Great! Now let’s also see whether our agent is capable of generating a review
    of a video based on the transcription:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'We get the following output:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Note how the agent was initially looking for a tool to make a review, to then
    realize that there is no tool yet that can do it manually thanks to its parametric
    knowledge. This is a great example of how LLMs are reasoning engines and endowed
    with common sense reasoning. As always, you can find the entire chain of thoughts
    in the book’s repository.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: The next step will be to put it all together and see whether the agent is capable
    of orchestrating all the tools, with some assistance in terms of prompt engineering.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have all the ingredients, we need to put them together into one
    single agent. To do so, we can follow these steps:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to add the DALL·E tool to the list of tools:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'This gives us the following output:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The next step will be to test the agent with the default prompt, and then try
    to refine the instructions with some prompt engineering. Let’s start with a pre-configured
    agent (you can find all the steps in the GitHub repository):'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'This gives us the following output:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The following is the accompanying visual output:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '![A person with dreadlocks and green eyes  Description automatically generated](img/B21714_10_08.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.8: Image generated by DALL·E based on the trailer review'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Well, even without any prompt engineering, the agent was able to orchestrate
    the tools and return the desired results!
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s try to make it more tailored toward our purpose. Similar to the
    CoPenny application, we don’t want the user to specify every time to generate
    a review alongside an image. So let’s modify the default prompt as follows:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The output obtained is as follows:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'This is accompanied by the following visual output:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '![A mountain with a lake and trees  Description automatically generated with
    medium confidence](img/B21714_10_09.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.9: Image generated by DALL·E based on a trailer review'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: Wow! Not only was the agent able to use all the tools with the proper scope
    but it also adapted the style to the type of channel we want to share our review
    on – in this case, Instagram.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: 'Option 3: Hard-coded approach with a sequential chain'
  id: totrans-269
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The third and last option offers yet another way of implementing a multimodal
    application, which performs the following tasks:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Generates a story based on a topic given by the user.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generates a social media post to promote the story.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generates an image to go along with the social media post.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will call this application **StoryScribe**.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement this, we will build separate LangChain chains for those single
    tasks, and then combine them into a `SequentialChain`. As we saw in *Chapter 1*,
    this is a type of chain that allows you to execute multiple chains in a sequence.
    You can specify the order of the chains and how they pass their outputs to the
    next chain. So, we first need to create individual chains, then combine them and
    run as a unique chain. Let’s follow these steps:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll start by initializing the story generator chain:'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'This gives us the following output:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Note that I’ve set the `output_key= "story"` parameter so that it can be easily
    linked as output to the next chain, which will be the social post generator:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The following output is then obtained:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Here, I used the output of `story_chain` as input to `social_chain`. When we
    combine all the chains together, this step will be automatically performed by
    the sequential chain.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let’s initialize an image generator chain:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Note that the output of the chain will be the prompt to pass to the DALL·E model.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to generate the image, we need to use the `DallEAPIWrapper()` module
    available in LangChain:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'This generates the following output:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: '![A child giving a flower to a child  Description automatically generated](img/B21714_10_10.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.10: Picture generated by DALL·E given a social media post'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: 'The final step will be to put it all together into a sequential chain:'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Here is our output:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Since we passed the `output_variables = ['post, 'image']` parameter to the chain,
    those will be the two outputs of the chain. With `SequentialChain`, we have the
    flexibility to decide as many output variables as we want, so that we can construct
    our output as we please.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: Overall, there are several ways to reach multimodality within your application,
    and LangChain offers many components that make it easier. Now, let’s compare these
    approaches.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: Comparing the three options
  id: totrans-299
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We examined three options to achieve this result: options 1 and 2 follow the
    “agentic” approach, using, respectively, pre-built toolkit and single tools combined;
    option 3, on the other hand, follows a hard-coded approach, letting the developer
    decide the order of actions to be done.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: 'All three come with pros and cons, so let’s wrap up some final considerations:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '**Flexibility vs control**: The agentic approach lets the LLM decide which
    actions to take and in which order. This implies greater flexibility for the end
    user since there are no constraints in terms of queries that can be done. On the
    other hand, having no control over the agent’s chain of thoughts could lead to
    mistakes that would need several tests of prompt engineering. Plus, as LLMs are
    non-deterministic, it is also hard to recreate mistakes to retrieve the wrong
    thought process. Under this point of view, the hard-coded approach is safer, since
    the developer has full control over the order of execution of the actions.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evaluations**: The agentic approach leverages the tools to generate the final
    answer so that we don’t have to bother to plan these actions. However, if the
    final output doesn’t satisfy us, it might be cumbersome to understand what is
    the main source of the error: it might be a wrong plan, rather than a tool that
    is not doing its job correctly, or maybe a wrong prompt overall. On the other
    hand, with the hard-coded approach, each chain has its own model that can be tested
    separately, so that it is easier to identify the step of the process where the
    main error has occurred.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Maintenance**: With the agentic approach, there is one component to maintain:
    the agent itself. We have in fact one prompt, one agent, and one LLM, while the
    toolkit or list of tools is pre-built and we don’t need to maintain them. On the
    other hand, with the hard-coded approach, for each chain, we need a separate prompt,
    model, and testing activities.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To conclude, there is no golden rule to decide which approach to follow: it’s
    up to the developer to decide depending on the relative weight of the above parameters.
    As a general rule of thumb, the first step should be to define the problem to
    solve and then evaluate the complexity of each approach with respect to that problem.
    If, for example, it is a task that can be entirely addressed with the Cognitive
    Services toolkit without even doing prompt engineering, that could be the easiest
    way to proceed; on the other hand, if it requires a lot of control over the single
    components as well as on the sequence of execution, a hard-coded approach is preferable.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we are going to build a sample front-end using Streamlit,
    built on top of StoryScribe.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: Developing the front-end with Streamlit
  id: totrans-307
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have seen the logic behind an LLM-powered StoryScribe, it is time
    to give our application a GUI. To do so, we will once again leverage Streamlit.
    As always, you can find the whole Python code in the GitHub book repository at
    [https://github.com/PacktPublishing/Building-LLM-Powered-Applications](Chapter_10.xhtml).
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: As per the previous sections, you need to create a `.py` file to run in your
    terminal via `streamlit run file.py`. In our case, the file will be named `storyscribe.py`.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the main steps to set up the front-end:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: 'Configuring the application webpage:'
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Initialize the dynamic variables to be used within the placeholders of the
    prompts:'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Initialize all the chains and the overall chain (I will omit here all the prompt
    templates; you can find them in the GitHub repository of the book):'
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Run the overall chain and print the results:'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'In this case, I’ve set the `output_variables = [''story'',''post'', ''image'']`
    parameter so that we will have also the story itself as output. The final result
    looks like the following:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B21714_10_11.png)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.11: Front-end of StoryScribe showing the story output'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: 'The following picture is the resulting Instagram post:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a painting  Description automatically generated](img/B21714_10_12.png)'
  id: totrans-323
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.12: Front-end of StoryScribe showing the social media post along
    with the generated image'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: With just a few lines of code, we were able to set up a simple front-end for
    StoryScribe with multimodal capabilities.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-326
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we introduced the concept of multimodality and how to achieve
    it even without multimodal models. We explored three different ways of achieving
    the objective of a multimodal application: an agentic approach with a pre-built
    toolkit, an agentic approach with the combination of single tools, and a hard-coded
    approach with chained models.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: We delved into the concrete implementation of three applications with the above
    methods, examining the pros and cons of each approach. We saw, for example, how
    an agentic approach gives higher flexibility to the end user at the price of less
    control of the backend plan of action.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 我们深入研究了使用上述方法的具体实现，并分析了每种方法的优缺点。例如，我们看到了代理方法如何在牺牲后端行动计划控制力的代价下，为最终用户提供更高的灵活性。
- en: Finally, we implemented a front-end with Streamlit to build a consumable application
    with the hard-coded approach.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用 Streamlit 实现了前端，以硬编码的方式构建了一个可消费的应用程序。
- en: With this chapter, we conclude Part 2 of the book, where we examined hands-on
    scenarios and built LLMs-powered applications. In the next chapter, we will focus
    on how to customize your LLMs even more with the process of fine-tuning, leveraging
    open-source models, and using custom data for this purpose.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本章，我们总结了本书的第二部分，其中我们探讨了实际场景并构建了由 LLMs 驱动的应用程序。在下一章中，我们将重点介绍如何通过微调过程、利用开源模型和使用自定义数据来进一步定制您的
    LLMs。
- en: References
  id: totrans-331
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Source code for YouTube tools: [https://github.com/venuv/langchain_yt_tools](https://github.com/venuv/langchain_yt_tools)'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: YouTube 工具的源代码：[https://github.com/venuv/langchain_yt_tools](https://github.com/venuv/langchain_yt_tools)
- en: 'LangChain YouTube tool: [https://python.langchain.com/docs/integrations/tools/youtube](https://python.langchain.com/docs/integrations/tools/youtube)'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'LangChain YouTube 工具: [https://python.langchain.com/docs/integrations/tools/youtube](https://python.langchain.com/docs/integrations/tools/youtube)'
- en: 'LangChain AzureCognitiveServicesToolkit: [https://python.langchain.com/docs/integrations/toolkits/azure_cognitive_services](https://python.langchain.com/docs/integrations/toolkits/azure_cognitive_services)'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'LangChain AzureCognitiveServices Toolkit: [https://python.langchain.com/docs/integrations/toolkits/azure_cognitive_services](https://python.langchain.com/docs/integrations/toolkits/azure_cognitive_services)'
- en: Join our community on Discord
  id: totrans-335
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/llm](https://packt.link/llm )'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/llm](https://packt.link/llm)'
- en: '![](img/QR_Code214329708533108046.png)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![二维码](img/QR_Code214329708533108046.png)'
