- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Building Multimodal Applications with LLMs
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 LLMs 构建多模态应用
- en: In this chapter, we are going beyond LLMs, to introduce the concept of multimodality
    while building agents. We will see the logic behind the combination of foundation
    models in different AI domains – language, images, and audio – into one single
    agent that can adapt to a variety of tasks. By the end of this chapter, you will
    be able to build your own multimodal agent, providing it with the tools and LLMs
    needed to perform various AI tasks.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将超越 LLMs，在构建代理的同时介绍多模态的概念。我们将看到将不同 AI 领域的基础模型（语言、图像和音频）组合成一个单一代理的逻辑，该代理可以适应各种任务。到本章结束时，你将能够构建自己的多模态代理，并为其提供执行各种
    AI 任务所需的工具和 LLMs。
- en: 'Throughout this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Introduction to multimodality and **large multimodal models** (**LMMs**)
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多模态和**大型多模态模型**（**LMMs**）的介绍
- en: Examples of emerging LMMs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新兴 LMMs 的示例
- en: How to build a multimodal agent with single-modal LLMs using LangChain
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用 LangChain 使用单模态 LLMs 构建多模态代理
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'To complete the tasks in this chapter, you will need the following:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成本章中的任务，你需要以下内容：
- en: A Hugging Face account and user access token.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 Hugging Face 账户和用户访问令牌。
- en: An OpenAI account and user access token.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 OpenAI 账户和用户访问令牌。
- en: Python 3.7.1 or later version.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 3.7.1 或更高版本。
- en: 'Python packages. Make sure to have the following Python packages installed:
    `langchain`, `python-dotenv`, `huggingface_hub`, `streamlit`, `pytube`, `openai`,
    and `youtube_search`. Those can be easily installed via `pip install` in your
    terminal.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 包。请确保已安装以下 Python 包：`langchain`、`python-dotenv`、`huggingface_hub`、`streamlit`、`pytube`、`openai`
    和 `youtube_search`。这些包可以通过在终端中运行 `pip install` 轻松安装。
- en: You can find all the code and examples in the book’s GitHub repository at [https://github.com/PacktPublishing/Building-LLM-Powered-Applications](Chapter_10.xhtml).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在本书的 GitHub 仓库中找到所有代码和示例：[https://github.com/PacktPublishing/Building-LLM-Powered-Applications](Chapter_10.xhtml)。
- en: Why multimodality?
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么需要多模态？
- en: In the context of Generative AI, multimodality refers to a model’s capability
    of processing data in various formats. For example, a multimodal model can communicate
    with humans via text, speech, images, or even videos, making the interaction extremely
    smooth and “human-like.”
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成式 AI 的背景下，多模态指的是模型处理各种数据格式的能力。例如，一个多模态模型可以通过文本、语音、图像甚至视频与人类进行交流，使交互极其流畅且“人性化”。
- en: 'In *Chapter 1*, we defined **large foundation models** (**LFMs**) as a type
    of pre-trained generative AI model that offers immense versatility by being adaptable
    for various specific tasks. LLMs, on the other hand, are a subset of foundation
    models that are able to process one type of data: natural language. Even though
    LLMs have proven to be not only excellent text understanders and generators but
    also reasoning engines to power applications and copilots, it soon became clear
    that we could aim at even more powerful applications.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *第 1 章* 中，我们将**大型基础模型**（**LFMs**）定义为一种预训练的生成式 AI 模型，通过适应各种特定任务而提供极大的灵活性。另一方面，LLMs
    是基础模型的一个子集，能够处理一种类型的数据：自然语言。尽管 LLMs 已经证明不仅是非常出色的文本理解和生成者，而且也是推理引擎，可以驱动应用程序和共飞行员，但很快就很清楚，我们可以追求更强大的应用。
- en: The dream is to have intelligent systems that are capable of handling multiple
    data formats – text, images, audio, video, etc – always powered by the reasoning
    engine, which makes them able to plan and execute actions with an agentic approach.
    Such an AI system would be a further milestone toward the reaching of **artificial
    general intelligence** (**AGI**).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有能够处理多种数据格式（文本、图像、音频、视频等）的智能系统的梦想——始终由推理引擎提供动力，这使得它们能够以代理的方式规划和执行行动。这样的 AI
    系统将是实现**人工通用智能**（**AGI**）的进一步里程碑。
- en: '**Definition**'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义**'
- en: AGI is a hypothetical type of **artificial intelligence** (**AI**) that can
    perform any intellectual task that a human can. AGI would have a general cognitive
    ability, similar to human intelligence, and be able to learn from experience,
    reason, plan, communicate, and solve problems across different domains. An AGI
    system would also be able to “perceive” the world as we do, meaning that it could
    process data in different formats, from text to images to sounds. Hence, AGI implies
    multimodality.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: AGI是一种假设的人工智能类型，**人工智能**（AI），能够执行人类可以完成的任何智力任务。AGI将具有一般认知能力，类似于人类智能，并且能够从经验中学习、推理、规划、沟通和解决不同领域的问题。AGI系统还将能够“感知”世界，这意味着它能够处理不同格式的数据，从文本到图像到声音。因此，AGI意味着多模态。
- en: Creating AGI is a primary goal of some AI research and a common topic in science
    fiction. However, there is no consensus on how to achieve AGI, what criteria to
    use to measure it, or when it might be possible. Some researchers argue that AGI
    could be achieved in years or decades, while others maintain that it might take
    a century or longer, or that it might never be achieved.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 创建AGI是某些人工智能研究的主要目标，也是科幻小说中的常见主题。然而，关于如何实现AGI、使用什么标准来衡量它或它何时可能实现，并没有达成共识。一些研究人员认为AGI可能在几年或几十年内实现，而其他人则认为可能需要一百年或更长时间，或者可能永远无法实现。
- en: However, AGI is not seen as the ultimate milestone in AI development. In fact,
    in recent months another definition has emerged in the context of AI – that is,
    Strong AI or Super AI, referring to an AI system that is more capable than a human.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，AGI并不被视为人工智能发展的最终里程碑。事实上，在最近几个月，在人工智能的背景下出现了另一种定义——那就是强人工智能或超级人工智能，指的是比人类更强大的AI系统。
- en: At the time of writing this book (February 2024), LMMs such as GPT-4 Turbo with
    Vision are a reality. However, those are not the only ways to reach multimodality.
    In this chapter, we are going to examine how to merge multiple AI systems to reach
    a multimodal AI assistant. The idea is that if we combine single-modal models,
    one for each data format we want to process, and then use an LLM as the brain
    of our agent to let it interact in dynamic ways with those models (that will be
    its tools), we can still achieve this goal. The following diagram shows the structure
    of a multimodal application that integrates various single-modal tools to perform
    a task – in this case, describing a picture aloud. The application uses image
    analysis to examine the picture, text generation to create some text that describes
    what it observes in the picture, and text-to-speech to convey this text to the
    user through speech.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本书时（2024年2月），具有视觉功能的LMMs，如GPT-4 Turbo，已经成为现实。然而，实现多模态的方法不止这些。在本章中，我们将探讨如何合并多个AI系统以实现多模态AI助手。想法是，如果我们结合单模态模型，每个模型对应我们想要处理的数据格式，然后使用LLM作为我们代理的大脑，让它以动态的方式与这些模型（将成为其工具）互动，我们仍然可以实现这个目标。以下图表显示了集成各种单模态工具以执行任务的多模态应用程序的结构——在这种情况下，是口头描述图片。该应用程序使用图像分析来检查图片，使用文本生成来创建描述图片中观察到的内容的文本，并使用文本到语音将此文本通过语音传达给用户。
- en: The LLM acts as the “reasoning engine” of the application, invoking the proper
    tools needed to accomplish the user’s query.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: LLM充当应用程序的“推理引擎”，调用完成用户查询所需的正确工具。
- en: '![A person talking to a speech bubble  Description automatically generated](img/B21714_10_01.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![一个人对着话泡说话  自动生成描述](img/B21714_10_01.png)'
- en: 'Figure 10.1: Illustration of multimodal application with single-modal tools'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1：使用单模态工具的多模态应用程序的示意图
- en: In the upcoming section, we are going to explore various approaches to building
    multimodal applications, all based on the idea of combining existing single-modal
    tools or models.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将探讨构建多模态应用程序的各种方法，所有这些方法都基于结合现有单模态工具或模型的想法。
- en: Building a multimodal agent with LangChain
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用LangChain构建多模态代理
- en: So far, we’ve covered the main aspects of multimodality and how to achieve it
    with modern LFMs. As we saw throughout Part 2 of this book, LangChain offers a
    variety of components that we leveraged massively, such as chains, agents, tools,
    and so on. As a result, we already have all the ingredients we need to start building
    our multimodal agent.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经涵盖了多模态的主要方面以及如何使用现代LFMs实现它。正如我们在本书的第二部分所看到的那样，LangChain提供了各种组件，我们大量利用了这些组件，例如链、代理、工具等等。因此，我们已经拥有了开始构建我们的多模态代理所需的所有成分。
- en: 'However, in this chapter, we will adopt three approaches to tackle the problem:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在本章中，我们将采用三种方法来解决这个问题：
- en: '**The agentic, out-of-the-box approach**: Here we will leverage the Azure Cognitive
    Services toolkit, which offers native integrations toward a set of AI models that
    can be consumed via API, and that covers various domains such as image, audio,
    OCR, etc.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代理式、现成方法**：在这里，我们将利用Azure认知服务工具包，它提供对一组可以通过API消费的AI模型的本地集成，并涵盖图像、音频、OCR等多个领域。'
- en: '**The agentic, custom approach**: Here, we are going to select single models
    and tools (including defining custom tools) and concatenate them into a single
    agent that can leverage all of them.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代理式、定制化方法**：在这里，我们将选择单个模型和工具（包括定义定制工具），并将它们连接成一个单一代理，使其能够利用所有这些工具。'
- en: '**The hard-coded approach**: Here, we are going to build separate chains and
    combine them into a sequential chain.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**硬编码方法**：在这里，我们将构建单独的链并将其组合成一个顺序链。'
- en: In the upcoming sections, we will cover all these approaches with concrete examples.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将通过具体的示例介绍所有这些方法。
- en: 'Option 1: Using an out-of-the-box toolkit for Azure AI Services'
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选项1：使用现成的Azure AI服务工具包
- en: Formerly known as Azure Cognitive Services, Azure AI Services are a set of cloud-based
    APIs and AI services developed by Microsoft that enable developers and data scientists
    to add cognitive capabilities to their apps. AI Services are meant to provide
    every developer with AI models to be integrated with programming languages such
    as Python, C#, or JavaScript.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 以前被称为Azure认知服务，Azure AI服务是由微软开发的一套基于云的API和AI服务，它使开发者和数据科学家能够将认知能力添加到他们的应用程序中。AI服务旨在为每位开发者提供AI模型，以便与Python、C#或JavaScript等编程语言集成。
- en: 'Azure AI Services cover various domains of AI, including speech, natural language,
    vision, and decision-making. All those services come with models that can be consumed
    via API, and you can decide to:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Azure AI服务涵盖了人工智能的各个领域，包括语音、自然语言、视觉和决策。所有这些服务都提供了可以通过API消费的模型，您可以选择：
- en: Leverage powerful pre-built models available as they are and ready to use.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用现成的强大模型，它们已经准备好使用。
- en: Customize those pre-built models with custom data so that they are tailored
    to your use case.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用定制数据定制这些预建模型，以便它们符合您的用例。
- en: Hence, considered all together, Azure AI Services can achieve the goal of multimodality,
    if properly orchestrated by an LLM as a reasoning engine, which is exactly the
    framework LangChain built.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，综合考虑，如果由一个LLM作为推理引擎正确编排，Azure AI服务可以实现多模态的目标，这正是LangChain构建的框架。
- en: Getting Started with AzureCognitiveServicesToolkit
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Azure认知服务工具包入门
- en: In fact, LangChain has a native integration with Azure AI Services called **AzureCognitiveServicesToolkit**,
    which can be passed as a parameter to an agent and leverage the multimodal capabilities
    of those models.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，LangChain与Azure AI服务有一个本地的集成，称为**Azure认知服务工具包**，它可以作为一个参数传递给代理，并利用这些模型的跨模态能力。
- en: The toolkit makes it easier to incorporate Azure AI services’ capabilities –
    such as image analysis, form recognition, speech-to-text, and text-to-speech –
    within your application. It can be used within an agent, which is then empowered
    to use the AI services to enhance its functionality and provide richer responses.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 工具包使将Azure AI服务的功能（如图像分析、表单识别、语音到文本和文本到语音）集成到您的应用程序中变得更容易。它可以在代理中使用，从而使代理能够使用AI服务来增强其功能并提供更丰富的响应。
- en: 'Currently, the integration supports the following tools:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，集成支持以下工具：
- en: '**AzureCogsImageAnalysisTool**: Used to analyze and extract metadata from images.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AzureCogsImageAnalysisTool**：用于分析和从图像中提取元数据。'
- en: '**AzureCogsSpeech2TextTool**: Used to convert speech to text.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AzureCogsSpeech2TextTool**：用于将语音转换为文本。'
- en: '**AzureCogsText2SpeechTool**: Used to synthetize text to speech with neural
    voices.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AzureCogsText2SpeechTool**：用于使用神经网络声音将文本合成为语音。'
- en: '**AzureCogsFormRecognizerTool**: Used to perform **optical character recognition**
    (**OCR**).'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AzureCogsFormRecognizerTool**：用于执行**光学字符识别**（OCR）。'
- en: '**Definition**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义**'
- en: OCR is a technology that converts different types of documents, such as scanned
    paper documents, PDFs, or images captured by a digital camera, into editable and
    searchable data. OCR can save time, cost, and resources by automating data entry
    and storage processes. It can also enable access to and editing of the original
    content of historical, legal, or other types of documents.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 光学字符识别（OCR）是一种技术，可以将不同类型的文档（如扫描的纸质文档、PDF或数字相机捕获的图像）转换为可编辑和可搜索的数据。OCR可以通过自动化数据录入和存储过程来节省时间、成本和资源。它还可以使人们能够访问和编辑历史、法律或其他类型的原始内容。
- en: For example, if you ask an agent what you can make with some ingredients, and
    provide an image of eggs and flour, the agent can use the Azure AI Services Image
    Analysis tool to extract the caption, objects, and tags from the image, and then
    use the provided LLM to suggest some recipes based on the ingredients. To implement
    this, let’s first set up our toolkit.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果您要求智能体用一些原料能做什么，并提供鸡蛋和面粉的图片，智能体可以使用Azure AI服务的图像分析工具从图片中提取标题、对象和标签，然后使用提供的LLM根据原料建议一些食谱。为了实现这一点，让我们首先设置我们的工具包。
- en: Setting up the toolkit
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置工具包
- en: 'To get started with the toolkit, you can follow these steps:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用工具包，您可以按照以下步骤操作：
- en: You first need to create a multi-service instance of Azure AI Services in Azure
    following the instructions at [https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?tabs=windows&pivots=azportal](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?tabs=windows&pivots=azportal).
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您首先需要在Azure中根据[https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?tabs=windows&pivots=azportal](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?tabs=windows&pivots=azportal)中的说明创建一个Azure
    AI服务的多服务实例。
- en: 'A multi-service resource allows you to access multiple AI services with a single
    key and endpoint to be passed to LangChain as environmental variables. You can
    find your keys and endpoint under the **Keys and Endpoint** tab in your resource
    panel:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 多服务资源允许您使用单个密钥和一个端点访问多个AI服务，并将其作为环境变量传递给LangChain。您可以在资源面板的“密钥和端点”选项卡下找到您的密钥和端点：
- en: '![A screenshot of a computer  Description automatically generated](img/B21714_10_02.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![计算机截图  自动生成的描述](img/B21714_10_02.png)'
- en: 'Figure 10.2: Screenshot of a multi-service instance of Azure AI Services'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2：Azure AI服务的多服务实例截图
- en: 'Once the resource is set, we can start building our LegalAgent. To do so, the
    first thing we need to do is set the AI services environmental variables in order
    to configure the toolkit. To do so, I’ve saved the following variables in my `.env`
    file:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦资源设置完成，我们就可以开始构建我们的LegalAgent。为此，我们首先需要做的是设置AI服务环境变量，以便配置工具包。为此，我已经在我的`.env`文件中保存了以下变量：
- en: '[PRE0]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then, you can load them as always alongside the other environmental variables:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，您可以像往常一样将它们与其他环境变量一起加载：
- en: '[PRE1]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, we can configure our toolkit and also see which tools we have, alongside
    their description:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以配置我们的工具包，并查看我们有哪些工具，以及它们的描述：
- en: '[PRE2]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The following is the corresponding output:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是对应的输出：
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, it’s time to initialize our agent. For this purpose, we will use a `STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION`
    agent that, as we saw in previous chapters, also allows for multi-tools input,
    since we will also add further tools in the *Leveraging multiple tools* section:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，是时候初始化我们的智能体了。为此，我们将使用一个`STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION`智能体，正如我们在前面的章节中看到的，它也允许多工具输入，因为我们将在“利用多个工具”部分添加更多工具：
- en: '[PRE4]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now we have all the ingredients to start testing our agent.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经拥有了开始测试我们的智能体的所有成分。
- en: Leveraging a single tool
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 利用单个工具
- en: 'To start easy, let’s simply ask the agent to describe the following picture,
    which will only require the `image_analysis` tool to be accomplished:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，让我们让智能体描述以下图片，这将只需要`image_analysis`工具来完成：
- en: '![A person holding a slingshot  Description automatically generated](img/B21714_10_03.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![手持弹弓的人  自动生成的描述](img/B21714_10_03.png)'
- en: 'Figure 10.3: Sample picture of a slingshot (source: [https://www.stylo24.it/wp-content/uploads/2020/03/fionda.jpg](https://www.stylo24.it/wp-content/uploads/2020/03/fionda.jpg))'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.3：弹弓的样本图片（来源：[https://www.stylo24.it/wp-content/uploads/2020/03/fionda.jpg](https://www.stylo24.it/wp-content/uploads/2020/03/fionda.jpg))
- en: 'Let’s pass the URL of this image as input to our model, as per the description
    of the `azure_cognitive_services_image_analysis` tool:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这张图片的URL作为输入传递给我们的模型，按照`azure_cognitive_services_image_analysis`工具的描述：
- en: '[PRE5]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We then get the following output:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随后得到了以下输出：
- en: '[PRE6]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '{'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: '"action": "azure_cognitive_services_image_analysis",'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '"action": "azure_cognitive_services_image_analysis",'
- en: '"action_input": "https://www.stylo24.it/wp-content/uploads/2020/03/fionda.jpg"'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '"action_input": "https://www.stylo24.it/wp-content/uploads/2020/03/fionda.jpg"'
- en: '}'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '[PRE7]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '{'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: '"action": "Final Answer",'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '"action": "Final Answer",'
- en: '"action_input": "The image is of a person holding a slingshot."'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '"action_input": "图片显示一个人手持弹弓。"'
- en: '}'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '[PRE8]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As you can see, the agent was able to retrieve the proper tool to address the
    user’s question. In this case, the question was very simple, so I want to challenge
    the same tool with a trickier question.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，代理能够检索到适当的工具来回答用户的问题。在这种情况下，问题非常简单，所以我想用更复杂的问题挑战同一个工具。
- en: 'The goal is to replicate the GPT-4 capabilities in its common-sense reasoning
    while working with images, as the following illustration from GPT-4’s earliest
    experiments shows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是在处理图像的同时复制 GPT-4 的常识推理能力，如下所示 GPT-4 早期实验的插图：
- en: '![A close up of a cell phone  Description automatically generated](img/B21714_10_04.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![手机特写  自动生成的描述](img/B21714_10_04.png)'
- en: 'Figure 10.4: Example of visual capabilities and common sense reasoning of GPT-4
    (source: [https://openai.com/research/gpt-4](https://openai.com/research/gpt-4))'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.4：GPT-4 的视觉能力和常识推理示例（来源：[https://openai.com/research/gpt-4](https://openai.com/research/gpt-4)）
- en: 'So let’s ask our model something more challenging. Let’s ask it to reason about
    the consequences of letting the slingshot go:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们向我们的模型提出一个更具挑战性的问题。让我们要求它推理放开弹弓的后果：
- en: '[PRE9]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We then obtain the following output:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们得到以下输出：
- en: '[PRE10]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '{'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: '"action": "azure_cognitive_services_image_analysis",'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '"action": "azure_cognitive_services_image_analysis",'
- en: '"action_input": "https://www.stylo24.it/wp-content/uploads/2020/03/fionda.jpg"'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '"action_input": "https://www.stylo24.it/wp-content/uploads/2020/03/fionda.jpg"'
- en: '}'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '[PRE11]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '{'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: '"action": "Final Answer",'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '"action": "Final Answer",'
- en: '"action_input": "If the person lets the slingshot go, it will fly through the
    air."'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '"action_input": "如果那个人放开弹弓，它将飞过空中。"'
- en: '}'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '[PRE12]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'It might seem an easy question, but the agent’s answers imply an extremely
    refined common sense reasoning: thanks to the metadata extracted from the image
    leveraging the `image_analysis` tool, the LLM was able to set up reasoning in
    terms of consequences given an action (the person letting the slingshot go).'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题可能看起来很简单，但代理的答案暗示了极其精细的常识推理：得益于从图像中提取的元数据，利用 `image_analysis` 工具，LLM 能够在给定一个动作（放开弹弓的人）的情况下建立关于后果的推理。
- en: In this example, the agent only leveraged one tool. But what happens if we ask
    it something that requires at least two tools?
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，代理只利用了一个工具。但如果我们要求它做至少需要两个工具的事情会发生什么呢？
- en: Leveraging multiple tools
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 利用多个工具
- en: Let’s say, for example, that we want the model to read a story aloud to us based
    on a picture.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 假设，例如，我们希望模型根据图片大声朗读故事。
- en: '![Meet Côtelette, the Kyrgyz stray dog that followed cyclists all season ...](img/B21714_10_05.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![遇见库特莱特，这只跟随自行车手整个赛季的吉尔吉斯流浪狗 ...](img/B21714_10_05.png)'
- en: 'Figure 10.5: Example of an input image for a story-telling agent (source: [https://i.redd.it/diawvlriobq11.jpg](https://i.redd.it/diawvlriobq11.jpg))'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.5：故事讲述代理的输入图像示例（来源：[https://i.redd.it/diawvlriobq11.jpg](https://i.redd.it/diawvlriobq11.jpg))
- en: 'Let’s ask the agent to do so:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们要求代理这样做：
- en: '[PRE13]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We obtain the following output:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了以下输出：
- en: '[PRE14]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '{'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: '"action": "azure_cognitive_services_image_analysis",'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '"action": "azure_cognitive_services_image_analysis",'
- en: '"action_input": {'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '"action_input": {'
- en: '"query": "https://i.redd.it/diawvlriobq11.jpg"'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '"query": "https://i.redd.it/diawvlriobq11.jpg"'
- en: '}'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '[PRE15]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '{'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: '"action": "azure_cognitive_services_text2speech",'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '"action": "azure_cognitive_services_text2speech",'
- en: '"action_input": {'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '"action_input": {'
- en: '"query": "Once upon a time, in a snowy winter wonderland, there lived a brave
    little dog named Snowy. Snowy loved to explore the vast snowy hills and mountains
    surrounding his home. One day, Snowy stumbled upon a group of lost animals who
    were stranded in the snow. They were cold, hungry, and scared. Without hesitation,
    Snowy stepped up to help. He led the group to a nearby shelter and found food
    and warmth for everyone. Snowy''s bravery and kindness earned him a reputation
    as a hero in the winter wonderland. From then on, he became known as Snowy the
    Savior of the Snow."'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '"query": "从前，在一个雪白的冬日仙境里，住着一只勇敢的小狗，名叫雪伊。雪伊喜欢探索他周围广阔的雪丘和山脉。有一天，雪伊偶然发现了一群迷失在雪中的动物。他们又冷又饿又害怕。没有犹豫，雪伊站出来帮助他们。他带领这群动物找到了附近的避难所，并为每个人找到了食物和温暖。雪伊的勇敢和善良为他赢得了冬日仙境中的英雄声誉。从那时起，他被称为雪伊，雪地救星。"'
- en: '}'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '[PRE16]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '{'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: '"action": "Final Answer",'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '"action": "Final Answer",'
- en: '"action_input": "I hope you enjoyed the story of Snowy the Savior of the Snow,
    the brave little dog who helped rescue a group of lost animals in the snowy winter
    wonderland. Thank you for the opportunity to share this tale with you."'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '"action_input": "我希望你喜欢雪地救星雪伊的故事，这只勇敢的小狗在雪白的冬日仙境中帮助救援了一群迷失的动物。感谢你给我机会与你分享这个故事。"'
- en: '}'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '[PRE17]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'As you can see, the agent was able to invoke two tools to accomplish the request:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，代理能够调用两个工具来完成请求：
- en: It first started with the `image_analysis` tool to generate the image caption
    used to produce the story.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它首先使用`image_analysis`工具生成用于制作故事的图像标题。
- en: Then, it invoked the `text2speech` tool to read it aloud to the user.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，它调用了`text2speech`工具，将其读给用户听。
- en: 'The agent saved the audio file in a temporary file, and you can listen to it
    directly by clicking on the URL. Alternatively, you can save the output as a Python
    variable and execute it as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 代理将音频文件保存在临时文件中，您可以通过点击URL直接收听。或者，您可以将输出保存为Python变量，并按以下方式执行：
- en: '[PRE18]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Finally, we can also modify the default prompt that comes with the agent type,
    to make it more customized with respect to our specific use case. To do so, we
    first need to inspect the template and then decide which part we can modify. To
    inspect the template, you can run the following command:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们还可以修改代理类型附带的自定义提示，使其更符合我们的特定用例。为此，我们首先需要检查模板，然后决定我们可以修改哪些部分。要检查模板，您可以运行以下命令：
- en: '[PRE19]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Here is our output:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的输出：
- en: '[PRE20]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '{{'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '{{'
- en: '"action": $TOOL_NAME,'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '"action": $TOOL_NAME,'
- en: '"action_input": $INPUT'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '"action_input": $INPUT'
- en: '}}'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '}}'
- en: '[PRE21]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: $JSON_BLOB
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: $JSON_BLOB
- en: '...'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '...'
- en: '[PRE22]$JSON_BLOB[PRE23]'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE22]$JSON_BLOB[PRE23]'
- en: 'Let’s modify the prefix of the prompt and pass it as `kwargs` to our agent:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们修改提示的前缀，并将其作为`kwargs`传递给我们的代理：
- en: '[PRE24]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: As you can see, now the agent acts more similar to a storyteller with a specific
    style. You can customize your prompt as you wish, always keeping in mind that
    each pre-built agent has its own prompt template, hence it is always recommended
    to first inspect it before customizing it.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，现在代理的行为更类似于一个具有特定风格的讲故事者。您可以按需自定义提示，但始终记住，每个预构建代理都有自己的提示模板，因此始终建议在自定义之前先检查它。
- en: Now that we have explored the out-of-the-box capabilities of the toolkit, let’s
    build an end-to-end application.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经探索了工具包的即用功能，让我们构建一个端到端的应用程序。
- en: Building an end-to-end application for invoice analysis
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建端到端发票分析应用程序
- en: Analyzing invoices might require a lot of manual work if not assisted by digital
    processes. To address this, we will build an AI assistant that is able to analyze
    invoices for us and tell us any relevant information aloud. We will call this
    application **CoPenny**.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有数字流程的帮助，分析发票可能需要大量的人工工作。为了解决这个问题，我们将构建一个能够为我们分析发票并大声告诉我们任何相关信息的人工智能助手。我们将把这个应用程序称为**CoPenny**。
- en: With CoPenny, individuals and enterprises could reduce the time of invoice analysis,
    as well as build toward document process automation and, more generally, digital
    process automation.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 使用CoPenny，个人和企业可以减少发票分析的时间，以及向文档流程自动化和更广泛的数字流程自动化迈进。
- en: '**Definition**'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义**'
- en: Document process automation is a strategy that uses technology to streamline
    and automate various document-related tasks and processes within an organization.
    It involves the use of software tools, including document capture, data extraction,
    workflow automation, and integration with other systems. For example, document
    process automation can help you extract, validate, and analyze data from invoices,
    receipts, forms, and other types of documents. Document process automation can
    save you time and money, improve accuracy and efficiency, and provide valuable
    insights and reports from your document data.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 文档流程自动化是一种使用技术来简化并自动化组织内部各种文档相关任务和流程的策略。它涉及使用包括文档捕获、数据提取、工作流自动化以及与其他系统集成的软件工具。例如，文档流程自动化可以帮助您从发票、收据、表格和其他类型的文档中提取、验证和分析数据。文档流程自动化可以节省您的时间和金钱，提高准确性和效率，并从您的文档数据中提供有价值的见解和报告。
- en: '**Digital process automation** (**DPA**) is a broader term that refers to automating
    any business process with digital technology. DPA can help you connect your apps,
    data, and services and boost your team’s productivity with cloud flows. DPA can
    also help you create more sophisticated and intuitive customer experiences, collaborate
    across your organization, and innovate with AI and ML.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '**数字流程自动化**（**DPA**）是一个更广泛的概念，指的是使用数字技术自动化任何业务流程。DPA可以帮助您连接您的应用程序、数据和云流服务，从而提高团队的生产力。DPA还可以帮助您创建更复杂和直观的客户体验，在整个组织中协作，并利用人工智能和机器学习进行创新。'
- en: 'To start building our application, we can follow these steps:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始构建我们的应用程序，我们可以遵循以下步骤：
- en: 'Using `AzureCognitiveServicesToolkit`, we will leverage the `azure_cognitive_services_form_recognizer`
    and `azure_cognitive_services_text2speech` tools, so we can limit the agent’s
    “powers” only to those two:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `AzureCognitiveServicesToolkit`，我们将利用 `azure_cognitive_services_form_recognizer`
    和 `azure_cognitive_services_text2speech` 工具，因此我们可以将代理的“能力”限制在这两个工具上：
- en: '[PRE25]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The following is the corresponding output:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是对应的输出：
- en: '[PRE26]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Let’s now initialize the agent with the default prompt and see the results.
    For this purpose, we will use a sample invoice as a template with which to query
    the agent:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们现在使用默认提示初始化代理并查看结果。为此，我们将使用一个样本发票作为模板来查询代理：
- en: '![A close-up of a receipt  Description automatically generated](img/B21714_10_06.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![收据的特写  自动生成的描述](img/B21714_10_06.png)'
- en: 'Figure 10.6: Sample template of a generic invoice (source: https://www.whiteelysee.fr/design/wp-content/uploads/2022/01/custom-t-shirt-order-form-template-free.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.6：通用发票的样本模板（来源：https://www.whiteelysee.fr/design/wp-content/uploads/2022/01/custom-t-shirt-order-form-template-free.jpg）
- en: 'Let’s start by asking the model to tell us all the men’s **stock-keeping units**
    (**SKUs**) on the invoice:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们先要求模型告诉我们发票上所有的男性**库存单位**（**SKU**）：
- en: '[PRE27]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We then get the following output (showing a truncated output; you can find
    the whole output in the book’s GitHub repository):'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们得到以下输出（显示截断的输出；您可以在本书的GitHub仓库中找到完整的输出）：
- en: '[PRE28]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '{'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: '"action": "azure_cognitive_services_form_recognizer",'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '"action": "azure_cognitive_services_form_recognizer",'
- en: '"action_input": {'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '"action_input": {'
- en: '"query": "https://www.whiteelysee.fr/design/wp-content/uploads/2022/01/custom-t-shirt-order-form-template-free.jpg"'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '"query": "https://www.whiteelysee.fr/design/wp-content/uploads/2022/01/custom-t-shirt-order-form-template-free.jpg"'
- en: '}'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '[PRE29]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We can also ask for multiple information (women’s SKUs, shipping address, and
    delivery dates) as follows (note that the delivery date is not specified, as we
    want our agent not to hallucinate):'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以要求提供多个信息（女性SKU、送货地址和交货日期）如下（请注意，交货日期未指定，因为我们希望代理不要产生幻觉）：
- en: '[PRE30]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This gives us the following output:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们以下输出：
- en: '[PRE31]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Finally, let’s also leverage the text2speech tool to produce the audio of the
    response:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们也利用文本转语音工具来生成响应的音频：
- en: '[PRE32]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: As per the previous example, you can listen to the audio by clicking on the
    URL in the chain, or using Python’s `Display` function if you save it as a variable.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的例子，您可以通过点击链中的URL或使用Python的 `Display` 函数来收听音频，如果您将其保存为变量。
- en: 'Now, we want our agent to be better tailored toward our goal. To do so, let’s
    customize the prompt giving specific instructions. In particular, we want the
    agent to produce the audio output without the user explicitly asking for it:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们希望我们的代理更好地符合我们的目标。为此，让我们定制提示，给出具体的指令。特别是，我们希望代理在用户没有明确要求的情况下生成音频输出：
- en: '[PRE33]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Let’s run the agent:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们运行代理：
- en: '[PRE34]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'This yields the following output:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下输出：
- en: '[PRE35]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '{'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: '"action": "azure_cognitive_services_form_recognizer",'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '"action": "azure_cognitive_services_form_recognizer",'
- en: '"action_input": {'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '"action_input": {'
- en: '"query": "https://www.whiteelysee.fr/design/wp-content/uploads/2022/01/custom-t-shirt-order-form-template-free.jpg"'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '"query": "https://www.whiteelysee.fr/design/wp-content/uploads/2022/01/custom-t-shirt-order-form-template-free.jpg"'
- en: '}'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '[PRE36]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: As you can see, now the agent saved the output into an audio file, even when
    the user didn’t ask explicitly for it.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，现在代理已将输出保存到音频文件中，即使用户没有明确要求。
- en: '`AzureCognitiveServicesToolkit` is a powerful integration that allows for native
    consumption of Azure AI Services. However, there are some pitfalls of this approach,
    including the limited number of AI services. In the next section, we are going
    to explore yet another option to achieve multimodality, with a more flexible approach
    while still keeping an agentic strategy.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '`AzureCognitiveServicesToolkit` 是一个强大的集成，允许原生消费 Azure 人工智能服务。然而，这种方法存在一些陷阱，包括人工智能服务的数量有限。在下一节中，我们将探讨另一种实现多模态的方法，采用更灵活的方法，同时仍然保持代理策略。'
- en: 'Option 2: Combining single tools into one agent'
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选项2：将单个工具组合成一个代理
- en: 'In this leg of our journey toward multimodality, we will leverage different
    tools as plug-ins to our `STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION` agent.
    Our goal is to build a copilot agent that will help us generate reviews about
    YouTube videos, as well as post those reviews on our social media with a nice
    description and related picture. In all of that, we want to make little or no
    effort, so we need our agent to perform the following steps:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们通往多模态的旅程的这一阶段，我们将利用不同的工具作为我们的`STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION`代理的插件。我们的目标是构建一个辅助代理，帮助我们生成关于YouTube视频的评论，并在我们的社交媒体上发布这些评论，附上优美的描述和相关图片。在所有这些过程中，我们希望尽可能少地付出努力，因此我们需要我们的代理执行以下步骤：
- en: Search and transcribe a YouTube video based on our input.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据我们的输入搜索并转录YouTube视频。
- en: Based on the transcription, generate a review with a length and style defined
    by the user query.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据转录内容，根据用户查询定义的长度和风格生成评论。
- en: Generate an image related to the video and the review.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据视频和评论生成相关图片。
- en: We will call our copilot **GPTuber**. In the following subsections, we will
    examine each tool and then put them all together.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把我们的辅助代理称为**GPTuber**。在接下来的子节中，我们将检查每个工具，然后将它们全部组合起来。
- en: YouTube tools and Whisper
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: YouTube工具和Whisper
- en: 'The first step of our agent will be to search and transcribe the YouTube video
    based on our input. To do so, there are two tools we need to leverage:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们代理的第一步将是根据我们的输入搜索并转录YouTube视频。为此，我们需要利用以下两个工具：
- en: '**YouTubeSearchTool**: An out-of-the-box tool offered by LangChain and adapted
    from [https://github.com/venuv/langchain_yt_tools](https://github.com/venuv/langchain_yt_tools).
    You can import and try the tool by running the following code, specifying the
    topic of the video and the number of videos you want the tool to return:'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**YouTubeSearchTool**：LangChain提供的一种现成工具，源自[https://github.com/venuv/langchain_yt_tools](https://github.com/venuv/langchain_yt_tools)。您可以通过运行以下代码导入并尝试此工具，指定视频主题和您希望工具返回的视频数量：'
- en: '[PRE37]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Here is the output:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出：
- en: '[PRE38]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The tool returns the URL of the video. To watch it, you can add it to [https://youtube.com
    domain](https://youtube.com).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 工具返回视频的URL。要观看它，您可以将其添加到[https://youtube.com 域名](https://youtube.com)。
- en: '**CustomYTTranscribeTool**: This is a custom tool that I’ve adapted from [https://github.com/venuv/langchain_yt_tools](https://github.com/venuv/langchain_yt_tools).
    It consists of transcribing the audio file retrieved from the previous tool using
    a speech-to-text model. In our case, we will be leveraging OpenAI’s **Whisper**.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CustomYTTranscribeTool**：这是一个自定义工具，我从中[https://github.com/venuv/langchain_yt_tools](https://github.com/venuv/langchain_yt_tools)进行了改编。它包括使用语音识别模型转录从上一个工具检索到的音频文件。在我们的案例中，我们将利用OpenAI的**Whisper**。'
- en: 'Whisper is a transformer-based model introduced by OpenAI in September 2022\.
    It works as follows:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper是OpenAI于2022年9月推出的一种基于transformer的模型。它的工作原理如下：
- en: It splits the input audio into 30-second chunks, converting them into spectrograms
    (visual representations of sound frequencies).
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它将输入音频分割成30秒的片段，将它们转换为声谱图（声音频率的视觉表示）。
- en: It then passes them to an encoder.
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，它将这些片段传递给一个编码器。
- en: The encoder then produces a sequence of hidden states that capture the information
    in the audio.
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编码器随后生成一系列隐藏状态，这些状态捕捉了音频中的信息。
- en: A decoder then predicts the corresponding text caption, using special tokens
    to indicate the task (such as language identification, speech transcription, or
    speech translation) and the output language.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后解码器预测相应的文本字幕，使用特殊标记来指示任务（如语言识别、语音转录或语音翻译）和输出语言。
- en: The decoder can also generate timestamps for each word or phrase in the caption.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解码器还可以为字幕中的每个单词或短语生成时间戳。
- en: Unlike most OpenAI models, Whisper is open-source.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 与大多数OpenAI模型不同，Whisper是开源的。
- en: 'Since this model takes as input only files and not URLs, within the custom
    tool, there is a function defined as `yt_get` (you can find it in the GitHub repository)
    that, starting from the video URL, downloads it into a `.mp4` file. Once downloaded,
    you can try Whisper with the following lines of code:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 由于此模型仅接受文件作为输入，而不是URL，在自定义工具中，定义了一个名为`yt_get`的函数（您可以在GitHub仓库中找到它），它从视频URL开始，将其下载为`.mp4`文件。下载完成后，您可以使用以下代码尝试Whisper：
- en: '[PRE39]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Here is the corresponding output:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是相应的输出：
- en: '[PRE40]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: By embedding Whisper in this custom tool, we can transcribe the output of the
    first tool into a transcript that will serve as input to the next tool. You can
    see the code and logic behind this embedding and the whole tool in this book’s
    GitHub repository at [https://github.com/PacktPublishing/Building-LLM-Powered-Applications](Chapter_10.xhtml),
    which is a modified version from [https://github.com/venuv/langchain_yt_tools](https://github.com/venuv/langchain_yt_tools).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将Whisper嵌入到这个自定义工具中，我们可以将第一个工具的输出转录成转录文本，作为下一个工具的输入。你可以在这个书籍的GitHub仓库中看到嵌入的代码和整个工具的逻辑[https://github.com/PacktPublishing/Building-LLM-Powered-Applications](Chapter_10.xhtml)，这是从[https://github.com/venuv/langchain_yt_tools](https://github.com/venuv/langchain_yt_tools)修改而来的版本。
- en: 'Since we already have two tools, we can start building our tools list and initializing
    our agent, using the following code:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经有两个工具，我们可以开始构建我们的工具列表并初始化我们的代理，使用以下代码：
- en: '[PRE41]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The following is the corresponding output:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是对应的输出：
- en: '[PRE42]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Great! We were able to generate the transcription of this video. The next step
    will be to generate a review alongside a picture. While the review can be written
    directly from the LLM and passed as a parameter to the model (so we don’t need
    another tool), the image generation will need an additional tool. For this purpose,
    we are going to use OpenAI’s DALL·E.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！我们成功生成了这个视频的转录。下一步将是生成一个与图片一起的评论。虽然评论可以直接从LLM中写出并作为参数传递给模型（因此我们不需要另一个工具），但图像生成将需要一个额外的工具。为此，我们将使用OpenAI的DALL·E。
- en: DALL·E and text generation
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DALL·E和文本生成
- en: Introduced by OpenAI in January 2021, DALL·E is a transformer-based model that
    can create images from text descriptions. It is based on GPT-3, which is also
    used for natural language processing tasks. It is trained on a large dataset of
    text-image pairs from the web and uses a vocabulary of tokens for both text and
    image concepts. DALL·E can produce multiple images for the same text, showing
    different interpretations and variations.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI于2021年1月推出的DALL·E是一个基于transformer的模型，可以从文本描述中创建图像。它基于GPT-3，也用于自然语言处理任务。它在一个大型文本-图像对数据集上进行训练，该数据集来自网络，并使用文本和图像概念的标记词汇表。DALL·E可以为相同的文本生成多个图像，展示不同的解释和变化。
- en: 'LangChain offers native integration with DALL·E, which you can use as a tool
    by running the following code (always setting the environmental variable of your
    `OPENAI_API_KEY` from the `.env` file):'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain提供了与DALL·E的原生集成，你可以通过运行以下代码将其用作工具（始终从`.env`文件设置你的`OPENAI_API_KEY`环境变量）：
- en: '[PRE43]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Here is the corresponding output:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是相应的输出：
- en: '[PRE44]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The following is the image that was generated, as requested:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是根据要求生成的图像：
- en: '![A house with bats flying in the sky  Description automatically generated](img/B21714_10_07.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![天空中飞着蝙蝠的房子  自动生成的描述](img/B21714_10_07.png)'
- en: 'Figure 10.7: Image generated by DALL·E upon the user’s input'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.7：根据用户输入由DALL·E生成的图像
- en: 'Great! Now let’s also see whether our agent is capable of generating a review
    of a video based on the transcription:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！现在让我们也看看我们的代理是否能够根据转录生成视频的评论：
- en: '[PRE45]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'We get the following output:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了以下输出：
- en: '[PRE46]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Note how the agent was initially looking for a tool to make a review, to then
    realize that there is no tool yet that can do it manually thanks to its parametric
    knowledge. This is a great example of how LLMs are reasoning engines and endowed
    with common sense reasoning. As always, you can find the entire chain of thoughts
    in the book’s repository.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到代理最初是在寻找一个工具来制作评论，然后意识到由于它的参数化知识，目前还没有工具能够手动完成这项工作。这是一个很好的例子，说明了LLM是如何作为推理引擎并拥有常识推理能力的。和往常一样，你可以在书籍的仓库中找到整个思维链。
- en: The next step will be to put it all together and see whether the agent is capable
    of orchestrating all the tools, with some assistance in terms of prompt engineering.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步将是将所有这些整合在一起，看看代理是否能够协调所有工具，并在提示工程方面提供一些帮助。
- en: Putting it all together
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 整合所有内容
- en: 'Now that we have all the ingredients, we need to put them together into one
    single agent. To do so, we can follow these steps:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经拥有了所有原料，我们需要将它们组合成一个单一的代理。为此，我们可以遵循以下步骤：
- en: 'First, we need to add the DALL·E tool to the list of tools:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要将DALL·E工具添加到工具列表中：
- en: '[PRE47]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'This gives us the following output:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们以下输出：
- en: '[PRE48]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The next step will be to test the agent with the default prompt, and then try
    to refine the instructions with some prompt engineering. Let’s start with a pre-configured
    agent (you can find all the steps in the GitHub repository):'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步将是使用默认提示测试代理，然后尝试通过一些提示工程来细化指令。让我们从一个预配置的代理开始（你可以在GitHub仓库中找到所有步骤）：
- en: '[PRE49]'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'This gives us the following output:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们以下输出：
- en: '[PRE50]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The following is the accompanying visual output:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是与之相伴的视觉输出：
- en: '![A person with dreadlocks and green eyes  Description automatically generated](img/B21714_10_08.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![一个留着 dreadlocks 和绿色眼睛的人  自动生成的描述](img/B21714_10_08.png)'
- en: 'Figure 10.8: Image generated by DALL·E based on the trailer review'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.8：基于预告片评论由DALL·E生成的图像
- en: Well, even without any prompt engineering, the agent was able to orchestrate
    the tools and return the desired results!
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，即使没有任何提示工程，代理也能够协调工具并返回所需的结果！
- en: 'Now, let’s try to make it more tailored toward our purpose. Similar to the
    CoPenny application, we don’t want the user to specify every time to generate
    a review alongside an image. So let’s modify the default prompt as follows:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们尝试使其更符合我们的目的。与CoPenny应用程序类似，我们不希望用户每次都指定在图像旁边生成评论。因此，让我们修改默认提示如下：
- en: '[PRE51]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The output obtained is as follows:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 得到的输出如下：
- en: '[PRE52]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'This is accompanied by the following visual output:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 这伴随着以下视觉输出：
- en: '![A mountain with a lake and trees  Description automatically generated with
    medium confidence](img/B21714_10_09.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![一个有湖和树木的山  中度置信度自动生成的描述](img/B21714_10_09.png)'
- en: 'Figure 10.9: Image generated by DALL·E based on a trailer review'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.9：基于预告片评论由DALL·E生成的图像
- en: Wow! Not only was the agent able to use all the tools with the proper scope
    but it also adapted the style to the type of channel we want to share our review
    on – in this case, Instagram.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！不仅代理能够使用所有工具并正确地调整范围，它还根据我们想要分享评论的渠道类型调整了风格——在这种情况下，是Instagram。
- en: 'Option 3: Hard-coded approach with a sequential chain'
  id: totrans-269
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选项3：使用顺序链的硬编码方法
- en: 'The third and last option offers yet another way of implementing a multimodal
    application, which performs the following tasks:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 第三种也是最后一种选项提供了另一种实现多模态应用的方法，它执行以下任务：
- en: Generates a story based on a topic given by the user.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据用户给出的主题生成故事。
- en: Generates a social media post to promote the story.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成社交媒体帖子以推广故事。
- en: Generates an image to go along with the social media post.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成与社交媒体帖子一起使用的图像。
- en: We will call this application **StoryScribe**.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把这个应用程序称为**StoryScribe**。
- en: 'To implement this, we will build separate LangChain chains for those single
    tasks, and then combine them into a `SequentialChain`. As we saw in *Chapter 1*,
    this is a type of chain that allows you to execute multiple chains in a sequence.
    You can specify the order of the chains and how they pass their outputs to the
    next chain. So, we first need to create individual chains, then combine them and
    run as a unique chain. Let’s follow these steps:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们将为这些单个任务构建单独的LangChain链，然后将它们组合成一个`SequentialChain`。正如我们在*第一章*中看到的，这是一种允许你按顺序执行多个链的链。你可以指定链的顺序以及它们如何将输出传递给下一个链。因此，我们首先需要创建单个链，然后将它们组合并作为一个独特的链运行。让我们按照以下步骤进行：
- en: 'We’ll start by initializing the story generator chain:'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先初始化故事生成器链：
- en: '[PRE53]'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'This gives us the following output:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们以下输出：
- en: '[PRE54]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Note that I’ve set the `output_key= "story"` parameter so that it can be easily
    linked as output to the next chain, which will be the social post generator:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意，我已经设置了`output_key= "story"`参数，以便它可以轻松地作为输出链接到下一个链，即社交帖子生成器：
- en: '[PRE55]'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The following output is then obtained:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 然后得到的输出如下：
- en: '[PRE56]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Here, I used the output of `story_chain` as input to `social_chain`. When we
    combine all the chains together, this step will be automatically performed by
    the sequential chain.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我将`story_chain`的输出用作`social_chain`的输入。当我们组合所有链时，这一步将由顺序链自动执行。
- en: 'Finally, let’s initialize an image generator chain:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们初始化一个图像生成器链：
- en: '[PRE57]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Note that the output of the chain will be the prompt to pass to the DALL·E model.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，链的输出将是传递给DALL·E模型的提示。
- en: 'In order to generate the image, we need to use the `DallEAPIWrapper()` module
    available in LangChain:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了生成图像，我们需要使用LangChain中可用的`DallEAPIWrapper()`模块：
- en: '[PRE58]'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'This generates the following output:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 这生成了以下输出：
- en: '![A child giving a flower to a child  Description automatically generated](img/B21714_10_10.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![一个孩子向另一个孩子献花  自动生成的描述](img/B21714_10_10.png)'
- en: 'Figure 10.10: Picture generated by DALL·E given a social media post'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.10：DALL·E根据社交媒体帖子生成的图片
- en: 'The final step will be to put it all together into a sequential chain:'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后一步是将所有这些组合成一个顺序链：
- en: '[PRE59]'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Here is our output:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们的输出：
- en: '[PRE60]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Since we passed the `output_variables = ['post, 'image']` parameter to the chain,
    those will be the two outputs of the chain. With `SequentialChain`, we have the
    flexibility to decide as many output variables as we want, so that we can construct
    our output as we please.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们向链传递了`output_variables = ['post', 'image']`参数，因此这些将成为链的两个输出。使用`SequentialChain`，我们可以灵活地决定我们想要的输出变量数量，这样我们就可以按我们的意愿构建输出。
- en: Overall, there are several ways to reach multimodality within your application,
    and LangChain offers many components that make it easier. Now, let’s compare these
    approaches.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，在您的应用程序中实现多模态有几种方法，LangChain提供了许多组件，使这变得更加容易。现在，让我们比较这些方法。
- en: Comparing the three options
  id: totrans-299
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较三种选项
- en: 'We examined three options to achieve this result: options 1 and 2 follow the
    “agentic” approach, using, respectively, pre-built toolkit and single tools combined;
    option 3, on the other hand, follows a hard-coded approach, letting the developer
    decide the order of actions to be done.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考察了三种实现这一结果的方法：选项1和2遵循“代理”方法，分别使用预构建的工具包和组合的单个工具；选项3则相反，遵循硬编码的方法，让开发者决定要执行的动作顺序。
- en: 'All three come with pros and cons, so let’s wrap up some final considerations:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 所有三种方法都有其优缺点，因此让我们总结一些最终的考虑因素：
- en: '**Flexibility vs control**: The agentic approach lets the LLM decide which
    actions to take and in which order. This implies greater flexibility for the end
    user since there are no constraints in terms of queries that can be done. On the
    other hand, having no control over the agent’s chain of thoughts could lead to
    mistakes that would need several tests of prompt engineering. Plus, as LLMs are
    non-deterministic, it is also hard to recreate mistakes to retrieve the wrong
    thought process. Under this point of view, the hard-coded approach is safer, since
    the developer has full control over the order of execution of the actions.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**灵活性 vs 控制**：代理方法让LLM决定采取哪些动作以及它们的顺序。这为最终用户提供了更大的灵活性，因为没有查询方面的限制。另一方面，没有控制代理思维链可能导致需要多次测试提示工程的错误。此外，由于LLM是非确定性的，重现错误以检索错误思维过程也很困难。从这个角度来看，硬编码的方法更安全，因为开发者可以完全控制动作执行的顺序。'
- en: '**Evaluations**: The agentic approach leverages the tools to generate the final
    answer so that we don’t have to bother to plan these actions. However, if the
    final output doesn’t satisfy us, it might be cumbersome to understand what is
    the main source of the error: it might be a wrong plan, rather than a tool that
    is not doing its job correctly, or maybe a wrong prompt overall. On the other
    hand, with the hard-coded approach, each chain has its own model that can be tested
    separately, so that it is easier to identify the step of the process where the
    main error has occurred.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评估**：代理方法利用工具生成最终答案，这样我们就不必麻烦地规划这些动作。然而，如果最终输出不令人满意，可能很麻烦去理解错误的主要来源：可能是错误的计划，而不是工具没有正确执行其工作，或者可能是整体错误的提示。另一方面，使用硬编码的方法，每个链都有自己的模型可以单独测试，因此更容易识别过程中主要错误发生的地方。'
- en: '**Maintenance**: With the agentic approach, there is one component to maintain:
    the agent itself. We have in fact one prompt, one agent, and one LLM, while the
    toolkit or list of tools is pre-built and we don’t need to maintain them. On the
    other hand, with the hard-coded approach, for each chain, we need a separate prompt,
    model, and testing activities.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**维护**：在代理方法中，有一个组件需要维护：代理本身。实际上，我们有一个提示，一个代理，一个LLM，而工具包或工具列表是预构建的，我们不需要维护它们。另一方面，在硬编码的方法中，对于每个链，我们需要一个单独的提示、模型和测试活动。'
- en: 'To conclude, there is no golden rule to decide which approach to follow: it’s
    up to the developer to decide depending on the relative weight of the above parameters.
    As a general rule of thumb, the first step should be to define the problem to
    solve and then evaluate the complexity of each approach with respect to that problem.
    If, for example, it is a task that can be entirely addressed with the Cognitive
    Services toolkit without even doing prompt engineering, that could be the easiest
    way to proceed; on the other hand, if it requires a lot of control over the single
    components as well as on the sequence of execution, a hard-coded approach is preferable.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，没有金科玉律来决定遵循哪种方法：这取决于开发者根据上述参数的相对权重来决定。一般来说，第一步应该是定义要解决的问题，然后根据该问题评估每种方法的复杂性。例如，如果这是一个可以用认知服务工具包完全解决的任务，甚至不需要进行提示工程，那么这可能是最简单的方法；另一方面，如果需要对单个组件以及执行顺序有大量控制，则硬编码方法更可取。
- en: In the next section, we are going to build a sample front-end using Streamlit,
    built on top of StoryScribe.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将使用 Streamlit 构建一个示例前端，Streamlit 是建立在 StoryScribe 之上的。
- en: Developing the front-end with Streamlit
  id: totrans-307
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Streamlit 开发前端
- en: Now that we have seen the logic behind an LLM-powered StoryScribe, it is time
    to give our application a GUI. To do so, we will once again leverage Streamlit.
    As always, you can find the whole Python code in the GitHub book repository at
    [https://github.com/PacktPublishing/Building-LLM-Powered-Applications](Chapter_10.xhtml).
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了 LLM 驱动的 StoryScribe 的逻辑，是时候给我们的应用程序添加一个图形用户界面了。为此，我们还将利用 Streamlit。一如既往，您可以在
    GitHub 书籍仓库中找到完整的 Python 代码，网址为 [https://github.com/PacktPublishing/Building-LLM-Powered-Applications](Chapter_10.xhtml)。
- en: As per the previous sections, you need to create a `.py` file to run in your
    terminal via `streamlit run file.py`. In our case, the file will be named `storyscribe.py`.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的章节，您需要创建一个 `.py` 文件，在您的终端中通过 `streamlit run file.py` 运行。在我们的情况下，文件将被命名为
    `storyscribe.py`。
- en: 'The following are the main steps to set up the front-end:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是为设置前端的主要步骤：
- en: 'Configuring the application webpage:'
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置应用程序网页：
- en: '[PRE61]'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Initialize the dynamic variables to be used within the placeholders of the
    prompts:'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化在提示占位符中使用的动态变量：
- en: '[PRE62]'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Initialize all the chains and the overall chain (I will omit here all the prompt
    templates; you can find them in the GitHub repository of the book):'
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化所有链和整体链（在此我将省略所有提示模板；您可以在书籍的 GitHub 仓库中找到它们）：
- en: '[PRE63]'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Run the overall chain and print the results:'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行整体链并打印结果：
- en: '[PRE64]'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'In this case, I’ve set the `output_variables = [''story'',''post'', ''image'']`
    parameter so that we will have also the story itself as output. The final result
    looks like the following:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我设置了 `output_variables = ['story','post', 'image']` 参数，这样我们也将获得故事本身作为输出。最终结果如下所示：
- en: '![A screenshot of a computer  Description automatically generated](img/B21714_10_11.png)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
  zh: '![一台计算机的截图 自动生成的描述](img/B21714_10_11.png)'
- en: 'Figure 10.11: Front-end of StoryScribe showing the story output'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.11：StoryScribe 的前端显示故事输出
- en: 'The following picture is the resulting Instagram post:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图片是生成的 Instagram 帖子：
- en: '![A screenshot of a painting  Description automatically generated](img/B21714_10_12.png)'
  id: totrans-323
  prefs: []
  type: TYPE_IMG
  zh: '![一幅绘画的截图 自动生成的描述](img/B21714_10_12.png)'
- en: 'Figure 10.12: Front-end of StoryScribe showing the social media post along
    with the generated image'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.12：StoryScribe 的前端显示社交媒体帖子以及生成的图像
- en: With just a few lines of code, we were able to set up a simple front-end for
    StoryScribe with multimodal capabilities.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 仅用几行代码，我们就能够为 StoryScribe 设置一个具有多模态功能的基础前端。
- en: Summary
  id: totrans-326
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we introduced the concept of multimodality and how to achieve
    it even without multimodal models. We explored three different ways of achieving
    the objective of a multimodal application: an agentic approach with a pre-built
    toolkit, an agentic approach with the combination of single tools, and a hard-coded
    approach with chained models.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了多模态的概念以及如何在没有任何多模态模型的情况下实现它。我们探讨了实现多模态应用程序目标的三种不同方法：带有预建工具包的代理方法、结合单个工具的代理方法以及使用链式模型的硬编码方法。
- en: We delved into the concrete implementation of three applications with the above
    methods, examining the pros and cons of each approach. We saw, for example, how
    an agentic approach gives higher flexibility to the end user at the price of less
    control of the backend plan of action.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 我们深入研究了使用上述方法的具体实现，并分析了每种方法的优缺点。例如，我们看到了代理方法如何在牺牲后端行动计划控制力的代价下，为最终用户提供更高的灵活性。
- en: Finally, we implemented a front-end with Streamlit to build a consumable application
    with the hard-coded approach.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用 Streamlit 实现了前端，以硬编码的方式构建了一个可消费的应用程序。
- en: With this chapter, we conclude Part 2 of the book, where we examined hands-on
    scenarios and built LLMs-powered applications. In the next chapter, we will focus
    on how to customize your LLMs even more with the process of fine-tuning, leveraging
    open-source models, and using custom data for this purpose.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本章，我们总结了本书的第二部分，其中我们探讨了实际场景并构建了由 LLMs 驱动的应用程序。在下一章中，我们将重点介绍如何通过微调过程、利用开源模型和使用自定义数据来进一步定制您的
    LLMs。
- en: References
  id: totrans-331
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Source code for YouTube tools: [https://github.com/venuv/langchain_yt_tools](https://github.com/venuv/langchain_yt_tools)'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: YouTube 工具的源代码：[https://github.com/venuv/langchain_yt_tools](https://github.com/venuv/langchain_yt_tools)
- en: 'LangChain YouTube tool: [https://python.langchain.com/docs/integrations/tools/youtube](https://python.langchain.com/docs/integrations/tools/youtube)'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'LangChain YouTube 工具: [https://python.langchain.com/docs/integrations/tools/youtube](https://python.langchain.com/docs/integrations/tools/youtube)'
- en: 'LangChain AzureCognitiveServicesToolkit: [https://python.langchain.com/docs/integrations/toolkits/azure_cognitive_services](https://python.langchain.com/docs/integrations/toolkits/azure_cognitive_services)'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'LangChain AzureCognitiveServices Toolkit: [https://python.langchain.com/docs/integrations/toolkits/azure_cognitive_services](https://python.langchain.com/docs/integrations/toolkits/azure_cognitive_services)'
- en: Join our community on Discord
  id: totrans-335
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/llm](https://packt.link/llm )'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/llm](https://packt.link/llm)'
- en: '![](img/QR_Code214329708533108046.png)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![二维码](img/QR_Code214329708533108046.png)'
