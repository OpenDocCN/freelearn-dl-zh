- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building Multimodal Applications with LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we are going beyond LLMs, to introduce the concept of multimodality
    while building agents. We will see the logic behind the combination of foundation
    models in different AI domains – language, images, and audio – into one single
    agent that can adapt to a variety of tasks. By the end of this chapter, you will
    be able to build your own multimodal agent, providing it with the tools and LLMs
    needed to perform various AI tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Throughout this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to multimodality and **large multimodal models** (**LMMs**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples of emerging LMMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to build a multimodal agent with single-modal LLMs using LangChain
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To complete the tasks in this chapter, you will need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A Hugging Face account and user access token.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An OpenAI account and user access token.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python 3.7.1 or later version.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Python packages. Make sure to have the following Python packages installed:
    `langchain`, `python-dotenv`, `huggingface_hub`, `streamlit`, `pytube`, `openai`,
    and `youtube_search`. Those can be easily installed via `pip install` in your
    terminal.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can find all the code and examples in the book’s GitHub repository at [https://github.com/PacktPublishing/Building-LLM-Powered-Applications](Chapter_10.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: Why multimodality?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the context of Generative AI, multimodality refers to a model’s capability
    of processing data in various formats. For example, a multimodal model can communicate
    with humans via text, speech, images, or even videos, making the interaction extremely
    smooth and “human-like.”
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Chapter 1*, we defined **large foundation models** (**LFMs**) as a type
    of pre-trained generative AI model that offers immense versatility by being adaptable
    for various specific tasks. LLMs, on the other hand, are a subset of foundation
    models that are able to process one type of data: natural language. Even though
    LLMs have proven to be not only excellent text understanders and generators but
    also reasoning engines to power applications and copilots, it soon became clear
    that we could aim at even more powerful applications.'
  prefs: []
  type: TYPE_NORMAL
- en: The dream is to have intelligent systems that are capable of handling multiple
    data formats – text, images, audio, video, etc – always powered by the reasoning
    engine, which makes them able to plan and execute actions with an agentic approach.
    Such an AI system would be a further milestone toward the reaching of **artificial
    general intelligence** (**AGI**).
  prefs: []
  type: TYPE_NORMAL
- en: '**Definition**'
  prefs: []
  type: TYPE_NORMAL
- en: AGI is a hypothetical type of **artificial intelligence** (**AI**) that can
    perform any intellectual task that a human can. AGI would have a general cognitive
    ability, similar to human intelligence, and be able to learn from experience,
    reason, plan, communicate, and solve problems across different domains. An AGI
    system would also be able to “perceive” the world as we do, meaning that it could
    process data in different formats, from text to images to sounds. Hence, AGI implies
    multimodality.
  prefs: []
  type: TYPE_NORMAL
- en: Creating AGI is a primary goal of some AI research and a common topic in science
    fiction. However, there is no consensus on how to achieve AGI, what criteria to
    use to measure it, or when it might be possible. Some researchers argue that AGI
    could be achieved in years or decades, while others maintain that it might take
    a century or longer, or that it might never be achieved.
  prefs: []
  type: TYPE_NORMAL
- en: However, AGI is not seen as the ultimate milestone in AI development. In fact,
    in recent months another definition has emerged in the context of AI – that is,
    Strong AI or Super AI, referring to an AI system that is more capable than a human.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing this book (February 2024), LMMs such as GPT-4 Turbo with
    Vision are a reality. However, those are not the only ways to reach multimodality.
    In this chapter, we are going to examine how to merge multiple AI systems to reach
    a multimodal AI assistant. The idea is that if we combine single-modal models,
    one for each data format we want to process, and then use an LLM as the brain
    of our agent to let it interact in dynamic ways with those models (that will be
    its tools), we can still achieve this goal. The following diagram shows the structure
    of a multimodal application that integrates various single-modal tools to perform
    a task – in this case, describing a picture aloud. The application uses image
    analysis to examine the picture, text generation to create some text that describes
    what it observes in the picture, and text-to-speech to convey this text to the
    user through speech.
  prefs: []
  type: TYPE_NORMAL
- en: The LLM acts as the “reasoning engine” of the application, invoking the proper
    tools needed to accomplish the user’s query.
  prefs: []
  type: TYPE_NORMAL
- en: '![A person talking to a speech bubble  Description automatically generated](img/B21714_10_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.1: Illustration of multimodal application with single-modal tools'
  prefs: []
  type: TYPE_NORMAL
- en: In the upcoming section, we are going to explore various approaches to building
    multimodal applications, all based on the idea of combining existing single-modal
    tools or models.
  prefs: []
  type: TYPE_NORMAL
- en: Building a multimodal agent with LangChain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we’ve covered the main aspects of multimodality and how to achieve it
    with modern LFMs. As we saw throughout Part 2 of this book, LangChain offers a
    variety of components that we leveraged massively, such as chains, agents, tools,
    and so on. As a result, we already have all the ingredients we need to start building
    our multimodal agent.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, in this chapter, we will adopt three approaches to tackle the problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The agentic, out-of-the-box approach**: Here we will leverage the Azure Cognitive
    Services toolkit, which offers native integrations toward a set of AI models that
    can be consumed via API, and that covers various domains such as image, audio,
    OCR, etc.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The agentic, custom approach**: Here, we are going to select single models
    and tools (including defining custom tools) and concatenate them into a single
    agent that can leverage all of them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The hard-coded approach**: Here, we are going to build separate chains and
    combine them into a sequential chain.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the upcoming sections, we will cover all these approaches with concrete examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Option 1: Using an out-of-the-box toolkit for Azure AI Services'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Formerly known as Azure Cognitive Services, Azure AI Services are a set of cloud-based
    APIs and AI services developed by Microsoft that enable developers and data scientists
    to add cognitive capabilities to their apps. AI Services are meant to provide
    every developer with AI models to be integrated with programming languages such
    as Python, C#, or JavaScript.
  prefs: []
  type: TYPE_NORMAL
- en: 'Azure AI Services cover various domains of AI, including speech, natural language,
    vision, and decision-making. All those services come with models that can be consumed
    via API, and you can decide to:'
  prefs: []
  type: TYPE_NORMAL
- en: Leverage powerful pre-built models available as they are and ready to use.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customize those pre-built models with custom data so that they are tailored
    to your use case.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hence, considered all together, Azure AI Services can achieve the goal of multimodality,
    if properly orchestrated by an LLM as a reasoning engine, which is exactly the
    framework LangChain built.
  prefs: []
  type: TYPE_NORMAL
- en: Getting Started with AzureCognitiveServicesToolkit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In fact, LangChain has a native integration with Azure AI Services called **AzureCognitiveServicesToolkit**,
    which can be passed as a parameter to an agent and leverage the multimodal capabilities
    of those models.
  prefs: []
  type: TYPE_NORMAL
- en: The toolkit makes it easier to incorporate Azure AI services’ capabilities –
    such as image analysis, form recognition, speech-to-text, and text-to-speech –
    within your application. It can be used within an agent, which is then empowered
    to use the AI services to enhance its functionality and provide richer responses.
  prefs: []
  type: TYPE_NORMAL
- en: 'Currently, the integration supports the following tools:'
  prefs: []
  type: TYPE_NORMAL
- en: '**AzureCogsImageAnalysisTool**: Used to analyze and extract metadata from images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AzureCogsSpeech2TextTool**: Used to convert speech to text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AzureCogsText2SpeechTool**: Used to synthetize text to speech with neural
    voices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AzureCogsFormRecognizerTool**: Used to perform **optical character recognition**
    (**OCR**).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Definition**'
  prefs: []
  type: TYPE_NORMAL
- en: OCR is a technology that converts different types of documents, such as scanned
    paper documents, PDFs, or images captured by a digital camera, into editable and
    searchable data. OCR can save time, cost, and resources by automating data entry
    and storage processes. It can also enable access to and editing of the original
    content of historical, legal, or other types of documents.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if you ask an agent what you can make with some ingredients, and
    provide an image of eggs and flour, the agent can use the Azure AI Services Image
    Analysis tool to extract the caption, objects, and tags from the image, and then
    use the provided LLM to suggest some recipes based on the ingredients. To implement
    this, let’s first set up our toolkit.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the toolkit
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To get started with the toolkit, you can follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: You first need to create a multi-service instance of Azure AI Services in Azure
    following the instructions at [https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?tabs=windows&pivots=azportal](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?tabs=windows&pivots=azportal).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A multi-service resource allows you to access multiple AI services with a single
    key and endpoint to be passed to LangChain as environmental variables. You can
    find your keys and endpoint under the **Keys and Endpoint** tab in your resource
    panel:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B21714_10_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.2: Screenshot of a multi-service instance of Azure AI Services'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the resource is set, we can start building our LegalAgent. To do so, the
    first thing we need to do is set the AI services environmental variables in order
    to configure the toolkit. To do so, I’ve saved the following variables in my `.env`
    file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, you can load them as always alongside the other environmental variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can configure our toolkit and also see which tools we have, alongside
    their description:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is the corresponding output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, it’s time to initialize our agent. For this purpose, we will use a `STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION`
    agent that, as we saw in previous chapters, also allows for multi-tools input,
    since we will also add further tools in the *Leveraging multiple tools* section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now we have all the ingredients to start testing our agent.
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging a single tool
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To start easy, let’s simply ask the agent to describe the following picture,
    which will only require the `image_analysis` tool to be accomplished:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A person holding a slingshot  Description automatically generated](img/B21714_10_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.3: Sample picture of a slingshot (source: [https://www.stylo24.it/wp-content/uploads/2020/03/fionda.jpg](https://www.stylo24.it/wp-content/uploads/2020/03/fionda.jpg))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s pass the URL of this image as input to our model, as per the description
    of the `azure_cognitive_services_image_analysis` tool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We then get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: '"action": "azure_cognitive_services_image_analysis",'
  prefs: []
  type: TYPE_NORMAL
- en: '"action_input": "https://www.stylo24.it/wp-content/uploads/2020/03/fionda.jpg"'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: '"action": "Final Answer",'
  prefs: []
  type: TYPE_NORMAL
- en: '"action_input": "The image is of a person holding a slingshot."'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the agent was able to retrieve the proper tool to address the
    user’s question. In this case, the question was very simple, so I want to challenge
    the same tool with a trickier question.
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal is to replicate the GPT-4 capabilities in its common-sense reasoning
    while working with images, as the following illustration from GPT-4’s earliest
    experiments shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A close up of a cell phone  Description automatically generated](img/B21714_10_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.4: Example of visual capabilities and common sense reasoning of GPT-4
    (source: [https://openai.com/research/gpt-4](https://openai.com/research/gpt-4))'
  prefs: []
  type: TYPE_NORMAL
- en: 'So let’s ask our model something more challenging. Let’s ask it to reason about
    the consequences of letting the slingshot go:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We then obtain the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: '"action": "azure_cognitive_services_image_analysis",'
  prefs: []
  type: TYPE_NORMAL
- en: '"action_input": "https://www.stylo24.it/wp-content/uploads/2020/03/fionda.jpg"'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: '"action": "Final Answer",'
  prefs: []
  type: TYPE_NORMAL
- en: '"action_input": "If the person lets the slingshot go, it will fly through the
    air."'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'It might seem an easy question, but the agent’s answers imply an extremely
    refined common sense reasoning: thanks to the metadata extracted from the image
    leveraging the `image_analysis` tool, the LLM was able to set up reasoning in
    terms of consequences given an action (the person letting the slingshot go).'
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the agent only leveraged one tool. But what happens if we ask
    it something that requires at least two tools?
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging multiple tools
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s say, for example, that we want the model to read a story aloud to us based
    on a picture.
  prefs: []
  type: TYPE_NORMAL
- en: '![Meet Côtelette, the Kyrgyz stray dog that followed cyclists all season ...](img/B21714_10_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.5: Example of an input image for a story-telling agent (source: [https://i.redd.it/diawvlriobq11.jpg](https://i.redd.it/diawvlriobq11.jpg))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s ask the agent to do so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We obtain the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: '"action": "azure_cognitive_services_image_analysis",'
  prefs: []
  type: TYPE_NORMAL
- en: '"action_input": {'
  prefs: []
  type: TYPE_NORMAL
- en: '"query": "https://i.redd.it/diawvlriobq11.jpg"'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: '"action": "azure_cognitive_services_text2speech",'
  prefs: []
  type: TYPE_NORMAL
- en: '"action_input": {'
  prefs: []
  type: TYPE_NORMAL
- en: '"query": "Once upon a time, in a snowy winter wonderland, there lived a brave
    little dog named Snowy. Snowy loved to explore the vast snowy hills and mountains
    surrounding his home. One day, Snowy stumbled upon a group of lost animals who
    were stranded in the snow. They were cold, hungry, and scared. Without hesitation,
    Snowy stepped up to help. He led the group to a nearby shelter and found food
    and warmth for everyone. Snowy''s bravery and kindness earned him a reputation
    as a hero in the winter wonderland. From then on, he became known as Snowy the
    Savior of the Snow."'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: '"action": "Final Answer",'
  prefs: []
  type: TYPE_NORMAL
- en: '"action_input": "I hope you enjoyed the story of Snowy the Savior of the Snow,
    the brave little dog who helped rescue a group of lost animals in the snowy winter
    wonderland. Thank you for the opportunity to share this tale with you."'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the agent was able to invoke two tools to accomplish the request:'
  prefs: []
  type: TYPE_NORMAL
- en: It first started with the `image_analysis` tool to generate the image caption
    used to produce the story.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, it invoked the `text2speech` tool to read it aloud to the user.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The agent saved the audio file in a temporary file, and you can listen to it
    directly by clicking on the URL. Alternatively, you can save the output as a Python
    variable and execute it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can also modify the default prompt that comes with the agent type,
    to make it more customized with respect to our specific use case. To do so, we
    first need to inspect the template and then decide which part we can modify. To
    inspect the template, you can run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is our output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '{{'
  prefs: []
  type: TYPE_NORMAL
- en: '"action": $TOOL_NAME,'
  prefs: []
  type: TYPE_NORMAL
- en: '"action_input": $INPUT'
  prefs: []
  type: TYPE_NORMAL
- en: '}}'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: $JSON_BLOB
  prefs: []
  type: TYPE_NORMAL
- en: '...'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]$JSON_BLOB[PRE23]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s modify the prefix of the prompt and pass it as `kwargs` to our agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, now the agent acts more similar to a storyteller with a specific
    style. You can customize your prompt as you wish, always keeping in mind that
    each pre-built agent has its own prompt template, hence it is always recommended
    to first inspect it before customizing it.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have explored the out-of-the-box capabilities of the toolkit, let’s
    build an end-to-end application.
  prefs: []
  type: TYPE_NORMAL
- en: Building an end-to-end application for invoice analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Analyzing invoices might require a lot of manual work if not assisted by digital
    processes. To address this, we will build an AI assistant that is able to analyze
    invoices for us and tell us any relevant information aloud. We will call this
    application **CoPenny**.
  prefs: []
  type: TYPE_NORMAL
- en: With CoPenny, individuals and enterprises could reduce the time of invoice analysis,
    as well as build toward document process automation and, more generally, digital
    process automation.
  prefs: []
  type: TYPE_NORMAL
- en: '**Definition**'
  prefs: []
  type: TYPE_NORMAL
- en: Document process automation is a strategy that uses technology to streamline
    and automate various document-related tasks and processes within an organization.
    It involves the use of software tools, including document capture, data extraction,
    workflow automation, and integration with other systems. For example, document
    process automation can help you extract, validate, and analyze data from invoices,
    receipts, forms, and other types of documents. Document process automation can
    save you time and money, improve accuracy and efficiency, and provide valuable
    insights and reports from your document data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Digital process automation** (**DPA**) is a broader term that refers to automating
    any business process with digital technology. DPA can help you connect your apps,
    data, and services and boost your team’s productivity with cloud flows. DPA can
    also help you create more sophisticated and intuitive customer experiences, collaborate
    across your organization, and innovate with AI and ML.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To start building our application, we can follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using `AzureCognitiveServicesToolkit`, we will leverage the `azure_cognitive_services_form_recognizer`
    and `azure_cognitive_services_text2speech` tools, so we can limit the agent’s
    “powers” only to those two:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is the corresponding output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s now initialize the agent with the default prompt and see the results.
    For this purpose, we will use a sample invoice as a template with which to query
    the agent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![A close-up of a receipt  Description automatically generated](img/B21714_10_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.6: Sample template of a generic invoice (source: https://www.whiteelysee.fr/design/wp-content/uploads/2022/01/custom-t-shirt-order-form-template-free.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by asking the model to tell us all the men’s **stock-keeping units**
    (**SKUs**) on the invoice:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then get the following output (showing a truncated output; you can find
    the whole output in the book’s GitHub repository):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: '"action": "azure_cognitive_services_form_recognizer",'
  prefs: []
  type: TYPE_NORMAL
- en: '"action_input": {'
  prefs: []
  type: TYPE_NORMAL
- en: '"query": "https://www.whiteelysee.fr/design/wp-content/uploads/2022/01/custom-t-shirt-order-form-template-free.jpg"'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also ask for multiple information (women’s SKUs, shipping address, and
    delivery dates) as follows (note that the delivery date is not specified, as we
    want our agent not to hallucinate):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This gives us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let’s also leverage the text2speech tool to produce the audio of the
    response:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As per the previous example, you can listen to the audio by clicking on the
    URL in the chain, or using Python’s `Display` function if you save it as a variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we want our agent to be better tailored toward our goal. To do so, let’s
    customize the prompt giving specific instructions. In particular, we want the
    agent to produce the audio output without the user explicitly asking for it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s run the agent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This yields the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: '"action": "azure_cognitive_services_form_recognizer",'
  prefs: []
  type: TYPE_NORMAL
- en: '"action_input": {'
  prefs: []
  type: TYPE_NORMAL
- en: '"query": "https://www.whiteelysee.fr/design/wp-content/uploads/2022/01/custom-t-shirt-order-form-template-free.jpg"'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, now the agent saved the output into an audio file, even when
    the user didn’t ask explicitly for it.
  prefs: []
  type: TYPE_NORMAL
- en: '`AzureCognitiveServicesToolkit` is a powerful integration that allows for native
    consumption of Azure AI Services. However, there are some pitfalls of this approach,
    including the limited number of AI services. In the next section, we are going
    to explore yet another option to achieve multimodality, with a more flexible approach
    while still keeping an agentic strategy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Option 2: Combining single tools into one agent'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this leg of our journey toward multimodality, we will leverage different
    tools as plug-ins to our `STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION` agent.
    Our goal is to build a copilot agent that will help us generate reviews about
    YouTube videos, as well as post those reviews on our social media with a nice
    description and related picture. In all of that, we want to make little or no
    effort, so we need our agent to perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Search and transcribe a YouTube video based on our input.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Based on the transcription, generate a review with a length and style defined
    by the user query.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate an image related to the video and the review.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will call our copilot **GPTuber**. In the following subsections, we will
    examine each tool and then put them all together.
  prefs: []
  type: TYPE_NORMAL
- en: YouTube tools and Whisper
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first step of our agent will be to search and transcribe the YouTube video
    based on our input. To do so, there are two tools we need to leverage:'
  prefs: []
  type: TYPE_NORMAL
- en: '**YouTubeSearchTool**: An out-of-the-box tool offered by LangChain and adapted
    from [https://github.com/venuv/langchain_yt_tools](https://github.com/venuv/langchain_yt_tools).
    You can import and try the tool by running the following code, specifying the
    topic of the video and the number of videos you want the tool to return:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The tool returns the URL of the video. To watch it, you can add it to [https://youtube.com
    domain](https://youtube.com).
  prefs: []
  type: TYPE_NORMAL
- en: '**CustomYTTranscribeTool**: This is a custom tool that I’ve adapted from [https://github.com/venuv/langchain_yt_tools](https://github.com/venuv/langchain_yt_tools).
    It consists of transcribing the audio file retrieved from the previous tool using
    a speech-to-text model. In our case, we will be leveraging OpenAI’s **Whisper**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Whisper is a transformer-based model introduced by OpenAI in September 2022\.
    It works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: It splits the input audio into 30-second chunks, converting them into spectrograms
    (visual representations of sound frequencies).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It then passes them to an encoder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The encoder then produces a sequence of hidden states that capture the information
    in the audio.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A decoder then predicts the corresponding text caption, using special tokens
    to indicate the task (such as language identification, speech transcription, or
    speech translation) and the output language.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The decoder can also generate timestamps for each word or phrase in the caption.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unlike most OpenAI models, Whisper is open-source.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since this model takes as input only files and not URLs, within the custom
    tool, there is a function defined as `yt_get` (you can find it in the GitHub repository)
    that, starting from the video URL, downloads it into a `.mp4` file. Once downloaded,
    you can try Whisper with the following lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the corresponding output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: By embedding Whisper in this custom tool, we can transcribe the output of the
    first tool into a transcript that will serve as input to the next tool. You can
    see the code and logic behind this embedding and the whole tool in this book’s
    GitHub repository at [https://github.com/PacktPublishing/Building-LLM-Powered-Applications](Chapter_10.xhtml),
    which is a modified version from [https://github.com/venuv/langchain_yt_tools](https://github.com/venuv/langchain_yt_tools).
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we already have two tools, we can start building our tools list and initializing
    our agent, using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the corresponding output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Great! We were able to generate the transcription of this video. The next step
    will be to generate a review alongside a picture. While the review can be written
    directly from the LLM and passed as a parameter to the model (so we don’t need
    another tool), the image generation will need an additional tool. For this purpose,
    we are going to use OpenAI’s DALL·E.
  prefs: []
  type: TYPE_NORMAL
- en: DALL·E and text generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Introduced by OpenAI in January 2021, DALL·E is a transformer-based model that
    can create images from text descriptions. It is based on GPT-3, which is also
    used for natural language processing tasks. It is trained on a large dataset of
    text-image pairs from the web and uses a vocabulary of tokens for both text and
    image concepts. DALL·E can produce multiple images for the same text, showing
    different interpretations and variations.
  prefs: []
  type: TYPE_NORMAL
- en: 'LangChain offers native integration with DALL·E, which you can use as a tool
    by running the following code (always setting the environmental variable of your
    `OPENAI_API_KEY` from the `.env` file):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the corresponding output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the image that was generated, as requested:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A house with bats flying in the sky  Description automatically generated](img/B21714_10_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.7: Image generated by DALL·E upon the user’s input'
  prefs: []
  type: TYPE_NORMAL
- en: 'Great! Now let’s also see whether our agent is capable of generating a review
    of a video based on the transcription:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Note how the agent was initially looking for a tool to make a review, to then
    realize that there is no tool yet that can do it manually thanks to its parametric
    knowledge. This is a great example of how LLMs are reasoning engines and endowed
    with common sense reasoning. As always, you can find the entire chain of thoughts
    in the book’s repository.
  prefs: []
  type: TYPE_NORMAL
- en: The next step will be to put it all together and see whether the agent is capable
    of orchestrating all the tools, with some assistance in terms of prompt engineering.
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have all the ingredients, we need to put them together into one
    single agent. To do so, we can follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to add the DALL·E tool to the list of tools:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This gives us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step will be to test the agent with the default prompt, and then try
    to refine the instructions with some prompt engineering. Let’s start with a pre-configured
    agent (you can find all the steps in the GitHub repository):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This gives us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the accompanying visual output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A person with dreadlocks and green eyes  Description automatically generated](img/B21714_10_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.8: Image generated by DALL·E based on the trailer review'
  prefs: []
  type: TYPE_NORMAL
- en: Well, even without any prompt engineering, the agent was able to orchestrate
    the tools and return the desired results!
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s try to make it more tailored toward our purpose. Similar to the
    CoPenny application, we don’t want the user to specify every time to generate
    a review alongside an image. So let’s modify the default prompt as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output obtained is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'This is accompanied by the following visual output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A mountain with a lake and trees  Description automatically generated with
    medium confidence](img/B21714_10_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.9: Image generated by DALL·E based on a trailer review'
  prefs: []
  type: TYPE_NORMAL
- en: Wow! Not only was the agent able to use all the tools with the proper scope
    but it also adapted the style to the type of channel we want to share our review
    on – in this case, Instagram.
  prefs: []
  type: TYPE_NORMAL
- en: 'Option 3: Hard-coded approach with a sequential chain'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The third and last option offers yet another way of implementing a multimodal
    application, which performs the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Generates a story based on a topic given by the user.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generates a social media post to promote the story.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generates an image to go along with the social media post.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will call this application **StoryScribe**.
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement this, we will build separate LangChain chains for those single
    tasks, and then combine them into a `SequentialChain`. As we saw in *Chapter 1*,
    this is a type of chain that allows you to execute multiple chains in a sequence.
    You can specify the order of the chains and how they pass their outputs to the
    next chain. So, we first need to create individual chains, then combine them and
    run as a unique chain. Let’s follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll start by initializing the story generator chain:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This gives us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that I’ve set the `output_key= "story"` parameter so that it can be easily
    linked as output to the next chain, which will be the social post generator:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following output is then obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Here, I used the output of `story_chain` as input to `social_chain`. When we
    combine all the chains together, this step will be automatically performed by
    the sequential chain.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let’s initialize an image generator chain:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that the output of the chain will be the prompt to pass to the DALL·E model.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to generate the image, we need to use the `DallEAPIWrapper()` module
    available in LangChain:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A child giving a flower to a child  Description automatically generated](img/B21714_10_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.10: Picture generated by DALL·E given a social media post'
  prefs: []
  type: TYPE_NORMAL
- en: 'The final step will be to put it all together into a sequential chain:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is our output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Since we passed the `output_variables = ['post, 'image']` parameter to the chain,
    those will be the two outputs of the chain. With `SequentialChain`, we have the
    flexibility to decide as many output variables as we want, so that we can construct
    our output as we please.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, there are several ways to reach multimodality within your application,
    and LangChain offers many components that make it easier. Now, let’s compare these
    approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing the three options
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We examined three options to achieve this result: options 1 and 2 follow the
    “agentic” approach, using, respectively, pre-built toolkit and single tools combined;
    option 3, on the other hand, follows a hard-coded approach, letting the developer
    decide the order of actions to be done.'
  prefs: []
  type: TYPE_NORMAL
- en: 'All three come with pros and cons, so let’s wrap up some final considerations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Flexibility vs control**: The agentic approach lets the LLM decide which
    actions to take and in which order. This implies greater flexibility for the end
    user since there are no constraints in terms of queries that can be done. On the
    other hand, having no control over the agent’s chain of thoughts could lead to
    mistakes that would need several tests of prompt engineering. Plus, as LLMs are
    non-deterministic, it is also hard to recreate mistakes to retrieve the wrong
    thought process. Under this point of view, the hard-coded approach is safer, since
    the developer has full control over the order of execution of the actions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evaluations**: The agentic approach leverages the tools to generate the final
    answer so that we don’t have to bother to plan these actions. However, if the
    final output doesn’t satisfy us, it might be cumbersome to understand what is
    the main source of the error: it might be a wrong plan, rather than a tool that
    is not doing its job correctly, or maybe a wrong prompt overall. On the other
    hand, with the hard-coded approach, each chain has its own model that can be tested
    separately, so that it is easier to identify the step of the process where the
    main error has occurred.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Maintenance**: With the agentic approach, there is one component to maintain:
    the agent itself. We have in fact one prompt, one agent, and one LLM, while the
    toolkit or list of tools is pre-built and we don’t need to maintain them. On the
    other hand, with the hard-coded approach, for each chain, we need a separate prompt,
    model, and testing activities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To conclude, there is no golden rule to decide which approach to follow: it’s
    up to the developer to decide depending on the relative weight of the above parameters.
    As a general rule of thumb, the first step should be to define the problem to
    solve and then evaluate the complexity of each approach with respect to that problem.
    If, for example, it is a task that can be entirely addressed with the Cognitive
    Services toolkit without even doing prompt engineering, that could be the easiest
    way to proceed; on the other hand, if it requires a lot of control over the single
    components as well as on the sequence of execution, a hard-coded approach is preferable.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we are going to build a sample front-end using Streamlit,
    built on top of StoryScribe.
  prefs: []
  type: TYPE_NORMAL
- en: Developing the front-end with Streamlit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have seen the logic behind an LLM-powered StoryScribe, it is time
    to give our application a GUI. To do so, we will once again leverage Streamlit.
    As always, you can find the whole Python code in the GitHub book repository at
    [https://github.com/PacktPublishing/Building-LLM-Powered-Applications](Chapter_10.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: As per the previous sections, you need to create a `.py` file to run in your
    terminal via `streamlit run file.py`. In our case, the file will be named `storyscribe.py`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the main steps to set up the front-end:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Configuring the application webpage:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize the dynamic variables to be used within the placeholders of the
    prompts:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize all the chains and the overall chain (I will omit here all the prompt
    templates; you can find them in the GitHub repository of the book):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the overall chain and print the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this case, I’ve set the `output_variables = [''story'',''post'', ''image'']`
    parameter so that we will have also the story itself as output. The final result
    looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B21714_10_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.11: Front-end of StoryScribe showing the story output'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following picture is the resulting Instagram post:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a painting  Description automatically generated](img/B21714_10_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.12: Front-end of StoryScribe showing the social media post along
    with the generated image'
  prefs: []
  type: TYPE_NORMAL
- en: With just a few lines of code, we were able to set up a simple front-end for
    StoryScribe with multimodal capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we introduced the concept of multimodality and how to achieve
    it even without multimodal models. We explored three different ways of achieving
    the objective of a multimodal application: an agentic approach with a pre-built
    toolkit, an agentic approach with the combination of single tools, and a hard-coded
    approach with chained models.'
  prefs: []
  type: TYPE_NORMAL
- en: We delved into the concrete implementation of three applications with the above
    methods, examining the pros and cons of each approach. We saw, for example, how
    an agentic approach gives higher flexibility to the end user at the price of less
    control of the backend plan of action.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we implemented a front-end with Streamlit to build a consumable application
    with the hard-coded approach.
  prefs: []
  type: TYPE_NORMAL
- en: With this chapter, we conclude Part 2 of the book, where we examined hands-on
    scenarios and built LLMs-powered applications. In the next chapter, we will focus
    on how to customize your LLMs even more with the process of fine-tuning, leveraging
    open-source models, and using custom data for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Source code for YouTube tools: [https://github.com/venuv/langchain_yt_tools](https://github.com/venuv/langchain_yt_tools)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LangChain YouTube tool: [https://python.langchain.com/docs/integrations/tools/youtube](https://python.langchain.com/docs/integrations/tools/youtube)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LangChain AzureCognitiveServicesToolkit: [https://python.langchain.com/docs/integrations/toolkits/azure_cognitive_services](https://python.langchain.com/docs/integrations/toolkits/azure_cognitive_services)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/llm](https://packt.link/llm )'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code214329708533108046.png)'
  prefs: []
  type: TYPE_IMG
