<html><head></head><body>
		<div id="_idContainer074">
			<h1 id="_idParaDest-257" class="chapter-number"><a id="_idTextAnchor256"/><st c="0">13</st></h1>
			<h1 id="_idParaDest-258"><a id="_idTextAnchor257"/><st c="3">Using Prompt Engineering to Improve RAG Efforts</st></h1>
			<p><st c="51">Pop quiz, what do you use to generate content from a </st><strong class="bold"><st c="105">large language </st></strong><span class="No-Break"><strong class="bold"><st c="120">model</st></strong></span><span class="No-Break"><st c="125"> (</st></span><span class="No-Break"><strong class="bold"><st c="127">LLM</st></strong></span><span class="No-Break"><st c="130">)?</st></span></p>
			<p><span class="No-Break"><st c="133">A prompt!</st></span></p>
			<p><st c="143">Clearly, the prompt is a key element for any generative AI application, and therefore any </st><strong class="bold"><st c="234">retrieval-augmented generation</st></strong><st c="264"> (</st><strong class="bold"><st c="266">RAG</st></strong><st c="269">) application. </st><st c="285">RAG systems blend the capabilities of information retrieval and generative language models to enhance the quality and relevance of generated text. </st><st c="432">Prompt engineering, in this context, involves the strategic formulation and refinement of input prompts to improve the retrieval of pertinent information, which subsequently enhances the generation process. </st><st c="639">Prompts are yet another area within the generative AI world that entire books can be written about. </st><st c="739">There are numerous strategies that focus on different areas of prompts that can be employed to improve the results of your LLM usage. </st><st c="873">However, we are going to focus specifically on the strategies that are more specific to </st><span class="No-Break"><st c="961">RAG applications.</st></span></p>
			<p><st c="978">In this chapter, we are going to focus our efforts on the </st><span class="No-Break"><st c="1037">following topics:</st></span></p>
			<ul>
				<li><st c="1054">Key prompt engineering concepts </st><span class="No-Break"><st c="1087">and parameters</st></span></li>
				<li><st c="1101">The fundamentals of prompt design and prompt engineering specifically for </st><span class="No-Break"><st c="1176">RAG applications</st></span></li>
				<li><st c="1192">Adapting prompts for different LLMs beyond just </st><span class="No-Break"><st c="1241">OpenAI models</st></span></li>
				<li><st c="1254">Code lab 13.1 – Creating custom </st><span class="No-Break"><st c="1287">prompt templates</st></span></li>
				<li><st c="1303">Code lab 13.2 – </st><span class="No-Break"><st c="1320">Prompting options</st></span></li>
			</ul>
			<p><st c="1337">By the end of this chapter, you will have a solid foundation in prompt engineering for RAG and be equipped with practical techniques to optimize prompts for retrieving relevant information, generating high-quality text, and adapting to your specific use case. </st><st c="1598">We will kick our discussion off by covering some of the key concepts within the prompting world, starting with </st><span class="No-Break"><st c="1709">prompt parameters.</st></span></p>
			<h1 id="_idParaDest-259"><a id="_idTextAnchor258"/><st c="1727">Technical requirements</st></h1>
			<p><st c="1750">The code for this chapter is placed in the following GitHub </st><span class="No-Break"><st c="1811">repository: </st></span><a href="https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_13 "><span class="No-Break"><st c="1823">https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_13</st></span></a></p>
			<h1 id="_idParaDest-260"><a id="_idTextAnchor259"/><st c="1920">Prompt parameters</st></h1>
			<p><st c="1938">There are numerous parameters that are common among most LLMs, but we are going to discuss a small subset that is most likely to have an impact on your RAG efforts: temperature, top-p, </st><span class="No-Break"><st c="2124">and seed.</st></span></p>
			<h2 id="_idParaDest-261"><a id="_idTextAnchor260"/><st c="2133">Temperature</st></h2>
			<p><st c="2145">If you think of your</st><a id="_idIndexMarker839"/><st c="2166"> output as a string of </st><strong class="bold"><st c="2189">tokens</st></strong><st c="2195">, an LLM, in a</st><a id="_idIndexMarker840"/><st c="2209"> basic sense, is predicting the </st><em class="italic"><st c="2241">next word</st></em><st c="2250"> (or token) based on the data you’ve provided and the previous tokens it has already generated. </st><st c="2346">The next word that the LLM predicts is a product of a probability distribution representing all potential words and </st><span class="No-Break"><st c="2462">their probabilities.</st></span></p>
			<p><st c="2482">In many cases, the probability of certain words is going to be much higher than most others, but there is still a probabilistic chance that the LLM selects one of the less likely words. </st><st c="2669">Temperature is the setting that dictates how likely it is for the model to choose a word further down the probability distribution. </st><st c="2801">In other words, this allows you to use temperature to set the degree of randomness of the model’s output. </st><st c="2907">You can pass temperature into your LLM definition as a parameter. </st><st c="2973">It is optional. </st><st c="2989">If you do not use it, the default is </st><strong class="source-inline"><st c="3026">1</st></strong><st c="3027">. You can set the temperature value between </st><strong class="source-inline"><st c="3071">0</st></strong><st c="3072"> and </st><strong class="source-inline"><st c="3077">2</st></strong><st c="3078">. Higher values will make the output more random, meaning it will strongly consider words further down the probability distribution, while lower values will do </st><span class="No-Break"><st c="3238">the opposite.</st></span></p>
			<p class="callout-heading"><st c="3251">Simple temperature example</st></p>
			<p class="callout"><st c="3278">Let’s review a very simple example of a </st><em class="italic"><st c="3319">next-word</st></em><st c="3328"> probability distribution to illustrate how temperature works. </st><st c="3391">Let’s say you have the sentence </st><strong class="source-inline"><st c="3423">The dog ran</st></strong><st c="3434"> and you are waiting for the model to predict the </st><em class="italic"><st c="3484">next word</st></em><st c="3493">. Let’s say that based on this model’s training and all of the other data it is considering as a part of this prediction, a very simple example of the conditional probability distribution for this is </st><span class="No-Break"><st c="3693">as follows:</st></span></p>
			<p class="callout"><strong class="source-inline"><st c="3704">P("next word" | "The dog ran") = {"down": 0.4, "to" : 0.3, "with": 0.2, "</st></strong><span class="No-Break"><strong class="source-inline"><st c="3778">away": 0.1}</st></strong></span></p>
			<p class="callout"><st c="3790">The total probabilities add up to </st><strong class="source-inline"><st c="3825">1</st></strong><st c="3826">. The most likely word is </st><strong class="source-inline"><st c="3852">down</st></strong><st c="3856"> and the second most likely word is </st><strong class="source-inline"><st c="3892">to</st></strong><st c="3894">. However, that does not mean that </st><strong class="source-inline"><st c="3929">away</st></strong><st c="3933"> will never appear in the inference. </st><st c="3970">The model will apply a probabilistic model to this selection and sometimes, randomly, the less likely word will be selected. </st><st c="4095">In some scenarios, this is an advantage for your RAG application, but in others, this may be a disadvantage. </st><st c="4204">If you set the temperature to </st><strong class="source-inline"><st c="4234">0</st></strong><st c="4235">, it will only use the most likely word. </st><st c="4276">If you set it to </st><strong class="source-inline"><st c="4293">2</st></strong><st c="4294">, it is much more likely to look at all of the options and randomly pick less likely words most of the time. </st><st c="4403">In other words, you can increase the random nature of the model by increasing </st><span class="No-Break"><st c="4481">the temperature.</st></span></p>
			<p><st c="4497">We have been using temperature since the beginning, setting it to zero. </st><st c="4570">Here is the line that we </st><span class="No-Break"><st c="4595">have added:</st></span></p>
			<pre class="source-code"><st c="4606">
llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0)</st></pre>
			<p><st c="4664">The intention here was to make the results of our code labs more predictable so that when you run them, you get something similar. </st><st c="4796">Your results will likely differ somewhat, but at least with a </st><strong class="source-inline"><st c="4858">0</st></strong><st c="4859"> temperature, they have a better chance of </st><span class="No-Break"><st c="4902">being similar.</st></span></p>
			<p><st c="4916">You may not always want to use a </st><strong class="source-inline"><st c="4950">0</st></strong><st c="4951"> temperature. </st><st c="4965">Consider scenarios, such as when you want a more </st><em class="italic"><st c="5014">creative</st></em><st c="5022"> output from the LLM, wherein you may want to use the temperature to your advantage in your </st><span class="No-Break"><st c="5114">RAG application.</st></span></p>
			<p><st c="5130">Temperature</st><a id="_idIndexMarker841"/><st c="5142"> and top-p are somewhat related in that they both manage randomness in your LLM’s output. </st><st c="5232">However, there are differences. </st><st c="5264">Let’s discuss top-p and talk about what these </st><span class="No-Break"><st c="5310">differences are.</st></span></p>
			<h2 id="_idParaDest-262"><a id="_idTextAnchor261"/><st c="5326">Top-p</st></h2>
			<p><st c="5332">Similar to temperature, top-p can </st><a id="_idIndexMarker842"/><st c="5367">also help you introduce randomness into your model’s output. </st><st c="5428">However, where temperature deals with the general emphasis on how random you would like your input to be, top-p can help you target a specific part of the probability distribution with that randomness. </st><st c="5630">In the simple example provided, we discussed the </st><span class="No-Break"><st c="5679">probability distribution:</st></span></p>
			<pre class="source-code"><st c="5704">
P("next word" | "The dog ran") = {"down": 0.4, "to" : 0.3,
    "with": 0.2, "away": 0.1}</st></pre>
			<p><st c="5789">Remember that we noted that the total probability represented here adds up to </st><strong class="source-inline"><st c="5868">1.0</st></strong><st c="5871">. With top-p, you can target what portion of that probability you would like included. </st><st c="5958">So for example, if you set the top-p to </st><strong class="source-inline"><st c="5998">0.7</st></strong><st c="6001">, it will only consider the first two words in this probability distribution, which add up to the first </st><strong class="source-inline"><st c="6105">0.7</st></strong><st c="6108"> (out of 1.0) of the probability distribution. </st><st c="6155">You do not have that kind of targeted control with temperature. </st><st c="6219">Top-p is also optional. </st><st c="6243">If you do not use it, the default is </st><strong class="source-inline"><st c="6280">1</st></strong><st c="6281">, meaning it considers </st><span class="No-Break"><st c="6304">all options.</st></span></p>
			<p><st c="6316">You may be tempted to use both temperature and top-p, but that can get very complicated and unpredictable. </st><st c="6424">Therefore, it is commonly recommended to use either, but not both at the </st><span class="No-Break"><st c="6497">same time.</st></span></p>
			<table id="table001-2" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><st c="6507">LLM parameter</st></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><st c="6521">Outcome</st></strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="6529">Temperature</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="6541">General randomness</st></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="6560">Top-p</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="6566">Focused randomness</st></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><st c="6585">Temperature + </st><span class="No-Break"><st c="6600">top-p</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="6605">Unpredictable complexity</st></span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="6630">Table 13.1 –Showing the type of outcomes from each LLM parameter</st></p>
			<p><st c="6695">Next, we’ll learn how to </st><a id="_idIndexMarker843"/><st c="6721">use top-p with your model. </st><st c="6748">It is slightly different than the other parameters we have used in that you have to pass it as part of the </st><strong class="source-inline"><st c="6855">model_kwargs</st></strong><st c="6867"> variables, which looks </st><span class="No-Break"><st c="6891">like this:</st></span></p>
			<pre class="source-code"><st c="6901">
llm = ChatOpenAI(model_name="gpt-4o-mini",
    model_kwargs={"top_p": 0.5})</st></pre>
			<p><st c="6973">The </st><strong class="source-inline"><st c="6978">model_kwargs</st></strong><st c="6990"> variables are a convenient way to pass parameters that are not built directly into LangChain but that exist in the LLM’s underlying API. </st><st c="7128">The top-p is a parameter for this ChatGPT model, but it may be called something different or not exist for other models. </st><st c="7249">Be sure to check the documentation for each API you use and to use the proper reference to access that model’s parameters. </st><st c="7372">Now that we have learned more about the parameters that help define randomness in our outputs, let’s learn about seed setting, which is </st><a id="_idIndexMarker844"/><st c="7508">meant to help us control the </st><span class="No-Break"><st c="7537">uncontrollable randomness.</st></span></p>
			<h2 id="_idParaDest-263"><a id="_idTextAnchor262"/><st c="7563">Seed</st></h2>
			<p><st c="7568">LLM responses are non-deterministic </st><a id="_idIndexMarker845"/><st c="7605">by default, meaning inference results can differ from request to request. </st><st c="7679">However, as data scientists, we often have the need for a more deterministic, reproducible outcome. </st><st c="7779">These details seem to be at odds with each other, but that does not have to be the case. </st><st c="7868">OpenAI and others have recently made efforts to offer some control toward deterministic outputs by giving you access to this seed parameter and the </st><strong class="source-inline"><st c="8016">system_fingerprint</st></strong> <span class="No-Break"><st c="8034">response field.</st></span></p>
			<p><st c="8050">The seed is a common setting in many software applications that involve generating random numbers or random data sequences. </st><st c="8175">By using a seed, you can still generate random sequences, but you can produce the same random sequence every time. </st><st c="8290">This gives you the control to receive (mostly) deterministic outputs across API calls. </st><st c="8377">You can set the seed parameter to any integer of your choice and use the same value across requests you’d like deterministic outputs for. </st><st c="8515">Furthermore, if you use a seed, even with other random settings such as temperature or top-p, you can still (mostly) rely on receiving the same </st><span class="No-Break"><st c="8659">exact response.</st></span></p>
			<p><st c="8674">It should be noted that your results can still be different, even with the use of a seed, because you are working with an API connected to a service where changes are constantly being made. </st><st c="8865">Those changes can cause different results over time. </st><st c="8918">Models such as ChatGPT provide a </st><strong class="source-inline"><st c="8951">system_fingerprint</st></strong><st c="8969"> field in their outputs, which you can compare to each other as an indication of system changes that may cause differences in the response. </st><st c="9109">If the </st><strong class="source-inline"><st c="9116">system_fingerprint</st></strong><st c="9134"> value changes from the last time you called that LLM API, while you were using the same seed, you may still see different outputs due to changes OpenAI made to </st><span class="No-Break"><st c="9295">their systems.</st></span></p>
			<p><st c="9309">The seed parameter is also optional and does not exist in the LangChain set of LLM parameters. </st><st c="9405">So, once again, like the </st><strong class="source-inline"><st c="9430">top-p</st></strong><st c="9435"> parameter, we must pass it through the </st><strong class="source-inline"><st c="9475">model_kwargs</st></strong><st c="9487"> parameter </st><span class="No-Break"><st c="9498">like this:</st></span></p>
			<pre class="source-code"><st c="9508">
optional_params = {
  "top_p": 0.5, "seed": 42
}
llm = ChatOpenAI(model_name="gpt-4o-mini", model_kwargs=optional_params)</st></pre>
			<p><st c="9628">Here, we add the seed parameter alongside the </st><strong class="source-inline"><st c="9675">top-p</st></strong><st c="9680"> parameter in a dictionary of parameters that we will pass to the </st><span class="No-Break"><strong class="source-inline"><st c="9746">model_kwargs</st></strong></span><span class="No-Break"><st c="9758"> parameter.</st></span></p>
			<p><st c="9769">There are many other parameters for the different models you could use that we encourage you to explore, but </st><a id="_idIndexMarker846"/><st c="9879">these parameters are likely to have the most impact on your </st><span class="No-Break"><st c="9939">RAG application.</st></span></p>
			<p><st c="9955">The next prompt-oriented key concept we will touch on is the </st><strong class="bold"><st c="10017">shot</st></strong><st c="10021"> concept, focusing on the amount of background information you provide to </st><span class="No-Break"><st c="10095">the LLM.</st></span></p>
			<h1 id="_idParaDest-264"><a id="_idTextAnchor263"/><st c="10103">Take your shot</st></h1>
			<p><strong class="bold"><st c="10118">No-shot</st></strong><st c="10126">, </st><strong class="bold"><st c="10128">single-shot</st></strong><st c="10139">, </st><strong class="bold"><st c="10141">few-shot</st></strong><st c="10149">, and </st><strong class="bold"><st c="10155">multi-shot</st></strong><st c="10165"> are common terms you will hear when talking about your </st><a id="_idIndexMarker847"/><st c="10221">prompting strategy. </st><st c="10241">They all stem from the same concept, where a shot is</st><a id="_idIndexMarker848"/><st c="10293"> one example you give to your LLM to help it determine how to respond to</st><a id="_idIndexMarker849"/><st c="10365"> your query. </st><st c="10378">If that is not clear, then I could give you an </st><a id="_idIndexMarker850"/><st c="10425">example of what I am talking about. </st><st c="10461">Oh wait, that is exactly</st><a id="_idIndexMarker851"/><st c="10485"> the idea behind the shot concept! </st><st c="10520">You can give no examples (no-shot), one example (single-shot), or more </st><a id="_idIndexMarker852"/><st c="10591">than one example (few-shot or multi-shot). </st><st c="10634">Each</st><a id="_idIndexMarker853"/><st c="10638"> shot is an example; each example is a shot. </st><st c="10683">Here is an </st><a id="_idIndexMarker854"/><st c="10694">example of what you would say to an LLM (we could call this single-shot, since I am only providing </st><span class="No-Break"><st c="10793">one example):</st></span></p>
			<pre class="source-code"><st c="10806">
"Give me a joke that uses an animal and some action that animal takes that is funny.
</st><st c="10892">Use this example to guide the joke you provide:
Joke-question: Why did the chicken cross the road?
</st><st c="10991">Joke-answer: To get to the other side."</st></pre>
			<p><st c="11030">The assumption here is that by providing that example, you are helping guide the LLM in how </st><span class="No-Break"><st c="11123">you respond.</st></span></p>
			<p><st c="11135">In a RAG application, you will often provide examples in your context. </st><st c="11207">That is not always the case, as sometimes context is just additional (but important) data. </st><st c="11298">However, if you are providing actual examples of questions and answers in the context with the intention of directing the LLM to answer the new user query in a similar manner, then you are using a shot approach. </st><st c="11510">You will find that some RAG applications follow the multi-shot pattern much more closely, but it really depends on the goal of your application and the data you </st><span class="No-Break"><st c="11671">have available.</st></span></p>
			<p><st c="11686">Examples and shots are not the only concepts that are important to understand in prompts, as you will also want to understand the difference in the terms referring to your approach to prompts. </st><st c="11880">We will talk about these </st><span class="No-Break"><st c="11905">approaches next.</st></span></p>
			<h1 id="_idParaDest-265"><a id="_idTextAnchor264"/><st c="11921">Prompting, prompt design, and prompt engineering revisited</st></h1>
			<p><st c="11980">In the vocabulary section of </st><a href="B22475_01.xhtml#_idTextAnchor015"><span class="No-Break"><em class="italic"><st c="12010">Chapter 1</st></em></span></a><st c="12019">, we discussed these three concepts and how they interplay. </st><st c="12079">As a refresher, we provided </st><span class="No-Break"><st c="12107">these bullets:</st></span></p>
			<ul>
				<li><strong class="bold"><st c="12121">Prompting</st></strong><st c="12131"> is the act of </st><a id="_idIndexMarker855"/><st c="12146">sending a query or </st><em class="italic"><st c="12165">prompt</st></em><st c="12171"> to </st><span class="No-Break"><st c="12175">an LLM.</st></span></li>
				<li><strong class="bold"><st c="12182">Prompt design</st></strong><st c="12196"> refers to the strategy you take to </st><em class="italic"><st c="12232">design</st></em><st c="12238"> the prompt you will send to the LLM. </st><st c="12276">Many different prompt design</st><a id="_idIndexMarker856"/><st c="12304"> strategies work in </st><span class="No-Break"><st c="12324">different scenarios.</st></span></li>
				<li><strong class="bold"><st c="12344">Prompt engineering</st></strong><st c="12363"> focuses</st><a id="_idIndexMarker857"/><st c="12371"> more on the technical aspects surrounding the prompt that you use to improve the outputs from the LLM. </st><st c="12475">For example, you may break up a complex query into two or three different LLM interactions, </st><em class="italic"><st c="12567">engineering</st></em><st c="12578"> it better to achieve </st><span class="No-Break"><st c="12600">superior results.</st></span></li>
			</ul>
			<p><st c="12617">We had promised to revisit these topics in </st><a href="B22475_13.xhtml#_idTextAnchor256"><span class="No-Break"><em class="italic"><st c="12661">Chapter 13</st></em></span></a><st c="12671">, and so we are here to deliver on that promise! </st><st c="12720">We will not only revisit these topics but also show you how this is actually performed in code. </st><st c="12816">Prompting is a relatively straightforward concept, so we will focus on the other two topics: design </st><span class="No-Break"><st c="12916">and engineering.</st></span></p>
			<h1 id="_idParaDest-266"><a id="_idTextAnchor265"/><st c="12932">Prompt design versus engineering approaches</st></h1>
			<p><st c="12976">When we discussed the different </st><em class="italic"><st c="13009">shot</st></em><st c="13013"> approaches in the </st><em class="italic"><st c="13032">Take your shot</st></em><st c="13046"> section, that fell under prompt design. </st><st c="13087">However, we also implemented prompt engineering when we filled in the prompt template with the question and context data we pulled from other parts of the RAG system. </st><st c="13254">When we fill this prompt with data from other parts of the system, you may remember that this is called hydrating, which is a specific prompt engineering approach. </st><st c="13418">Prompt design and prompt engineering have significant overlap and so you will often hear the terms used interchangeably. </st><st c="13539">In our case, we are going to talk about them together, particularly how they can be used to improve our </st><span class="No-Break"><st c="13643">RAG application.</st></span></p>
			<p><st c="13659">I have seen these concepts described in many different ways over the past few years, and so it would seem that our field still hasn’t formed a complete definition of each or drawn the line between them. </st><st c="13863">For the purpose of understanding these concepts for this book, the way I would describe the difference between prompt design and prompt engineering is that prompt engineering is a broader concept that encompasses not only the design of the prompt but also the optimization and fine-tuning of the entire interaction between the user and the </st><span class="No-Break"><st c="14203">language model.</st></span></p>
			<p><st c="14218">There are numerous prompt design techniques, which could all be used to improve your RAG application, in theory. </st><st c="14332">It is important to keep track of the options you have and understand which scenarios each approach is most applicable to. </st><st c="14454">It will take some experimentation with different prompt design approaches to determine which is best for your application. </st><st c="14577">There is no one-size-fits-all solution for prompt design. </st><st c="14635">We will provide a short list of examples, but we highly encourage you to learn more about prompt design from other sources and take note of which approaches might help your specific </st><span class="No-Break"><st c="14817">applications out:</st></span></p>
			<ul>
				<li><span class="No-Break"><strong class="bold"><st c="14834">Shot design</st></strong></span><span class="No-Break"><st c="14846">:</st></span><ul><li><st c="14848">The starting</st><a id="_idIndexMarker858"/><st c="14860"> point for any prompt design </st><span class="No-Break"><st c="14889">thought process</st></span></li><li><st c="14904">Involves carefully crafting the initial prompt to use examples to help guide the AI model toward the </st><span class="No-Break"><st c="15006">desired output</st></span></li><li><st c="15020">Can be applied and/or mixed with other design patterns to enhance the quality and relevance of the </st><span class="No-Break"><st c="15120">generated content</st></span></li></ul></li>
				<li><span class="No-Break"><strong class="bold"><st c="15137">Chain-of-thought prompting</st></strong></span><span class="No-Break"><st c="15164">:</st></span><ul><li><st c="15166">Breaks down complex problems</st><a id="_idIndexMarker859"/><st c="15194"> into smaller, more manageable steps, prompting the LLM for intermediate reasoning at </st><span class="No-Break"><st c="15280">each step</st></span></li><li><st c="15289">Enhances the quality of LLM-generated answers by providing a clear, step-by-step thought process, ensuring better understanding and more </st><span class="No-Break"><st c="15427">accurate responses</st></span></li></ul></li>
				<li><strong class="bold"><st c="15445">Personas (</st></strong><span class="No-Break"><strong class="bold"><st c="15456">role prompting)</st></strong></span><span class="No-Break"><st c="15472">:</st></span><ul><li><st c="15474">Involves creating a fictional</st><a id="_idIndexMarker860"/><st c="15503"> character based on a representative segment of a user population or group, defined with details such as name, occupation, demographics, personal story, pain points, </st><span class="No-Break"><st c="15669">and challenges</st></span></li><li><st c="15683">Ensures that the output is relevant, useful, and consistent with the needs and preferences of the target audience, giving the content more personality </st><span class="No-Break"><st c="15835">and style</st></span></li><li><st c="15844">A powerful tool for developing effective language models that align with the needs </st><span class="No-Break"><st c="15928">of users</st></span></li></ul></li>
				<li><strong class="bold"><st c="15936">Chain of </st></strong><span class="No-Break"><strong class="bold"><st c="15946">density (summarization)</st></strong></span><span class="No-Break"><st c="15969">:</st></span><ul><li><st c="15971">Focuses on ensuring that the </st><a id="_idIndexMarker861"/><st c="16000">LLM has done a proper job summarizing content, checking that no vital information has been left out and that the summary is </st><span class="No-Break"><st c="16124">concise enough</st></span></li><li><st c="16138">Uses entity density as the LLM iterates through a summary, ensuring that the most important entities </st><span class="No-Break"><st c="16240">are included</st></span></li></ul></li>
				<li><strong class="bold"><st c="16252">Tree of thoughts (exploration </st></strong><span class="No-Break"><strong class="bold"><st c="16283">over thoughts)</st></strong></span><ul><li><st c="16297">Starts with an initial prompt, which</st><a id="_idIndexMarker862"/><st c="16334"> generates multiple thought options, and iteratively selects the best options to generate the next round </st><span class="No-Break"><st c="16439">of thoughts</st></span></li><li><st c="16450">Allows for a more diverse and comprehensive exploration of ideas and concepts until the </st><a id="_idIndexMarker863"/><st c="16539">desired output text </st><span class="No-Break"><st c="16559">is generated</st></span></li></ul></li>
				<li><span class="No-Break"><strong class="bold"><st c="16571">Graph prompting</st></strong></span><ul><li><st c="16587">A new prompting framework</st><a id="_idIndexMarker864"/><st c="16613"> specifically designed for working with </st><span class="No-Break"><st c="16653">graph-structured data</st></span></li><li><st c="16674">Enables the LLM to understand and generate content based on the relationships between entities in </st><span class="No-Break"><st c="16773">a graph</st></span></li></ul></li>
				<li><span class="No-Break"><strong class="bold"><st c="16780">Knowledge augmentation</st></strong></span><ul><li><st c="16803">Involves augmenting prompts</st><a id="_idIndexMarker865"/><st c="16831"> with extra, relevant information to improve the quality and accuracy of the </st><span class="No-Break"><st c="16908">generated content</st></span></li><li><st c="16925">Can be achieved through techniques such as RAG, which incorporates external knowledge into </st><span class="No-Break"><st c="17017">the prompt</st></span></li></ul></li>
				<li><strong class="bold"><st c="17027">Show Me versus Tell </st></strong><span class="No-Break"><strong class="bold"><st c="17048">Me prompts</st></strong></span><ul><li><st c="17058">Two different approaches to</st><a id="_idIndexMarker866"/><st c="17086"> providing instructions to generative AI models: </st><em class="italic"><st c="17135">Show Me</st></em><st c="17142"> involves providing examples or demonstrations, while </st><em class="italic"><st c="17196">Tell Me</st></em><st c="17203"> involves providing explicit instructions </st><span class="No-Break"><st c="17245">or documentation</st></span></li><li><st c="17261">Using both approaches offers flexibility and can potentially increase the accuracy of the generative AI’s responses based on the specific context and complexity of </st><span class="No-Break"><st c="17426">the task</st></span></li></ul></li>
			</ul>
			<p><st c="17434">This list is just scratching the surface, as there are numerous other approaches that can be employed to improve prompt engineering and generative AI performance. </st><st c="17598">As the field of prompt engineering continues to evolve, new and innovative techniques are likely to emerge, further enhancing the capabilities of generative </st><span class="No-Break"><st c="17755">AI models.</st></span></p>
			<p><st c="17765">Let’s talk about the fundamentals of prompt design that can help with RAG </st><span class="No-Break"><st c="17840">applications next.</st></span></p>
			<h1 id="_idParaDest-267"><a id="_idTextAnchor266"/><st c="17858">Fundamentals of prompt design</st></h1>
			<p><st c="17888">When designing prompts for RAG applications, it’s essential to keep the following fundamentals </st><span class="No-Break"><st c="17984">in mind:</st></span></p>
			<ul>
				<li><strong class="bold"><st c="17992">Be concise and specific</st></strong><st c="18016">: Clearly </st><a id="_idIndexMarker867"/><st c="18027">define the task you want the AI model to perform, and provide only the necessary information to complete the task effectively. </st><st c="18154">For example, saying </st><strong class="source-inline"><st c="18174">Please analyze the given context and provide an answer to the question, taking into account all the relevant information and details</st></strong><st c="18306"> would be less concise and specific than saying </st><strong class="source-inline"><st c="18354">Based on the context provided, answer the following question: [</st></strong><span class="No-Break"><strong class="source-inline"><st c="18417">specific question]</st></strong></span><span class="No-Break"><st c="18436">.</st></span></li>
				<li><strong class="bold"><st c="18437">Ask one task at a time</st></strong><st c="18460">: Break down complex tasks into smaller, more manageable sub-tasks, and create separate prompts for each sub-task to ensure better results. </st><st c="18601">For example, if you said </st><strong class="source-inline"><st c="18626">Summarize the main points of the context, identify the key entities mentioned, and then answer the given question</st></strong><st c="18739">, that is multiple tasks you are asking for at the same time. </st><st c="18801">You would likely have better results if you broke this into multiple prompts and said something similar </st><span class="No-Break"><st c="18905">to this:</st></span><ul><li><strong class="source-inline"><st c="18913">Summarize the main points of the following </st></strong><span class="No-Break"><strong class="source-inline"><st c="18957">context: [context]</st></strong></span></li><li><strong class="source-inline"><st c="18975">Identify the key entities mentioned in the following summary: [summary from </st></strong><span class="No-Break"><strong class="source-inline"><st c="19052">previous prompt]</st></strong></span></li><li><strong class="source-inline"><st c="19068">Using the context and entities identified, answer the following question: [</st></strong><span class="No-Break"><strong class="source-inline"><st c="19144">specific question]</st></strong></span></li></ul></li>
				<li><strong class="bold"><st c="19163">Turn generative tasks into classification tasks</st></strong><st c="19211">: When possible, rephrase open-ended </st><a id="_idIndexMarker868"/><st c="19249">generative tasks as classification tasks with a limited set of options, as this can lead to more accurate and focused responses from the AI model. </st><st c="19396">For example, instead of saying </st><strong class="source-inline"><st c="19427">Based on the context, what is the sentiment expressed towards the topic?</st></strong><st c="19499">, you would likely have better results if you said </st><strong class="source-inline"><st c="19550">Based on the context, classify the sentiment expressed towards the topic as either positive, negative, </st></strong><span class="No-Break"><strong class="source-inline"><st c="19653">or neutral</st></strong></span><span class="No-Break"><st c="19663">.</st></span></li>
				<li><strong class="bold"><st c="19664">Improve response quality by including examples</st></strong><st c="19711">: We’ll build off the one-or-more-shot concept we talked about previously. </st><st c="19787">This fundamental concept focuses on providing relevant examples in your prompts to guide the AI model toward the desired output format and style. </st><st c="19933">As an example, rather than saying </st><strong class="source-inline"><st c="19967">Answer the following question based on the provided context</st></strong><st c="20026">, you would want to say something similar to </st><strong class="source-inline"><st c="20071">Using the examples below as a guide, answer the following question based on the provided context: Example 1: [question] [context] [answer] Example 2: [question] [context] [answer] Current question: [question] </st></strong><span class="No-Break"><strong class="source-inline"><st c="20280">Context: [context]</st></strong></span><span class="No-Break"><st c="20298">.</st></span></li>
				<li><strong class="bold"><st c="20299">Start simple and iterate gradually</st></strong><st c="20334">: Begin with simple prompts and gradually add more elements and context as needed to achieve better results. </st><st c="20444">This iterative approach allows you to fine-tune your prompts and optimize performance. </st><st c="20531">For example, the </st><strong class="source-inline"><st c="20548">Summarize the main points of the article, identify key entities, and answer the following question: [question]. </st><st c="20660">Provide examples and use the following format for your answer: [format]. </st><st c="20733">Article: [lengthy article text]</st></strong><st c="20764"> prompt will likely be less effective than prompting with multiple iterations such as </st><span class="No-Break"><st c="20850">the following:</st></span><ul><li><strong class="bold"><st c="20864">Iteration 1</st></strong><st c="20876">: </st><strong class="source-inline"><st c="20879">Summarize the main points of the following article: [</st></strong><span class="No-Break"><strong class="source-inline"><st c="20932">article text]</st></strong></span></li><li><strong class="bold"><st c="20946">Iteration 2</st></strong><st c="20958">: </st><strong class="source-inline"><st c="20961">Summarize the main points and identify key entities in the following article: [</st></strong><span class="No-Break"><strong class="source-inline"><st c="21040">article text]</st></strong></span></li><li><strong class="bold"><st c="21054">Iteration 3</st></strong><st c="21066">: </st><strong class="source-inline"><st c="21069">Based on the summary and key entities, answer the following question: [question] Article: [</st></strong><span class="No-Break"><strong class="source-inline"><st c="21160">article text]</st></strong></span></li></ul></li>
				<li><strong class="bold"><st c="21174">Place instructions at the beginning of the prompt</st></strong><st c="21224">: Clearly state the instructions at the start of the prompt, and use clear separators such as </st><strong class="source-inline"><st c="21319">###</st></strong><st c="21322"> to distinguish between the instruction and context sections. </st><st c="21384">This helps the AI model better understand and follow the given instructions. </st><st c="21461">For example, you will have less success with a prompt such as </st><strong class="source-inline"><st c="21523">[Context] Please use the above context to answer the following question: [question]. </st><st c="21608">Provide your answer in a concise manner</st></strong><st c="21647"> compared to this</st><a id="_idIndexMarker869"/><st c="21664"> prompt: </st><strong class="source-inline"><st c="21673">Instructions: Using the context provided below, answer the question in a concise manner. </st><st c="21762">Context: [context] </st></strong><span class="No-Break"><strong class="source-inline"><st c="21781">Question: [question]</st></strong></span><span class="No-Break"><st c="21801">.</st></span></li>
			</ul>
			<p><st c="21802">While the fundamentals of prompt design provide a solid foundation for creating effective prompts, it’s important to remember that different language models may require specific adaptations to achieve optimal results. </st><st c="22021">Let’s discuss that </st><span class="No-Break"><st c="22040">topic next.</st></span></p>
			<h1 id="_idParaDest-268"><a id="_idTextAnchor267"/><st c="22051">Adapting prompts for different LLMs</st></h1>
			<p><st c="22087">As the </st><a id="_idIndexMarker870"/><st c="22095">AI landscape evolves, people are no longer solely relying on</st><a id="_idIndexMarker871"/><st c="22155"> OpenAI for their language modeling needs. </st><st c="22198">Other players, such as Anthropic with their Claude models, have gained popularity due to their ability to handle long context windows. </st><st c="22333">Google is also releasing (and will continue to release) powerful models. </st><st c="22406">Moreover, the open source model community is catching up, with models such as Llama proving to be </st><span class="No-Break"><st c="22504">viable alternatives.</st></span></p>
			<p><st c="22524">However, it’s important to note that prompts do not seamlessly transfer from one LLM to another. </st><st c="22622">Each LLM may have specific tricks and techniques that work best for its architecture. </st><st c="22708">For example, Claude-3 prefers XML encoding when prompting, while Llama3 uses a specific syntax when labeling different parts of your prompt, such as SYS and INST. </st><st c="22871">Here is an example prompt for Llama models using the SYS and </st><span class="No-Break"><st c="22932">INST tags:</st></span></p>
			<ul>
				<li><strong class="source-inline"><st c="22942">&lt;SYS&gt; You are an AI assistant created to provide helpful and informative responses to user </st></strong><span class="No-Break"><strong class="source-inline"><st c="23034">questions. </st><st c="23045">&lt;/SYS&gt;</st></strong></span></li>
				<li><strong class="source-inline"><st c="23051">&lt;INST&gt; Analyze the user's question below and provide a clear, concise answer using your knowledge base. </st><st c="23156">If the question is unclear, ask </st></strong><span class="No-Break"><strong class="source-inline"><st c="23188">for clarification.</st></strong></span></li>
				<li><st c="23206">User’s question:</st><strong class="source-inline"><st c="23223"> "What are the main advantages of using renewable energy sources compared to fossil </st></strong><span class="No-Break"><strong class="source-inline"><st c="23307">fuels?" </st><st c="23315">&lt;/INST&gt;</st></strong></span></li>
			</ul>
			<p><st c="23322">In this example, the </st><strong class="source-inline"><st c="23344">SYS</st></strong><st c="23347"> tag briefly establishes the AI’s role as an assistant designed to offer helpful responses. </st><st c="23439">The </st><strong class="source-inline"><st c="23443">INST</st></strong><st c="23447"> tag provides specific instructions for answering the user’s question, which is included within the </st><strong class="source-inline"><st c="23547">INST</st></strong><st c="23551"> block. </st><strong class="bold"><st c="23559">SYS</st></strong><st c="23562"> is used as a shorthand for </st><strong class="bold"><st c="23590">system</st></strong><st c="23596"> or </st><strong class="bold"><st c="23600">system message</st></strong><st c="23614">, while </st><strong class="bold"><st c="23622">INST</st></strong><st c="23626"> is used in the place </st><span class="No-Break"><st c="23648">of </st></span><span class="No-Break"><strong class="bold"><st c="23651">instructions</st></strong></span><span class="No-Break"><st c="23663">.</st></span></p>
			<p><st c="23664">When </st><a id="_idIndexMarker872"/><st c="23670">designing prompts for RAG applications, it’s </st><a id="_idIndexMarker873"/><st c="23715">crucial to consider the specific requirements and best practices associated with the chosen LLM to ensure optimal performance and results. </st><st c="23854">All of the most well-known models have prompting documentation that can explain what you need to do if you </st><span class="No-Break"><st c="23961">use them.</st></span></p>
			<p><st c="23970">Now, let’s put all of the concepts we’ve covered in the first part of this chapter into practice with a </st><span class="No-Break"><st c="24075">code lab!</st></span></p>
			<h1 id="_idParaDest-269"><a id="_idTextAnchor268"/><st c="24084">Code lab 13.1 – Custom prompt template</st></h1>
			<p><st c="24123">The</st><a id="_idIndexMarker874"/><st c="24127"> Prompt template is a class representing the mechanism to manage and use prompts in LangChain. </st><st c="24222">As with most templates, there is text provided, as well as variables that represent inputs to the template. </st><st c="24330">Using the </st><strong class="source-inline"><st c="24340">PromptTemplate</st></strong><st c="24354"> package to manage your prompts ensures that it works well within the LangChain ecosystem. </st><st c="24445">This code builds off the code we completed in </st><a href="B22475_08.xhtml#_idTextAnchor152"><span class="No-Break"><em class="italic"><st c="24491">Chapter 8</st></em></span></a><st c="24500">’s </st><em class="italic"><st c="24504">8.3 code lab</st></em><st c="24516">, and can be found in the </st><strong class="source-inline"><st c="24542">CHAPTER13</st></strong><st c="24551"> directory of the GitHub repo </st><span class="No-Break"><st c="24581">as</st></span><span class="No-Break"><strong class="source-inline"><st c="24583"> CHAPTER13-1_PROMPT_TEMPLATES.ipynb</st></strong></span><span class="No-Break"><st c="24618">.</st></span></p>
			<p><st c="24619">As a refresher, this is the template we have used </st><span class="No-Break"><st c="24670">the most:</st></span></p>
			<pre class="source-code"><st c="24679">
prompt = hub.pull("jclemens24/rag-prompt")</st></pre>
			<p><st c="24722">Printing this prompt out looks </st><span class="No-Break"><st c="24754">like this:</st></span></p>
			<pre class="source-code"><st c="24764">
You are an assistant for question-answering tasks. </st><st c="24816">Use the following pieces of retrieved context to answer the question. </st><st c="24886">If you don't know the answer, just say that you don't know.
</st><st c="24946">Question: {question}
Context: {context}
Answer:</st></pre>
			<p><st c="24993">This is stored in the </st><strong class="source-inline"><st c="25016">PromptTemplate</st></strong><st c="25030"> object that you can print out. </st><st c="25062">If you do, you will see something </st><span class="No-Break"><st c="25096">like this:</st></span></p>
			<pre class="source-code"><st c="25106">
ChatPromptTemplate(input_variables=['context', 'question'], metadata={'lc_hub_owner': 'jclemens24', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '1a1f3ccb9a5a92363310e3b130843dfb2540239366ebe712ddd94982acc06734'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. </st><st c="25488">Use the following pieces of retrieved context to answer the question. </st><st c="25558">If you don't know the answer, just say that you don't know.\nQuestion: {question} \nContext: {context} \nAnswer:"))])</st></pre>
			<p><st c="25675">So, as we can </st><a id="_idIndexMarker875"/><st c="25690">see here, there is more to the full </st><strong class="source-inline"><st c="25726">PromptTemplate</st></strong><st c="25740"> object than just the text and variables. </st><st c="25782">First, we can say that this is a specific version of the </st><strong class="source-inline"><st c="25839">PromptTemplate</st></strong><st c="25853"> object called a </st><strong class="source-inline"><st c="25870">ChatPromptTemplate</st></strong><st c="25888"> object, which suggests that it is designed to be most useful in chat scenarios. </st><st c="25969">The input variables are </st><strong class="source-inline"><st c="25993">context</st></strong><st c="26000"> and </st><strong class="source-inline"><st c="26005">question</st></strong><st c="26013">, which show up later in the template string itself. </st><st c="26066">In a moment, we will design a custom template, but this particular template comes from the LangChain hub. </st><st c="26172">You can see metadata here indicating the owner, repo, and commit hash associated with </st><span class="No-Break"><st c="26258">the hub.</st></span></p>
			<p><st c="26266">Let’s start our code lab by replacing this prompt template with our own </st><span class="No-Break"><st c="26339">customized template.</st></span></p>
			<p><st c="26359">We are already importing the LangChain package </st><span class="No-Break"><st c="26407">for this:</st></span></p>
			<pre class="source-code"><st c="26416">
from langchain_core.prompts import PromptTemplate</st></pre>
			<p><st c="26466">There is no need to add any more imports for this! </st><st c="26518">We are going to replace </st><span class="No-Break"><st c="26542">this code:</st></span></p>
			<pre class="source-code"><st c="26552">
prompt = hub.pull("jclemens24/rag-prompt")</st></pre>
			<p><st c="26595">Here is the code </st><a id="_idIndexMarker876"/><st c="26613">we will be replacing </st><span class="No-Break"><st c="26634">it with:</st></span></p>
			<pre class="source-code"><st c="26642">
prompt = PromptTemplate.from_template(
    """
    You are an environment expert assisting others in
    understanding what large companies are doing to
    improve the environment. </st><st c="26809">Use the following pieces
    of retrieved context with information about what
    a particular company is doing to improve the
    environment to answer the question.
    </st><st c="26964">If you don't know the answer, just say that you don't know.
    </st><st c="27024">Question: {question}
    Context: {context}
    Answer:"""
)</st></pre>
			<p><st c="27076">As you can see, we have customized this prompt template to focus specifically on the topic that our data (the Google environmental report) is focused on. </st><st c="27231">We use a personas prompt design pattern to establish a role that we want the LLM to play, which we hope will make it more in tune with our </st><span class="No-Break"><st c="27370">specific topic(s).</st></span></p>
			<p><st c="27388">Prompt templates take a dictionary as input, where each key represents a variable in the prompt template to fill in. </st><st c="27506">The output from a </st><strong class="source-inline"><st c="27524">PromptTemplate</st></strong><st c="27538"> object is a </st><strong class="source-inline"><st c="27551">PromptValue</st></strong><st c="27562"> variable, which can be passed to an LLM or ChatModel instance either directly or as a step in an </st><span class="No-Break"><st c="27660">LCEL chain.</st></span></p>
			<p><st c="27671">Print out the prompt object using </st><span class="No-Break"><st c="27706">this code:</st></span></p>
			<pre class="source-code"><st c="27716">
print(prompt)</st></pre>
			<p><st c="27730">The output will </st><a id="_idIndexMarker877"/><st c="27747">be </st><span class="No-Break"><st c="27750">as follows:</st></span></p>
			<pre class="source-code"><st c="27761">
input_variables=['context', 'question'] template="\n    You are an environment expert assisting others in \n    understanding what large companies are doing to \n    improve the environment. </st><st c="27944">Use the following pieces \n    of retrieved context with information about what \n    a particular company is doing to improve the \n    environment to answer the question. </st><st c="28108">\n    \n    If you don't know the answer, just say that you don't know.\n    \n    Question: {question} \n    Context: {context} \n    \n    Answer:"</st></pre>
			<p><st c="28236">We see that it has captured the input variables without us having to explicitly </st><span class="No-Break"><st c="28317">state them:</st></span></p>
			<pre class="source-code"><st c="28328">
input_variables=['context', 'question']</st></pre>
			<p><st c="28368">We can print out just the text using </st><span class="No-Break"><st c="28406">this line:</st></span></p>
			<pre class="source-code"><st c="28416">
print(prompt.template)</st></pre>
			<p><st c="28439">This gives you a more human-readable version of the last output we showed you, but just the </st><span class="No-Break"><st c="28532">prompt itself.</st></span></p>
			<p><st c="28546">You will notice that just below this, we already have a customized prompt template focused on determining our </st><span class="No-Break"><st c="28657">relevance score:</st></span></p>
			<pre class="source-code"><st c="28673">
relevance_prompt_template = PromptTemplate.from_template(
    """
    Given the following question and retrieved context, determine if the context is relevant to the question.
    </st><st c="28842">Provide a score from 1 to 5, where 1 is not at all relevant and 5 is highly relevant.
    </st><st c="28928">Return ONLY the numeric score, without any additional text or explanation.
    </st><st c="29003">Question: {question}
    Retrieved Context: {retrieved_context}
    Relevance Score:"""
)</st></pre>
			<p><st c="29084">If you run the remainder of the code, you can see the difference it makes to the final outcome of the RAG application. </st><st c="29204">This prompt template is considered a String prompt template, meaning it is created using a plain string that contains the prompt text along with placeholders for dynamic content (e.g., </st><strong class="source-inline"><st c="29389">{question}</st></strong><st c="29399"> and </st><strong class="source-inline"><st c="29404">{retrieved_context}</st></strong><st c="29423">). </st><st c="29427">You can also format with the </st><strong class="source-inline"><st c="29456">ChatPromptTemplate</st></strong><st c="29474"> object, which is used to format a list of messages. </st><st c="29527">It consists of a list of </st><span class="No-Break"><st c="29552">templates itself.</st></span></p>
			<p><st c="29569">Prompt templates </st><a id="_idIndexMarker878"/><st c="29587">play a key supporting role in maximizing the performance of RAG systems. </st><st c="29660">We will use prompt templates as the primary element in the remaining code labs in this chapter. </st><st c="29756">However, we will now shift our focus to a series of concepts to keep in mind when we write our prompts. </st><st c="29860">Our next code lab focuses on all of these concepts, starting with </st><span class="No-Break"><st c="29926">prompt formatting.</st></span></p>
			<h1 id="_idParaDest-270"><a id="_idTextAnchor269"/><st c="29944">Code lab 13.2 – Prompting options</st></h1>
			<p><st c="29978">This code can be found in</st><a id="_idIndexMarker879"/><st c="30004"> the </st><strong class="source-inline"><st c="30009">CHAPTER13-2_PROMPT_OPTIONS.ipynb</st></strong><st c="30041"> file in the </st><strong class="source-inline"><st c="30054">CHAPTER13</st></strong><st c="30063"> directory of the </st><span class="No-Break"><st c="30081">GitHub repository.</st></span></p>
			<p><st c="30099">Generally, when you approach the design of your prompt, there are a variety of general concepts that you will want to keep in mind. </st><st c="30232">These include iterating, summarizing, transforming, and expanding. </st><st c="30299">Each of these concepts has different use cases and they can all often be combined in various ways. </st><st c="30398">You will find it useful, when improving your RAG applications, to have this foundational knowledge of how to design your prompts. </st><st c="30528">We will walk through different prompt approaches using a real-world scenario wherein you are helping to write prompts for the marketing department at your company. </st><st c="30692">We will start </st><span class="No-Break"><st c="30706">with iterating.</st></span></p>
			<h2 id="_idParaDest-271"><a id="_idTextAnchor270"/><st c="30721">Iterating</st></h2>
			<p><st c="30731">This concept is simply focused on iterating</st><a id="_idIndexMarker880"/><st c="30775"> your prompt to get better results. </st><st c="30811">It is rare that your first prompt will be the best and final prompt you could use. </st><st c="30894">This section focuses on some basic techniques and concepts to keep in mind to help you quickly iterate your prompt to make it more suitable for your </st><span class="No-Break"><st c="31043">RAG application.</st></span></p>
			<h2 id="_idParaDest-272"><a id="_idTextAnchor271"/><st c="31059">Iterating the tone</st></h2>
			<p><st c="31078">Your boss just called. </st><st c="31102">They have told </st><a id="_idIndexMarker881"/><st c="31117">you that the marketing people are saying that they want to use the output from the RAG application in their marketing materials, but that it has to be provided in more of a marketing fact sheet format. </st><st c="31319">No problem; we can prompt design that right </st><span class="No-Break"><st c="31363">in there!</st></span></p>
			<p><st c="31372">We will add a second prompt after the </st><span class="No-Break"><st c="31411">first prompt:</st></span></p>
			<pre class="source-code"><st c="31424">
prompt2 = PromptTemplate.from_template(
    """Your task is to help a marketing team create a
    description for the website about the environmental
    initiatives our clients are promoting.
    </st><st c="31606">Write a marketing description based on the information
    provided in the context delimited by triple backticks.
    </st><st c="31716">If you don't know the answer, just say that you don't know.
    </st><st c="31776">Question: {question}
    Context: ```{context}```
    Answer:"""
)</st></pre>
			<p><st c="31834">You then need to change the prompt in the </st><strong class="source-inline"><st c="31877">rag_chain_from_docs</st></strong><st c="31896"> chain to </st><strong class="source-inline"><st c="31906">prompt2</st></strong><st c="31913">. Look just past the </st><span class="No-Break"><strong class="source-inline"><st c="31934">RunnablePassthrough()</st></strong></span><span class="No-Break"><st c="31955"> line:</st></span></p>
			<pre class="source-code"><st c="31961">
rag_chain_from_docs = (
    …
             "answer": (
                RunnablePassthrough()
                | prompt2 </st><strong class="bold"><st c="32032"># &lt;- update here</st></strong><st c="32048">
                | llm
                | str_output_parser
            )
        …
)</st></pre>
			<p><st c="32080">Then, rerun the </st><a id="_idIndexMarker882"/><st c="32097">code from </st><strong class="source-inline"><st c="32107">prompt2</st></strong><st c="32114"> down for </st><span class="No-Break"><st c="32124">this result:</st></span></p>
			<pre class="source-code"><st c="32136">
Google is at the forefront of environmental sustainability, leveraging its technological prowess to drive impactful initiatives across various domains. </st><st c="32289">Here are some of the key environmental initiatives that Google is promoting:
</st><strong class="bold"><st c="32366">Empowering Individuals</st></strong><st c="32388">: Google aims to help individuals make more sustainable choices through its products. </st><st c="32475">In 2022, Google reached its goal of assisting 1 billion people in making eco-friendly decisions. </st><st c="32572">This was achieved through features like eco-friendly routing in Google Maps, energy-efficient settings in Google Nest thermostats, and carbon emissions information in Google Flights. </st><st c="32755">By 2030, Google aspires to help reduce 1 gigaton of carbon equivalent emissions annually.
</st><strong class="bold"><st c="32845">…[TRUNCATED FOR BREVITY]…</st></strong><st c="32870">
By organizing information about the planet and making it actionable through technology, Google is helping to create a more sustainable future. </st><st c="33014">The company's efforts span from individual empowerment to global partnerships, all aimed at reducing environmental impact and fostering a healthier planet.</st></pre>
			<p><st c="33169">If you read</st><a id="_idIndexMarker883"/><st c="33181"> through the complete output, you will notice that this does seem more marketing-oriented. </st><st c="33272">This may be what the marketing team is looking for. </st><st c="33324">However, you just remembered that your boss also said that this was going to go in a small square on the website that can only hold 50 words </st><span class="No-Break"><st c="33465">at most!</st></span></p>
			<h2 id="_idParaDest-273"><a id="_idTextAnchor272"/><st c="33473">Shorten the length</st></h2>
			<p><st c="33492">For </st><strong class="source-inline"><st c="33497">prompt3</st></strong><st c="33504">, we just need to add </st><a id="_idIndexMarker884"/><st c="33526">this little snippet: </st><strong class="source-inline"><st c="33547">Use at most 50 words</st></strong><st c="33567">. That looks </st><span class="No-Break"><st c="33580">like this:</st></span></p>
			<pre class="source-code"><st c="33590">
prompt3 = PromptTemplate.from_template(
    """Your task is to help a marketing team create a
    description for the website about the environmental
    initiatives our clients are promoting.
    </st><st c="33772">Write a marketing description based on the information
    provided in the context delimited by triple backticks.
    </st><st c="33882">If you don't know the answer, just say that you don't know.
    </st><st c="33942">Use at most 50 words.
    </st><st c="33964">Question: {question}
    Context: ```{context}```
    Answer:"""
)</st></pre>
			<p><st c="34022">Update the prompt in the chain to </st><strong class="source-inline"><st c="34057">prompt3</st></strong><st c="34064">. Run the remaining code, and you will have </st><span class="No-Break"><st c="34108">this output:</st></span></p>
			<pre class="source-code"><st c="34120">
Google's environmental initiatives include promoting electric vehicles, sustainable agriculture, net-zero carbon operations, water stewardship, and a circular economy. </st><st c="34289">They aim to help individuals and partners reduce carbon emissions, optimize resource use, and support climate action through technology and data-driven solutions.</st></pre>
			<p><st c="34451">The marketing team</st><a id="_idIndexMarker885"/><st c="34470"> loves your work! </st><st c="34488">Things are great! </st><st c="34506">Good job! </st><st c="34516">Time </st><span class="No-Break"><st c="34521">goes by…</st></span></p>
			<p><st c="34529">A month later, it is decided that instead of focusing on all your client’s environmental efforts, it would be best to focus on the </st><span class="No-Break"><st c="34661">technology-related efforts.</st></span></p>
			<h2 id="_idParaDest-274"><a id="_idTextAnchor273"/><st c="34688">Changing the focus</st></h2>
			<p><st c="34707">We want the LLM to focus on aspects </st><a id="_idIndexMarker886"/><st c="34744">that are more specific to technology. </st><st c="34782">This is for a pamphlet, so it can be longer again. </st><st c="34833">Let’s design </st><strong class="source-inline"><st c="34846">prompt4</st></strong><st c="34853"> to direct the LLM to change its focus and we will take out the </st><span class="No-Break"><st c="34917">length limitation:</st></span></p>
			<pre class="source-code"><st c="34935">
prompt4 = PromptTemplate.from_template(
    """Your task is to help a marketing team create a
    description for the website about the environmental
    initiatives our clients are promoting.
    </st><st c="35117">Write a marketing description based on the information
    provided in the context delimited by triple backticks.
    </st><st c="35227">The description is intended for a technology audience,
    so this should focus on only the aspects of the
    company's efforts that relate to using technology. </st><st c="35381">If
    you don't know the answer, just say that you don't
    know.
    </st><st c="35441">Question: {question}
    Context: ```{context}```
    Answer:"""
)</st></pre>
			<p><st c="35499">Again, remember</st><a id="_idIndexMarker887"/><st c="35515"> to update the prompt in the chain to </st><strong class="source-inline"><st c="35553">prompt4</st></strong><st c="35560"> and then run the rest of the code to get </st><span class="No-Break"><st c="35602">the output:</st></span></p>
			<pre class="source-code"><st c="35613">
Google is at the forefront of leveraging technology to drive environmental sustainability. </st><st c="35705">Here are some of the key initiatives that highlight their commitment to using technology for a greener future:
</st><strong class="bold"><st c="35816">Eco-Friendly Product Features</st></strong><st c="35845">:
</st><strong class="bold"><st c="35848">Google Maps</st></strong><st c="35859">: Introduced eco-friendly routing, which has helped prevent over 1.2 million metric tons of carbon emissions since its launch.
</st><strong class="bold"><st c="35987">Google Nest</st></strong><st c="35998">: Energy efficiency features in Google Nest thermostats help users reduce their energy consumption.
</st><strong class="bold"><st c="36099">Google Flights</st></strong><st c="36113">: Provides carbon emissions information to help travelers make more sustainable choices.
</st><strong class="bold"><st c="36203">…TRUNCATED FOR BREVITY…</st></strong>
<strong class="bold"><st c="36226">Sustainability-Focused Accelerators</st></strong><st c="36262">:
</st><strong class="bold"><st c="36265">Google for Startups Accelerator</st></strong><st c="36296">: Supports early-stage innovations aimed at tackling sustainability challenges, fostering the growth of technologies that can positively impact the planet.
</st><st c="36453">Google's comprehensive approach to environmental sustainability leverages their technological expertise to create significant positive impacts. </st><st c="36597">By integrating sustainability features into their products, optimizing their operations, and collaborating with partners, Google is driving forward a more sustainable future.</st></pre>
			<p><st c="36771">Again, we had to shorten it here in the book, but if you look at this in the code, the results are impressive. </st><st c="36883">There is clearly a higher focus on the technological aspects of their environmental aspects. </st><st c="36976">Your marketing team </st><span class="No-Break"><st c="36996">is impressed!</st></span></p>
			<p><st c="37009">This was a fun example, but this is not too far from what happens when building these types of systems. </st><st c="37114">In the real world, you will likely iterate significantly more times, but taking an iterative approach to your prompt design will help you get to a more optimal RAG application just as much as any other part of your </st><span class="No-Break"><st c="37329">RAG system.</st></span></p>
			<p><st c="37340">Next, let’s </st><a id="_idIndexMarker888"/><st c="37353">talk about how to take a lot of data and compact it into a much smaller amount, also known </st><span class="No-Break"><st c="37444">as summarization.</st></span></p>
			<h2 id="_idParaDest-275"><a id="_idTextAnchor274"/><st c="37461">Summarizing</st></h2>
			<p><st c="37473">Summarization</st><a id="_idIndexMarker889"/><st c="37487"> is a popular use of RAG. </st><st c="37513">Taking massive amounts </st><a id="_idIndexMarker890"/><st c="37536">of data that is internal to a company and digesting it into smaller, more concise information can be a quick and easy way to boost productivity. </st><st c="37681">This is particularly useful for jobs that rely on information or keep up with rapidly changing information. </st><st c="37789">We’ve already seen how to design a prompt to use a word limit, which was in </st><strong class="source-inline"><st c="37865">prompt3</st></strong><st c="37872">. However, in this case, we are going to focus the LLM more on summarizing the content, rather than trying to be an expert or write a marketing piece. </st><st c="38023">The code is </st><span class="No-Break"><st c="38035">as follows:</st></span></p>
			<pre class="source-code"><st c="38046">
prompt5 = PromptTemplate.from_template(
    """Your task is to generate a short summary of what a
    company is doing to improve the environment.
    </st><st c="38186">Summarize the retrieved context below, delimited by
    triple backticks, in at most 30 words.
    </st><st c="38277">If you don't know the answer, just say that you don't
    know.
    </st><st c="38337">Question: {question}
    Context: ```{context}```
    Answer:"""
)</st></pre>
			<p><st c="38395">Update to </st><strong class="source-inline"><st c="38406">prompt5</st></strong><st c="38413"> in the chain, and then run the rest of the code again. </st><st c="38469">The results are </st><span class="No-Break"><st c="38485">as follows:</st></span></p>
			<pre class="source-code"><st c="38496">
Google's environmental initiatives include achieving net-zero carbon, promoting water stewardship, supporting a circular economy, and leveraging technology to help partners reduce emissions.</st></pre>
			<p><st c="38687">OK, this is great, short, and summarizing. </st><st c="38731">Nothing but </st><span class="No-Break"><st c="38743">the facts!</st></span></p>
			<p><st c="38753">The next example is</st><a id="_idIndexMarker891"/><st c="38773"> another situation where we can focus the LLM, but with the added effort </st><span class="No-Break"><st c="38846">of summarizing.</st></span></p>
			<h2 id="_idParaDest-276"><a id="_idTextAnchor275"/><st c="38861">Summarizing with a focus</st></h2>
			<p><st c="38886">For </st><strong class="source-inline"><st c="38891">prompt6</st></strong><st c="38898">, we are going to</st><a id="_idIndexMarker892"/><st c="38915"> maintain most of what we had in the previous prompt. </st><st c="38969">However, we will try to focus the LLM specifically on the eco-friendly aspects of </st><span class="No-Break"><st c="39051">their products:</st></span></p>
			<pre class="source-code"><st c="39066">
prompt6 = PromptTemplate.from_template(
    """Your task is to generate a short summary of what a
    company is doing to improve the environment.
    </st><st c="39206">Summarize the retrieved context below, delimited by
    triple backticks, in at most 30 words, and focusing
    on any aspects that mention the eco-friendliness of
    their products.
    </st><st c="39378">If you don't know the answer, just say that you don't
    know.
    </st><st c="39438">Question: {question}
    Context: ```{context}```
    Answer:"""
)</st></pre>
			<p><st c="39496">Update the prompt in the chain to </st><strong class="source-inline"><st c="39531">prompt6</st></strong><st c="39538">, and then run the code to get </st><span class="No-Break"><st c="39569">this output:</st></span></p>
			<pre class="source-code"><st c="39581">
Google's environmental initiatives include eco-friendly routing in Google Maps, energy-efficient Google Nest thermostats, and carbon emissions information in Google Flights.</st></pre>
			<p><st c="39755">This is short, and if you check it against the more verbose descriptions, it does seem to focus specifically on the products that were featured in the PDF. </st><st c="39912">This was a pretty good result, but often when you ask for a summary, even when you focus the LLM on specific aspects, the LLM can still include information you did not want to be included. </st><st c="40101">To combat this, we</st><a id="_idIndexMarker893"/><st c="40119"> turn to the </st><span class="No-Break"><em class="italic"><st c="40132">extract</st></em></span><span class="No-Break"><st c="40139"> method.</st></span></p>
			<h2 id="_idParaDest-277"><a id="_idTextAnchor276"/><st c="40147">extract instead of summarize</st></h2>
			<p><st c="40176">If you run into the common problem with the</st><a id="_idIndexMarker894"/><st c="40220"> summary including too much unwanted information, try using the word </st><em class="italic"><st c="40289">extract</st></em><st c="40296"> rather than </st><em class="italic"><st c="40309">summarize</st></em><st c="40318">. This may seem like a small bit of nuance, but it can make a big difference to the LLM. </st><em class="italic"><st c="40407">Extract</st></em><st c="40414"> gives the impression that you are pulling specific information out, rather than just trying to capture the overall data in the entire text. </st><st c="40555">The LLM does not miss this nuance, and this can be a good technique to help you avoid this challenge that summarization sometimes poses. </st><st c="40692">We will design </st><strong class="source-inline"><st c="40707">prompt7</st></strong><st c="40714"> with this change </st><span class="No-Break"><st c="40732">in mind:</st></span></p>
			<pre class="source-code"><st c="40740">
prompt7 = PromptTemplate.from_template(
    """Your task is to generate a short summary of what a
    company is doing to improve the environment.
    </st><st c="40880">From the retrieved context below, delimited by
    triple backticks, extract the information focusing
    on any aspects that mention the eco-friendliness of
    their products. </st><st c="41046">Limit to 30 words.
    </st><st c="41065">If you don't know the answer, just say that you don't
    know.
    </st><st c="41125">Question: {question}
    Context: ```{context}```
    Answer:"""
)</st></pre>
			<p><st c="41183">Update the prompt in the chain to </st><strong class="source-inline"><st c="41218">prompt7</st></strong><st c="41225">, and then run the code to get </st><span class="No-Break"><st c="41256">this output:</st></span></p>
			<pre class="source-code"><st c="41268">
Google's environmental initiatives include eco-friendly routing in Google Maps, energy efficiency features in Google Nest thermostats, and carbon emissions information in Google Flights to help users make sustainable choices.</st></pre>
			<p><st c="41494">This is a slightly different response compared to </st><strong class="source-inline"><st c="41545">prompt6</st></strong><st c="41552">, but we already had a well-focused result. </st><st c="41596">When your summary responses have unnecessary data, try this technique to help improve </st><span class="No-Break"><st c="41682">your results.</st></span></p>
			<p><st c="41695">Iterating and</st><a id="_idIndexMarker895"/><st c="41709"> summarization are not the only concepts to understand to improve your prompting efforts, however. </st><st c="41808">We will next talk about how to utilize your RAG application to infer information from your </st><span class="No-Break"><st c="41899">existing data.</st></span></p>
			<h2 id="_idParaDest-278"><a id="_idTextAnchor277"/><st c="41913">Inference</st></h2>
			<p><st c="41923">At the root of inference, you are</st><a id="_idIndexMarker896"/><st c="41957"> asking the model to look at your data and provide some sort of additional analysis. </st><st c="42042">This often involves extracting labels, names, and topics, or even determining the sentiment of the text. </st><st c="42147">These capabilities have far-reaching implications for RAG applications in that they enable tasks that were considered to solely be in the domain of human readers not too long ago. </st><st c="42327">Let’s start with a simple Boolean-style sentiment analysis, wherein we consider whether a text is positive </st><span class="No-Break"><st c="42434">or negative:</st></span></p>
			<pre class="source-code"><st c="42446">
prompt8 = PromptTemplate.from_template(
    """Your task is to generate a short summary of what a
    company is doing to improve the environment.
    </st><st c="42586">From the retrieved context below, delimited by
    triple backticks, extract the information focusing
    on any aspects that mention the eco-friendliness of
    their products. </st><st c="42752">Limit to 30 words.
    </st><st c="42771">After this summary, determine what the sentiment
    of context is, providing your answer as a single word,
    either "positive" or "negative". </st><st c="42908">If you don't know the
    answer, just say that you don't know.
    </st><st c="42968">Question: {question}
    Context: ```{context}```
    Answer:"""
)</st></pre>
			<p><st c="43026">In this code, we build off the summary from the previous prompt but add an </st><em class="italic"><st c="43102">analysis</st></em><st c="43110"> of the sentiment of the data that the LLM is digesting as well. </st><st c="43175">In this case, it determines the sentiment to </st><span class="No-Break"><st c="43220">be</st></span><span class="No-Break"><a id="_idIndexMarker897"/></span><span class="No-Break"><st c="43222"> positive:</st></span></p>
			<pre class="source-code"><st c="43232">
Google is enhancing eco-friendliness through features like eco-friendly routing in Maps, energy-efficient Nest thermostats, and carbon emissions data in Flights, aiming to reduce emissions significantly.
</st><st c="43437">Sentiment: positive</st></pre>
			<p><st c="43456">Another common application in a similar analytic vane as this is extracting specific data from </st><span class="No-Break"><st c="43552">the context.</st></span></p>
			<h2 id="_idParaDest-279"><a id="_idTextAnchor278"/><st c="43564">Extracting key data</st></h2>
			<p><st c="43584">As a reference point, you are now tasked to</st><a id="_idIndexMarker898"/><st c="43628"> identify any specific products your customer mentions in their documentation in relation to their environmental efforts. </st><st c="43750">In this case, Google (the client) has many products, but they only mention a handful of them in this document. </st><st c="43861">How would you quickly pull those products out and identify them? </st><st c="43926">Let’s try this with </st><span class="No-Break"><st c="43946">our prompt:</st></span></p>
			<pre class="source-code"><st c="43957">
prompt9 = PromptTemplate.from_template(
    """Your task is to generate a short summary of what a
    company is doing to improve the environment.
    </st><st c="44097">From the retrieved context below, delimited by
    triple backticks, extract the information focusing
    on any aspects that mention the eco-friendliness of
    their products. </st><st c="44263">Limit to 30 words.
    </st><st c="44282">After this summary, determine any specific products
    that are identified in the context below, delimited
    by triple backticks.  </st><st c="44407">Indicate that this is a list
    of related products with the words 'Related products: '
    and then list those product names after those words.
    </st><st c="44545">If you don't know the answer, just say that you don't
    know.
    </st><st c="44605">Question: {question}
    Context: ```{context}```
    Answer:"""
)</st></pre>
			<p><st c="44663">In this code, we continue to build off previous prompts, but instead of asking for a sentiment analysis, we are asking for the list of products related to the text we retrieved. </st><st c="44842">The GPT-4o-mini model we are using is successful in following these instructions, listing each </st><a id="_idIndexMarker899"/><st c="44937">of the products specifically named in </st><span class="No-Break"><st c="44975">the text:</st></span></p>
			<pre class="source-code"><st c="44984">
Google is enhancing eco-friendliness through products like eco-friendly routing in Google Maps, energy efficiency features in Google Nest thermostats, and carbon emissions information in Google Flights.
</st><st c="45188">Related products: Google Maps, Google Nest thermostats, Google Flights</st></pre>
			<p><st c="45258">Once again, the LLM is able to handle everything we ask of it. </st><st c="45322">However, sometimes we just want to get an overall sense of the topic. </st><st c="45392">We will discuss the concept of inference using the </st><span class="No-Break"><st c="45443">LLM next.</st></span></p>
			<h2 id="_idParaDest-280"><a id="_idTextAnchor279"/><st c="45452">Inferring topics</st></h2>
			<p><st c="45469">You might think of this as an extreme </st><a id="_idIndexMarker900"/><st c="45508">case of summarization. </st><st c="45531">In this example, we are taking thousands of words and summarizing them into one short set of topics. </st><st c="45632">Can this be done? </st><st c="45650">Let’s try! </st><st c="45661">We’ll be starting with </st><span class="No-Break"><st c="45684">this code:</st></span></p>
			<pre class="source-code"><st c="45694">
prompt10 = PromptTemplate.from_template(
    """Your task is to generate a short summary of what a
    company is doing to improve the environment.
    </st><st c="45835">From the retrieved context below, delimited by
    triple backticks, extract the information focusing
    on any aspects that mention the eco-friendliness of
    their products. </st><st c="46001">Limit to 30 words.
    </st><st c="46020">After this summary, determine eight topics that are
    being discussed in the context below delimited
    by triple backticks.
    </st><st c="46140">Make each item one or two words long.
    </st><st c="46178">Indicate that this is a list of related topics
    with the words 'Related topics: '
    and then list those topics after those words.
    </st><st c="46305">If you don't know the answer, just say that you don't
    know.
    </st><st c="46365">Question: {question}
    Context: ```{context}```
    Answer:"""
)</st></pre>
			<p><st c="46423">Here, we use a similar approach as in previous prompts, but instead of asking for the product list, we are asking for the list of at least eight topics related to the text we retrieved. </st><st c="46610">Once again, the GPT-4o mini model we use is successful in following these instructions, listing eight</st><a id="_idIndexMarker901"/><st c="46711"> highly relevant topics specifically covered in </st><span class="No-Break"><st c="46759">the text:</st></span></p>
			<pre class="source-code"><st c="46768">
Google is enhancing eco-friendliness through products like eco-friendly routing in Google Maps, energy-efficient Google Nest thermostats, and carbon emissions information in Google Flights.
</st><st c="46959">Related topics:
1. </st><st c="46978">Electric vehicles
2. </st><st c="46999">Net-zero carbon
3. </st><st c="47018">Water stewardship
4. </st><st c="47039">Circular economy
5. </st><st c="47059">Supplier engagement
6. </st><st c="47082">Climate resilience
7. </st><st c="47104">Renewable energy
8. </st><st c="47124">AI for sustainability</st></pre>
			<p><st c="47145">We have covered iterating, summarization, and inference, which all show great promise for improving your prompting efforts. </st><st c="47270">Yet another concept we will cover </st><span class="No-Break"><st c="47304">is transformation.</st></span></p>
			<h2 id="_idParaDest-281"><a id="_idTextAnchor280"/><st c="47322">Transformation</st></h2>
			<p><st c="47337">Transformation</st><a id="_idIndexMarker902"/><st c="47352"> is about taking your current data and transforming</st><a id="_idIndexMarker903"/><st c="47403"> it into a different state or format. </st><st c="47441">A very common example is language translation, but there are many others, including putting data into a certain coding format such as JSON or HTML. </st><st c="47589">You can also apply transformations such as checking for spelling or </st><span class="No-Break"><st c="47657">grammar errors.</st></span></p>
			<p><st c="47672">We will begin with </st><span class="No-Break"><st c="47692">language translation.</st></span></p>
			<h3><st c="47713">Language transformation (translation)</st></h3>
			<p><st c="47751">Marketing called</st><a id="_idIndexMarker904"/><st c="47768"> again. </st><st c="47776">The work you have done so far has been stellar but now things are ramping up and we are going international! </st><st c="47885">The first international markets we have chosen include speakers of Spanish and French. </st><st c="47972">A new investor in our firm is also a big fan of anything that has to do with pirates, so yes, we are going to cover that dialect as well! </st><st c="48110">Since we are talking about </st><em class="italic"><st c="48137">transformations</st></em><st c="48152">, we call this </st><a id="_idIndexMarker905"/><st c="48167">language transformation, but it is also very common to see the term </st><em class="italic"><st c="48235">translation</st></em><st c="48246"> used in this context. </st><st c="48269">Let’s </st><span class="No-Break"><st c="48275">get started:</st></span></p>
			<pre class="source-code"><st c="48287">
prompt11 = PromptTemplate.from_template(
    """Your task is to generate a short summary of what a
    company is doing to improve the environment.
    </st><st c="48428">From the retrieved context below, delimited by
    triple backticks, extract the information focusing
    on any aspects that mention the eco-friendliness of
    their products. </st><st c="48594">Limit to 30 words.
    </st><st c="48613">Translate the summary into three additional languages,
    Spanish, French, and English Pirate:
    labeling each language with a format like this:
    English: [summary]
    Spanish: [summary]
    French: [summary]
    English pirate: [summary]
    If you don't know the answer, just say that you don't
    know.
    </st><st c="48895">Question: {question}
    Context: ```{context}```
    Answer:"""
)</st></pre>
			<p><st c="48953">In this code, we build </st><a id="_idIndexMarker906"/><st c="48977">off former prompts, but then task the LLM to generate four different versions of the short summary: English, Spanish, French, and English pirate. </st><st c="49123">Clearly, we should be speaking in pirate more often, as this is</st><a id="_idIndexMarker907"/><st c="49186"> the </st><span class="No-Break"><st c="49191">most entertaining:</st></span></p>
			<pre class="source-code"><st c="49209">
English: Google enhances eco-friendliness through features like eco-friendly routing in Maps, energy-efficient Nest thermostats, and carbon emissions info in Flights, helping reduce carbon emissions significantly.
</st><st c="49424">Spanish: Google mejora la eco-amigabilidad con funciones como rutas ecológicas en Maps, termostatos Nest eficientes en energía e información de emisiones de carbono en Flights, ayudando a reducir significativamente las emisiones de carbono.
</st><st c="49665">French: Google améliore l'éco-responsabilité avec des fonctionnalités telles que les itinéraires écologiques dans Maps, les thermostats Nest économes en énergie et les informations sur les émissions de carbone dans Flights, aidant à réduire significativement les émissions de carbone.
</st><st c="49950">English pirate: Google be makin' things greener with eco-routes in Maps, energy-savin' Nest thermostats, and carbon info in Flights, helpin' to cut down on carbon emissions mightily.</st></pre>
			<p><st c="50132">Language translation is a popular use of RAG, but there are other transformations that could be of use. </st><st c="50237">Let’s review an example of adding a different tone to </st><span class="No-Break"><st c="50291">a summary.</st></span></p>
			<h3><st c="50301">Tone transformation</st></h3>
			<p><st c="50321">Our efforts so far </st><a id="_idIndexMarker908"/><st c="50341">have been successful in writing summaries and even</st><a id="_idIndexMarker909"/><st c="50391"> marketing copy, but now we need to expand to other channels, such as email, and give our summaries a more friendly tone to match this format. </st><st c="50534">For this, we will apply </st><span class="No-Break"><st c="50558">tone transformation:</st></span></p>
			<pre class="source-code"><st c="50578">
prompt12 = PromptTemplate.from_template(
    """Your task is to generate a short summary of what a
    company is doing to improve the environment.
    </st><st c="50719">From the retrieved context below, delimited by
    triple backticks, extract the information focusing
    on any aspects that mention the eco-friendliness of
    their products. </st><st c="50885">Limit to 30 words.
    </st><st c="50904">After providing the summary, translate the summary
    into an email format with a more friendly and
    casual tone. </st><st c="51014">If you don't know the answer, just say
    that you don't know.
    </st><st c="51074">Question: {question}
    Context: ```{context}```
    Answer:"""
)</st></pre>
			<p><st c="51132">Here, we continue the original </st><a id="_idIndexMarker910"/><st c="51164">summarization, but then we task the LLM with writing an email using the same information with a </st><span class="No-Break"><st c="51260">casual</st></span><span class="No-Break"><a id="_idIndexMarker911"/></span><span class="No-Break"><st c="51266"> tone:</st></span></p>
			<pre class="source-code"><st c="51272">
Google is enhancing eco-friendliness through features like eco-friendly routing in Google Maps, energy-efficient Google Nest thermostats, and carbon emissions data in Google Flights.
</st><strong class="bold"><st c="51456">Email Format:</st></strong><st c="51469">
Subject: Exciting Eco-Friendly Features from Google!
</st><st c="51523">Hi [Recipient's Name],
I hope you're doing well! </st><st c="51572">I wanted to share some cool updates from Google about their efforts to help the environment. </st><st c="51665">They've introduced some awesome features like eco-friendly routing in Google Maps, energy-efficient Google Nest thermostats, and even carbon emissions data in Google Flights. </st><st c="51840">It's great to see such big steps towards a greener future!
</st><st c="51899">Best, [Your Name]</st></pre>
			<p><st c="51916">As we continue to see with these examples, there are so many ways LLMs can be used to improve RAG applications. </st><st c="52029">Other options that are less applicable in our example (but still very valuable in other scenarios) include translating into a specific coding format, or from one coding format to another. </st><st c="52217">Spelling and grammar checks are also popular transformations</st><a id="_idIndexMarker912"/><st c="52277"> that </st><a id="_idIndexMarker913"/><st c="52283">can </st><span class="No-Break"><st c="52287">be applied.</st></span></p>
			<p><st c="52298">We have covered iterating, summarization, inference, and transformation. </st><st c="52372">There is one more concept we will cover to conclude this code </st><span class="No-Break"><st c="52434">lab: expansion.</st></span></p>
			<h2 id="_idParaDest-282"><a id="_idTextAnchor281"/><st c="52449">Expansion</st></h2>
			<p><st c="52459">The goal of expansion, in</st><a id="_idIndexMarker914"/><st c="52485"> many ways, can be thought of as a reverse of the goals of previous concepts we covered, such as summarization. </st><st c="52597">With summarization, we are taking a lot of data and consolidating it into smaller amounts of data while trying to preserve the meaning of that data. </st><st c="52746">Expansion seeks to do the opposite, taking a small amount of data and </st><em class="italic"><st c="52816">expanding</st></em><st c="52825"> it to a larger set of information. </st><st c="52861">Let’s walk through a scenario where this can be implemented: the expansion of a </st><span class="No-Break"><st c="52941">short summary.</st></span></p>
			<h3><st c="52955">Expand on a short text</st></h3>
			<p><st c="52978">Our efforts continue to grow! </st><st c="53009">Our latest task is to take the environmental concepts we have been discussing and start promoting their benefits to investors in our client. </st><st c="53150">In this next prompt, we will take the short summary we have been given and assume for a moment that this is all the content that we have available. </st><st c="53298">We will then ask the LLM to expand on that short summary with a focus on how it can appeal to investors. </st><st c="53403">Let’s see what it can come </st><span class="No-Break"><st c="53430">up with:</st></span></p>
			<pre class="source-code"><st c="53438">
prompt13 = PromptTemplate.from_template(
    """Your task is to generate a short summary of what a
    company is doing to improve the environment.
    </st><st c="53579">From the retrieved context below, delimited by
    triple backticks, extract the information focusing
    on any aspects that mention the eco-friendliness of
    their products. </st><st c="53745">Limit to 30 words.
    </st><st c="53764">After providing the summary, provide a much longer
    description of what the company is doing to improve
    the environment, using only the summary you have
    generated as the basis for this description. </st><st c="53961">If you
    don't know the answer, just say that you don't know.
    </st><st c="54021">Question: {question}
    Context: ```{context}```
    Answer:"""
)</st></pre>
			<p><st c="54079">In a real-world scenario, you</st><a id="_idIndexMarker915"/><st c="54109"> will probably mostly not have the original context available when you want the LLM to expand on your data. </st><st c="54217">Therefore, by telling the LLM to limit the source for its expansion to just what is provided in the summary, we are simulating this scenario more accurately. </st><st c="54375">Our LLM does not disappoint us, as it provides an expanded description of the environmental efforts mentioned in the original summary, and then tailors it </st><span class="No-Break"><st c="54530">for investors:</st></span></p>
			<pre class="source-code">
<strong class="bold"><st c="54544">Summary:</st></strong><st c="54553"> Google offers eco-friendly routing in Google Maps, energy-efficient Google Nest thermostats, and carbon emissions information in Google Flights to help users make sustainable choices.
</st><strong class="bold"><st c="54738">Broader Description:</st></strong><st c="54758"> Google is actively enhancing the eco-friendliness of its products by integrating features that promote sustainability. </st><st c="54878">For instance, Google Maps now includes eco-friendly routing options, Google Nest thermostats are designed for energy efficiency, and Google Flights provides carbon emissions information. </st><st c="55065">These initiatives not only help users make more environmentally conscious decisions but also demonstrate Google's commitment to reducing its carbon footprint. </st><st c="55224">For investors, this focus on sustainability can be a significant value proposition, as it aligns with the growing consumer demand for eco-friendly products and can lead to long-term cost savings and regulatory advantages. </st><st c="55446">Additionally, it positions Google as a leader in environmental responsibility, potentially enhancing its brand reputation and market share.</st></pre>
			<p><st c="55585">That is just one example of </st><a id="_idIndexMarker916"/><st c="55614">how the expansion concept can be utilized. </st><st c="55657">Consider how and when an expansion of your data in your RAG application can be utilized </st><span class="No-Break"><st c="55745">as well.</st></span></p>
			<p><st c="55753">This concludes all the key concepts for how to improve your prompt design: iteration, summarization, inference, transformation, and expansion. </st><st c="55897">These concepts form the foundation of many of the more in-depth and complex concepts that can make your RAG application more effective. </st><st c="56033">Consider this the start of your knowledge in this area and continue to track advances and new techniques as they </st><span class="No-Break"><st c="56146">become known.</st></span></p>
			<h1 id="_idParaDest-283"><a id="_idTextAnchor282"/><st c="56159">Summary</st></h1>
			<p><st c="56167">In this chapter, we explored the crucial role of prompt engineering in enhancing the performance and effectiveness of RAG systems. </st><st c="56299">By strategically designing and refining input prompts, we can improve the retrieval of relevant information and subsequently enhance the quality of generated text. </st><st c="56463">We discussed various prompt design techniques, such as shot design, chain-of-thought prompting, personas, and knowledge augmentation, which can be applied to optimize </st><span class="No-Break"><st c="56630">RAG applications.</st></span></p>
			<p><st c="56647">Throughout the chapter, we discussed the fundamental concepts of prompt design, including the importance of being concise, specific, and well-defined, as well as the need to iterate gradually and use clear separators. </st><st c="56866">We also highlighted the fact that different LLMs require different prompts, as well as the importance of adapting prompts to the specific model </st><span class="No-Break"><st c="57010">being used.</st></span></p>
			<p><st c="57021">Through a series of code labs, we learned how to create custom prompt templates using the </st><strong class="source-inline"><st c="57112">PromptTemplate</st></strong><st c="57126"> class in LangChain, as well as how to apply various prompting concepts to improve our RAG efforts. </st><st c="57226">These concepts included iterating to refine prompts, summarizing to condense information, inferring to extract additional insights, transforming data into different formats or tones, and expanding on short summaries to generate more comprehensive descriptions. </st><st c="57487">We also explored the use of prompt parameters, such as temperature, top-p, and seed, to control the randomness and determinism of </st><span class="No-Break"><st c="57617">LLM outputs.</st></span></p>
			<p><st c="57629">By leveraging the techniques and concepts covered in this chapter, we can significantly enhance the performance of our RAG applications, making them more effective at retrieving relevant information, generating high-quality text, and adapting to specific use cases. </st><st c="57896">As the field of prompt engineering continues to evolve, staying up to date with the latest techniques and best practices will be essential for maximizing the potential of RAG systems in </st><span class="No-Break"><st c="58082">various domains.</st></span></p>
			<p><st c="58098">In our next and final chapter, we will discuss some more advanced techniques that you can use to make potential significant improvements to your </st><span class="No-Break"><st c="58244">RAG application!</st></span></p>
		</div>
	<div id="charCountTotal" value="58260"/></body></html>