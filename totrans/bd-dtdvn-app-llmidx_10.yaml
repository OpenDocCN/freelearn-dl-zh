- en: '<html:html><html:head><html:title>Customizing and Deploying Our LlamaIndex
    Project</html:title></html:head> <html:body><html:div class="epub-source"><html:h1
    id="_idParaDest-199">Customizing and Deploying Our LlamaIndex Project</html:h1>
    <html:div id="_idContainer110"><html:p>Customizing <html:strong class="bold">Retrieval-Augmented
    Generation</html:strong> ( <html:strong class="bold">RAG</html:strong> ) components
    and optimizing performance <html:a id="_idIndexMarker929"></html:a>is critical
    to building robust, production-ready applications with LlamaIndex. This chapter
    explores methods for leveraging <html:a id="_idIndexMarker930"></html:a>open source
    models, intelligent routing across <html:strong class="bold">large language models</html:strong>
    ( <html:strong class="bold">LLMs</html:strong> ), and using community-built modules
    to increase flexibility and cost-effectiveness. Advanced tracing, evaluation methods,
    and deployment options are explored to gain deep insight, ensure reliable operation,
    and streamline the development <html:span class="No-Break">life cycle.</html:span></html:p>
    <html:p>Throughout this chapter, we’re going to cover the following <html:span
    class="No-Break">main topics:</html:span></html:p> <html:ul><html:li>Customizing
    our <html:span class="No-Break">RAG components</html:span></html:li> <html:li>Using
    advanced tracing and <html:span class="No-Break">evaluation techniques</html:span></html:li>
    <html:li>Introduction to deployment <html:span class="No-Break">with Streamlit</html:span></html:li>
    <html:li>Hands-on – a step-by-step <html:span class="No-Break">deployment guide</html:span></html:li></html:ul>
    <html:a id="_idTextAnchor199"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>Technical
    requirements</html:title></html:head> <html:body><html:div class="epub-source"><html:h1
    id="_idParaDest-200">Technical requirements</html:h1> <html:div id="_idContainer110"><html:p>For
    this chapter, you will need to install the following package in <html:span class="No-Break">your
    environment:</html:span></html:p> <html:ul><html:li><html:em class="italic">Arize
    AI</html:em> <html:span class="No-Break"><html:em class="italic">Phoenix</html:em></html:span>
    <html:span class="No-Break">:</html:span> <html:a><html:span class="No-Break">https://pypi.org/project/arize-phoenix/</html:span></html:a></html:li></html:ul>
    <html:p>Three additional integration packages are required in order to run the
    <html:span class="No-Break">sample code:</html:span></html:p> <html:ul><html:li><html:em
    class="italic">Hugging Face</html:em> <html:span class="No-Break"><html:em class="italic">embeddings</html:em></html:span>
    <html:span class="No-Break">:</html:span> <html:a><html:span class="No-Break">https://pypi.org/project/llama-index-embeddings-huggingface/</html:span></html:a></html:li>
    <html:li><html:em class="italic">Zephyr query</html:em> <html:span class="No-Break"><html:em
    class="italic">engine</html:em></html:span> <html:span class="No-Break">:</html:span>
    <html:a><html:span class="No-Break">https://pypi.org/project/llama-index-packs-zephyr-query-engine/</html:span></html:a></html:li>
    <html:li><html:em class="italic">Neutrino</html:em> <html:span class="No-Break"><html:em
    class="italic">LLM</html:em></html:span> <html:span class="No-Break">:</html:span>
    <html:a><html:span class="No-Break">https://pypi.org/project/llama-index-llms-neutrino/</html:span></html:a></html:li></html:ul>
    <html:p>All code samples from this chapter can be found in the <html:code class="literal">ch9</html:code>
    subfolder of the book’s <html:span class="No-Break">GitHub repository:</html:span></html:p>
    <html:p><html:a><html:span class="No-Break">https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex</html:span></html:a></html:p>
    <html:a id="_idTextAnchor200"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>Customizing
    our RAG components</html:title></html:head> <html:body><html:div class="epub-source"><html:h1
    id="_idParaDest-201">Customizing our RAG components</html:h1> <html:div id="_idContainer110">client
    = OpenAI(base_url="http://localhost:1234/v1") from llama_index.llms.openai import
    OpenAI llm = OpenAI(     api_base=''http://localhost:1234/v1'',     temperature=0.7
    ) print(llm.complete(''Who is Lionel Messi?'')) pip install llama-index-llms-neutrino
    from llama_index.core.llms import ChatMessage from llama_index.llms.neutrino import
    Neutrino llm = Neutrino(     api_key="<your-Neutrino_API_key>",     router="<Neutrino-router_ID>"
    ) while True:     user_message = input("Ask a question: ")     if user_message.lower()
    == ''exit'':         print("Exiting chat")         break     response = llm.complete(user_message)
        print(f"LLM answer: {response}")     print(f"Answered by: {response.raw[''model'']}")
    from llama_index.core import Settings Settings.llm = llm pip install llama-index-embeddings-huggingface
    from zephyr_pack.base import ZephyrQueryEnginePack from llama_index.readers import
    SimpleDirectoryReader reader = SimpleDirectoryReader(''files'') documents = reader.load_data()
    zephyr_qe = ZephyrQueryEnginePack(documents) response=zephyr_qe.run(     "Enumerate
    famous buildings in ancient Rome"     ) print(response) pip install chromadb llamaindex-cli
    rag --files files -q "What can you tell me about ancient Rome?" --verbose llamaindex-cli
    rag --chat <html:p>For starters, let’s talk about which components <html:a id="_idIndexMarker931"></html:a>of
    a RAG workflow can be customized in LlamaIndex. The short answer is <html:em class="italic">pretty
    much all of them, as we have seen already in the previous chapters</html:em> .
    The fact that the framework itself is flexible and allows customization of all
    the core components is a definite advantage. But leaving aside the framework itself,
    the core of a RAG workflow is actually the LLM and the embedding model it uses.
    In all the examples given so far, we have used the default configuration of LlamaIndex
    – which is based on OpenAI models. But, as we already briefly discussed in <html:a><html:span
    class="No-Break"><html:em class="italic">Chapter 3</html:em></html:span></html:a>
    , <html:em class="italic">Kickstarting Your Journey with LlamaIndex</html:em>
    , there are both good reasons and enough options available to choose other models
    – both commercial variants offered by established companies in this market, and
    open source models, which can be hosted locally, offering private alternatives,
    and substantially reducing the costs of a <html:span class="No-Break">large-scale
    implementation.</html:span></html:p> <html:p>But first, <html:span class="No-Break">some
    background.</html:span></html:p> <html:a id="_idTextAnchor201"></html:a><html:h2
    id="_idParaDest-202">How LLaMA and LLaMA 2 changed the open source landscape</html:h2>
    <html:p>In early 2023, Meta AI introduced the <html:strong class="bold">Large
    Language Model Meta AI</html:strong> ( <html:strong class="bold">LLaMA</html:strong>
    ) family, offering <html:a id="_idIndexMarker932"></html:a>a notable leap in accessibility
    for LLMs <html:a id="_idIndexMarker933"></html:a>by releasing model weights <html:a
    id="_idIndexMarker934"></html:a>to the research community. Following this, LLaMA
    2 was launched in July 2023, with improvements such as increased data for training
    and expanded model sizes, alongside models fine-tuned for dialogue under less
    restrictive commercial use conditions. Meta developed and launched three versions
    of LLaMA 2 with 7, 13, and 70 billion parameters, respectively. While the basic
    structure of these models stayed similar to the original LLaMA versions, they
    were trained with 40% additional data compared to the original models, in order
    to enhance their <html:span class="No-Break">foundational capabilities.</html:span></html:p>
    <html:p>Despite some controversy regarding its open source status, the initiative
    marked a significant contribution to the open source ecosystem, triggering a new
    wave of community-based research and application development. The model consistently
    showcased competitive performance in tests against other leading LLMs, proving
    its <html:span class="No-Break">advanced capabilities.</html:span></html:p> <html:p>Further
    down the line, these releases have led to the creation of tools such as <html:em
    class="italic">llama.cpp</html:em> by Georgi Gerganov ( <html:a>https://github.com/ggerganov/llama.cpp</html:a>
    ), enabling the operation of these sophisticated models on more modest hardware,
    thus democratizing access to cutting-edge <html:span class="No-Break">AI technologies.</html:span></html:p>
    <html:p class="callout-heading">Quick note</html:p> <html:p class="callout"><html:em
    class="italic">llama.cpp</html:em> is an efficient C/C++ implementation <html:a
    id="_idIndexMarker935"></html:a>of Meta’s LLaMA architecture for LLM inference.
    Hugely popular in the open source community, with more than 43,000 stars on GitHub
    and over 930 releases, this foundational framework has sparked the development
    of many other similar tools and services such as Ollama, Local.AI, and others.
    These updates and advances signaled that AI research was changing, focusing more
    on making information freely available and making sure AI models can run on simpler
    computers and other edge devices. This opened <html:a id="_idIndexMarker936"></html:a>up
    more possibilities for using <html:strong class="bold">generative AI</html:strong>
    ( <html:strong class="bold">GenAI</html:strong> ) and encouraged new ideas and
    <html:span class="No-Break">improvements everywhere.</html:span></html:p> <html:p>I
    won’t go into a detailed discussion <html:a id="_idIndexMarker937"></html:a>of
    all the currently available <html:a id="_idIndexMarker938"></html:a>tools for
    running local LLMs. This is because there is already a plethora of available methods
    by which various open source models can be run on the local system. And not just
    local LLMs: there’s also an increasing number of service providers offering access
    either to their own proprietary AI models or providing cloud-hosted access to
    open source models, and the good news is that LlamaIndex already provides built-in
    support for many of them. You can always consult the official documentation of
    the framework for a detailed overview of the supported models, along with examples
    <html:a id="_idIndexMarker939"></html:a>of how they can be <html:span class="No-Break">used:</html:span>
    <html:a><html:span class="No-Break">https://docs.llamaindex.ai/en/stable/module_guides/models/llms/modules.html</html:span></html:a>
    <html:span class="No-Break">.</html:span></html:p> <html:p>Instead, I will try
    to offer you an alternative that I personally find very convenient for two important
    reasons: it is very easy to implement, and your existing code can be reused with
    only a few minimal changes. For beginner coders and tinkerers wanting to quickly
    experiment with an idea <html:a id="_idIndexMarker940"></html:a>or build simple
    <html:a id="_idIndexMarker941"></html:a>prototypes, this may be one of the <html:span
    class="No-Break">best solutions.</html:span></html:p> <html:a id="_idTextAnchor202"></html:a><html:h2
    id="_idParaDest-203">Running a local LLM using LM Studio</html:h2> <html:p>Built
    on top <html:a id="_idIndexMarker942"></html:a>of the <html:code class="literal">llama.cpp</html:code>
    library, <html:strong class="bold">LM Studio</html:strong> ( <html:a>https://lmstudio.ai/</html:a>
    ) provides a very user-friendly <html:a id="_idIndexMarker943"></html:a>graphical
    interface <html:a id="_idIndexMarker944"></html:a>for LLMs. It allows us to download,
    configure, and locally run almost any open source model available on Hugging Face.
    A great resource, especially for non-technical users, LM Studio offers two ways
    of interacting with a local LLM: through a chat UI similar to OpenAI’s ChatGPT
    or via an OpenAI-compatible local server. This second option makes it particularly
    useful because we can easily adapt any LlamaIndex application natively designed
    to use OpenAI’s LLMs with very few modifications. We’ll get to that in a moment,
    but first, let’s see how to get things started with <html:span class="No-Break">LM
    Studio.</html:span></html:p> <html:p>To start using this tool, you’ll first have
    to download and install the right version, depending on your operating system.
    Releases are available for Mac, Windows, and Linux. The installation steps are
    self-explanatory and well documented on <html:span class="No-Break">their website.</html:span></html:p>
    <html:p>Once installed, the LM Studio GUI starts with a <html:strong class="bold">Model
    Discovery</html:strong> screen where you can type any model or model family name
    and get a list of matching <html:a id="_idIndexMarker945"></html:a>model builds
    available for download. We’ll use the popular <html:strong class="bold">Zephyr-7B</html:strong>
    model for our example ( <html:a>https://huggingface.co/HuggingFaceH4/zephyr-7b-beta</html:a>
    ). I have specifically chosen Zephyr because, albeit a compact model, it demonstrates
    the effectiveness of distilling <html:a id="_idIndexMarker946"></html:a>an LLM
    into a more manageable size. Derived from <html:strong class="bold">Mistral-7B</html:strong>
    , Zephyr-7B establishes a new benchmark for chat models with 7 billion <html:a
    id="_idIndexMarker947"></html:a>parameters, surpassing the performance of <html:strong
    class="bold">LLAMA2-CHAT-70B</html:strong> on the Hugging <html:a id="_idIndexMarker948"></html:a>Face
    <html:em class="italic">LMSYS Chatbot Arena Leaderboard</html:em> ( <html:a>https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard</html:a>
    ). <html:span class="No-Break"><html:em class="italic">Figure 9</html:em></html:span>
    <html:em class="italic">.1</html:em> shows a typical output when searching for
    the <html:span class="No-Break"><html:code class="literal">zephyr-7b</html:code></html:span>
    <html:span class="No-Break">keyword:</html:span></html:p> <html:p class="IMG---Caption"
    lang="en-US">Figure 9.1 – LM Studio screenshot displaying search results</html:p>
    <html:p>In the search results screen, you’ll see <html:span class="No-Break">two
    panels:</html:span></html:p> <html:ul><html:li>The one on the left contains all
    models that match your search query. In our case, these are different builds of
    the <html:span class="No-Break">Zephyr-7B model</html:span></html:li> <html:li>The
    right panel lists all the <html:strong class="bold">Generative Pre-trained Transformer-Generated
    Unified Format</html:strong> ( <html:strong class="bold">GGUF</html:strong> )
    file versions available <html:a id="_idIndexMarker949"></html:a><html:span class="No-Break">for
    download</html:span></html:li></html:ul> <html:p class="callout-heading">About
    GGUF files</html:p> <html:p class="callout">GGUF is a specific file format <html:a
    id="_idIndexMarker950"></html:a>used for storing models for inference. Enhancing
    model sharing and usage efficiency, this format has quickly become a popular way
    of storing and distributing models throughout the open <html:span class="No-Break">source
    community.</html:span></html:p> <html:p>For most models, you’ll get an entire
    <html:a id="_idIndexMarker951"></html:a>list of GGUF files <html:a id="_idIndexMarker952"></html:a>available.
    Each one will have its own characteristics, but probably <html:a id="_idIndexMarker953"></html:a>the
    most important characteristic is the <html:span class="No-Break"><html:strong
    class="bold">quantization</html:strong></html:span> <html:span class="No-Break">level.</html:span></html:p>
    <html:h3>Understanding LLM quantization</html:h3> <html:p>Running an open source
    LLM <html:a id="_idIndexMarker954"></html:a>on typical consumer hardware can prove
    challenging mainly because of its large memory footprint and high computational
    requirements. While some consumer-grade GPUs can aid in this regard, they may
    not be as effective as enterprise-level hardware in handling the demands of LLMs.
    That’s why we need quantization. The goal of applying quantization – a post-training
    optimization technique – to an AI model is to optimize it for better performance
    and efficiency, particularly in terms of speed and memory usage, without significantly
    compromising its accuracy or <html:span class="No-Break">output quality.</html:span></html:p>
    <html:p>The quantization process achieves this by converting the model’s parameters
    – typically stored as 32-bit floating-point <html:a id="_idIndexMarker955"></html:a>numbers
    – to lower-bit representations, such as <html:strong class="bold">16-bit floating-point</html:strong>
    ( <html:code class="literal">FP16</html:code> ), <html:strong class="bold">8-bit
    integers</html:strong> ( <html:code class="literal">INT8</html:code> ), or even
    lower. It’s a kind <html:a id="_idIndexMarker956"></html:a>of approximation process
    that works by reducing the numerical precision used to represent the model’s parameters,
    combined with complex techniques to maintain as much accuracy as possible. Modern
    quantization techniques are designed to minimize accuracy loss, often resulting
    in models that are nearly as accurate as their <html:span class="No-Break">full-precision
    counterparts.</html:span></html:p> <html:p class="callout-heading">A simple analogy
    to help you better understand the concept</html:p> <html:p class="callout">Imagine
    you have a recipe that calls for very precise measurements, such as <html:code
    class="literal">1.4732</html:code> cups of flour. In practice, you might round
    this to 1.5 cups, as the difference is negligible in most cases and the difference
    will not affect the end result. This is similar to quantization, where we reduce
    the precision of the model’s parameters to make the model more efficient while
    maintaining acceptable accuracy. But instead of cups of flour, we reduce the numerical
    precision of the model’s parameters. Instead of using 16 bits to store a parameter
    as 23.7, we could quantize it into 8 bits as 23\. This directly translates to
    less memory usage and faster processing times. However, there is a trade-off between
    model size, speed, <html:span class="No-Break">and accuracy.</html:span></html:p>
    <html:p>With an acceptable loss of accuracy, this process can significantly reduce
    the size of the model and the computational resources required for both training
    and inference phases, making it more feasible to deploy these models on consumer
    hardware. Generally, the lower the bit representation (such as <html:code class="literal">INT4</html:code>
    or even binary), the smaller and faster the model becomes, but at a higher risk
    of <html:span class="No-Break">accuracy loss.</html:span></html:p> <html:p>Being
    built on top of llama.cpp, LM Studio can take advantage of any compatible GPUs
    that could be used during the inference process. This feature is commonly called
    <html:em class="italic">GPU offloading</html:em> and means that computing operations
    <html:a id="_idIndexMarker957"></html:a>can be partially or even entirely transferred
    from the CPU to the GPU. Given the fact that a modern GPU is capable of handling
    highly parallel computing tasks more efficiently than CPUs, this can dramatically
    speed up the inference process. It also reduces the load on the CPU, thus providing
    an overall balanced improvement of system performance. The main limitation when
    attempting GPU offloading is the amount of video memory available on your GPU.
    In order to run efficiently, the GPU must load the model in the video <html:span
    class="No-Break">memory first.</html:span></html:p> <html:p>Because of this, apart
    from the quantization <html:a id="_idIndexMarker958"></html:a>level, the GGUF
    files in the right panel will also have a flag showing three possible compatibility
    scenarios, each represented by a <html:span class="No-Break">different color:</html:span></html:p>
    <html:ul><html:li><html:strong class="bold">Green</html:strong> : This means your
    GPU has enough video memory to load the model and execute the inference. In most
    cases, this is the <html:span class="No-Break">ideal scenario</html:span></html:li>
    <html:li><html:strong class="bold">Blue</html:strong> : Not ideal, but still provides
    a considerable uplift <html:span class="No-Break">in performance</html:span></html:li>
    <html:li><html:strong class="bold">Gray</html:strong> : This may or may not work
    depending on the <html:span class="No-Break">model architecture</html:span></html:li>
    <html:li><html:strong class="bold">Red</html:strong> : Unfortunately, this means
    you won’t be able to run this version on your machine, the most probable reason
    being that its size exceeds your total <html:span class="No-Break">system memory</html:span></html:li></html:ul>
    <html:p class="callout-heading">Pro tip</html:p> <html:p class="callout">A very
    handy tool for approximating the required VRAM for a particular model given a
    particular quantization level <html:a id="_idIndexMarker959"></html:a>can be found
    on the Hugging Face <html:span class="No-Break">website:</html:span> <html:a><html:span
    class="No-Break">https://huggingface.co/spaces/hf-accelerate/model-memory-usage</html:span></html:a></html:p>
    <html:h3>So, which model should you choose?</html:h3> <html:p>The general rule
    of thumb <html:a id="_idIndexMarker960"></html:a>is that with a lower quantization
    level, less memory will be required and the inference process will be faster.
    The trade-off is decreased accuracy. For example, a 3-bit quantization will always
    result in less accuracy than a <html:span class="No-Break">6-bit quantization.</html:span></html:p>
    <html:p>Once you’ve made a decision on the exact model version, the next step
    is to download the model on your machine. But first, make sure you have the necessary
    space on your hard drive. There’s a handy status <html:a id="_idIndexMarker961"></html:a>bar
    on the bottom of the UI to monitor the status of <html:span class="No-Break">the
    download.</html:span></html:p> <html:p>After the download is complete, moving
    to the <html:strong class="bold">Chats</html:strong> screen will display something
    similar to <html:span class="No-Break"><html:em class="italic">Figure 9</html:em></html:span>
    <html:span class="No-Break"><html:em class="italic">.2</html:em></html:span> <html:span
    class="No-Break">:</html:span></html:p> <html:p class="IMG---Caption" lang="en-US">Figure
    9.2 – LM Studio’s chat UI</html:p> <html:p>This is the interaction method that
    I mentioned at the beginning of this section – the one resembling the ChatGPT
    interface. In this screen, you’ll be able to do <html:span class="No-Break">the
    following:</html:span></html:p> <html:ol><html:li>Select the desired AI model
    from a list of all downloaded ones. To choose your model, use the <html:em class="italic">model
    selector</html:em> on top of the screen. You’ll have to wait for a few moments
    until the model is loaded <html:span class="No-Break">into memory.</html:span></html:li>
    <html:li>Configure any available parameters of the model using the <html:em class="italic">configuration
    panel</html:em> on the right side. We’ll talk in more detail about that in <html:span
    class="No-Break">a moment.</html:span></html:li> <html:li>See a list of previous
    chats on the <html:span class="No-Break">left side.</html:span></html:li> <html:li>Chat
    with the model using a familiar interface inspired <html:span class="No-Break">by
    ChatGPT.</html:span></html:li></html:ol> <html:p>There are a number of parameters
    that you can tweak in the configuration panel. The most important ones are <html:span
    class="No-Break">the following:</html:span></html:p> <html:ul><html:li><html:strong
    class="bold">Preset</html:strong> : Some models come with predefined configurations
    that you can load from presets. For an easy start, I would recommend selecting
    the model’s specific preset from the list. For example, there is a Zephyr preset
    that can be used with all <html:span class="No-Break">Zephyr-based models</html:span></html:li>
    <html:li><html:strong class="bold">System Prompt</html:strong> : This prompt will
    set the initial context of <html:span class="No-Break">the conversation</html:span></html:li>
    <html:li><html:strong class="bold">GPU Offload</html:strong> : Allows you to configure
    the number of model layers to be offloaded to the GPU. Depending on the model
    you’re using and your available GPU, you may want to gradually experiment with
    increasing values while checking for model stability. Higher values can sometimes
    produce errors. If you feel confident, use -1 to offload all the model’s layers
    to <html:span class="No-Break">the GPU</html:span></html:li> <html:li><html:strong
    class="bold">Context Length</html:strong> : Allows you to define the maximum context
    window to <html:span class="No-Break">be used</html:span></html:li></html:ul>
    <html:p>Changing some of these parameters may trigger a model reload, so you’ll
    have to be patient until it completes <html:a id="_idIndexMarker962"></html:a>the
    process. Once you have customized everything, the floor is yours – enjoy chatting
    with your <html:span class="No-Break">local LLM.</html:span></html:p> <html:h3>So
    far, so good, but where’s the RAG part in all this?</html:h3> <html:p>For that,
    we’ll have to go <html:a id="_idIndexMarker963"></html:a>to the <html:strong class="bold">Local
    Inference Server</html:strong> screen, which you can do by pressing the double-arrow
    icon on the left-side menu. You’ll be presented with a UI similar to <html:span
    class="No-Break"><html:em class="italic">Figure 9</html:em></html:span> <html:span
    class="No-Break"><html:em class="italic">.3</html:em></html:span> <html:span class="No-Break">:</html:span></html:p>
    <html:p class="IMG---Caption" lang="en-US">Figure 9.3 – The local Inference Server
    interface in LM Studio</html:p> <html:p>The configuration options from the right-side
    panel <html:a id="_idIndexMarker964"></html:a>are almost identical to the ones
    in the <html:strong class="bold">Chat</html:strong> screen. In the beginning,
    you can leave the <html:em class="italic">server configuration</html:em> options
    as default. The <html:em class="italic">usage</html:em> section tells you how
    to interact with the API. One of the great aspects of LM Studio is that it emulates
    the OpenAI API. That means your already existing code will need very few changes
    to work with a local LLM hosted through <html:span class="No-Break">LM Studio.</html:span></html:p>
    <html:p>All you have to do at this point is to click the <html:strong class="bold">Start
    Server</html:strong> button, and you’re good <html:span class="No-Break">to go.</html:span></html:p>
    <html:p class="callout-heading">Quick note</html:p> <html:p class="callout">Please
    keep in mind that while the API server is running, the chat UI will be disabled,
    so you won’t be able to use both at the <html:span class="No-Break">same time.</html:span></html:p>
    <html:p>Let’s see exactly what we need to change in our code if we want to port
    it to a local LLM using this method. If we look at the recommendation in the <html:em
    class="italic">usage</html:em> section, we’ll see that a single change <html:span
    class="No-Break">is necessary:</html:span></html:p> <html:p>However, because LlamaIndex
    has its own implementation of the OpenAI API client, in our case, we’ll have to
    use the <html:code class="literal">api_base</html:code> parameter <html:span class="No-Break">like
    this:</html:span></html:p> <html:p>As you can see, the only real change <html:a
    id="_idIndexMarker965"></html:a>we have to make is pointing the <html:code class="literal">llm</html:code>
    instance toward our local server instead of the OpenAI one. The rest of the code
    remains unchanged. After running this example, you’ll see actual requests coming
    from our code and responses coming from the API in LM Studio’s log screen. If
    you want to permanently reconfigure the LLM in the entire code, you’ll have to
    define a <html:code class="literal">Settings</html:code> object and use it to
    configure global settings, as I showed you in <html:a><html:span class="No-Break"><html:em
    class="italic">Chapter 3</html:em></html:span></html:a> , <html:em class="italic">Kickstarting
    Your Journey with LlamaIndex</html:em> , in the <html:em class="italic">Customizing
    the LLM used by</html:em> <html:span class="No-Break"><html:em class="italic">LlamaIndex</html:em></html:span>
    <html:span class="No-Break">section.</html:span></html:p> <html:p>Neat, isn’t
    it? Our data is now completely private, and we don’t have to pay for using an
    AI model in our RAG workflows anymore. Of course, there’s still a cost, albeit
    in electricity rather than tokens. The capability to run local models on modest
    hardware unlocks numerous possibilities that extend beyond mere text generation.
    This includes the opportunity to embrace <html:a id="_idIndexMarker966"></html:a>fully
    multimodal experiences with models such as <html:strong class="bold">LLaVa</html:strong>
    ( <html:a>https://huggingface.co/docs/transformers/main/en/model_doc/llava</html:a>
    ), allowing for a wider range of applications: a wonderful tool that serves as
    an excellent resource for rapid prototyping or exploring <html:span class="No-Break">diverse
    ideas.</html:span></html:p> <html:p>However, keep in mind that LM Studio <html:a
    id="_idIndexMarker967"></html:a>is governed by a licensing model, which restricts
    its use to personal, non-commercial purposes. To utilize LM Studio for commercial
    applications, obtaining permission from the developers <html:span class="No-Break">is
    necessary.</html:span></html:p> <html:a id="_idTextAnchor203"></html:a><html:h2
    id="_idParaDest-204">Routing between LLMs using services such as Neutrino or OpenRouter</html:h2>
    <html:p>Sometimes, a single LLM <html:a id="_idIndexMarker968"></html:a>may not
    be ideal for every single <html:a id="_idIndexMarker969"></html:a>interaction.
    In complex RAG scenarios, finding the best mix between cost, latency, and precision
    could prove to be a difficult task when forced to choose a single LLM for everything.
    But what if we could find a way to mix different LLMs in the same app and dynamically
    choose which one to use for each individual interaction? That <html:a id="_idIndexMarker970"></html:a>is
    the exact purpose of third-party services such as <html:strong class="bold">Neutrino</html:strong>
    ( <html:a>https://www.neutrinoapp.com/</html:a> ) and <html:strong class="bold">OpenRouter</html:strong>
    ( <html:a>https://openrouter.ai/</html:a> ). These types of services <html:a id="_idIndexMarker971"></html:a>can
    significantly enhance a RAG workflow by providing intelligent routing capabilities
    for queries across <html:span class="No-Break">different LLMs.</html:span></html:p>
    <html:p>Neutrino’s smart model router, for <html:a id="_idIndexMarker972"></html:a>instance,
    allows you to intelligently route queries to the most suited LLM for the prompt,
    optimizing both response quality and cost efficiency. This can be particularly
    useful in a RAG workflow where different types of queries may require different
    LLM strengths or specialties. For example, one model might be more effective at
    understanding and parsing the initial user query, while another might be better
    suited for generating responses based on retrieved documents. By employing a router,
    we can dynamically select the most suitable model for each task without hardcoding
    model choices into our application, thus enhancing flexibility and potentially
    improving the overall performance of our RAG system. <html:span class="No-Break"><html:em
    class="italic">Figure 9</html:em></html:span> <html:em class="italic">.4</html:em>
    describes the working mechanism of a <html:span class="No-Break">Neutrino router:</html:span></html:p>
    <html:p class="IMG---Caption" lang="en-US">Figure 9.4 – A diagram of the Neutrino
    smart routing feature</html:p> <html:p>The great news is that both Neutrino <html:a
    id="_idIndexMarker973"></html:a>and OpenRouter are supported as integration packages
    <html:a id="_idIndexMarker974"></html:a>in LlamaIndex. Let’s have a look at a
    simple example that uses a custom <html:a id="_idIndexMarker975"></html:a>Neutrino
    router to dynamically <html:a id="_idIndexMarker976"></html:a>choose between different
    LLMs depending on the user query. To run this example, make sure you first install
    the Neutrino integration package by running the <html:span class="No-Break">following
    command:</html:span></html:p> <html:p>Once the package is installed, you should
    first sign up for an account and obtain an API key on the Neutrino website. The
    next step is to create an LLM router by selecting your desired LLMs as well as
    a <html:em class="italic">fallback</html:em> LLM. The fallback model will be used
    by default in case of errors or whenever the router cannot determine which LLM
    to use. During the router setup, you will also have the option of choosing to
    use Neutrino as a provider for the AI models or utilize your own API keys for
    each LLM. The last step in the router setup process requires you to provide a
    <html:em class="italic">router ID</html:em> . This ID will be used in the code
    to specify the router used by <html:span class="No-Break">the service.</html:span></html:p>
    <html:p>Here is how we can use the Neutrino router <html:span class="No-Break">in
    LlamaIndex:</html:span></html:p> <html:p>The code first initializes the Neutrino
    router in the form of a LlamaIndex <html:code class="literal">llm</html:code>
    object, for which you’ll need to provide your Neutrino API key and the ID of the
    router you have defined. Next, it runs in a loop, continually taking questions
    from the user until the <html:code class="literal">''exit''</html:code> keyword
    <html:span class="No-Break">is received:</html:span></html:p> <html:p>The questions
    are submitted <html:a id="_idIndexMarker977"></html:a>to the Neutrino router,
    and, in return, the script <html:a id="_idIndexMarker978"></html:a>not only prints
    <html:a id="_idIndexMarker979"></html:a>the answer <html:a id="_idIndexMarker980"></html:a>but
    also the name of the LLM that was chosen by the router to generate the answer.
    You can play around and experiment with different types of questions. Based on
    whichever models you selected when you defined the router, you’ll see that it
    will send the questions to different LLMs, depending on their capabilities. Another,
    more general approach in using such a router would be to use the <html:code class="literal">Settings</html:code>
    class to create a global configuration using that <html:span class="No-Break"><html:code
    class="literal">llm</html:code></html:span> <html:span class="No-Break">object:</html:span></html:p>
    <html:p>This has the advantage that it configures every subsequent LlamaIndex
    component in our code to use the <html:span class="No-Break">Neutrino router.</html:span></html:p>
    <html:p class="callout-heading">Pro tip</html:p> <html:p class="callout">If you’re
    not entirely satisfied with the decisions made by the router, Neutrino also gives
    you the ability to fine-tune your defined router by uploading a list of examples
    on which the router can be <html:span class="No-Break">trained:</html:span> <html:a><html:span
    class="No-Break">https://platform.neutrinoapp.com/training-studio</html:span></html:a></html:p>
    <html:p>And Neutrino is just one example. OpenRouter works in a similar way, but
    it’s mostly focused on optimizing the cost of LLM calls, not necessarily <html:span
    class="No-Break">the quality.</html:span></html:p> <html:p>There are also other
    providers <html:a id="_idIndexMarker981"></html:a>offering similar <html:a id="_idIndexMarker982"></html:a>services,
    and the concept <html:a id="_idIndexMarker983"></html:a>is bound to become more
    and more popular as hundreds of new AI models <html:a id="_idIndexMarker984"></html:a>emerge
    every week. The ability to use LLM routing services enhances the RAG workflow
    by abstracting the complexity of model selection and management. As a result,
    we can focus on building and optimizing our applications instead of managing the
    underlying <html:span class="No-Break">AI models.</html:span></html:p> <html:a
    id="_idTextAnchor204"></html:a><html:h2 id="_idParaDest-205">What about customizing
    embedding models?</html:h2> <html:p>Another important component <html:a id="_idIndexMarker985"></html:a>that
    can be considered for customization in a RAG scenario is the underlying embedding
    model. Intensively used in scenarios where vector store indexes are employed,
    the embedding model can also be a source of concern regarding cost and privacy.
    That is why we may sometimes prefer using a local model in our RAG workflow. Again,
    the good news is that LlamaIndex provides out-of-the-box support for more than
    30 embedding models. They can be used by installing embedding <html:a id="_idIndexMarker986"></html:a>integration
    packages, documented on the <html:em class="italic">LlamaHub</html:em> <html:span
    class="No-Break">website:</html:span> <html:a><html:span class="No-Break">https://llamahub.ai/?tab=embeddings</html:span></html:a>
    <html:span class="No-Break">.</html:span></html:p> <html:p>You can find a very
    simple example of how to configure LlamaIndex to use a local embedding model from
    Hugging Face in <html:a><html:span class="No-Break"><html:em class="italic">Chapter
    5</html:em></html:span></html:a> , <html:em class="italic">Indexing with LlamaIndex</html:em>
    , in the <html:em class="italic">Understanding</html:em> <html:span class="No-Break"><html:em
    class="italic">embeddings</html:em></html:span> <html:span class="No-Break">section.</html:span></html:p>
    <html:a id="_idTextAnchor205"></html:a><html:h2 id="_idParaDest-206">Leveraging
    the Plug and Play convenience of using Llama Packs</html:h2> <html:p>The fact
    that LlamaIndex <html:a id="_idIndexMarker987"></html:a>offers us such a rich
    framework <html:a id="_idIndexMarker988"></html:a>of low-level elements and methods
    for RAG is a double-edged sword. On the one hand, it is extremely useful to have
    a tool available for almost any practical problem you have to solve. On the other
    hand, to successfully implement these tools, we must first spend a fair amount
    of time familiarizing ourselves with each one. Then comes the fine-tuning <html:a
    id="_idIndexMarker989"></html:a>and optimization phase <html:a id="_idIndexMarker990"></html:a>for
    each component. We are already talking about a significant effort in the development
    and optimization process. Sometimes, in order to be able to test an idea with
    a rapid prototype, it would be preferable if we already had some advanced ready-made
    modules. Imagine some <html:em class="italic">Lego</html:em> pieces already structured
    into functional sub-assemblies: a roof, a window, a bus stop, and so on. Well,
    we have that <html:span class="No-Break">to hand.</html:span></html:p> <html:p>Created
    and continually improved by the flourishing LlamaIndex community, <html:strong
    class="bold">Llama Packs</html:strong> are pre-packaged modules that can be used
    to quickly build LLM applications. Just like some pre-built Lego structures, they
    provide reusable components such as LLMs, embedding models, and vector indexes
    that have been preconfigured to work together for various use cases in building
    a RAG pipeline. They are ready-to-use modules that can be downloaded and initialized
    with parameters to achieve a specific goal outside of <html:span class="No-Break">the
    box.</html:span></html:p> <html:p class="callout-heading">Example</html:p> <html:p
    class="callout">A pack could contain a full RAG pipeline to enable semantic search
    over text or an entire agent construct that could be immediately invoked in <html:span
    class="No-Break">our app.</html:span></html:p> <html:p>Llama Packs act as templates
    that can be inspected, customized, and extended as needed. The code for each pack
    is available, so developers can modify it or take inspiration to build their own
    applications. The beauty of this concept is that it provides <html:strong class="bold">Plug
    and Play</html:strong> ( <html:strong class="bold">PnP</html:strong> ) solutions
    without bloating the main code base of the framework. You can still use various
    integration packages together with the core components of LlamaIndex, and you
    can definitely customize any of these packs according to <html:span class="No-Break">your
    needs.</html:span></html:p> <html:p>You’ll find a collection of all the published
    Llama Packs, together with all the other integration packages, available on LlamaHub
    ( <html:a>https://llamahub.ai/?tab=llama_packs</html:a> ). There’s a <html:em
    class="italic">README</html:em> file for each pack that provides details about
    its usage, and most of them also have detailed examples that you can follow and
    <html:span class="No-Break">experiment with.</html:span></html:p> <html:p>Using
    them is very straightforward. Because, in this section, we talk about customizations
    in general and, among other options, moving our RAG workflows to local, open source
    models, I’m going to show you an example in the same line. We’ll explore a Llama
    Pack that allows for the creation of a query engine that relies entirely on locally
    hosted AI models. The pack implements <html:code class="literal">HuggingFaceH4/zephyr-7b-beta</html:code>
    as the LLM used for inference and <html:code class="literal">BAAI/bge-base-en-v1.5</html:code>
    as the embedding model. The pack <html:a id="_idIndexMarker991"></html:a>is called
    Zephyr Query Engine Pack, and <html:a id="_idIndexMarker992"></html:a>you can
    find it <html:span class="No-Break">here:</html:span> <html:a><html:span class="No-Break">https://llamahub.ai/l/llama_packs-zephyr_query_engine</html:span></html:a>
    <html:span class="No-Break">.</html:span></html:p> <html:p>In a similar way to
    how LM Studio works, this pack can leverage existing GPUs to accelerate the inference
    process. Let’s see how <html:span class="No-Break">it works.</html:span></html:p>
    <html:p>The first step in using any Llama Pack <html:a id="_idIndexMarker993"></html:a>is
    to download the actual modules <html:a id="_idIndexMarker994"></html:a>on your
    local environment. This can be accomplished in three <html:span class="No-Break">different
    ways:</html:span></html:p> <html:ul><html:li>By installing the corresponding integration
    package. In our example, that would be accomplished with the <html:span class="No-Break">following
    command:</html:span> <html:p class="list-inset">This method is simple and permanently
    installs the required pack into your local environment. Its only disadvantage
    is that you cannot inspect and modify the pack code. For that purpose, the other
    two methods <html:span class="No-Break">are recommended.</html:span></html:p></html:li>
    <html:li>By using the <html:strong class="bold">command-line interface</html:strong>
    ( <html:strong class="bold">CLI</html:strong> ). Here’s <html:span class="No-Break">an</html:span>
    <html:span class="No-Break"><html:a id="_idIndexMarker995"></html:a></html:span><html:span
    class="No-Break">example:</html:span> <html:p class="list-inset">We’ll discuss
    the CLI tool in more detail in the <html:span class="No-Break">next section.</html:span></html:p></html:li>
    <html:li>Directly in the code, using the from llama_index.llama_pack import download_llama_pack
    download_llama_pack(     "ZephyrQueryEnginePack", "./zephyr_pack" ) <html:code
    class="literal">download_llama_pack()</html:code> method and specifying a download
    location <html:span class="No-Break">like this:</html:span></html:li></html:ul>
    <html:p>Once downloaded into your local environment, the pack contents will be
    stored in a subfolder called <html:code class="literal">zephyr_pack</html:code>
    . You can inspect and modify anything in the code, adjusting it to your own needs.
    You will also need to install the Hugging Face <html:code class="literal">embeddings</html:code>
    integration package before running <html:span class="No-Break">the example:</html:span></html:p>
    <html:p>Here’s a simple example of how to use this pack <html:span class="No-Break">after
    downloading:</html:span></html:p> <html:p>Notice that we’re using the <html:code
    class="literal">run()</html:code> method, which, in this case, is a wrapper for
    the <html:code class="literal">query()</html:code> method used by the regular
    <html:span class="No-Break">query engine.</html:span></html:p> <html:p>This is
    just one of the more than 50 packs already available on LlamaHub at this moment.
    And the number keeps growing. The great news is that all of them are well documented
    and follow pretty much the same implementation model. So, next time you’re faced
    with a practical scenario that needs combining low-level components into more
    advanced elements, instead of reinventing the wheel, I encourage you to spend
    some time browsing LlamaHub for a potential ready-made solution for your problem.
    Llama Packs accelerates LLM app development <html:a id="_idIndexMarker996"></html:a>by
    letting developers tap into pre-built components tailored for common <html:a id="_idIndexMarker997"></html:a>use
    cases. Both ready-made solutions and customizable templates are available to <html:span
    class="No-Break">kickstart projects.</html:span></html:p> <html:a id="_idTextAnchor206"></html:a><html:h2
    id="_idParaDest-207">Using the Llama CLI</html:h2> <html:p>Another very useful
    tool in the LlamaIndex <html:a id="_idIndexMarker998"></html:a>arsenal is the
    <html:code class="literal">llamaindex-cli</html:code> utility. Installed together
    with the LlamaIndex libraries, the tool can be accessed very easily from the command
    line and can be used for various purposes, including <html:span class="No-Break">the
    following:</html:span></html:p> <html:ul><html:li>Downloading Llama Packs, as
    seen in the previous section. The syntax to download a Llama Pack is given <html:span
    class="No-Break">as follows:</html:span></html:li> <html:li>Upgrading source code
    from versions older than LlamaIndex v. <html:code class="literal">0.10</html:code>
    . Due to the fact that version 0.10 brought many changes related to the code structure
    and how to use certain modules in the framework, the authors of LlamaIndex provided
    developers with this automatic upgrade tool. Basically, it automatically modifies
    the code written on older versions and updates it to the new structure introduced
    with v0.10 for an easier transition. The syntax used for this feature is the following
    to process all sources in a given <html:span class="No-Break">folder simultaneously:</html:span>
    <html:p class="list-inset">Or execute the following command to upgrade a <html:span
    class="No-Break">single file:</html:span></html:p></html:li> <html:li>By far the
    most interesting capability is enabled by using the <html:code class="literal">rag</html:code>
    argument. This feature allows you to build a RAG workflow directly from the command
    line without having to write any code. By default, the command-line RAG mode uses
    local storage for embeddings based on a Chroma DB database and OpenAI’s GPT-3.5
    Turbo model as LLM. For privacy reasons, keep in mind that this means that all
    data we upload will be sent to OpenAI <html:span class="No-Break">by default.</html:span></html:li></html:ul>
    <html:h3>How RAG works in the command line</html:h3> <html:p>Before we can use
    the RAG <html:a id="_idIndexMarker999"></html:a>mode from the command <html:a
    id="_idIndexMarker1000"></html:a>line, we must first install the ChromaDB library
    in our <html:span class="No-Break">local environment:</html:span></html:p> <html:p>The
    <html:code class="literal">llamaindex-cli</html:code> utility offers a variety
    of command-line parameters that enable users to interact with language models
    and manage local files efficiently. Here are descriptions of the most important
    <html:span class="No-Break">command-line parameters:</html:span></html:p> <html:ul><html:li><html:code
    class="literal">--help</html:code> : Displays a help message, providing an overview
    of available commands and <html:span class="No-Break">their usage.</html:span></html:li>
    <html:li><html:code class="literal">--files <FILES></html:code> : Defines the
    name of the file or directory from where the tool will ingest our proprietary
    data. The contents will be ingested and embedded into the local vector database,
    enabling the RAG CLI tool to index the specified files and later retrieve context
    from them at <html:span class="No-Break">query time.</html:span></html:li> <html:li><html:code
    class="literal">--question <QUESTION></html:code> : Specifies the question you
    want to ask about the ingested files. Used for querying indexed content, leveraging
    the power of the LLM to extract information from our <html:span class="No-Break">proprietary
    data.</html:span></html:li> <html:li><html:code class="literal">--chat</html:code>
    : Opens a chat <html:strong class="bold">read-eval-print loop</html:strong> (
    <html:strong class="bold">REPL</html:strong> ) for an interactive Q&A session
    within <html:a id="_idIndexMarker1001"></html:a>the terminal. This provides a
    conversational interface to query the <html:span class="No-Break">ingested documents.</html:span></html:li>
    <html:li><html:code class="literal">--verbose</html:code> : Enables verbose output
    during execution, offering detailed information about the tool’s operations that
    can be useful for troubleshooting and understanding the tool’s <html:span class="No-Break">inner
    workings.</html:span></html:li> <html:li><html:code class="literal">--clear</html:code>
    : Clears out all currently embedded data from the local vector database. Because
    a Chroma database is used to store the embeddings, these will persist across the
    sessions. The <html:code class="literal">--clear</html:code> command is the equivalent
    of <html:span class="No-Break">a reset.</html:span></html:li> <html:li><html:code
    class="literal">--create-llama</html:code> : Initiates the creation of a LlamaIndex
    application based on the selected files. This parameter extends the tool’s functionality
    beyond simple Q&A, enabling the development of full-stack applications with a
    backend and frontend, leveraging the ingested data. You’ll find a complete <html:a
    id="_idIndexMarker1002"></html:a>example of how to use it <html:span class="No-Break">here:</html:span>
    <html:a><html:span class="No-Break">https://www.npmjs.com/package/create-llama#example</html:span></html:a>
    <html:span class="No-Break">.</html:span></html:li></html:ul> <html:p>Talking
    about examples, let’s have a look at a simple way to have a conversation with
    our files using the CLI RAG feature. We’ll use the contents of the <html:code
    class="literal">ch9\files</html:code> folder from our GitHub repository. So, make
    sure you’re running this script from inside that folder, which should contain
    some <html:span class="No-Break">sample files:</html:span></html:p> <html:p>Alternatively,
    once the files have been ingested, for an interactive chat session with the data,
    you can use the <html:span class="No-Break">following command:</html:span></html:p>
    <html:p>And just in case you need <html:a id="_idIndexMarker1003"></html:a>to
    customize the mechanics <html:a id="_idIndexMarker1004"></html:a>of the CLI RAG,
    a complete example can be found in the official documentation <html:a id="_idIndexMarker1005"></html:a>of
    the framework, <html:span class="No-Break">here:</html:span> <html:a href="https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/rag_cli.html"
    target="_blank"><html:span class="No-Break">https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/rag_cli.html</html:span></html:a>
    <html:span class="No-Break">.</html:span></html:p> <html:p>Next, it’s time to
    dive deeper into the logic of our <html:span class="No-Break">LlamaIndex applications.</html:span></html:p>
    <html:a id="_idTextAnchor207"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>Using
    advanced tracing and evaluation techniques</html:title></html:head> <html:body><html:div
    class="epub-source"><html:h1 id="_idParaDest-208">Using advanced tracing and evaluation
    techniques</html:h1> <html:div id="_idContainer110">pip install "arize-phoenix[llama-index]"
    llama-hub html2text from llama_index.core import (     SimpleDirectoryReader,
        VectorStoreIndex,     set_global_handler ) import phoenix as px px.launch_app()
    set_global_handler("arize_phoenix") documents = SimpleDirectoryReader(''files'').load_data()
    index = VectorStoreIndex.from_documents(documents) qe = index.as_query_engine()
    response = qe.query("Tell me about ancient Rome") print(response) input("Press
    <ENTER> to exit") from llama_index.core import (     SimpleDirectoryReader,     VectorStoreIndex,
        set_global_handler ) import phoenix as px px.launch_app() set_global_handler("arize_phoenix")
    documents = SimpleDirectoryReader(''files'').load_data() index = VectorStoreIndex.from_documents(documents)
    qe = index.as_query_engine() response = qe.query("Tell me about ancient Rome")
    print(response) from phoenix.session.evaluation import (     get_qa_with_reference,
        get_retrieved_documents ) from phoenix.experimental.evals import (     HallucinationEvaluator,
        RelevanceEvaluator,     QAEvaluator,     OpenAIModel,     run_evals ) from
    phoenix.trace import DocumentEvaluations, SpanEvaluations model = OpenAIModel(model="gpt-4-turbo-preview")
    retrieved_documents_df = get_retrieved_documents(px.Client()) queries_df = get_qa_with_reference(px.Client())
    hallucination_evaluator = HallucinationEvaluator(model) qa_correctness_evaluator
    = QAEvaluator(model) relevance_evaluator = RelevanceEvaluator(model) hallucination_eval_df,
    qa_correctness_eval_df = run_evals(     dataframe=queries_df,     evaluators=[hallucination_evaluator,
    qa_correctness_evaluator],     provide_explanation=True, ) relevance_eval_df =
    run_evals(     dataframe=retrieved_documents_df,     evaluators=[relevance_evaluator],
        provide_explanation=True, )[0] px.Client().log_evaluations(     SpanEvaluations(
            eval_name="Hallucination",         dataframe=hallucination_eval_df),     SpanEvaluations(
            eval_name="QA Correctness",         dataframe=qa_correctness_eval_df),
        DocumentEvaluations(         eval_name="Relevance",         dataframe=relevance_eval_df),
    ) input("Press <ENTER> to exit") <html:p>The process of building <html:a id="_idIndexMarker1006"></html:a>an
    LLM-based application <html:a id="_idIndexMarker1007"></html:a>using a tool such
    as LlamaIndex is very developer-friendly since the framework abstracts away a
    lot of technical stuff. But at the same time, this complicates things, for the
    very same reason. When things don’t work as planned, developers need to have effective
    ways of understanding why. They have to peel back all these layers of abstraction
    in order to pinpoint the root causes. In other words, we need to be able to see
    the inner mechanics of our code, understand how different components interact,
    and be able to identify underlying issues. That’s where tracing becomes a really
    important feature. On the other hand, because we have so many tools available
    and so many ways of building our solution, we need a way to benchmark different
    combinations and determine the best mix of tools and orchestrations. That’s where
    evaluation comes into play. Evaluation is essential for comparing various tool
    and method combinations until we find the right configuration for our specific
    needs. Together, tracing and evaluation form the backbone of a successful RAG
    development process, ensuring both transparency and <html:span class="No-Break">optimal
    performance.</html:span></html:p> <html:p>In <html:a><html:span class="No-Break"><html:em
    class="italic">Chapter 3</html:em></html:span></html:a> , <html:em class="italic">Kickstarting
    Your Journey with LlamaIndex</html:em> , we already discussed simple logging methods
    that we can use to better understand what’s happening under the hood of our LlamaIndex
    apps. Now, it’s time to discover a much more advanced way in which we can understand
    and evaluate RAG applications. In this section, I will explain advanced tracing
    <html:a id="_idIndexMarker1008"></html:a>and evaluation using the <html:strong
    class="bold">Phoenix framework</html:strong> developed by Arize AI ( <html:a>https://phoenix.arize.com/</html:a>
    ). Integrating LlamaIndex with specialized tracing and evaluation tools provides
    a sophisticated approach to understanding and optimizing RAG applications. Phoenix
    provides the necessary instrumentation together with a great visualization UI,
    making our RAG execution workflow really simple <html:span class="No-Break">to
    understand.</html:span></html:p> <html:p>To make use of the advanced <html:a id="_idIndexMarker1009"></html:a>capabilities
    <html:a id="_idIndexMarker1010"></html:a>of the Phoenix framework, we must first
    install some necessary libraries in <html:span class="No-Break">our environment:</html:span></html:p>
    <html:a id="_idTextAnchor208"></html:a><html:h2 id="_idParaDest-209">Tracing our
    RAG workflows using Phoenix</html:h2> <html:p>In Phoenix, tracing <html:a id="_idIndexMarker1011"></html:a>is
    built on <html:a id="_idIndexMarker1012"></html:a>the concept of <html:strong
    class="bold">spans</html:strong> and <html:strong class="bold">traces</html:strong>
    , which are fundamental for capturing the detailed execution flow of applications.
    A span represents a specific operation or unit of work within the application,
    tracking the start and end times, along with metadata that provides context about
    the operation. These spans are nested within traces, which aggregate multiple
    spans to depict the entire journey of a request through the application. This
    hierarchical structure allows developers to drill down into specific operations,
    understanding how each component contributes to the overall process. Phoenix’s
    tracing capabilities are designed to seamlessly integrate with LlamaIndex, enabling
    developers to instrument their RAG applications with <html:span class="No-Break">minimal
    effort.</html:span></html:p> <html:p>Because it features a client-server architecture,
    Phoenix is able to gather traces both locally and remotely. We can automatically
    collect telemetry data about each operation, including data ingestion, indexing,
    retrieval, processing, and any subsequent LLM calls. In the background, this data
    is collected by the Phoenix server, where it can be visualized and analyzed in
    <html:span class="No-Break">real time.</html:span></html:p> <html:p>Once the necessary
    requirements have been installed, using Phoenix is really easy. There are many
    advanced capabilities that you can explore with this framework, but I will show
    you the most simple and straightforward way to use it for tracing the execution
    of a LlamaIndex application. We’ll make use of a special method called <html:code
    class="literal">set_global_handler</html:code> , which conveniently configures
    LlamaIndex to use a certain tracing tool for every operation – in our case, the
    <html:span class="No-Break">Phoenix framework.</html:span></html:p> <html:p>Make
    sure you install <html:a id="_idIndexMarker1013"></html:a>the required packages
    before running <html:a id="_idIndexMarker1014"></html:a><html:span class="No-Break">the
    example:</html:span></html:p> <html:p>Here is <html:span class="No-Break">the
    code:</html:span></html:p> <html:p>Apart from the obvious imports that will provide
    our basic RAG functionality, we’re also importing <html:code class="literal">set_global_handler</html:code>
    and the Phoenix library. The next part will be responsible for starting the Phoenix
    server and configuring LlamaIndex to use it as a global <html:span class="No-Break">callback
    handler:</html:span></html:p> <html:p>From now on, every single operation performed
    by our app will generate traces that will get collected by the Phoenix server.
    Let’s build a simple query engine based on a <html:code class="literal">VectorStoreIndex</html:code>
    index and run a <html:span class="No-Break">random query:</html:span></html:p>
    <html:p>Because we need the server to be live in order to visualize the trace,
    we keep the script running with <html:span class="No-Break">this line:</html:span></html:p>
    <html:p>Now, with the script still running in the background, we can access the
    Phoenix UI at this URL: http://localhost:6006/. <html:span class="No-Break"><html:em
    class="italic">Figure 9</html:em></html:span> <html:em class="italic">.5</html:em>
    shows what you’ll find in the Phoenix <html:span class="No-Break">server UI:</html:span></html:p>
    <html:p class="IMG---Caption" lang="en-US">Figure 9.5 – A screenshot from the
    Phoenix server UI depicting our tracing output</html:p> <html:p>Looking at this
    screenshot, we can see <html:a id="_idIndexMarker1015"></html:a>that the Phoenix
    server UI <html:a id="_idIndexMarker1016"></html:a>helps us visualize a complete
    trace of our code, divided into multiple spans. If you have successfully executed
    the previous sample code, our trace should consist of three different spans, each
    displayed on a separate line.</html:p> <html:p>Let’s talk about the columns in
    <html:span class="No-Break">the screenshot:</html:span></html:p> <html:ul><html:li>The
    first column, <html:code class="literal">kind</html:code> , contains the type
    of the span. It can be <html:code class="literal">chain</html:code> , <html:code
    class="literal">retriever</html:code> , <html:code class="literal">re-ranker</html:code>
    , <html:code class="literal">llm</html:code> , <html:code class="literal">embedding</html:code>
    , <html:code class="literal">tool</html:code> , or <html:code class="literal">agent</html:code>
    . We are already familiar with what these concepts represent in LlamaIndex, except
    for a chain. In Phoenix, a chain can be either the starting point for a series
    of operations in an LLM application or a connector linking different steps within
    the application workflow. In our example, the screenshot contains three spans:
    two chains and an embedding. They are displayed in the reverse order of their
    operation, beginning with the <html:span class="No-Break">last one.</html:span></html:li>
    <html:li>The second column, <html:em class="italic">name</html:em> , provides
    a more detailed description of the span. We can see that in our example, the first
    span represents a <html:em class="italic">query</html:em> , the second one is
    an <html:em class="italic">embedding</html:em> , and the third is a <html:em class="italic">Node-parsing</html:em>
    operation. The logic of our code is now clear: it first parsed the ingested documents
    into Nodes, then created a vector index by embedding the Nodes, and the final
    step was to run a query against <html:span class="No-Break">that index.</html:span></html:li>
    <html:li>The next two columns, <html:em class="italic">input,</html:em> and <html:em
    class="italic">output</html:em> , show exactly what went as an input into the
    span and what was the final output produced by it. In our example, we only have
    values in these fields for the query span as this does not apply to the <html:span
    class="No-Break">other ones.</html:span></html:li> <html:li>The <html:em class="italic">evaluations</html:em>
    column displays the results of the evaluation for each span. For now, that column
    should be empty as we have not yet executed any evaluation. We’ll cover this topic
    in the <html:span class="No-Break">next section.</html:span></html:li> <html:li><html:em
    class="italic">start time</html:em> provides an exact timestamp for <html:span
    class="No-Break">each span.</html:span></html:li> <html:li><html:em class="italic">latency</html:em>
    measures the total execution time for each span. This is really useful when trying
    to optimize our code for <html:span class="No-Break">increased performance.</html:span></html:li>
    <html:li>As the name implies, <html:em class="italic">total tokens</html:em> count
    the total number of tokens used by the <html:span class="No-Break">corresponding
    operation.</html:span></html:li> <html:li>The last column, <html:em class="italic">status</html:em>
    , indicates whether the operation was completed successfully <html:span class="No-Break">or
    not.</html:span></html:li></html:ul> <html:p>Here comes the best <html:a id="_idIndexMarker1017"></html:a>part.
    If we now <html:a id="_idIndexMarker1018"></html:a>click on the <html:em class="italic">kind</html:em>
    column of the query span – the first one in our list – we’ll get a detailed visualization,
    similar to the one depicted in <html:span class="No-Break"><html:em class="italic">Figure
    9</html:em></html:span> <html:span class="No-Break"><html:em class="italic">.6</html:em></html:span>
    <html:span class="No-Break">:</html:span></html:p> <html:p class="IMG---Caption"
    lang="en-US">Figure 9.6 – Trace details visualized on the Phoenix server UI</html:p>
    <html:p>As you can see, we can now get a detailed understanding of each individual
    step performed during this span. In this case, we see a decomposed view of the
    query engine operation: first, the retrieval part, and then the final response
    synthesis using the LLM. By clicking on each individual step, we can explore its
    attributes and underlying mechanics. And because Phoenix runs locally, all this
    data <html:span class="No-Break">remains private.</html:span></html:p> <html:p
    class="callout-heading">Practical exercise</html:p> <html:p class="callout">Here’s
    a useful exercise you could attempt now. Try to reconfigure some of the samples
    discussed in the previous chapters to use the Phoenix framework. You’ll get a
    better understanding of how different components work in LlamaIndex and also have
    a chance to familiarize yourself with this <html:span class="No-Break">great tool.</html:span></html:p>
    <html:p>And if you want to go deeper <html:a id="_idIndexMarker1019"></html:a>and
    explore more advanced tracing <html:a id="_idIndexMarker1020"></html:a>features
    of Phoenix, you’ll find everything you need in their official <html:span class="No-Break">documentation:</html:span>
    <html:a><html:span class="No-Break">https://docs.arize.com/phoenix/</html:span></html:a>
    <html:span class="No-Break">.</html:span></html:p> <html:p>Next, let’s talk about
    how we can use Phoenix to evaluate and optimize our <html:span class="No-Break">RAG
    apps.</html:span></html:p> <html:a id="_idTextAnchor209"></html:a><html:h2 id="_idParaDest-210">Evaluating
    our RAG system</html:h2> <html:p>When developing LLM-based <html:a id="_idIndexMarker1021"></html:a>systems,
    proper evaluation is essential for checking how well a RAG pipeline works. In
    general, LLM applications have to deal with very diverse inputs, and there is
    usually not a single, absolute answer they are supposed to return. That means
    evaluating them can prove to be a <html:span class="No-Break">challenging task.</html:span></html:p>
    <html:p>In general, evaluating a RAG pipeline involves assessing key aspects such
    as <html:span class="No-Break">the following:</html:span></html:p> <html:ul><html:li><html:strong
    class="bold">Retrieval quality</html:strong> : Evaluating the relevance and effectiveness
    of the retrieved Nodes in providing the necessary information to answer <html:span
    class="No-Break">the query</html:span></html:li> <html:li><html:strong class="bold">Generation
    quality</html:strong> : Assessing the quality of the final output, including its
    correctness, coherence, and adherence to the <html:span class="No-Break">provided
    context</html:span></html:li> <html:li><html:strong class="bold">Faithfulness</html:strong>
    : Ensuring that the generated output is faithful to the retrieved information
    and does not introduce hallucinations <html:span class="No-Break">or inconsistencies</html:span></html:li>
    <html:li><html:strong class="bold">Efficiency</html:strong> : Measuring the computational
    efficiency and scalability of the RAG pipeline, especially in real-world scenarios
    with <html:span class="No-Break">large-scale datasets</html:span></html:li> <html:li><html:strong
    class="bold">Robustness</html:strong> : Testing the RAG system’s ability to handle
    diverse queries, edge cases, and potential <html:span class="No-Break">adversarial
    inputs</html:span></html:li></html:ul> <html:p>To address these evaluation challenges,
    several tools and frameworks have been developed to facilitate the evaluation
    process. These tools aim to provide automated metrics, reference-based comparisons,
    and also human-in-the-loop evaluation methodologies. By leveraging these evaluation
    frameworks, we can obtain insights into the strengths and weaknesses of our RAG
    pipelines, identify areas for improvement, and iterate on their designs to enhance
    <html:span class="No-Break">overall performance.</html:span></html:p> <html:p>Because
    in the previous section, we’ve seen how Phoenix can help us with its tracing functionality,
    I’d like to continue building on the previous example and first explore some of
    the evaluation features provided by <html:span class="No-Break">this framework.</html:span></html:p>
    <html:h3>Using the Phoenix framework evaluation features</html:h3> <html:p>Since
    manual labeling <html:a id="_idIndexMarker1022"></html:a>and testing evaluation
    data <html:a id="_idIndexMarker1023"></html:a>can be very time-consuming, Phoenix
    uses GPT-4 as a reference to decide on the correctness of our RAG’s answers. This
    framework provides out-of-the-box support for batch processing, custom datasets,
    and pre-tested evaluation templates. Unlike traditional, more basic evaluation
    libraries that lack rigor for production environments, Phoenix ensures data science
    rigor, high throughput, and flexibility across different environments, making
    it significantly faster and more adaptable for evaluating both the model and the
    context in which it is used. Phoenix can be used <html:a id="_idIndexMarker1024"></html:a>for
    evaluating two important dimensions of a RAG workflow: <html:strong class="bold">retrieval</html:strong>
    and <html:span class="No-Break"><html:strong class="bold">LLM inference</html:strong></html:span>
    <html:span class="No-Break">.</html:span></html:p> <html:p>For retrieval, Phoenix
    evaluates <html:a id="_idIndexMarker1025"></html:a>the relevancy of the retrieved
    context. In other words, it verifies if the retrieved Nodes actually contain an
    answer to the query or not. When evaluating LLM inference, the framework checks
    three <html:span class="No-Break">main attributes:</html:span></html:p> <html:ul><html:li><html:strong
    class="bold">Correctness</html:strong> : This verifies if the system has accurately
    answered <html:span class="No-Break">a question</html:span></html:li> <html:li><html:strong
    class="bold">Hallucinations</html:strong> : This aims to identify any unrealistic
    or fabricated responses by the LLM in relation to the context it <html:span class="No-Break">was
    provided</html:span></html:li> <html:li><html:strong class="bold">Toxicity</html:strong>
    : This checks for any harmful content in the AI’s responses, including racism,
    bias, or <html:span class="No-Break">general toxicity</html:span></html:li></html:ul>
    <html:p>Because a complex RAG scenario could sometimes rely on many individual
    spans, being able to individually evaluate each one becomes an essential feature.
    This way, we can isolate the source of errors and stop them from propagating further
    in the flow. Since it uses an LLM for running evaluations, Phoenix returns not
    just the test result but also an explanation provided by the model. This can be
    very useful for understanding the root cause of a failed evaluation and pinpointing
    the misbehaving component in our <html:span class="No-Break">RAG application.</html:span></html:p>
    <html:p>Let’s have a look at a simple example to understand how Phoenix can be
    used for evaluation. In order to minimize costs and keep the code simple, we’re
    going to use the previous approach we used for the tracing example. We’re going
    to ingest the contents of our <html:code class="literal">ch9/files</html:code>
    folder, create a vector index, and run a simple query against the index. In a
    real scenario, you would probably run these evaluators against a much larger dataset
    in order to cover as many edge cases as possible and increase the probability
    of finding underlying issues in the pipeline. Here is <html:span class="No-Break">an
    example:</html:span></html:p> <html:p>So far, the first part is identical <html:a
    id="_idIndexMarker1026"></html:a>to the previous <html:a id="_idIndexMarker1027"></html:a>example.
    It’s time to add the part responsible for the evaluation. We’ll begin by importing
    the necessary Phoenix components. Two functions, <html:code class="literal">get_retrieved_documents()</html:code>
    and <html:code class="literal">get_qa_with_reference()</html:code> , will be responsible
    for fetching the documents retrieved by queries and the queries with their reference
    answers for evaluation. We’re also importing three of the Phoenix evaluators:
    <html:code class="literal">HallucinationEvaluator</html:code> , <html:code class="literal">QAEvaluator</html:code>
    , and <html:code class="literal">RelevanceEvaluator</html:code> . These evaluators
    will assess the hallucination in responses, the correctness of question-answer
    pairs, and the relevance of retrieved documents, respectively. We also need to
    import <html:code class="literal">run_evals()</html:code> , which will be responsible
    for performing the evaluation tasks and returning DataFrames containing the evaluation
    results. Finally, the <html:code class="literal">DocumentEvaluations</html:code>
    and <html:code class="literal">SpanEvaluations</html:code> classes will be used
    to encapsulate evaluation results and display these results in the Phoenix <html:span
    class="No-Break">server UI:</html:span></html:p> <html:p>Now that the imports
    <html:a id="_idIndexMarker1028"></html:a>are complete, it’s time to prepare <html:a
    id="_idIndexMarker1029"></html:a>our evaluations. First, we declare the LLM that
    will be used to perform evaluations. This should always be the best <html:span
    class="No-Break">available model:</html:span></html:p> <html:p>Once the evaluation
    model has been defined, it’s time to prepare our data. We’ll fetch the retrieved
    documents and queries in separate data frames. These data frames will later become
    the input for <html:span class="No-Break">evaluator functions:</html:span></html:p>
    <html:p>Now that we have the data, we need to define evaluator functions and run
    the <html:span class="No-Break">actual evaluations:</html:span></html:p> <html:p>When
    running the evaluators, notice <html:a id="_idIndexMarker1030"></html:a>that I’m
    setting <html:a id="_idIndexMarker1031"></html:a>the <html:code class="literal">provide_explanation</html:code>
    argument to <html:code class="literal">True</html:code> . This ensures that explanations
    for the evaluation scores are included in the response from the LLM. The last
    part involves encapsulating the results in corresponding <html:code class="literal">SpanEvaluations</html:code>
    and <html:code class="literal">DocumentEvaluations</html:code> classes and sending
    them to the Phoenix server so that they can be properly displayed in <html:span
    class="No-Break">the UI:</html:span></html:p> <html:p>Just like in the previous
    example, the input at the end keeps the script running until the user decides
    to exit by pressing the <html:em class="italic">Enter</html:em> key. This allows
    us to view and interact with the Phoenix app before closing it. If everything
    went smoothly, accessing the UI at http://localhost:6006/ should reveal an output
    similar to what we can see in <html:span class="No-Break"><html:em class="italic">Figure
    9</html:em></html:span> <html:span class="No-Break"><html:em class="italic">.7</html:em></html:span>
    <html:span class="No-Break">:</html:span></html:p> <html:p class="IMG---Caption"
    lang="en-US">Figure 9.7 – Visualizing evaluation results in the Phoenix server
    UI</html:p> <html:p>As you can see, the <html:em class="italic">evaluations</html:em>
    column has been updated with the values returned by the evaluators we just executed.
    We can now see the results, as well as the rationale for each <html:span class="No-Break">individual
    score.</html:span></html:p> <html:p>The topic of evaluating RAG apps is huge and
    could probably become the subject of an entirely separate book. There are many
    nuances and different approaches that could be considered regarding evaluation.
    I’ve only shown you a tool – Phoenix – but there are many other options for this
    purpose, including LlamaIndex’s own instrumentation. If you’re planning to explore
    this topic deeper, I encourage you to start by reading the LlamaIndex <html:a
    id="_idIndexMarker1032"></html:a>official documentation here: <html:a>https://docs.llamaindex.ai/en/stable/module_guides/evaluating/root.html</html:a>
    . Also, get a better understanding of the complete <html:a id="_idIndexMarker1033"></html:a>capabilities
    of the Phoenix framework <html:a id="_idIndexMarker1034"></html:a>by reading their
    official <html:a id="_idIndexMarker1035"></html:a>documentation <html:span class="No-Break">here:</html:span>
    <html:span class="No-Break">https://docs.arize.com/phoenix/</html:span> <html:span
    class="No-Break">.</html:span></html:p> <html:h3>Other alternatives for evaluation
    – RAGAS</html:h3> <html:p>While Phoenix provides a comprehensive evaluation <html:a
    id="_idIndexMarker1036"></html:a>framework for RAG pipelines, there are other
    alternatives available. Another notable framework is <html:strong class="bold">Retrieval-Augmented
    Generation Assessment</html:strong> ( <html:strong class="bold">RAGAS</html:strong>
    ), which is based on the techniques introduced by Es et al. (2023) in their paper,
    <html:em class="italic">RAGAS: Automated Evaluation of Retrieval Augmented Generation</html:em>
    ( <html:a>https://doi.org/10.48550/arXiv.2309.15217</html:a> ). The RAGAS framework
    provides a practical implementation of these evaluation methods, along with additional
    features <html:span class="No-Break">and integrations.</html:span></html:p> <html:p>RAGAS
    is specifically designed for evaluating and analyzing RAG systems. It offers a
    standardized approach to assess various aspects of a RAG pipeline, including retrieval
    quality, generation quality, and the interplay between the retrieval and <html:span
    class="No-Break">generation components.</html:span></html:p> <html:p>Key features
    of RAGAS <html:a id="_idIndexMarker1037"></html:a>include <html:span class="No-Break">the
    following:</html:span></html:p> <html:ul><html:li><html:strong class="bold">Retrieval
    evaluation</html:strong> : RAGAS assesses the quality of the retrieval component
    by measuring the relevance of the retrieved Nodes <html:a id="_idIndexMarker1038"></html:a>to
    the given query using metrics such as <html:strong class="bold">Recall@k</html:strong>
    – the proportion of relevant Nodes retrieved within the top <html:em class="italic">k</html:em>
    results, where k is a user-defined parameter. Another metric that measures retrieval
    quality is the <html:strong class="bold">Mean Reciprocal Rank</html:strong> (
    <html:strong class="bold">MRR</html:strong> ) – measuring how quickly <html:a
    id="_idIndexMarker1039"></html:a>the system finds the first <html:span class="No-Break">relevant
    Node.</html:span></html:li> <html:li><html:strong class="bold">Generation evaluation</html:strong>
    : RAGAS also evaluates the quality of the generated text using a combination of
    automatic metrics <html:a id="_idIndexMarker1040"></html:a>and human evaluation.
    The automatic metrics include <html:strong class="bold">Bilingual Evaluation Understudy</html:strong>
    ( <html:strong class="bold">BLEU</html:strong> ), which measures the similarity
    between the generated text and a reference text by comparing overlapping word
    sequences, and <html:strong class="bold">Recall-Oriented Understudy for Gisting
    Evaluation</html:strong> ( <html:strong class="bold">ROUGE</html:strong> ), which
    calculates the overlap of words <html:a id="_idIndexMarker1041"></html:a>and word
    sequences between the generated text and the reference text. To complement these
    automatic metrics, RAGAS also incorporates human evaluation to assess aspects
    such as fluency, coherence, and relevance of the generated output, providing a
    comprehensive assessment of the <html:span class="No-Break">generation quality.</html:span></html:li>
    <html:li><html:strong class="bold">Retrieval-generation interplay</html:strong>
    : The framework also analyzes the interplay between the retrieval and generation
    components by measuring how much the generated text relies on the retrieved Nodes.
    It introduces <html:a id="_idIndexMarker1042"></html:a>metrics such as <html:strong
    class="bold">Retrieval Dependency</html:strong> ( <html:strong class="bold">RD</html:strong>
    ), which quantifies how much the generated text depends <html:a id="_idIndexMarker1043"></html:a>on
    the retrieved Nodes, and <html:strong class="bold">Retrieval Relevance</html:strong>
    ( <html:strong class="bold">RR</html:strong> ), which measures the relevance of
    the retrieved Nodes to the generated text to quantify <html:span class="No-Break">this
    relationship.</html:span></html:li> <html:li><html:strong class="bold">Simulation</html:strong>
    : RAGAS includes a simulation <html:a id="_idIndexMarker1044"></html:a>component
    that allows us to simulate different retrieval scenarios and analyze their impact
    on the generation quality. This helps in understanding the robustness and generalization
    ability of RAG models under various retrieval conditions. By manipulating the
    retrieval results, users can test how the RAG model performs under scenarios such
    as retrieving irrelevant, partially relevant, or noisy data. The simulation feature
    provides insights into the interplay between the retrieval and generation components,
    enabling us to identify strengths and weaknesses and guide improvements in the
    <html:span class="No-Break">RAG model.</html:span></html:li> <html:li><html:strong
    class="bold">Fine-grained analysis</html:strong> : RAGAS enables fine-grained
    analysis of RAG pipelines by providing tools to visualize and interpret the retrieval-generation
    process, such as attention weights and individual <html:span class="No-Break">Node
    contributions.</html:span></html:li></html:ul> <html:p>A key advantage of this
    framework is that it enables reference-free evaluation of RAG pipelines, meaning
    it does not rely on ground truth annotations. This allows for more efficient and
    scalable <html:span class="No-Break">evaluation cycles.</html:span></html:p> <html:p>Compared
    to Phoenix, RAGAS offers a more focused evaluation framework specifically tailored
    for RAG systems. While Phoenix provides a general-purpose evaluation platform
    with features such as tracing, hallucination detection, and relevance assessment,
    RAGAS goes deeper into the intricacies of retrieval-generation interplay and also
    offers simulation capabilities. The framework provides seamless integration with
    LlamaIndex, simplifying the evaluation of LlamaIndex-based RAG systems. To keep
    things simple, I have not included any code examples in this case, but you can
    find detailed examples and documentation on the official project’s page, at this
    <html:span class="No-Break">URL:</html:span> <html:a><html:span class="No-Break">https://docs.ragas.io/en/stable/howtos/integrations/llamaindex.html</html:span></html:a>
    <html:span class="No-Break">.</html:span></html:p> <html:p>It’s worth noting that
    RAGAS is a more recent framework compared to Phoenix, and while it shows great
    promise, it may take some time for it to see the same level of adoption in the
    <html:span class="No-Break">research community.</html:span></html:p> <html:p class="callout-heading">Important
    note</html:p> <html:p class="callout">One thing to always keep in mind in terms
    of evaluation is the concept of model drift, which we have already covered in
    <html:a><html:span class="No-Break"><html:em class="italic">Chapter 7</html:em></html:span></html:a>
    , <html:em class="italic">Querying Our Data, Part 2 – Postprocessing and Response
    Synthesis</html:em> section. Model drift can impact our RAG pipeline when the
    LLM’s behavior gradually deviates from its intended purpose or when the quality
    of the generated output deteriorates. Regular or even continuous evaluation can
    help detect and mitigate this phenomenon, ensuring the RAG system remains reliable
    and effective in <html:span class="No-Break">production environments.</html:span></html:p>
    <html:p>By mastering tracing and evaluation techniques, you’ll be able to create
    a complete system for finding and fixing problems in an LLM application. Using
    evaluations and tracing together, you can spot where things go wrong, figure out
    why, and see which part of your application needs to <html:span class="No-Break">be
    improved.</html:span></html:p> <html:p>It’s now time to focus our attention <html:a
    id="_idIndexMarker1045"></html:a>on our side project: the PITS tutor. In this
    chapter, we’ll finally get to deploy its components and run it as a standalone
    application. But first, let’s have a short introduction to the different deployment
    options provided <html:span class="No-Break">by</html:span> <html:span class="No-Break"><html:strong
    class="bold">Streamlit</html:strong></html:span> <html:span class="No-Break">.</html:span></html:p>
    <html:a id="_idTextAnchor210"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>Introduction
    to deployment with Streamlit</html:title></html:head> <html:body><html:div class="epub-source"><html:h1
    id="_idParaDest-211">Introduction to deployment with Streamlit</html:h1> <html:div
    id="_idContainer110"><html:p>As I explained in <html:span class="No-Break"><html:em
    class="italic">Chapter 2</html:em></html:span> , <html:em class="italic">LlamaIndex:
    The Hidden Jewel - An Introduction to the LlamaIndex Ecosystem</html:em> , I chose
    Streamlit <html:a id="_idIndexMarker1046"></html:a>as the backbone for our side
    project because of its simplicity and the many deployment options it provides.
    Streamlit offers an easy approach to deploying your applications, making it possible
    to share your work with a broader audience with minimal effort. If you successfully
    followed the installation steps in <html:a><html:span class="No-Break"><html:em
    class="italic">Chapter 2</html:em></html:span></html:a> , your local environment
    should already be ready for the next steps. However, just in case, before proceeding,
    make sure you have completed the necessary installation mentioned in <html:a><html:span
    class="No-Break"><html:em class="italic">Chapter 2</html:em></html:span></html:a>
    , in the <html:em class="italic">Discovering Streamlit – the perfect tool for
    quick build and</html:em> <html:span class="No-Break"><html:em class="italic">deployment</html:em></html:span>
    <html:span class="No-Break">section.</html:span></html:p> <html:p>Now that we’re
    all set up, let’s explore the deployment options available for Streamlit applications.
    Beyond the simplest method of running apps on your local machine, Streamlit offers
    a variety of web deployment solutions <html:a id="_idIndexMarker1047"></html:a>to
    cater to different needs <html:span class="No-Break">and preferences:</html:span></html:p>
    <html:ul><html:li><html:strong class="bold">Streamlit Community Cloud</html:strong>
    : This user-friendly platform is the most straightforward option for deploying
    Streamlit apps, enabling users to deploy directly from their GitHub repositories
    in just a few clicks. It requires minimal configuration, and once deployed, your
    app will be accessible via a unique URL on Streamlit Community Cloud, making it
    easy to share <html:span class="No-Break">with others.</html:span></html:li> <html:li><html:strong
    class="bold">Custom cloud services</html:strong> : For those seeking greater control
    over their deployment environment, Streamlit <html:a id="_idIndexMarker1048"></html:a>apps
    can be deployed on various cloud services, including <html:strong class="bold">Amazon
    Web Services</html:strong> ( <html:strong class="bold">AWS</html:strong> ), <html:strong
    class="bold">Google Cloud Platform</html:strong> ( <html:strong class="bold">GCP</html:strong>
    ), and Azure. Deployment <html:a id="_idIndexMarker1049"></html:a>on these platforms
    might involve additional steps such as containerizing your app with Docker and
    configuring cloud-specific services such as AWS Elastic Beanstalk, Google App
    Engine, or Azure <html:span class="No-Break">App Service.</html:span></html:li>
    <html:li><html:strong class="bold">Self-hosting</html:strong> : If you have your
    own servers, opting to self-host your Streamlit applications gives you maximum
    control over the deployment environment and resources. This method involves setting
    up a server environment capable of running Python applications, installing Streamlit,
    and configuring your network for Streamlit app access. The self-hosting option
    answers to specific requirements for security, performance, or customization that
    cloud platforms <html:span class="No-Break">cannot meet.</html:span></html:li>
    <html:li><html:strong class="bold">Heroku</html:strong> : Heroku (https://www.heroku.com/)
    is another well-known platform <html:a id="_idIndexMarker1050"></html:a>for deploying
    Streamlit apps <html:a id="_idIndexMarker1051"></html:a>due to its simplicity
    and a free tier suitable for small projects <html:span class="No-Break">and prototypes.</html:span></html:li>
    <html:li><html:strong class="bold">Streamlit in Snowflake</html:strong> : For
    use cases prioritizing security and <html:strong class="bold">role-based access
    control</html:strong> ( <html:strong class="bold">RBAC</html:strong> ), Streamlit’s
    integration with Snowflake <html:a id="_idIndexMarker1052"></html:a>offers a secure
    coding and deployment environment within the Snowflake platform. You can easily
    sign up for a trial Snowflake account, create a warehouse and database for your
    apps, and deploy Streamlit applications directly <html:a id="_idIndexMarker1053"></html:a><html:span
    class="No-Break">within Snowflake.</html:span></html:li></html:ul> <html:p>Each
    of these deployment options offers unique benefits, with different advantages
    in terms of level of control, scalability, security requirements, and budget constraints.
    However, I have chosen to show you the simplest option and probably the most appropriate
    choice for our PITS application: deployment in Streamlit Community Cloud. However,
    for a commercial-ready solution, the other options would have been a <html:span
    class="No-Break">better choice.</html:span></html:p> <html:a id="_idTextAnchor211"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>HANDS-ON
    – a step-by-step deployment guide</html:title></html:head> <html:body><html:div
    class="epub-source"><html:h1 id="_idParaDest-212">HANDS-ON – a step-by-step deployment
    guide</html:h1> <html:div id="_idContainer110">from user_onboarding import user_onboarding
    from session_functions import load_session, delete_session, save_session from
    logging_functions import reset_log from quiz_UI import show_quiz from training_UI
    import show_training_UI import streamlit as st def main():     st.set_page_config(layout="wide")
        st.sidebar.title(''P.I.T.S.'')     st.sidebar.markdown(''### Your Personalized
    Intelligent Tutoring System'')     if ''show_quiz'' in st.session_state and     st.session_state[''show_quiz'']:
            show_quiz(st.session_state[''study_subject''])     elif ''resume_session''
    in st.session_state and     st.session_state[''resume_session'']:         st.session_state[''show_quiz'']
    = False         show_training_UI(st.session_state[''user_name''],         st.session_state[''study_subject''])
        elif not load_session(st.session_state):         user_onboarding() else:         st.write(f"Welcome
    back {st.session_state[''user_name'']}!")         col1, col2 = st.columns(2)         if
    col1.button(f"Resume your study of         {st.session_state[''study_subject'']}"):
                st.session_state[''resume_session''] = True             st.rerun()
            if col2.button(''Start a new session''):             delete_session(st.session_state)
                reset_log()             for key in list(st.session_state.keys()):
                    del st.session_state[key]             st.rerun() <html:p>It’s
    time to share our PITS tutoring <html:a id="_idIndexMarker1054"></html:a>application
    with the world. However, as a quick side note, keep in mind that the current version
    is far from being ready for use in a multi-user, real-world environment. To keep
    the code base small and the deployment steps simple, I have designed PITS as a
    pure experiment in LlamaIndex. After all, the purpose of this book was not to
    delve into the architectural intricacies of building a full-fledged Streamlit
    application but rather to explain the tools and features that are available in
    LlamaIndex. This is the main reason why some of the PITS source files are not
    explained in detail in this book. Rest assured, however, that you will find plenty
    of comments in these modules, and if the comments available in the GitHub code
    aren’t enough, you can always explore the official Streamlit documentation and
    get a better <html:a id="_idIndexMarker1055"></html:a>understanding of the framework
    <html:span class="No-Break">here:</html:span> <html:a><html:span class="No-Break">https://docs.streamlit.io/</html:span></html:a>
    <html:span class="No-Break">.</html:span></html:p> <html:p>However, a brief introduction
    to the way Streamlit applications is built is in order. We’ll use one of the PITS
    UI files as an example, and I’ll walk you through the code to give you a basic
    understanding of the principles of Streamlit applications. Here is the code for
    <html:code class="literal">app.py</html:code> , our main program in the PITS structure.
    This code is responsible for orchestrating the execution of the various components
    that make up the tutoring application. It acts as the central hub, routing users
    through the onboarding process, handling session management, and dynamically presenting
    the quiz and training interfaces based on user interactions and <html:span class="No-Break">session
    data:</html:span></html:p> <html:p>We start by importing <html:a id="_idIndexMarker1056"></html:a>the
    necessary modules and components, including Streamlit. We also import several
    custom functions from the other modules, such as <html:code class="literal">user_onboarding</html:code>
    , <html:code class="literal">load_session</html:code> , <html:code class="literal">delete_session</html:code>
    , <html:code class="literal">save_session</html:code> , <html:code class="literal">reset_log</html:code>
    , <html:code class="literal">show_quiz</html:code> , and <html:code class="literal">show_training_UI</html:code>
    , each serving a specific role in the application’s flow. Following the imports,
    the <html:code class="literal">main()</html:code> function encapsulates the <html:span
    class="No-Break">application’s logic:</html:span></html:p> <html:p>The use of
    <html:code class="literal">st.set_page_config</html:code> at the beginning establishes
    the basic layout of our web application. Streamlit provides a sidebar feature,
    and we’ll make use of that to streamline our UI. Next, the application’s flow
    is primarily controlled through conditional statements that check for the presence
    of certain keys in Streamlit’s <html:code class="literal">(st.session_state)</html:code>
    session state. This session state acts as persistent storage across reruns of
    the app within the same browser session, allowing the application to remember
    user choices, entered information, and other <html:span class="No-Break">stateful
    data:</html:span></html:p> <html:p class="callout-heading">A quick note on Streamlit’s
    session state</html:p> <html:p class="callout">Web applications are inherently
    <html:a id="_idIndexMarker1057"></html:a>stateless, meaning each request and response
    between the client and server are independent. Streamlit’s session state allows
    us to overcome this by providing a way to maintain state across reruns of the
    app within the same browser session. This is essential for creating an interactive
    and user-friendly experience, as it allows the application to remember user choices,
    inputs, and actions without requiring the user to re-enter data after <html:span
    class="No-Break">every interaction.</html:span></html:p> <html:p>I’ll briefly
    explain what happens in the previous part of <html:span class="No-Break">the code:</html:span></html:p>
    <html:ul><html:li><html:strong class="bold">Quiz display logic</html:strong> :
    If the user has opted to take a quiz ( <html:code class="literal">''show_quiz''
    in st.session_state</html:code> ), the quiz interface is displayed by <html:span
    class="No-Break">calling</html:span> <html:span class="No-Break"><html:code class="literal">show_quiz()</html:code></html:span>
    <html:span class="No-Break">.</html:span></html:li> <html:li><html:strong class="bold">Resuming
    sessions</html:strong> : If the user has already chosen to resume an existing
    session ( <html:code class="literal">st.session_state[''resume_session'']=True</html:code>
    ), the app will take them directly to the <html:span class="No-Break">training
    UI.</html:span></html:li> <html:li><html:strong class="bold">User onboarding and
    session management</html:strong> : <html:code class="literal">load_session(st.session_state)</html:code>
    checks whether session data exists. If not, the user is directed to the onboarding
    process <html:span class="No-Break">through</html:span> <html:span class="No-Break"><html:code
    class="literal">user_onboarding().</html:code></html:span></html:li></html:ul>
    <html:p>Next, let’s see what happens <html:a id="_idIndexMarker1058"></html:a>when
    an existing session is found but <html:code class="literal">show quiz</html:code>
    is <html:code class="literal">False</html:code> and the user hasn’t clicked on
    the <html:strong class="bold">Resume session</html:strong> <html:span class="No-Break">button
    yet:</html:span></html:p> <html:p>The first operation in this <html:code class="literal">else</html:code>
    block is displaying a welcome back message. The app then displays two buttons,
    allowing the user to decide whether they want to resume the existing training
    session or start a fresh one. Choosing to start a new session will basically reset
    everything and rerun the entire code to start the application from the beginning.
    Resuming the session at this point will determine <html:a id="_idIndexMarker1059"></html:a>the
    app to run <html:code class="literal">show_training_UI</html:code> and continue
    the existing <html:span class="No-Break">training session.</html:span></html:p>
    <html:a id="_idTextAnchor212"></html:a><html:h2 id="_idParaDest-213">Deploying
    our PITS project on Streamlit Community Cloud</html:h2> <html:p>Because of the
    way <html:a id="_idIndexMarker1060"></html:a>the internal folder <html:a id="_idIndexMarker1061"></html:a>structure
    of the Streamlit Community Cloud environment is implemented, we’ll have to make
    a few modifications to our PITS folder structure. The plan is to deploy the application
    straight from a GitHub repository. However, one of the requirements for deploying
    from GitHub into the Community Cloud environment is that the main <html:code class="literal">.py</html:code>
    file is hosted in the <html:code class="literal">root</html:code> folder of the
    repository. That is not the case for PITS as the folder structure is a bit different.
    <html:code class="literal">app.py</html:code> , which is the main file in our
    case, is currently found in the <html:code class="literal">Building-Data-Driven-Applications-with-LlamaIndex\PITS_APP</html:code>
    folder. To fix that, we’ll first make a copy of the <html:code class="literal">PITS_APP</html:code>
    subfolder, and then we’ll initiate a new GitHub repository from that new folder.
    To keep things simple and require minimum changes, I will guide you on how to
    create a new repository containing just the PITS app and then deploy it from your
    own <html:span class="No-Break">GitHub account:</html:span></html:p> <html:ol><html:li>First,
    let’s create a copy <html:a id="_idIndexMarker1062"></html:a>of our local <html:code
    class="literal">PITS_APP</html:code> subfolder. Open Command Prompt <html:a id="_idIndexMarker1063"></html:a>and
    navigate to the <html:code class="literal">Building-Data-Driven-Applications-with-LlamaIndex</html:code>
    folder of your cloned repository. From that folder, type the <html:span class="No-Break">following
    command:</html:span></html:li> <html:li>This will create a folder on your <html:code
    class="literal">C:</html:code> drive containing only the source files of the PITS
    application. If you navigate to the newly created folder and list its contents
    with the <html:code class="literal">dir</html:code> command, the output should
    look like <html:span class="No-Break"><html:em class="italic">Figure 9</html:em></html:span>
    <html:span class="No-Break"><html:em class="italic">.8</html:em></html:span> <html:span
    class="No-Break">:</html:span></html:li></html:ol> <html:p class="IMG---Caption"
    lang="en-US">Figure 9.8 – The contents of the C:\PITS_APP folder</html:p> <html:ol><html:li>The
    next step is to sign in to your GitHub account and create a new repository. Let’s
    name it <html:code class="literal">PITS_ONLINE</html:code> , as in <html:span
    class="No-Break"><html:em class="italic">Figure 9</html:em></html:span> <html:span
    class="No-Break"><html:em class="italic">.9</html:em></html:span> <html:span class="No-Break">:</html:span></html:li></html:ol>
    <html:p class="IMG---Caption" lang="en-US">Figure 9.9 – Creating a new GitHub
    repository named PITS_ONLINE</html:p> <html:ol><html:li>Once created, note the
    repository <html:a id="_idIndexMarker1064"></html:a>URL for the next steps. Next,
    we’ll initialize <html:a id="_idIndexMarker1065"></html:a>a new local repository
    in the desired folder. Open your CLI and navigate to the folder you want to turn
    into a separate repository – <html:code class="literal">C:\PITS_APP</html:code>
    – then execute the <html:span class="No-Break">following command:</html:span></html:li>
    <html:li>Next, add and commit the existing files by running the <html:span class="No-Break">following
    command:</html:span></html:li> <html:li>It’s now time to link your local repository
    to the GitHub repository you created. Replace the URL with your GitHub URL and
    append <html:code class="literal">.git</html:code> at the end in the <html:span
    class="No-Break">following command:</html:span></html:li> <html:li>And finally,
    we push the contents to the new online repository with the <html:span class="No-Break">following
    command:</html:span></html:li></html:ol> <html:p>If everything went smoothly you
    should now have a brand-new GitHub repository containing the PITS <html:span class="No-Break">source
    code.</html:span></html:p> <html:p>Let’s handle the Community Cloud <html:span
    class="No-Break">deployment next.</html:span></html:p> <html:p>Deploying Streamlit
    applications into their Community Cloud environment is a fairly simple and straightforward
    process. To begin our deployment, the first step is to sign up for a free Streamlit
    account here: <html:a>https://share.streamlit.io/signup</html:a> . The best option
    is to use your GitHub account both for signing up and signing in to your Streamlit
    account. Once logged in, simply click on the <html:strong class="bold">New app</html:strong>
    button to begin the deployment process. You’ll be taken to a screen similar to
    what you can see in <html:span class="No-Break"><html:em class="italic">Figure
    9</html:em></html:span> <html:span class="No-Break"><html:em class="italic">.10</html:em></html:span>
    <html:span class="No-Break">:</html:span></html:p> <html:p class="IMG---Caption"
    lang="en-US">Figure 9.10 – Deploying an application into Streamlit Community Cloud</html:p>
    <html:p>If you signed in to Streamlit <html:a id="_idIndexMarker1066"></html:a>using
    GitHub, you should already <html:a id="_idIndexMarker1067"></html:a>have the <html:code
    class="literal">PITS_ONLINE</html:code> repository listed as an option. Select
    it, then, under the <html:strong class="bold">Main file path</html:strong> field,
    change the default value to <html:code class="literal">app.py</html:code> and
    then click <html:strong class="bold">Deploy</html:strong> . From here, the Streamlit
    deployment service takes over and prepares the required environment for your application.
    This might take a while, but if you want to check on the progress, you can always
    expand the <html:strong class="bold">Manage app</html:strong> section on the bottom
    right of your screen. When everything is ready, the application should <html:span
    class="No-Break">start automatically.</html:span></html:p> <html:p>You can now
    ingest your existing training materials, have PITS generate slides and narrations
    about your desired study topic, and ask its chatbot any questions related to <html:span
    class="No-Break">the contents.</html:span></html:p> <html:p class="callout-heading">Important
    note</html:p> <html:p class="callout">Don’t forget, you’re using your own API
    key. To keep costs under control, you should first experiment on a limited scale
    by uploading some small training resources and always keeping an eye on the OpenAI
    API usage. The good news is that the majority of the cost is incurred during slides
    and narration generation. However, once that is completed, the resulting material
    is stored and reused in <html:span class="No-Break">future sessions.</html:span></html:p>
    <html:p>Simple, isn’t it? Although offering an environment with limited resources,
    the Streamlit Community Cloud service makes it really easy to deploy simple apps
    and share quick prototypes. Your app is now online and can easily be shared with
    <html:span class="No-Break">other users.</html:span></html:p> <html:p>If anything
    went wrong, though, and you didn’t manage to complete the deployment, head over
    to the official <html:a id="_idIndexMarker1068"></html:a>documentation, and look
    for a solution: <html:a>https://docs.streamlit.io/streamlit-community-cloud/deploy-your-app</html:a>
    . In the Streamlit documentation, you’ll also find additional deployment options
    and configurations available <html:a id="_idIndexMarker1069"></html:a>that might
    be useful for your <html:span class="No-Break">future</html:span> <html:span class="No-Break"><html:a
    id="_idIndexMarker1070"></html:a></html:span><html:span class="No-Break">projects.</html:span></html:p>
    <html:a id="_idTextAnchor213"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>Summary</html:title></html:head>
    <html:body><html:div class="epub-source"><html:h1 id="_idParaDest-214">Summary</html:h1>
    <html:div id="_idContainer110"><html:p>In this chapter, we explored customizing
    and enhancing RAG workflows with LlamaIndex. We covered techniques to leverage
    open source LLMs such as Zephyr using tools such as LM Studio, offering cost-effective
    and privacy-focused alternatives to commercial models. The chapter discussed intelligent
    routing across multiple LLMs with services such as Neutrino and OpenRouter for
    optimized performance. Community-built Llama Packs were highlighted as powerful
    ways to rapidly prototype and build advanced components, and the chapter introduced
    the Llama CLI for streamlining RAG development and <html:span class="No-Break">deployment
    workflows.</html:span></html:p> <html:p>We talked about advanced tracing with
    Phoenix, allowing us to gain deep insight into application execution flows and
    pinpoint problems through visualization. The evaluation of RAG systems was covered
    using Phoenix’s relevance, hallucination, and QA correctness evaluators, ensuring
    the robust performance of our LlamaIndex apps. Streamlit’s deployment options,
    especially the Community Cloud service for easy application sharing, simplified
    the deployment process. A step-by-step guide demonstrated how to deploy the PITS
    tutoring application to <html:span class="No-Break">the cloud.</html:span></html:p>
    <html:p>With a strong grasp of customization, evaluation, and deployment techniques,
    developers can now build production-ready, optimized RAG applications tailored
    to their <html:span class="No-Break">unique requirements.</html:span></html:p>
    <html:p>Our journey continues with an exploration of the role of prompt engineering
    in enhancing the effectiveness of GenAI within the <html:span class="No-Break">LlamaIndex
    framework.</html:span></html:p></html:div></html:div></html:body></html:html>'
  prefs: []
  type: TYPE_NORMAL
