- en: <title>Customizing and Deploying Our LlamaIndex Project</title>
  prefs: []
  type: TYPE_NORMAL
- en: Customizing and Deploying Our LlamaIndex Project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Customizing **Retrieval-Augmented Generation** ( **RAG** ) components and optimizing
    performance is critical to building robust, production-ready applications with
    LlamaIndex. This chapter explores methods for leveraging open source models, intelligent
    routing across **large language models** ( **LLMs** ), and using community-built
    modules to increase flexibility and cost-effectiveness. Advanced tracing, evaluation
    methods, and deployment options are explored to gain deep insight, ensure reliable
    operation, and streamline the development life cycle.
  prefs: []
  type: TYPE_NORMAL
- en: 'Throughout this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Customizing our RAG components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using advanced tracing and evaluation techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to deployment with Streamlit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hands-on – a step-by-step deployment guide
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <title>Technical requirements</title>
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this chapter, you will need to install the following package in your environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Arize AI* *Phoenix* : https://pypi.org/project/arize-phoenix/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Three additional integration packages are required in order to run the sample
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Hugging Face* *embeddings* : https://pypi.org/project/llama-index-embeddings-huggingface/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Zephyr query* *engine* : https://pypi.org/project/llama-index-packs-zephyr-query-engine/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Neutrino* *LLM* : https://pypi.org/project/llama-index-llms-neutrino/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All code samples from this chapter can be found in the `ch9` subfolder of the
    book’s GitHub repository:'
  prefs: []
  type: TYPE_NORMAL
- en: https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex
  prefs: []
  type: TYPE_NORMAL
- en: <title>Customizing our RAG components</title>
  prefs: []
  type: TYPE_NORMAL
- en: Customizing our RAG components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'client = OpenAI(base_url="http://localhost:1234/v1") from llama_index.llms.openai
    import OpenAI llm = OpenAI(     api_base=''http://localhost:1234/v1'',     temperature=0.7
    ) print(llm.complete(''Who is Lionel Messi?'')) pip install llama-index-llms-neutrino
    from llama_index.core.llms import ChatMessage from llama_index.llms.neutrino import
    Neutrino llm = Neutrino(     api_key="<your-Neutrino_API_key>",     router="<Neutrino-router_ID>"
    ) while True:     user_message = input("Ask a question: ")     if user_message.lower()
    == ''exit'':         print("Exiting chat")         break     response = llm.complete(user_message)
        print(f"LLM answer: {response}")     print(f"Answered by: {response.raw[''model'']}")
    from llama_index.core import Settings Settings.llm = llm pip install llama-index-embeddings-huggingface
    from zephyr_pack.base import ZephyrQueryEnginePack from llama_index.readers import
    SimpleDirectoryReader reader = SimpleDirectoryReader(''files'') documents = reader.load_data()
    zephyr_qe = ZephyrQueryEnginePack(documents) response=zephyr_qe.run(     "Enumerate
    famous buildings in ancient Rome"     ) print(response) pip install chromadb llamaindex-cli
    rag --files files -q "What can you tell me about ancient Rome?" --verbose llamaindex-cli
    rag --chat'
  prefs: []
  type: TYPE_NORMAL
- en: For starters, let’s talk about which components of a RAG workflow can be customized
    in LlamaIndex. The short answer is *pretty much all of them, as we have seen already
    in the previous chapters* . The fact that the framework itself is flexible and
    allows customization of all the core components is a definite advantage. But leaving
    aside the framework itself, the core of a RAG workflow is actually the LLM and
    the embedding model it uses. In all the examples given so far, we have used the
    default configuration of LlamaIndex – which is based on OpenAI models. But, as
    we already briefly discussed in *Chapter 3* , *Kickstarting Your Journey with
    LlamaIndex* , there are both good reasons and enough options available to choose
    other models – both commercial variants offered by established companies in this
    market, and open source models, which can be hosted locally, offering private
    alternatives, and substantially reducing the costs of a large-scale implementation.
  prefs: []
  type: TYPE_NORMAL
- en: But first, some background.
  prefs: []
  type: TYPE_NORMAL
- en: How LLaMA and LLaMA 2 changed the open source landscape
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In early 2023, Meta AI introduced the **Large Language Model Meta AI** ( **LLaMA**
    ) family, offering a notable leap in accessibility for LLMs by releasing model
    weights to the research community. Following this, LLaMA 2 was launched in July
    2023, with improvements such as increased data for training and expanded model
    sizes, alongside models fine-tuned for dialogue under less restrictive commercial
    use conditions. Meta developed and launched three versions of LLaMA 2 with 7,
    13, and 70 billion parameters, respectively. While the basic structure of these
    models stayed similar to the original LLaMA versions, they were trained with 40%
    additional data compared to the original models, in order to enhance their foundational
    capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Despite some controversy regarding its open source status, the initiative marked
    a significant contribution to the open source ecosystem, triggering a new wave
    of community-based research and application development. The model consistently
    showcased competitive performance in tests against other leading LLMs, proving
    its advanced capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Further down the line, these releases have led to the creation of tools such
    as *llama.cpp* by Georgi Gerganov ( https://github.com/ggerganov/llama.cpp ),
    enabling the operation of these sophisticated models on more modest hardware,
    thus democratizing access to cutting-edge AI technologies.
  prefs: []
  type: TYPE_NORMAL
- en: Quick note
  prefs: []
  type: TYPE_NORMAL
- en: '*llama.cpp* is an efficient C/C++ implementation of Meta’s LLaMA architecture
    for LLM inference. Hugely popular in the open source community, with more than
    43,000 stars on GitHub and over 930 releases, this foundational framework has
    sparked the development of many other similar tools and services such as Ollama,
    Local.AI, and others. These updates and advances signaled that AI research was
    changing, focusing more on making information freely available and making sure
    AI models can run on simpler computers and other edge devices. This opened up
    more possibilities for using **generative AI** ( **GenAI** ) and encouraged new
    ideas and improvements everywhere.'
  prefs: []
  type: TYPE_NORMAL
- en: 'I won’t go into a detailed discussion of all the currently available tools
    for running local LLMs. This is because there is already a plethora of available
    methods by which various open source models can be run on the local system. And
    not just local LLMs: there’s also an increasing number of service providers offering
    access either to their own proprietary AI models or providing cloud-hosted access
    to open source models, and the good news is that LlamaIndex already provides built-in
    support for many of them. You can always consult the official documentation of
    the framework for a detailed overview of the supported models, along with examples
    of how they can be used: https://docs.llamaindex.ai/en/stable/module_guides/models/llms/modules.html
    .'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead, I will try to offer you an alternative that I personally find very
    convenient for two important reasons: it is very easy to implement, and your existing
    code can be reused with only a few minimal changes. For beginner coders and tinkerers
    wanting to quickly experiment with an idea or build simple prototypes, this may
    be one of the best solutions.'
  prefs: []
  type: TYPE_NORMAL
- en: Running a local LLM using LM Studio
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Built on top of the `llama.cpp` library, **LM Studio** ( https://lmstudio.ai/
    ) provides a very user-friendly graphical interface for LLMs. It allows us to
    download, configure, and locally run almost any open source model available on
    Hugging Face. A great resource, especially for non-technical users, LM Studio
    offers two ways of interacting with a local LLM: through a chat UI similar to
    OpenAI’s ChatGPT or via an OpenAI-compatible local server. This second option
    makes it particularly useful because we can easily adapt any LlamaIndex application
    natively designed to use OpenAI’s LLMs with very few modifications. We’ll get
    to that in a moment, but first, let’s see how to get things started with LM Studio.'
  prefs: []
  type: TYPE_NORMAL
- en: To start using this tool, you’ll first have to download and install the right
    version, depending on your operating system. Releases are available for Mac, Windows,
    and Linux. The installation steps are self-explanatory and well documented on
    their website.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once installed, the LM Studio GUI starts with a **Model Discovery** screen
    where you can type any model or model family name and get a list of matching model
    builds available for download. We’ll use the popular **Zephyr-7B** model for our
    example ( https://huggingface.co/HuggingFaceH4/zephyr-7b-beta ). I have specifically
    chosen Zephyr because, albeit a compact model, it demonstrates the effectiveness
    of distilling an LLM into a more manageable size. Derived from **Mistral-7B**
    , Zephyr-7B establishes a new benchmark for chat models with 7 billion parameters,
    surpassing the performance of **LLAMA2-CHAT-70B** on the Hugging Face *LMSYS Chatbot
    Arena Leaderboard* ( https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard
    ). *Figure 9* *.1* shows a typical output when searching for the `zephyr-7b` keyword:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_09_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 – LM Studio screenshot displaying search results
  prefs: []
  type: TYPE_NORMAL
- en: 'In the search results screen, you’ll see two panels:'
  prefs: []
  type: TYPE_NORMAL
- en: The one on the left contains all models that match your search query. In our
    case, these are different builds of the Zephyr-7B model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The right panel lists all the **Generative Pre-trained Transformer-Generated
    Unified Format** ( **GGUF** ) file versions available for download
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: About GGUF files
  prefs: []
  type: TYPE_NORMAL
- en: GGUF is a specific file format used for storing models for inference. Enhancing
    model sharing and usage efficiency, this format has quickly become a popular way
    of storing and distributing models throughout the open source community.
  prefs: []
  type: TYPE_NORMAL
- en: For most models, you’ll get an entire list of GGUF files available. Each one
    will have its own characteristics, but probably the most important characteristic
    is the **quantization** level.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding LLM quantization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Running an open source LLM on typical consumer hardware can prove challenging
    mainly because of its large memory footprint and high computational requirements.
    While some consumer-grade GPUs can aid in this regard, they may not be as effective
    as enterprise-level hardware in handling the demands of LLMs. That’s why we need
    quantization. The goal of applying quantization – a post-training optimization
    technique – to an AI model is to optimize it for better performance and efficiency,
    particularly in terms of speed and memory usage, without significantly compromising
    its accuracy or output quality.
  prefs: []
  type: TYPE_NORMAL
- en: The quantization process achieves this by converting the model’s parameters
    – typically stored as 32-bit floating-point numbers – to lower-bit representations,
    such as **16-bit floating-point** ( `FP16` ), **8-bit integers** ( `INT8` ), or
    even lower. It’s a kind of approximation process that works by reducing the numerical
    precision used to represent the model’s parameters, combined with complex techniques
    to maintain as much accuracy as possible. Modern quantization techniques are designed
    to minimize accuracy loss, often resulting in models that are nearly as accurate
    as their full-precision counterparts.
  prefs: []
  type: TYPE_NORMAL
- en: A simple analogy to help you better understand the concept
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you have a recipe that calls for very precise measurements, such as
    `1.4732` cups of flour. In practice, you might round this to 1.5 cups, as the
    difference is negligible in most cases and the difference will not affect the
    end result. This is similar to quantization, where we reduce the precision of
    the model’s parameters to make the model more efficient while maintaining acceptable
    accuracy. But instead of cups of flour, we reduce the numerical precision of the
    model’s parameters. Instead of using 16 bits to store a parameter as 23.7, we
    could quantize it into 8 bits as 23\. This directly translates to less memory
    usage and faster processing times. However, there is a trade-off between model
    size, speed, and accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: With an acceptable loss of accuracy, this process can significantly reduce the
    size of the model and the computational resources required for both training and
    inference phases, making it more feasible to deploy these models on consumer hardware.
    Generally, the lower the bit representation (such as `INT4` or even binary), the
    smaller and faster the model becomes, but at a higher risk of accuracy loss.
  prefs: []
  type: TYPE_NORMAL
- en: Being built on top of llama.cpp, LM Studio can take advantage of any compatible
    GPUs that could be used during the inference process. This feature is commonly
    called *GPU offloading* and means that computing operations can be partially or
    even entirely transferred from the CPU to the GPU. Given the fact that a modern
    GPU is capable of handling highly parallel computing tasks more efficiently than
    CPUs, this can dramatically speed up the inference process. It also reduces the
    load on the CPU, thus providing an overall balanced improvement of system performance.
    The main limitation when attempting GPU offloading is the amount of video memory
    available on your GPU. In order to run efficiently, the GPU must load the model
    in the video memory first.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because of this, apart from the quantization level, the GGUF files in the right
    panel will also have a flag showing three possible compatibility scenarios, each
    represented by a different color:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Green** : This means your GPU has enough video memory to load the model and
    execute the inference. In most cases, this is the ideal scenario'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Blue** : Not ideal, but still provides a considerable uplift in performance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gray** : This may or may not work depending on the model architecture'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Red** : Unfortunately, this means you won’t be able to run this version on
    your machine, the most probable reason being that its size exceeds your total
    system memory'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pro tip
  prefs: []
  type: TYPE_NORMAL
- en: 'A very handy tool for approximating the required VRAM for a particular model
    given a particular quantization level can be found on the Hugging Face website:
    https://huggingface.co/spaces/hf-accelerate/model-memory-usage'
  prefs: []
  type: TYPE_NORMAL
- en: So, which model should you choose?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The general rule of thumb is that with a lower quantization level, less memory
    will be required and the inference process will be faster. The trade-off is decreased
    accuracy. For example, a 3-bit quantization will always result in less accuracy
    than a 6-bit quantization.
  prefs: []
  type: TYPE_NORMAL
- en: Once you’ve made a decision on the exact model version, the next step is to
    download the model on your machine. But first, make sure you have the necessary
    space on your hard drive. There’s a handy status bar on the bottom of the UI to
    monitor the status of the download.
  prefs: []
  type: TYPE_NORMAL
- en: 'After the download is complete, moving to the **Chats** screen will display
    something similar to *Figure 9* *.2* :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_09_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 – LM Studio’s chat UI
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the interaction method that I mentioned at the beginning of this section
    – the one resembling the ChatGPT interface. In this screen, you’ll be able to
    do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Select the desired AI model from a list of all downloaded ones. To choose your
    model, use the *model selector* on top of the screen. You’ll have to wait for
    a few moments until the model is loaded into memory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Configure any available parameters of the model using the *configuration panel*
    on the right side. We’ll talk in more detail about that in a moment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: See a list of previous chats on the left side.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chat with the model using a familiar interface inspired by ChatGPT.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'There are a number of parameters that you can tweak in the configuration panel.
    The most important ones are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Preset** : Some models come with predefined configurations that you can load
    from presets. For an easy start, I would recommend selecting the model’s specific
    preset from the list. For example, there is a Zephyr preset that can be used with
    all Zephyr-based models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**System Prompt** : This prompt will set the initial context of the conversation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GPU Offload** : Allows you to configure the number of model layers to be
    offloaded to the GPU. Depending on the model you’re using and your available GPU,
    you may want to gradually experiment with increasing values while checking for
    model stability. Higher values can sometimes produce errors. If you feel confident,
    use -1 to offload all the model’s layers to the GPU'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Context Length** : Allows you to define the maximum context window to be
    used'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Changing some of these parameters may trigger a model reload, so you’ll have
    to be patient until it completes the process. Once you have customized everything,
    the floor is yours – enjoy chatting with your local LLM.
  prefs: []
  type: TYPE_NORMAL
- en: So far, so good, but where’s the RAG part in all this?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For that, we’ll have to go to the **Local Inference Server** screen, which
    you can do by pressing the double-arrow icon on the left-side menu. You’ll be
    presented with a UI similar to *Figure 9* *.3* :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Image96391.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 – The local Inference Server interface in LM Studio
  prefs: []
  type: TYPE_NORMAL
- en: The configuration options from the right-side panel are almost identical to
    the ones in the **Chat** screen. In the beginning, you can leave the *server configuration*
    options as default. The *usage* section tells you how to interact with the API.
    One of the great aspects of LM Studio is that it emulates the OpenAI API. That
    means your already existing code will need very few changes to work with a local
    LLM hosted through LM Studio.
  prefs: []
  type: TYPE_NORMAL
- en: All you have to do at this point is to click the **Start Server** button, and
    you’re good to go.
  prefs: []
  type: TYPE_NORMAL
- en: Quick note
  prefs: []
  type: TYPE_NORMAL
- en: Please keep in mind that while the API server is running, the chat UI will be
    disabled, so you won’t be able to use both at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see exactly what we need to change in our code if we want to port it
    to a local LLM using this method. If we look at the recommendation in the *usage*
    section, we’ll see that a single change is necessary:'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, because LlamaIndex has its own implementation of the OpenAI API client,
    in our case, we’ll have to use the `api_base` parameter like this:'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the only real change we have to make is pointing the `llm` instance
    toward our local server instead of the OpenAI one. The rest of the code remains
    unchanged. After running this example, you’ll see actual requests coming from
    our code and responses coming from the API in LM Studio’s log screen. If you want
    to permanently reconfigure the LLM in the entire code, you’ll have to define a
    `Settings` object and use it to configure global settings, as I showed you in
    *Chapter 3* , *Kickstarting Your Journey with LlamaIndex* , in the *Customizing
    the LLM used by* *LlamaIndex* section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Neat, isn’t it? Our data is now completely private, and we don’t have to pay
    for using an AI model in our RAG workflows anymore. Of course, there’s still a
    cost, albeit in electricity rather than tokens. The capability to run local models
    on modest hardware unlocks numerous possibilities that extend beyond mere text
    generation. This includes the opportunity to embrace fully multimodal experiences
    with models such as **LLaVa** ( https://huggingface.co/docs/transformers/main/en/model_doc/llava
    ), allowing for a wider range of applications: a wonderful tool that serves as
    an excellent resource for rapid prototyping or exploring diverse ideas.'
  prefs: []
  type: TYPE_NORMAL
- en: However, keep in mind that LM Studio is governed by a licensing model, which
    restricts its use to personal, non-commercial purposes. To utilize LM Studio for
    commercial applications, obtaining permission from the developers is necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Routing between LLMs using services such as Neutrino or OpenRouter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes, a single LLM may not be ideal for every single interaction. In complex
    RAG scenarios, finding the best mix between cost, latency, and precision could
    prove to be a difficult task when forced to choose a single LLM for everything.
    But what if we could find a way to mix different LLMs in the same app and dynamically
    choose which one to use for each individual interaction? That is the exact purpose
    of third-party services such as **Neutrino** ( https://www.neutrinoapp.com/ )
    and **OpenRouter** ( https://openrouter.ai/ ). These types of services can significantly
    enhance a RAG workflow by providing intelligent routing capabilities for queries
    across different LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Neutrino’s smart model router, for instance, allows you to intelligently route
    queries to the most suited LLM for the prompt, optimizing both response quality
    and cost efficiency. This can be particularly useful in a RAG workflow where different
    types of queries may require different LLM strengths or specialties. For example,
    one model might be more effective at understanding and parsing the initial user
    query, while another might be better suited for generating responses based on
    retrieved documents. By employing a router, we can dynamically select the most
    suitable model for each task without hardcoding model choices into our application,
    thus enhancing flexibility and potentially improving the overall performance of
    our RAG system. *Figure 9* *.4* describes the working mechanism of a Neutrino
    router:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_09_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 – A diagram of the Neutrino smart routing feature
  prefs: []
  type: TYPE_NORMAL
- en: 'The great news is that both Neutrino and OpenRouter are supported as integration
    packages in LlamaIndex. Let’s have a look at a simple example that uses a custom
    Neutrino router to dynamically choose between different LLMs depending on the
    user query. To run this example, make sure you first install the Neutrino integration
    package by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: Once the package is installed, you should first sign up for an account and obtain
    an API key on the Neutrino website. The next step is to create an LLM router by
    selecting your desired LLMs as well as a *fallback* LLM. The fallback model will
    be used by default in case of errors or whenever the router cannot determine which
    LLM to use. During the router setup, you will also have the option of choosing
    to use Neutrino as a provider for the AI models or utilize your own API keys for
    each LLM. The last step in the router setup process requires you to provide a
    *router ID* . This ID will be used in the code to specify the router used by the
    service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how we can use the Neutrino router in LlamaIndex:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code first initializes the Neutrino router in the form of a LlamaIndex
    `llm` object, for which you’ll need to provide your Neutrino API key and the ID
    of the router you have defined. Next, it runs in a loop, continually taking questions
    from the user until the `''exit''` keyword is received:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The questions are submitted to the Neutrino router, and, in return, the script
    not only prints the answer but also the name of the LLM that was chosen by the
    router to generate the answer. You can play around and experiment with different
    types of questions. Based on whichever models you selected when you defined the
    router, you’ll see that it will send the questions to different LLMs, depending
    on their capabilities. Another, more general approach in using such a router would
    be to use the `Settings` class to create a global configuration using that `llm`
    object:'
  prefs: []
  type: TYPE_NORMAL
- en: This has the advantage that it configures every subsequent LlamaIndex component
    in our code to use the Neutrino router.
  prefs: []
  type: TYPE_NORMAL
- en: Pro tip
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’re not entirely satisfied with the decisions made by the router, Neutrino
    also gives you the ability to fine-tune your defined router by uploading a list
    of examples on which the router can be trained: https://platform.neutrinoapp.com/training-studio'
  prefs: []
  type: TYPE_NORMAL
- en: And Neutrino is just one example. OpenRouter works in a similar way, but it’s
    mostly focused on optimizing the cost of LLM calls, not necessarily the quality.
  prefs: []
  type: TYPE_NORMAL
- en: There are also other providers offering similar services, and the concept is
    bound to become more and more popular as hundreds of new AI models emerge every
    week. The ability to use LLM routing services enhances the RAG workflow by abstracting
    the complexity of model selection and management. As a result, we can focus on
    building and optimizing our applications instead of managing the underlying AI
    models.
  prefs: []
  type: TYPE_NORMAL
- en: What about customizing embedding models?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another important component that can be considered for customization in a RAG
    scenario is the underlying embedding model. Intensively used in scenarios where
    vector store indexes are employed, the embedding model can also be a source of
    concern regarding cost and privacy. That is why we may sometimes prefer using
    a local model in our RAG workflow. Again, the good news is that LlamaIndex provides
    out-of-the-box support for more than 30 embedding models. They can be used by
    installing embedding integration packages, documented on the *LlamaHub* website:
    https://llamahub.ai/?tab=embeddings .'
  prefs: []
  type: TYPE_NORMAL
- en: You can find a very simple example of how to configure LlamaIndex to use a local
    embedding model from Hugging Face in *Chapter 5* , *Indexing with LlamaIndex*
    , in the *Understanding* *embeddings* section.
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging the Plug and Play convenience of using Llama Packs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The fact that LlamaIndex offers us such a rich framework of low-level elements
    and methods for RAG is a double-edged sword. On the one hand, it is extremely
    useful to have a tool available for almost any practical problem you have to solve.
    On the other hand, to successfully implement these tools, we must first spend
    a fair amount of time familiarizing ourselves with each one. Then comes the fine-tuning
    and optimization phase for each component. We are already talking about a significant
    effort in the development and optimization process. Sometimes, in order to be
    able to test an idea with a rapid prototype, it would be preferable if we already
    had some advanced ready-made modules. Imagine some *Lego* pieces already structured
    into functional sub-assemblies: a roof, a window, a bus stop, and so on. Well,
    we have that to hand.'
  prefs: []
  type: TYPE_NORMAL
- en: Created and continually improved by the flourishing LlamaIndex community, **Llama
    Packs** are pre-packaged modules that can be used to quickly build LLM applications.
    Just like some pre-built Lego structures, they provide reusable components such
    as LLMs, embedding models, and vector indexes that have been preconfigured to
    work together for various use cases in building a RAG pipeline. They are ready-to-use
    modules that can be downloaded and initialized with parameters to achieve a specific
    goal outside of the box.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: A pack could contain a full RAG pipeline to enable semantic search over text
    or an entire agent construct that could be immediately invoked in our app.
  prefs: []
  type: TYPE_NORMAL
- en: Llama Packs act as templates that can be inspected, customized, and extended
    as needed. The code for each pack is available, so developers can modify it or
    take inspiration to build their own applications. The beauty of this concept is
    that it provides **Plug and Play** ( **PnP** ) solutions without bloating the
    main code base of the framework. You can still use various integration packages
    together with the core components of LlamaIndex, and you can definitely customize
    any of these packs according to your needs.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll find a collection of all the published Llama Packs, together with all
    the other integration packages, available on LlamaHub ( https://llamahub.ai/?tab=llama_packs
    ). There’s a *README* file for each pack that provides details about its usage,
    and most of them also have detailed examples that you can follow and experiment
    with.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using them is very straightforward. Because, in this section, we talk about
    customizations in general and, among other options, moving our RAG workflows to
    local, open source models, I’m going to show you an example in the same line.
    We’ll explore a Llama Pack that allows for the creation of a query engine that
    relies entirely on locally hosted AI models. The pack implements `HuggingFaceH4/zephyr-7b-beta`
    as the LLM used for inference and `BAAI/bge-base-en-v1.5` as the embedding model.
    The pack is called Zephyr Query Engine Pack, and you can find it here: https://llamahub.ai/l/llama_packs-zephyr_query_engine
    .'
  prefs: []
  type: TYPE_NORMAL
- en: In a similar way to how LM Studio works, this pack can leverage existing GPUs
    to accelerate the inference process. Let’s see how it works.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step in using any Llama Pack is to download the actual modules on
    your local environment. This can be accomplished in three different ways:'
  prefs: []
  type: TYPE_NORMAL
- en: 'By installing the corresponding integration package. In our example, that would
    be accomplished with the following command:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This method is simple and permanently installs the required pack into your local
    environment. Its only disadvantage is that you cannot inspect and modify the pack
    code. For that purpose, the other two methods are recommended.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'By using the **command-line interface** ( **CLI** ). Here’s an example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ll discuss the CLI tool in more detail in the next section.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Directly in the code, using the from llama_index.llama_pack import download_llama_pack
    download_llama_pack(     "ZephyrQueryEnginePack", "./zephyr_pack" ) `download_llama_pack()`
    method and specifying a download location like this:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once downloaded into your local environment, the pack contents will be stored
    in a subfolder called `zephyr_pack` . You can inspect and modify anything in the
    code, adjusting it to your own needs. You will also need to install the Hugging
    Face `embeddings` integration package before running the example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a simple example of how to use this pack after downloading:'
  prefs: []
  type: TYPE_NORMAL
- en: Notice that we’re using the `run()` method, which, in this case, is a wrapper
    for the `query()` method used by the regular query engine.
  prefs: []
  type: TYPE_NORMAL
- en: This is just one of the more than 50 packs already available on LlamaHub at
    this moment. And the number keeps growing. The great news is that all of them
    are well documented and follow pretty much the same implementation model. So,
    next time you’re faced with a practical scenario that needs combining low-level
    components into more advanced elements, instead of reinventing the wheel, I encourage
    you to spend some time browsing LlamaHub for a potential ready-made solution for
    your problem. Llama Packs accelerates LLM app development by letting developers
    tap into pre-built components tailored for common use cases. Both ready-made solutions
    and customizable templates are available to kickstart projects.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Llama CLI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another very useful tool in the LlamaIndex arsenal is the `llamaindex-cli`
    utility. Installed together with the LlamaIndex libraries, the tool can be accessed
    very easily from the command line and can be used for various purposes, including
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Downloading Llama Packs, as seen in the previous section. The syntax to download
    a Llama Pack is given as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Upgrading source code from versions older than LlamaIndex v. `0.10` . Due to
    the fact that version 0.10 brought many changes related to the code structure
    and how to use certain modules in the framework, the authors of LlamaIndex provided
    developers with this automatic upgrade tool. Basically, it automatically modifies
    the code written on older versions and updates it to the new structure introduced
    with v0.10 for an easier transition. The syntax used for this feature is the following
    to process all sources in a given folder simultaneously:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Or execute the following command to upgrade a single file:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: By far the most interesting capability is enabled by using the `rag` argument.
    This feature allows you to build a RAG workflow directly from the command line
    without having to write any code. By default, the command-line RAG mode uses local
    storage for embeddings based on a Chroma DB database and OpenAI’s GPT-3.5 Turbo
    model as LLM. For privacy reasons, keep in mind that this means that all data
    we upload will be sent to OpenAI by default.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How RAG works in the command line
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before we can use the RAG mode from the command line, we must first install
    the ChromaDB library in our local environment:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `llamaindex-cli` utility offers a variety of command-line parameters that
    enable users to interact with language models and manage local files efficiently.
    Here are descriptions of the most important command-line parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`--help` : Displays a help message, providing an overview of available commands
    and their usage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--files <FILES>` : Defines the name of the file or directory from where the
    tool will ingest our proprietary data. The contents will be ingested and embedded
    into the local vector database, enabling the RAG CLI tool to index the specified
    files and later retrieve context from them at query time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--question <QUESTION>` : Specifies the question you want to ask about the
    ingested files. Used for querying indexed content, leveraging the power of the
    LLM to extract information from our proprietary data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--chat` : Opens a chat **read-eval-print loop** ( **REPL** ) for an interactive
    Q&A session within the terminal. This provides a conversational interface to query
    the ingested documents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--verbose` : Enables verbose output during execution, offering detailed information
    about the tool’s operations that can be useful for troubleshooting and understanding
    the tool’s inner workings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--clear` : Clears out all currently embedded data from the local vector database.
    Because a Chroma database is used to store the embeddings, these will persist
    across the sessions. The `--clear` command is the equivalent of a reset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--create-llama` : Initiates the creation of a LlamaIndex application based
    on the selected files. This parameter extends the tool’s functionality beyond
    simple Q&A, enabling the development of full-stack applications with a backend
    and frontend, leveraging the ingested data. You’ll find a complete example of
    how to use it here: https://www.npmjs.com/package/create-llama#example .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Talking about examples, let’s have a look at a simple way to have a conversation
    with our files using the CLI RAG feature. We’ll use the contents of the `ch9\files`
    folder from our GitHub repository. So, make sure you’re running this script from
    inside that folder, which should contain some sample files:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, once the files have been ingested, for an interactive chat session
    with the data, you can use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: 'And just in case you need to customize the mechanics of the CLI RAG, a complete
    example can be found in the official documentation of the framework, here: [https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/rag_cli.html](https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/rag_cli.html)
    .'
  prefs: []
  type: TYPE_NORMAL
- en: Next, it’s time to dive deeper into the logic of our LlamaIndex applications.
  prefs: []
  type: TYPE_NORMAL
- en: <title>Using advanced tracing and evaluation techniques</title>
  prefs: []
  type: TYPE_NORMAL
- en: Using advanced tracing and evaluation techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: pip install "arize-phoenix[llama-index]" llama-hub html2text from llama_index.core
    import (     SimpleDirectoryReader,     VectorStoreIndex,     set_global_handler
    ) import phoenix as px px.launch_app() set_global_handler("arize_phoenix") documents
    = SimpleDirectoryReader('files').load_data() index = VectorStoreIndex.from_documents(documents)
    qe = index.as_query_engine() response = qe.query("Tell me about ancient Rome")
    print(response) input("Press <ENTER> to exit") from llama_index.core import (
        SimpleDirectoryReader,     VectorStoreIndex,     set_global_handler ) import
    phoenix as px px.launch_app() set_global_handler("arize_phoenix") documents =
    SimpleDirectoryReader('files').load_data() index = VectorStoreIndex.from_documents(documents)
    qe = index.as_query_engine() response = qe.query("Tell me about ancient Rome")
    print(response) from phoenix.session.evaluation import (     get_qa_with_reference,
        get_retrieved_documents ) from phoenix.experimental.evals import (     HallucinationEvaluator,
        RelevanceEvaluator,     QAEvaluator,     OpenAIModel,     run_evals ) from
    phoenix.trace import DocumentEvaluations, SpanEvaluations model = OpenAIModel(model="gpt-4-turbo-preview")
    retrieved_documents_df = get_retrieved_documents(px.Client()) queries_df = get_qa_with_reference(px.Client())
    hallucination_evaluator = HallucinationEvaluator(model) qa_correctness_evaluator
    = QAEvaluator(model) relevance_evaluator = RelevanceEvaluator(model) hallucination_eval_df,
    qa_correctness_eval_df = run_evals(     dataframe=queries_df,     evaluators=[hallucination_evaluator,
    qa_correctness_evaluator],     provide_explanation=True, ) relevance_eval_df =
    run_evals(     dataframe=retrieved_documents_df,     evaluators=[relevance_evaluator],
        provide_explanation=True, )[0] px.Client().log_evaluations(     SpanEvaluations(
            eval_name="Hallucination",         dataframe=hallucination_eval_df),     SpanEvaluations(
            eval_name="QA Correctness",         dataframe=qa_correctness_eval_df),
        DocumentEvaluations(         eval_name="Relevance",         dataframe=relevance_eval_df),
    ) input("Press <ENTER> to exit")
  prefs: []
  type: TYPE_NORMAL
- en: The process of building an LLM-based application using a tool such as LlamaIndex
    is very developer-friendly since the framework abstracts away a lot of technical
    stuff. But at the same time, this complicates things, for the very same reason.
    When things don’t work as planned, developers need to have effective ways of understanding
    why. They have to peel back all these layers of abstraction in order to pinpoint
    the root causes. In other words, we need to be able to see the inner mechanics
    of our code, understand how different components interact, and be able to identify
    underlying issues. That’s where tracing becomes a really important feature. On
    the other hand, because we have so many tools available and so many ways of building
    our solution, we need a way to benchmark different combinations and determine
    the best mix of tools and orchestrations. That’s where evaluation comes into play.
    Evaluation is essential for comparing various tool and method combinations until
    we find the right configuration for our specific needs. Together, tracing and
    evaluation form the backbone of a successful RAG development process, ensuring
    both transparency and optimal performance.
  prefs: []
  type: TYPE_NORMAL
- en: In *Chapter 3* , *Kickstarting Your Journey with LlamaIndex* , we already discussed
    simple logging methods that we can use to better understand what’s happening under
    the hood of our LlamaIndex apps. Now, it’s time to discover a much more advanced
    way in which we can understand and evaluate RAG applications. In this section,
    I will explain advanced tracing and evaluation using the **Phoenix framework**
    developed by Arize AI ( https://phoenix.arize.com/ ). Integrating LlamaIndex with
    specialized tracing and evaluation tools provides a sophisticated approach to
    understanding and optimizing RAG applications. Phoenix provides the necessary
    instrumentation together with a great visualization UI, making our RAG execution
    workflow really simple to understand.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make use of the advanced capabilities of the Phoenix framework, we must
    first install some necessary libraries in our environment:'
  prefs: []
  type: TYPE_NORMAL
- en: Tracing our RAG workflows using Phoenix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In Phoenix, tracing is built on the concept of **spans** and **traces** , which
    are fundamental for capturing the detailed execution flow of applications. A span
    represents a specific operation or unit of work within the application, tracking
    the start and end times, along with metadata that provides context about the operation.
    These spans are nested within traces, which aggregate multiple spans to depict
    the entire journey of a request through the application. This hierarchical structure
    allows developers to drill down into specific operations, understanding how each
    component contributes to the overall process. Phoenix’s tracing capabilities are
    designed to seamlessly integrate with LlamaIndex, enabling developers to instrument
    their RAG applications with minimal effort.
  prefs: []
  type: TYPE_NORMAL
- en: Because it features a client-server architecture, Phoenix is able to gather
    traces both locally and remotely. We can automatically collect telemetry data
    about each operation, including data ingestion, indexing, retrieval, processing,
    and any subsequent LLM calls. In the background, this data is collected by the
    Phoenix server, where it can be visualized and analyzed in real time.
  prefs: []
  type: TYPE_NORMAL
- en: Once the necessary requirements have been installed, using Phoenix is really
    easy. There are many advanced capabilities that you can explore with this framework,
    but I will show you the most simple and straightforward way to use it for tracing
    the execution of a LlamaIndex application. We’ll make use of a special method
    called `set_global_handler` , which conveniently configures LlamaIndex to use
    a certain tracing tool for every operation – in our case, the Phoenix framework.
  prefs: []
  type: TYPE_NORMAL
- en: 'Make sure you install the required packages before running the example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Apart from the obvious imports that will provide our basic RAG functionality,
    we’re also importing `set_global_handler` and the Phoenix library. The next part
    will be responsible for starting the Phoenix server and configuring LlamaIndex
    to use it as a global callback handler:'
  prefs: []
  type: TYPE_NORMAL
- en: 'From now on, every single operation performed by our app will generate traces
    that will get collected by the Phoenix server. Let’s build a simple query engine
    based on a `VectorStoreIndex` index and run a random query:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Because we need the server to be live in order to visualize the trace, we keep
    the script running with this line:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, with the script still running in the background, we can access the Phoenix
    UI at this URL: http://localhost:6006/. *Figure 9* *.5* shows what you’ll find
    in the Phoenix server UI:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_09_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.5 – A screenshot from the Phoenix server UI depicting our tracing output
  prefs: []
  type: TYPE_NORMAL
- en: Looking at this screenshot, we can see that the Phoenix server UI helps us visualize
    a complete trace of our code, divided into multiple spans. If you have successfully
    executed the previous sample code, our trace should consist of three different
    spans, each displayed on a separate line.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s talk about the columns in the screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first column, `kind` , contains the type of the span. It can be `chain`
    , `retriever` , `re-ranker` , `llm` , `embedding` , `tool` , or `agent` . We are
    already familiar with what these concepts represent in LlamaIndex, except for
    a chain. In Phoenix, a chain can be either the starting point for a series of
    operations in an LLM application or a connector linking different steps within
    the application workflow. In our example, the screenshot contains three spans:
    two chains and an embedding. They are displayed in the reverse order of their
    operation, beginning with the last one.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The second column, *name* , provides a more detailed description of the span.
    We can see that in our example, the first span represents a *query* , the second
    one is an *embedding* , and the third is a *Node-parsing* operation. The logic
    of our code is now clear: it first parsed the ingested documents into Nodes, then
    created a vector index by embedding the Nodes, and the final step was to run a
    query against that index.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next two columns, *input,* and *output* , show exactly what went as an input
    into the span and what was the final output produced by it. In our example, we
    only have values in these fields for the query span as this does not apply to
    the other ones.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *evaluations* column displays the results of the evaluation for each span.
    For now, that column should be empty as we have not yet executed any evaluation.
    We’ll cover this topic in the next section.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*start time* provides an exact timestamp for each span.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*latency* measures the total execution time for each span. This is really useful
    when trying to optimize our code for increased performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As the name implies, *total tokens* count the total number of tokens used by
    the corresponding operation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The last column, *status* , indicates whether the operation was completed successfully
    or not.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here comes the best part. If we now click on the *kind* column of the query
    span – the first one in our list – we’ll get a detailed visualization, similar
    to the one depicted in *Figure 9* *.6* :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_09_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.6 – Trace details visualized on the Phoenix server UI
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, we can now get a detailed understanding of each individual
    step performed during this span. In this case, we see a decomposed view of the
    query engine operation: first, the retrieval part, and then the final response
    synthesis using the LLM. By clicking on each individual step, we can explore its
    attributes and underlying mechanics. And because Phoenix runs locally, all this
    data remains private.'
  prefs: []
  type: TYPE_NORMAL
- en: Practical exercise
  prefs: []
  type: TYPE_NORMAL
- en: Here’s a useful exercise you could attempt now. Try to reconfigure some of the
    samples discussed in the previous chapters to use the Phoenix framework. You’ll
    get a better understanding of how different components work in LlamaIndex and
    also have a chance to familiarize yourself with this great tool.
  prefs: []
  type: TYPE_NORMAL
- en: 'And if you want to go deeper and explore more advanced tracing features of
    Phoenix, you’ll find everything you need in their official documentation: https://docs.arize.com/phoenix/
    .'
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s talk about how we can use Phoenix to evaluate and optimize our RAG
    apps.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating our RAG system
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When developing LLM-based systems, proper evaluation is essential for checking
    how well a RAG pipeline works. In general, LLM applications have to deal with
    very diverse inputs, and there is usually not a single, absolute answer they are
    supposed to return. That means evaluating them can prove to be a challenging task.
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, evaluating a RAG pipeline involves assessing key aspects such as
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Retrieval quality** : Evaluating the relevance and effectiveness of the retrieved
    Nodes in providing the necessary information to answer the query'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generation quality** : Assessing the quality of the final output, including
    its correctness, coherence, and adherence to the provided context'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Faithfulness** : Ensuring that the generated output is faithful to the retrieved
    information and does not introduce hallucinations or inconsistencies'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Efficiency** : Measuring the computational efficiency and scalability of
    the RAG pipeline, especially in real-world scenarios with large-scale datasets'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Robustness** : Testing the RAG system’s ability to handle diverse queries,
    edge cases, and potential adversarial inputs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To address these evaluation challenges, several tools and frameworks have been
    developed to facilitate the evaluation process. These tools aim to provide automated
    metrics, reference-based comparisons, and also human-in-the-loop evaluation methodologies.
    By leveraging these evaluation frameworks, we can obtain insights into the strengths
    and weaknesses of our RAG pipelines, identify areas for improvement, and iterate
    on their designs to enhance overall performance.
  prefs: []
  type: TYPE_NORMAL
- en: Because in the previous section, we’ve seen how Phoenix can help us with its
    tracing functionality, I’d like to continue building on the previous example and
    first explore some of the evaluation features provided by this framework.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Phoenix framework evaluation features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Since manual labeling and testing evaluation data can be very time-consuming,
    Phoenix uses GPT-4 as a reference to decide on the correctness of our RAG’s answers.
    This framework provides out-of-the-box support for batch processing, custom datasets,
    and pre-tested evaluation templates. Unlike traditional, more basic evaluation
    libraries that lack rigor for production environments, Phoenix ensures data science
    rigor, high throughput, and flexibility across different environments, making
    it significantly faster and more adaptable for evaluating both the model and the
    context in which it is used. Phoenix can be used for evaluating two important
    dimensions of a RAG workflow: **retrieval** and **LLM inference** .'
  prefs: []
  type: TYPE_NORMAL
- en: 'For retrieval, Phoenix evaluates the relevancy of the retrieved context. In
    other words, it verifies if the retrieved Nodes actually contain an answer to
    the query or not. When evaluating LLM inference, the framework checks three main
    attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Correctness** : This verifies if the system has accurately answered a question'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hallucinations** : This aims to identify any unrealistic or fabricated responses
    by the LLM in relation to the context it was provided'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Toxicity** : This checks for any harmful content in the AI’s responses, including
    racism, bias, or general toxicity'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because a complex RAG scenario could sometimes rely on many individual spans,
    being able to individually evaluate each one becomes an essential feature. This
    way, we can isolate the source of errors and stop them from propagating further
    in the flow. Since it uses an LLM for running evaluations, Phoenix returns not
    just the test result but also an explanation provided by the model. This can be
    very useful for understanding the root cause of a failed evaluation and pinpointing
    the misbehaving component in our RAG application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s have a look at a simple example to understand how Phoenix can be used
    for evaluation. In order to minimize costs and keep the code simple, we’re going
    to use the previous approach we used for the tracing example. We’re going to ingest
    the contents of our `ch9/files` folder, create a vector index, and run a simple
    query against the index. In a real scenario, you would probably run these evaluators
    against a much larger dataset in order to cover as many edge cases as possible
    and increase the probability of finding underlying issues in the pipeline. Here
    is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, the first part is identical to the previous example. It’s time to add
    the part responsible for the evaluation. We’ll begin by importing the necessary
    Phoenix components. Two functions, `get_retrieved_documents()` and `get_qa_with_reference()`
    , will be responsible for fetching the documents retrieved by queries and the
    queries with their reference answers for evaluation. We’re also importing three
    of the Phoenix evaluators: `HallucinationEvaluator` , `QAEvaluator` , and `RelevanceEvaluator`
    . These evaluators will assess the hallucination in responses, the correctness
    of question-answer pairs, and the relevance of retrieved documents, respectively.
    We also need to import `run_evals()` , which will be responsible for performing
    the evaluation tasks and returning DataFrames containing the evaluation results.
    Finally, the `DocumentEvaluations` and `SpanEvaluations` classes will be used
    to encapsulate evaluation results and display these results in the Phoenix server
    UI:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that the imports are complete, it’s time to prepare our evaluations. First,
    we declare the LLM that will be used to perform evaluations. This should always
    be the best available model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the evaluation model has been defined, it’s time to prepare our data.
    We’ll fetch the retrieved documents and queries in separate data frames. These
    data frames will later become the input for evaluator functions:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the data, we need to define evaluator functions and run the
    actual evaluations:'
  prefs: []
  type: TYPE_NORMAL
- en: 'When running the evaluators, notice that I’m setting the `provide_explanation`
    argument to `True` . This ensures that explanations for the evaluation scores
    are included in the response from the LLM. The last part involves encapsulating
    the results in corresponding `SpanEvaluations` and `DocumentEvaluations` classes
    and sending them to the Phoenix server so that they can be properly displayed
    in the UI:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Just like in the previous example, the input at the end keeps the script running
    until the user decides to exit by pressing the *Enter* key. This allows us to
    view and interact with the Phoenix app before closing it. If everything went smoothly,
    accessing the UI at http://localhost:6006/ should reveal an output similar to
    what we can see in *Figure 9* *.7* :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Image96429.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.7 – Visualizing evaluation results in the Phoenix server UI
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the *evaluations* column has been updated with the values returned
    by the evaluators we just executed. We can now see the results, as well as the
    rationale for each individual score.
  prefs: []
  type: TYPE_NORMAL
- en: 'The topic of evaluating RAG apps is huge and could probably become the subject
    of an entirely separate book. There are many nuances and different approaches
    that could be considered regarding evaluation. I’ve only shown you a tool – Phoenix
    – but there are many other options for this purpose, including LlamaIndex’s own
    instrumentation. If you’re planning to explore this topic deeper, I encourage
    you to start by reading the LlamaIndex official documentation here: https://docs.llamaindex.ai/en/stable/module_guides/evaluating/root.html
    . Also, get a better understanding of the complete capabilities of the Phoenix
    framework by reading their official documentation here: https://docs.arize.com/phoenix/
    .'
  prefs: []
  type: TYPE_NORMAL
- en: Other alternatives for evaluation – RAGAS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'While Phoenix provides a comprehensive evaluation framework for RAG pipelines,
    there are other alternatives available. Another notable framework is **Retrieval-Augmented
    Generation Assessment** ( **RAGAS** ), which is based on the techniques introduced
    by Es et al. (2023) in their paper, *RAGAS: Automated Evaluation of Retrieval
    Augmented Generation* ( https://doi.org/10.48550/arXiv.2309.15217 ). The RAGAS
    framework provides a practical implementation of these evaluation methods, along
    with additional features and integrations.'
  prefs: []
  type: TYPE_NORMAL
- en: RAGAS is specifically designed for evaluating and analyzing RAG systems. It
    offers a standardized approach to assess various aspects of a RAG pipeline, including
    retrieval quality, generation quality, and the interplay between the retrieval
    and generation components.
  prefs: []
  type: TYPE_NORMAL
- en: 'Key features of RAGAS include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Retrieval evaluation** : RAGAS assesses the quality of the retrieval component
    by measuring the relevance of the retrieved Nodes to the given query using metrics
    such as **Recall@k** – the proportion of relevant Nodes retrieved within the top
    *k* results, where k is a user-defined parameter. Another metric that measures
    retrieval quality is the **Mean Reciprocal Rank** ( **MRR** ) – measuring how
    quickly the system finds the first relevant Node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generation evaluation** : RAGAS also evaluates the quality of the generated
    text using a combination of automatic metrics and human evaluation. The automatic
    metrics include **Bilingual Evaluation Understudy** ( **BLEU** ), which measures
    the similarity between the generated text and a reference text by comparing overlapping
    word sequences, and **Recall-Oriented Understudy for Gisting Evaluation** ( **ROUGE**
    ), which calculates the overlap of words and word sequences between the generated
    text and the reference text. To complement these automatic metrics, RAGAS also
    incorporates human evaluation to assess aspects such as fluency, coherence, and
    relevance of the generated output, providing a comprehensive assessment of the
    generation quality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Retrieval-generation interplay** : The framework also analyzes the interplay
    between the retrieval and generation components by measuring how much the generated
    text relies on the retrieved Nodes. It introduces metrics such as **Retrieval
    Dependency** ( **RD** ), which quantifies how much the generated text depends
    on the retrieved Nodes, and **Retrieval Relevance** ( **RR** ), which measures
    the relevance of the retrieved Nodes to the generated text to quantify this relationship.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Simulation** : RAGAS includes a simulation component that allows us to simulate
    different retrieval scenarios and analyze their impact on the generation quality.
    This helps in understanding the robustness and generalization ability of RAG models
    under various retrieval conditions. By manipulating the retrieval results, users
    can test how the RAG model performs under scenarios such as retrieving irrelevant,
    partially relevant, or noisy data. The simulation feature provides insights into
    the interplay between the retrieval and generation components, enabling us to
    identify strengths and weaknesses and guide improvements in the RAG model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fine-grained analysis** : RAGAS enables fine-grained analysis of RAG pipelines
    by providing tools to visualize and interpret the retrieval-generation process,
    such as attention weights and individual Node contributions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A key advantage of this framework is that it enables reference-free evaluation
    of RAG pipelines, meaning it does not rely on ground truth annotations. This allows
    for more efficient and scalable evaluation cycles.
  prefs: []
  type: TYPE_NORMAL
- en: 'Compared to Phoenix, RAGAS offers a more focused evaluation framework specifically
    tailored for RAG systems. While Phoenix provides a general-purpose evaluation
    platform with features such as tracing, hallucination detection, and relevance
    assessment, RAGAS goes deeper into the intricacies of retrieval-generation interplay
    and also offers simulation capabilities. The framework provides seamless integration
    with LlamaIndex, simplifying the evaluation of LlamaIndex-based RAG systems. To
    keep things simple, I have not included any code examples in this case, but you
    can find detailed examples and documentation on the official project’s page, at
    this URL: https://docs.ragas.io/en/stable/howtos/integrations/llamaindex.html
    .'
  prefs: []
  type: TYPE_NORMAL
- en: It’s worth noting that RAGAS is a more recent framework compared to Phoenix,
    and while it shows great promise, it may take some time for it to see the same
    level of adoption in the research community.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: One thing to always keep in mind in terms of evaluation is the concept of model
    drift, which we have already covered in *Chapter 7* , *Querying Our Data, Part
    2 – Postprocessing and Response Synthesis* section. Model drift can impact our
    RAG pipeline when the LLM’s behavior gradually deviates from its intended purpose
    or when the quality of the generated output deteriorates. Regular or even continuous
    evaluation can help detect and mitigate this phenomenon, ensuring the RAG system
    remains reliable and effective in production environments.
  prefs: []
  type: TYPE_NORMAL
- en: By mastering tracing and evaluation techniques, you’ll be able to create a complete
    system for finding and fixing problems in an LLM application. Using evaluations
    and tracing together, you can spot where things go wrong, figure out why, and
    see which part of your application needs to be improved.
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s now time to focus our attention on our side project: the PITS tutor. In
    this chapter, we’ll finally get to deploy its components and run it as a standalone
    application. But first, let’s have a short introduction to the different deployment
    options provided by **Streamlit** .'
  prefs: []
  type: TYPE_NORMAL
- en: <title>Introduction to deployment with Streamlit</title>
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to deployment with Streamlit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As I explained in *Chapter 2* , *LlamaIndex: The Hidden Jewel - An Introduction
    to the LlamaIndex Ecosystem* , I chose Streamlit as the backbone for our side
    project because of its simplicity and the many deployment options it provides.
    Streamlit offers an easy approach to deploying your applications, making it possible
    to share your work with a broader audience with minimal effort. If you successfully
    followed the installation steps in *Chapter 2* , your local environment should
    already be ready for the next steps. However, just in case, before proceeding,
    make sure you have completed the necessary installation mentioned in *Chapter
    2* , in the *Discovering Streamlit – the perfect tool for quick build and* *deployment*
    section.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we’re all set up, let’s explore the deployment options available for
    Streamlit applications. Beyond the simplest method of running apps on your local
    machine, Streamlit offers a variety of web deployment solutions to cater to different
    needs and preferences:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Streamlit Community Cloud** : This user-friendly platform is the most straightforward
    option for deploying Streamlit apps, enabling users to deploy directly from their
    GitHub repositories in just a few clicks. It requires minimal configuration, and
    once deployed, your app will be accessible via a unique URL on Streamlit Community
    Cloud, making it easy to share with others.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Custom cloud services** : For those seeking greater control over their deployment
    environment, Streamlit apps can be deployed on various cloud services, including
    **Amazon Web Services** ( **AWS** ), **Google Cloud Platform** ( **GCP** ), and
    Azure. Deployment on these platforms might involve additional steps such as containerizing
    your app with Docker and configuring cloud-specific services such as AWS Elastic
    Beanstalk, Google App Engine, or Azure App Service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Self-hosting** : If you have your own servers, opting to self-host your Streamlit
    applications gives you maximum control over the deployment environment and resources.
    This method involves setting up a server environment capable of running Python
    applications, installing Streamlit, and configuring your network for Streamlit
    app access. The self-hosting option answers to specific requirements for security,
    performance, or customization that cloud platforms cannot meet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Heroku** : Heroku (https://www.heroku.com/) is another well-known platform
    for deploying Streamlit apps due to its simplicity and a free tier suitable for
    small projects and prototypes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Streamlit in Snowflake** : For use cases prioritizing security and **role-based
    access control** ( **RBAC** ), Streamlit’s integration with Snowflake offers a
    secure coding and deployment environment within the Snowflake platform. You can
    easily sign up for a trial Snowflake account, create a warehouse and database
    for your apps, and deploy Streamlit applications directly within Snowflake.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Each of these deployment options offers unique benefits, with different advantages
    in terms of level of control, scalability, security requirements, and budget constraints.
    However, I have chosen to show you the simplest option and probably the most appropriate
    choice for our PITS application: deployment in Streamlit Community Cloud. However,
    for a commercial-ready solution, the other options would have been a better choice.'
  prefs: []
  type: TYPE_NORMAL
- en: <title>HANDS-ON – a step-by-step deployment guide</title>
  prefs: []
  type: TYPE_NORMAL
- en: HANDS-ON – a step-by-step deployment guide
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'from user_onboarding import user_onboarding from session_functions import load_session,
    delete_session, save_session from logging_functions import reset_log from quiz_UI
    import show_quiz from training_UI import show_training_UI import streamlit as
    st def main():     st.set_page_config(layout="wide")     st.sidebar.title(''P.I.T.S.'')
        st.sidebar.markdown(''### Your Personalized Intelligent Tutoring System'')
        if ''show_quiz'' in st.session_state and     st.session_state[''show_quiz'']:
            show_quiz(st.session_state[''study_subject''])     elif ''resume_session''
    in st.session_state and     st.session_state[''resume_session'']:         st.session_state[''show_quiz'']
    = False         show_training_UI(st.session_state[''user_name''],         st.session_state[''study_subject''])
        elif not load_session(st.session_state):         user_onboarding() else:         st.write(f"Welcome
    back {st.session_state[''user_name'']}!")         col1, col2 = st.columns(2)         if
    col1.button(f"Resume your study of         {st.session_state[''study_subject'']}"):
                st.session_state[''resume_session''] = True             st.rerun()
            if col2.button(''Start a new session''):             delete_session(st.session_state)
                reset_log()             for key in list(st.session_state.keys()):
                    del st.session_state[key]             st.rerun()'
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s time to share our PITS tutoring application with the world. However, as
    a quick side note, keep in mind that the current version is far from being ready
    for use in a multi-user, real-world environment. To keep the code base small and
    the deployment steps simple, I have designed PITS as a pure experiment in LlamaIndex.
    After all, the purpose of this book was not to delve into the architectural intricacies
    of building a full-fledged Streamlit application but rather to explain the tools
    and features that are available in LlamaIndex. This is the main reason why some
    of the PITS source files are not explained in detail in this book. Rest assured,
    however, that you will find plenty of comments in these modules, and if the comments
    available in the GitHub code aren’t enough, you can always explore the official
    Streamlit documentation and get a better understanding of the framework here:
    https://docs.streamlit.io/ .'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, a brief introduction to the way Streamlit applications is built is
    in order. We’ll use one of the PITS UI files as an example, and I’ll walk you
    through the code to give you a basic understanding of the principles of Streamlit
    applications. Here is the code for `app.py` , our main program in the PITS structure.
    This code is responsible for orchestrating the execution of the various components
    that make up the tutoring application. It acts as the central hub, routing users
    through the onboarding process, handling session management, and dynamically presenting
    the quiz and training interfaces based on user interactions and session data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by importing the necessary modules and components, including Streamlit.
    We also import several custom functions from the other modules, such as `user_onboarding`
    , `load_session` , `delete_session` , `save_session` , `reset_log` , `show_quiz`
    , and `show_training_UI` , each serving a specific role in the application’s flow.
    Following the imports, the `main()` function encapsulates the application’s logic:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The use of `st.set_page_config` at the beginning establishes the basic layout
    of our web application. Streamlit provides a sidebar feature, and we’ll make use
    of that to streamline our UI. Next, the application’s flow is primarily controlled
    through conditional statements that check for the presence of certain keys in
    Streamlit’s `(st.session_state)` session state. This session state acts as persistent
    storage across reruns of the app within the same browser session, allowing the
    application to remember user choices, entered information, and other stateful
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: A quick note on Streamlit’s session state
  prefs: []
  type: TYPE_NORMAL
- en: Web applications are inherently stateless, meaning each request and response
    between the client and server are independent. Streamlit’s session state allows
    us to overcome this by providing a way to maintain state across reruns of the
    app within the same browser session. This is essential for creating an interactive
    and user-friendly experience, as it allows the application to remember user choices,
    inputs, and actions without requiring the user to re-enter data after every interaction.
  prefs: []
  type: TYPE_NORMAL
- en: 'I’ll briefly explain what happens in the previous part of the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Quiz display logic** : If the user has opted to take a quiz ( `''show_quiz''
    in st.session_state` ), the quiz interface is displayed by calling `show_quiz()`
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resuming sessions** : If the user has already chosen to resume an existing
    session ( `st.session_state[''resume_session'']=True` ), the app will take them
    directly to the training UI.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**User onboarding and session management** : `load_session(st.session_state)`
    checks whether session data exists. If not, the user is directed to the onboarding
    process through `user_onboarding().`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, let’s see what happens when an existing session is found but `show quiz`
    is `False` and the user hasn’t clicked on the **Resume session** button yet:'
  prefs: []
  type: TYPE_NORMAL
- en: The first operation in this `else` block is displaying a welcome back message.
    The app then displays two buttons, allowing the user to decide whether they want
    to resume the existing training session or start a fresh one. Choosing to start
    a new session will basically reset everything and rerun the entire code to start
    the application from the beginning. Resuming the session at this point will determine
    the app to run `show_training_UI` and continue the existing training session.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying our PITS project on Streamlit Community Cloud
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Because of the way the internal folder structure of the Streamlit Community
    Cloud environment is implemented, we’ll have to make a few modifications to our
    PITS folder structure. The plan is to deploy the application straight from a GitHub
    repository. However, one of the requirements for deploying from GitHub into the
    Community Cloud environment is that the main `.py` file is hosted in the `root`
    folder of the repository. That is not the case for PITS as the folder structure
    is a bit different. `app.py` , which is the main file in our case, is currently
    found in the `Building-Data-Driven-Applications-with-LlamaIndex\PITS_APP` folder.
    To fix that, we’ll first make a copy of the `PITS_APP` subfolder, and then we’ll
    initiate a new GitHub repository from that new folder. To keep things simple and
    require minimum changes, I will guide you on how to create a new repository containing
    just the PITS app and then deploy it from your own GitHub account:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s create a copy of our local `PITS_APP` subfolder. Open Command
    Prompt and navigate to the `Building-Data-Driven-Applications-with-LlamaIndex`
    folder of your cloned repository. From that folder, type the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This will create a folder on your `C:` drive containing only the source files
    of the PITS application. If you navigate to the newly created folder and list
    its contents with the `dir` command, the output should look like *Figure 9* *.8*
    :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B21861_09_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.8 – The contents of the C:\PITS_APP folder
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to sign in to your GitHub account and create a new repository.
    Let’s name it `PITS_ONLINE` , as in *Figure 9* *.9* :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B21861_09_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.9 – Creating a new GitHub repository named PITS_ONLINE
  prefs: []
  type: TYPE_NORMAL
- en: 'Once created, note the repository URL for the next steps. Next, we’ll initialize
    a new local repository in the desired folder. Open your CLI and navigate to the
    folder you want to turn into a separate repository – `C:\PITS_APP` – then execute
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, add and commit the existing files by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'It’s now time to link your local repository to the GitHub repository you created.
    Replace the URL with your GitHub URL and append `.git` at the end in the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'And finally, we push the contents to the new online repository with the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If everything went smoothly you should now have a brand-new GitHub repository
    containing the PITS source code.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s handle the Community Cloud deployment next.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deploying Streamlit applications into their Community Cloud environment is
    a fairly simple and straightforward process. To begin our deployment, the first
    step is to sign up for a free Streamlit account here: https://share.streamlit.io/signup
    . The best option is to use your GitHub account both for signing up and signing
    in to your Streamlit account. Once logged in, simply click on the **New app**
    button to begin the deployment process. You’ll be taken to a screen similar to
    what you can see in *Figure 9* *.10* :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_09_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.10 – Deploying an application into Streamlit Community Cloud
  prefs: []
  type: TYPE_NORMAL
- en: If you signed in to Streamlit using GitHub, you should already have the `PITS_ONLINE`
    repository listed as an option. Select it, then, under the **Main file path**
    field, change the default value to `app.py` and then click **Deploy** . From here,
    the Streamlit deployment service takes over and prepares the required environment
    for your application. This might take a while, but if you want to check on the
    progress, you can always expand the **Manage app** section on the bottom right
    of your screen. When everything is ready, the application should start automatically.
  prefs: []
  type: TYPE_NORMAL
- en: You can now ingest your existing training materials, have PITS generate slides
    and narrations about your desired study topic, and ask its chatbot any questions
    related to the contents.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Don’t forget, you’re using your own API key. To keep costs under control, you
    should first experiment on a limited scale by uploading some small training resources
    and always keeping an eye on the OpenAI API usage. The good news is that the majority
    of the cost is incurred during slides and narration generation. However, once
    that is completed, the resulting material is stored and reused in future sessions.
  prefs: []
  type: TYPE_NORMAL
- en: Simple, isn’t it? Although offering an environment with limited resources, the
    Streamlit Community Cloud service makes it really easy to deploy simple apps and
    share quick prototypes. Your app is now online and can easily be shared with other
    users.
  prefs: []
  type: TYPE_NORMAL
- en: 'If anything went wrong, though, and you didn’t manage to complete the deployment,
    head over to the official documentation, and look for a solution: https://docs.streamlit.io/streamlit-community-cloud/deploy-your-app
    . In the Streamlit documentation, you’ll also find additional deployment options
    and configurations available that might be useful for your future projects.'
  prefs: []
  type: TYPE_NORMAL
- en: <title>Summary</title>
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored customizing and enhancing RAG workflows with LlamaIndex.
    We covered techniques to leverage open source LLMs such as Zephyr using tools
    such as LM Studio, offering cost-effective and privacy-focused alternatives to
    commercial models. The chapter discussed intelligent routing across multiple LLMs
    with services such as Neutrino and OpenRouter for optimized performance. Community-built
    Llama Packs were highlighted as powerful ways to rapidly prototype and build advanced
    components, and the chapter introduced the Llama CLI for streamlining RAG development
    and deployment workflows.
  prefs: []
  type: TYPE_NORMAL
- en: We talked about advanced tracing with Phoenix, allowing us to gain deep insight
    into application execution flows and pinpoint problems through visualization.
    The evaluation of RAG systems was covered using Phoenix’s relevance, hallucination,
    and QA correctness evaluators, ensuring the robust performance of our LlamaIndex
    apps. Streamlit’s deployment options, especially the Community Cloud service for
    easy application sharing, simplified the deployment process. A step-by-step guide
    demonstrated how to deploy the PITS tutoring application to the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: With a strong grasp of customization, evaluation, and deployment techniques,
    developers can now build production-ready, optimized RAG applications tailored
    to their unique requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Our journey continues with an exploration of the role of prompt engineering
    in enhancing the effectiveness of GenAI within the LlamaIndex framework.
  prefs: []
  type: TYPE_NORMAL
