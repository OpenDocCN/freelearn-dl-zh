["```py\n    openai==1.37.1\n    datasets==2.20.0\n    tqdm==4.66.4 \n    ```", "```py\n    import concurrent.futures\n    import json\n    import random\n    import re\n    from concurrent.futures import ThreadPoolExecutor\n    from typing import List, Tuple\n    from datasets import Dataset\n    from openai import OpenAI\n    from pydantic import BaseModel, Field\n    from tqdm.auto import tqdm \n    ```", "```py\n    def load_articles_from_json(file_path: str) -> Dataset:\n        with open(file_path, \"r\") as file:\n            data = json.load(file)\n        return Dataset.from_dict(\n            {\n                \"id\": [item[\"id\"] for item in data[\"artifact_data\"]],\n                \"content\": [item[\"content\"] for item in data[\"artifact_data\"]],\n                \"platform\": [item[\"platform\"] for item in data[\"artifact_data\"]],\n                \"author_id\": [item[\"author_id\"] for item in data[\"artifact_data\"]],\n                \"author_full_name\": [item[\"author_full_name\"] for item in data[\"artifact_data\"]],\n                \"link\": [item[\"link\"] for item in data[\"artifact_data\"]],\n            }\n        ) \n    ```", "```py\ndef clean_text(text):\n    text = re.sub(r\"[^\\w\\s.,!?']\", \" \", text)\n    text = re.sub(r\"\\s+\", \" \", text)\n    return text.strip() \n```", "```py\ndef extract_substrings(dataset: Dataset, min_length: int = 1000, max_length: int = 2000) -> List[str]:\n    extracts = []\n    sentence_pattern = r\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s\"\n    for article in dataset[\"content\"]:\n        cleaned_article = clean_text(article)\n        sentences = re.split(sentence_pattern, cleaned_article)\n        current_chunk = \"\"\n        for sentence in sentences:\n            sentence = sentence.strip()\n            if not sentence:\n                continue\n            if len(current_chunk) + len(sentence) <= max_length:\n                current_chunk += sentence + \" \"\n            else:\n                if len(current_chunk) >= min_length:\n                    extracts.append(current_chunk.strip())\n                current_chunk = sentence + \" \"\n        if len(current_chunk) >= min_length:\n            extracts.append(current_chunk.strip())\n    return extracts \n```", "```py\n    class InstructionAnswerSet:\n        def __init__(self, pairs: List[Tuple[str, str]]):\n            self.pairs = pairs\n        @classmethod\n        def from_json(cls, json_str: str) -> 'InstructionAnswerSet':\n            data = json.loads(json_str)\n            pairs = [(pair['instruction'], pair['answer'])\n                     for pair in data['instruction_answer_pairs']]\n            return cls(pairs)\n        def __iter__(self):\n            return iter(self.pairs) \n    ```", "```py\ndef generate_instruction_answer_pairs(\n    extract: str, client: OpenAI\n) -> List[Tuple[str, str]]:\n    prompt = f\"\"\"Based on the following extract, generate five instruction-answer pairs. Each instruction \\\nmust ask to write about a specific topic contained in the context. each answer \\\nmust provide a relevant paragraph based on the information found in the \\\ncontext. Only use concepts from the context to generate the instructions. \\\nInstructions must never explicitly mention a context, a system, a course, or an extract. \\\nInstructions must be self-contained and general. \\\nAnswers must imitate the writing style of the context. \\\nExample instruction: Explain the concept of an LLM Twin. \\\nExample answer: An LLM Twin is essentially an AI character that mimics your writing style, personality, and voice. \\\nIt's designed to write just like you by incorporating these elements into a language model. \\\nThe idea is to create a digital replica of your writing habits using advanced AI techniques. \\\nProvide your response in JSON format with the following structure:\n{{\n    \"instruction_answer_pairs\": [\n        {{\"instruction\": \"...\", \"answer\": \"...\"}},\n        ...\n    ]\n}}\nExtract:\n{extract}\n\"\"\" \n```", "```py\n completion = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {\n                \"role\": \"system\", \"content\": \"You are a helpful assistant who \\\n            generates instruction-answer pairs based on the given context. \\\n            Provide your response in JSON format.\",\n            },\n            {\"role\": \"user\", \"content\": prompt},\n        ],\n        response_format={\"type\": \"json_object\"},\n        max_tokens=1200,\n        temperature=0.7,\n    )\n    # Parse the structured output\n    result = InstructionAnswerSet.from_json(completion.choices[0].message.content)\n    # Convert to list of tuples\n    return result.pairs \n```", "```py\ndef create_instruction_dataset(\n    dataset: Dataset, client: OpenAI, num_workers: int = 4\n) -> Dataset:\n    extracts = extract_substrings(dataset)\n    instruction_answer_pairs = []\n    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n        futures = [executor.submit(generate_instruction_answer_pairs, extract, client)\n            for extract in extracts\n        ]\n        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)\n        ):\n            instruction_answer_pairs.extend(future.result())\n    instructions, answers = zip(*instruction_answer_pairs)\n    return Dataset.from_dict(\n        {\"instruction\": list(instructions), \"output\": list(answers)}\n    ) \n```", "```py\n    def main(dataset_id: str) -> Dataset:\n        client = OpenAI()\n        # 1\\. Load the raw data\n        raw_dataset = load_articles_from_json(\"cleaned_documents.json\")\n        print(\"Raw dataset:\")\n        print(raw_dataset.to_pandas())\n        # 2\\. Create instructiondataset\n    instruction_dataset = create_instruction_dataset(raw_dataset, client)\n        print(\"Instruction dataset:\")\n        print(instruction_dataset.to_pandas())\n        # 3\\. Train/test split and export\n        filtered_dataset = instruction_dataset.train_test_split(test_size=0.1)\n        filtered_dataset.push_to_hub(\"mlabonne/llmtwin\")\n        return filtered_dataset\n    **Dataset({**\n     **features: [****'instruction'****,** **'output'****],**\n     **num_rows:** **3335**\n    **})** \n    ```", "```py\n<&#124;im_start&#124;>system\nYou are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.<&#124;im_end&#124;>\n<&#124;im_start&#124;>user\nConcepts: building, shop, town\nWrite a sentence that includes all these words.<&#124;im_end&#124;>\n<&#124;im_start&#124;>assistant\nIn our little town, there is a shop inside a big building where people go to buy their favorite toys and candies.<&#124;im_end&#124;> \n```", "```py\n### Instruction: What is the capital of France?\n### Response: The capital of France is Paris.<EOS> \n```", "```py\n<&#124;im_start&#124;>user\nWhat is the capital of France?<&#124;im_end&#124;>\n<&#124;im_start&#124;>assistant\nThe capital of France is Paris.<&#124;im_end&#124;> \n```", "```py\n<&#124;begin_of_text&#124;><&#124;start_header_id&#124;>user<&#124;end_header_id&#124;>\nWhat is the capital of France?<&#124;eot_id&#124;><&#124;start_header_id&#124;>assistant<&#124;end_header_id&#124;>\nThe capital of France is Paris.<&#124;eot_id&#124;> \n```", "```py\n<&#124;user&#124;>\nWhat is the capital of France?<&#124;end&#124;>\n<&#124;assistant&#124;>\nThe capital of France is Paris.<&#124;end&#124;> \n```", "```py\n<bos><start_of_turn>user\nWhat is the capital of France?<end_of_turn>\n<start_of_turn>model\nThe capital of France is Paris.<end_of_turn> \n```", "```py\n    HF_TOKEN = YOUR_API_KEY \n    ```", "```py\n    COMET_API_KEY = YOUR_API_KEY \n    ```", "```py\n    import os\n    import torch\n    from trl import SFTTrainer\n    from datasets import load_dataset, concatenate_datasets\n    from transformers import TrainingArguments, TextStreamerfrom unsloth import FastLanguageModel, is_bfloat16_supported \n    ```", "```py\nmax_seq_length = 2048\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"meta-llama/Meta-Llama-3.1-8B\",\n    max_seq_length=max_seq_length,\n    load_in_4bit=False,\n) \n```", "```py\n    model = FastLanguageModel.get_peft_model(\n        model,\n        r=32,\n        lora_alpha=32,\n        lora_dropout=0,\n        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"],\n    ) \n    ```", "```py\n    dataset1 = load_dataset(\"mlabonne/llmtwin\")\n    dataset2 = load_dataset(\"mlabonne/FineTome-Alpaca-100k\", split=\"train[:10000]\")\n    dataset = concatenate_datasets([dataset1, dataset2]) \n    ```", "```py\n    alpaca_template = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n    ### Instruction:\n    {}\n    ### Response:\n    {}\"\"\"\n    EOS_TOKEN = tokenizer.eos_token\n    dataset = dataset.map(format_samples, batched=True, remove_columns=dataset.column_names) \n    ```", "```py\n    dataset = dataset.train_test_split(test_size=0.05) \n    ```", "```py\n    trainer = SFTTrainer(\n        model=model,\n       tokenizer=tokenizer,\n        train_dataset=dataset[\"train\"],\n        eval_dataset=dataset[\"test\"],\n        dataset_text_field=\"text\",\n        max_seq_length=max_seq_length,\n        dataset_num_proc=2,\n        packing=True,\n        args=TrainingArguments(\n            learning_rate=3e-4,\n            lr_scheduler_type=\"linear\",\n            per_device_train_batch_size=2,\n            gradient_accumulation_steps=8,\n            num_train_epochs=3,\n            fp16=not is_bfloat16_supported(),\n            bf16=is_bfloat16_supported(),\n            logging_steps=1,\n            optim=\"adamw_8bit\",\n            weight_decay=0.01,\n            warmup_steps=10,\n            output_dir=\"output\",\n            report_to=\"comet_ml\",\n            seed=0,\n        ),\n    )\n    trainer.train() \n    ```", "```py\nFastLanguageModel.for_inference(model)\nmessage = alpaca_prompt.format(\"Write a paragraph to introduce supervised fine-tuning.\", \"\")\ninputs = tokenizer([message], return_tensors=\"pt\").to(\"cuda\")\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=256, use_cache=True) \n```", "```py\n    Supervised fine-tuning is a method used to enhance a language model by providing it with a curated dataset of instructions and their corresponding answers. This process is designed to align the model's \n    responses with human expectations, thereby improving its accuracy and relevance. The goal is to ensure that the model can respond effectively to a wide range of queries, making it a valuable tool for applications such as chatbots and virtual assistants. \n    ```", "```py\n    model.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_16bit\")\n    model.push_to_hub_merged(\"mlabonne/TwinLlama-3.1-8B\", tokenizer, save_method=\"merged_16bit\") \n    ```"]