- en: Chapter 2. Finding and Working with Words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to tokenizer factories – finding words in a character stream
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combining tokenizers – lowercase tokenizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combining tokenizers – stop word tokenizers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Lucene/Solr tokenizers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Lucene/Solr tokenizers with LingPipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating tokenizers with unit tests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modifying tokenizer factories
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding words for languages without white spaces
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An important part of building NLP systems is to work with the appropriate unit
    for processing. This chapter addresses the abstraction layer associated with the
    word level of processing. This is called tokenization, which amounts to grouping
    adjacent characters into meaningful chunks in support of classification, entity
    finding, and the rest of NLP.
  prefs: []
  type: TYPE_NORMAL
- en: LingPipe provides a broad range of tokenizer needs, which are not covered in
    this book. Look at the Javadoc for tokenizers that do stemming, Soundex (tokens
    based on what English words sound like), and more.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to tokenizer factories – finding words in a character stream
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LingPipe tokenizers are built on a common pattern of a base tokenizer that can
    be used on its own, or can be as the source for subsequent filtering tokenizers.
    Filtering tokenizers manipulate the tokens/white spaces provided by the base tokenizer.
    This recipe covers our most commonly used tokenizer, `IndoEuropeanTokenizerFactory`,
    which is good for languages that use the Indo-European style of punctuation and
    word separators—examples include English, Spanish, and French. As always, the
    Javadoc has useful information.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`IndoEuropeanTokenizerFactory` creates tokenizers with built-in support for
    alpha-numerics, numbers, and other common constructs in Indo-European languages.'
  prefs: []
  type: TYPE_NORMAL
- en: The tokenization rules are roughly based on those used in MUC-6 but are necessarily
    more fine grained, because the MUC tokenizers are based on lexical and semantic
    information, such as whether a string is an abbreviation.
  prefs: []
  type: TYPE_NORMAL
- en: MUC-6 refers to the Message Understanding Conference that originated the idea
    of government-sponsored competitions between contractors in 1995\. The informal
    term was *Bake off*, in reference to the Pillsbury Bake-Off that started in 1949,
    and one of the authors was a participant as postdoc in MUC-6\. MUC drove much
    of the innovation in the evaluation of NLP systems.
  prefs: []
  type: TYPE_NORMAL
- en: LingPipe tokenizers are built using the LingPipe `TokenizerFactory` interface,
    which provides a way of invoking different types of tokenizers using the same
    interface. This is very useful in creating filtered tokenizers, which are constructed
    as a chain of tokenizers and modify their output in some way. A `TokenizerFactory`
    instance might be created either as a basic tokenizer, which takes simple parameters
    in its construction, or as a filtered tokenizer, which takes other tokenizer factory
    objects as parameters. In either case, an instance of `TokenizerFactory` has a
    single `tokenize()` method, which takes input as a character array, a start index,
    and the number of characters to process and outputs a `Tokenizer` object. The
    `Tokenizer` object represents the state of tokenizing a particular slice of string
    and provides a stream of tokens. While `TokenizerFactory` is thread safe and/or
    serializable, tokenizer instances are typically neither thread safe nor serializable.
    The `Tokenizer` object provides methods to iterate over the tokens in the string
    and to provide token positions of the tokens in the underlying text.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Download the JAR file and source for the book if you have not already done so.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It is all pretty simple. The following are the steps to get started with tokenization:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to the `cookbook` directory and invoke the following class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will lead us to a command prompt, which asks us to type in some text:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If we type a sentence such as: `It''s no use growing older if you only learn
    new ways of misbehaving yourself`, we will get the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Examine the output and note what the tokens and white spaces are. The text is
    from the short story, *The Stampeding of Lady Bastable*, by Saki.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The code is so simple that it can be included in its entirety as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This recipe starts with the creation of `TokenizerFactory tokFactory` in the
    first statement of the `main()` method. Note that a singleton `IndoEuropeanTokenizerFactory.INSTANCE`
    is used. The factory will produce tokenizers for a given string, which is evident
    in the line, `Tokenizer tokenizer = tokFactory.tokenizer(input.toCharArray(),
    0, input.length())`. The entered string is converted to a character array with
    `input.toCharArray()` as the first argument to the `tokenizer` method and the
    start and finish offsets provided into the created character array.
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting `tokenizer` provides tokens for the provided slice of character
    array, and the white spaces and tokens are printed out in the `while` loop. Calling
    the `tokenizer.nextToken()` method does a few things:'
  prefs: []
  type: TYPE_NORMAL
- en: The method returns the next token or null if there is no next token. The null
    then ends the loop; otherwise, the loop continues.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The method also increments the corresponding white space. There is always a
    white space with a token, but it might be the empty string.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IndoEuropeanTokenizerFactory` assumes a fairly standard abstraction over characters
    that break down as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Characters from the beginning of the `char` array to the first token are ignored
    and not reported as white space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Characters from the end of the last token to the end of the `char` array are
    reported as the next white space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: White spaces can be the empty string because of two adjoining tokens—note the
    apostrophe in the output and corresponding white spaces
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This means that it is not possible to reconstruct the original string necessarily
    if the input does not start with a token. Fortunately, tokenizers are easily modified
    for customized needs. We will see this later in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tokenization can be arbitrarily complex. The LingPipe tokenizers are intended
    to cover most common uses, but you might need to create your own tokenizer to
    have fine-grained control, for example, Victoria's Secret with "Victoria's" as
    the token. Consult the source for `IndoEuropeanTokenizerFactory` if such customization
    is needed, to see how arbitrary tokenization is done here.
  prefs: []
  type: TYPE_NORMAL
- en: Combining tokenizers – lowercase tokenizer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We mentioned in the previous recipe that LingPipe tokenizers can be basic or
    filtered. Basic tokenizers, such as the Indo-European tokenizer, don't need much
    in terms of parameterization, none at all as a matter of fact. However, filtered
    tokenizers need a tokenizer as a parameter. What we're doing with filtered tokenizers
    is invoking multiple tokenizers where a base tokenizer is usually modified by
    a filter to produce a different tokenizer.
  prefs: []
  type: TYPE_NORMAL
- en: LingPipe provides several basic tokenizers, such as `IndoEuropeanTokenizerFactory`
    or `CharacterTokenizerFactory`. A complete list can be found in the Javadoc for
    LingPipe. In this section, we'll show you how to combine an Indo-European tokenizer
    with a lowercase tokenizer. This is a fairly common process that many search engines
    implement for Indo-European languages.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You will need to download the JAR file for the book and have Java and Eclipse
    set up so that you can run the example.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This works just the same way as the previous recipe. Perform the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Invoke the `RunLowerCaseTokenizerFactory` class from the command line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, in the command prompt, let''s use the following example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can see in the preceding output that all the tokens are converted to lowercase,
    including the word `UPPERCASE`, which was typed in uppercase. As this example
    uses an Indo-European tokenizer as its base tokenizer, you can see that the number
    4.5 is retained as `4.5` instead of being broken up into 4 and 5.
  prefs: []
  type: TYPE_NORMAL
- en: 'The way we put tokenizers together is very simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Here, we created a tokenizer that returns case and white space normalized tokens
    produced using an Indo-European tokenizer. The tokenizer created from the tokenizer
    factory is a filtered tokenizer that starts with the Indo-European base tokenizer,
    which is then modified by `LowerCaseTokenizer` to produce the lowercase tokenizer.
    This is then once again modified by `WhiteSpaceNormTokenizerFactory` to produce
    a lowercase, white space-normalized Indo-European tokenizer.
  prefs: []
  type: TYPE_NORMAL
- en: Case normalization is applied where the case of words doesn't matter much; for
    example, search engines often store case-normalized words in their indexes. Now,
    we will use case-normalized tokens in the upcoming examples on classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For more details on how filtered tokenizers are built, see the Javadoc for the
    abstract class, `ModifiedTokenizerFactory.`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combining tokenizers – stop word tokenizers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similarly to the way in which we put together a lowercase and white space normalized
    tokenizer, we can use a filtered tokenizer to create a tokenizer that filters
    out stop words. Once again, using search engines as our example, we can remove
    commonly occurring words from our input set so as to normalize the text. The stop
    words that are typically removed convey very little information by themselves,
    although they might convey information in context.
  prefs: []
  type: TYPE_NORMAL
- en: The input is tokenized using whatever base tokenizer is set up, and then, the
    resulting tokens are filtered out by the stop tokenizer to produce a token stream
    that is free of the stop words specified when the stop tokenizer is initialized.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You will need to download the JAR file for the book and have Java and Eclipse
    set up so that you can run the example.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we did earlier, we will go through the steps of interacting with the tokenizer:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Invoke the `RunStopTokenizerFactory` class from the command line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, in the prompt, let''s use the following example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that we lose adjacency information. In the input, we have `fox is jumping`,
    but the tokens came out as `fox` followed by `jumping`, because `is` was filtered.
    This can be a problem for token-based processes that need accurate adjacency information.
    In the *Foreground- or background-driven interesting phrase detection* recipe
    of [Chapter 4](part0051_split_000.html#page "Chapter 4. Tagging Words and Tokens"),
    *Tagging Words and Tokens*, we will show a length-based filtering tokenizer that
    preserves adjacency.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The stop words used in this `StopTokenizerFactory` filter are just a very short
    list of words, `is`, `of`, `the`, and `to`. Obviously, this list can be much longer
    if required. As you saw in the preceding output, the words `the` and `is` have
    been removed from the tokenized output. This is done with a very simple step:
    we instantiate `StopTokenizerFactory` in `src/com/lingpipe/cookbook/chapter2/RunStopTokenizerFactory.java`.
    The relevant code is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: As we're using `LowerCaseTokenizerFactory` as one of the filters in the tokenizer
    factory, we can get away with the stop words that contain only lowercase words.
    If we want to preserve the case of the input tokens and continue to remove the
    stop words, we will need to add uppercase or mixed-case versions as well.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The complete list of filtered tokenizers provided by LingPipe can be found on
    the Javadoc page at [http://alias-i.com/lingpipe/docs/api/com/aliasi/tokenizer/ModifyTokenTokenizerFactory.html](http://alias-i.com/lingpipe/docs/api/com/aliasi/tokenizer/ModifyTokenTokenizerFactory.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Lucene/Solr tokenizers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The very popular search engine, Lucene, includes many analysis modules, which
    provide general purpose tokenizers as well as language-specific tokenizers from
    Arabic to Thai. As of Lucene 4, most of these different analyzers can be found
    in separate JAR files. We will cover Lucene tokenizers, because they can be used
    as LingPipe tokenizers, as you will see in the next recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Much like the LingPipe tokenizers, Lucene tokenizers also can be split into
    basic tokenizers and filtered tokenizers. Basic tokenizers take a reader as input,
    and filtered tokenizers take other tokenizers as input. We will look at an example
    of using a standard Lucene analyzer along with a lowercase-filtered tokenizer.
    A Lucene analyzer essentially maps a field to a token stream. So, if you have
    an existing Lucene index, you can use the analyzer with the field name instead
    of the raw tokenizer, as we will show in the later part of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You will need to download the JAR file for the book and have Java and Eclipse
    set up so that you can run the example. Some of the Lucene analyzers used in the
    examples are part of the `lib` directory. However, if you'd like to experiment
    with other language analyzers, download them from the Apache Lucene website at
    [https://lucene.apache.org](https://lucene.apache.org).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Remember that we are not using a LingPipe tokenizer in this recipe but introducing
    the Lucene tokenizer classes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Invoke the `RunLuceneTokenizer` class from the command line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, in the prompt, let''s use the following example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s review the following code to see how the Lucene tokenizers differ in
    invocation from the previous examples—the relevant part of the code from `src/com/lingpipe/cookbook/chapter2/RunLuceneTokenizer.java`
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'All the input is now wrapped up, and it is time to construct the actual tokenizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The input text is used to construct `StandardTokenizer` with Lucene's versioning
    system supplied—this produces an instance of `TokenStream`. Then, we used `LowerCaseFilter`
    to create the final filtered `tokenStream` with the base `tokenStream` as an argument.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Lucene, we need to attach the attributes we''re interested in from the token
    stream; this is done by the `addAttribute` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that in Lucene 4, once the tokenizer has been instantiated, the `reset()`
    method must be called before using the tokenizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The `tokenStream` is wrapped up with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An excellent introduction to Lucene is in *Text Processing with Java*, *Mitzi
    Morris*, *Colloquial Media Corporation*, where the guts of what we explained earlier
    are made clearer than what we can provide in a recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Using Lucene/Solr tokenizers with LingPipe
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can use these Lucene tokenizers with LingPipe; this is useful because Lucene
    has such a rich set of them. We are going to show how to wrap a Lucene `TokenStream`
    into a LingPipe `TokenizerFactory` by extending the `Tokenizer` abstract class.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will shake things up a bit and have a recipe that is not interactive. Perform
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Invoke the `LuceneAnalyzerTokenizerFactory` class from the command line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `main()` method in the class specifies the input:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding snippet creates a Lucene `StandardAnalyzer` and uses it to construct
    a LingPipe `TokenizerFactory`. The output is as follows—the `StandardAnalyzer`
    filters stop words, so the token `are` is filtered:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The white spaces report as `default` because the implementation does not accurately
    provide white spaces but goes with a default. We will discuss this limitation
    in the *How it works…* section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s take a look at the `LuceneAnalyzerTokenizerFactory` class. This class
    implements the LingPipe `TokenizerFactory` interface by wrapping a Lucene analyzer.
    We will start with the class definition from `src/com/lingpipe/cookbook/chapter2/LuceneAnalyzerTokenizerFactory.java`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The constructor stores the analyzer and the name of the field as private variables.
    As this class implements the `TokenizerFactory` interface, we need to implement
    the `tokenizer()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The `tokenizer()` method creates a new character-array reader and passes it
    to the Lucene analyzer to convert it to a `TokenStream`. An instance of `LuceneTokenStreamTokenizer`
    is created based on the token stream. `LuceneTokenStreamTokenizer` is a nested
    static class that extends LingPipe''s `Tokenizer` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The constructor stores `TokenStream` and attaches the term and the offset attributes.
    In the previous recipe, we saw that the term and the offset attributes contain
    the token string, and the token start and end offsets into the input text. The
    token offsets are also initialized to `-1` before any tokens are found:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: We will implement the `nextToken()` method and use the `incrementToken()` method
    of the token stream to retrieve any tokens from the token stream. We will set
    the token start and end offsets using `OffsetAttribute`. If the token stream is
    finished or the `incrementToken()` method throws an I/O exception, we will end
    and close the `TokenStream`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `nextWhitespace()` method has some limitations, because `offsetAttribute`
    is focused on the current token where LingPipe tokenizers quantize the input into
    the next token and next offset. A general solution here will be quite challenging,
    because there might not be any well-defined white spaces between tokens—think
    character ngrams. So, the `default` string is supplied just to make it clear.
    The method is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The code also covers how to serialize the tokenizer, but we will not cover this
    in the recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating tokenizers with unit tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will not evaluate Indo-European tokenizers like the other components of
    LingPipe with measures such as precision and recall. Instead, we will develop
    them with unit tests, because our tokenizers are heuristically constructed and
    expected to perform perfectly on example data—if a tokenizer fails to tokenize
    a known case, then it is a bug, not a reduction in performance. Why is this? There
    are a few reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Many tokenizers are very "mechanistic" and are amenable to the rigidity of the
    unit test framework. For example, the `RegExTokenizerFactory` is obviously a candidate
    to unit test rather than an evaluation harness.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The heuristic rules that drive most tokenizers are very general, and there is
    no issue of over-fitting training data at the expense of a deployed system. If
    you have a known bad case, you can just go and fix the tokenizer and add a unit
    test.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tokens and white spaces are assumed to be semantically neutral, which means
    that tokens don''t change depending on context. This is not totally true with
    our Indo-European tokenizer, because it treats `.` differently if it is part of
    a decimal or at the end of a sentence, for example, `3.14 is pi.`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: It might be appropriate to use an evaluation metric for statistics-based tokenizers;
    this is discussed in the *Finding words for languages without white spaces* recipe
    in this chapter. See the *Evaluation of sentence detection* recipe in [Chapter
    5](part0061_split_000.html#page "Chapter 5. Finding Spans in Text – Chunking"),
    *Finding Spans in Text – Chunking*, for appropriate span-based evaluation techniques.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will forgo running the code step and just get right into the source to put
    together a tokenizer evaluator. The source is in `src/com/lingpipe/chapter2/TestTokenizerFactory.java`.
    Perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code sets up a base tokenizer factory with a regular expression—look
    at the Javadoc for the class if you are not clear about what is being constructed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `checkTokens` method takes `TokenizerFactory`, an array of `String` that
    is the desired tokenization, and `String` that is to be tokenized. It follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The method is quite intolerant of errors, because it exits the program if the
    token arrays are not of the same length or if any of the tokens are not equal.
    A proper unit test framework such as JUnit will be a better framework, but that
    is beyond the scope of the book. You can look at the LingPipe unit tests in `lingpipe.4.1.0`/`src/com/aliasi/test`
    for how JUnit is used.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `checkTokensAndWhiteSpaces()` method checks white spaces as well as tokens.
    It follows the same basic ideas of `checkTokens()`, so we leave it unexplained.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Modifying tokenizer factories
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will describe a tokenizer that modifies the tokens in the
    token stream. We will extend the `ModifyTokenTokenizerFactory` class to return
    text that is rotated by 13 places in the English alphabet, also known as rot-13\.
    Rot-13 is a very simple substitution cipher, which replaces a letter with the
    letter that follows after 13 places. For example, the letter `a` will be replaced
    by the letter `n`, and the letter `z` will be replaced by the letter `m`. This
    is a reciprocal cypher, which means that applying the same cypher twice recovers
    the original text.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will invoke the `Rot13TokenizerFactory` class from the command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: You can see that the input text, which was mixed case and in normal English,
    has been transformed into its Rot-13 equivalent. You can see that the second time
    around, we passed the Rot-13 modified text as input and got the original text
    back, except that it was all lowercase.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`Rot13TokenizerFactory` extends the `ModifyTokenTokenizerFactory` class. We
    will override the `modifyToken()` method, which operates a token at a time and,
    in this case, converts the token to its Rot-13 equivalent. There is a similar
    `modifyWhiteSpace` (String) method, which modifies the white spaces if required:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The start and end offsets of the tokens themselves remain the same as that of
    the underlying tokenizer. Here, we will use an Indo-European tokenizer as our
    base tokenizer. Filter it once through `LowerCaseTokenizer` and then through `Rot13Tokenizer`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `rot13` method is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Finding words for languages without white spaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Languages such as Chinese do not have word boundaries. For example, 木卫三是围绕木星运转的一颗卫星，公转周期约为7天
    from Wikipedia is a sentence in Chinese that translates roughly into "Ganymede
    is running around Jupiter's moons, orbital period of about seven days" as done
    by the machine translation service at [https://translate.google.com](https://translate.google.com).
    Notice the absence of white spaces.
  prefs: []
  type: TYPE_NORMAL
- en: Finding tokens in this sort of data requires a very different approach that
    is based on character-language models and our spell-checking class. This recipe
    encodes finding words by treating untokenized text as *misspelled* text, where
    the *correction* inserts a space to delimit tokens. Of course, there is nothing
    misspelled about Chinese, Japanese, Vietnamese, and other non-word delimiting
    orthographies, but we have encoded it in our spelling-correction class.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will approximate non-word delimiting orthographies with de-white spaced English.
    This is sufficient to understand the recipe and can be easily modified to the
    actual language when needed. Get a 100,000 or so words of English and get them
    to the disk in UTF-8 encoding. The reason for fixing the encoding is that the
    input is assumed to be UTF-8—you can change it by changing the encoding and recompiling
    the recipe.
  prefs: []
  type: TYPE_NORMAL
- en: We used *A Connecticut Yankee in King Arthur's Court* by Mark Twain, downloaded
    from Project Gutenberg ([http://www.gutenberg.org/](http://www.gutenberg.org/)).
    Project Gutenberg is an excellent source of texts that are in the public domain,
    and Mark Twain is fine writer—we highly recommend the book. Place your selected
    text in the cookbook directory or work with our default.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will run a program, play with it a bit, and explain what it does and how
    it does it, using the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Type the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You might not get the perfect output. How good is Mark Twain at recovering
    proper white space from the Java program that generated it? Let''s find out:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding way was not very good, but we are not being very fair; let''s
    use the concatenated source of LingPipe as training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This is the perfect space insertion.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For all the fun and games, there is very little code involved. The cool thing
    is that we are building on the character-language models from [Chapter 1](part0014_split_000.html#page
    "Chapter 1. Simple Classifiers"), *Simple Classifiers*. The source is in `src/com/lingpipe/chapter2/TokenizeWithoutWhiteSpaces.java`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The `main()` method starts up with the creation of `NgramProcessLM`. Next up,
    we will access a class for edit distance that is designed to only add spaces to
    a character stream. That's it. `Editdistance` is typically a fairly crude measure
    of string similarity that scores how many edits need to happen to to `string1`
    to make it the same as `string2`. A lot of information on this is Javadoc `com.aliasi.spell`.
    For example, `com.aliasi.spell.EditDistance` has an excellent discussion of the
    basics.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `EditDistance` class implements the standard notion of edit distance, with
    or without transposition. The distance without transposition is known as the Levenshtein
    distance, and with transposition, it is known as the Damerau-Levenstein distance.
  prefs: []
  type: TYPE_NORMAL
- en: Read the Javadoc with LingPipe; it has a lot of useful of information that we
    don't have space for in this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'So far we configured and constructed a `TrainSpellChecker` class. The next
    step is to naturally train it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: We slurped up a text file, assuming it is UTF-8; if not, correct the character
    encoding and recompile. Then, we replaced all the multiple white spaces with a
    single one. This might not be the best move if multiple white spaces have meaning.
    This is followed by training, just like we trained language models in [Chapter
    1](part0014_split_000.html#page "Chapter 1. Simple Classifiers"), *Simple Classifiers*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next up, we will compile and configure the spell checker:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: The next interesting line compiles `spellChecker`, which translates all the
    counts in the underlying language model to precomputed probabilities, which is
    much faster. The compilation step can write to a disk, so it can be used later
    without training; however, visit the Javadoc for `AbstractExternalizable` on how
    to do this. The next lines configure `CompiledSpellChecker` to only consider the
    edits that insert characters and to check for the exact string matches, but it
    forbids deletions, substitutions, and transpositions. Finally, only one insert
    is allowed. It should be clear that we are using a very limited portion of the
    capabilities of `CompiledSpellChecker`, but this is exactly what is called for—insert
    a space or don't.
  prefs: []
  type: TYPE_NORMAL
- en: 'Last up is our standard I/O routine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The mechanics of the `CompiledSpellChecker` and `WeightedEditDistance` classes
    are better described in either the Javadoc or the *Using edit distance and language
    models for spelling correction* recipe in [Chapter 6](part0075_split_000.html#page
    "Chapter 6. String Comparison and Clustering"), *String Comparison and Clustering*.
    However, the basic idea is that the string entered is compared to the language
    model just trained, resulting in a score that shows how good a fit this string
    is to the model. This string is going to be one huge word without any white spaces—but
    note that there is no tokenizer at work here, so the spell checker starts inserting
    spaces and reassessing the score of the resulting sequence. It keeps these sequences
    where insertion of spaces increases the score of the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that the language model was trained on text with white spaces. The
    spell checker tries to insert a space everywhere it can and keeps a set of "best
    so far" insertions of white spaces. In the end, it returns the best scoring series
    of edits.
  prefs: []
  type: TYPE_NORMAL
- en: Note that to complete the tokenizer, the appropriate `TokenizerFactory` needs
    to be applied to the white space-modified text, but this is left as an exercise
    for the reader.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`CompiledSpellChecker` allows for an *n*-best output as well; this allows for
    multiple possible analyses of the text. In a high-coverage/recall situation such
    as a research search engine, it might serve to allow the application of multiple
    tokenizations. Also, the edit costs can be manipulated by extending the `WeightedEditDistance`
    class directly to tune the system.'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It will be unhelpful to not actually provide non-English resources for this
    recipe. We built and evaluated a Chinese tokenizer using resources available on
    the web for research use. Our tutorial on Chinese word segmentation covers this
    in detail. You can find the Chinese word segmentation tutorial at [http://alias-i.com/lingpipe/demos/tutorial/chineseTokens/read-me.html](http://alias-i.com/lingpipe/demos/tutorial/chineseTokens/read-me.html).
  prefs: []
  type: TYPE_NORMAL
