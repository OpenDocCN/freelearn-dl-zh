<html><head></head><body>
<div><h1 id="_idParaDest-5"><a id="_idTextAnchor004"/>Preface</h1>
<p><em class="italic">Building AI Agents with LLMs, RAG, and Knowledge Graphs</em> introduces you to the evolving landscape of large language models (LLMs) and AI agents, offering both a theoretical foundation and practical guidance. It begins by explaining how text data can be represented and processed using deep learning, then progresses to modern architectures such as the Transformer model. From there, the book explores how LLMs are scaled and fine-tuned, and how their capabilities can be extended with tools, external memory systems, and agent-based frameworks.  Technologies such as retrieval-augmented generation (RAG), GraphRAG, and multi-agent systems are explained in detail, with a focus on real-world applications and deployment. By the end of the book, you will have a clear understanding of how to build intelligent, tool-using AI agents and the role these systems play in shaping the future of AI.</p>
<h1 id="_idParaDest-6"><a id="_idTextAnchor005"/>Who this book is for</h1>
<p>This book is intended for software engineers, data scientists, and researchers who want to understand and build applications using LLMs and AI agents. A basic understanding of Python programming and foundational concepts in machine learning is recommended to fully benefit from the content. While no deep expertise in NLP is required, familiarity with neural networks, REST APIs, and general software development practices will help you follow the examples and implement real-world systems. Whether you’re looking to build intelligent agents, explore the inner workings of LLMs, or deploy AI applications at scale, this book provides both the theoretical background and practical guidance to get started.</p>
<h1 id="_idParaDest-7"><a id="_idTextAnchor006"/>What this book covers</h1>
<p><a href="B21257_01.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">Analyzing Text Data with Deep Learning</em>, introduces how to process and represent natural language in a format suitable for machine learning models. It covers various text encoding techniques, from basic one-hot encoding and bag of words to more advanced representations such as TF-IDF and word2vec. The chapter then explores key deep learning architectures for sequential data, such as RNNs, LSTMs, GRUs, and CNNs, and demonstrates how to apply them to text classification tasks. By the end of this chapter you will understand how these foundations enable modern language models such as ChatGPT.</p>
<p><a href="B21257_02.xhtml#_idTextAnchor032"><em class="italic">Chapter 2</em></a>, <em class="italic">The Transformer: The Model Behind the Modern AI Revolution</em>, introduces attention mechanisms and explains how they evolved into the transformer architecture. The chapter highlights the limitations of earlier models such as RNNs and LSTMs, and shows how transformers overcame them to become the foundation of modern NLP. Key topics include self-attention, masked language modeling, training techniques, and internal model visualization. The chapter concludes by demonstrating real-world applications and laying the groundwork for understanding today’s LLMs.</p>
<p><a href="B21257_03.xhtml#_idTextAnchor042"><em class="italic">Chapter 3</em></a>, <em class="italic">Exploring LLMs as a Powerful AI Engine</em>, examines how the large-scale training of transformer models gave rise to today’s LLMs. The chapter explores their evolution, capabilities, and limitations, including techniques such as instruction tuning, fine-tuning, and alignment. It also introduces more compact and efficient LLM variants, multimodal models that handle multiple data types, and understanding challenges such as hallucinations, ethical concerns, and prompt engineering.</p>
<p><a href="B21257_04.xhtml#_idTextAnchor058"><em class="italic">Chapter 4</em></a>, <em class="italic">Building a Web Scraping Agent with an LLM</em>, introduces the concept of AI agents as an extension of LLMs, aimed at overcoming their ability to perform actions. The chapter explores the key characteristics of agents, and distinctions between single and multi-agent systems. It also presents the main libraries used for building agents and guides you through the creation of a web-scraping agent capable of retrieving information from the internet.</p>
<p><a href="B21257_05.xhtml#_idTextAnchor077"><em class="italic">Chapter 5</em></a>, <em class="italic">Extending Your Agent with RAG to Prevent Hallucinations</em>, explores how RAG could overcome key limitations of LLMs, such as outdated knowledge and hallucinations. The chapter explains how RAG enables an LLM to access external information sources through embedding and vector databases, thereby improving accuracy and adaptability. It also compares RAG with fine-tuning and demonstrates its practical use by building a movie recommendation agent.</p>
<p><a href="B21257_06.xhtml#_idTextAnchor090"><em class="italic">Chapter 6</em></a>, <em class="italic">Advanced RAG Techniques for Information Retrieval and Augmentation</em>, expands on the basic RAG architecture by introducing enhancements at every stage of the pipeline—data ingestion, indexing, retrieval, and generation. The chapter explores modular RAG, techniques for scaling systems with large datasets and user bases, and key concerns such as robustness and privacy. It also highlights current challenges and open questions surrounding the future development of RAG-based systems.</p>
<p><a href="B21257_07.xhtml#_idTextAnchor113"><em class="italic">Chapter 7</em></a>, <em class="italic">Creating and Connecting a Knowledge Graph to an AI Agent</em>, explores how to structure textual knowledge into knowledge graphs (KGs) to enhance information retrieval and reasoning in AI agents. The chapter introduces the concept of GraphRAG, where KGs are used to augment LLMs with structured contextual data. It covers how LLMs can be used to build KGs by extracting entities and relationships, how to use graphs for querying and reasoning, and discusses the benefits, limitations, and future directions of combining different approaches.</p>
<p><a href="B21257_08.xhtml#_idTextAnchor137"><em class="italic">Chapter 8</em></a>, <em class="italic">Reinforcement Learning and AI Agents</em>, explores how agents can learn by interacting with dynamic environments, adjusting their behavior based on experience. It introduces the fundamentals of reinforcement learning, explains how agents make decisions and improve over time, and demonstrates how neural networks can be used to guide behavior. The chapter concludes by discussing how LLMs can be combined with reinforcement learning to build more capable AI systems.</p>
<p><a href="B21257_09.xhtml#_idTextAnchor156"><em class="italic">Chapter 9</em></a>, <em class="italic">Creating Single- and Multi-Agent Systems</em>, explores how LLMs can be extended with tools and other models to form autonomous agents. It introduces the concept of single-agent and multi-agent systems, shows how LLMs can interact with APIs or external models, and presents key examples such as HuggingGPT. The chapter also covers agent coordination strategies, real-world applications in complex domains, and emerging business paradigms such as SaaS, MaaS, DaaS, and RaaS.</p>
<p><a href="B21257_10.xhtml#_idTextAnchor179"><em class="italic">Chapter 10</em></a>, <em class="italic">Building an AI Agent Application</em>, addresses the challenges of scaling and deploying AI agents in real-world applications. It introduces Streamlit as a rapid prototyping framework to create both frontend and backend components of an agent-based system. The chapter also covers key operational aspects such as asynchronous programming, containerization with Docker, and best practices for building scalable, production-ready AI solutions.</p>
<p><a href="B21257_11.xhtml#_idTextAnchor215"><em class="italic">Chapter 11</em></a>, <em class="italic">The Future Ahead</em>, explores the transformative potential of AI agents across industries such as healthcare and beyond. Building on the advancements discussed in earlier chapters, it reflects on the remaining technical and ethical challenges facing LLMs and agent systems. The chapter concludes by examining open questions and future directions in the development and deployment of intelligent AI agents.</p>
<h1 id="_idParaDest-8"><a id="_idTextAnchor007"/>To get the most out of this book</h1>
<p>You should have a basic understanding of Python and be familiar with fundamental programming concepts such as functions, classes, and modules. A general knowledge of machine learning and neural networks (such as what a model is and how training works) will help in following the deeper technical content. While prior experience with deep learning frameworks or LLMs is not required, it will enhance your ability to apply the techniques discussed. The book is designed to be progressive, so concepts are introduced step by step, but a technical mindset is essential.</p>
<table class="No-Table-Style _idGenTablePara-1" id="table001">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">Software/hardware covered in </strong><strong class="bold">the book</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Operating </strong><strong class="bold">system requirements</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Python 3.10+</p>
</td>
<td class="No-Table-Style">
<p>Windows, macOS, or Linux</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>PyTorch/Transformers</p>
</td>
<td class="No-Table-Style">
<p>Windows, macOS, or Linux</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Streamlit</p>
</td>
<td class="No-Table-Style">
<p>Windows, macOS, or Linux</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Docker</p>
</td>
<td class="No-Table-Style">
<p>Windows, macOS, or Linux</p>
</td>
</tr>
</tbody>
</table>
<p>For readers without access to a local GPU, using Google Colab is a convenient option. A Google Colab Pro account is recommended, as it provides access to more powerful GPUs such as NVIDIA T4 or A100, which can greatly improve performance when running embedding models, fine-tuning, or working with agents.</p>
<p><strong class="bold">If you are using the digital version of this book, we advise you to type the code yourself or access the code from the book’s GitHub repository (a link is available in the next section). Doing so will help you avoid any potential errors related to the copying and pasting </strong><strong class="bold">of code.</strong></p>
<h1 id="_idParaDest-9"><a id="_idTextAnchor008"/>Download the example code files</h1>
<p>You can download the example code files for this book from GitHub at <a href="https://github.com/PacktPublishing/Modern-AI-Agents/tree/main">https://github.com/PacktPublishing/Modern-AI-Agents/tree/main</a>. If there’s an update to the code, it will be updated in the GitHub repository.</p>
<p>We also have other code bundles from our rich catalog of books and videos available at <a href="http://https://github.com/PacktPublishing/">https://github.com/PacktPublishing/</a>. Check them out!</p>
<h1 id="_idParaDest-10"><a id="_idTextAnchor009"/>Conventions used</h1>
<p>There are a number of text conventions used throughout this book.</p>
<p><code>Code in text</code>: Indicates code words in text, database table names, folder names, filenames, file extensions, pathnames, dummy URLs, user input, and X/Twitter handles. Here is an example: “The <code>process_frame</code> function is used to preprocess frames from the game to make them more suitable for training an RL agent.”</p>
<p>A block of code is set as follows:</p>
<pre class="source-code">
self.critic_linear = nn.Linear(512, 1)
self.actor_linear = nn.Linear(512, num_actions)</pre> <p>When we wish to draw your attention to a particular part of a code block, the relevant lines or items are set in bold:</p>
<pre class="source-code">
[default]
exten =&gt; s,1,Dial(Zap/1|30)
exten =&gt; s,2,Voicemail(u100)
<strong class="bold">exten =&gt; s,102,Voicemail(b100)</strong>
exten =&gt; i,1,Voicemail(s0)</pre> <p>Any command-line input or output is written as follows:</p>
<pre class="console">
streamlit run https://raw.githubusercontent.com/streamlit/my_apps/ master/my_app.py</pre> <p><strong class="bold">Bold</strong>: Indicates a new term, an important word, or words that you see onscreen. For instance, words in menus or dialog boxes appear in <strong class="bold">bold</strong>. Here is an example: “Once we have our tokens ready, we can enter our question and click <strong class="bold">Submit</strong>.”</p>
<p class="callout-heading">Tips or important notes</p>
<p class="callout">Appear like this.</p>
<h1 id="_idParaDest-11"><a id="_idTextAnchor010"/>Get in touch</h1>
<p>Feedback from our readers is always welcome.</p>
<p><strong class="bold">General feedback</strong>: If you have questions about any aspect of this book, email us at <a href="mailto:customercare@packtpub.com">customercare@packtpub.com</a> and mention the book title in the subject of your message.</p>
<p><strong class="bold">Errata</strong>: Although we have taken every care to ensure the accuracy of our content, mistakes do happen. If you have found a mistake in this book, we would be grateful if you would report this to us. Please visit <a href="http://www.packtpub.com/support/errata">www.packtpub.com/support/errata</a> and fill in the form.</p>
<p><strong class="bold">Piracy</strong>: If you come across any illegal copies of our works in any form on the internet, we would be grateful if you would provide us with the location address or website name. Please contact us at <a href="mailto:copyright@packt.com">copyright@packt.com</a> with a link to the material.</p>
<p><strong class="bold">If you are interested in becoming an author</strong>: If there is a topic that you have expertise in and you are interested in either writing or contributing to a book, please visit <a href="http://authors.packtpub.com">authors.packtpub.com</a>.</p>
<h1 id="_idParaDest-12"><a id="_idTextAnchor011"/>Share Your Thoughts</h1>
<p>Once you’ve read <em class="italic">Building AI Agents with LLMs, RAG, and Knowledge Graphs</em>, we’d love to hear your thoughts! Please <a href="https://packt.link/r/1-835-08706-X">click here to go straight to the Amazon review page</a> for this book and share your feedback.</p>
<p>Your review is important to us and the tech community and will help us make sure we’re delivering excellent quality content.</p>
<h1 id="_idParaDest-13"><a id="_idTextAnchor012"/>Download a free PDF copy of this book</h1>
<p>Thanks for purchasing this book!</p>
<p>Do you like to read on the go but are unable to carry your print books everywhere?</p>
<p>Is your eBook purchase not compatible with the device of your choice?</p>
<p>Don’t worry, now with every Packt book you get a DRM-free PDF version of that book at no cost.</p>
<p>Read anywhere, any place, on any device. Search, copy, and paste code from your favorite technical books directly into your application. </p>
<p>The perks don’t stop there, you can get exclusive access to discounts, newsletters, and great free content in your inbox daily</p>
<p>Follow these simple steps to get the benefits:</p>
<ol>
<li>Scan the QR code or visit the link below</li>
</ol>
<div><div><img alt="" src="img/B21257_QR_Free_PDF.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><a href="https://packt.link/free-ebook/978-1-83508-706-0">https://packt.link/free-ebook/978-1-83508-706-0</a></p>
<ol>
<li value="2">Submit your proof of purchase</li>
<li>That’s it! We’ll send your free PDF and other benefits to your email directly</li>
</ol>
</div>


<div><h1 id="_idParaDest-14" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor013"/>Part 1: 
The AI Agent Engine: From Text to Large Language Models</h1>
<p>This part lays the foundation for understanding how modern AI agents process and generate language. It begins by exploring how raw text can be represented in numerical form suitable for deep learning models, introducing techniques such as word embeddings and basic neural architectures. The focus then shifts to the Transformer model and explains how attention mechanisms revolutionized natural language processing. Finally, it examines how large language models (LLMs) are built by scaling transformers, discussing training strategies, instruction tuning, fine-tuning, and the evolution toward models capable of general-purpose reasoning. Together, these chapters provide the technical and conceptual groundwork for building intelligent AI agents.</p>
<p>This part has the following chapters:</p>
<ul>
<li><a href="B21257_01.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a><em class="italic">, Analyzing Text Data with Deep Learning</em></li>
<li><a href="B21257_02.xhtml#_idTextAnchor032"><em class="italic">Chapter 2</em></a><em class="italic">, The Transformer: The Model Behind the Modern AI Revolution</em></li>
<li><a href="B21257_03.xhtml#_idTextAnchor042"><em class="italic">Chapter 3</em></a><em class="italic">, Exploring LLMs as a Powerful AI Engine</em></li>
</ul>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
</body></html>