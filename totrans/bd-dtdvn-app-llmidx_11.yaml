- en: <title>Prompt Engineering Guidelines and Best Practices</title>
  prefs: []
  type: TYPE_NORMAL
- en: Prompt Engineering Guidelines and Best Practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we embark on an exploration of how the latest advancements
    in technology are reshaping our interaction with digital tools and applications.
    As the digital landscape evolves, the traditional interfaces we’ve relied upon
    for decades are being reimagined, paving the way for more intuitive and efficient
    forms of communication between humans and machines. At the heart of this transformation
    is the advent of conversational interfaces powered by **natural language** ( **NL**
    ). As a result, understanding how to write effective prompts to customize the
    behavior of our LlamaIndex components becomes a critical skill in building and
    improving RAG applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, in this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Why prompts are your secret weapon
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding how LlamaIndex uses prompts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customizing default prompts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The golden rules of prompt engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <title>Technical requirements</title>
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'All code samples from this chapter can be found in the `ch10` subfolder of
    the book’s GitHub repository: https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex
    .'
  prefs: []
  type: TYPE_NORMAL
- en: <title>Why prompts are your secret weapon</title>
  prefs: []
  type: TYPE_NORMAL
- en: Why prompts are your secret weapon
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I was 6 years old when I started writing my first lines of code using a ZX Spectrum
    computer. At the time, in the mid-1980s, computers were still a new thing in the
    world, and not many people understood the extraordinary impact they were going
    to have on human society. Today, we all live in a reality dominated, and in many
    ways, driven by technology. The way we relate to technology has also changed fundamentally
    in the last 40 years. Almost all human activities have come to be touched to a
    greater or lesser extent by technological progress.
  prefs: []
  type: TYPE_NORMAL
- en: What hasn’t changed much is the way we interact with technology. With a few
    notable exceptions – such as the introduction of touch screens and voice interfaces
    – our interaction with technology has remained almost unchanged. We use, as we
    did 40 years ago, rudimentary methods to get computers to perform the functions
    we need.
  prefs: []
  type: TYPE_NORMAL
- en: Clarification
  prefs: []
  type: TYPE_NORMAL
- en: 'When I say rudimentary, I’m not necessarily referring to the sophistication
    of the interface itself – although functionally, if we were to compare a modern-day
    keyboard or mouse, we would find that even here, the advances are not fantastic.
    I’m referring rather to another aspect that unfortunately continues to stagnate:
    the bandwidth that our current interfaces can offer.'
  prefs: []
  type: TYPE_NORMAL
- en: The way we currently interact with our technology is long due for replacement.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go through a simple rationale together:'
  prefs: []
  type: TYPE_NORMAL
- en: The computing power offered by IT systems continues to grow at a rapid pace.
    Even if Moore’s law – see *Figure 10* *.1* – is arguably no longer considered
    a valid benchmark, progress is far from slowing down ( https://en.wikipedia.org/wiki/Moore%27s_law
    ).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We live in a world dominated almost entirely by applications. At the moment,
    applications are the layer between the user and the machine that makes our interaction
    with a computer possible – apps running on local systems, apps running on mobile
    devices, or apps running in the cloud. Each app offers a very specific set of
    functionalities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many apps are designed to run only on specific platforms and cannot be easily
    ported to other platforms. This means a different app for each specific platform.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many applications overlap in terms of functionality. For a given task there
    are, in most cases, dozens of different applications that can perform it. So,
    there is a lot of duplication.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our interaction with technology has remained broadly the same bandwidth as 40
    years ago. We use almost the same types of interfaces – keyboard, mouse, touchscreen,
    gesture- or voice-based – to control application logic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Almost every application comes with its own UI. There is a mandatory learning
    curve that users have to go through to learn how to operate each application.
    If we multiply this time by the number of applications that a typical user uses
    on a regular basis, we find that we actually spend a lot of time learning to use
    a tool effectively, and this eats into the actual time we spend using the tool
    to be productive.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of software applications – including both publicly available applications
    and those used privately by organizations – is already huge. There are already
    more than 1 billion applications in the world. That’s without taking into account
    the fact that an application very often exists in several different versions.
    And the number is growing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From an evolutionary point of view, the capacity of the human brain has remained
    unchanged throughout this time. Neuroplasticity gives us a remarkable ability
    to learn and adapt to new technologies, but unfortunately, evolution itself cannot
    keep up with technological progress.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B21861_10_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 – According to Moore’s law, the number of transistors roughly doubles
    every 2 years
  prefs: []
  type: TYPE_NORMAL
- en: 'See where I’m aiming? This very specific way of interacting with technology,
    combined with the rapid evolution of technology, is slowly making us victims of
    our own success. On the one hand, we have managed to build a huge number of specialized
    tools capable of solving a huge number of problems. But now, we have a bigger
    problem: we have so many tools that organizing and using them efficiently has
    become an extremely complicated process. A new paradigm is needed.'
  prefs: []
  type: TYPE_NORMAL
- en: Conversational interfaces, based on **natural language processing** ( **NLP**
    ), present themselves as a promising alternative to the current way of interacting
    with technology. They represent a natural evolution in the way we communicate
    with our devices. Instead of relying on complex visual interfaces and input methods
    that require effort and time to learn, conversational interfaces allow us to use
    NL – the most fundamental and intuitive form of human communication.
  prefs: []
  type: TYPE_NORMAL
- en: This is where a new core competency in this new paradigm comes in.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt engineering
  prefs: []
  type: TYPE_NORMAL
- en: As human-machine interaction becomes increasingly dependent on NL, the ability
    to formulate effective prompts that guide **artificial intelligence** ( **AI**
    ) algorithms toward desired responses or actions becomes essential. This skill
    involves not only formulating prompts clearly but also anticipating how different
    formulations may influence the interpretation and execution of commands by the
    AI.
  prefs: []
  type: TYPE_NORMAL
- en: Conversational interfaces transform the interaction with technology into a dialogue
    where linguistic precision and understanding of algorithmic subtleties become
    key factors in achieving desired outcomes. The ability to interact directly and
    effectively with computer systems using NL can significantly reduce the barrier
    between humans and technology. It offers a pathway to democratizing access to
    technology, making it accessible to a wider range of users, regardless of their
    technical expertise.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are already indications that the intensive use of prompts in our everyday
    interactions with LLMs can improve even our interpersonal communication skills,
    as shown, for example, by this study: Liu et al. (2023), *Improving Interpersonal
    Communication by Simulating Audiences with Language* *Models* ( https://doi.org/10.48550/arXiv.2311.00687
    ).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine computer systems that can replace the functionality of dozens or even
    hundreds of different applications but without the complexity of traditional interfaces.
    Language interaction: a form of technology where LLMs, augmented with RAG, take
    the place of applications and operating systems, giving us a universal and much
    simpler way to use computing power. Without getting too deep into the area of
    speculation, if I were to make a medium-to-long-term prediction, this is the direction
    I think we are heading in. In the short term, classical computing systems will
    continue to prevail. At first, conversational agent-based interfaces will gradually
    simplify user interaction with them, masking the complexity of the backend application
    layer. Then, as dedicated AI hardware becomes a commodity, a large part of the
    applications will be phased out of the ecosystem, and the functionality they provide
    will be taken over by AI models.'
  prefs: []
  type: TYPE_NORMAL
- en: And I think this whole exposition justifies the title I have chosen for this
    section. Next, let’s discover together how prompts are used by LlamaIndex for
    LLM interactions.
  prefs: []
  type: TYPE_NORMAL
- en: <title>Understanding how LlamaIndex uses prompts</title>
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how LlamaIndex uses prompts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'from llama_index.core import SummaryIndex, SimpleDirectoryReader documents
    = SimpleDirectoryReader("files").load_data() summary_index = SummaryIndex.from_documents(documents)
    qe = summary_index.as_query_engine() prompts = qe.get_prompts() for k, p in prompts.items():
        print(f"Prompt Key: {k}")     print("Text:")     print(p.get_template())     print("\n")'
  prefs: []
  type: TYPE_NORMAL
- en: In terms of mechanics, a RAG-based application follows exactly the same rules
    and principles of interaction that a simple user would use in a chat session with
    an LLM. A major difference comes from the fact that RAG is actually a kind of
    prompt engineer on steroids. Behind the scenes, for almost every indexing, retrieval,
    metadata extraction, or final response synthesis operation, the RAG framework
    programmatically produces prompts. These prompts are enriched with context and
    then sent to the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 'In LlamaIndex, for each type of operation that requires an LLM, there is a
    default prompt that is used as a template. Take `TitleExtractor` as an example.
    This is one of the metadata extractors that we already talked about in *Chapter
    4* , *Ingesting Data into Our RAG Workflow* . The `TitleExtractor` class uses
    two predefined prompt templates to get titles from text nodes inside documents.
    It does this in two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: It gets potential titles from individual text Nodes using the `node_template`
    argument, which creates prompts to generate appropriate titles
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Combines the individual Node titles into one overall comprehensive title for
    the whole Document using the `combine_template` prompt
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The default values for the `TitleExtractor` prompts are stored in two constants:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at these two default templates used by `TitleExtractor` , we can easily
    understand how they work. Each template contains a *fixed* text part and a *dynamic*
    part, designated by `{context_str}` or other variables. That is where LlamaIndex
    will actually inject the text content of our Nodes during execution, as seen in
    *Figure 10* *.2* :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_10_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 – How prompts are built by injecting variables into a prompt template
  prefs: []
  type: TYPE_NORMAL
- en: 'The prompt templates used by metadata extractors such as `TitleExtractor` are
    defined directly within the `metadata_extractors.py` module. The relative path
    of this module in the LlamaIndex GitHub repository is `llama-index-core/llama_index/core/extractors/metadata_extractors.py`
    . However, this is just an exception as the vast majority of the default templates
    are defined in two other key modules: `llama-index-core/llama_index/core/prompts/default_prompts.py`
    and `llama-index-core/llama_index/core/prompts/chat_prompts.py` .'
  prefs: []
  type: TYPE_NORMAL
- en: 'Because a RAG workflow built with LlamaIndex can have so many different components
    that rely on LLM interactions and not all prompt templates can be easily located
    within the code base, the framework provides a simple method to identify the templates
    used by a specific component. That method is called `get_prompts()` and can be
    used with agents, retrievers, query engines, response synthesizers, and many other
    RAG components. Here is a simple example of how we can use it to obtain a list
    of prompt templates used by a query engine built on top of `SummaryIndex` :'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first part of the code should be very straightforward at this point. We
    import `SummaryIndex` and `SimpleDirectoryReader` and then ingest the two sample
    files that should have been cloned from our GitHub repository. Once the files
    have been ingested as Documents, we build an index and a query engine from that
    index. In this example, we won’t run any queries because we don’t need to. We
    just want to see the prompts. Therefore, the next step retrieves a dictionary
    containing the default prompts used within the query engine:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The dictionary returned by the `get_prompts()` method maps keys, which identify
    the different prompt types used by the query engine, to values that are the actual
    prompt templates. The last part of the code is responsible for iterating and displaying
    the keys and their corresponding templates:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 10* *.3* shows the results after running this sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_10_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 – The two prompt templates used by the SummaryIndex query engine
  prefs: []
  type: TYPE_NORMAL
- en: 'Examining the output, we’ll see the two templates used by the query engine:
    `text_qa_template` and `refine_template` . You’ll notice that both keys begin
    with the text `response_synthesizer:` . This indicates the exact component of
    the query engine that actually uses the prompts – in our case, the response synthesizer.
    Following the same logic, we can use the `get_prompts()` method on many other
    types of RAG components in order to understand prompts used under the hood.'
  prefs: []
  type: TYPE_NORMAL
- en: Pro tip
  prefs: []
  type: TYPE_NORMAL
- en: An alternative option to inspect the underlying prompts would be to use an advanced
    tracing method – such as the one using the Arize AI Phoenix framework, presented
    in *Chapter 9* , *Customizing and Deploying Our LlamaIndex Project* . Phoenix
    provides a visual representation of the execution flow, making it easier to understand
    how and when different prompts are used, in addition to displaying the final prompts
    with the inserted context. One caveat of using that method, though, is that instead
    of getting the original prompt templates, we’ll see the final prompts – also including
    any context already inserted in the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a reliable technique for inspecting prompts, the next step
    explores ways in which can customize them. Building on the title extractor and
    query engine examples, in the next section, we’ll explore how to customize prompts
    used by various RAG components.
  prefs: []
  type: TYPE_NORMAL
- en: <title>Customizing default prompts</title>
  prefs: []
  type: TYPE_NORMAL
- en: Customizing default prompts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Context information is below. --------------------- {context_str} ---------------------
    Given the context information from llama_index.core import SummaryIndex, SimpleDirectoryReader
    from llama_index.core import PromptTemplate documents = SimpleDirectoryReader("files").load_data()
    summary_index = SummaryIndex.from_documents(documents) qe = summary_index.as_query_engine()
    print(qe.query("Who burned Rome?")) print("------------------------") new_qa_template
    = ( "Context information is below." "---------------------" "{context_str}" "---------------------"
    "Given the context information " " template = PromptTemplate(new_qa_template)
    qe. Update_prompts(     {"response_synthesizer: text_qa_template": template} )
    print(qe.query("Who burned Rome?")) from llama_index.core import SimpleDirectoryReader
    from llama_index.core.node_parser import SentenceSplitter from llama_index.core.extractors
    import TitleExtractor reader = SimpleDirectoryReader(''files'') documents = reader.load_data()
    parser = SentenceSplitter() nodes = parser.get_nodes_from_documents(documents)
    title_extractor = TitleExtractor(summaries=["self"]) meta = title_extractor.extract(nodes)
    print("\nFirst title: " +meta[0][''document_title'']) print("Second title: " +meta[1][''document_title''])
    First title: "The Enduring Influence of Ancient Rome: Architecture, Engineering,
    Conquest, and Legacy" Second title: "The Enduring Bond: Dogs as Loyal Companions
    - Exploring the Unbreakable Connection Between Humans and Man''s Best Friend"
    combine_template = (     "{context_str}. Based on the above candidate titles "
        "and content, what is the comprehensive title for "     "this document? Keep
    it under 6 words. Title: " ) title_extractor = TitleExtractor(     summaries=["self"],
        combine_template=combine_template ) meta = title_extractor.extract(nodes)
    print("\nFirst title: "+meta[0][''document_title'']) print("Second title: "+meta[1][''document_title'']
    First title: "Roman Legacy: Architecture, Engineering, Conquest" Second title:
    "Man''s Best Friend: The Enduring Bond"'
  prefs: []
  type: TYPE_NORMAL
- en: 'While the default prompts provided by LlamaIndex are designed to work well
    in most scenarios, there may be instances where customization is necessary or
    desirable. For example, you might want to adjust prompts to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Incorporate domain-specific knowledge or terminology
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adapt prompts to a particular writing style or tone
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modify prompts to prioritize certain types of information or outputs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Experiment with different prompt structures to optimize performance or quality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By customizing prompts, we can fine-tune the interaction between the RAG components
    and the language model, potentially leading to improved accuracy, relevance, and
    overall effectiveness of our application.
  prefs: []
  type: TYPE_NORMAL
- en: The good news is that we can modify the behavior of various LlamaIndex components
    by supplying our own custom prompt templates. The not-so-good news is that contrary
    to common expectations, writing a good prompt template is not a trivial task.
    One would have to consider many intricacies such as accuracy, relevance, query
    formulation, prompt size, output formatting, and others. Because of the involved
    complexity, the recommended approach for customization is to start with the default
    prompts and use them as a foundation for making any desired modifications. Changes
    should be incremental and ideally followed by rigorous evaluation against a diverse
    set of edge cases. We will have a more detailed discussion about general principles
    and best practices for writing prompts in the next section. For now, let us focus
    on the methods used for prompt customization.
  prefs: []
  type: TYPE_NORMAL
- en: 'In LlamaIndex, every RAG component that exposes the `get_prompts()` method
    also provides an equivalent for modifying these prompt templates – the `update_prompts()`
    method. So, this is the easiest way to change a particular prompt template. Let’s
    take our example from the previous section and experiment with a different prompt.
    This time, we will adapt the `text_qa_template` template to also rely on the LLM’s
    own knowledge when answering the query. The default `text_qa_template` template
    would normally look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we’ll make a very subtle change to this template
    and see how that will affect the behavior of our query engine. Let’s have a look
    at the code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, the code is identical to the previous example, with only one additional
    import that I will explain in a few moments. This time, though, we’ll first run
    a query using the default template. We’ll use this response as a reference later:'
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s now time to change the `prompt_template` template. We first define a string
    containing the new version:'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you carefully compare the new version with the original template, you’ll
    notice a subtle but very important change. In this new version, I’m instructing
    the model to apply not just the knowledge provided in the retrieved context but
    also use its own knowledge on the matter. It’s time to make use of that new import
    we added at the beginning of our code. Because the `update_prompts()` method requires
    the prompts to be in the `BasePromptTemplate` format, we must first make sure
    that our new prompt is structured like this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’re now ready to rerun the query:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s have a look at the final output shown in *Figure 10* *.4* :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_10_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.4 – The query output before and after updating the prompt templates
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the output, that slight modification in the `text_qa_template`
    template of the query engine completely changed its behavior. In a similar fashion,
    instead of changing the answering approach, we could have instructed the LLM to
    answer in a certain linguistic style, speak in rhymes, or anything else we might
    need. I think the value this feature provides for a RAG application is pretty
    clear by now.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, not all LlamaIndex components support the `update_prompts()`
    method. Take, for example, the `TitleExtractor` metadata extractor that I mentioned
    in the previous section. Although metadata extractors do not support the `update_prompts()`
    method, the good news is that we can still change their underlying prompt templates
    by using arguments. In particular, the two templates used by `TitleExtractor`
    can be customized with the `node_template` and `combine_template` arguments. Let’s
    have a look at an example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first part of the example is responsible for ingesting our sample files
    as Documents and then chunking them into individual Nodes. Let’s extract the titles,
    first by using the default prompt templates that we saw in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output so far should be something similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s define a custom prompt template and pass it as an argument to `TitleExtractor`
    for the second run:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Because we’ve added an extra instruction in this custom prompt, the extractor
    should now generate shorter titles. The output for the second run should be something
    along the lines of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: After seeing the basic mechanics of prompt customization, it’s time to move
    on to more advanced methods.
  prefs: []
  type: TYPE_NORMAL
- en: Using advanced prompting techniques in LlamaIndex
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'LlamaIndex offers several advanced prompting techniques that enable you to
    create more customized and expressive prompts, reuse existing prompts, and express
    certain operations more concisely. These techniques include partial formatting,
    prompt template variable mappings, and prompt function mappings. *Table 10.1*
    breaks down the purpose and potential use cases for each method:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Method** | **Purpose** |'
  prefs: []
  type: TYPE_TB
- en: '| Partial formatting | Allows you to partially format a prompt by filling in
    some variables but leaving others to be filled in later. This is useful because
    it allows you to format variables as they become available, rather than maintaining
    all the required prompt variables until the end. The method is particularly useful
    in a multi-step RAG scenario that gradually builds the prompt by gathering different
    user inputs. |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt template variable mappings | They let you specify a mapping between
    some *expected* prompt keys and the keys actually used in your template, enabling
    you to reuse existing string templates without modifying the template variables.
    It is similar to creating an *alias* for template keys. |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt function mappings | This feature allows you to dynamically inject
    certain values, depending on other values or conditions, during query time by
    passing functions as template variables instead of fixed values. |'
  prefs: []
  type: TYPE_TB
- en: Table 10.1 – An overview of the more advanced prompting techniques provided
    by LlamaIndex
  prefs: []
  type: TYPE_NORMAL
- en: 'You’ll find detailed code examples for all three methods in the official LlamaIndex
    documentation here: https://docs.llamaindex.ai/en/stable/examples/prompts/advanced_prompts.html
    .'
  prefs: []
  type: TYPE_NORMAL
- en: Having all these new cool gadgets in our knowledge inventory, we can now refine
    and tailor the dialogue between our application and the LLM, allowing us to customize
    the behavior of almost any RAG component of LlamaIndex.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the final section of this chapter, we move our focus to an important aspect
    of maximizing our RAG setup’s potential: the art and science of prompt engineering.'
  prefs: []
  type: TYPE_NORMAL
- en: <title>The golden rules of prompt engineering</title>
  prefs: []
  type: TYPE_NORMAL
- en: The golden rules of prompt engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Classify the following reviews as positive or negative sentiment: <The food
    was delicious and the service was excellent!> // Positive <I waited over an hour
    and my meal arrived cold.> // Negative <The ambiance was nice but the dishes were
    overpriced.> // Output: There are 15 students in a class. 8 students have dogs
    as pets. If 3 more students get a dog, how many of them would have a dog as a
    pet then? Step 1) Initially there are 15 students and 8 have dogs Step 2) 3 more
    students will get dogs soon Step 3) So the final number is the initial 8 students
    with dogs plus the 3 new students = 8 + 3 = 11 Therefore, the number of students
    that would have a dog as a pet is 11. A factory makes 100 items daily. On Tuesday,
    they boost production by 40% for a special order. However, to adjust inventory,
    they cut Thursday''s output by 20% from Tuesday''s high. Then, expecting a sales
    increase, Friday''s output rises by 10% over the day before. Calculate the production
    numbers for Tuesday, Thursday, and Friday. Let''s simulate a verbal conversation
    between three experts who tackle a complex puzzle. Each expert outlines one step
    in their thought process before exchanging insights with the others, without adding
    any unnecessary remarks. As they progress, any expert who identifies a flaw in
    their reasoning exits the discussion. The process continues until a solution is
    found or all available options have been exhausted. The problem they need to solve
    is: "Using only numbers 3, 3, 7, 7 and basic arithmetic operations, is it possible
    to obtain the value 25?"'
  prefs: []
  type: TYPE_NORMAL
- en: This section is not intended to serve as a definitive guide to prompt engineering.
    In fact, the field is an ever-expanding one. Since many LLMs are demonstrating
    emerging capabilities that were not initially anticipated, it is only natural
    that our methods of interacting with these linguistic experts will also be refined
    over time. In other words, as LLMs evolve to better model and understand human
    nature, we in turn learn new ways of interacting with them. In this section, I
    aim to present some of the most commonly used techniques in prompt engineering,
    as well as the basic principles that govern the field. As stated in the previous
    section, writing a good prompt requires a fine balance between several parameters.
    Here are some of the most important aspects to consider when building prompts
    for a RAG application.
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy and clarity in expression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The prompt should be clear and precise, avoiding ambiguity. The more clearly
    you state what you need, the more likely you are to get a relevant response. It’s
    important to articulate the question or task in a way that leaves little room
    for misinterpretation. Make no assumptions about the model’s ability to understand
    your message. These assumptions are usually biased and tend to produce hallucinations
    in return.
  prefs: []
  type: TYPE_NORMAL
- en: Directiveness
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How directive the prompt is can significantly impact the response. A prompt
    can range from open-ended – encouraging creative or broad responses – to highly
    specific – requesting a very particular type of answer. The level of directiveness
    should match the intended outcome. Given that we’re actually building prompt templates
    that mix a static part with dynamically retrieved content, consider exceptional
    scenarios and edge cases in which the model might misunderstand the prompt. Use
    clear instructions or commands (for example, `Summarize` , `Analyze` , and `Explain`
    ) to guide the model on the desired task. Our prompts must be broad enough to
    accommodate varied inputs yet detailed enough to direct the model effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Context quality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is a major pain point for building an effective RAG system. Both the quality
    and structure of our proprietary knowledge base as well as the ability to retrieve
    the most relevant context from it are very important aspects. *Garbage in, garbage
    out* may be regarded as a general rule applicable to this subject. Try to remove
    any inconsistencies in the data, special characters that might derail the LLM,
    duplicate data, and even grammatical errors in the text. These types of quality
    issues will unfortunately affect both the retrieval and the final response synthesis.
    Experiment with different retrieval strategies, such as the ones discussed in
    *Chapter 6* , *Querying Our Data, Part 1 – Context Retrieval* . Try different
    values for `similarity_top_k` , `chunk_size` , and `chunk_overlap` , as discussed
    in *Chapter 4* , *Ingesting Data into Our RAG Workflow* . Employ re-rankers and
    Node postprocessors to increase the context quality, as we did in *Chapter 7*
    , *Querying Our Data, Part 2 – Postprocessing and* *Response Synthesis* .
  prefs: []
  type: TYPE_NORMAL
- en: Context quantity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There’s a balance between being concise and offering sufficient detail. A prompt
    should be brief enough to maintain focus but detailed enough to convey the specific
    requirements of the task or question. Too little context may result in answers
    that lack depth or relevance, while too much may confuse the model or lead it
    off-topic.
  prefs: []
  type: TYPE_NORMAL
- en: In RAG scenarios, as the amount of context provided in a prompt increase, it’s
    important to consider the potential impact on the alignment and accuracy of generated
    responses. While providing more context can be beneficial in many cases, as it
    gives the language model a broader understanding of the task at hand, there are
    also risks associated with excessively long prompts.
  prefs: []
  type: TYPE_NORMAL
- en: For example, when a prompt becomes too long, there is a higher chance of introducing
    irrelevant or contradictory information. This can lead to misalignment between
    the intended task and the model’s understanding of it. The model may give too
    much attention to tangential details or lose focus on the core objective. Maintaining
    a clear and concise prompt helps ensure that the model stays aligned with the
    desired output.
  prefs: []
  type: TYPE_NORMAL
- en: Also, as the context grows, the model has to process and consider a larger amount
    of information. This increased **cognitive load** can lead to a decrease in accuracy.
    The model may struggle to identify the most relevant pieces of information or
    may give undue importance to less significant details. Additionally, longer prompts
    are more likely to contain ambiguities or inconsistencies, which can further degrade
    the accuracy of the responses.
  prefs: []
  type: TYPE_NORMAL
- en: Cognitive load in the context of LLMs
  prefs: []
  type: TYPE_NORMAL
- en: Cognitive load refers to the amount of processing effort and resources required
    by the language model to process, understand, and generate a response based on
    the provided context. In the case of RAG systems, the cognitive load is directly
    related to the quantity and complexity of the information present in the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Node postprocessors such as `SimilarityPostprocessor` or `SentenceEmbeddingOptimizer`
    can partially mitigate this issue by filtering less relevant Nodes or shortening
    their content, and therefore reducing the final prompt submitted to the LLM. We
    covered these methods in *Chapter 7* , *Querying Our Data, Part 2 – Postprocessing
    and Response Synthesis* . Moreover, if the retrieved context is inherently long,
    consider breaking it down into smaller, more manageable chunks.
  prefs: []
  type: TYPE_NORMAL
- en: Context ordering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The overall effectiveness of our RAG pipeline does not rely just on the quantity
    and quality of context. Especially when dealing with longer context, most LLMs
    may perform differently when trying to extract the key information from that context,
    depending on where exactly that key information is placed. A good approach is
    to structure the prompt hierarchically, with the most critical information at
    the beginning or at the end. This ensures that the model prioritizes the core
    instructions and context. That’s where tools such as Node re-rankers or the `LongContextReorder`
    postprocessor may become useful.
  prefs: []
  type: TYPE_NORMAL
- en: Side note
  prefs: []
  type: TYPE_NORMAL
- en: There’s an increasingly popular RAG evaluation technique called the *needle
    in a haystack test* , in which researchers gauge the model’s ability to notice
    and recall a very specific piece of information from a larger context provided
    to the LLM. This specific information looks unsuspecting and is usually seamlessly
    blended into the overall context. In many ways, this method is similar to testing
    a human’s ability to pay attention to a certain text and then recall key information
    in that text.
  prefs: []
  type: TYPE_NORMAL
- en: Required output format
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In most cases, when building RAG workflows, we need LLMs to generate structured
    or semi-structured outputs. In almost all scenarios, we need the output to be
    predictable in terms of format, size, or language. Sometimes, providing a few
    examples in our prompt may lead to better responses, but that’s not a silver bullet
    for all scenarios. That’s were using output parsers and Pydantic programs becomes
    really important. We talked about these topics in *Chapter 7* , *Querying Our
    Data, Part 2 – Postprocessing and* *Response Synthesis* .
  prefs: []
  type: TYPE_NORMAL
- en: Inference cost
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In most cases, we’ll be running our applications within very specific cost constraints.
    Ignoring token usage would be a clear mistake. So, make sure you’re doing cost
    estimations, and always keep track of token usage. In addition, you could use
    tools such as `LongLLMLinguaPostprocessor` for prompt compression. We talked about
    this Node postprocessor in *Chapter 7* , *Querying Our Data, Part 2 – Postprocessing
    and Response Synthesis* . Prompt compression techniques have the potential to
    improve not only cost efficiency but also the quality of the final response by
    eliminating redundant information from our context and keeping just key information.
  prefs: []
  type: TYPE_NORMAL
- en: Overall system latency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While this parameter depends on many factors, bloated, inefficient, or ambiguous
    prompts can also negatively affect system latency. It’s just like talking to a
    real person. The longer and less efficient the query, the more processing will
    be required from the model in order to best understand the actual intent behind
    the query. Longer processing times will negatively impact the overall user experience.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt engineering is a continuous process of experimentation and iteration.
    Regularly evaluate the performance of your prompts and refine them based on the
    results. Remember – this is a long game, and the rules are being constantly re-written.
    Try to keep your knowledge up to date with the latest advancements and techniques
    in prompt engineering, as the field is rapidly evolving.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right LLM for the task
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the world of AI, not all LLMs are equal. In *Chapter 9* , *Customizing and
    Deploying Our LlamaIndex Project* , we already saw how easy is to customize different
    components of our RAG pipeline, including the underlying LLM. But there are actually
    many options available, so which one should we select for the job? Choosing the
    *wrong* LLM for a particular task will likely cancel many of the efforts we invested
    in crafting the actual prompts. It’s pretty much like trying to get an answer
    from the wrong person. If you’re persuasive enough, chances are you’ll get an
    answer at some point. However, that may not be the answer you were looking for.
  prefs: []
  type: TYPE_NORMAL
- en: That’s why understanding the different flavors of LLMs and knowing which one
    qualifies for a given task is essential. Several key characteristics should be
    useful for our model selection. Let’s look at these next.
  prefs: []
  type: TYPE_NORMAL
- en: Model architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Models can have different underlying architectures, and these may determine
    their inherent capabilities. For example, encoder-only models are specialized
    in encoding and classifying input text, useful for categorizing text into defined
    categories, such as with **Bidirectional Encoder Representations from Transformers**
    ( **BERT** ), which excels in **next sentence prediction** ( **NSP** ) tasks (
    https://en.wikipedia.org/wiki/BERT_(language_model) ).
  prefs: []
  type: TYPE_NORMAL
- en: Encoder-decoder models are capable of both understanding input text and generating
    responses, making them ideal for text generation and comprehension tasks, such
    as translation and summarizing articles. One example that fits in this category
    is **Bidirectional and Auto-Regressive Transformer** ( **BART** ) ( https://huggingface.co/docs/transformers/en/model_doc/bart
    ).
  prefs: []
  type: TYPE_NORMAL
- en: Decoder-only models can decode or generate subsequent words or tokens from a
    given prompt and are primarily used for text generation. Models such as **Generative
    Pre-trained Transformer** ( **GPT** ), Mistral, Claude, and LLaMa are superstars
    in this domain.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are also more exotic architectures such as **Mixture-of-Experts** ( **MoE**
    ), which essentially leverage a *sparse MoE* framework to offer dynamic, token-specific
    processing – see Shazeer et al. (2017), *Outrageously Large Neural Networks: The
    Sparsely-Gated Mixture-of-Experts Layer* ( https://doi.org/10.48550/arXiv.1701.06538
    ). This approach can significantly enhance performance across a range of domains,
    including mathematics, code generation, and multilingual tasks, as demonstrated
    by **Mixtral 8x7B** .'
  prefs: []
  type: TYPE_NORMAL
- en: Model size
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Model size is another critical factor to consider when selecting an LLM, as
    it directly impacts both the potential computational cost and the model’s capabilities.
    The number of parameters within an LLM, ranging from weights to biases adjusted
    during training, serves as a proxy for understanding the model’s complexity and,
    by extension, its operational expense. Larger models, such as GPT-4 with its estimated
    1.76 trillion parameters, offer profound capabilities but come with higher costs
    and requirements for computational resources. On the other hand, medium-sized
    models, typically under 10 billion parameters, strike a balance between affordability
    and performance, making them suitable for a wide array of applications without
    breaking the bank.
  prefs: []
  type: TYPE_NORMAL
- en: Inference speed
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: That’s also a key parameter as it determines how quickly a model can process
    input and generate output. While larger models may offer enhanced performance
    in terms of output quality and depth, their inference speed tends to be slower
    due to the sheer volume of computations required. It’s important to note that
    inference speed is influenced by various factors beyond just the number of parameters,
    including the efficiency of the model architecture and the computational infrastructure
    used. Techniques to reduce inference time, such as model pruning, quantization,
    and leveraging specialized hardware, can significantly improve the usability of
    LLMs in real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: To make things even more complex, apart from these characteristics, LLMs can
    be specialized for various tasks or domains, enhancing their performance in specific
    scenarios. This specialization arises from the type of data and the training objectives
    used to fine-tune the model. Let’s look at some common specializations next.
  prefs: []
  type: TYPE_NORMAL
- en: Chat models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Chat models are optimized for conversational interactions. They are designed
    to engage users in dialogue, providing responses that mimic human-like conversation.
    These models are adept at back-and-forth exchanges and can maintain context over
    a series of interactions.
  prefs: []
  type: TYPE_NORMAL
- en: They are the ideal choice for building chatbots or virtual assistants where
    the interaction is more casual or conversational. These models are used in applications
    requiring natural, engaging dialogue with users, such as customer service bots,
    entertainment applications, or virtual companions. As a particular characteristic,
    they tend to be more open-ended in their responses, aiming to generate replies
    that are engaging, contextually relevant, and sometimes even entertaining.
  prefs: []
  type: TYPE_NORMAL
- en: Instruct models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Instruct models are fine-tuned to understand and execute specific instructions
    or queries. They prioritize executing the given task based on the instruction
    over engaging in a dialogue. That makes them suitable for scenarios where the
    user needs the model to perform a particular task, such as summarizing a document,
    generating code based on a prompt, or providing detailed explanations. These models
    are preferred in educational tools, productivity applications, and anywhere a
    direct, clear response to a query is needed, such as in the intricate workflow
    of a RAG application.
  prefs: []
  type: TYPE_NORMAL
- en: They are more focused on accuracy and relevance to the task at hand rather than
    maintaining a conversational tone. Their responses are tailored toward fulfilling
    the user’s request as efficiently and effectively as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Codex models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: These models are optimized for understanding and generating code. They have
    been trained in a vast corpus of programming languages and can assist with coding
    tasks, debug code, explain code snippets, and even generate entire programs based
    on a description. This makes them the perfect candidates for integrating into
    development environments, coding education tools, and anywhere automated coding
    assistance is beneficial.
  prefs: []
  type: TYPE_NORMAL
- en: Summarization models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Specialized in condensing long texts into shorter summaries while retaining
    key information and context. These models focus on capturing the essence of the
    content and presenting it concisely. They are useful for news aggregation services,
    research, content creation, and any scenario where quick insights from long documents
    are needed.
  prefs: []
  type: TYPE_NORMAL
- en: Translation models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As the name implies these models are designed to translate text from one language
    to another. They have been trained on large multilingual datasets to understand
    and translate between languages with high accuracy, and they are best suited for
    global communication platforms, content localization, and educational tools aimed
    at language learners.
  prefs: []
  type: TYPE_NORMAL
- en: Question-answering models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Fine-tuned to understand questions posed in NL and provide accurate answers
    by referencing provided texts or their vast training data, these models are key
    in building intelligent search engines, educational aids, and interactive knowledge
    bases.
  prefs: []
  type: TYPE_NORMAL
- en: And the list could probably go on with other types of models, fine-tuned for
    specific domains or applications. Also, keep in mind that because these different
    specializations tend to enhance or diminish certain capabilities of the model,
    our carefully crafted prompts may yield inconsistent results. For one model, a
    prompt may lead to near-perfect responses, while for another it could barely hit
    an average mark.
  prefs: []
  type: TYPE_NORMAL
- en: When choosing your LLM, it’s essential to weigh the trade-offs between all these
    characteristics and the specific requirements of your RAG application. Understanding
    these aspects helps in selecting a model that not only fits within your budget
    but also meets your performance and speed expectations. Whether you’re deploying
    an LLM for real-time applications requiring quick responses or complex tasks demanding
    deep understanding and generation capabilities, the chosen model will have a profound
    impact on the outcomes of your LlamaIndex application. But keep in mind that you’re
    never limited to using a single model for your entire RAG logic. As LlamaIndex
    gives you endless possibilities for customization, working with a suite of different
    models can also be an option. You just have to experiment and evaluate until you
    find the ideal mix and purpose for each one.
  prefs: []
  type: TYPE_NORMAL
- en: Common methods used for creating effective prompts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While simple prompts can be useful for many tasks, more advanced techniques
    are often required for complex reasoning or multi-step processes. While definitely
    not exhaustive, this section covers several powerful prompting techniques that
    can significantly enhance the performance of language models in our RAG applications.
    Since there’s already an abundance of study materials, free courses, and plenty
    of examples available on the web, in case you’re not yet familiar with these methods,
    take this list as a mere starting point for your future learning path.
  prefs: []
  type: TYPE_NORMAL
- en: Few-shot prompting, also known as k-shot prompting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As described in the paper by Brown et al. (2020), *Language Models are Few-Shot
    Learners* ( https://doi.org/10.48550/arXiv.2005.14165 ), for complex tasks involving
    LLMs, few-shot prompting with demonstrations can enable in-context learning and
    improve performance. This method relies on providing a few examples of the task,
    along with the expected output, to condition the model. You can experiment with
    different numbers of examples (for example, one-shot, three-shot, and five-shot)
    to find the optimal balance, hence the *k-shot* alternative name.
  prefs: []
  type: TYPE_NORMAL
- en: What about zero-shot prompting?
  prefs: []
  type: TYPE_NORMAL
- en: For reference, *zero-shot prompting* involves presenting a model with a question
    without any preceding contextual question/answer pairs. This approach is more
    challenging for the model compared to one-shot or few-shot prompting, due to the
    absence of context.
  prefs: []
  type: TYPE_NORMAL
- en: 'When using few-shot prompting, keep in mind that the format you use for the
    examples and the distribution of the input text are important factors that can
    affect performance. While the few-shot prompting method increases the probability
    of a correct answer for simpler tasks, it may still struggle with more complex
    reasoning scenarios. Here’s a practical prompt example using this technique:'
  prefs: []
  type: TYPE_NORMAL
- en: Providing the model with a few examples in this style enables in-context learning
    and improves performance on the task without requiring fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Chain-of-Thought (CoT) prompting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First introduced in the paper by Wei et al. (2023), *Chain-of-Thought Prompting
    Elicits Reasoning in Large Language Models* ( https://doi.org/10.48550/arXiv.2201.11903
    ), this method provides impressive results for LLM tasks requiring reasoning or
    multi-step processes. We can use CoT prompting to encourage the model to break
    down the problem and show its thought process. We can include examples in our
    prompts, demonstrating the step-by-step reasoning process in the prompt. Here
    is a practical prompt example:'
  prefs: []
  type: TYPE_NORMAL
- en: The first part of the prompt demonstrates the reasoning process, guiding the
    LLM to better answer the second part – which represents the actual task.
  prefs: []
  type: TYPE_NORMAL
- en: Self-consistency
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Self-consistency aims to improve the performance of CoT prompting by sampling
    multiple, diverse reasoning paths and using the generations to select the most
    consistent answer. First introduced in the paper by Wang et al. (2023), *Self-Consistency
    Improves Chain of Thought Reasoning in Language Models* ( https://doi.org/10.48550/arXiv.2203.11171
    ), the self-consistency method helps boost performance on tasks involving arithmetic
    and commonsense reasoning by replacing the more traditional CoT prompting. Self-consistency
    involves providing few-shot CoT examples, generating multiple reasoning paths,
    and then selecting the most consistent answer based on these paths.
  prefs: []
  type: TYPE_NORMAL
- en: This approach acknowledges that language models, like humans, may sometimes
    make mistakes or take incorrect reasoning steps. However, by leveraging the diversity
    of reasoning paths and selecting the most consistent answer, self-consistency
    can potentially provide better answers than CoT prompting.
  prefs: []
  type: TYPE_NORMAL
- en: Tree of Thoughts (ToT) prompting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'ToT is a framework that generalizes over CoT prompting and encourages the exploration
    of thoughts that serve as intermediate steps for general problem-solving with
    language models. Under the hood, it maintains a *tree of thoughts* , where thoughts
    represent coherent language sequences that serve as intermediate steps toward
    solving a problem. The language model’s ability to generate and evaluate thoughts
    is combined with specialized search algorithms to enable systematic exploration
    of thoughts. ToT prompting involves prompting the language model to evaluate intermediate
    thoughts as *sure* / *maybe* / *impossible* with regard to reaching the desired
    solution and then using search algorithms to explore the most promising paths.
    The method was presented for the first time in the following papers: Yao et al.
    (2023), *Tree of Thoughts: Deliberate Problem Solving with Large Language Models*
    ( https://doi.org/10.48550/arXiv.2305.10601 ), and Long et al. (2023), *Large
    Language Model Guided* *Tree-of-Thought* ( https://doi.org/10.48550/arXiv.2305.08291
    ).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a sample prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: Prompt chaining
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This method relies on breaking down complex tasks into subtasks and using a
    chain of prompts, where each prompt’s output serves as an input for the next.
    Similar to the approach I used for the PITS application in the `training_material_builder.py`
    module, prompt chaining can improve the reliability, transparency, and controllability
    of the application. By default, in RAG applications, we use separate prompts for
    retrieving relevant information and generating a final output based on the retrieved
    context.
  prefs: []
  type: TYPE_NORMAL
- en: By following these golden rules and methods, you can develop more effective
    and reliable RAG applications using LlamaIndex and leverage the full potential
    of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: <title>Summary</title>
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter explored the importance of prompt engineering in building effective
    RAG applications with LlamaIndex. We learned how to inspect and customize the
    default prompts used by various components.
  prefs: []
  type: TYPE_NORMAL
- en: The chapter provided an overview of key principles and best practices for crafting
    high-quality prompts, as well as advanced prompting techniques. Additionally,
    it emphasized the significance of choosing the right language model for the task
    at hand and understanding their different architectures, capabilities, and trade-offs.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we talked about some simple yet powerful prompting methods, such as
    few-shot prompting, CoT prompting, self-consistency, ToT, and prompt chaining
    to enhance the reasoning and problem-solving abilities of language models. Mastering
    prompt engineering is crucial for unlocking the full potential of LLMs in RAG
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: As we prepare to wrap up our journey, I invite you to join me in the final chapter
    of this book, where I will do my best to equip you with some additional learning
    tools and provide you with a bit of guidance on your future learning path.
  prefs: []
  type: TYPE_NORMAL
