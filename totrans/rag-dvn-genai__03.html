<html><head></head><body>
  <div><h1 class="chapterNumber">3</h1>
    <h1 id="_idParaDest-76" class="chapterTitle">Building Index-Based RAG with LlamaIndex, Deep Lake, and OpenAI</h1>
    <p class="normal">Indexes increase precision and speed performances, but they offer more than that. Indexes transform retrieval-augmented generative AI by adding a layer of transparency. With an index, the source of a response generated by a RAG model is fully traceable, offering visibility into the precise location and detailed content of the data used. This improvement not only mitigates issues like bias and hallucinations but also addresses concerns around copyright and data integrity.</p>
    <p class="normal">In this chapter, we’ll explore how indexed data allows for greater control over generative AI applications. If the output is unsatisfactory, it’s no longer a mystery why, since the index allows us to identify and examine the exact data source of the issue. This capability makes it possible to refine data inputs, tweak system configurations, or switch components, such as vector store software and generative models, to achieve better outcomes.</p>
    <p class="normal">We will begin the chapter by laying out the architecture of an index-based RAG pipeline that will enhance speed, precision, and traceability. We will show how LlamaIndex, Deep Lake, and OpenAI can be seamlessly integrated without having to create all the necessary functions ourselves. This provides a solid base to start building from. Then, we’ll introduce the main indexing types we’ll use in our programs, such as vector, tree, list, and keyword indexes. Then, we will build a domain-specific drone technology LLM RAG agent that a user can interact with. Drone technology is expanding to all domains, such as fire detection, traffic information, and sports events; hence, I’ve decided to use it in our example. The goal of this chapter is to prepare an LLM drone technology dataset that we will enhance with multimodal data in the next chapter. We will also illustrate the key indexing types in code.</p>
    <p class="normal">By the end of this chapter, you’ll be adept at manipulating index-based RAG through vector stores, datasets, and LLMs, and know how to optimize retrieval systems and ensure full traceability. You will discover how our integrated toolkit—combining LlamaIndex, Deep Lake, and OpenAI—not only simplifies technical complexities but also frees your time to develop and hone your analytical skills, enabling you to dive deeper into understanding RAG-driven generative AI.</p>
    <p class="normal">We’ll cover the following topics in this chapter:</p>
    <ul>
      <li class="bulletList">Building a semantic search engine with a LlamaIndex framework and indexing methods</li>
      <li class="bulletList">Populating Deep Lake vector stores</li>
      <li class="bulletList">Integration of LlamaIndex, Deep Lake, and OpenAI</li>
      <li class="bulletList">Score ranking and cosine similarity metrics</li>
      <li class="bulletList">Metadata enhancement for traceability</li>
      <li class="bulletList">Query setup and generation configuration</li>
      <li class="bulletList">Introducing automated document ranking</li>
      <li class="bulletList">Vector, tree, list, and keyword indexing types</li>
    </ul>
    <h1 id="_idParaDest-77" class="heading-1">Why use index-based RAG?</h1>
    <p class="normal">Index-based search<a id="_idIndexMarker173"/> takes advanced RAG-driven generative AI to another level. It increases the speed of retrieval when faced with large volumes of data, taking us from raw chunks of data to organized, indexed nodes that we can trace from the output back to the source of a document and its location.</p>
    <p class="normal">Let’s understand the differences <a id="_idIndexMarker174"/>between a vector-based similarity search and an index-based search by analyzing the architecture of an index-based RAG.</p>
    <h2 id="_idParaDest-78" class="heading-2">Architecture</h2>
    <p class="normal">Index-based search is faster than<a id="_idIndexMarker175"/> vector-based search in RAG because it directly accesses relevant data using indices, while vector-based search sequentially compares embeddings across all records. We implemented a vector-based similarity search program in <em class="chapterRef">Chapter 2</em>, <em class="italic">RAG Embedding Vector Stores with Deep Lake and OpenAI</em>, as shown in <em class="italic">Figure 3.1</em>:</p>
    <ul>
      <li class="bulletList">We collected and prepared data in <em class="italic">Pipeline #1: Data Collection and Preparation</em></li>
      <li class="bulletList">We embedded the data and stored the prepared data in a vector store in <em class="italic">Pipeline #2: Embeddings and vector store</em></li>
      <li class="bulletList">We then ran retrieval queries and generative AI with <em class="italic">Pipeline #3</em> to process user input, run retrievals based on vector similarity searches, augment the input, generate a response, and apply performance metrics.</li>
    </ul>
    <p class="normal">This approach <a id="_idIndexMarker176"/>is flexible because it gives you many ways to implement each component, depending on the needs of your project.</p>
    <figure class="mediaobject"><img src="img/B31169_03_01.png" alt="A diagram of a process  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 3.1: RAG-driven generative AI pipelines, as described in Chapter 2, with additional functionality</p>
    <p class="normal">However, implementing index-based searches will take us into the future of AI, which will be faster, more precise, and traceable. We will follow the same process as in <em class="italic">Chapter 2</em>, with three pipelines, to make sure that you are ready to work in a team in which the tasks are specialized. Since we are using the same pipelines as in <em class="italic">Chapter 2</em>, let’s add the functions from that chapter to them, as shown in <em class="italic">Figure 3.1</em>:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Pipeline Component #1 and D2-Index</strong>: We will collect data and preprocess it. However, this <a id="_idIndexMarker177"/>time, we will prepare the data source one document at a time and store them in separate files. We will then add their name and location to the metadata we load into the vector store. The metadata will help us trace a response all the way back to the exact file that the retrieval function processed. We will have a direct link from a response to the data that it was based on.</li>
      <li class="bulletList"><strong class="keyWord">Pipeline Component #2 and D3-Index</strong>: We will load the data into a vector store by installing and using the innovative integrated <code class="inlineCode">llama-index-vector-stores-deeplake</code> package, which includes everything we need in an optimized starter scenario: chunking, embedding, storage, and even LLM integration. We have everything we need to get to work on index-based RAG in a few lines of code! This way, once we have a solid program, we can customize and expand the pipelines as we wish, as we did, for example, in <em class="italic">Chapter 2</em>, when we explicitly chose the LLM models and chunking sizes.</li>
      <li class="bulletList"><strong class="keyWord">Pipeline Component #3 and D4-Index</strong>: We will load the data in a dataset by installing and using the innovative integrated <code class="inlineCode">llama-index-vector-stores-deeplake</code> package, which includes everything we need to get indexed-based retrieval and generation started, including automated ranking and scoring. The process is seamless and extremely productive. We’ll leverage LlamaIndex with Deep Lake to streamline information retrieval and processing. An integrated retriever will efficiently fetch relevant data from the Deep Lake repository, while an LLM agent will then intelligently synthesize and interact with the retrieved information to generate meaningful insights or actions. Indexes are designed for fast retrieval, and we will implement several indexing methods.</li>
      <li class="bulletList"><strong class="keyWord">Pipeline Component #3 and E1-Index</strong>: We will add a time and score metric to evaluate the<a id="_idIndexMarker178"/> output.</li>
    </ul>
    <p class="normal">In the previous chapter, we implemented vector-based similarity search and retrieval. We embedded documents to transform data into high-dimensional vectors. Then, we performed retrieval by calculating distances between vectors. In this chapter, we will go further and create a vector store. However, we will load the data into a dataset that will be reorganized using retrieval indexing types. <em class="italic">Table 3.1</em> shows the differences between vector-based and index-based search and retrieval methods:</p>
    <table id="table001-1" class="table-container">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Feature</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Vector-based similarity search and retrieval</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Index-based vector, tree, list, and keyword search and retrieval</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Flexibility</p>
          </td>
          <td class="table-cell">
            <p class="normal">High</p>
          </td>
          <td class="table-cell">
            <p class="normal">Medium (precomputed structure)</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Speed</p>
          </td>
          <td class="table-cell">
            <p class="normal">Slower<a id="_idIndexMarker179"/> with large datasets</p>
          </td>
          <td class="table-cell">
            <p class="normal">Fast and optimized for quick retrieval</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Scalability</p>
          </td>
          <td class="table-cell">
            <p class="normal">Limited by<a id="_idIndexMarker180"/> real-time processing</p>
          </td>
          <td class="table-cell">
            <p class="normal">Highly scalable with large datasets</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Complexity</p>
          </td>
          <td class="table-cell">
            <p class="normal">Simpler setup</p>
          </td>
          <td class="table-cell">
            <p class="normal">More complex and requires an indexing step</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Update Frequency</p>
          </td>
          <td class="table-cell">
            <p class="normal">Easy to update</p>
          </td>
          <td class="table-cell">
            <p class="normal">Requires re-indexing for updates</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="packt_figref">Table 3.1: Vector-based and index-based characteristics</p>
    <p class="normal">We will now build a semantic index-based RAG program with Deep Lake, LlamaIndex, and OpenAI.</p>
    <h1 id="_idParaDest-79" class="heading-1">Building a semantic search engine and generative agent for drone technology</h1>
    <p class="normal">In this section, we will build a<a id="_idIndexMarker181"/> semantic index-based search engine and generative AI agent engine using Deep Lake vector stores, LlamaIndex, and OpenAI. As mentioned earlier, drone technology is expanding in domains such as fire detection and traffic control. As such, the program’s goal is to provide an index-based RAG agent for drone technology questions and answers. The program will demonstrate how drones use computer vision techniques to identify vehicles and other objects. We will implement the architecture illustrated in <em class="italic">Figure 3.1</em>, described in the <em class="italic">Architecture</em> section of this chapter.</p>
    <div><p class="normal">Open <code class="inlineCode">2-Deep_Lake_LlamaIndex_OpenAI_indexing.ipynb</code> from the GitHub repository of this chapter. The titles of this section are the same as the section titles in the notebook, so you can match the explanations with the code.</p>
    </div>
    <p class="normal">We will first begin by installing the environment. Then, we will build the three main pipelines of the program:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Pipeline 1</strong>: Collecting and preparing the documents. Using sources like GitHub and Wikipedia, collect and clean documents for indexing.</li>
      <li class="bulletList"><strong class="keyWord">Pipeline 2</strong>: Creating and populating a Deep Lake vector store. Create and populate a Deep Lake vector store with the prepared documents.</li>
      <li class="bulletList"><strong class="keyWord">Pipeline 3</strong>: Index-based RAG for query processing and generation. Applying time and score performances with LLMs and cosine similarity metrics.</li>
    </ul>
    <p class="normal">When possible, break<a id="_idIndexMarker182"/> your project down into separate pipelines so that teams can progress independently and in parallel. The pipelines in this chapter are an example of how this can be done, but there are many other ways to do this, depending on your project. For now, we will begin by installing the environment.</p>
    <h2 id="_idParaDest-80" class="heading-2">Installing the environment</h2>
    <p class="normal">The environment is mostly the <a id="_idIndexMarker183"/>same as in the previous chapter. Let’s focus on the packages that integrate LlamaIndex, vector store capabilities for Deep Lake, and also OpenAI modules. This integration is a major step forward to seamless cross-platform implementations:</p>
    <pre class="programlisting code"><code class="hljs-code">!pip install llama-index-vector-stores-deeplake==0.1.6
</code></pre>
    <p class="normal">The program requires additional Deep Lake functionalities:</p>
    <pre class="programlisting code"><code class="hljs-code">!pip install deeplake==3.9.8
</code></pre>
    <p class="normal">The program also requires LlamaIndex functionalities:</p>
    <pre class="programlisting code"><code class="hljs-code">!pip install llama-index==0.10.64
</code></pre>
    <p class="normal">Let’s now check if the packages can be properly imported from <code class="inlineCode">llama-index</code>, including vector stores for Deep Lake:</p>
    <pre class="programlisting code"><code class="hljs-code">from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Document
from llama_index.vector_stores.deeplake import DeepLakeVectorStore
</code></pre>
    <p class="normal">With that, we have installed the environment. We will now collect and prepare the documents.</p>
    <h2 id="_idParaDest-81" class="heading-2">Pipeline 1: Collecting and preparing the documents</h2>
    <p class="normal">In this section, we will <a id="_idIndexMarker184"/>collect and prepare the drone-related <a id="_idIndexMarker185"/>documents with the metadata necessary to trace the documents back to their source. The goal is to trace a response’s content back to the exact chunk of data retrieved to find its source. First, we will create a data directory in which we will load the documents:</p>
    <pre class="programlisting code"><code class="hljs-code">!mkdir data
</code></pre>
    <p class="normal">Now, we will use a heterogeneous corpus for the drone technology data that we will process using <code class="inlineCode">BeautifulSoup</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">import requests
from bs4 import BeautifulSoup
import re
import os
urls = [
    "https://github.com/VisDrone/VisDrone-Dataset",
    "https://paperswithcode.com/dataset/visdrone",
    "https://openaccess.thecvf.com/content_ECCVW_2018/papers/11133/Zhu_VisDrone-DET2018_The_Vision_Meets_Drone_Object_Detection_in_Image_Challenge_ECCVW_2018_paper.pdf",
    "https://github.com/VisDrone/VisDrone2018-MOT-toolkit",
    "https://en.wikipedia.org/wiki/Object_detection",
    "https://en.wikipedia.org/wiki/Computer_vision",…
]
</code></pre>
    <p class="normal">The corpus contains a list of sites related to drones, computer vision, and related technologies. However, the list also contains noisy links such as <a href="https://keras.io/">https://keras.io/</a> and <a href="https://pytorch.org/">https://pytorch.org/</a>, which do <em class="italic">not</em> contain the specific information we are looking for.</p>
    <div><p class="normal">In real-life projects, we will not always have the luxury of working on perfect, pertinent, structured, and well-formatted data. Our RAG pipelines must be sufficiently robust to retrieve relevant data in a noisy environment.</p>
    </div>
    <p class="normal">In this case, we <a id="_idIndexMarker186"/>are working with<a id="_idIndexMarker187"/> unstructured data in various formats and variable quality as related to drone technology. Of course, in a closed environment, we can work with the persons or organizations that produce the documents, but we must be ready for any type of document in a fast-moving, digital world.</p>
    <p class="normal">The code will fetch and clean the data, as it did in <em class="italic">Chapter 2</em>:</p>
    <pre class="programlisting code"><code class="hljs-code">def clean_text(content):
    # Remove references and unwanted characters
    content = re.sub(r'\[\d+\]', '', content)   # Remove references
    content = re.sub(r'[^\w\s\.]', '', content)  # Remove punctuation (except periods)
    return content
def fetch_and_clean(url):
    try:
        response = requests.get(url)
        response.raise_for_status()  # Raise exception for bad responses (e.g., 404)
        soup = BeautifulSoup(response.content, 'html.parser')
        # Prioritize "mw-parser-output" but fall back to "content" class if not found
        content = soup.find('div', {'class': 'mw-parser-output'}) or soup.find('div', {'id': 'content'})
        if content is None:
            return None
        # Remove specific sections, including nested ones
        for section_title in ['References', 'Bibliography', 'External links', 'See also', 'Notes']:
            section = content.find('span', id=section_title)
            while section:
                for sib in section.parent.find_next_siblings():
                    sib.decompose()
                section.parent.decompose()
                section = content.find('span', id=section_title)
        # Extract and clean text
        text = content.get_text(separator=' ', strip=True)
        text = clean_text(text)
        return text
    except requests.exceptions.RequestException as e:
        print(f"Error fetching content from {url}: {e}")
        return None  # Return None on error
</code></pre>
    <p class="normal">Each project will require specific names and paths for the original data. In this case, we will introduce an additional function to save each piece of text with the name of its data source, by creating a keyword based on its URL:</p>
    <pre class="programlisting code"><code class="hljs-code"># Directory to store the output files
output_dir = './data/'
os.makedirs(output_dir, exist_ok=True)
# Processing each URL and writing its content to a separate file
for url in urls:
    article_name = url.split('/')[-1].replace('.html',")  # Handle .html extension
    filename = os.path.join(output_dir, article_name + '.txt')  # Create a filename for the article
    clean_article_text = fetch_and_clean(url)
    with open(filename, 'w', encoding='utf-8') as file:
        file.write(clean_article_text)
print(f"Content(ones that were possible) written to files in the '{output_dir}' directory.")
</code></pre>
    <p class="normal">The output<a id="_idIndexMarker188"/> shows that the goal is achieved, although <a id="_idIndexMarker189"/>some documents could not be decoded:</p>
    <pre class="programlisting con"><code class="hljs-con">WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
Content(ones that were possible) written to files in the './data/' directory.
</code></pre>
    <p class="normal">Depending on the project’s goals, you can choose to investigate and ensure that all documents are retrieved, or estimate that you have enough data for user queries.</p>
    <p class="normal">If we check <code class="inlineCode">./data/</code>, we will find that each article is now in a separate file, as shown in the content of the directory:</p>
    <figure class="mediaobject"><img src="img/B31169_03_02.png" alt="A screenshot of a computer  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 3.2: List of prepared documents</p>
    <p class="normal">The program now loads the documents from <code class="inlineCode">./data/</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"># load documents
documents = SimpleDirectoryReader("./data/").load_data()
</code></pre>
    <p class="normal">The<a id="_idIndexMarker190"/> LlamaIndex <code class="inlineCode">SimpleDirectoryReader</code> class<a id="_idIndexMarker191"/> is designed for working with unstructured data. It recursively scans the directory and identifies and loads all supported file types, such as <code class="inlineCode">.txt</code>, <code class="inlineCode">.pdf</code>, and <code class="inlineCode">.docx</code>. It then extracts the content from each file and returns a list of document objects with its text and metadata, such as the filename and file path. Let’s display the first entry of this list of dictionaries of the documents:</p>
    <pre class="programlisting code"><code class="hljs-code">documents[0]
</code></pre>
    <p class="normal">The output shows that the directory reader has provided fully transparent information on the source of its data, including the name of the document, such as <code class="inlineCode">1804.06985.txt</code> in this case:</p>
    <pre class="programlisting con"><code class="hljs-con">'/content/data/1804.06985.txt', 'file_name': '1804.06985.txt', 'file_type': 'text/plain', 'file_size': 3698, 'creation_date': '2024-05-27', 'last_modified_date': '2024-05-27'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='High Energy Physics  Theory arXiv1804.06985 hepth Submitted on 19 Apr 2018 Title A Near Horizon Extreme Binary Black Hole Geometry Authors Jacob Ciafre  Maria J. Rodriguez View a PDF of the paper titled A Near Horizon Extreme Binary Black Hole Geometry by Jacob Ciafre and Maria J. Rodriguez View PDF Abstract A new solution of fourdimensional vacuum General Relativity is presented…
</code></pre>
    <p class="normal">The<a id="_idIndexMarker192"/> content of this document contains noise <a id="_idIndexMarker193"/>that seems unrelated to the drone technology information we are looking for. But that is exactly the point of this program, which aims to do the following:</p>
    <ul>
      <li class="bulletList">Start with all the raw, unstructured, loosely drone-related data we can get our hands on</li>
      <li class="bulletList">Simulate how real-life projects often begin</li>
      <li class="bulletList">Evaluate how well an index-based RAG generative AI program can perform in a challenging environment</li>
    </ul>
    <p class="normal">Let’s now create and populate a Deep Lake vector store in complete transparency.</p>
    <h2 id="_idParaDest-82" class="heading-2">Pipeline 2: Creating and populating a Deep Lake vector store</h2>
    <p class="normal">In this section, we will <a id="_idIndexMarker194"/>create a Deep Lake vector store and populate it with the data in our documents. We will implement<a id="_idIndexMarker195"/> a standard tensor configuration with:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">text (str)</code>: The text is the content of one of the text files listed in the dictionary of documents. It will be seamless, and chunking will be optimized, breaking the text into meaningful chunks.</li>
      <li class="bulletList"><code class="inlineCode">metadata(json)</code>: In this case, the metadata will contain the filename source of each chunk of text for full transparency and control. We will see how to access this information in code.</li>
      <li class="bulletList"><code class="inlineCode">embedding (float32)</code>: The embedding is seamless, using an OpenAI embedding model called directly by the <code class="inlineCode">LlamaIndex-Deep Lake-OpenAI</code> package.</li>
      <li class="bulletList"><code class="inlineCode">id (str, auto-populated)</code>: A unique ID is attributed automatically to each chunk. The vector store will also contain an index, which is a number from <code class="inlineCode">0</code> to <code class="inlineCode">n</code>, but it cannot be used semantically, since it will change each time we modify the dataset. However, the unique ID field will remain unchanged until we decide to optimize it with index-based search strategies, as we will see in the <em class="italic">Pipeline 3: Index-based RAG</em> section that follows.</li>
    </ul>
    <p class="normal">The program first defines our vector store and dataset paths:</p>
    <pre class="programlisting code"><code class="hljs-code">from llama_index.core import StorageContext
vector_store_path = "hub://denis76/drone_v2"
dataset_path = "hub://denis76/drone_v2"
</code></pre>
    <p class="normal">Replace the vector store and dataset paths with your account name and the name of the dataset you wish to use:</p>
    <pre class="programlisting code"><code class="hljs-code">vector_store_path = "hub://[YOUR VECTOR STORE/
</code></pre>
    <p class="normal">We then create a vector store, populate it, and create an index over the documents:</p>
    <pre class="programlisting code"><code class="hljs-code"># overwrite=True will overwrite dataset, False will append it
vector_store = DeepLakeVectorStore(dataset_path=dataset_path, overwrite=True)
storage_context = StorageContext.from_defaults(vector_store=vector_store)
# Create an index over the documents
index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)
)
</code></pre>
    <p class="normal">Notice<a id="_idIndexMarker196"/> that <code class="inlineCode">overwrite</code> is set to <code class="inlineCode">True</code> to <a id="_idIndexMarker197"/>create the vector store and overwrite any existing one. If <code class="inlineCode">overwrite=False</code>, the dataset will be appended.</p>
    <p class="normal">The index created will be reorganized by the indexing methods, which will rearrange and create new indexes when necessary. However, the responses will always provide the original source of the data. The output confirms that the dataset has been created and the data is uploaded:</p>
    <pre class="programlisting con"><code class="hljs-con">Your Deep Lake dataset has been successfully created!
Uploading data to deeplake dataset.
100%|██████████| 41/41 [00:02&lt;00:00, 18.15it/s]
</code></pre>
    <p class="normal">The output also shows the structure of the dataset once it is populated:</p>
    <pre class="programlisting con"><code class="hljs-con">Dataset(path='hub://denis76/drone_v2', tensors=['text', 'metadata', 'embedding', 'id'])
</code></pre>
    <p class="normal">The data is stored in tensors with their type and shape:</p>
    <figure class="mediaobject"> <img src="img/B31169_03_03.png" alt="A screenshot of a computer  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 3.3: Dataset structure</p>
    <p class="normal">We will now<a id="_idIndexMarker198"/> load our dataset in <a id="_idIndexMarker199"/>memory:</p>
    <pre class="programlisting code"><code class="hljs-code">import deeplake
ds = deeplake.load(dataset_path)  # Load the dataset
</code></pre>
    <p class="normal">We can visualize the dataset online by clicking on the link provided in the output:</p>
    <pre class="programlisting con"><code class="hljs-con">/
This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/denis76/drone_v2
hub://denis76/drone_v2 loaded successfully.
This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/denis76/drone_v1
hub://denis76/drone_v2 loaded successfully.
</code></pre>
    <p class="normal">We can also decide to add code to display the dataset. We begin by loading the data in a pandas DataFrame:</p>
    <pre class="programlisting code"><code class="hljs-code">import json
import pandas as pd
import numpy as np
# Assuming 'ds' is your loaded Deep Lake dataset
# Create a dictionary to hold the data
data = {}
# Iterate through the tensors in the dataset
for tensor_name in ds.tensors:
    tensor_data = ds[tensor_name].numpy()
    # Check if the tensor is multi-dimensional
    if tensor_data.ndim &gt; 1:
        # Flatten multi-dimensional tensors
        data[tensor_name] = [np.array(e).flatten().tolist() for e in tensor_data]
    else:
        # Convert 1D tensors directly to lists and decode text
        if tensor_name == "text":
            data[tensor_name] = [t.tobytes().decode('utf-8') if t else "" for t in tensor_data]
        else:
            data[tensor_name] = tensor_data.tolist()
# Create a Pandas DataFrame from the dictionary
df = pd.DataFrame(data)
</code></pre>
    <p class="normal">Then, we create a function to display a record:</p>
    <pre class="programlisting code"><code class="hljs-code"># Function to display a selected record
def display_record(record_number):
    record = df.iloc[record_number]
    display_data = {
        "ID": record["id"] if "id" in record else "N/A",
        "Metadata": record["metadata"] if "metadata" in record else "N/A",
        "Text": record["text"] if "text" in record else "N/A",
        "Embedding": record["embedding"] if "embedding" in record else "N/A"
    }
</code></pre>
    <p class="normal">Finally, we<a id="_idIndexMarker200"/> can select a record and <a id="_idIndexMarker201"/>display each field:</p>
    <pre class="programlisting code"><code class="hljs-code"># Function call to display a record
rec = 0  # Replace with the desired record number
display_record(rec)
</code></pre>
    <p class="normal">The <code class="inlineCode">id</code> is a unique string code:</p>
    <pre class="programlisting con"><code class="hljs-con">ID:
['a89cdb8c-3a85-42ff-9d5f-98f93f414df6']
</code></pre>
    <p class="normal">The <code class="inlineCode">metadata</code> field <a id="_idIndexMarker202"/>contains the <a id="_idIndexMarker203"/>information we need to trace the content back to the original file and file path, as well as everything we need to understand this record, from the source to the embedded vector. It also contains the information of the node created from the record’s data, which can then be used for the indexing engine we will run in <em class="italic">Pipeline 3</em>:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">file_path</code>: Path to the file in the dataset <code class="inlineCode">(/content/data/1804.06985.txt</code>).</li>
      <li class="bulletList"><code class="inlineCode">file_name</code>: Name of the file (<code class="inlineCode">`1804.06985.txt`</code>).</li>
      <li class="bulletList"><code class="inlineCode">file_type</code>: Type of file (<code class="inlineCode">`text/plain`</code>).</li>
      <li class="bulletList"><code class="inlineCode">file_size</code>: Size of the file in bytes (<code class="inlineCode">`3700`</code>).</li>
      <li class="bulletList"><code class="inlineCode">creation_date</code>: Date the file was created (<code class="inlineCode">`2024-08-09`</code>).</li>
      <li class="bulletList"><code class="inlineCode">last_modified_date</code>: Date the file was last modified (<code class="inlineCode">`2024-08-09`</code>).</li>
      <li class="bulletList"><code class="inlineCode">_node_content</code>: Detailed content of the node, including the following main items:<ul>
          <li class="bulletList level-2"><code class="inlineCode">id_</code>: Unique identifier for the node (<code class="inlineCode">`a89cdb8c-3a85-42ff-9d5f-98f93f414df6 `</code>).</li>
          <li class="bulletList level-2"><code class="inlineCode">embedding</code>: Embedding related to the text (<code class="inlineCode">null</code>).</li>
          <li class="bulletList level-2"><code class="inlineCode">metadata</code>: Repeated metadata about the file.</li>
          <li class="bulletList level-2"><code class="inlineCode">excluded_embed_metadata_keys</code>: Keys excluded from embedding metadata (not necessary for embedding).</li>
          <li class="bulletList level-2"><code class="inlineCode">excluded_llm_metadata_keys</code>: Keys excluded from LLM metadata (not necessary for an LLM).</li>
          <li class="bulletList level-2"><code class="inlineCode">relationships</code>: Information about relationships to other nodes.</li>
          <li class="bulletList level-2"><code class="inlineCode">text</code>: Actual text content of the document. It can be the text itself, an abstract, a summary, or any other approach to optimize search functions.</li>
          <li class="bulletList level-2"><code class="inlineCode">start_char_idx</code>: Starting character index of the text.</li>
          <li class="bulletList level-2"><code class="inlineCode">end_char_idx</code>: Ending character index of the text.</li>
          <li class="bulletList level-2"><code class="inlineCode">text_template</code>: Template for displaying text with metadata.</li>
          <li class="bulletList level-2"><code class="inlineCode">metadata_template</code>: Template for displaying metadata.</li>
          <li class="bulletList level-2"><code class="inlineCode">metadata_seperator</code>: Separator used in metadata display.</li>
          <li class="bulletList level-2"><code class="inlineCode">class_name</code>: Type of node (e.g., <code class="inlineCode">`TextNode`</code>).</li>
        </ul>
      </li>
      <li class="bulletList"><code class="inlineCode">_node_type</code>: Type of node (<code class="inlineCode">`TextNode`</code>).</li>
      <li class="bulletList"><code class="inlineCode">document_id</code>: Identifier for the document (<code class="inlineCode">`61e7201d-0359-42b4-9a5f-32c4d67f345e`</code>).</li>
      <li class="bulletList"><code class="inlineCode">doc_id</code>: Document ID, same as <code class="inlineCode">document_id</code>.</li>
      <li class="bulletList"><code class="inlineCode">ref_doc_id</code>: Reference document ID, same as <code class="inlineCode">document_id</code>.</li>
    </ul>
    <p class="normal">The <code class="inlineCode">text</code> field contains the field of this chunk of data, not the whole original text:</p>
    <pre class="programlisting con"><code class="hljs-con">['High Energy Physics  Theory arXiv1804.06985 hepth Submitted on 19 Apr 2018 Title A Near Horizon Extreme Binary Black Hole Geometry Authors Jacob Ciafre  Maria J. Rodriguez View a PDF of the paper titled A Near Horizon Extreme Binary…
</code></pre>
    <p class="normal">The <code class="inlineCode">Embedding</code> field contains the embedded vector of the text content:</p>
    <pre class="programlisting con"><code class="hljs-con">[-0.0009671939187683165, 0.010151553899049759, -0.010979819111526012, -0.003061748342588544, -0.00865076668560505, 0.02144993655383587, -0.01412297785282135, -0.02674516849219799, -0.008693241514265537, -0.03383851423859596, 0.011404570192098618, 0.015956487506628036, -0.013691147789359093, 0.008856062777340412,…]
</code></pre>
    <p class="normal">The structure<a id="_idIndexMarker204"/> and format of <a id="_idIndexMarker205"/>RAG datasets vary from one domain or project to another. However, the following four columns of this dataset provide valuable information on the evolution of AI:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">id</code>: The <code class="inlineCode">id</code> is the index we will be using to organize the chunks of text of the <code class="inlineCode">text</code> column in the dataset. The chunks will be transformed into <em class="italic">nodes</em> that can contain the original text, summaries of the original text, and additional information, such as the source of the data used for the output that is stored in the metadata column. We created this index in <strong class="keyWord">Pipeline 2</strong> of this notebook when we created the vector store. However, we can generate indexes in memory on an existing database that contains no indexes, as we will see in <em class="chapterRef">Chapter 4</em>, <em class="italic">Multimodal Modular RAG for Drone Technology</em>.</li>
      <li class="bulletList"><code class="inlineCode">metadata</code>: The metadata was generated automatically in <strong class="keyWord">Pipeline 1</strong> when Deep Lake’s <code class="inlineCode">SimpleDirectoryReader</code> loaded the source documents in a documents object, and also when the vector store was created. In <em class="chapterRef">Chapter 2</em>, <em class="italic">RAG Embedding Vector Stores with Deep Lake and OpenAI</em>, we only had one file source of data. In this chapter, we stored the data in one file for each data source (URL).</li>
      <li class="bulletList"><code class="inlineCode">text</code>: The text processed by Deep Lake’s vector store creation functionality that we ran in <strong class="keyWord">Pipeline 2</strong> automatically chunked the data, without us having to configure the size of the chunks, as we did in the <em class="italic">Retrieving a batch of prepared documents</em> section in <em class="chapterRef">Chapter 2</em>. Once again, the process is seamless. We will see how smart chunking is done in the <em class="italic">Optimized chunking</em> section of <em class="italic">Pipeline 3: Index-based RAG</em> in this chapter.</li>
      <li class="bulletList"><code class="inlineCode">embedding</code>: The embedding for each chunk of data was generated through an embedding model that we do not have to configure. We could choose an embedding model, as we did in the <em class="italic">Data embedding and storage</em> section in <em class="chapterRef">Chapter 2</em>, <em class="italic">RAG Embedding Vector Stores with Deep Lake and OpenAI</em>. We selected an embedding model and wrote a function. In this program, Deep Lake selects the embedding model and embeds the data, without us having to write a single line of code.</li>
    </ul>
    <p class="normal">We can see that<a id="_idIndexMarker206"/> embedding, chunking, indexing, and other data processing functions are now encapsulated in platforms and <a id="_idIndexMarker207"/>frameworks, such as Activeloop Deep Lake, LlamaIndex, OpenAI, LangChain, Hugging Face, Chroma, and many others. Progressively, the initial excitement of generative AI models and RAG will fade, and they will become industrialized, encapsulated, and commonplace components of AI pipelines. AI is evolving, and it might be helpful to facilitate a platform that offers a default configuration based on effective practices. Then, once we have implemented a basic configuration, we can customize and expand the pipelines as necessary for our projects.</p>
    <p class="normal">We are now ready to run index-based RAG.</p>
    <h2 id="_idParaDest-83" class="heading-2">Pipeline 3: Index-based RAG</h2>
    <p class="normal">In this section, we will implement an <a id="_idIndexMarker208"/>index-based RAG pipeline using <code class="inlineCode">LlamaIndex</code>, which uses the data we have prepared and processed with Deep Lake. We will retrieve relevant information from the heterogeneous (noise-containing) drone-related document collection and synthesize the response through OpenAI’s LLM models. We will implement four index engines:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Vector Store Index Engine</strong>: Creates a <a id="_idIndexMarker209"/>vector store index from the documents, enabling efficient similarity-based searches.</li>
      <li class="bulletList"><strong class="keyWord">Tree Index</strong>: Builds a <a id="_idIndexMarker210"/>hierarchical tree index from the documents, offering an alternative retrieval structure.</li>
      <li class="bulletList"><strong class="keyWord">List Index</strong>: Constructs<a id="_idIndexMarker211"/> a straightforward list index from the documents.</li>
      <li class="bulletList"><strong class="keyWord">Keyword Table Index</strong>: Creates <a id="_idIndexMarker212"/>an index based on keywords extracted from the documents.</li>
    </ul>
    <p class="normal">We will implement <a id="_idIndexMarker213"/>querying with an LLM:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Query Response and Source</strong>: Queries the index with user input, retrieves the relevant documents, and returns a synthesized response along with source information.</li>
    </ul>
    <p class="normal">We will measure the responses with a <em class="italic">time-weighted average metric with LLM score and cosine similarity</em> that calculates a time-weighted average, based on retrieval and similarity scores. The content and execution times might vary from one run to another due to the stochastic algorithms implemented.</p>
    <h3 id="_idParaDest-84" class="heading-3">User input and query parameters</h3>
    <p class="normal">The user input <a id="_idIndexMarker214"/>will be the reference question for the four index engines we will run. We will evaluate each response based on the index engine’s retrievals and measure the outputs, using time and score ratios. The input will be submitted to the four index and query engines we will build later.</p>
    <p class="normal">The user input is:</p>
    <pre class="programlisting code"><code class="hljs-code">user_input="How do drones identify vehicles?"
</code></pre>
    <p class="normal">The four<a id="_idIndexMarker215"/> query engines that implement an LLM (in this case, an OpenAI model) will seamlessly be called with the same parameters. The three parameters that we will set are:</p>
    <pre class="programlisting code"><code class="hljs-code">#similarity_top_k
k=3
#temperature
temp=0.1
#num_output
mt=1024
</code></pre>
    <p class="normal">These key <a id="_idIndexMarker216"/>parameters are:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">k=3</code>: The query engine will be required to find the top 3 most probable responses by setting the top-k (most probable choices) to 3. In this case, k will serve as a ranking function that will force the LLM to select the top documents.</li>
      <li class="bulletList"><code class="inlineCode">temp=0.1</code>: A low temperature such as <code class="inlineCode">0.1</code> will encourage the LLM to produce precise results. If the temperature is increased to <code class="inlineCode">0.9</code>, for example, the response will be more creative. However, in this case, we are exploring drone technology, which requires precision.</li>
      <li class="bulletList"><code class="inlineCode">mt=1024</code>: This parameter will limit the number of tokens of the output to <code class="inlineCode">1,024</code>.</li>
    </ul>
    <p class="normal">The user input and parameters will be applied to the four query engines. Let’s now build the cosine similarity metric.</p>
    <h3 id="_idParaDest-85" class="heading-3">Cosine similarity metric</h3>
    <p class="normal">The cosine similarity<a id="_idIndexMarker217"/> metric was described in the <em class="italic">Evaluating the Output with the Cosine Similarity</em> section in <em class="italic">Chapter 2</em>. If necessary, take the time to go through that section again. Here, we will create a function for the responses:</p>
    <pre class="programlisting code"><code class="hljs-code">from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('all-MiniLM-L6-v2')
def calculate_cosine_similarity_with_embeddings(text1, text2):
    embeddings1 = model.encode(text1)
    embeddings2 = model.encode(text2)
    similarity = cosine_similarity([embeddings1], [embeddings2])
    return similarity[0][0]
</code></pre>
    <p class="normal">The function <a id="_idIndexMarker218"/>uses <code class="inlineCode">sklearn</code> and also Hugging Face’s <code class="inlineCode">SentenceTransformer</code>. The program first creates the vector store engine.</p>
    <h1 id="_idParaDest-86" class="heading-1">Vector store index query engine</h1>
    <p class="normal"><code class="inlineCode">VectorStoreIndex</code> is a type of index <a id="_idIndexMarker219"/>within LlamaIndex that implements vector embeddings to represent and retrieve information from documents. These documents with similar meanings will have embeddings that are closer together in the vector space, as we explored in the previous chapter. However, this time, the <code class="inlineCode">VectorStoreIndex</code> does not automatically use the existing Deep Lake vector store. It can create a new in-memory vector index, re-embed the documents, and create a new index structure. We will take this approach further in <em class="chapterRef">Chapter 4</em>, <em class="italic">Multimodal Modular RAG for Drone Technology</em>, when we implement a dataset that contains no indexes or embeddings.</p>
    <div><p class="normal">There is no silver bullet to deciding which indexing method is suitable for your project! The best way to make a choice is to test the vector, tree, list, and keyword indexes introduced in this chapter.</p>
    </div>
    <p class="normal">We will first create the vector store index:</p>
    <pre class="programlisting code"><code class="hljs-code">from llama_index.core import VectorStoreIndex
vector_store_index = VectorStoreIndex.from_documents(documents)
</code></pre>
    <p class="normal">We then display the vector store index we created:</p>
    <pre class="programlisting code"><code class="hljs-code">print(type(vector_store_index))
</code></pre>
    <p class="normal">We will receive the following output, which confirms that the engine was created:</p>
    <pre class="programlisting con"><code class="hljs-con">&lt;class 'llama_index.core.indices.vector_store.base.VectorStoreIndex'&gt;
</code></pre>
    <p class="normal">We now need a query engine to retrieve and synthesize the document(s) retrieved with an LLM—in our case, an OpenAI model (installed with <code class="inlineCode">!pip install llama-index-vector-stores-deeplake==0.1.2</code>):</p>
    <pre class="programlisting code"><code class="hljs-code">vector_query_engine = vector_store_index.as_query_engine(similarity_top_k=k, temperature=temp, num_output=mt)
</code></pre>
    <p class="normal">We defined the <a id="_idIndexMarker220"/>parameters of the query engine in the <em class="italic">User input and query parameters</em> subsection. We can now query the dataset and generate a response.</p>
    <h2 id="_idParaDest-87" class="heading-2">Query response and source</h2>
    <p class="normal">Let’s define a function that will<a id="_idIndexMarker221"/> manage the query and return information on the content of the response:</p>
    <pre class="programlisting code"><code class="hljs-code">import pandas as pd
import textwrap
def index_query(input_query):
    response = vector_query_engine.query(input_query)
    # Optional: Print a formatted view of the response (remove if you don't need it in the output)
    print(textwrap.fill(str(response), 100))
    node_data = []
    for node_with_score in response.source_nodes:
        node = node_with_score.node
        node_info = {
            'Node ID': node.id_,
            'Score': node_with_score.score,
            'Text': node.text
        }
        node_data.append(node_info)
    df = pd.DataFrame(node_data)
    # Instead of printing, return the DataFrame and the response object
    return df, response,
</code></pre>
    <p class="normal"><code class="inlineCode">index_query(input_query)</code> executes a query using a vector query engine and processes the results into a structured format. The function takes an input query and retrieves relevant information, using the query engine in a pandas DataFrame: <code class="inlineCode">Node ID</code>, <code class="inlineCode">Score</code>, <code class="inlineCode">File Path</code>, <code class="inlineCode">Filename</code>, and <code class="inlineCode">Text</code>.</p>
    <p class="normal">The code will now call the query:</p>
    <pre class="programlisting code"><code class="hljs-code">import time
#start the timer
start_time = time.time()
df, response = index_query(user_input)
# Stop the timer
end_time = time.time()
# Calculate and print the execution time
elapsed_time = end_time - start_time
print(f"Query execution time: {elapsed_time:.4f} seconds")
print(df.to_markdown(index=False, numalign="left", stralign="left"))  # Display the DataFrame using markdown
</code></pre>
    <p class="normal">We will evaluate the <a id="_idIndexMarker222"/>time it takes for the query to retrieve the relevant data and generate a response synthesis with the LLM (in this case, an OpenAI model). The output of the semantic search first returns a response synthesized by the LLM:</p>
    <pre class="programlisting con"><code class="hljs-con">Drones can automatically identify vehicles across different cameras with different viewpoints and hardware specifications using reidentification methods.
</code></pre>
    <p class="normal">The output then displays the elapsed time of the query:</p>
    <pre class="programlisting con"><code class="hljs-con">Query execution time: 0.8831 seconds
</code></pre>
    <p class="normal">The output now displays node information. The score of each node of three <code class="inlineCode">k=3</code> documents was retrieved with their text excerpts:</p>
    <figure class="mediaobject"><img src="img/B31169_03_04.png" alt="A close-up of a number  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 3.4: Node information output</p>
    <p class="normal">The ID of the <a id="_idIndexMarker223"/>node guarantees full transparency and can be traced back to the original document, even when the index engines re-index the dataset. We can obtain the node source of the first node, for example, with the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">nodeid=response.source_nodes[0].node_id
nodeid
</code></pre>
    <p class="normal">The output provides the node ID:</p>
    <pre class="programlisting con"><code class="hljs-con">4befdb13-305d-42db-a616-5d9932c17ac8
</code></pre>
    <p class="normal">We can drill down and retrieve the full text of the node containing the document that was synthesized by the LLM:</p>
    <pre class="programlisting code"><code class="hljs-code">response.source_nodes[0].get_text()
</code></pre>
    <p class="normal">The output will display the following text:</p>
    <pre class="programlisting con"><code class="hljs-con">['These activities can be carried out with different approaches that include photogrammetry SfM thermography multispectral images 3D field scanning NDVI maps etc. Agriculture forestry and environmental studies edit Main article Agricultural drone As global demand for food production grows exponentially resources are depleted farmland is…
</code></pre>
    <p class="normal">We can also peek into the nodes and retrieve their chunk size.</p>
    <h2 id="_idParaDest-88" class="heading-2">Optimized chunking</h2>
    <p class="normal">We can predefine the chunk size, or<a id="_idIndexMarker224"/> we can let LlamaIndex select it for us. In this case, the code determines the chunk size automatically:</p>
    <pre class="programlisting code"><code class="hljs-code">for node_with_score in response.source_nodes:
    node = node_with_score.node  # Extract the Node object from NodeWithScore
    chunk_size = len(node.text)
    print(f"Node ID: {node.id_}, Chunk Size: {chunk_size} characters")
</code></pre>
    <p class="normal">The advantage of an automated chunk size is that it can be variable. For example, in this case, the chunk size shown in the size of the output nodes is probably in the 4000-to-5500-character range:</p>
    <pre class="programlisting con"><code class="hljs-con">Node ID: 83a135c6-dddd-402e-9423-d282e6524160, Chunk Size: 4417 characters
Node ID: 7b7b55fe-0354-45bc-98da-0a715ceaaab0, Chunk Size: 1806 characters
Node ID: 18528a16-ce77-46a9-bbc6-5e8f05418d95, Chunk Size: 3258 characters
</code></pre>
    <p class="normal">The chunking<a id="_idIndexMarker225"/> function does not linearly cut content but optimizes the chunks for semantic search.</p>
    <h2 id="_idParaDest-89" class="heading-2">Performance metric</h2>
    <p class="normal">We will also implement a performance metric<a id="_idIndexMarker226"/> based on the accuracy of the queries and the time elapsed. This function calculates and prints a performance metric for a query, along with its execution time. The metric is based on the weighted average relevance scores of the retrieved information, divided by the time it took to get the results. Higher scores indicate better performance.</p>
    <p class="normal">We first calculate the sum of the scores and the average score, and then we divide the weighted average by the time elapsed to perform the query:</p>
    <pre class="programlisting code"><code class="hljs-code">import numpy as np
def info_metrics(response):
  # Calculate the performance (handling None scores)
  scores = [node.score for node in response.source_nodes if node.score is not None]
  if scores:  # Check if there are any valid scores
      weights = np.exp(scores) / np.sum(np.exp(scores))
      perf = np.average(scores, weights=weights) / elapsed_time
  else:
      perf = 0  # Or some other default value if all scores are None
           
</code></pre>
    <p class="normal">The result is a ratio based on the average weight divided by the elapsed time:</p>
    <pre class="programlisting code"><code class="hljs-code">perf = np.average(scores, weights=weights) / elapsed_time
</code></pre>
    <p class="normal">We can then call the function:</p>
    <pre class="programlisting code"><code class="hljs-code">info_metrics(response)
</code></pre>
    <p class="normal">The output provides an estimation of the quality of the response:</p>
    <pre class="programlisting con"><code class="hljs-con">Average score: 0.8374
Query execution time: 1.3266 seconds
Performance metric: 0.6312
</code></pre>
    <p class="normal">This performance metric is not an absolute value. It’s an indicator that we can use to compare this output with the other index engines. It may also vary from one run to another, due to the stochastic nature of machine learning algorithms. Additionally, the quality of the output depends on the user’s subjective perception. In any case, this metric will help <a id="_idIndexMarker227"/>compare the query engines’ performances in this chapter.</p>
    <p class="normal">We can already see that the average score is satisfactory, even though we loaded heterogeneous and sometimes unrelated documents in the dataset. The integrated retriever and synthesizer functionality of LlamaIndex, Deep Lake, and OpenAI have proven to be highly effective.</p>
    <h1 id="_idParaDest-90" class="heading-1">Tree index query engine</h1>
    <p class="normal">The tree index in LlamaIndex creates <a id="_idIndexMarker228"/>a hierarchical structure for managing and querying text documents efficiently. However, think of something other than a classical hierarchical structure! The tree index engine optimizes the hierarchy, content, and order of the nodes, as shown in <em class="italic">Figure 3.5</em>:</p>
    <p class="packt_figref"><img src="img/B31169_03_05.png" alt="A diagram of a tree index  Description automatically generated"/></p>
    <p class="packt_figref">Figure 3.5: Optimized tree index</p>
    <p class="normal">The tree<a id="_idIndexMarker229"/> index organizes documents in a tree structure, with broader summaries at higher levels and detailed information at lower levels. Each node in the tree summarizes the text it covers. The tree index is efficient for large datasets and queries large collections of documents rapidly by breaking them down into manageable optimized chunks. Thus, the optimization of the tree structure allows for rapid retrieval by traversing the relevant nodes without wasting time.</p>
    <p class="normal">Organizing this part of the pipeline and adjusting parameters such as tree depth and summary methods can be a specialized task for a team member. Depending on the project and workload, working on the tree structure could be part of <strong class="keyWord">Pipeline 2</strong> when creating and populating a vector store. Alternatively, the tree structure can be created in memory at the beginning of each session. The flexibility of the structure and implementation of tree structures and index engines, in general, can be a fascinating and valuable specialization in a RAG-driven generative AI team.</p>
    <p class="normal">In this index model, the LLM (an OpenAI model in this case) acts like it is answering a multiple-choice question when selecting the best nodes during a query. It analyzes the query, compares it with the summaries of the current node’s children, and decides which path to follow to find the most relevant information.</p>
    <p class="normal">The integrated LlamaIndex-Deep Lake-OpenAI process in this chapter is industrializing components seamlessly, taking AI to another level. LLM models can now be used for embedding, document ranking, and conversational agents. The market offers various language models from providers like OpenAI, Cohere, AI21 Labs, and Hugging Face. LLMs have evolved from the early days of being perceived as magic to becoming industrialized, seamless, multifunctional, and integrated components of broader AI pipelines.</p>
    <p class="normal">Let’s create a <a id="_idIndexMarker230"/>tree index in two lines of code:</p>
    <pre class="programlisting code"><code class="hljs-code">from llama_index.core import TreeIndex
tree_index = TreeIndex.from_documents(documents)
</code></pre>
    <p class="normal">The code then checks the class we just created:</p>
    <pre class="programlisting code"><code class="hljs-code">print(type(tree_index))
</code></pre>
    <p class="normal">The output confirms that we are in the <code class="inlineCode">TreeIndex</code> class:</p>
    <pre class="programlisting con"><code class="hljs-con">&lt;class 'llama_index.core.indices.tree.base.TreeIndex'&gt;
</code></pre>
    <p class="normal">We can now make our tree index the query engine:</p>
    <pre class="programlisting code"><code class="hljs-code">tree_query_engine = tree_index.as_query_engine(similarity_top_k=k, temperature=temp, num_output=mt)
</code></pre>
    <p class="normal">The parameters of the LLM are those defined in the <em class="italic">User input and query parameters</em> section. The code now calls the query, measures the time elapsed, and processes the response:</p>
    <pre class="programlisting code"><code class="hljs-code">import time
import textwrap
# Start the timer
start_time = time.time()
response = tree_query_engine.query(user_input)
# Stop the timer
end_time = time.time()
# Calculate and print the execution time
elapsed_time = end_time - start_time
print(f"Query execution time: {elapsed_time:.4f} seconds")
print(textwrap.fill(str(response), 100))
</code></pre>
    <p class="normal">The query <a id="_idIndexMarker231"/>time and the response are both satisfactory:</p>
    <pre class="programlisting con"><code class="hljs-con">Query execution time: 4.3360 seconds
Drones identify vehicles using computer vision technology related to object detection. This
technology involves detecting instances of semantic objects of a certain class, such as vehicles, in
digital images and videos. Drones can be equipped with object detection algorithms, such as YOLOv3
models trained on datasets like COCO, to detect vehicles in real-time by analyzing the visual data
captured by the drone's cameras.
</code></pre>
    <p class="normal">Let’s apply a performance metric to the output.</p>
    <h2 id="_idParaDest-91" class="heading-2">Performance metric</h2>
    <p class="normal">This performance metric<a id="_idIndexMarker232"/> will calculate the cosine similarity defined in the <em class="italic">Cosine similarity metric</em> section between the user input and the response of our RAG pipeline:</p>
    <pre class="programlisting code"><code class="hljs-code">similarity_score = calculate_cosine_similarity_with_embeddings(user_input, str(response))
print(f"Cosine Similarity Score: {similarity_score:.3f}")
print(f"Query execution time: {elapsed_time:.4f} seconds")
performance=similarity_score/elapsed_time
print(f"Performance metric: {performance:.4f}")
</code></pre>
    <p class="normal">The output shows that although the quality of the response was satisfactory, the execution time was slow, which brings the performance metric down:</p>
    <pre class="programlisting con"><code class="hljs-con">Cosine Similarity Score: 0.731
Query execution time: 4.3360 seconds
Performance metric: 0.1686
</code></pre>
    <p class="normal">Of course, the execution time depends on the server (power) and the data (noise). As established earlier, the execution times might vary from one run to another, due to the stochastic algorithms used. Also, when the dataset increases in volume, the execution times of all the indexing types may change.</p>
    <p class="normal">The list index query engine may or may not be better in this case. Let’s run it to find out.</p>
    <h1 id="_idParaDest-92" class="heading-1">List index query engine</h1>
    <p class="normal">Don’t think of<a id="_idIndexMarker233"/> <code class="inlineCode">ListIndex</code> as simply a list of nodes. The query engine will process the user input and each document as a prompt for an LLM. The LLM will evaluate the semantic similarity relationship between the documents and the query, thus implicitly ranking and selecting the most relevant nodes. LlamaIndex will filter the documents based on the rankings obtained, and it can also take the task further by synthesizing information from multiple nodes and documents.</p>
    <p class="normal">We can see that the selection process with an LLM is not rule-based. Nothing is predefined, which means that the selection is prompt-based by combining the user input with a collection of documents. The LLM evaluates each document in the list <em class="italic">independently</em>, assigning a score based on its perceived relevance to the query. This score isn’t relative to other documents; it’s a measure of how well the LLM thinks the current document answers the question. Then, the top-k documents are retained by the query engine if we wish, as in the function used in this section.</p>
    <p class="normal">Like the tree index, the list index can also be created in two lines of code:</p>
    <pre class="programlisting code"><code class="hljs-code">from llama_index.core import ListIndex
list_index = ListIndex.from_documents(documents)
</code></pre>
    <p class="normal">The code verifies the class that we are using:</p>
    <pre class="programlisting code"><code class="hljs-code">print(type(list_index))
</code></pre>
    <p class="normal">The output confirms that we are in the <code class="inlineCode">list</code> class:</p>
    <pre class="programlisting con"><code class="hljs-con">&lt;class 'llama_index.core.indices.list.base.SummaryIndex'&gt;
</code></pre>
    <p class="normal">The list index is a <code class="inlineCode">SummaryIndex</code>, which shows the large amount of document summary optimization that is running under the hood! We can now utilize our list index as a query engine in the seamless framework provided by LlamaIndex:</p>
    <pre class="programlisting code"><code class="hljs-code">list_query_engine = list_index.as_query_engine(similarity_top_k=k, temperature=temp, num_output=mt)
</code></pre>
    <p class="normal">The LLM parameters remain unchanged so that we can compare the indexing types. We can now run our query, wrap the response up, and display the output:</p>
    <pre class="programlisting code"><code class="hljs-code">#start the timer
start_time = time.time()
response = list_query_engine.query(user_input)
# Stop the timer
end_time = time.time()
# Calculate and print the execution time
elapsed_time = end_time - start_time
print(f"Query execution time: {elapsed_time:.4f} seconds")
print(textwrap.fill(str(response), 100))
</code></pre>
    <p class="normal">The output shows a longer execution time but an acceptable response:</p>
    <pre class="programlisting con"><code class="hljs-con">Query execution time: 16.3123 seconds
Drones can identify vehicles through computer vision systems that process image data captured by
cameras mounted on the drones. These systems use techniques like object recognition and detection to
analyze the images and identify specific objects, such as vehicles, based on predefined models or
features. By processing the visual data in real-time, drones can effectively identify vehicles in
their surroundings.
</code></pre>
    <p class="normal">The execution <a id="_idIndexMarker234"/>time is longer because the query goes through a list, not an optimized tree. However, we cannot draw conclusions from this because each project or even each sub-task of a project has different requirements. Next, let’s apply the performance metric.</p>
    <h2 id="_idParaDest-93" class="heading-2">Performance metric</h2>
    <p class="normal">We will use the <a id="_idIndexMarker235"/>cosine similarity, as we did for the tree index, to evaluate the similarity score:</p>
    <pre class="programlisting code"><code class="hljs-code">similarity_score = calculate_cosine_similarity_with_embeddings(user_input, str(response))
print(f"Cosine Similarity Score: {similarity_score:.3f}")
print(f"Query execution time: {elapsed_time:.4f} seconds")
performance=similarity_score/elapsed_time
print(f"Performance metric: {performance:.4f}")
</code></pre>
    <p class="normal">The performance metric is lower than the tree index due to the longer execution time:</p>
    <pre class="programlisting con"><code class="hljs-con">Cosine Similarity Score: 0.775
Query execution time: 16.3123 seconds
Performance metric: 0.0475
</code></pre>
    <p class="normal">Again, remember that this execution time may vary from one run to another, due to the stochastic algorithms implemented.</p>
    <p class="normal">If we look back at the <a id="_idIndexMarker236"/>performance metric of each indexing type, we can see that, for the moment, the vector store index was the fastest. Once again, let’s not jump to conclusions. Each project might produce surprising results, depending on the type and complexity of the data processed. Next, let’s examine the keyword index.</p>
    <h1 id="_idParaDest-94" class="heading-1">Keyword index query engine</h1>
    <p class="normal"><code class="inlineCode">KeywordTableIndex</code> is a <a id="_idIndexMarker237"/>type of index in LlamaIndex, designed to extract keywords from your documents and organize them in a table-like structure. This structure makes it easier to query and retrieve relevant information based on specific keywords or topics. Once again, don’t think about this function as a simple list of extracted keywords. The extracted keywords are organized into a table-like format where each keyword is associated with an ID that points to the related nodes.</p>
    <p class="normal">The program creates the keyword index in two lines of code:</p>
    <pre class="programlisting code"><code class="hljs-code">from llama_index.core import KeywordTableIndex
keyword_index = KeywordTableIndex.from_documents(documents)
</code></pre>
    <p class="normal">Let’s extract the data and create a pandas DataFrame to see how the index is structured:</p>
    <pre class="programlisting code"><code class="hljs-code"># Extract data for DataFrame
data = []
for keyword, doc_ids in keyword_index.index_struct.table.items():
    for doc_id in doc_ids:
        data.append({"Keyword": keyword, "Document ID": doc_id})
# Create the DataFrame
df = pd.DataFrame(data)
df
</code></pre>
    <p class="normal">The output shows that each keyword is associated with an ID that contains a document or a summary, depending on the way LlamaIndex optimizes the index:</p>
    <figure class="mediaobject"><img src="img/B31169_03_06.png" alt="A screenshot of a computer  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 3.6: Keywords linked to document IDs in a DataFrame</p>
    <p class="normal">We now define the keyword index as the query engine:</p>
    <pre class="programlisting code"><code class="hljs-code">keyword_query_engine = keyword_index.as_query_engine(similarity_top_k=k, temperature=temp, num_output=mt)
</code></pre>
    <p class="normal">Let’s run<a id="_idIndexMarker238"/> the keyword query and see how well and fast it can produce a response:</p>
    <pre class="programlisting code"><code class="hljs-code">import time
# Start the timer
start_time = time.time()
# Execute the query (using .query() method)
response = keyword_query_engine.query(user_input)
# Stop the timer
end_time = time.time()
# Calculate and print the execution time
elapsed_time = end_time - start_time
print(f"Query execution time: {elapsed_time:.4f} seconds")
print(textwrap.fill(str(response), 100))
</code></pre>
    <p class="normal">The output is satisfactory, as well as the execution time:</p>
    <pre class="programlisting con"><code class="hljs-con">Query execution time: 2.4282 seconds
Drones can identify vehicles through various means such as visual recognition using onboard cameras, sensors, and image processing algorithms. They can also utilize technologies like artificial intelligence and machine learning to analyze and classify vehicles based on their shapes, sizes, and movement patterns. Additionally, drones can be equipped with specialized software for object detection and tracking to identify vehicles accurately.
</code></pre>
    <p class="normal">We can now<a id="_idIndexMarker239"/> measure the output with a performance metric.</p>
    <h2 id="_idParaDest-95" class="heading-2">Performance metric</h2>
    <p class="normal">The code runs the same metric as for the<a id="_idIndexMarker240"/> tree and list index:</p>
    <pre class="programlisting code"><code class="hljs-code">similarity_score = calculate_cosine_similarity_with_embeddings(user_input, str(response))
print(f"Cosine Similarity Score: {similarity_score:.3f}")
print(f"Query execution time: {elapsed_time:.4f} seconds")
performance=similarity_score/elapsed_time
print(f"Performance metric: {performance:.4f}")
</code></pre>
    <p class="normal">The performance metric is acceptable:</p>
    <pre class="programlisting con"><code class="hljs-con">Cosine Similarity Score: 0.801
Query execution time: 2.4282 seconds
Performance metric: 0.3299
</code></pre>
    <p class="normal">Once again, we can draw no conclusions. The results of all the indexing types are relatively satisfactory. However, each project comes with its dataset complexity and machine power availability. Also, the execution times may vary from one run to another, due to the stochastic algorithms employed.</p>
    <p class="normal">With that, we have reviewed some of the main indexing types and retrieval strategies. Let’s summarize the chapter and move on to multimodal modular retrieval and generation strategies.</p>
    <h1 id="_idParaDest-96" class="heading-1">Summary</h1>
    <p class="normal">This chapter explored the transformative impact of index-based search on RAG and introduced a pivotal advancement: <em class="italic">full traceability</em>. The documents become nodes that contain chunks of data, with the source of a query leading us all the way back to the original data. Indexes also increase the speed of retrievals, which is critical as the volume of datasets increases. Another pivotal advance is the integration of technologies such as LlamaIndex, Deep Lake, and OpenAI, which are emerging in another era of AI. The most advanced AI models, such as OpenAI GPT-4o, Hugging Face, and Cohere, are becoming seamless <em class="italic">components</em> in a RAG-driven generative AI pipeline, like GPUs in a computer. </p>
    <p class="normal">We started by detailing the architecture of an index-based RAG generative AI pipeline, illustrating how these sophisticated technologies can be seamlessly integrated to boost the creation of advanced indexing and retrieval systems. The complexity of AI implementation is changing the way we organize separate pipelines and functionality for a team working in parallel on projects that scale and involve large amounts of data. We saw how every response generated can be traced back to its source, providing clear visibility into the origins and accuracy of the information used. We illustrated the advanced RAG technology implemented through drone technology.</p>
    <p class="normal">Throughout the chapter, we introduced the essential tools to build these systems, including vector stores, datasets, chunking, embedding, node creation, ranking, and indexing methods. We implemented the LlamaIndex framework, Deep Lake vector stores, and OpenAI’s models. We also built a Python program that collects data and adds critical metadata to pinpoint the origin of every chunk of data in a dataset. We highlighted the pivotal role of indexes (vector, tree, list, and keyword types) in giving us greater control over generative AI applications, enabling precise adjustments and improvements. </p>
    <p class="normal">We then thoroughly examined indexed-based RAG through detailed walkthroughs in Python notebooks, guiding you through setting up vector stores, conducting advanced queries, and ensuring the traceability of AI-generated responses. We introduced metrics based on the quality of a response and the time elapsed to obtain it. Exploring drone technology with LLMs showed us the new skillsets required to build solid AI pipelines, and we learned how drone technology involves computer vision and, thus, multimodal nodes.</p>
    <p class="normal">In the upcoming chapter, we include multimodal data in our datasets and expand multimodular RAG.</p>
    <h1 id="_idParaDest-97" class="heading-1">Questions</h1>
    <p class="normal">Answer the following questions with <em class="italic">Yes</em> or <em class="italic">No</em>:</p>
    <ul>
      <li class="bulletList">Do indexes increase precision and speed in retrieval-augmented generative AI? </li>
      <li class="bulletList">Can indexes offer traceability for RAG outputs? </li>
      <li class="bulletList">Is index-based search slower than vector-based search for large datasets? </li>
      <li class="bulletList">Does LlamaIndex integrate seamlessly with Deep Lake and OpenAI? </li>
      <li class="bulletList">Are tree, list, vector, and keyword indexes the only types of indexes? </li>
      <li class="bulletList">Does the keyword index rely on semantic understanding to retrieve data? </li>
      <li class="bulletList">Is LlamaIndex capable of automatically handling chunking and embedding? </li>
      <li class="bulletList">Are metadata enhancements crucial for ensuring the traceability of RAG-generated outputs? </li>
      <li class="bulletList">Can real-time updates easily be applied to an index-based search system? </li>
      <li class="bulletList">Is cosine similarity a metric used in this chapter to evaluate query accuracy?</li>
    </ul>
    <h1 id="_idParaDest-98" class="heading-1">References</h1>
    <ul>
      <li class="bulletList">LlamaIndex: <a href="https://docs.llamaindex.ai/en/stable/">https://docs.llamaindex.ai/en/stable/</a></li>
      <li class="bulletList">Activeloop Deep Lake: <a href="https://docs.activeloop.ai/">https://docs.activeloop.ai/</a></li>
      <li class="bulletList">OpenAI: <a href="https://platform.openai.com/docs/overview">https://platform.openai.com/docs/overview</a></li>
    </ul>
    <h1 id="_idParaDest-99" class="heading-1">Further reading</h1>
    <ul>
      <li class="bulletList">High-Level Concepts (RAG), LlamaIndex: <a href="https://docs.llamaindex.ai/en/stable/getting_started/concepts/">https://docs.llamaindex.ai/en/stable/getting_started/concepts/</a></li>
    </ul>
    <h1 id="_idParaDest-100" class="heading-1">Join our community on Discord</h1>
    <p class="normal">Join our community’s Discord space for discussions with the author and other readers:</p>
    <p class="normal"><a href="https://www.packt.link/rag">https://www.packt.link/rag</a></p>
    <p class="normal"><img src="img/QR_Code50409000288080484.png" alt=""/></p>
  </div>
</body></html>