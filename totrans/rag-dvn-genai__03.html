<html><head></head><body>
  <div id="_idContainer042" class="Basic-Text-Frame">
    <h1 class="chapterNumber"><span class="koboSpan" id="kobo.1.1">3</span></h1>
    <h1 id="_idParaDest-76" class="chapterTitle"><span class="koboSpan" id="kobo.2.1">Building Index-Based RAG with LlamaIndex, Deep Lake, and OpenAI</span></h1>
    <p class="normal"><span class="koboSpan" id="kobo.3.1">Indexes increase precision and speed performances, but they offer more than that. </span><span class="koboSpan" id="kobo.3.2">Indexes transform retrieval-augmented generative AI by adding a layer of transparency. </span><span class="koboSpan" id="kobo.3.3">With an index, the source of a response generated by a RAG model is fully traceable, offering visibility into the precise location and detailed content of the data used. </span><span class="koboSpan" id="kobo.3.4">This improvement not only mitigates issues like bias and hallucinations but also addresses concerns around copyright and data integrity.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.4.1">In this chapter, we’ll explore how indexed data allows for greater control over generative AI applications. </span><span class="koboSpan" id="kobo.4.2">If the output is unsatisfactory, it’s no longer a mystery why, since the index allows us to identify and examine the exact data source of the issue. </span><span class="koboSpan" id="kobo.4.3">This capability makes it possible to refine data inputs, tweak system configurations, or switch components, such as vector store software and generative models, to achieve better outcomes.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.5.1">We will begin the chapter by laying out the architecture of an index-based RAG pipeline that will enhance speed, precision, and traceability. </span><span class="koboSpan" id="kobo.5.2">We will show how LlamaIndex, Deep Lake, and OpenAI can be seamlessly integrated without having to create all the necessary functions ourselves. </span><span class="koboSpan" id="kobo.5.3">This provides a solid base to start building from. </span><span class="koboSpan" id="kobo.5.4">Then, we’ll introduce the main indexing types we’ll use in our programs, such as vector, tree, list, and keyword indexes. </span><span class="koboSpan" id="kobo.5.5">Then, we will build a domain-specific drone technology LLM RAG agent that a user can interact with. </span><span class="koboSpan" id="kobo.5.6">Drone technology is expanding to all domains, such as fire detection, traffic information, and sports events; hence, I’ve decided to use it in our example. </span><span class="koboSpan" id="kobo.5.7">The goal of this chapter is to prepare an LLM drone technology dataset that we will enhance with multimodal data in the next chapter. </span><span class="koboSpan" id="kobo.5.8">We will also illustrate the key indexing types in code.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.6.1">By the end of this chapter, you’ll be adept at manipulating index-based RAG through vector stores, datasets, and LLMs, and know how to optimize retrieval systems and ensure full traceability. </span><span class="koboSpan" id="kobo.6.2">You will discover how our integrated toolkit—combining LlamaIndex, Deep Lake, and OpenAI—not only simplifies technical complexities but also frees your time to develop and hone your analytical skills, enabling you to dive deeper into understanding RAG-driven generative AI.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.7.1">We’ll cover the following topics in this chapter:</span></p>
    <ul>
      <li class="bulletList"><span class="koboSpan" id="kobo.8.1">Building a semantic search engine with a LlamaIndex framework and indexing methods</span></li>
      <li class="bulletList"><span class="koboSpan" id="kobo.9.1">Populating Deep Lake vector stores</span></li>
      <li class="bulletList"><span class="koboSpan" id="kobo.10.1">Integration of LlamaIndex, Deep Lake, and OpenAI</span></li>
      <li class="bulletList"><span class="koboSpan" id="kobo.11.1">Score ranking and cosine similarity metrics</span></li>
      <li class="bulletList"><span class="koboSpan" id="kobo.12.1">Metadata enhancement for traceability</span></li>
      <li class="bulletList"><span class="koboSpan" id="kobo.13.1">Query setup and generation configuration</span></li>
      <li class="bulletList"><span class="koboSpan" id="kobo.14.1">Introducing automated document ranking</span></li>
      <li class="bulletList"><span class="koboSpan" id="kobo.15.1">Vector, tree, list, and keyword indexing types</span></li>
    </ul>
    <h1 id="_idParaDest-77" class="heading-1"><span class="koboSpan" id="kobo.16.1">Why use index-based RAG?</span></h1>
    <p class="normal"><span class="koboSpan" id="kobo.17.1">Index-based search</span><a id="_idIndexMarker173"/><span class="koboSpan" id="kobo.18.1"> takes advanced RAG-driven generative AI to another level. </span><span class="koboSpan" id="kobo.18.2">It increases the speed of retrieval when faced with large volumes of data, taking us from raw chunks of data to organized, indexed nodes that we can trace from the output back to the source of a document and its location.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.19.1">Let’s understand the differences </span><a id="_idIndexMarker174"/><span class="koboSpan" id="kobo.20.1">between a vector-based similarity search and an index-based search by analyzing the architecture of an index-based RAG.</span></p>
    <h2 id="_idParaDest-78" class="heading-2"><span class="koboSpan" id="kobo.21.1">Architecture</span></h2>
    <p class="normal"><span class="koboSpan" id="kobo.22.1">Index-based search is faster than</span><a id="_idIndexMarker175"/><span class="koboSpan" id="kobo.23.1"> vector-based search in RAG because it directly accesses relevant data using indices, while vector-based search sequentially compares embeddings across all records. </span><span class="koboSpan" id="kobo.23.2">We implemented a vector-based similarity search program in </span><em class="chapterRef"><span class="koboSpan" id="kobo.24.1">Chapter 2</span></em><span class="koboSpan" id="kobo.25.1">, </span><em class="italic"><span class="koboSpan" id="kobo.26.1">RAG Embedding Vector Stores with Deep Lake and OpenAI</span></em><span class="koboSpan" id="kobo.27.1">, as shown in </span><em class="italic"><span class="koboSpan" id="kobo.28.1">Figure 3.1</span></em><span class="koboSpan" id="kobo.29.1">:</span></p>
    <ul>
      <li class="bulletList"><span class="koboSpan" id="kobo.30.1">We collected and prepared data in </span><em class="italic"><span class="koboSpan" id="kobo.31.1">Pipeline #1: Data Collection and Preparation</span></em></li>
      <li class="bulletList"><span class="koboSpan" id="kobo.32.1">We embedded the data and stored the prepared data in a vector store in </span><em class="italic"><span class="koboSpan" id="kobo.33.1">Pipeline #2: Embeddings and vector store</span></em></li>
      <li class="bulletList"><span class="koboSpan" id="kobo.34.1">We then ran retrieval queries and generative AI with </span><em class="italic"><span class="koboSpan" id="kobo.35.1">Pipeline #3</span></em><span class="koboSpan" id="kobo.36.1"> to process user input, run retrievals based on vector similarity searches, augment the input, generate a response, and apply performance metrics.</span></li>
    </ul>
    <p class="normal"><span class="koboSpan" id="kobo.37.1">This approach </span><a id="_idIndexMarker176"/><span class="koboSpan" id="kobo.38.1">is flexible because it gives you many ways to implement each component, depending on the needs of your project.</span></p>
    <figure class="mediaobject"><span class="koboSpan" id="kobo.39.1"><img src="../Images/B31169_03_01.png" alt="A diagram of a process  Description automatically generated"/></span></figure>
    <p class="packt_figref"><span class="koboSpan" id="kobo.40.1">Figure 3.1: RAG-driven generative AI pipelines, as described in Chapter 2, with additional functionality</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.41.1">However, implementing index-based searches will take us into the future of AI, which will be faster, more precise, and traceable. </span><span class="koboSpan" id="kobo.41.2">We will follow the same process as in </span><em class="italic"><span class="koboSpan" id="kobo.42.1">Chapter 2</span></em><span class="koboSpan" id="kobo.43.1">, with three pipelines, to make sure that you are ready to work in a team in which the tasks are specialized. </span><span class="koboSpan" id="kobo.43.2">Since we are using the same pipelines as in </span><em class="italic"><span class="koboSpan" id="kobo.44.1">Chapter 2</span></em><span class="koboSpan" id="kobo.45.1">, let’s add the functions from that chapter to them, as shown in </span><em class="italic"><span class="koboSpan" id="kobo.46.1">Figure 3.1</span></em><span class="koboSpan" id="kobo.47.1">:</span></p>
    <ul>
      <li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.48.1">Pipeline Component #1 and D2-Index</span></strong><span class="koboSpan" id="kobo.49.1">: We will collect data and preprocess it. </span><span class="koboSpan" id="kobo.49.2">However, this </span><a id="_idIndexMarker177"/><span class="koboSpan" id="kobo.50.1">time, we will prepare the data source one document at a time and store them in separate files. </span><span class="koboSpan" id="kobo.50.2">We will then add their name and location to the metadata we load into the vector store. </span><span class="koboSpan" id="kobo.50.3">The metadata will help us trace a response all the way back to the exact file that the retrieval function processed. </span><span class="koboSpan" id="kobo.50.4">We will have a direct link from a response to the data that it was based on.</span></li>
      <li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.51.1">Pipeline Component #2 and D3-Index</span></strong><span class="koboSpan" id="kobo.52.1">: We will load the data into a vector store by installing and using the innovative integrated </span><code class="inlineCode"><span class="koboSpan" id="kobo.53.1">llama-index-vector-stores-deeplake</span></code><span class="koboSpan" id="kobo.54.1"> package, which includes everything we need in an optimized starter scenario: chunking, embedding, storage, and even LLM integration. </span><span class="koboSpan" id="kobo.54.2">We have everything we need to get to work on index-based RAG in a few lines of code! </span><span class="koboSpan" id="kobo.54.3">This way, once we have a solid program, we can customize and expand the pipelines as we wish, as we did, for example, in </span><em class="italic"><span class="koboSpan" id="kobo.55.1">Chapter 2</span></em><span class="koboSpan" id="kobo.56.1">, when we explicitly chose the LLM models and chunking sizes.</span></li>
      <li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.57.1">Pipeline Component #3 and D4-Index</span></strong><span class="koboSpan" id="kobo.58.1">: We will load the data in a dataset by installing and using the innovative integrated </span><code class="inlineCode"><span class="koboSpan" id="kobo.59.1">llama-index-vector-stores-deeplake</span></code><span class="koboSpan" id="kobo.60.1"> package, which includes everything we need to get indexed-based retrieval and generation started, including automated ranking and scoring. </span><span class="koboSpan" id="kobo.60.2">The process is seamless and extremely productive. </span><span class="koboSpan" id="kobo.60.3">We’ll leverage LlamaIndex with Deep Lake to streamline information retrieval and processing. </span><span class="koboSpan" id="kobo.60.4">An integrated retriever will efficiently fetch relevant data from the Deep Lake repository, while an LLM agent will then intelligently synthesize and interact with the retrieved information to generate meaningful insights or actions. </span><span class="koboSpan" id="kobo.60.5">Indexes are designed for fast retrieval, and we will implement several indexing methods.</span></li>
      <li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.61.1">Pipeline Component #3 and E1-Index</span></strong><span class="koboSpan" id="kobo.62.1">: We will add a time and score metric to evaluate the</span><a id="_idIndexMarker178"/><span class="koboSpan" id="kobo.63.1"> output.</span></li>
    </ul>
    <p class="normal"><span class="koboSpan" id="kobo.64.1">In the previous chapter, we implemented vector-based similarity search and retrieval. </span><span class="koboSpan" id="kobo.64.2">We embedded documents to transform data into high-dimensional vectors. </span><span class="koboSpan" id="kobo.64.3">Then, we performed retrieval by calculating distances between vectors. </span><span class="koboSpan" id="kobo.64.4">In this chapter, we will go further and create a vector store. </span><span class="koboSpan" id="kobo.64.5">However, we will load the data into a dataset that will be reorganized using retrieval indexing types. </span><em class="italic"><span class="koboSpan" id="kobo.65.1">Table 3.1</span></em><span class="koboSpan" id="kobo.66.1"> shows the differences between vector-based and index-based search and retrieval methods:</span></p>
    <table id="table001-1" class="table-container">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.67.1">Feature</span></strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.68.1">Vector-based similarity search and retrieval</span></strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.69.1">Index-based vector, tree, list, and keyword search and retrieval</span></strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><span class="koboSpan" id="kobo.70.1">Flexibility</span></p>
          </td>
          <td class="table-cell">
            <p class="normal"><span class="koboSpan" id="kobo.71.1">High</span></p>
          </td>
          <td class="table-cell">
            <p class="normal"><span class="koboSpan" id="kobo.72.1">Medium (precomputed structure)</span></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><span class="koboSpan" id="kobo.73.1">Speed</span></p>
          </td>
          <td class="table-cell">
            <p class="normal"><span class="koboSpan" id="kobo.74.1">Slower</span><a id="_idIndexMarker179"/><span class="koboSpan" id="kobo.75.1"> with large datasets</span></p>
          </td>
          <td class="table-cell">
            <p class="normal"><span class="koboSpan" id="kobo.76.1">Fast and optimized for quick retrieval</span></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><span class="koboSpan" id="kobo.77.1">Scalability</span></p>
          </td>
          <td class="table-cell">
            <p class="normal"><span class="koboSpan" id="kobo.78.1">Limited by</span><a id="_idIndexMarker180"/><span class="koboSpan" id="kobo.79.1"> real-time processing</span></p>
          </td>
          <td class="table-cell">
            <p class="normal"><span class="koboSpan" id="kobo.80.1">Highly scalable with large datasets</span></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><span class="koboSpan" id="kobo.81.1">Complexity</span></p>
          </td>
          <td class="table-cell">
            <p class="normal"><span class="koboSpan" id="kobo.82.1">Simpler setup</span></p>
          </td>
          <td class="table-cell">
            <p class="normal"><span class="koboSpan" id="kobo.83.1">More complex and requires an indexing step</span></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><span class="koboSpan" id="kobo.84.1">Update Frequency</span></p>
          </td>
          <td class="table-cell">
            <p class="normal"><span class="koboSpan" id="kobo.85.1">Easy to update</span></p>
          </td>
          <td class="table-cell">
            <p class="normal"><span class="koboSpan" id="kobo.86.1">Requires re-indexing for updates</span></p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="packt_figref"><span class="koboSpan" id="kobo.87.1">Table 3.1: Vector-based and index-based characteristics</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.88.1">We will now build a semantic index-based RAG program with Deep Lake, LlamaIndex, and OpenAI.</span></p>
    <h1 id="_idParaDest-79" class="heading-1"><span class="koboSpan" id="kobo.89.1">Building a semantic search engine and generative agent for drone technology</span></h1>
    <p class="normal"><span class="koboSpan" id="kobo.90.1">In this section, we will build a</span><a id="_idIndexMarker181"/><span class="koboSpan" id="kobo.91.1"> semantic index-based search engine and generative AI agent engine using Deep Lake vector stores, LlamaIndex, and OpenAI. </span><span class="koboSpan" id="kobo.91.2">As mentioned earlier, drone technology is expanding in domains such as fire detection and traffic control. </span><span class="koboSpan" id="kobo.91.3">As such, the program’s goal is to provide an index-based RAG agent for drone technology questions and answers. </span><span class="koboSpan" id="kobo.91.4">The program will demonstrate how drones use computer vision techniques to identify vehicles and other objects. </span><span class="koboSpan" id="kobo.91.5">We will implement the architecture illustrated in </span><em class="italic"><span class="koboSpan" id="kobo.92.1">Figure 3.1</span></em><span class="koboSpan" id="kobo.93.1">, described in the </span><em class="italic"><span class="koboSpan" id="kobo.94.1">Architecture</span></em><span class="koboSpan" id="kobo.95.1"> section of this chapter.</span></p>
    <div class="note">
      <p class="normal"><span class="koboSpan" id="kobo.96.1">Open </span><code class="inlineCode"><span class="koboSpan" id="kobo.97.1">2-Deep_Lake_LlamaIndex_OpenAI_indexing.ipynb</span></code><span class="koboSpan" id="kobo.98.1"> from the GitHub repository of this chapter. </span><span class="koboSpan" id="kobo.98.2">The titles of this section are the same as the section titles in the notebook, so you can match the explanations with the code.</span></p>
    </div>
    <p class="normal"><span class="koboSpan" id="kobo.99.1">We will first begin by installing the environment. </span><span class="koboSpan" id="kobo.99.2">Then, we will build the three main pipelines of the program:</span></p>
    <ul>
      <li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.100.1">Pipeline 1</span></strong><span class="koboSpan" id="kobo.101.1">: Collecting and preparing the documents. </span><span class="koboSpan" id="kobo.101.2">Using sources like GitHub and Wikipedia, collect and clean documents for indexing.</span></li>
      <li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.102.1">Pipeline 2</span></strong><span class="koboSpan" id="kobo.103.1">: Creating and populating a Deep Lake vector store. </span><span class="koboSpan" id="kobo.103.2">Create and populate a Deep Lake vector store with the prepared documents.</span></li>
      <li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.104.1">Pipeline 3</span></strong><span class="koboSpan" id="kobo.105.1">: Index-based RAG for query processing and generation. </span><span class="koboSpan" id="kobo.105.2">Applying time and score performances with LLMs and cosine similarity metrics.</span></li>
    </ul>
    <p class="normal"><span class="koboSpan" id="kobo.106.1">When possible, break</span><a id="_idIndexMarker182"/><span class="koboSpan" id="kobo.107.1"> your project down into separate pipelines so that teams can progress independently and in parallel. </span><span class="koboSpan" id="kobo.107.2">The pipelines in this chapter are an example of how this can be done, but there are many other ways to do this, depending on your project. </span><span class="koboSpan" id="kobo.107.3">For now, we will begin by installing the environment.</span></p>
    <h2 id="_idParaDest-80" class="heading-2"><span class="koboSpan" id="kobo.108.1">Installing the environment</span></h2>
    <p class="normal"><span class="koboSpan" id="kobo.109.1">The environment is mostly the </span><a id="_idIndexMarker183"/><span class="koboSpan" id="kobo.110.1">same as in the previous chapter. </span><span class="koboSpan" id="kobo.110.2">Let’s focus on the packages that integrate LlamaIndex, vector store capabilities for Deep Lake, and also OpenAI modules. </span><span class="koboSpan" id="kobo.110.3">This integration is a major step forward to seamless cross-platform implementations:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.111.1">!pip install llama-index-vector-stores-deeplake==</span><span class="hljs-number"><span class="koboSpan" id="kobo.112.1">0.1.6</span></span>
</code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.113.1">The program requires additional Deep Lake functionalities:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.114.1">!pip install deeplake==</span><span class="hljs-number"><span class="koboSpan" id="kobo.115.1">3.9.8</span></span>
</code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.116.1">The program also requires LlamaIndex functionalities:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.117.1">!pip install llama-index==</span><span class="hljs-number"><span class="koboSpan" id="kobo.118.1">0.10.64</span></span>
</code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.119.1">Let’s now check if the packages can be properly imported from </span><code class="inlineCode"><span class="koboSpan" id="kobo.120.1">llama-index</span></code><span class="koboSpan" id="kobo.121.1">, including vector stores for Deep Lake:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.122.1">from</span></span><span class="koboSpan" id="kobo.123.1"> llama_index.core </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.124.1">import</span></span><span class="koboSpan" id="kobo.125.1"> VectorStoreIndex, SimpleDirectoryReader, Document
</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.126.1">from</span></span><span class="koboSpan" id="kobo.127.1"> llama_index.vector_stores.deeplake </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.128.1">import</span></span><span class="koboSpan" id="kobo.129.1"> DeepLakeVectorStore
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.130.1">With that, we have installed the environment. </span><span class="koboSpan" id="kobo.130.2">We will now collect and prepare the documents.</span></p>
    <h2 id="_idParaDest-81" class="heading-2"><span class="koboSpan" id="kobo.131.1">Pipeline 1: Collecting and preparing the documents</span></h2>
    <p class="normal"><span class="koboSpan" id="kobo.132.1">In this section, we will </span><a id="_idIndexMarker184"/><span class="koboSpan" id="kobo.133.1">collect and prepare the drone-related </span><a id="_idIndexMarker185"/><span class="koboSpan" id="kobo.134.1">documents with the metadata necessary to trace the documents back to their source. </span><span class="koboSpan" id="kobo.134.2">The goal is to trace a response’s content back to the exact chunk of data retrieved to find its source. </span><span class="koboSpan" id="kobo.134.3">First, we will create a data directory in which we will load the documents:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.135.1">!mkdir data
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.136.1">Now, we will use a heterogeneous corpus for the drone technology data that we will process using </span><code class="inlineCode"><span class="koboSpan" id="kobo.137.1">BeautifulSoup</span></code><span class="koboSpan" id="kobo.138.1">:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.139.1">import</span></span><span class="koboSpan" id="kobo.140.1"> requests
</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.141.1">from</span></span><span class="koboSpan" id="kobo.142.1"> bs4 </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.143.1">import</span></span><span class="koboSpan" id="kobo.144.1"> BeautifulSoup
</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.145.1">import</span></span><span class="koboSpan" id="kobo.146.1"> re
</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.147.1">import</span></span><span class="koboSpan" id="kobo.148.1"> os
urls = [
    </span><span class="hljs-string"><span class="koboSpan" id="kobo.149.1">"https://github.com/VisDrone/VisDrone-Dataset"</span></span><span class="koboSpan" id="kobo.150.1">,
    </span><span class="hljs-string"><span class="koboSpan" id="kobo.151.1">"https://paperswithcode.com/dataset/visdrone"</span></span><span class="koboSpan" id="kobo.152.1">,
    </span><span class="hljs-string"><span class="koboSpan" id="kobo.153.1">"https://openaccess.thecvf.com/content_ECCVW_2018/papers/11133/Zhu_VisDrone-DET2018_The_Vision_Meets_Drone_Object_Detection_in_Image_Challenge_ECCVW_2018_paper.pdf"</span></span><span class="koboSpan" id="kobo.154.1">,
    </span><span class="hljs-string"><span class="koboSpan" id="kobo.155.1">"https://github.com/VisDrone/VisDrone2018-MOT-toolkit"</span></span><span class="koboSpan" id="kobo.156.1">,
    </span><span class="hljs-string"><span class="koboSpan" id="kobo.157.1">"https://en.wikipedia.org/wiki/Object_detection"</span></span><span class="koboSpan" id="kobo.158.1">,
    </span><span class="hljs-string"><span class="koboSpan" id="kobo.159.1">"https://en.wikipedia.org/wiki/Computer_vision"</span></span><span class="koboSpan" id="kobo.160.1">,…
]
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.161.1">The corpus contains a list of sites related to drones, computer vision, and related technologies. </span><span class="koboSpan" id="kobo.161.2">However, the list also contains noisy links such as </span><a href="https://keras.io/"><span class="url"><span class="koboSpan" id="kobo.162.1">https://keras.io/</span></span></a><span class="koboSpan" id="kobo.163.1"> and </span><a href="https://pytorch.org/"><span class="url"><span class="koboSpan" id="kobo.164.1">https://pytorch.org/</span></span></a><span class="koboSpan" id="kobo.165.1">, which do </span><em class="italic"><span class="koboSpan" id="kobo.166.1">not</span></em><span class="koboSpan" id="kobo.167.1"> contain the specific information we are looking for.</span></p>
    <div class="note">
      <p class="normal"><span class="koboSpan" id="kobo.168.1">In real-life projects, we will not always have the luxury of working on perfect, pertinent, structured, and well-formatted data. </span><span class="koboSpan" id="kobo.168.2">Our RAG pipelines must be sufficiently robust to retrieve relevant data in a noisy environment.</span></p>
    </div>
    <p class="normal"><span class="koboSpan" id="kobo.169.1">In this case, we </span><a id="_idIndexMarker186"/><span class="koboSpan" id="kobo.170.1">are working with</span><a id="_idIndexMarker187"/><span class="koboSpan" id="kobo.171.1"> unstructured data in various formats and variable quality as related to drone technology. </span><span class="koboSpan" id="kobo.171.2">Of course, in a closed environment, we can work with the persons or organizations that produce the documents, but we must be ready for any type of document in a fast-moving, digital world.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.172.1">The code will fetch and clean the data, as it did in </span><em class="italic"><span class="koboSpan" id="kobo.173.1">Chapter 2</span></em><span class="koboSpan" id="kobo.174.1">:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.175.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.176.1">clean_text</span></span><span class="koboSpan" id="kobo.177.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.178.1">content</span></span><span class="koboSpan" id="kobo.179.1">):
    </span><span class="hljs-comment"><span class="koboSpan" id="kobo.180.1"># Remove references and unwanted characters</span></span><span class="koboSpan" id="kobo.181.1">
    content = re.sub(</span><span class="hljs-string"><span class="koboSpan" id="kobo.182.1">r'\[\d+\]'</span></span><span class="koboSpan" id="kobo.183.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.184.1">''</span></span><span class="koboSpan" id="kobo.185.1">, content)   </span><span class="hljs-comment"><span class="koboSpan" id="kobo.186.1"># Remove references</span></span><span class="koboSpan" id="kobo.187.1">
    content = re.sub(</span><span class="hljs-string"><span class="koboSpan" id="kobo.188.1">r'[^\w\s\.]'</span></span><span class="koboSpan" id="kobo.189.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.190.1">''</span></span><span class="koboSpan" id="kobo.191.1">, content)  </span><span class="hljs-comment"><span class="koboSpan" id="kobo.192.1"># Remove punctuation (except periods)</span></span>
    <span class="hljs-keyword"><span class="koboSpan" id="kobo.193.1">return</span></span><span class="koboSpan" id="kobo.194.1"> content
</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.195.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.196.1">fetch_and_clean</span></span><span class="koboSpan" id="kobo.197.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.198.1">url</span></span><span class="koboSpan" id="kobo.199.1">):
    </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.200.1">try</span></span><span class="koboSpan" id="kobo.201.1">:
        response = requests.get(url)
        response.raise_for_status()  </span><span class="hljs-comment"><span class="koboSpan" id="kobo.202.1"># Raise exception for bad responses (e.g., 404)</span></span><span class="koboSpan" id="kobo.203.1">
        soup = BeautifulSoup(response.content, </span><span class="hljs-string"><span class="koboSpan" id="kobo.204.1">'html.parser'</span></span><span class="koboSpan" id="kobo.205.1">)
        </span><span class="hljs-comment"><span class="koboSpan" id="kobo.206.1"># Prioritize "mw-parser-output" but fall back to "content" class if not found</span></span><span class="koboSpan" id="kobo.207.1">
        content = soup.find(</span><span class="hljs-string"><span class="koboSpan" id="kobo.208.1">'div'</span></span><span class="koboSpan" id="kobo.209.1">, {</span><span class="hljs-string"><span class="koboSpan" id="kobo.210.1">'class'</span></span><span class="koboSpan" id="kobo.211.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.212.1">'</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.213.1">mw-parser-output'</span></span><span class="koboSpan" id="kobo.214.1">}) </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.215.1">or</span></span><span class="koboSpan" id="kobo.216.1"> soup.find(</span><span class="hljs-string"><span class="koboSpan" id="kobo.217.1">'div'</span></span><span class="koboSpan" id="kobo.218.1">, {</span><span class="hljs-string"><span class="koboSpan" id="kobo.219.1">'id'</span></span><span class="koboSpan" id="kobo.220.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.221.1">'content'</span></span><span class="koboSpan" id="kobo.222.1">})
        </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.223.1">if</span></span><span class="koboSpan" id="kobo.224.1"> content </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.225.1">is</span></span> <span class="hljs-literal"><span class="koboSpan" id="kobo.226.1">None</span></span><span class="koboSpan" id="kobo.227.1">:
            </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.228.1">return</span></span> <span class="hljs-literal"><span class="koboSpan" id="kobo.229.1">None</span></span>
        <span class="hljs-comment"><span class="koboSpan" id="kobo.230.1"># Remove specific sections, including nested ones</span></span>
        <span class="hljs-keyword"><span class="koboSpan" id="kobo.231.1">for</span></span><span class="koboSpan" id="kobo.232.1"> section_title </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.233.1">in</span></span><span class="koboSpan" id="kobo.234.1"> [</span><span class="hljs-string"><span class="koboSpan" id="kobo.235.1">'References'</span></span><span class="koboSpan" id="kobo.236.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.237.1">'Bibliography'</span></span><span class="koboSpan" id="kobo.238.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.239.1">'External links'</span></span><span class="koboSpan" id="kobo.240.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.241.1">'See also'</span></span><span class="koboSpan" id="kobo.242.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.243.1">'Notes'</span></span><span class="koboSpan" id="kobo.244.1">]:
            section = content.find(</span><span class="hljs-string"><span class="koboSpan" id="kobo.245.1">'span'</span></span><span class="koboSpan" id="kobo.246.1">, </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.247.1">id</span></span><span class="koboSpan" id="kobo.248.1">=section_title)
            </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.249.1">while</span></span><span class="koboSpan" id="kobo.250.1"> section:
                </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.251.1">for</span></span><span class="koboSpan" id="kobo.252.1"> sib </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.253.1">in</span></span><span class="koboSpan" id="kobo.254.1"> section.parent.find_next_siblings():
                    sib.decompose()
                section.parent.decompose()
                section = content.find(</span><span class="hljs-string"><span class="koboSpan" id="kobo.255.1">'span'</span></span><span class="koboSpan" id="kobo.256.1">, </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.257.1">id</span></span><span class="koboSpan" id="kobo.258.1">=section_title)
        </span><span class="hljs-comment"><span class="koboSpan" id="kobo.259.1"># Extract and clean text</span></span><span class="koboSpan" id="kobo.260.1">
        text = content.get_text(separator=</span><span class="hljs-string"><span class="koboSpan" id="kobo.261.1">' '</span></span><span class="koboSpan" id="kobo.262.1">, strip=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.263.1">True</span></span><span class="koboSpan" id="kobo.264.1">)
        text = clean_text(text)
        </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.265.1">return</span></span><span class="koboSpan" id="kobo.266.1"> text
    </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.267.1">except</span></span><span class="koboSpan" id="kobo.268.1"> requests.exceptions.RequestException </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.269.1">as</span></span><span class="koboSpan" id="kobo.270.1"> e:
        </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.271.1">print</span></span><span class="koboSpan" id="kobo.272.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.273.1">f"Error fetching content from </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.274.1">{url}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.275.1">: </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.276.1">{e}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.277.1">"</span></span><span class="koboSpan" id="kobo.278.1">)
        </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.279.1">return</span></span> <span class="hljs-literal"><span class="koboSpan" id="kobo.280.1">None</span></span>  <span class="hljs-comment"><span class="koboSpan" id="kobo.281.1"># Return None on error</span></span>
</code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.282.1">Each project will require specific names and paths for the original data. </span><span class="koboSpan" id="kobo.282.2">In this case, we will introduce an additional function to save each piece of text with the name of its data source, by creating a keyword based on its URL:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.283.1"># Directory to store the output files</span></span><span class="koboSpan" id="kobo.284.1">
output_dir = </span><span class="hljs-string"><span class="koboSpan" id="kobo.285.1">'./data/'</span></span><span class="koboSpan" id="kobo.286.1">
os.makedirs(output_dir, exist_ok=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.287.1">True</span></span><span class="koboSpan" id="kobo.288.1">)
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.289.1"># Processing each URL and writing its content to a separate file</span></span>
<span class="hljs-keyword"><span class="koboSpan" id="kobo.290.1">for</span></span><span class="koboSpan" id="kobo.291.1"> url </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.292.1">in</span></span><span class="koboSpan" id="kobo.293.1"> urls:
    article_name = url.split(</span><span class="hljs-string"><span class="koboSpan" id="kobo.294.1">'/'</span></span><span class="koboSpan" id="kobo.295.1">)[-</span><span class="hljs-number"><span class="koboSpan" id="kobo.296.1">1</span></span><span class="koboSpan" id="kobo.297.1">].replace('.html',")  </span><span class="hljs-comment"><span class="koboSpan" id="kobo.298.1"># Handle .html extension</span></span><span class="koboSpan" id="kobo.299.1">
    filename = os.path.join(output_dir, article_name + </span><span class="hljs-string"><span class="koboSpan" id="kobo.300.1">'.txt'</span></span><span class="koboSpan" id="kobo.301.1">)  </span><span class="hljs-comment"><span class="koboSpan" id="kobo.302.1"># Create a filename for the article</span></span><span class="koboSpan" id="kobo.303.1">
    clean_article_text = fetch_and_clean(url)
    </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.304.1">with</span></span> <span class="hljs-built_in"><span class="koboSpan" id="kobo.305.1">open</span></span><span class="koboSpan" id="kobo.306.1">(filename, </span><span class="hljs-string"><span class="koboSpan" id="kobo.307.1">'w'</span></span><span class="koboSpan" id="kobo.308.1">, encoding=</span><span class="hljs-string"><span class="koboSpan" id="kobo.309.1">'utf-8'</span></span><span class="koboSpan" id="kobo.310.1">) </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.311.1">as</span></span><span class="koboSpan" id="kobo.312.1"> file:
        file.write(clean_article_text)
</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.313.1">print</span></span><span class="koboSpan" id="kobo.314.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.315.1">f"Content(ones that were possible) written to files in the '</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.316.1">{output_dir}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.317.1">' directory."</span></span><span class="koboSpan" id="kobo.318.1">)
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.319.1">The output</span><a id="_idIndexMarker188"/><span class="koboSpan" id="kobo.320.1"> shows that the goal is achieved, although </span><a id="_idIndexMarker189"/><span class="koboSpan" id="kobo.321.1">some documents could not be decoded:</span></p>
    <pre class="programlisting con"><code class="hljs-con"><span class="koboSpan" id="kobo.322.1">WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.
</span><span class="koboSpan" id="kobo.322.2">Content(ones that were possible) written to files in the './data/' directory.
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.323.1">Depending on the project’s goals, you can choose to investigate and ensure that all documents are retrieved, or estimate that you have enough data for user queries.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.324.1">If we check </span><code class="inlineCode"><span class="koboSpan" id="kobo.325.1">./data/</span></code><span class="koboSpan" id="kobo.326.1">, we will find that each article is now in a separate file, as shown in the content of the directory:</span></p>
    <figure class="mediaobject"><span class="koboSpan" id="kobo.327.1"><img src="../Images/B31169_03_02.png" alt="A screenshot of a computer  Description automatically generated"/></span></figure>
    <p class="packt_figref"><span class="koboSpan" id="kobo.328.1">Figure 3.2: List of prepared documents</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.329.1">The program now loads the documents from </span><code class="inlineCode"><span class="koboSpan" id="kobo.330.1">./data/</span></code><span class="koboSpan" id="kobo.331.1">:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.332.1"># load documents</span></span><span class="koboSpan" id="kobo.333.1">
documents = SimpleDirectoryReader(</span><span class="hljs-string"><span class="koboSpan" id="kobo.334.1">"./data/"</span></span><span class="koboSpan" id="kobo.335.1">).load_data()
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.336.1">The</span><a id="_idIndexMarker190"/><span class="koboSpan" id="kobo.337.1"> LlamaIndex </span><code class="inlineCode"><span class="koboSpan" id="kobo.338.1">SimpleDirectoryReader</span></code><span class="koboSpan" id="kobo.339.1"> class</span><a id="_idIndexMarker191"/><span class="koboSpan" id="kobo.340.1"> is designed for working with unstructured data. </span><span class="koboSpan" id="kobo.340.2">It recursively scans the directory and identifies and loads all supported file types, such as </span><code class="inlineCode"><span class="koboSpan" id="kobo.341.1">.txt</span></code><span class="koboSpan" id="kobo.342.1">, </span><code class="inlineCode"><span class="koboSpan" id="kobo.343.1">.pdf</span></code><span class="koboSpan" id="kobo.344.1">, and </span><code class="inlineCode"><span class="koboSpan" id="kobo.345.1">.docx</span></code><span class="koboSpan" id="kobo.346.1">. </span><span class="koboSpan" id="kobo.346.2">It then extracts the content from each file and returns a list of document objects with its text and metadata, such as the filename and file path. </span><span class="koboSpan" id="kobo.346.3">Let’s display the first entry of this list of dictionaries of the documents:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.347.1">documents[</span><span class="hljs-number"><span class="koboSpan" id="kobo.348.1">0</span></span><span class="koboSpan" id="kobo.349.1">]
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.350.1">The output shows that the directory reader has provided fully transparent information on the source of its data, including the name of the document, such as </span><code class="inlineCode"><span class="koboSpan" id="kobo.351.1">1804.06985.txt</span></code><span class="koboSpan" id="kobo.352.1"> in this case:</span></p>
    <pre class="programlisting con"><code class="hljs-con"><span class="koboSpan" id="kobo.353.1">'/content/data/1804.06985.txt', 'file_name': '1804.06985.txt', 'file_type': 'text/plain', 'file_size': 3698, 'creation_date': '2024-05-27', 'last_modified_date': '2024-05-27'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='High Energy Physics  Theory arXiv1804.06985 hepth Submitted on 19 Apr 2018 Title A Near Horizon Extreme Binary Black Hole Geometry Authors Jacob Ciafre  Maria J. </span><span class="koboSpan" id="kobo.353.2">Rodriguez View a PDF of the paper titled A Near Horizon Extreme Binary Black Hole Geometry by Jacob Ciafre and Maria J. </span><span class="koboSpan" id="kobo.353.3">Rodriguez View PDF Abstract A new solution of fourdimensional vacuum General Relativity is presented…
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.354.1">The</span><a id="_idIndexMarker192"/><span class="koboSpan" id="kobo.355.1"> content of this document contains noise </span><a id="_idIndexMarker193"/><span class="koboSpan" id="kobo.356.1">that seems unrelated to the drone technology information we are looking for. </span><span class="koboSpan" id="kobo.356.2">But that is exactly the point of this program, which aims to do the following:</span></p>
    <ul>
      <li class="bulletList"><span class="koboSpan" id="kobo.357.1">Start with all the raw, unstructured, loosely drone-related data we can get our hands on</span></li>
      <li class="bulletList"><span class="koboSpan" id="kobo.358.1">Simulate how real-life projects often begin</span></li>
      <li class="bulletList"><span class="koboSpan" id="kobo.359.1">Evaluate how well an index-based RAG generative AI program can perform in a challenging environment</span></li>
    </ul>
    <p class="normal"><span class="koboSpan" id="kobo.360.1">Let’s now create and populate a Deep Lake vector store in complete transparency.</span></p>
    <h2 id="_idParaDest-82" class="heading-2"><span class="koboSpan" id="kobo.361.1">Pipeline 2: Creating and populating a Deep Lake vector store</span></h2>
    <p class="normal"><span class="koboSpan" id="kobo.362.1">In this section, we will </span><a id="_idIndexMarker194"/><span class="koboSpan" id="kobo.363.1">create a Deep Lake vector store and populate it with the data in our documents. </span><span class="koboSpan" id="kobo.363.2">We will implement</span><a id="_idIndexMarker195"/><span class="koboSpan" id="kobo.364.1"> a standard tensor configuration with:</span></p>
    <ul>
      <li class="bulletList"><code class="inlineCode"><span class="koboSpan" id="kobo.365.1">text (str)</span></code><span class="koboSpan" id="kobo.366.1">: The text is the content of one of the text files listed in the dictionary of documents. </span><span class="koboSpan" id="kobo.366.2">It will be seamless, and chunking will be optimized, breaking the text into meaningful chunks.</span></li>
      <li class="bulletList"><code class="inlineCode"><span class="koboSpan" id="kobo.367.1">metadata(json)</span></code><span class="koboSpan" id="kobo.368.1">: In this case, the metadata will contain the filename source of each chunk of text for full transparency and control. </span><span class="koboSpan" id="kobo.368.2">We will see how to access this information in code.</span></li>
      <li class="bulletList"><code class="inlineCode"><span class="koboSpan" id="kobo.369.1">embedding (float32)</span></code><span class="koboSpan" id="kobo.370.1">: The embedding is seamless, using an OpenAI embedding model called directly by the </span><code class="inlineCode"><span class="koboSpan" id="kobo.371.1">LlamaIndex-Deep Lake-OpenAI</span></code><span class="koboSpan" id="kobo.372.1"> package.</span></li>
      <li class="bulletList"><code class="inlineCode"><span class="koboSpan" id="kobo.373.1">id (str, auto-populated)</span></code><span class="koboSpan" id="kobo.374.1">: A unique ID is attributed automatically to each chunk. </span><span class="koboSpan" id="kobo.374.2">The vector store will also contain an index, which is a number from </span><code class="inlineCode"><span class="koboSpan" id="kobo.375.1">0</span></code><span class="koboSpan" id="kobo.376.1"> to </span><code class="inlineCode"><span class="koboSpan" id="kobo.377.1">n</span></code><span class="koboSpan" id="kobo.378.1">, but it cannot be used semantically, since it will change each time we modify the dataset. </span><span class="koboSpan" id="kobo.378.2">However, the unique ID field will remain unchanged until we decide to optimize it with index-based search strategies, as we will see in the </span><em class="italic"><span class="koboSpan" id="kobo.379.1">Pipeline 3: Index-based RAG</span></em><span class="koboSpan" id="kobo.380.1"> section that follows.</span></li>
    </ul>
    <p class="normal"><span class="koboSpan" id="kobo.381.1">The program first defines our vector store and dataset paths:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.382.1">from</span></span><span class="koboSpan" id="kobo.383.1"> llama_index.core </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.384.1">import</span></span><span class="koboSpan" id="kobo.385.1"> StorageContext
vector_store_path = </span><span class="hljs-string"><span class="koboSpan" id="kobo.386.1">"hub://denis76/drone_v2"</span></span><span class="koboSpan" id="kobo.387.1">
dataset_path = </span><span class="hljs-string"><span class="koboSpan" id="kobo.388.1">"hub://denis76/drone_v2"</span></span>
</code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.389.1">Replace the vector store and dataset paths with your account name and the name of the dataset you wish to use:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.390.1">vector_store_path = </span><span class="hljs-string"><span class="koboSpan" id="kobo.391.1">"hub://[YOUR VECTOR STORE/</span></span>
</code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.392.1">We then create a vector store, populate it, and create an index over the documents:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.393.1"># overwrite=True will overwrite dataset, False will append it</span></span><span class="koboSpan" id="kobo.394.1">
vector_store = DeepLakeVectorStore(dataset_path=dataset_path, overwrite=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.395.1">True</span></span><span class="koboSpan" id="kobo.396.1">)
storage_context = StorageContext.from_defaults(vector_store=vector_store)
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.397.1"># Create an index over the documents</span></span><span class="koboSpan" id="kobo.398.1">
index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)
)
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.399.1">Notice</span><a id="_idIndexMarker196"/><span class="koboSpan" id="kobo.400.1"> that </span><code class="inlineCode"><span class="koboSpan" id="kobo.401.1">overwrite</span></code><span class="koboSpan" id="kobo.402.1"> is set to </span><code class="inlineCode"><span class="koboSpan" id="kobo.403.1">True</span></code><span class="koboSpan" id="kobo.404.1"> to </span><a id="_idIndexMarker197"/><span class="koboSpan" id="kobo.405.1">create the vector store and overwrite any existing one. </span><span class="koboSpan" id="kobo.405.2">If </span><code class="inlineCode"><span class="koboSpan" id="kobo.406.1">overwrite=False</span></code><span class="koboSpan" id="kobo.407.1">, the dataset will be appended.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.408.1">The index created will be reorganized by the indexing methods, which will rearrange and create new indexes when necessary. </span><span class="koboSpan" id="kobo.408.2">However, the responses will always provide the original source of the data. </span><span class="koboSpan" id="kobo.408.3">The output confirms that the dataset has been created and the data is uploaded:</span></p>
    <pre class="programlisting con"><code class="hljs-con"><span class="koboSpan" id="kobo.409.1">Your Deep Lake dataset has been successfully created!
</span><span class="koboSpan" id="kobo.409.2">Uploading data to deeplake dataset.
</span><span class="koboSpan" id="kobo.409.3">100%|██████████| 41/41 [00:02&lt;00:00, 18.15it/s]
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.410.1">The output also shows the structure of the dataset once it is populated:</span></p>
    <pre class="programlisting con"><code class="hljs-con"><span class="koboSpan" id="kobo.411.1">Dataset(path='hub://denis76/drone_v2', tensors=['text', 'metadata', 'embedding', 'id'])
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.412.1">The data is stored in tensors with their type and shape:</span></p>
    <figure class="mediaobject"> <span class="koboSpan" id="kobo.413.1"><img src="../Images/B31169_03_03.png" alt="A screenshot of a computer  Description automatically generated"/></span></figure>
    <p class="packt_figref"><span class="koboSpan" id="kobo.414.1">Figure 3.3: Dataset structure</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.415.1">We will now</span><a id="_idIndexMarker198"/><span class="koboSpan" id="kobo.416.1"> load our dataset in </span><a id="_idIndexMarker199"/><span class="koboSpan" id="kobo.417.1">memory:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.418.1">import</span></span><span class="koboSpan" id="kobo.419.1"> deeplake
ds = deeplake.load(dataset_path)  </span><span class="hljs-comment"><span class="koboSpan" id="kobo.420.1"># Load the dataset</span></span>
</code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.421.1">We can visualize the dataset online by clicking on the link provided in the output:</span></p>
    <pre class="programlisting con"><code class="hljs-con"><span class="koboSpan" id="kobo.422.1">/
This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/denis76/drone_v2
hub://denis76/drone_v2 loaded successfully.
</span><span class="koboSpan" id="kobo.422.2">This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/denis76/drone_v1
hub://denis76/drone_v2 loaded successfully.
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.423.1">We can also decide to add code to display the dataset. </span><span class="koboSpan" id="kobo.423.2">We begin by loading the data in a pandas DataFrame:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.424.1">import</span></span><span class="koboSpan" id="kobo.425.1"> json
</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.426.1">import</span></span><span class="koboSpan" id="kobo.427.1"> pandas </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.428.1">as</span></span><span class="koboSpan" id="kobo.429.1"> pd
</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.430.1">import</span></span><span class="koboSpan" id="kobo.431.1"> numpy </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.432.1">as</span></span><span class="koboSpan" id="kobo.433.1"> np
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.434.1"># Assuming 'ds' is your loaded Deep Lake dataset</span></span>
<span class="hljs-comment"><span class="koboSpan" id="kobo.435.1"># Create a dictionary to hold the data</span></span><span class="koboSpan" id="kobo.436.1">
data = {}
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.437.1"># Iterate through the tensors in the dataset</span></span>
<span class="hljs-keyword"><span class="koboSpan" id="kobo.438.1">for</span></span><span class="koboSpan" id="kobo.439.1"> tensor_name </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.440.1">in</span></span><span class="koboSpan" id="kobo.441.1"> ds.tensors:
    tensor_data = ds[tensor_name].numpy()
    </span><span class="hljs-comment"><span class="koboSpan" id="kobo.442.1"># Check if the tensor is multi-dimensional</span></span>
    <span class="hljs-keyword"><span class="koboSpan" id="kobo.443.1">if</span></span><span class="koboSpan" id="kobo.444.1"> tensor_data.ndim &gt; </span><span class="hljs-number"><span class="koboSpan" id="kobo.445.1">1</span></span><span class="koboSpan" id="kobo.446.1">:
        </span><span class="hljs-comment"><span class="koboSpan" id="kobo.447.1"># Flatten multi-dimensional tensors</span></span><span class="koboSpan" id="kobo.448.1">
        data[tensor_name] = [np.array(e).flatten().tolist() </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.449.1">for</span></span><span class="koboSpan" id="kobo.450.1"> e </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.451.1">in</span></span><span class="koboSpan" id="kobo.452.1"> tensor_data]
    </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.453.1">else</span></span><span class="koboSpan" id="kobo.454.1">:
        </span><span class="hljs-comment"><span class="koboSpan" id="kobo.455.1"># Convert 1D tensors directly to lists and decode text</span></span>
        <span class="hljs-keyword"><span class="koboSpan" id="kobo.456.1">if</span></span><span class="koboSpan" id="kobo.457.1"> tensor_name == </span><span class="hljs-string"><span class="koboSpan" id="kobo.458.1">"text"</span></span><span class="koboSpan" id="kobo.459.1">:
            data[tensor_name] = [t.tobytes().decode(</span><span class="hljs-string"><span class="koboSpan" id="kobo.460.1">'utf-8'</span></span><span class="koboSpan" id="kobo.461.1">) </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.462.1">if</span></span><span class="koboSpan" id="kobo.463.1"> t </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.464.1">else</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.465.1">""</span></span> <span class="hljs-keyword"><span class="koboSpan" id="kobo.466.1">for</span></span><span class="koboSpan" id="kobo.467.1"> t </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.468.1">in</span></span><span class="koboSpan" id="kobo.469.1"> tensor_data]
        </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.470.1">else</span></span><span class="koboSpan" id="kobo.471.1">:
            data[tensor_name] = tensor_data.tolist()
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.472.1"># Create a Pandas DataFrame from the dictionary</span></span><span class="koboSpan" id="kobo.473.1">
df = pd.DataFrame(data)
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.474.1">Then, we create a function to display a record:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.475.1"># Function to display a selected record</span></span>
<span class="hljs-keyword"><span class="koboSpan" id="kobo.476.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.477.1">display_record</span></span><span class="koboSpan" id="kobo.478.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.479.1">record_number</span></span><span class="koboSpan" id="kobo.480.1">):
    record = df.iloc[record_number]
    display_data = {
        </span><span class="hljs-string"><span class="koboSpan" id="kobo.481.1">"</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.482.1">ID"</span></span><span class="koboSpan" id="kobo.483.1">: record[</span><span class="hljs-string"><span class="koboSpan" id="kobo.484.1">"id"</span></span><span class="koboSpan" id="kobo.485.1">] </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.486.1">if</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.487.1">"id"</span></span> <span class="hljs-keyword"><span class="koboSpan" id="kobo.488.1">in</span></span><span class="koboSpan" id="kobo.489.1"> record </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.490.1">else</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.491.1">"N/A"</span></span><span class="koboSpan" id="kobo.492.1">,
        </span><span class="hljs-string"><span class="koboSpan" id="kobo.493.1">"Metadata"</span></span><span class="koboSpan" id="kobo.494.1">: record[</span><span class="hljs-string"><span class="koboSpan" id="kobo.495.1">"metadata"</span></span><span class="koboSpan" id="kobo.496.1">] </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.497.1">if</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.498.1">"</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.499.1">metadata"</span></span> <span class="hljs-keyword"><span class="koboSpan" id="kobo.500.1">in</span></span><span class="koboSpan" id="kobo.501.1"> record </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.502.1">else</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.503.1">"N/A"</span></span><span class="koboSpan" id="kobo.504.1">,
        </span><span class="hljs-string"><span class="koboSpan" id="kobo.505.1">"Text"</span></span><span class="koboSpan" id="kobo.506.1">: record[</span><span class="hljs-string"><span class="koboSpan" id="kobo.507.1">"text"</span></span><span class="koboSpan" id="kobo.508.1">] </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.509.1">if</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.510.1">"text"</span></span> <span class="hljs-keyword"><span class="koboSpan" id="kobo.511.1">in</span></span><span class="koboSpan" id="kobo.512.1"> record </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.513.1">else</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.514.1">"</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.515.1">N/A"</span></span><span class="koboSpan" id="kobo.516.1">,
        </span><span class="hljs-string"><span class="koboSpan" id="kobo.517.1">"Embedding"</span></span><span class="koboSpan" id="kobo.518.1">: record[</span><span class="hljs-string"><span class="koboSpan" id="kobo.519.1">"embedding"</span></span><span class="koboSpan" id="kobo.520.1">] </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.521.1">if</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.522.1">"embedding"</span></span> <span class="hljs-keyword"><span class="koboSpan" id="kobo.523.1">in</span></span><span class="koboSpan" id="kobo.524.1"> record </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.525.1">else</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.526.1">"N/A"</span></span><span class="koboSpan" id="kobo.527.1">
    }
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.528.1">Finally, we</span><a id="_idIndexMarker200"/><span class="koboSpan" id="kobo.529.1"> can select a record and </span><a id="_idIndexMarker201"/><span class="koboSpan" id="kobo.530.1">display each field:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.531.1"># Function call to display a record</span></span><span class="koboSpan" id="kobo.532.1">
rec = </span><span class="hljs-number"><span class="koboSpan" id="kobo.533.1">0</span></span>  <span class="hljs-comment"><span class="koboSpan" id="kobo.534.1"># Replace with the desired record number</span></span><span class="koboSpan" id="kobo.535.1">
display_record(rec)
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.536.1">The </span><code class="inlineCode"><span class="koboSpan" id="kobo.537.1">id</span></code><span class="koboSpan" id="kobo.538.1"> is a unique string code:</span></p>
    <pre class="programlisting con"><code class="hljs-con"><span class="koboSpan" id="kobo.539.1">ID:
['a89cdb8c-3a85-42ff-9d5f-98f93f414df6']
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.540.1">The </span><code class="inlineCode"><span class="koboSpan" id="kobo.541.1">metadata</span></code><span class="koboSpan" id="kobo.542.1"> field </span><a id="_idIndexMarker202"/><span class="koboSpan" id="kobo.543.1">contains the </span><a id="_idIndexMarker203"/><span class="koboSpan" id="kobo.544.1">information we need to trace the content back to the original file and file path, as well as everything we need to understand this record, from the source to the embedded vector. </span><span class="koboSpan" id="kobo.544.2">It also contains the information of the node created from the record’s data, which can then be used for the indexing engine we will run in </span><em class="italic"><span class="koboSpan" id="kobo.545.1">Pipeline 3</span></em><span class="koboSpan" id="kobo.546.1">:</span></p>
    <ul>
      <li class="bulletList"><code class="inlineCode"><span class="koboSpan" id="kobo.547.1">file_path</span></code><span class="koboSpan" id="kobo.548.1">: Path to the file in the dataset </span><code class="inlineCode"><span class="koboSpan" id="kobo.549.1">(/content/data/1804.06985.txt</span></code><span class="koboSpan" id="kobo.550.1">).</span></li>
      <li class="bulletList"><code class="inlineCode"><span class="koboSpan" id="kobo.551.1">file_name</span></code><span class="koboSpan" id="kobo.552.1">: Name of the file (</span><code class="inlineCode"><span class="koboSpan" id="kobo.553.1">`1804.06985.txt`</span></code><span class="koboSpan" id="kobo.554.1">).</span></li>
      <li class="bulletList"><code class="inlineCode"><span class="koboSpan" id="kobo.555.1">file_type</span></code><span class="koboSpan" id="kobo.556.1">: Type of file (</span><code class="inlineCode"><span class="koboSpan" id="kobo.557.1">`text/plain`</span></code><span class="koboSpan" id="kobo.558.1">).</span></li>
      <li class="bulletList"><code class="inlineCode"><span class="koboSpan" id="kobo.559.1">file_size</span></code><span class="koboSpan" id="kobo.560.1">: Size of the file in bytes (</span><code class="inlineCode"><span class="koboSpan" id="kobo.561.1">`3700`</span></code><span class="koboSpan" id="kobo.562.1">).</span></li>
      <li class="bulletList"><code class="inlineCode"><span class="koboSpan" id="kobo.563.1">creation_date</span></code><span class="koboSpan" id="kobo.564.1">: Date the file was created (</span><code class="inlineCode"><span class="koboSpan" id="kobo.565.1">`2024-08-09`</span></code><span class="koboSpan" id="kobo.566.1">).</span></li>
      <li class="bulletList"><code class="inlineCode"><span class="koboSpan" id="kobo.567.1">last_modified_date</span></code><span class="koboSpan" id="kobo.568.1">: Date the file was last modified (</span><code class="inlineCode"><span class="koboSpan" id="kobo.569.1">`2024-08-09`</span></code><span class="koboSpan" id="kobo.570.1">).</span></li>
      <li class="bulletList"><code class="inlineCode"><span class="koboSpan" id="kobo.571.1">_node_content</span></code><span class="koboSpan" id="kobo.572.1">: Detailed content of the node, including the following main items:</span><ul>
          <li class="bulletList level-2"><code class="inlineCode"><span class="koboSpan" id="kobo.573.1">id_</span></code><span class="koboSpan" id="kobo.574.1">: Unique identifier for the node (</span><code class="inlineCode"><span class="koboSpan" id="kobo.575.1">`a89cdb8c-3a85-42ff-9d5f-98f93f414df6 `</span></code><span class="koboSpan" id="kobo.576.1">).</span></li>
          <li class="bulletList level-2"><code class="inlineCode"><span class="koboSpan" id="kobo.577.1">embedding</span></code><span class="koboSpan" id="kobo.578.1">: Embedding related to the text (</span><code class="inlineCode"><span class="koboSpan" id="kobo.579.1">null</span></code><span class="koboSpan" id="kobo.580.1">).</span></li>
          <li class="bulletList level-2"><code class="inlineCode"><span class="koboSpan" id="kobo.581.1">metadata</span></code><span class="koboSpan" id="kobo.582.1">: Repeated metadata about the file.</span></li>
          <li class="bulletList level-2"><code class="inlineCode"><span class="koboSpan" id="kobo.583.1">excluded_embed_metadata_keys</span></code><span class="koboSpan" id="kobo.584.1">: Keys excluded from embedding metadata (not necessary for embedding).</span></li>
          <li class="bulletList level-2"><code class="inlineCode"><span class="koboSpan" id="kobo.585.1">excluded_llm_metadata_keys</span></code><span class="koboSpan" id="kobo.586.1">: Keys excluded from LLM metadata (not necessary for an LLM).</span></li>
          <li class="bulletList level-2"><code class="inlineCode"><span class="koboSpan" id="kobo.587.1">relationships</span></code><span class="koboSpan" id="kobo.588.1">: Information about relationships to other nodes.</span></li>
          <li class="bulletList level-2"><code class="inlineCode"><span class="koboSpan" id="kobo.589.1">text</span></code><span class="koboSpan" id="kobo.590.1">: Actual text content of the document. </span><span class="koboSpan" id="kobo.590.2">It can be the text itself, an abstract, a summary, or any other approach to optimize search functions.</span></li>
          <li class="bulletList level-2"><code class="inlineCode"><span class="koboSpan" id="kobo.591.1">start_char_idx</span></code><span class="koboSpan" id="kobo.592.1">: Starting character index of the text.</span></li>
          <li class="bulletList level-2"><code class="inlineCode"><span class="koboSpan" id="kobo.593.1">end_char_idx</span></code><span class="koboSpan" id="kobo.594.1">: Ending character index of the text.</span></li>
          <li class="bulletList level-2"><code class="inlineCode"><span class="koboSpan" id="kobo.595.1">text_template</span></code><span class="koboSpan" id="kobo.596.1">: Template for displaying text with metadata.</span></li>
          <li class="bulletList level-2"><code class="inlineCode"><span class="koboSpan" id="kobo.597.1">metadata_template</span></code><span class="koboSpan" id="kobo.598.1">: Template for displaying metadata.</span></li>
          <li class="bulletList level-2"><code class="inlineCode"><span class="koboSpan" id="kobo.599.1">metadata_seperator</span></code><span class="koboSpan" id="kobo.600.1">: Separator used in metadata display.</span></li>
          <li class="bulletList level-2"><code class="inlineCode"><span class="koboSpan" id="kobo.601.1">class_name</span></code><span class="koboSpan" id="kobo.602.1">: Type of node (e.g., </span><code class="inlineCode"><span class="koboSpan" id="kobo.603.1">`TextNode`</span></code><span class="koboSpan" id="kobo.604.1">).</span></li>
        </ul>
      </li>
      <li class="bulletList"><code class="inlineCode"><span class="koboSpan" id="kobo.605.1">_node_type</span></code><span class="koboSpan" id="kobo.606.1">: Type of node (</span><code class="inlineCode"><span class="koboSpan" id="kobo.607.1">`TextNode`</span></code><span class="koboSpan" id="kobo.608.1">).</span></li>
      <li class="bulletList"><code class="inlineCode"><span class="koboSpan" id="kobo.609.1">document_id</span></code><span class="koboSpan" id="kobo.610.1">: Identifier for the document (</span><code class="inlineCode"><span class="koboSpan" id="kobo.611.1">`61e7201d-0359-42b4-9a5f-32c4d67f345e`</span></code><span class="koboSpan" id="kobo.612.1">).</span></li>
      <li class="bulletList"><code class="inlineCode"><span class="koboSpan" id="kobo.613.1">doc_id</span></code><span class="koboSpan" id="kobo.614.1">: Document ID, same as </span><code class="inlineCode"><span class="koboSpan" id="kobo.615.1">document_id</span></code><span class="koboSpan" id="kobo.616.1">.</span></li>
      <li class="bulletList"><code class="inlineCode"><span class="koboSpan" id="kobo.617.1">ref_doc_id</span></code><span class="koboSpan" id="kobo.618.1">: Reference document ID, same as </span><code class="inlineCode"><span class="koboSpan" id="kobo.619.1">document_id</span></code><span class="koboSpan" id="kobo.620.1">.</span></li>
    </ul>
    <p class="normal"><span class="koboSpan" id="kobo.621.1">The </span><code class="inlineCode"><span class="koboSpan" id="kobo.622.1">text</span></code><span class="koboSpan" id="kobo.623.1"> field contains the field of this chunk of data, not the whole original text:</span></p>
    <pre class="programlisting con"><code class="hljs-con"><span class="koboSpan" id="kobo.624.1">['High Energy Physics  Theory arXiv1804.06985 hepth Submitted on 19 Apr 2018 Title A Near Horizon Extreme Binary Black Hole Geometry Authors Jacob Ciafre  Maria J. </span><span class="koboSpan" id="kobo.624.2">Rodriguez View a PDF of the paper titled A Near Horizon Extreme Binary…
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.625.1">The </span><code class="inlineCode"><span class="koboSpan" id="kobo.626.1">Embedding</span></code><span class="koboSpan" id="kobo.627.1"> field contains the embedded vector of the text content:</span></p>
    <pre class="programlisting con"><code class="hljs-con"><span class="koboSpan" id="kobo.628.1">[-0.0009671939187683165, 0.010151553899049759, -0.010979819111526012, -0.003061748342588544, -0.00865076668560505, 0.02144993655383587, -0.01412297785282135, -0.02674516849219799, -0.008693241514265537, -0.03383851423859596, 0.011404570192098618, 0.015956487506628036, -0.013691147789359093, 0.008856062777340412,…]
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.629.1">The structure</span><a id="_idIndexMarker204"/><span class="koboSpan" id="kobo.630.1"> and format of </span><a id="_idIndexMarker205"/><span class="koboSpan" id="kobo.631.1">RAG datasets vary from one domain or project to another. </span><span class="koboSpan" id="kobo.631.2">However, the following four columns of this dataset provide valuable information on the evolution of AI:</span></p>
    <ul>
      <li class="bulletList"><code class="inlineCode"><span class="koboSpan" id="kobo.632.1">id</span></code><span class="koboSpan" id="kobo.633.1">: The </span><code class="inlineCode"><span class="koboSpan" id="kobo.634.1">id</span></code><span class="koboSpan" id="kobo.635.1"> is the index we will be using to organize the chunks of text of the </span><code class="inlineCode"><span class="koboSpan" id="kobo.636.1">text</span></code><span class="koboSpan" id="kobo.637.1"> column in the dataset. </span><span class="koboSpan" id="kobo.637.2">The chunks will be transformed into </span><em class="italic"><span class="koboSpan" id="kobo.638.1">nodes</span></em><span class="koboSpan" id="kobo.639.1"> that can contain the original text, summaries of the original text, and additional information, such as the source of the data used for the output that is stored in the metadata column. </span><span class="koboSpan" id="kobo.639.2">We created this index in </span><strong class="keyWord"><span class="koboSpan" id="kobo.640.1">Pipeline 2</span></strong><span class="koboSpan" id="kobo.641.1"> of this notebook when we created the vector store. </span><span class="koboSpan" id="kobo.641.2">However, we can generate indexes in memory on an existing database that contains no indexes, as we will see in </span><em class="chapterRef"><span class="koboSpan" id="kobo.642.1">Chapter 4</span></em><span class="koboSpan" id="kobo.643.1">, </span><em class="italic"><span class="koboSpan" id="kobo.644.1">Multimodal Modular RAG for Drone Technology</span></em><span class="koboSpan" id="kobo.645.1">.</span></li>
      <li class="bulletList"><code class="inlineCode"><span class="koboSpan" id="kobo.646.1">metadata</span></code><span class="koboSpan" id="kobo.647.1">: The metadata was generated automatically in </span><strong class="keyWord"><span class="koboSpan" id="kobo.648.1">Pipeline 1</span></strong><span class="koboSpan" id="kobo.649.1"> when Deep Lake’s </span><code class="inlineCode"><span class="koboSpan" id="kobo.650.1">SimpleDirectoryReader</span></code><span class="koboSpan" id="kobo.651.1"> loaded the source documents in a documents object, and also when the vector store was created. </span><span class="koboSpan" id="kobo.651.2">In </span><em class="chapterRef"><span class="koboSpan" id="kobo.652.1">Chapter 2</span></em><span class="koboSpan" id="kobo.653.1">, </span><em class="italic"><span class="koboSpan" id="kobo.654.1">RAG Embedding Vector Stores with Deep Lake and OpenAI</span></em><span class="koboSpan" id="kobo.655.1">, we only had one file source of data. </span><span class="koboSpan" id="kobo.655.2">In this chapter, we stored the data in one file for each data source (URL).</span></li>
      <li class="bulletList"><code class="inlineCode"><span class="koboSpan" id="kobo.656.1">text</span></code><span class="koboSpan" id="kobo.657.1">: The text processed by Deep Lake’s vector store creation functionality that we ran in </span><strong class="keyWord"><span class="koboSpan" id="kobo.658.1">Pipeline 2</span></strong><span class="koboSpan" id="kobo.659.1"> automatically chunked the data, without us having to configure the size of the chunks, as we did in the </span><em class="italic"><span class="koboSpan" id="kobo.660.1">Retrieving a batch of prepared documents</span></em><span class="koboSpan" id="kobo.661.1"> section in </span><em class="chapterRef"><span class="koboSpan" id="kobo.662.1">Chapter 2</span></em><span class="koboSpan" id="kobo.663.1">. </span><span class="koboSpan" id="kobo.663.2">Once again, the process is seamless. </span><span class="koboSpan" id="kobo.663.3">We will see how smart chunking is done in the </span><em class="italic"><span class="koboSpan" id="kobo.664.1">Optimized chunking</span></em><span class="koboSpan" id="kobo.665.1"> section of </span><em class="italic"><span class="koboSpan" id="kobo.666.1">Pipeline 3: Index-based RAG</span></em><span class="koboSpan" id="kobo.667.1"> in this chapter.</span></li>
      <li class="bulletList"><code class="inlineCode"><span class="koboSpan" id="kobo.668.1">embedding</span></code><span class="koboSpan" id="kobo.669.1">: The embedding for each chunk of data was generated through an embedding model that we do not have to configure. </span><span class="koboSpan" id="kobo.669.2">We could choose an embedding model, as we did in the </span><em class="italic"><span class="koboSpan" id="kobo.670.1">Data embedding and storage</span></em><span class="koboSpan" id="kobo.671.1"> section in </span><em class="chapterRef"><span class="koboSpan" id="kobo.672.1">Chapter 2</span></em><span class="koboSpan" id="kobo.673.1">, </span><em class="italic"><span class="koboSpan" id="kobo.674.1">RAG Embedding Vector Stores with Deep Lake and OpenAI</span></em><span class="koboSpan" id="kobo.675.1">. </span><span class="koboSpan" id="kobo.675.2">We selected an embedding model and wrote a function. </span><span class="koboSpan" id="kobo.675.3">In this program, Deep Lake selects the embedding model and embeds the data, without us having to write a single line of code.</span></li>
    </ul>
    <p class="normal"><span class="koboSpan" id="kobo.676.1">We can see that</span><a id="_idIndexMarker206"/><span class="koboSpan" id="kobo.677.1"> embedding, chunking, indexing, and other data processing functions are now encapsulated in platforms and </span><a id="_idIndexMarker207"/><span class="koboSpan" id="kobo.678.1">frameworks, such as Activeloop Deep Lake, LlamaIndex, OpenAI, LangChain, Hugging Face, Chroma, and many others. </span><span class="koboSpan" id="kobo.678.2">Progressively, the initial excitement of generative AI models and RAG will fade, and they will become industrialized, encapsulated, and commonplace components of AI pipelines. </span><span class="koboSpan" id="kobo.678.3">AI is evolving, and it might be helpful to facilitate a platform that offers a default configuration based on effective practices. </span><span class="koboSpan" id="kobo.678.4">Then, once we have implemented a basic configuration, we can customize and expand the pipelines as necessary for our projects.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.679.1">We are now ready to run index-based RAG.</span></p>
    <h2 id="_idParaDest-83" class="heading-2"><span class="koboSpan" id="kobo.680.1">Pipeline 3: Index-based RAG</span></h2>
    <p class="normal"><span class="koboSpan" id="kobo.681.1">In this section, we will implement an </span><a id="_idIndexMarker208"/><span class="koboSpan" id="kobo.682.1">index-based RAG pipeline using </span><code class="inlineCode"><span class="koboSpan" id="kobo.683.1">LlamaIndex</span></code><span class="koboSpan" id="kobo.684.1">, which uses the data we have prepared and processed with Deep Lake. </span><span class="koboSpan" id="kobo.684.2">We will retrieve relevant information from the heterogeneous (noise-containing) drone-related document collection and synthesize the response through OpenAI’s LLM models. </span><span class="koboSpan" id="kobo.684.3">We will implement four index engines:</span></p>
    <ul>
      <li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.685.1">Vector Store Index Engine</span></strong><span class="koboSpan" id="kobo.686.1">: Creates a </span><a id="_idIndexMarker209"/><span class="koboSpan" id="kobo.687.1">vector store index from the documents, enabling efficient similarity-based searches.</span></li>
      <li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.688.1">Tree Index</span></strong><span class="koboSpan" id="kobo.689.1">: Builds a </span><a id="_idIndexMarker210"/><span class="koboSpan" id="kobo.690.1">hierarchical tree index from the documents, offering an alternative retrieval structure.</span></li>
      <li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.691.1">List Index</span></strong><span class="koboSpan" id="kobo.692.1">: Constructs</span><a id="_idIndexMarker211"/><span class="koboSpan" id="kobo.693.1"> a straightforward list index from the documents.</span></li>
      <li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.694.1">Keyword Table Index</span></strong><span class="koboSpan" id="kobo.695.1">: Creates </span><a id="_idIndexMarker212"/><span class="koboSpan" id="kobo.696.1">an index based on keywords extracted from the documents.</span></li>
    </ul>
    <p class="normal"><span class="koboSpan" id="kobo.697.1">We will implement </span><a id="_idIndexMarker213"/><span class="koboSpan" id="kobo.698.1">querying with an LLM:</span></p>
    <ul>
      <li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.699.1">Query Response and Source</span></strong><span class="koboSpan" id="kobo.700.1">: Queries the index with user input, retrieves the relevant documents, and returns a synthesized response along with source information.</span></li>
    </ul>
    <p class="normal"><span class="koboSpan" id="kobo.701.1">We will measure the responses with a </span><em class="italic"><span class="koboSpan" id="kobo.702.1">time-weighted average metric with LLM score and cosine similarity</span></em><span class="koboSpan" id="kobo.703.1"> that calculates a time-weighted average, based on retrieval and similarity scores. </span><span class="koboSpan" id="kobo.703.2">The content and execution times might vary from one run to another due to the stochastic algorithms implemented.</span></p>
    <h3 id="_idParaDest-84" class="heading-3"><span class="koboSpan" id="kobo.704.1">User input and query parameters</span></h3>
    <p class="normal"><span class="koboSpan" id="kobo.705.1">The user input </span><a id="_idIndexMarker214"/><span class="koboSpan" id="kobo.706.1">will be the reference question for the four index engines we will run. </span><span class="koboSpan" id="kobo.706.2">We will evaluate each response based on the index engine’s retrievals and measure the outputs, using time and score ratios. </span><span class="koboSpan" id="kobo.706.3">The input will be submitted to the four index and query engines we will build later.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.707.1">The user input is:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.708.1">user_input=</span><span class="hljs-string"><span class="koboSpan" id="kobo.709.1">"How do drones identify vehicles?"</span></span>
</code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.710.1">The four</span><a id="_idIndexMarker215"/><span class="koboSpan" id="kobo.711.1"> query engines that implement an LLM (in this case, an OpenAI model) will seamlessly be called with the same parameters. </span><span class="koboSpan" id="kobo.711.2">The three parameters that we will set are:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.712.1">#similarity_top_k</span></span><span class="koboSpan" id="kobo.713.1">
k=</span><span class="hljs-number"><span class="koboSpan" id="kobo.714.1">3</span></span>
<span class="hljs-comment"><span class="koboSpan" id="kobo.715.1">#temperature</span></span><span class="koboSpan" id="kobo.716.1">
temp=</span><span class="hljs-number"><span class="koboSpan" id="kobo.717.1">0.1</span></span>
<span class="hljs-comment"><span class="koboSpan" id="kobo.718.1">#num_output</span></span><span class="koboSpan" id="kobo.719.1">
mt=</span><span class="hljs-number"><span class="koboSpan" id="kobo.720.1">1024</span></span>
</code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.721.1">These key </span><a id="_idIndexMarker216"/><span class="koboSpan" id="kobo.722.1">parameters are:</span></p>
    <ul>
      <li class="bulletList"><code class="inlineCode"><span class="koboSpan" id="kobo.723.1">k=3</span></code><span class="koboSpan" id="kobo.724.1">: The query engine will be required to find the top 3 most probable responses by setting the top-k (most probable choices) to 3. </span><span class="koboSpan" id="kobo.724.2">In this case, k will serve as a ranking function that will force the LLM to select the top documents.</span></li>
      <li class="bulletList"><code class="inlineCode"><span class="koboSpan" id="kobo.725.1">temp=0.1</span></code><span class="koboSpan" id="kobo.726.1">: A low temperature such as </span><code class="inlineCode"><span class="koboSpan" id="kobo.727.1">0.1</span></code><span class="koboSpan" id="kobo.728.1"> will encourage the LLM to produce precise results. </span><span class="koboSpan" id="kobo.728.2">If the temperature is increased to </span><code class="inlineCode"><span class="koboSpan" id="kobo.729.1">0.9</span></code><span class="koboSpan" id="kobo.730.1">, for example, the response will be more creative. </span><span class="koboSpan" id="kobo.730.2">However, in this case, we are exploring drone technology, which requires precision.</span></li>
      <li class="bulletList"><code class="inlineCode"><span class="koboSpan" id="kobo.731.1">mt=1024</span></code><span class="koboSpan" id="kobo.732.1">: This parameter will limit the number of tokens of the output to </span><code class="inlineCode"><span class="koboSpan" id="kobo.733.1">1,024</span></code><span class="koboSpan" id="kobo.734.1">.</span></li>
    </ul>
    <p class="normal"><span class="koboSpan" id="kobo.735.1">The user input and parameters will be applied to the four query engines. </span><span class="koboSpan" id="kobo.735.2">Let’s now build the cosine similarity metric.</span></p>
    <h3 id="_idParaDest-85" class="heading-3"><span class="koboSpan" id="kobo.736.1">Cosine similarity metric</span></h3>
    <p class="normal"><span class="koboSpan" id="kobo.737.1">The cosine similarity</span><a id="_idIndexMarker217"/><span class="koboSpan" id="kobo.738.1"> metric was described in the </span><em class="italic"><span class="koboSpan" id="kobo.739.1">Evaluating the Output with the Cosine Similarity</span></em><span class="koboSpan" id="kobo.740.1"> section in </span><em class="italic"><span class="koboSpan" id="kobo.741.1">Chapter 2</span></em><span class="koboSpan" id="kobo.742.1">. </span><span class="koboSpan" id="kobo.742.2">If necessary, take the time to go through that section again. </span><span class="koboSpan" id="kobo.742.3">Here, we will create a function for the responses:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.743.1">from</span></span><span class="koboSpan" id="kobo.744.1"> sklearn.feature_extraction.text </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.745.1">import</span></span><span class="koboSpan" id="kobo.746.1"> TfidfVectorizer
</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.747.1">from</span></span><span class="koboSpan" id="kobo.748.1"> sklearn.metrics.pairwise </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.749.1">import</span></span><span class="koboSpan" id="kobo.750.1"> cosine_similarity
</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.751.1">from</span></span><span class="koboSpan" id="kobo.752.1"> sentence_transformers </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.753.1">import</span></span><span class="koboSpan" id="kobo.754.1"> SentenceTransformer
model = SentenceTransformer(</span><span class="hljs-string"><span class="koboSpan" id="kobo.755.1">'all-MiniLM-L6-v2'</span></span><span class="koboSpan" id="kobo.756.1">)
</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.757.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.758.1">calculate_cosine_similarity_with_embeddings</span></span><span class="koboSpan" id="kobo.759.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.760.1">text1, text2</span></span><span class="koboSpan" id="kobo.761.1">):
    embeddings1 = model.encode(text1)
    embeddings2 = model.encode(text2)
    similarity = cosine_similarity([embeddings1], [embeddings2])
    </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.762.1">return</span></span><span class="koboSpan" id="kobo.763.1"> similarity[</span><span class="hljs-number"><span class="koboSpan" id="kobo.764.1">0</span></span><span class="koboSpan" id="kobo.765.1">][</span><span class="hljs-number"><span class="koboSpan" id="kobo.766.1">0</span></span><span class="koboSpan" id="kobo.767.1">]
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.768.1">The function </span><a id="_idIndexMarker218"/><span class="koboSpan" id="kobo.769.1">uses </span><code class="inlineCode"><span class="koboSpan" id="kobo.770.1">sklearn</span></code><span class="koboSpan" id="kobo.771.1"> and also Hugging Face’s </span><code class="inlineCode"><span class="koboSpan" id="kobo.772.1">SentenceTransformer</span></code><span class="koboSpan" id="kobo.773.1">. </span><span class="koboSpan" id="kobo.773.2">The program first creates the vector store engine.</span></p>
    <h1 id="_idParaDest-86" class="heading-1"><span class="koboSpan" id="kobo.774.1">Vector store index query engine</span></h1>
    <p class="normal"><code class="inlineCode"><span class="koboSpan" id="kobo.775.1">VectorStoreIndex</span></code><span class="koboSpan" id="kobo.776.1"> is a type of index </span><a id="_idIndexMarker219"/><span class="koboSpan" id="kobo.777.1">within LlamaIndex that implements vector embeddings to represent and retrieve information from documents. </span><span class="koboSpan" id="kobo.777.2">These documents with similar meanings will have embeddings that are closer together in the vector space, as we explored in the previous chapter. </span><span class="koboSpan" id="kobo.777.3">However, this time, the </span><code class="inlineCode"><span class="koboSpan" id="kobo.778.1">VectorStoreIndex</span></code><span class="koboSpan" id="kobo.779.1"> does not automatically use the existing Deep Lake vector store. </span><span class="koboSpan" id="kobo.779.2">It can create a new in-memory vector index, re-embed the documents, and create a new index structure. </span><span class="koboSpan" id="kobo.779.3">We will take this approach further in </span><em class="chapterRef"><span class="koboSpan" id="kobo.780.1">Chapter 4</span></em><span class="koboSpan" id="kobo.781.1">, </span><em class="italic"><span class="koboSpan" id="kobo.782.1">Multimodal Modular RAG for Drone Technology</span></em><span class="koboSpan" id="kobo.783.1">, when we implement a dataset that contains no indexes or embeddings.</span></p>
    <div class="note">
      <p class="normal"><span class="koboSpan" id="kobo.784.1">There is no silver bullet to deciding which indexing method is suitable for your project! </span><span class="koboSpan" id="kobo.784.2">The best way to make a choice is to test the vector, tree, list, and keyword indexes introduced in this chapter.</span></p>
    </div>
    <p class="normal"><span class="koboSpan" id="kobo.785.1">We will first create the vector store index:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.786.1">from</span></span><span class="koboSpan" id="kobo.787.1"> llama_index.core </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.788.1">import</span></span><span class="koboSpan" id="kobo.789.1"> VectorStoreIndex
vector_store_index = VectorStoreIndex.from_documents(documents)
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.790.1">We then display the vector store index we created:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.791.1">print</span></span><span class="koboSpan" id="kobo.792.1">(</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.793.1">type</span></span><span class="koboSpan" id="kobo.794.1">(vector_store_index))
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.795.1">We will receive the following output, which confirms that the engine was created:</span></p>
    <pre class="programlisting con"><code class="hljs-con"><span class="koboSpan" id="kobo.796.1">&lt;class 'llama_index.core.indices.vector_store.base.VectorStoreIndex'&gt;
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.797.1">We now need a query engine to retrieve and synthesize the document(s) retrieved with an LLM—in our case, an OpenAI model (installed with </span><code class="inlineCode"><span class="koboSpan" id="kobo.798.1">!pip install llama-index-vector-stores-deeplake==0.1.2</span></code><span class="koboSpan" id="kobo.799.1">):</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.800.1">vector_query_engine = vector_store_index.as_query_engine(similarity_top_k=k, temperature=temp, num_output=mt)
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.801.1">We defined the </span><a id="_idIndexMarker220"/><span class="koboSpan" id="kobo.802.1">parameters of the query engine in the </span><em class="italic"><span class="koboSpan" id="kobo.803.1">User input and query parameters</span></em><span class="koboSpan" id="kobo.804.1"> subsection. </span><span class="koboSpan" id="kobo.804.2">We can now query the dataset and generate a response.</span></p>
    <h2 id="_idParaDest-87" class="heading-2"><span class="koboSpan" id="kobo.805.1">Query response and source</span></h2>
    <p class="normal"><span class="koboSpan" id="kobo.806.1">Let’s define a function that will</span><a id="_idIndexMarker221"/><span class="koboSpan" id="kobo.807.1"> manage the query and return information on the content of the response:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.808.1">import</span></span><span class="koboSpan" id="kobo.809.1"> pandas </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.810.1">as</span></span><span class="koboSpan" id="kobo.811.1"> pd
</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.812.1">import</span></span><span class="koboSpan" id="kobo.813.1"> textwrap
</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.814.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.815.1">index_query</span></span><span class="koboSpan" id="kobo.816.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.817.1">input_query</span></span><span class="koboSpan" id="kobo.818.1">):
    response = vector_query_engine.query(input_query)
    </span><span class="hljs-comment"><span class="koboSpan" id="kobo.819.1"># Optional: Print a formatted view of the response (remove if you don't need it in the output)</span></span>
    <span class="hljs-built_in"><span class="koboSpan" id="kobo.820.1">print</span></span><span class="koboSpan" id="kobo.821.1">(textwrap.fill(</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.822.1">str</span></span><span class="koboSpan" id="kobo.823.1">(response), </span><span class="hljs-number"><span class="koboSpan" id="kobo.824.1">100</span></span><span class="koboSpan" id="kobo.825.1">))
    node_data = []
    </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.826.1">for</span></span><span class="koboSpan" id="kobo.827.1"> node_with_score </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.828.1">in</span></span><span class="koboSpan" id="kobo.829.1"> response.source_nodes:
        node = node_with_score.node
        node_info = {
            </span><span class="hljs-string"><span class="koboSpan" id="kobo.830.1">'Node ID'</span></span><span class="koboSpan" id="kobo.831.1">: node.id_,
            </span><span class="hljs-string"><span class="koboSpan" id="kobo.832.1">'Score'</span></span><span class="koboSpan" id="kobo.833.1">: node_with_score.score,
            </span><span class="hljs-string"><span class="koboSpan" id="kobo.834.1">'Text'</span></span><span class="koboSpan" id="kobo.835.1">: node.text
        }
        node_data.append(node_info)
    df = pd.DataFrame(node_data)
    </span><span class="hljs-comment"><span class="koboSpan" id="kobo.836.1"># Instead of printing, return the DataFrame and the response object</span></span>
    <span class="hljs-keyword"><span class="koboSpan" id="kobo.837.1">return</span></span><span class="koboSpan" id="kobo.838.1"> df, response,
</span></code></pre>
    <p class="normal"><code class="inlineCode"><span class="koboSpan" id="kobo.839.1">index_query(input_query)</span></code><span class="koboSpan" id="kobo.840.1"> executes a query using a vector query engine and processes the results into a structured format. </span><span class="koboSpan" id="kobo.840.2">The function takes an input query and retrieves relevant information, using the query engine in a pandas DataFrame: </span><code class="inlineCode"><span class="koboSpan" id="kobo.841.1">Node ID</span></code><span class="koboSpan" id="kobo.842.1">, </span><code class="inlineCode"><span class="koboSpan" id="kobo.843.1">Score</span></code><span class="koboSpan" id="kobo.844.1">, </span><code class="inlineCode"><span class="koboSpan" id="kobo.845.1">File Path</span></code><span class="koboSpan" id="kobo.846.1">, </span><code class="inlineCode"><span class="koboSpan" id="kobo.847.1">Filename</span></code><span class="koboSpan" id="kobo.848.1">, and </span><code class="inlineCode"><span class="koboSpan" id="kobo.849.1">Text</span></code><span class="koboSpan" id="kobo.850.1">.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.851.1">The code will now call the query:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.852.1">import</span></span><span class="koboSpan" id="kobo.853.1"> time
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.854.1">#start the timer</span></span><span class="koboSpan" id="kobo.855.1">
start_time = time.time()
df, response = index_query(user_input)
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.856.1"># Stop the timer</span></span><span class="koboSpan" id="kobo.857.1">
end_time = time.time()
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.858.1"># Calculate and print the execution time</span></span><span class="koboSpan" id="kobo.859.1">
elapsed_time = end_time - start_time
</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.860.1">print</span></span><span class="koboSpan" id="kobo.861.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.862.1">f"Query execution time: </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.863.1">{elapsed_time:</span></span><span class="hljs-number"><span class="koboSpan" id="kobo.864.1">.4</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.865.1">f}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.866.1"> seconds"</span></span><span class="koboSpan" id="kobo.867.1">)
</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.868.1">print</span></span><span class="koboSpan" id="kobo.869.1">(df.to_markdown(index=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.870.1">False</span></span><span class="koboSpan" id="kobo.871.1">, numalign=</span><span class="hljs-string"><span class="koboSpan" id="kobo.872.1">"left"</span></span><span class="koboSpan" id="kobo.873.1">, stralign=</span><span class="hljs-string"><span class="koboSpan" id="kobo.874.1">"left"</span></span><span class="koboSpan" id="kobo.875.1">))  </span><span class="hljs-comment"><span class="koboSpan" id="kobo.876.1"># Display the DataFrame using markdown</span></span>
</code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.877.1">We will evaluate the </span><a id="_idIndexMarker222"/><span class="koboSpan" id="kobo.878.1">time it takes for the query to retrieve the relevant data and generate a response synthesis with the LLM (in this case, an OpenAI model). </span><span class="koboSpan" id="kobo.878.2">The output of the semantic search first returns a response synthesized by the LLM:</span></p>
    <pre class="programlisting con"><code class="hljs-con"><span class="koboSpan" id="kobo.879.1">Drones can automatically identify vehicles across different cameras with different viewpoints and hardware specifications using reidentification methods.
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.880.1">The output then displays the elapsed time of the query:</span></p>
    <pre class="programlisting con"><code class="hljs-con"><span class="koboSpan" id="kobo.881.1">Query execution time: 0.8831 seconds
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.882.1">The output now displays node information. </span><span class="koboSpan" id="kobo.882.2">The score of each node of three </span><code class="inlineCode"><span class="koboSpan" id="kobo.883.1">k=3</span></code><span class="koboSpan" id="kobo.884.1"> documents was retrieved with their text excerpts:</span></p>
    <figure class="mediaobject"><span class="koboSpan" id="kobo.885.1"><img src="../Images/B31169_03_04.png" alt="A close-up of a number  Description automatically generated"/></span></figure>
    <p class="packt_figref"><span class="koboSpan" id="kobo.886.1">Figure 3.4: Node information output</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.887.1">The ID of the </span><a id="_idIndexMarker223"/><span class="koboSpan" id="kobo.888.1">node guarantees full transparency and can be traced back to the original document, even when the index engines re-index the dataset. </span><span class="koboSpan" id="kobo.888.2">We can obtain the node source of the first node, for example, with the following code:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.889.1">nodeid=response.source_nodes[</span><span class="hljs-number"><span class="koboSpan" id="kobo.890.1">0</span></span><span class="koboSpan" id="kobo.891.1">].node_id
nodeid
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.892.1">The output provides the node ID:</span></p>
    <pre class="programlisting con"><code class="hljs-con"><span class="koboSpan" id="kobo.893.1">4befdb13-305d-42db-a616-5d9932c17ac8
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.894.1">We can drill down and retrieve the full text of the node containing the document that was synthesized by the LLM:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.895.1">response.source_nodes[</span><span class="hljs-number"><span class="koboSpan" id="kobo.896.1">0</span></span><span class="koboSpan" id="kobo.897.1">].get_text()
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.898.1">The output will display the following text:</span></p>
    <pre class="programlisting con"><code class="hljs-con"><span class="koboSpan" id="kobo.899.1">['These activities can be carried out with different approaches that include photogrammetry SfM thermography multispectral images 3D field scanning NDVI maps etc. </span><span class="koboSpan" id="kobo.899.2">Agriculture forestry and environmental studies edit Main article Agricultural drone As global demand for food production grows exponentially resources are depleted farmland is…
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.900.1">We can also peek into the nodes and retrieve their chunk size.</span></p>
    <h2 id="_idParaDest-88" class="heading-2"><span class="koboSpan" id="kobo.901.1">Optimized chunking</span></h2>
    <p class="normal"><span class="koboSpan" id="kobo.902.1">We can predefine the chunk size, or</span><a id="_idIndexMarker224"/><span class="koboSpan" id="kobo.903.1"> we can let LlamaIndex select it for us. </span><span class="koboSpan" id="kobo.903.2">In this case, the code determines the chunk size automatically:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.904.1">for</span></span><span class="koboSpan" id="kobo.905.1"> node_with_score </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.906.1">in</span></span><span class="koboSpan" id="kobo.907.1"> response.source_nodes:
    node = node_with_score.node  </span><span class="hljs-comment"><span class="koboSpan" id="kobo.908.1"># Extract the Node object from NodeWithScore</span></span><span class="koboSpan" id="kobo.909.1">
    chunk_size = </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.910.1">len</span></span><span class="koboSpan" id="kobo.911.1">(node.text)
    </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.912.1">print</span></span><span class="koboSpan" id="kobo.913.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.914.1">f"Node ID: </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.915.1">{node.id_}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.916.1">, Chunk Size: </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.917.1">{chunk_size}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.918.1"> characters"</span></span><span class="koboSpan" id="kobo.919.1">)
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.920.1">The advantage of an automated chunk size is that it can be variable. </span><span class="koboSpan" id="kobo.920.2">For example, in this case, the chunk size shown in the size of the output nodes is probably in the 4000-to-5500-character range:</span></p>
    <pre class="programlisting con"><code class="hljs-con"><span class="koboSpan" id="kobo.921.1">Node ID: 83a135c6-dddd-402e-9423-d282e6524160, Chunk Size: 4417 characters
Node ID: 7b7b55fe-0354-45bc-98da-0a715ceaaab0, Chunk Size: 1806 characters
Node ID: 18528a16-ce77-46a9-bbc6-5e8f05418d95, Chunk Size: 3258 characters
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.922.1">The chunking</span><a id="_idIndexMarker225"/><span class="koboSpan" id="kobo.923.1"> function does not linearly cut content but optimizes the chunks for semantic search.</span></p>
    <h2 id="_idParaDest-89" class="heading-2"><span class="koboSpan" id="kobo.924.1">Performance metric</span></h2>
    <p class="normal"><span class="koboSpan" id="kobo.925.1">We will also implement a performance metric</span><a id="_idIndexMarker226"/><span class="koboSpan" id="kobo.926.1"> based on the accuracy of the queries and the time elapsed. </span><span class="koboSpan" id="kobo.926.2">This function calculates and prints a performance metric for a query, along with its execution time. </span><span class="koboSpan" id="kobo.926.3">The metric is based on the weighted average relevance scores of the retrieved information, divided by the time it took to get the results. </span><span class="koboSpan" id="kobo.926.4">Higher scores indicate better performance.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.927.1">We first calculate the sum of the scores and the average score, and then we divide the weighted average by the time elapsed to perform the query:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.928.1">import</span></span><span class="koboSpan" id="kobo.929.1"> numpy </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.930.1">as</span></span><span class="koboSpan" id="kobo.931.1"> np
</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.932.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.933.1">info_metrics</span></span><span class="koboSpan" id="kobo.934.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.935.1">response</span></span><span class="koboSpan" id="kobo.936.1">):
  </span><span class="hljs-comment"><span class="koboSpan" id="kobo.937.1"># Calculate the performance (handling None scores)</span></span><span class="koboSpan" id="kobo.938.1">
  scores = [node.score </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.939.1">for</span></span><span class="koboSpan" id="kobo.940.1"> node </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.941.1">in</span></span><span class="koboSpan" id="kobo.942.1"> response.source_nodes </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.943.1">if</span></span><span class="koboSpan" id="kobo.944.1"> node.score </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.945.1">is</span></span> <span class="hljs-keyword"><span class="koboSpan" id="kobo.946.1">not</span></span> <span class="hljs-literal"><span class="koboSpan" id="kobo.947.1">None</span></span><span class="koboSpan" id="kobo.948.1">]
  </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.949.1">if</span></span><span class="koboSpan" id="kobo.950.1"> scores:  </span><span class="hljs-comment"><span class="koboSpan" id="kobo.951.1"># Check if there are any valid scores</span></span><span class="koboSpan" id="kobo.952.1">
      weights = np.exp(scores) / np.</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.953.1">sum</span></span><span class="koboSpan" id="kobo.954.1">(np.exp(scores))
      perf = np.average(scores, weights=weights) / elapsed_time
  </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.955.1">else</span></span><span class="koboSpan" id="kobo.956.1">:
      perf = </span><span class="hljs-number"><span class="koboSpan" id="kobo.957.1">0</span></span>  <span class="hljs-comment"><span class="koboSpan" id="kobo.958.1"># Or some other default value if all scores are None</span></span>
           
</code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.959.1">The result is a ratio based on the average weight divided by the elapsed time:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.960.1">perf = np.average(scores, weights=weights) / elapsed_time
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.961.1">We can then call the function:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.962.1">info_metrics(response)
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.963.1">The output provides an estimation of the quality of the response:</span></p>
    <pre class="programlisting con"><code class="hljs-con"><span class="koboSpan" id="kobo.964.1">Average score: 0.8374
Query execution time: 1.3266 seconds
Performance metric: 0.6312
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.965.1">This performance metric is not an absolute value. </span><span class="koboSpan" id="kobo.965.2">It’s an indicator that we can use to compare this output with the other index engines. </span><span class="koboSpan" id="kobo.965.3">It may also vary from one run to another, due to the stochastic nature of machine learning algorithms. </span><span class="koboSpan" id="kobo.965.4">Additionally, the quality of the output depends on the user’s subjective perception. </span><span class="koboSpan" id="kobo.965.5">In any case, this metric will help </span><a id="_idIndexMarker227"/><span class="koboSpan" id="kobo.966.1">compare the query engines’ performances in this chapter.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.967.1">We can already see that the average score is satisfactory, even though we loaded heterogeneous and sometimes unrelated documents in the dataset. </span><span class="koboSpan" id="kobo.967.2">The integrated retriever and synthesizer functionality of LlamaIndex, Deep Lake, and OpenAI have proven to be highly effective.</span></p>
    <h1 id="_idParaDest-90" class="heading-1"><span class="koboSpan" id="kobo.968.1">Tree index query engine</span></h1>
    <p class="normal"><span class="koboSpan" id="kobo.969.1">The tree index in LlamaIndex creates </span><a id="_idIndexMarker228"/><span class="koboSpan" id="kobo.970.1">a hierarchical structure for managing and querying text documents efficiently. </span><span class="koboSpan" id="kobo.970.2">However, think of something other than a classical hierarchical structure! </span><span class="koboSpan" id="kobo.970.3">The tree index engine optimizes the hierarchy, content, and order of the nodes, as shown in </span><em class="italic"><span class="koboSpan" id="kobo.971.1">Figure 3.5</span></em><span class="koboSpan" id="kobo.972.1">:</span></p>
    <p class="packt_figref"><span class="koboSpan" id="kobo.973.1"><img src="../Images/B31169_03_05.png" alt="A diagram of a tree index  Description automatically generated"/></span></p>
    <p class="packt_figref"><span class="koboSpan" id="kobo.974.1">Figure 3.5: Optimized tree index</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.975.1">The tree</span><a id="_idIndexMarker229"/><span class="koboSpan" id="kobo.976.1"> index organizes documents in a tree structure, with broader summaries at higher levels and detailed information at lower levels. </span><span class="koboSpan" id="kobo.976.2">Each node in the tree summarizes the text it covers. </span><span class="koboSpan" id="kobo.976.3">The tree index is efficient for large datasets and queries large collections of documents rapidly by breaking them down into manageable optimized chunks. </span><span class="koboSpan" id="kobo.976.4">Thus, the optimization of the tree structure allows for rapid retrieval by traversing the relevant nodes without wasting time.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.977.1">Organizing this part of the pipeline and adjusting parameters such as tree depth and summary methods can be a specialized task for a team member. </span><span class="koboSpan" id="kobo.977.2">Depending on the project and workload, working on the tree structure could be part of </span><strong class="keyWord"><span class="koboSpan" id="kobo.978.1">Pipeline 2</span></strong><span class="koboSpan" id="kobo.979.1"> when creating and populating a vector store. </span><span class="koboSpan" id="kobo.979.2">Alternatively, the tree structure can be created in memory at the beginning of each session. </span><span class="koboSpan" id="kobo.979.3">The flexibility of the structure and implementation of tree structures and index engines, in general, can be a fascinating and valuable specialization in a RAG-driven generative AI team.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.980.1">In this index model, the LLM (an OpenAI model in this case) acts like it is answering a multiple-choice question when selecting the best nodes during a query. </span><span class="koboSpan" id="kobo.980.2">It analyzes the query, compares it with the summaries of the current node’s children, and decides which path to follow to find the most relevant information.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.981.1">The integrated LlamaIndex-Deep Lake-OpenAI process in this chapter is industrializing components seamlessly, taking AI to another level. </span><span class="koboSpan" id="kobo.981.2">LLM models can now be used for embedding, document ranking, and conversational agents. </span><span class="koboSpan" id="kobo.981.3">The market offers various language models from providers like OpenAI, Cohere, AI21 Labs, and Hugging Face. </span><span class="koboSpan" id="kobo.981.4">LLMs have evolved from the early days of being perceived as magic to becoming industrialized, seamless, multifunctional, and integrated components of broader AI pipelines.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.982.1">Let’s create a </span><a id="_idIndexMarker230"/><span class="koboSpan" id="kobo.983.1">tree index in two lines of code:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.984.1">from</span></span><span class="koboSpan" id="kobo.985.1"> llama_index.core </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.986.1">import</span></span><span class="koboSpan" id="kobo.987.1"> TreeIndex
tree_index = TreeIndex.from_documents(documents)
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.988.1">The code then checks the class we just created:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.989.1">print</span></span><span class="koboSpan" id="kobo.990.1">(</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.991.1">type</span></span><span class="koboSpan" id="kobo.992.1">(tree_index))
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.993.1">The output confirms that we are in the </span><code class="inlineCode"><span class="koboSpan" id="kobo.994.1">TreeIndex</span></code><span class="koboSpan" id="kobo.995.1"> class:</span></p>
    <pre class="programlisting con"><code class="hljs-con"><span class="koboSpan" id="kobo.996.1">&lt;class 'llama_index.core.indices.tree.base.TreeIndex'&gt;
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.997.1">We can now make our tree index the query engine:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.998.1">tree_query_engine = tree_index.as_query_engine(similarity_top_k=k, temperature=temp, num_output=mt)
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.999.1">The parameters of the LLM are those defined in the </span><em class="italic"><span class="koboSpan" id="kobo.1000.1">User input and query parameters</span></em><span class="koboSpan" id="kobo.1001.1"> section. </span><span class="koboSpan" id="kobo.1001.2">The code now calls the query, measures the time elapsed, and processes the response:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1002.1">import</span></span><span class="koboSpan" id="kobo.1003.1"> time
</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1004.1">import</span></span><span class="koboSpan" id="kobo.1005.1"> textwrap
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.1006.1"># Start the timer</span></span><span class="koboSpan" id="kobo.1007.1">
start_time = time.time()
response = tree_query_engine.query(user_input)
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.1008.1"># Stop the timer</span></span><span class="koboSpan" id="kobo.1009.1">
end_time = time.time()
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.1010.1"># Calculate and print the execution time</span></span><span class="koboSpan" id="kobo.1011.1">
elapsed_time = end_time - start_time
</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1012.1">print</span></span><span class="koboSpan" id="kobo.1013.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1014.1">f"Query execution time: </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1015.1">{elapsed_time:</span></span><span class="hljs-number"><span class="koboSpan" id="kobo.1016.1">.4</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1017.1">f}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1018.1"> seconds"</span></span><span class="koboSpan" id="kobo.1019.1">)
</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1020.1">print</span></span><span class="koboSpan" id="kobo.1021.1">(textwrap.fill(</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1022.1">str</span></span><span class="koboSpan" id="kobo.1023.1">(response), </span><span class="hljs-number"><span class="koboSpan" id="kobo.1024.1">100</span></span><span class="koboSpan" id="kobo.1025.1">))
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.1026.1">The query </span><a id="_idIndexMarker231"/><span class="koboSpan" id="kobo.1027.1">time and the response are both satisfactory:</span></p>
    <pre class="programlisting con"><code class="hljs-con"><span class="koboSpan" id="kobo.1028.1">Query execution time: 4.3360 seconds
Drones identify vehicles using computer vision technology related to object detection. </span><span class="koboSpan" id="kobo.1028.2">This
technology involves detecting instances of semantic objects of a certain class, such as vehicles, in
digital images and videos. </span><span class="koboSpan" id="kobo.1028.3">Drones can be equipped with object detection algorithms, such as YOLOv3
models trained on datasets like COCO, to detect vehicles in real-time by analyzing the visual data
captured by the drone's cameras.
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.1029.1">Let’s apply a performance metric to the output.</span></p>
    <h2 id="_idParaDest-91" class="heading-2"><span class="koboSpan" id="kobo.1030.1">Performance metric</span></h2>
    <p class="normal"><span class="koboSpan" id="kobo.1031.1">This performance metric</span><a id="_idIndexMarker232"/><span class="koboSpan" id="kobo.1032.1"> will calculate the cosine similarity defined in the </span><em class="italic"><span class="koboSpan" id="kobo.1033.1">Cosine similarity metric</span></em><span class="koboSpan" id="kobo.1034.1"> section between the user input and the response of our RAG pipeline:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.1035.1">similarity_score = calculate_cosine_similarity_with_embeddings(user_input, </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1036.1">str</span></span><span class="koboSpan" id="kobo.1037.1">(response))
</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1038.1">print</span></span><span class="koboSpan" id="kobo.1039.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1040.1">f"Cosine Similarity Score: </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1041.1">{similarity_score:</span></span><span class="hljs-number"><span class="koboSpan" id="kobo.1042.1">.3</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1043.1">f}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1044.1">"</span></span><span class="koboSpan" id="kobo.1045.1">)
</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1046.1">print</span></span><span class="koboSpan" id="kobo.1047.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1048.1">f"Query execution time: </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1049.1">{elapsed_time:</span></span><span class="hljs-number"><span class="koboSpan" id="kobo.1050.1">.4</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1051.1">f}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1052.1"> seconds"</span></span><span class="koboSpan" id="kobo.1053.1">)
performance=similarity_score/elapsed_time
</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1054.1">print</span></span><span class="koboSpan" id="kobo.1055.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1056.1">f"Performance metric: </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1057.1">{performance:</span></span><span class="hljs-number"><span class="koboSpan" id="kobo.1058.1">.4</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1059.1">f}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1060.1">"</span></span><span class="koboSpan" id="kobo.1061.1">)
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.1062.1">The output shows that although the quality of the response was satisfactory, the execution time was slow, which brings the performance metric down:</span></p>
    <pre class="programlisting con"><code class="hljs-con"><span class="koboSpan" id="kobo.1063.1">Cosine Similarity Score: 0.731
Query execution time: 4.3360 seconds
Performance metric: 0.1686
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.1064.1">Of course, the execution time depends on the server (power) and the data (noise). </span><span class="koboSpan" id="kobo.1064.2">As established earlier, the execution times might vary from one run to another, due to the stochastic algorithms used. </span><span class="koboSpan" id="kobo.1064.3">Also, when the dataset increases in volume, the execution times of all the indexing types may change.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.1065.1">The list index query engine may or may not be better in this case. </span><span class="koboSpan" id="kobo.1065.2">Let’s run it to find out.</span></p>
    <h1 id="_idParaDest-92" class="heading-1"><span class="koboSpan" id="kobo.1066.1">List index query engine</span></h1>
    <p class="normal"><span class="koboSpan" id="kobo.1067.1">Don’t think of</span><a id="_idIndexMarker233"/> <code class="inlineCode"><span class="koboSpan" id="kobo.1068.1">ListIndex</span></code><span class="koboSpan" id="kobo.1069.1"> as simply a list of nodes. </span><span class="koboSpan" id="kobo.1069.2">The query engine will process the user input and each document as a prompt for an LLM. </span><span class="koboSpan" id="kobo.1069.3">The LLM will evaluate the semantic similarity relationship between the documents and the query, thus implicitly ranking and selecting the most relevant nodes. </span><span class="koboSpan" id="kobo.1069.4">LlamaIndex will filter the documents based on the rankings obtained, and it can also take the task further by synthesizing information from multiple nodes and documents.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.1070.1">We can see that the selection process with an LLM is not rule-based. </span><span class="koboSpan" id="kobo.1070.2">Nothing is predefined, which means that the selection is prompt-based by combining the user input with a collection of documents. </span><span class="koboSpan" id="kobo.1070.3">The LLM evaluates each document in the list </span><em class="italic"><span class="koboSpan" id="kobo.1071.1">independently</span></em><span class="koboSpan" id="kobo.1072.1">, assigning a score based on its perceived relevance to the query. </span><span class="koboSpan" id="kobo.1072.2">This score isn’t relative to other documents; it’s a measure of how well the LLM thinks the current document answers the question. </span><span class="koboSpan" id="kobo.1072.3">Then, the top-k documents are retained by the query engine if we wish, as in the function used in this section.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.1073.1">Like the tree index, the list index can also be created in two lines of code:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1074.1">from</span></span><span class="koboSpan" id="kobo.1075.1"> llama_index.core </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1076.1">import</span></span><span class="koboSpan" id="kobo.1077.1"> ListIndex
list_index = ListIndex.from_documents(documents)
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.1078.1">The code verifies the class that we are using:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.1079.1">print</span></span><span class="koboSpan" id="kobo.1080.1">(</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1081.1">type</span></span><span class="koboSpan" id="kobo.1082.1">(list_index))
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.1083.1">The output confirms that we are in the </span><code class="inlineCode"><span class="koboSpan" id="kobo.1084.1">list</span></code><span class="koboSpan" id="kobo.1085.1"> class:</span></p>
    <pre class="programlisting con"><code class="hljs-con"><span class="koboSpan" id="kobo.1086.1">&lt;class 'llama_index.core.indices.list.base.SummaryIndex'&gt;
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.1087.1">The list index is a </span><code class="inlineCode"><span class="koboSpan" id="kobo.1088.1">SummaryIndex</span></code><span class="koboSpan" id="kobo.1089.1">, which shows the large amount of document summary optimization that is running under the hood! </span><span class="koboSpan" id="kobo.1089.2">We can now utilize our list index as a query engine in the seamless framework provided by LlamaIndex:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.1090.1">list_query_engine = list_index.as_query_engine(similarity_top_k=k, temperature=temp, num_output=mt)
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.1091.1">The LLM parameters remain unchanged so that we can compare the indexing types. </span><span class="koboSpan" id="kobo.1091.2">We can now run our query, wrap the response up, and display the output:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.1092.1">#start the timer</span></span><span class="koboSpan" id="kobo.1093.1">
start_time = time.time()
response = list_query_engine.query(user_input)
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.1094.1"># Stop the timer</span></span><span class="koboSpan" id="kobo.1095.1">
end_time = time.time()
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.1096.1"># Calculate and print the execution time</span></span><span class="koboSpan" id="kobo.1097.1">
elapsed_time = end_time - start_time
</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1098.1">print</span></span><span class="koboSpan" id="kobo.1099.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1100.1">f"Query execution time: </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1101.1">{elapsed_time:</span></span><span class="hljs-number"><span class="koboSpan" id="kobo.1102.1">.4</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1103.1">f}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1104.1"> seconds"</span></span><span class="koboSpan" id="kobo.1105.1">)
</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1106.1">print</span></span><span class="koboSpan" id="kobo.1107.1">(textwrap.fill(</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1108.1">str</span></span><span class="koboSpan" id="kobo.1109.1">(response), </span><span class="hljs-number"><span class="koboSpan" id="kobo.1110.1">100</span></span><span class="koboSpan" id="kobo.1111.1">))
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.1112.1">The output shows a longer execution time but an acceptable response:</span></p>
    <pre class="programlisting con"><code class="hljs-con"><span class="koboSpan" id="kobo.1113.1">Query execution time: 16.3123 seconds
Drones can identify vehicles through computer vision systems that process image data captured by
cameras mounted on the drones. </span><span class="koboSpan" id="kobo.1113.2">These systems use techniques like object recognition and detection to
analyze the images and identify specific objects, such as vehicles, based on predefined models or
features. </span><span class="koboSpan" id="kobo.1113.3">By processing the visual data in real-time, drones can effectively identify vehicles in
their surroundings.
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.1114.1">The execution </span><a id="_idIndexMarker234"/><span class="koboSpan" id="kobo.1115.1">time is longer because the query goes through a list, not an optimized tree. </span><span class="koboSpan" id="kobo.1115.2">However, we cannot draw conclusions from this because each project or even each sub-task of a project has different requirements. </span><span class="koboSpan" id="kobo.1115.3">Next, let’s apply the performance metric.</span></p>
    <h2 id="_idParaDest-93" class="heading-2"><span class="koboSpan" id="kobo.1116.1">Performance metric</span></h2>
    <p class="normal"><span class="koboSpan" id="kobo.1117.1">We will use the </span><a id="_idIndexMarker235"/><span class="koboSpan" id="kobo.1118.1">cosine similarity, as we did for the tree index, to evaluate the similarity score:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.1119.1">similarity_score = calculate_cosine_similarity_with_embeddings(user_input, </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1120.1">str</span></span><span class="koboSpan" id="kobo.1121.1">(response))
</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1122.1">print</span></span><span class="koboSpan" id="kobo.1123.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1124.1">f"Cosine Similarity Score: </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1125.1">{similarity_score:</span></span><span class="hljs-number"><span class="koboSpan" id="kobo.1126.1">.3</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1127.1">f}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1128.1">"</span></span><span class="koboSpan" id="kobo.1129.1">)
</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1130.1">print</span></span><span class="koboSpan" id="kobo.1131.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1132.1">f"Query execution time: </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1133.1">{elapsed_time:</span></span><span class="hljs-number"><span class="koboSpan" id="kobo.1134.1">.4</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1135.1">f}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1136.1"> seconds"</span></span><span class="koboSpan" id="kobo.1137.1">)
performance=similarity_score/elapsed_time
</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1138.1">print</span></span><span class="koboSpan" id="kobo.1139.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1140.1">f"Performance metric: </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1141.1">{performance:</span></span><span class="hljs-number"><span class="koboSpan" id="kobo.1142.1">.4</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1143.1">f}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1144.1">"</span></span><span class="koboSpan" id="kobo.1145.1">)
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.1146.1">The performance metric is lower than the tree index due to the longer execution time:</span></p>
    <pre class="programlisting con"><code class="hljs-con"><span class="koboSpan" id="kobo.1147.1">Cosine Similarity Score: 0.775
Query execution time: 16.3123 seconds
Performance metric: 0.0475
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.1148.1">Again, remember that this execution time may vary from one run to another, due to the stochastic algorithms implemented.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.1149.1">If we look back at the </span><a id="_idIndexMarker236"/><span class="koboSpan" id="kobo.1150.1">performance metric of each indexing type, we can see that, for the moment, the vector store index was the fastest. </span><span class="koboSpan" id="kobo.1150.2">Once again, let’s not jump to conclusions. </span><span class="koboSpan" id="kobo.1150.3">Each project might produce surprising results, depending on the type and complexity of the data processed. </span><span class="koboSpan" id="kobo.1150.4">Next, let’s examine the keyword index.</span></p>
    <h1 id="_idParaDest-94" class="heading-1"><span class="koboSpan" id="kobo.1151.1">Keyword index query engine</span></h1>
    <p class="normal"><code class="inlineCode"><span class="koboSpan" id="kobo.1152.1">KeywordTableIndex</span></code><span class="koboSpan" id="kobo.1153.1"> is a </span><a id="_idIndexMarker237"/><span class="koboSpan" id="kobo.1154.1">type of index in LlamaIndex, designed to extract keywords from your documents and organize them in a table-like structure. </span><span class="koboSpan" id="kobo.1154.2">This structure makes it easier to query and retrieve relevant information based on specific keywords or topics. </span><span class="koboSpan" id="kobo.1154.3">Once again, don’t think about this function as a simple list of extracted keywords. </span><span class="koboSpan" id="kobo.1154.4">The extracted keywords are organized into a table-like format where each keyword is associated with an ID that points to the related nodes.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.1155.1">The program creates the keyword index in two lines of code:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1156.1">from</span></span><span class="koboSpan" id="kobo.1157.1"> llama_index.core </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1158.1">import</span></span><span class="koboSpan" id="kobo.1159.1"> KeywordTableIndex
keyword_index = KeywordTableIndex.from_documents(documents)
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.1160.1">Let’s extract the data and create a pandas DataFrame to see how the index is structured:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.1161.1"># Extract data for DataFrame</span></span><span class="koboSpan" id="kobo.1162.1">
data = []
</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1163.1">for</span></span><span class="koboSpan" id="kobo.1164.1"> keyword, doc_ids </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1165.1">in</span></span><span class="koboSpan" id="kobo.1166.1"> keyword_index.index_struct.table.items():
    </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1167.1">for</span></span><span class="koboSpan" id="kobo.1168.1"> doc_id </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1169.1">in</span></span><span class="koboSpan" id="kobo.1170.1"> doc_ids:
        data.append({</span><span class="hljs-string"><span class="koboSpan" id="kobo.1171.1">"Keyword"</span></span><span class="koboSpan" id="kobo.1172.1">: keyword, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1173.1">"Document ID"</span></span><span class="koboSpan" id="kobo.1174.1">: doc_id})
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.1175.1"># Create the DataFrame</span></span><span class="koboSpan" id="kobo.1176.1">
df = pd.DataFrame(data)
df
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.1177.1">The output shows that each keyword is associated with an ID that contains a document or a summary, depending on the way LlamaIndex optimizes the index:</span></p>
    <figure class="mediaobject"><span class="koboSpan" id="kobo.1178.1"><img src="../Images/B31169_03_06.png" alt="A screenshot of a computer  Description automatically generated"/></span></figure>
    <p class="packt_figref"><span class="koboSpan" id="kobo.1179.1">Figure 3.6: Keywords linked to document IDs in a DataFrame</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.1180.1">We now define the keyword index as the query engine:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.1181.1">keyword_query_engine = keyword_index.as_query_engine(similarity_top_k=k, temperature=temp, num_output=mt)
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.1182.1">Let’s run</span><a id="_idIndexMarker238"/><span class="koboSpan" id="kobo.1183.1"> the keyword query and see how well and fast it can produce a response:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1184.1">import</span></span><span class="koboSpan" id="kobo.1185.1"> time
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.1186.1"># Start the timer</span></span><span class="koboSpan" id="kobo.1187.1">
start_time = time.time()
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.1188.1"># Execute the query (using .query() method)</span></span><span class="koboSpan" id="kobo.1189.1">
response = keyword_query_engine.query(user_input)
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.1190.1"># Stop the timer</span></span><span class="koboSpan" id="kobo.1191.1">
end_time = time.time()
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.1192.1"># Calculate and print the execution time</span></span><span class="koboSpan" id="kobo.1193.1">
elapsed_time = end_time - start_time
</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1194.1">print</span></span><span class="koboSpan" id="kobo.1195.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1196.1">f"Query execution time: </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1197.1">{elapsed_time:</span></span><span class="hljs-number"><span class="koboSpan" id="kobo.1198.1">.4</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1199.1">f}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1200.1"> seconds"</span></span><span class="koboSpan" id="kobo.1201.1">)
</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1202.1">print</span></span><span class="koboSpan" id="kobo.1203.1">(textwrap.fill(</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1204.1">str</span></span><span class="koboSpan" id="kobo.1205.1">(response), </span><span class="hljs-number"><span class="koboSpan" id="kobo.1206.1">100</span></span><span class="koboSpan" id="kobo.1207.1">))
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.1208.1">The output is satisfactory, as well as the execution time:</span></p>
    <pre class="programlisting con"><code class="hljs-con"><span class="koboSpan" id="kobo.1209.1">Query execution time: 2.4282 seconds
Drones can identify vehicles through various means such as visual recognition using onboard cameras, sensors, and image processing algorithms. </span><span class="koboSpan" id="kobo.1209.2">They can also utilize technologies like artificial intelligence and machine learning to analyze and classify vehicles based on their shapes, sizes, and movement patterns. </span><span class="koboSpan" id="kobo.1209.3">Additionally, drones can be equipped with specialized software for object detection and tracking to identify vehicles accurately.
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.1210.1">We can now</span><a id="_idIndexMarker239"/><span class="koboSpan" id="kobo.1211.1"> measure the output with a performance metric.</span></p>
    <h2 id="_idParaDest-95" class="heading-2"><span class="koboSpan" id="kobo.1212.1">Performance metric</span></h2>
    <p class="normal"><span class="koboSpan" id="kobo.1213.1">The code runs the same metric as for the</span><a id="_idIndexMarker240"/><span class="koboSpan" id="kobo.1214.1"> tree and list index:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.1215.1">similarity_score = calculate_cosine_similarity_with_embeddings(user_input, </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1216.1">str</span></span><span class="koboSpan" id="kobo.1217.1">(response))
</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1218.1">print</span></span><span class="koboSpan" id="kobo.1219.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1220.1">f"Cosine Similarity Score: </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1221.1">{similarity_score:</span></span><span class="hljs-number"><span class="koboSpan" id="kobo.1222.1">.3</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1223.1">f}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1224.1">"</span></span><span class="koboSpan" id="kobo.1225.1">)
</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1226.1">print</span></span><span class="koboSpan" id="kobo.1227.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1228.1">f"Query execution time: </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1229.1">{elapsed_time:</span></span><span class="hljs-number"><span class="koboSpan" id="kobo.1230.1">.4</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1231.1">f}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1232.1"> seconds"</span></span><span class="koboSpan" id="kobo.1233.1">)
performance=similarity_score/elapsed_time
</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1234.1">print</span></span><span class="koboSpan" id="kobo.1235.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1236.1">f"Performance metric: </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1237.1">{performance:</span></span><span class="hljs-number"><span class="koboSpan" id="kobo.1238.1">.4</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1239.1">f}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1240.1">"</span></span><span class="koboSpan" id="kobo.1241.1">)
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.1242.1">The performance metric is acceptable:</span></p>
    <pre class="programlisting con"><code class="hljs-con"><span class="koboSpan" id="kobo.1243.1">Cosine Similarity Score: 0.801
Query execution time: 2.4282 seconds
Performance metric: 0.3299
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.1244.1">Once again, we can draw no conclusions. </span><span class="koboSpan" id="kobo.1244.2">The results of all the indexing types are relatively satisfactory. </span><span class="koboSpan" id="kobo.1244.3">However, each project comes with its dataset complexity and machine power availability. </span><span class="koboSpan" id="kobo.1244.4">Also, the execution times may vary from one run to another, due to the stochastic algorithms employed.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.1245.1">With that, we have reviewed some of the main indexing types and retrieval strategies. </span><span class="koboSpan" id="kobo.1245.2">Let’s summarize the chapter and move on to multimodal modular retrieval and generation strategies.</span></p>
    <h1 id="_idParaDest-96" class="heading-1"><span class="koboSpan" id="kobo.1246.1">Summary</span></h1>
    <p class="normal"><span class="koboSpan" id="kobo.1247.1">This chapter explored the transformative impact of index-based search on RAG and introduced a pivotal advancement: </span><em class="italic"><span class="koboSpan" id="kobo.1248.1">full traceability</span></em><span class="koboSpan" id="kobo.1249.1">. </span><span class="koboSpan" id="kobo.1249.2">The documents become nodes that contain chunks of data, with the source of a query leading us all the way back to the original data. </span><span class="koboSpan" id="kobo.1249.3">Indexes also increase the speed of retrievals, which is critical as the volume of datasets increases. </span><span class="koboSpan" id="kobo.1249.4">Another pivotal advance is the integration of technologies such as LlamaIndex, Deep Lake, and OpenAI, which are emerging in another era of AI. </span><span class="koboSpan" id="kobo.1249.5">The most advanced AI models, such as OpenAI GPT-4o, Hugging Face, and Cohere, are becoming seamless </span><em class="italic"><span class="koboSpan" id="kobo.1250.1">components</span></em><span class="koboSpan" id="kobo.1251.1"> in a RAG-driven generative AI pipeline, like GPUs in a computer. </span></p>
    <p class="normal"><span class="koboSpan" id="kobo.1252.1">We started by detailing the architecture of an index-based RAG generative AI pipeline, illustrating how these sophisticated technologies can be seamlessly integrated to boost the creation of advanced indexing and retrieval systems. </span><span class="koboSpan" id="kobo.1252.2">The complexity of AI implementation is changing the way we organize separate pipelines and functionality for a team working in parallel on projects that scale and involve large amounts of data. </span><span class="koboSpan" id="kobo.1252.3">We saw how every response generated can be traced back to its source, providing clear visibility into the origins and accuracy of the information used. </span><span class="koboSpan" id="kobo.1252.4">We illustrated the advanced RAG technology implemented through drone technology.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.1253.1">Throughout the chapter, we introduced the essential tools to build these systems, including vector stores, datasets, chunking, embedding, node creation, ranking, and indexing methods. </span><span class="koboSpan" id="kobo.1253.2">We implemented the LlamaIndex framework, Deep Lake vector stores, and OpenAI’s models. </span><span class="koboSpan" id="kobo.1253.3">We also built a Python program that collects data and adds critical metadata to pinpoint the origin of every chunk of data in a dataset. </span><span class="koboSpan" id="kobo.1253.4">We highlighted the pivotal role of indexes (vector, tree, list, and keyword types) in giving us greater control over generative AI applications, enabling precise adjustments and improvements. </span></p>
    <p class="normal"><span class="koboSpan" id="kobo.1254.1">We then thoroughly examined indexed-based RAG through detailed walkthroughs in Python notebooks, guiding you through setting up vector stores, conducting advanced queries, and ensuring the traceability of AI-generated responses. </span><span class="koboSpan" id="kobo.1254.2">We introduced metrics based on the quality of a response and the time elapsed to obtain it. </span><span class="koboSpan" id="kobo.1254.3">Exploring drone technology with LLMs showed us the new skillsets required to build solid AI pipelines, and we learned how drone technology involves computer vision and, thus, multimodal nodes.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.1255.1">In the upcoming chapter, we include multimodal data in our datasets and expand multimodular RAG.</span></p>
    <h1 id="_idParaDest-97" class="heading-1"><span class="koboSpan" id="kobo.1256.1">Questions</span></h1>
    <p class="normal"><span class="koboSpan" id="kobo.1257.1">Answer the following questions with </span><em class="italic"><span class="koboSpan" id="kobo.1258.1">Yes</span></em><span class="koboSpan" id="kobo.1259.1"> or </span><em class="italic"><span class="koboSpan" id="kobo.1260.1">No</span></em><span class="koboSpan" id="kobo.1261.1">:</span></p>
    <ul>
      <li class="bulletList"><span class="koboSpan" id="kobo.1262.1">Do indexes increase precision and speed in retrieval-augmented generative AI? </span></li>
      <li class="bulletList"><span class="koboSpan" id="kobo.1263.1">Can indexes offer traceability for RAG outputs? </span></li>
      <li class="bulletList"><span class="koboSpan" id="kobo.1264.1">Is index-based search slower than vector-based search for large datasets? </span></li>
      <li class="bulletList"><span class="koboSpan" id="kobo.1265.1">Does LlamaIndex integrate seamlessly with Deep Lake and OpenAI? </span></li>
      <li class="bulletList"><span class="koboSpan" id="kobo.1266.1">Are tree, list, vector, and keyword indexes the only types of indexes? </span></li>
      <li class="bulletList"><span class="koboSpan" id="kobo.1267.1">Does the keyword index rely on semantic understanding to retrieve data? </span></li>
      <li class="bulletList"><span class="koboSpan" id="kobo.1268.1">Is LlamaIndex capable of automatically handling chunking and embedding? </span></li>
      <li class="bulletList"><span class="koboSpan" id="kobo.1269.1">Are metadata enhancements crucial for ensuring the traceability of RAG-generated outputs? </span></li>
      <li class="bulletList"><span class="koboSpan" id="kobo.1270.1">Can real-time updates easily be applied to an index-based search system? </span></li>
      <li class="bulletList"><span class="koboSpan" id="kobo.1271.1">Is cosine similarity a metric used in this chapter to evaluate query accuracy?</span></li>
    </ul>
    <h1 id="_idParaDest-98" class="heading-1"><span class="koboSpan" id="kobo.1272.1">References</span></h1>
    <ul>
      <li class="bulletList"><span class="koboSpan" id="kobo.1273.1">LlamaIndex: </span><a href="https://docs.llamaindex.ai/en/stable/"><span class="url"><span class="koboSpan" id="kobo.1274.1">https://docs.llamaindex.ai/en/stable/</span></span></a></li>
      <li class="bulletList"><span class="koboSpan" id="kobo.1275.1">Activeloop Deep Lake: </span><a href="https://docs.activeloop.ai/"><span class="url"><span class="koboSpan" id="kobo.1276.1">https://docs.activeloop.ai/</span></span></a></li>
      <li class="bulletList"><span class="koboSpan" id="kobo.1277.1">OpenAI: </span><a href="https://platform.openai.com/docs/overview"><span class="url"><span class="koboSpan" id="kobo.1278.1">https://platform.openai.com/docs/overview</span></span></a></li>
    </ul>
    <h1 id="_idParaDest-99" class="heading-1"><span class="koboSpan" id="kobo.1279.1">Further reading</span></h1>
    <ul>
      <li class="bulletList"><span class="koboSpan" id="kobo.1280.1">High-Level Concepts (RAG), LlamaIndex: </span><a href="https://docs.llamaindex.ai/en/stable/getting_started/concepts/"><span class="url"><span class="koboSpan" id="kobo.1281.1">https://docs.llamaindex.ai/en/stable/getting_started/concepts/</span></span></a></li>
    </ul>
    <h1 id="_idParaDest-100" class="heading-1"><span class="koboSpan" id="kobo.1282.1">Join our community on Discord</span></h1>
    <p class="normal"><span class="koboSpan" id="kobo.1283.1">Join our community’s Discord space for discussions with the author and other readers:</span></p>
    <p class="normal"><a href="https://www.packt.link/rag"><span class="url"><span class="koboSpan" id="kobo.1284.1">https://www.packt.link/rag</span></span></a></p>
    <p class="normal"><span class="koboSpan" id="kobo.1285.1"><img src="../Images/QR_Code50409000288080484.png" alt=""/></span></p>
  </div>
</body></html>