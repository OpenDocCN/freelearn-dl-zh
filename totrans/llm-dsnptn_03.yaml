- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data Augmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data augmentation plays a pivotal role in enhancing the performance and generalization
    capabilities of LLMs. By artificially expanding the training dataset, we can expose
    our models to a wider range of linguistic variations and contexts, improving their
    ability to handle diverse inputs and generate more coherent and contextually appropriate
    outputs.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of LLMs, data augmentation takes on unique challenges and opportunities.
    Unlike **image data**, where simple transformations such as rotation or flipping
    can create valid new samples, **text data** requires more nuanced approaches to
    maintain semantic integrity and linguistic coherence. The main goals of data augmentation
    for LLMs include increasing dataset size and diversity, addressing data imbalance
    and bias, improving model robustness to variations in input, and enhancing generalization
    to unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 3**.1*, I illustrate the key aspects of data augmentation.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – Key elements of data augmentation](img/B31249_03_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 – Key elements of data augmentation
  prefs: []
  type: TYPE_NORMAL
- en: There are three main components, namely **Techniques**, **Considerations**,
    and **Evaluation**. Each has specific sub-components, which we’ll cover in detail
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you’ll have learned about the **data augmentation**
    pattern in depth, from increasing the diversity of your training dataset to maintaining
    its integrity:'
  prefs: []
  type: TYPE_NORMAL
- en: Text data augmentation techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leveraging existing LLMs for data generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multilingual data augmentation strategies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Semantic preservation in text augmentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Balancing augmentation and data quality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating the impact of data augmentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text data augmentation techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Text data augmentation encompasses a wide range of techniques, from simple word-level
    manipulations to more complex semantic transformations.
  prefs: []
  type: TYPE_NORMAL
- en: Synonym replacement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This technique involves replacing words in the original text with their synonyms.
    We can use **WordNet**, a lexical database for the English language, to find synonyms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The `synonym_replacement` function takes a text input and replaces a specified
    number (the default is 1) of words with their synonyms. The default value of 1
    is chosen to minimize text alteration, preserving meaning and readability while
    allowing easy experimentation. You can increase this number if you want more replacements.
  prefs: []
  type: TYPE_NORMAL
- en: The function splits the text into words, creates a list of unique alphanumeric
    words, shuffles this list, and then iterates through it. For each word, it attempts
    to find synonyms using an undefined `get_synonyms` function. If synonyms are found,
    it randomly selects one and replaces all occurrences of the original word in the
    text. The function keeps track of how many words have been replaced and stops
    when it reaches the specified number. Finally, it rejoins the modified words into
    a single string and returns it.
  prefs: []
  type: TYPE_NORMAL
- en: Back-translation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This method involves translating the text to another language and then back
    to the original language. It’s particularly effective for introducing natural
    variations in sentence structure and word choice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Text generation with T5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Text-To-Text Transfer Transformer** (**T5**) model, developed by Google
    Research, is a versatile **natural language processing** (**NLP**) model based
    on the transformer architecture. Its key innovation is framing all NLP tasks as
    text-to-text problems, allowing it to handle multiple tasks without task-specific
    architectures. Pre-trained on a large web text corpus using a “span corruption”
    objective, T5 is available in various sizes and has demonstrated powerful performance
    across a wide range of NLP tasks.
  prefs: []
  type: TYPE_NORMAL
- en: T5 handles a wide range of text-based tasks by framing them all as text-to-text
    problems. This means that irrespective of the task, whether it’s summarization,
    translation, question answering, or classification, both the input and output
    are treated as text. This unified approach allows T5 to perform various tasks
    without needing task-specific modifications, making it highly adaptable for different
    use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'When it comes to data augmentation, T5 plays a key role by generating variations
    of existing text data, which is essential for expanding and diversifying datasets.
    Data augmentation is especially valuable when training machine learning models,
    as it helps them generalize better by exposing them to a wider variety of examples,
    reducing overfitting and improving robustness. Here’s how T5 aids in data augmentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Paraphrasing**: T5 can rephrase sentences while maintaining their original
    meaning. For example, if the input is “The movie was boring,” T5 could generate
    a paraphrased version such as “The film was dull.” This variety in expression
    provides additional examples for a model to learn from, helping it generalize
    better to different ways of phrasing the same idea.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Synonym replacement**: T5 can replace words with their synonyms, creating
    slight variations in meaning while retaining the overall sentiment or context.
    For instance, from “The movie was long and tedious,” T5 might generate “The film
    was lengthy and boring.” This simple modification increases the diversity of the
    dataset, offering more training examples for models that rely on understanding
    slight variations in language.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sentiment-based transformation**: T5 can also transform the sentiment of
    a sentence. For example, given a negative sentence such as “The movie was very
    disappointing,” T5 can generate a neutral or positive version, such as “The movie
    had a slow start but improved later.” This capability allows for the creation
    of multiple examples across different sentiment categories, which is particularly
    useful in tasks such as sentiment analysis, where a model needs to distinguish
    between positive, neutral, and negative sentiments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text expansion**: T5 can take a short sentence and expand it by adding more
    context, details, or descriptions. For instance, from the sentence “The event
    was great,” T5 could generate a more detailed version such as “The event was great,
    with excellent speakers and engaging discussions.” By adding more context, T5
    provides additional variations of the sentence that help in training models to
    handle more complex inputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can use a pre-trained T5 model to generate variations of input text. This
    method is particularly powerful as it can produce more diverse and contextually
    rich augmentations. Let’s see this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This function takes a text input, a pre-trained T5 model, its tokenizer, and
    the number of paraphrases to generate (the default is 1). The default of 1 return
    sequence is chosen for simplicity, but you can request multiple paraphrases by
    increasing this value.
  prefs: []
  type: TYPE_NORMAL
- en: The function encodes the input text with a `"paraphrase:"` prefix, limiting
    it to `512` tokens. It then uses the model to generate paraphrases with a maximum
    length of `150` tokens. The generation process uses beam search with 5 beams,
    prevents repetition of 2-grams, and applies `50`) and `0.95`) `512`, `150`, `5`,
    `2`, `50`, `0.95`) can also be adjusted based on specific use cases to control
    the length, diversity, and quality of the generated paraphrases.
  prefs: []
  type: TYPE_NORMAL
- en: The function decodes and returns the generated paraphrases, skipping any special
    tokens added during the process.
  prefs: []
  type: TYPE_NORMAL
- en: Using temperature control as an additional parameter in language generation
    systems allows fine-tuning the balance between creativity and coherence. Temperature
    is a scalar value, typically ranging from 0 to 1, that influences the probability
    distribution over the next token during generation. Low values (close to 0) concentrate
    the distribution, making the model more deterministic and coherent but potentially
    repetitive or conservative. High values (close to 1) flatten the distribution,
    increasing randomness and diversity at the cost of coherence.
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging existing LLMs for data generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most powerful approaches to data augmentation for LLMs is to use
    existing models to generate new training examples. This technique, often referred
    to as **self-supervised learning** or **model-based data augmentation**, allows
    us to create vast amounts of diverse, high-quality training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll explore how to use **GPT-4o** and the **OpenAI API** for data generation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This function sends a single user message containing the provided prompt for
    a chat completion request. It limits the response to a maximum of `150` tokens,
    which balances between getting a substantive response and controlling the output
    length. The `n` parameter, set to `num_samples`, determines the number of alternative
    completions to generate. A temperature of `0.7` is used, which provides a balance
    between creativity and coherence in the generated text: high values increase randomness,
    while low values would make the output more deterministic. The function then extracts
    and returns the content of each generated completion, stripping any leading or
    trailing whitespace. These parameters (`150` tokens, `0.7` temperature) can be
    adjusted based on specific needs for output length and creativity.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When using this approach, we need to consider the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Prompt engineering**: Crafting effective prompts is needed for generating
    relevant and diverse samples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Quality control**: Implement filtering mechanisms to ensure the generated
    data meets your quality standards.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Diversity**: Use temperature and top-p sampling to control the randomness
    and diversity of generated samples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ve explored data augmentation techniques using GPT-4o and examined essential
    considerations. Now, let’s turn our attention to strategies for multilingual data
    augmentation.
  prefs: []
  type: TYPE_NORMAL
- en: Multilingual data augmentation strategies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For LLMs designed to handle multiple languages, multilingual data augmentation
    is essential. We can adapt our previous techniques to work across languages.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-lingual back-translation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Translate the text into multiple languages before translating it back to the
    original language:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The `cross_lingual_back_translation` function takes a text input and generates
    augmented versions of it by first translating it into multiple target languages
    (defaulting to French, German, and Spanish) and then back to English. The function
    uses the `Translator` object to perform these translations, storing each back-translated
    version in a list, which is returned as the output.
  prefs: []
  type: TYPE_NORMAL
- en: Multilingual T5 augmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can use a multilingual T5 model to generate paraphrases in different languages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The `multilingual_t5_augmentation` function uses a T5 model to augment a given
    text by translating it into multiple target languages (defaulting to French, German,
    and Spanish). For each target language, it encodes the text with a prompt for
    translation, generates the translated output using the model, and decodes the
    result. The translated texts are collected in a list and returned as the augmented
    versions of the original text.
  prefs: []
  type: TYPE_NORMAL
- en: Semantic preservation in text augmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Maintaining semantic integrity is crucial when augmenting data for LLMs. We
    must ensure that our techniques don’t alter the original meaning of the text.
  prefs: []
  type: TYPE_NORMAL
- en: Use of sentence embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By comparing the **sentence embeddings** of the original and augmented texts,
    you can ensure **semantic similarity**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We define two functions for measuring and filtering text based on semantic
    similarity:'
  prefs: []
  type: TYPE_NORMAL
- en: '`semantic_similarity(original, augmented, model)` calculates the semantic similarity
    between two texts using the cosine similarity of their embeddings. It uses a provided
    model (probably a sentence embedding model) to encode the original and augmented
    texts into vector representations. The cosine similarity between these vectors
    is then computed, resulting in a value between -1 and 1, where 1 indicates perfect
    similarity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`filter_by_semantic_similarity(original, augmented_list, model, threshold=0.8)`
    filters a list of augmented texts based on their semantic similarity to the original.
    The `semantic_similarity` function compares each augmented text with the original.
    The default threshold is set to `0.8`: by default, it will keep only the augmented
    texts that have a similarity of `0.8` or higher to the original. This threshold
    is commonly used in NLP tasks, as it typically indicates a strong semantic similarity
    while allowing some variation. It can be adjusted based on how strict or lenient
    you want the filtering to be: a higher threshold will result in more similar (but
    possibly fewer) augmentations; a lower threshold will allow more diverse (but
    potentially less relevant) augmentations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contextual word embeddings for synonym replacement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can use **contextual word embeddings** to find more appropriate synonyms
    based on the context. Contextual word embeddings refer to the use of word representations
    generated by language models that capture the meaning of a word within its specific
    sentence or passage, rather than treating the word as having a fixed meaning.
    Unlike traditional static embeddings where a word has the same vector regardless
    of context, contextual embeddings assign different vectors to the same word depending
    on its surrounding words. This allows for more accurate synonym replacement as
    the chosen synonym aligns not only with the dictionary meaning but also with how
    the word is used in a particular context. For example, the word “bank” in “river
    bank” versus “savings bank” would be represented differently, leading to contextually
    appropriate synonym suggestions such as “shore” or “financial institution,” respectively.
    The following code snippet shows how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This function performs context-aware word replacement using a language model:'
  prefs: []
  type: TYPE_NORMAL
- en: It takes a text input, a pre-trained language model, its tokenizer, and the
    number of words to replace (the default is 1).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The text is split into words, and a copy is made for modification.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The function iterates `n` times (the default is 1). It does the following each
    time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Randomly selects a word index
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Tokenizes the entire text
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Runs it through the model to get contextualized embeddings
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Extracts the embedding of the chosen word
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Finds similar words based on this embedding (using an undefined `find_similar_words`
    function)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If similar words are found, it randomly chooses one to replace the original
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, it joins the modified words back into a string and returns it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The default `n`=1 is chosen to make minimal changes while still introducing
    variation. This preserves most of the original meaning and structure. You can
    increase `n` to get more replacements, but higher values might alter the text’s
    meaning more significantly.
  prefs: []
  type: TYPE_NORMAL
- en: This method is more context-aware than simple synonym replacement as it considers
    the word’s usage in the full text when finding replacements. The exact behavior
    will depend on the model and tokenizer used, as well as the implementation of
    the `find_similar_words` function.
  prefs: []
  type: TYPE_NORMAL
- en: Balancing augmentation and data quality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While data augmentation can significantly improve LLM performance, we need to
    strike a balance between quantity and quality.
  prefs: []
  type: TYPE_NORMAL
- en: You should limit the proportion of augmented data in your training set. A common
    practice is to start with a 1:1 ratio of original to augmented data and adjust
    based on model performance.
  prefs: []
  type: TYPE_NORMAL
- en: Quality filtering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can implement quality checks to filter out low-quality augmented samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Human-in-the-loop validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For critical applications, incorporate human validation into your augmentation
    pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: '**Human-in-the-loop** (**HITL**) validation is a control mechanism used in
    AI pipelines where humans are deliberately inserted into automated workflows to
    ensure correctness, especially in tasks involving subjective judgment, sensitive
    content, or critical decision-making. This is particularly important in applications
    where data quality directly affects safety, fairness, or compliance—for example,
    healthcare diagnostics, legal document analysis, or autonomous systems. In the
    context of data augmentation, where the goal is to expand the training dataset
    by generating variations of existing samples, HITL is used to validate whether
    the generated samples are coherent, accurate, and aligned with the intended label
    or task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This function is designed to manually validate a list of augmented text samples
    by soliciting binary feedback—yes or no—from a human operator. Its presence within
    an augmentation pipeline acknowledges that not all automatically generated data
    can be trusted at face value. The decision to retain or discard a given sample
    is made interactively, reinforcing human oversight in tasks where semantic integrity
    is non-negotiable.
  prefs: []
  type: TYPE_NORMAL
- en: Each iteration of the function’s loop represents a decision point. The human
    validator is shown the generated text and asked to assess whether it meets the
    expected criteria. These criteria are typically based on task-specific requirements
    such as grammaticality, semantic equivalence to the original data, tone appropriateness,
    or domain alignment. For example, in a medical text classification task, a paraphrased
    sentence must preserve all critical clinical entities. A slight shift in terminology
    introduced by an augmentation technique could mislead the model if not caught
    during validation. This is where human evaluation becomes indispensable.
  prefs: []
  type: TYPE_NORMAL
- en: The logic behind converting the input to lowercase is to handle inconsistent
    user input. Whether the user types `Y`, `y`, or any other casing, the comparison
    becomes case-agnostic. Only if the input is equivalent to `y` does the function
    accept the sample. This binary check is deliberately strict to prevent ambiguous
    approvals. The rejected samples are silently discarded and not logged or returned,
    implying that any further inspection or correction of rejected samples would need
    to be implemented separately.
  prefs: []
  type: TYPE_NORMAL
- en: The function concludes by returning a list of only those samples that have been
    explicitly validated. This output can then be used to expand the training dataset
    with higher confidence in the integrity of the new data points. Importantly, this
    approach does not replace automated quality checks but supplements them in high-stakes
    applications. HITL validation is particularly useful when deploying models in
    environments where false positives or negatives carry high costs, such as legal
    recommendation systems, fraud detection, or autonomous navigation. The manual
    validation process helps to mitigate risks that stem from over-reliance on generative
    augmentation methods that lack explicit semantic guarantees.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a larger system, this kind of function would usually be embedded in a broader
    workflow where automated filters screen out obviously low-quality or irrelevant
    augmentations first. The human validator would only evaluate the borderline or
    high-impact cases. For operational efficiency, the interaction would typically
    be handled via a web interface or integrated annotation tool rather than a command-line
    prompt. However, the function demonstrates the principle in its simplest form:
    human judgment is used as the final arbiter of quality before incorporating augmented
    data into model training.'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the impact of data augmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To assess the effectiveness of our data augmentation techniques, we need to
    assess their impact on LLM performance.
  prefs: []
  type: TYPE_NORMAL
- en: Perplexity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can measure a model’s perplexity (see [*Chapter 2*](B31249_02.xhtml#_idTextAnchor035))
    on a held-out test set before and after data augmentation to assess whether it
    has improved the model’s ability to predict unseen text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This function, `evaluate_perplexity`, calculates the perplexity of a language
    model on a given test dataset. Here’s a breakdown:'
  prefs: []
  type: TYPE_NORMAL
- en: It takes a pre-trained language model, its tokenizer, and a test dataset as
    input.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model is set to evaluation mode to disable dropout and other training-specific
    behavior.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It initializes variables to track the total loss and total number of tokens
    processed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each text in the test data, the following is carried out:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The text is tokenized and converted to tensors.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The model processes the input, calculating the loss.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The loss is accumulated, weighted by the number of tokens in the input.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After processing all texts, it calculates the perplexity using the following
    formula: `exp(total_loss /` `total_tokens)`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This implementation uses the model in a zero-shot manner, treating each input
    as both the context and the target to predict. The use of `torch.no_grad()` ensures
    that no gradients are computed, making the evaluation more efficient.
  prefs: []
  type: TYPE_NORMAL
- en: This function assumes the model and data are compatible (i.e., the model can
    handle the maximum sequence length of the data). In practice, you might need to
    add checks or truncation to handle very long sequences.
  prefs: []
  type: TYPE_NORMAL
- en: Task-specific metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can evaluate the model on downstream tasks relevant to your use case, such
    as text classification or question answering:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This function assesses the performance of a classification model on a test
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: It takes a pre-trained classification model, its tokenizer, test data (text),
    and corresponding test labels as inputs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model is set to evaluation mode to disable dropout and other training-specific
    behavior.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It processes each text in the test data, tokenizing it and using the model to
    make predictions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After processing all texts, it calculates two evaluation metrics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Accuracy**: The proportion of correct predictions out of all predictions
    made.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**F1 score**: A balanced measure of the model’s precision and recall. The F1
    score is the harmonic mean of **precision** (the ratio of true positive predictions
    to all positive predictions) and **recall** (the ratio of true positive predictions
    to all actual positive instances).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The F1 score formula is *F1 = 2 * (precision * recall) / (precision +* *recall)*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The F1 score ranges from 0 to 1, where 1 indicates perfect precision and recall.
    It’s particularly useful for imbalanced datasets where accuracy alone might be
    misleading. The weighted average calculates F1 for each class and averages them,
    weighted by the number of instances in each class.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The function returns both the accuracy and F1 score, providing a more comprehensive
    evaluation of the model’s performance across potentially imbalanced classes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This implementation also uses `torch.no_grad()` for efficiency and assumes that
    the necessary scikit-learn metrics are imported. In practice, you might want to
    add error handling for unexpected model outputs or mismatched prediction/label
    counts.
  prefs: []
  type: TYPE_NORMAL
- en: Diversity metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It’s important to assess the diversity of your augmented dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This function takes a collection of texts as input and computes **diversity
    metrics**. Once this has been done, this function returns a dictionary with these
    two metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Vocabulary size** (ranging from 1 to the total number of words): This gives
    an idea of lexical diversity. A high number suggests diverse word usage across
    the texts. This metric splits each text into words, combines all words from all
    texts, and then counts the number of unique words using a **set**. In this context,
    a set refers to a data structure that automatically removes duplicate elements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unique trigrams** (ranging from 1 to the total number of trigrams): These
    indicate character-level diversity. A high number suggests varied character sequences,
    potentially indicating diverse sentence structures or word choices. This metric
    creates trigrams (sequences of three characters) from each text and counts the
    number of unique trigrams using a set that only contains unique elements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These metrics are useful for comparing the diversity of original texts versus
    augmented texts or for assessing the variety in a dataset. However, the results
    should be interpreted in context, as high diversity might indicate incoherence
    or noise in the data.
  prefs: []
  type: TYPE_NORMAL
- en: By systematically applying these techniques, we can quantify the impact of our
    data augmentation strategies on LLM performance and make informed decisions about
    which techniques to use and how to fine-tune our augmentation pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored advanced data augmentation techniques for LLMs,
    covering text manipulation methods, leveraging existing models for data generation,
    multilingual strategies, semantic preservation, quality control, and several metrics.
    We also discussed the importance of balancing augmentation with data quality and
    provided practical Python implementations for various techniques.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll focus on handling large datasets for LLM training.
  prefs: []
  type: TYPE_NORMAL
