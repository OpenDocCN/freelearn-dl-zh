- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data Augmentation
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data augmentation plays a pivotal role in enhancing the performance and generalization
    capabilities of LLMs. By artificially expanding the training dataset, we can expose
    our models to a wider range of linguistic variations and contexts, improving their
    ability to handle diverse inputs and generate more coherent and contextually appropriate
    outputs.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: In the context of LLMs, data augmentation takes on unique challenges and opportunities.
    Unlike **image data**, where simple transformations such as rotation or flipping
    can create valid new samples, **text data** requires more nuanced approaches to
    maintain semantic integrity and linguistic coherence. The main goals of data augmentation
    for LLMs include increasing dataset size and diversity, addressing data imbalance
    and bias, improving model robustness to variations in input, and enhancing generalization
    to unseen data.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 3**.1*, I illustrate the key aspects of data augmentation.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – Key elements of data augmentation](img/B31249_03_001.jpg)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 – Key elements of data augmentation
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: There are three main components, namely **Techniques**, **Considerations**,
    and **Evaluation**. Each has specific sub-components, which we’ll cover in detail
    in this chapter.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you’ll have learned about the **data augmentation**
    pattern in depth, from increasing the diversity of your training dataset to maintaining
    its integrity:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Text data augmentation techniques
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leveraging existing LLMs for data generation
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multilingual data augmentation strategies
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Semantic preservation in text augmentation
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Balancing augmentation and data quality
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating the impact of data augmentation
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text data augmentation techniques
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Text data augmentation encompasses a wide range of techniques, from simple word-level
    manipulations to more complex semantic transformations.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Synonym replacement
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This technique involves replacing words in the original text with their synonyms.
    We can use **WordNet**, a lexical database for the English language, to find synonyms:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The `synonym_replacement` function takes a text input and replaces a specified
    number (the default is 1) of words with their synonyms. The default value of 1
    is chosen to minimize text alteration, preserving meaning and readability while
    allowing easy experimentation. You can increase this number if you want more replacements.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: The function splits the text into words, creates a list of unique alphanumeric
    words, shuffles this list, and then iterates through it. For each word, it attempts
    to find synonyms using an undefined `get_synonyms` function. If synonyms are found,
    it randomly selects one and replaces all occurrences of the original word in the
    text. The function keeps track of how many words have been replaced and stops
    when it reaches the specified number. Finally, it rejoins the modified words into
    a single string and returns it.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Back-translation
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This method involves translating the text to another language and then back
    to the original language. It’s particularly effective for introducing natural
    variations in sentence structure and word choice:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法涉及将文本翻译成另一种语言，然后再翻译回原始语言。这对于引入句子结构和词汇选择中的自然变化特别有效：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Text generation with T5
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用T5进行文本生成
- en: The **Text-To-Text Transfer Transformer** (**T5**) model, developed by Google
    Research, is a versatile **natural language processing** (**NLP**) model based
    on the transformer architecture. Its key innovation is framing all NLP tasks as
    text-to-text problems, allowing it to handle multiple tasks without task-specific
    architectures. Pre-trained on a large web text corpus using a “span corruption”
    objective, T5 is available in various sizes and has demonstrated powerful performance
    across a wide range of NLP tasks.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 由谷歌研究开发的**文本到文本迁移转换器**（**T5**）模型是一个基于转换器架构的多功能**自然语言处理**（**NLP**）模型。其关键创新是将所有NLP任务框架化为文本到文本问题，这使得它能够处理多个任务而无需特定于任务的架构。使用“跨度损坏”目标在大型网络文本语料库上预训练，T5有多种尺寸，并在广泛的NLP任务中展示了强大的性能。
- en: T5 handles a wide range of text-based tasks by framing them all as text-to-text
    problems. This means that irrespective of the task, whether it’s summarization,
    translation, question answering, or classification, both the input and output
    are treated as text. This unified approach allows T5 to perform various tasks
    without needing task-specific modifications, making it highly adaptable for different
    use cases.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: T5通过将所有基于文本的任务框架化为文本到文本问题来处理广泛的文本任务。这意味着无论任务是什么，无论是摘要、翻译、问答还是分类，输入和输出都被视为文本。这种统一的方法使得T5能够在无需特定于任务的修改的情况下执行各种任务，使其高度适应不同的用例。
- en: 'When it comes to data augmentation, T5 plays a key role by generating variations
    of existing text data, which is essential for expanding and diversifying datasets.
    Data augmentation is especially valuable when training machine learning models,
    as it helps them generalize better by exposing them to a wider variety of examples,
    reducing overfitting and improving robustness. Here’s how T5 aids in data augmentation:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 当谈到数据增强时，T5通过生成现有文本数据的变体，在扩展和多样化数据集方面发挥着关键作用。数据增强在训练机器学习模型时尤其有价值，因为它通过让模型接触到更广泛的示例来帮助它们更好地泛化，减少过拟合并提高鲁棒性。以下是T5如何帮助数据增强的说明：
- en: '**Paraphrasing**: T5 can rephrase sentences while maintaining their original
    meaning. For example, if the input is “The movie was boring,” T5 could generate
    a paraphrased version such as “The film was dull.” This variety in expression
    provides additional examples for a model to learn from, helping it generalize
    better to different ways of phrasing the same idea.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**释义**：T5可以在保持原意的同时重新表述句子。例如，如果输入是“这部电影很无聊”，T5可以生成一个释义版本，如“这部电影很乏味。”这种表达方式的多样性为模型提供了额外的学习示例，有助于它更好地泛化到不同的表述方式。'
- en: '**Synonym replacement**: T5 can replace words with their synonyms, creating
    slight variations in meaning while retaining the overall sentiment or context.
    For instance, from “The movie was long and tedious,” T5 might generate “The film
    was lengthy and boring.” This simple modification increases the diversity of the
    dataset, offering more training examples for models that rely on understanding
    slight variations in language.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**同义词替换**：T5可以用同义词替换单词，在保留整体情感或上下文的同时，创造轻微的意义变化。例如，从“这部电影很长且无聊”中，T5可能会生成“这部电影很冗长且乏味。”这种简单的修改增加了数据集的多样性，为依赖于理解语言微小变化的模型提供了更多的训练示例。'
- en: '**Sentiment-based transformation**: T5 can also transform the sentiment of
    a sentence. For example, given a negative sentence such as “The movie was very
    disappointing,” T5 can generate a neutral or positive version, such as “The movie
    had a slow start but improved later.” This capability allows for the creation
    of multiple examples across different sentiment categories, which is particularly
    useful in tasks such as sentiment analysis, where a model needs to distinguish
    between positive, neutral, and negative sentiments.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于情感的转换**：T5还可以转换句子的情感。例如，给定一个负面句子，如“这部电影非常令人失望”，T5可以生成一个中立或正面的版本，如“这部电影开始得很慢，但后来有所改进。”这种能力允许在不同情感类别中创建多个示例，这在如情感分析等任务中特别有用，在这些任务中，模型需要区分积极、中立和消极的情感。'
- en: '**Text expansion**: T5 can take a short sentence and expand it by adding more
    context, details, or descriptions. For instance, from the sentence “The event
    was great,” T5 could generate a more detailed version such as “The event was great,
    with excellent speakers and engaging discussions.” By adding more context, T5
    provides additional variations of the sentence that help in training models to
    handle more complex inputs.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本扩展**：T5可以接受简短的句子并通过添加更多上下文、细节或描述来扩展它。例如，从句子“事件很棒”中，T5可以生成一个更详细的版本，如“事件很棒，有出色的演讲和引人入胜的讨论。”通过添加更多上下文，T5提供了句子的额外变体，有助于训练模型处理更复杂的输入。'
- en: 'We can use a pre-trained T5 model to generate variations of input text. This
    method is particularly powerful as it can produce more diverse and contextually
    rich augmentations. Let’s see this:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用预训练的T5模型生成输入文本的变体。这种方法特别强大，因为它可以产生更多样化和上下文丰富的增强。让我们看看这个例子：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This function takes a text input, a pre-trained T5 model, its tokenizer, and
    the number of paraphrases to generate (the default is 1). The default of 1 return
    sequence is chosen for simplicity, but you can request multiple paraphrases by
    increasing this value.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数接受文本输入、预训练的T5模型、其分词器以及要生成的释义数量（默认为1）。默认的1个返回序列是为了简单起见，但你可以通过增加此值来请求多个释义。
- en: The function encodes the input text with a `"paraphrase:"` prefix, limiting
    it to `512` tokens. It then uses the model to generate paraphrases with a maximum
    length of `150` tokens. The generation process uses beam search with 5 beams,
    prevents repetition of 2-grams, and applies `50`) and `0.95`) `512`, `150`, `5`,
    `2`, `50`, `0.95`) can also be adjusted based on specific use cases to control
    the length, diversity, and quality of the generated paraphrases.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 函数使用`"paraphrase:"`前缀对输入文本进行编码，限制其长度为`512`个标记。然后使用模型生成最大长度为`150`个标记的释义。生成过程使用5个beam的beam搜索，防止2-gram重复，并应用`50`)
    和 `0.95`) `512`, `150`, `5`, `2`, `50`, `0.95`)，这些参数也可以根据具体用例进行调整，以控制生成释义的长度、多样性和质量。
- en: The function decodes and returns the generated paraphrases, skipping any special
    tokens added during the process.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 函数解码并返回生成的释义，跳过在过程中添加的任何特殊标记。
- en: Using temperature control as an additional parameter in language generation
    systems allows fine-tuning the balance between creativity and coherence. Temperature
    is a scalar value, typically ranging from 0 to 1, that influences the probability
    distribution over the next token during generation. Low values (close to 0) concentrate
    the distribution, making the model more deterministic and coherent but potentially
    repetitive or conservative. High values (close to 1) flatten the distribution,
    increasing randomness and diversity at the cost of coherence.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在语言生成系统中使用温度控制作为额外的参数，允许微调创造性和连贯性之间的平衡。温度是一个介于0到1之间的标量值，它在生成过程中影响下一个标记的概率分布。低值（接近0）使分布集中，使模型更确定性和连贯，但可能重复或保守。高值（接近1）使分布平坦，增加随机性和多样性，但牺牲了连贯性。
- en: Leveraging existing LLMs for data generation
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用现有LLMs进行数据生成
- en: One of the most powerful approaches to data augmentation for LLMs is to use
    existing models to generate new training examples. This technique, often referred
    to as **self-supervised learning** or **model-based data augmentation**, allows
    us to create vast amounts of diverse, high-quality training data.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对于LLMs的数据增强，最强大的方法之一是使用现有模型生成新的训练示例。这种技术通常被称为**自监督学习**或**基于模型的 数据增强**，它使我们能够创建大量多样化、高质量的训练数据。
- en: 'We’ll explore how to use **GPT-4o** and the **OpenAI API** for data generation:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探讨如何使用**GPT-4o**和**OpenAI API**进行数据生成：
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This function sends a single user message containing the provided prompt for
    a chat completion request. It limits the response to a maximum of `150` tokens,
    which balances between getting a substantive response and controlling the output
    length. The `n` parameter, set to `num_samples`, determines the number of alternative
    completions to generate. A temperature of `0.7` is used, which provides a balance
    between creativity and coherence in the generated text: high values increase randomness,
    while low values would make the output more deterministic. The function then extracts
    and returns the content of each generated completion, stripping any leading or
    trailing whitespace. These parameters (`150` tokens, `0.7` temperature) can be
    adjusted based on specific needs for output length and creativity.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: 'When using this approach, we need to consider the following:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '**Prompt engineering**: Crafting effective prompts is needed for generating
    relevant and diverse samples.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Quality control**: Implement filtering mechanisms to ensure the generated
    data meets your quality standards.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Diversity**: Use temperature and top-p sampling to control the randomness
    and diversity of generated samples.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ve explored data augmentation techniques using GPT-4o and examined essential
    considerations. Now, let’s turn our attention to strategies for multilingual data
    augmentation.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Multilingual data augmentation strategies
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For LLMs designed to handle multiple languages, multilingual data augmentation
    is essential. We can adapt our previous techniques to work across languages.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Cross-lingual back-translation
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Translate the text into multiple languages before translating it back to the
    original language:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `cross_lingual_back_translation` function takes a text input and generates
    augmented versions of it by first translating it into multiple target languages
    (defaulting to French, German, and Spanish) and then back to English. The function
    uses the `Translator` object to perform these translations, storing each back-translated
    version in a list, which is returned as the output.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Multilingual T5 augmentation
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can use a multilingual T5 model to generate paraphrases in different languages:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The `multilingual_t5_augmentation` function uses a T5 model to augment a given
    text by translating it into multiple target languages (defaulting to French, German,
    and Spanish). For each target language, it encodes the text with a prompt for
    translation, generates the translated output using the model, and decodes the
    result. The translated texts are collected in a list and returned as the augmented
    versions of the original text.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Semantic preservation in text augmentation
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Maintaining semantic integrity is crucial when augmenting data for LLMs. We
    must ensure that our techniques don’t alter the original meaning of the text.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Use of sentence embeddings
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By comparing the **sentence embeddings** of the original and augmented texts,
    you can ensure **semantic similarity**:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We define two functions for measuring and filtering text based on semantic
    similarity:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '`semantic_similarity(original, augmented, model)` calculates the semantic similarity
    between two texts using the cosine similarity of their embeddings. It uses a provided
    model (probably a sentence embedding model) to encode the original and augmented
    texts into vector representations. The cosine similarity between these vectors
    is then computed, resulting in a value between -1 and 1, where 1 indicates perfect
    similarity.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`filter_by_semantic_similarity(original, augmented_list, model, threshold=0.8)`
    filters a list of augmented texts based on their semantic similarity to the original.
    The `semantic_similarity` function compares each augmented text with the original.
    The default threshold is set to `0.8`: by default, it will keep only the augmented
    texts that have a similarity of `0.8` or higher to the original. This threshold
    is commonly used in NLP tasks, as it typically indicates a strong semantic similarity
    while allowing some variation. It can be adjusted based on how strict or lenient
    you want the filtering to be: a higher threshold will result in more similar (but
    possibly fewer) augmentations; a lower threshold will allow more diverse (but
    potentially less relevant) augmentations.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contextual word embeddings for synonym replacement
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can use **contextual word embeddings** to find more appropriate synonyms
    based on the context. Contextual word embeddings refer to the use of word representations
    generated by language models that capture the meaning of a word within its specific
    sentence or passage, rather than treating the word as having a fixed meaning.
    Unlike traditional static embeddings where a word has the same vector regardless
    of context, contextual embeddings assign different vectors to the same word depending
    on its surrounding words. This allows for more accurate synonym replacement as
    the chosen synonym aligns not only with the dictionary meaning but also with how
    the word is used in a particular context. For example, the word “bank” in “river
    bank” versus “savings bank” would be represented differently, leading to contextually
    appropriate synonym suggestions such as “shore” or “financial institution,” respectively.
    The following code snippet shows how it works:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This function performs context-aware word replacement using a language model:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: It takes a text input, a pre-trained language model, its tokenizer, and the
    number of words to replace (the default is 1).
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The text is split into words, and a copy is made for modification.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The function iterates `n` times (the default is 1). It does the following each
    time:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Randomly selects a word index
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Tokenizes the entire text
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Runs it through the model to get contextualized embeddings
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Extracts the embedding of the chosen word
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Finds similar words based on this embedding (using an undefined `find_similar_words`
    function)
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If similar words are found, it randomly chooses one to replace the original
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, it joins the modified words back into a string and returns it.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The default `n`=1 is chosen to make minimal changes while still introducing
    variation. This preserves most of the original meaning and structure. You can
    increase `n` to get more replacements, but higher values might alter the text’s
    meaning more significantly.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: This method is more context-aware than simple synonym replacement as it considers
    the word’s usage in the full text when finding replacements. The exact behavior
    will depend on the model and tokenizer used, as well as the implementation of
    the `find_similar_words` function.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Balancing augmentation and data quality
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While data augmentation can significantly improve LLM performance, we need to
    strike a balance between quantity and quality.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: You should limit the proportion of augmented data in your training set. A common
    practice is to start with a 1:1 ratio of original to augmented data and adjust
    based on model performance.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Quality filtering
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can implement quality checks to filter out low-quality augmented samples:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Human-in-the-loop validation
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For critical applications, incorporate human validation into your augmentation
    pipeline.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '**Human-in-the-loop** (**HITL**) validation is a control mechanism used in
    AI pipelines where humans are deliberately inserted into automated workflows to
    ensure correctness, especially in tasks involving subjective judgment, sensitive
    content, or critical decision-making. This is particularly important in applications
    where data quality directly affects safety, fairness, or compliance—for example,
    healthcare diagnostics, legal document analysis, or autonomous systems. In the
    context of data augmentation, where the goal is to expand the training dataset
    by generating variations of existing samples, HITL is used to validate whether
    the generated samples are coherent, accurate, and aligned with the intended label
    or task:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This function is designed to manually validate a list of augmented text samples
    by soliciting binary feedback—yes or no—from a human operator. Its presence within
    an augmentation pipeline acknowledges that not all automatically generated data
    can be trusted at face value. The decision to retain or discard a given sample
    is made interactively, reinforcing human oversight in tasks where semantic integrity
    is non-negotiable.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Each iteration of the function’s loop represents a decision point. The human
    validator is shown the generated text and asked to assess whether it meets the
    expected criteria. These criteria are typically based on task-specific requirements
    such as grammaticality, semantic equivalence to the original data, tone appropriateness,
    or domain alignment. For example, in a medical text classification task, a paraphrased
    sentence must preserve all critical clinical entities. A slight shift in terminology
    introduced by an augmentation technique could mislead the model if not caught
    during validation. This is where human evaluation becomes indispensable.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: The logic behind converting the input to lowercase is to handle inconsistent
    user input. Whether the user types `Y`, `y`, or any other casing, the comparison
    becomes case-agnostic. Only if the input is equivalent to `y` does the function
    accept the sample. This binary check is deliberately strict to prevent ambiguous
    approvals. The rejected samples are silently discarded and not logged or returned,
    implying that any further inspection or correction of rejected samples would need
    to be implemented separately.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: The function concludes by returning a list of only those samples that have been
    explicitly validated. This output can then be used to expand the training dataset
    with higher confidence in the integrity of the new data points. Importantly, this
    approach does not replace automated quality checks but supplements them in high-stakes
    applications. HITL validation is particularly useful when deploying models in
    environments where false positives or negatives carry high costs, such as legal
    recommendation systems, fraud detection, or autonomous navigation. The manual
    validation process helps to mitigate risks that stem from over-reliance on generative
    augmentation methods that lack explicit semantic guarantees.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: 'In a larger system, this kind of function would usually be embedded in a broader
    workflow where automated filters screen out obviously low-quality or irrelevant
    augmentations first. The human validator would only evaluate the borderline or
    high-impact cases. For operational efficiency, the interaction would typically
    be handled via a web interface or integrated annotation tool rather than a command-line
    prompt. However, the function demonstrates the principle in its simplest form:
    human judgment is used as the final arbiter of quality before incorporating augmented
    data into model training.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the impact of data augmentation
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To assess the effectiveness of our data augmentation techniques, we need to
    assess their impact on LLM performance.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Perplexity
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can measure a model’s perplexity (see [*Chapter 2*](B31249_02.xhtml#_idTextAnchor035))
    on a held-out test set before and after data augmentation to assess whether it
    has improved the model’s ability to predict unseen text:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This function, `evaluate_perplexity`, calculates the perplexity of a language
    model on a given test dataset. Here’s a breakdown:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: It takes a pre-trained language model, its tokenizer, and a test dataset as
    input.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model is set to evaluation mode to disable dropout and other training-specific
    behavior.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It initializes variables to track the total loss and total number of tokens
    processed.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each text in the test data, the following is carried out:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The text is tokenized and converted to tensors.
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The model processes the input, calculating the loss.
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The loss is accumulated, weighted by the number of tokens in the input.
  id: totrans-110
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After processing all texts, it calculates the perplexity using the following
    formula: `exp(total_loss /` `total_tokens)`.'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This implementation uses the model in a zero-shot manner, treating each input
    as both the context and the target to predict. The use of `torch.no_grad()` ensures
    that no gradients are computed, making the evaluation more efficient.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 此实现以零样本方式使用模型，将每个输入视为上下文和预测的目标。使用`torch.no_grad()`确保不计算梯度，使评估更高效。
- en: This function assumes the model and data are compatible (i.e., the model can
    handle the maximum sequence length of the data). In practice, you might need to
    add checks or truncation to handle very long sequences.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数假设模型和数据兼容（即模型可以处理数据的最大序列长度）。在实际应用中，您可能需要添加检查或截断以处理非常长的序列。
- en: Task-specific metrics
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特定任务指标
- en: 'You can evaluate the model on downstream tasks relevant to your use case, such
    as text classification or question answering:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以对与您的用例相关的下游任务进行模型评估，例如文本分类或问答：
- en: '[PRE11]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This function assesses the performance of a classification model on a test
    dataset:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数评估分类模型在测试数据集上的性能：
- en: It takes a pre-trained classification model, its tokenizer, test data (text),
    and corresponding test labels as inputs.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它接受一个预训练的分类模型、其分词器、测试数据（文本）和相应的测试标签作为输入。
- en: The model is set to evaluation mode to disable dropout and other training-specific
    behavior.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型设置为评估模式以禁用dropout和其他特定于训练的行为。
- en: It processes each text in the test data, tokenizing it and using the model to
    make predictions.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它处理测试数据中的每个文本，对其进行分词，并使用模型进行预测。
- en: 'After processing all texts, it calculates two evaluation metrics:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 处理完所有文本后，它计算两个评估指标：
- en: '**Accuracy**: The proportion of correct predictions out of all predictions
    made.'
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确率**：所有预测中正确预测的比例。'
- en: '**F1 score**: A balanced measure of the model’s precision and recall. The F1
    score is the harmonic mean of **precision** (the ratio of true positive predictions
    to all positive predictions) and **recall** (the ratio of true positive predictions
    to all actual positive instances).'
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**F1分数**：模型精确率和召回率的平衡度量。F1分数是**精确率**（所有正预测中真正预测的比例）和**召回率**（所有实际正实例中真正预测的比例）的调和平均数。'
- en: The F1 score formula is *F1 = 2 * (precision * recall) / (precision +* *recall)*.
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: F1分数的公式是 *F1 = 2 * (精确率 * 召回率) / (精确率 + 召回率)*。
- en: The F1 score ranges from 0 to 1, where 1 indicates perfect precision and recall.
    It’s particularly useful for imbalanced datasets where accuracy alone might be
    misleading. The weighted average calculates F1 for each class and averages them,
    weighted by the number of instances in each class.
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: F1分数的范围从0到1，其中1表示完美的精确度和召回率。对于仅准确率可能具有误导性的不平衡数据集，它特别有用。加权平均计算每个类的F1分数，并按每个类中实例的数量加权平均。
- en: The function returns both the accuracy and F1 score, providing a more comprehensive
    evaluation of the model’s performance across potentially imbalanced classes.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 函数返回准确率和F1分数，提供了对模型在可能不平衡的类别上的性能的更全面评估。
- en: This implementation also uses `torch.no_grad()` for efficiency and assumes that
    the necessary scikit-learn metrics are imported. In practice, you might want to
    add error handling for unexpected model outputs or mismatched prediction/label
    counts.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 此实现还使用`torch.no_grad()`以提高效率，并假设已导入必要的scikit-learn指标。在实际应用中，您可能需要添加错误处理以处理意外的模型输出或预测/标签计数不匹配。
- en: Diversity metrics
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多样性指标
- en: 'It’s important to assess the diversity of your augmented dataset:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 评估增强数据集的多样性很重要：
- en: '[PRE12]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This function takes a collection of texts as input and computes **diversity
    metrics**. Once this has been done, this function returns a dictionary with these
    two metrics:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数接受一组文本作为输入，并计算**多样性指标**。一旦完成，此函数将返回一个包含这两个指标的字典：
- en: '**Vocabulary size** (ranging from 1 to the total number of words): This gives
    an idea of lexical diversity. A high number suggests diverse word usage across
    the texts. This metric splits each text into words, combines all words from all
    texts, and then counts the number of unique words using a **set**. In this context,
    a set refers to a data structure that automatically removes duplicate elements.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词汇量大小**（范围从1到总单词数）：这可以给出词汇多样性的概念。高数值表明文本中使用了多样化的词汇。此指标将每个文本拆分为单词，然后将所有文本中的所有单词合并，并使用**集合**来计算唯一单词的数量。在此上下文中，集合指的是一种数据结构，它自动删除重复元素。'
- en: '**Unique trigrams** (ranging from 1 to the total number of trigrams): These
    indicate character-level diversity. A high number suggests varied character sequences,
    potentially indicating diverse sentence structures or word choices. This metric
    creates trigrams (sequences of three characters) from each text and counts the
    number of unique trigrams using a set that only contains unique elements.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**独特的三元组**（范围从1到三元组的总数）：这些指标表示字符级别的多样性。高数值表明字符序列多样化，可能表明句子结构或词汇选择多样化。此指标通过从每个文本中创建三元组（三个字符的序列）并使用仅包含唯一元素的集合来计算独特三元组的数量。'
- en: These metrics are useful for comparing the diversity of original texts versus
    augmented texts or for assessing the variety in a dataset. However, the results
    should be interpreted in context, as high diversity might indicate incoherence
    or noise in the data.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这些指标可用于比较原始文本与增强文本之间的多样性，或评估数据集中的多样性。然而，结果应在特定背景下进行解读，因为高多样性可能表明数据中的不连贯性或噪声。
- en: By systematically applying these techniques, we can quantify the impact of our
    data augmentation strategies on LLM performance and make informed decisions about
    which techniques to use and how to fine-tune our augmentation pipeline.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 通过系统地应用这些技术，我们可以量化我们的数据增强策略对LLM性能的影响，并就使用哪些技术和如何微调我们的增强流程做出明智的决定。
- en: Summary
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored advanced data augmentation techniques for LLMs,
    covering text manipulation methods, leveraging existing models for data generation,
    multilingual strategies, semantic preservation, quality control, and several metrics.
    We also discussed the importance of balancing augmentation with data quality and
    provided practical Python implementations for various techniques.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了针对LLM的高级数据增强技术，包括文本操作方法、利用现有模型进行数据生成、多语言策略、语义保留、质量控制以及多个指标。我们还讨论了平衡增强与数据质量的重要性，并提供了各种技术的实用Python实现。
- en: In the next chapter, we’ll focus on handling large datasets for LLM training.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将专注于处理LLM训练的大型数据集。
