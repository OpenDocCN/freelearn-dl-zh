- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Data Augmentation
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据增强
- en: Data augmentation plays a pivotal role in enhancing the performance and generalization
    capabilities of LLMs. By artificially expanding the training dataset, we can expose
    our models to a wider range of linguistic variations and contexts, improving their
    ability to handle diverse inputs and generate more coherent and contextually appropriate
    outputs.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强在增强LLMs（大型语言模型）的性能和泛化能力方面发挥着关键作用。通过人工扩大训练数据集，我们可以让我们的模型接触到更广泛的语言变化和上下文，提高它们处理各种输入和生成更连贯、上下文相关输出的能力。
- en: In the context of LLMs, data augmentation takes on unique challenges and opportunities.
    Unlike **image data**, where simple transformations such as rotation or flipping
    can create valid new samples, **text data** requires more nuanced approaches to
    maintain semantic integrity and linguistic coherence. The main goals of data augmentation
    for LLMs include increasing dataset size and diversity, addressing data imbalance
    and bias, improving model robustness to variations in input, and enhancing generalization
    to unseen data.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLMs的背景下，数据增强面临着独特的挑战和机遇。与**图像数据**不同，图像数据可以通过简单的变换（如旋转或翻转）来创建有效的新的样本，**文本数据**需要更细致的方法来保持语义完整性和语言连贯性。LLMs数据增强的主要目标包括增加数据集大小和多样性，解决数据不平衡和偏差问题，提高模型对输入变化的鲁棒性，以及增强对未见数据的泛化能力。
- en: In *Figure 3**.1*, I illustrate the key aspects of data augmentation.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图3.1*中，我展示了数据增强的关键方面。
- en: '![Figure 3.1 – Key elements of data augmentation](img/B31249_03_001.jpg)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![图3.1 – 数据增强的关键元素](img/B31249_03_001.jpg)'
- en: Figure 3.1 – Key elements of data augmentation
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1 – 数据增强的关键元素
- en: There are three main components, namely **Techniques**, **Considerations**,
    and **Evaluation**. Each has specific sub-components, which we’ll cover in detail
    in this chapter.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 有三个主要组成部分，即**技术**、**考虑因素**和**评估**。每个部分都有具体的子组件，我们将在本章中详细讨论。
- en: 'By the end of this chapter, you’ll have learned about the **data augmentation**
    pattern in depth, from increasing the diversity of your training dataset to maintaining
    its integrity:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您将深入了解数据增强模式，从增加训练数据集的多样性到保持其完整性：
- en: Text data augmentation techniques
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本数据增强技术
- en: Leveraging existing LLMs for data generation
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用现有LLMs进行数据生成
- en: Multilingual data augmentation strategies
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多语言数据增强策略
- en: Semantic preservation in text augmentation
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本增强中的语义保留
- en: Balancing augmentation and data quality
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平衡增强和数据质量
- en: Evaluating the impact of data augmentation
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估数据增强的影响
- en: Text data augmentation techniques
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本数据增强技术
- en: Text data augmentation encompasses a wide range of techniques, from simple word-level
    manipulations to more complex semantic transformations.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据增强包括一系列技术，从简单的单词级别操作到更复杂的语义转换。
- en: Synonym replacement
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 同义词替换
- en: 'This technique involves replacing words in the original text with their synonyms.
    We can use **WordNet**, a lexical database for the English language, to find synonyms:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术涉及用同义词替换原始文本中的单词。我们可以使用**WordNet**，一个英语词汇数据库，来查找同义词：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The `synonym_replacement` function takes a text input and replaces a specified
    number (the default is 1) of words with their synonyms. The default value of 1
    is chosen to minimize text alteration, preserving meaning and readability while
    allowing easy experimentation. You can increase this number if you want more replacements.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '`synonym_replacement`函数接受一个文本输入，并用同义词替换指定数量的单词（默认为1）。选择默认值1是为了最小化文本修改，保留意义和可读性，同时允许轻松实验。如果您想进行更多替换，可以增加这个数字。'
- en: The function splits the text into words, creates a list of unique alphanumeric
    words, shuffles this list, and then iterates through it. For each word, it attempts
    to find synonyms using an undefined `get_synonyms` function. If synonyms are found,
    it randomly selects one and replaces all occurrences of the original word in the
    text. The function keeps track of how many words have been replaced and stops
    when it reaches the specified number. Finally, it rejoins the modified words into
    a single string and returns it.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 函数将文本拆分为单词，创建一个唯一的字母数字单词列表，然后打乱这个列表，并遍历它。对于每个单词，它尝试使用一个未定义的`get_synonyms`函数来查找同义词。如果找到了同义词，它将随机选择一个并替换文本中所有原始单词的出现。该函数会跟踪已替换的单词数量，并在达到指定数量时停止。最后，它将修改后的单词重新组合成一个字符串并返回。
- en: Back-translation
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 反向翻译
- en: 'This method involves translating the text to another language and then back
    to the original language. It’s particularly effective for introducing natural
    variations in sentence structure and word choice:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法涉及将文本翻译成另一种语言，然后再翻译回原始语言。这对于引入句子结构和词汇选择中的自然变化特别有效：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Text generation with T5
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用T5进行文本生成
- en: The **Text-To-Text Transfer Transformer** (**T5**) model, developed by Google
    Research, is a versatile **natural language processing** (**NLP**) model based
    on the transformer architecture. Its key innovation is framing all NLP tasks as
    text-to-text problems, allowing it to handle multiple tasks without task-specific
    architectures. Pre-trained on a large web text corpus using a “span corruption”
    objective, T5 is available in various sizes and has demonstrated powerful performance
    across a wide range of NLP tasks.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 由谷歌研究开发的**文本到文本迁移转换器**（**T5**）模型是一个基于转换器架构的多功能**自然语言处理**（**NLP**）模型。其关键创新是将所有NLP任务框架化为文本到文本问题，这使得它能够处理多个任务而无需特定于任务的架构。使用“跨度损坏”目标在大型网络文本语料库上预训练，T5有多种尺寸，并在广泛的NLP任务中展示了强大的性能。
- en: T5 handles a wide range of text-based tasks by framing them all as text-to-text
    problems. This means that irrespective of the task, whether it’s summarization,
    translation, question answering, or classification, both the input and output
    are treated as text. This unified approach allows T5 to perform various tasks
    without needing task-specific modifications, making it highly adaptable for different
    use cases.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: T5通过将所有基于文本的任务框架化为文本到文本问题来处理广泛的文本任务。这意味着无论任务是什么，无论是摘要、翻译、问答还是分类，输入和输出都被视为文本。这种统一的方法使得T5能够在无需特定于任务的修改的情况下执行各种任务，使其高度适应不同的用例。
- en: 'When it comes to data augmentation, T5 plays a key role by generating variations
    of existing text data, which is essential for expanding and diversifying datasets.
    Data augmentation is especially valuable when training machine learning models,
    as it helps them generalize better by exposing them to a wider variety of examples,
    reducing overfitting and improving robustness. Here’s how T5 aids in data augmentation:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 当谈到数据增强时，T5通过生成现有文本数据的变体，在扩展和多样化数据集方面发挥着关键作用。数据增强在训练机器学习模型时尤其有价值，因为它通过让模型接触到更广泛的示例来帮助它们更好地泛化，减少过拟合并提高鲁棒性。以下是T5如何帮助数据增强的说明：
- en: '**Paraphrasing**: T5 can rephrase sentences while maintaining their original
    meaning. For example, if the input is “The movie was boring,” T5 could generate
    a paraphrased version such as “The film was dull.” This variety in expression
    provides additional examples for a model to learn from, helping it generalize
    better to different ways of phrasing the same idea.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**释义**：T5可以在保持原意的同时重新表述句子。例如，如果输入是“这部电影很无聊”，T5可以生成一个释义版本，如“这部电影很乏味。”这种表达方式的多样性为模型提供了额外的学习示例，有助于它更好地泛化到不同的表述方式。'
- en: '**Synonym replacement**: T5 can replace words with their synonyms, creating
    slight variations in meaning while retaining the overall sentiment or context.
    For instance, from “The movie was long and tedious,” T5 might generate “The film
    was lengthy and boring.” This simple modification increases the diversity of the
    dataset, offering more training examples for models that rely on understanding
    slight variations in language.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**同义词替换**：T5可以用同义词替换单词，在保留整体情感或上下文的同时，创造轻微的意义变化。例如，从“这部电影很长且无聊”中，T5可能会生成“这部电影很冗长且乏味。”这种简单的修改增加了数据集的多样性，为依赖于理解语言微小变化的模型提供了更多的训练示例。'
- en: '**Sentiment-based transformation**: T5 can also transform the sentiment of
    a sentence. For example, given a negative sentence such as “The movie was very
    disappointing,” T5 can generate a neutral or positive version, such as “The movie
    had a slow start but improved later.” This capability allows for the creation
    of multiple examples across different sentiment categories, which is particularly
    useful in tasks such as sentiment analysis, where a model needs to distinguish
    between positive, neutral, and negative sentiments.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于情感的转换**：T5还可以转换句子的情感。例如，给定一个负面句子，如“这部电影非常令人失望”，T5可以生成一个中立或正面的版本，如“这部电影开始得很慢，但后来有所改进。”这种能力允许在不同情感类别中创建多个示例，这在如情感分析等任务中特别有用，在这些任务中，模型需要区分积极、中立和消极的情感。'
- en: '**Text expansion**: T5 can take a short sentence and expand it by adding more
    context, details, or descriptions. For instance, from the sentence “The event
    was great,” T5 could generate a more detailed version such as “The event was great,
    with excellent speakers and engaging discussions.” By adding more context, T5
    provides additional variations of the sentence that help in training models to
    handle more complex inputs.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本扩展**：T5可以接受简短的句子并通过添加更多上下文、细节或描述来扩展它。例如，从句子“事件很棒”中，T5可以生成一个更详细的版本，如“事件很棒，有出色的演讲和引人入胜的讨论。”通过添加更多上下文，T5提供了句子的额外变体，有助于训练模型处理更复杂的输入。'
- en: 'We can use a pre-trained T5 model to generate variations of input text. This
    method is particularly powerful as it can produce more diverse and contextually
    rich augmentations. Let’s see this:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用预训练的T5模型生成输入文本的变体。这种方法特别强大，因为它可以产生更多样化和上下文丰富的增强。让我们看看这个例子：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This function takes a text input, a pre-trained T5 model, its tokenizer, and
    the number of paraphrases to generate (the default is 1). The default of 1 return
    sequence is chosen for simplicity, but you can request multiple paraphrases by
    increasing this value.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数接受文本输入、预训练的T5模型、其分词器以及要生成的释义数量（默认为1）。默认的1个返回序列是为了简单起见，但你可以通过增加此值来请求多个释义。
- en: The function encodes the input text with a `"paraphrase:"` prefix, limiting
    it to `512` tokens. It then uses the model to generate paraphrases with a maximum
    length of `150` tokens. The generation process uses beam search with 5 beams,
    prevents repetition of 2-grams, and applies `50`) and `0.95`) `512`, `150`, `5`,
    `2`, `50`, `0.95`) can also be adjusted based on specific use cases to control
    the length, diversity, and quality of the generated paraphrases.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 函数使用`"paraphrase:"`前缀对输入文本进行编码，限制其长度为`512`个标记。然后使用模型生成最大长度为`150`个标记的释义。生成过程使用5个beam的beam搜索，防止2-gram重复，并应用`50`)
    和 `0.95`) `512`, `150`, `5`, `2`, `50`, `0.95`)，这些参数也可以根据具体用例进行调整，以控制生成释义的长度、多样性和质量。
- en: The function decodes and returns the generated paraphrases, skipping any special
    tokens added during the process.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 函数解码并返回生成的释义，跳过在过程中添加的任何特殊标记。
- en: Using temperature control as an additional parameter in language generation
    systems allows fine-tuning the balance between creativity and coherence. Temperature
    is a scalar value, typically ranging from 0 to 1, that influences the probability
    distribution over the next token during generation. Low values (close to 0) concentrate
    the distribution, making the model more deterministic and coherent but potentially
    repetitive or conservative. High values (close to 1) flatten the distribution,
    increasing randomness and diversity at the cost of coherence.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在语言生成系统中使用温度控制作为额外的参数，允许微调创造性和连贯性之间的平衡。温度是一个介于0到1之间的标量值，它在生成过程中影响下一个标记的概率分布。低值（接近0）使分布集中，使模型更确定性和连贯，但可能重复或保守。高值（接近1）使分布平坦，增加随机性和多样性，但牺牲了连贯性。
- en: Leveraging existing LLMs for data generation
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用现有LLMs进行数据生成
- en: One of the most powerful approaches to data augmentation for LLMs is to use
    existing models to generate new training examples. This technique, often referred
    to as **self-supervised learning** or **model-based data augmentation**, allows
    us to create vast amounts of diverse, high-quality training data.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对于LLMs的数据增强，最强大的方法之一是使用现有模型生成新的训练示例。这种技术通常被称为**自监督学习**或**基于模型的 数据增强**，它使我们能够创建大量多样化、高质量的训练数据。
- en: 'We’ll explore how to use **GPT-4o** and the **OpenAI API** for data generation:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探讨如何使用**GPT-4o**和**OpenAI API**进行数据生成：
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This function sends a single user message containing the provided prompt for
    a chat completion request. It limits the response to a maximum of `150` tokens,
    which balances between getting a substantive response and controlling the output
    length. The `n` parameter, set to `num_samples`, determines the number of alternative
    completions to generate. A temperature of `0.7` is used, which provides a balance
    between creativity and coherence in the generated text: high values increase randomness,
    while low values would make the output more deterministic. The function then extracts
    and returns the content of each generated completion, stripping any leading or
    trailing whitespace. These parameters (`150` tokens, `0.7` temperature) can be
    adjusted based on specific needs for output length and creativity.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数发送包含提供的提示的单个用户消息，用于聊天完成请求。它将响应限制在最多 `150` 个标记，这平衡了获得实质性响应和控制输出长度的需求。`n` 参数，设置为
    `num_samples`，决定了要生成的替代完成内容的数量。使用 `0.7` 的温度，这为生成的文本提供了创造性和连贯性之间的平衡：高值增加随机性，而低值会使输出更确定。然后函数提取并返回每个生成的完成内容的文本，去除任何前导或尾随空白。这些参数（`150`
    个标记，`0.7` 温度）可以根据输出长度和创造性的具体需求进行调整。
- en: 'When using this approach, we need to consider the following:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用这种方法时，我们需要考虑以下因素：
- en: '**Prompt engineering**: Crafting effective prompts is needed for generating
    relevant and diverse samples.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提示工程**：需要精心设计提示以生成相关和多样化的样本。'
- en: '**Quality control**: Implement filtering mechanisms to ensure the generated
    data meets your quality standards.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**质量控制**：实施过滤机制以确保生成数据符合您的质量标准。'
- en: '**Diversity**: Use temperature and top-p sampling to control the randomness
    and diversity of generated samples.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多样性**：使用温度和 top-p 采样来控制生成样本的随机性和多样性。'
- en: We’ve explored data augmentation techniques using GPT-4o and examined essential
    considerations. Now, let’s turn our attention to strategies for multilingual data
    augmentation.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经探讨了使用 GPT-4o 的数据增强技术并检查了基本考虑因素。现在，让我们将注意力转向多语言数据增强的策略。
- en: Multilingual data augmentation strategies
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多语言数据增强策略
- en: For LLMs designed to handle multiple languages, multilingual data augmentation
    is essential. We can adapt our previous techniques to work across languages.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 对于旨在处理多种语言的 LLM，多语言数据增强是必不可少的。我们可以调整我们之前的技术以跨语言工作。
- en: Cross-lingual back-translation
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 跨语言回译
- en: 'Translate the text into multiple languages before translating it back to the
    original language:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在将其翻译回原始语言之前，将文本翻译成多种语言：
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `cross_lingual_back_translation` function takes a text input and generates
    augmented versions of it by first translating it into multiple target languages
    (defaulting to French, German, and Spanish) and then back to English. The function
    uses the `Translator` object to perform these translations, storing each back-translated
    version in a list, which is returned as the output.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '`cross_lingual_back_translation` 函数接收一个文本输入，通过首先将其翻译成多种目标语言（默认为法语、德语和西班牙语），然后将其翻译回英语来生成其增强版本。该函数使用
    `Translator` 对象执行这些翻译，将每个回译版本存储在一个列表中，并将其作为输出返回。'
- en: Multilingual T5 augmentation
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多语言 T5 增强
- en: 'You can use a multilingual T5 model to generate paraphrases in different languages:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用多语言 T5 模型在不同语言中生成释义：
- en: '[PRE5]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The `multilingual_t5_augmentation` function uses a T5 model to augment a given
    text by translating it into multiple target languages (defaulting to French, German,
    and Spanish). For each target language, it encodes the text with a prompt for
    translation, generates the translated output using the model, and decodes the
    result. The translated texts are collected in a list and returned as the augmented
    versions of the original text.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`multilingual_t5_augmentation` 函数使用 T5 模型通过将其翻译成多种目标语言（默认为法语、德语和西班牙语）来增强给定的文本。对于每种目标语言，它使用翻译提示对文本进行编码，使用模型生成翻译输出，并解码结果。翻译的文本被收集在一个列表中，并作为原始文本的增强版本返回。'
- en: Semantic preservation in text augmentation
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本增强中的语义保留
- en: Maintaining semantic integrity is crucial when augmenting data for LLMs. We
    must ensure that our techniques don’t alter the original meaning of the text.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在为 LLM 增强数据时保持语义完整性至关重要。我们必须确保我们的技术不会改变文本的原始含义。
- en: Use of sentence embeddings
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 句子嵌入的使用
- en: 'By comparing the **sentence embeddings** of the original and augmented texts,
    you can ensure **semantic similarity**:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 通过比较原始文本和增强文本的 **句子嵌入**，您可以确保 **语义相似性**：
- en: '[PRE6]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We define two functions for measuring and filtering text based on semantic
    similarity:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了两个用于根据语义相似度测量和过滤文本的函数：
- en: '`semantic_similarity(original, augmented, model)` calculates the semantic similarity
    between two texts using the cosine similarity of their embeddings. It uses a provided
    model (probably a sentence embedding model) to encode the original and augmented
    texts into vector representations. The cosine similarity between these vectors
    is then computed, resulting in a value between -1 and 1, where 1 indicates perfect
    similarity.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`semantic_similarity(original, augmented, model)` 使用两个文本嵌入的余弦相似度计算两个文本之间的语义相似度。它使用提供的模型（可能是句子嵌入模型）将原始文本和增强文本编码为向量表示。然后计算这些向量之间的余弦相似度，得到一个介于
    -1 和 1 之间的值，其中 1 表示完美相似。'
- en: '`filter_by_semantic_similarity(original, augmented_list, model, threshold=0.8)`
    filters a list of augmented texts based on their semantic similarity to the original.
    The `semantic_similarity` function compares each augmented text with the original.
    The default threshold is set to `0.8`: by default, it will keep only the augmented
    texts that have a similarity of `0.8` or higher to the original. This threshold
    is commonly used in NLP tasks, as it typically indicates a strong semantic similarity
    while allowing some variation. It can be adjusted based on how strict or lenient
    you want the filtering to be: a higher threshold will result in more similar (but
    possibly fewer) augmentations; a lower threshold will allow more diverse (but
    potentially less relevant) augmentations.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`filter_by_semantic_similarity(original, augmented_list, model, threshold=0.8)`
    根据与原始文本的语义相似度过滤增强文本列表。`semantic_similarity` 函数将每个增强文本与原始文本进行比较。默认阈值设置为 `0.8`：默认情况下，它将仅保留与原始文本相似度达到
    `0.8` 或更高的增强文本。此阈值在 NLP 任务中常用，因为它通常表示强语义相似度，同时允许一些变化。可以根据您希望过滤有多严格或多宽松来调整此阈值：更高的阈值将导致更多相似（但可能更少）的增强；更低的阈值将允许更多样化（但可能不太相关）的增强。'
- en: Contextual word embeddings for synonym replacement
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用于同义词替换的上下文词嵌入
- en: 'You can use **contextual word embeddings** to find more appropriate synonyms
    based on the context. Contextual word embeddings refer to the use of word representations
    generated by language models that capture the meaning of a word within its specific
    sentence or passage, rather than treating the word as having a fixed meaning.
    Unlike traditional static embeddings where a word has the same vector regardless
    of context, contextual embeddings assign different vectors to the same word depending
    on its surrounding words. This allows for more accurate synonym replacement as
    the chosen synonym aligns not only with the dictionary meaning but also with how
    the word is used in a particular context. For example, the word “bank” in “river
    bank” versus “savings bank” would be represented differently, leading to contextually
    appropriate synonym suggestions such as “shore” or “financial institution,” respectively.
    The following code snippet shows how it works:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 **上下文词嵌入** 来根据上下文找到更合适的同义词。上下文词嵌入是指使用语言模型生成的词表示，这些表示捕获了单词在其特定句子或段落中的意义，而不是将单词视为具有固定意义。与传统的静态嵌入不同，其中单词的向量无论在什么上下文中都相同，上下文嵌入根据其周围的单词为相同的单词分配不同的向量。这允许进行更准确的同义词替换，因为所选的同义词不仅与词典意义相符，而且与单词在特定上下文中的使用方式相符。例如，“bank”在“river
    bank”与“savings bank”中的表示就不同，这会导致上下文适当的同义词建议，如“shore”或“financial institution”。以下代码片段显示了它是如何工作的：
- en: '[PRE7]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This function performs context-aware word replacement using a language model:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数使用语言模型进行上下文感知的单词替换：
- en: It takes a text input, a pre-trained language model, its tokenizer, and the
    number of words to replace (the default is 1).
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它接受文本输入、预训练的语言模型、其分词器和要替换的单词数量（默认为 1）。
- en: The text is split into words, and a copy is made for modification.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将文本拆分为单词，并创建一个用于修改的副本。
- en: 'The function iterates `n` times (the default is 1). It does the following each
    time:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 函数迭代 `n` 次（默认为 1）。每次执行以下操作：
- en: Randomly selects a word index
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机选择一个单词索引
- en: Tokenizes the entire text
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对整个文本进行分词
- en: Runs it through the model to get contextualized embeddings
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将其通过模型运行以获取上下文嵌入
- en: Extracts the embedding of the chosen word
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取所选单词的嵌入
- en: Finds similar words based on this embedding (using an undefined `find_similar_words`
    function)
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据此嵌入查找相似单词（使用未定义的 `find_similar_words` 函数）
- en: If similar words are found, it randomly chooses one to replace the original
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果找到相似单词，则随机选择一个来替换原始单词
- en: Finally, it joins the modified words back into a string and returns it.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，它将修改后的单词重新组合成一个字符串并返回。
- en: The default `n`=1 is chosen to make minimal changes while still introducing
    variation. This preserves most of the original meaning and structure. You can
    increase `n` to get more replacements, but higher values might alter the text’s
    meaning more significantly.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的`n`=1是为了在引入变化的同时做出最小的改变。这保留了大部分原始意义和结构。您可以增加`n`以获得更多的替换，但更高的值可能会更显著地改变文本的意义。
- en: This method is more context-aware than simple synonym replacement as it considers
    the word’s usage in the full text when finding replacements. The exact behavior
    will depend on the model and tokenizer used, as well as the implementation of
    the `find_similar_words` function.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 与简单的同义词替换相比，这种方法更注重上下文，因为它在寻找替换词时考虑了单词在全文中的使用情况。确切的行为将取决于所使用的模型和分词器，以及`find_similar_words`函数的实现。
- en: Balancing augmentation and data quality
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 平衡增强和数据质量
- en: While data augmentation can significantly improve LLM performance, we need to
    strike a balance between quantity and quality.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然数据增强可以显著提高大型语言模型（LLM）的性能，但我们需要在数量和质量之间取得平衡。
- en: You should limit the proportion of augmented data in your training set. A common
    practice is to start with a 1:1 ratio of original to augmented data and adjust
    based on model performance.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该限制训练集中增强数据的比例。一种常见的做法是开始时以原始数据与增强数据1:1的比例开始，并根据模型性能进行调整。
- en: Quality filtering
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 质量过滤
- en: 'You can implement quality checks to filter out low-quality augmented samples:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以实施质量检查以过滤掉低质量的增强样本：
- en: '[PRE8]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Human-in-the-loop validation
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人工参与循环验证
- en: For critical applications, incorporate human validation into your augmentation
    pipeline.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 对于关键应用，将人工验证纳入您的增强流程中。
- en: '**Human-in-the-loop** (**HITL**) validation is a control mechanism used in
    AI pipelines where humans are deliberately inserted into automated workflows to
    ensure correctness, especially in tasks involving subjective judgment, sensitive
    content, or critical decision-making. This is particularly important in applications
    where data quality directly affects safety, fairness, or compliance—for example,
    healthcare diagnostics, legal document analysis, or autonomous systems. In the
    context of data augmentation, where the goal is to expand the training dataset
    by generating variations of existing samples, HITL is used to validate whether
    the generated samples are coherent, accurate, and aligned with the intended label
    or task:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**人工参与循环**（**HITL**）验证是一种在人工智能流程中使用的控制机制，其中人类被故意插入到自动化工作流程中，以确保正确性，尤其是在涉及主观判断、敏感内容或关键决策的任务中。这在数据质量直接影响安全、公平或合规性的应用中尤为重要——例如，医疗诊断、法律文件分析或自主系统。在数据增强的背景下，其目标是通过对现有样本生成变体来扩展训练数据集，HITL用于验证生成的样本是否连贯、准确，并与预期的标签或任务保持一致：'
- en: '[PRE9]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This function is designed to manually validate a list of augmented text samples
    by soliciting binary feedback—yes or no—from a human operator. Its presence within
    an augmentation pipeline acknowledges that not all automatically generated data
    can be trusted at face value. The decision to retain or discard a given sample
    is made interactively, reinforcing human oversight in tasks where semantic integrity
    is non-negotiable.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数旨在通过从人类操作员那里获取二元反馈（是或否）来手动验证一系列增强文本样本。它在增强流程中的存在承认了并非所有自动生成数据都可以仅凭表面价值信赖。保留或丢弃给定样本的决定是交互式做出的，这加强了在语义完整性不可协商的任务中的人类监督。
- en: Each iteration of the function’s loop represents a decision point. The human
    validator is shown the generated text and asked to assess whether it meets the
    expected criteria. These criteria are typically based on task-specific requirements
    such as grammaticality, semantic equivalence to the original data, tone appropriateness,
    or domain alignment. For example, in a medical text classification task, a paraphrased
    sentence must preserve all critical clinical entities. A slight shift in terminology
    introduced by an augmentation technique could mislead the model if not caught
    during validation. This is where human evaluation becomes indispensable.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 函数循环的每一迭代代表一个决策点。人类验证者会看到生成的文本，并被要求评估它是否符合预期的标准。这些标准通常基于特定任务的要求，如语法正确性、与原始数据的语义等效性、语气适当性或领域一致性。例如，在医疗文本分类任务中，改写的句子必须保留所有关键的临床实体。如果不在验证期间捕捉到，增强技术引入的术语上的微小变化可能会误导模型。这就是人类评估变得不可或缺的地方。
- en: The logic behind converting the input to lowercase is to handle inconsistent
    user input. Whether the user types `Y`, `y`, or any other casing, the comparison
    becomes case-agnostic. Only if the input is equivalent to `y` does the function
    accept the sample. This binary check is deliberately strict to prevent ambiguous
    approvals. The rejected samples are silently discarded and not logged or returned,
    implying that any further inspection or correction of rejected samples would need
    to be implemented separately.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 将输入转换为小写的逻辑是为了处理不一致的用户输入。无论用户输入`Y`、`y`或任何其他大小写，比较都变得不区分大小写。只有当输入等同于`y`时，函数才接受样本。这种二进制检查故意严格，以防止模糊的批准。被拒绝的样本被静默丢弃，不记录或返回，这意味着任何进一步检查或更正被拒绝样本都需要单独实现。
- en: The function concludes by returning a list of only those samples that have been
    explicitly validated. This output can then be used to expand the training dataset
    with higher confidence in the integrity of the new data points. Importantly, this
    approach does not replace automated quality checks but supplements them in high-stakes
    applications. HITL validation is particularly useful when deploying models in
    environments where false positives or negatives carry high costs, such as legal
    recommendation systems, fraud detection, or autonomous navigation. The manual
    validation process helps to mitigate risks that stem from over-reliance on generative
    augmentation methods that lack explicit semantic guarantees.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 函数通过返回一个仅包含明确验证的样本列表来结束。然后可以使用这些输出以更高的信心扩展训练数据集。重要的是，这种方法并不取代自动质量检查，而是在高风险应用中补充它们。在部署模型的环境中使用HITL验证特别有用，在这些环境中，假阳性或假阴性具有高昂的成本，例如法律推荐系统、欺诈检测或自主导航。人工验证过程有助于减轻过度依赖缺乏明确语义保证的生成增强方法所带来的风险。
- en: 'In a larger system, this kind of function would usually be embedded in a broader
    workflow where automated filters screen out obviously low-quality or irrelevant
    augmentations first. The human validator would only evaluate the borderline or
    high-impact cases. For operational efficiency, the interaction would typically
    be handled via a web interface or integrated annotation tool rather than a command-line
    prompt. However, the function demonstrates the principle in its simplest form:
    human judgment is used as the final arbiter of quality before incorporating augmented
    data into model training.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个更大的系统中，这类功能通常会嵌入到一个更广泛的流程中，其中自动过滤器首先筛选出明显低质量或不相关的增强。人工验证员只会评估边缘或高影响案例。为了提高操作效率，交互通常通过网页界面或集成注释工具而不是命令行提示来处理。然而，这个功能以最简单的方式展示了原理：在将增强数据纳入模型训练之前，人类判断被用作质量最终裁决者。
- en: Evaluating the impact of data augmentation
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估数据增强的影响
- en: To assess the effectiveness of our data augmentation techniques, we need to
    assess their impact on LLM performance.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估我们数据增强技术的有效性，我们需要评估它们对大型语言模型（LLM）性能的影响。
- en: Perplexity
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 感疑度
- en: 'You can measure a model’s perplexity (see [*Chapter 2*](B31249_02.xhtml#_idTextAnchor035))
    on a held-out test set before and after data augmentation to assess whether it
    has improved the model’s ability to predict unseen text:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在数据增强前后，在保留的测试集上测量模型的可疑度（见[*第2章*](B31249_02.xhtml#_idTextAnchor035)），以评估它是否提高了模型预测未见文本的能力：
- en: '[PRE10]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This function, `evaluate_perplexity`, calculates the perplexity of a language
    model on a given test dataset. Here’s a breakdown:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数`evaluate_perplexity`计算给定测试数据集上语言模型的疑惑度。以下是分解：
- en: It takes a pre-trained language model, its tokenizer, and a test dataset as
    input.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它接受一个预训练的语言模型、其分词器和测试数据集作为输入。
- en: The model is set to evaluation mode to disable dropout and other training-specific
    behavior.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型被设置为评估模式以禁用dropout和其他特定于训练的行为。
- en: It initializes variables to track the total loss and total number of tokens
    processed.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它初始化变量以跟踪总损失和总处理令牌数。
- en: 'For each text in the test data, the following is carried out:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于测试数据中的每个文本，执行以下操作：
- en: The text is tokenized and converted to tensors.
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 文本被分词并转换为张量。
- en: The model processes the input, calculating the loss.
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型处理输入，计算损失。
- en: The loss is accumulated, weighted by the number of tokens in the input.
  id: totrans-110
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 损失被累积，并按输入中令牌的数量加权。
- en: 'After processing all texts, it calculates the perplexity using the following
    formula: `exp(total_loss /` `total_tokens)`.'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 处理完所有文本后，它使用以下公式计算疑惑度：`exp(total_loss / total_tokens)`。
- en: This implementation uses the model in a zero-shot manner, treating each input
    as both the context and the target to predict. The use of `torch.no_grad()` ensures
    that no gradients are computed, making the evaluation more efficient.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 此实现以零样本方式使用模型，将每个输入视为上下文和预测的目标。使用`torch.no_grad()`确保不计算梯度，使评估更高效。
- en: This function assumes the model and data are compatible (i.e., the model can
    handle the maximum sequence length of the data). In practice, you might need to
    add checks or truncation to handle very long sequences.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数假设模型和数据兼容（即模型可以处理数据的最大序列长度）。在实际应用中，您可能需要添加检查或截断以处理非常长的序列。
- en: Task-specific metrics
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特定任务指标
- en: 'You can evaluate the model on downstream tasks relevant to your use case, such
    as text classification or question answering:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以对与您的用例相关的下游任务进行模型评估，例如文本分类或问答：
- en: '[PRE11]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This function assesses the performance of a classification model on a test
    dataset:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数评估分类模型在测试数据集上的性能：
- en: It takes a pre-trained classification model, its tokenizer, test data (text),
    and corresponding test labels as inputs.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它接受一个预训练的分类模型、其分词器、测试数据（文本）和相应的测试标签作为输入。
- en: The model is set to evaluation mode to disable dropout and other training-specific
    behavior.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型设置为评估模式以禁用dropout和其他特定于训练的行为。
- en: It processes each text in the test data, tokenizing it and using the model to
    make predictions.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它处理测试数据中的每个文本，对其进行分词，并使用模型进行预测。
- en: 'After processing all texts, it calculates two evaluation metrics:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 处理完所有文本后，它计算两个评估指标：
- en: '**Accuracy**: The proportion of correct predictions out of all predictions
    made.'
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确率**：所有预测中正确预测的比例。'
- en: '**F1 score**: A balanced measure of the model’s precision and recall. The F1
    score is the harmonic mean of **precision** (the ratio of true positive predictions
    to all positive predictions) and **recall** (the ratio of true positive predictions
    to all actual positive instances).'
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**F1分数**：模型精确率和召回率的平衡度量。F1分数是**精确率**（所有正预测中真正预测的比例）和**召回率**（所有实际正实例中真正预测的比例）的调和平均数。'
- en: The F1 score formula is *F1 = 2 * (precision * recall) / (precision +* *recall)*.
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: F1分数的公式是 *F1 = 2 * (精确率 * 召回率) / (精确率 + 召回率)*。
- en: The F1 score ranges from 0 to 1, where 1 indicates perfect precision and recall.
    It’s particularly useful for imbalanced datasets where accuracy alone might be
    misleading. The weighted average calculates F1 for each class and averages them,
    weighted by the number of instances in each class.
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: F1分数的范围从0到1，其中1表示完美的精确度和召回率。对于仅准确率可能具有误导性的不平衡数据集，它特别有用。加权平均计算每个类的F1分数，并按每个类中实例的数量加权平均。
- en: The function returns both the accuracy and F1 score, providing a more comprehensive
    evaluation of the model’s performance across potentially imbalanced classes.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 函数返回准确率和F1分数，提供了对模型在可能不平衡的类别上的性能的更全面评估。
- en: This implementation also uses `torch.no_grad()` for efficiency and assumes that
    the necessary scikit-learn metrics are imported. In practice, you might want to
    add error handling for unexpected model outputs or mismatched prediction/label
    counts.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 此实现还使用`torch.no_grad()`以提高效率，并假设已导入必要的scikit-learn指标。在实际应用中，您可能需要添加错误处理以处理意外的模型输出或预测/标签计数不匹配。
- en: Diversity metrics
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多样性指标
- en: 'It’s important to assess the diversity of your augmented dataset:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 评估增强数据集的多样性很重要：
- en: '[PRE12]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This function takes a collection of texts as input and computes **diversity
    metrics**. Once this has been done, this function returns a dictionary with these
    two metrics:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数接受一组文本作为输入，并计算**多样性指标**。一旦完成，此函数将返回一个包含这两个指标的字典：
- en: '**Vocabulary size** (ranging from 1 to the total number of words): This gives
    an idea of lexical diversity. A high number suggests diverse word usage across
    the texts. This metric splits each text into words, combines all words from all
    texts, and then counts the number of unique words using a **set**. In this context,
    a set refers to a data structure that automatically removes duplicate elements.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词汇量大小**（范围从1到总单词数）：这可以给出词汇多样性的概念。高数值表明文本中使用了多样化的词汇。此指标将每个文本拆分为单词，然后将所有文本中的所有单词合并，并使用**集合**来计算唯一单词的数量。在此上下文中，集合指的是一种数据结构，它自动删除重复元素。'
- en: '**Unique trigrams** (ranging from 1 to the total number of trigrams): These
    indicate character-level diversity. A high number suggests varied character sequences,
    potentially indicating diverse sentence structures or word choices. This metric
    creates trigrams (sequences of three characters) from each text and counts the
    number of unique trigrams using a set that only contains unique elements.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**独特的三元组**（范围从1到三元组的总数）：这些指标表示字符级别的多样性。高数值表明字符序列多样化，可能表明句子结构或词汇选择多样化。此指标通过从每个文本中创建三元组（三个字符的序列）并使用仅包含唯一元素的集合来计算独特三元组的数量。'
- en: These metrics are useful for comparing the diversity of original texts versus
    augmented texts or for assessing the variety in a dataset. However, the results
    should be interpreted in context, as high diversity might indicate incoherence
    or noise in the data.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这些指标可用于比较原始文本与增强文本之间的多样性，或评估数据集中的多样性。然而，结果应在特定背景下进行解读，因为高多样性可能表明数据中的不连贯性或噪声。
- en: By systematically applying these techniques, we can quantify the impact of our
    data augmentation strategies on LLM performance and make informed decisions about
    which techniques to use and how to fine-tune our augmentation pipeline.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 通过系统地应用这些技术，我们可以量化我们的数据增强策略对LLM性能的影响，并就使用哪些技术和如何微调我们的增强流程做出明智的决定。
- en: Summary
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored advanced data augmentation techniques for LLMs,
    covering text manipulation methods, leveraging existing models for data generation,
    multilingual strategies, semantic preservation, quality control, and several metrics.
    We also discussed the importance of balancing augmentation with data quality and
    provided practical Python implementations for various techniques.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了针对LLM的高级数据增强技术，包括文本操作方法、利用现有模型进行数据生成、多语言策略、语义保留、质量控制以及多个指标。我们还讨论了平衡增强与数据质量的重要性，并提供了各种技术的实用Python实现。
- en: In the next chapter, we’ll focus on handling large datasets for LLM training.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将专注于处理LLM训练的大型数据集。
