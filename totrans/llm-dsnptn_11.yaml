- en: '11'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fine-Tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this design pattern, you’ll learn about effective strategies for **fine-tuning**
    pre-trained language models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fine-tuning LLMs addresses a fundamental optimization problem in transfer learning:
    Pre-training on large datasets helps LLMs learn general language skills and knowledge,
    but the differences between the pre-training data and the data for specific tasks
    can reduce performance. Fine-tuning uses a smaller, carefully chosen dataset for
    the task to update the model, making it better suited to the task’s needs. This
    process retains useful knowledge from pre-training while refining the model’s
    ability to perform effectively on the target task.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll be covering the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing transfer learning and fine-tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Strategies for freezing and unfreezing layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning rate scheduling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Domain-specific techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Few-shot and zero-shot fine-tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continual fine-tuning and catastrophic forgetting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing transfer learning and fine-tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use the following code blocks to demonstrate transfer learning with
    GPT-2, handling model initialization, data processing, and the fine-tuning workflow.
    We will use the Transformers library and WikiText dataset to fine-tune a pre-trained
    language model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we load and initialize the GPT-2 model and tokenizer with configured
    padding:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, the following code block manages dataset loading and text tokenization
    with a sequence length of `512`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we set up our training configuration, initialize the trainer, and
    execute fine-tuning:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The code sets up a `fine_tune_lm` function that prepares and executes language
    model fine-tuning. It first tokenizes the dataset using batched processing, then
    configures training parameters including epochs, batch sizes, warmup steps, and
    weight decay. Next, it initializes a trainer with the model, arguments, and datasets,
    runs the training process, and finally saves the fine-tuned model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Batch size has a significant impact on both training stability and performance.
    Larger batch sizes allow for more parallelization and faster training on capable
    hardware but require more memory. They can provide more stable gradient estimates
    by averaging over more examples, potentially leading to better convergence. However,
    very large batches may generalize poorly compared to smaller ones, as they can
    cause the model to converge to sharper minima. Smaller batch sizes introduce more
    noise in gradient updates, which can help escape local minima and potentially
    find better solutions, but training takes longer. Finding the optimal batch size
    involves balancing hardware constraints, convergence stability, and generalization
    performance for your specific model and dataset.
  prefs: []
  type: TYPE_NORMAL
- en: When fine-tuning LLMs, we often don’t need to update all the model’s parameters.
    Selectively **freezing** and **unfreezing** layers can lead to more efficient
    and effective fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Strategies for freezing and unfreezing layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The idea behind selectively freezing and unfreezing layers is rooted in how
    knowledge is structured and distributed across a deep neural network. Lower layers
    in LLMs tend to capture more general language representations—such as syntax,
    part-of-speech, and morphology—while higher layers are more specialized and task-dependent.
    This hierarchical organization allows us to leverage the general-purpose linguistic
    knowledge already encoded in the early layers while fine-tuning only the task-specific
    portions of the network.
  prefs: []
  type: TYPE_NORMAL
- en: By freezing the lower layers, we preserve their pre-trained capabilities and
    prevent catastrophic forgetting, which can occur if the entire model is updated
    indiscriminately on a narrow domain dataset. This also drastically reduces the
    number of trainable parameters, leading to lower memory usage and faster convergence.
    Meanwhile, selectively unfreezing the upper layers allows the model to adapt its
    representations for new tasks or domains without disturbing its core language
    understanding capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at how we can implement this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we implement selective layer freezing by disabling gradients for all
    layers except the specified number of final layers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we manage progressive layer unfreezing across training epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we configure optimized training parameters for the gradual unfreezing
    process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This implementation introduces two key strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: '`freeze_layers`: This function freezes all layers except for the last `num_layers_to_freeze`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gradual_unfreeze`: This function gradually unfreezes layers over the course
    of training'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The gradual unfreezing approach allows the model to adapt its higher-level features
    first, then progressively fine-tune lower-level features. This can lead to better
    performance and help prevent catastrophic forgetting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Catastrophic forgetting is reduced because of the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Layer freezing preserves knowledge in earlier layers by disabling gradient updates
    for them, maintaining the fundamental representations learned during pre-training
    while only adapting task-specific later layers. This retains the model’s general
    knowledge while allowing adaptation to new tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradual unfreezing implements a staged approach where training begins with only
    the final layers unfrozen (which contain more task-specific representations),
    then progressively unfreezes earlier layers. This allows the model to adapt higher-level
    features first before making more fundamental changes, providing a gentle transition
    that helps maintain previously learned patterns.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The training configuration supports these approaches with a carefully balanced
    learning rate, increased warmup steps, and higher weight decay that further prevents
    drastic parameter shifts. The increased epochs allow for more gradual adaptation
    while save and evaluation checkpoints provide monitoring to prevent overfitting
    during the unfreezing process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Together, these techniques create a more controlled fine-tuning process that
    preserves general knowledge while effectively adapting to new tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Finetuning performance can be significantly improved by applying appropriate
    learning rate scheduling, which we’ll visit next.
  prefs: []
  type: TYPE_NORMAL
- en: Learning rate scheduling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As mentioned, proper **learning rate scheduling** is often used for effective
    fine-tuning. The following code demonstrates common learning rate scheduling techniques
    for LLM fine-tuning, offering both **linear** and **cosine warmup** strategies
    to optimize training:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we set up the scheduling framework with the required imports and function
    initialization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we configure optimized training parameters with improved defaults:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we implement learning rate scheduling with dynamic warmup steps calculation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This implementation provides two common learning rate scheduling strategies:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`0` to the initial `lr` during warmup, then decreases linearly to `0`. We discussed
    this in [*Chapter 7*](B31249_07.xhtml#_idTextAnchor108) under the *Loss functions
    and optimization strategies* section. However, it is important to note that we
    need to use the same warmup schedule for fine-tuning as well. Warmup helps prevent
    sudden weight updates early in training, ensuring smoother convergence.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cosine schedule with warmup**: Similar to the linear schedule, but in this
    case, the decrease follows a cosine curve.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: These scheduling strategies can help stabilize training and potentially lead
    to better convergence.
  prefs: []
  type: TYPE_NORMAL
- en: Domain-specific fine-tuning techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When fine-tuning LLMs for specific domains, we often need to adapt our approach.
    Let’s look at an example of domain-specific fine-tuning for a scientific corpus.
    The following code implements domain-specific fine-tuning for scientific text
    using custom dataset preparation and training configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we set up the dataset preparation for scientific text with a specified
    block size and language modeling collator:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we handle dataset preparation for both training and evaluation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we configure optimized training parameters for scientific domain adaptation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This implementation includes several domain-specific considerations:'
  prefs: []
  type: TYPE_NORMAL
- en: '`TextDataset` to handle domain-specific text files'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Smaller batch size**: Scientific texts often have longer sequences, so we
    reduce the batch size'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**More epochs**: Domain adaptation might require more training iterations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regular evaluation**: After each epoch, we evaluate the model to track validation
    loss and key domain-specific metrics, ensuring proper adaptation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When fine-tuning for specific domains, consider the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Adapting the vocabulary for domain-specific terms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using domain-specific evaluation metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Potentially modifying the model architecture for domain-specific features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following section, we’ll explore a couple of strategies for fine-tuning
    models with little to no labeled data from the target domain.
  prefs: []
  type: TYPE_NORMAL
- en: Few-shot and zero-shot fine-tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Few-shot** and **zero-shot learning** are powerful techniques for adapting
    LLMs to new tasks with minimal or no task-specific training data. Let’s implement
    a few-shot fine-tuning approach:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We create a prompt that includes a few examples of the task:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The model is then fine-tuned on this prompt-based dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `few_shot_fine_tune` function implements few-shot fine-tuning, which adapts
    a pre-trained model to new tasks using minimal examples. It takes a model, tokenizer,
    dataset, and configuration parameters (`num_shots=5`, `num_epochs=3`), then prepares
    a small subset of the data with `prepare_few_shot_dataset`, configures training
    with `TrainingArguments` (specifying output locations, batch sizes, and optimization
    parameters), initializes a `Trainer` object with these components, executes the
    training process via `trainer.train()`, and finally returns the trained model
    wrapped in the `Trainer` object—all using the Hugging Face Transformers library
    framework commonly used for language models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The fine-tuned model can then generalize to new instances of the task:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This implementation demonstrates few-shot fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: For zero-shot learning, you would typically rely on the pre-trained model’s
    ability to understand task descriptions without any task-specific examples or
    fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Continual fine-tuning and catastrophic forgetting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Continual fine-tuning** involves adapting a model to new tasks while retaining
    performance on previous tasks. However, this can lead to **catastrophic forgetting**.
    Catastrophic forgetting in LLMs refers to the phenomenon where a model loses previously
    learned information when fine-tuned on new tasks or data without appropriate mechanisms
    to preserve prior knowledge.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s implement a simple strategy to mitigate this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we calculate parameter importance and implement **elastic weight consolidation**
    (**EWC**) loss for preserving critical weights:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then implement the following code, which manages sequential training on
    multiple tasks while maintaining previous knowledge:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we define optimized training parameters for continual learning:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This implementation introduces several key concepts for continual fine-tuning:'
  prefs: []
  type: TYPE_NORMAL
- en: '**EWC**: We implement a simplified version of EWC, which adds a penalty term
    to the loss function to prevent drastic changes to important parameters for previous
    tasks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Importance computation**: We calculate the importance of each parameter based
    on its gradient magnitude on the previous task'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Continual fine-tuning loop**: We fine-tune the model on each task sequentially,
    using EWC to mitigate forgetting'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evaluation on all tasks**: After fine-tuning on each new task, we evaluate
    the model’s performance on all previous tasks to monitor forgetting'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The key considerations for continual fine-tuning are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Balance between plasticity and stability**: EWC helps maintain this balance,
    allowing the model to learn new tasks while preserving knowledge of previous ones'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Computational overhead**: Computing importance and applying EWC increases
    the computational cost of training'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Task similarity**: The effectiveness of continual fine-tuning can depend
    on the similarity between tasks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Additional strategies to consider for mitigating catastrophic forgetting include
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Gradient episodic memory** (**GEM**): In this approach, a small episodic
    memory of data from previous tasks is stored and used to constrain the gradient
    updates on new tasks, as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Progressive neural networks**: Here, a new “column” of layers for each new
    task is created, while lateral connections to previously learned features are
    maintained.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learning without Forgetting (LwF)**: In this approach, knowledge distillation
    is employed to preserve the model’s performance on previous tasks:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: These advanced techniques can be particularly useful when fine-tuning LLMs across
    a diverse range of tasks or domains.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fine-tuning patterns for LLMs encompass a wide range of techniques, from basic
    transfer learning to advanced continual learning strategies. By mastering these
    patterns, you can effectively adapt pre-trained models to new tasks and domains,
    optimize performance, and mitigate issues such as catastrophic forgetting. As
    the field of LLMs continues to evolve, staying updated with the latest fine-tuning
    techniques will be crucial for developing state-of-the-art language models tailored
    to specific applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the key takeaways from this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fine-tuning adapts pre-trained LLMs**: Fine-tuning is the key process for
    adapting general-purpose, pre-trained LLMs to specific tasks and datasets, bridging
    the gap between general language understanding and specialized performance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Layer management is crucial**: Strategically freezing and unfreezing layers
    (especially gradual unfreezing) is critical for balancing the preservation of
    pre-trained knowledge with adaptation to the new task'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learning rate scheduling stabilizes training**: Using learning rate schedules
    with warmup (linear or cosine) is essential for stable and effective fine-tuning,
    preventing drastic early updates and promoting convergence'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Domain/task specificity matters**: Techniques such as domain-specific vocabulary
    adaptation, custom data handling, and few-shot/zero-shot approaches are vital
    for maximizing performance on specialized tasks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Catastrophic forgetting must be addressed**: In continual learning scenarios,
    techniques such as EWC, GEM, and others are necessary to prevent the model from
    losing previously learned information when trained on new tasks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will explore model pruning in the next chapter. Model pruning systematically
    removes redundant or less important neural connections in LLMs while preserving
    core functionality, essentially creating a lighter, more efficient version that
    maintains similar performance but requires fewer computational resources.
  prefs: []
  type: TYPE_NORMAL
