- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: Fine-Tuning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调
- en: In this design pattern, you’ll learn about effective strategies for **fine-tuning**
    pre-trained language models.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个设计模式中，你将学习到微调预训练语言模型的有效策略**。
- en: 'Fine-tuning LLMs addresses a fundamental optimization problem in transfer learning:
    Pre-training on large datasets helps LLMs learn general language skills and knowledge,
    but the differences between the pre-training data and the data for specific tasks
    can reduce performance. Fine-tuning uses a smaller, carefully chosen dataset for
    the task to update the model, making it better suited to the task’s needs. This
    process retains useful knowledge from pre-training while refining the model’s
    ability to perform effectively on the target task.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 微调大型语言模型（LLMs）解决了迁移学习中的一个基本优化问题：在大数据集上预训练有助于LLMs学习通用的语言技能和知识，但预训练数据与特定任务数据之间的差异可能会降低性能。微调使用较小且精心选择的任务数据集来更新模型，使其更适合任务需求。这个过程保留了预训练中的有用知识，同时提高了模型在目标任务上有效执行的能力。
- en: 'In this chapter, we’ll be covering the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Implementing transfer learning and fine-tuning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现迁移学习和微调
- en: Strategies for freezing and unfreezing layers
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层冻结和解冻策略
- en: Learning rate scheduling
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率调度
- en: Domain-specific techniques
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 领域特定技术
- en: Few-shot and zero-shot fine-tuning
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 少样本和零样本微调
- en: Continual fine-tuning and catastrophic forgetting
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 持续微调和灾难性遗忘
- en: Implementing transfer learning and fine-tuning
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现迁移学习和微调
- en: 'We will use the following code blocks to demonstrate transfer learning with
    GPT-2, handling model initialization, data processing, and the fine-tuning workflow.
    We will use the Transformers library and WikiText dataset to fine-tune a pre-trained
    language model:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下代码块来演示使用GPT-2的迁移学习，包括模型初始化、数据处理和微调工作流程。我们将使用Transformers库和WikiText数据集来微调预训练语言模型：
- en: 'First, we load and initialize the GPT-2 model and tokenizer with configured
    padding:'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们使用配置的填充加载并初始化GPT-2模型和标记器：
- en: '[PRE0]'
  id: totrans-14
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then, the following code block manages dataset loading and text tokenization
    with a sequence length of `512`:'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，以下代码块使用`512`的序列长度管理数据集加载和文本标记化：
- en: '[PRE1]'
  id: totrans-16
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Finally, we set up our training configuration, initialize the trainer, and
    execute fine-tuning:'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们设置训练配置，初始化训练器，并执行微调：
- en: '[PRE2]'
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The code sets up a `fine_tune_lm` function that prepares and executes language
    model fine-tuning. It first tokenizes the dataset using batched processing, then
    configures training parameters including epochs, batch sizes, warmup steps, and
    weight decay. Next, it initializes a trainer with the model, arguments, and datasets,
    runs the training process, and finally saves the fine-tuned model.
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 代码设置了一个`fine_tune_lm`函数，用于准备和执行语言模型微调。它首先使用批量处理对数据集进行标记化，然后配置包括epoch、批大小、预热步骤和权重衰减的训练参数。接下来，它使用模型、参数和数据集初始化训练器，运行训练过程，并最终保存微调后的模型。
- en: Batch size has a significant impact on both training stability and performance.
    Larger batch sizes allow for more parallelization and faster training on capable
    hardware but require more memory. They can provide more stable gradient estimates
    by averaging over more examples, potentially leading to better convergence. However,
    very large batches may generalize poorly compared to smaller ones, as they can
    cause the model to converge to sharper minima. Smaller batch sizes introduce more
    noise in gradient updates, which can help escape local minima and potentially
    find better solutions, but training takes longer. Finding the optimal batch size
    involves balancing hardware constraints, convergence stability, and generalization
    performance for your specific model and dataset.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 批大小对训练稳定性和性能都有显著影响。较大的批大小允许更多的并行化，并在强大的硬件上更快地训练，但需要更多的内存。它们可以通过平均更多示例来提供更稳定的梯度估计，从而可能实现更好的收敛。然而，与较小的批大小相比，非常大的批大小可能泛化较差，因为它们可能导致模型收敛到更尖锐的局部最小值。较小的批大小在梯度更新中引入更多噪声，这有助于逃离局部最小值，并可能找到更好的解决方案，但训练时间更长。找到最佳批大小需要平衡特定模型和数据集的硬件约束、收敛稳定性和泛化性能。
- en: When fine-tuning LLMs, we often don’t need to update all the model’s parameters.
    Selectively **freezing** and **unfreezing** layers can lead to more efficient
    and effective fine-tuning.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当微调LLMs时，我们通常不需要更新所有模型的参数。选择性地**冻结**和**解冻**层可以导致更高效和有效的微调。
- en: Strategies for freezing and unfreezing layers
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层冻结和解冻策略
- en: The idea behind selectively freezing and unfreezing layers is rooted in how
    knowledge is structured and distributed across a deep neural network. Lower layers
    in LLMs tend to capture more general language representations—such as syntax,
    part-of-speech, and morphology—while higher layers are more specialized and task-dependent.
    This hierarchical organization allows us to leverage the general-purpose linguistic
    knowledge already encoded in the early layers while fine-tuning only the task-specific
    portions of the network.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 选择性冻结和解冻层的理念源于知识在深度神经网络中的结构和分布方式。LLM中的底层倾向于捕获更多通用语言表示，例如句法、词性和形态，而高层则更专业且与任务相关。这种层次结构允许我们利用早期层中已经编码的通用语言知识，同时仅微调网络的任务特定部分。
- en: By freezing the lower layers, we preserve their pre-trained capabilities and
    prevent catastrophic forgetting, which can occur if the entire model is updated
    indiscriminately on a narrow domain dataset. This also drastically reduces the
    number of trainable parameters, leading to lower memory usage and faster convergence.
    Meanwhile, selectively unfreezing the upper layers allows the model to adapt its
    representations for new tasks or domains without disturbing its core language
    understanding capabilities.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 通过冻结底层，我们保留了它们的预训练能力并防止了灾难性遗忘，这在整个模型在狭窄领域数据集上无差别更新时可能会发生。这也大大减少了可训练参数的数量，从而降低了内存使用并加快了收敛速度。同时，选择性解冻上层允许模型在不干扰其核心语言理解能力的情况下，为新任务或领域调整其表示。
- en: 'Let’s look at how we can implement this:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们如何实现这一点：
- en: 'First, we implement selective layer freezing by disabling gradients for all
    layers except the specified number of final layers:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们通过禁用除指定数量的最终层之外的所有层的梯度来实现选择性层冻结：
- en: '[PRE3]'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Then, we manage progressive layer unfreezing across training epochs:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们在训练周期中管理渐进层解冻：
- en: '[PRE4]'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Finally, we configure optimized training parameters for the gradual unfreezing
    process:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们为渐进解冻过程配置优化的训练参数：
- en: '[PRE5]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This implementation introduces two key strategies:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 此实现引入了两种关键策略：
- en: '`freeze_layers`: This function freezes all layers except for the last `num_layers_to_freeze`'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`freeze_layers`：此函数冻结所有层，除了最后`num_layers_to_freeze`层'
- en: '`gradual_unfreeze`: This function gradually unfreezes layers over the course
    of training'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gradual_unfreeze`：此函数在训练过程中逐步解冻层'
- en: The gradual unfreezing approach allows the model to adapt its higher-level features
    first, then progressively fine-tune lower-level features. This can lead to better
    performance and help prevent catastrophic forgetting.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 渐进解冻方法允许模型首先适应其高级特征，然后逐步微调低级特征。这可以提高性能并帮助防止灾难性遗忘。
- en: 'Catastrophic forgetting is reduced because of the following reasons:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 由于以下原因，灾难性遗忘得到了减少：
- en: Layer freezing preserves knowledge in earlier layers by disabling gradient updates
    for them, maintaining the fundamental representations learned during pre-training
    while only adapting task-specific later layers. This retains the model’s general
    knowledge while allowing adaptation to new tasks.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层冻结通过禁用早期层的梯度更新来保留这些层中的知识，在预训练期间保持学习到的基本表示，同时仅适应特定任务的后期层。这保留了模型的一般知识，同时允许对新任务进行适应。
- en: Gradual unfreezing implements a staged approach where training begins with only
    the final layers unfrozen (which contain more task-specific representations),
    then progressively unfreezes earlier layers. This allows the model to adapt higher-level
    features first before making more fundamental changes, providing a gentle transition
    that helps maintain previously learned patterns.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 渐进解冻实施了一种分阶段的方法，其中训练开始时仅解冻最终层（其中包含更多特定于任务的表示），然后逐步解冻早期层。这允许模型首先适应高级特征，然后再进行更根本的变化，提供一种温和的过渡，有助于保持之前学习到的模式。
- en: The training configuration supports these approaches with a carefully balanced
    learning rate, increased warmup steps, and higher weight decay that further prevents
    drastic parameter shifts. The increased epochs allow for more gradual adaptation
    while save and evaluation checkpoints provide monitoring to prevent overfitting
    during the unfreezing process.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练配置通过精心平衡的学习率、增加预热步骤和更高的权重衰减来支持这些方法，进一步防止参数发生剧烈变化。增加的周期允许更渐进的适应，同时保存和评估检查点提供监控，以防止解冻过程中的过拟合。
- en: Together, these techniques create a more controlled fine-tuning process that
    preserves general knowledge while effectively adapting to new tasks.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术共同创造了一个更受控制的微调过程，在适应新任务的同时保留了一般知识。
- en: Finetuning performance can be significantly improved by applying appropriate
    learning rate scheduling, which we’ll visit next.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 通过应用适当的 学习率调度，可以显著提高微调性能，我们将在下一节中探讨。
- en: Learning rate scheduling
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习率调度
- en: 'As mentioned, proper **learning rate scheduling** is often used for effective
    fine-tuning. The following code demonstrates common learning rate scheduling techniques
    for LLM fine-tuning, offering both **linear** and **cosine warmup** strategies
    to optimize training:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，适当的 **学习率调度** 通常用于有效的微调。以下代码演示了LLM微调的常见学习率调度技术，提供了**线性**和**余弦预热**策略以优化训练：
- en: 'First, we set up the scheduling framework with the required imports and function
    initialization:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们使用所需的导入和函数初始化设置调度框架：
- en: '[PRE6]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, we configure optimized training parameters with improved defaults:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们使用改进的默认值配置优化训练参数：
- en: '[PRE7]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Finally, we implement learning rate scheduling with dynamic warmup steps calculation:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们实现具有动态预热步骤计算的 学习率调度：
- en: '[PRE8]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This implementation provides two common learning rate scheduling strategies:'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此实现提供了两种常见的学习率调度策略：
- en: '`0` to the initial `lr` during warmup, then decreases linearly to `0`. We discussed
    this in [*Chapter 7*](B31249_07.xhtml#_idTextAnchor108) under the *Loss functions
    and optimization strategies* section. However, it is important to note that we
    need to use the same warmup schedule for fine-tuning as well. Warmup helps prevent
    sudden weight updates early in training, ensuring smoother convergence.'
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在预热期间，从`0`线性减少到初始`lr`。我们曾在[*第7章*](B31249_07.xhtml#_idTextAnchor108)的*损失函数和优化策略*部分讨论过这一点。然而，需要注意的是，我们还需要在微调中使用相同的预热调度。预热有助于防止训练早期突然的权重更新，从而确保更平滑的收敛。
- en: '**Cosine schedule with warmup**: Similar to the linear schedule, but in this
    case, the decrease follows a cosine curve.'
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**带预热余弦调度**：类似于线性调度，但在此情况下，下降遵循余弦曲线。'
- en: These scheduling strategies can help stabilize training and potentially lead
    to better convergence.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这些调度策略可以帮助稳定训练并可能带来更好的收敛。
- en: Domain-specific fine-tuning techniques
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 领域特定微调技术
- en: 'When fine-tuning LLMs for specific domains, we often need to adapt our approach.
    Let’s look at an example of domain-specific fine-tuning for a scientific corpus.
    The following code implements domain-specific fine-tuning for scientific text
    using custom dataset preparation and training configuration:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 当为特定领域微调LLM时，我们通常需要调整我们的方法。让我们看看一个针对科学语料库的领域特定微调示例。以下代码使用自定义数据集准备和训练配置实现了科学文本的领域特定微调：
- en: 'First, we set up the dataset preparation for scientific text with a specified
    block size and language modeling collator:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们使用指定的块大小和语言模型整理器设置科学文本数据集的准备：
- en: '[PRE9]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, we handle dataset preparation for both training and evaluation:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们处理训练和评估的数据集准备：
- en: '[PRE10]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Finally, we configure optimized training parameters for scientific domain adaptation:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们为科学领域适应配置优化训练参数：
- en: '[PRE11]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This implementation includes several domain-specific considerations:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 此实现包括几个领域特定考虑因素：
- en: '`TextDataset` to handle domain-specific text files'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TextDataset`处理领域特定文本文件'
- en: '**Smaller batch size**: Scientific texts often have longer sequences, so we
    reduce the batch size'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**较小的批量大小**：科学文本通常有更长的序列，因此我们减少批量大小'
- en: '**More epochs**: Domain adaptation might require more training iterations'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更多epoch**：领域适应可能需要更多的训练迭代'
- en: '**Regular evaluation**: After each epoch, we evaluate the model to track validation
    loss and key domain-specific metrics, ensuring proper adaptation'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定期评估**：在每个epoch之后，我们评估模型以跟踪验证损失和关键领域特定指标，确保适当的适应。'
- en: 'When fine-tuning for specific domains, consider the following steps:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 当为特定领域进行微调时，请考虑以下步骤：
- en: Adapting the vocabulary for domain-specific terms
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适应领域特定术语的词汇表
- en: Using domain-specific evaluation metrics
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用领域特定评估指标
- en: Potentially modifying the model architecture for domain-specific features
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能修改模型架构以适应领域特定特征
- en: In the following section, we’ll explore a couple of strategies for fine-tuning
    models with little to no labeled data from the target domain.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨几种用于微调模型且来自目标领域几乎没有标记数据的策略。
- en: Few-shot and zero-shot fine-tuning
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 少样本和零样本微调
- en: '**Few-shot** and **zero-shot learning** are powerful techniques for adapting
    LLMs to new tasks with minimal or no task-specific training data. Let’s implement
    a few-shot fine-tuning approach:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**少样本**和**零样本学习**是强大的技术，可以用于将LLMs适应新任务，而无需或仅需最少量的特定任务训练数据。让我们实现一个少样本微调方法：'
- en: 'We create a prompt that includes a few examples of the task:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建一个包含任务几个示例的提示：
- en: '[PRE12]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The model is then fine-tuned on this prompt-based dataset:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型随后在基于提示的数据集上进行微调：
- en: '[PRE13]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The `few_shot_fine_tune` function implements few-shot fine-tuning, which adapts
    a pre-trained model to new tasks using minimal examples. It takes a model, tokenizer,
    dataset, and configuration parameters (`num_shots=5`, `num_epochs=3`), then prepares
    a small subset of the data with `prepare_few_shot_dataset`, configures training
    with `TrainingArguments` (specifying output locations, batch sizes, and optimization
    parameters), initializes a `Trainer` object with these components, executes the
    training process via `trainer.train()`, and finally returns the trained model
    wrapped in the `Trainer` object—all using the Hugging Face Transformers library
    framework commonly used for language models.
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`few_shot_fine_tune`函数实现了少样本微调，它使用最少量的示例将预训练模型适应新任务。它接受一个模型、分词器、数据集和配置参数（`num_shots=5`，`num_epochs=3`），然后使用`prepare_few_shot_dataset`准备数据的小子集，使用`TrainingArguments`配置训练（指定输出位置、批大小和优化参数），使用这些组件初始化一个`Trainer`对象，通过`trainer.train()`执行训练过程，并最终返回包裹在`Trainer`对象中的训练模型——所有这些操作都使用Hugging
    Face Transformers库框架，该框架是语言模型中常用的一种。'
- en: 'The fine-tuned model can then generalize to new instances of the task:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 微调后的模型可以推广到新任务的新实例：
- en: '[PRE14]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This implementation demonstrates few-shot fine-tuning.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 此实现展示了少样本微调。
- en: For zero-shot learning, you would typically rely on the pre-trained model’s
    ability to understand task descriptions without any task-specific examples or
    fine-tuning.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 对于零样本学习，你通常会依赖预训练模型理解任务描述的能力，无需任何特定任务的示例或微调。
- en: Continual fine-tuning and catastrophic forgetting
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 持续微调和灾难性遗忘
- en: '**Continual fine-tuning** involves adapting a model to new tasks while retaining
    performance on previous tasks. However, this can lead to **catastrophic forgetting**.
    Catastrophic forgetting in LLMs refers to the phenomenon where a model loses previously
    learned information when fine-tuned on new tasks or data without appropriate mechanisms
    to preserve prior knowledge.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**持续微调**涉及在保留先前任务性能的同时适应新任务。然而，这可能导致**灾难性遗忘**。在LLMs中，灾难性遗忘指的是在没有适当机制来保留先前知识的情况下，模型在针对新任务或数据微调时丢失先前学习的信息。'
- en: 'Let’s implement a simple strategy to mitigate this:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实施一个简单的策略来减轻这一点：
- en: 'First, we calculate parameter importance and implement **elastic weight consolidation**
    (**EWC**) loss for preserving critical weights:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们计算参数重要性并实现**弹性权重巩固**（**EWC**）损失以保留关键权重：
- en: '[PRE15]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We then implement the following code, which manages sequential training on
    multiple tasks while maintaining previous knowledge:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们接下来实现以下代码，该代码在多个任务上管理顺序训练的同时保持先前知识：
- en: '[PRE16]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Finally, we define optimized training parameters for continual learning:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们为持续学习定义优化训练参数：
- en: '[PRE17]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This implementation introduces several key concepts for continual fine-tuning:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 此实现引入了持续微调的几个关键概念：
- en: '**EWC**: We implement a simplified version of EWC, which adds a penalty term
    to the loss function to prevent drastic changes to important parameters for previous
    tasks'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**EWC**：我们实现EWC的简化版本，它将惩罚项添加到损失函数中，以防止对先前任务的重要参数产生剧烈变化'
- en: '**Importance computation**: We calculate the importance of each parameter based
    on its gradient magnitude on the previous task'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重要性计算**：我们根据先前任务上梯度的幅度计算每个参数的重要性'
- en: '**Continual fine-tuning loop**: We fine-tune the model on each task sequentially,
    using EWC to mitigate forgetting'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**持续微调循环**：我们按顺序在每个任务上微调模型，使用EWC来减轻遗忘'
- en: '**Evaluation on all tasks**: After fine-tuning on each new task, we evaluate
    the model’s performance on all previous tasks to monitor forgetting'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对所有任务的评估**：在针对每个新任务进行微调后，我们评估模型在所有先前任务上的性能以监控遗忘'
- en: 'The key considerations for continual fine-tuning are as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 持续微调的关键考虑因素如下：
- en: '**Balance between plasticity and stability**: EWC helps maintain this balance,
    allowing the model to learn new tasks while preserving knowledge of previous ones'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可塑性和稳定性之间的平衡**：EWC有助于维持这种平衡，使模型能够在学习新任务的同时保留对先前任务的知识'
- en: '**Computational overhead**: Computing importance and applying EWC increases
    the computational cost of training'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算开销**：计算重要性和应用EWC增加了训练的计算成本'
- en: '**Task similarity**: The effectiveness of continual fine-tuning can depend
    on the similarity between tasks'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任务相似性**：持续微调的有效性可能取决于任务之间的相似性'
- en: 'Additional strategies to consider for mitigating catastrophic forgetting include
    the following:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到缓解灾难性遗忘的策略，以下是一些额外的策略：
- en: '**Gradient episodic memory** (**GEM**): In this approach, a small episodic
    memory of data from previous tasks is stored and used to constrain the gradient
    updates on new tasks, as follows:'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**梯度周期性记忆（GEM）**：在此方法中，存储并使用来自先前任务的小周期性数据记忆来约束新任务上的梯度更新，如下所示：'
- en: '[PRE18]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '**Progressive neural networks**: Here, a new “column” of layers for each new
    task is created, while lateral connections to previously learned features are
    maintained.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**渐进式神经网络**：在这里，为每个新任务创建一个新的“列”层，同时保持到先前学习特征的横向连接。'
- en: '**Learning without Forgetting (LwF)**: In this approach, knowledge distillation
    is employed to preserve the model’s performance on previous tasks:'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无遗忘学习（LwF）**：在此方法中，采用知识蒸馏来保留模型在先前任务上的性能：'
- en: '[PRE19]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: These advanced techniques can be particularly useful when fine-tuning LLMs across
    a diverse range of tasks or domains.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这些高级技术当在多样化的任务或领域微调LLM时特别有用。
- en: Summary
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Fine-tuning patterns for LLMs encompass a wide range of techniques, from basic
    transfer learning to advanced continual learning strategies. By mastering these
    patterns, you can effectively adapt pre-trained models to new tasks and domains,
    optimize performance, and mitigate issues such as catastrophic forgetting. As
    the field of LLMs continues to evolve, staying updated with the latest fine-tuning
    techniques will be crucial for developing state-of-the-art language models tailored
    to specific applications.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大型语言模型（LLM）的微调模式包括一系列技术，从基本的迁移学习到高级的持续学习策略。通过掌握这些模式，您可以有效地将预训练模型适应于新任务和领域，优化性能，并缓解灾难性遗忘等问题。随着LLM领域的持续发展，跟上最新的微调技术对于开发针对特定应用的尖端语言模型至关重要。
- en: 'Here are the key takeaways from this chapter:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的关键要点如下：
- en: '**Fine-tuning adapts pre-trained LLMs**: Fine-tuning is the key process for
    adapting general-purpose, pre-trained LLMs to specific tasks and datasets, bridging
    the gap between general language understanding and specialized performance'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**微调适应预训练LLM**：微调是将通用、预训练的LLM适应于特定任务和数据集的关键过程，它弥合了通用语言理解与特定性能之间的差距'
- en: '**Layer management is crucial**: Strategically freezing and unfreezing layers
    (especially gradual unfreezing) is critical for balancing the preservation of
    pre-trained knowledge with adaptation to the new task'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**层管理至关重要**：战略性地冻结和解冻层（尤其是逐步解冻）对于在保留预训练知识与新任务适应之间取得平衡至关重要'
- en: '**Learning rate scheduling stabilizes training**: Using learning rate schedules
    with warmup (linear or cosine) is essential for stable and effective fine-tuning,
    preventing drastic early updates and promoting convergence'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习率调度稳定训练**：使用带有预热（线性或余弦）的学习率调度对于稳定和有效的微调至关重要，可以防止剧烈的早期更新并促进收敛'
- en: '**Domain/task specificity matters**: Techniques such as domain-specific vocabulary
    adaptation, custom data handling, and few-shot/zero-shot approaches are vital
    for maximizing performance on specialized tasks'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**领域/任务特定性很重要**：诸如领域特定词汇适应、自定义数据处理以及少样本/零样本方法等技术对于在特定任务上最大化性能至关重要'
- en: '**Catastrophic forgetting must be addressed**: In continual learning scenarios,
    techniques such as EWC, GEM, and others are necessary to prevent the model from
    losing previously learned information when trained on new tasks'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**必须解决灾难性遗忘问题**：在持续学习场景中，EWC、GEM等技术在训练新任务时防止模型丢失先前学习的信息是必要的'
- en: We will explore model pruning in the next chapter. Model pruning systematically
    removes redundant or less important neural connections in LLMs while preserving
    core functionality, essentially creating a lighter, more efficient version that
    maintains similar performance but requires fewer computational resources.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一章探讨模型剪枝。模型剪枝系统地从LLM中移除冗余或不太重要的神经连接，同时保留核心功能，本质上创建了一个更轻、更高效的版本，它保持了相似的性能但需要更少的计算资源。
