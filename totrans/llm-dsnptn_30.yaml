- en: '30'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Agentic Patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this final chapter, we’ll explore patterns for creating more autonomous and
    goal-directed AI agents using LLMs. You’ll learn about goal-setting and planning
    in LLM-based agents, implementing memory and state management, and strategies
    for decision-making and action selection. We’ll cover techniques for learning
    and adaptation in agentic LLM systems and discuss the ethical considerations and
    safety measures necessary when developing such systems.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you’ll be able to design and implement sophisticated
    AI agents powered by LLMs, opening up new possibilities for autonomous AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will be covering the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to agentic AI systems based on LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goal-setting and planning in LLM-based agents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing memory and state management for LLM agents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision-making and action selection in LLM-based agents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning and adaptation in agentic LLM systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ethical considerations and safety in LLM-based agentic AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Future prospects of agentic AI using LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to agentic AI systems based on LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Agentic AI systems using LLMs are designed to operate autonomously, make decisions,
    and take actions to achieve specified goals. These systems combine the powerful
    language understanding and generation capabilities of LLMs with goal-oriented
    behavior and environmental interaction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by implementing a basic structure for an LLM-based agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Here, the `LLMAgent` class is initialized with an LLM (`llm`) and a list of
    possible actions (`action_space`). It also maintains a memory of observations
    and a `current_goal`, which will be used to guide the agent’s actions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we define two methods: `set_goal`, which allows the agent to set its
    goal, and `perceive`, which enables the agent to take in observations from the
    environment and store them in its memory.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we use the `think` method to generate a thorough process based on the
    agent’s goal and recent observations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The agent asks the language model for advice on the next step by providing a
    context string, which includes the current goal and the last five observations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the agent has a thought, it must decide on the next action. The `decide`
    method uses the thought to generate a context, asking the LLM to pick the best
    action from the available options:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, the `act` method simulates taking an action by randomly selecting an
    outcome (success, failure, or an unexpected result). In real scenarios, this would
    involve interacting with the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the `run_step` method orchestrates the entire process of thinking,
    deciding, acting, and perceiving the outcome, completing one cycle of interaction
    with the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Now that we understand the fundamental principles, let’s translate these concepts
    into code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s implement a basic LLM-based agent, establishing the core structure for
    autonomous operation. The agent is initialized with a hypothetical language model
    (`llm`) and a set of actions. It sets a goal and perceives the environment to
    begin interacting with it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following `for` loop, the agent runs for five steps, and each thought,
    action, and outcome is printed to show how the agent interacts with its environment
    over time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Having established the fundamentals of agent behavior, let’s explore more advanced
    capabilities. The following section focuses on goal-setting and planning, enabling
    the agent to proactively work toward complex objectives.
  prefs: []
  type: TYPE_NORMAL
- en: Goal-setting and planning in LLM-based agents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To enhance our agent with more sophisticated goal-setting and planning capabilities,
    let’s implement a hierarchical goal structure and a planning mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we define a `HierarchicalGoal` class; this class allows the agent to
    break down large tasks into smaller subgoals:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The agent can complete these subgoals step by step, marking each as completed
    when done.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we have a `PlanningAgent` class, which inherits from `LLMAgent` but adds
    the ability to handle hierarchical goals. It stores goals in a stack, working
    through subgoals as they are completed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The `think` method now also includes planning. If no current plan exists, it
    asks the LLM to generate a step-by-step plan to achieve the current goal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, the `create_plan` method generates a plan by prompting the LLM with the
    current goal and the list of actions. The generated plan is split into individual
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The `update_goals` method checks whether the current goal is complete. If it
    is, it moves on to the next goal or subgoal and resets the plan accordingly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The `run_step` method orchestrates the goal-setting and planning process, updating
    the goals as necessary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Let’s take a look at an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code snippet, the agent operates with a hierarchical goal
    to “escape the room.” As the agent runs through multiple steps, it works through
    its subgoals, such as finding the key and unlocking the door, with each step updating
    the agent’s internal goal stack and plan:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In real-world applications, AI agent planning outputs from LLMs require constraints
    and validation due to the LLM’s potential to generate impractical, unsafe, or
    constraint-violating plans; therefore, techniques such as rule-based systems,
    simulations, human-in-the-loop review, formal verification, and API/type validations
    are essential to ensure that generated plans adhere to physical, legal, ethical,
    and operational limitations, ultimately enhancing safety, reliability, and effectiveness.
  prefs: []
  type: TYPE_NORMAL
- en: Having demonstrated the agent’s ability to pursue hierarchical goals, the next
    step is to enhance its capacity to learn from past experiences. The next section
    introduces a sophisticated memory system, enabling the agent to retain context
    and recall relevant information when making decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing memory and state management for LLM agents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To improve our agent’s ability to maintain context and learn from past experiences,
    let’s implement a more sophisticated memory system. This will enable the agent
    to recall relevant past observations when deciding on actions.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we define the `MemoryEntry` class, which represents an entry in the
    agent’s memory. Each entry contains the text of the observation and its corresponding
    embedding vector, which helps with similarity searches:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we define the `EpisodicMemory` class; this handles the agent’s memory,
    storing a fixed number of observations (`capacity`). This memory can grow up to
    the specified limit, at which point older entries are removed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code uses content-based episodic memory, which leverages semantic
    similarity search. The memory stores past observations (episodes) as text along
    with their vector embeddings, and retrieves relevant memories based on the semantic
    similarity (using cosine similarity) between a query embedding and the stored
    embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The `retrieve_relevant` method searches for the most relevant past observations
    based on cosine similarity, returning the top *k* matching entries.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we define the `MemoryAwareAgent` class; this class extends `PlanningAgent`
    by integrating an episodic memory system. This allows the agent to store and retrieve
    relevant past experiences during decision-making:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The `think` function defined in the following code incorporates relevant past
    experiences. The agent retrieves memories similar to its current goal and uses
    these in the context provided to the LLM when deciding what to do next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code snippet orchestrates an AI agent’s decision-making process
    by first retrieving relevant memories based on the current goal, then constructing
    a comprehensive context for the LLM that includes the goal, current plan, recent
    observations, and retrieved memories, and finally utilizing the LLM to generate
    a response that determines the agent’s next action or thought based on the provided
    contextual information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s check out an example usage of the memory-aware agent. In this example,
    the agent is enhanced with memory capabilities. It now uses its past experiences
    to inform its decisions and actions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Now that our agent can remember and recall past experiences, we’ll focus on
    making better decisions. The next section introduces a structured approach to
    action selection, allowing the agent to choose the most effective action using
    an LLM. Keep in mind that memory retrieval is similarity-based, which works best
    when embedding quality is high.
  prefs: []
  type: TYPE_NORMAL
- en: Decision-making and action selection in LLM-based agents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To improve the agent’s decision-making capabilities, we can introduce a more
    structured approach to action selection, evaluating potential actions based on
    multiple factors.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first define the `ActionEvaluator` class, which uses the LLM to evaluate
    actions based on three key criteria: relevance to the current goal, probability
    of success, and potential impact. These evaluations help the agent choose the
    best possible action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We then evaluate `"action"` that is passed into the `evaluate_action` function
    as a parameter based on the following criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: Relevance to the current goal (0-1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimated success probability (0-1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Potential impact on overall progress (0-1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, we have the `StrategicDecisionAgent` class, which extends `MemoryAwareAgent`
    to include a more strategic approach to decision-making. It evaluates all possible
    actions, scoring them based on their relevance, success probability, and impact,
    and selects the action with the highest score:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s check out an example usage of `StrategicDecisionAgent`. In this example,
    the agent uses more sophisticated decision-making strategies by evaluating actions
    based on various factors before selecting the optimal one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Over several steps, the agent strategically navigates a maze by continually
    evaluating the best actions to take based on its goal and environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: We will now conclude the chapter by discussing further enhancements for learning,
    ethical considerations, and future prospects for LLM-based agents.
  prefs: []
  type: TYPE_NORMAL
- en: Learning and adaptation in agentic LLM systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To enable our agent to learn and adapt from its experiences, let’s implement
    a simple reinforcement learning mechanism. This will allow the agent to improve
    its performance over time by learning from the outcomes of its actions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We define the `AdaptiveLearningAgent` class, which extends `StrategicDecisionAgent`
    by introducing a simple Q-learning mechanism. It keeps track of `q_values`, which
    represents the expected rewards for taking specific actions in given states. The
    agent uses a learning rate to update these values based on new experiences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, the agent decides its action based on a balance between exploration (trying
    random actions) and exploitation (using actions it has learned to be effective).
    The agent uses its Q-values to select the most rewarding action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We write the `get_state_representation` method to create a simplified representation
    of the current state, including the goal and the most recent observation. This
    state is used to look up and update Q-values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The `update_q_values` method updates the Q-values based on the outcome of the
    agent’s actions. It adjusts the expected reward for a state-action pair, factoring
    in both the immediate reward and the potential future rewards (via `next_max_q`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The `run_step` method now not only performs the standard sequence of thinking,
    deciding, acting, and perceiving but also updates the agent’s Q-values based on
    the outcome. The `compute_reward` method assigns a numeric reward depending on
    whether the outcome was successful, failed, or neutral:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s see an example usage of `AdaptiveLearningAgent`. In this example, the
    agent is designed to explore and learn from a new environment. It uses reinforcement
    learning to gradually improve its ability to make effective decisions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The agent operates for 20 steps, learning from each action it takes. It prints
    out its thoughts, actions, and Q-values, showing how it updates its understanding
    of the environment over time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have equipped our agent with a basic reinforcement learning mechanism,
    allowing it to adapt and improve its decision-making over time, we also need to
    address the ethical implications of such autonomous systems. In the following
    section, we will explore how to integrate ethical safeguards into our agentic
    LLM system to ensure responsible and aligned behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Ethical considerations and safety in LLM-based agentic AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When developing agentic AI systems based on LLMs, it’s crucial to consider
    ethical implications and implement safety measures. To ensure that the agent acts
    within ethical boundaries, we can add an ethical constraint system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The `EthicalConstraint` class defines ethical rules the agent must follow. Each
    rule is described and enforced by a check function (`check_function`), which evaluates
    whether an action violates the ethical constraints.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `EthicalAgent` class extends `AdaptiveLearningAgent` by integrating ethical
    constraints. If the agent selects an action that violates one of its ethical rules,
    it chooses a different action that complies with the rules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The following ethical constraints prevent the agent from causing harm or violating
    privacy. They can be passed to `EthicalAgent` as part of its initialization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: This code defines two Python functions, `no_harm` and `respect_privacy`, which
    serve as ethical constraints for an AI agent. The `no_harm` function checks whether
    a given action contains any keywords related to causing harm (such as “attack”
    or “destroy”), returning `True` if the action is deemed safe and `False` if it
    contains harmful keywords. Similarly, the `respect_privacy` function checks whether
    an action contains keywords related to privacy violations (such as “spy” or “hack”),
    also returning `True` for safe actions and `False` for actions violating privacy.
    These functions are designed to be used by an `EthicalAgent` to ensure its actions
    align with ethical guidelines by preventing it from performing harmful or privacy-violating
    actions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s check out an example usage of `EthicalAgent`. In this example, the agent
    is tasked with gathering information about an alien civilization while following
    ethical guidelines to avoid harm and respect privacy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The agent operates within the constraints, ensuring that its actions do not
    violate ethical rules. It prints out its thoughts, actions, and outcomes as it
    interacts with its environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Future prospects of agentic AI using LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Looking to the future, several exciting possibilities for agentic AI using
    LLMs come to the forefront:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Multi-agent collaboration**: Agents working together in a shared environment
    can exchange information, strategize, and coordinate their actions for more complex
    tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Long-term memory and continual learning**: Agents could maintain a lifelong
    memory and continue learning from their interactions, becoming more intelligent
    over time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integration with robotics and physical world interaction**: As LLM-based
    agents evolve, they may integrate with physical systems, enabling autonomous robots
    to perform tasks in the real world.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Meta-learning and self-improvement**: Future agents could learn to optimize
    their learning processes, becoming better at learning from experiences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Explainable AI and transparent decision-making**: Ensuring that LLM-based
    agents can explain their decisions is crucial for building trust and ensuring
    accountability in AI systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Agent sandboxing and simulation environments**: Creating restricted “walled
    gardens” limits an agent’s access to resources, preventing unintended system impacts,
    while simulation environments, such as those offered by E2B, allow developers
    to replicate real-world scenarios, including interactions with tools, files, and
    simulated web browsers, enabling the identification and mitigation of potential
    issues and risks, including adversarial prompts, thereby enhancing agent reliability
    and safety.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Agentic patterns for LLMs open up exciting possibilities for creating autonomous,
    goal-directed AI systems. By implementing sophisticated planning, memory management,
    decision-making, and learning mechanisms, we can create agents that can operate
    effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Future directions in LLM patterns and their development
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Several promising LLM design patterns are emerging, with innovations coming
    from open source communities as well as frontier model developers, thus shaping
    the design patterns of future models. This section highlights some of these key
    innovations, including **Mixture of Experts** (**MoE**) architectures, **Group
    Relative Policy Optimization** (**GRPO**), **Self-Principled Critique Tuning**
    (**SPCT**), and emerging patterns documented in the publication *OpenAI GPT-4.5
    System* *Card* ([https://openai.com/index/gpt-4-5-system-card/](https://openai.com/index/gpt-4-5-system-card/)).
  prefs: []
  type: TYPE_NORMAL
- en: '**MoE architectures** are a type of neural network architecture where, instead
    of a single large network, there are multiple smaller “expert” networks. During
    inference, a “routing network” dynamically selects and activates only a specific
    subset of these expert networks based on the input, optimizing computational efficiency.
    Unlike dense models, which engage all parameters for every task, MoE models route
    computations through sparsely activated sub-networks. This method reduces redundancy
    and tailors computational resources to the demands of specific tasks, allowing
    for efficient scaling to trillion-parameter models without a proportional increase
    in computational cost. DeepSeek’s implementation exemplifies this approach.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Streamlined reinforcement learning with GRPO** streamlines the reinforcement
    learning process. GRPO is a reinforcement learning technique that generates multiple
    responses to each prompt, calculates their average reward, and uses this baseline
    to evaluate relative performance. This method was introduced by DeepSeek, an open
    source AI company from China. GRPO replaces traditional value networks with group-based
    reward averaging, reducing memory overhead and maintaining stable policy updates.
    By fostering internal self-assessment through comparing multiple reasoning paths,
    GRPO enables adaptive problem solving.'
  prefs: []
  type: TYPE_NORMAL
- en: GRPO enhances safety by incorporating **Kullback–Leibler** (**KL**) **divergence
    penalties**, which constrain policy updates. KL divergence measures how one probability
    distribution diverges from a second, expected probability distribution. In this
    context, it measures the difference between the model’s updated behavior (policy)
    and its previous, baseline behavior. KL divergence penalties are a term that’s
    added to the reward function that penalizes the model if its updated behavior
    deviates too much from that baseline, helping to ensure stability and prevent
    the model from shifting to undesirable behaviors.
  prefs: []
  type: TYPE_NORMAL
- en: The **SPCT framework** integrates self-critique mechanisms directly into the
    model’s reward system, enabling autonomous alignment with ethical guidelines.
    SPCT involves the model generating its own responses, as well as generating internal
    critiques of those responses against predefined principles (e.g., safety guidelines
    and ethical considerations). By generating internal critiques, the model refines
    outputs without relying on external classifiers or human feedback, promoting autonomous
    learning and alignment.
  prefs: []
  type: TYPE_NORMAL
- en: We can also implement **scalable alignment techniques**, which utilize data
    derived from smaller, more easily controlled models to train larger, more capable
    ones, allowing for scalable alignment without requiring a proportional increase
    in human oversight. This technique focuses on improving the model’s steerability,
    its understanding of nuance, and its ability to engage in natural and productive
    conversations, going beyond traditional methods such as **supervised fine-tuning**
    and RLHF to foster safer and more collaborative AI systems. While GPT-4.5 development
    emphasized new, scalable methods to align the model better with human needs and
    intent using data derived from smaller models, future models are expected to incorporate
    more advanced techniques such as GRPO and SPCT to further enhance alignment and
    safety. This focus will continue to ensure steerability, understanding nuance,
    and facilitating more natural conversation.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI has also paved the way for comprehensive safety evaluation via its **Preparedness
    Framework** (*Preparedness Framework (Beta)*, [https://cdn.openai.com/openai-preparedness-framework-beta.pdf](https://cdn.openai.com/openai-preparedness-framework-beta.pdf)).
    This framework represents a core design pattern for responsible AI development
    that involves applying a rigorous evaluation process systematically before model
    deployment. This proactive framework encompasses a wide range of internal and
    external tests, including assessments for disallowed content generation, jailbreak
    robustness, hallucinations, bias, and specific catastrophic risks such as chemical/biological
    weapons, persuasion, cybersecurity threats, and model autonomy. The framework
    also utilizes red teaming exercises and third-party audits to provide comprehensive
    risk assessments, culminating in a classification of the model’s risk level across
    different categories. By thoroughly evaluating potential risks before release,
    OpenAI aims to ensure the safe and responsible deployment of its LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, let’s talk about GPT-4.5’s **instruction hierarchy enforcement**. To
    improve robustness against prompt injection and ensure predictable behavior, models
    are trained to prioritize instructions given in the system message over potentially
    conflicting instructions within the user message, which is evaluated explicitly
    using targeted tests. Future advancements could enhance this pattern by incorporating
    more dynamic and context-aware methods for managing instruction conflicts.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our book on LLM design patterns. In this book, we covered the
    core design patterns. We plan to publish another book on more advanced design
    patterns in the near future, to cover security, safety, governance, and various
    other topics.
  prefs: []
  type: TYPE_NORMAL
