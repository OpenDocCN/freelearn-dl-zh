- en: '30'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '30'
- en: Agentic Patterns
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代理模式
- en: In this final chapter, we’ll explore patterns for creating more autonomous and
    goal-directed AI agents using LLMs. You’ll learn about goal-setting and planning
    in LLM-based agents, implementing memory and state management, and strategies
    for decision-making and action selection. We’ll cover techniques for learning
    and adaptation in agentic LLM systems and discuss the ethical considerations and
    safety measures necessary when developing such systems.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后一章，我们将探讨使用LLM创建更自主和目标导向的AI代理的模式。您将了解基于LLM的代理中的目标设定和规划，实现记忆和状态管理，以及决策和行动选择的策略。我们将涵盖代理式LLM系统中的学习和适应技术，并讨论在开发此类系统时必要的伦理考虑和安全措施。
- en: By the end of this chapter, you’ll be able to design and implement sophisticated
    AI agents powered by LLMs, opening up new possibilities for autonomous AI systems.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您将能够设计和实现由LLM驱动的复杂AI代理，为自主AI系统开辟新的可能性。
- en: 'In this chapter, we will be covering the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Introduction to agentic AI systems based on LLMs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于LLM的代理式AI系统简介
- en: Goal-setting and planning in LLM-based agents
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于LLM的代理中的目标设定和规划
- en: Implementing memory and state management for LLM agents
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为LLM代理实现记忆和状态管理
- en: Decision-making and action selection in LLM-based agents
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于LLM的代理中的决策和行动选择
- en: Learning and adaptation in agentic LLM systems
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理式LLM系统中的学习和适应
- en: Ethical considerations and safety in LLM-based agentic AI
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于LLM的代理式AI的伦理考虑和安全
- en: Future prospects of agentic AI using LLMs
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LLM的代理式AI的未来前景
- en: Introduction to agentic AI systems based on LLMs
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于LLM的代理式AI系统简介
- en: Agentic AI systems using LLMs are designed to operate autonomously, make decisions,
    and take actions to achieve specified goals. These systems combine the powerful
    language understanding and generation capabilities of LLMs with goal-oriented
    behavior and environmental interaction.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LLM的代理式AI系统旨在自主运行、做出决策并采取行动以实现特定目标。这些系统结合了LLM强大的语言理解和生成能力，以及以目标为导向的行为和环境交互。
- en: 'Let’s start by implementing a basic structure for an LLM-based agent:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从实现一个基于LLM的代理的基本结构开始：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here, the `LLMAgent` class is initialized with an LLM (`llm`) and a list of
    possible actions (`action_space`). It also maintains a memory of observations
    and a `current_goal`, which will be used to guide the agent’s actions.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`LLMAgent`类通过一个LLM（`llm`）和一个可能的动作列表（`action_space`）进行初始化。它还维护一个观察记忆和一个`current_goal`，这些将被用来指导代理的动作。
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here, we define two methods: `set_goal`, which allows the agent to set its
    goal, and `perceive`, which enables the agent to take in observations from the
    environment and store them in its memory.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们定义了两个方法：`set_goal`，允许代理设置其目标，以及`perceive`，它使代理能够从环境中获取观察并将它们存储在其记忆中。
- en: 'Next, we use the `think` method to generate a thorough process based on the
    agent’s goal and recent observations:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用`think`方法根据代理的目标和最近观察生成一个详细的过程：
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The agent asks the language model for advice on the next step by providing a
    context string, which includes the current goal and the last five observations.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 代理通过提供一个包含当前目标和最后五个观察结果的上下文字符串，向语言模型请求下一步的建议：
- en: 'Once the agent has a thought, it must decide on the next action. The `decide`
    method uses the thought to generate a context, asking the LLM to pick the best
    action from the available options:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦代理有了想法，它必须决定下一步的行动。`decide`方法使用这个想法生成一个上下文，请求LLM从可用选项中选择最佳行动：
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Then, the `act` method simulates taking an action by randomly selecting an
    outcome (success, failure, or an unexpected result). In real scenarios, this would
    involve interacting with the environment:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，`act`方法通过随机选择一个结果（成功、失败或意外结果）来模拟采取行动。在真实场景中，这将涉及与环境交互：
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Finally, the `run_step` method orchestrates the entire process of thinking,
    deciding, acting, and perceiving the outcome, completing one cycle of interaction
    with the environment:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`run_step`方法协调了思考、决定、行动和感知结果的全过程，完成与环境的一次交互周期：
- en: '[PRE5]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now that we understand the fundamental principles, let’s translate these concepts
    into code.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了基本原理，让我们将这些概念转化为代码。
- en: 'Let’s implement a basic LLM-based agent, establishing the core structure for
    autonomous operation. The agent is initialized with a hypothetical language model
    (`llm`) and a set of actions. It sets a goal and perceives the environment to
    begin interacting with it:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现一个基本的基于LLM的智能体，建立自主操作的核心结构。智能体初始化时包含一个假设的语言模型（`llm`）和一组行动。它设定一个目标并感知环境以开始与之互动：
- en: '[PRE6]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In the following `for` loop, the agent runs for five steps, and each thought,
    action, and outcome is printed to show how the agent interacts with its environment
    over time:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的`for`循环中，智能体运行五步，每个思想、行动和结果都会打印出来，以展示智能体如何随着时间的推移与环境互动：
- en: '[PRE7]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Having established the fundamentals of agent behavior, let’s explore more advanced
    capabilities. The following section focuses on goal-setting and planning, enabling
    the agent to proactively work toward complex objectives.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在确立了智能体行为的基础之后，让我们探索更高级的能力。下一节将重点介绍目标设定和规划，使智能体能够主动向复杂目标迈进。
- en: Goal-setting and planning in LLM-based agents
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于LLM的智能体的目标设定和规划
- en: To enhance our agent with more sophisticated goal-setting and planning capabilities,
    let’s implement a hierarchical goal structure and a planning mechanism.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了增强我们的智能体，使其具有更高级的目标设定和规划能力，让我们实现分层目标结构和规划机制。
- en: 'First, we define a `HierarchicalGoal` class; this class allows the agent to
    break down large tasks into smaller subgoals:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义一个`HierarchicalGoal`类；这个类允许智能体将大任务分解成更小的子目标：
- en: '[PRE8]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The agent can complete these subgoals step by step, marking each as completed
    when done.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体可以逐步完成这些子目标，并在完成后将其标记为完成。
- en: 'Next, we have a `PlanningAgent` class, which inherits from `LLMAgent` but adds
    the ability to handle hierarchical goals. It stores goals in a stack, working
    through subgoals as they are completed:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们有一个`PlanningAgent`类，它继承自`LLMAgent`但增加了处理分层目标的能力。它将目标存储在堆栈中，在完成子目标时进行处理：
- en: '[PRE9]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The `think` method now also includes planning. If no current plan exists, it
    asks the LLM to generate a step-by-step plan to achieve the current goal:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '`think`方法现在也包括规划。如果没有当前计划，它将要求LLM生成一个逐步计划来实现当前目标：'
- en: '[PRE10]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then, the `create_plan` method generates a plan by prompting the LLM with the
    current goal and the list of actions. The generated plan is split into individual
    steps:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，`create_plan`方法通过向LLM提示当前目标和行动列表来生成一个计划。生成的计划被拆分为单独的步骤：
- en: '[PRE11]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The `update_goals` method checks whether the current goal is complete. If it
    is, it moves on to the next goal or subgoal and resets the plan accordingly:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '`update_goals`方法检查当前目标是否完成。如果是，它将转向下一个目标或子目标，并相应地重置计划：'
- en: '[PRE12]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The `run_step` method orchestrates the goal-setting and planning process, updating
    the goals as necessary:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`run_step`方法协调目标设定和规划过程，必要时更新目标：'
- en: '[PRE13]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Let’s take a look at an example.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个例子。
- en: 'In the following code snippet, the agent operates with a hierarchical goal
    to “escape the room.” As the agent runs through multiple steps, it works through
    its subgoals, such as finding the key and unlocking the door, with each step updating
    the agent’s internal goal stack and plan:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码片段中，智能体以“逃离房间”的分层目标进行操作。随着智能体运行多个步骤，它会解决其子目标，例如找到钥匙并打开门，每个步骤都会更新智能体的内部目标堆栈和计划：
- en: '[PRE14]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In real-world applications, AI agent planning outputs from LLMs require constraints
    and validation due to the LLM’s potential to generate impractical, unsafe, or
    constraint-violating plans; therefore, techniques such as rule-based systems,
    simulations, human-in-the-loop review, formal verification, and API/type validations
    are essential to ensure that generated plans adhere to physical, legal, ethical,
    and operational limitations, ultimately enhancing safety, reliability, and effectiveness.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界的应用中，由于LLM可能生成不切实际、不安全或违反约束的计划，因此需要从LLMs的智能体规划输出中进行约束和验证；因此，诸如基于规则的系统、模拟、人工审查、形式验证和API/类型验证等技术对于确保生成的计划遵守物理、法律、伦理和操作限制至关重要，从而提高安全性、可靠性和有效性。
- en: Having demonstrated the agent’s ability to pursue hierarchical goals, the next
    step is to enhance its capacity to learn from past experiences. The next section
    introduces a sophisticated memory system, enabling the agent to retain context
    and recall relevant information when making decisions.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在证明了智能体追求分层目标的能力后，下一步是增强其从以往经验中学习的能力。下一节介绍了一个复杂的记忆系统，使智能体在做出决策时能够保留上下文并回忆相关信息。
- en: Implementing memory and state management for LLM agents
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现LLM智能体的记忆和状态管理
- en: To improve our agent’s ability to maintain context and learn from past experiences,
    let’s implement a more sophisticated memory system. This will enable the agent
    to recall relevant past observations when deciding on actions.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高我们的智能体维持上下文和从过去经验中学习的能力，让我们实现一个更复杂的记忆系统。这将使智能体在决定行动时能够回忆起相关的过去观察。
- en: 'First, we define the `MemoryEntry` class, which represents an entry in the
    agent’s memory. Each entry contains the text of the observation and its corresponding
    embedding vector, which helps with similarity searches:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义了 `MemoryEntry` 类，它代表智能体记忆中的一个条目。每个条目包含观察文本及其相应的嵌入向量，这有助于相似度搜索：
- en: '[PRE15]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Then, we define the `EpisodicMemory` class; this handles the agent’s memory,
    storing a fixed number of observations (`capacity`). This memory can grow up to
    the specified limit, at which point older entries are removed:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义了 `EpisodicMemory` 类；它处理智能体的记忆，存储固定数量的观察（容量）。这个记忆可以增长到指定的限制，此时较老的条目将被移除：
- en: '[PRE16]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The following code uses content-based episodic memory, which leverages semantic
    similarity search. The memory stores past observations (episodes) as text along
    with their vector embeddings, and retrieves relevant memories based on the semantic
    similarity (using cosine similarity) between a query embedding and the stored
    embeddings:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码使用基于内容的情景记忆，它利用语义相似度搜索。记忆将过去的观察（情景）作为文本存储，并附带其向量嵌入，并根据查询嵌入与存储嵌入之间的语义相似度（使用余弦相似度）检索相关记忆：
- en: '[PRE17]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The `retrieve_relevant` method searches for the most relevant past observations
    based on cosine similarity, returning the top *k* matching entries.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`retrieve_relevant` 方法根据余弦相似度搜索最相关的过去观察，返回前 *k* 个匹配条目。'
- en: 'Then, we define the `MemoryAwareAgent` class; this class extends `PlanningAgent`
    by integrating an episodic memory system. This allows the agent to store and retrieve
    relevant past experiences during decision-making:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义了 `MemoryAwareAgent` 类；这个类通过集成一个情景记忆系统扩展了 `PlanningAgent`。这允许智能体在决策过程中存储和检索相关的过去经验：
- en: '[PRE18]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The `think` function defined in the following code incorporates relevant past
    experiences. The agent retrieves memories similar to its current goal and uses
    these in the context provided to the LLM when deciding what to do next:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码中定义的 `think` 函数结合了相关的过去经验。智能体检索与其当前目标相似的记忆，并在决定下一步行动时将这些记忆用于提供给LLM的上下文中：
- en: '[PRE19]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The preceding code snippet orchestrates an AI agent’s decision-making process
    by first retrieving relevant memories based on the current goal, then constructing
    a comprehensive context for the LLM that includes the goal, current plan, recent
    observations, and retrieved memories, and finally utilizing the LLM to generate
    a response that determines the agent’s next action or thought based on the provided
    contextual information.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段通过首先根据当前目标检索相关记忆，然后为LLM构建一个包含目标、当前计划、最近观察和检索到的记忆的全面上下文，最后利用LLM生成一个响应，根据提供的上下文信息确定智能体的下一步行动或思维，来协调AI智能体的决策过程。
- en: 'Let’s check out an example usage of the memory-aware agent. In this example,
    the agent is enhanced with memory capabilities. It now uses its past experiences
    to inform its decisions and actions:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看记忆感知智能体的一个示例用法。在这个例子中，智能体增强了记忆能力。现在它使用其过去经验来指导其决策和行动：
- en: '[PRE20]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Now that our agent can remember and recall past experiences, we’ll focus on
    making better decisions. The next section introduces a structured approach to
    action selection, allowing the agent to choose the most effective action using
    an LLM. Keep in mind that memory retrieval is similarity-based, which works best
    when embedding quality is high.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们智能体能够记住和回忆过去经验，我们将专注于做出更好的决策。下一节介绍了一种结构化的行动选择方法，允许智能体使用LLM选择最有效的行动。请注意，记忆检索是基于相似度的，当嵌入质量高时效果最佳。
- en: Decision-making and action selection in LLM-based agents
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于LLM的智能体的决策和行动选择
- en: To improve the agent’s decision-making capabilities, we can introduce a more
    structured approach to action selection, evaluating potential actions based on
    multiple factors.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高智能体的决策能力，我们可以引入一个更结构化的行动选择方法，根据多个因素评估潜在的行动。
- en: 'We first define the `ActionEvaluator` class, which uses the LLM to evaluate
    actions based on three key criteria: relevance to the current goal, probability
    of success, and potential impact. These evaluations help the agent choose the
    best possible action:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义了`ActionEvaluator`类，该类使用LLM根据三个关键标准来评估动作：与当前目标的关联性、成功的概率以及潜在的影响。这些评估有助于智能体选择最佳可能的动作：
- en: '[PRE21]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We then evaluate `"action"` that is passed into the `evaluate_action` function
    as a parameter based on the following criteria:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们根据以下标准评估传递给`evaluate_action`函数的`"action"`参数：
- en: Relevance to the current goal (0-1)
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与当前目标的关联性（0-1）
- en: Estimated success probability (0-1)
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 估计的成功概率（0-1）
- en: Potential impact on overall progress (0-1)
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对整体进展的潜在影响（0-1）
- en: '[PRE22]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Lastly, we have the `StrategicDecisionAgent` class, which extends `MemoryAwareAgent`
    to include a more strategic approach to decision-making. It evaluates all possible
    actions, scoring them based on their relevance, success probability, and impact,
    and selects the action with the highest score:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有`StrategicDecisionAgent`类，该类通过包括更战略性的决策方法扩展了`MemoryAwareAgent`。它评估所有可能的行为，根据它们的关联性、成功概率和影响进行评分，并选择得分最高的动作：
- en: '[PRE23]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let’s check out an example usage of `StrategicDecisionAgent`. In this example,
    the agent uses more sophisticated decision-making strategies by evaluating actions
    based on various factors before selecting the optimal one:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看`StrategicDecisionAgent`的一个示例用法。在这个例子中，智能体通过评估基于各种因素的动作来选择最佳动作，从而使用更复杂的决策策略：
- en: '[PRE24]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Over several steps, the agent strategically navigates a maze by continually
    evaluating the best actions to take based on its goal and environment:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在几个步骤中，智能体通过不断评估基于其目标和环境的最佳动作来策略性地导航迷宫：
- en: '[PRE25]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We will now conclude the chapter by discussing further enhancements for learning,
    ethical considerations, and future prospects for LLM-based agents.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将通过讨论进一步的学习增强、伦理考虑以及基于LLM的智能体的未来前景来结束本章：
- en: Learning and adaptation in agentic LLM systems
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在智能体LLM系统中进行学习和适应
- en: To enable our agent to learn and adapt from its experiences, let’s implement
    a simple reinforcement learning mechanism. This will allow the agent to improve
    its performance over time by learning from the outcomes of its actions.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使我们的智能体能够从其经验中学习和适应，让我们实现一个简单的强化学习机制。这将允许智能体通过学习其动作的结果来随着时间的推移提高其性能。
- en: 'We define the `AdaptiveLearningAgent` class, which extends `StrategicDecisionAgent`
    by introducing a simple Q-learning mechanism. It keeps track of `q_values`, which
    represents the expected rewards for taking specific actions in given states. The
    agent uses a learning rate to update these values based on new experiences:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了`AdaptiveLearningAgent`类，该类通过引入简单的Q学习机制扩展了`StrategicDecisionAgent`。它跟踪`q_values`，这代表在给定状态下采取特定动作的预期奖励。智能体使用学习率根据新经验更新这些值：
- en: '[PRE26]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Next, the agent decides its action based on a balance between exploration (trying
    random actions) and exploitation (using actions it has learned to be effective).
    The agent uses its Q-values to select the most rewarding action:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，智能体根据探索（尝试随机动作）和利用（使用已学到的有效动作）之间的平衡来决定其动作。智能体使用其Q值来选择最有奖励的动作：
- en: '[PRE27]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We write the `get_state_representation` method to create a simplified representation
    of the current state, including the goal and the most recent observation. This
    state is used to look up and update Q-values:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们编写了`get_state_representation`方法来创建当前状态的简化表示，包括目标和最近的观察。这个状态用于查找和更新Q值：
- en: '[PRE28]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The `update_q_values` method updates the Q-values based on the outcome of the
    agent’s actions. It adjusts the expected reward for a state-action pair, factoring
    in both the immediate reward and the potential future rewards (via `next_max_q`):'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '`update_q_values`方法根据智能体动作的结果更新Q值。它调整状态-动作对的预期奖励，考虑了即时的奖励和潜在的未来的奖励（通过`next_max_q`）：'
- en: '[PRE29]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The `run_step` method now not only performs the standard sequence of thinking,
    deciding, acting, and perceiving but also updates the agent’s Q-values based on
    the outcome. The `compute_reward` method assigns a numeric reward depending on
    whether the outcome was successful, failed, or neutral:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的`run_step`方法不仅执行了标准的思考、决策、行动和感知的顺序，而且还根据结果更新智能体的Q值。`compute_reward`方法根据结果是否成功、失败或中性分配一个数值奖励：
- en: '[PRE30]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Let’s see an example usage of `AdaptiveLearningAgent`. In this example, the
    agent is designed to explore and learn from a new environment. It uses reinforcement
    learning to gradually improve its ability to make effective decisions:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看`AdaptiveLearningAgent`的一个示例用法。在这个例子中，代理被设计为探索并从新环境中学习。它使用强化学习来逐步提高其做出有效决策的能力：
- en: '[PRE31]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The agent operates for 20 steps, learning from each action it takes. It prints
    out its thoughts, actions, and Q-values, showing how it updates its understanding
    of the environment over time:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 代理操作20步，从它采取的每个动作中学习。它打印出它的想法、动作和Q值，展示了它如何随着时间的推移更新对环境的理解：
- en: '[PRE32]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Now that we have equipped our agent with a basic reinforcement learning mechanism,
    allowing it to adapt and improve its decision-making over time, we also need to
    address the ethical implications of such autonomous systems. In the following
    section, we will explore how to integrate ethical safeguards into our agentic
    LLM system to ensure responsible and aligned behavior.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经为我们的代理配备了基本的强化学习机制，允许它随着时间的推移适应和改进其决策能力，我们还需要解决此类自主系统的道德影响。在下一节中，我们将探讨如何将道德保障集成到我们的代理LLM系统中，以确保负责任和一致的行为。
- en: Ethical considerations and safety in LLM-based agentic AI
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于LLM的代理人工智能的道德考量与安全性
- en: 'When developing agentic AI systems based on LLMs, it’s crucial to consider
    ethical implications and implement safety measures. To ensure that the agent acts
    within ethical boundaries, we can add an ethical constraint system:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发基于LLM的代理人工智能系统时，考虑道德影响和实施安全措施至关重要。为确保代理在道德范围内行事，我们可以添加一个道德约束系统：
- en: '[PRE33]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The `EthicalConstraint` class defines ethical rules the agent must follow. Each
    rule is described and enforced by a check function (`check_function`), which evaluates
    whether an action violates the ethical constraints.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '`EthicalConstraint` 类定义了代理必须遵守的道德规则。每个规则都由一个检查函数（`check_function`）进行描述和执行，该函数评估一个动作是否违反了道德约束。'
- en: 'The `EthicalAgent` class extends `AdaptiveLearningAgent` by integrating ethical
    constraints. If the agent selects an action that violates one of its ethical rules,
    it chooses a different action that complies with the rules:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`EthicalAgent`类通过集成道德约束扩展了`AdaptiveLearningAgent`。如果代理选择了一个违反其道德规则的动作，它会选择一个符合规则的不同动作：'
- en: '[PRE34]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The following ethical constraints prevent the agent from causing harm or violating
    privacy. They can be passed to `EthicalAgent` as part of its initialization:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 以下道德约束阻止代理造成伤害或侵犯隐私。它们可以作为初始化的一部分传递给`EthicalAgent`：
- en: '[PRE35]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: This code defines two Python functions, `no_harm` and `respect_privacy`, which
    serve as ethical constraints for an AI agent. The `no_harm` function checks whether
    a given action contains any keywords related to causing harm (such as “attack”
    or “destroy”), returning `True` if the action is deemed safe and `False` if it
    contains harmful keywords. Similarly, the `respect_privacy` function checks whether
    an action contains keywords related to privacy violations (such as “spy” or “hack”),
    also returning `True` for safe actions and `False` for actions violating privacy.
    These functions are designed to be used by an `EthicalAgent` to ensure its actions
    align with ethical guidelines by preventing it from performing harmful or privacy-violating
    actions.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码定义了两个Python函数，`no_harm`和`respect_privacy`，它们作为AI代理的道德约束。`no_harm`函数检查给定的动作是否包含任何与造成伤害相关的关键词（例如“攻击”或“摧毁”），如果动作被认为安全则返回`True`，如果包含有害关键词则返回`False`。同样，`respect_privacy`函数检查动作是否包含与隐私侵犯相关的关键词（例如“间谍”或“黑客”），对于安全动作也返回`True`，对于违反隐私的动作返回`False`。这些函数被设计为供`EthicalAgent`使用，以确保其行动符合道德准则，通过防止其执行有害或侵犯隐私的行为。
- en: 'Let’s check out an example usage of `EthicalAgent`. In this example, the agent
    is tasked with gathering information about an alien civilization while following
    ethical guidelines to avoid harm and respect privacy:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看`EthicalAgent`的一个示例用法。在这个例子中，代理的任务是在遵循道德准则以避免伤害和尊重隐私的同时收集关于外星文明的信息：
- en: '[PRE36]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The agent operates within the constraints, ensuring that its actions do not
    violate ethical rules. It prints out its thoughts, actions, and outcomes as it
    interacts with its environment:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 代理在约束范围内操作，确保其行动不违反道德规则。它在与环境互动时打印出它的想法、动作和结果：
- en: '[PRE37]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Future prospects of agentic AI using LLMs
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于LLM的代理人工智能的未来前景
- en: 'Looking to the future, several exciting possibilities for agentic AI using
    LLMs come to the forefront:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 看向未来，基于LLM的代理人工智能的几个令人兴奋的可能性浮出水面：
- en: '**Multi-agent collaboration**: Agents working together in a shared environment
    can exchange information, strategize, and coordinate their actions for more complex
    tasks.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多智能体协作**：在共享环境中共同工作的智能体可以交换信息、制定策略并协调其行动以完成更复杂的任务。'
- en: '**Long-term memory and continual learning**: Agents could maintain a lifelong
    memory and continue learning from their interactions, becoming more intelligent
    over time.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**长期记忆与持续学习**：智能体可以维持终身记忆，并从其交互中持续学习，随着时间的推移变得越来越智能。'
- en: '**Integration with robotics and physical world interaction**: As LLM-based
    agents evolve, they may integrate with physical systems, enabling autonomous robots
    to perform tasks in the real world.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与机器人及物理世界交互的集成**：随着基于LLM的智能体的发展，它们可能与物理系统集成，使自主机器人能够在现实世界中执行任务。'
- en: '**Meta-learning and self-improvement**: Future agents could learn to optimize
    their learning processes, becoming better at learning from experiences.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**元学习和自我改进**：未来的智能体可以学会优化其学习过程，从而在从经验中学习方面变得更好。'
- en: '**Explainable AI and transparent decision-making**: Ensuring that LLM-based
    agents can explain their decisions is crucial for building trust and ensuring
    accountability in AI systems.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可解释人工智能和透明决策**：确保基于LLM的智能体能够解释其决策对于建立信任和确保AI系统的问责制至关重要。'
- en: '**Agent sandboxing and simulation environments**: Creating restricted “walled
    gardens” limits an agent’s access to resources, preventing unintended system impacts,
    while simulation environments, such as those offered by E2B, allow developers
    to replicate real-world scenarios, including interactions with tools, files, and
    simulated web browsers, enabling the identification and mitigation of potential
    issues and risks, including adversarial prompts, thereby enhancing agent reliability
    and safety.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**智能体沙盒和模拟环境**：创建受限的“围栏花园”限制了智能体对资源的访问，防止了意外系统影响，而模拟环境，如E2B提供的，允许开发者复制现实世界场景，包括与工具、文件和模拟网络浏览器的交互，从而识别和缓解潜在问题和风险，包括对抗性提示，从而提高智能体的可靠性和安全性。'
- en: Summary
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Agentic patterns for LLMs open up exciting possibilities for creating autonomous,
    goal-directed AI systems. By implementing sophisticated planning, memory management,
    decision-making, and learning mechanisms, we can create agents that can operate
    effectively.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的智能体模式为创建自主、目标导向的AI系统开辟了令人兴奋的可能性。通过实施复杂的规划、内存管理、决策和学习机制，我们可以创建能够有效操作的智能体。
- en: Future directions in LLM patterns and their development
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM模式及其发展的未来方向
- en: Several promising LLM design patterns are emerging, with innovations coming
    from open source communities as well as frontier model developers, thus shaping
    the design patterns of future models. This section highlights some of these key
    innovations, including **Mixture of Experts** (**MoE**) architectures, **Group
    Relative Policy Optimization** (**GRPO**), **Self-Principled Critique Tuning**
    (**SPCT**), and emerging patterns documented in the publication *OpenAI GPT-4.5
    System* *Card* ([https://openai.com/index/gpt-4-5-system-card/](https://openai.com/index/gpt-4-5-system-card/)).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 几种有前景的LLM设计模式正在出现，创新来自开源社区以及前沿模型开发者，从而塑造了未来模型的设计模式。本节重点介绍了一些这些关键创新，包括**专家混合**（**MoE**）架构、**组相对策略优化**（**GRPO**）、**自原理解调优**（**SPCT**），以及发表在《OpenAI
    GPT-4.5系统》*Card*中的新兴模式[https://openai.com/index/gpt-4-5-system-card/](https://openai.com/index/gpt-4-5-system-card/)。
- en: '**MoE architectures** are a type of neural network architecture where, instead
    of a single large network, there are multiple smaller “expert” networks. During
    inference, a “routing network” dynamically selects and activates only a specific
    subset of these expert networks based on the input, optimizing computational efficiency.
    Unlike dense models, which engage all parameters for every task, MoE models route
    computations through sparsely activated sub-networks. This method reduces redundancy
    and tailors computational resources to the demands of specific tasks, allowing
    for efficient scaling to trillion-parameter models without a proportional increase
    in computational cost. DeepSeek’s implementation exemplifies this approach.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '**MoE架构**是一种神经网络架构，其中不是单个大型网络，而是有多个较小的“专家”网络。在推理过程中，“路由网络”根据输入动态选择并激活这些专家网络的一个特定子集，从而优化计算效率。与涉及每个任务的所有参数的密集模型不同，MoE模型通过稀疏激活的子网络进行计算路由。这种方法减少了冗余，并将计算资源定制到特定任务的需求，允许在计算成本不成比例增加的情况下高效扩展到万亿参数模型。DeepSeek的实现展示了这种方法。'
- en: '**Streamlined reinforcement learning with GRPO** streamlines the reinforcement
    learning process. GRPO is a reinforcement learning technique that generates multiple
    responses to each prompt, calculates their average reward, and uses this baseline
    to evaluate relative performance. This method was introduced by DeepSeek, an open
    source AI company from China. GRPO replaces traditional value networks with group-based
    reward averaging, reducing memory overhead and maintaining stable policy updates.
    By fostering internal self-assessment through comparing multiple reasoning paths,
    GRPO enables adaptive problem solving.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用GRPO的简化强化学习**简化了强化学习过程。GRPO是一种强化学习技术，它对每个提示生成多个响应，计算它们的平均奖励，并使用这个基线来评估相对性能。这种方法由DeepSeek，一家来自中国的开源AI公司引入。GRPO用基于群体的奖励平均取代了传统的价值网络，减少了内存开销并保持了策略更新的稳定性。通过比较多个推理路径来培养内部自我评估，GRPO能够实现适应性问题解决。'
- en: GRPO enhances safety by incorporating **Kullback–Leibler** (**KL**) **divergence
    penalties**, which constrain policy updates. KL divergence measures how one probability
    distribution diverges from a second, expected probability distribution. In this
    context, it measures the difference between the model’s updated behavior (policy)
    and its previous, baseline behavior. KL divergence penalties are a term that’s
    added to the reward function that penalizes the model if its updated behavior
    deviates too much from that baseline, helping to ensure stability and prevent
    the model from shifting to undesirable behaviors.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: GRPO通过引入**Kullback–Leibler**（**KL**）**散度惩罚**来增强安全性，这些惩罚限制了策略更新。KL散度衡量一个概率分布与第二个预期概率分布的差异。在这种情况下，它衡量模型更新后的行为（策略）与其先前的基线行为之间的差异。KL散度惩罚是添加到奖励函数中的一个术语，如果模型的更新行为与基线差异太大，则会惩罚模型，有助于确保稳定性并防止模型转向不可取的行为。
- en: The **SPCT framework** integrates self-critique mechanisms directly into the
    model’s reward system, enabling autonomous alignment with ethical guidelines.
    SPCT involves the model generating its own responses, as well as generating internal
    critiques of those responses against predefined principles (e.g., safety guidelines
    and ethical considerations). By generating internal critiques, the model refines
    outputs without relying on external classifiers or human feedback, promoting autonomous
    learning and alignment.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**SPCT框架**将自我批评机制直接集成到模型的奖励系统中，使模型能够自主地与道德规范保持一致。SPCT包括模型生成自己的响应，以及根据预定义的原则（例如，安全指南和伦理考量）对这些响应进行内部批评。通过生成内部批评，模型可以在不依赖外部分类器或人类反馈的情况下优化输出，促进自主学习和一致性。'
- en: We can also implement **scalable alignment techniques**, which utilize data
    derived from smaller, more easily controlled models to train larger, more capable
    ones, allowing for scalable alignment without requiring a proportional increase
    in human oversight. This technique focuses on improving the model’s steerability,
    its understanding of nuance, and its ability to engage in natural and productive
    conversations, going beyond traditional methods such as **supervised fine-tuning**
    and RLHF to foster safer and more collaborative AI systems. While GPT-4.5 development
    emphasized new, scalable methods to align the model better with human needs and
    intent using data derived from smaller models, future models are expected to incorporate
    more advanced techniques such as GRPO and SPCT to further enhance alignment and
    safety. This focus will continue to ensure steerability, understanding nuance,
    and facilitating more natural conversation.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以实施 **可扩展的对齐技术**，这些技术利用来自较小、更容易控制的模型的数据来训练更大、更强大的模型，从而在不要求成比例增加人工监督的情况下实现可扩展的对齐。这种技术侧重于提高模型的可控性、对细微差别的理解以及进行自然和富有成效对话的能力，超越了传统的如
    **监督微调** 和 RLHF 等方法，以培养更安全、更协作的 AI 系统。虽然 GPT-4.5 的开发强调了使用来自较小模型的数据来更好地对齐模型以适应人类需求和意图的新、可扩展的方法，但未来的模型预计将结合更先进的
    GRPO 和 SPCT 等技术，以进一步增强对齐和安全。这种关注将继续确保可控性、理解细微差别并促进更自然的对话。
- en: OpenAI has also paved the way for comprehensive safety evaluation via its **Preparedness
    Framework** (*Preparedness Framework (Beta)*, [https://cdn.openai.com/openai-preparedness-framework-beta.pdf](https://cdn.openai.com/openai-preparedness-framework-beta.pdf)).
    This framework represents a core design pattern for responsible AI development
    that involves applying a rigorous evaluation process systematically before model
    deployment. This proactive framework encompasses a wide range of internal and
    external tests, including assessments for disallowed content generation, jailbreak
    robustness, hallucinations, bias, and specific catastrophic risks such as chemical/biological
    weapons, persuasion, cybersecurity threats, and model autonomy. The framework
    also utilizes red teaming exercises and third-party audits to provide comprehensive
    risk assessments, culminating in a classification of the model’s risk level across
    different categories. By thoroughly evaluating potential risks before release,
    OpenAI aims to ensure the safe and responsible deployment of its LLMs.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 还通过其 **准备框架**（*准备框架（Beta）*，[https://cdn.openai.com/openai-preparedness-framework-beta.pdf](https://cdn.openai.com/openai-preparedness-framework-beta.pdf)）铺平了全面安全评估的道路。这个框架代表了负责任
    AI 开发的核心设计模式，它涉及在模型部署之前系统地应用严格的评估流程。这个主动框架包括广泛的内部和外部测试，包括对不允许的内容生成、越狱鲁棒性、幻觉、偏见以及特定灾难性风险（如化学/生物武器、说服、网络安全威胁和模型自主性）的评估。该框架还利用红队演习和第三方审计来提供全面的风险评估，最终对不同类别中模型的危险级别进行分类。通过在发布前彻底评估潜在风险，OpenAI
    旨在确保其 LLMs 的安全且负责任地部署。
- en: Finally, let’s talk about GPT-4.5’s **instruction hierarchy enforcement**. To
    improve robustness against prompt injection and ensure predictable behavior, models
    are trained to prioritize instructions given in the system message over potentially
    conflicting instructions within the user message, which is evaluated explicitly
    using targeted tests. Future advancements could enhance this pattern by incorporating
    more dynamic and context-aware methods for managing instruction conflicts.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们来谈谈 GPT-4.5 的 **指令层次结构执行**。为了提高对提示注入的鲁棒性并确保可预测的行为，模型被训练优先考虑系统消息中给出的指令，而不是用户消息中可能冲突的指令，这通过有针对性的测试进行明确评估。未来的进步可以通过结合更多动态和上下文感知的方法来管理指令冲突，从而增强这种模式。
- en: This concludes our book on LLM design patterns. In this book, we covered the
    core design patterns. We plan to publish another book on more advanced design
    patterns in the near future, to cover security, safety, governance, and various
    other topics.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书关于 LLM 设计模式的讨论就到这里。在这本书中，我们涵盖了核心设计模式。我们计划在不久的将来出版另一本书，介绍更高级的设计模式，涵盖安全、安全、治理以及各种其他主题。
