["```py\n    import torch\n    from torch.quantization import quantize_dynamic\n    # Assume 'model' is a pre-trained LLM (e.g., transformer-based model)\n    model = ...\n    # Apply dynamic quantization on linear layers for INT8 precision\n    quantized_model = quantize_dynamic(\n        model, {torch.nn.Linear}, dtype=torch.qint8\n    )\n    # Check size reduction\n    print(f\"Original model size: {torch.cuda.memory_allocated()} bytes\")\n    print(f\"Quantized model size: {torch.cuda.memory_allocated()} bytes\")\n    ```", "```py\n    import torch\n    import torch.nn as nn\n    import torch.quantization\n    # Define a simple model\n    class SimpleModel(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.fc = nn.Linear(784, 256)\n            self.relu = nn.ReLU()\n            self.out = nn.Linear(256, 10)\n        def forward(self, x):\n            x = self.relu(self.fc(x))\n            return self.out(x)\n    # Create and prepare model for static quantization\n    model_fp32 = SimpleModel()\n    model_fp32.eval()\n    model_fp32.qconfig = torch.quantization.get_default_qconfig(\n        'fbgemm')\n    prepared_model = torch.quantization.prepare(model_fp32)\n    # Calibration step: run representative data through the model\n    # (This example uses random data; replace with real samples)\n    for _ in range(100):\n        sample_input = torch.randn(1, 784)\n        prepared_model(sample_input)\n    # Convert to quantized version\n    quantized_model = torch.quantization.convert(prepared_model)\n    # Model is now statically quantized and ready for inference\n    print(quantized_model\n    ```", "```py\nimport torch\nimport torch.quantization as quant\n# Load pre-trained model\nmodel = ...\n# Convert model to quantization-ready state\nmodel.eval()\nmodel.qconfig = torch.quantization.default_qconfig\n# Prepare for static quantization\nmodel_prepared = quant.prepare(model)\n# Apply quantization\nmodel_quantized = quant.convert(model_prepared)\n```", "```py\nimport torch.quantization as quant\n# Set up QAT\nmodel.train()\nmodel.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n# Prepare for QAT\nmodel_prepared = quant.prepare_qat(model)\n# Training loop (for simplicity, only showing initialization)\nfor epoch in range(num_epochs):\n    train_one_epoch(model_prepared, train_loader, optimizer)\n    validate(model_prepared, val_loader)\n# Convert to quantized version\nmodel_quantized = quant.convert(model_prepared.eval())\n```", "```py\nfrom torch.cuda.amp import autocast\n# Mixed precision in LLM training or inference\nmodel = ...\n# Use FP16 where possible, fall back to FP32 for sensitive computations\nwith autocast():\n    output = model(input_data)\n```", "```py\nimport torch\nimport torch.nn.utils.prune as prune\nimport torch.quantization as quant\n# Step 1: Prune the model\nmodel = ...  # Pre-trained LLM model\nfor name, module in model.named_modules():\n    if isinstance(module, torch.nn.Linear):\n        prune.l1_unstructured(module, name='weight', amount=0.5)  \n        # Prune 50% of the weights\n        prune.remove(module, 'weight')\n# Step 2: Apply dynamic quantization to the pruned model\nquantized_model = quant.quantize_dynamic(\n    model, {torch.nn.Linear}, dtype=torch.qint8  # Convert to INT8 precision\n)\n# Check size reduction\nprint(\"Original model size:\", torch.cuda.memory_allocated())\nprint(\"Quantized model size:\", torch.cuda.memory_allocated())\n```", "```py\n    import torch\n    import torch.nn.functional as F\n    teacher_model = ...  # Larger, fully trained model\n    student_model = ...  # Smaller model to be trained through distillation\n    ```", "```py\n    def distillation_loss(\n        student_outputs, teacher_outputs, temperature=2.0\n    ):\n        teacher_probs = F.softmax(\n            teacher_outputs / temperature, dim=1)\n        student_probs = F.log_softmax(\n            student_outputs / temperature, dim=1)\n        return F.kl_div(student_probs, teacher_probs,\n            reduction='batchmean')\n    ```", "```py\n    optimizer = torch.optim.Adam(student_model.parameters(),\n        lr=1e-4)\n    for batch in train_loader:\n        inputs, _ = batch\n        optimizer.zero_grad()\n    ```", "```py\n        teacher_outputs = teacher_model(inputs)\n        student_outputs = student_model(inputs)\n    ```", "```py\n        loss = distillation_loss(student_outputs, teacher_outputs)\n        loss.backward()\n        optimizer.step()\n    ```", "```py\n    quantized_student_model = quant.quantize_dynamic(\n        student_model, {torch.nn.Linear}, dtype=torch.qint8\n    )\n    ```", "```py\n    print(\"Quantized student model size:\",\n        torch.cuda.memory_allocated())\n    ```"]