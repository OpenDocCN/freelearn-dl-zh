<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Best Practices, Tips, and Tricks</h1>
                </header>
            
            <article>
                
<p class="p1"><span class="s1">In this chapter, we provide some advice on best practices, tips, and tricks for writing and analyzing neuroevolution algorithms. By the end of this chapter, you will know how to start working with the problem at hand, how to tune the hyperparameters of the neuroevolution algorithm, how to use advanced visualization tools, and what metrics can be used to the analyze the algorithm's performance. Also, you will learn about the best coding practices for Python, which will help you in the implementation of your projects.</span></p>
<p class="p1"><span class="s1">In this chapter, we will cover the following topics:</span></p>
<ul class="ul1">
<li class="li3"><span class="s1">Starting with problem analysis</span></li>
<li class="li3"><span class="s1">Selecting the optimal search optimization method</span></li>
<li class="li3"><span class="s1">Using advanced visualization tools</span></li>
<li class="li3"><span class="s1">Tuning hyperparameters and knowing what should be tuned</span></li>
<li class="li3"><span class="s1">Understanding which performance metrics to collect</span></li>
<li class="li1"><span class="s1">Python coding tips and tricks</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Starting with problem analysis</h1>
                </header>
            
            <article>
                
<p class="p3"><span class="s1">Starting with a proper analysis of the problem space is a recipe for success. Neuroevolution is lenient with programmer errors. Such mistakes are a part of the environment, and the evolution process can adapt to them. However, there is a particular category of mistakes that can hinder the evolution process from finding a successful solution: the numerical stability of the evolution process. Most types of activation function are designed to operate in a range of inputs between zero and one. As a result, too large or negative values do not have much influence on the evolution process.</span></p>
<p class="p3"><span class="s1">Thus, you may need to preprocess the input data to avoid these numeric issues. </span><span class="s1">Do not skip the analysis of the input data samples and data preprocessing steps.</span></p>
<p class="p3"><span class="s1">Next, we discuss how to preprocess the input data.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preprocessing data</h1>
                </header>
            
            <article>
                
<p class="p3"><span class="s1">Always examine the range of possible data inputs and check for outliers. If you find that the scale of one input parameter differs from another by an order of magnitude, you need to preprocess the input data samples. Otherwise, the input data features with higher magnitude will have such a significant impact on the training process that they will ultimately outweigh the contribution of the other input data features. However, the small signals produced by the data inputs with small magnitude are often crucial to finding a successful solution. Delicate input signals can characterize subtle but valuable traits in the underlying process.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data standardization</h1>
                </header>
            
            <article>
                
<p class="p2"><span class="s2">Most machine learning algorithms greatly benefit from input data that is normally distributed; that is, it has a <span>mean and unit variance of </span>zero.<span class="Apple-converted-space"> </span>The common approach to scaling input data to have a zero mean and unit variance is given by the following formula:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/13bb8ddc-72e8-472e-b76e-4b115090c91b.png" style="width:7.00em;height:3.25em;"/></p>
<p class="p1"><span class="s1">Note that <img class="fm-editor-equation" src="assets/85488aed-53c1-400e-8237-9a4847110a9b.png" style="width:0.75em;height:0.92em;"/> is a scaled input score, <img class="fm-editor-equation" src="assets/c6a79923-909c-43e0-8b69-29a757133063.png" style="width:0.92em;height:0.92em;"/> is the input data sample, <img class="fm-editor-equation" src="assets/dd2b4597-5c3a-40bf-b0de-42547a1f9881.png" style="width:0.92em;height:0.92em;"/> is the mean of training samples, and <img class="fm-editor-equation" src="assets/6551f51e-dd0f-451c-8716-d3a86ff7ae33.png" style="width:0.75em;height:1.00em;"/> is the standard deviation of the training samples.</span></p>
<p class="p1"><span class="s1">You can use the Scikit-learn Python library to apply standard scaling to your input data samples. The following source code is an example of this:</span></p>
<pre>&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler<br/>&gt;&gt;&gt; data = [[0, 0], [0, 0], [1, 1], [1, 1]]<br/>&gt;&gt;&gt; scaler = StandardScaler()<br/>&gt;&gt;&gt; print(scaler.fit(data))<br/>StandardScaler(copy=True, with_mean=True, with_std=True)<br/>&gt;&gt;&gt; print(scaler.mean_)<br/>[0.5 0.5]<br/>&gt;&gt;&gt; print(scaler.transform(data))<br/>[[-1. -1.]<br/> [-1. -1.]<br/> [ 1. 1.]<br/> [ 1. 1.]]</pre>
<p class="p1"><span class="s1">In the code, we first create the input data samples. After that, <kbd>StandardScaler</kbd> is used to center and scale the input samples. The <span>results</span> of the data transformation are shown in the last lines of the code.</span></p>
<p class="p1"><span class="s1">Another method of data preprocessing is scaling features to fit into a specific range, which we discuss next.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Scaling inputs to a range</h1>
                </header>
            
            <article>
                
<p class="p1"><span class="s1">Scaling inputs to fit into a specific range is another method of data preprocessing. This method is an alternative to standardization. Range scaling produces data samples that lie within a given range between the minimum and maximum values. Often, this method is used to scale input data in a range between zero and one. You can use <kbd>MinMaxScaler</kbd> of the Scikit-learn Python library to scale data in a range, as shown in the following example:</span></p>
<pre>&gt;&gt;&gt; import sklearn.preprocessing<br/>&gt;&gt;&gt; X_train = np.array([[ 1., -1., 2.],<br/>... [ 2., 0., 0.],<br/>... [ 0., 1., -1.]])<br/>...<br/>&gt;&gt;&gt; min_max_scaler = preprocessing.MinMaxScaler()<br/>&gt;&gt;&gt; X_train_minmax = min_max_scaler.fit_transform(X_train)<br/>&gt;&gt;&gt; X_train_minmax<br/>array([[0.5 , 0. , 1. ],<br/>       [1. , 0.5 , 0.33333333],<br/>       [0. , 1. , 0. ]])</pre>
<p class="p1"><span class="s1">The code starts with the creation of a sample dataset and transforms it using the <kbd>MinMaxScaler</kbd> class. In the final output, you can see the results of the range-scaling transformation.</span></p>
<p class="p3"><span class="s1">Sometimes, you need to have data samples with the same units. This type of preprocessing is called <strong>normalization</strong>. We discuss it in the next section.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data normalization</h1>
                </header>
            
            <article>
                
<p class="p1"><span class="s1">Often, your input data features have different units of measurement. For example, in the pole-balancing experiment, the cart position was measured in meters, the linear speed was in meters per second, and the angular speed was in radians per second. It is beneficial to normalize input data to simplify the comparison between input data features.</span></p>
<p class="p1"><span class="s1">The process of normalization effectively eliminates the units of measurement from the input data samples. After that, all the samples will be in the range between zero and one.</span></p>
<p class="p1"><span class="s1">There are different types of normalization in statistics. We already mentioned two methods: data standardization and data range scaling. Additionally, Scikit-learn provides a specialized transformer to perform data normalization, which scales individual samples to what is known as a unit norm. The following code demonstrates how to use it:</span></p>
<pre>&gt;&gt;&gt; import sklearn.preprocessing<br/>&gt;&gt;&gt; X = [[ 1., -1., 2.],<br/>... [ 2., 0., 0.],<br/>... [ 0., 1., -1.]]<br/>&gt;&gt;&gt; X_normalized = preprocessing.normalize(X, norm='l2')<br/>&gt;&gt;&gt; X_normalized <br/>array([[ 0.40..., -0.40..., 0.81...],<br/>       [ 1. ..., 0. ..., 0. ...],<br/>       [ 0. ..., 0.70..., -0.70...]])</pre>
<p class="p2"><span class="s1">The code creates the test data sample and applies normalization to it using the <kbd>l2</kbd> norm and outputs the results.</span></p>
<div class="p2 packt_infobox"><span class="s1">The Scikit-learn library provides the implementation of many other data preprocessing methods. It would be useful for you to get familiar with them. You can find an excellent tutorial at <a href="https://scikit-learn.org/stable/modules/preprocessing.html">https://scikit-learn.org/stable/modules/preprocessing.html</a>.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding the problem domain</h1>
                </header>
            
            <article>
                
<p class="p1"><span class="s1">In this book, some of the experiments we have <span>discussed have been</span> related to real processes in the physical world. To find successful solutions for such processes, you need to understand the underlying physical laws and principles. For example, the problem of balancing the cart-pole apparatus requires us to define a full set of equations of motion to write an accurate simulator of the task.</span></p>
<p class="p1"><span class="s1">Also for most tasks in the field of robotics, you need to write a simulator that uses the correct physical model and equations of the underlying apparatus. You need to fully comprehend the physics of the process to implement the simulator correctly. And even if you use a ready-made simulator, understanding the physical principles implemented in it is incredibly beneficial for you because understanding the dynamics of the real-world process allows you to tune the hyperparameters of the training algorithm appropriately.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Writing good simulators</h1>
                </header>
            
            <article>
                
<p class="p3"><span class="s1">When working on a specific problem, it is crucial to write an appropriate simulator that correctly implements the specifics of the simulated process. If you use such a simulator, you will be able to run long episodes of training, which is impossible when using direct inputs from physical devices.</span></p>
<p class="p3"><span class="s1">A good simulator should allow you to control the duration of the single time step of the simulated process. During neuroevolution, you need to evaluate each individual in the population against the given simulator. Thus, it makes sense during training to make the duration of a single time step as small as possible to increase the execution speed. On the other hand, when a solution has been found and you need to test it by hand, it would be beneficial if you could run the simulator at normal execution speed.</span></p>
<p class="p3"><span class="s1">Also, you can use the existing mature simulators for your projects, which can save you a lot of time. Familiarize yourself with well-established open source simulator packages. They often provide advanced physics simulation as well as a collection of premade building blocks for your virtual robots and environments. You can start your search at <a href="https://github.com/cyberbotics/webots"><span class="s2">https://github.com/cyberbotics/webots</span></a>.</span></p>
<p class="p3"><span class="s1">Next, we discuss how to select the right search optimization method for your experiment.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Selecting the optimal search optimization method</h1>
                </header>
            
            <article>
                
<p class="p3"><span class="s1">In this book, we have presented you with two basic search optimization methods: goal-oriented search and Novelty Search. The former method is more straightforward to implement and easier to understand. However, Novelty Search is handy in cases where the fitness function has a deceptive landscape with many local optima traps.</span></p>
<p class="p3"><span class="s1">In the next section, we briefly discuss both methods to remind you of the details and to help you choose which one to use in a given situation. We start with goal-oriented search.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Goal-oriented search optimization</h1>
                </header>
            
            <article>
                
<p class="p3"><span class="s1">Goal-oriented search optimization is based on measuring the proximity of the solution to the ultimate goal. To calculate the average distance to the goal, it often uses a metric such as the mean squared error. Next, we discuss the particulars of the mean squared error metric.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Mean squared error</h1>
                </header>
            
            <article>
                
<p class="p2"><span class="s2">The mean squared error is the average squared difference between the obtained results and the actual values. It's given by the following formula:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ccef67ed-0e0f-4b78-94a1-3681954b0c54.png" style="width:12.58em;height:3.42em;"/></p>
<p class="p1"><span class="s1">Here <img class="fm-editor-equation" src="assets/e19aaa43-32a0-4e11-9d20-2f402a953a2d.png" style="width:1.00em;height:0.92em;"/> is the estimated value, and <img class="fm-editor-equation" src="assets/ef7783ea-55f1-4fa6-93e5-436e18ae5a1d.png" style="width:0.92em;height:1.00em;"/> is the actual value.</span></p>
<p class="p1"><span class="s1">We used a variation of the mean squared error to define an objective function for the XOR experiment. Next, we discuss goal-oriented metrics for problems related to positioning in Euclidean space.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Euclidean distance</h1>
                </header>
            
            <article>
                
<p class="p2"><span class="s2">Euclidean distance is an appropriate metric for tasks related to navigation through the Euclidean problem space. In the Euclidean problem space, we define the problem goal as a point with particular coordinates.</span></p>
<p class="p2"><span class="s2">Using the Euclidean distance, it is easy to calculate the distance between the position of the navigational agent and the target point it is trying to reach. The following formula calculates the Euclidean distance between two vectors:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/bc216503-a6b1-4b91-aa4e-807473a46650.png" style="width:10.25em;height:3.92em;"/></p>
<p class="p1"><span class="s1">Here <img class="fm-editor-equation" src="assets/14d9fbed-2477-4e01-9c09-e7b526688945.png" style="width:0.83em;height:0.92em;"/> is the Euclidean distance between the vector with agent position <img class="fm-editor-equation" src="assets/198589e9-f5ec-4f59-ad3f-2d06418c83b2.png" style="width:0.92em;height:0.83em;"/> and the vector with the ultimate goal of the agent, <img class="fm-editor-equation" src="assets/2136a3bc-3fe7-4887-9cf1-d7347fff6c6a.png" style="width:0.75em;height:0.92em;"/>. We used this metric to define the objective function of an agent navigating through the maze in <a href="22365f85-3003-4b67-8e1e-cc89fa5e259b.xhtml" target="_blank">Chapter 5</a>, <em>Autonomous Maze Navigation</em>.</span></p>
<p class="p1"><span class="s1">However, the problem of autonomous maze navigation is caused by the deceptive fitness function landscape, which makes goal-oriented search optimization inefficient. Next, we discuss the Novelty Search optimization method, which is able to address this inefficiency.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Novelty Search optimization</h1>
                </header>
            
            <article>
                
<p class="p3"><span class="s1">As we have mentioned, navigation through the maze is a deceptive problem that requires a different approach to define the fitness function. In <a href="22365f85-3003-4b67-8e1e-cc89fa5e259b.xhtml" target="_blank">Chapter 5</a>, <em>Autonomous Maze Navigation</em>, we presented you with a particular maze configuration that produces areas with strong local optima of goal-oriented fitness scores. As a result, the training process can be trapped inside these areas and will fail to produce a successful solution. The Novelty Search optimization method was devised to address issues with deceptive fitness function landscapes.</span></p>
<p class="p3"><span class="s1">Novelty Search rewards the novelty of a solution rather than its proximity to the ultimate goal. Furthermore, the novelty metric, which is used to calculate the fitness score of each solution, completely ignores the proximity of the solution to the ultimate goal. There are two popular approaches to calculate the novelty score:</span></p>
<ul class="ul1">
<li class="li3"><span class="s1">The novelty score is calculated from differences in the architectures of the solutions.</span></li>
<li class="li3"><span class="s1">The novelty is calculated using the unique variations in the behavior of the solutions in the common behavioral space.</span></li>
</ul>
<p class="p3"><span class="s1">The former calculates a difference between the encoding of the current solution and all the previous solutions. The latter compares the result produced by the current solution in the behavioral space to results produced by other solutions.</span></p>
<p class="p3"><span class="s1">We used novelty scores based on the uniqueness of the exposed behavior to define the fitness function of the maze solvers. The trajectory of the maze solver through the maze entirely determines the behavioral space of the agent and can be used to calculate the novelty score. In this case, the novelty score is the Euclidean distance between the trajectory vectors of the current solution and all other solutions.</span></p>
<p class="p3"><span class="s1">Now that we have discussed the importance of choosing an appropriate search optimization method, we can move on to the discussion of another important aspect of a successful experiment. You need to have a good visualization of the results of the experiment to get insights into its performance. Next, we discuss the visualization of results.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Advanced visualization</h1>
                </header>
            
            <article>
                
<p class="p3"><span class="s1">Almost always, proper visualization of inputs and results is crucial to the success of your experiment. With proper visualization, you will get intuitive insights about what has gone wrong and what needs to be fixed.</span></p>
<p class="p3"><span class="s1">Always try to visualize the simulator execution environment. Such visualization can save you hours of debugging when you get an unexpected result. Usually, with adequate visualization, you can see that something has gone wrong at a glance, such as a maze solver that got stuck up in a corner.</span></p>
<p class="p3"><span class="s1">With neuroevolution algorithms, you also need to visualize the performance of the genetic algorithm execution per generation. You need to visualize speciation from generation to generation to see whether the evolutionary process has stagnated. Stagnated evolution fails to create enough species to maintain healthy diversity among solvers. On the other hand, too many species hinder evolution by reducing the chances of reproduction between different organisms.</span></p>
<p class="p1"><span class="s1">Another important visualization allows us to see the topology of the produced phenotype <strong>artificial neural network </strong>(<strong>ANN</strong>). It is useful to visually inspect the topologies of the produced solution to check whether it satisfies our expectations. For example, when we discussed the modular retina problem in <a href="9f3dce4d-2cc7-4307-a704-bfcfe4ad56b4.xhtml" target="_blank">Chapter 8</a>, <em>ES-HyperNEAT and the Retina Problem</em>, it was beneficial to see that modular structures evolved in the topology of the successful solutions.</span></p>
<p class="p1"><span class="s1">You need to familiarize yourself with the standard Python scientific plotting libraries to create adequate visualizations for the results of your experiments. It is essential to develop good practical skills with such visualization libraries as Matplotlib (<a href="https://matplotlib.org/">https://matplotlib.org</a>) and Seaborn (<a href="https://seaborn.pydata.org/">https://seaborn.pydata.org</a>).</span></p>
<p class="p1"><span class="s1">Next, we discuss the importance of hyperparameter tuning for the performance of the neuroevolution process.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tuning hyperparameters</h1>
                </header>
            
            <article>
                
<p class="p3"><span class="s1">With proper tuning of the hyperparameters, you can make tremendous improvements in the training speed and efficiency of the neuroevolution process. Here are some practical tips:</span></p>
<ul class="ul1">
<li class="li3"><span class="s1">Do short runs with different seed values of the random number generator and note how the algorithm performance changes. After that, choose the seed value that gives the best performance and use it for the long runs. </span></li>
<li class="li3"><span class="s1"><span class="s1">You can increase the number of species in the population by decreasing the compatibility threshold and by slightly increasing the value of the disjoint/excess weight coefficient.</span></span></li>
<li class="li3"><span class="s1">If the <span>process of</span> neuroevolution has stumbled while trying to find a solution, try to decrease the value of the NEAT survival threshold. This coefficient maintains the ratio of the best organisms within a population that got the chance to reproduce. By doing this, you increase the quality of the individuals allowed to reproduce based on their fitness score.</span></li>
<li class="li3"><span class="s1">By increasing the maximum stagnation age, you can guarantee that species survive long enough to have a chance to introduce beneficial mutations in the later stages of evolution. Sometimes, such an operation can help to revitalize the neuroevolution process that stalls. However, you should always try with the small stagnation age values (15-20) to initiate a quick rotation of the species, and only increase this parameter significantly if all other tweaks fail.</span></li>
<li class="li3"><span class="s1">After hyperparameter adjustments, do a short run for a few tens of generations to see the performance change dynamics. Pay special attention to the number of species—there should be at least more than one species in the population. Too many species is also a bad sign. Usually, 5 to 20 species is a good range.</span></li>
<li class="li3"><span class="s1">Use the visualization of the experiment results to get quick insights into the performance of the experiment. Never pass up the chance to visualize the ANN topology of the discovered solutions. These visualizations can give you priceless insights on how to tune up the neuroevolutionary process.</span></li>
<li class="li3"><span class="s1">Don't waste your time on long evolutionary runs. If the experiment fails to find a successful solution in 1,000 generations, there's a good chance that something is wrong with your code or the library you are using. For most simple problems, a successful solution can be found even in 100 generations.</span></li>
<li class="li3"><span class="s1">The population size is a critical parameter of the evolutionary process. With large populations, you get great diversity from the very beginning of the process, which boosts the process. However, large populations are hard to compute. Thus, there is always a trade-off between population size and computational cost. As a rule of thumb, if you struggle to find other suitable hyperparameters, try to increase the population size and see if it helps. But be ready to wait extra time for the neuroevolution process to complete.</span></li>
<li class="li3"><span class="s1">Always print the debug information, allowing you to restart the experiment from any stage of evaluation. It is always painful when you find a solution after two days of computation, but due to some programming mistakes your program crashes when trying to output the congratulations message. You need to output at least the random seed value at the beginning of each trial. This can guarantee that you can accurately recreate all the generations of the evolution in case of failure.</span></li>
</ul>
<p class="p3"><span class="s1">Do not underestimate the importance of hyperparameter tuning. Even taking into account that the neuroevolution process can handle many programming mistakes, <span>choosing </span>the right hyperparameters can significantly improve the efficiency of the process. As a result, you will be able to find a successful solution in hundreds of generations instead of thousands or more.</span></p>
<p class="p3"><span class="s1">To compare the performance of the different solutions, you need to use appropriate performance metrics, which we discuss next.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Performance metrics</h1>
                </header>
            
            <article>
                
<p class="p3"><span class="s1">After a successful solution is found, it is crucial to compare it with other solutions to estimate how good it is. There are many important statistical metrics that compare different models.</span></p>
<p class="p3"><span class="s1">Become familiar with concepts such as precision score, recall score, F1 score, ROC AUC, and accuracy. Understanding these metrics will help you to compare the results produced by different models in various classification tasks. Next, we give a brief overview of these metrics.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Precision score</h1>
                </header>
            
            <article>
                
<p class="p3"><span class="s1">The precision score attempts to answer the question of how many among the positive identifications are actually correct. The precision score can be calculated as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/39c39b2d-035b-43e9-8372-a535fd670b69.png" style="width:9.67em;height:2.25em;"/></p>
<p class="p1"><span class="s1">TP is the true positives, and FP is the false positives.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Recall score</h1>
                </header>
            
            <article>
                
<p class="p3"><span class="s1">The recall score answers the question of how many actual positives were identified correctly. The recall score can be given with the following formula:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/be020c54-1660-43ad-973f-46acac16effc.png" style="width:8.25em;height:2.25em;"/></p>
<p class="p1"><span class="s1">TP is the true positives, and FN is the false negatives.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">F1 score</h1>
                </header>
            
            <article>
                
<p class="p3"><span class="s1">The F1 score is a weighted average between the precision and recall scores. The best value of the F1 score is one, and the worst is zero. The F1 score allows measuring classification accuracy specific to a particular class. It can be defined as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/49c867e3-4a74-4078-93bd-428b49e683b1.png" style="width:11.33em;height:2.42em;"/></p>
<p class="p1"><span class="s1">Here <img class="fm-editor-equation" src="assets/42109623-2159-48c7-8c3f-a93054e4a0f5.png" style="width:4.67em;height:1.08em;"/> is the precision score, and <img class="fm-editor-equation" src="assets/967b6214-cb80-4ddd-bade-8097bc89c112.png" style="width:2.75em;height:1.08em;"/> is the recall score relative to a specific positive class.</span></p>
<p>In the next section, we will look at the<strong> </strong><span><strong>Receiver Operating Characteristic</strong> (<strong>ROC</strong>)</span> curve<span> and <strong>Area Under the Curve</strong> (<strong>AUC</strong>).</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">ROC AUC</h1>
                </header>
            
            <article>
                
<p class="p3"><span class="s1">We create the ROC by plotting true positive rates against false positive rates at different thresholds. It shows the performance of the classification model at different thresholds.</span></p>
<p class="p3"><span class="s1">The <strong>true positive rate</strong> (<strong>TPR</strong>) is a synonym of recall, which we discussed earlier. It can be given by this formula:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/bb8832b2-2521-4823-b658-e1b62c928f99.png" style="width:7.58em;height:2.08em;"/></p>
<p class="p1"><span class="s1">The false positive rate is calculated as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/9a3b7842-92a1-42a1-a859-3314c28e4fd7.png" style="width:8.08em;height:2.25em;"/></p>
<p class="p1"><span class="s1">TN is the true negatives.</span></p>
<p class="p1"><span class="s1">The AUC allows us to estimate the discrimination power of the classification model, that is, the ability of the model to correctly rank the random positive points more highly than the random negative points. Here is an example of an ROC curve:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-809 image-border" src="assets/ee44a12d-d1a8-43a0-ba60-0e96da607c86.png" style="width:31.92em;height:24.58em;"/></p>
<div class="p1 CDPAlignCenter CDPAlign packt_figref"><span class="s1">The ROC curve example</span></div>
<p class="p1"><span class="s1">In the graph, you can see the ROC curve example. The larger the AUC is, the more accurate the classifier model. The dashed line shows the worst classifier accuracy. In general, the closer the ROC curve is to the upper-left corner, the better the performance of the classification model.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Accuracy</h1>
                </header>
            
            <article>
                
<p class="p1"><span class="s1">Accuracy is a metric measuring how many correct predictions our model was able to produce. The accuracy is given by the following formula:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/299fba72-dbb7-4085-b6f4-120015f8d9d2.png" style="width:16.08em;height:2.33em;"/></p>
<p class="p1"><span class="s1">FP is the</span> false positives<span class="s1">, and FN is the</span> false negatives.</p>
<div class="p1 packt_infobox"><span class="s1">More details about described metrics can be found at <a href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html">https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html</a>.</span></div>
<p class="p1"><span class="s1">Next, we discuss Python coding tips.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Python coding tips and tricks</h1>
                </header>
            
            <article>
                
<p class="p1"><span class="s1">Having decided to work with Python, it is vital to learn the best coding practices of the language. Here, we provide you with some tips and give you directions to continue with self-learning.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Coding tips and tricks</h1>
                </header>
            
            <article>
                
<p class="p1"><span class="s1">The following coding tips and tricks will help you master Python:</span></p>
<ul class="ul1">
<li class="li3"><span class="s1">You should learn how to use the popular machine learning libraries, such as NumPy (<a href="https://numpy.org/"><span class="s2">https://numpy.org</span></a>), pandas (<a href="https://pandas.pydata.org/"><span class="s2">https://pandas.pydata.org</span></a>), and Scikit-learn (<a href="https://scikit-learn.org/stable/"><span class="s2">https://scikit-learn.org/stable/</span></a>). Mastering these libraries will give you tremendous power in data manipulation and analysis. This will help you to avoid many mistakes, and enable easy debugging of the results collected from the experiments.</span></li>
<li class="li3"><span class="s1">Learn about the object-oriented coding paradigm. This will allow you to write a clean and maintainable source code that is easy to understand. You can start at <a href="https://www.datacamp.com/community/tutorials/python-oop-tutorial"><span class="s2">https://www.datacamp.com/community/tutorials/python-oop-tutorial</span></a>.</span></li>
<li class="li1"><span class="s1">Do not write everything into one huge function. Break your code into smaller reusable blocks implemented as functions or classes that can be reused in multiple projects and easily debugged.</span></li>
<li class="li3"><span class="s1">Print the relevant debug output to understand what is going on in your implementation. Having enough debugging outputs allows you to understand what is going wrong with the execution.</span></li>
<li class="li3"><span class="s1">Write comments <span>related </span>to the functions, classes, and intricate places in your source code. Good comments can significantly help in code understandability. Writing the comments before starting the implementation also helps you to clarify your thoughts.</span></li>
<li class="li3"><span class="s1"><span class="s1">When writing comments to the function, describe all the input and output parameters and their default values, if any exist.</span></span></li>
<li class="li3"><span class="s1">If you decide to continue with Python, spend some time learning about the <span>Python standard </span>libraries. Python is a mature programming language with many utility functions embedded into its standard libraries. It also has many functions allowing advanced data manipulations, which can be used in machine learning tasks. More details about standard Python libraries can be found at <a href="https://docs.python.org/3/library/index.html"><span class="s2">https://docs.python.org/3/library/index.html</span></a>.</span></li>
<li class="li3"><span class="s1">Follow the standard Python source code conventions when giving names to variables and classes. Adhering to the standard naming conventions makes your code more readable and easy to understand for anyone experienced with Python. You can find more details at <a href="https://docs.python-guide.org/writing/style/"><span class="s2">https://docs.python-guide.org/writing/style/</span></a> and <a href="https://www.python.org/dev/peps/pep-0008/"><span class="s2">https://www.python.org/dev/peps/pep-0008/</span></a>.</span></li>
<li class="li3"><span class="s1">Make yourself familiar with modern version control systems, such as Git. A <span><strong>Version Control System</strong> (</span><strong>VCS</strong>) is a potent tool at your disposal that may save you hours and even days of attempts to recover your lost work caused by a hard drive crash. You can learn about Git at <a href="https://github.github.com/training-kit/downloads/github-git-cheat-sheet.pdf"><span class="s2">https://github.github.com/training-kit/downloads/github-git-cheat-sheet.pdf</span></a> and <a href="https://www.atlassian.com/git/tutorials"><span class="s2">https://www.atlassian.com/git/tutorials</span></a>.</span></li>
<li class="li3"><span class="s1">Learn about online code repositories such as GitHub (<a href="https://github.com">https://github.com</a>) and Bitbucket (<a href="https://bitbucket.org/"><span class="s2">https://bitbucket.org</span></a>), where you can share your source code and study the source code of other data scientists.</span></li>
</ul>
<p class="p1"><span class="s1">Another important aspect of writing good implementations is setting up the working environment correctly and using adequate programming tools.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Working environment and programming tools</h1>
                </header>
            
            <article>
                
<p class="p3"><span class="s1">It is always a good idea to use one of the mature Python package managers, such as the Anaconda Distribution, to set up your working environment properly. As an additional benefit, you will get a lot of free scientific and machine learning packages that are ready to be installed with a single command. Furthermore, the Anaconda Distribution handles the management of all indirect dependencies and helps you keep all your packages up to date. You can find the Anaconda Distribution at <a href="https://www.anaconda.com/distribution/"><span class="s2">https://www.anaconda.com/distribution/</span></a>.</span></p>
<p class="p3"><span class="s1">Always create a new virtual Python environment for each of your experiments. After that, if something goes</span><span class="s1"> wrong with dependencies, you will be able to clean everything with one command and start from scratch. The new Python environment can be created with the Anaconda Distribution as follows:</span></p>
<pre><strong>$ conda create --name &lt;name&gt;</strong><br/><strong>$ conda activate &lt;name&gt;</strong></pre>
<p class="p1"><span class="s1">When creating a new environment, always specify the exact Python version you are planning to use in it. Providing the exact version will help you to avoid many surprises caused by incompatibility. The Python version can be defined for the new environment as follows:</span></p>
<pre><strong>$ conda create --name &lt;name&gt; python=3.5</strong></pre>
<p class="p1"><span class="s1">If you need to use a new dependency in your project, first check that the appropriate installation package exists in Anaconda Cloud. By using libraries from Anaconda Cloud, you can avoid the problems of indirect dependency installation. Furthermore, some frameworks, such as TensorFlow, require the installation of additional system drivers and headers. This task can be very cumbersome and require additional expertise.</span></p>
<p class="p1"><span class="s1">Use a good code editor that supports code completion, documentation browsing, and maintaining virtual Python environments. A good editor to start with is Visual Studio Code—a free editor provided by Microsoft. You can find it at <a href="https://code.visualstudio.com/"><span class="s2">https://code.visualstudio.com</span></a>.</span></p>
<p class="p1"><span class="s1">Make yourself familiar with modern Linux systems, such as Ubuntu. Most machine learning libraries are much easier to use with Linux. This is especially true for libraries that use GPU acceleration. More details about Ubuntu and its installation can be found at <a href="https://ubuntu.com/"><span class="s2">https://ubuntu.com</span></a>.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="p1"><span class="s1">In this chapter, we provided you with practical tips that we hope will make your life easier. You learned about the standard methods of data preprocessing and about conventional statistical metrics that can be used to evaluate the performance of the models you created. Finally, you learned how to improve your coding skills and where to look for additional information about Python and machine learning topics.</span></p>
<p>In the next chapter, we will look at a few concluding remarks based on what we have learned in the book and where we can use the concepts we have learned in the future.</p>


            </article>

            
        </section>
    </body></html>