["```py\nfrom torch.utils.data import DataLoader\nfrom transformers import (\n    AutoTokenizer, AutoModelForCausalLM, AdamW,\n    get_linear_schedule_with_warmup\nfrom datasets import load_dataset\nimport torch\nfrom torch.nn import functional as F\nimport wandb\n```", "```py\n# Dataset Creation: Ingestion and Preprocessing\ndataset = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train\")\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\ndef preprocess_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True,\n        max_length=512, padding=\"max_length\")\ntokenized_dataset = dataset.map(preprocess_function,\n    batched=True, remove_columns=dataset.column_names)\n```", "```py\n# Dataset Creation: Loading\ntrain_dataloader = DataLoader(\n    tokenized_dataset, shuffle=True, batch_size=8)\n```", "```py\n# Model Architecture\nmodel = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n# Optimization\noptimizer = AdamW(model.parameters(), lr=5e-5)\n```", "```py\n    num_epochs = 3\n    num_training_steps = num_epochs * len(train_dataloader)\n    lr_scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=100,\n        num_training_steps=num_training_steps)\n    ```", "```py\n    wandb.init(project=\"llm_training\", name=\"gpt2_finetune\")\n    device = torch.device(\"cuda\" if torch.cuda.is_available() \n        else \"cpu\")\n    model.to(device)\n    for epoch in range(num_epochs):\n        model.train()\n        for batch in train_dataloader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(batch)\n            loss = outputs.loss\n            loss.backward()\n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n            wandb.log({\"loss\": loss.item()})\n    ```", "```py\n        model.eval()\n        eval_loss = 0\n        with torch.no_grad():\n            for batch in train_dataloader:  # Using training data for simplicity\n                batch = {k: v.to(device) for k, v in batch.items()}\n                outputs = model(batch)\n                eval_loss += outputs.loss.item()\n        eval_loss /= len(train_dataloader)\n        wandb.log({\"eval_loss\": eval_loss})\n    ```", "```py\n        torch.save(model.state_dict(), \n            f\"model_checkpoint_epoch_{epoch}.pt\")\n    wandb.finish()\n    ```", "```py\n    from datasets import load_dataset, concatenate_datasets\n    from transformers import AutoTokenizer\n    from torch.utils.data import DataLoader\n    import numpy as np\n    ```", "```py\n    wiki_dataset = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train\")\n    books_dataset = load_dataset(\"bookcorpus\", split=\"train\")\n    # Combine datasets\n    combined_dataset = concatenate_\n        datasets([wiki_dataset, books_dataset])\n    ```", "```py\n    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n    def preprocess_function(examples):\n        # Tokenize the texts\n        tokenized = tokenizer(\n            examples[\"text\"], truncation=True, max_length=1024)\n        # Create input_ids and attention_mask\n        input_ids = tokenized[\"input_ids\"]\n        attention_mask = tokenized[\"attention_mask\"]\n        # Create labels for causal language modeling\n        labels = [\n            ids[1:] + [tokenizer.eos_token_id] for ids in input_ids]\n        return {\"input_ids\": input_ids, \n            \"attention_mask\": attention_mask, \"labels\": labels}\n    # Apply preprocessing\n    tokenized_dataset = combined_dataset.map(\n        preprocess_function,\n        batched=True,\n        remove_columns=combined_dataset.column_names,\n        num_proc=4  # Adjust based on your CPU cores\n    )\n    ```", "```py\n    train_dataloader = DataLoader(\n        tokenized_dataset,\n        shuffle=True,\n        batch_size=16,\n        collate_fn=lambda x: {k: np.stack([xi[k] for xi in x]) \n            for k in x[0]}\n    )\n    ```", "```py\nfrom transformers import GPT2Config, GPT2LMHeadModel\n# Define custom model configuration\nconfig = GPT2Config(\n    vocab_size=50257,  # GPT-2 vocabulary size\n    n_positions=1024,  # Maximum sequence length\n    n_ctx=1024,        # Context size\n    n_embd=768,        # Embedding dimension\n    n_layer=12,        # Number of transformer layers\n    n_head=12          # Number of attention heads\n)\n# Initialize the model with custom configuration\nmodel = GPT2LMHeadModel(config)\nprint(f\"Model parameters: {model.num_parameters():,}\")\n```", "```py\n    import torch\n    from torch.optim import AdamW\n    from transformers import get_linear_schedule_with_warmup\n    ```", "```py\n    optimizer = AdamW(model.parameters(), lr=5e-5, \n        weight_decay=0.01)\n    ```", "```py\n    num_epochs = 3\n    total_steps = len(train_dataloader) * num_epochs\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=100,\n        num_training_steps=total_steps\n    )\n    ```", "```py\n    device = torch.device(\"cuda\" if torch.cuda.is_available() \n        else \"cpu\")\n    model.to(device)\n    for epoch in range(num_epochs):\n        model.train()\n        for batch in train_dataloader:\n            batch = {k: torch.tensor(v).to(device) \n                for k, v in batch.items()\n            }\n            outputs = model(batch)\n            loss = outputs.loss\n            loss.backward()\n    ```", "```py\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n    ```", "```py\n    from torch.utils.tensorboard import SummaryWriter\n    import time\n    # Initialize TensorBoard writer\n    writer = SummaryWriter()\n    ```", "```py\n    model.train()\n    total_loss = 0\n    log_interval = 100\n    start_time = time.time()\n    ```", "```py\n    for i, batch in enumerate(train_dataloader):\n        batch = {k: torch.tensor(v).to(device) \n            for k, v in batch.items()}\n        outputs = model(batch)\n        loss = outputs.loss\n        total_loss += loss.item()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), \n            max_norm=1.0)\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n    ```", "```py\n        if (i + 1) % log_interval == 0:\n            cur_loss = total_loss / log_interval\n            elapsed = time.time() - start_time\n            writer.add_scalar(\n                'training_loss', cur_loss, global_step=i\n            )\n            writer.add_scalar(\n                'learning_rate', scheduler.get_last_lr()[0], \n                global_step=i\n            )\n            print(\n                f'| epoch {epoch:3d} '\n                f'| {i:5d}/{len(train_dataloader):5d} batches | '\n                f'lr {scheduler.get_last_lr()[0]:02.2f} | '\n                f'ms/batch {elapsed * 1000 / log_interval:5.2f} | '\n                f'loss {cur_loss:5.2f}'\n            )\n            total_loss = 0\n            start_time = time.time()\n    writer.close()\n    ```", "```py\n    class LLMTrainer:\n        def __init__(self, model, train_dataloader, optimizer,\n        scheduler, device\n        ):\n            self.model = model\n            self.train_dataloader = train_dataloader\n            self.optimizer = optimizer\n            self.scheduler = scheduler\n            self.device = device\n            self.writer = SummaryWriter()\n    ```", "```py\n        def train_epoch(self):\n            self.model.train()\n            total_loss = 0\n            log_interval = 100\n            start_time = time.time()\n            for i, batch in enumerate(self.train_dataloader):\n                batch = {k: torch.tensor(v).to(self.device)\n                    for k, v in batch.items()\n                }\n                outputs = self.model(batch)\n                loss = outputs.loss\n                total_loss += loss.item()\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(\n                    self.model.parameters(), max_norm=1.0)\n                self.optimizer.step()\n                self.scheduler.step()\n                self.optimizer.zero_grad()\n    ```", "```py\n                if (i + 1) % log_interval == 0:\n                    cur_loss = total_loss / log_interval\n                    elapsed = time.time() - start_time\n                    self.writer.add_scalar(\n                        'training_loss', cur_loss, global_step=i\n                    )\n                    self.writer.add_scalar(\n                        'learning_rate', \n                        self.scheduler.get_last_lr()[0], \n                        global_step=i\n                    )\n                        print(\n                            f'| {i:5d}/'\n                            f'{len(self.train_dataloader):5d} '\n                            f'batches | '\n                            f'lr '\n                            f'{self.scheduler.get_last_lr()'\n                            f'[0]:02.2f} | '\n                            f'ms/batch '\n                            f'{elapsed * 1000 / log_interval:5.2f} '\n                            f'| '\n                            f'loss '\n                            f'{cur_loss:5.2f}'\n                        )\n                    total_loss = 0\n                    start_time = time.time()\n    ```", "```py\n        def train(self, num_epochs):\n            for epoch in range(num_epochs):\n                print(f'Starting epoch {epoch+1}')\n                self.train_epoch()\n                # Here you could add validation, checkpointing, etc.\n            self.writer.close()\n    ```", "```py\n    trainer = LLMTrainer(model, train_dataloader,\n        optimizer, scheduler, device)\n    trainer.train(num_epochs=3)\n    ```", "```py\nStarting epoch 1\n|   100/1000 batches | lr 0.01 | ms/batch 45.67 | loss 2.35\n|   200/1000 batches | lr 0.01 | ms/batch 44.89 | loss 2.10\n|   300/1000 batches | lr 0.01 | ms/batch 46.12 | loss 1.95\n|   400/1000 batches | lr 0.01 | ms/batch 45.50 | loss 1.80\n|   500/1000 batches | lr 0.01 | ms/batch 44.75 | loss 1.65\n|   600/1000 batches | lr 0.009 | ms/batch 45.30 | loss 1.50\n|   700/1000 batches | lr 0.009 | ms/batch 44.95 | loss 1.40\n|   800/1000 batches | lr 0.009 | ms/batch 45.10 | loss 1.30\n|   900/1000 batches | lr 0.009 | ms/batch 45.00 | loss 1.25\n|  1000/1000 batches | lr 0.009 | ms/batch 44.80 | loss 1.20\nStarting epoch 2\n|   100/1000 batches | lr 0.009 | ms/batch 44.60 | loss 1.18\n|   200/1000 batches | lr 0.009 | ms/batch 44.70 | loss 1.15\n|   300/1000 batches | lr 0.009 | ms/batch 44.80 | loss 1.12\n|   400/1000 batches | lr 0.008 | ms/batch 44.50 | loss 1.10\n|   500/1000 batches | lr 0.008 | ms/batch 44.60 | loss 1.08\n|   600/1000 batches | lr 0.008 | ms/batch 44.55 | loss 1.05\n|   700/1000 batches | lr 0.008 | ms/batch 44.65 | loss 1.03\n|   800/1000 batches | lr 0.007 | ms/batch 44.50 | loss 1.00\n|   900/1000 batches | lr 0.007 | ms/batch 44.60 | loss 0.98\n|  1000/1000 batches | lr 0.007 | ms/batch 44.55 | loss 0.95\nStarting epoch 3\n|   100/1000 batches | lr 0.007 | ms/batch 44.50 | loss 0.93\n|   200/1000 batches | lr 0.007 | ms/batch 44.60 | loss 0.90\n|   300/1000 batches | lr 0.006 | ms/batch 44.55 | loss 0.88\n|   400/1000 batches | lr 0.006 | ms/batch 44.50 | loss 0.85\n|   500/1000 batches | lr 0.006 | ms/batch 44.60 | loss 0.83\n|   600/1000 batches | lr 0.006 | ms/batch 44.55 | loss 0.80\n|   700/1000 batches | lr 0.005 | ms/batch 44.50 | loss 0.78\n|   800/1000 batches | lr 0.005 | ms/batch 44.60 | loss 0.75\n|   900/1000 batches | lr 0.005 | ms/batch 44.55 | loss 0.73\n|  1000/1000 batches | lr 0.005 | ms/batch 44.50 | loss 0.70\nTraining completed. Writer closed.\n```", "```py\n    import torch.cuda.amp as amp\n    class LargeScaleLLMTrainer(LLMTrainer):\n        def __init__(self, model, train_dataloader,\n            optimizer, scheduler, device, accumulation_steps=4\n        ):\n            super().__init__(model, train_dataloader,\n                optimizer, scheduler, device)\n            self.accumulation_steps = accumulation_steps\n            self.scaler = amp.GradScaler()\n    ```", "```py\n        def train_epoch(self):\n            self.model.train()\n            total_loss = 0\n            log_interval = 100\n            start_time = time.time()\n            for i, batch in enumerate(self.train_dataloader):\n                batch = {\n                    k: torch.tensor(v).to(self.device)\n                    for k, v in batch.items()\n                }\n                with amp.autocast():\n                    outputs = self.model(batch)\n                    loss = outputs.loss / self.accumulation_steps\n                self.scaler.scale(loss).backward()\n    ```", "```py\n                if (i + 1) % self.accumulation_steps == 0:\n                    self.scaler.unscale_(self.optimizer)\n                    torch.nn.utils.clip_grad_norm_(\n                        self.model.parameters(), max_norm=1.0\n                    )\n                    self.scaler.step(self.optimizer)\n                    self.scaler.update()\n                    self.scheduler.step()\n                    self.optimizer.zero_grad()\n                total_loss += loss.item() * self.accumulation_steps\n    ```", "```py\n                if (i + 1) % log_interval == 0:\n                    cur_loss = total_loss / log_interval\n                    elapsed = time.time() start_time\n                    self.writer.add_scalar('training_loss',\n                        cur_loss, global_step=i)\n                    self.writer.add_scalar('learning_rate',\n                        self.scheduler.get_last_lr()[0],\n                        global_step=i)\n                    print(\n                        f'| {i:5d}/{len(self.train_dataloader):5d}\n                            batches | '\n                        f'lr {self.scheduler.get_last_lr()[0]:02.2f}\n                            | '\n                        f'ms/batch {elapsed * 1000 /\n                            log_interval:5.2f} | '\n                        f'loss {cur_loss:5.2f}'\n                     )\n                    total_loss = 0\n                    start_time = time.time()\n    ```", "```py\n    large_trainer = LargeScaleLLMTrainer(\n        model, train_dataloader, optimizer, scheduler, device)\n    large_trainer.train(num_epochs=3)\n    ```"]