- en: '12'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Correcting and Optimizing Your Generative AI Application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Until this point, you’ve read about how to build a **generative AI** (**GenAI**)
    application, its various components, and how they fit together. You've gained
    a solid understanding of what makes them work (and not work) well. You’re also
    aware of some of the challenges of GenAI applications and how to identify them.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you’ll begin unraveling the mystery of how to *improve* your
    GenAI application once you’ve identified its shortcomings. You will also learn
    about optimizing and fine-tuning your GenAI application, so it’s a reliable, effective,
    and stable machine working in your favor, instead of a rogue actor bringing chaos.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will discuss several well-known techniques to improve your GenAI
    application, so you can be confident in your finished product. Ideally, you will
    perform all of these techniques. The chapter will define each of these and explain
    how they can improve your application. Then, you will complete a robust example
    of each of these as an activity. By the end of this chapter, you will have many
    ideas on how to improve your application.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Baselining
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training and evaluation datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Few-shot prompting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieval and reranking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Late interaction strategies, including in-application feedback and user feedback
    loops
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Query rewriting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing and red teaming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Information post-processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter does not contain any coding. However, it builds upon all the previous
    chapters to describe various methodologies for improving and optimizing your GenAI
    application output. To recreate some of the examples, you’ll simply need to use
    your favorite **large language model** (**LLM**) provider and recreate the attempts
    yourself. This chapter uses ChatGPT.
  prefs: []
  type: TYPE_NORMAL
- en: Baselining
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Baselining**, in the context of GenAI, refers to the process of defining
    a standard or a reference output for the AI model to compare future outputs. This
    standard serves as a crucial benchmark for evaluating the model’s performance,
    consistency, and improvements over time. By establishing a baseline, developers
    and stakeholders can objectively measure how the AI performs relative to a predefined
    set of expectations, ensuring that the model meets and maintains desired standards.'
  prefs: []
  type: TYPE_NORMAL
- en: In GenAI, baselining is essential for several reasons. Firstly, it provides
    a clear metric for assessing the quality and performance of the AI model. Secondly,
    it helps in tracking the model’s progress and improvements over time. Finally,
    baselining is a tool to help ensure consistency in the model’s outputs, via detection
    of output variability. All of these are vital for maintaining reliability and
    trust in the AI system.
  prefs: []
  type: TYPE_NORMAL
- en: 'The aspects of the AI model that can be baselined are numerous and highly dependent
    on the specific application and its goals. Some common elements that might be
    baselined include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Accuracy**: This involves measuring the correctness of the model’s outputs.
    For instance, in a language model, accuracy can be gauged by how well the generated
    text matches the expected text or how often it provides the correct information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Speed of response**: This refers to the time it takes for the model to generate
    an output after receiving an input. Faster response times are generally preferred,
    especially in real-time applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Effectiveness**: This can be a measure of how well the AI meets its intended
    purpose. For example, in a recommendation system, effectiveness might be assessed
    by the relevance and personalization of the recommendations provided.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**User satisfaction**: This subjective metric can be gauged through user feedback
    and surveys, reflecting how satisfied users are with the AI’s performance and
    outputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Establishing a baseline standard alongside your current performance also helps
    you—the engineer—determine whether you are improving results over time. This knowledge
    is crucial for ensuring that your application is not degrading in performance.
    In some industries, baseline performance indicators may be required to meet industry
    or regulatory standards and may be a reporting requirement for your application
    or organization.
  prefs: []
  type: TYPE_NORMAL
- en: Once you evaluate the initial performance of your application, you’ll want to
    document these results. Subsequently, ensure that you consistently compare the
    model’s outputs to the baseline during each training and update cycle. Comprehensive
    documentation provides a reference that can be used to compare future outputs
    and identify trends or issues in the model’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: Regular evaluation of the model’s outputs against the baseline is also critical.
    During subsequent iterations of training and updates, these evaluations can help
    in detecting deviations from the expected (baseline) performance. If the model’s
    performance drops below the baseline, it can indicate a problem that needs to
    be addressed, such as data drift, changes in user behavior, or issues with the
    training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Training and evaluation datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To create your baseline, you will need to create an **evaluation dataset**.
    An evaluation dataset is a series of questions asked of your application to determine
    whether it meets the standards you have identified. Note that the evaluation dataset
    is not to be confused with the **training dataset**, which is the data that you
    used to *train* your model. The evaluation dataset should be a wholly different
    set of questions and answers. Effectively, the training dataset is akin to the
    notes and sources that you’d give to a student to learn, while the evaluation
    dataset is like the final exam. You don’t want to make that exam too easy!
  prefs: []
  type: TYPE_NORMAL
- en: Training datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As its name suggests, a training dataset is a collection of data used to teach
    or *train* a machine learning model. It contains input-output pairs where the
    input data is fed to the model, and the model learns to produce the correct output.
    This process involves adjusting the model’s parameters so that it can generalize
    well to new, unseen data. The quality and diversity of the training dataset directly
    impact the performance and accuracy of the trained model.
  prefs: []
  type: TYPE_NORMAL
- en: High-quality training data ensures that the model can recognize patterns and
    make accurate predictions or generate appropriate responses. Therefore, your training
    dataset should be representative of the problem domain, covering a wide range
    of scenarios that the model would be expected to encounter in real-world applications.
    This helps in reducing biases and improving the model’s generalizability.
  prefs: []
  type: TYPE_NORMAL
- en: 'The types of data in the training dataset might include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Labeled data**: This is the primary type of data used in supervised learning.
    Each data point consists of an input and a corresponding correct output, or label.
    For instance, in a text classification task, labeled data might include sentences
    paired with their respective categories.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unlabeled data**: Used in unsupervised learning, this data does not come
    with predefined labels. The model tries to find patterns and structures in the
    data. For example, clustering algorithms use unlabeled data to group similar data
    points together.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mixed data**: Semi-supervised learning uses a combination of labeled and
    unlabeled data. This approach leverages the large amounts of unlabeled data available
    while benefiting from the smaller labeled dataset to guide the learning process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Diverse data**: Including diverse data ensures that the model can handle
    various inputs. This might include different languages, dialects, formats, and
    contexts. For certain types of applications, this might include training data
    that is both human-readable documentation as well as code bases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Despite all that, you might wish to also include **supplemental training data**.
    Supplemental training data refers to additional data used to fine-tune or enhance
    the performance of an already trained model. There are many reasons to do this,
    but let’s talk about three that are particularly compelling:'
  prefs: []
  type: TYPE_NORMAL
- en: Supplemental data can help adapt a general model to a specific domain. For example,
    a language model trained on general text might be fine-tuned with medical literature
    to perform better in healthcare applications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supplemental training data can be used to enhance the model’s ability in particular
    areas where it might be weak. For example, adding more data related to financial
    transactions can help a fraud detection model become more accurate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As new information becomes available, supplemental training data can be used
    to update the model’s knowledge. This is especially relevant for applications
    requiring up-to-date information, such as news generation or where the industry
    is rapidly evolving (such as technology).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluation datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In addition to your training data and supplemental data, you’ll also need an
    evaluation dataset. Evaluation datasets are crucial because they provide a controlled
    and consistent way to measure the performance of your AI model. They serve as
    a benchmark for comparison, ensuring that the model’s outputs can be objectively
    assessed against predefined criteria. By using a standard dataset, you can reliably
    track improvements, identify weaknesses, and maintain the quality of the model
    over time. It helps in validating that the model is not only performing well during
    the development phase but also generalizing effectively to new, unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The content of an evaluation dataset depends on the specific application and
    its goals. Generally, it should include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Representative queries**: A variety of questions or inputs that the AI is
    likely to encounter in real-world usage. These should cover different scenarios
    and edge cases to ensure a comprehensive evaluation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Expected outputs**: Corresponding correct or ideal responses for each query,
    against which the AI’s responses will be compared.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Diverse data**: Data that reflects the diversity of inputs the model will
    face, including variations in language, format, and context. This helps in assessing
    the model’s robustness and ability to handle different types of input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, the evaluation dataset for the MongoDB documentation chatbot includes
    questions and answers to the top 250 search terms, top 250 support questions by
    volume, and some of the most common questions asked about MongoDB. This can take
    the form of simple keywords or actual phrases in full-sentence format, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: These terms and questions were retrieved from a combination of sources, which
    will vary depending on your infrastructure. For MongoDB, this infrastructure comes
    from the Google search console for [mongodb.com](http://mongodb.com) as well as
    the support chat, community forums, and Stack Overflow.
  prefs: []
  type: TYPE_NORMAL
- en: Determining the right amount of evaluation involves balancing thoroughness with
    practicality. You should have enough data to cover a wide range of scenarios and
    ensure the outputs of your GenAI application are consistently accurate and reliable.
    Typically, this involves hundreds or even thousands of data points, depending
    on the complexity of the application.
  prefs: []
  type: TYPE_NORMAL
- en: That said, while more data can provide a more comprehensive assessment, there
    is a point of diminishing returns where additional data does not significantly
    improve the evaluation but adds to the complexity and resource requirements. **Over-evaluation**
    can also lead to overfitting of the evaluation dataset rather than improving overall
    performance. Returning to the earlier student/exam analogy, you don’t want your
    evaluation exam to be an exact replica of the training materials because all you
    would be testing then is whether the student was able to memorize a question and
    response. You would not be testing how well the student has learned the material.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, ensuring high-quality, accurate, and comprehensive training and
    evaluation datasets can reduce the likelihood of the model learning incorrect
    patterns. This requires significant effort upfront before deployment of the GenAI
    application but can dramatically improve your GenAI accuracy and depth of response
    as well as ensure its quality of responses to your users.
  prefs: []
  type: TYPE_NORMAL
- en: Few-shot prompting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In many cases, you will be asking your GenAI application to produce new content
    or summarize existing content, in which case what you need to do is provide the
    existing application with a properly engineered prompt. Most of the time, having
    the user simply request what they need is sufficient. But in cases where the outputs
    are complex, you will find that the quality and accuracy of the GenAI application’s
    response are improved by using a technique called **few-shot prompting**. Few-shot
    prompting is when you provide an example as part of the input to the LLM so that
    it can see exactly what type of syntax and response you need. You can also include
    a definition as part of the example in case you believe the input might be a term
    with which the LLM would not be familiar, or in case you’re using a business-specific
    term.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try out an example using GPT-4.
  prefs: []
  type: TYPE_NORMAL
- en: '**Example 1**: Let’s see how the LLM responds to a request that does not use
    few-shot prompting.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the user input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Example 2**: Now, let’s try this example with few-shot prompting.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the user input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: You can see how, by providing the example text, you can enhance the output to
    make it match whatever syntax you prefer. The additional prompt information need
    not be terribly difficult to produce either. If you can provide an example output
    to your GenAI application, its results will be much nearer to what you desire.
  prefs: []
  type: TYPE_NORMAL
- en: Retrieval and reranking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Retrieval and reranking are key techniques used to enhance the performance and
    accuracy of LLMs. First, understand that by retrieving relevant context or documents,
    an LLM provides more accurate and contextually relevant responses. This is particularly
    useful when the model’s training data does not cover the specifics of the query
    or when up-to-date information is required.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the context of LLMs, **retrieval** can involve searching through a vast
    collection of documents, knowledge bases, or other data sources to find pieces
    of information that are pertinent to a given query or task. Let’s have a look
    at the two different types of retrieval:'
  prefs: []
  type: TYPE_NORMAL
- en: '`cars` in your query, it returns documents that contain the word *cars*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Embedding-based retrieval**: This uses vector embeddings to find matching
    documents. Both the query and documents are transformed into vectors in a high-dimensional
    space. Retrieval then involves finding vectors (that is, documents) that are close
    to the query vector.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reranking** is the process of reordering the retrieved documents or pieces
    of information to prioritize the most relevant ones. After the initial retrieval,
    the documents are ranked based on their relevance to the query. Retrieved documents
    are initially ranked based on their similarity to the query using methods such
    as cosine similarity in embedding space. However, a more sophisticated model can
    rerank these initially retrieved documents by considering additional features
    and context.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at the following examples.
  prefs: []
  type: TYPE_NORMAL
- en: '**Example 1**: Recommending restaurants with a GenAI application.'
  prefs: []
  type: TYPE_NORMAL
- en: You have built a GenAI application that provides restaurant recommendations.
    A user requests restaurants currently open near them. When examining the potential
    restaurants to provide to the user, the application looks at the distance from
    the user’s current location or provided address and the current local time and
    opening hours.
  prefs: []
  type: TYPE_NORMAL
- en: It will then rank the results so that the closest restaurant is the first one
    shown to the user. This is a perfectly fine solution. But you may want to have
    smarter results that are dynamically reranked based on other criteria, such as
    user ratings for the restaurants. You may want to show a higher-rated restaurant
    that is three miles away first, rather than a one-star restaurant that is one
    mile away. As the user gives feedback on the results, you may want to rerank dynamically,
    expanding your pool of restaurants as you get more information about what the
    user would prefer (including, say, the type of cuisine or ambiance).
  prefs: []
  type: TYPE_NORMAL
- en: By reranking the results, the most relevant and useful information is prioritized,
    improving the overall quality of the LLM’s output. It helps in filtering out less
    relevant or redundant information, ensuring the response is precise and useful.
  prefs: []
  type: TYPE_NORMAL
- en: 'When combined, retrieval and reranking significantly enhance LLM outputs with
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The model can access and utilize relevant information that might not be present
    in its training data, providing more accurate and contextually appropriate answers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By focusing on the most relevant information through reranking, the model’s
    responses become more precise, reducing errors and irrelevant content.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieval can pull in the latest information from updated sources, making the
    model’s responses more current.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These techniques allow the model to handle specific, detailed queries efficiently
    without needing to retrain the entire model frequently.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Example 2**: Summarizing the latest research on quantum computing.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s another practical example. Suppose you ask an LLM about the latest research
    on quantum computing. The steps of the output would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Retrieval**: The model searches through a large database of scientific papers
    and articles to find relevant documents on quantum computing.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Reranking**: The initially retrieved documents are then reranked, with the
    most recent and pertinent studies placed at the top.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Response generation**: The LLM uses the top-ranked documents to generate
    a detailed and accurate response about the latest research trends in quantum computing.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By incorporating retrieval and reranking, the LLM can provide a well-informed,
    up-to-date, and contextually accurate answer, vastly improving the user experience.
  prefs: []
  type: TYPE_NORMAL
- en: Late interaction strategies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that you’re ready to take your application into production, there are still
    a few more things you can do to help improve the user experience and create a
    feedback loop in order to get a better signal as to the behavior of your GenAI
    application. This next set of recommendations focuses on **late interaction strategies**,
    sometimes referred to as **contextualized late interaction over** **BERT** (**ColBERT**).
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s define **interaction**. Interaction refers to the process of evaluating
    the relevance between a query and a document by comparing their representations.
    A late processing strategy is one where the interaction between the query and
    document representations occurs later in the process, typically after both have
    been independently encoded. Early interaction models are where query and document
    embeddings interact at earlier stages, typically before or during their encoding
    by the model.
  prefs: []
  type: TYPE_NORMAL
- en: Second, let’s dig a little bit into the internal workings. When a user interacts
    with a GenAI application, they input a query that is encoded into a dense vector
    representation. Potential responses, usually documents or passages, are also encoded
    into dense vector representations. The system performs similarity matching between
    the query and document embeddings, returning the documents with the highest similarity
    scores as the best matches.
  prefs: []
  type: TYPE_NORMAL
- en: To enhance relevance, you don’t return all matching results to the user. Instead,
    you aim to provide the most relevant results or a summarized version of the result
    set. Late interaction models such as ColBERT improve efficiency by focusing on
    the most promising query-document pairs rather than considering all possible pairs,
    yielding more precise results and a better user experience. This selective approach
    allows for more precise and relevant results, enhancing the user experience.
  prefs: []
  type: TYPE_NORMAL
- en: If you need to focus on improving search results, consider implementing ColBERT
    or similar techniques to enhance retrieval performance and provide more relevant
    results for user queries.
  prefs: []
  type: TYPE_NORMAL
- en: Query rewriting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Query rewriting**, or **query reformulation**, is a technique used to improve
    the quality of the answers provided by LLMs. This process involves modifying the
    original query to make it clearer, more specific, or more detailed, which can
    help the model generate better responses. LLMs do not explicitly rewrite queries
    in the background, so this effort is manual unless you have implemented a workflow
    that will evaluate and rewrite the user’s query before it’s processed.'
  prefs: []
  type: TYPE_NORMAL
- en: Rewriting a query can make it clearer and more precise, reducing ambiguity and
    ensuring the model understands exactly what is being asked. Adding relevant context
    or details to the query can help the model provide more accurate and contextually
    appropriate answers and can help disambiguate terms that have multiple meanings,
    ensuring the response aligns with the intended meaning. In addition, reformulating
    the query to include additional relevant details can lead to more comprehensive
    answers.
  prefs: []
  type: TYPE_NORMAL
- en: How does query rewriting work? It’s important to understand user intent for
    your GenAI application. What is the *purpose* of your application, and what kinds
    of questions will your application attempt to answer? Understanding what sort
    of response users expect versus what your application might deliver is key. After
    that, you can do the following activities, which are not mutually exclusive, meaning
    that you can perform some, just one, or none of these.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, based on the **intent**, the user query can be augmented with
    additional **context** and details. This activity substantially expands the user
    query (and increases the token count per query) but will typically yield much
    better results.
  prefs: []
  type: TYPE_NORMAL
- en: To take an easy example, imagine that your application generates images. The
    user requests `a picture of a kitten`, a quite simple query that could have endless
    results.
  prefs: []
  type: TYPE_NORMAL
- en: 'To help the user get better results, you can add three buttons in the UI so
    that the user can select a `a picture of a kitten`, the query is modified to the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Here, for each button style, you can add the terms that augment the user query
    and then apply them before submission.
  prefs: []
  type: TYPE_NORMAL
- en: 'As another example, consider this user query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'A meaningful rewrite could be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This rewritten query with additional context helps the system understand that
    the user is asking for a specific product and time period, leading to a more accurate
    and useful response.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, when conducting query rewrites, you’ll want to **simplify the language**.
    Complex queries can be simplified or broken down into simpler parts, making it
    easier for the model to process and respond accurately. This method involves taking
    a large query and breaking it into constituent parts (which typically is achieved
    via a series of input fields/forms) and then unifying each data entry into a single
    submitted query. This guides your user into constructing a well-formed query without
    specialized knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, imagine your user has only a single-entry field to input their
    query. In such a case, they may leave out relevant information or provide irrelevant
    information that could impact accuracy or increase the possibility of hallucination.
    Instead, if you were to provide the user with a series of fields, each with clear
    instructions, and then assemble the inputted information into a query that was
    fed into the GenAI application, you would get a better outcome than a free-form
    text entry.
  prefs: []
  type: TYPE_NORMAL
- en: For practical implementation, you could consider a workflow in which the system
    itself analyzes the query for intent and context, reviews the query’s complexity,
    and then rewrites the query to be clearer, more specific, or more detailed. The
    reformulated query can then be used to generate the response.
  prefs: []
  type: TYPE_NORMAL
- en: Testing and red teaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Testing AI systems is critical to ensure their accuracy, reliability, and overall
    performance. Typically, in software engineering, automated testing is used as
    part of the software development process. GenAI applications are no different.
    You’ll want to routinely and regularly test the outputs to ensure there are no
    radical shifts in output quality.
  prefs: []
  type: TYPE_NORMAL
- en: Testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Just like your typical software engineering features, you’ll want to include
    the phases of unit testing, integration testing, performance testing, and user
    acceptance into your test plan. However, the specifics of how this is done vary
    from one use case to another.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the context of GenAI applications, **unit testing** still has the same basic
    tenets and involves testing individual components or modules of the application
    to ensure they function correctly. However, in the case of GenAI applications,
    your unit tests will need to also include steps such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input validation**: Ensure that the application correctly handles and validates
    various input types, formats, and ranges. Test for edge cases, such as empty inputs,
    excessively large inputs, or malformed data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pre-processing**: Verify that any pre-processing steps, such as tokenization,
    normalization, or feature extraction, are performed correctly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model loading**: Test that the model is correctly loaded from its storage
    location, and verify that the correct version is being used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model inference**: Ensure that the model generates outputs without errors
    given valid inputs. Test the inference function with controlled inputs to verify
    expected behavior, such as deterministic responses for certain prompts or scenarios.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output format**: Validate that the generated outputs meet the expected format
    and structure. This includes checking that outputs are complete, correctly formatted,
    and adhere to any length or content constraints.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Post-processing**: Test any post-processing steps that modify or enhance
    the model’s output, such as cleaning up text, converting formats, or applying
    additional business logic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Proper functioning**: The outputs should work. If your GenAI application
    outputs code, you will need to test that the code itself compiles and behaves
    as intended.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are just a few of the items that you should include for unit testing your
    GenAI application.
  prefs: []
  type: TYPE_NORMAL
- en: '**Integration testing** focuses on verifying that the components of your GenAI
    system work together as needed. This means you’ll be testing the interactions
    between components to check the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Whether your data ingestion pipeline pulls the correct data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How recommendations are presented to the user (formatting, for instance, if
    this is done by another library or tool)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: API load testing, if you’re using another LLM such as OpenAI or Anthropic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You’ll want to evaluate processing time, efficiency, and scalability via **performance
    testing**. This might include activities such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Load testing your application for how it handles a large volume of simultaneous
    queries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assessing the inference time of self-hosted models on various hardware configurations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measuring how many token limits should be set for input and output to control
    costs and processing time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measuring the time taken for the model to generate outputs and ensuring it meets
    performance requirements. This can be especially important for applications with
    real-time constraints.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In addition to this routine testing, you have more to add to your test suite.
    In general, it is also recommended that GenAI applications go through **additional
    testing** for the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bias and fairness**: If your model is making recommendations that affect
    lives and livelihoods, you’ll want to carefully consider training data biases
    for different demographic groups.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Robustness**: To ensure your GenAI application is resilient to variations
    and noise, you’ll want to test with adversarial examples and edge cases to evaluate
    its ability to handle unexpected inputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once you’ve gotten through all of that, you’ll want to think about **user acceptance
    testing**, which is one of the most exciting parts of the process, as you will
    see in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Red teaming
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If your GenAI application will accept natural language prompts and inputs from
    human beings, then the practice of **red teaming** cannot be recommended enough.
    Red teaming involves simulating real-world, challenging, or adversarial situations
    to identify vulnerabilities and weaknesses in your GenAI application. This approach
    is borrowed from cybersecurity practices and is particularly important for ensuring
    your GenAI application meets user expectations.
  prefs: []
  type: TYPE_NORMAL
- en: This involves having a large pool of *users* who will ask real-world questions,
    but they are not limited by *scripts* as to what they may ask. The reason for
    red teaming is that GenAI applications can, and often do, produce different outputs
    that vary widely, even with similar or the same input. Not only that but the quality
    of the generated output is often subjective and depends on human judgment. So,
    while traditional software applications produce predictable and consistent results,
    the same is not true of GenAI. Let’s take an example to see how this works.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a chatbot application, you might have routine automated testing that would
    ask your GenAI application the top 200 most common user questions and then evaluate
    them for correctness. With a red team, you would have 50 users ask whatever questions
    they wanted, and then record both the questions asked and the responses. This
    might yield insights such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: If a user asks a question in a similar way but not with the exact same wording,
    they receive incorrect or less correct answers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some users will ask malicious questions and the GenAI application will respond
    poorly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other users ask questions that are not part of the training data, and the GenAI
    application hallucinates answers (or gives no answer at all), thus identifying
    the need to expand your training data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When users ask many questions in a row, the application stalls.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When a user asks specific question types, they are dissatisfied with the output
    because the application lacks high-quality training data or the formatting of
    the reply is undesirable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When properly prompted, the GenAI application will share details of other users’
    sessions, thus identifying a security issue.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To enable the red-teaming phase, it is recommended that you record every question
    asked by every user, as well as every response given, and then ask testers to
    rate the response with notes. While this level of detailed user testing is strenuous
    and uncommon in software development, it is incredibly valuable to see how your
    application performs in real-world scenarios, with real human beings, before production.
  prefs: []
  type: TYPE_NORMAL
- en: Due to the scale and scope of some AI systems, fully testing each component
    is impossible. Effective testing and red teaming rely on using judgment in terms
    of which parts of the system are most risky. It may be true that giving occasionally
    not-quite-accurate advice is a non-impactful event. However, the potential harm
    of a single hallucination could be quite high. You will want to consider the severity
    of harm, the likelihood of inaccuracy, and the ability to retract or rectify the
    inaccuracy as your standard measures of risk. Using those simple, albeit subjective,
    measures can assist you in determining to what extent you test each aspect of
    the system, and the size of your red team.
  prefs: []
  type: TYPE_NORMAL
- en: To give yourself a sense of what sorts of harms and incidents you will be testing
    for—which are too many to enumerate—you will find it helpful to review the AI
    Incident Database at [https://incidentdatabase.ai/](https://incidentdatabase.ai/).
    Upon review of this tool, you may find your specific use case (or ones like it)
    and what incidents have already been reported, so that you can test and think
    through the repercussions of inaccuracies.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, one incident that is detailed here involved an application that
    made staffing-level recommendations. However, the algorithm-based recommendations
    left facilities understaffed, leading to critical incidents of neglect, injury,
    and death. Those incidents then prompted lawsuits and even legislation against
    healthcare providers using AI.
  prefs: []
  type: TYPE_NORMAL
- en: Information post-processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You might know that the main way in which GenAI differs from previous forms
    of AI or analytics is that it generates new content efficiently. But did you know
    that that content is often in *unstructured* forms, for example, written text
    or images? When you see outputs that are nicely formatted, in bulleted lists,
    multiple fonts, and so on, it is a form of **information post-processing**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Information post-processing refers to the series of steps taken after an AI
    model generates an initial response, but before that response is sent to the user.
    This crucial step enhances the output of GenAI models, refining raw responses
    to make them more useful, accurate, and contextually appropriate. It can take
    many forms, so this chapter will only discuss some of the most useful ones along
    with information on how to implement them:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fact-checking**: Verifying the accuracy of the information provided. This
    can involve checking facts against reliable sources or databases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Formatting**: Structuring the information in a clear and readable format,
    such as bullet points, paragraphs, or tables. This may also include style changes
    such as bold, text color, or font to enhance readability and emphasis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Grammar, style, and tone checking**: At times, the resulting text provided
    by GenAI applications is not up to par or consistent with the exact messaging,
    tone, and style that one would expect a human being to write. Post-processing
    tools and vendors can take generated text outputs and markedly improve them for
    readability, making them match reader expectations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Information post-processing is a vital component in the lifecycle of GenAI outputs.
    It bridges the gap between raw model outputs and polished, user-ready responses,
    enhancing accuracy, readability, relevance, and overall user satisfaction. By
    implementing effective post-processing strategies, AI systems can deliver higher-quality
    and more reliable results.
  prefs: []
  type: TYPE_NORMAL
- en: There are entire services springing up around this valuable step in the GenAI
    process, so engineers do not have to build it themselves.
  prefs: []
  type: TYPE_NORMAL
- en: Other remedies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some other technical remedies can be employed even more easily than the ones
    detailed in this chapter. Some of these may improve the accuracy and performance
    of your GenAI application, though the level of effort involved varies. As an example,
    during MongoDB’s testing of GPT, it was discovered that the accuracy rate for
    the same set of questions was improved by 7% between GPT-3.5 and GPT-4\. Getting
    such a level of improvement in accuracy via prompting, retrieval augmentation,
    or late interaction strategies is certainly possible but would have been difficult.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, it is worth investigating every avenue of potential improvement, including
    areas such as hardware upgrades, code optimization, concurrency management, database
    query optimization, and even just upgrading your software. All of these can improve
    the results of your GenAI application and should be independently investigated:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hardware and software upgrades**: Upgrade computational resources, such as
    using more powerful GPUs, scaling horizontally with more servers, or updating
    to the latest version of the software, to outsize impacts on both accuracy and
    performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Code optimization**: Refactor and optimize code to improve efficiency, reduce
    computational load, and handle data more effectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Network optimization**: Reduce network latency by optimizing data transfer,
    caching responses, and minimizing API call overheads.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Concurrency management**: Implement concurrency and parallel processing techniques
    to handle multiple requests efficiently.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Database optimization**: Optimize database queries and interactions to reduce
    I/O overhead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Implementing mechanisms to correct and optimize your GenAI application can have
    many forms and can be implemented before, during, and after answers are generated.
    For optimal performance, you’ll want to train your GenAI model with high-quality
    data, supplement existing models with your specific use case data, and have thorough
    evaluation datasets and record the model’s performance to establish a baseline
    of accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have that baseline, however, you can immediately begin improving upon
    it with the techniques discussed in this chapter. Among these techniques is one-
    or few-shot prompting. It involves providing the GenAI model with a single example
    or prompt to guide its response, enabling the model to generate relevant and contextually
    appropriate outputs with minimal training data. You can also try retrieving and
    reranking relevant documents or data points based on the user’s query, and then
    reordering these results to prioritize the most relevant and useful information
    before generating a final response. Query rewriting is another technique that
    can improve clarity, specificity, or context, helping the AI model understand
    and respond more accurately to the user’s requests.
  prefs: []
  type: TYPE_NORMAL
- en: Formatting GenAI responses via structuring and presenting the AI-generated content
    in a clear, organized, and readable manner can enhance the overall user experience
    and ensure the information is easily digestible. Similarly, implementing late
    interaction strategies such as ColBERT can improve the relevance and accuracy
    of the retrieved information. By testing, red teaming, and recording your results,
    you can track your progress in improving the performance, security, and quality
    of responses over time.
  prefs: []
  type: TYPE_NORMAL
- en: GenAI technologies are changing (and will continue to change) the face of the
    software industry. With these optimization strategies in place, your GenAI application
    will be well equipped to adapt and excel in an ever-evolving landscape.
  prefs: []
  type: TYPE_NORMAL
- en: 'Appendix: Further Reading'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In addition to the links provided within the chapters, here are some resources
    to take your learning journey forward.
  prefs: []
  type: TYPE_NORMAL
- en: '[**Chapter 1**](B22495_01.xhtml#_idTextAnchor009)**, Getting Started with**
    **Generative AI**'
  prefs: []
  type: TYPE_NORMAL
- en: Gryka, Maciej. “Invest in RAG” in “Building reliable systems out of unreliable
    agents.” *The Rainforest Blog*, April 3, 2024\. [https://www.rainforestqa.com/blog/building-reliable-systems-out-of-unreliable-agents#Invest_in_RAG](https://www.rainforestqa.com/blog/building-reliable-systems-out-of-unreliable-agents#Invest_in_RAG).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '“The Black Box: Even AI’s creators don’t understand it.” July 2023\. *Unexplainable*.
    Produced by Vox Creative. Podcast, Spotify, 36:15\. [https://open.spotify.com/episode/3npjXNCtUSGRUjVR4EYb4Y?si=-XpudYVzSEKfhD0-2NBjEQ](https://open.spotify.com/episode/3npjXNCtUSGRUjVR4EYb4Y?si=-XpudYVzSEKfhD0-2NBjEQ).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Chapter 2**](B22495_02.xhtml#_idTextAnchor021)**, Building Blocks of** **Intelligent
    Applications**'
  prefs: []
  type: TYPE_NORMAL
- en: Naveed et al. “A Comprehensive Overview of Large Language Models.” arXiv, July
    12, 2023\. [https://arxiv.org/abs/2307.06435](https://arxiv.org/abs/2307.06435).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Chapter 3**](B22495_03.xhtml#_idTextAnchor041)**, Large** **Language Models**'
  prefs: []
  type: TYPE_NORMAL
- en: “Speech and Language Processing,” n.d., [https://web.stanford.edu/~jurafsky/slp3/](https://web.stanford.edu/~jurafsky/slp3/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hochreiter, Sepp, and Jürgen Schmidhuber. “Long Short-Term Memory.” *Neural
    Computation* 9, no. 8 (November 1, 1997): 1735–80\. [https://doi.org/10.1162/neco.1997.9.8.1735](https://doi.org/10.1162/neco.1997.9.8.1735).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan
    N. Gomez, Lukasz Kaiser, and Illia Polosukhin. “Attention Is All You Need.” *arXiv
    (Cornell University)*, January 1, 2017\. [https://doi.org/10.48550/arxiv.1706.03762](https://doi.org/10.48550/arxiv.1706.03762).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Prompt Engineering Guide – Nextra,” n.d., [https://www.promptingguide.ai/](https://www.promptingguide.ai/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Chapter 4**](B22495_04.xhtml#_idTextAnchor061)**,** **Embedding Models**'
  prefs: []
  type: TYPE_NORMAL
- en: 'A. Aruna Gladys and V. Vetriselvi, “Survey on multimodal approaches to emotion
    recognition,” *Neurocomputing* 556 (November 1, 2023): 126693, [https://doi.org/10.1016/j.neucom.2023.126693](https://doi.org/10.1016/j.neucom.2023.126693).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sumit Kumar, “Positive and Negative Sampling Strategies for Representation Learning
    in Semantic Search,” Sumit’s Diary, March 22, 2023, [https://blog.reachsumit.com/posts/2023/03/pairing-for-representation](https://blog.reachsumit.com/posts/2023/03/pairing-for-representation).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tomas Mikolov et al., “Efficient Estimation of Word Representations in Vector
    Space,” arXiv.org, January 16, 2013, [https://arxiv.org/abs/1301.3781](https://arxiv.org/abs/1301.3781).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI, “GPT-4”. GPT-4 Research, March 14, 2023\. [https://openai.com/index/gpt-4-research](https://openai.com/index/gpt-4-research).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jeffrey Pennington, “GloVe: Global Vectors for Word Representation,” n.d.,
    [https://nlp.stanford.edu/projects/glove](https://nlp.stanford.edu/projects/glove).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jacob Devlin et al., “BERT: Pre-training of Deep Bidirectional Transformers
    for Language Understanding,” arXiv.org, October 11, 2018, [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “fastText,” n.d., [https://fasttext.cc/](https://fasttext.cc/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and
    Zettlemoyer, L. “Deep contextualized word representations,” arXiv:1802.05365,
    March 22, 2018\. [https://arxiv.org/pdf/1802.05365](https://arxiv.org/pdf/1802.05365).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Karen Simonyan and Andrew Zisserman, “Very Deep Convolutional Networks for Large-Scale
    Image Recognition,” arXiv.org, September 4, 2014, [https://arxiv.org/abs/1409.1556v6](https://arxiv.org/abs/1409.1556v6).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kaiming He et al., “Deep Residual Learning for Image Recognition,” arXiv.org,
    December 10, 2015, [https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aurora Cramer, Ho-Hsiang Wu, Justin Salamon, and Juan Pablo Bello, “OpenL3 —
    OpenL3 0.4.2 documentation,” n.d., [https://openl3.readthedocs.io/en/latest/#](https://openl3.readthedocs.io/en/latest/#).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Google | vggish | Kaggle,” n.d., [https://www.kaggle.com/models/google/vggish](https://www.kaggle.com/models/google/vggish).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tran, D., Bourdev, L., Fergus, R., Torresani, L., and Paluri, M., “Learning
    Spatiotemporal Features with 3D Convolutional Networks.” arXiv:1412.0767, October
    7, 2015\. [https://arxiv.org/pdf/1412.0767](https://arxiv.org/pdf/1412.0767).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Grover, A., and Leskovec, J. “Node2Vec: Scalable Feature Learning for Networks.”
    *Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery
    and Data Mining*, 2016\. [https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf](https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bryan Perozzi, Rami Al-Rfou, and Steven Skiena, “DeepWalk,” August 24, 2014,
    [https://doi.org/10.1145/2623330.2623732](https://doi.org/10.1145/2623330.2623732).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang, S., and Xu, Y. “Json2Vec: A Representation Learning Method for JSON
    Data.” arXiv:2002.05707, February 13, 2020\. [https://arxiv.org/pdf/2002.05707](https://arxiv.org/pdf/2002.05707).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alec Radford et al., “Learning Transferable Visual Models From Natural Language
    Supervision,” arXiv.org, February 26, 2021, [https://arxiv.org/abs/2103.00020](https://arxiv.org/abs/2103.00020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Chapter 5**](B22495_05.xhtml#_idTextAnchor115)**,** **Vector Databases**'
  prefs: []
  type: TYPE_NORMAL
- en: Yu. A. Malkov and D. A. Yashunin, “Efficient and robust approximate nearest
    neighbor search using Hierarchical Navigable Small World graphs,” arXiv.org, March
    30, 2016, [http://arxiv.org/abs/1603.09320](http://arxiv.org/abs/1603.09320).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yikun Han, Chunjiang Liu, and Pengfei Wang, “A Comprehensive Survey on Vector
    Database: Storage and Retrieval Technique, Challenge,” arXiv.org, October 18,
    2023, [http://arxiv.org/abs/2310.11703](http://arxiv.org/abs/2310.11703).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhi Jing et al., “When Large Language Models Meet Vector Databases: A Survey,”
    arXiv.org, January 30, 2024, [http://arxiv.org/abs/2402.01763](http://arxiv.org/abs/2402.01763).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Doug Turnbull, “What Is a Judgment List?,” Doug Turnbull’s Blog, February 21,
    2021, [https://softwaredoug.com/blog/2021/02/21/what-is-a-judgment-list](https://softwaredoug.com/blog/2021/02/21/what-is-a-judgment-list).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Building RAG-based LLM Applications for Production,” Anyscale, n.d., [https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1](https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “How to Perform Hybrid Search - MongoDB Atlas,” n.d., [https://www.mongodb.com/docs/atlas/atlas-vector-search/tutorials/reciprocal-rank-fusion/](https://www.mongodb.com/docs/atlas/atlas-vector-search/tutorials/reciprocal-rank-fusion/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Review Deployment Options - MongoDB Atlas,” n.d., [https://www.mongodb.com/docs/atlas/atlas-vector-search/deployment-options/](https://www.mongodb.com/docs/atlas/atlas-vector-search/deployment-options/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Chapter 6**](B22495_06.xhtml#_idTextAnchor137)**, AI/ML** **Application
    Design**'
  prefs: []
  type: TYPE_NORMAL
- en: “How to Index Fields for Vector Search - MongoDB Atlas,” n.d., [https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-type/#considerations](https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-type/#considerations).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lauren Schaefer Daniel Coupal, “Bloated Documents | MongoDB,” May 31, 2022,
    [https://www.mongodb.com/developer/products/mongodb/schema-design-anti-pattern-bloated-documents/](https://www.mongodb.com/developer/products/mongodb/schema-design-anti-pattern-bloated-documents/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Daniel Coupal, “Building with Patterns: The Extended Reference Pattern,” MongoDB,
    March 19, 2019, [https://www.mongodb.com/blog/post/building-with-patterns-the-extended-reference-pattern](https://www.mongodb.com/blog/post/building-with-patterns-the-extended-reference-pattern).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Atlas Cluster Sizing and Tier Selection - MongoDB Atlas,” n.d., [https://www.mongodb.com/docs/atlas/sizing-tier-selection/](https://www.mongodb.com/docs/atlas/sizing-tier-selection/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Customize Cluster Storage - MongoDB Atlas,” n.d., [https://www.mongodb.com/docs/atlas/customize-storage/](https://www.mongodb.com/docs/atlas/customize-storage/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Amazon EBS volume types - Amazon EBS,” n.d., [https://docs.aws.amazon.com/ebs/latest/userguide/ebs-volume-types.html#gp3-ebs-volume-type](https://docs.aws.amazon.com/ebs/latest/userguide/ebs-volume-types.html#gp3-ebs-volume-type).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Customize Cluster Storage - MongoDB Atlas,” n.d., [https://www.mongodb.com/docs/atlas/customize-storage/](https://www.mongodb.com/docs/atlas/customize-storage/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Chapter 7**](B22495_07.xhtml#_idTextAnchor162)**, Useful Frameworks, Libraries,**
    **and APIs**'
  prefs: []
  type: TYPE_NORMAL
- en: “MongoDB Atlas,” LangChain, n.d., [https://python.langchain.com/v0.2/docs/integrations/vectorstores/mongodb_atlas/](https://python.langchain.com/v0.2/docs/integrations/vectorstores/mongodb_atlas/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “How to Index Fields for Vector Search - MongoDB Atlas,” n.d., [https://www.mongodb.com/docs/atlas/atlas-vector-search/manage-indexes/](https://www.mongodb.com/docs/atlas/atlas-vector-search/manage-indexes/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Get Started with the LangChain Integration - MongoDB Atlas,” n.d., [https://www.mongodb.com/docs/atlas/atlas-vector-search/ai-integrations/langchain/](https://www.mongodb.com/docs/atlas/atlas-vector-search/ai-integrations/langchain/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “MongoDB with Python - MongoDB Documentation,” n.d., [https://www.mongodb.com/docs/languages/python/#integrations](https://www.mongodb.com/docs/languages/python/#integrations).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Transformers,” n.d., [https://huggingface.co/docs/transformers/en/index](https://huggingface.co/docs/transformers/en/index).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “OpenAI developer platform,” OpenAI Platform, n.d., [https://platform.openai.com/docs/overview](https://platform.openai.com/docs/overview).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Chapter 8**](B22495_08.xhtml#_idTextAnchor180)**, Implementing Vector Search
    in** **AI Applications**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Yunfan Gao et al., “Retrieval-Augmented Generation for Large Language Models:
    A Survey,” arXiv.org, December 18, 2023, [https://arxiv.org/abs/2312.10997](https://arxiv.org/abs/2312.10997).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rupak Roy, “Harness LLM Output-parsers like CommaSeparatedListOutputParser,
    PydanticOutputParser and more for a Structured Ai | by Rupak (Bob) Roy - II |
    Medium | Medium,” *Medium*, August 14, 2024, [https://bobrupakroy.medium.com/harness-llm-output-parsers-for-a-structured-ai-7b456d231834](https://bobrupakroy.medium.com/harness-llm-output-parsers-for-a-structured-ai-7b456d231834).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mirjam Minor and Eduard Kaucher, “Retrieval Augmented Generation with LLMs for
    Explaining Business Process Models,” in *Lecture Notes in Computer Science*, 2024,
    175–90, [https://doi.org/10.1007/978-3-031-63646-2_12](https://doi.org/10.1007/978-3-031-63646-2_12).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Chapter 9**](B22495_09.xhtml#_idTextAnchor193)**, LLM** **Output Evaluation**'
  prefs: []
  type: TYPE_NORMAL
- en: “Papers with Code - Measuring Massive Multitask Language Understanding,” September
    7, 2020, [https://paperswithcode.com/paper/measuring-massive-multitask-language](https://paperswithcode.com/paper/measuring-massive-multitask-language).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '“Papers with Code - HellaSwag: Can a Machine Really Finish Your Sentence?,”
    May 19, 2019, [https://paperswithcode.com/paper/hellaswag-can-a-machine-really-finish-your](https://paperswithcode.com/paper/hellaswag-can-a-machine-really-finish-your).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Papers with Code - Evaluating Large Language Models Trained on Code,” July
    7, 2021, [https://paperswithcode.com/paper/evaluating-large-language-models-trained-on](https://paperswithcode.com/paper/evaluating-large-language-models-trained-on).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Introduction | Ragas,” n.d., [https://docs.ragas.io/en/stable/index.html](https://docs.ragas.io/en/stable/index.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Chapter 10**](B22495_10.xhtml#_idTextAnchor214)**, Refining the Semantic
    Data Model to** **Improve Accuracy**'
  prefs: []
  type: TYPE_NORMAL
- en: “SentenceTransformers Documentation — Sentence Transformers documentation,”
    n.d., [https://sbert.net/](https://sbert.net/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Train and Fine-Tune Sentence Transformers Models,” n.d., [https://huggingface.co/blog/how-to-train-sentence-transformers](https://huggingface.co/blog/how-to-train-sentence-transformers).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Run Vector Search Queries - MongoDB Atlas,” n.d., [https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-stage/#atlas-vector-search-pre-filter](https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-stage/#atlas-vector-search-pre-filter).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Knowledge Graph RAG Query Engine - LlamaIndex,” n.d., [https://docs.llamaindex.ai/en/stable/examples/query_engine/knowledge_graph_rag_query_engine/](https://docs.llamaindex.ai/en/stable/examples/query_engine/knowledge_graph_rag_query_engine/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Chapter 11**](B22495_11.xhtml#_idTextAnchor232)**, Common Failures of**
    **Generative AI**'
  prefs: []
  type: TYPE_NORMAL
- en: Lance Eliot, “Doctors Relying On Generative AI To Summarize Medical Notes Might
    Unknowingly Be Taking Big Risks,” *Forbes*, July 2, 2024, [https://www.forbes.com/sites/lanceeliot/2024/02/05/doctors-relying-on-generative-ai-to-summarize-medical-notes-might-unknowingly-be-taking-big-risks/](https://www.forbes.com/sites/lanceeliot/2024/02/05/doctors-relying-on-generative-ai-to-summarize-medical-notes-might-unknowingly-be-taking-big-risks/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Markman, Ofer. “Time to Strategize: 85% of Data is Garbage or Siloed.” *Filo
    Focus*, February 11, 2024\. [https://www.filo.systems/blog/85-percent-of-data-is-not-actionable-time-to-restrategize](https://www.filo.systems/blog/85-percent-of-data-is-not-actionable-time-to-restrategize).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Neeman, Ella, Roee Aharoni, Or Honovich, et al. “DisentQA: Disentangling Parametric
    and Contextual Knowledge with Counterfactual Question Answering.” arXiv.org, November
    10, 2022\. [https://arxiv.org/pdf/2211.05655](https://arxiv.org/pdf/2211.05655).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sharma, Mrinank, Meg Tong, Tomasz Korbak, et al. “Towards Understanding Sycophancy
    in Language Models.” arXiv.org, October 20, 2023\. [https://arxiv.org/abs/2310.13548](https://arxiv.org/abs/2310.13548).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sparkes, Matthew. “AI chatbots become more sycophantic as they get more advanced.”
    *New Scientist*, August 17, 2023\. [https://www.newscientist.com/article/2386915-ai-chatbots-become-more-sycophantic-as-they-get-more-advanced/](https://www.newscientist.com/article/2386915-ai-chatbots-become-more-sycophantic-as-they-get-more-advanced/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei, Jerry, Da Huang, Yifeng Lu, et al. “Simple synthetic data reduces sycophancy
    in large language models.” arXiv.org, August 7, 2023\. [https://arxiv.org/abs/2308.03958](https://arxiv.org/abs/2308.03958).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Chapter 12**](B22495_12.xhtml#_idTextAnchor253)**, Correcting and Optimizing
    Your Generative** **AI Application**'
  prefs: []
  type: TYPE_NORMAL
- en: Chui, Michael, Roger Roberts, Tanya Rodchenko, et al. “What every CEO should
    know about generative AI.” McKinsey Digital, May 12, 2023\. [https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/what-every-ceo-should-know-about-generative-ai](https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/what-every-ceo-should-know-about-generative-ai).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xiao, Han. “What is ColBERT and Late Interaction and Why They Matter in Search?,”
    Jina AI, February 20, 2024\. [https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/](https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
