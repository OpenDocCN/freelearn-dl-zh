- en: '18'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Adversarial Robustness
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Adversarial attacks** on LLMs are designed to manipulate the model’s output
    by making small, often imperceptible changes to the input. These attacks can expose
    vulnerabilities in LLMs and potentially lead to security risks or unintended behaviors
    in real-world applications.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll discover techniques for creating and defending against
    **adversarial examples** in LLMs. Adversarial examples are carefully crafted inputs
    designed to intentionally mislead the model into producing incorrect or unexpected
    outputs. You’ll learn about textual adversarial attacks, methods to generate these
    examples, and techniques to make your models more robust. We’ll also cover evaluation
    methods and discuss the real-world implications of adversarial attacks on LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll be covering the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Types of textual adversarial attacks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adversarial training techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating robustness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trade-offs in the adversarial training of LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-world implications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Types of textual adversarial attacks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Textual adversarial attacks involve subtly modifying text inputs to mislead
    models into making incorrect predictions, often with changes imperceptible to
    humans.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three levels of attacks to keep in mind:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Character-level attacks**: Modifying individual characters in words'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Word-level attacks**: Replacing words with synonyms or similar words'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sentence-level attacks**: Modifying sentence structure while preserving meaning'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, a fourth type of attack to consider involves **universal adversarial
    triggers**, which are specific phrases that, when added, cause the model to behave
    incorrectly across various inputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some illustrative examples of universal adversarial triggers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**For sentiment analysis**: Imagine a sentiment analysis model trained to classify
    movie reviews as “positive” or “negative.” A universal adversarial trigger might
    be a seemingly nonsensical phrase such as “invisible carpet notebook”. If this
    trigger is appended to various positive reviews, such as “This movie was fantastic,”
    resulting in “This movie was fantastic invisible carpet notebook,” the model might
    incorrectly classify them as “negative.” Similarly, adding the same trigger to
    other positive reviews such as “I really enjoyed the acting” could also flip its
    prediction to “negative.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**For text classification (e.g., topic identification)**: Consider a model
    classifying news articles into categories such as “Sports,” “Politics,” or “Technology.”
    A universal adversarial trigger could be “the the grand elephant”. Appending this
    trigger to a “Sports” article such as “The team won the championship” to get “The
    team won the championship the the grand elephant” might cause the model to misclassify
    it as “Politics.” Likewise, adding it to a “Technology” article about a new gadget
    could also lead to an incorrect classification.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**For natural language inference (NLI)**: In NLI, a model determines the relationship
    between two sentences (e.g., *entailment*, *contradiction*, *neutral*). A trigger
    such as “always dance quickly” might be designed to consistently force an “entailment”
    prediction. For example, given the premise “A person is walking,” and the hypothesis
    “A person is moving,” the model would correctly predict “entailment.” However,
    if the trigger is added to the premise, “A person is walking always dance quickly,”
    the model might be tricked into still predicting “entailment” even if the relationship
    becomes less clear or even nonsensical.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For simplicity, in this section, we will discuss two types of attacks. First,
    let’s implement a simple character-level attack:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This code defines a `character_level_attack` function that aims to create a
    slightly altered version of an input text by randomly modifying individual characters.
    For each character in the input text, there is a probability (set by the `prob`
    parameter, defaulting to `0.1`) that it will be changed. If a character is selected
    for modification and it is an alphabetic character, it will be replaced by a random
    lowercase or uppercase letter. Non-alphabetic characters (such as spaces and punctuation)
    are left unchanged. The function then joins the potentially modified characters
    back into a string, producing the “attacked” text.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of this code will display two lines. The first line, labeled `"Original:"`,
    will show the initial input text: `"The quick brown fox jumps over the lazy dog."`.
    The second line, labeled `"Attacked:"`, will present the modified text. Due to
    the random nature of the character replacement based on the `prob` value, the
    `"Attacked:"` text will likely have some of its alphabetic characters replaced
    by other random letters. For example, “The” might become “Tge”, “quick” could
    be “quicj”, and so on. The number and specific locations of these changes will
    vary each time the code is executed because of the random selection process.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, as another example, let’s implement a more sophisticated word-level attack
    using synonym replacement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This function retrieves synonyms for a given word based on its part of speech.
    It uses WordNet, a lexical database for the English language, to find synonyms
    while ensuring they are different from the original word.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s implement a word-level attack:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This code snippet defines a function `word_level_attack` that attempts to create
    a subtly altered version of an input text by randomly replacing some words with
    their synonyms. It first tokenizes the input text into individual words and then
    determines the part-of-speech (POS) tag for each word. For each word, there’s
    a probability (set by the `prob` parameter, defaulting to `0.2`) that the word
    will be targeted for replacement. If a word is chosen, its POS tag is used to
    find potential synonyms from the WordNet lexical database. If synonyms are found,
    a random synonym replaces the original word in the output; otherwise, the original
    word is kept.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of this code will display two lines. The first line, labeled `"Original:"`,
    will show the initial input text: `"The intelligent scientist conducted groundbreaking
    research."`. The second line, labeled `"Attacked:"`, will present the modified
    text. Due to the random nature of the word replacement based on the prob value,
    the `"Attacked:"` text will likely have some words replaced by their synonyms.
    For instance, “intelligent” might be replaced by “smart” or “clever,” “conducted”
    by “carried_out” or “did,” and “groundbreaking” by “innovative” or “pioneering.”
    The specific changes will vary each time the code is executed because of the random
    selection of words and their synonyms.'
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial training techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Adversarial training involves exposing the model to adversarial examples during
    the training process to improve its robustness. Here’s a simplified example of
    how you might implement adversarial training for an LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This function performs a single step of adversarial training. It generates adversarial
    perturbations using the **Fast Gradient Sign Method** (**FGSM**) and combines
    the loss from both clean and adversarial inputs. FGSM is a single-step adversarial
    attack that efficiently generates adversarial examples by calculating the gradient
    of the loss function with respect to the input data and then adding a small perturbation
    in the direction of the gradient’s sign. This perturbation, scaled by a small
    epsilon, aims to maximize the model’s prediction error, causing misclassification
    while being almost imperceptible to humans.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use this in a full training loop, employ the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This function iterates over the training data, performing adversarial training
    steps for each batch. It updates the model parameters using the combined loss
    from clean and adversarial inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating robustness
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To evaluate the robustness of an LLM, we can measure its performance on both
    clean and adversarial inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This function evaluates the model’s performance on both clean and adversarially
    attacked inputs. It processes each item in the test dataset, generating predictions
    for both the original and attacked versions of the input.
  prefs: []
  type: TYPE_NORMAL
- en: 'You should also calculate the evaluation metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The provided Python code defines a function called `calculate_metrics` that
    takes three arguments: the true labels of the test data, the model’s predictions
    on the original (clean) test data, and the model’s predictions on the adversarially
    attacked versions of the test data. Inside the function, it utilizes the `accuracy_score`
    and `f1_score` functions from the `sklearn.metrics` library to calculate four
    key evaluation metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: The accuracy of the model’s predictions on the clean data (`clean_accuracy`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The accuracy on the adversarial data (`adv_accuracy`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The weighted F1 score on the clean data (`clean_f1`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The weighted F1 score on the adversarial data (`adv_f1`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The function then returns these four scores as a dictionary, where each metric’s
    name is the key and its calculated value is the corresponding value.
  prefs: []
  type: TYPE_NORMAL
- en: Each of the calculated scores provides a different perspective on the model’s
    performance. Accuracy represents the overall proportion of correctly classified
    instances out of the total number of instances. A high accuracy on clean data
    indicates the model performs well on original, unperturbed inputs, while a low
    accuracy suggests poor general performance. Conversely, a high accuracy on adversarial
    data implies the model is robust against the specific type of attack used, meaning
    the attacks are not very effective at fooling the model. A low accuracy on adversarial
    data, despite potentially high clean accuracy, highlights the model’s vulnerability
    to these attacks. The F1 score, particularly the weighted version used here to
    account for potential class imbalance, provides a balanced measure of precision
    and recall. A high F1 score on clean data signifies good performance in terms
    of both correctly identifying positive instances and avoiding false positives.
    Similarly, a high F1 score on adversarial data indicates robustness, as the model
    maintains good precision and recall even under attack. A low F1 score on either
    clean or adversarial data suggests the model struggles with either precision or
    recall, or both, in those respective conditions. Comparing the clean and adversarial
    scores reveals the extent to which the attacks degrade the model’s performance;
    a significant drop indicates a lack of robustness.
  prefs: []
  type: TYPE_NORMAL
- en: Trade-offs in the adversarial training of LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Adversarial training can improve model robustness, but it often comes with
    trade-offs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Increased computational cost**: Generating adversarial examples during training
    is computationally expensive'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Potential decrease in clean accuracy**: Focusing on adversarial robustness
    might slightly reduce performance on clean inputs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generalization to unseen attacks**: Models might become robust to specific
    types of attacks but remain vulnerable to others'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To visualize these trade-offs, you could create a plot comparing clean and
    adversarial accuracy across different levels of adversarial training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This function creates a plot to visualize how increasing the strength of adversarial
    training (epsilon) affects both clean and adversarial accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Real-world implications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Understanding the real-world implications of adversarial attacks on LLMs is
    crucial for responsible deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Security risks**: Adversarial attacks could be used to bypass content filters
    or manipulate model outputs in security-critical applications'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Misinformation**: Attackers could potentially use adversarial techniques
    to generate fake news or misleading content that evades detection systems'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**User trust**: If LLMs are easily fooled by adversarial inputs, it could erode
    user trust in AI systems'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Legal and ethical concerns**: The ability to manipulate LLM outputs raises
    ethical questions about responsibility and accountability in AI-driven decision-making'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Robustness in diverse environments**: Real-world deployment of LLMs requires
    evaluating their performance under diverse adverse conditions, rather than relying
    solely on clean laboratory settings'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To address these implications, consider implementing robust deployment practices
    and red teaming exercises:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This class encapsulates best practices for deploying robust LLMs, including
    input validation, attack detection, and output post-processing.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Addressing adversarial robustness in LLMs is crucial for their safe and reliable
    deployment in real-world applications. By implementing the techniques and considerations
    discussed in this chapter, you can work toward developing LLMs that are more resilient
    to adversarial attacks while maintaining high performance on clean inputs.
  prefs: []
  type: TYPE_NORMAL
- en: In the upcoming chapter, we will explore **Reinforcement Learning from Human
    Feedback** (**RLHF**) for LLM training.
  prefs: []
  type: TYPE_NORMAL
