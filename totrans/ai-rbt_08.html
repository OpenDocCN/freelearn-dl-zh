<html><head></head><body>
<div id="_idContainer095">
<h1 class="chapter-number" id="_idParaDest-128"><a id="_idTextAnchor235"/><span class="koboSpan" id="kobo.1.1">8</span></h1>
<h1 id="_idParaDest-129"><a id="_idTextAnchor236"/><span class="koboSpan" id="kobo.2.1">Putting Things Away</span></h1>
<p><span class="koboSpan" id="kobo.3.1">Imagine that you have to get to Grandma’s house, which, according to legend, is </span><em class="italic"><span class="koboSpan" id="kobo.4.1">over the hills and through the woods</span></em><span class="koboSpan" id="kobo.5.1">, and two states away. </span><span class="koboSpan" id="kobo.5.2">That would be two countries away if you live in Europe. </span><span class="koboSpan" id="kobo.5.3">To plan your trip, you can start in one of two ways. </span><span class="koboSpan" id="kobo.5.4">Ignoring the fact that Google has taken away most map reading and navigation skills from today’s youth, you would get out a map and do one of </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.7.1">Start at your house and try to find the roads that are closest to a straight line to </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">Grandma’s house</span></span></li>
<li><span class="koboSpan" id="kobo.9.1">Start at Grandma’s house and try to find roads leading to </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">your home</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.11.1">From either direction, you will find that the road or path you seek forks, intersects, changes, meanders, and may even come to a dead end. </span><span class="koboSpan" id="kobo.11.2">Also, all roads are not created equally – some are bigger, with higher speed limits, and some are smaller, with more stop signs. </span><span class="koboSpan" id="kobo.11.3">In the end, you pick your route by the combination of decisions that results in the lowest cost. </span><span class="koboSpan" id="kobo.11.4">This cost may be in terms of </span><em class="italic"><span class="koboSpan" id="kobo.12.1">time</span></em><span class="koboSpan" id="kobo.13.1"> – how long to get there. </span><span class="koboSpan" id="kobo.13.2">It may be in terms of </span><em class="italic"><span class="koboSpan" id="kobo.14.1">distance</span></em><span class="koboSpan" id="kobo.15.1"> – how many miles to cover. </span><span class="koboSpan" id="kobo.15.2">Or it may be in </span><em class="italic"><span class="koboSpan" id="kobo.16.1">monetary</span></em><span class="koboSpan" id="kobo.17.1"> terms – there is a toll road that charges an </span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">extra fee.</span></span></p>
<p><span class="koboSpan" id="kobo.19.1">In this chapter, we will be discussing several ways to solve problems involving choosing a chain of multiple decisions where there is some metric – such as cost – to help us select which combination is somehow the best. </span><span class="koboSpan" id="kobo.19.2">There is a lot of information here that is widely used in robotics, and we will be expanding our horizons a bit beyond our toy-grabbing robot to look at robot path planning and decision-making in general. </span><span class="koboSpan" id="kobo.19.3">These are critical skills for any robotics practitioner, so they are included here. </span><span class="koboSpan" id="kobo.19.4">This chapter covers the basics of </span><a id="_idIndexMarker566"/><span class="koboSpan" id="kobo.20.1">decision-making processes for </span><strong class="bold"><span class="koboSpan" id="kobo.21.1">artificial intelligence</span></strong><span class="koboSpan" id="kobo.22.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.23.1">AI</span></strong><span class="koboSpan" id="kobo.24.1">) where the problem can be described in terms of either a </span><strong class="bold"><span class="koboSpan" id="kobo.25.1">classification problem</span></strong><span class="koboSpan" id="kobo.26.1"> (determining </span><a id="_idIndexMarker567"/><span class="koboSpan" id="kobo.27.1">whether this situation belongs to one or more groups of similar situations) or a </span><strong class="bold"><span class="koboSpan" id="kobo.28.1">regression problem</span></strong><span class="koboSpan" id="kobo.29.1"> (fitting or </span><a id="_idIndexMarker568"/><span class="koboSpan" id="kobo.30.1">approximating a function that can be a curve or a path). </span><span class="koboSpan" id="kobo.30.2">Finally, we will be applying two approaches to our robot problem – an expert system and </span><span class="No-Break"><span class="koboSpan" id="kobo.31.1">random forests.</span></span></p>
<p><span class="koboSpan" id="kobo.32.1">This chapter will cover the </span><span class="No-Break"><span class="koboSpan" id="kobo.33.1">following topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.34.1">Decision trees and </span><span class="No-Break"><span class="koboSpan" id="kobo.35.1">random forests</span></span></li>
<li><span class="koboSpan" id="kobo.36.1">Path planning, grid searches, and the A* (</span><span class="No-Break"><span class="koboSpan" id="kobo.37.1">A-star) algorithm</span></span></li>
<li><span class="koboSpan" id="kobo.38.1">Dynamic planning with the D* (</span><span class="No-Break"><span class="koboSpan" id="kobo.39.1">D-star) technique</span></span></li>
<li><span class="koboSpan" id="kobo.40.1">Expert systems and </span><span class="No-Break"><span class="koboSpan" id="kobo.41.1">knowledge bases</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.42.1">At first glance, the concepts we will cover in this section – namely, path planning, decision trees, random forests, grid searches, and GPS route finders – don’t have much in common, other than all being part of computer algorithms used in AI. </span><span class="koboSpan" id="kobo.42.2">From my point of view, they are all basically the same concept and approach problems in the </span><span class="No-Break"><span class="koboSpan" id="kobo.43.1">same way.</span></span></p>
<h1 id="_idParaDest-130"><a id="_idTextAnchor237"/><span class="koboSpan" id="kobo.44.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.45.1">The one </span><a id="_idIndexMarker569"/><span class="koboSpan" id="kobo.46.1">tool we use for this chapter, you should have already installed from earlier chapters – </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.47.1">scikit-learn</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.48.1"> (</span></span><a href="http://scikit-learn.org/stable/developers/advanced_installation.html"><span class="No-Break"><span class="koboSpan" id="kobo.49.1">http://scikit-learn.org/stable/developers/advanced_installation.html</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.50.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.51.1">Or, if you have the </span><strong class="source-inline"><span class="koboSpan" id="kobo.52.1">pip</span></strong><span class="koboSpan" id="kobo.53.1"> installer in Python, you can install it using the </span><span class="No-Break"><span class="koboSpan" id="kobo.54.1">following command:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.55.1">
pip install –U scikit-learn</span></pre> <p><span class="koboSpan" id="kobo.56.1">You’ll find the code for this chapter </span><span class="No-Break"><span class="koboSpan" id="kobo.57.1">at </span></span><a href="https://github.com/PacktPublishing/Artificial-Intelligence-for-Robotics-2e"><span class="No-Break"><span class="koboSpan" id="kobo.58.1">https://github.com/PacktPublishing/Artificial-Intelligence-for-Robotics-2e</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.59.1">.</span></span></p>
<h1 id="_idParaDest-131"><a id="_idTextAnchor238"/><span class="koboSpan" id="kobo.60.1">Task analysis</span></h1>
<p><span class="koboSpan" id="kobo.61.1">Our task in this chapter is one that you may have been waiting for if you have been keeping score since </span><a href="B19846_03.xhtml#_idTextAnchor043"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.62.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.63.1">, where we discussed our storyboards. </span><span class="koboSpan" id="kobo.63.2">We need to navigate around the room on our wheels and find a path to our destination, whether that is picking up a toy or driving to </span><span class="No-Break"><span class="koboSpan" id="kobo.64.1">a toybox.</span></span></p>
<p><span class="koboSpan" id="kobo.65.1">To achieve this, we will be using </span><strong class="bold"><span class="koboSpan" id="kobo.66.1">decision trees</span></strong><span class="koboSpan" id="kobo.67.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.68.1">classification</span></strong><span class="koboSpan" id="kobo.69.1"> (a type of </span><strong class="bold"><span class="koboSpan" id="kobo.70.1">unsupervised learning</span></strong><span class="koboSpan" id="kobo.71.1">), </span><strong class="bold"><span class="koboSpan" id="kobo.72.1">fishbone diagrams</span></strong><span class="koboSpan" id="kobo.73.1">, which are good for troubleshooting, and finally, </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.74.1">path planni</span><a id="_idTextAnchor239"/><a id="_idTextAnchor240"/><span class="koboSpan" id="kobo.75.1">ng</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.76.1">.</span></span></p>
<h1 id="_idParaDest-132"><a id="_idTextAnchor241"/><span class="koboSpan" id="kobo.77.1">Introducing decision trees</span></h1>
<p><span class="koboSpan" id="kobo.78.1">The concept of a </span><strong class="bold"><span class="koboSpan" id="kobo.79.1">decision tree</span></strong><span class="koboSpan" id="kobo.80.1"> is fairly simple. </span><span class="koboSpan" id="kobo.80.2">You are walking down the sidewalk and come to a corner. </span><span class="koboSpan" id="kobo.80.3">Here, you can go right, turn left, or go straight ahead. </span><span class="koboSpan" id="kobo.80.4">That is your decision. </span><span class="koboSpan" id="kobo.80.5">After </span><a id="_idIndexMarker570"/><span class="koboSpan" id="kobo.81.1">making the decision – to turn left – you now have different decisions ahead of you than if you turned right. </span><span class="koboSpan" id="kobo.81.2">Each decision creates paths that lead to </span><span class="No-Break"><span class="koboSpan" id="kobo.82.1">other decisions.</span></span></p>
<p><span class="koboSpan" id="kobo.83.1">As we are walking down the sidewalk, we have a goal in mind. </span><span class="koboSpan" id="kobo.83.2">We are not just wandering around aimlessly; we are trying to get to some goal. </span><span class="koboSpan" id="kobo.83.3">One or more combinations of decisions will get us to the goal. </span><span class="koboSpan" id="kobo.83.4">Let’s say the goal is to get to the grocery store to buy bread. </span><span class="koboSpan" id="kobo.83.5">There may be four or five paths down sidewalks that will get you to the store, but each path may be different in length or may have different paths. </span><span class="koboSpan" id="kobo.83.6">If one path goes up a hill, that may be harder than taking the level path. </span><span class="koboSpan" id="kobo.83.7">Another path may have you wait at a traffic light, which costs time. </span><span class="koboSpan" id="kobo.83.8">We assign a value to each of these attributes and generally want to pick the path with the lowest cost, or the highest reward, depending on </span><span class="No-Break"><span class="koboSpan" id="kobo.84.1">the problem.</span></span></p>
<p><span class="koboSpan" id="kobo.85.1">In the following decision tree, we can break down the actions of the robot in order to pick up a toy. </span><span class="koboSpan" id="kobo.85.2">We start by looking at the toy aspect ratio (the length versus width of the bounding box we detected in </span><a href="B19846_04.xhtml#_idTextAnchor126"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.86.1">Chapter 4</span></em></span></a><span class="koboSpan" id="kobo.87.1">). </span><span class="koboSpan" id="kobo.87.2">We adjust the wrist of the robot arm based on the narrowest part of the toy. </span><span class="koboSpan" id="kobo.87.3">Then, we try to pick up the toy with that wrist position. </span><span class="koboSpan" id="kobo.87.4">If we are successful, we lift the toy off the ground and carry it to the toybox. </span><span class="koboSpan" id="kobo.87.5">If we fail, we try another position. </span><span class="koboSpan" id="kobo.87.6">After trying all of the positions, we go on to the next toy and try to come back to this toy later, hopefully from a different angle. </span><span class="koboSpan" id="kobo.87.7">You can see the utility of breaking down our actions this way, and it ends up that decision trees are useful for a lot of things, as we will see in </span><span class="No-Break"><span class="koboSpan" id="kobo.88.1">this cha</span><a id="_idTextAnchor242"/><span class="koboSpan" id="kobo.89.1">pter:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer083">
<span class="koboSpan" id="kobo.90.1"><img alt="Figure 8.1 – A simple decision tree on how to pick up toys" src="image/B19846_08_1.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.91.1">Figure 8.1 – A simple decision tree on how to pick up toys</span></p>
<p><span class="koboSpan" id="kobo.92.1">The general problem with decision tree-type problems is one of </span><em class="italic"><span class="koboSpan" id="kobo.93.1">exponential growth</span></em><span class="koboSpan" id="kobo.94.1">. </span><span class="koboSpan" id="kobo.94.2">Let’s consider a chess game, a favorite problem set for AI. </span><span class="koboSpan" id="kobo.94.3">We have 20 choices for an opening move (8 pawns and 2 knights, each with 2 possible moves). </span><span class="koboSpan" id="kobo.94.4">Each of these 20 moves has 20 possible next moves, and so on. </span><span class="koboSpan" id="kobo.94.5">So the first move has 20 choices, and the second move </span><a id="_idIndexMarker571"/><span class="koboSpan" id="kobo.95.1">has 400 choices. </span><span class="koboSpan" id="kobo.95.2">The third move has 197,281 choices! </span><span class="koboSpan" id="kobo.95.3">We soon have a very, very large decision tree as we try to plan ahead. </span><span class="koboSpan" id="kobo.95.4">We can say that each of these possible decisions is a </span><strong class="bold"><span class="koboSpan" id="kobo.96.1">branch</span></strong><span class="koboSpan" id="kobo.97.1">, the state we are in after making the decision is a </span><strong class="bold"><span class="koboSpan" id="kobo.98.1">leaf</span></strong><span class="koboSpan" id="kobo.99.1">, and the entire conceptual structure is a </span><span class="No-Break"><span class="koboSpan" id="kobo.100.1">decision tree.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.101.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.102.1">The secret to working with decision trees is to ruthlessly prune the branches so you consider as few decisions </span><span class="No-Break"><span class="koboSpan" id="kobo.103.1">as possible.</span></span></p>
<p><span class="koboSpan" id="kobo.104.1">There are two ways to deal with a decision tree (actually, there are three – see if you can guess the third before I </span><span class="No-Break"><span class="koboSpan" id="kobo.105.1">explain it):</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.106.1">The first way is to start at the beginning and work outward towards your goal. </span><span class="koboSpan" id="kobo.106.2">You may come to a dead end, which means back-tracking or possibly starting over. </span><span class="koboSpan" id="kobo.106.3">We are </span><a id="_idIndexMarker572"/><span class="koboSpan" id="kobo.107.1">going to call this </span><strong class="bold"><span class="koboSpan" id="kobo.108.1">forward chaining</span></strong><span class="koboSpan" id="kobo.109.1"> (chain, as we are making a path of links from leaf to leaf in </span><span class="No-Break"><span class="koboSpan" id="kobo.110.1">the tree).</span></span></li>
<li><span class="koboSpan" id="kobo.111.1">The other way is to start with the goal and work up the tree toward the start. </span><span class="koboSpan" id="kobo.111.2">This is </span><strong class="bold"><span class="koboSpan" id="kobo.112.1">backward chaining</span></strong><span class="koboSpan" id="kobo.113.1">. </span><span class="koboSpan" id="kobo.113.2">The cool thing about backward chaining is that there are a </span><a id="_idIndexMarker573"/><span class="koboSpan" id="kobo.114.1">lot fewer branches to traverse. </span><span class="koboSpan" id="kobo.114.2">You can guess that a major problem with backward chaining is you have to know what all the leaves are in advance before you can use them. </span><span class="koboSpan" id="kobo.114.3">In many problems, such as a grid search or a path planner, this is possible. </span><span class="koboSpan" id="kobo.114.4">It does not work in chess, with an exponentially </span><span class="No-Break"><span class="koboSpan" id="kobo.115.1">massive tree.</span></span></li>
<li><span class="koboSpan" id="kobo.116.1">The third technique? </span><span class="koboSpan" id="kobo.116.2">No one says we can’t do both – we could combine both forward and backward chaining and meet somewhere in </span><span class="No-Break"><span class="koboSpan" id="kobo.117.1">the m</span><a id="_idTextAnchor243"/><span class="koboSpan" id="kobo.118.1">iddle.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.119.1">The choice of </span><a id="_idIndexMarker574"/><span class="koboSpan" id="kobo.120.1">decision tree shapes, chaining techniques, and construction is based on </span><span class="No-Break"><span class="koboSpan" id="kobo.121.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.122.1">What data </span><span class="No-Break"><span class="koboSpan" id="kobo.123.1">is available?</span></span></li>
<li><span class="koboSpan" id="kobo.124.1">What information is known or unknown? </span><span class="koboSpan" id="kobo.124.2">How is the path scored </span><span class="No-Break"><span class="koboSpan" id="kobo.125.1">or graded?</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.126.1">There are also different kinds of solutions for path planning using decision trees. </span><span class="koboSpan" id="kobo.126.2">If you were given unlimited resources, the biggest computer, perfect knowledge in advance, and are willing to wait, then you </span><a id="_idIndexMarker575"/><span class="koboSpan" id="kobo.127.1">can generate an </span><strong class="bold"><span class="koboSpan" id="kobo.128.1">optimal path</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.129.1">or solution.</span></span></p>
<p><span class="koboSpan" id="kobo.130.1">One of my lessons learned from years of developing practical AI-based robots and unmanned vehicles is that any solution that meets all of the criteria or goals is an acceptable and usable solution, and you don’t have to wait and continue to compute the perfect or optimal solution. </span><span class="koboSpan" id="kobo.130.2">Often then, a </span><em class="italic"><span class="koboSpan" id="kobo.131.1">good enough</span></em><span class="koboSpan" id="kobo.132.1"> solution is found in 1/10 or even 1/100 the time of an optimal solution, because an optimal solution requires an ex</span><a id="_idTextAnchor244"/><span class="koboSpan" id="kobo.133.1">haustive search that may have to consider all possible paths </span><span class="No-Break"><span class="koboSpan" id="kobo.134.1">and combinations.</span></span></p>
<p><span class="koboSpan" id="kobo.135.1">So, how do we approach making our decision trees work faster, or more efficiently? </span><span class="koboSpan" id="kobo.135.2">We do what any good gardener would do – start pruning </span><span class="No-Break"><span class="koboSpan" id="kobo.136.1">our</span><a id="_idTextAnchor245"/><span class="koboSpan" id="kobo.137.1"> trees.</span></span></p>
<h2 id="_idParaDest-133"><a id="_idTextAnchor246"/><span class="koboSpan" id="kobo.138.1">What do we mean by pruning?</span></h2>
<p><span class="koboSpan" id="kobo.139.1">Sometimes in the computer business, we have to make metaphors to help explain to people how something works. </span><span class="koboSpan" id="kobo.139.2">You may remember the desktop metaphor that Apple, and later, Windows, adopted to help explain graphical operating systems. </span><span class="koboSpan" id="kobo.139.3">Sometimes, we just run those metaphors into the ground, such as the trash can to delete files, or </span><em class="italic"><span class="koboSpan" id="kobo.140.1">Clippy</span></em><span class="koboSpan" id="kobo.141.1">, the paper </span><span class="No-Break"><span class="koboSpan" id="kobo.142.1">clip assistant.</span></span></p>
<p><span class="koboSpan" id="kobo.143.1">You may feel that I’ve gone </span><a id="_idIndexMarker576"/><span class="koboSpan" id="kobo.144.1">off the metaphorical deep end when I discuss </span><strong class="bold"><span class="koboSpan" id="kobo.145.1">pruning</span></strong><span class="koboSpan" id="kobo.146.1"> your decision trees. </span><span class="koboSpan" id="kobo.146.2">What’s next, fertilizer and tree spikes? </span><span class="koboSpan" id="kobo.146.3">Actually, pruning is a critical concept in decision tree-type systems. </span><span class="koboSpan" id="kobo.146.4">Each branch in your tree can lead to hundreds </span><a id="_idIndexMarker577"/><span class="koboSpan" id="kobo.147.1">or thousands of sub-branches. </span><span class="koboSpan" id="kobo.147.2">If you can decide early that a branch is not useful, you can cut it out and you don’t have to process any of the branches or leaves in that branch. </span><span class="koboSpan" id="kobo.147.3">The sooner you can discover that a path is not getting you to your goal, the quicker you can reduce the time and effort involved in creating a solution, which is a real-time system such as a robot, a self-driving car, or an autonomous aircraft; this can spell the difference between usable </span><span class="No-Break"><span class="koboSpan" id="kobo.148.1">and </span><a id="_idTextAnchor247"/><span class="koboSpan" id="kobo.149.1">worthless.</span></span></p>
<p><span class="koboSpan" id="kobo.150.1">Let’s run through a quick example in which we use the pruning method. </span><span class="koboSpan" id="kobo.150.2">One great use for a decision </span><a id="_idIndexMarker578"/><span class="koboSpan" id="kobo.151.1">tree process is </span><strong class="bold"><span class="koboSpan" id="kobo.152.1">Fault Detection, Isolation, and Recovery</span></strong><span class="koboSpan" id="kobo.153.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.154.1">FDIR</span></strong><span class="koboSpan" id="kobo.155.1">). </span><span class="koboSpan" id="kobo.155.2">This is a typical function of a robot. </span><span class="koboSpan" id="kobo.155.3">Let’s make a decision tree for FDIR in the case of our Tinman robot not moving. </span><span class="koboSpan" id="kobo.155.4">What automated steps could we take to detect the fault, isolate the problem, and then recover? </span><span class="koboSpan" id="kobo.155.5">One technique </span><a id="_idIndexMarker579"/><span class="koboSpan" id="kobo.156.1">we can use is </span><strong class="bold"><span class="koboSpan" id="kobo.157.1">root cause analysis</span></strong><span class="koboSpan" id="kobo.158.1">, where we try to figure out our problem by systematically listing and then eliminating (pruning) causing factors and seeing whether the symptoms match. </span><span class="koboSpan" id="kobo.158.2">One way to approach root </span><a id="_idIndexMarker580"/><span class="koboSpan" id="kobo.159.1">cause analysis is to use a special form of decision tree called a </span><strong class="bold"><span class="koboSpan" id="kobo.160.1">fishbone diagram</span></strong><span class="koboSpan" id="kobo.161.1">, or </span><strong class="bold"><span class="koboSpan" id="kobo.162.1">Ishikawa diagram</span></strong><span class="koboSpan" id="kobo.163.1">. </span><span class="koboSpan" id="kobo.163.2">This diagram is named after </span><a id="_idIndexMarker581"/><span class="koboSpan" id="kobo.164.1">its inventor, Professor Kaoru Ishikawa from the University of Tokyo. </span><span class="koboSpan" id="kobo.164.2">In his 1968 paper, </span><em class="italic"><span class="koboSpan" id="kobo.165.1">Guide to Quality Control</span></em><span class="koboSpan" id="kobo.166.1">, the fishbone diagram is named because of its shape, which has a central spine and ribs jutting off on either side. </span><span class="koboSpan" id="kobo.166.2">I know, the metaphors are getting deep when we have a decision </span><em class="italic"><span class="koboSpan" id="kobo.167.1">tree</span></em><span class="koboSpan" id="kobo.168.1"> in the shape of </span><span class="No-Break"><span class="koboSpan" id="kobo.169.1">a </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.170.1">fish</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.171.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.172.1">Now, we begin to have a problem. </span><span class="koboSpan" id="kobo.172.2">Remember that in a robot, a problem is a symptom, not a cause. </span><span class="koboSpan" id="kobo.172.3">Our problem is the robot is not moving. </span><span class="koboSpan" id="kobo.172.4">What can cause this problem? </span><span class="koboSpan" id="kobo.172.5">Let’s make </span><span class="No-Break"><span class="koboSpan" id="kobo.173.1">a list:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.174.1">The </span><span class="No-Break"><span class="koboSpan" id="kobo.175.1">drive system</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.176.1">The software</span></span></li>
<li><span class="koboSpan" id="kobo.177.1">The </span><span class="No-Break"><span class="koboSpan" id="kobo.178.1">communication system</span></span></li>
<li><span class="koboSpan" id="kobo.179.1">The battery </span><span class="No-Break"><span class="koboSpan" id="kobo.180.1">and wiring</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.181.1">The sensors</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.182.1">Operator error</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.183.1">Now, for each of these, we subdivide our branches into smaller branches. </span><span class="koboSpan" id="kobo.183.2">What parts of the </span><em class="italic"><span class="koboSpan" id="kobo.184.1">drive system</span></em><span class="koboSpan" id="kobo.185.1"> can cause the robot to not be able to move? </span><span class="koboSpan" id="kobo.185.2">The wheels could be stuck. </span><span class="koboSpan" id="kobo.185.3">The motors could </span><a id="_idIndexMarker582"/><span class="koboSpan" id="kobo.186.1">not be getting power. </span><span class="koboSpan" id="kobo.186.2">The gears could be jammed. </span><span class="koboSpan" id="kobo.186.3">The motor </span><a id="_idIndexMarker583"/><span class="koboSpan" id="kobo.187.1">driver could have overheated. </span><span class="koboSpan" id="kobo.187.2">Here is my fishbone diagram to illustrate the problem of the robot </span><span class="No-Break"><span class="koboSpan" id="kobo.188.1">not moving:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer084">
<span class="koboSpan" id="kobo.189.1"><img alt="Figure 8.2 – A fishbone, or Ishikawa, diagram is commonly used for troubleshooting" src="image/B19846_08_2.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.190.1">Figure 8.2 – A fishbone, or Ishikawa, diagram is commonly used for troubleshooting</span></p>
<p><span class="koboSpan" id="kobo.191.1">For each of these factors, you can consider what would be the symptoms of that problem being the cause. </span><span class="koboSpan" id="kobo.191.2">If the gears in the motors are jammed, then the motors can’t turn and the wheels can’t turn. </span><span class="koboSpan" id="kobo.191.3">If we can check any of these factors off, we can prune or eliminate the gears from our diagram or decision tree. </span><span class="koboSpan" id="kobo.191.4">We check the gears, and the wheels and motors turn by hand, so the gears are not the cause. </span><span class="koboSpan" id="kobo.191.5">We prune that branch. </span><span class="koboSpan" id="kobo.191.6">If we have an automated way of doing testing, we can automatically prune branches, which we will be able to do in the later examples in </span><span class="No-Break"><span class="koboSpan" id="kobo.192.1">this chapter.</span></span></p>
<p><span class="koboSpan" id="kobo.193.1">How about the battery? </span><span class="koboSpan" id="kobo.193.2">The battery could need charging (dead battery), the battery could be disconnected, or a power wire could be loose. </span><span class="koboSpan" id="kobo.193.3">We check the battery voltage – that is OK, so prune that leaf off the tree. </span><span class="koboSpan" id="kobo.193.4">We check the wiring – nothing loose. </span><span class="koboSpan" id="kobo.193.5">The battery bran</span><a id="_idTextAnchor248"/><span class="koboSpan" id="kobo.194.1">ch </span><span class="No-Break"><span class="koboSpan" id="kobo.195.1">gets pruned.</span></span></p>
<p><span class="koboSpan" id="kobo.196.1">And so we go on until </span><a id="_idIndexMarker584"/><span class="koboSpan" id="kobo.197.1">we have something that either matches all our symptoms or </span><a id="_idIndexMarker585"/><span class="koboSpan" id="kobo.198.1">is the last one left. </span><span class="koboSpan" id="kobo.198.2">Let’s say the last branch was communications. </span><span class="koboSpan" id="kobo.198.3">Now what? </span><span class="koboSpan" id="kobo.198.4">We ask, “What things in communications would cause us not to move?” </span><span class="koboSpan" id="kobo.198.5">Our first answer is that motor command messages are not getting through to our robot over the network. </span><span class="koboSpan" id="kobo.198.6">We check the log and see, indeed, no motor messages are present (</span><strong class="source-inline"><span class="koboSpan" id="kobo.199.1">cmd_vel</span></strong><span class="koboSpan" id="kobo.200.1">, in our case). </span><span class="koboSpan" id="kobo.200.2">There is our problem, but what caused the problem? </span><span class="koboSpan" id="kobo.200.3">The network could be broken (checked – no, the network is OK), or the IP address could be wrong (no, that’s OK). </span><span class="koboSpan" id="kobo.200.4">We look to see whether any recent changes were made to the control software, and indeed, there were. </span><span class="koboSpan" id="kobo.200.5">We revert to the previous version and see the robot move. </span><span class="koboSpan" id="kobo.200.6">There is our problem and we used a decision tree to </span><span class="No-Break"><span class="koboSpan" id="kobo.201.1">find it.</span></span></p>
<p><span class="koboSpan" id="kobo.202.1">So, in this case, we solved our problem almost entirely by pruning branches and leaves off our tree until only one path was left, or we arrived at </span><span class="No-Break"><span class="koboSpan" id="kobo.203.1">our goal.</span></span></p>
<p><span class="koboSpan" id="kobo.204.1">How can we prune branches in software? </span><span class="koboSpan" id="kobo.204.2">We can look for </span><em class="italic"><span class="koboSpan" id="kobo.205.1">dead ends</span></em><span class="koboSpan" id="kobo.206.1">. </span><span class="koboSpan" id="kobo.206.2">Dead ends are leaves – parts of the tree that end and have no future branches. </span><span class="koboSpan" id="kobo.206.3">When we reach a dead end, we can not only prune that leaf but also the parts of the path that exclusively lead to that branch. </span><span class="koboSpan" id="kobo.206.4">This would be a </span><strong class="bold"><span class="koboSpan" id="kobo.207.1">backward-chaining</span></strong><span class="koboSpan" id="kobo.208.1"> approach to pruning, as we start at the end and </span><span class="No-Break"><span class="koboSpan" id="kobo.209.1">work backward.</span></span></p>
<p><span class="koboSpan" id="kobo.210.1">We can also see sections of the tree that are unused, or never referenced or called. </span><span class="koboSpan" id="kobo.210.2">We can remove entire sections in this manner. </span><span class="koboSpan" id="kobo.210.3">This is </span><strong class="bold"><span class="koboSpan" id="kobo.211.1">forward chaining</span></strong><span class="koboSpan" id="kobo.212.1"> because </span><a id="_idTextAnchor249"/><span class="koboSpan" id="kobo.213.1">we are traversing the tree in the forward direction, from the front to </span><span class="No-Break"><span class="koboSpan" id="kobo.214.1">the back.</span></span></p>
<p><span class="koboSpan" id="kobo.215.1">Up to this point, we, the humans in the story, have been making these decision trees by hand. </span><span class="koboSpan" id="kobo.215.2">We have not even discussed how we write a program to allow the robot to use trees to make decisions. </span><span class="koboSpan" id="kobo.215.3">Wouldn’t it be a lot nicer if the computer was doing all the hard work of making the tree, deciding the branches, and labeling the nodes instead of us? </span><span class="koboSpan" id="kobo.215.4">That is exactly what we will discuss in the </span><span class="No-Break"><span class="koboSpan" id="kobo.216.1">next section.</span></span></p>
<h2 id="_idParaDest-134"><span class="koboSpan" id="kobo.217.1">Creating self-classifyi</span><a id="_idTextAnchor250"/><span class="koboSpan" id="kobo.218.1">ng decision trees</span></h2>
<p><span class="koboSpan" id="kobo.219.1">Let’s consider the problem of classifying toys. </span><span class="koboSpan" id="kobo.219.2">We may want to come up with a more efficient robot, which sorts toys in some manner instead of just dumping them in a box. </span><span class="koboSpan" id="kobo.219.3">In an ideal world, out of a population of 20 toys, we would have some characteristics that divided </span><a id="_idIndexMarker586"/><span class="koboSpan" id="kobo.220.1">the group evenly in half – 10 and 10. </span><span class="koboSpan" id="kobo.220.2">Let’s say it is length – half of the toys are under six inches long and half are over. </span><span class="koboSpan" id="kobo.220.3">Then, it would also be ideal if some other characteristic divided each of those groups of 10 in half – into four groups </span><span class="No-Break"><span class="koboSpan" id="kobo.221.1">of five.</span></span></p>
<p><span class="koboSpan" id="kobo.222.1">Let’s say it’s </span><em class="italic"><span class="koboSpan" id="kobo.223.1">color</span></em><span class="koboSpan" id="kobo.224.1"> – we have five red toys, five blue toys, five green toys, and five yellow toys. </span><span class="koboSpan" id="kobo.224.2">You may recognize that we are doing what biologists do in classifying new species – we are creating a </span><strong class="bold"><span class="koboSpan" id="kobo.225.1">taxonomy</span></strong><span class="koboSpan" id="kobo.226.1">. </span><span class="koboSpan" id="kobo.226.2">Now, we pick </span><a id="_idIndexMarker587"/><span class="koboSpan" id="kobo.227.1">another attribute that separates the toys into even smaller groups – it might be what kind of toy it is or what size wheels it has. </span><span class="koboSpan" id="kobo.227.2">I think you get the picture. </span><span class="koboSpan" id="kobo.227.3">Let’s look at </span><span class="No-Break"><span class="koboSpan" id="kobo.228.1">an example.</span></span></p>
<p><span class="koboSpan" id="kobo.229.1">Now, what would be great is if we could list all the toys and all the attributes in a table, and let the computer figure out how many groups and what kinds there are. </span><span class="koboSpan" id="kobo.229.2">We could create a table like </span><span class="No-Break"><span class="koboSpan" id="kobo.230.1">this one:</span></span></p>
<table class="T---Table _idGenTablePara-1" id="table001-2">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<thead>
<tr class="T---Table">
<td class="T---Table T---Body T---Header">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.231.1">Type</span></strong></span></p>
</td>
<td class="T---Table T---Body T---Header">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.232.1">Length</span></strong></span></p>
</td>
<td class="T---Table T---Body T---Header">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.233.1">Width</span></strong></span></p>
</td>
<td class="T---Table T---Body T---Header">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.234.1">Weight</span></strong></span></p>
</td>
<td class="T---Table T---Body T---Header">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.235.1">Color</span></strong></span></p>
</td>
<td class="T---Table T---Body T---Header">
<p><strong class="bold"><span class="koboSpan" id="kobo.236.1">Number </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.237.1">of wheels</span></strong></span></p>
</td>
<td class="T---Table T---Body T---Header">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.238.1">Noise</span></strong></span></p>
</td>
<td class="T---Table T---Body T---Header">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.239.1">Soft</span></strong></span></p>
</td>
<td class="T---Table T---Body T---Header">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.240.1">Material</span></strong></span></p>
</td>
<td class="T---Table T---Body T---Header">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.241.1">Eyes</span></strong></span></p>
</td>
<td class="T---Table T---Body T---Header">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.242.1">Toy Name</span></strong></span></p>
</td>
</tr>
</thead>
<tbody>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.243.1">car</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.244.1">3</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.245.1">1</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.246.1">35</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.247.1">red</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.248.1">4</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.249.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.250.1">hard</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.251.1">metal</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.252.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.253.1">hotwheels</span></span></p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.254.1">car</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.255.1">3</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.256.1">1</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.257.1">35</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.258.1">orange</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.259.1">4</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.260.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.261.1">hard</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.262.1">metal</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.263.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.264.1">hotwheels</span></span></p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.265.1">car</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.266.1">3</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.267.1">1</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.268.1">35</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.269.1">blue</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.270.1">4</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.271.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.272.1">hard</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.273.1">metal</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.274.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.275.1">hotwheels</span></span></p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.276.1">car</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.277.1">3</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.278.1">1</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.279.1">35</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.280.1">blue</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.281.1">4</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.282.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.283.1">hard</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.284.1">metal</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.285.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.286.1">hotwheels</span></span></p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.287.1">car</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.288.1">3</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.289.1">1</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.290.1">35</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.291.1">white</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.292.1">4</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.293.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.294.1">hard</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.295.1">metal</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.296.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.297.1">hotwheels</span></span></p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.298.1">stuffed</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.299.1">5</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.300.1">5</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.301.1">50</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.302.1">white</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.303.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.304.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.305.1">verysoft</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.306.1">fur</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.307.1">2</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.308.1">plush</span></span></p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.309.1">stuffed</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.310.1">7</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.311.1">5</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.312.1">55</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.313.1">brown</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.314.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.315.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.316.1">verysoft</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.317.1">fur</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.318.1">3</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.319.1">plush</span></span></p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.320.1">action</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.321.1">2</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.322.1">4</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.323.1">80</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.324.1">gray</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.325.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.326.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.327.1">hard</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.328.1">metal</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.329.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.330.1">slinky</span></span></p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.331.1">build</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.332.1">2</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.333.1">2</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.334.1">125</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.335.1">wood</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.336.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.337.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.338.1">hard</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.339.1">wood</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.340.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.341.1">wood </span><span class="No-Break"><span class="koboSpan" id="kobo.342.1">block 2x2</span></span></p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.343.1">build</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.344.1">2</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.345.1">2</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.346.1">75</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.347.1">wood</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.348.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.349.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.350.1">hard</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.351.1">wood</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.352.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.353.1">wood </span><span class="No-Break"><span class="koboSpan" id="kobo.354.1">block triangle</span></span></p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.355.1">build</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.356.1">4</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.357.1">2</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.358.1">250</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.359.1">wood</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.360.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.361.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.362.1">hard</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.363.1">wood</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.364.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.365.1">wood </span><span class="No-Break"><span class="koboSpan" id="kobo.366.1">block 4x2</span></span></p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.367.1">dish</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.368.1">3</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.369.1">3</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.370.1">79</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.371.1">blue</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.372.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.373.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.374.1">hard</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.375.1">ceramic</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.376.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.377.1">teapot</span></span></p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.378.1">aircraft</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.379.1">7</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.380.1">5</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.381.1">65</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.382.1">white</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.383.1">4</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.384.1">1</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.385.1">hard</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.386.1">plastic</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.387.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.388.1">space shuttle</span></span></p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.389.1">aircraft</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.390.1">13</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.391.1">7</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.392.1">500</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.393.1">green</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.394.1">8</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.395.1">1</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.396.1">hard</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.397.1">plastic</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.398.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.399.1">Thunderbird 2</span></span></p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.400.1">car</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.401.1">5</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.402.1">1</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.403.1">333</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.404.1">yellow</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.405.1">6</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.406.1">1</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.407.1">hard</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.408.1">metal</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.409.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.410.1">school bus</span></span></p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.411.1">music</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.412.1">12</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.413.1">4</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.414.1">130</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.415.1">wood</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.416.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.417.1">2</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.418.1">hard</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.419.1">wood</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.420.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.421.1">toy guitar</span></span></p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.422.1">music</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.423.1">5</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.424.1">2</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.425.1">100</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.426.1">yellow</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.427.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.428.1">1</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.429.1">hard</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.430.1">plastic</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.431.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.432.1">play </span></p>
<p><span class="No-Break"><span class="koboSpan" id="kobo.433.1">microphone</span></span></p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.434.1">music</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.435.1">4</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.436.1">4</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.437.1">189</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.438.1">white</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.439.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.440.1">2</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.441.1">hard</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.442.1">wood</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.443.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.444.1">toy drum</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.445.1">Table 8.1 – A table of attributes for a group of toys used for classification</span></p>
<p><span class="koboSpan" id="kobo.446.1">We now have </span><a id="_idIndexMarker588"/><span class="koboSpan" id="kobo.447.1">a problem we have to solve. </span><span class="koboSpan" id="kobo.447.2">We will be </span><a id="_idIndexMarker589"/><span class="koboSpan" id="kobo.448.1">using a decision tree classifier that is provided with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.449.1">scikit-learn</span></strong><span class="koboSpan" id="kobo.450.1"> Python package called </span><strong class="source-inline"><span class="koboSpan" id="kobo.451.1">DecisionTreeClassifier</span></strong><span class="koboSpan" id="kobo.452.1">. </span><span class="koboSpan" id="kobo.452.2">This program cannot use strings as input data. </span><span class="koboSpan" id="kobo.452.3">We will have to convert all of our string data into some sort of numeric figure. </span><span class="koboSpan" id="kobo.452.4">Fortunately, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.453.1">scikit-learn</span></strong><span class="koboSpan" id="kobo.454.1"> library provides us with a function just for this purpose. </span><span class="koboSpan" id="kobo.454.2">It provides several encoding functions that convert strings into numbers. </span><span class="koboSpan" id="kobo.454.3">The function we will use is called </span><strong class="source-inline"><span class="koboSpan" id="kobo.455.1">LabelEncoder</span></strong><span class="koboSpan" id="kobo.456.1">. </span><span class="koboSpan" id="kobo.456.2">This function takes an array of strings and converts it into an enumerated set </span><span class="No-Break"><span class="koboSpan" id="kobo.457.1">of integers.</span></span></p>
<p><span class="koboSpan" id="kobo.458.1">We can take our first column, which has the type of toy. </span><span class="koboSpan" id="kobo.458.2">My nomenclature is </span><em class="italic"><span class="koboSpan" id="kobo.459.1">toy = toy car</span></em><span class="koboSpan" id="kobo.460.1">, </span><em class="italic"><span class="koboSpan" id="kobo.461.1">stuffed = stuffed animal</span></em><span class="koboSpan" id="kobo.462.1">, </span><em class="italic"><span class="koboSpan" id="kobo.463.1">aircraft = toy aircraft</span></em><span class="koboSpan" id="kobo.464.1">, and </span><em class="italic"><span class="koboSpan" id="kobo.465.1">music = toy musical instrument</span></em><span class="koboSpan" id="kobo.466.1">. </span><span class="koboSpan" id="kobo.466.2">We also have </span><em class="italic"><span class="koboSpan" id="kobo.467.1">action</span></em><span class="koboSpan" id="kobo.468.1"> for </span><em class="italic"><span class="koboSpan" id="kobo.469.1">action toy</span></em><span class="koboSpan" id="kobo.470.1">, and </span><em class="italic"><span class="koboSpan" id="kobo.471.1">build</span></em><span class="koboSpan" id="kobo.472.1"> for </span><em class="italic"><span class="koboSpan" id="kobo.473.1">building toy</span></em><span class="koboSpan" id="kobo.474.1"> (that is, blocks, LEGO™, and so on). </span><span class="koboSpan" id="kobo.474.2">We’ll have to turn these into some sort </span><span class="No-Break"><span class="koboSpan" id="kobo.475.1">of numbers.</span></span></p>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.476.1">LabelEncoder</span></strong><span class="koboSpan" id="kobo.477.1"> will convert a column in our data table that is populated with strings. </span><span class="koboSpan" id="kobo.477.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.478.1">type</span></strong><span class="koboSpan" id="kobo.479.1"> column from the data is shown in the </span><span class="No-Break"><span class="koboSpan" id="kobo.480.1">following code:</span></span></p>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.481.1">['car' 'car' 'car' 'car' 'car' 'stuffed' 'stuffed' 'action' 'build' 'build' 'build' 'dish' 'aircraft' 'aircraft' 'car' 'music' '</span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.482.1">music' 'music']</span></strong></span></p>
<p><span class="koboSpan" id="kobo.483.1">It converts it to the label-encoded </span><span class="No-Break"><span class="koboSpan" id="kobo.484.1">toy type:</span></span></p>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.485.1">[3 3 3 3 3 6 6 0 2 2 2 4 1 1 3 5 </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.486.1">5 5]</span></strong></span></p>
<p><span class="koboSpan" id="kobo.487.1">You can see </span><a id="_idIndexMarker590"/><span class="koboSpan" id="kobo.488.1">that everywhere where it said </span><strong class="source-inline"><span class="koboSpan" id="kobo.489.1">car</span></strong><span class="koboSpan" id="kobo.490.1">, we now </span><a id="_idIndexMarker591"/><span class="koboSpan" id="kobo.491.1">have the number </span><strong class="source-inline"><span class="koboSpan" id="kobo.492.1">3</span></strong><span class="koboSpan" id="kobo.493.1">. </span><span class="koboSpan" id="kobo.493.2">You can also see that </span><strong class="source-inline"><span class="koboSpan" id="kobo.494.1">6</span></strong><span class="koboSpan" id="kobo.495.1"> = </span><strong class="source-inline"><span class="koboSpan" id="kobo.496.1">stuffed</span></strong><span class="koboSpan" id="kobo.497.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.498.1">0</span></strong><span class="koboSpan" id="kobo.499.1"> = </span><strong class="source-inline"><span class="koboSpan" id="kobo.500.1">action</span></strong><span class="koboSpan" id="kobo.501.1">, and so on. </span><span class="koboSpan" id="kobo.501.2">Why the odd numbering? </span><span class="koboSpan" id="kobo.501.3">The encoder first sorts the strings in </span><span class="No-Break"><span class="koboSpan" id="kobo.502.1">alphabetical order.</span></span></p>
<p><span class="koboSpan" id="kobo.503.1">We are going to just dive right in from here to create a </span><span class="No-Break"><span class="koboSpan" id="kobo.504.1">classification progr</span><a id="_idTextAnchor251"/><span class="koboSpan" id="kobo.505.1">am:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.506.1">Here is our decision tree </span><span class="No-Break"><span class="koboSpan" id="kobo.507.1">classifier program:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.508.1">
# decision tree classifier
# author: Francis X Govers III #
# example from book "Artificial Intelligence for Robotics" #</span></pre></li> <li><span class="koboSpan" id="kobo.509.1">We first import the libraries we will be using. </span><span class="koboSpan" id="kobo.509.2">There is an extra library called </span><strong class="source-inline"><span class="koboSpan" id="kobo.510.1">graphviz</span></strong><span class="koboSpan" id="kobo.511.1"> that is useful for drawing pictures of decision trees. </span><span class="koboSpan" id="kobo.511.2">You can install it with </span><span class="No-Break"><span class="koboSpan" id="kobo.512.1">the following:</span></span><pre class="source-code">
<strong class="bold"><span class="koboSpan" id="kobo.513.1">pip install graphviz</span></strong></pre></li> <li><span class="koboSpan" id="kobo.514.1">We are going to be using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.515.1">pandas</span></strong><span class="koboSpan" id="kobo.516.1"> package, which provides a lot of data </span><span class="No-Break"><span class="koboSpan" id="kobo.517.1">table-handling tools:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.518.1">
from sklearn import tree
import numpy as np
import pandas as pd
import sklearn.preprocessing as preproc
import graphviz</span></pre></li> <li><span class="koboSpan" id="kobo.519.1">Our first step </span><a id="_idIndexMarker592"/><span class="koboSpan" id="kobo.520.1">is to read in our data. </span><span class="koboSpan" id="kobo.520.2">I created my table in Microsoft Excel and exported it as a </span><strong class="bold"><span class="koboSpan" id="kobo.521.1">comma-separated values</span></strong><span class="koboSpan" id="kobo.522.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.523.1">CSV</span></strong><span class="koboSpan" id="kobo.524.1">) format. </span><span class="koboSpan" id="kobo.524.2">This allows us to read in the data file directly with the column headers. </span><span class="koboSpan" id="kobo.524.3">I print out the shape and size of the data file for reference. </span><span class="koboSpan" id="kobo.524.4">My version of </span><a id="_idIndexMarker593"/><span class="koboSpan" id="kobo.525.1">the file has 18 rows and 11 columns. </span><span class="koboSpan" id="kobo.525.2">The last </span><a id="_idIndexMarker594"/><span class="koboSpan" id="kobo.526.1">column is just a note to myself on the actual name of each toy. </span><span class="koboSpan" id="kobo.526.2">We will not be using the last column for anything. </span><span class="koboSpan" id="kobo.526.3">We are building a classifier that will separate the toys </span><span class="No-Break"><span class="koboSpan" id="kobo.527.1">by type:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.528.1">
toyData = pd.read_csv("toy_classifier_tree.csv")
print ("Data length ",len(toyData))
print ("Data Shape ",toyData.shape)</span></pre></li> <li><span class="koboSpan" id="kobo.529.1">Now, we can start building our decision tree classifier. </span><span class="koboSpan" id="kobo.529.2">We first build an instantiation of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.530.1">DecisionTreeClassifer</span></strong><span class="koboSpan" id="kobo.531.1"> object. </span><span class="koboSpan" id="kobo.531.2">There are two different </span><a id="_idIndexMarker595"/><span class="koboSpan" id="kobo.532.1">types of </span><strong class="bold"><span class="koboSpan" id="kobo.533.1">decision tree classification</span></strong><span class="koboSpan" id="kobo.534.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.535.1">DTC</span></strong><span class="koboSpan" id="kobo.536.1">) algorithms to </span><span class="No-Break"><span class="koboSpan" id="kobo.537.1">choose from:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.538.1">Gini coefficient</span></strong><span class="koboSpan" id="kobo.539.1">: The Gini coefficient was developed in 1912 by the Italian statistician </span><a id="_idIndexMarker596"/><span class="koboSpan" id="kobo.540.1">Corrado Gini in </span><a id="_idIndexMarker597"/><span class="koboSpan" id="kobo.541.1">his paper, </span><em class="italic"><span class="koboSpan" id="kobo.542.1">Variabilita e Mutabilita</span></em><span class="koboSpan" id="kobo.543.1">. </span><span class="koboSpan" id="kobo.543.2">This coefficient, or index, measures the amount of inequality in a group of numbers. </span><span class="koboSpan" id="kobo.543.3">A zero value means all the members of the group are </span><span class="No-Break"><span class="koboSpan" id="kobo.544.1">the same.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.545.1">Entropy method</span></strong><span class="koboSpan" id="kobo.546.1">: Entropy, when we are talking about AI, refers to the amount </span><a id="_idIndexMarker598"/><span class="koboSpan" id="kobo.547.1">of uncertainty in </span><a id="_idIndexMarker599"/><span class="koboSpan" id="kobo.548.1">a set of data. </span><span class="koboSpan" id="kobo.548.2">This concept comes from information theory, in which it measures the amount of uncertainty in a random variable. </span><span class="koboSpan" id="kobo.548.3">The concept was introduced by Claude Shannon in the 1940s. </span><span class="koboSpan" id="kobo.548.4">To create a decision tree, the algorithm tries to decrease entropy (reduce uncertainty) by splitting the group at a point where each child node is more homogenous than </span><span class="No-Break"><span class="koboSpan" id="kobo.549.1">its parent.</span></span></li></ul></li>
</ol>
<p><span class="koboSpan" id="kobo.550.1">Here, we are going to use the Gini coefficient. </span><span class="koboSpan" id="kobo.550.2">If we had a group of toy cars that were all the same size and all red, then the Gini coefficient of the group would be 0. </span><span class="koboSpan" id="kobo.550.3">If the members of the group are all different, then the Gini coefficient is closer to 1. </span><span class="koboSpan" id="kobo.550.4">The Gini coefficient is given by the </span><span class="No-Break"><span class="koboSpan" id="kobo.551.1">following equation:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.552.1">G</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.553.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.554.1">S</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.555.1">)</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.556.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.557.1">1</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.558.1">−</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.559.1">∑</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.560.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.561.1">i</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.562.1">=</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.563.1">1</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.564.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.565.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.566.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.567.1">p</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.568.1"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.569.1">i</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.570.1"> </span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.571.1">2</span></span></span></p>
<p><span class="koboSpan" id="kobo.572.1">We have 4 toy cars out of 18 toys, so the probability of a toy car being in a group is </span><em class="italic"><span class="koboSpan" id="kobo.573.1">4/18</span></em><span class="koboSpan" id="kobo.574.1"> or 0.222. </span><span class="koboSpan" id="kobo.574.2">The decision tree will continue to subdivide classes until the Gini coefficient of the group </span><span class="No-Break"><span class="koboSpan" id="kobo.575.1">is 0:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.576.1">
dTree = tree.DecisionTreeClassifier(criter</span><a id="_idTextAnchor252"/><span class="koboSpan" id="kobo.577.1">ion ="gini")</span></pre> <ol>
<li value="6"><span class="koboSpan" id="kobo.578.1">We need to </span><a id="_idIndexMarker600"/><span class="koboSpan" id="kobo.579.1">separate out the values in our data table. </span><span class="koboSpan" id="kobo.579.2">The data in the first column, which is called column </span><strong class="source-inline"><span class="koboSpan" id="kobo.580.1">0</span></strong><span class="koboSpan" id="kobo.581.1"> in Python, are our classification labels. </span><span class="koboSpan" id="kobo.581.2">We need to pull those out separately, as they are used </span><a id="_idIndexMarker601"/><span class="koboSpan" id="kobo.582.1">to separate the toys into classes. </span><span class="koboSpan" id="kobo.582.2">From our previous work with neural networks, these would be our outputs or the label data we have used in other machine learning processes. </span><span class="koboSpan" id="kobo.582.3">We will be training our classifier to predict the class of the toy based on the attributes in the table (size, weight, color, and so on). </span><span class="koboSpan" id="kobo.582.4">We use slicing to pull the data out of the pandas table. </span><span class="koboSpan" id="kobo.582.5">Our pandas data table is called </span><strong class="source-inline"><span class="koboSpan" id="kobo.583.1">toyData</span></strong><span class="koboSpan" id="kobo.584.1">. </span><span class="koboSpan" id="kobo.584.2">If we want the entries in the table, we need to ask for </span><strong class="source-inline"><span class="koboSpan" id="kobo.585.1">toyData.values</span></strong><span class="koboSpan" id="kobo.586.1">, which will be returned as a </span><span class="No-Break"><span class="koboSpan" id="kobo.587.1">2D array:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.588.1">
dataValues=toyData.values[:,1:10]
classValues = toyData.values[:,0]</span></pre></li> </ol>
<p><span class="koboSpan" id="kobo.589.1">If you are not familiar with slicing notation in Python, the statement </span><strong class="source-inline"><span class="koboSpan" id="kobo.590.1">toyData.values[:,1:10]</span></strong><span class="koboSpan" id="kobo.591.1"> returns just the columns in our table from 1 to 10 – it leaves column 0 out. </span><span class="koboSpan" id="kobo.591.2">We actually have 11 columns in our table, but since Python starts numbering them at 0, we end up needing 1 to 10. </span><span class="koboSpan" id="kobo.591.3">You will probably guess that the other notation just grabs the data in the </span><span class="No-Break"><span class="koboSpan" id="kobo.592.1">first column.</span></span></p>
<ol>
<li value="7"><span class="koboSpan" id="kobo.593.1">This is the label encoder that we talked about – it will convert the strings in our data into numbers. </span><span class="koboSpan" id="kobo.593.2">For example, colors such as </span><em class="italic"><span class="koboSpan" id="kobo.594.1">red</span></em><span class="koboSpan" id="kobo.595.1">, </span><em class="italic"><span class="koboSpan" id="kobo.596.1">green</span></em><span class="koboSpan" id="kobo.597.1">, and </span><em class="italic"><span class="koboSpan" id="kobo.598.1">blue</span></em><span class="koboSpan" id="kobo.599.1"> will be converted to numbers such as </span><em class="italic"><span class="koboSpan" id="kobo.600.1">0</span></em><span class="koboSpan" id="kobo.601.1">, </span><em class="italic"><span class="koboSpan" id="kobo.602.1">1</span></em><span class="koboSpan" id="kobo.603.1">, and </span><em class="italic"><span class="koboSpan" id="kobo.604.1">2</span></em><span class="koboSpan" id="kobo.605.1">. </span><span class="koboSpan" id="kobo.605.2">The first item to be encoded is the list of class values that we use to label the data. </span><span class="koboSpan" id="kobo.605.3">We use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.606.1">LabelEncoder.fit()</span></strong><span class="koboSpan" id="kobo.607.1"> function to come up with the formula for converting strings to numbers, and then the </span><strong class="source-inline"><span class="koboSpan" id="kobo.608.1">LabelEncoder.transform()</span></strong><span class="koboSpan" id="kobo.609.1"> function to apply it. </span><span class="koboSpan" id="kobo.609.2">Note that </span><strong class="source-inline"><span class="koboSpan" id="kobo.610.1">fit()</span></strong><span class="koboSpan" id="kobo.611.1"> does not produce </span><span class="No-Break"><span class="koboSpan" id="kobo.612.1">an output.</span></span></li>
<li><span class="koboSpan" id="kobo.613.1">Finally, we need </span><a id="_idIndexMarker602"/><span class="koboSpan" id="kobo.614.1">to make the string text and </span><a id="_idIndexMarker603"/><span class="koboSpan" id="kobo.615.1">the list of encoded numbers match up. </span><span class="koboSpan" id="kobo.615.2">What </span><strong class="source-inline"><span class="koboSpan" id="kobo.616.1">LabelEncoder</span></strong><span class="koboSpan" id="kobo.617.1"> will do is sort the strings alphabetically and start numbering them from </span><em class="italic"><span class="koboSpan" id="kobo.618.1">A</span></em><span class="koboSpan" id="kobo.619.1">, ignoring any duplicates. </span><span class="koboSpan" id="kobo.619.2">If we put in </span><strong class="source-inline"><span class="koboSpan" id="kobo.620.1">car, car, car, block, stuffed, airplane</span></strong><span class="koboSpan" id="kobo.621.1">, we will get </span><strong class="source-inline"><span class="koboSpan" id="kobo.622.1">2,2,2,1,3,0</span></strong><span class="koboSpan" id="kobo.623.1"> as the encoding, and we will have to know that </span><strong class="source-inline"><span class="koboSpan" id="kobo.624.1">airplane</span></strong><span class="koboSpan" id="kobo.625.1"> = </span><strong class="source-inline"><span class="koboSpan" id="kobo.626.1">0</span></strong><span class="koboSpan" id="kobo.627.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.628.1">block</span></strong><span class="koboSpan" id="kobo.629.1"> = </span><strong class="source-inline"><span class="koboSpan" id="kobo.630.1">1</span></strong><span class="koboSpan" id="kobo.631.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.632.1">car</span></strong><span class="koboSpan" id="kobo.633.1"> = </span><strong class="source-inline"><span class="koboSpan" id="kobo.634.1">2</span></strong><span class="koboSpan" id="kobo.635.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.636.1">stuffed</span></strong><span class="koboSpan" id="kobo.637.1"> = </span><strong class="source-inline"><span class="koboSpan" id="kobo.638.1">3</span></strong><span class="koboSpan" id="kobo.639.1">. </span><span class="koboSpan" id="kobo.639.2">We need to </span><a id="_idIndexMarker604"/><span class="koboSpan" id="kobo.640.1">generate a </span><strong class="bold"><span class="koboSpan" id="kobo.641.1">decoder ring</span></strong><span class="koboSpan" id="kobo.642.1"> to match up the numbers and text descriptions that look like </span><strong class="source-inline"><span class="koboSpan" id="kobo.643.1">airplane, block, car, stuffed</span></strong><span class="koboSpan" id="kobo.644.1">. </span><span class="koboSpan" id="kobo.644.2">We duplicate the </span><strong class="source-inline"><span class="koboSpan" id="kobo.645.1">LabelEncoder</span></strong><span class="koboSpan" id="kobo.646.1"> function by using two functions on our list of string-formatted </span><span class="No-Break"><span class="koboSpan" id="kobo.647.1">class names:</span></span><ul><li><span class="koboSpan" id="kobo.648.1">We use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.649.1">set()</span></strong><span class="koboSpan" id="kobo.650.1"> function to </span><span class="No-Break"><span class="koboSpan" id="kobo.651.1">eliminate duplicates</span></span></li><li><span class="koboSpan" id="kobo.652.1">We use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.653.1">sorted()</span></strong><span class="koboSpan" id="kobo.654.1"> function to sort in the </span><span class="No-Break"><span class="koboSpan" id="kobo.655.1">correct order</span></span></li></ul></li>
</ol>
<p><span class="koboSpan" id="kobo.656.1">Now, our class name table and the enumerations generated by </span><strong class="source-inline"><span class="koboSpan" id="kobo.657.1">LabelEncoder</span></strong><span class="koboSpan" id="kobo.658.1"> match. </span><span class="koboSpan" id="kobo.658.2">We’ll need </span><span class="No-Break"><span class="koboSpan" id="kobo.659.1">this later:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.660.1">
lencoder = preproc.LabelEncoder() lencoder.fit(classValues)
classes = lencoder.transform(classValues)
classValues = list(sorted(se</span><a id="_idTextAnchor253"/><span class="koboSpan" id="kobo.661.1">t(classValues)))</span></pre> <ol>
<li value="9"><span class="koboSpan" id="kobo.662.1">To make it easy on ourselves, I created a function to automatically find out which columns in our data are composed of strings and to convert those columns into numbers. </span><span class="koboSpan" id="kobo.662.2">We start by building an empty list to hold our data. </span><span class="koboSpan" id="kobo.662.3">We will iterate through the columns in our data and look to see whether the first data value is a string. </span><span class="koboSpan" id="kobo.662.4">If it is, we will convert that whole column into numbers using the </span><a id="_idIndexMarker605"/><span class="koboSpan" id="kobo.663.1">label encoder object (</span><strong class="source-inline"><span class="koboSpan" id="kobo.664.1">lencoder</span></strong><span class="koboSpan" id="kobo.665.1">) we created. </span><span class="koboSpan" id="kobo.665.2">The label encoding process has two parts. </span><span class="koboSpan" id="kobo.665.3">We call </span><strong class="source-inline"><span class="koboSpan" id="kobo.666.1">lencoder.fit()</span></strong><span class="koboSpan" id="kobo.667.1"> to see how many unique strings we have in our column and to create </span><a id="_idIndexMarker606"/><span class="koboSpan" id="kobo.668.1">a number for each one. </span><span class="koboSpan" id="kobo.668.2">Then, we use </span><strong class="source-inline"><span class="koboSpan" id="kobo.669.1">lencoder.transpose</span></strong><span class="koboSpan" id="kobo.670.1"> to insert those numbers into </span><span class="No-Break"><span class="koboSpan" id="kobo.671.1">a list:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.672.1">
newData = []
for ii in range(len(dataValues[0]))
line = dataValues[:,ii]
if type(line[0])==str:
     lencoder.fit(line)
  line = lencoder.transform(line)</span></pre></li> <li><span class="koboSpan" id="kobo.673.1">Now, we put all of the data back into the </span><strong class="source-inline"><span class="koboSpan" id="kobo.674.1">newData</span></strong><span class="koboSpan" id="kobo.675.1"> list, but there is a problem – we have turned all our columns into rows! </span><span class="koboSpan" id="kobo.675.2">We use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.676.1">transpose</span></strong><span class="koboSpan" id="kobo.677.1"> function from </span><strong class="source-inline"><span class="koboSpan" id="kobo.678.1">numpy</span></strong><span class="koboSpan" id="kobo.679.1"> to correct this problem. </span><span class="koboSpan" id="kobo.679.2">But wait! </span><span class="koboSpan" id="kobo.679.3">We don’t have an array anymore, as we turned it into a list so we could take it apart and put it back together again (you can’t do that with a </span><strong class="source-inline"><span class="koboSpan" id="kobo.680.1">numpy</span></strong><span class="koboSpan" id="kobo.681.1"> array – believe me, </span><span class="No-Break"><span class="koboSpan" id="kobo.682.1">I tried):</span></span><pre class="source-code"><span class="koboSpan" id="kobo.683.1">
newData.append(line)
newDataArray = np.asarray(newData)
newDataArray = np.transpose(newDataArray)</span></pre></li> <li><span class="koboSpan" id="kobo.684.1">Now, all of our preprocessing is done, so we can finally call the real </span><strong class="source-inline"><span class="koboSpan" id="kobo.685.1">DecisionTreeClassifer</span></strong><span class="koboSpan" id="kobo.686.1">. </span><span class="koboSpan" id="kobo.686.2">It takes </span><span class="No-Break"><span class="koboSpan" id="kobo.687.1">two arguments:</span></span><ul><li><span class="koboSpan" id="kobo.688.1">The array of our </span><span class="No-Break"><span class="koboSpan" id="kobo.689.1">data values</span></span></li><li><span class="koboSpan" id="kobo.690.1">The array of class types that we want the decision tree to divide our </span><span class="No-Break"><span class="koboSpan" id="kobo.691.1">groups into</span></span></li></ul></li>
</ol>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.692.1">DecisionTreeClassifier</span></strong><span class="koboSpan" id="kobo.693.1"> will determine what specific data from the table is useful for predicting what class one of our toys </span><span class="No-Break"><span class="koboSpan" id="kobo.694.1">fits into:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.695.1">
dTree = dTree.fit(newDataArray,classes)</span></pre> <p><span class="koboSpan" id="kobo.696.1">That’s it – one line. </span><span class="koboSpan" id="kobo.696.2">But wait – we want to see the results. </span><span class="koboSpan" id="kobo.696.3">If we just try and print out the decision tree, we get </span><span class="No-Break"><span class="koboSpan" id="kobo.697.1">the following:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.698.1">
DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None, max_features=None, max_leaf_nodes=None,
min_impurity_split=1e-07, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=None, splitter='best')</span></pre> <p><span class="koboSpan" id="kobo.699.1">That does </span><a id="_idIndexMarker607"/><span class="koboSpan" id="kobo.700.1">not tell us anything; that is a description </span><a id="_idIndexMarker608"/><span class="koboSpan" id="kobo.701.1">of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.702.1">DecisionTreeClassifier</span></strong><span class="koboSpan" id="kobo.703.1"> object (it does show us all of the parameters we can set, which is why I put </span><span class="No-Break"><span class="koboSpan" id="kobo.704.1">it here).</span></span></p>
<ol>
<li value="12"><span class="koboSpan" id="kobo.705.1">So, we use a package called </span><strong class="source-inline"><span class="koboSpan" id="kobo.706.1">graphviz</span></strong><span class="koboSpan" id="kobo.707.1">, which is very good at printing decision trees. </span><span class="koboSpan" id="kobo.707.2">We can even pass our column names and class names into the graph. </span><span class="koboSpan" id="kobo.707.3">The final two lines output the graph as a </span><strong class="source-inline"><span class="koboSpan" id="kobo.708.1">.pdf</span></strong><span class="koboSpan" id="kobo.709.1"> file and store it on the </span><span class="No-Break"><span class="koboSpan" id="kobo.710.1">hard drive:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.711.1">
c_data=tree.export_graphviz(dTree,out_file=None,feature_names=toyData.colum ns, class_names=classValues, filled = True, rounded=True,special_characters=True)
graph = graphviz.Source(c_data)
graph.render("toy_graph_gini")</span></pre></li> </ol>
<p><span class="koboSpan" id="kobo.712.1">And here is the result. </span><span class="koboSpan" id="kobo.712.2">I will warn you, this </span><span class="No-Break"><span class="koboSpan" id="kobo.713.1">is addictive:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer085">
<span class="koboSpan" id="kobo.714.1"><img alt="Figure 8.3 – The output of the decision tree using t﻿he Gini index method" src="image/B19846_08_3.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.715.1">Figure 8.3 – The output of the decision tree using t</span><a id="_idTextAnchor254"/><span class="koboSpan" id="kobo.716.1">he Gini index method</span></p>
<p><span class="koboSpan" id="kobo.717.1">We can </span><a id="_idIndexMarker609"/><span class="koboSpan" id="kobo.718.1">quickly check our solution by looking at </span><a id="_idIndexMarker610"/><span class="koboSpan" id="kobo.719.1">our input table and seeing whether the numbers line up. </span><span class="koboSpan" id="kobo.719.2">We should see </span><span class="No-Break"><span class="koboSpan" id="kobo.720.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.721.1">Five </span><span class="No-Break"><span class="koboSpan" id="kobo.722.1">toy cars</span></span></li>
<li><span class="koboSpan" id="kobo.723.1">Three </span><span class="No-Break"><span class="koboSpan" id="kobo.724.1">building blocks</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.725.1">One dish</span></span></li>
<li><span class="koboSpan" id="kobo.726.1">One </span><span class="No-Break"><span class="koboSpan" id="kobo.727.1">action toy</span></span></li>
<li><span class="koboSpan" id="kobo.728.1">Two </span><span class="No-Break"><span class="koboSpan" id="kobo.729.1">stuffed animals</span></span></li>
<li><span class="koboSpan" id="kobo.730.1">Three </span><span class="No-Break"><span class="koboSpan" id="kobo.731.1">musical instruments</span></span></li>
<li><span class="koboSpan" id="kobo.732.1">Two </span><span class="No-Break"><span class="koboSpan" id="kobo.733.1">toy airplanes</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.734.1">And that is indeed </span><span class="No-Break"><span class="koboSpan" id="kobo.735.1">the case.</span></span></p>
<p><span class="koboSpan" id="kobo.736.1">The other number to look at is the Gini index. </span><span class="koboSpan" id="kobo.736.2">As shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.737.1">Figure 8</span></em></span><em class="italic"><span class="koboSpan" id="kobo.738.1">.3</span></em><span class="koboSpan" id="kobo.739.1">, the top-level box shows that the index for the entire group has an overall value of </span><strong class="source-inline"><span class="koboSpan" id="kobo.740.1">0.8166</span></strong><span class="koboSpan" id="kobo.741.1">, which is close to 1 and shows a high degree of heterogeneity. </span><span class="koboSpan" id="kobo.741.2">As we progress down the tree, the Gini numbers get smaller and smaller until reaching </span><strong class="source-inline"><span class="koboSpan" id="kobo.742.1">0</span></strong><span class="koboSpan" id="kobo.743.1"> at each of the identified groups, which shows that the items in those groups share all of the </span><span class="No-Break"><span class="koboSpan" id="kobo.744.1">same attributes.</span></span></p>
<p><span class="koboSpan" id="kobo.745.1">What does this graph tell us? </span><span class="koboSpan" id="kobo.745.2">First of all, we can separate the toy cars by only one attribute – </span><em class="italic"><span class="koboSpan" id="kobo.746.1">width</span></em><span class="koboSpan" id="kobo.747.1">. </span><span class="koboSpan" id="kobo.747.2">Only the toy cars are less than 1.5 inches wide (38 mm). </span><span class="koboSpan" id="kobo.747.3">We don’t need to look at color, weight, or anything other than width to separate all the toy cars from everything else. </span><span class="koboSpan" id="kobo.747.4">We see we have 5 toy cars out of our 18 toys, so we have 13 left to classify. </span><span class="koboSpan" id="kobo.747.5">Our next division comes in length. </span><span class="koboSpan" id="kobo.747.6">We have 7 toys less than 4.5 inches long (11 cm) and 5 that are longer. </span><span class="koboSpan" id="kobo.747.7">Of the group of five, two have eyes and three do not. </span><span class="koboSpan" id="kobo.747.8">The toys with eyes </span><a id="_idIndexMarker611"/><span class="koboSpan" id="kobo.748.1">are the two stuffed animals. </span><span class="koboSpan" id="kobo.748.2">If you follow </span><a id="_idIndexMarker612"/><span class="koboSpan" id="kobo.749.1">the tree, the branches that lead to the toy music instruments are width &gt; 1.5 inches, length &gt; 4.5 inches, and no eyes, and they are indeed larger than the other toys in length and width, and don’t </span><span class="No-Break"><span class="koboSpan" id="kobo.750.1">have eyes.</span></span></p>
<p><span class="koboSpan" id="kobo.751.1">None of the other bits matter in terms of classifying. </span><span class="koboSpan" id="kobo.751.2">That means that an attribute such as </span><em class="italic"><span class="koboSpan" id="kobo.752.1">color</span></em><span class="koboSpan" id="kobo.753.1"> is a poor predictor of what class a toy belongs to – which makes sense. </span><span class="koboSpan" id="kobo.753.2">Our other useful criteria are the </span><em class="italic"><span class="koboSpan" id="kobo.754.1">number of wheels</span></em><span class="koboSpan" id="kobo.755.1">, the </span><em class="italic"><span class="koboSpan" id="kobo.756.1">weight</span></em><span class="koboSpan" id="kobo.757.1">, and the </span><em class="italic"><span class="koboSpan" id="kobo.758.1">length</span></em><span class="koboSpan" id="kobo.759.1">. </span><span class="koboSpan" id="kobo.759.2">That data is sufficient to classify all our toys into groups. </span><span class="koboSpan" id="kobo.759.3">You can see that the Gini index of each leaf node is indeed </span><strong class="source-inline"><span class="koboSpan" id="kobo.760.1">0</span></strong><span class="koboSpan" id="kobo.761.1">. </span><span class="koboSpan" id="kobo.761.2">I added some additional labeling to the graph to make the illustration clearer, as the program uses the class number rather than the class name in </span><span class="No-Break"><span class="koboSpan" id="kobo.762.1">the graph.</span></span></p>
<p><span class="koboSpan" id="kobo.763.1">So, that exercise was satisfactory – we were able to create an automatic decision tree from our toy data that classified our toys. </span><span class="koboSpan" id="kobo.763.2">We can even use that data to classify a new toy and predict which class it might belong to. </span><span class="koboSpan" id="kobo.763.3">If we found that that new toy violated the classification somehow, then we would need to re-rerun the classification process and make a new </span><span class="No-Break"><span class="koboSpan" id="kobo.764.1">decision table.</span></span></p>
<p><span class="koboSpan" id="kobo.765.1">There is </span><a id="_idIndexMarker613"/><span class="koboSpan" id="kobo.766.1">another type of process for creating decision trees and subdividing </span><a id="_idIndexMarker614"/><span class="koboSpan" id="kobo.767.1">data into categories. </span><span class="koboSpan" id="kobo.767.2">That is called the </span><strong class="bold"><span class="koboSpan" id="kobo.768.1">entropy model</span></strong><span class="koboSpan" id="kobo.769.1">, or </span><strong class="bold"><span class="koboSpan" id="kobo.770.1">information gain</span></strong><span class="koboSpan" id="kobo.771.1">.</span><a id="_idTextAnchor255"/><a id="_idTextAnchor256"/><a id="_idTextAnchor257"/><span class="koboSpan" id="kobo.772.1"> Let’s discuss </span><span class="No-Break"><span class="koboSpan" id="kobo.773.1">this next.</span></span></p>
<h2 id="_idParaDest-135"><a id="_idTextAnchor258"/><span class="koboSpan" id="kobo.774.1">Understanding entropy</span></h2>
<p><strong class="bold"><span class="koboSpan" id="kobo.775.1">Entropy</span></strong><span class="koboSpan" id="kobo.776.1"> is a measurement </span><a id="_idIndexMarker615"/><span class="koboSpan" id="kobo.777.1">of the amount of disorder in the sample of data provided. </span><span class="koboSpan" id="kobo.777.2">We can also call this process </span><strong class="bold"><span class="koboSpan" id="kobo.778.1">information gain</span></strong><span class="koboSpan" id="kobo.779.1"> since we are measuring how much each criterion contributed to our knowledge of which class it </span><span class="No-Break"><span class="koboSpan" id="kobo.780.1">belongs to.</span></span></p>
<p><span class="koboSpan" id="kobo.781.1">The formula </span><a id="_idIndexMarker616"/><span class="koboSpan" id="kobo.782.1">for entropy is a negative log base 2 function that is still primarily looking at the probability of a class belonging to a population, which is just the number of individuals belonging to each class divided by the total number in </span><span class="No-Break"><span class="koboSpan" id="kobo.783.1">the sample:</span></span></p>
<p><em class="italic"><span class="koboSpan" id="kobo.784.1">Entropy = -p*log2(p) – </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.785.1">p_i*log2(p_i)</span></em></span></p>
<p><span class="koboSpan" id="kobo.786.1">To substitute entropy as our group criteria in our program, we only have to change </span><span class="No-Break"><span class="koboSpan" id="kobo.787.1">one line:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.788.1">
dTree = tree.DecisionTreeClassifier(criterion ="entropy")</span></pre> <p><span class="koboSpan" id="kobo.789.1">The results are shown in the </span><span class="No-Break"><span class="koboSpan" id="kobo.790.1">following diagram:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer086">
<span class="koboSpan" id="kobo.791.1"><img alt="Figure 8.4 – Output of the decision tree using ﻿entropy (information gain)" src="image/B19846_08_4.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.792.1">Figure 8.4 – Output of the decision tree using </span><a id="_idTextAnchor259"/><span class="koboSpan" id="kobo.793.1">entropy (information gain)</span></p>
<p><span class="koboSpan" id="kobo.794.1">You can note that entropy starts at 2.55 for our whole group, and decreases to 0 at the leaf nodes (ends of the branches). </span><span class="koboSpan" id="kobo.794.2">We can check that we have seven classifications, but you can see that the entropy method selected different criteria from the Gini method. </span><span class="koboSpan" id="kobo.794.3">For example, the Gini classifier started with </span><strong class="source-inline"><span class="koboSpan" id="kobo.795.1">Length</span></strong><span class="koboSpan" id="kobo.796.1">, and the entropy classifier started with </span><strong class="source-inline"><span class="koboSpan" id="kobo.797.1">Material</span></strong><span class="koboSpan" id="kobo.798.1">. </span><span class="koboSpan" id="kobo.798.2">The entropy method also chose </span><strong class="source-inline"><span class="koboSpan" id="kobo.799.1">Noise</span></strong><span class="koboSpan" id="kobo.800.1"> (whether the toy makes a noise or not) and correctly selected that the only toys that make a noise were the toy musical instruments and the toy airplanes, which have electronic sound boxes that make </span><span class="No-Break"><span class="koboSpan" id="kobo.801.1">airplane sounds.</span></span></p>
<p><span class="koboSpan" id="kobo.802.1">There is one item </span><a id="_idIndexMarker617"/><span class="koboSpan" id="kobo.803.1">that causes some concern, however. </span><span class="koboSpan" id="kobo.803.2">There are two blocks that show </span><strong class="source-inline"><span class="koboSpan" id="kobo.804.1">Material</span></strong><span class="koboSpan" id="kobo.805.1">, dividing the toy’s values in material less than 2.5. </span><strong class="source-inline"><span class="koboSpan" id="kobo.806.1">Material</span></strong><span class="koboSpan" id="kobo.807.1"> is a discrete value. </span><span class="koboSpan" id="kobo.807.2">We can generate a list of materials and run this through our </span><strong class="source-inline"><span class="koboSpan" id="kobo.808.1">sorted(set(list))</span></strong><span class="koboSpan" id="kobo.809.1"> process to get the unique values in </span><span class="No-Break"><span class="koboSpan" id="kobo.810.1">sorted order:</span></span></p>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.811.1">['ceramic', 'fur', 'metal', '</span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.812.1">plastic', 'wood']</span></strong></span></p>
<p><span class="koboSpan" id="kobo.813.1">So, a </span><strong class="source-inline"><span class="koboSpan" id="kobo.814.1">Material</span></strong><span class="koboSpan" id="kobo.815.1"> value of 2.5 or less would be either ceramic or fur. </span><span class="koboSpan" id="kobo.815.2">Fur and ceramic have nothing </span><a id="_idIndexMarker618"/><span class="koboSpan" id="kobo.816.1">in common, other than where they are found in the alphabet. </span><span class="koboSpan" id="kobo.816.2">This is a rather troubling relationship, which is an artifact of how we encoded our data as a sequential set of numbers. </span><span class="koboSpan" id="kobo.816.3">This implies relationships and grouping that don’t really exist. </span><span class="koboSpan" id="kobo.816.4">How can we </span><span class="No-Break"><span class="koboSpan" id="kobo.817.1">correct this?</span></span></p>
<p><span class="koboSpan" id="kobo.818.1">As a matter of fact, there is a process for handling just this sort of problem. </span><span class="koboSpan" id="kobo.818.2">This technique is widely used in AI programs and is a </span><em class="italic"><span class="koboSpan" id="kobo.819.1">must-have</span></em><span class="koboSpan" id="kobo.820.1"> tool for working with classification, either here in the decision tree section or with neural networks. </span><span class="koboSpan" id="kobo.820.2">This tool has the strange </span><a id="_idIndexMarker619"/><span class="koboSpan" id="kobo.821.1">name of </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.822.1">one-hot encoding</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.823.1">.</span></span></p>
<h2 id="_idParaDest-136"><a id="_idTextAnchor260"/><span class="koboSpan" id="kobo.824.1">Implementing one-hot encoding</span></h2>
<p><span class="koboSpan" id="kobo.825.1">The concept for one-hot encoding is pretty simple. </span><span class="koboSpan" id="kobo.825.2">Instead of replacing a category with an enumeration, we add </span><a id="_idIndexMarker620"/><span class="koboSpan" id="kobo.826.1">one column to our data for each </span><a id="_idIndexMarker621"/><span class="koboSpan" id="kobo.827.1">possible value and set it to be a </span><strong class="source-inline"><span class="koboSpan" id="kobo.828.1">1</span></strong><span class="koboSpan" id="kobo.829.1"> or </span><strong class="source-inline"><span class="koboSpan" id="kobo.830.1">0</span></strong><span class="koboSpan" id="kobo.831.1"> based on that value. </span><span class="koboSpan" id="kobo.831.2">The name comes from the fact that only one column</span><a id="_idTextAnchor261"/><span class="koboSpan" id="kobo.832.1"> in the set is </span><em class="italic"><span class="koboSpan" id="kobo.833.1">hot</span></em> <span class="No-Break"><span class="koboSpan" id="kobo.834.1">or selected.</span></span></p>
<p><span class="koboSpan" id="kobo.835.1">We can apply this principle to our example. </span><span class="koboSpan" id="kobo.835.2">We can replace the one column, </span><strong class="source-inline"><span class="koboSpan" id="kobo.836.1">Material</span></strong><span class="koboSpan" id="kobo.837.1">, with five columns for each material type in our database: </span><strong class="source-inline"><span class="koboSpan" id="kobo.838.1">ceramic</span></strong><span class="koboSpan" id="kobo.839.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.840.1">fur</span></strong><span class="koboSpan" id="kobo.841.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.842.1">metal</span></strong><span class="koboSpan" id="kobo.843.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.844.1">plastic</span></strong><span class="koboSpan" id="kobo.845.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.846.1">and </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.847.1">wood</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.848.1">:</span></span></p>
<table class="T---Table _idGenTablePara-1" id="table002">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<thead>
<tr class="T---Table">
<td class="T---Table T---Body T---Header">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.849.1">Material</span></strong></span></p>
</td>
<td class="T---Table T---Body T---Header">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.850.1">ceramic</span></strong></span></p>
</td>
<td class="T---Table T---Body T---Header">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.851.1">fur</span></strong></span></p>
</td>
<td class="T---Table T---Body T---Header">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.852.1">metal</span></strong></span></p>
</td>
<td class="T---Table T---Body T---Header">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.853.1">plastic</span></strong></span></p>
</td>
<td class="T---Table T---Body T---Header">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.854.1">wood</span></strong></span></p>
</td>
</tr>
</thead>
<tbody>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.855.1">metal</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.856.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.857.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.858.1">1</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.859.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.860.1">0</span></p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.861.1">metal</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.862.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.863.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.864.1">1</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.865.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.866.1">0</span></p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.867.1">metal</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.868.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.869.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.870.1">1</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.871.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.872.1">0</span></p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.873.1">metal</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.874.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.875.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.876.1">1</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.877.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.878.1">0</span></p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.879.1">metal</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.880.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.881.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.882.1">1</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.883.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.884.1">0</span></p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.885.1">fur</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.886.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.887.1">1</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.888.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.889.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.890.1">0</span></p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.891.1">fur</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.892.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.893.1">1</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.894.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.895.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.896.1">0</span></p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.897.1">metal</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.898.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.899.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.900.1">1</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.901.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.902.1">0</span></p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.903.1">wood</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.904.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.905.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.906.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.907.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.908.1">1</span></p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.909.1">wood</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.910.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.911.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.912.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.913.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.914.1">1</span></p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.915.1">wood</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.916.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.917.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.918.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.919.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.920.1">1</span></p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.921.1">ceramic</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.922.1">1</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.923.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.924.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.925.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.926.1">0</span></p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.927.1">plastic</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.928.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.929.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.930.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.931.1">1</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.932.1">0</span></p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.933.1">plastic</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.934.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.935.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.936.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.937.1">1</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.938.1">0</span></p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.939.1">metal</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.940.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.941.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.942.1">1</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.943.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.944.1">0</span></p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.945.1">wood</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.946.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.947.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.948.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.949.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.950.1">1</span></p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.951.1">plastic</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.952.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.953.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.954.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.955.1">1</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.956.1">0</span></p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><span class="koboSpan" id="kobo.957.1">wood</span></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.958.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.959.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.960.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.961.1">0</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="koboSpan" id="kobo.962.1">1</span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.963.1">Table 8.2 – One-hot encoding data structure for the Material category</span></p>
<p><span class="koboSpan" id="kobo.964.1">This does cause some structural complications to our program. </span><span class="koboSpan" id="kobo.964.2">We must insert columns for each of our types, which replaces 3 columns with 14 </span><span class="No-Break"><span class="koboSpan" id="kobo.965.1">new columns.</span></span></p>
<p><span class="koboSpan" id="kobo.966.1">I’ve found </span><a id="_idIndexMarker622"/><span class="koboSpan" id="kobo.967.1">two functions that we can use to convert </span><a id="_idIndexMarker623"/><span class="koboSpan" id="kobo.968.1">text categories into one-hot encoded </span><span class="No-Break"><span class="koboSpan" id="kobo.969.1">multiple columns:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.970.1">One is </span><strong class="source-inline"><span class="koboSpan" id="kobo.971.1">OneHotEncoder</span></strong><span class="koboSpan" id="kobo.972.1">, which is part of </span><strong class="source-inline"><span class="koboSpan" id="kobo.973.1">scikit-learn</span></strong><span class="koboSpan" id="kobo.974.1">. </span><span class="koboSpan" id="kobo.974.2">It is used like </span><strong class="source-inline"><span class="koboSpan" id="kobo.975.1">LabelEncoder</span></strong><span class="koboSpan" id="kobo.976.1"> – in fact, you must use both functions at the same time. </span><span class="koboSpan" id="kobo.976.2">You have to convert the string data to numeric form with </span><strong class="source-inline"><span class="koboSpan" id="kobo.977.1">LabelEncoder</span></strong><span class="koboSpan" id="kobo.978.1"> and then apply </span><strong class="source-inline"><span class="koboSpan" id="kobo.979.1">OneHotEncoder</span></strong><span class="koboSpan" id="kobo.980.1"> to convert that to the one-bit-per-value form that </span><span class="No-Break"><span class="koboSpan" id="kobo.981.1">we want.</span></span></li>
<li><span class="koboSpan" id="kobo.982.1">The simpler way is with a pandas function called </span><strong class="source-inline"><span class="koboSpan" id="kobo.983.1">get_dummies()</span></strong><span class="koboSpan" id="kobo.984.1">. </span><span class="koboSpan" id="kobo.984.2">The name is apparently because we are creating dummy values to replace a string with numbers. </span><span class="koboSpan" id="kobo.984.3">It does perform the same function. </span><span class="koboSpan" id="kobo.984.4">The steps involved are quite a bit simpler than using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.985.1">OneHotEncoder</span></strong><span class="koboSpan" id="kobo.986.1"> process, so that will be the one in </span><span class="No-Break"><span class="koboSpan" id="kobo.987.1">our example.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.988.1">Let’s look at the steps we need to follow to </span><span class="No-Break"><span class="koboSpan" id="kobo.989.1">implement this:</span></span></p>
<ol>
<li value="1"><span class="koboSpan" id="kobo.990.1">The top header section is the same as before – we have the </span><span class="No-Break"><span class="koboSpan" id="kobo.991.1">same imports:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.992.1">
# decision tree classifier
# with One Hot Encoding and Gini criteria #
# Author: Francis X Govers III #
# Example from book "Artificial Intelligence for Robotics" #
from sklearn import tree
import numpy as np
import pandas as pd
import sklearn.preprocessing as preproc
import graphviz</span></pre></li> <li><span class="koboSpan" id="kobo.993.1">We will begin by reading in our table as before. </span><span class="koboSpan" id="kobo.993.2">I added an extra column at my end called </span><strong class="source-inline"><span class="koboSpan" id="kobo.994.1">Toy Name</span></strong><span class="koboSpan" id="kobo.995.1"> so I could keep track of which toy is which. </span><span class="koboSpan" id="kobo.995.2">We don’t need this column for the decision tree, so we can take it out with the pandas </span><strong class="source-inline"><span class="koboSpan" id="kobo.996.1">del</span></strong><span class="koboSpan" id="kobo.997.1"> function by specifying the name of the column </span><span class="No-Break"><span class="koboSpan" id="kobo.998.1">to remove:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.999.1">
toyData = pd.read_csv("toy_classifier_tree.csv")
del toyData["Toy Name"]   # we don't need this for now</span></pre></li> <li><span class="koboSpan" id="kobo.1000.1">Now, we are going to create a list of the columns we are going to remove and replace from the pandas </span><strong class="source-inline"><span class="koboSpan" id="kobo.1001.1">dataTable</span></strong><span class="koboSpan" id="kobo.1002.1">. </span><span class="koboSpan" id="kobo.1002.2">These are the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1003.1">Color</span></strong><span class="koboSpan" id="kobo.1004.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1005.1">Soft</span></strong><span class="koboSpan" id="kobo.1006.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1007.1">Material</span></strong><span class="koboSpan" id="kobo.1008.1"> columns. </span><span class="koboSpan" id="kobo.1008.2">I used the term </span><em class="italic"><span class="koboSpan" id="kobo.1009.1">Soft</span></em><span class="koboSpan" id="kobo.1010.1"> to identify toys that were soft and squished easily (as compared to hard plastic or metal) because that is a separate criterion we may need for using our robot hand. </span><span class="koboSpan" id="kobo.1010.2">We generate the dummy values and </span><a id="_idIndexMarker624"/><span class="koboSpan" id="kobo.1011.1">replace the 3 columns with 18 new columns. </span><span class="koboSpan" id="kobo.1011.2">pandas automatically names the columns with a combination of the old </span><a id="_idIndexMarker625"/><span class="koboSpan" id="kobo.1012.1">column name and the value. </span><span class="koboSpan" id="kobo.1012.2">For example, the single </span><strong class="source-inline"><span class="koboSpan" id="kobo.1013.1">Color</span></strong><span class="koboSpan" id="kobo.1014.1"> column is replaced by </span><strong class="source-inline"><span class="koboSpan" id="kobo.1015.1">Color_white</span></strong><span class="koboSpan" id="kobo.1016.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1017.1">Color_blue</span></strong><span class="koboSpan" id="kobo.1018.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1019.1">Color_green</span></strong><span class="koboSpan" id="kobo.1020.1">, and </span><span class="No-Break"><span class="koboSpan" id="kobo.1021.1">so on:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1022.1">
textCols = ['Color','Soft','Material']
toyData = pd.get_dummies(toyData,columns=textCo</span><a id="_idTextAnchor262"/><span class="koboSpan" id="kobo.1023.1">ls)</span></pre></li> <li><span class="koboSpan" id="kobo.1024.1">I put a </span><strong class="source-inline"><span class="koboSpan" id="kobo.1025.1">print</span></strong><span class="koboSpan" id="kobo.1026.1"> statement here just to check that everything got assembled correctly. </span><span class="koboSpan" id="kobo.1026.2">It is optional. </span><span class="koboSpan" id="kobo.1026.3">I’ve been really impressed with pandas for data tables – there is a lot of capability there to do database-type functions and </span><span class="No-Break"><span class="koboSpan" id="kobo.1027.1">data analysis:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1028.1">
print toyData</span></pre></li> <li><span class="koboSpan" id="kobo.1029.1">Now, we are ready to generate our decision tree. </span><span class="koboSpan" id="kobo.1029.2">We instantiate the object and call it </span><strong class="source-inline"><span class="koboSpan" id="kobo.1030.1">dTree</span></strong><span class="koboSpan" id="kobo.1031.1">, setting the classification criteria to Gini. </span><span class="koboSpan" id="kobo.1031.2">We then extract the data values from our </span><strong class="source-inline"><span class="koboSpan" id="kobo.1032.1">toyData</span></strong><span class="koboSpan" id="kobo.1033.1"> dataframe, and put the class values in the first (0th) column into the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1034.1">classValues</span></strong><span class="koboSpan" id="kobo.1035.1"> variable, using array </span><span class="No-Break"><span class="koboSpan" id="kobo.1036.1">slicing operators:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1037.1">
dTree = tree.DecisionTreeClassifier(criterion ="gini")
dataValues=toyData.values[:,1:]
classValues = toyData.values[:,0]</span></pre></li> <li><span class="koboSpan" id="kobo.1038.1">We still need to convert the class names into an enumerated type using </span><strong class="source-inline"><span class="koboSpan" id="kobo.1039.1">LabelEncoder</span></strong><span class="koboSpan" id="kobo.1040.1">, just as we did in the previous two examples. </span><span class="koboSpan" id="kobo.1040.2">We don’t need to one-hot encode. </span><span class="koboSpan" id="kobo.1040.3">Each class represents an end state for our classification example – the leaves on our decision tree. </span><span class="koboSpan" id="kobo.1040.4">If we were doing a neural network classifier, these would be our output neurons. </span><span class="koboSpan" id="kobo.1040.5">One big difference is that when using a </span><a id="_idIndexMarker626"/><span class="koboSpan" id="kobo.1041.1">decision tree, the computer tells you what </span><a id="_idIndexMarker627"/><span class="koboSpan" id="kobo.1042.1">the criteria were that it used to classify and segregate items. </span><span class="koboSpan" id="kobo.1042.2">With a neural network, it will do the classification but you have no way of knowing what criteria </span><span class="No-Break"><span class="koboSpan" id="kobo.1043.1">were used:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1044.1">
lencoder = preproc.LabelEncoder()
lencoder.fit(classValues)
classes = lencoder.transform(classValues)</span></pre></li> <li><span class="koboSpan" id="kobo.1045.1">As we said, to use the class value names in the final output, we have to eliminate any duplicate names and sort them alphabetically. </span><span class="koboSpan" id="kobo.1045.2">This pair of nested functions </span><span class="No-Break"><span class="koboSpan" id="kobo.1046.1">does that:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1047.1">
classValues = list(sorted(set(classValu</span><a id="_idTextAnchor263"/><span class="koboSpan" id="kobo.1048.1">es)))</span></pre></li> <li><span class="koboSpan" id="kobo.1049.1">This is the conclusion of our program. </span><span class="koboSpan" id="kobo.1049.2">Actually creating the decision tree only takes one line of code, now that we have set up all the data. </span><span class="koboSpan" id="kobo.1049.3">We use the same steps as before, and then create the graphic with </span><strong class="source-inline"><span class="koboSpan" id="kobo.1050.1">graphviz</span></strong><span class="koboSpan" id="kobo.1051.1"> and save the image as a PDF. </span><span class="koboSpan" id="kobo.1051.2">That was not hard at all – now that we have had all that practice setting </span><span class="No-Break"><span class="koboSpan" id="kobo.1052.1">this up:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1053.1">
print ""
dTree = dTree.fit(dataValues,classes)
c_data=tree.export_graphviz(dTree,out_file=None,feature_names=toyData.columns,
class_names=classValues, filled = True, rounded=True,special_characters=True)
graph = graphviz.Source(c_data) graph.render("toy_decision_tree_graph_oneHot_gini")</span></pre></li> </ol>
<p><span class="koboSpan" id="kobo.1054.1">The result is the flowchart shown in the following figure. </span><span class="koboSpan" id="kobo.1054.2">This output with one-hot encoding </span><a id="_idIndexMarker628"/><span class="koboSpan" id="kobo.1055.1">is a bit easier to read than </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1056.1">Figure 8</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1057.1">.4</span></em><span class="koboSpan" id="kobo.1058.1"> because we </span><a id="_idIndexMarker629"/><span class="koboSpan" id="kobo.1059.1">can see the numbers in each category. </span><span class="koboSpan" id="kobo.1059.2">You’ll note that each leaf (end node) has only one category with a count (two stuffed animals and three </span><span class="No-Break"><span class="koboSpan" id="kobo.1060.1">musical instruments):</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer087">
<span class="koboSpan" id="kobo.1061.1"><img alt="Figure 8.5 – The output of the decision tree using one-hot encoding is much easier ﻿﻿﻿to read" src="image/B19846_08_5.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1062.1">Figure 8.5 – The output of the decision tree using one-hot encoding is much easier </span><a id="_idTextAnchor264"/><a id="_idTextAnchor265"/><a id="_idTextAnchor266"/><span class="koboSpan" id="kobo.1063.1">to read</span></p>
<p><span class="koboSpan" id="kobo.1064.1">Since we’ve been able to describe and make all sorts of decision trees, what would we have if we used a whole bunch of them? </span><span class="koboSpan" id="kobo.1064.2">A forest! </span><span class="koboSpan" id="kobo.1064.3">Let’s explore what this might </span><span class="No-Break"><span class="koboSpan" id="kobo.1065.1">look like.</span></span></p>
<h1 id="_idParaDest-137"><a id="_idTextAnchor267"/><span class="koboSpan" id="kobo.1066.1">Random forests</span></h1>
<p><span class="koboSpan" id="kobo.1067.1">I really wanted to add this section on </span><strong class="bold"><span class="koboSpan" id="kobo.1068.1">random forest classifiers</span></strong><span class="koboSpan" id="kobo.1069.1">, but not just because the name </span><a id="_idIndexMarker630"/><span class="koboSpan" id="kobo.1070.1">sounds so cool. </span><span class="koboSpan" id="kobo.1070.2">While I may have been accused of stretching metaphors to the breaking point, this time, the name may have inspired the name of this type of decision tree process. </span><span class="koboSpan" id="kobo.1070.3">We have learned how to make decision trees, and we have learned that they have some weak points. </span><span class="koboSpan" id="kobo.1070.4">It is best if the data really belongs to distinct and differentiated groups. </span><span class="koboSpan" id="kobo.1070.5">They are not very tolerant of noise in the data. </span><span class="koboSpan" id="kobo.1070.6">And they really gets unwieldy if you want to scale them up – you can imagine how big a graph would get with 200 classes rather than the 6 or 7 we were </span><span class="No-Break"><span class="koboSpan" id="kobo.1071.1">dealing with.</span></span></p>
<p><span class="koboSpan" id="kobo.1072.1">If you want to take advantage of the simplicity and utility of decision trees but want to handle more data, more uncertainty, and more classes, you can use a random forest, which, just as the name indicates, is just a whole batch of randomly generated decision trees. </span><span class="koboSpan" id="kobo.1072.2">Let’s step through </span><span class="No-Break"><span class="koboSpan" id="kobo.1073.1">the process:</span></span></p>
<ol>
<li value="1"><span class="koboSpan" id="kobo.1074.1">We collect our database of information but, instead of 18 rows in our database, we have 10,000 records or 1 million records. </span><span class="koboSpan" id="kobo.1074.2">We subdivide this data into random sets – we generate 100 sets of data each </span><em class="italic"><span class="koboSpan" id="kobo.1075.1">randomly</span></em><span class="koboSpan" id="kobo.1076.1"> chosen from all of our data – and we put them in </span><em class="italic"><span class="koboSpan" id="kobo.1077.1">random</span></em><span class="koboSpan" id="kobo.1078.1"> order. </span><span class="koboSpan" id="kobo.1078.2">We also pull out one set of data to use as a test set, just as we did for the </span><span class="No-Break"><span class="koboSpan" id="kobo.1079.1">neural networks.</span></span></li>
<li><span class="koboSpan" id="kobo.1080.1">Now, for each set of random data, we make a decision tree using the same process we have </span><span class="No-Break"><span class="koboSpan" id="kobo.1081.1">already learned.</span></span></li>
<li><span class="koboSpan" id="kobo.1082.1">Now, we have this collection of 100 classification engines, each generated from a different, randomly generated subset of data. </span><span class="koboSpan" id="kobo.1082.2">We now test our random forest by taking data from the test set and running through all 100 of the trees in our forest. </span><span class="koboSpan" id="kobo.1082.3">Each tree will provide an estimate of the classification of the data in our test record. </span><span class="koboSpan" id="kobo.1082.4">If we are still classifying toys, then one of the trees would estimate that we are describing a toy car. </span><span class="koboSpan" id="kobo.1082.5">Another may think it’s a musical instrument. </span><span class="koboSpan" id="kobo.1082.6">We take each estimate and treat it as a vote. </span><span class="koboSpan" id="kobo.1082.7">Then, the majority rules – the class that the majority of the trees selected is the winner. </span><span class="koboSpan" id="kobo.1082.8">And that is all there is </span><span class="No-Break"><span class="koboSpan" id="kobo.1083.1">to it.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.1084.1">The setup and program are just the same as what we did before, but you can’t draw a decision tree from a random forest, or just create a tree as an end in itself because that is not what a random forest does – if you just need a decision tree, you know how to do that. </span><span class="koboSpan" id="kobo.1084.2">What you can do is to use a random forest like a neural network, as either a classification engine (to what class does this data belong?) or a regression engine that approximates a </span><span class="No-Break"><span class="koboSpan" id="kobo.1085.1">non-linear curve.</span></span></p>
<p><span class="koboSpan" id="kobo.1086.1">At this point, you can </span><a id="_idIndexMarker631"/><span class="koboSpan" id="kobo.1087.1">conclude with me that decision trees are really useful for a lot of things. </span><span class="koboSpan" id="kobo.1087.2">But did you know you can navigate with them? </span><span class="koboSpan" id="kobo.1087.3">The next section covers path planning for robots – using a different type of </span><span class="No-Break"><span class="koboSpan" id="kobo.1088.1">decisio</span><a id="_idTextAnchor268"/><a id="_idTextAnchor269"/><a id="_idTextAnchor270"/><span class="koboSpan" id="kobo.1089.1">n tree.</span></span></p>
<h1 id="_idParaDest-138"><a id="_idTextAnchor271"/><span class="koboSpan" id="kobo.1090.1">Introducing robot path planning</span></h1>
<p><span class="koboSpan" id="kobo.1091.1">In this section, we will be applying decision tree techniques to perform robot navigation. </span><span class="koboSpan" id="kobo.1091.2">Some people </span><a id="_idIndexMarker632"/><span class="koboSpan" id="kobo.1092.1">like to refer to these as </span><strong class="bold"><span class="koboSpan" id="kobo.1093.1">graph-based solutions</span></strong><span class="koboSpan" id="kobo.1094.1">, but any </span><a id="_idIndexMarker633"/><span class="koboSpan" id="kobo.1095.1">sort of navigation problem ends up being a decision tree. </span><span class="koboSpan" id="kobo.1095.2">Consider as you drive your car, can you divide your navigation problems into a set of decisions – turn right, turn left, or </span><span class="No-Break"><span class="koboSpan" id="kobo.1096.1">go straight?</span></span></p>
<p><span class="koboSpan" id="kobo.1097.1">We are going to </span><a id="_idIndexMarker634"/><span class="koboSpan" id="kobo.1098.1">take what we have learned so far and press on to a problem related to classification, and that is </span><strong class="bold"><span class="koboSpan" id="kobo.1099.1">grid searching</span></strong><span class="koboSpan" id="kobo.1100.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.1101.1">path finding</span></strong><span class="koboSpan" id="kobo.1102.1">. </span><span class="koboSpan" id="kobo.1102.2">We will be learning </span><a id="_idIndexMarker635"/><span class="koboSpan" id="kobo.1103.1">about the famous and widely used </span><strong class="bold"><span class="koboSpan" id="kobo.1104.1">A*</span></strong><span class="koboSpan" id="kobo.1105.1"> (pronounced </span><strong class="bold"><span class="koboSpan" id="kobo.1106.1">A-star</span></strong><span class="koboSpan" id="kobo.1107.1">) algorithm. </span><span class="koboSpan" id="kobo.1107.2">This will start with grid navigation methods, topological path finding, such as GPS route finding, and finally, expert systems. </span><span class="koboSpan" id="kobo.1107.3">You will see that these are all versions and variations on the topic of decision trees that we have </span><span class="No-Break"><span class="koboSpan" id="kobo.1108.1">already learned.</span></span></p>
<p><span class="koboSpan" id="kobo.1109.1">Some problems and datasets, particularly in robotics, lend themselves to a grid-based solution as a simplification of the navigation problem. </span><span class="koboSpan" id="kobo.1109.2">It makes a lot of sense that, if we were trying to plot a path around a house or through a field for a robot, we would divide the ground into some sort of checkerboard grid and use that to plot coordinates that the robot can drive to. </span><span class="koboSpan" id="kobo.1109.3">We could use latitude and longitude, or we could pick some reference point as zero – such as our starting position – and measure off some rectangular grid relative to the robot. </span><span class="koboSpan" id="kobo.1109.4">The grid serves the same purpose in chess, limiting the number of positions under consideration for potential future movement and limiting and delineating our possible paths through </span><span class="No-Break"><span class="koboSpan" id="kobo.1110.1">the space.</span></span></p>
<p><span class="koboSpan" id="kobo.1111.1">While this section deals with gridded path finding, regardless of whether maps are involved or not, there are robot navigation paradigms that don’t use maps and even some that don’t use grids, or use grids with uneven spacing. </span><span class="koboSpan" id="kobo.1111.2">I’ve designed robot navigation systems with multiple-layer maps where some layers were mutable – changeable – and some were not. </span><span class="koboSpan" id="kobo.1111.3">This is a rich and fertile ground for imagination and experimentation, and I recommend further research if you find this topic interesting. </span><span class="koboSpan" id="kobo.1111.4">For now, let’s start with a description of the coordinate system we’ll </span><span class="No-Break"><span class="koboSpan" id="kobo.1112.1">be using.</span></span></p>
<h2 id="_idParaDest-139"><a id="_idTextAnchor272"/><span class="koboSpan" id="kobo.1113.1">Understanding the coordinate system</span></h2>
<p><span class="koboSpan" id="kobo.1114.1">Let’s get back to the topic at hand. </span><span class="koboSpan" id="kobo.1114.2">We have a robot and room that is roughly rectangular, and within that rectangle are also some roughly rectangular obstacles in the form of furniture, chairs, bookcases, a fireplace, and so on. </span><span class="koboSpan" id="kobo.1114.3">It is a simple concept to consider that </span><a id="_idIndexMarker636"/><span class="koboSpan" id="kobo.1115.1">we mark off a grid to represent this space and create an array of numbers that matches the physical room with a virtual room. </span><span class="koboSpan" id="kobo.1115.2">We set our grid spacing at 1 cm – each grid square is 1 cm x 1 cm, giving us a grid with 580 x 490 squares or 284,200 squares. </span><span class="koboSpan" id="kobo.1115.3">We represent each square by an unsigned integer in a 2D array in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1116.1">robot’s memory.</span></span></p>
<p><span class="koboSpan" id="kobo.1117.1">Now, we are going </span><a id="_idIndexMarker637"/><span class="koboSpan" id="kobo.1118.1">to need some other data. </span><span class="koboSpan" id="kobo.1118.2">We have a starting location and a goal location, specified as grid coordinates. </span><span class="koboSpan" id="kobo.1118.3">We’ll put </span><strong class="source-inline"><span class="koboSpan" id="kobo.1119.1">0,0</span></strong><span class="koboSpan" id="kobo.1120.1"> for the grid in the nearest and leftmost corner of the room so that all our directions and angles will be positive. </span><span class="koboSpan" id="kobo.1120.2">In the way I’ve drawn the room map for you in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1121.1">Figure 8</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1122.1">.6</span></em><span class="koboSpan" id="kobo.1123.1">, that corner will always be the lower-left corner of our map. </span><span class="koboSpan" id="kobo.1123.2">In standard </span><em class="italic"><span class="koboSpan" id="kobo.1124.1">right-hand rule</span></em><span class="koboSpan" id="kobo.1125.1"> notation, left turns are positive angles and right turns are negative. </span><span class="koboSpan" id="kobo.1125.2">The </span><em class="italic"><span class="koboSpan" id="kobo.1126.1">x</span></em><span class="koboSpan" id="kobo.1127.1"> direction is horizontal and the </span><em class="italic"><span class="koboSpan" id="kobo.1128.1">y</span></em><span class="koboSpan" id="kobo.1129.1"> direction is vertical on the page. </span><span class="koboSpan" id="kobo.1129.2">For the robot, the </span><em class="italic"><span class="koboSpan" id="kobo.1130.1">x</span></em><span class="koboSpan" id="kobo.1131.1"> axis is out the right side and the </span><em class="italic"><span class="koboSpan" id="kobo.1132.1">y</span></em><span class="koboSpan" id="kobo.1133.1"> axis is the direct</span><a id="_idTextAnchor273"/><span class="koboSpan" id="kobo.1134.1">ion </span><span class="No-Break"><span class="koboSpan" id="kobo.1135.1">of motion.</span></span></p>
<p><span class="koboSpan" id="kobo.1136.1">You may think it odd that I’m giving these details, but setting up the proper coordinate system is the first step in doing grid searches and path planning. </span><span class="koboSpan" id="kobo.1136.2">We are using Cartesian coordinates indoors. </span><span class="koboSpan" id="kobo.1136.3">We would use different rules outdoors with latitude and longitude. </span><span class="koboSpan" id="kobo.1136.4">There, we might want to use </span><em class="italic"><span class="koboSpan" id="kobo.1137.1">north-east-down</span></em><span class="koboSpan" id="kobo.1138.1"> (north is positive, south is negative, east is positive, west is negative, the </span><em class="italic"><span class="koboSpan" id="kobo.1139.1">z</span></em><span class="koboSpan" id="kobo.1140.1"> axis is down, and the </span><em class="italic"><span class="koboSpan" id="kobo.1141.1">x</span></em><span class="koboSpan" id="kobo.1142.1"> axis is aligned on the robot with the direction </span><span class="No-Break"><span class="koboSpan" id="kobo.1143.1">of travel):</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer088">
<span class="koboSpan" id="kobo.1144.1"><img alt="Figure 8.6 – Coordinate frames for Earth navigation and indoor navigation" src="image/B19846_08_6.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1145.1">Figure 8.6 – Coordinate frames for Earth navigation and indoor navigation</span></p>
<p><span class="koboSpan" id="kobo.1146.1">We will be looking at this room map in more </span><span class="No-Break"><span class="koboSpan" id="kobo.1147.1">detail later.</span></span></p>
<p><span class="koboSpan" id="kobo.1148.1">So, we have </span><a id="_idIndexMarker638"/><span class="koboSpan" id="kobo.1149.1">our grid and a coordinate system that we agree upon, or at least </span><a id="_idIndexMarker639"/><span class="koboSpan" id="kobo.1150.1">agree that we both understand. </span><span class="koboSpan" id="kobo.1150.2">We also have a starting location and an ending location. </span><span class="koboSpan" id="kobo.1150.3">Our objective is to determine the best path for the robot from the start to the finish point. </span><span class="koboSpan" id="kobo.1150.4">And in between, we have to plan a path around any obstacles that may be in </span><span class="No-Break"><span class="koboSpan" id="kobo.1151.1">the way.</span></span></p>
<p><span class="koboSpan" id="kobo.1152.1">Next, we have to talk </span><span class="No-Break"><span class="koboSpan" id="kobo.1153.1">about knowledge.</span></span></p>
<h2 id="_idParaDest-140"><a id="_idTextAnchor274"/><span class="koboSpan" id="kobo.1154.1">Developing a map based on our knowledge</span></h2>
<p><span class="koboSpan" id="kobo.1155.1">There are </span><a id="_idIndexMarker640"/><span class="koboSpan" id="kobo.1156.1">basically two kinds of grid search </span><a id="_idIndexMarker641"/><span class="koboSpan" id="kobo.1157.1">and path </span><span class="No-Break"><span class="koboSpan" id="kobo.1158.1">finding routines:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1159.1">A priori knowledge</span></strong><span class="koboSpan" id="kobo.1160.1">, where you know where everything is on </span><span class="No-Break"><span class="koboSpan" id="kobo.1161.1">the map</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1162.1">A posteriori knowledge</span></strong><span class="koboSpan" id="kobo.1163.1">, where you don’t know where the </span><span class="No-Break"><span class="koboSpan" id="kobo.1164.1">obstacles are</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1165.1">We will start in the easier position where we can do our path planning with perfect knowledge of the layout of the room – we have </span><span class="No-Break"><span class="koboSpan" id="kobo.1166.1">a map.</span></span></p>
<p><span class="koboSpan" id="kobo.1167.1">We really have three goals we are trying to achieve simultaneously with </span><span class="No-Break"><span class="koboSpan" id="kobo.1168.1">path planning:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.1169.1">Reach </span><span class="No-Break"><span class="koboSpan" id="kobo.1170.1">our goal</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.1171.1">Avoid obstacles</span></span></li>
<li><span class="koboSpan" id="kobo.1172.1">Take </span><a id="_idTextAnchor275"/><span class="koboSpan" id="kobo.1173.1">the </span><span class="No-Break"><span class="koboSpan" id="kobo.1174.1">shortest path</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1175.1">We can talk </span><a id="_idIndexMarker642"/><span class="koboSpan" id="kobo.1176.1">about how we might go about this. </span><span class="koboSpan" id="kobo.1176.2">We can start with our pencil at the start point and draw an imaginary line from our start to the goal. </span><span class="koboSpan" id="kobo.1176.3">If there are </span><a id="_idIndexMarker643"/><span class="koboSpan" id="kobo.1177.1">no obstacles in the way, we are done. </span><span class="koboSpan" id="kobo.1177.2">But wait – our pencil is a tiny line on paper. </span><span class="koboSpan" id="kobo.1177.3">Our robot is somewhat chubbier – it has a significant width as it drives around. </span><span class="koboSpan" id="kobo.1177.4">How do we judge whether the robot is going down some narrow passage that it won’t fit into? </span><span class="koboSpan" id="kobo.1177.5">We need to modify </span><span class="No-Break"><span class="koboSpan" id="kobo.1178.1">our map!</span></span></p>
<p><span class="koboSpan" id="kobo.1179.1">We have our grid, or a piece of paper that represents the grid. </span><span class="koboSpan" id="kobo.1179.2">We can draw on that grid the outlines of all the obstacles, to scale. </span><span class="koboSpan" id="kobo.1179.3">We have two chairs, two tables, a fireplace, two ottomans, and four bookcases. </span><span class="koboSpan" id="kobo.1179.4">We color in all the obstacles in the darkest black we can. </span><span class="koboSpan" id="kobo.1179.5">Now, we get a lighter colored pencil – say a blue color – and draw an outline around all of the furniture that is half the width of the robot. </span><span class="koboSpan" id="kobo.1179.6">Our robot is 32 cm wide, so half of that is 16 cm, a nice even number. </span><span class="koboSpan" id="kobo.1179.7">Our grid is 1 cm per square, so we make a 16-square border around everything. </span><span class="koboSpan" id="kobo.1179.8">It </span><a id="_idTextAnchor276"/><span class="koboSpan" id="kobo.1180.1">looks </span><span class="No-Break"><span class="koboSpan" id="kobo.1181.1">like this:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer089">
<span class="koboSpan" id="kobo.1182.1"><img alt="Figure 8.7 – Adding safety boundaries to obstacles helps prevent collisions" src="image/B19846_08_7.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1183.1">Figure 8.7 – Adding safety boundaries to obstacles helps prevent collisions</span></p>
<p><span class="koboSpan" id="kobo.1184.1">So, now our map has two colors – obstacles and a </span><em class="italic"><span class="koboSpan" id="kobo.1185.1">keep-out</span></em><span class="koboSpan" id="kobo.1186.1"> border. </span><span class="koboSpan" id="kobo.1186.2">We are going to keep the </span><a id="_idIndexMarker644"/><span class="koboSpan" id="kobo.1187.1">center of the robot out of the keep-out zone, and then we will not hit anything. </span><span class="koboSpan" id="kobo.1187.2">This should make sense. </span><span class="koboSpan" id="kobo.1187.3">As for judging passages and </span><a id="_idIndexMarker645"/><span class="koboSpan" id="kobo.1188.1">doorways, if the keep-out zones touch on either side (so if there are no white squares left in the middle), then the robot is too big to pass. </span><span class="koboSpan" id="kobo.1188.2">You can see this around the ottoman in the upper-left corner of </span><span class="No-Break"><span class="koboSpan" id="kobo.1189.1">the illustration.</span></span></p>
<p><span class="koboSpan" id="kobo.1190.1">We look at our line now. </span><span class="koboSpan" id="kobo.1190.2">We need a way to write a computer algorithm that determines the white squares that the robot can pass through that gets us from the start point to the </span><span class="No-Break"><span class="koboSpan" id="kobo.1191.1">finish point.</span></span></p>
<p><span class="koboSpan" id="kobo.1192.1">Since we have the goal in Cartesian coordinates and we have our start spot, we can express the distance in a straight line from the start to the finish. </span><span class="koboSpan" id="kobo.1192.2">If the start point is </span><strong class="source-inline"><span class="koboSpan" id="kobo.1193.1">x1, y1</span></strong><span class="koboSpan" id="kobo.1194.1"> and the finish point is </span><strong class="source-inline"><span class="koboSpan" id="kobo.1195.1">x2, y2</span></strong><span class="koboSpan" id="kobo.1196.1">, then the distance is the square root of the sums of the difference between </span><span class="No-Break"><span class="koboSpan" id="kobo.1197.1">the points:</span></span></p>
<p><em class="italic"><span class="koboSpan" id="kobo.1198.1">distance = sqrt(x2-x1)^2 + (</span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1199.1">y2-y1)^2)</span></em></span></p>
<p><span class="koboSpan" id="kobo.1200.1">One approach for developing </span><a id="_idIndexMarker646"/><span class="koboSpan" id="kobo.1201.1">a path planning algorithm is to use a </span><strong class="bold"><span class="koboSpan" id="kobo.1202.1">wavefront method</span></strong><span class="koboSpan" id="kobo.1203.1">. </span><span class="koboSpan" id="kobo.1203.2">We know where the start is. </span><span class="koboSpan" id="kobo.1203.3">We go out in every direction to the eight squares adjacent to the start point. </span><span class="koboSpan" id="kobo.1203.4">If any of those hit an obstacle or keep-out zone, we throw it </span><a id="_idIndexMarker647"/><span class="koboSpan" id="kobo.1204.1">out as a possible path. </span><span class="koboSpan" id="kobo.1204.2">We keep track of how we got to each square, which, in my illustration (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1205.1">Figure 8</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1206.1">.8</span></em><span class="koboSpan" id="kobo.1207.1">), is indicated by the arrows. </span><span class="koboSpan" id="kobo.1207.2">We use </span><a id="_idIndexMarker648"/><span class="koboSpan" id="kobo.1208.1">the information on how we got to the square because we don’t yet know where we are going next. </span><span class="koboSpan" id="kobo.1208.2">Now, we take all the new squares and do the same thing again – grabbing one square, seeing which of its eight neighbors is a legal move, and then putting an arrow (or a pointer to the location of the previous square) in it to keep track of how we got there. </span><span class="koboSpan" id="kobo.1208.3">We continue to do this until we get to our goal. </span><span class="koboSpan" id="kobo.1208.4">We keep a record of the order of the squares we examined and follow the arrows backward to our </span><span class="No-Break"><span class="koboSpan" id="kobo.1209.1">starting point.</span></span></p>
<p><span class="koboSpan" id="kobo.1210.1">If more than one square has a path leading to the current square, then we take the closest one, which is to say the shortest path. </span><span class="koboSpan" id="kobo.1210.2">We follow these predecessors all the way back to the starting point, and that is </span><span class="No-Break"><span class="koboSpan" id="kobo.1211.1">our path:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer090">
<span class="koboSpan" id="kobo.1212.1"><img alt="Figure 8.8 – The wavefront approach to path planning has very little math involved. Each figure is a step in the process, starting at the upper left and going across, then down" src="image/B19846_08_8.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1213.1">Figure 8.8 – The wavefront approach to path planning has very little math involved. </span><span class="koboSpan" id="kobo.1213.2">Each figure is a step in the process, starting at the upper left and going across, then down</span></p>
<p><span class="koboSpan" id="kobo.1214.1">You will notice in this example that I allowed the robot to make diagonal turns to get from one </span><a id="_idIndexMarker649"/><span class="koboSpan" id="kobo.1215.1">square to another. </span><span class="koboSpan" id="kobo.1215.2">I could have also specified that only right-angle turns are allowed, but that is not very efficient and is hard on the </span><a id="_idIndexMarker650"/><span class="koboSpan" id="kobo.1216.1">robot’s drive system. </span><span class="koboSpan" id="kobo.1216.2">Only allowing right-angle turns simplifies the processing somewhat, since you only have to consider four neighbors around a </span><a id="_idTextAnchor277"/><span class="koboSpan" id="kobo.1217.1">square instead </span><span class="No-Break"><span class="koboSpan" id="kobo.1218.1">of eight.</span></span></p>
<p><span class="koboSpan" id="kobo.1219.1">Another approach </span><a id="_idIndexMarker651"/><span class="koboSpan" id="kobo.1220.1">for developing a path planning algorithm that would look promising is the </span><strong class="bold"><span class="koboSpan" id="kobo.1221.1">Greedy Best-First</span></strong><span class="koboSpan" id="kobo.1222.1"> approach. </span><span class="koboSpan" id="kobo.1222.2">Instead of keeping a record and checking all of the grid points as we did in the wavefront method, we just keep the single best path square out of the eight we just tested. </span><span class="koboSpan" id="kobo.1222.3">The measure we use to decide which square to keep is the one that is closest to our straight-line path. </span><span class="koboSpan" id="kobo.1222.4">Another way of saying this is to say it’s the square that is closest to the goal. </span><span class="koboSpan" id="kobo.1222.5">We remove squares that are blocked by obstacles, of course. </span><span class="koboSpan" id="kobo.1222.6">The net result is we are considering a lot (really a lot!) fewer squares than the wavefront method of </span><span class="No-Break"><span class="koboSpan" id="kobo.1223.1">path planning:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer091">
<span class="koboSpan" id="kobo.1224.1"><img alt="Figure 8.9 – The aptly named “Greedy Best-First” algorithm is fast, but can get stuck" src="image/B19846_08_9.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1225.1">Figure 8.9 – The aptly named “Greedy Best-First” algorithm is fast, but can get stuck</span></p>
<p><span class="koboSpan" id="kobo.1226.1">Does the </span><a id="_idIndexMarker652"/><span class="koboSpan" id="kobo.1227.1">greedy technique work for all cases? </span><span class="No-Break"><span class="koboSpan" id="kobo.1228.1">Not really.</span></span></p>
<p><span class="koboSpan" id="kobo.1229.1">Why not? </span><span class="koboSpan" id="kobo.1229.2">That seems </span><a id="_idIndexMarker653"/><span class="koboSpan" id="kobo.1230.1">a simple algorithm, and we are only considering legal moves. </span><span class="koboSpan" id="kobo.1230.2">The problem is it can’t deal with a </span><strong class="bold"><span class="koboSpan" id="kobo.1231.1">local minima</span></strong><span class="koboSpan" id="kobo.1232.1">. </span><span class="koboSpan" id="kobo.1232.2">What is a local minima? </span><span class="koboSpan" id="kobo.1232.3">It is a place on the map where the robot would have to go backward to find a good path. </span><span class="koboSpan" id="kobo.1232.4">The easiest type of minima to visualize is a U-shaped area where the robot can get in but not back out. </span><span class="koboSpan" id="kobo.1232.5">The Greedy Best-First algorithm is also not trying to find the shortest path, just a </span><span class="No-Break"><span class="koboSpan" id="kobo.1233.1">valid path:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer092">
<span class="koboSpan" id="kobo.1234.1"><img alt="Figure 8.10 – A “local minima” can occur when no straight path exists, and the robot will have to back up or reverse direction" src="image/B19846_08_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1235.1">Figure 8.10 – A “local minima” can occur when no straight path exists, and the robot will have to back up or reverse direction</span></p>
<p><span class="koboSpan" id="kobo.1236.1">If we want </span><a id="_idIndexMarker654"/><span class="koboSpan" id="kobo.1237.1">to find the shortest path, we need to do some </span><span class="No-Break"><span class="koboSpan" id="kobo.1238.1">more math.</span></span></p>
<p><span class="koboSpan" id="kobo.1239.1">A more </span><a id="_idIndexMarker655"/><span class="koboSpan" id="kobo.1240.1">systematic and mathematical way to approach finding the shortest path around obstacles for a grid search problem is the </span><strong class="bold"><span class="koboSpan" id="kobo.1241.1">A* algorithm</span></strong><span class="koboSpan" id="kobo.1242.1">, first developed for Shakey </span><span class="No-Break"><span class="koboSpan" id="kobo.1243.1">the Robot.</span></span></p>
<h2 id="_idParaDest-141"><a id="_idTextAnchor278"/><span class="koboSpan" id="kobo.1244.1">Introducing the A* algorithm</span></h2>
<p><span class="koboSpan" id="kobo.1245.1">Honestly, you can’t really write a book about robotics without mentioning the A* algorithm. </span><span class="koboSpan" id="kobo.1245.2">A* has its origins with </span><em class="italic"><span class="koboSpan" id="kobo.1246.1">Shakey the Robot</span></em><span class="koboSpan" id="kobo.1247.1"> at Stanford University back in 1968. </span><span class="koboSpan" id="kobo.1247.2">This was </span><a id="_idIndexMarker656"/><span class="koboSpan" id="kobo.1248.1">one of the first map-navigating robots. </span><span class="koboSpan" id="kobo.1248.2">Nils Nilsson and his team </span><a id="_idIndexMarker657"/><span class="koboSpan" id="kobo.1249.1">were trying to find a method to navigate Shakey around the hallways at Stanford and started trying different algorithms. </span><span class="koboSpan" id="kobo.1249.2">The first was called </span><em class="italic"><span class="koboSpan" id="kobo.1250.1">A1</span></em><span class="koboSpan" id="kobo.1251.1">, the second </span><em class="italic"><span class="koboSpan" id="kobo.1252.1">A2</span></em><span class="koboSpan" id="kobo.1253.1">, and so forth. </span><span class="koboSpan" id="kobo.1253.2">After several iterations, the team decided that a combination of techniques worked best. </span><span class="koboSpan" id="kobo.1253.3">In computer science, A* means the letter A followed by anything else, and thus the A-star </span><span class="No-Break"><span class="koboSpan" id="kobo.1254.1">was named.</span></span></p>
<p><span class="koboSpan" id="kobo.1255.1">The concept of the A-star process is very much like what we have already been doing with our other path planners. </span><span class="koboSpan" id="kobo.1255.2">Like the wavefront planner, we start by considering the neighbors around our starting location. </span><span class="koboSpan" id="kobo.1255.3">We will compute an estimate for each square based on two factors: the distance from the starting location and the distance in a straight line to the goal. </span><span class="koboSpan" id="kobo.1255.4">We are going to use these factors to find the path with the lowest cumulative cost. </span><span class="koboSpan" id="kobo.1255.5">We calculate that cost by adding up the value for each grid square that is part of the path. </span><span class="koboSpan" id="kobo.1255.6">The formula is </span><span class="No-Break"><span class="koboSpan" id="kobo.1256.1">as follows:</span></span></p>
<p><em class="italic"><span class="koboSpan" id="kobo.1257.1">F(n) = g(n) + </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1258.1">h(n)</span></em></span></p>
<p><span class="koboSpan" id="kobo.1259.1">Here, </span><em class="italic"><span class="koboSpan" id="kobo.1260.1">F(n)</span></em><span class="koboSpan" id="kobo.1261.1"> refers to the contribution of this square to the path cost, </span><em class="italic"><span class="koboSpan" id="kobo.1262.1">g(n)</span></em><span class="koboSpan" id="kobo.1263.1"> represents the distance from this </span><a id="_idIndexMarker658"/><span class="koboSpan" id="kobo.1264.1">square from the start position along the path chosen (that is, the sum of the path cost), and </span><em class="italic"><span class="koboSpan" id="kobo.1265.1">h(n)</span></em><span class="koboSpan" id="kobo.1266.1"> is the straight line distance from this square to </span><a id="_idIndexMarker659"/><span class="koboSpan" id="kobo.1267.1">the goal, which is a heuristic or estimate of the distance remaining to the goal. </span><span class="koboSpan" id="kobo.1267.2">Since we don’t know what other obstacles we have to go around later, we use this guess as a measuring stick to </span><span class="No-Break"><span class="koboSpan" id="kobo.1268.1">compare paths.</span></span></p>
<p><span class="koboSpan" id="kobo.1269.1">This value represents the cost or contribution of this square if it were a part of the final path. </span><span class="koboSpan" id="kobo.1269.2">We will select the square to be part of the path that has the lowest combined cost. </span><span class="koboSpan" id="kobo.1269.3">As with the wavefront planner, we keep track of the predecessor square or the square that was traversed before this one to reconstruct </span><span class="No-Break"><span class="koboSpan" id="kobo.1270.1">our path:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer093">
<span class="koboSpan" id="kobo.1271.1"><img alt="Figure 8.11 – The A-star computation uses the distance to start (G) and the distance to the goal (H)" src="image/B19846_08_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1272.1">Figure 8.11 – The A-star computation uses the distance to start (G) and the distance to the goal (H)</span></p>
<p><span class="koboSpan" id="kobo.1273.1">The preceding diagram illustrates the A* algorithm. </span><span class="koboSpan" id="kobo.1273.2">Each square is evaluated based on the sum of the </span><a id="_idIndexMarker660"/><span class="koboSpan" id="kobo.1274.1">distance along a path back to the start (</span><em class="italic"><span class="koboSpan" id="kobo.1275.1">G</span></em><span class="koboSpan" id="kobo.1276.1">), and an estimate </span><a id="_idIndexMarker661"/><span class="koboSpan" id="kobo.1277.1">of the remaining distance to the goal (</span><em class="italic"><span class="koboSpan" id="kobo.1278.1">H</span></em><span class="koboSpan" id="kobo.1279.1">). </span><span class="koboSpan" id="kobo.1279.2">The yellow squares represent the path selected </span><span class="No-Break"><span class="koboSpan" id="kobo.1280.1">so far.</span></span></p>
<p><span class="koboSpan" id="kobo.1281.1">Let’s illustrate how the A* </span><span class="No-Break"><span class="koboSpan" id="kobo.1282.1">algorithm works:</span></span></p>
<ol>
<li value="1"><span class="koboSpan" id="kobo.1283.1">We keep a set of all the grid squares on the map we have computed values f</span><a id="_idTextAnchor279"/><span class="koboSpan" id="kobo.1284.1">or. </span><span class="koboSpan" id="kobo.1284.2">We’ll call this </span><strong class="source-inline"><span class="koboSpan" id="kobo.1285.1">exploredMap</span></strong><span class="koboSpan" id="kobo.1286.1">. </span><span class="koboSpan" id="kobo.1286.2">Our map grid square object looks </span><span class="No-Break"><span class="koboSpan" id="kobo.1287.1">like this:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1288.1">
# globals
mapLength = 1280
mapWidth = 1200
mapSize = mapLength*mapWidth
map = []</span></pre></li> <li><span class="koboSpan" id="kobo.1289.1">Now, we will fill in our map with zeros to initialize everything. </span><span class="koboSpan" id="kobo.1289.2">We will define the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1290.1">mapGridSquare</span></strong><span class="koboSpan" id="kobo.1291.1"> function later in the code – it creates our </span><span class="No-Break"><span class="koboSpan" id="kobo.1292.1">data structures:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1293.1">
for ii in range(0, mapWidth):
    for jj in range(0,mapLength):
        mapSq = mapGridSquare()#defined later
        mapSq.position = [ii,jj]
        mapSq.sType =EMPTY</span></pre></li> <li><span class="koboSpan" id="kobo.1294.1">The next </span><a id="_idIndexMarker662"/><span class="koboSpan" id="kobo.1295.1">section creates all of the obstacles on the map. </span><span class="koboSpan" id="kobo.1295.2">We put </span><a id="_idIndexMarker663"/><span class="koboSpan" id="kobo.1296.1">the location of which grid squares to </span><em class="italic"><span class="koboSpan" id="kobo.1297.1">fill-in</span></em><span class="koboSpan" id="kobo.1298.1"> or </span><span class="No-Break"><span class="koboSpan" id="kobo.1299.1">make impassable:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1300.1">
# create obstacles
obstacles = [[1,1],[1,2],[1,3],[45,18],[32,15] …..[1000,233]]
# iterate through obstacles and mark on the map
for pos in obstacles:
    map[pos]. </span><span class="koboSpan" id="kobo.1300.2">sType = OBSTACLE
pathGrid = []</span></pre></li> <li><span class="koboSpan" id="kobo.1301.1">Now, we declare our starting and </span><span class="No-Break"><span class="koboSpan" id="kobo.1302.1">ending positions:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1303.1">
START = [322, 128]
GOAL = [938,523]
exploredMap = []
A_Star_navigation(start, goal, exploredMap, map)</span></pre></li> <li><span class="koboSpan" id="kobo.1304.1">In this section, we are creating our data structures to keep track of all of the computations we make. </span><span class="koboSpan" id="kobo.1304.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1305.1">G</span></strong><span class="koboSpan" id="kobo.1306.1"> value is the computed distance from the start, and the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1307.1">H</span></strong><span class="koboSpan" id="kobo.1308.1"> value is the estimated distance to the goal. </span><strong class="source-inline"><span class="koboSpan" id="kobo.1309.1">F</span></strong><span class="koboSpan" id="kobo.1310.1"> is just the sum of these two. </span><span class="koboSpan" id="kobo.1310.2">We also create a function to compute </span><span class="No-Break"><span class="koboSpan" id="kobo.1311.1">these values:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1312.1">
def mapGridSquare():
    def __init__(self):
        self.F_value = 0.0  #total of G and H
        self.G_value = 0.0  # distance to start
        self.H_value = 0.0  # distance to goal
        self.position=[0,0]   # grid location x and y
        self. </span><span class="koboSpan" id="kobo.1312.2">predecessor =None   # pointer to previous square
        self.sType = PATH
    def compute(self, goal, start):
        self.G_value = distance(goal.position,self.position)
        self.H_value = distance(start.position,self.position
        self.F_value = self.G_value + self.H_value
        return self.F_value</span></pre></li> <li><span class="koboSpan" id="kobo.1313.1">We need a </span><a id="_idIndexMarker664"/><span class="koboSpan" id="kobo.1314.1">function to trace the path from the goal back to the start </span><a id="_idIndexMarker665"/><span class="koboSpan" id="kobo.1315.1">once we’ve completed the map computations. </span><span class="koboSpan" id="kobo.1315.2">This function is </span><span class="No-Break"><span class="koboSpan" id="kobo.1316.1">called </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1317.1">reconstructPath</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1318.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1319.1">
def reconstructPath(current):
    totalPath=[current]
    done=False
    while not done:
        a_square = current.predecessor
        if a_square == None:  # at start position?
</span><span class="koboSpan" id="kobo.1319.2">            done = True
        totalPath.append(a_square)
        current = a_square
    return totalPath</span></pre></li> <li><span class="koboSpan" id="kobo.1320.1">We </span><a id="_idIndexMarker666"/><span class="koboSpan" id="kobo.1321.1">create a </span><strong class="source-inline"><span class="koboSpan" id="kobo.1322.1">findMin</span></strong><span class="koboSpan" id="kobo.1323.1"> function to locate the grid block that we have explored with the lowest </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1324.1">F</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1325.1"> score:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1326.1">
def findMin(map):
    minmap = []
    for square in map:
        if minmap == []:
            minmap = square
            continue
        if square.F_value &lt; minmap.F_value:
            minmap = square
    return minmap</span></pre></li> <li><span class="koboSpan" id="kobo.1327.1">Then, we </span><a id="_idIndexMarker667"/><span class="koboSpan" id="kobo.1328.1">create the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1329.1">navigation</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.1330.1">function itself:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1331.1">
def A_Star_navigation(start, goal, exploredMap, map):
    while len(exploredMap&gt;0):
        current = findMin(exploredMap)
        if current.position == goal.position:
            # we are done – we are at the goal
            return reconstructPath(current)
        neighbors = getNeighbors(current)</span></pre></li> <li><span class="koboSpan" id="kobo.1332.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1333.1">neighbors</span></strong><span class="koboSpan" id="kobo.1334.1"> function returns all the neighbors of the current square that are not marked </span><span class="No-Break"><span class="koboSpan" id="kobo.1335.1">as obstacles:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1336.1">
        for a_square in neighbors:
            if a_square.predecessor == None:</span></pre></li> <li><span class="koboSpan" id="kobo.1337.1">We only compute each grid </span><span class="No-Break"><span class="koboSpan" id="kobo.1338.1">square once:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1339.1">
                old_score = a_square.F_value
    score = a_square.compute(GOAL, START)</span></pre></li> <li><span class="koboSpan" id="kobo.1340.1">Now, we look </span><a id="_idIndexMarker668"/><span class="koboSpan" id="kobo.1341.1">for the square that has the lowest </span><strong class="source-inline"><span class="koboSpan" id="kobo.1342.1">G</span></strong><span class="koboSpan" id="kobo.1343.1"> value – that is, the one closest to </span><span class="No-Break"><span class="koboSpan" id="kobo.1344.1">the start:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1345.1">
    if a_square.G_value &lt; current.G_value:
        a_square.predecessor = current
        current = a_square
        current.compute(GOAL, START)
        exploredMap.append(current)</span></pre></li> </ol>
<p><span class="koboSpan" id="kobo.1346.1">So, in this section, we’ve covered the A* approach to finding the shortest path on a map, given that </span><a id="_idIndexMarker669"/><span class="koboSpan" id="kobo.1347.1">we know where all of the obstacles are in advance. </span><span class="koboSpan" id="kobo.1347.2">But what if we don’t? </span><span class="koboSpan" id="kobo.1347.3">Another</span><a id="_idTextAnchor280"/><a id="_idTextAnchor281"/><span class="koboSpan" id="kobo.1348.1"> method we can use is the </span><span class="No-Break"><span class="koboSpan" id="kobo.1349.1">D* algorithm.</span></span></p>
<h2 id="_idParaDest-142"><a id="_idTextAnchor282"/><span class="koboSpan" id="kobo.1350.1">Introducing the D* (D-star or dynamic A*) algorithm</span></h2>
<p><span class="koboSpan" id="kobo.1351.1">Earlier in the chapter, I talked about </span><em class="italic"><span class="koboSpan" id="kobo.1352.1">a priori</span></em><span class="koboSpan" id="kobo.1353.1"> knowledge. </span><span class="koboSpan" id="kobo.1353.2">The A-star algorithm, for all its usefulness, requires that obstacles in the entire map be known in advance. </span><span class="koboSpan" id="kobo.1353.3">What do we do if we </span><a id="_idIndexMarker670"/><span class="koboSpan" id="kobo.1354.1">are planning a movement into an unknown space, where we will create the map as we go along? </span><span class="koboSpan" id="kobo.1354.2">If we have a robot with sensors, such as sonar or lidar, then the robot will be </span><a id="_idIndexMarker671"/><span class="koboSpan" id="kobo.1355.1">detecting and identifying obstacles as it goes. </span><span class="koboSpan" id="kobo.1355.2">So, it must continually replan its route based on </span><span class="No-Break"><span class="koboSpan" id="kobo.1356.1">increasing information.</span></span></p>
<p><span class="koboSpan" id="kobo.1357.1">The A* process is only run one time to plan a route for a robot before it begins to move. </span><strong class="bold"><span class="koboSpan" id="kobo.1358.1">D*</span></strong><span class="koboSpan" id="kobo.1359.1">, a dynamic replanning process, is constantly updating the robot’s </span><a id="_idTextAnchor283"/><span class="koboSpan" id="kobo.1360.1">path as new information </span><span class="No-Break"><span class="koboSpan" id="kobo.1361.1">becomes available.</span></span></p>
<p><span class="koboSpan" id="kobo.1362.1">The D* algorithm allows for replanning by adding some additional information to each grid square. </span><span class="koboSpan" id="kobo.1362.2">You will remember that in A*, we had the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1363.1">G</span></strong><span class="koboSpan" id="kobo.1364.1"> value (distance to the start along the path), and the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1365.1">H</span></strong><span class="koboSpan" id="kobo.1366.1"> value (straight-line distance to the goal). </span><span class="koboSpan" id="kobo.1366.2">D-star adds a tag to the square that can have several </span><span class="No-Break"><span class="koboSpan" id="kobo.1367.1">possible values:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.1368.1">The square’s tag could be </span><strong class="source-inline"><span class="koboSpan" id="kobo.1369.1">NEW</span></strong><span class="koboSpan" id="kobo.1370.1"> for a new square that had never been </span><span class="No-Break"><span class="koboSpan" id="kobo.1371.1">explored before.</span></span></li>
<li><span class="koboSpan" id="kobo.1372.1">It could be </span><strong class="source-inline"><span class="koboSpan" id="kobo.1373.1">OPEN</span></strong><span class="koboSpan" id="kobo.1374.1"> for tags that have been evaluated and are being considered as part of </span><span class="No-Break"><span class="koboSpan" id="kobo.1375.1">the path.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.1376.1">CLOSED</span></strong><span class="koboSpan" id="kobo.1377.1"> is for squares that have been dropped </span><span class="No-Break"><span class="koboSpan" id="kobo.1378.1">from consideration.</span></span></li>
<li><span class="koboSpan" id="kobo.1379.1">The next two tags are </span><strong class="source-inline"><span class="koboSpan" id="kobo.1380.1">RAISED</span></strong><span class="koboSpan" id="kobo.1381.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1382.1">LOWERED</span></strong><span class="koboSpan" id="kobo.1383.1">. </span><span class="koboSpan" id="kobo.1383.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1384.1">RAISED</span></strong><span class="koboSpan" id="kobo.1385.1"> flag is set if a sensor reading or additional information caused the cost of that square to increase, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1386.1">LOWERED</span></strong><span class="koboSpan" id="kobo.1387.1"> is the opposite. </span><span class="koboSpan" id="kobo.1387.2">For </span><strong class="source-inline"><span class="koboSpan" id="kobo.1388.1">LOWERED</span></strong><span class="koboSpan" id="kobo.1389.1"> squares, we need to propagate the new path cost to the neighbors of the now lower-cost square, so that they can be re-evaluated. </span><span class="koboSpan" id="kobo.1389.2">This may cause tags to change on the neighboring squares. </span><strong class="source-inline"><span class="koboSpan" id="kobo.1390.1">RAISED</span></strong><span class="koboSpan" id="kobo.1391.1"> squares have increased cost, and so may be dropped from the path, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1392.1">LOWERED</span></strong><span class="koboSpan" id="kobo.1393.1"> squares have reduced cost and may be added into </span><span class="No-Break"><span class="koboSpan" id="kobo.1394.1">the path.</span></span></li>
</ul>
<p class="callout-heading"><span class="koboSpan" id="kobo.1395.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.1396.1">Keep in mind that changes in cost values ripple through the D* evaluation of paths like a wave as the path is backtracked all the way to the start when the </span><span class="No-Break"><span class="koboSpan" id="kobo.1397.1">values change.</span></span></p>
<p><span class="koboSpan" id="kobo.1398.1">Another major </span><a id="_idIndexMarker672"/><span class="koboSpan" id="kobo.1399.1">difference between D* and A* is that D* starts at the goal </span><a id="_idIndexMarker673"/><span class="koboSpan" id="kobo.1400.1">and works backward toward the start. </span><span class="koboSpan" id="kobo.1400.2">This allows D* to know the exact cost to the target – it is using the actual path distance to the goal from the current position and not a heuristic or estimate of the distance to go, as </span><span class="No-Break"><span class="koboSpan" id="kobo.1401.1">A* did.</span></span></p>
<p><span class="koboSpan" id="kobo.1402.1">This is a good time to remind you that all these grid-searching techniques we just covered are still variations of decision trees. </span><span class="koboSpan" id="kobo.1402.2">We are going from leaf to leaf – which we have been calling grid squares, but they are still leaves of a decision tree. </span><span class="koboSpan" id="kobo.1402.3">We set some criteria for choosing which of several paths to take, which make branching paths. </span><span class="koboSpan" id="kobo.1402.4">We are working toward some goal or endpoint in each case. </span><span class="koboSpan" id="kobo.1402.5">I bring this up because, in the next section, we will combine decision trees and the type of</span><a id="_idTextAnchor284"/><span class="koboSpan" id="kobo.1403.1"> path planning we learned from the A* and D* algorithm</span><a id="_idTextAnchor285"/><span class="koboSpan" id="kobo.1404.1">s to find a path through streets with </span><span class="No-Break"><span class="koboSpan" id="kobo.1405.1">a GPS.</span></span></p>
<h2 id="_idParaDest-143"><a id="_idTextAnchor286"/><span class="koboSpan" id="kobo.1406.1">GPS path finding</span></h2>
<p><span class="koboSpan" id="kobo.1407.1">I wanted </span><a id="_idIndexMarker674"/><span class="koboSpan" id="kobo.1408.1">to have the opportunity (since we have come this far) to talk just for a little bit about </span><strong class="bold"><span class="koboSpan" id="kobo.1409.1">topological path planners</span></strong><span class="koboSpan" id="kobo.1410.1">. </span><span class="koboSpan" id="kobo.1410.2">This is an alternative method to the grid-based techniques we used in the preceding sections. </span><span class="koboSpan" id="kobo.1410.3">There are types </span><a id="_idIndexMarker675"/><span class="koboSpan" id="kobo.1411.1">of problems and types of navigation where a grid-based </span><a id="_idIndexMarker676"/><span class="koboSpan" id="kobo.1412.1">approach is not appropriate or would require astronomical amounts of detailed data that may </span><a id="_idTextAnchor287"/><span class="koboSpan" id="kobo.1413.1">not be available or practical in a </span><span class="No-Break"><span class="koboSpan" id="kobo.1414.1">small robot.</span></span></p>
<p><span class="koboSpan" id="kobo.1415.1">As an example, I wanted to talk about how your GPS in your car finds a route along streets to reach your destination. </span><span class="koboSpan" id="kobo.1415.2">You must have wondered about how that box has enough information in its tiny brain to provide turn-by-turn directions from one place to another. </span><span class="koboSpan" id="kobo.1415.3">You may have imagined, if you stopped to think about it, that the GPS was using the same map you were viewing on the LCD screen to determine where you need to go. </span><span class="koboSpan" id="kobo.1415.4">You would also think that some sort of grid-based search took place, such as the A* algorithm we discussed in such detail. </span><span class="koboSpan" id="kobo.1415.5">And you would </span><span class="No-Break"><span class="koboSpan" id="kobo.1416.1">be wrong.</span></span></p>
<p><span class="koboSpan" id="kobo.1417.1">The data that the GPS uses to plan a route does not look like a map at all. </span><span class="koboSpan" id="kobo.1417.2">Instead, it is a </span><strong class="bold"><span class="koboSpan" id="kobo.1418.1">topological network</span></strong><span class="koboSpan" id="kobo.1419.1"> that shows </span><a id="_idIndexMarker677"/><span class="koboSpan" id="kobo.1420.1">how streets are interconnected. </span><span class="koboSpan" id="kobo.1420.2">In format, it looks more like a database of vectors (which have a direction and a magnitude, or distance), rather than an </span><em class="italic"><span class="koboSpan" id="kobo.1421.1">X, Y</span></em><span class="koboSpan" id="kobo.1422.1"> gridded raster map made up of pixels. </span><span class="koboSpan" id="kobo.1422.2">The database format also takes up a lot less room in the GPS internal storage. </span><span class="koboSpan" id="kobo.1422.3">The streets are divided by </span><strong class="bold"><span class="koboSpan" id="kobo.1423.1">nodes</span></strong><span class="koboSpan" id="kobo.1424.1"> or points where roads intersect or change. </span><span class="koboSpan" id="kobo.1424.2">Each node shows which streets are connected. </span><span class="koboSpan" id="kobo.1424.3">The nodes are connected by </span><strong class="bold"><span class="koboSpan" id="kobo.1425.1">links</span></strong><span class="koboSpan" id="kobo.1426.1">, which allow you to traverse the data from node to node. </span><span class="koboSpan" id="kobo.1426.2">The links represent the roads and have a length, along with cost data about the quality of the road. </span><span class="koboSpan" id="kobo.1426.3">The cost data is used to compute the desirability of the route. </span><span class="koboSpan" id="kobo.1426.4">A limited access highway with a high-speed limit would have a low cost, and a small side street or dirt road with a lot of stop signs would have a high cost since that link is both less desirable </span><span class="No-Break"><span class="koboSpan" id="kobo.1427.1">and slower.</span></span></p>
<p><span class="koboSpan" id="kobo.1428.1">The technique that </span><a id="_idIndexMarker678"/><span class="koboSpan" id="kobo.1429.1">most GPS path planners use is called </span><strong class="bold"><span class="koboSpan" id="kobo.1430.1">Dijkstra’s algorithm</span></strong><span class="koboSpan" id="kobo.1431.1">, after Edsger W. </span><span class="koboSpan" id="kobo.1431.2">Dijkstra, from the Netherlands. </span><span class="koboSpan" id="kobo.1431.3">He wanted to find the shortest path from Rotterdam to Groningen, back in 1956. </span><span class="koboSpan" id="kobo.1431.4">His graph-based solution has withstood the test of time and is very commonly used for GPS routing. </span><span class="koboSpan" id="kobo.1431.5">It’s not of any help to us for our robot, so you can research this on </span><span class="No-Break"><span class="koboSpan" id="kobo.1432.1">your own.</span></span></p>
<p><span class="koboSpan" id="kobo.1433.1">We use the same procedures with the GPS road network database as we would when working the A-star process on a grid map. </span><span class="koboSpan" id="kobo.1433.2">We evaluate each node, and progress outward from our start node, choosing the path that takes us closest in the direction of </span><span class="No-Break"><span class="koboSpan" id="kobo.1434.1">our destination:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer094">
<span class="koboSpan" id="kobo.1435.1"><img alt="Figure 8.12 – A road-based network can be represented as a series of nodes (circles) and links (lines)" src="image/B19846_08_12.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1436.1">Figure 8.12 – A road-based network can be represented as a series of nodes (circles) and links (lines)</span></p>
<p><span class="koboSpan" id="kobo.1437.1">Many GPS </span><a id="_idIndexMarker679"/><span class="koboSpan" id="kobo.1438.1">systems also simultaneously try to backward-chain from the endpoint – the goal or destination – and try to meet somewhere </span><a id="_idIndexMarker680"/><span class="koboSpan" id="kobo.1439.1">in the middle. </span><span class="koboSpan" id="kobo.1439.2">An amazing amount of work has gone into making our current crop of GPS systems small, lightweight, and reliable. </span><span class="koboSpan" id="kobo.1439.3">Of course, they are de</span><a id="_idTextAnchor288"/><span class="koboSpan" id="kobo.1440.1">pendent on up-to-date information in </span><span class="No-Break"><span class="koboSpan" id="kobo.1441.1">the database.</span></span></p>
<h1 id="_idParaDest-144"><a id="_idTextAnchor289"/><span class="koboSpan" id="kobo.1442.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.1443.1">Well, this has been a very busy chapter. </span><span class="koboSpan" id="kobo.1443.2">We covered the uses of decision trees for a variety of applications. </span><span class="koboSpan" id="kobo.1443.3">The basic decision tree has leaves (nodes) and links, or branches, that each represent a decision or a change in a path. </span><span class="koboSpan" id="kobo.1443.4">We learned about fishbone diagrams and root cause analysis, a special type of decision tree. </span><span class="koboSpan" id="kobo.1443.5">We showed a method using </span><strong class="source-inline"><span class="koboSpan" id="kobo.1444.1">scikit-learn</span></strong><span class="koboSpan" id="kobo.1445.1"> to have the computer build a classification decision tree for us and create a usable graph. </span><span class="koboSpan" id="kobo.1445.2">We discussed the concept of random forests, which are just an evolved form of using groups of decision trees to perform prediction or regression. </span><span class="koboSpan" id="kobo.1445.3">Then, we got into graph search algorithms and path planners, spending some time on the A* (or A-star) algorithm, which is widely used for making routes and paths. </span><span class="koboSpan" id="kobo.1445.4">For times when we do not have a map created in advance, the D* (or dynamic A-star) process can use dynamic replanning to continually adjust the robot’s path to reach its goal. </span><span class="koboSpan" id="kobo.1445.5">Finally, we introduced topological graph path planning and discussed how GPS systems find a route for you to the </span><span class="No-Break"><span class="koboSpan" id="kobo.1446.1">coffee shop.</span></span></p>
<p><span class="koboSpan" id="kobo.1447.1">In our next chapter, we’ll be talking about giving your robot an artificial personality, </span><a id="_idTextAnchor290"/><span class="koboSpan" id="kobo.1448.1">by simulating emotions using a Monte </span><span class="No-Break"><span class="koboSpan" id="kobo.1449.1">Carlo model.</span></span></p>
<h1 id="_idParaDest-145"><a id="_idTextAnchor291"/><span class="koboSpan" id="kobo.1450.1">Questions</span></h1>
<ol>
<li value="1"><span class="koboSpan" id="kobo.1451.1">What are the three ways to traverse a </span><span class="No-Break"><span class="koboSpan" id="kobo.1452.1">decision tree?</span></span></li>
<li><span class="koboSpan" id="kobo.1453.1">In the fishbone diagram example, how does one go about pruning the branches of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1454.1">decision tree?</span></span></li>
<li><span class="koboSpan" id="kobo.1455.1">What is the role of the Gini evaluator in creating </span><span class="No-Break"><span class="koboSpan" id="kobo.1456.1">a classification?</span></span></li>
<li><span class="koboSpan" id="kobo.1457.1">In the toy classifier example using Gini indexing, which attributes of the toy were not used by the decision tree? </span><span class="No-Break"><span class="koboSpan" id="kobo.1458.1">Why not?</span></span></li>
<li><span class="koboSpan" id="kobo.1459.1">Which color for the toys was used as a criterion by one of the classification techniques </span><span class="No-Break"><span class="koboSpan" id="kobo.1460.1">we tried?</span></span></li>
<li><span class="koboSpan" id="kobo.1461.1">Give an example of label encoding and one-hot encoding for menu items at </span><span class="No-Break"><span class="koboSpan" id="kobo.1462.1">a restaurant.</span></span></li>
<li><span class="koboSpan" id="kobo.1463.1">In the A* algorithm, discuss the different ways that </span><strong class="source-inline"><span class="koboSpan" id="kobo.1464.1">G()</span></strong><span class="koboSpan" id="kobo.1465.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1466.1">H()</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.1467.1">are computed.</span></span></li>
<li><span class="koboSpan" id="kobo.1468.1">In the A* algorithm, why is </span><strong class="source-inline"><span class="koboSpan" id="kobo.1469.1">H()</span></strong><span class="koboSpan" id="kobo.1470.1"> considered a heuristic and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1471.1">G()</span></strong><span class="koboSpan" id="kobo.1472.1"> is not? </span><span class="koboSpan" id="kobo.1472.2">Also, in the D* algorithm, heuristics are not used. </span><span class="No-Break"><span class="koboSpan" id="kobo.1473.1">Why not?</span></span></li>
<li><span class="koboSpan" id="kobo.1474.1">In the D* algorithm, why is there a </span><strong class="source-inline"><span class="koboSpan" id="kobo.1475.1">R</span><a id="_idTextAnchor292"/><span class="koboSpan" id="kobo.1476.1">AISED</span></strong><span class="koboSpan" id="kobo.1477.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1478.1">LOWERED</span></strong><span class="koboSpan" id="kobo.1479.1"> tag and not just a </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1480.1">CHANGED</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1481.1"> flag?</span></span></li>
</ol>
<h1 id="_idParaDest-146"><a id="_idTextAnchor293"/><span class="koboSpan" id="kobo.1482.1">Further reading</span></h1>
<ul>
<li><em class="italic"><span class="koboSpan" id="kobo.1483.1">Introduction to the A* </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1484.1">Algorithm</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1485.1">: </span></span><a href="https://www.redblobgames.com/pathfinding/a-star/introduction.html"><span class="No-Break"><span class="koboSpan" id="kobo.1486.1">https://www.redblobgames.com/pathfinding/a-star/introduction.html</span></span></a></li>
<li><em class="italic"><span class="koboSpan" id="kobo.1487.1">Introduction to AI Robotics</span></em><span class="koboSpan" id="kobo.1488.1"> by Robin R. </span><span class="koboSpan" id="kobo.1488.2">Murphy, MIT </span><span class="No-Break"><span class="koboSpan" id="kobo.1489.1">Press, 2000</span></span></li>
<li><em class="italic"><span class="koboSpan" id="kobo.1490.1">How Decision Tree Algorithm </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1491.1">Works</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1492.1">: </span></span><a href="https://dataaspirant.com/2017/01/30/how-decision-tree-algorithm-works/"><span class="No-Break"><span class="koboSpan" id="kobo.1493.1">https://dataaspirant.com/2017/01/30/how-decision-tree-algorithm-works/</span></span></a></li>
<li><em class="italic"><span class="koboSpan" id="kobo.1494.1">Game Programming </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1495.1">Heuristics</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1496.1">: </span></span><a href="http://theory.stanford.edu/~amitp/GameProgramming/Heuristics.html"><span class="No-Break"><span class="koboSpan" id="kobo.1497.1">http://theory.stanford.edu/~amitp/GameProgramming/Heuristics.html</span></span></a></li>
<li><em class="italic"><span class="koboSpan" id="kobo.1498.1">D*Lite Algorithm Blog (Project Fast Replanning)</span></em><span class="koboSpan" id="kobo.1499.1"> by Sven </span><span class="No-Break"><span class="koboSpan" id="kobo.1500.1">Koening: </span></span><a href="http://idm-lab.org/project-a.html"><span class="No-Break"><span class="koboSpan" id="kobo.1501.1">http://idm-lab.org/project-a.html</span></span></a></li>
<li><em class="italic"><span class="koboSpan" id="kobo.1502.1">Graph-Based Path Planning for Mobile Robots</span></em><span class="koboSpan" id="kobo.1503.1">, Dissertation by David Wooden, School of Electrical and Computer Engineering, Georgia Institute of Technology, </span><span class="No-Break"><span class="koboSpan" id="kobo.1504.1">December 2006</span></span></li>
<li><em class="italic"><span class="koboSpan" id="kobo.1505.1">The Focused D* Algorithm for Real-Time Replanning</span></em><span class="koboSpan" id="kobo.1506.1"> by Anthony </span><span class="No-Break"><span class="koboSpan" id="kobo.1507.1">Stentz: </span></span><a href="https://robotics.caltech.edu/~jwb/courses/ME132/handouts/Dstar_ijcai95.pdf"><span class="No-Break"><span class="koboSpan" id="kobo.1508.1">https://robotics.caltech.edu/~jwb/courses/ME132/handouts/Dstar_ijcai95.pdf</span></span></a></li>
</ul>
</div>
<div>
<div class="IMG---Figure" id="_idContainer096">
</div>
</div>
</body></html>