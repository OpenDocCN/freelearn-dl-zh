<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-128"><a id="_idTextAnchor235"/>8</h1>
<h1 id="_idParaDest-129"><a id="_idTextAnchor236"/>Putting Things Away</h1>
<p>Imagine that you have to get to Grandma’s house, which, according to legend, is <em class="italic">over the hills and through the woods</em>, and two states away. That would be two countries away if you live in Europe. To plan your trip, you can start in one of two ways. Ignoring the fact that Google has taken away most map reading and navigation skills from today’s youth, you would get out a map and do one of the following:</p>
<ul>
<li>Start at your house and try to find the roads that are closest to a straight line to Grandma’s house</li>
<li>Start at Grandma’s house and try to find roads leading to your home</li>
</ul>
<p>From either direction, you will find that the road or path you seek forks, intersects, changes, meanders, and may even come to a dead end. Also, all roads are not created equally – some are bigger, with higher speed limits, and some are smaller, with more stop signs. In the end, you pick your route by the combination of decisions that results in the lowest cost. This cost may be in terms of <em class="italic">time</em> – how long to get there. It may be in terms of <em class="italic">distance</em> – how many miles to cover. Or it may be in <em class="italic">monetary</em> terms – there is a toll road that charges an extra fee.</p>
<p>In this chapter, we will be discussing several ways to solve problems involving choosing a chain of multiple decisions where there is some metric – such as cost – to help us select which combination is somehow the best. There is a lot of information here that is widely used in robotics, and we will be expanding our horizons a bit beyond our toy-grabbing robot to look at robot path planning and decision-making in general. These are critical skills for any robotics practitioner, so they are included here. This chapter covers the basics of <a id="_idIndexMarker566"/>decision-making processes for <strong class="bold">artificial intelligence</strong> (<strong class="bold">AI</strong>) where the problem can be described in terms of either a <strong class="bold">classification problem</strong> (determining <a id="_idIndexMarker567"/>whether this situation belongs to one or more groups of similar situations) or a <strong class="bold">regression problem</strong> (fitting or <a id="_idIndexMarker568"/>approximating a function that can be a curve or a path). Finally, we will be applying two approaches to our robot problem – an expert system and random forests.</p>
<p>This chapter will cover the following topics:</p>
<ul>
<li>Decision trees and random forests</li>
<li>Path planning, grid searches, and the A* (A-star) algorithm</li>
<li>Dynamic planning with the D* (D-star) technique</li>
<li>Expert systems and knowledge bases</li>
</ul>
<p>At first glance, the concepts we will cover in this section – namely, path planning, decision trees, random forests, grid searches, and GPS route finders – don’t have much in common, other than all being part of computer algorithms used in AI. From my point of view, they are all basically the same concept and approach problems in the same way.</p>
<h1 id="_idParaDest-130"><a id="_idTextAnchor237"/>Technical requirements</h1>
<p>The one <a id="_idIndexMarker569"/>tool we use for this chapter, you should have already installed from earlier chapters – <strong class="bold">scikit-learn</strong> (<a href="http://scikit-learn.org/stable/developers/advanced_installation.html">http://scikit-learn.org/stable/developers/advanced_installation.html</a>).</p>
<p>Or, if you have the <code>pip</code> installer in Python, you can install it using the following command:</p>
<pre class="console">
pip install –U scikit-learn</pre> <p>You’ll find the code for this chapter at <a href="https://github.com/PacktPublishing/Artificial-Intelligence-for-Robotics-2e">https://github.com/PacktPublishing/Artificial-Intelligence-for-Robotics-2e</a>.</p>
<h1 id="_idParaDest-131"><a id="_idTextAnchor238"/>Task analysis</h1>
<p>Our task in this chapter is one that you may have been waiting for if you have been keeping score since <a href="B19846_03.xhtml#_idTextAnchor043"><em class="italic">Chapter 3</em></a>, where we discussed our storyboards. We need to navigate around the room on our wheels and find a path to our destination, whether that is picking up a toy or driving to a toybox.</p>
<p>To achieve this, we will be using <strong class="bold">decision trees</strong>, <strong class="bold">classification</strong> (a type of <strong class="bold">unsupervised learning</strong>), <strong class="bold">fishbone diagrams</strong>, which are good for troubleshooting, and finally, <strong class="bold">path planni<a id="_idTextAnchor239"/><a id="_idTextAnchor240"/>ng</strong>.</p>
<h1 id="_idParaDest-132"><a id="_idTextAnchor241"/>Introducing decision trees</h1>
<p>The concept of a <strong class="bold">decision tree</strong> is fairly simple. You are walking down the sidewalk and come to a corner. Here, you can go right, turn left, or go straight ahead. That is your decision. After <a id="_idIndexMarker570"/>making the decision – to turn left – you now have different decisions ahead of you than if you turned right. Each decision creates paths that lead to other decisions.</p>
<p>As we are walking down the sidewalk, we have a goal in mind. We are not just wandering around aimlessly; we are trying to get to some goal. One or more combinations of decisions will get us to the goal. Let’s say the goal is to get to the grocery store to buy bread. There may be four or five paths down sidewalks that will get you to the store, but each path may be different in length or may have different paths. If one path goes up a hill, that may be harder than taking the level path. Another path may have you wait at a traffic light, which costs time. We assign a value to each of these attributes and generally want to pick the path with the lowest cost, or the highest reward, depending on the problem.</p>
<p>In the following decision tree, we can break down the actions of the robot in order to pick up a toy. We start by looking at the toy aspect ratio (the length versus width of the bounding box we detected in <a href="B19846_04.xhtml#_idTextAnchor126"><em class="italic">Chapter 4</em></a>). We adjust the wrist of the robot arm based on the narrowest part of the toy. Then, we try to pick up the toy with that wrist position. If we are successful, we lift the toy off the ground and carry it to the toybox. If we fail, we try another position. After trying all of the positions, we go on to the next toy and try to come back to this toy later, hopefully from a different angle. You can see the utility of breaking down our actions this way, and it ends up that decision trees are useful for a lot of things, as we will see in this cha<a id="_idTextAnchor242"/>pter:</p>
<div><div><img alt="Figure 8.1 – A simple decision tree on how to pick up toys" src="img/B19846_08_1.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1 – A simple decision tree on how to pick up toys</p>
<p>The general problem with decision tree-type problems is one of <em class="italic">exponential growth</em>. Let’s consider a chess game, a favorite problem set for AI. We have 20 choices for an opening move (8 pawns and 2 knights, each with 2 possible moves). Each of these 20 moves has 20 possible next moves, and so on. So the first move has 20 choices, and the second move <a id="_idIndexMarker571"/>has 400 choices. The third move has 197,281 choices! We soon have a very, very large decision tree as we try to plan ahead. We can say that each of these possible decisions is a <strong class="bold">branch</strong>, the state we are in after making the decision is a <strong class="bold">leaf</strong>, and the entire conceptual structure is a decision tree.</p>
<p class="callout-heading">Note</p>
<p class="callout">The secret to working with decision trees is to ruthlessly prune the branches so you consider as few decisions as possible.</p>
<p>There are two ways to deal with a decision tree (actually, there are three – see if you can guess the third before I explain it):</p>
<ul>
<li>The first way is to start at the beginning and work outward towards your goal. You may come to a dead end, which means back-tracking or possibly starting over. We are <a id="_idIndexMarker572"/>going to call this <strong class="bold">forward chaining</strong> (chain, as we are making a path of links from leaf to leaf in the tree).</li>
<li>The other way is to start with the goal and work up the tree toward the start. This is <strong class="bold">backward chaining</strong>. The cool thing about backward chaining is that there are a <a id="_idIndexMarker573"/>lot fewer branches to traverse. You can guess that a major problem with backward chaining is you have to know what all the leaves are in advance before you can use them. In many problems, such as a grid search or a path planner, this is possible. It does not work in chess, with an exponentially massive tree.</li>
<li>The third technique? No one says we can’t do both – we could combine both forward and backward chaining and meet somewhere in the m<a id="_idTextAnchor243"/>iddle.</li>
</ul>
<p>The choice of <a id="_idIndexMarker574"/>decision tree shapes, chaining techniques, and construction is based on the following:</p>
<ul>
<li>What data is available?</li>
<li>What information is known or unknown? How is the path scored or graded?</li>
</ul>
<p>There are also different kinds of solutions for path planning using decision trees. If you were given unlimited resources, the biggest computer, perfect knowledge in advance, and are willing to wait, then you <a id="_idIndexMarker575"/>can generate an <strong class="bold">optimal path</strong> or solution.</p>
<p>One of my lessons learned from years of developing practical AI-based robots and unmanned vehicles is that any solution that meets all of the criteria or goals is an acceptable and usable solution, and you don’t have to wait and continue to compute the perfect or optimal solution. Often then, a <em class="italic">good enough</em> solution is found in 1/10 or even 1/100 the time of an optimal solution, because an optimal solution requires an ex<a id="_idTextAnchor244"/>haustive search that may have to consider all possible paths and combinations.</p>
<p>So, how do we approach making our decision trees work faster, or more efficiently? We do what any good gardener would do – start pruning our<a id="_idTextAnchor245"/> trees.</p>
<h2 id="_idParaDest-133"><a id="_idTextAnchor246"/>What do we mean by pruning?</h2>
<p>Sometimes in the computer business, we have to make metaphors to help explain to people how something works. You may remember the desktop metaphor that Apple, and later, Windows, adopted to help explain graphical operating systems. Sometimes, we just run those metaphors into the ground, such as the trash can to delete files, or <em class="italic">Clippy</em>, the paper clip assistant.</p>
<p>You may feel that I’ve gone <a id="_idIndexMarker576"/>off the metaphorical deep end when I discuss <strong class="bold">pruning</strong> your decision trees. What’s next, fertilizer and tree spikes? Actually, pruning is a critical concept in decision tree-type systems. Each branch in your tree can lead to hundreds <a id="_idIndexMarker577"/>or thousands of sub-branches. If you can decide early that a branch is not useful, you can cut it out and you don’t have to process any of the branches or leaves in that branch. The sooner you can discover that a path is not getting you to your goal, the quicker you can reduce the time and effort involved in creating a solution, which is a real-time system such as a robot, a self-driving car, or an autonomous aircraft; this can spell the difference between usable and <a id="_idTextAnchor247"/>worthless.</p>
<p>Let’s run through a quick example in which we use the pruning method. One great use for a decision <a id="_idIndexMarker578"/>tree process is <strong class="bold">Fault Detection, Isolation, and Recovery</strong> (<strong class="bold">FDIR</strong>). This is a typical function of a robot. Let’s make a decision tree for FDIR in the case of our Tinman robot not moving. What automated steps could we take to detect the fault, isolate the problem, and then recover? One technique <a id="_idIndexMarker579"/>we can use is <strong class="bold">root cause analysis</strong>, where we try to figure out our problem by systematically listing and then eliminating (pruning) causing factors and seeing whether the symptoms match. One way to approach root <a id="_idIndexMarker580"/>cause analysis is to use a special form of decision tree called a <strong class="bold">fishbone diagram</strong>, or <strong class="bold">Ishikawa diagram</strong>. This diagram is named after <a id="_idIndexMarker581"/>its inventor, Professor Kaoru Ishikawa from the University of Tokyo. In his 1968 paper, <em class="italic">Guide to Quality Control</em>, the fishbone diagram is named because of its shape, which has a central spine and ribs jutting off on either side. I know, the metaphors are getting deep when we have a decision <em class="italic">tree</em> in the shape of a <em class="italic">fish</em>.</p>
<p>Now, we begin to have a problem. Remember that in a robot, a problem is a symptom, not a cause. Our problem is the robot is not moving. What can cause this problem? Let’s make a list:</p>
<ul>
<li>The drive system</li>
<li>The software</li>
<li>The communication system</li>
<li>The battery and wiring</li>
<li>The sensors</li>
<li>Operator error</li>
</ul>
<p>Now, for each of these, we subdivide our branches into smaller branches. What parts of the <em class="italic">drive system</em> can cause the robot to not be able to move? The wheels could be stuck. The motors could <a id="_idIndexMarker582"/>not be getting power. The gears could be jammed. The motor <a id="_idIndexMarker583"/>driver could have overheated. Here is my fishbone diagram to illustrate the problem of the robot not moving:</p>
<div><div><img alt="Figure 8.2 – A fishbone, or Ishikawa, diagram is commonly used for troubleshooting" src="img/B19846_08_2.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.2 – A fishbone, or Ishikawa, diagram is commonly used for troubleshooting</p>
<p>For each of these factors, you can consider what would be the symptoms of that problem being the cause. If the gears in the motors are jammed, then the motors can’t turn and the wheels can’t turn. If we can check any of these factors off, we can prune or eliminate the gears from our diagram or decision tree. We check the gears, and the wheels and motors turn by hand, so the gears are not the cause. We prune that branch. If we have an automated way of doing testing, we can automatically prune branches, which we will be able to do in the later examples in this chapter.</p>
<p>How about the battery? The battery could need charging (dead battery), the battery could be disconnected, or a power wire could be loose. We check the battery voltage – that is OK, so prune that leaf off the tree. We check the wiring – nothing loose. The battery bran<a id="_idTextAnchor248"/>ch gets pruned.</p>
<p>And so we go on until <a id="_idIndexMarker584"/>we have something that either matches all our symptoms or <a id="_idIndexMarker585"/>is the last one left. Let’s say the last branch was communications. Now what? We ask, “What things in communications would cause us not to move?” Our first answer is that motor command messages are not getting through to our robot over the network. We check the log and see, indeed, no motor messages are present (<code>cmd_vel</code>, in our case). There is our problem, but what caused the problem? The network could be broken (checked – no, the network is OK), or the IP address could be wrong (no, that’s OK). We look to see whether any recent changes were made to the control software, and indeed, there were. We revert to the previous version and see the robot move. There is our problem and we used a decision tree to find it.</p>
<p>So, in this case, we solved our problem almost entirely by pruning branches and leaves off our tree until only one path was left, or we arrived at our goal.</p>
<p>How can we prune branches in software? We can look for <em class="italic">dead ends</em>. Dead ends are leaves – parts of the tree that end and have no future branches. When we reach a dead end, we can not only prune that leaf but also the parts of the path that exclusively lead to that branch. This would be a <strong class="bold">backward-chaining</strong> approach to pruning, as we start at the end and work backward.</p>
<p>We can also see sections of the tree that are unused, or never referenced or called. We can remove entire sections in this manner. This is <strong class="bold">forward chaining</strong> because <a id="_idTextAnchor249"/>we are traversing the tree in the forward direction, from the front to the back.</p>
<p>Up to this point, we, the humans in the story, have been making these decision trees by hand. We have not even discussed how we write a program to allow the robot to use trees to make decisions. Wouldn’t it be a lot nicer if the computer was doing all the hard work of making the tree, deciding the branches, and labeling the nodes instead of us? That is exactly what we will discuss in the next section.</p>
<h2 id="_idParaDest-134">Creating self-classifyi<a id="_idTextAnchor250"/>ng decision trees</h2>
<p>Let’s consider the problem of classifying toys. We may want to come up with a more efficient robot, which sorts toys in some manner instead of just dumping them in a box. In an ideal world, out of a population of 20 toys, we would have some characteristics that divided <a id="_idIndexMarker586"/>the group evenly in half – 10 and 10. Let’s say it is length – half of the toys are under six inches long and half are over. Then, it would also be ideal if some other characteristic divided each of those groups of 10 in half – into four groups of five.</p>
<p>Let’s say it’s <em class="italic">color</em> – we have five red toys, five blue toys, five green toys, and five yellow toys. You may recognize that we are doing what biologists do in classifying new species – we are creating a <strong class="bold">taxonomy</strong>. Now, we pick <a id="_idIndexMarker587"/>another attribute that separates the toys into even smaller groups – it might be what kind of toy it is or what size wheels it has. I think you get the picture. Let’s look at an example.</p>
<p>Now, what would be great is if we could list all the toys and all the attributes in a table, and let the computer figure out how many groups and what kinds there are. We could create a table like this one:</p>
<table class="T---Table _idGenTablePara-1" id="table001-2">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<thead>
<tr class="T---Table">
<td class="T---Table T---Body T---Header">
<p><strong class="bold">Type</strong></p>
</td>
<td class="T---Table T---Body T---Header">
<p><strong class="bold">Length</strong></p>
</td>
<td class="T---Table T---Body T---Header">
<p><strong class="bold">Width</strong></p>
</td>
<td class="T---Table T---Body T---Header">
<p><strong class="bold">Weight</strong></p>
</td>
<td class="T---Table T---Body T---Header">
<p><strong class="bold">Color</strong></p>
</td>
<td class="T---Table T---Body T---Header">
<p><strong class="bold">Number </strong><strong class="bold">of wheels</strong></p>
</td>
<td class="T---Table T---Body T---Header">
<p><strong class="bold">Noise</strong></p>
</td>
<td class="T---Table T---Body T---Header">
<p><strong class="bold">Soft</strong></p>
</td>
<td class="T---Table T---Body T---Header">
<p><strong class="bold">Material</strong></p>
</td>
<td class="T---Table T---Body T---Header">
<p><strong class="bold">Eyes</strong></p>
</td>
<td class="T---Table T---Body T---Header">
<p><strong class="bold">Toy Name</strong></p>
</td>
</tr>
</thead>
<tbody>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p>car</p>
</td>
<td class="T---Table T---Body T---Body">
<p>3</p>
</td>
<td class="T---Table T---Body T---Body">
<p>1</p>
</td>
<td class="T---Table T---Body T---Body">
<p>35</p>
</td>
<td class="T---Table T---Body T---Body">
<p>red</p>
</td>
<td class="T---Table T---Body T---Body">
<p>4</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>hard</p>
</td>
<td class="T---Table T---Body T---Body">
<p>metal</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>hotwheels</p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p>car</p>
</td>
<td class="T---Table T---Body T---Body">
<p>3</p>
</td>
<td class="T---Table T---Body T---Body">
<p>1</p>
</td>
<td class="T---Table T---Body T---Body">
<p>35</p>
</td>
<td class="T---Table T---Body T---Body">
<p>orange</p>
</td>
<td class="T---Table T---Body T---Body">
<p>4</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>hard</p>
</td>
<td class="T---Table T---Body T---Body">
<p>metal</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>hotwheels</p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p>car</p>
</td>
<td class="T---Table T---Body T---Body">
<p>3</p>
</td>
<td class="T---Table T---Body T---Body">
<p>1</p>
</td>
<td class="T---Table T---Body T---Body">
<p>35</p>
</td>
<td class="T---Table T---Body T---Body">
<p>blue</p>
</td>
<td class="T---Table T---Body T---Body">
<p>4</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>hard</p>
</td>
<td class="T---Table T---Body T---Body">
<p>metal</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>hotwheels</p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p>car</p>
</td>
<td class="T---Table T---Body T---Body">
<p>3</p>
</td>
<td class="T---Table T---Body T---Body">
<p>1</p>
</td>
<td class="T---Table T---Body T---Body">
<p>35</p>
</td>
<td class="T---Table T---Body T---Body">
<p>blue</p>
</td>
<td class="T---Table T---Body T---Body">
<p>4</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>hard</p>
</td>
<td class="T---Table T---Body T---Body">
<p>metal</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>hotwheels</p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p>car</p>
</td>
<td class="T---Table T---Body T---Body">
<p>3</p>
</td>
<td class="T---Table T---Body T---Body">
<p>1</p>
</td>
<td class="T---Table T---Body T---Body">
<p>35</p>
</td>
<td class="T---Table T---Body T---Body">
<p>white</p>
</td>
<td class="T---Table T---Body T---Body">
<p>4</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>hard</p>
</td>
<td class="T---Table T---Body T---Body">
<p>metal</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>hotwheels</p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p>stuffed</p>
</td>
<td class="T---Table T---Body T---Body">
<p>5</p>
</td>
<td class="T---Table T---Body T---Body">
<p>5</p>
</td>
<td class="T---Table T---Body T---Body">
<p>50</p>
</td>
<td class="T---Table T---Body T---Body">
<p>white</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>verysoft</p>
</td>
<td class="T---Table T---Body T---Body">
<p>fur</p>
</td>
<td class="T---Table T---Body T---Body">
<p>2</p>
</td>
<td class="T---Table T---Body T---Body">
<p>plush</p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p>stuffed</p>
</td>
<td class="T---Table T---Body T---Body">
<p>7</p>
</td>
<td class="T---Table T---Body T---Body">
<p>5</p>
</td>
<td class="T---Table T---Body T---Body">
<p>55</p>
</td>
<td class="T---Table T---Body T---Body">
<p>brown</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>verysoft</p>
</td>
<td class="T---Table T---Body T---Body">
<p>fur</p>
</td>
<td class="T---Table T---Body T---Body">
<p>3</p>
</td>
<td class="T---Table T---Body T---Body">
<p>plush</p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p>action</p>
</td>
<td class="T---Table T---Body T---Body">
<p>2</p>
</td>
<td class="T---Table T---Body T---Body">
<p>4</p>
</td>
<td class="T---Table T---Body T---Body">
<p>80</p>
</td>
<td class="T---Table T---Body T---Body">
<p>gray</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>hard</p>
</td>
<td class="T---Table T---Body T---Body">
<p>metal</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>slinky</p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p>build</p>
</td>
<td class="T---Table T---Body T---Body">
<p>2</p>
</td>
<td class="T---Table T---Body T---Body">
<p>2</p>
</td>
<td class="T---Table T---Body T---Body">
<p>125</p>
</td>
<td class="T---Table T---Body T---Body">
<p>wood</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>hard</p>
</td>
<td class="T---Table T---Body T---Body">
<p>wood</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>wood block 2x2</p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p>build</p>
</td>
<td class="T---Table T---Body T---Body">
<p>2</p>
</td>
<td class="T---Table T---Body T---Body">
<p>2</p>
</td>
<td class="T---Table T---Body T---Body">
<p>75</p>
</td>
<td class="T---Table T---Body T---Body">
<p>wood</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>hard</p>
</td>
<td class="T---Table T---Body T---Body">
<p>wood</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>wood block triangle</p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p>build</p>
</td>
<td class="T---Table T---Body T---Body">
<p>4</p>
</td>
<td class="T---Table T---Body T---Body">
<p>2</p>
</td>
<td class="T---Table T---Body T---Body">
<p>250</p>
</td>
<td class="T---Table T---Body T---Body">
<p>wood</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>hard</p>
</td>
<td class="T---Table T---Body T---Body">
<p>wood</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>wood block 4x2</p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p>dish</p>
</td>
<td class="T---Table T---Body T---Body">
<p>3</p>
</td>
<td class="T---Table T---Body T---Body">
<p>3</p>
</td>
<td class="T---Table T---Body T---Body">
<p>79</p>
</td>
<td class="T---Table T---Body T---Body">
<p>blue</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>hard</p>
</td>
<td class="T---Table T---Body T---Body">
<p>ceramic</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>teapot</p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p>aircraft</p>
</td>
<td class="T---Table T---Body T---Body">
<p>7</p>
</td>
<td class="T---Table T---Body T---Body">
<p>5</p>
</td>
<td class="T---Table T---Body T---Body">
<p>65</p>
</td>
<td class="T---Table T---Body T---Body">
<p>white</p>
</td>
<td class="T---Table T---Body T---Body">
<p>4</p>
</td>
<td class="T---Table T---Body T---Body">
<p>1</p>
</td>
<td class="T---Table T---Body T---Body">
<p>hard</p>
</td>
<td class="T---Table T---Body T---Body">
<p>plastic</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>space shuttle</p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p>aircraft</p>
</td>
<td class="T---Table T---Body T---Body">
<p>13</p>
</td>
<td class="T---Table T---Body T---Body">
<p>7</p>
</td>
<td class="T---Table T---Body T---Body">
<p>500</p>
</td>
<td class="T---Table T---Body T---Body">
<p>green</p>
</td>
<td class="T---Table T---Body T---Body">
<p>8</p>
</td>
<td class="T---Table T---Body T---Body">
<p>1</p>
</td>
<td class="T---Table T---Body T---Body">
<p>hard</p>
</td>
<td class="T---Table T---Body T---Body">
<p>plastic</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>Thunderbird 2</p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p>car</p>
</td>
<td class="T---Table T---Body T---Body">
<p>5</p>
</td>
<td class="T---Table T---Body T---Body">
<p>1</p>
</td>
<td class="T---Table T---Body T---Body">
<p>333</p>
</td>
<td class="T---Table T---Body T---Body">
<p>yellow</p>
</td>
<td class="T---Table T---Body T---Body">
<p>6</p>
</td>
<td class="T---Table T---Body T---Body">
<p>1</p>
</td>
<td class="T---Table T---Body T---Body">
<p>hard</p>
</td>
<td class="T---Table T---Body T---Body">
<p>metal</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>school bus</p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p>music</p>
</td>
<td class="T---Table T---Body T---Body">
<p>12</p>
</td>
<td class="T---Table T---Body T---Body">
<p>4</p>
</td>
<td class="T---Table T---Body T---Body">
<p>130</p>
</td>
<td class="T---Table T---Body T---Body">
<p>wood</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>2</p>
</td>
<td class="T---Table T---Body T---Body">
<p>hard</p>
</td>
<td class="T---Table T---Body T---Body">
<p>wood</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>toy guitar</p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p>music</p>
</td>
<td class="T---Table T---Body T---Body">
<p>5</p>
</td>
<td class="T---Table T---Body T---Body">
<p>2</p>
</td>
<td class="T---Table T---Body T---Body">
<p>100</p>
</td>
<td class="T---Table T---Body T---Body">
<p>yellow</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>1</p>
</td>
<td class="T---Table T---Body T---Body">
<p>hard</p>
</td>
<td class="T---Table T---Body T---Body">
<p>plastic</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>play </p>
<p>microphone</p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p>music</p>
</td>
<td class="T---Table T---Body T---Body">
<p>4</p>
</td>
<td class="T---Table T---Body T---Body">
<p>4</p>
</td>
<td class="T---Table T---Body T---Body">
<p>189</p>
</td>
<td class="T---Table T---Body T---Body">
<p>white</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>2</p>
</td>
<td class="T---Table T---Body T---Body">
<p>hard</p>
</td>
<td class="T---Table T---Body T---Body">
<p>wood</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>toy drum</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 8.1 – A table of attributes for a group of toys used for classification</p>
<p>We now have <a id="_idIndexMarker588"/>a problem we have to solve. We will be <a id="_idIndexMarker589"/>using a decision tree classifier that is provided with the <code>scikit-learn</code> Python package called <code>DecisionTreeClassifier</code>. This program cannot use strings as input data. We will have to convert all of our string data into some sort of numeric figure. Fortunately, the <code>scikit-learn</code> library provides us with a function just for this purpose. It provides several encoding functions that convert strings into numbers. The function we will use is called <code>LabelEncoder</code>. This function takes an array of strings and converts it into an enumerated set of integers.</p>
<p>We can take our first column, which has the type of toy. My nomenclature is <em class="italic">toy = toy car</em>, <em class="italic">stuffed = stuffed animal</em>, <em class="italic">aircraft = toy aircraft</em>, and <em class="italic">music = toy musical instrument</em>. We also have <em class="italic">action</em> for <em class="italic">action toy</em>, and <em class="italic">build</em> for <em class="italic">building toy</em> (that is, blocks, LEGO™, and so on). We’ll have to turn these into some sort of numbers.</p>
<p><code>LabelEncoder</code> will convert a column in our data table that is populated with strings. The <code>type</code> column from the data is shown in the following code:</p>
<p><code>['car' 'car' 'car' 'car' 'car' 'stuffed' 'stuffed' 'action' 'build' 'build' 'build' 'dish' 'aircraft' 'aircraft' 'car' 'music' '</code><code>music' 'music']</code></p>
<p>It converts it to the label-encoded toy type:</p>
<p><code>[3 3 3 3 3 6 6 0 2 2 2 4 1 1 3 5 </code><code>5 5]</code></p>
<p>You can see <a id="_idIndexMarker590"/>that everywhere where it said <code>car</code>, we now <a id="_idIndexMarker591"/>have the number <code>3</code>. You can also see that <code>6</code> = <code>stuffed</code>, <code>0</code> = <code>action</code>, and so on. Why the odd numbering? The encoder first sorts the strings in alphabetical order.</p>
<p>We are going to just dive right in from here to create a classification progr<a id="_idTextAnchor251"/>am:</p>
<ol>
<li>Here is our decision tree classifier program:<pre class="source-code">
# decision tree classifier
# author: Francis X Govers III #
# example from book "Artificial Intelligence for Robotics" #</pre></li> <li>We first import the libraries we will be using. There is an extra library called <code>graphviz</code> that is useful for drawing pictures of decision trees. You can install it with the following:<pre class="source-code">
<code>pandas</code> package, which provides a lot of data table-handling tools:<pre class="source-code">
from sklearn import tree
import numpy as np
import pandas as pd
import sklearn.preprocessing as preproc
import graphviz</pre></li> <li>Our first step <a id="_idIndexMarker592"/>is to read in our data. I created my table in Microsoft Excel and exported it as a <strong class="bold">comma-separated values</strong> (<strong class="bold">CSV</strong>) format. This allows us to read in the data file directly with the column headers. I print out the shape and size of the data file for reference. My version of <a id="_idIndexMarker593"/>the file has 18 rows and 11 columns. The last <a id="_idIndexMarker594"/>column is just a note to myself on the actual name of each toy. We will not be using the last column for anything. We are building a classifier that will separate the toys by type:<pre class="source-code">
toyData = pd.read_csv("toy_classifier_tree.csv")
print ("Data length ",len(toyData))
print ("Data Shape ",toyData.shape)</pre></li> <li>Now, we can start building our decision tree classifier. We first build an instantiation of the <code>DecisionTreeClassifer</code> object. There are two different <a id="_idIndexMarker595"/>types of <strong class="bold">decision tree classification</strong> (<strong class="bold">DTC</strong>) algorithms to choose from:<ul><li><strong class="bold">Gini coefficient</strong>: The Gini coefficient was developed in 1912 by the Italian statistician <a id="_idIndexMarker596"/>Corrado Gini in <a id="_idIndexMarker597"/>his paper, <em class="italic">Variabilita e Mutabilita</em>. This coefficient, or index, measures the amount of inequality in a group of numbers. A zero value means all the members of the group are the same.</li><li><strong class="bold">Entropy method</strong>: Entropy, when we are talking about AI, refers to the amount <a id="_idIndexMarker598"/>of uncertainty in <a id="_idIndexMarker599"/>a set of data. This concept comes from information theory, in which it measures the amount of uncertainty in a random variable. The concept was introduced by Claude Shannon in the 1940s. To create a decision tree, the algorithm tries to decrease entropy (reduce uncertainty) by splitting the group at a point where each child node is more homogenous than its parent.</li></ul></li>
</ol>
<p>Here, we are going to use the Gini coefficient. If we had a group of toy cars that were all the same size and all red, then the Gini coefficient of the group would be 0. If the members of the group are all different, then the Gini coefficient is closer to 1. The Gini coefficient is given by the following equation:</p>
<p>G(S) = 1− ∑ i=1 n p i 2</p>
<p>We have 4 toy cars out of 18 toys, so the probability of a toy car being in a group is <em class="italic">4/18</em> or 0.222. The decision tree will continue to subdivide classes until the Gini coefficient of the group is 0:</p>
<pre class="source-code">
dTree = tree.DecisionTreeClassifier(criter<a id="_idTextAnchor252"/>ion ="gini")</pre> <ol>
<li value="6">We need to <a id="_idIndexMarker600"/>separate out the values in our data table. The data in the first column, which is called column <code>0</code> in Python, are our classification labels. We need to pull those out separately, as they are used <a id="_idIndexMarker601"/>to separate the toys into classes. From our previous work with neural networks, these would be our outputs or the label data we have used in other machine learning processes. We will be training our classifier to predict the class of the toy based on the attributes in the table (size, weight, color, and so on). We use slicing to pull the data out of the pandas table. Our pandas data table is called <code>toyData</code>. If we want the entries in the table, we need to ask for <code>toyData.values</code>, which will be returned as a 2D array:<pre class="source-code">
dataValues=toyData.values[:,1:10]
classValues = toyData.values[:,0]</pre></li> </ol>
<p>If you are not familiar with slicing notation in Python, the statement <code>toyData.values[:,1:10]</code> returns just the columns in our table from 1 to 10 – it leaves column 0 out. We actually have 11 columns in our table, but since Python starts numbering them at 0, we end up needing 1 to 10. You will probably guess that the other notation just grabs the data in the first column.</p>
<ol>
<li value="7">This is the label encoder that we talked about – it will convert the strings in our data into numbers. For example, colors such as <em class="italic">red</em>, <em class="italic">green</em>, and <em class="italic">blue</em> will be converted to numbers such as <em class="italic">0</em>, <em class="italic">1</em>, and <em class="italic">2</em>. The first item to be encoded is the list of class values that we use to label the data. We use the <code>LabelEncoder.fit()</code> function to come up with the formula for converting strings to numbers, and then the <code>LabelEncoder.transform()</code> function to apply it. Note that <code>fit()</code> does not produce an output.</li>
<li>Finally, we need <a id="_idIndexMarker602"/>to make the string text and <a id="_idIndexMarker603"/>the list of encoded numbers match up. What <code>LabelEncoder</code> will do is sort the strings alphabetically and start numbering them from <em class="italic">A</em>, ignoring any duplicates. If we put in <code>car, car, car, block, stuffed, airplane</code>, we will get <code>2,2,2,1,3,0</code> as the encoding, and we will have to know that <code>airplane</code> = <code>0</code>, <code>block</code> = <code>1</code>, <code>car</code> = <code>2</code>, and <code>stuffed</code> = <code>3</code>. We need to <a id="_idIndexMarker604"/>generate a <code>airplane, block, car, stuffed</code>. We duplicate the <code>LabelEncoder</code> function by using two functions on our list of string-formatted class names:<ul><li>We use the <code>set()</code> function to eliminate duplicates</li><li>We use the <code>sorted()</code> function to sort in the correct order</li></ul></li>
</ol>
<p>Now, our class name table and the enumerations generated by <code>LabelEncoder</code> match. We’ll need this later:</p>
<pre class="source-code">
lencoder = preproc.LabelEncoder() lencoder.fit(classValues)
classes = lencoder.transform(classValues)
classValues = list(sorted(se<a id="_idTextAnchor253"/>t(classValues)))</pre> <ol>
<li value="9">To make it easy on ourselves, I created a function to automatically find out which columns in our data are composed of strings and to convert those columns into numbers. We start by building an empty list to hold our data. We will iterate through the columns in our data and look to see whether the first data value is a string. If it is, we will convert that whole column into numbers using the <a id="_idIndexMarker605"/>label encoder object (<code>lencoder</code>) we created. The label encoding process has two parts. We call <code>lencoder.fit()</code> to see how many unique strings we have in our column and to create <a id="_idIndexMarker606"/>a number for each one. Then, we use <code>lencoder.transpose</code> to insert those numbers into a list:<pre class="source-code">
newData = []
for ii in range(len(dataValues[0]))
line = dataValues[:,ii]
if type(line[0])==str:
     lencoder.fit(line)
  line = lencoder.transform(line)</pre></li> <li>Now, we put all of the data back into the <code>newData</code> list, but there is a problem – we have turned all our columns into rows! We use the <code>transpose</code> function from <code>numpy</code> to correct this problem. But wait! We don’t have an array anymore, as we turned it into a list so we could take it apart and put it back together again (you can’t do that with a <code>numpy</code> array – believe me, I tried):<pre class="source-code">
newData.append(line)
newDataArray = np.asarray(newData)
newDataArray = np.transpose(newDataArray)</pre></li> <li>Now, all of our preprocessing is done, so we can finally call the real <code>DecisionTreeClassifer</code>. It takes two arguments:<ul><li>The array of our data values</li><li>The array of class types that we want the decision tree to divide our groups into</li></ul></li>
</ol>
<p><code>DecisionTreeClassifier</code> will determine what specific data from the table is useful for predicting what class one of our toys fits into:</p>
<pre class="source-code">
dTree = dTree.fit(newDataArray,classes)</pre> <p>That’s it – one line. But wait – we want to see the results. If we just try and print out the decision tree, we get the following:</p>
<pre class="source-code">
DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None, max_features=None, max_leaf_nodes=None,
min_impurity_split=1e-07, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=None, splitter='best')</pre> <p>That does <a id="_idIndexMarker607"/>not tell us anything; that is a description <a id="_idIndexMarker608"/>of the <code>DecisionTreeClassifier</code> object (it does show us all of the parameters we can set, which is why I put it here).</p>
<ol>
<li value="12">So, we use a package called <code>graphviz</code>, which is very good at printing decision trees. We can even pass our column names and class names into the graph. The final two lines output the graph as a <code>.pdf</code> file and store it on the hard drive:<pre class="source-code">
c_data=tree.export_graphviz(dTree,out_file=None,feature_names=toyData.colum ns, class_names=classValues, filled = True, rounded=True,special_characters=True)
graph = graphviz.Source(c_data)
graph.render("toy_graph_gini")</pre></li> </ol>
<p>And here is the result. I will warn you, this is addictive:</p>
<div><div><img alt="Figure 8.3 – The output of the decision tree using t﻿he Gini index method" src="img/B19846_08_3.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.3 – The output of the decision tree using t<a id="_idTextAnchor254"/>he Gini index method</p>
<p>We can <a id="_idIndexMarker609"/>quickly check our solution by looking at <a id="_idIndexMarker610"/>our input table and seeing whether the numbers line up. We should see the following:</p>
<ul>
<li>Five toy cars</li>
<li>Three building blocks</li>
<li>One dish</li>
<li>One action toy</li>
<li>Two stuffed animals</li>
<li>Three musical instruments</li>
<li>Two toy airplanes</li>
</ul>
<p>And that is indeed the case.</p>
<p>The other number to look at is the Gini index. As shown in <em class="italic">Figure 8</em><em class="italic">.3</em>, the top-level box shows that the index for the entire group has an overall value of <code>0.8166</code>, which is close to 1 and shows a high degree of heterogeneity. As we progress down the tree, the Gini numbers get smaller and smaller until reaching <code>0</code> at each of the identified groups, which shows that the items in those groups share all of the same attributes.</p>
<p>What does this graph tell us? First of all, we can separate the toy cars by only one attribute – <em class="italic">width</em>. Only the toy cars are less than 1.5 inches wide (38 mm). We don’t need to look at color, weight, or anything other than width to separate all the toy cars from everything else. We see we have 5 toy cars out of our 18 toys, so we have 13 left to classify. Our next division comes in length. We have 7 toys less than 4.5 inches long (11 cm) and 5 that are longer. Of the group of five, two have eyes and three do not. The toys with eyes <a id="_idIndexMarker611"/>are the two stuffed animals. If you follow <a id="_idIndexMarker612"/>the tree, the branches that lead to the toy music instruments are width &gt; 1.5 inches, length &gt; 4.5 inches, and no eyes, and they are indeed larger than the other toys in length and width, and don’t have eyes.</p>
<p>None of the other bits matter in terms of classifying. That means that an attribute such as <em class="italic">color</em> is a poor predictor of what class a toy belongs to – which makes sense. Our other useful criteria are the <em class="italic">number of wheels</em>, the <em class="italic">weight</em>, and the <em class="italic">length</em>. That data is sufficient to classify all our toys into groups. You can see that the Gini index of each leaf node is indeed <code>0</code>. I added some additional labeling to the graph to make the illustration clearer, as the program uses the class number rather than the class name in the graph.</p>
<p>So, that exercise was satisfactory – we were able to create an automatic decision tree from our toy data that classified our toys. We can even use that data to classify a new toy and predict which class it might belong to. If we found that that new toy violated the classification somehow, then we would need to re-rerun the classification process and make a new decision table.</p>
<p>There is <a id="_idIndexMarker613"/>another type of process for creating decision trees and subdividing <a id="_idIndexMarker614"/>data into categories. That is called the <strong class="bold">entropy model</strong>, or <strong class="bold">information gain</strong>.<a id="_idTextAnchor255"/><a id="_idTextAnchor256"/><a id="_idTextAnchor257"/> Let’s discuss this next.</p>
<h2 id="_idParaDest-135"><a id="_idTextAnchor258"/>Understanding entropy</h2>
<p><strong class="bold">Entropy</strong> is a measurement <a id="_idIndexMarker615"/>of the amount of disorder in the sample of data provided. We can also call this process <strong class="bold">information gain</strong> since we are measuring how much each criterion contributed to our knowledge of which class it belongs to.</p>
<p>The formula <a id="_idIndexMarker616"/>for entropy is a negative log base 2 function that is still primarily looking at the probability of a class belonging to a population, which is just the number of individuals belonging to each class divided by the total number in the sample:</p>
<p><em class="italic">Entropy = -p*log2(p) – </em><em class="italic">p_i*log2(p_i)</em></p>
<p>To substitute entropy as our group criteria in our program, we only have to change one line:</p>
<pre class="source-code">
dTree = tree.DecisionTreeClassifier(criterion ="entropy")</pre> <p>The results are shown in the following diagram:</p>
<div><div><img alt="Figure 8.4 – Output of the decision tree using ﻿entropy (information gain)" src="img/B19846_08_4.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.4 – Output of the decision tree using <a id="_idTextAnchor259"/>entropy (information gain)</p>
<p>You can note that entropy starts at 2.55 for our whole group, and decreases to 0 at the leaf nodes (ends of the branches). We can check that we have seven classifications, but you can see that the entropy method selected different criteria from the Gini method. For example, the Gini classifier started with <code>Length</code>, and the entropy classifier started with <code>Material</code>. The entropy method also chose <code>Noise</code> (whether the toy makes a noise or not) and correctly selected that the only toys that make a noise were the toy musical instruments and the toy airplanes, which have electronic sound boxes that make airplane sounds.</p>
<p>There is one item <a id="_idIndexMarker617"/>that causes some concern, however. There are two blocks that show <code>Material</code>, dividing the toy’s values in material less than 2.5. <code>Material</code> is a discrete value. We can generate a list of materials and run this through our <code>sorted(set(list))</code> process to get the unique values in sorted order:</p>
<p><code>['ceramic', 'fur', 'metal', '</code><code>plastic', 'wood']</code></p>
<p>So, a <code>Material</code> value of 2.5 or less would be either ceramic or fur. Fur and ceramic have nothing <a id="_idIndexMarker618"/>in common, other than where they are found in the alphabet. This is a rather troubling relationship, which is an artifact of how we encoded our data as a sequential set of numbers. This implies relationships and grouping that don’t really exist. How can we correct this?</p>
<p>As a matter of fact, there is a process for handling just this sort of problem. This technique is widely used in AI programs and is a <em class="italic">must-have</em> tool for working with classification, either here in the decision tree section or with neural networks. This tool has the strange <a id="_idIndexMarker619"/>name of <strong class="bold">one-hot encoding</strong>.</p>
<h2 id="_idParaDest-136"><a id="_idTextAnchor260"/>Implementing one-hot encoding</h2>
<p>The concept for one-hot encoding is pretty simple. Instead of replacing a category with an enumeration, we add <a id="_idIndexMarker620"/>one column to our data for each <a id="_idIndexMarker621"/>possible value and set it to be a <code>1</code> or <code>0</code> based on that value. The name comes from the fact that only one column<a id="_idTextAnchor261"/> in the set is <em class="italic">hot</em> or selected.</p>
<p>We can apply this principle to our example. We can replace the one column, <code>Material</code>, with five columns for each material type in our database: <code>ceramic</code>, <code>fur</code>, <code>metal</code>, <code>plastic</code>, and <code>wood</code>:</p>
<table class="T---Table _idGenTablePara-1" id="table002">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<thead>
<tr class="T---Table">
<td class="T---Table T---Body T---Header">
<p><strong class="bold">Material</strong></p>
</td>
<td class="T---Table T---Body T---Header">
<p><strong class="bold">ceramic</strong></p>
</td>
<td class="T---Table T---Body T---Header">
<p><strong class="bold">fur</strong></p>
</td>
<td class="T---Table T---Body T---Header">
<p><strong class="bold">metal</strong></p>
</td>
<td class="T---Table T---Body T---Header">
<p><strong class="bold">plastic</strong></p>
</td>
<td class="T---Table T---Body T---Header">
<p><strong class="bold">wood</strong></p>
</td>
</tr>
</thead>
<tbody>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p>metal</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>1</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p>metal</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>1</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p>metal</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>1</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p>metal</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>1</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p>metal</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>1</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p>fur</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>1</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p>fur</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>1</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p>metal</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>1</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p>wood</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>1</p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p>wood</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>1</p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p>wood</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>1</p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p>ceramic</p>
</td>
<td class="T---Table T---Body T---Body">
<p>1</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p>plastic</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>1</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p>plastic</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>1</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p>metal</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>1</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p>wood</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>1</p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p>plastic</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>1</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p>wood</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>1</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 8.2 – One-hot encoding data structure for the Material category</p>
<p>This does cause some structural complications to our program. We must insert columns for each of our types, which replaces 3 columns with 14 new columns.</p>
<p>I’ve found <a id="_idIndexMarker622"/>two functions that we can use to convert <a id="_idIndexMarker623"/>text categories into one-hot encoded multiple columns:</p>
<ul>
<li>One is <code>OneHotEncoder</code>, which is part of <code>scikit-learn</code>. It is used like <code>LabelEncoder</code> – in fact, you must use both functions at the same time. You have to convert the string data to numeric form with <code>LabelEncoder</code> and then apply <code>OneHotEncoder</code> to convert that to the one-bit-per-value form that we want.</li>
<li>The simpler way is with a pandas function called <code>get_dummies()</code>. The name is apparently because we are creating dummy values to replace a string with numbers. It does perform the same function. The steps involved are quite a bit simpler than using the <code>OneHotEncoder</code> process, so that will be the one in our example.</li>
</ul>
<p>Let’s look at the steps we need to follow to implement this:</p>
<ol>
<li value="1">The top header section is the same as before – we have the same imports:<pre class="source-code">
# decision tree classifier
# with One Hot Encoding and Gini criteria #
# Author: Francis X Govers III #
# Example from book "Artificial Intelligence for Robotics" #
from sklearn import tree
import numpy as np
import pandas as pd
import sklearn.preprocessing as preproc
import graphviz</pre></li> <li>We will begin by reading in our table as before. I added an extra column at my end called <code>Toy Name</code> so I could keep track of which toy is which. We don’t need this column for the decision tree, so we can take it out with the pandas <code>del</code> function by specifying the name of the column to remove:<pre class="source-code">
toyData = pd.read_csv("toy_classifier_tree.csv")
del toyData["Toy Name"]   # we don't need this for now</pre></li> <li>Now, we are going to create a list of the columns we are going to remove and replace from the pandas <code>dataTable</code>. These are the <code>Color</code>, <code>Soft</code>, and <code>Material</code> columns. I used the term <em class="italic">Soft</em> to identify toys that were soft and squished easily (as compared to hard plastic or metal) because that is a separate criterion we may need for using our robot hand. We generate the dummy values and <a id="_idIndexMarker624"/>replace the 3 columns with 18 new columns. pandas automatically names the columns with a combination of the old <a id="_idIndexMarker625"/>column name and the value. For example, the single <code>Color</code> column is replaced by <code>Color_white</code>, <code>Color_blue</code>, <code>Color_green</code>, and so on:<pre class="source-code">
textCols = ['Color','Soft','Material']
toyData = pd.get_dummies(toyData,columns=textCo<a id="_idTextAnchor262"/>ls)</pre></li> <li>I put a <code>print</code> statement here just to check that everything got assembled correctly. It is optional. I’ve been really impressed with pandas for data tables – there is a lot of capability there to do database-type functions and data analysis:<pre class="source-code">
print toyData</pre></li> <li>Now, we are ready to generate our decision tree. We instantiate the object and call it <code>dTree</code>, setting the classification criteria to Gini. We then extract the data values from our <code>toyData</code> dataframe, and put the class values in the first (0th) column into the <code>classValues</code> variable, using array slicing operators:<pre class="source-code">
dTree = tree.DecisionTreeClassifier(criterion ="gini")
dataValues=toyData.values[:,1:]
classValues = toyData.values[:,0]</pre></li> <li>We still need to convert the class names into an enumerated type using <code>LabelEncoder</code>, just as we did in the previous two examples. We don’t need to one-hot encode. Each class represents an end state for our classification example – the leaves on our decision tree. If we were doing a neural network classifier, these would be our output neurons. One big difference is that when using a <a id="_idIndexMarker626"/>decision tree, the computer tells you what <a id="_idIndexMarker627"/>the criteria were that it used to classify and segregate items. With a neural network, it will do the classification but you have no way of knowing what criteria were used:<pre class="source-code">
lencoder = preproc.LabelEncoder()
lencoder.fit(classValues)
classes = lencoder.transform(classValues)</pre></li> <li>As we said, to use the class value names in the final output, we have to eliminate any duplicate names and sort them alphabetically. This pair of nested functions does that:<pre class="source-code">
classValues = list(sorted(set(classValu<a id="_idTextAnchor263"/>es)))</pre></li> <li>This is the conclusion of our program. Actually creating the decision tree only takes one line of code, now that we have set up all the data. We use the same steps as before, and then create the graphic with <code>graphviz</code> and save the image as a PDF. That was not hard at all – now that we have had all that practice setting this up:<pre class="source-code">
print ""
dTree = dTree.fit(dataValues,classes)
c_data=tree.export_graphviz(dTree,out_file=None,feature_names=toyData.columns,
class_names=classValues, filled = True, rounded=True,special_characters=True)
graph = graphviz.Source(c_data) graph.render("toy_decision_tree_graph_oneHot_gini")</pre></li> </ol>
<p>The result is the flowchart shown in the following figure. This output with one-hot encoding <a id="_idIndexMarker628"/>is a bit easier to read than <em class="italic">Figure 8</em><em class="italic">.4</em> because we <a id="_idIndexMarker629"/>can see the numbers in each category. You’ll note that each leaf (end node) has only one category with a count (two stuffed animals and three musical instruments):</p>
<div><div><img alt="Figure 8.5 – The output of the decision tree using one-hot encoding is much easier ﻿﻿﻿to read" src="img/B19846_08_5.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.5 – The output of the decision tree using one-hot encoding is much easier <a id="_idTextAnchor264"/><a id="_idTextAnchor265"/><a id="_idTextAnchor266"/>to read</p>
<p>Since we’ve been able to describe and make all sorts of decision trees, what would we have if we used a whole bunch of them? A forest! Let’s explore what this might look like.</p>
<h1 id="_idParaDest-137"><a id="_idTextAnchor267"/>Random forests</h1>
<p>I really wanted to add this section on <strong class="bold">random forest classifiers</strong>, but not just because the name <a id="_idIndexMarker630"/>sounds so cool. While I may have been accused of stretching metaphors to the breaking point, this time, the name may have inspired the name of this type of decision tree process. We have learned how to make decision trees, and we have learned that they have some weak points. It is best if the data really belongs to distinct and differentiated groups. They are not very tolerant of noise in the data. And they really gets unwieldy if you want to scale them up – you can imagine how big a graph would get with 200 classes rather than the 6 or 7 we were dealing with.</p>
<p>If you want to take advantage of the simplicity and utility of decision trees but want to handle more data, more uncertainty, and more classes, you can use a random forest, which, just as the name indicates, is just a whole batch of randomly generated decision trees. Let’s step through the process:</p>
<ol>
<li value="1">We collect our database of information but, instead of 18 rows in our database, we have 10,000 records or 1 million records. We subdivide this data into random sets – we generate 100 sets of data each <em class="italic">randomly</em> chosen from all of our data – and we put them in <em class="italic">random</em> order. We also pull out one set of data to use as a test set, just as we did for the neural networks.</li>
<li>Now, for each set of random data, we make a decision tree using the same process we have already learned.</li>
<li>Now, we have this collection of 100 classification engines, each generated from a different, randomly generated subset of data. We now test our random forest by taking data from the test set and running through all 100 of the trees in our forest. Each tree will provide an estimate of the classification of the data in our test record. If we are still classifying toys, then one of the trees would estimate that we are describing a toy car. Another may think it’s a musical instrument. We take each estimate and treat it as a vote. Then, the majority rules – the class that the majority of the trees selected is the winner. And that is all there is to it.</li>
</ol>
<p>The setup and program are just the same as what we did before, but you can’t draw a decision tree from a random forest, or just create a tree as an end in itself because that is not what a random forest does – if you just need a decision tree, you know how to do that. What you can do is to use a random forest like a neural network, as either a classification engine (to what class does this data belong?) or a regression engine that approximates a non-linear curve.</p>
<p>At this point, you can <a id="_idIndexMarker631"/>conclude with me that decision trees are really useful for a lot of things. But did you know you can navigate with them? The next section covers path planning for robots – using a different type of decisio<a id="_idTextAnchor268"/><a id="_idTextAnchor269"/><a id="_idTextAnchor270"/>n tree.</p>
<h1 id="_idParaDest-138"><a id="_idTextAnchor271"/>Introducing robot path planning</h1>
<p>In this section, we will be applying decision tree techniques to perform robot navigation. Some people <a id="_idIndexMarker632"/>like to refer to these as <strong class="bold">graph-based solutions</strong>, but any <a id="_idIndexMarker633"/>sort of navigation problem ends up being a decision tree. Consider as you drive your car, can you divide your navigation problems into a set of decisions – turn right, turn left, or go straight?</p>
<p>We are going to <a id="_idIndexMarker634"/>take what we have learned so far and press on to a problem related to classification, and that is <strong class="bold">grid searching</strong> and <strong class="bold">path finding</strong>. We will be learning <a id="_idIndexMarker635"/>about the famous and widely used <strong class="bold">A*</strong> (pronounced <strong class="bold">A-star</strong>) algorithm. This will start with grid navigation methods, topological path finding, such as GPS route finding, and finally, expert systems. You will see that these are all versions and variations on the topic of decision trees that we have already learned.</p>
<p>Some problems and datasets, particularly in robotics, lend themselves to a grid-based solution as a simplification of the navigation problem. It makes a lot of sense that, if we were trying to plot a path around a house or through a field for a robot, we would divide the ground into some sort of checkerboard grid and use that to plot coordinates that the robot can drive to. We could use latitude and longitude, or we could pick some reference point as zero – such as our starting position – and measure off some rectangular grid relative to the robot. The grid serves the same purpose in chess, limiting the number of positions under consideration for potential future movement and limiting and delineating our possible paths through the space.</p>
<p>While this section deals with gridded path finding, regardless of whether maps are involved or not, there are robot navigation paradigms that don’t use maps and even some that don’t use grids, or use grids with uneven spacing. I’ve designed robot navigation systems with multiple-layer maps where some layers were mutable – changeable – and some were not. This is a rich and fertile ground for imagination and experimentation, and I recommend further research if you find this topic interesting. For now, let’s start with a description of the coordinate system we’ll be using.</p>
<h2 id="_idParaDest-139"><a id="_idTextAnchor272"/>Understanding the coordinate system</h2>
<p>Let’s get back to the topic at hand. We have a robot and room that is roughly rectangular, and within that rectangle are also some roughly rectangular obstacles in the form of furniture, chairs, bookcases, a fireplace, and so on. It is a simple concept to consider that <a id="_idIndexMarker636"/>we mark off a grid to represent this space and create an array of numbers that matches the physical room with a virtual room. We set our grid spacing at 1 cm – each grid square is 1 cm x 1 cm, giving us a grid with 580 x 490 squares or 284,200 squares. We represent each square by an unsigned integer in a 2D array in the robot’s memory.</p>
<p>Now, we are going <a id="_idIndexMarker637"/>to need some other data. We have a starting location and a goal location, specified as grid coordinates. We’ll put <code>0,0</code> for the grid in the nearest and leftmost corner of the room so that all our directions and angles will be positive. In the way I’ve drawn the room map for you in <em class="italic">Figure 8</em><em class="italic">.6</em>, that corner will always be the lower-left corner of our map. In standard <em class="italic">right-hand rule</em> notation, left turns are positive angles and right turns are negative. The <em class="italic">x</em> direction is horizontal and the <em class="italic">y</em> direction is vertical on the page. For the robot, the <em class="italic">x</em> axis is out the right side and the <em class="italic">y</em> axis is the direct<a id="_idTextAnchor273"/>ion of motion.</p>
<p>You may think it odd that I’m giving these details, but setting up the proper coordinate system is the first step in doing grid searches and path planning. We are using Cartesian coordinates indoors. We would use different rules outdoors with latitude and longitude. There, we might want to use <em class="italic">north-east-down</em> (north is positive, south is negative, east is positive, west is negative, the <em class="italic">z</em> axis is down, and the <em class="italic">x</em> axis is aligned on the robot with the direction of travel):</p>
<div><div><img alt="Figure 8.6 – Coordinate frames for Earth navigation and indoor navigation" src="img/B19846_08_6.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.6 – Coordinate frames for Earth navigation and indoor navigation</p>
<p>We will be looking at this room map in more detail later.</p>
<p>So, we have <a id="_idIndexMarker638"/>our grid and a coordinate system that we agree upon, or at least <a id="_idIndexMarker639"/>agree that we both understand. We also have a starting location and an ending location. Our objective is to determine the best path for the robot from the start to the finish point. And in between, we have to plan a path around any obstacles that may be in the way.</p>
<p>Next, we have to talk about knowledge.</p>
<h2 id="_idParaDest-140"><a id="_idTextAnchor274"/>Developing a map based on our knowledge</h2>
<p>There are <a id="_idIndexMarker640"/>basically two kinds of grid search <a id="_idIndexMarker641"/>and path finding routines:</p>
<ul>
<li><strong class="bold">A priori knowledge</strong>, where you know where everything is on the map</li>
<li><strong class="bold">A posteriori knowledge</strong>, where you don’t know where the obstacles are</li>
</ul>
<p>We will start in the easier position where we can do our path planning with perfect knowledge of the layout of the room – we have a map.</p>
<p>We really have three goals we are trying to achieve simultaneously with path planning:</p>
<ul>
<li>Reach our goal</li>
<li>Avoid obstacles</li>
<li>Take <a id="_idTextAnchor275"/>the shortest path</li>
</ul>
<p>We can talk <a id="_idIndexMarker642"/>about how we might go about this. We can start with our pencil at the start point and draw an imaginary line from our start to the goal. If there are <a id="_idIndexMarker643"/>no obstacles in the way, we are done. But wait – our pencil is a tiny line on paper. Our robot is somewhat chubbier – it has a significant width as it drives around. How do we judge whether the robot is going down some narrow passage that it won’t fit into? We need to modify our map!</p>
<p>We have our grid, or a piece of paper that represents the grid. We can draw on that grid the outlines of all the obstacles, to scale. We have two chairs, two tables, a fireplace, two ottomans, and four bookcases. We color in all the obstacles in the darkest black we can. Now, we get a lighter colored pencil – say a blue color – and draw an outline around all of the furniture that is half the width of the robot. Our robot is 32 cm wide, so half of that is 16 cm, a nice even number. Our grid is 1 cm per square, so we make a 16-square border around everything. It <a id="_idTextAnchor276"/>looks like this:</p>
<div><div><img alt="Figure 8.7 – Adding safety boundaries to obstacles helps prevent collisions" src="img/B19846_08_7.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.7 – Adding safety boundaries to obstacles helps prevent collisions</p>
<p>So, now our map has two colors – obstacles and a <em class="italic">keep-out</em> border. We are going to keep the <a id="_idIndexMarker644"/>center of the robot out of the keep-out zone, and then we will not hit anything. This should make sense. As for judging passages and <a id="_idIndexMarker645"/>doorways, if the keep-out zones touch on either side (so if there are no white squares left in the middle), then the robot is too big to pass. You can see this around the ottoman in the upper-left corner of the illustration.</p>
<p>We look at our line now. We need a way to write a computer algorithm that determines the white squares that the robot can pass through that gets us from the start point to the finish point.</p>
<p>Since we have the goal in Cartesian coordinates and we have our start spot, we can express the distance in a straight line from the start to the finish. If the start point is <code>x1, y1</code> and the finish point is <code>x2, y2</code>, then the distance is the square root of the sums of the difference between the points:</p>
<p><em class="italic">distance = sqrt(x2-x1)^2 + (</em><em class="italic">y2-y1)^2)</em></p>
<p>One approach for developing <a id="_idIndexMarker646"/>a path planning algorithm is to use a <strong class="bold">wavefront method</strong>. We know where the start is. We go out in every direction to the eight squares adjacent to the start point. If any of those hit an obstacle or keep-out zone, we throw it <a id="_idIndexMarker647"/>out as a possible path. We keep track of how we got to each square, which, in my illustration (<em class="italic">Figure 8</em><em class="italic">.8</em>), is indicated by the arrows. We use <a id="_idIndexMarker648"/>the information on how we got to the square because we don’t yet know where we are going next. Now, we take all the new squares and do the same thing again – grabbing one square, seeing which of its eight neighbors is a legal move, and then putting an arrow (or a pointer to the location of the previous square) in it to keep track of how we got there. We continue to do this until we get to our goal. We keep a record of the order of the squares we examined and follow the arrows backward to our starting point.</p>
<p>If more than one square has a path leading to the current square, then we take the closest one, which is to say the shortest path. We follow these predecessors all the way back to the starting point, and that is our path:</p>
<div><div><img alt="Figure 8.8 – The wavefront approach to path planning has very little math involved. Each figure is a step in the process, starting at the upper left and going across, then down" src="img/B19846_08_8.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.8 – The wavefront approach to path planning has very little math involved. Each figure is a step in the process, starting at the upper left and going across, then down</p>
<p>You will notice in this example that I allowed the robot to make diagonal turns to get from one <a id="_idIndexMarker649"/>square to another. I could have also specified that only right-angle turns are allowed, but that is not very efficient and is hard on the <a id="_idIndexMarker650"/>robot’s drive system. Only allowing right-angle turns simplifies the processing somewhat, since you only have to consider four neighbors around a <a id="_idTextAnchor277"/>square instead of eight.</p>
<p>Another approach <a id="_idIndexMarker651"/>for developing a path planning algorithm that would look promising is the <strong class="bold">Greedy Best-First</strong> approach. Instead of keeping a record and checking all of the grid points as we did in the wavefront method, we just keep the single best path square out of the eight we just tested. The measure we use to decide which square to keep is the one that is closest to our straight-line path. Another way of saying this is to say it’s the square that is closest to the goal. We remove squares that are blocked by obstacles, of course. The net result is we are considering a lot (really a lot!) fewer squares than the wavefront method of path planning:</p>
<div><div><img alt="Figure 8.9 – The aptly named “Greedy Best-First” algorithm is fast, but can get stuck" src="img/B19846_08_9.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.9 – The aptly named “Greedy Best-First” algorithm is fast, but can get stuck</p>
<p>Does the <a id="_idIndexMarker652"/>greedy technique work for all cases? Not really.</p>
<p>Why not? That seems <a id="_idIndexMarker653"/>a simple algorithm, and we are only considering legal moves. The problem is it can’t deal with a <strong class="bold">local minima</strong>. What is a local minima? It is a place on the map where the robot would have to go backward to find a good path. The easiest type of minima to visualize is a U-shaped area where the robot can get in but not back out. The Greedy Best-First algorithm is also not trying to find the shortest path, just a valid path:</p>
<div><div><img alt="Figure 8.10 – A “local minima” can occur when no straight path exists, and the robot will have to back up or reverse direction" src="img/B19846_08_10.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.10 – A “local minima” can occur when no straight path exists, and the robot will have to back up or reverse direction</p>
<p>If we want <a id="_idIndexMarker654"/>to find the shortest path, we need to do some more math.</p>
<p>A more <a id="_idIndexMarker655"/>systematic and mathematical way to approach finding the shortest path around obstacles for a grid search problem is the <strong class="bold">A* algorithm</strong>, first developed for Shakey the Robot.</p>
<h2 id="_idParaDest-141"><a id="_idTextAnchor278"/>Introducing the A* algorithm</h2>
<p>Honestly, you can’t really write a book about robotics without mentioning the A* algorithm. A* has its origins with <em class="italic">Shakey the Robot</em> at Stanford University back in 1968. This was <a id="_idIndexMarker656"/>one of the first map-navigating robots. Nils Nilsson and his team <a id="_idIndexMarker657"/>were trying to find a method to navigate Shakey around the hallways at Stanford and started trying different algorithms. The first was called <em class="italic">A1</em>, the second <em class="italic">A2</em>, and so forth. After several iterations, the team decided that a combination of techniques worked best. In computer science, A* means the letter A followed by anything else, and thus the A-star was named.</p>
<p>The concept of the A-star process is very much like what we have already been doing with our other path planners. Like the wavefront planner, we start by considering the neighbors around our starting location. We will compute an estimate for each square based on two factors: the distance from the starting location and the distance in a straight line to the goal. We are going to use these factors to find the path with the lowest cumulative cost. We calculate that cost by adding up the value for each grid square that is part of the path. The formula is as follows:</p>
<p><em class="italic">F(n) = g(n) + </em><em class="italic">h(n)</em></p>
<p>Here, <em class="italic">F(n)</em> refers to the contribution of this square to the path cost, <em class="italic">g(n)</em> represents the distance from this <a id="_idIndexMarker658"/>square from the start position along the path chosen (that is, the sum of the path cost), and <em class="italic">h(n)</em> is the straight line distance from this square to <a id="_idIndexMarker659"/>the goal, which is a heuristic or estimate of the distance remaining to the goal. Since we don’t know what other obstacles we have to go around later, we use this guess as a measuring stick to compare paths.</p>
<p>This value represents the cost or contribution of this square if it were a part of the final path. We will select the square to be part of the path that has the lowest combined cost. As with the wavefront planner, we keep track of the predecessor square or the square that was traversed before this one to reconstruct our path:</p>
<div><div><img alt="Figure 8.11 – The A-star computation uses the distance to start (G) and the distance to the goal (H)" src="img/B19846_08_11.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.11 – The A-star computation uses the distance to start (G) and the distance to the goal (H)</p>
<p>The preceding diagram illustrates the A* algorithm. Each square is evaluated based on the sum of the <a id="_idIndexMarker660"/>distance along a path back to the start (<em class="italic">G</em>), and an estimate <a id="_idIndexMarker661"/>of the remaining distance to the goal (<em class="italic">H</em>). The yellow squares represent the path selected so far.</p>
<p>Let’s illustrate how the A* algorithm works:</p>
<ol>
<li value="1">We keep a set of all the grid squares on the map we have computed values f<a id="_idTextAnchor279"/>or. We’ll call this <code>exploredMap</code>. Our map grid square object looks like this:<pre class="source-code">
# globals
mapLength = 1280
mapWidth = 1200
mapSize = mapLength*mapWidth
map = []</pre></li> <li>Now, we will fill in our map with zeros to initialize everything. We will define the <code>mapGridSquare</code> function later in the code – it creates our data structures:<pre class="source-code">
for ii in range(0, mapWidth):
    for jj in range(0,mapLength):
        mapSq = mapGridSquare()#defined later
        mapSq.position = [ii,jj]
        mapSq.sType =EMPTY</pre></li> <li>The next <a id="_idIndexMarker662"/>section creates all of the obstacles on the map. We put <a id="_idIndexMarker663"/>the location of which grid squares to <em class="italic">fill-in</em> or make impassable:<pre class="source-code">
# create obstacles
obstacles = [[1,1],[1,2],[1,3],[45,18],[32,15] …..[1000,233]]
# iterate through obstacles and mark on the map
for pos in obstacles:
    map[pos]. sType = OBSTACLE
pathGrid = []</pre></li> <li>Now, we declare our starting and ending positions:<pre class="source-code">
START = [322, 128]
GOAL = [938,523]
exploredMap = []
A_Star_navigation(start, goal, exploredMap, map)</pre></li> <li>In this section, we are creating our data structures to keep track of all of the computations we make. The <code>G</code> value is the computed distance from the start, and the <code>H</code> value is the estimated distance to the goal. <code>F</code> is just the sum of these two. We also create a function to compute these values:<pre class="source-code">
def mapGridSquare():
    def __init__(self):
        self.F_value = 0.0  #total of G and H
        self.G_value = 0.0  # distance to start
        self.H_value = 0.0  # distance to goal
        self.position=[0,0]   # grid location x and y
        self. predecessor =None   # pointer to previous square
        self.sType = PATH
    def compute(self, goal, start):
        self.G_value = distance(goal.position,self.position)
        self.H_value = distance(start.position,self.position
        self.F_value = self.G_value + self.H_value
        return self.F_value</pre></li> <li>We need a <a id="_idIndexMarker664"/>function to trace the path from the goal back to the start <a id="_idIndexMarker665"/>once we’ve completed the map computations. This function is called <code>reconstructPath</code>:<pre class="source-code">
def reconstructPath(current):
    totalPath=[current]
    done=False
    while not done:
        a_square = current.predecessor
        if a_square == None:  # at start position?
            done = True
        totalPath.append(a_square)
        current = a_square
    return totalPath</pre></li> <li>We <a id="_idIndexMarker666"/>create a <code>findMin</code> function to locate the grid block that we have explored with the lowest <code>F</code> score:<pre class="source-code">
def findMin(map):
    minmap = []
    for square in map:
        if minmap == []:
            minmap = square
            continue
        if square.F_value &lt; minmap.F_value:
            minmap = square
    return minmap</pre></li> <li>Then, we <a id="_idIndexMarker667"/>create the <code>navigation</code> function itself:<pre class="source-code">
def A_Star_navigation(start, goal, exploredMap, map):
    while len(exploredMap&gt;0):
        current = findMin(exploredMap)
        if current.position == goal.position:
            # we are done – we are at the goal
            return reconstructPath(current)
        neighbors = getNeighbors(current)</pre></li> <li>The <code>neighbors</code> function returns all the neighbors of the current square that are not marked as obstacles:<pre class="source-code">
        for a_square in neighbors:
            if a_square.predecessor == None:</pre></li> <li>We only compute each grid square once:<pre class="source-code">
                old_score = a_square.F_value
    score = a_square.compute(GOAL, START)</pre></li> <li>Now, we look <a id="_idIndexMarker668"/>for the square that has the lowest <code>G</code> value – that is, the one closest to the start:<pre class="source-code">
    if a_square.G_value &lt; current.G_value:
        a_square.predecessor = current
        current = a_square
        current.compute(GOAL, START)
        exploredMap.append(current)</pre></li> </ol>
<p>So, in this section, we’ve covered the A* approach to finding the shortest path on a map, given that <a id="_idIndexMarker669"/>we know where all of the obstacles are in advance. But what if we don’t? Another<a id="_idTextAnchor280"/><a id="_idTextAnchor281"/> method we can use is the D* algorithm.</p>
<h2 id="_idParaDest-142"><a id="_idTextAnchor282"/>Introducing the D* (D-star or dynamic A*) algorithm</h2>
<p>Earlier in the chapter, I talked about <em class="italic">a priori</em> knowledge. The A-star algorithm, for all its usefulness, requires that obstacles in the entire map be known in advance. What do we do if we <a id="_idIndexMarker670"/>are planning a movement into an unknown space, where we will create the map as we go along? If we have a robot with sensors, such as sonar or lidar, then the robot will be <a id="_idIndexMarker671"/>detecting and identifying obstacles as it goes. So, it must continually replan its route based on increasing information.</p>
<p>The A* process is only run one time to plan a route for a robot before it begins to move. <strong class="bold">D*</strong>, a dynamic replanning process, is constantly updating the robot’s <a id="_idTextAnchor283"/>path as new information becomes available.</p>
<p>The D* algorithm allows for replanning by adding some additional information to each grid square. You will remember that in A*, we had the <code>G</code> value (distance to the start along the path), and the <code>H</code> value (straight-line distance to the goal). D-star adds a tag to the square that can have several possible values:</p>
<ul>
<li>The square’s tag could be <code>NEW</code> for a new square that had never been explored before.</li>
<li>It could be <code>OPEN</code> for tags that have been evaluated and are being considered as part of the path.</li>
<li><code>CLOSED</code> is for squares that have been dropped from consideration.</li>
<li>The next two tags are <code>RAISED</code> and <code>LOWERED</code>. The <code>RAISED</code> flag is set if a sensor reading or additional information caused the cost of that square to increase, and <code>LOWERED</code> is the opposite. For <code>LOWERED</code> squares, we need to propagate the new path cost to the neighbors of the now lower-cost square, so that they can be re-evaluated. This may cause tags to change on the neighboring squares. <code>RAISED</code> squares have increased cost, and so may be dropped from the path, and <code>LOWERED</code> squares have reduced cost and may be added into the path.</li>
</ul>
<p class="callout-heading">Note</p>
<p class="callout">Keep in mind that changes in cost values ripple through the D* evaluation of paths like a wave as the path is backtracked all the way to the start when the values change.</p>
<p>Another major <a id="_idIndexMarker672"/>difference between D* and A* is that D* starts at the goal <a id="_idIndexMarker673"/>and works backward toward the start. This allows D* to know the exact cost to the target – it is using the actual path distance to the goal from the current position and not a heuristic or estimate of the distance to go, as A* did.</p>
<p>This is a good time to remind you that all these grid-searching techniques we just covered are still variations of decision trees. We are going from leaf to leaf – which we have been calling grid squares, but they are still leaves of a decision tree. We set some criteria for choosing which of several paths to take, which make branching paths. We are working toward some goal or endpoint in each case. I bring this up because, in the next section, we will combine decision trees and the type of<a id="_idTextAnchor284"/> path planning we learned from the A* and D* algorithm<a id="_idTextAnchor285"/>s to find a path through streets with a GPS.</p>
<h2 id="_idParaDest-143"><a id="_idTextAnchor286"/>GPS path finding</h2>
<p>I wanted <a id="_idIndexMarker674"/>to have the opportunity (since we have come this far) to talk just for a little bit about <strong class="bold">topological path planners</strong>. This is an alternative method to the grid-based techniques we used in the preceding sections. There are types <a id="_idIndexMarker675"/>of problems and types of navigation where a grid-based <a id="_idIndexMarker676"/>approach is not appropriate or would require astronomical amounts of detailed data that may <a id="_idTextAnchor287"/>not be available or practical in a small robot.</p>
<p>As an example, I wanted to talk about how your GPS in your car finds a route along streets to reach your destination. You must have wondered about how that box has enough information in its tiny brain to provide turn-by-turn directions from one place to another. You may have imagined, if you stopped to think about it, that the GPS was using the same map you were viewing on the LCD screen to determine where you need to go. You would also think that some sort of grid-based search took place, such as the A* algorithm we discussed in such detail. And you would be wrong.</p>
<p>The data that the GPS uses to plan a route does not look like a map at all. Instead, it is a <strong class="bold">topological network</strong> that shows <a id="_idIndexMarker677"/>how streets are interconnected. In format, it looks more like a database of vectors (which have a direction and a magnitude, or distance), rather than an <em class="italic">X, Y</em> gridded raster map made up of pixels. The database format also takes up a lot less room in the GPS internal storage. The streets are divided by <strong class="bold">nodes</strong> or points where roads intersect or change. Each node shows which streets are connected. The nodes are connected by <strong class="bold">links</strong>, which allow you to traverse the data from node to node. The links represent the roads and have a length, along with cost data about the quality of the road. The cost data is used to compute the desirability of the route. A limited access highway with a high-speed limit would have a low cost, and a small side street or dirt road with a lot of stop signs would have a high cost since that link is both less desirable and slower.</p>
<p>The technique that <a id="_idIndexMarker678"/>most GPS path planners use is called <strong class="bold">Dijkstra’s algorithm</strong>, after Edsger W. Dijkstra, from the Netherlands. He wanted to find the shortest path from Rotterdam to Groningen, back in 1956. His graph-based solution has withstood the test of time and is very commonly used for GPS routing. It’s not of any help to us for our robot, so you can research this on your own.</p>
<p>We use the same procedures with the GPS road network database as we would when working the A-star process on a grid map. We evaluate each node, and progress outward from our start node, choosing the path that takes us closest in the direction of our destination:</p>
<div><div><img alt="Figure 8.12 – A road-based network can be represented as a series of nodes (circles) and links (lines)" src="img/B19846_08_12.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.12 – A road-based network can be represented as a series of nodes (circles) and links (lines)</p>
<p>Many GPS <a id="_idIndexMarker679"/>systems also simultaneously try to backward-chain from the endpoint – the goal or destination – and try to meet somewhere <a id="_idIndexMarker680"/>in the middle. An amazing amount of work has gone into making our current crop of GPS systems small, lightweight, and reliable. Of course, they are de<a id="_idTextAnchor288"/>pendent on up-to-date information in the database.</p>
<h1 id="_idParaDest-144"><a id="_idTextAnchor289"/>Summary</h1>
<p>Well, this has been a very busy chapter. We covered the uses of decision trees for a variety of applications. The basic decision tree has leaves (nodes) and links, or branches, that each represent a decision or a change in a path. We learned about fishbone diagrams and root cause analysis, a special type of decision tree. We showed a method using <code>scikit-learn</code> to have the computer build a classification decision tree for us and create a usable graph. We discussed the concept of random forests, which are just an evolved form of using groups of decision trees to perform prediction or regression. Then, we got into graph search algorithms and path planners, spending some time on the A* (or A-star) algorithm, which is widely used for making routes and paths. For times when we do not have a map created in advance, the D* (or dynamic A-star) process can use dynamic replanning to continually adjust the robot’s path to reach its goal. Finally, we introduced topological graph path planning and discussed how GPS systems find a route for you to the coffee shop.</p>
<p>In our next chapter, we’ll be talking about giving your robot an artificial personality, <a id="_idTextAnchor290"/>by simulating emotions using a Monte Carlo model.</p>
<h1 id="_idParaDest-145"><a id="_idTextAnchor291"/>Questions</h1>
<ol>
<li value="1">What are the three ways to traverse a decision tree?</li>
<li>In the fishbone diagram example, how does one go about pruning the branches of the decision tree?</li>
<li>What is the role of the Gini evaluator in creating a classification?</li>
<li>In the toy classifier example using Gini indexing, which attributes of the toy were not used by the decision tree? Why not?</li>
<li>Which color for the toys was used as a criterion by one of the classification techniques we tried?</li>
<li>Give an example of label encoding and one-hot encoding for menu items at a restaurant.</li>
<li>In the A* algorithm, discuss the different ways that <code>G()</code> and <code>H()</code> are computed.</li>
<li>In the A* algorithm, why is <code>H()</code> considered a heuristic and <code>G()</code> is not? Also, in the D* algorithm, heuristics are not used. Why not?</li>
<li>In the D* algorithm, why is there a <code>R<a id="_idTextAnchor292"/>AISED</code> and <code>LOWERED</code> tag and not just a <code>CHANGED</code> flag?</li>
</ol>
<h1 id="_idParaDest-146"><a id="_idTextAnchor293"/>Further reading</h1>
<ul>
<li><em class="italic">Introduction to the A* </em><em class="italic">Algorithm</em>: <a href="https://www.redblobgames.com/pathfinding/a-star/introduction.html">https://www.redblobgames.com/pathfinding/a-star/introduction.html</a></li>
<li><em class="italic">Introduction to AI Robotics</em> by Robin R. Murphy, MIT Press, 2000</li>
<li><em class="italic">How Decision Tree Algorithm </em><em class="italic">Works</em>: <a href="https://dataaspirant.com/2017/01/30/how-decision-tree-algorithm-works/">https://dataaspirant.com/2017/01/30/how-decision-tree-algorithm-works/</a></li>
<li><em class="italic">Game Programming </em><em class="italic">Heuristics</em>: <a href="http://theory.stanford.edu/~amitp/GameProgramming/Heuristics.html">http://theory.stanford.edu/~amitp/GameProgramming/Heuristics.html</a></li>
<li><em class="italic">D*Lite Algorithm Blog (Project Fast Replanning)</em> by Sven Koening: <a href="http://idm-lab.org/project-a.html">http://idm-lab.org/project-a.html</a></li>
<li><em class="italic">Graph-Based Path Planning for Mobile Robots</em>, Dissertation by David Wooden, School of Electrical and Computer Engineering, Georgia Institute of Technology, December 2006</li>
<li><em class="italic">The Focused D* Algorithm for Real-Time Replanning</em> by Anthony Stentz: <a href="https://robotics.caltech.edu/~jwb/courses/ME132/handouts/Dstar_ijcai95.pdf">https://robotics.caltech.edu/~jwb/courses/ME132/handouts/Dstar_ijcai95.pdf</a></li>
</ul>
</div>
<div><div></div>
</div>
</body></html>