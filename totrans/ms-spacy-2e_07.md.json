["```py\nYou are an expert summarization system. Your task is to accept Text as input and summarize the Text in a concise way.\n{%- if max_n_words -%}\n{# whitespace #}\nThe summary must not, under any circumstances, contain more than {{ max_n_words }} words.\n{%- endif -%}\n{# whitespace #}\n{%- if prompt_examples -%}\n{# whitespace #}\nBelow are some examples (only use these as a guide):\n{# whitespace #}\n{%- for example in prompt_examples -%}\n{# whitespace #}\nText:\n'''\n{{ example.text }}\n'''\nSummary:\n'''\n{{ example.summary }}\n'''\n{# whitespace #}\n{%- endfor -%}\n{# whitespace #}\n{%- endif -%}\n{# whitespace #}\nHere is the Text that needs to be summarized:\n'''\n{{ text }}\n'''\nSummary:\n```", "```py\n{%- if max_n_words -%}\n{# whitespace #}\nThe summary must not, under any circumstances, contain more than {{ max_n_words }} words.\n{%- endif -%}\n```", "```py\n    [nlp]\n    lang = \"en\"\n    pipeline = [\"llm\"]\n    ```", "```py\n    [components]\n    [components.llm]\n    factory = \"llm\"\n    [components.llm.task]\n    @llm_tasks = \"spacy.Summarization.v1\"\n    examples = null\n    max_n_words = null\n    [components.llm.model]\n    @llm_models = \"spacy.Claude-2.v2\"\n    config = {\"max_tokens_to_sample\": 1024}\n    ```", "```py\n    from spacy_llm.util import assemble\n    nlp = assemble(\"config.cfg\")\n    content = \"\"\"\n    As we saw on Chapter 6, Language Modeling is the task of predicting the next token given the sequence of previous tokens.\n    [...]\n    Now that you know what LLMs are and how to interact with them, let's use a spacy-llm component in a pipeline. In the next section we're going to create a pipeline to summarize texts using a LLM.\n    \"\"\"\n    doc = nlp(content)\n    print(doc._.summary)\n    ```", "```py\n    'Here is a concise summary of the key points from the text:\\n\\nLanguage models predict the next token in a sequence. Pre-trained language models (PLMs) are trained in a self-supervised way to learn general representations of language. PLMs are fine-tuned for downstream tasks. Large language models (LLMs) like GPT-3 have billions of parameters and are trained on huge datasets. LLMs can perform a variety of tasks including translation, coding assistance, scientific writing, and legal analysis. However, LLMs require lots of compute resources, are slow, and can sometimes \"hallucinate\" plausible but incorrect information. We interact with LLMs using prompts that provide instructions, context, input data, and indicate the desired output format. Spacy-llm allows defining LLM components in spaCy pipelines using tasks to specify prompts and models to connect to the LLM. The text then explains we will create a pipeline to summarize text using a LLM component.'\n    ```", "```py\nQuote: We must balance conspicuous consumption with conscious capitalism.\nContext: Business ethics.\n```", "```py\nYou are an expert at extracting context from text.\nYour tasks is to accept a quote as input and provide the context of the quote.\nThis context will be used to group the quotes together.\nDo not put any other text in your answer and provide the context in 3 words max. The quote should have one context only.\n{# whitespace #}\n{# whitespace #}\nHere is the quote that needs classification\n{# whitespace #}\n{# whitespace #}\nQuote:\n'''\n{{ text }}\n```", "```py\n    from pathlib import Path\n    from spacy_llm.registry import registry\n    import jinja2\n    from typing import Iterable\n    from spacy.tokens import Doc\n    TEMPLATE_DIR = Path(\"templates\")\n    ```", "```py\n    def read_template(name: str) -> str:\n        \"\"\"Read the text from a Jinja template using pathlib\"\"\"\n        path = TEMPLATE_DIR / f\"{name}.jinja\"\n        if not path.exists():\n            raise ValueError(f\"{name} is not a valid template.\")\n        return path.read_text()\n    ```", "```py\n    class QuoteContextExtractTask:\n        def __init__(self, template: str = \"quotecontextextract\",\n                     field: str = \"context\"):\n            self._template = read_template(template)\n            self._field = field\n    ```", "```py\n    def generate_prompts(self, docs: Iterable[Doc]) -> Iterable[str]:\n        environment = jinja2.Environment()\n        _template = environment.from_string(self._template)\n        for doc in docs:\n            prompt = _template.render(\n                text=doc.text,\n            )\n            yield prompt\n    ```", "```py\n      def _check_doc_extension(self):\n          \"\"\"Add extension if need be.\"\"\"\n          if not Doc.has_extension(self._field):\n              Doc.set_extension(self._field, default=None)\n        def parse_responses(\n            self, docs: Iterable[Doc], responses: Iterable[str]\n        ) -> Iterable[Doc]:\n            self._check_doc_extension()\n            for doc, prompt_response in zip(docs, responses):\n                try:\n                    setattr(\n                        doc._,\n                        self._field,\n                        prompt_response[0].strip(),\n                    )\n                except ValueError:\n                    setattr(doc._, self._field, None)\n                yield doc\n    ```", "```py\n    @registry.llm_tasks(\"my_namespace.QuoteContextExtractTask.v1\")\n    def make_quote_extraction() -> \"QuoteContextExtractTask\":\n        return QuoteContextExtractTask()\n    ```", "```py\nfrom pathlib import Path\nfrom spacy_llm.registry import registry\nimport jinja2\nfrom typing import Iterable\nfrom spacy.tokens import Doc\nTEMPLATE_DIR = Path(\"templates\")\ndef read_template(name: str) -> str:\n    \"\"\"Read the text from a Jinja template using pathlib\"\"\"\n    path = TEMPLATE_DIR / f\"{name}.jinja\"\n    if not path.exists():\n        raise ValueError(f\"{name} is not a valid template.\")\n    return path.read_text()\n@registry.llm_tasks(\"my_namespace.QuoteContextExtractTask.v1\")\ndef make_quote_extraction() -> \"QuoteContextExtractTask\":\n    return QuoteContextExtractTask()\nclass QuoteContextExtractTask:\n    def __init__(self, template: str = \"quote_context_extract\",\n                 field: str = \"context\"):\n        self._template = read_template(template)\n        self._field = field\n    def generate_prompts(self, \n        docs: Iterable[Doc]) -> Iterable[str]:\n        environment = jinja2.Environment()\n        _template = environment.from_string(self_template)\n        for doc in docs:\n            prompt = _template.render(\n                text=doc.text,\n            )\n            yield prompt\n    def _check_doc_extension(self):\n        \"\"\"Add extension if need be.\"\"\"\n        if not Doc.has_extension(self._field):\n            Doc.set_extension(self._field, default=None)\n  def parse_responses(\n      self, docs: Iterable[Doc], responses: Iterable[str]\n  ) -> Iterable[Doc]:\n        self._check_doc_extension()\n        for doc, prompt_response in zip(docs, responses):\n            try:\n                setattr(\n                    doc._,\n                    self._field,\n                    prompt_response[0].strip(),\n                ),\n            except ValueError:\n                setattr(doc._, self._field, None)\n            yield doc\n```", "```py\n    [nlp]\n    lang = \"en\"\n    pipeline = [\"llm\"]\n    [components]\n    [components.llm]\n    factory = \"llm\"\n    [components.llm.task]\n    @llm_tasks = \"my_namespace.QuoteContextExtractTask.v1\"\n    [components.llm.model]\n    @llm_models = \"spacy.Claude-2.v2\"\n    config = {\"max_tokens_to_sample\": 1024}\n    ```", "```py\n    from spacy_llm.util import assemble\n    from quote import QuoteContextExtractTask\n    nlp = assemble(\"config_custom_task.cfg\")\n    quote = \"Life isn't about getting and having, it's about giving and being.\"\n    doc = nlp(quote)\n    print(\"Context:\", doc._.context)\n    >>> Context: self-improvement\n    ```"]