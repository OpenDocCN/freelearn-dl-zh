<html><head></head><body>
		<div id="_idContainer042" class="calibre2">
			<h1 id="_idParaDest-223" class="chapter-number"><a id="_idTextAnchor231" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.1.1">9</span></h1>
			<h1 id="_idParaDest-224" class="calibre7"><a id="_idTextAnchor232" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.2.1">Natural Language Understanding</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.3.1">In this chapter, we will explore recipes that will allow us to interpret and understand the text contained in short as well as</span><a id="_idIndexMarker482" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.4.1"> long passages. </span><strong class="bold"><span class="kobospan" id="kobo.5.1">Natural language understanding</span></strong><span class="kobospan" id="kobo.6.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.7.1">NLU</span></strong><span class="kobospan" id="kobo.8.1">) is a very broad term and the various systems developed as part of NLU do not interpret or understand a passage of text the same way a human reader would. </span><span class="kobospan" id="kobo.8.2">However, based on the specificity of the task, we can create some applications that can be combined to generate an interpretation or understanding that can be used to solve a given problem related to </span><span><span class="kobospan" id="kobo.9.1">text processing.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.10.1">Organizations that have a huge document corpus need a seamless way to search through documents. </span><span class="kobospan" id="kobo.10.2">More specifically, what users really need is an answer to a specific question without having to glean through a list of documents that are returned as part of a document search. </span><span class="kobospan" id="kobo.10.3">Users would prefer the query to be formulated as a question in natural language and the answer to be emitted in the </span><span><span class="kobospan" id="kobo.11.1">same manner.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.12.1">Another set of applications is that of document summarization and text entailment. </span><span class="kobospan" id="kobo.12.2">While processing a large set of documents, it is helpful if the document length can be shortened without the loss of meaning or context. </span><span class="kobospan" id="kobo.12.3">Additionally, it’s important to determine whether the information contained in the document at the sentence level </span><span><span class="kobospan" id="kobo.13.1">entails itself.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.14.1">While we work on processing and classifying the documents, there are always challenges in understanding why or how the model assigns a label to a piece of text – more specifically, what parts of the text contribute to the </span><span><span class="kobospan" id="kobo.15.1">different labels.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.16.1">This chapter will cover different techniques to explore the various aspects previously described. </span><span class="kobospan" id="kobo.16.2">We will follow recipes that will allow us to perform these tasks and understand the underlying building blocks that help us achieve the </span><span><span class="kobospan" id="kobo.17.1">end goals.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.18.1">As part of this chapter, we will build recipes for the </span><span><span class="kobospan" id="kobo.19.1">following tasks:</span></span></p>
			<ul class="calibre15">
				<li class="calibre14"><span class="kobospan" id="kobo.20.1">Answering questions from a short </span><span><span class="kobospan" id="kobo.21.1">text passage</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.22.1">Answering questions from a long </span><span><span class="kobospan" id="kobo.23.1">text passage</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.24.1">Answering questions from a document corpus in an </span><span><span class="kobospan" id="kobo.25.1">extractive manner</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.26.1">Answering questions from a document corpus in an </span><span><span class="kobospan" id="kobo.27.1">abstractive manner</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.28.1">Summarizing text using pretrained models based </span><span><span class="kobospan" id="kobo.29.1">on Transformers</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.30.1">Detecting </span><span><span class="kobospan" id="kobo.31.1">sentence entailment</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.32.1">Enhancing explainability via a </span><span><span class="kobospan" id="kobo.33.1">classifier-invariant approach</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.34.1">Enhancing explainability via </span><span><span class="kobospan" id="kobo.35.1">text generation</span></span><a id="_idTextAnchor233" class="calibre6 pcalibre pcalibre1"/><a id="_idTextAnchor234" class="calibre6 pcalibre pcalibre1"/></li>
			</ul>
			<h1 id="_idParaDest-225" class="calibre7"><a id="_idTextAnchor235" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.36.1">Technical requirements</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.37.1">The code for this chapter is in a folder named </span><strong class="source-inline"><span class="kobospan" id="kobo.38.1">Chapter9</span></strong><span class="kobospan" id="kobo.39.1"> in the GitHub repository of the </span><span><span class="kobospan" id="kobo.40.1">book (</span></span><a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter09" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.41.1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter09</span></span></a><span><span class="kobospan" id="kobo.42.1">).</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.43.1">As in previous chapters, the packages required for this chapter are part of the </span><strong class="source-inline"><span class="kobospan" id="kobo.44.1">poetry</span></strong><span class="kobospan" id="kobo.45.1"> environment. </span><span class="kobospan" id="kobo.45.2">Alternatively, you can install all the packages using the </span><span><strong class="source-inline"><span class="kobospan" id="kobo.46.1">requirements.txt</span></strong></span><span><span class="kobospan" id="kobo.47.1"> file.</span></span></p>
			<h1 id="_idParaDest-226" class="calibre7"><a id="_idTextAnchor236" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.48.1">Answering questions from a short text passage</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.49.1">To get started with question </span><a id="_idIndexMarker483" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.50.1">answering, we will start with </span><a id="_idIndexMarker484" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.51.1">a simple recipe that can answer a question from a </span><span><span class="kobospan" id="kobo.52.1">short passage.</span></span></p>
			<h2 id="_idParaDest-227" class="calibre5"><a id="_idTextAnchor237" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.53.1">Getting ready</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.54.1">As part of this chapter, we will use the libraries from Hugging Face (</span><a href="http://huggingface.co" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.55.1">huggingface.co</span></a><span class="kobospan" id="kobo.56.1">). </span><span class="kobospan" id="kobo.56.2">For this recipe, we will use the </span><strong class="source-inline"><span class="kobospan" id="kobo.57.1">BertForQuestionAnswering</span></strong><span class="kobospan" id="kobo.58.1"> and </span><strong class="source-inline"><span class="kobospan" id="kobo.59.1">BertTokenizer</span></strong><span class="kobospan" id="kobo.60.1"> modules from the Transformers package. </span><span class="kobospan" id="kobo.60.2">The </span><strong class="source-inline"><span class="kobospan" id="kobo.61.1">BertForQuestionAnswering</span></strong><span class="kobospan" id="kobo.62.1"> model uses the base BERT large uncased model that was trained on the </span><strong class="source-inline"><span class="kobospan" id="kobo.63.1">SQuAD</span></strong><span class="kobospan" id="kobo.64.1"> dataset and fine-tuned for the question-answering task. </span><span class="kobospan" id="kobo.64.2">This pre-trained model can be used to load a text passage and answer questions based on the contents of the passage. </span><span class="kobospan" id="kobo.64.3">You can use the </span><strong class="source-inline"><span class="kobospan" id="kobo.65.1">9.1_question_answering.ipynb</span></strong><span class="kobospan" id="kobo.66.1"> notebook from the code site if you need to work from an </span><span><span class="kobospan" id="kobo.67.1">existing notebook.</span></span></p>
			<h2 id="_idParaDest-228" class="calibre5"><a id="_idTextAnchor238" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.68.1">How to do it...</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.69.1">In this recipe, we will load a pretrained model that has been trained on the SQuAD </span><span><span class="kobospan" id="kobo.70.1">dataset (</span></span><a href="https://huggingface.co/datasets/squad" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.71.1">https://huggingface.co/datasets/squad</span></span></a><span><span class="kobospan" id="kobo.72.1">).</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.73.1">The recipe does the </span><span><span class="kobospan" id="kobo.74.1">following things:</span></span></p>
			<ul class="calibre15">
				<li class="calibre14"><span class="kobospan" id="kobo.75.1">It initializes a question-answering pipeline based on the pre-trained </span><strong class="source-inline1"><span class="kobospan" id="kobo.76.1">BertForQuestionAnswering</span></strong><span class="kobospan" id="kobo.77.1"> model and </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.78.1">BertTokenizer</span></strong></span><span><span class="kobospan" id="kobo.79.1"> tokenizer.</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.80.1">It further</span><a id="_idIndexMarker485" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.81.1"> initializes a context passage and a question and emits the output of the answer based on these two parameters. </span><span class="kobospan" id="kobo.81.2">It also prints the exact text of </span><span><span class="kobospan" id="kobo.82.1">the answer.</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.83.1">It asks a follow-up question to the same pipeline by just changing the question text, and prints the </span><a id="_idIndexMarker486" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.84.1">exact text answer to </span><span><span class="kobospan" id="kobo.85.1">the question.</span></span></li>
			</ul>
			<p class="calibre3"><span class="kobospan" id="kobo.86.1">The steps for the recipe are </span><span><span class="kobospan" id="kobo.87.1">as follows:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.88.1">Do the necessary imports to import the necessary types and functions from the </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.89.1">datasets</span></strong></span><span><span class="kobospan" id="kobo.90.1"> package:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.91.1">
from transformers import (
    pipeline, BertForQuestionAnswering, BertTokenizer)
import torch</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.92.1">In this step, we initialize the model and tokenizer, respectively, using the pre-trained </span><strong class="source-inline1"><span class="kobospan" id="kobo.93.1">bert-large-uncased-whole-word-masking-finetuned-squad</span></strong><span class="kobospan" id="kobo.94.1"> artifacts. </span><span class="kobospan" id="kobo.94.2">These will be downloaded from the Hugging Face website if they are not present locally on the machine as part of these calls. </span><span class="kobospan" id="kobo.94.3">We have chosen the specific model and tokenizer for our recipe, but feel free to explore other models on the Hugging Face site that might suit your needs. </span><span class="kobospan" id="kobo.94.4">As a generic step for this and the following recipe, we discover whether there are any GPU devices in the system and attempt to use them. </span><span class="kobospan" id="kobo.94.5">If a GPU is not detected, we use the </span><span><span class="kobospan" id="kobo.95.1">CPU instead:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.96.1">
device = torch.device("cuda" if torch.cuda.is_available() 
    else "cpu")
qa_model = BertForQuestionAnswering.from_pretrained(
    'bert-large-uncased-whole-word-masking-finetuned-squad',
    device_map=device)
qa_tokenizer = BertTokenizer.from_pretrained(
    'bert-large-uncased-whole-word-masking-finetuned-squad',
    device=device)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.97.1">In this step, we initialize a question-answering pipeline with the model and tokenizer. </span><span class="kobospan" id="kobo.97.2">The task type</span><a id="_idIndexMarker487" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.98.1"> for this pipeline is </span><a id="_idIndexMarker488" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.99.1">set </span><span><span class="kobospan" id="kobo.100.1">to </span></span><span><strong class="source-inline1"><span class="kobospan" id="kobo.101.1">question-answering</span></strong></span><span><span class="kobospan" id="kobo.102.1">:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.103.1">
question_answer_pipeline = pipeline(
    "question-answering", model=qa_model,
    tokenizer=qa_tokenizer)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.104.1">In this step, we initialize a </span><strong class="source-inline1"><span class="kobospan" id="kobo.105.1">context</span></strong><span class="kobospan" id="kobo.106.1"> passage. </span><span class="kobospan" id="kobo.106.2">This passage was generated as part of our </span><em class="italic"><span class="kobospan" id="kobo.107.1">Text Generation via Transformers</span></em><span class="kobospan" id="kobo.108.1"> example in </span><a href="B18411_08.xhtml#_idTextAnchor205" class="calibre6 pcalibre pcalibre1"><span><em class="italic"><span class="kobospan" id="kobo.109.1">Chapter 8</span></em></span></a><span class="kobospan" id="kobo.110.1">.  </span><span class="kobospan" id="kobo.110.2">It’s entirely acceptable if you want to use a </span><span><span class="kobospan" id="kobo.111.1">different passage:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.112.1">
context = "The cat had no business entering the neighbors garage, but she was there to help. </span><span class="kobospan1" id="kobo.112.2">The neighbor, who asked not to be identified, said she didn't know what to make of the cat's behavior. </span><span class="kobospan1" id="kobo.112.3">She said it seemed like it was trying to get into her home, and that she was afraid for her life. </span><span class="kobospan1" id="kobo.112.4">The neighbor said that when she went to check on her cat, it ran into the neighbor's garage and hit her in the face, knocking her to the ground."</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.113.1">In this step, we initialize a question text, invoke the pipeline with the context and question, and store the result in a variable. </span><span class="kobospan" id="kobo.113.2">The type of the result is a Python </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.114.1">dict</span></strong></span><span><span class="kobospan" id="kobo.115.1"> object:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.116.1">
question = "Where was the cat trying to enter?"
</span><span class="kobospan1" id="kobo.116.2">result = question_answer_pipeline(question=question, 
    context=context)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.117.1">In this step, we print the value of the result. </span><span class="kobospan" id="kobo.117.2">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.118.1">score</span></strong><span class="kobospan" id="kobo.119.1"> value shows the probability of the</span><a id="_idIndexMarker489" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.120.1"> answer. </span><span class="kobospan" id="kobo.120.2">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.121.1">start</span></strong><span class="kobospan" id="kobo.122.1"> and </span><strong class="source-inline1"><span class="kobospan" id="kobo.123.1">end</span></strong><span class="kobospan" id="kobo.124.1"> values denote the start and end character indices in the </span><strong class="source-inline1"><span class="kobospan" id="kobo.125.1">context</span></strong><span class="kobospan" id="kobo.126.1"> passage that constitute the answer. </span><span class="kobospan" id="kobo.126.2">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.127.1">answer</span></strong><span class="kobospan" id="kobo.128.1"> value denotes the actual text of </span><span><span class="kobospan" id="kobo.129.1">the answer:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.130.1">
print(result)</span></pre></li>			</ol>
			<pre class="console"><span class="kobospan1" id="kobo.131.1">
{'score': 0.25, 'start': 33, 'end': 54, 'answer': 'the neighbors garage,'}</span></pre>			<ol class="calibre13">
				<li value="7" class="calibre14"><span class="kobospan" id="kobo.132.1">In this step,  we print</span><a id="_idIndexMarker490" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.133.1"> the exact text answer. </span><span class="kobospan" id="kobo.133.2">This is present in the </span><strong class="source-inline1"><span class="kobospan" id="kobo.134.1">answer</span></strong><span class="kobospan" id="kobo.135.1"> key in the </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.136.1">result</span></strong></span><span><span class="kobospan" id="kobo.137.1"> dictionary:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.138.1">
print(result['answer'])</span></pre></li>			</ol>
			<pre class="console"><span class="kobospan1" id="kobo.139.1">
the neighbors garage,</span></pre>			<ol class="calibre13">
				<li value="8" class="calibre14"><span class="kobospan" id="kobo.140.1">In this step, we ask another question using the same context and print </span><span><span class="kobospan" id="kobo.141.1">the result:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.142.1">
question = "What did the cat do after entering the garage"
result = question_answer_pipeline(
    question=question, context=context)
print(result['answer'])</span></pre></li>			</ol>
			<pre class="console"><span class="kobospan1" id="kobo.143.1">
hit her in the face, knocking her to the</span><a id="_idTextAnchor239" class="pcalibre pcalibre1 calibre20"/><span class="kobospan1" id="kobo.144.1"> ground.</span></pre>			<h1 id="_idParaDest-229" class="calibre7"><a id="_idTextAnchor240" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.145.1">Answering questions from a long text passage</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.146.1">In the previous recipe, we learned an </span><a id="_idIndexMarker491" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.147.1">approach to extract the answer to a question, given a context. </span><span class="kobospan" id="kobo.147.2">This pattern involves the model retrieving the answer from the given context. </span><span class="kobospan" id="kobo.147.3">The model cannot answer a question that is not contained in the </span><a id="_idIndexMarker492" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.148.1">context. </span><span class="kobospan" id="kobo.148.2">This does serve a purpose where we want an answer from a given context. </span><span class="kobospan" id="kobo.148.3">This type of question-answering system is</span><a id="_idIndexMarker493" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.149.1"> defined as </span><strong class="bold"><span class="kobospan" id="kobo.150.1">Closed Domain Question </span></strong><span><strong class="bold"><span class="kobospan" id="kobo.151.1">Answering</span></strong></span><span><span class="kobospan" id="kobo.152.1"> (</span></span><span><strong class="bold"><span class="kobospan" id="kobo.153.1">CDQA</span></strong></span><span><span class="kobospan" id="kobo.154.1">).</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.155.1">There is another system of question answering that can answer questions that are general in nature. </span><span class="kobospan" id="kobo.155.2">These systems are trained on larger corpora. </span><span class="kobospan" id="kobo.155.3">This training provides them with the ability to answer questions that</span><a id="_idIndexMarker494" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.156.1"> are open in nature. </span><span class="kobospan" id="kobo.156.2">These systems are called </span><strong class="bold"><span class="kobospan" id="kobo.157.1">Open Domain Question Answering</span></strong><span class="kobospan" id="kobo.158.1"> (</span><span><strong class="bold"><span class="kobospan" id="kobo.159.1">ODQA</span></strong></span><span><span class="kobospan" id="kobo.160.1">) systems.</span></span></p>
			<h2 id="_idParaDest-230" class="calibre5"><a id="_idTextAnchor241" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.161.1">Getting ready</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.162.1">As part of this recipe, we will </span><a id="_idIndexMarker495" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.163.1">use the </span><strong class="bold"><span class="kobospan" id="kobo.164.1">DeepPavlov</span></strong><span class="kobospan" id="kobo.165.1"> (</span><a href="https://deeppavlov.ai" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.166.1">https://deeppavlov.ai</span></a><span class="kobospan" id="kobo.167.1">) ODQA system to answer an open question. </span><span class="kobospan" id="kobo.167.2">We will use the </span><strong class="source-inline"><span class="kobospan" id="kobo.168.1">deeppavlov</span></strong><span class="kobospan" id="kobo.169.1"> library along with the </span><strong class="bold"><span class="kobospan" id="kobo.170.1">Knowledge Base Question Answering</span></strong><span class="kobospan" id="kobo.171.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.172.1">KBQA</span></strong><span class="kobospan" id="kobo.173.1">) model. </span><span class="kobospan" id="kobo.173.2">This model has </span><a id="_idIndexMarker496" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.174.1">been trained on English wiki data as a knowledge base. </span><span class="kobospan" id="kobo.174.2">It uses various NLP techniques such as entity linking and disambiguation, knowledge graphs, and so on to extract the exact answer to </span><span><span class="kobospan" id="kobo.175.1">the question.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.176.1">This recipe needs a few </span><a id="_idIndexMarker497" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.177.1">steps to set up the right environment for its execution. </span><span class="kobospan" id="kobo.177.2">The </span><strong class="source-inline"><span class="kobospan" id="kobo.178.1">poetry</span></strong><span class="kobospan" id="kobo.179.1"> file for this recipe is in the </span><strong class="source-inline"><span class="kobospan" id="kobo.180.1">9.2_QA_on_long_passages</span></strong><span class="kobospan" id="kobo.181.1"> folder. </span><span class="kobospan" id="kobo.181.2">We will also need to install and download the document corpus by performing the </span><span><span class="kobospan" id="kobo.182.1">following command:</span></span></p>
			<pre class="console"><span class="kobospan1" id="kobo.183.1">
python -m deeppavlov install kbqa_cq_en</span></pre>			<p class="calibre3"><span class="kobospan" id="kobo.184.1">You can also use the </span><strong class="source-inline"><span class="kobospan" id="kobo.185.1">9.2_QA_on_long_passages.ipynb</span></strong><span class="kobospan" id="kobo.186.1"> notebook, which is contained within the </span><span><span class="kobospan" id="kobo.187.1">same folder.</span></span></p>
			<h2 id="_idParaDest-231" class="calibre5"><a id="_idTextAnchor242" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.188.1">How to do it...</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.189.1">In this recipe, we will initialize the KBQA model based on the </span><strong class="source-inline"><span class="kobospan" id="kobo.190.1">DeepPavlov</span></strong><span class="kobospan" id="kobo.191.1"> library and use it to answer an open question. </span><span class="kobospan" id="kobo.191.2">The steps for the recipe are </span><span><span class="kobospan" id="kobo.192.1">as follows:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.193.1">Do the </span><span><span class="kobospan" id="kobo.194.1">necessary imports:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.195.1">
from deeppavlov import build_model</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.196.1">In this step, we initialize the KBQA model, </span><strong class="source-inline1"><span class="kobospan" id="kobo.197.1">kbqa_cq_en</span></strong><span class="kobospan" id="kobo.198.1">, which is passed to the </span><strong class="source-inline1"><span class="kobospan" id="kobo.199.1">build_model</span></strong><span class="kobospan" id="kobo.200.1"> method as an argument. </span><span class="kobospan" id="kobo.200.2">We also set the </span><strong class="source-inline1"><span class="kobospan" id="kobo.201.1">download</span></strong><span class="kobospan" id="kobo.202.1"> argument to </span><strong class="source-inline1"><span class="kobospan" id="kobo.203.1">True</span></strong><span class="kobospan" id="kobo.204.1"> so that the model is downloaded as well in case it is </span><span><span class="kobospan" id="kobo.205.1">missing locally:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.206.1">
kbqa_model = build_model('kbqa_cq_en', download=True)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.207.1">We use the initialized model and pass it a couple of questions that we want to </span><span><span class="kobospan" id="kobo.208.1">be answered:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.209.1">
result = kbqa_model(['What is the capital of Egypt?', 
    'Who is Bill Clinton\'s wife?'])</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.210.1">We print the result as returned by the model. </span><span class="kobospan" id="kobo.210.2">The result contains </span><span><span class="kobospan" id="kobo.211.1">three arrays.</span></span><p class="calibre3"><span class="kobospan" id="kobo.212.1">The first array contains the exact answers to the question ordered in the same way as the original input. </span><span class="kobospan" id="kobo.212.2">In this case, the answers </span><strong class="source-inline"><span class="kobospan" id="kobo.213.1">Cairo</span></strong><span class="kobospan" id="kobo.214.1"> and </span><strong class="source-inline"><span class="kobospan" id="kobo.215.1">Hillary Clinton</span></strong><span class="kobospan" id="kobo.216.1"> are in the same order as the questions they </span><span><span class="kobospan" id="kobo.217.1">pertain to.</span></span></p><p class="calibre3"><span class="kobospan" id="kobo.218.1">You might observe</span><a id="_idIndexMarker498" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.219.1"> some additional artifacts in the output. </span><span class="kobospan" id="kobo.219.2">These are internal identifiers that are generated by the library. </span><span class="kobospan" id="kobo.219.3">We have omitted them </span><span><span class="kobospan" id="kobo.220.1">for brevity:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.221.1">
[['Cairo', 'Hillary Clinton']]</span></pre></li>			</ol>
			<h2 id="_idParaDest-232" class="calibre5"><a id="_idTextAnchor243" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.222.1">See also</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.223.1">For more</span><a id="_idIndexMarker499" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.224.1"> information on the internal details of the working of DeepPavlov, please refer </span><span><span class="kobospan" id="kobo.225.1">to </span></span><a href="https://deeppavlov.ai" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.226.1">https</span><span id="_idTextAnchor244"/><span class="kobospan" id="kobo.227.1">://deeppavlov.ai</span></span></a><span><span class="kobospan" id="kobo.228.1">.</span></span></p>
			<h1 id="_idParaDest-233" class="calibre7"><a id="_idTextAnchor245" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.229.1">Answering questions from a document corpus in an extractive manner</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.230.1">For the use cases where we have a</span><a id="_idIndexMarker500" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.231.1"> document </span><a id="_idIndexMarker501" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.232.1">corpus that contains a large number of documents, it’s not feasible to load the document content at runtime to answer a question. </span><span class="kobospan" id="kobo.232.2">Such an approach would lead to long query times and would not be suitable for </span><span><span class="kobospan" id="kobo.233.1">production-grade systems.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.234.1">In this recipe, we will learn how to preprocess the documents and transform them into a form for faster reading, indexing, and retrieval that allows the system to extract the answer for a given question with</span><a id="_idTextAnchor246" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.235.1"> short </span><span><span class="kobospan" id="kobo.236.1">query times.</span></span></p>
			<h2 id="_idParaDest-234" class="calibre5"><a id="_idTextAnchor247" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.237.1">Getting ready</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.238.1">As part of this </span><a id="_idIndexMarker502" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.239.1">recipe, we will use the </span><strong class="bold"><span class="kobospan" id="kobo.240.1">Haystack</span></strong><span class="kobospan" id="kobo.241.1"> (</span><a href="https://haystack.deepset.ai/" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.242.1">https://haystack.deepset.ai/</span></a><span class="kobospan" id="kobo.243.1">) framework to build a </span><strong class="bold"><span class="kobospan" id="kobo.244.1">QA system</span></strong><span class="kobospan" id="kobo.245.1"> that can answer questions from a document corpus. </span><span class="kobospan" id="kobo.245.2">We will download a</span><a id="_idIndexMarker503" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.246.1"> dataset based on </span><em class="italic"><span class="kobospan" id="kobo.247.1">Game of Thrones</span></em><span class="kobospan" id="kobo.248.1"> and index it. </span><span class="kobospan" id="kobo.248.2">For our QA system to be performant, we will need to index the documents beforehand. </span><span class="kobospan" id="kobo.248.3">Once the documents are indexed, answering a question follows a </span><span><span class="kobospan" id="kobo.249.1">two-step process:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.250.1">Retriever</span></strong><span class="kobospan" id="kobo.251.1">: Since we have many</span><a id="_idIndexMarker504" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.252.1"> documents, scanning each document to fetch an answer is not a feasible approach. </span><span class="kobospan" id="kobo.252.2">We will first retrieve a set of candidate documents that can possibly contain an answer to our question. </span><span class="kobospan" id="kobo.252.3">This step is performed using a </span><strong class="source-inline1"><span class="kobospan" id="kobo.253.1">Retriever</span></strong><span class="kobospan" id="kobo.254.1"> component. </span><span class="kobospan" id="kobo.254.2">This searches through the pre-created index to filter the number of documents that we will need to scan to retrieve the </span><span><span class="kobospan" id="kobo.255.1">exact answer.</span></span></li>
				<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.256.1">Reader</span></strong><span class="kobospan" id="kobo.257.1">: Once we have a candidate set of documents that could contain the answer, we will </span><a id="_idIndexMarker505" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.258.1">search these documents to retrieve the exact answer to </span><span><span class="kobospan" id="kobo.259.1">our question.</span></span></li>
			</ol>
			<p class="calibre3"><span class="kobospan" id="kobo.260.1">We will discuss the details of these components throughout this recipe. </span><span class="kobospan" id="kobo.260.2">You can use the </span><strong class="source-inline"><span class="kobospan" id="kobo.261.1">9.3_QA_on_document_corpus.ipynb</span></strong><span class="kobospan" id="kobo.262.1"> notebook from the code site if you need to work from an existing notebook. </span><span class="kobospan" id="kobo.262.2">To start with, let’s set up </span><span><span class="kobospan" id="kobo.263.1">the prerequisites.</span></span></p>
			<h2 id="_idParaDest-235" class="calibre5"><a id="_idTextAnchor248" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.264.1">How to do it...</span></h2>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.265.1">In this step, we do the </span><span><span class="kobospan" id="kobo.266.1">necessary imports:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.267.1">
import os
from haystack.document_stores import InMemoryDocumentStore
from haystack.nodes import BM25Retriever, FARMReader
from haystack.pipelines import ExtractiveQAPipeline
from haystack.pipelines.standard_pipelines import( 
    TextIndexingPipeline)
from haystack.utils import (fetch_archive_from_http, 
    print_answers)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.268.1">In this step, we specify</span><a id="_idIndexMarker506" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.269.1"> a folder that will be used to save our dataset. </span><span class="kobospan" id="kobo.269.2">Then, we retrieve the dataset from the source. </span><span class="kobospan" id="kobo.269.3">The second parameter to the </span><strong class="source-inline1"><span class="kobospan" id="kobo.270.1">fetch_archive_from_http</span></strong><span class="kobospan" id="kobo.271.1"> method is the folder in which the dataset will be downloaded. </span><span class="kobospan" id="kobo.271.2">We set the parameter to the</span><a id="_idIndexMarker507" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.272.1"> folder that we defined in the first line. </span><span class="kobospan" id="kobo.272.2">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.273.1">fetch_archive_from_http</span></strong><span class="kobospan" id="kobo.274.1"> method decompresses the archive </span><strong class="source-inline1"><span class="kobospan" id="kobo.275.1">.zip</span></strong><span class="kobospan" id="kobo.276.1"> file and extracts all files into the same folder. </span><span class="kobospan" id="kobo.276.2">We then read from the folder and create a list of files contained in the folder. </span><span class="kobospan" id="kobo.276.3">We also print the number of files that </span><span><span class="kobospan" id="kobo.277.1">are present:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.278.1">
doc_dir = "data/got_dataset"
fetch_archive_from_http(
    url="https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-qa/datasets/documents/wiki_gameofthrones_txt1.zip",
    output_dir=doc_dir,
    )
files_to_index = [doc_dir + "/" + f </span><strong class="bold1"><span class="kobospan1" id="kobo.279.1">for</span></strong><span class="kobospan1" id="kobo.280.1"> f </span><strong class="bold1"><span class="kobospan1" id="kobo.281.1">in</span></strong><span class="kobospan1" id="kobo.282.1"> os.listdir(
    doc_dir)]
print(len(files_to_index))
183</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.283.1">We initialize a</span><a id="_idIndexMarker508" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.284.1"> document </span><a id="_idIndexMarker509" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.285.1">store based on the files. </span><span class="kobospan" id="kobo.285.2">We create an indexing pipeline based on the document store and execute the indexing operation. </span><span class="kobospan" id="kobo.285.3">To achieve this, we initialize an </span><strong class="source-inline1"><span class="kobospan" id="kobo.286.1">InMemoryDocumentStore</span></strong><span class="kobospan" id="kobo.287.1"> instance. </span><span class="kobospan" id="kobo.287.2">In this method call, we set the </span><strong class="source-inline1"><span class="kobospan" id="kobo.288.1">use_bm25</span></strong><span class="kobospan" id="kobo.289.1"> argument as </span><strong class="source-inline1"><span class="kobospan" id="kobo.290.1">True</span></strong><span class="kobospan" id="kobo.291.1">. </span><span class="kobospan" id="kobo.291.2">The document </span><a id="_idIndexMarker510" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.292.1">store uses </span><strong class="bold"><span class="kobospan" id="kobo.293.1">Best Match 25</span></strong><span class="kobospan" id="kobo.294.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.295.1">bm25</span></strong><span class="kobospan" id="kobo.296.1">) as the algorithm for the retriever step. </span><span class="kobospan" id="kobo.296.2">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.297.1">bm25</span></strong><span class="kobospan" id="kobo.298.1"> algorithm is a simple bag-of-words-based algorithm that uses a scoring function. </span><span class="kobospan" id="kobo.298.2">This function utilizes the number of times a term is present in the document and the length of the document. </span><a href="B18411_03.xhtml#_idTextAnchor067" class="calibre6 pcalibre pcalibre1"><span><em class="italic"><span class="kobospan" id="kobo.299.1">Chapter 3</span></em></span></a><span class="kobospan" id="kobo.300.1"> covers the </span><strong class="source-inline1"><span class="kobospan" id="kobo.301.1">bm25</span></strong><span class="kobospan" id="kobo.302.1"> algorithm in more detail and we recommend you refer to that chapter for better understanding. </span><span class="kobospan" id="kobo.302.2">Note that there are various other </span><strong class="source-inline1"><span class="kobospan" id="kobo.303.1">DocumentStore</span></strong><span class="kobospan" id="kobo.304.1"> options such as </span><strong class="source-inline1"><span class="kobospan" id="kobo.305.1">ElasticSearch</span></strong><span class="kobospan" id="kobo.306.1">, </span><strong class="source-inline1"><span class="kobospan" id="kobo.307.1">OpenSearch</span></strong><span class="kobospan" id="kobo.308.1">, and so on. </span><span class="kobospan" id="kobo.308.2">We used an </span><strong class="source-inline1"><span class="kobospan" id="kobo.309.1">InMemoryDocumentStore</span></strong><span class="kobospan" id="kobo.310.1"> document store to keep </span><a id="_idIndexMarker511" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.311.1">the recipe simple and focus on the retriever and </span><span><span class="kobospan" id="kobo.312.1">reader concepts:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.313.1">
document_store = InMemoryDocumentStore(use_bm25=True)
indexing_pipeline = TextIndexingPipeline(document_store)
indexing_pipeline.run_batch(file_paths=files_to_index)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.314.1">Once we have loaded the documents, we initialize our retriever and reader instances. </span><span class="kobospan" id="kobo.314.2">To achieve this, we initialize the retriever and the reader components. </span><strong class="source-inline1"><span class="kobospan" id="kobo.315.1">BM25Retriever</span></strong><span class="kobospan" id="kobo.316.1"> uses the </span><strong class="source-inline1"><span class="kobospan" id="kobo.317.1">bm25</span></strong><span class="kobospan" id="kobo.318.1"> scoring function to retrieve the initial set of documents. </span><span class="kobospan" id="kobo.318.2">For the</span><a id="_idIndexMarker512" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.319.1"> reader, we initialize the </span><strong class="source-inline1"><span class="kobospan" id="kobo.320.1">FARMReader</span></strong><span class="kobospan" id="kobo.321.1"> object. </span><span class="kobospan" id="kobo.321.2">This is based on deepset’s FARM framework, which can utilize the QA models from Hugging Face. </span><span class="kobospan" id="kobo.321.3">In our case, we use the </span><strong class="source-inline1"><span class="kobospan" id="kobo.322.1">deepset/roberta-base-squad2</span></strong><span class="kobospan" id="kobo.323.1"> model as a reader. </span><span class="kobospan" id="kobo.323.2">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.324.1">use_gpu</span></strong><span class="kobospan" id="kobo.325.1"> argument can be set appropriately based on whether your device has a GPU </span><span><span class="kobospan" id="kobo.326.1">or not:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.327.1">
retriever = BM25Retriever(document_store=document_store)
reader = FARMReader(
    model_name_or_path="deepset/roberta-base-squad2",
    use_gpu=True)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.328.1">We now create a pipeline that we can use to answer questions. </span><span class="kobospan" id="kobo.328.2">After having initialized the retriever and reader in the </span><a id="_idIndexMarker513" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.329.1">previous step, we want to combine them for querying. </span><span class="kobospan" id="kobo.329.2">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.330.1">pipeline</span></strong><span class="kobospan" id="kobo.331.1"> abstraction from the Haystack framework allows us to integrate the reader and retriever together using a series of pipelines that address different use cases. </span><span class="kobospan" id="kobo.331.2">In this instance, we will use </span><strong class="source-inline1"><span class="kobospan" id="kobo.332.1">ExtractiveQAPipeline</span></strong><span class="kobospan" id="kobo.333.1"> for our QA system. </span><span class="kobospan" id="kobo.333.2">After the initialization of the pipeline, we generate the answer to a question from the </span><em class="italic"><span class="kobospan" id="kobo.334.1">Game of Thrones</span></em><span class="kobospan" id="kobo.335.1"> series. </span><span class="kobospan" id="kobo.335.2">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.336.1">run</span></strong><span class="kobospan" id="kobo.337.1"> method takes the question as the query. </span><span class="kobospan" id="kobo.337.2">The second argument, </span><strong class="source-inline1"><span class="kobospan" id="kobo.338.1">params</span></strong><span class="kobospan" id="kobo.339.1">, dictates how</span><a id="_idIndexMarker514" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.340.1"> the results from the retriever and reader are combined to present </span><span><span class="kobospan" id="kobo.341.1">the answer:</span></span><ul class="calibre19"><li class="calibre14"><strong class="source-inline1"><span class="kobospan" id="kobo.342.1">"Retriever": {"top_k": 10}</span></strong><span class="kobospan" id="kobo.343.1">: The </span><strong class="source-inline1"><span class="kobospan" id="kobo.344.1">top_k</span></strong><span class="kobospan" id="kobo.345.1"> keyword argument specifies that the top-k (in this case, </span><strong class="source-inline1"><span class="kobospan" id="kobo.346.1">10</span></strong><span class="kobospan" id="kobo.347.1">) results from the retriever are used by the reader to search for the </span><span><span class="kobospan" id="kobo.348.1">exact answer</span></span></li><li class="calibre14"><strong class="source-inline1"><span class="kobospan" id="kobo.349.1">"Reader": {"top_k": 5}</span></strong><span class="kobospan" id="kobo.350.1">: The </span><strong class="source-inline1"><span class="kobospan" id="kobo.351.1">top_k</span></strong><span class="kobospan" id="kobo.352.1"> keyword argument specifies that the top-k (in this case, </span><strong class="source-inline1"><span class="kobospan" id="kobo.353.1">5</span></strong><span class="kobospan" id="kobo.354.1">) results from the reader are presented as the output of </span><span><span class="kobospan" id="kobo.355.1">the method:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.356.1">
pipe = ExtractiveQAPipeline(reader, retriever)
prediction = pipe.run(
    query="Who is the father of Arya Stark?",
    params={"Retriever": {"top_k": 10}, "Reader": {"top_k": 5}}
)</span></pre></li></ul></li>				<li class="calibre14"><span class="kobospan" id="kobo.357.1">We print the answer to </span><a id="_idIndexMarker515" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.358.1">our question. </span><span class="kobospan" id="kobo.358.2">The system prints out the exact answer along with the associated context that it used to extract the answer from. </span><span class="kobospan" id="kobo.358.3">Note that we use the value of </span><strong class="source-inline1"><span class="kobospan" id="kobo.359.1">all</span></strong><span class="kobospan" id="kobo.360.1"> for the </span><strong class="source-inline1"><span class="kobospan" id="kobo.361.1">details</span></strong><span class="kobospan" id="kobo.362.1"> argument. </span><span class="kobospan" id="kobo.362.2">Using the </span><strong class="source-inline1"><span class="kobospan" id="kobo.363.1">all</span></strong><span class="kobospan" id="kobo.364.1"> value for the same argument prints out </span><strong class="source-inline1"><span class="kobospan" id="kobo.365.1">start</span></strong><span class="kobospan" id="kobo.366.1"> and </span><strong class="source-inline1"><span class="kobospan" id="kobo.367.1">end</span></strong><span class="kobospan" id="kobo.368.1"> spans for the answer along with all the auxiliary information. </span><span class="kobospan" id="kobo.368.2">Setting the value of </span><strong class="source-inline1"><span class="kobospan" id="kobo.369.1">medium</span></strong><span class="kobospan" id="kobo.370.1"> for the </span><strong class="source-inline1"><span class="kobospan" id="kobo.371.1">details</span></strong><span class="kobospan" id="kobo.372.1"> argument provides the relative score of each answer. </span><span class="kobospan" id="kobo.372.2">This score can be used to filter out the results further based on the accuracy requirements of the system. </span><span class="kobospan" id="kobo.372.3">Using the argument of </span><strong class="source-inline1"><span class="kobospan" id="kobo.373.1">medium</span></strong><span class="kobospan" id="kobo.374.1"> presents only the answer and the context. </span><span class="kobospan" id="kobo.374.2">We encourage you to </span><a id="_idIndexMarker516" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.375.1">make a suitable choice based on </span><span><span class="kobospan" id="kobo.376.1">your requirements:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.377.1">
print_answers(prediction, details="all")
'Query: Who is the father of Arya Stark?'
</span><span class="kobospan1" id="kobo.377.2">'Answers:'
[&lt;Answer {'answer': 'Eddard',
'type': 'extractive',
'score': 0.993372917175293,
'context': "s Nymeria after a legendary warrior queen. </span><span class="kobospan1" id="kobo.377.3">She travels with her father, Eddard, to King's Landing when he is made Hand of the King. </span><span class="kobospan1" id="kobo.377.4">Before she leaves,", 'offsets_in_document': [{'start': 207, 'end': 213}], 'offsets_in_context': [{'start': 72, 'end': 78}], 'document_ids': ['9e3c863097d66aeed9992e0b6bf1f2f4'], 'meta': {'_split_id': 3}}&gt;,
&lt;Answer {'answer': 'Ned',
'type': 'extractive',
'score': 0.9753613471984863,
'context': "k in the television series.\n\n====Season 1====\nArya accompanies her father Ned and her sister Sansa to King's Landing. </span><span class="kobospan1" id="kobo.377.5">Before their departure, Arya's h", 'offsets_in_document': [{'start': 630, 'end': 633}], 'offsets_in_context': [{'start': 74, 'end': 77}], 'document_ids': ['7d3360fa29130e69ea6b2ba5c5a8f9c8'], 'meta': {'_split_id': 10}}&gt;,
&lt;Answer {'answer': 'Lord Eddard Stark',
'type': 'extractive',
'score': 0.9177322387695312,
'context': 'rk daughters.\n\nDuring the Tourney of the Hand to honour her father Lord Eddard Stark, Sansa Stark is enchanted by the knights performing in the event.', 'offsets_in_document': [{'start': 280, 'end': 297}], 'offsets_in_context': [{'start': 67, 'end': 84}], 'document_ids': ['5dbccad397381605eba063f71dd500a6'], 'meta': {'_split_id': 3}}&gt;,
&lt;Answer {'answer': 'Ned',
'type': 'extractive',
'score': 0.8396496772766113,
'context': " girl disguised as a boy all along and is surprised to learn she is Arya, Ned Stark's daughter. </span><span class="kobospan1" id="kobo.377.6">After the Goldcloaks get help from Ser Amory Lorch and", 'offsets_in_document': [{'start': 848, 'end': 851}], 'offsets_in_context': [{'start': 74, 'end': 77}], 'document_ids': ['257088f56d2faba55e2ef2ebd19502dc'], 'meta': {'_split_id': 31}}&gt;,
&lt;Answer {'answer': 'King Robert',
'type': 'extractive',
'score': 0.6922298073768616,
'context': "en refuses to yield Gendry, who is actually a bastard son of the late King Robert, to the Lannisters.  The Night's Watch convoy is overrun and massacr", 'offsets_in_document': [{'start': 579, 'end': 590}], 'offsets_in_context': [{'start': 70, 'end': 81}], 'document_ids': ['4d51b1876e8a7eac8132b97e2af04</span><a id="_idTextAnchor249" class="pcalibre pcalibre1 calibre20"/><span class="kobospan1" id="kobo.378.1">401'], 'meta': {'_split_id': 4}}&gt;]</span></pre></li>			</ol>
			<h2 id="_idParaDest-236" class="calibre5"><a id="_idTextAnchor250" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.379.1">See also</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.380.1">For a QA system to</span><a id="_idIndexMarker517" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.381.1"> work in a high-performance </span><a id="_idIndexMarker518" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.382.1">production system, it is recommended to use a different document store from an in-memory one. </span><span class="kobospan" id="kobo.382.2">We recommend you refer to </span><a href="https://docs.haystack.deepset.ai/docs/document_store" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.383.1">https://docs.haystack.deepset.ai/docs/document_store</span></a><span class="kobospan" id="kobo.384.1"> and use an appropriate document store based on your </span><span><span class="kobospan" id="kobo.385.1">production-grade requirements.</span></span></p>
			<h1 id="_idParaDest-237" class="calibre7"><a id="_idTextAnchor251" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.386.1">Answering questions from a document corpus in an abstractive manner</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.387.1">In the previous recipe, we learned how to build a QA system based on the document corpora. </span><span class="kobospan" id="kobo.387.2">The answers that were retrieved</span><a id="_idIndexMarker519" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.388.1"> were extractive in nature (i.e., the answer snippet was a piece of text copied verbatim from the document</span><a id="_idIndexMarker520" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.389.1"> source). </span><span class="kobospan" id="kobo.389.2">There are techniques to generate an abstractive answer too, which is more readable by end users compared to an </span><span><span class="kobospan" id="kobo.390.1">extractive one.</span></span></p>
			<h2 id="_idParaDest-238" class="calibre5"><a id="_idTextAnchor252" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.391.1">Getting ready</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.392.1">For this recipe, we will build a QA system that will provide answers that are abstractive in nature. </span><span class="kobospan" id="kobo.392.2">We will load the </span><strong class="source-inline"><span class="kobospan" id="kobo.393.1">bilgeyucel/seven-wonders</span></strong><span class="kobospan" id="kobo.394.1"> dataset from the Hugging Face site and initialize a retriever from it. </span><span class="kobospan" id="kobo.394.2">This dataset has content about the seven wonders of the ancient world. </span><span class="kobospan" id="kobo.394.3">To generate the answers, we will use the </span><strong class="source-inline"><span class="kobospan" id="kobo.395.1">PromptNode</span></strong><span class="kobospan" id="kobo.396.1"> component from the Haystack framework to set up a pipeline that can generate answers in an abstractive fashion. </span><span class="kobospan" id="kobo.396.2">You can use the </span><strong class="source-inline"><span class="kobospan" id="kobo.397.1">9.4_abstractive_qa_on_document_corpus.ipynb</span></strong><span class="kobospan" id="kobo.398.1"> notebook from the code site if you need to work from an existing notebook. </span><span class="kobospan" id="kobo.398.2">Let’s </span><span><span class="kobospan" id="kobo.399.1">get started.</span></span></p>
			<h2 id="_idParaDest-239" class="calibre5"><a id="_idTextAnchor253" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.400.1">How to do it</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.401.1">The steps are </span><span><span class="kobospan" id="kobo.402.1">as follows:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.403.1">In this step, we do the </span><a id="_idIndexMarker521" class="calibre6 pcalibre pcalibre1"/><span><span class="kobospan" id="kobo.404.1">necessary imports:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.405.1">
from datasets import load_dataset
from haystack.document_stores import InMemoryDocumentStore
from haystack.nodes import (
    BM25Retriever, PromptNode,
    PromptTemplate, AnswerParser)
from haystack.pipelines import Pipeline</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.406.1">As part of this step, we load the </span><strong class="source-inline1"><span class="kobospan" id="kobo.407.1">bilgeyucel/seven-wonders</span></strong><span class="kobospan" id="kobo.408.1"> dataset into an in-memory</span><a id="_idIndexMarker522" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.409.1"> document store. </span><span class="kobospan" id="kobo.409.2">This dataset has been created out of the Wikipedia pages of </span><em class="italic"><span class="kobospan" id="kobo.410.1">Seven Wonders of the Ancient World</span></em><span class="kobospan" id="kobo.411.1"> (</span><a href="https://en.wikipedia.org/wiki/Wonders_of_the_World" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.412.1">https://en.wikipedia.org/wiki/Wonders_of_the_World</span></a><span class="kobospan" id="kobo.413.1">). </span><span class="kobospan" id="kobo.413.2">This dataset has been preprocessed and uploaded to the Hugging Face site, and can be easily downloaded by using the </span><strong class="source-inline1"><span class="kobospan" id="kobo.414.1">datasets</span></strong><span class="kobospan" id="kobo.415.1"> module from Hugging Face. </span><span class="kobospan" id="kobo.415.2">We use </span><strong class="source-inline1"><span class="kobospan" id="kobo.416.1">InMemoryDocumentStore</span></strong><span class="kobospan" id="kobo.417.1"> as our document store, with </span><strong class="source-inline1"><span class="kobospan" id="kobo.418.1">bm25</span></strong><span class="kobospan" id="kobo.419.1"> as the search algorithm. </span><span class="kobospan" id="kobo.419.2">We write the documents from the dataset into the document store. </span><span class="kobospan" id="kobo.419.3">To have a performant query time performance, the </span><strong class="source-inline1"><span class="kobospan" id="kobo.420.1">write_documents</span></strong><span class="kobospan" id="kobo.421.1"> method automatically</span><a id="_idIndexMarker523" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.422.1"> optimizes how the documents are written. </span><span class="kobospan" id="kobo.422.2">Once the documents are written into, we initialize the retriever based on </span><strong class="source-inline1"><span class="kobospan" id="kobo.423.1">bm25</span></strong><span class="kobospan" id="kobo.424.1">, similar to our </span><span><span class="kobospan" id="kobo.425.1">previous recipe:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.426.1">
dataset = load_dataset("bilgeyucel/seven-wonders", 
    split="train")
document_store = InMemoryDocumentStore(use_bm25=True)
document_store.write_documents(dataset)
retriever = BM25Retriever(document_store=document_store)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.427.1">As part of this step, we initialize a prompt template. </span><span class="kobospan" id="kobo.427.2">We can define the task we want the model to</span><a id="_idIndexMarker524" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.428.1"> perform as a simple instruction in English using the </span><strong class="source-inline1"><span class="kobospan" id="kobo.429.1">prompt</span></strong><span class="kobospan" id="kobo.430.1"> argument. </span><span class="kobospan" id="kobo.430.2">It also takes two inline arguments, </span><strong class="source-inline1"><span class="kobospan" id="kobo.431.1">document</span></strong><span class="kobospan" id="kobo.432.1"> and </span><strong class="source-inline1"><span class="kobospan" id="kobo.433.1">query</span></strong><span class="kobospan" id="kobo.434.1">. </span><span class="kobospan" id="kobo.434.2">These arguments are expected to be in the execution context at runtime. </span><span class="kobospan" id="kobo.434.3">The second argument, </span><strong class="source-inline1"><span class="kobospan" id="kobo.435.1">output_parser</span></strong><span class="kobospan" id="kobo.436.1">, takes an </span><strong class="source-inline1"><span class="kobospan" id="kobo.437.1">AnswerParser</span></strong><span class="kobospan" id="kobo.438.1"> object. </span><span class="kobospan" id="kobo.438.2">This object instructs the </span><strong class="source-inline1"><span class="kobospan" id="kobo.439.1">PromptNode</span></strong><span class="kobospan" id="kobo.440.1"> object to store the results in the </span><strong class="source-inline1"><span class="kobospan" id="kobo.441.1">answers</span></strong><span class="kobospan" id="kobo.442.1"> element. </span><span class="kobospan" id="kobo.442.2">After defining the prompt, we initialize a </span><strong class="source-inline1"><span class="kobospan" id="kobo.443.1">PromptNode</span></strong><span class="kobospan" id="kobo.444.1"> object with a model and the prompt template. </span><span class="kobospan" id="kobo.444.2">We use the </span><strong class="source-inline1"><span class="kobospan" id="kobo.445.1">google/flan-t5-large</span></strong><span class="kobospan" id="kobo.446.1"> model as the answer generator. </span><span class="kobospan" id="kobo.446.2">This model is based on the Google T5 language model and has been fine-tuned (</span><strong class="source-inline1"><span class="kobospan" id="kobo.447.1">flan</span></strong><span class="kobospan" id="kobo.448.1"> stands for </span><strong class="bold"><span class="kobospan" id="kobo.449.1">fine-tuning language models</span></strong><span class="kobospan" id="kobo.450.1">). </span><span class="kobospan" id="kobo.450.2">Fine-tuning a language model with an instruction dataset allows the language model to</span><a id="_idIndexMarker525" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.451.1"> perform tasks following simple instructions and generating text based on the given context and instruction. </span><span class="kobospan" id="kobo.451.2">One of the fine-tuning steps as part of this model training was to operate on human written instructions as tasks. </span><span class="kobospan" id="kobo.451.3">This allowed the model to perform different downstream tasks on instructions alone</span><a id="_idIndexMarker526" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.452.1"> and reduced the need for any few-shot examples to be </span><span><span class="kobospan" id="kobo.453.1">trained on.</span></span><pre class="source-code"><span class="kobospan1" id="kobo.454.1">
rag_prompt = PromptTemplate(
    prompt="""Synthesize a comprehensive answer from the following text for the given question.
</span><span class="kobospan1" id="kobo.454.2">        Provide a clear and concise response that summarizes the key points and information presented in the text.
</span><span class="kobospan1" id="kobo.454.3">        Your answer should be in your own words and be no longer than 50 words.
</span><span class="kobospan1" id="kobo.454.4">        \n\n Related text: {join(documents)} \n\n Question: {query} \n\n Answer:""",
    output_parser=AnswerParser(),
)
prompt_node = PromptNode(
    model_name_or_path="google/flan-t5-large",
    default_prompt_template=rag_prompt, use_gpu=True)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.455.1">We now create a pipeline and add the </span><strong class="source-inline1"><span class="kobospan" id="kobo.456.1">retriever</span></strong><span class="kobospan" id="kobo.457.1"> and </span><strong class="source-inline1"><span class="kobospan" id="kobo.458.1">prompt_node</span></strong><span class="kobospan" id="kobo.459.1"> components that we initialized in the previous steps. </span><span class="kobospan" id="kobo.459.2">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.460.1">retriever</span></strong><span class="kobospan" id="kobo.461.1"> component operates on</span><a id="_idIndexMarker527" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.462.1"> the query supplied by the user and generates a set of results. </span><span class="kobospan" id="kobo.462.2">These results are </span><a id="_idIndexMarker528" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.463.1">passed to the prompt node, which uses the configured </span><strong class="source-inline1"><span class="kobospan" id="kobo.464.1">flan-t5-model</span></strong><span class="kobospan" id="kobo.465.1"> to generate </span><span><span class="kobospan" id="kobo.466.1">the answer:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.467.1">
pipe = Pipeline()
pipe.add_node(component=retriever, name="retriever", 
    inputs=["Query"])
pipe.add_node(component=prompt_node,
    name="prompt_node", inputs=["retriever"])</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.468.1">Once the pipeline is set up, we use it to answer questions on the content based on </span><span><span class="kobospan" id="kobo.469.1">the dataset:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.470.1">
output = pipe.run(query="What is the Great Pyramid of Giza?")
print(output["answers"][0].answer)
output = pipe.run(query="Where are the hanging gardens?")
print(output["answers"][0].answer)</span></pre></li>			</ol>
			<pre class="console"><span class="kobospan1" id="kobo.471.1">
The Great Pyramid of Giza was built in the early 26th century BC during a period of around 27 years.[3]
The Hanging Gardens of Semiramis are the only one of the Seven Wonders for which the l</span><a id="_idTextAnchor254" class="pcalibre pcalibre1 calibre20"/><span class="kobospan1" id="kobo.472.1">ocation has not been definitively established.</span></pre>			<h2 id="_idParaDest-240" class="calibre5"><a id="_idTextAnchor255" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.473.1">See also</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.474.1">Please refer to the prompt </span><a id="_idIndexMarker529" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.475.1">engineering guide </span><a id="_idIndexMarker530" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.476.1">on Haystack on how to generate prompts for your use </span><span><span class="kobospan" id="kobo.477.1">cases (</span></span><a href="https://docs.haystack.deepset.ai/docs/prompt-engineering-guidelines" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.478.1">https://docs.haystac</span><span id="_idTextAnchor256"/><span class="kobospan" id="kobo.479.1">k.deepset.ai/docs/prompt-engineering-guidelines</span></span></a><span><span class="kobospan" id="kobo.480.1">).</span></span></p>
			<h1 id="_idParaDest-241" class="calibre7"><a id="_idTextAnchor257" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.481.1">Summarizing text using pre-trained models based on Transformers</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.482.1">We will now explore techniques for </span><a id="_idIndexMarker531" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.483.1">performing text summarization. </span><span class="kobospan" id="kobo.483.2">Generating a summary for a long passage of text allows NLP practitioners to extract the relevant information for their use cases and use these summaries for other downstream tasks. </span><span class="kobospan" id="kobo.483.3">As part of the summarization, we will explore</span><a id="_idIndexMarker532" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.484.1"> recipes that use Transformer models to generate </span><span><span class="kobospan" id="kobo.485.1">the summaries.</span></span></p>
			<h2 id="_idParaDest-242" class="calibre5"><a id="_idTextAnchor258" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.486.1">Getting ready</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.487.1">Our first recipe for summarization</span><a id="_idIndexMarker533" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.488.1"> will use the Google </span><strong class="bold"><span class="kobospan" id="kobo.489.1">Text-to-Text Transfer Transformer</span></strong><span class="kobospan" id="kobo.490.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.491.1">T5</span></strong><span class="kobospan" id="kobo.492.1">) model for summarization. </span><span class="kobospan" id="kobo.492.2">You can use the </span><strong class="source-inline"><span class="kobospan" id="kobo.493.1">9.5_summarization.ipynb</span></strong><span class="kobospan" id="kobo.494.1"> notebook from the code site if you need to work from an </span><span><span class="kobospan" id="kobo.495.1">existing notebook.</span></span></p>
			<h2 id="_idParaDest-243" class="calibre5"><a id="_idTextAnchor259" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.496.1">How to do it</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.497.1">Let’s </span><span><span class="kobospan" id="kobo.498.1">get started:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.499.1">Do the </span><span><span class="kobospan" id="kobo.500.1">necessary imports:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.501.1">
from transformers import pipeline</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.502.1">As part of this step, we initialize the input passage that we need to summarize along with the pipeline. </span><span class="kobospan" id="kobo.502.2">We also calculate the length of the passage since this will be used as an argument to </span><a id="_idIndexMarker534" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.503.1">be passed to the pipeline during the task execution in the next step. </span><span class="kobospan" id="kobo.503.2">Since we have defined the task as </span><strong class="source-inline1"><span class="kobospan" id="kobo.504.1">summarization</span></strong><span class="kobospan" id="kobo.505.1">, the object returned by the pipeline module is of the </span><strong class="source-inline1"><span class="kobospan" id="kobo.506.1">SummarizationPipeline</span></strong><span class="kobospan" id="kobo.507.1"> type. </span><span class="kobospan" id="kobo.507.2">We also pass </span><strong class="source-inline1"><span class="kobospan" id="kobo.508.1">t5-large</span></strong><span class="kobospan" id="kobo.509.1"> as the model parameter for the pipeline. </span><span class="kobospan" id="kobo.509.2">This model is based on the </span><strong class="source-inline1"><span class="kobospan" id="kobo.510.1">Encoder-Decoder </span></strong><span class="kobospan" id="kobo.511.1">Transformer model and acts as a pure sequence-to-sequence model. </span><span class="kobospan" id="kobo.511.2">That means the input and output to/from the model are text sequences. </span><span class="kobospan" id="kobo.511.3">This model was pre-trained using the denoising objective of finding masked words in a sentence followed by fine-tuning on specific downstream tasks such as summarization, textual</span><a id="_idIndexMarker535" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.512.1"> entailment, language translation, and </span><span><span class="kobospan" id="kobo.513.1">so on:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.514.1">
passage = "The color of animals is by no means a matter of chance; it depends on many considerations, but in the majority of cases tends to protect the animal from danger by rendering it less conspicuous. </span><span class="kobospan1" id="kobo.514.2">Perhaps it may be said that if coloring is mainly protective, there ought to be but few brightly colored animals. </span><span class="kobospan1" id="kobo.514.3">There are, however, not a few cases in which vivid colors are themselves protective. </span><span class="kobospan1" id="kobo.514.4">The kingfisher itself, though so brightly colored, is by no means easy to see. </span><span class="kobospan1" id="kobo.514.5">The blue harmonizes with the water, and the bird as it darts along the stream looks almost like a flash of sunlight."
</span><span class="kobospan1" id="kobo.514.6">passage_length = len(passage.split(' '))
pipeline_instance = pipeline("summarization", model="t5-large")</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.515.1">We now use the </span><strong class="source-inline1"><span class="kobospan" id="kobo.516.1">pipeline_instance</span></strong><span class="kobospan" id="kobo.517.1"> initialized in the previous step and pass the text passage to it to perform the </span><strong class="source-inline1"><span class="kobospan" id="kobo.518.1">summarization</span></strong><span class="kobospan" id="kobo.519.1"> step. </span><span class="kobospan" id="kobo.519.2">A string array can be passed as well if multiple sequences are to be summarized. </span><span class="kobospan" id="kobo.519.3">We pass </span><strong class="source-inline1"><span class="kobospan" id="kobo.520.1">max_length=512</span></strong><span class="kobospan" id="kobo.521.1"> as the second argument. </span><span class="kobospan" id="kobo.521.2">The T5 model is memory-intensive and the compute requirements grow quadratically with the increase in the input text length. </span><span class="kobospan" id="kobo.521.3">This step might take a few minutes to complete based on the compute capability of the environment you are executing </span><span><span class="kobospan" id="kobo.522.1">this on:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.523.1">
pipeline_result = pipeline_instance(
    passage, max_length=passage_length)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.524.1">Once the </span><strong class="source-inline1"><span class="kobospan" id="kobo.525.1">summarization</span></strong><span class="kobospan" id="kobo.526.1"> step is</span><a id="_idIndexMarker536" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.527.1"> complete, we extract the result from the output and print it. </span><span class="kobospan" id="kobo.527.2">The pipeline returns a list of dictionaries. </span><span class="kobospan" id="kobo.527.3">Each list item corresponds to the input argument. </span><span class="kobospan" id="kobo.527.4">In this case, since we passed only one string as input, the first item in the list is the output dictionary that contains our summary. </span><span class="kobospan" id="kobo.527.5">The summary </span><a id="_idIndexMarker537" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.528.1">can be retrieved by indexing the dictionary on the </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.529.1">summary_text</span></strong></span><span><span class="kobospan" id="kobo.530.1"> element:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.531.1">
result = pipeline_result[0]["summary_text"]
print(result)</span></pre></li>			</ol>
			<pre class="console"><span class="kobospan1" id="kobo.532.1">
the color of animals is by no means a matter of chance; it depends on many considerations . </span><span class="kobospan1" id="kobo.532.2">in the majority of cases, coloring tends to protect the animal from danger . </span><span class="kobospan1" id="kobo.532.3">there are, however, not a few cases in which vivid colors are themselves protective .</span></pre>			<h2 id="_idParaDest-244" class="calibre5"><a id="_idTextAnchor260" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.533.1">There’s more…</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.534.1">Now that we have seen how we can generate a summary using the T5 model, we can use the same code framework and tweak it slightly to use other models to </span><span><span class="kobospan" id="kobo.535.1">generate summaries.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.536.1">The following lines would be common for the other summarization recipes that we are using. </span><span class="kobospan" id="kobo.536.2">We added an extra variable named </span><strong class="source-inline"><span class="kobospan" id="kobo.537.1">device</span></strong><span class="kobospan" id="kobo.538.1">, which we will use in our pipelines. </span><span class="kobospan" id="kobo.538.2">We set this variable to the value of</span><a id="_idIndexMarker538" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.539.1"> the device that we will use to generate the summary. </span><span class="kobospan" id="kobo.539.2">If a GPU is present and configured in the system, it will be used; otherwise, the summarization will be performed using </span><span><span class="kobospan" id="kobo.540.1">the CPU:</span></span></p>
			<pre class="source-code"><span class="kobospan1" id="kobo.541.1">
from transformers import pipeline
import torch
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
passage = "The color of animals is by no means a matter of chance; it depends on many considerations, but in the majority of cases tends to protect the animal from danger by rendering it less conspicuous. </span><span class="kobospan1" id="kobo.541.2">Perhaps it may be said that if coloring is mainly protective, there ought to be but few brightly colored animals. </span><span class="kobospan1" id="kobo.541.3">There are, however, not a few cases in which vivid colors are themselves protective. </span><span class="kobospan1" id="kobo.541.4">The kingfisher itself, though so brightly colored, is by no means easy to see. </span><span class="kobospan1" id="kobo.541.5">The blue harmonizes with the water, and the bird as it darts along the stream looks almost like a flash of sunlight."</span></pre>			<p class="calibre3"><span class="kobospan" id="kobo.542.1">In the following example, we use</span><a id="_idIndexMarker539" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.543.1"> the </span><strong class="bold"><span class="kobospan" id="kobo.544.1">BART</span></strong><span class="kobospan" id="kobo.545.1"> model (</span><a href="https://huggingface.co/facebook/bart-large-cnn" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.546.1">https://huggingface.co/facebook/bart-large-cnn</span></a><span class="kobospan" id="kobo.547.1">) from Facebook. </span><span class="kobospan" id="kobo.547.2">This model was trained using a denoising objective. </span><span class="kobospan" id="kobo.547.3">A function adds some random piece of text to an input sequence. </span><span class="kobospan" id="kobo.547.4">The</span><a id="_idIndexMarker540" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.548.1"> model is trained based on the objective to denoise or remove the noisy text from the input sequence. </span><span class="kobospan" id="kobo.548.2">The model was further fine-tuned using the </span><strong class="bold"><span class="kobospan" id="kobo.549.1">CNN DailyMail</span></strong><span class="kobospan" id="kobo.550.1"> dataset</span><a id="_idIndexMarker541" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.551.1"> (</span><a href="https://huggingface.co/datasets/abisee/cnn_dailymail" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.552.1">https://huggingface.co/datasets/abisee/cnn_dailymail</span></a><span class="kobospan" id="kobo.553.1">) </span><span><span class="kobospan" id="kobo.554.1">for summarization:</span></span></p>
			<pre class="source-code"><span class="kobospan1" id="kobo.555.1">
pipeline_instance = pipeline("summarization", 
    model="facebook/bart-large-cnn", device=device)
pipeline_result = pipeline_instance(passage, 
    max_length=passage_length)
result = pipeline_result[0]["summary_text"]
print(result)
The color of animals is by no means a matter of chance; it depends on many considerations, but in the majority of cases tends to protect the animal from danger by rendering it less conspicuous. </span><span class="kobospan1" id="kobo.555.2">There are, however, not a few cases in which vivid colors are themselves protective. </span><span class="kobospan1" id="kobo.555.3">The blue harmonizes with the water, and the bird as it darts along the stream looks almost like a flash of sunlight.</span></pre>			<p class="calibre3"><span class="kobospan" id="kobo.556.1">As we observe from the generated summary, it is verbose and extractive in nature. </span><span class="kobospan" id="kobo.556.2">Let’s try generating a summary with </span><span><span class="kobospan" id="kobo.557.1">another model.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.558.1">In the following example, we</span><a id="_idIndexMarker542" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.559.1"> use the </span><strong class="bold"><span class="kobospan" id="kobo.560.1">PEGASUS</span></strong><span class="kobospan" id="kobo.561.1"> model from Google (</span><a href="https://huggingface.co/google/pegasus-large" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.562.1">https://huggingface.co/google/pegasus-large</span></a><span class="kobospan" id="kobo.563.1">) for summarization. </span><span class="kobospan" id="kobo.563.2">This model is a Transformer-based Encoder-Decoder model that was pre-trained with a large news and web page corpus – C4 (</span><a href="https://huggingface.co/datasets/allenai/c4" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.564.1">https://huggingface.co/datasets/allenai/c4</span></a><span class="kobospan" id="kobo.565.1">) and the HugeNews dataset – on a training objective of </span><a id="_idIndexMarker543" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.566.1">detecting important sentences. </span><span class="kobospan" id="kobo.566.2">HugeNews is a dataset of 1.5 billion articles curated from news and news-like websites from 2013–2019. </span><span class="kobospan" id="kobo.566.3">This model was further fine-tuned for summarization using the subset of the same dataset. </span><span class="kobospan" id="kobo.566.4">The training objective for the fine-tuning involved masking important sentences and making the model generate a summary that has these important sentences. </span><span class="kobospan" id="kobo.566.5">This model generates </span><span><span class="kobospan" id="kobo.567.1">abstract </span></span><span><a id="_idIndexMarker544" class="calibre6 pcalibre pcalibre1"/></span><span><span class="kobospan" id="kobo.568.1">summaries:</span></span></p>
			<pre class="source-code"><span class="kobospan1" id="kobo.569.1">
pipeline_instance = pipeline("summarization", 
    model="google/pegasus-large", device=device)
pipeline_result = pipeline_instance([passage, passage], 
    max_length=passage_length)
result = pipeline_result[0]["summary_text"]
print(result)
Perhaps it may be said that if coloring is mainly protective, there ought to be but few brightly colored animals.</span></pre>			<p class="calibre3"><span class="kobospan" id="kobo.570.1">As we observe from the generated summary, it is concise </span><span><span class="kobospan" id="kobo.571.1">and abstractive.</span></span></p>
			<h2 id="_idParaDest-245" class="calibre5"><a id="_idTextAnchor261" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.572.1">See also</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.573.1">As many new and improved </span><a id="_idIndexMarker545" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.574.1">models for summarization are always in the works, we recommend that you refer to the models on the Hugging Face site (</span><a href="https://huggingface.co/models?pipeline_tag=summarization" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.575.1">https://huggingface.co/models?pipeline_tag=summarization</span></a><span class="kobospan" id="kobo.576.1">) and make the respective choice based on </span><span><span class="kobospan" id="kobo.577.1">your requirements.</span></span></p>
			<h1 id="_idParaDest-246" class="calibre7"><a id="_idTextAnchor262" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.578.1">Detecting sentence entailment</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.579.1">In this recipe, we will explore</span><a id="_idIndexMarker546" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.580.1"> techniques to detect </span><strong class="bold"><span class="kobospan" id="kobo.581.1">textual entailment</span></strong><span class="kobospan" id="kobo.582.1">, given a set of two sentences. </span><span class="kobospan" id="kobo.582.2">The first sentence in the set is the </span><strong class="source-inline"><span class="kobospan" id="kobo.583.1">premise</span></strong><span class="kobospan" id="kobo.584.1">, which sets up a context. </span><span class="kobospan" id="kobo.584.2">The second sentence is the </span><strong class="source-inline"><span class="kobospan" id="kobo.585.1">hypothesis</span></strong><span class="kobospan" id="kobo.586.1">, which serves as the claim. </span><span class="kobospan" id="kobo.586.2">Textual entailment identifies the contextual relationship between the </span><strong class="source-inline"><span class="kobospan" id="kobo.587.1">premise</span></strong><span class="kobospan" id="kobo.588.1"> and the </span><strong class="source-inline"><span class="kobospan" id="kobo.589.1">hypothesis</span></strong><span class="kobospan" id="kobo.590.1">. </span><span class="kobospan" id="kobo.590.2">These relationships can be of </span><a id="_idIndexMarker547" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.591.1">three types, defined </span><span><span class="kobospan" id="kobo.592.1">as follows:</span></span></p>
			<ul class="calibre15">
				<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.593.1">Entailment</span></strong><span class="kobospan" id="kobo.594.1"> – The hypothesis supports </span><span><span class="kobospan" id="kobo.595.1">the </span></span><span><a id="_idIndexMarker548" class="calibre6 pcalibre pcalibre1"/></span><span><span class="kobospan" id="kobo.596.1">premise</span></span></li>
				<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.597.1">Contradiction</span></strong><span class="kobospan" id="kobo.598.1"> – The </span><a id="_idIndexMarker549" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.599.1">hypothesis contradicts </span><span><span class="kobospan" id="kobo.600.1">the premise</span></span></li>
				<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.601.1">Neutral</span></strong><span class="kobospan" id="kobo.602.1"> – The hypothesis </span><a id="_idIndexMarker550" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.603.1">neither supports nor contradicts </span><span><span class="kobospan" id="kobo.604.1">the premise</span></span></li>
			</ul>
			<h2 id="_idParaDest-247" class="calibre5"><a id="_idTextAnchor263" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.605.1">Getting ready</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.606.1">We will use the Transformers library to detect text entailment. </span><span class="kobospan" id="kobo.606.2">You can use the </span><strong class="source-inline"><span class="kobospan" id="kobo.607.1">9.6_textual_entailment.ipynb</span></strong><span class="kobospan" id="kobo.608.1"> notebook from the code site if you need to work from an </span><span><span class="kobospan" id="kobo.609.1">existing notebook.</span></span></p>
			<h2 id="_idParaDest-248" class="calibre5"><a id="_idTextAnchor264" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.610.1">How to do it...</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.611.1">In this recipe, we will initialize different sets of sentences that are related through each of the previously defined relationships and explore methods to detect these relationships. </span><span class="kobospan" id="kobo.611.2">Let’s </span><span><span class="kobospan" id="kobo.612.1">get started:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.613.1">Do the </span><span><span class="kobospan" id="kobo.614.1">necessary imports:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.615.1">
import torch
from transformers import T5Tokenizer, T5ForConditionalGeneration</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.616.1">Initialize the device, the tokenizer, and the model. </span><span class="kobospan" id="kobo.616.2">In this case, we are using Google’s </span><strong class="source-inline1"><span class="kobospan" id="kobo.617.1">t5-small</span></strong><span class="kobospan" id="kobo.618.1"> model. </span><span class="kobospan" id="kobo.618.2">We set the </span><strong class="source-inline1"><span class="kobospan" id="kobo.619.1">legacy</span></strong><span class="kobospan" id="kobo.620.1"> flag to </span><strong class="source-inline1"><span class="kobospan" id="kobo.621.1">False</span></strong><span class="kobospan" id="kobo.622.1"> since we don’t need to use the legacy behavior of the model. </span><span class="kobospan" id="kobo.622.2">We set the </span><strong class="source-inline1"><span class="kobospan" id="kobo.623.1">device</span></strong><span class="kobospan" id="kobo.624.1"> value based on whatever device we have available in our execution environment. </span><span class="kobospan" id="kobo.624.2">Similarly, for the model, we set the </span><strong class="source-inline1"><span class="kobospan" id="kobo.625.1">model</span></strong><span class="kobospan" id="kobo.626.1"> name and </span><strong class="source-inline1"><span class="kobospan" id="kobo.627.1">device</span></strong><span class="kobospan" id="kobo.628.1"> parameter similar to the tokenizer. </span><span class="kobospan" id="kobo.628.2">We set the </span><strong class="source-inline1"><span class="kobospan" id="kobo.629.1">return_dict</span></strong><span class="kobospan" id="kobo.630.1"> parameter as </span><strong class="source-inline1"><span class="kobospan" id="kobo.631.1">True</span></strong><span class="kobospan" id="kobo.632.1"> so that we get the model results as a </span><a id="_idIndexMarker551" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.633.1">dictionary instead of </span><span><span class="kobospan" id="kobo.634.1">a tuple:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.635.1">
device = torch.device("cuda" </span><strong class="bold1"><span class="kobospan1" id="kobo.636.1">if</span></strong><span class="kobospan1" id="kobo.637.1"> torch.cuda.is_available() 
    </span><strong class="bold1"><span class="kobospan1" id="kobo.638.1">else</span></strong><span class="kobospan1" id="kobo.639.1"> "cpu")
tokenizer = T5Tokenizer.from_pretrained(
    't5-small', legacy=False, device=device)
model = T5ForConditionalGeneration.from_pretrained(
    't5-small', return_dict=True, device_map=device)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.640.1">We initialize the </span><strong class="source-inline1"><span class="kobospan" id="kobo.641.1">premise</span></strong><span class="kobospan" id="kobo.642.1"> and </span><strong class="source-inline1"><span class="kobospan" id="kobo.643.1">hypothesis</span></strong><span class="kobospan" id="kobo.644.1"> sentences. </span><span class="kobospan" id="kobo.644.2">In this case, the hypothesis supports </span><span><span class="kobospan" id="kobo.645.1">the premise:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.646.1">
premise = "The corner coffee shop serves the most awesome coffee I have ever had."
</span><span class="kobospan1" id="kobo.646.2">hypothesis = "I love the coffee served by the corner coffee shop."</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.647.1">In this step, we call the tokenizer with the </span><strong class="source-inline1"><span class="kobospan" id="kobo.648.1">mnli premise</span></strong><span class="kobospan" id="kobo.649.1"> and </span><strong class="source-inline1"><span class="kobospan" id="kobo.650.1">hypothesis</span></strong><span class="kobospan" id="kobo.651.1"> values. </span><span class="kobospan" id="kobo.651.2">This is a simple text concatenation step to set up the tokenizer for the </span><strong class="source-inline1"><span class="kobospan" id="kobo.652.1">entailment</span></strong><span class="kobospan" id="kobo.653.1"> task. </span><span class="kobospan" id="kobo.653.2">We read the </span><strong class="source-inline1"><span class="kobospan" id="kobo.654.1">input_ids</span></strong><span class="kobospan" id="kobo.655.1"> property to get the token identifiers for the concatenated string. </span><span class="kobospan" id="kobo.655.2">Once we have the token IDs, we use the model to generate the entailment prediction. </span><span class="kobospan" id="kobo.655.3">This returns a list of tensors with the predictions, which we use in the </span><span><span class="kobospan" id="kobo.656.1">next step:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.657.1">
input_ids = tokenizer(
    "mnli premise: " + premise + " hypothesis: " + hypothesis,
    return_tensors="pt").input_ids
entailment_ids = model.generate(input_ids.to(device), 
    max_new_tokens=20)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.658.1">In this step, we call the </span><strong class="source-inline1"><span class="kobospan" id="kobo.659.1">decode</span></strong><span class="kobospan" id="kobo.660.1"> method of the tokenizer and pass it the first tensor (or vector) of the tensors that were returned by the </span><strong class="source-inline1"><span class="kobospan" id="kobo.661.1">generate</span></strong><span class="kobospan" id="kobo.662.1"> call of the model. </span><span class="kobospan" id="kobo.662.2">We also</span><a id="_idIndexMarker552" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.663.1"> instruct the tokenizer to skip the special tokens that are used by the tokenizer internally. </span><span class="kobospan" id="kobo.663.2">The tokenizer generates the string label from the vector that is passed in. </span><span class="kobospan" id="kobo.663.3">We print the prediction result. </span><span class="kobospan" id="kobo.663.4">In this case, the generated prediction by the model </span><span><span class="kobospan" id="kobo.664.1">is </span></span><span><strong class="source-inline1"><span class="kobospan" id="kobo.665.1">entailment</span></strong></span><span><span class="kobospan" id="kobo.666.1">:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.667.1">
prediction = tokenizer.decode(
    entailment_ids[0], skip_special_tokens=True, device=device)
print(prediction)</span></pre></li>			</ol>
			<pre class="console"><span class="kobospan1" id="kobo.668.1">
entailment</span></pre>			<h2 id="_idParaDest-249" class="calibre5"><a id="_idTextAnchor265" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.669.1">There’s more...</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.670.1">Now that we have shown an </span><a id="_idIndexMarker553" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.671.1">example in the case of entailment with a single sentence, the same framework can be used to process a batch of sentences to generate entailment predictions. </span><span class="kobospan" id="kobo.671.2">We will tailor </span><em class="italic"><span class="kobospan" id="kobo.672.1">steps 3</span></em><span class="kobospan" id="kobo.673.1">,</span><em class="italic"><span class="kobospan" id="kobo.674.1">4</span></em><span class="kobospan" id="kobo.675.1">, and </span><em class="italic"><span class="kobospan" id="kobo.676.1">5</span></em><span class="kobospan" id="kobo.677.1"> from the previous recipe for this example. </span><span class="kobospan" id="kobo.677.2">We initialize an array of two sentences for both </span><strong class="source-inline"><span class="kobospan" id="kobo.678.1">premise</span></strong><span class="kobospan" id="kobo.679.1"> and </span><strong class="source-inline"><span class="kobospan" id="kobo.680.1">hypothesis</span></strong><span class="kobospan" id="kobo.681.1">, respectively. </span><span class="kobospan" id="kobo.681.2">Both the </span><strong class="source-inline"><span class="kobospan" id="kobo.682.1">premise</span></strong><span class="kobospan" id="kobo.683.1"> sentences are the same, while the </span><strong class="source-inline"><span class="kobospan" id="kobo.684.1">hypothesis</span></strong><span class="kobospan" id="kobo.685.1"> sentences are of </span><strong class="source-inline"><span class="kobospan" id="kobo.686.1">entailment</span></strong><span class="kobospan" id="kobo.687.1"> and </span><span><strong class="source-inline"><span class="kobospan" id="kobo.688.1">contradiction</span></strong></span><span><span class="kobospan" id="kobo.689.1">, respectively:</span></span></p>
			<pre class="source-code"><span class="kobospan1" id="kobo.690.1">
premise = ["The corner coffee shop serves the most awesome coffee I have ever had.", "The corner coffee shop serves the most awesome coffee I have ever had."]
hypothesis = ["I love the coffee served by the corner coffee shop.", "I find the coffee served by the corner coffee shop too bitter for my taste."]</span></pre>			<p class="calibre3"><span class="kobospan" id="kobo.691.1">Since we have an array of sentences for both </span><strong class="source-inline"><span class="kobospan" id="kobo.692.1">premises</span></strong><span class="kobospan" id="kobo.693.1"> and </span><strong class="source-inline"><span class="kobospan" id="kobo.694.1">hypothesis</span></strong><span class="kobospan" id="kobo.695.1">, we create an array of concatenated inputs that combine the </span><strong class="source-inline"><span class="kobospan" id="kobo.696.1">tokenizer</span></strong><span class="kobospan" id="kobo.697.1"> instruction. </span><span class="kobospan" id="kobo.697.2">This array is used to pass to the tokenizer and we use the token IDs returned by </span><strong class="source-inline"><span class="kobospan" id="kobo.698.1">tokenizer</span></strong><span class="kobospan" id="kobo.699.1"> in the </span><span><span class="kobospan" id="kobo.700.1">next step:</span></span></p>
			<pre class="source-code"><span class="kobospan1" id="kobo.701.1">
premises_and_hypotheses = [f"mnli premise: {pre} 
    hypothesis: {hyp}" </span><strong class="bold1"><span class="kobospan1" id="kobo.702.1">for</span></strong><span class="kobospan1" id="kobo.703.1"> pre, hyp </span><strong class="bold1"><span class="kobospan1" id="kobo.704.1">in</span></strong><span class="kobospan1" id="kobo.705.1"> zip(premise, hypothesis)]
input_ids = tokenizer(
    text=premises_and_hypotheses, padding=True,
    return_tensors="pt").input_ids</span></pre>			<p class="calibre3"><span class="kobospan" id="kobo.706.1">We now generate the predictions using the same methodology that we used earlier. </span><span class="kobospan" id="kobo.706.2">However, in this step, we generate the inference label by iterating through the tensors returned by the model output </span><a id="_idIndexMarker554" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.707.1">and printing </span><span><span class="kobospan" id="kobo.708.1">the prediction:</span></span></p>
			<pre class="source-code"><span class="kobospan1" id="kobo.709.1">
entailment_ids = model.generate(input_ids.to(device), 
    max_new_tokens=20)
</span><strong class="bold1"><span class="kobospan1" id="kobo.710.1">for</span></strong><span class="kobospan1" id="kobo.711.1"> _tensor </span><strong class="bold1"><span class="kobospan1" id="kobo.712.1">in</span></strong><span class="kobospan1" id="kobo.713.1"> entailment_ids:
    entailment = tokenizer.decode(_tensor,</span><a id="_idTextAnchor266" class="pcalibre pcalibre1 calibre20"/><span class="kobospan1" id="kobo.714.1">
        skip_special_tokens=True, device=device)
    print(entailment)</span></pre>			<h1 id="_idParaDest-250" class="calibre7"><a id="_idTextAnchor267" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.715.1">Enhancing explainability via a classifier-invariant approach</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.716.1">Now, we will explore recipes that will allow</span><a id="_idIndexMarker555" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.717.1"> us to understand the decisions made by text classifiers. </span><span class="kobospan" id="kobo.717.2">We will explore techniques that will use a sentiment classifier and NLP explainability libraries to interpret the classification labels and their relation to the input text, especially in the aspect of individual words in </span><span><span class="kobospan" id="kobo.718.1">the text.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.719.1">Though a lot of the current models for</span><a id="_idIndexMarker556" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.720.1"> text classification in NLP are based on deep neural networks, it is difficult to interpret the results of classification via the network weights or parameters. </span><span class="kobospan" id="kobo.720.2">It is equally challenging to map these network parameters to the individual components or words in the input. </span><span class="kobospan" id="kobo.720.3">However, there are still a few techniques in the NLP space to help us understand the decisions made by the classifier. </span><span class="kobospan" id="kobo.720.4">We will explore these techniques in the current recipe and the </span><span><span class="kobospan" id="kobo.721.1">following one.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.722.1">In this recipe, we will learn</span><a id="_idIndexMarker557" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.723.1"> how to interpret the feature importance of each word in a text passage while being invariant of the classifier model. </span><span class="kobospan" id="kobo.723.2">This technique can be used for any text classifier as we treat the classifier as a black box and use the results of the predictions to infer the results from an </span><span><span class="kobospan" id="kobo.724.1">explainability perspective.</span></span></p>
			<h2 id="_idParaDest-251" class="calibre5"><a id="_idTextAnchor268" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.725.1">Getting ready</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.726.1">We will use the </span><strong class="source-inline"><span class="kobospan" id="kobo.727.1">lime</span></strong><span class="kobospan" id="kobo.728.1"> library for explainability. </span><span class="kobospan" id="kobo.728.2">You can use the </span><strong class="source-inline"><span class="kobospan" id="kobo.729.1">9.7_explanability_via_classifier.ipynb</span><a id="_idTextAnchor269" class="calibre6 pcalibre pcalibre1"/></strong><span class="kobospan" id="kobo.730.1"> notebook from the code site if you want to work from an </span><span><span class="kobospan" id="kobo.731.1">existing </span></span><span><a id="_idIndexMarker558" class="calibre6 pcalibre pcalibre1"/></span><span><span class="kobospan" id="kobo.732.1">notebook.</span></span></p>
			<h2 id="_idParaDest-252" class="calibre5"><a id="_idTextAnchor270" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.733.1">How to do it...</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.734.1">In this recipe, we will repurpose a classifier that we built in the </span><em class="italic"><span class="kobospan" id="kobo.735.1">Transformers</span></em><span class="kobospan" id="kobo.736.1"> chapter and use it to generate a sentiment prediction. </span><span class="kobospan" id="kobo.736.2">We will call this classifier multiple times with a perturbation of the input to understand the contribution of each word to the sentiment. </span><span class="kobospan" id="kobo.736.3">Let’s </span><span><span class="kobospan" id="kobo.737.1">get started:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.738.1">Do the </span><span><span class="kobospan" id="kobo.739.1">necessary imports:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.740.1">
import numpy as np
import torch
from lime.lime_text import LimeTextExplainer
from transformers import pipeline</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.741.1">In this step, we initialize the device and the pipeline for sentiment classification. </span><span class="kobospan" id="kobo.741.2">For more details on this step, please refer to chapter-8. </span><pre class="source-code"><span class="kobospan1" id="kobo.742.1">
device = torch.device(
    "cuda" if torch.cuda. </span><span class="kobospan1" id="kobo.742.2">is_available() else "cpu")
roberta_pipe = pipeline(
    "sentiment-analysis",
    model="siebert/sentiment-roberta-large-english",
    tokenizer="siebert/sentiment-roberta-large-english",
    top_k=1,
    device=device
)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.743.1">In this step, we initialize a</span><a id="_idIndexMarker559" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.744.1"> sample text passage along with setting the print options. </span><span class="kobospan" id="kobo.744.2">Setting the </span><a id="_idIndexMarker560" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.745.1">print options allows us to print the outputs in the later steps in an </span><span><span class="kobospan" id="kobo.746.1">easy-to-read format:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.747.1">
sample_text = "I really liked the Oppenheimer movie and found it truly entertaining and full of substance."
</span><span class="kobospan1" id="kobo.747.2">np.set_printoptions(suppress = True,
    formatter = {'float_kind':'{:f}'.format},
    precision = 2)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.748.1">In this step, we create a wrapper function for sentiment classification. </span><span class="kobospan" id="kobo.748.2">This method is used by the explainer to invoke the classification pipeline multiple times to gauge the</span><a id="_idIndexMarker561" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.749.1"> contribution of each word in </span><span><span class="kobospan" id="kobo.750.1">the passage:</span></span><pre class="source-code">
<strong class="bold1"><span class="kobospan1" id="kobo.751.1">def</span></strong><span class="kobospan1" id="kobo.752.1"> predict_prob(texts):
    preds = roberta_pipe(texts)
    preds = np.array([
        [label[0]['score'], 1 - label[0]['score']]
        </span><strong class="bold1"><span class="kobospan1" id="kobo.753.1">if</span></strong><span class="kobospan1" id="kobo.754.1"> label[0]['label'] == 'NEGATIVE'
        </span><strong class="bold1"><span class="kobospan1" id="kobo.755.1">else</span></strong><span class="kobospan1" id="kobo.756.1"> [1 - label[0]['score'], label[0]['score']]
        </span><strong class="bold1"><span class="kobospan1" id="kobo.757.1">for</span></strong><span class="kobospan1" id="kobo.758.1"> label </span><strong class="bold1"><span class="kobospan1" id="kobo.759.1">in</span></strong><span class="kobospan1" id="kobo.760.1"> preds
    ])
    </span><strong class="bold1"><span class="kobospan1" id="kobo.761.1">return</span></strong><span class="kobospan1" id="kobo.762.1"> preds</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.763.1">In this step, we instantiate the </span><strong class="source-inline1"><span class="kobospan" id="kobo.764.1">LimeTextExplainer</span></strong><span class="kobospan" id="kobo.765.1"> class and call the </span><strong class="source-inline1"><span class="kobospan" id="kobo.766.1">explain_instance</span></strong><span class="kobospan" id="kobo.767.1"> method for it. </span><span class="kobospan" id="kobo.767.2">This method takes the sample text along with the </span><strong class="source-inline1"><span class="kobospan" id="kobo.768.1">classifier</span></strong><span class="kobospan" id="kobo.769.1"> wrapper</span><a id="_idIndexMarker562" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.770.1"> function. </span><span class="kobospan" id="kobo.770.2">The wrapper function passed to this method expects it to take a single instance of a string and return the probabilities of the target classes. </span><span class="kobospan" id="kobo.770.3">In this case, our wrapper function accepts a simple string and returns the probabilities for the </span><strong class="source-inline1"><span class="kobospan" id="kobo.771.1">NEGATIVE</span></strong><span class="kobospan" id="kobo.772.1"> and </span><strong class="source-inline1"><span class="kobospan" id="kobo.773.1">POSITIVE</span></strong><span class="kobospan" id="kobo.774.1"> classes, respectively, and in </span><span><span class="kobospan" id="kobo.775.1">that order:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.776.1">
explainer = LimeTextExplainer(
    class_names=['NEGATIVE', 'POSITIVE'])
exp = explainer.explain_instance(
    text_instance=sample_text,
    classifier_fn=predict_prob)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.777.1">In this step, we print the </span><a id="_idIndexMarker563" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.778.1">class probabilities for the sample text. </span><span class="kobospan" id="kobo.778.2">As we observe, the sample text has been assigned</span><a id="_idIndexMarker564" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.779.1"> a </span><strong class="source-inline1"><span class="kobospan" id="kobo.780.1">POSITIVE</span></strong><span class="kobospan" id="kobo.781.1"> sentiment as per </span><span><span class="kobospan" id="kobo.782.1">the classifier:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.783.1">
original_prediction = predict_prob(sample_text)
print(original_prediction)</span></pre></li>			</ol>
			<pre class="console"><span class="kobospan1" id="kobo.784.1">
[[0.001083 0.998917]]</span></pre>			<ol class="calibre13">
				<li value="7" class="calibre14"><span class="kobospan" id="kobo.785.1">In this step, we print the explanations. </span><span class="kobospan" id="kobo.785.2">As we observe from the probabilities for each word, the words </span><strong class="source-inline1"><span class="kobospan" id="kobo.786.1">entertaining</span></strong><span class="kobospan" id="kobo.787.1"> and </span><strong class="source-inline1"><span class="kobospan" id="kobo.788.1">liked</span></strong><span class="kobospan" id="kobo.789.1"> contributed the most to the </span><strong class="source-inline1"><span class="kobospan" id="kobo.790.1">POSITIVE</span></strong><span class="kobospan" id="kobo.791.1"> class. </span><span class="kobospan" id="kobo.791.2">There are some words that contribute negatively to the positive sentiment, but overall, the sentence is classified </span><span><span class="kobospan" id="kobo.792.1">as positive:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.793.1">
print(np.array(exp.as_list()))</span></pre></li>			</ol>
			<pre class="console"><span class="kobospan1" id="kobo.794.1">
[['liked' '0.02466976195824297']
 ['entertaining' '0.023293546246506702']
 ['and' '0.018718510660163126']
 ['truly' '0.015312955730851004']
 ['Oppenheimer' '-0.012689413190611268']
 ['substance' '0.011282896692531665']
 ['of' '-0.007935237702088416']
 ['movie' '0.00665836523527015']
 ['it' '0.004033408096240486']
 ['found' '0.003214157926470171']]</span></pre>			<ol class="calibre13">
				<li value="8" class="calibre14"><span class="kobospan" id="kobo.795.1">Let’s initialize another text to</span><a id="_idIndexMarker565" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.796.1"> something with a </span><span><span class="kobospan" id="kobo.797.1">negative sentiment:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.798.1">
modified_text = "I found the Oppenheimer movie very slow, boring and veering on being too scientific."</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.799.1">Get the class </span><a id="_idIndexMarker566" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.800.1">probability as predicted by the classifier for the new text and </span><span><span class="kobospan" id="kobo.801.1">print it:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.802.1">
new_prediction = predict_prob(modified_text)
print(new_prediction)</span></pre></li>			</ol>
			<pre class="console"><span class="kobospan1" id="kobo.803.1">
[[0.999501 0.000499]]</span></pre>			<ol class="calibre13">
				<li value="10" class="calibre14"><span class="kobospan" id="kobo.804.1">Use the </span><strong class="source-inline1"><span class="kobospan" id="kobo.805.1">explainer</span></strong><span class="kobospan" id="kobo.806.1"> instance to evaluate the text and print the contribution of each word to the negative sentiment. </span><span class="kobospan" id="kobo.806.2">We observe that the words </span><strong class="source-inline1"><span class="kobospan" id="kobo.807.1">boring</span></strong><span class="kobospan" id="kobo.808.1"> and </span><strong class="source-inline1"><span class="kobospan" id="kobo.809.1">slow</span></strong><span class="kobospan" id="kobo.810.1"> contributed most to the </span><span><span class="kobospan" id="kobo.811.1">negative sentiment:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.812.1">
exp = explainer.explain_instance(
    text_instance=modified_text,
    classifier_fn=predict_prob)
print(np.array(exp.as_list()))</span></pre></li>			</ol>
			<pre class="console"><span class="kobospan1" id="kobo.813.1">
[['boring' '-0.1541527292742657']
 ['slow' '-0.13677434672789646']
 ['too' '-0.07536450832681185']
 ['veering' '-0.06154593708589755']
 ['Oppenheimer' '-0.021333762714731672']
 ['found' '0.015601753307753232']
 ['movie' '0.011810474276051267']
 ['I' '0.010142608</span><a id="_idTextAnchor271" class="pcalibre pcalibre1 calibre20"/><span class="kobospan1" id="kobo.814.1">38624105']
 ['the' '-0.008070326804220167']
 ['scientific' '-0.006083605323956207']]</span></pre>			<h2 id="_idParaDest-253" class="calibre5"><a id="_idTextAnchor272" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.815.1">There’s more...</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.816.1">Now that we have </span><a id="_idIndexMarker567" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.817.1">seen how to interpret the word contributions</span><a id="_idIndexMarker568" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.818.1"> for the sentiment classification, we want to further improve our recipe to provide a visual representation of </span><span><span class="kobospan" id="kobo.819.1">the explainability:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.820.1">Continuing on from </span><em class="italic"><span class="kobospan" id="kobo.821.1">step 5</span></em><span class="kobospan" id="kobo.822.1"> in the recipe, we can also print the explanations </span><span><span class="kobospan" id="kobo.823.1">using </span></span><span><strong class="source-inline1"><span class="kobospan" id="kobo.824.1">pyplot</span></strong></span><span><span class="kobospan" id="kobo.825.1">:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.826.1">
exp = explainer.explain_instance(text_instance=sample_text,
    classifier_fn=predict_prob)
_ = exp.as_pyplot_figure()</span></pre></li>			</ol>
			<div class="calibre2">
				<div id="_idContainer040" class="img---figure">
					<span class="kobospan" id="kobo.827.1"><img src="image/B18411_09_1.jpg" alt="" role="presentation" class="calibre4"/></span>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.828.1">Figure 9.1 – Probability contribution of each word in the sentence to the final class</span></p>
			<ol class="calibre13">
				<li value="2" class="calibre14"><span class="kobospan" id="kobo.829.1">We can also highlight the</span><a id="_idIndexMarker569" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.830.1"> exact words in the text. </span><span class="kobospan" id="kobo.830.2">The contribution of each word is also highlighted</span><a id="_idIndexMarker570" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.831.1"> using a light or dark shade of the assigned class, which, in this case, is orange. </span><span class="kobospan" id="kobo.831.2">The words with the blue high</span><a id="_idTextAnchor273" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.832.1">lights are the ones that contribute against the </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.833.1">POSITIVE</span></strong></span><span><span class="kobospan" id="kobo.834.1"> class:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.835.1">
exp.show_in_notebook()</span></pre></li>			</ol>
			<div class="calibre2">
				<div id="_idContainer041" class="img---figure">
					<span class="kobospan" id="kobo.836.1"><img src="image/B18411_09_2.jpg" alt="" role="presentation" class="calibre4"/></span>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.837.1">Figure 9.2 – The highlighted class association for each word</span></p>
			<h1 id="_idParaDest-254" class="calibre7"><a id="_idTextAnchor274" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.838.1">Enhancing explainability via text generation</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.839.1">In this recipe, we will learn how to </span><a id="_idIndexMarker571" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.840.1">understand the inference emitted by the classifier using text generation. </span><span class="kobospan" id="kobo.840.2">We will use the same classifier that we used in the </span><em class="italic"><span class="kobospan" id="kobo.841.1">Explainability via a classifier invariant approach</span></em><span class="kobospan" id="kobo.842.1"> recipe. </span><span class="kobospan" id="kobo.842.2">To better understand the behavior of the classifier in a random setting, we will replace the words in the input </span><a id="_idIndexMarker572" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.843.1">sentence with </span><span><span class="kobospan" id="kobo.844.1">different tokens.</span></span></p>
			<h2 id="_idParaDest-255" class="calibre5"><a id="_idTextAnchor275" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.845.1">Getting ready</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.846.1">We will need to install a </span><strong class="source-inline"><span class="kobospan" id="kobo.847.1">spacy</span></strong><span class="kobospan" id="kobo.848.1"> artifact for this recipe. </span><span class="kobospan" id="kobo.848.2">Please use the following command in your environment before starting </span><span><span class="kobospan" id="kobo.849.1">this recipe.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.850.1">Now that we have installed </span><strong class="source-inline"><span class="kobospan" id="kobo.851.1">spacy</span></strong><span class="kobospan" id="kobo.852.1">, we will need to download the </span><strong class="source-inline"><span class="kobospan" id="kobo.853.1">en_core_web_sm</span></strong><span class="kobospan" id="kobo.854.1"> pipeline using the following </span><span><span class="kobospan" id="kobo.855.1">step beforehand:</span></span></p>
			<pre class="console"><span class="kobospan1" id="kobo.856.1">
python3 -m spacy download en_core_web_sm</span></pre>			<p class="calibre3"><span class="kobospan" id="kobo.857.1">You can use the </span><strong class="source-inline"><span class="kobospan" id="kobo.858.1">9.8_explanability_via_generation.ipynb</span></strong><span class="kobospan" id="kobo.859.1"> notebook from the code site if you need to work from an </span><span><span class="kobospan" id="kobo.860.1">existing notebook.</span></span></p>
			<h2 id="_idParaDest-256" class="calibre5"><a id="_idTextAnchor276" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.861.1">How to do it</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.862.1">Let’s </span><span><span class="kobospan" id="kobo.863.1">get started:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.864.1">Do the </span><span><span class="kobospan" id="kobo.865.1">necessary imports:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.866.1">
import numpy as np
import spacy
import time
import torch
from anchor import anchor_text
from transformers import pipeline</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.867.1">In this step, we initialize the </span><strong class="source-inline1"><span class="kobospan" id="kobo.868.1">spacy</span></strong><span class="kobospan" id="kobo.869.1"> pipeline with the </span><strong class="source-inline1"><span class="kobospan" id="kobo.870.1">en_core_web_sm</span></strong><span class="kobospan" id="kobo.871.1"> model. </span><span class="kobospan" id="kobo.871.2">This pipeline contains the components for </span><strong class="source-inline1"><span class="kobospan" id="kobo.872.1">tok2vec</span></strong><span class="kobospan" id="kobo.873.1">, </span><strong class="source-inline1"><span class="kobospan" id="kobo.874.1">tagger</span></strong><span class="kobospan" id="kobo.875.1">, </span><strong class="source-inline1"><span class="kobospan" id="kobo.876.1">parser</span></strong><span class="kobospan" id="kobo.877.1">, </span><strong class="source-inline1"><span class="kobospan" id="kobo.878.1">ner</span></strong><span class="kobospan" id="kobo.879.1">, </span><strong class="source-inline1"><span class="kobospan" id="kobo.880.1">lemmatizer</span></strong><span class="kobospan" id="kobo.881.1">, and so on, and is optimized for </span><span><span class="kobospan" id="kobo.882.1">the CPU:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.883.1">
nlp = spacy.load('en_core_web_sm')</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.884.1"> In this step, we initialize the device and our classifier. </span><span class="kobospan" id="kobo.884.2">We use the same sentence classifier that we used in the </span><em class="italic"><span class="kobospan" id="kobo.885.1">Explainability via a classifier invariant approach</span></em><span class="kobospan" id="kobo.886.1"> recipe. </span><span class="kobospan" id="kobo.886.2">The idea is to </span><a id="_idIndexMarker573" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.887.1">understand the same classifier and observe how its classification behaves for different inputs, as</span><a id="_idIndexMarker574" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.888.1"> generated by the </span><strong class="source-inline1"><span class="kobospan" id="kobo.889.1">anchor</span></strong> <span><span class="kobospan" id="kobo.890.1">explainability library:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.891.1">
device = torch.device("cuda" </span><strong class="bold1"><span class="kobospan1" id="kobo.892.1">if</span></strong><span class="kobospan1" id="kobo.893.1"> torch.cuda.is_available(</span><strong class="bold1"><span class="kobospan1" id="kobo.894.1"># Load model directly</span></strong><span class="kobospan1" id="kobo.895.1">
from transformers import( AutoTokenizer, 
    AutoModelForSequenceClassification)
tokenizer = AutoTokenizer.from_pretrained(
    "jonathanfernandes/imdb_model")
model = AutoModelForSequenceClassification.from_pretrained(
    "jonathanfernandes/imdb_model")) </span><strong class="bold1"><span class="kobospan1" id="kobo.896.1">else</span></strong><span class="kobospan1" id="kobo.897.1"> "cpu")
classifier = pipeline(
    "sentiment-analysis",
    model="siebert/sentiment-roberta-large-english",
    tokenizer="siebert/sentiment-roberta-large-english",
    top_k=1,
    device=device)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.898.1">In this step, we define a function that takes a list of sentences and emits a list of </span><strong class="source-inline1"><span class="kobospan" id="kobo.899.1">POSITIVE</span></strong><span class="kobospan" id="kobo.900.1"> or </span><strong class="source-inline1"><span class="kobospan" id="kobo.901.1">NEGATIVE</span></strong><span class="kobospan" id="kobo.902.1"> labels </span><a id="_idIndexMarker575" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.903.1">for them.  </span><span class="kobospan" id="kobo.903.2">This method internally calls the classifier that was initialized in the </span><span><span class="kobospan" id="kobo.904.1">previous</span></span><span><a id="_idIndexMarker576" class="calibre6 pcalibre pcalibre1"/></span><span><span class="kobospan" id="kobo.905.1"> step:</span></span><pre class="source-code">
<strong class="bold1"><span class="kobospan1" id="kobo.906.1">def</span></strong><span class="kobospan1" id="kobo.907.1"> predict_prob(texts):
    preds = classifier(texts)
    preds = np.array([
        0 </span><strong class="bold1"><span class="kobospan1" id="kobo.908.1">if</span></strong><span class="kobospan1" id="kobo.909.1"> label[0]['label'] == 'NEGATIVE' </span><strong class="bold1"><span class="kobospan1" id="kobo.910.1">else</span></strong><span class="kobospan1" id="kobo.911.1"> 1
        </span><strong class="bold1"><span class="kobospan1" id="kobo.912.1">for</span></strong><span class="kobospan1" id="kobo.913.1"> label </span><strong class="bold1"><span class="kobospan1" id="kobo.914.1">in</span></strong><span class="kobospan1" id="kobo.915.1"> preds])
    </span><strong class="bold1"><span class="kobospan1" id="kobo.916.1">return</span></strong><span class="kobospan1" id="kobo.917.1"> preds</span></pre><p class="calibre3"><span class="kobospan" id="kobo.918.1">In this step, we initialize the Anchor explainer. </span><span class="kobospan" id="kobo.918.2">We instantiate it with the </span><strong class="source-inline"><span class="kobospan" id="kobo.919.1">spacy</span></strong><span class="kobospan" id="kobo.920.1"> pipeline, the class labels, and </span><strong class="source-inline"><span class="kobospan" id="kobo.921.1">use_unk_distribution</span></strong><span class="kobospan" id="kobo.922.1"> as true. </span><span class="kobospan" id="kobo.922.2">The class labels in this case are </span><strong class="source-inline"><span class="kobospan" id="kobo.923.1">NEGATIVE</span></strong><span class="kobospan" id="kobo.924.1"> and </span><strong class="source-inline"><span class="kobospan" id="kobo.925.1">POSITIVE</span></strong><span class="kobospan" id="kobo.926.1">. </span><span class="kobospan" id="kobo.926.2">The </span><strong class="source-inline"><span class="kobospan" id="kobo.927.1">use_unk_distribution</span></strong><span class="kobospan" id="kobo.928.1"> parameter specifies that the explainer uses the </span><strong class="source-inline"><span class="kobospan" id="kobo.929.1">UNK</span></strong><span class="kobospan" id="kobo.930.1"> token for masked words when it generates text for explanability.explainer = anchor_text.AnchorText(nlp, [‘NEGATIVE’, ‘</span><span><span class="kobospan" id="kobo.931.1">POSITIVE’], use_unk_distribution=True)</span></span></p></li>				<li class="calibre14"><span class="kobospan" id="kobo.932.1">In this step, we initialize a passage of text. </span><span class="kobospan" id="kobo.932.2">We use that text sentence to predict its class probability by using the </span><strong class="source-inline1"><span class="kobospan" id="kobo.933.1">predict_prob</span></strong><span class="kobospan" id="kobo.934.1"> method and print </span><span><span class="kobospan" id="kobo.935.1">the prediction:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.936.1">
text = 'The little mermaid is a good story.'
</span><span class="kobospan1" id="kobo.936.2">pred = explainer.class_names[predict_prob([text])[0]]
print('Prediction: %s' % pred)
Prediction: POSITIVE</span></pre><p class="calibre3"><span class="kobospan" id="kobo.937.1">In this step, we call the </span><strong class="source-inline"><span class="kobospan" id="kobo.938.1">explain_instance</span></strong><span class="kobospan" id="kobo.939.1"> method for the explainer instance. </span><span class="kobospan" id="kobo.939.2">We pass it the input sentence, the </span><strong class="source-inline"><span class="kobospan" id="kobo.940.1">predict_prob</span></strong><span class="kobospan" id="kobo.941.1"> method, and a </span><strong class="source-inline"><span class="kobospan" id="kobo.942.1">threshold</span></strong><span class="kobospan" id="kobo.943.1">. </span><span class="kobospan" id="kobo.943.2">The explainer instance uses the </span><strong class="source-inline"><span class="kobospan" id="kobo.944.1">predict_prob</span></strong><span class="kobospan" id="kobo.945.1"> method to invoke the classifier for different variations of the input sentence to explain what words contribute the most. </span><span class="kobospan" id="kobo.945.2">It also identifies what class labels are emitted when some words in the input sentence are replaced by the </span><strong class="source-inline"><span class="kobospan" id="kobo.946.1">UNK</span></strong><span class="kobospan" id="kobo.947.1"> token. </span><span class="kobospan" id="kobo.947.2">The </span><strong class="source-inline"><span class="kobospan" id="kobo.948.1">threshold</span></strong><span class="kobospan" id="kobo.949.1"> parameter defines the minimum probability for a given class under which all the generated </span><a id="_idIndexMarker577" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.950.1">samples are to be ignored. </span><span class="kobospan" id="kobo.950.2">This effectively means that all the sentences generated by the </span><a id="_idIndexMarker578" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.951.1">explainer will have the probability greater than the threshold, for a given class.exp = explainer.explain_instance(text, </span><span><span class="kobospan" id="kobo.952.1">predict_prob, threshold=0.95)</span></span></p></li>				<li class="calibre14"><span class="kobospan" id="kobo.953.1">We print the </span><strong class="source-inline1"><span class="kobospan" id="kobo.954.1">anchor</span></strong><span class="kobospan" id="kobo.955.1"> words that contribute the most to the </span><strong class="source-inline1"><span class="kobospan" id="kobo.956.1">POSITIVE</span></strong><span class="kobospan" id="kobo.957.1"> label in this case. </span><span class="kobospan" id="kobo.957.2">We also print the precision as measured by the explainer. </span><span class="kobospan" id="kobo.957.3">We observe that it identifies the words </span><strong class="source-inline1"><span class="kobospan" id="kobo.958.1">good</span></strong><span class="kobospan" id="kobo.959.1">, </span><strong class="source-inline1"><span class="kobospan" id="kobo.960.1">a</span></strong><span class="kobospan" id="kobo.961.1">, and </span><strong class="source-inline1"><span class="kobospan" id="kobo.962.1">is</span></strong><span class="kobospan" id="kobo.963.1"> as contributing the most to the </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.964.1">POSITIVE</span></strong></span><span><span class="kobospan" id="kobo.965.1"> classification:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.966.1">
print('Anchor: %s' % (' AND '.join(exp.names())))
print('Precision: %.2f' % exp.precision())</span></pre></li>			</ol>
			<pre class="console"><span class="kobospan1" id="kobo.967.1">
Anchor: good AND a AND is
Precision: 1.00</span></pre>			<ol class="calibre13">
				<li value="7" class="calibre14"><span class="kobospan" id="kobo.968.1">We print some of the possible sentences that the explainer believes would result in a </span><strong class="source-inline1"><span class="kobospan" id="kobo.969.1">POSITIVE</span></strong><span class="kobospan" id="kobo.970.1"> classification. </span><span class="kobospan" id="kobo.970.2">The explainer perturbs the input sentence by replacing one or more of the words with the </span><strong class="source-inline1"><span class="kobospan" id="kobo.971.1">UNK</span></strong><span class="kobospan" id="kobo.972.1"> token and invokes the classifier method on the perturbed sentence. </span><span class="kobospan" id="kobo.972.2">There are some interesting observations on how the classifier behaves. </span><span class="kobospan" id="kobo.972.3">For example, the sentence </span><strong class="source-inline1"><span class="kobospan" id="kobo.973.1">The UNK UNK is a good story UNK</span></strong><span class="kobospan" id="kobo.974.1"> has been labeled as </span><strong class="source-inline1"><span class="kobospan" id="kobo.975.1">POSITIVE</span></strong><span class="kobospan" id="kobo.976.1">. </span><span class="kobospan" id="kobo.976.2">This indicates that the title of the story is irrelevant to the classification. </span><span class="kobospan" id="kobo.976.3">Another interesting example is the sentence </span><strong class="source-inline1"><span class="kobospan" id="kobo.977.1">The UNK mermaid is a good UNK UNK</span></strong><span class="kobospan" id="kobo.978.1">. </span><span class="kobospan" id="kobo.978.2">In this sentence, we observe that the classifier is invariant to the object in context, which, in </span><a id="_idIndexMarker579" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.979.1">this case, is </span><span><span class="kobospan" id="kobo.980.1">a story:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.981.1">
print('\n'.join([x[0] </span><strong class="bold1"><span class="kobospan1" id="kobo.982.1">for</span></strong><span class="kobospan1" id="kobo.983.1"> x </span><strong class="bold1"><span class="kobospan1" id="kobo.984.1">in</span></strong><span class="kobospan1" id="kobo.985.1"> exp.examples(
    only_same_prediction=True)]))</span></pre></li>			</ol>
			<pre class="console"><span class="kobospan1" id="kobo.986.1">
The little UNK is a good UNK .
</span><span class="kobospan1" id="kobo.986.2">The UNK mermaid is a good story .
</span><span class="kobospan1" id="kobo.986.3">The UNK UNK is a good story UNK
UNK little mermaid is a good story UNK
The UNK mermaid is a good UNK .
</span><span class="kobospan1" id="kobo.986.4">UNK little UNK is a good UNK .
</span><span class="kobospan1" id="kobo.986.5">The little mermaid is a good story UNK
The UNK UNK is a good UNK .
</span><span class="kobospan1" id="kobo.986.6">The little UNK is a good UNK .
</span><span class="kobospan1" id="kobo.986.7">The little mermaid is a good UNK .</span></pre>			<ol class="calibre13">
				<li value="8" class="calibre14"><span class="kobospan" id="kobo.987.1">Similar to the previous step, we now ask the explainer to print sentences that would result in a </span><strong class="source-inline1"><span class="kobospan" id="kobo.988.1">NEGATIVE</span></strong><span class="kobospan" id="kobo.989.1"> classification. </span><span class="kobospan" id="kobo.989.2">In this particular case, the explainer was unable to generate </span><a id="_idIndexMarker580" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.990.1">any negative examples by just replacing the words. </span><span class="kobospan" id="kobo.990.2">The explainer is unable to generate any </span><strong class="source-inline1"><span class="kobospan" id="kobo.991.1">NEGATIVE</span></strong><span class="kobospan" id="kobo.992.1"> examples. </span><span class="kobospan" id="kobo.992.2">This happens because the explainer can only use the </span><strong class="source-inline1"><span class="kobospan" id="kobo.993.1">UNK</span></strong><span class="kobospan" id="kobo.994.1"> token to perturb the input sentence. </span><span class="kobospan" id="kobo.994.2">And since the </span><strong class="source-inline1"><span class="kobospan" id="kobo.995.1">UNK</span></strong><span class="kobospan" id="kobo.996.1"> token is not associated with any </span><strong class="source-inline1"><span class="kobospan" id="kobo.997.1">POSITIVE</span></strong><span class="kobospan" id="kobo.998.1"> or </span><strong class="source-inline1"><span class="kobospan" id="kobo.999.1">NEGATIVE</span></strong><span class="kobospan" id="kobo.1000.1"> sentiment, using just that token does not provide a way to affect the classifier to generate a </span><strong class="source-inline1"><span class="kobospan" id="kobo.1001.1">NEGATIVE</span></strong><span class="kobospan" id="kobo.1002.1"> classification. </span><span class="kobospan" id="kobo.1002.2">We get no output from </span><span><span class="kobospan" id="kobo.1003.1">this step:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.1004.1">
print('\n'.join([x[0] </span><strong class="bold1"><span class="kobospan1" id="kobo.1005.1">for</span></strong><span class="kobospan1" id="kobo.1006.1"> x </span><strong class="bold1"><span class="kobospan1" id="kobo.1007.1">in</span></strong><span class="kobospan1" id="kobo.1008.1"> exp.examples(
    only_different_prediction=True)]))</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.1009.1">So far, we used the </span><strong class="source-inline1"><span class="kobospan" id="kobo.1010.1">UNK</span></strong><span class="kobospan" id="kobo.1011.1"> token to vary or perturb the input to the classifier. </span><span class="kobospan" id="kobo.1011.2">The presence of the </span><strong class="source-inline1"><span class="kobospan" id="kobo.1012.1">UNK</span></strong><span class="kobospan" id="kobo.1013.1"> token in the text makes it unnatural. </span><span class="kobospan" id="kobo.1013.2">To understand the classifier better, it would be useful to enumerate natural sentences and understand how those affect the classification. </span><span class="kobospan" id="kobo.1013.3">We will use </span><strong class="bold"><span class="kobospan" id="kobo.1014.1">BERT</span></strong><span class="kobospan" id="kobo.1015.1"> to perturb the input and get the explainer to generate natural sentences. </span><span class="kobospan" id="kobo.1015.2">This will help us better understand how </span><a id="_idIndexMarker581" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.1016.1">the results differ in the context of sentences that </span><span><span class="kobospan" id="kobo.1017.1">are natural:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.1018.1">
explainer = anchor_text.AnchorText(nlp, 
    ['negative', 'positive'],
    use_unk_distribution=False)
exp = explainer.explain_instance(text, 
    predict_prob, threshold=0.95)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.1019.1">We now print some sentences for which the classifier thinks the label would be </span><strong class="source-inline1"><span class="kobospan" id="kobo.1020.1">POSITIVE</span></strong><span class="kobospan" id="kobo.1021.1">. </span><span class="kobospan" id="kobo.1021.2">In this instance, we observe that the explainer generates sentences that are natural. </span><span class="kobospan" id="kobo.1021.3">For example, the generated sentence </span><strong class="source-inline1"><span class="kobospan" id="kobo.1022.1">my little mermaid tells a good story</span></strong><span class="kobospan" id="kobo.1023.1"> replaced the word </span><strong class="source-inline1"><span class="kobospan" id="kobo.1024.1">the</span></strong><span class="kobospan" id="kobo.1025.1"> in the original sentence with </span><strong class="source-inline1"><span class="kobospan" id="kobo.1026.1">my</span></strong><span class="kobospan" id="kobo.1027.1">. </span><span class="kobospan" id="kobo.1027.2">This word was</span><a id="_idIndexMarker582" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.1028.1"> generated via BERT. </span><span class="kobospan" id="kobo.1028.2">BERT uses the encoder part of the Transformer architecture and has been trained to predict missing words in a sentence by masking them. </span><span class="kobospan" id="kobo.1028.3">The explainer in this case masks the individual words in the input sentence and uses BERT to generate the replacement word. </span><span class="kobospan" id="kobo.1028.4">Since the underlying model to generate text is a probabilistic model, your output might differ from the following and also vary </span><span><span class="kobospan" id="kobo.1029.1">between runs:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.1030.1">
print('\n'.join([x[0] </span><strong class="bold1"><span class="kobospan1" id="kobo.1031.1">for</span></strong><span class="kobospan1" id="kobo.1032.1"> x </span><strong class="bold1"><span class="kobospan1" id="kobo.1033.1">in</span></strong><span class="kobospan1" id="kobo.1034.1"> exp.examples(
    only_same_prediction=True)]))</span></pre></li>			</ol>
			<pre class="console"><span class="kobospan1" id="kobo.1035.1">
the weeping mermaid gives his good story .
</span><span class="kobospan1" id="kobo.1035.2">Me ##rmaid mermaid : a good story .
</span><span class="kobospan1" id="kobo.1035.3">rainbow moon mermaid theater " good story "
my little mermaid tells a good story .
</span><span class="kobospan1" id="kobo.1035.4">Pretty little mermaid tells a good story .
</span><span class="kobospan1" id="kobo.1035.5">My black mermaid song sweet good story ;
" little mermaid : very good story .
</span><span class="kobospan1" id="kobo.1035.6">This damned mermaid gives a good story .
</span><span class="kobospan1" id="kobo.1035.7">| " mermaid " : good story .
</span><span class="kobospan1" id="kobo.1035.8">Me ##rmaid mermaid : very good story .</span></pre>			<ol class="calibre13">
				<li value="11" class="calibre14"><span class="kobospan" id="kobo.1036.1">We now print some sentences for which the classifier thinks the label would be </span><strong class="source-inline1"><span class="kobospan" id="kobo.1037.1">NEGATIVE</span></strong><span class="kobospan" id="kobo.1038.1">. </span><span class="kobospan" id="kobo.1038.2">Though</span><a id="_idIndexMarker583" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.1039.1"> not all the sentences appear to have a </span><strong class="source-inline1"><span class="kobospan" id="kobo.1040.1">NEGATIVE</span></strong><span class="kobospan" id="kobo.1041.1"> sentiment, there are quite a few of them with such </span><span><span class="kobospan" id="kobo.1042.1">a</span></span><span><a id="_idIndexMarker584" class="calibre6 pcalibre pcalibre1"/></span><span><span class="kobospan" id="kobo.1043.1"> sentiment:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.1044.1">
print('\n'.join([x[0] </span><strong class="bold1"><span class="kobospan1" id="kobo.1045.1">for</span></strong><span class="kobospan1" id="kobo.1046.1"> x </span><strong class="bold1"><span class="kobospan1" id="kobo.1047.1">in</span></strong><span class="kobospan1" id="kobo.1048.1"> exp.examples(
    only_different_prediction=True)]))</span></pre></li>			</ol>
			<pre class="console"><span class="kobospan1" id="kobo.1049.1">
' til mermaid brings a good story …
only little mermaid : too good story ##book
smash hit mermaid with any good story ...
</span><span class="kobospan1" id="kobo.1049.2">nor did mermaid tell a good story !
</span><span class="kobospan1" id="kobo.1049.3">† denotes mermaid side / good story .
</span><span class="kobospan1" id="kobo.1049.4">no native mermaid has a good story .
</span><span class="kobospan1" id="kobo.1049.5">no ordinary mermaid is a good story .
</span><span class="kobospan1" id="kobo.1049.6">Very little mermaid ain any good story yet
miss rainbow mermaid made a good story .
</span><span class="kobospan1" id="kobo.1049.7">The gorgeous mermaid ain your good story (</span></pre>		</div>
	</body></html>