<html><head></head><body>
		<div><h1 id="_idParaDest-223" class="chapter-number"><a id="_idTextAnchor231" class="calibre6 pcalibre pcalibre1"/>9</h1>
			<h1 id="_idParaDest-224" class="calibre7"><a id="_idTextAnchor232" class="calibre6 pcalibre pcalibre1"/>Natural Language Understanding</h1>
			<p class="calibre3">In this chapter, we will explore recipes that will allow us to interpret and understand the text contained in short as well as<a id="_idIndexMarker482" class="calibre6 pcalibre pcalibre1"/> long passages. <strong class="bold">Natural language understanding</strong> (<strong class="bold">NLU</strong>) is a very broad term and the various systems developed as part of NLU do not interpret or understand a passage of text the same way a human reader would. However, based on the specificity of the task, we can create some applications that can be combined to generate an interpretation or understanding that can be used to solve a given problem related to text processing.</p>
			<p class="calibre3">Organizations that have a huge document corpus need a seamless way to search through documents. More specifically, what users really need is an answer to a specific question without having to glean through a list of documents that are returned as part of a document search. Users would prefer the query to be formulated as a question in natural language and the answer to be emitted in the same manner.</p>
			<p class="calibre3">Another set of applications is that of document summarization and text entailment. While processing a large set of documents, it is helpful if the document length can be shortened without the loss of meaning or context. Additionally, it’s important to determine whether the information contained in the document at the sentence level entails itself.</p>
			<p class="calibre3">While we work on processing and classifying the documents, there are always challenges in understanding why or how the model assigns a label to a piece of text – more specifically, what parts of the text contribute to the different labels.</p>
			<p class="calibre3">This chapter will cover different techniques to explore the various aspects previously described. We will follow recipes that will allow us to perform these tasks and understand the underlying building blocks that help us achieve the end goals.</p>
			<p class="calibre3">As part of this chapter, we will build recipes for the following tasks:</p>
			<ul class="calibre15">
				<li class="calibre14">Answering questions from a short text passage</li>
				<li class="calibre14">Answering questions from a long text passage</li>
				<li class="calibre14">Answering questions from a document corpus in an extractive manner</li>
				<li class="calibre14">Answering questions from a document corpus in an abstractive manner</li>
				<li class="calibre14">Summarizing text using pretrained models based on Transformers</li>
				<li class="calibre14">Detecting sentence entailment</li>
				<li class="calibre14">Enhancing explainability via a classifier-invariant approach</li>
				<li class="calibre14">Enhancing explainability via text generation<a id="_idTextAnchor233" class="calibre6 pcalibre pcalibre1"/><a id="_idTextAnchor234" class="calibre6 pcalibre pcalibre1"/></li>
			</ul>
			<h1 id="_idParaDest-225" class="calibre7"><a id="_idTextAnchor235" class="calibre6 pcalibre pcalibre1"/>Technical requirements</h1>
			<p class="calibre3">The code for this chapter is in a folder named <code>Chapter9</code> in the GitHub repository of the book (<a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter09" class="calibre6 pcalibre pcalibre1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter09</a>).</p>
			<p class="calibre3">As in previous chapters, the packages required for this chapter are part of the <code>poetry</code> environment. Alternatively, you can install all the packages using the <code>requirements.txt</code> file.</p>
			<h1 id="_idParaDest-226" class="calibre7"><a id="_idTextAnchor236" class="calibre6 pcalibre pcalibre1"/>Answering questions from a short text passage</h1>
			<p class="calibre3">To get started with question <a id="_idIndexMarker483" class="calibre6 pcalibre pcalibre1"/>answering, we will start with <a id="_idIndexMarker484" class="calibre6 pcalibre pcalibre1"/>a simple recipe that can answer a question from a short passage.</p>
			<h2 id="_idParaDest-227" class="calibre5"><a id="_idTextAnchor237" class="calibre6 pcalibre pcalibre1"/>Getting ready</h2>
			<p class="calibre3">As part of this chapter, we will use the libraries from Hugging Face (<a href="http://huggingface.co" class="calibre6 pcalibre pcalibre1">huggingface.co</a>). For this recipe, we will use the <code>BertForQuestionAnswering</code> and <code>BertTokenizer</code> modules from the Transformers package. The <code>BertForQuestionAnswering</code> model uses the base BERT large uncased model that was trained on the <code>SQuAD</code> dataset and fine-tuned for the question-answering task. This pre-trained model can be used to load a text passage and answer questions based on the contents of the passage. You can use the <code>9.1_question_answering.ipynb</code> notebook from the code site if you need to work from an existing notebook.</p>
			<h2 id="_idParaDest-228" class="calibre5"><a id="_idTextAnchor238" class="calibre6 pcalibre pcalibre1"/>How to do it...</h2>
			<p class="calibre3">In this recipe, we will load a pretrained model that has been trained on the SQuAD dataset (<a href="https://huggingface.co/datasets/squad" class="calibre6 pcalibre pcalibre1">https://huggingface.co/datasets/squad</a>).</p>
			<p class="calibre3">The recipe does the following things:</p>
			<ul class="calibre15">
				<li class="calibre14">It initializes a question-answering pipeline based on the pre-trained <strong class="source-inline1">BertForQuestionAnswering</strong> model and <strong class="source-inline1">BertTokenizer</strong> tokenizer.</li>
				<li class="calibre14">It further<a id="_idIndexMarker485" class="calibre6 pcalibre pcalibre1"/> initializes a context passage and a question and emits the output of the answer based on these two parameters. It also prints the exact text of the answer.</li>
				<li class="calibre14">It asks a follow-up question to the same pipeline by just changing the question text, and prints the <a id="_idIndexMarker486" class="calibre6 pcalibre pcalibre1"/>exact text answer to the question.</li>
			</ul>
			<p class="calibre3">The steps for the recipe are as follows:</p>
			<ol class="calibre13">
				<li class="calibre14">Do the necessary imports to import the necessary types and functions from the <strong class="source-inline1">datasets</strong> package:<pre class="source-code">
from transformers import (
    pipeline, BertForQuestionAnswering, BertTokenizer)
import torch</pre></li>				<li class="calibre14">In this step, we initialize the model and tokenizer, respectively, using the pre-trained <strong class="source-inline1">bert-large-uncased-whole-word-masking-finetuned-squad</strong> artifacts. These will be downloaded from the Hugging Face website if they are not present locally on the machine as part of these calls. We have chosen the specific model and tokenizer for our recipe, but feel free to explore other models on the Hugging Face site that might suit your needs. As a generic step for this and the following recipe, we discover whether there are any GPU devices in the system and attempt to use them. If a GPU is not detected, we use the CPU instead:<pre class="source-code">
device = torch.device("cuda" if torch.cuda.is_available() 
    else "cpu")
qa_model = BertForQuestionAnswering.from_pretrained(
    'bert-large-uncased-whole-word-masking-finetuned-squad',
    device_map=device)
qa_tokenizer = BertTokenizer.from_pretrained(
    'bert-large-uncased-whole-word-masking-finetuned-squad',
    device=device)</pre></li>				<li class="calibre14">In this step, we initialize a question-answering pipeline with the model and tokenizer. The task type<a id="_idIndexMarker487" class="calibre6 pcalibre pcalibre1"/> for this pipeline is <a id="_idIndexMarker488" class="calibre6 pcalibre pcalibre1"/>set to <strong class="source-inline1">question-answering</strong>:<pre class="source-code">
question_answer_pipeline = pipeline(
    "question-answering", model=qa_model,
    tokenizer=qa_tokenizer)</pre></li>				<li class="calibre14">In this step, we initialize a <strong class="source-inline1">context</strong> passage. This passage was generated as part of our <em class="italic">Text Generation via Transformers</em> example in <a href="B18411_08.xhtml#_idTextAnchor205" class="calibre6 pcalibre pcalibre1"><em class="italic">Chapter 8</em></a>.  It’s entirely acceptable if you want to use a different passage:<pre class="source-code">
context = "The cat had no business entering the neighbors garage, but she was there to help. The neighbor, who asked not to be identified, said she didn't know what to make of the cat's behavior. She said it seemed like it was trying to get into her home, and that she was afraid for her life. The neighbor said that when she went to check on her cat, it ran into the neighbor's garage and hit her in the face, knocking her to the ground."</pre></li>				<li class="calibre14">In this step, we initialize a question text, invoke the pipeline with the context and question, and store the result in a variable. The type of the result is a Python <strong class="source-inline1">dict</strong> object:<pre class="source-code">
question = "Where was the cat trying to enter?"
result = question_answer_pipeline(question=question, 
    context=context)</pre></li>				<li class="calibre14">In this step, we print the value of the result. The <strong class="source-inline1">score</strong> value shows the probability of the<a id="_idIndexMarker489" class="calibre6 pcalibre pcalibre1"/> answer. The <strong class="source-inline1">start</strong> and <strong class="source-inline1">end</strong> values denote the start and end character indices in the <strong class="source-inline1">context</strong> passage that constitute the answer. The <strong class="source-inline1">answer</strong> value denotes the actual text of the answer:<pre class="source-code">
print(result)</pre></li>			</ol>
			<pre class="console">
{'score': 0.25, 'start': 33, 'end': 54, 'answer': 'the neighbors garage,'}</pre>			<ol class="calibre13">
				<li value="7" class="calibre14">In this step,  we print<a id="_idIndexMarker490" class="calibre6 pcalibre pcalibre1"/> the exact text answer. This is present in the <strong class="source-inline1">answer</strong> key in the <strong class="source-inline1">result</strong> dictionary:<pre class="source-code">
print(result['answer'])</pre></li>			</ol>
			<pre class="console">
the neighbors garage,</pre>			<ol class="calibre13">
				<li value="8" class="calibre14">In this step, we ask another question using the same context and print the result:<pre class="source-code">
question = "What did the cat do after entering the garage"
result = question_answer_pipeline(
    question=question, context=context)
print(result['answer'])</pre></li>			</ol>
			<pre class="console">
hit her in the face, knocking her to the<a id="_idTextAnchor239" class="pcalibre pcalibre1 calibre20"/> ground.</pre>			<h1 id="_idParaDest-229" class="calibre7"><a id="_idTextAnchor240" class="calibre6 pcalibre pcalibre1"/>Answering questions from a long text passage</h1>
			<p class="calibre3">In the previous recipe, we learned an <a id="_idIndexMarker491" class="calibre6 pcalibre pcalibre1"/>approach to extract the answer to a question, given a context. This pattern involves the model retrieving the answer from the given context. The model cannot answer a question that is not contained in the <a id="_idIndexMarker492" class="calibre6 pcalibre pcalibre1"/>context. This does serve a purpose where we want an answer from a given context. This type of question-answering system is<a id="_idIndexMarker493" class="calibre6 pcalibre pcalibre1"/> defined as <strong class="bold">Closed Domain Question </strong><strong class="bold">Answering</strong> (<strong class="bold">CDQA</strong>).</p>
			<p class="calibre3">There is another system of question answering that can answer questions that are general in nature. These systems are trained on larger corpora. This training provides them with the ability to answer questions that<a id="_idIndexMarker494" class="calibre6 pcalibre pcalibre1"/> are open in nature. These systems are called <strong class="bold">Open Domain Question Answering</strong> (<strong class="bold">ODQA</strong>) systems.</p>
			<h2 id="_idParaDest-230" class="calibre5"><a id="_idTextAnchor241" class="calibre6 pcalibre pcalibre1"/>Getting ready</h2>
			<p class="calibre3">As part of this recipe, we will <a id="_idIndexMarker495" class="calibre6 pcalibre pcalibre1"/>use the <code>deeppavlov</code> library along with the <strong class="bold">Knowledge Base Question Answering</strong> (<strong class="bold">KBQA</strong>) model. This model has <a id="_idIndexMarker496" class="calibre6 pcalibre pcalibre1"/>been trained on English wiki data as a knowledge base. It uses various NLP techniques such as entity linking and disambiguation, knowledge graphs, and so on to extract the exact answer to the question.</p>
			<p class="calibre3">This recipe needs a few <a id="_idIndexMarker497" class="calibre6 pcalibre pcalibre1"/>steps to set up the right environment for its execution. The <code>poetry</code> file for this recipe is in the <code>9.2_QA_on_long_passages</code> folder. We will also need to install and download the document corpus by performing the following command:</p>
			<pre class="console">
python -m deeppavlov install kbqa_cq_en</pre>			<p class="calibre3">You can also use the <code>9.2_QA_on_long_passages.ipynb</code> notebook, which is contained within the same folder.</p>
			<h2 id="_idParaDest-231" class="calibre5"><a id="_idTextAnchor242" class="calibre6 pcalibre pcalibre1"/>How to do it...</h2>
			<p class="calibre3">In this recipe, we will initialize the KBQA model based on the <code>DeepPavlov</code> library and use it to answer an open question. The steps for the recipe are as follows:</p>
			<ol class="calibre13">
				<li class="calibre14">Do the necessary imports:<pre class="source-code">
from deeppavlov import build_model</pre></li>				<li class="calibre14">In this step, we initialize the KBQA model, <strong class="source-inline1">kbqa_cq_en</strong>, which is passed to the <strong class="source-inline1">build_model</strong> method as an argument. We also set the <strong class="source-inline1">download</strong> argument to <strong class="source-inline1">True</strong> so that the model is downloaded as well in case it is missing locally:<pre class="source-code">
kbqa_model = build_model('kbqa_cq_en', download=True)</pre></li>				<li class="calibre14">We use the initialized model and pass it a couple of questions that we want to be answered:<pre class="source-code">
result = kbqa_model(['What is the capital of Egypt?', 
    'Who is Bill Clinton\'s wife?'])</pre></li>				<li class="calibre14">We print the result as returned by the model. The result contains three arrays.<p class="calibre3">The first array contains the exact answers to the question ordered in the same way as the original input. In this case, the answers <code>Cairo</code> and <code>Hillary Clinton</code> are in the same order as the questions they pertain to.</p><p class="calibre3">You might observe<a id="_idIndexMarker498" class="calibre6 pcalibre pcalibre1"/> some additional artifacts in the output. These are internal identifiers that are generated by the library. We have omitted them for brevity:</p><pre class="source-code">
[['Cairo', 'Hillary Clinton']]</pre></li>			</ol>
			<h2 id="_idParaDest-232" class="calibre5"><a id="_idTextAnchor243" class="calibre6 pcalibre pcalibre1"/>See also</h2>
			<p class="calibre3">For more<a id="_idIndexMarker499" class="calibre6 pcalibre pcalibre1"/> information on the internal details of the working of DeepPavlov, please refer to <a href="https://deeppavlov.ai" class="calibre6 pcalibre pcalibre1">https://deeppavlov.ai</a>.</p>
			<h1 id="_idParaDest-233" class="calibre7"><a id="_idTextAnchor245" class="calibre6 pcalibre pcalibre1"/>Answering questions from a document corpus in an extractive manner</h1>
			<p class="calibre3">For the use cases where we have a<a id="_idIndexMarker500" class="calibre6 pcalibre pcalibre1"/> document <a id="_idIndexMarker501" class="calibre6 pcalibre pcalibre1"/>corpus that contains a large number of documents, it’s not feasible to load the document content at runtime to answer a question. Such an approach would lead to long query times and would not be suitable for production-grade systems.</p>
			<p class="calibre3">In this recipe, we will learn how to preprocess the documents and transform them into a form for faster reading, indexing, and retrieval that allows the system to extract the answer for a given question with<a id="_idTextAnchor246" class="calibre6 pcalibre pcalibre1"/> short query times.</p>
			<h2 id="_idParaDest-234" class="calibre5"><a id="_idTextAnchor247" class="calibre6 pcalibre pcalibre1"/>Getting ready</h2>
			<p class="calibre3">As part of this <a id="_idIndexMarker502" class="calibre6 pcalibre pcalibre1"/>recipe, we will use the <strong class="bold">Haystack</strong> (<a href="https://haystack.deepset.ai/" class="calibre6 pcalibre pcalibre1">https://haystack.deepset.ai/</a>) framework to build a <strong class="bold">QA system</strong> that can answer questions from a document corpus. We will download a<a id="_idIndexMarker503" class="calibre6 pcalibre pcalibre1"/> dataset based on <em class="italic">Game of Thrones</em> and index it. For our QA system to be performant, we will need to index the documents beforehand. Once the documents are indexed, answering a question follows a two-step process:</p>
			<ol class="calibre13">
				<li class="calibre14"><strong class="bold">Retriever</strong>: Since we have many<a id="_idIndexMarker504" class="calibre6 pcalibre pcalibre1"/> documents, scanning each document to fetch an answer is not a feasible approach. We will first retrieve a set of candidate documents that can possibly contain an answer to our question. This step is performed using a <strong class="source-inline1">Retriever</strong> component. This searches through the pre-created index to filter the number of documents that we will need to scan to retrieve the exact answer.</li>
				<li class="calibre14"><strong class="bold">Reader</strong>: Once we have a candidate set of documents that could contain the answer, we will <a id="_idIndexMarker505" class="calibre6 pcalibre pcalibre1"/>search these documents to retrieve the exact answer to our question.</li>
			</ol>
			<p class="calibre3">We will discuss the details of these components throughout this recipe. You can use the <code>9.3_QA_on_document_corpus.ipynb</code> notebook from the code site if you need to work from an existing notebook. To start with, let’s set up the prerequisites.</p>
			<h2 id="_idParaDest-235" class="calibre5"><a id="_idTextAnchor248" class="calibre6 pcalibre pcalibre1"/>How to do it...</h2>
			<ol class="calibre13">
				<li class="calibre14">In this step, we do the necessary imports:<pre class="source-code">
import os
from haystack.document_stores import InMemoryDocumentStore
from haystack.nodes import BM25Retriever, FARMReader
from haystack.pipelines import ExtractiveQAPipeline
from haystack.pipelines.standard_pipelines import( 
    TextIndexingPipeline)
from haystack.utils import (fetch_archive_from_http, 
    print_answers)</pre></li>				<li class="calibre14">In this step, we specify<a id="_idIndexMarker506" class="calibre6 pcalibre pcalibre1"/> a folder that will be used to save our dataset. Then, we retrieve the dataset from the source. The second parameter to the <strong class="source-inline1">fetch_archive_from_http</strong> method is the folder in which the dataset will be downloaded. We set the parameter to the<a id="_idIndexMarker507" class="calibre6 pcalibre pcalibre1"/> folder that we defined in the first line. The <strong class="source-inline1">fetch_archive_from_http</strong> method decompresses the archive <strong class="source-inline1">.zip</strong> file and extracts all files into the same folder. We then read from the folder and create a list of files contained in the folder. We also print the number of files that are present:<pre class="source-code">
doc_dir = "data/got_dataset"
fetch_archive_from_http(
    url="https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-qa/datasets/documents/wiki_gameofthrones_txt1.zip",
    output_dir=doc_dir,
    )
files_to_index = [doc_dir + "/" + f <strong class="bold1">for</strong> f <strong class="bold1">in</strong> os.listdir(
    doc_dir)]
print(len(files_to_index))
183</pre></li>				<li class="calibre14">We initialize a<a id="_idIndexMarker508" class="calibre6 pcalibre pcalibre1"/> document <a id="_idIndexMarker509" class="calibre6 pcalibre pcalibre1"/>store based on the files. We create an indexing pipeline based on the document store and execute the indexing operation. To achieve this, we initialize an <strong class="source-inline1">InMemoryDocumentStore</strong> instance. In this method call, we set the <strong class="source-inline1">use_bm25</strong> argument as <strong class="source-inline1">True</strong>. The document <a id="_idIndexMarker510" class="calibre6 pcalibre pcalibre1"/>store uses <strong class="bold">Best Match 25</strong> (<strong class="bold">bm25</strong>) as the algorithm for the retriever step. The <strong class="source-inline1">bm25</strong> algorithm is a simple bag-of-words-based algorithm that uses a scoring function. This function utilizes the number of times a term is present in the document and the length of the document. <a href="B18411_03.xhtml#_idTextAnchor067" class="calibre6 pcalibre pcalibre1"><em class="italic">Chapter 3</em></a> covers the <strong class="source-inline1">bm25</strong> algorithm in more detail and we recommend you refer to that chapter for better understanding. Note that there are various other <strong class="source-inline1">DocumentStore</strong> options such as <strong class="source-inline1">ElasticSearch</strong>, <strong class="source-inline1">OpenSearch</strong>, and so on. We used an <strong class="source-inline1">InMemoryDocumentStore</strong> document store to keep <a id="_idIndexMarker511" class="calibre6 pcalibre pcalibre1"/>the recipe simple and focus on the retriever and reader concepts:<pre class="source-code">
document_store = InMemoryDocumentStore(use_bm25=True)
indexing_pipeline = TextIndexingPipeline(document_store)
indexing_pipeline.run_batch(file_paths=files_to_index)</pre></li>				<li class="calibre14">Once we have loaded the documents, we initialize our retriever and reader instances. To achieve this, we initialize the retriever and the reader components. <strong class="source-inline1">BM25Retriever</strong> uses the <strong class="source-inline1">bm25</strong> scoring function to retrieve the initial set of documents. For the<a id="_idIndexMarker512" class="calibre6 pcalibre pcalibre1"/> reader, we initialize the <strong class="source-inline1">FARMReader</strong> object. This is based on deepset’s FARM framework, which can utilize the QA models from Hugging Face. In our case, we use the <strong class="source-inline1">deepset/roberta-base-squad2</strong> model as a reader. The <strong class="source-inline1">use_gpu</strong> argument can be set appropriately based on whether your device has a GPU or not:<pre class="source-code">
retriever = BM25Retriever(document_store=document_store)
reader = FARMReader(
    model_name_or_path="deepset/roberta-base-squad2",
    use_gpu=True)</pre></li>				<li class="calibre14">We now create a pipeline that we can use to answer questions. After having initialized the retriever and reader in the <a id="_idIndexMarker513" class="calibre6 pcalibre pcalibre1"/>previous step, we want to combine them for querying. The <strong class="source-inline1">pipeline</strong> abstraction from the Haystack framework allows us to integrate the reader and retriever together using a series of pipelines that address different use cases. In this instance, we will use <strong class="source-inline1">ExtractiveQAPipeline</strong> for our QA system. After the initialization of the pipeline, we generate the answer to a question from the <em class="italic">Game of Thrones</em> series. The <strong class="source-inline1">run</strong> method takes the question as the query. The second argument, <strong class="source-inline1">params</strong>, dictates how<a id="_idIndexMarker514" class="calibre6 pcalibre pcalibre1"/> the results from the retriever and reader are combined to present the answer:<ul class="calibre19"><li class="calibre14"><strong class="source-inline1">"Retriever": {"top_k": 10}</strong>: The <strong class="source-inline1">top_k</strong> keyword argument specifies that the top-k (in this case, <strong class="source-inline1">10</strong>) results from the retriever are used by the reader to search for the exact answer</li><li class="calibre14"><strong class="source-inline1">"Reader": {"top_k": 5}</strong>: The <strong class="source-inline1">top_k</strong> keyword argument specifies that the top-k (in this case, <strong class="source-inline1">5</strong>) results from the reader are presented as the output of the method:<pre class="source-code">
pipe = ExtractiveQAPipeline(reader, retriever)
prediction = pipe.run(
    query="Who is the father of Arya Stark?",
    params={"Retriever": {"top_k": 10}, "Reader": {"top_k": 5}}
)</pre></li></ul></li>				<li class="calibre14">We print the answer to <a id="_idIndexMarker515" class="calibre6 pcalibre pcalibre1"/>our question. The system prints out the exact answer along with the associated context that it used to extract the answer from. Note that we use the value of <strong class="source-inline1">all</strong> for the <strong class="source-inline1">details</strong> argument. Using the <strong class="source-inline1">all</strong> value for the same argument prints out <strong class="source-inline1">start</strong> and <strong class="source-inline1">end</strong> spans for the answer along with all the auxiliary information. Setting the value of <strong class="source-inline1">medium</strong> for the <strong class="source-inline1">details</strong> argument provides the relative score of each answer. This score can be used to filter out the results further based on the accuracy requirements of the system. Using the argument of <strong class="source-inline1">medium</strong> presents only the answer and the context. We encourage you to <a id="_idIndexMarker516" class="calibre6 pcalibre pcalibre1"/>make a suitable choice based on your requirements:<pre class="source-code">
print_answers(prediction, details="all")
'Query: Who is the father of Arya Stark?'
'Answers:'
[&lt;Answer {'answer': 'Eddard',
'type': 'extractive',
'score': 0.993372917175293,
'context': "s Nymeria after a legendary warrior queen. She travels with her father, Eddard, to King's Landing when he is made Hand of the King. Before she leaves,", 'offsets_in_document': [{'start': 207, 'end': 213}], 'offsets_in_context': [{'start': 72, 'end': 78}], 'document_ids': ['9e3c863097d66aeed9992e0b6bf1f2f4'], 'meta': {'_split_id': 3}}&gt;,
&lt;Answer {'answer': 'Ned',
'type': 'extractive',
'score': 0.9753613471984863,
'context': "k in the television series.\n\n====Season 1====\nArya accompanies her father Ned and her sister Sansa to King's Landing. Before their departure, Arya's h", 'offsets_in_document': [{'start': 630, 'end': 633}], 'offsets_in_context': [{'start': 74, 'end': 77}], 'document_ids': ['7d3360fa29130e69ea6b2ba5c5a8f9c8'], 'meta': {'_split_id': 10}}&gt;,
&lt;Answer {'answer': 'Lord Eddard Stark',
'type': 'extractive',
'score': 0.9177322387695312,
'context': 'rk daughters.\n\nDuring the Tourney of the Hand to honour her father Lord Eddard Stark, Sansa Stark is enchanted by the knights performing in the event.', 'offsets_in_document': [{'start': 280, 'end': 297}], 'offsets_in_context': [{'start': 67, 'end': 84}], 'document_ids': ['5dbccad397381605eba063f71dd500a6'], 'meta': {'_split_id': 3}}&gt;,
&lt;Answer {'answer': 'Ned',
'type': 'extractive',
'score': 0.8396496772766113,
'context': " girl disguised as a boy all along and is surprised to learn she is Arya, Ned Stark's daughter. After the Goldcloaks get help from Ser Amory Lorch and", 'offsets_in_document': [{'start': 848, 'end': 851}], 'offsets_in_context': [{'start': 74, 'end': 77}], 'document_ids': ['257088f56d2faba55e2ef2ebd19502dc'], 'meta': {'_split_id': 31}}&gt;,
&lt;Answer {'answer': 'King Robert',
'type': 'extractive',
'score': 0.6922298073768616,
'context': "en refuses to yield Gendry, who is actually a bastard son of the late King Robert, to the Lannisters.  The Night's Watch convoy is overrun and massacr", 'offsets_in_document': [{'start': 579, 'end': 590}], 'offsets_in_context': [{'start': 70, 'end': 81}], 'document_ids': ['4d51b1876e8a7eac8132b97e2af04<a id="_idTextAnchor249" class="pcalibre pcalibre1 calibre20"/>401'], 'meta': {'_split_id': 4}}&gt;]</pre></li>			</ol>
			<h2 id="_idParaDest-236" class="calibre5"><a id="_idTextAnchor250" class="calibre6 pcalibre pcalibre1"/>See also</h2>
			<p class="calibre3">For a QA system to<a id="_idIndexMarker517" class="calibre6 pcalibre pcalibre1"/> work in a high-performance <a id="_idIndexMarker518" class="calibre6 pcalibre pcalibre1"/>production system, it is recommended to use a different document store from an in-memory one. We recommend you refer to <a href="https://docs.haystack.deepset.ai/docs/document_store" class="calibre6 pcalibre pcalibre1">https://docs.haystack.deepset.ai/docs/document_store</a> and use an appropriate document store based on your production-grade requirements.</p>
			<h1 id="_idParaDest-237" class="calibre7"><a id="_idTextAnchor251" class="calibre6 pcalibre pcalibre1"/>Answering questions from a document corpus in an abstractive manner</h1>
			<pre> source). There are techniques to generate an abstractive answer too, which is more readable by end users compared to an extractive one.</pre>
			<h2 id="_idParaDest-238" class="calibre5"><a id="_idTextAnchor252" class="calibre6 pcalibre pcalibre1"/>Getting ready</h2>
			<p class="calibre3">For this recipe, we will build a QA system that will provide answers that are abstractive in nature. We will load the <code>bilgeyucel/seven-wonders</code> dataset from the Hugging Face site and initialize a retriever from it. This dataset has content about the seven wonders of the ancient world. To generate the answers, we will use the <code>PromptNode</code> component from the Haystack framework to set up a pipeline that can generate answers in an abstractive fashion. You can use the <code>9.4_abstractive_qa_on_document_corpus.ipynb</code> notebook from the code site if you need to work from an existing notebook. Let’s get started.</p>
			<h2 id="_idParaDest-239" class="calibre5"><a id="_idTextAnchor253" class="calibre6 pcalibre pcalibre1"/>How to do it</h2>
			<p class="calibre3">The steps are as follows:</p>
			<ol class="calibre13">
				<li class="calibre14">In this step, we do the <a id="_idIndexMarker521" class="calibre6 pcalibre pcalibre1"/>necessary imports:<pre class="source-code">
from datasets import load_dataset
from haystack.document_stores import InMemoryDocumentStore
from haystack.nodes import (
    BM25Retriever, PromptNode,
    PromptTemplate, AnswerParser)
from haystack.pipelines import Pipeline</pre></li>				<li class="calibre14">As part of this step, we load the <strong class="source-inline1">bilgeyucel/seven-wonders</strong> dataset into an in-memory<a id="_idIndexMarker522" class="calibre6 pcalibre pcalibre1"/> document store. This dataset has been created out of the Wikipedia pages of <em class="italic">Seven Wonders of the Ancient World</em> (<a href="https://en.wikipedia.org/wiki/Wonders_of_the_World" class="calibre6 pcalibre pcalibre1">https://en.wikipedia.org/wiki/Wonders_of_the_World</a>). This dataset has been preprocessed and uploaded to the Hugging Face site, and can be easily downloaded by using the <strong class="source-inline1">datasets</strong> module from Hugging Face. We use <strong class="source-inline1">InMemoryDocumentStore</strong> as our document store, with <strong class="source-inline1">bm25</strong> as the search algorithm. We write the documents from the dataset into the document store. To have a performant query time performance, the <strong class="source-inline1">write_documents</strong> method automatically<a id="_idIndexMarker523" class="calibre6 pcalibre pcalibre1"/> optimizes how the documents are written. Once the documents are written into, we initialize the retriever based on <strong class="source-inline1">bm25</strong>, similar to our previous recipe:<pre class="source-code">
dataset = load_dataset("bilgeyucel/seven-wonders", 
    split="train")
document_store = InMemoryDocumentStore(use_bm25=True)
document_store.write_documents(dataset)
retriever = BM25Retriever(document_store=document_store)</pre></li>				<li class="calibre14">As part of this step, we initialize a prompt template. We can define the task we want the model to<a id="_idIndexMarker524" class="calibre6 pcalibre pcalibre1"/> perform as a simple instruction in English using the <code>document</code> and <strong class="source-inline1">query</strong>. These arguments are expected to be in the execution context at runtime. The second argument, <strong class="source-inline1">output_parser</strong>, takes an <strong class="source-inline1">AnswerParser</strong> object. This object instructs the <strong class="source-inline1">PromptNode</strong> object to store the results in the <strong class="source-inline1">answers</strong> element. After defining the prompt, we initialize a <strong class="source-inline1">PromptNode</strong> object with a model and the prompt template. We use the <strong class="source-inline1">google/flan-t5-large</strong> model as the answer generator. This model is based on the Google T5 language model and has been fine-tuned (<strong class="source-inline1">flan</strong> stands for <strong class="bold">fine-tuning language models</strong>). Fine-tuning a language model with an instruction dataset allows the language model to<a id="_idIndexMarker525" class="calibre6 pcalibre pcalibre1"/> perform tasks following simple instructions and generating text based on the given context and instruction. One of the fine-tuning steps as part of this model training was to operate on human written instructions as tasks. This allowed the model to perform different downstream tasks on instructions alone<a id="_idIndexMarker526" class="calibre6 pcalibre pcalibre1"/> and reduced the need for any few-shot examples to be trained on.<pre class="source-code">
rag_prompt = PromptTemplate(
    prompt="""Synthesize a comprehensive answer from the following text for the given question.
        Provide a clear and concise response that summarizes the key points and information presented in the text.
        Your answer should be in your own words and be no longer than 50 words.
        \n\n Related text: {join(documents)} \n\n Question: {query} \n\n Answer:""",
    output_parser=AnswerParser(),
)
prompt_node = PromptNode(
    model_name_or_path="google/flan-t5-large",
    default_prompt_template=rag_prompt, use_gpu=True)</pre></li>				<li class="calibre14">We now create a pipeline and add the <strong class="source-inline1">retriever</strong> and <strong class="source-inline1">prompt_node</strong> components that we initialized in the previous steps. The <strong class="source-inline1">retriever</strong> component operates on<a id="_idIndexMarker527" class="calibre6 pcalibre pcalibre1"/> the query supplied by the user and generates a set of results. These results are <a id="_idIndexMarker528" class="calibre6 pcalibre pcalibre1"/>passed to the prompt node, which uses the configured <strong class="source-inline1">flan-t5-model</strong> to generate the answer:<pre class="source-code">
pipe = Pipeline()
pipe.add_node(component=retriever, name="retriever", 
    inputs=["Query"])
pipe.add_node(component=prompt_node,
    name="prompt_node", inputs=["retriever"])</pre></li>				<li class="calibre14">Once the pipeline is set up, we use it to answer questions on the content based on the dataset:<pre class="source-code">
output = pipe.run(query="What is the Great Pyramid of Giza?")
print(output["answers"][0].answer)
output = pipe.run(query="Where are the hanging gardens?")
print(output["answers"][0].answer)</pre></li>			</ol>
			<pre class="console">
The Great Pyramid of Giza was built in the early 26th century BC during a period of around 27 years.[3]
The Hanging Gardens of Semiramis are the only one of the Seven Wonders for which the l<a id="_idTextAnchor254" class="pcalibre pcalibre1 calibre20"/>ocation has not been definitively established.</pre>			<h2 id="_idParaDest-240" class="calibre5"><a id="_idTextAnchor255" class="calibre6 pcalibre pcalibre1"/>See also</h2>
			<p class="calibre3">Please refer to the prompt <a id="_idIndexMarker529" class="calibre6 pcalibre pcalibre1"/>engineering guide <a id="_idIndexMarker530" class="calibre6 pcalibre pcalibre1"/>on Haystack on how to generate prompts for your use cases (<a href="https://docs.haystack.deepset.ai/docs/prompt-engineering-guidelines" class="calibre6 pcalibre pcalibre1">https://docs.haystack.deepset.ai/docs/prompt-engineering-guidelines</a>).</p>
			<h1 id="_idParaDest-241" class="calibre7"><a id="_idTextAnchor257" class="calibre6 pcalibre pcalibre1"/>Summarizing text using pre-trained models based on Transformers</h1>
			<p class="calibre3">We will now explore techniques for <a id="_idIndexMarker531" class="calibre6 pcalibre pcalibre1"/>performing text summarization. Generating a summary for a long passage of text allows NLP practitioners to extract the relevant information for their use cases and use these summaries for other downstream tasks. As part of the summarization, we will explore<a id="_idIndexMarker532" class="calibre6 pcalibre pcalibre1"/> recipes that use Transformer models to generate the summaries.</p>
			<h2 id="_idParaDest-242" class="calibre5"><a id="_idTextAnchor258" class="calibre6 pcalibre pcalibre1"/>Getting ready</h2>
			<p class="calibre3">Our first recipe for summarization<a id="_idIndexMarker533" class="calibre6 pcalibre pcalibre1"/> will use the Google <code>9.5_summarization.ipynb</code> notebook from the code site if you need to work from an existing notebook.</p>
			<h2 id="_idParaDest-243" class="calibre5"><a id="_idTextAnchor259" class="calibre6 pcalibre pcalibre1"/>How to do it</h2>
			<p class="calibre3">Let’s get started:</p>
			<ol class="calibre13">
				<li class="calibre14">Do the necessary imports:<pre class="source-code">
from transformers import pipeline</pre></li>				<li class="calibre14">As part of this step, we initialize the input passage that we need to summarize along with the pipeline. We also calculate the length of the passage since this will be used as an argument to <a id="_idIndexMarker534" class="calibre6 pcalibre pcalibre1"/>be passed to the pipeline during the task execution in the next step. Since we have defined the task as <strong class="source-inline1">summarization</strong>, the object returned by the pipeline module is of the <strong class="source-inline1">SummarizationPipeline</strong> type. We also pass <strong class="source-inline1">t5-large</strong> as the model parameter for the pipeline. This model is based on the <strong class="source-inline1">Encoder-Decoder </strong>Transformer model and acts as a pure sequence-to-sequence model. That means the input and output to/from the model are text sequences. This model was pre-trained using the denoising objective of finding masked words in a sentence followed by fine-tuning on specific downstream tasks such as summarization, textual<a id="_idIndexMarker535" class="calibre6 pcalibre pcalibre1"/> entailment, language translation, and so on:<pre class="source-code">
passage = "The color of animals is by no means a matter of chance; it depends on many considerations, but in the majority of cases tends to protect the animal from danger by rendering it less conspicuous. Perhaps it may be said that if coloring is mainly protective, there ought to be but few brightly colored animals. There are, however, not a few cases in which vivid colors are themselves protective. The kingfisher itself, though so brightly colored, is by no means easy to see. The blue harmonizes with the water, and the bird as it darts along the stream looks almost like a flash of sunlight."
passage_length = len(passage.split(' '))
pipeline_instance = pipeline("summarization", model="t5-large")</pre></li>				<li class="calibre14">We now use the <strong class="source-inline1">pipeline_instance</strong> initialized in the previous step and pass the text passage to it to perform the <strong class="source-inline1">summarization</strong> step. A string array can be passed as well if multiple sequences are to be summarized. We pass <strong class="source-inline1">max_length=512</strong> as the second argument. The T5 model is memory-intensive and the compute requirements grow quadratically with the increase in the input text length. This step might take a few minutes to complete based on the compute capability of the environment you are executing this on:<pre class="source-code">
pipeline_result = pipeline_instance(
    passage, max_length=passage_length)</pre></li>				<li class="calibre14">Once the <strong class="source-inline1">summarization</strong> step is<a id="_idIndexMarker536" class="calibre6 pcalibre pcalibre1"/> complete, we extract the result from the output and print it. The pipeline returns a list of dictionaries. Each list item corresponds to the input argument. In this case, since we passed only one string as input, the first item in the list is the output dictionary that contains our summary. The summary <a id="_idIndexMarker537" class="calibre6 pcalibre pcalibre1"/>can be retrieved by indexing the dictionary on the <strong class="source-inline1">summary_text</strong> element:<pre class="source-code">
result = pipeline_result[0]["summary_text"]
print(result)</pre></li>			</ol>
			<pre class="console">
the color of animals is by no means a matter of chance; it depends on many considerations . in the majority of cases, coloring tends to protect the animal from danger . there are, however, not a few cases in which vivid colors are themselves protective .</pre>			<h2 id="_idParaDest-244" class="calibre5"><a id="_idTextAnchor260" class="calibre6 pcalibre pcalibre1"/>There’s more…</h2>
			<p class="calibre3">Now that we have seen how we can generate a summary using the T5 model, we can use the same code framework and tweak it slightly to use other models to generate summaries.</p>
			<p class="calibre3">The following lines would be common for the other summarization recipes that we are using. We added an extra variable named <code>device</code>, which we will use in our pipelines. We set this variable to the value of<a id="_idIndexMarker538" class="calibre6 pcalibre pcalibre1"/> the device that we will use to generate the summary. If a GPU is present and configured in the system, it will be used; otherwise, the summarization will be performed using the CPU:</p>
			<pre class="source-code">
from transformers import pipeline
import torch
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
passage = "The color of animals is by no means a matter of chance; it depends on many considerations, but in the majority of cases tends to protect the animal from danger by rendering it less conspicuous. Perhaps it may be said that if coloring is mainly protective, there ought to be but few brightly colored animals. There are, however, not a few cases in which vivid colors are themselves protective. The kingfisher itself, though so brightly colored, is by no means easy to see. The blue harmonizes with the water, and the bird as it darts along the stream looks almost like a flash of sunlight."</pre>			<p class="calibre3">In the following example, we use<a id="_idIndexMarker539" class="calibre6 pcalibre pcalibre1"/> the <strong class="bold">BART</strong> model (<a href="https://huggingface.co/facebook/bart-large-cnn" class="calibre6 pcalibre pcalibre1">https://huggingface.co/facebook/bart-large-cnn</a>) from Facebook. This model was trained using a denoising objective. A function adds some random piece of text to an input sequence. The<a id="_idIndexMarker540" class="calibre6 pcalibre pcalibre1"/> model is trained based on the objective to denoise or remove the noisy text from the input sequence. The model was further fine-tuned using the <strong class="bold">CNN DailyMail</strong> dataset<a id="_idIndexMarker541" class="calibre6 pcalibre pcalibre1"/> (<a href="https://huggingface.co/datasets/abisee/cnn_dailymail" class="calibre6 pcalibre pcalibre1">https://huggingface.co/datasets/abisee/cnn_dailymail</a>) for summarization:</p>
			<pre class="source-code">
pipeline_instance = pipeline("summarization", 
    model="facebook/bart-large-cnn", device=device)
pipeline_result = pipeline_instance(passage, 
    max_length=passage_length)
result = pipeline_result[0]["summary_text"]
print(result)
The color of animals is by no means a matter of chance; it depends on many considerations, but in the majority of cases tends to protect the animal from danger by rendering it less conspicuous. There are, however, not a few cases in which vivid colors are themselves protective. The blue harmonizes with the water, and the bird as it darts along the stream looks almost like a flash of sunlight.</pre>			<p class="calibre3">As we observe from the generated summary, it is verbose and extractive in nature. Let’s try generating a summary with another model.</p>
			<p class="calibre3">In the following example, we<a id="_idIndexMarker542" class="calibre6 pcalibre pcalibre1"/> use the <strong class="bold">PEGASUS</strong> model from Google (<a href="https://huggingface.co/google/pegasus-large" class="calibre6 pcalibre pcalibre1">https://huggingface.co/google/pegasus-large</a>) for summarization. This model is a Transformer-based Encoder-Decoder model that was pre-trained with a large news and web page corpus – C4 (<a href="https://huggingface.co/datasets/allenai/c4" class="calibre6 pcalibre pcalibre1">https://huggingface.co/datasets/allenai/c4</a>) and the HugeNews dataset – on a training objective of <a id="_idIndexMarker543" class="calibre6 pcalibre pcalibre1"/>detecting important sentences. HugeNews is a dataset of 1.5 billion articles curated from news and news-like websites from 2013–2019. This model was further fine-tuned for summarization using the subset of the same dataset. The training objective for the fine-tuning involved masking important sentences and making the model generate a summary that has these important sentences. This model generates abstract <a id="_idIndexMarker544" class="calibre6 pcalibre pcalibre1"/>summaries:</p>
			<pre class="source-code">
pipeline_instance = pipeline("summarization", 
    model="google/pegasus-large", device=device)
pipeline_result = pipeline_instance([passage, passage], 
    max_length=passage_length)
result = pipeline_result[0]["summary_text"]
print(result)
Perhaps it may be said that if coloring is mainly protective, there ought to be but few brightly colored animals.</pre>			<p class="calibre3">As we observe from the generated summary, it is concise and abstractive.</p>
			<h2 id="_idParaDest-245" class="calibre5"><a id="_idTextAnchor261" class="calibre6 pcalibre pcalibre1"/>See also</h2>
			<p class="calibre3">As many new and improved <a id="_idIndexMarker545" class="calibre6 pcalibre pcalibre1"/>models for summarization are always in the works, we recommend that you refer to the models on the Hugging Face site (<a href="https://huggingface.co/models?pipeline_tag=summarization" class="calibre6 pcalibre pcalibre1">https://huggingface.co/models?pipeline_tag=summarization</a>) and make the respective choice based on your requirements.</p>
			<h1 id="_idParaDest-246" class="calibre7"><a id="_idTextAnchor262" class="calibre6 pcalibre pcalibre1"/>Detecting sentence entailment</h1>
			<p class="calibre3">In this recipe, we will explore<a id="_idIndexMarker546" class="calibre6 pcalibre pcalibre1"/> techniques to detect <code>premise</code>, which sets up a context. The second sentence is the <code>hypothesis</code>, which serves as the claim. Textual entailment identifies the contextual relationship between the <code>premise</code> and the <code>hypothesis</code>. These relationships can be of <a id="_idIndexMarker547" class="calibre6 pcalibre pcalibre1"/>three types, defined as follows:</p>
			<ul class="calibre15">
				<li class="calibre14"><strong class="bold">Entailment</strong> – The hypothesis supports the <a id="_idIndexMarker548" class="calibre6 pcalibre pcalibre1"/>premise</li>
				<li class="calibre14"><strong class="bold">Contradiction</strong> – The <a id="_idIndexMarker549" class="calibre6 pcalibre pcalibre1"/>hypothesis contradicts the premise</li>
				<li class="calibre14"><strong class="bold">Neutral</strong> – The hypothesis <a id="_idIndexMarker550" class="calibre6 pcalibre pcalibre1"/>neither supports nor contradicts the premise</li>
			</ul>
			<h2 id="_idParaDest-247" class="calibre5"><a id="_idTextAnchor263" class="calibre6 pcalibre pcalibre1"/>Getting ready</h2>
			<p class="calibre3">We will use the Transformers library to detect text entailment. You can use the <code>9.6_textual_entailment.ipynb</code> notebook from the code site if you need to work from an existing notebook.</p>
			<h2 id="_idParaDest-248" class="calibre5"><a id="_idTextAnchor264" class="calibre6 pcalibre pcalibre1"/>How to do it...</h2>
			<p class="calibre3">In this recipe, we will initialize different sets of sentences that are related through each of the previously defined relationships and explore methods to detect these relationships. Let’s get started:</p>
			<ol class="calibre13">
				<li class="calibre14">Do the necessary imports:<pre class="source-code">
import torch
from transformers import T5Tokenizer, T5ForConditionalGeneration</pre></li>				<li class="calibre14">Initialize the device, the tokenizer, and the model. In this case, we are using Google’s <strong class="source-inline1">t5-small</strong> model. We set the <strong class="source-inline1">legacy</strong> flag to <strong class="source-inline1">False</strong> since we don’t need to use the legacy behavior of the model. We set the <strong class="source-inline1">device</strong> value based on whatever device we have available in our execution environment. Similarly, for the model, we set the <strong class="source-inline1">model</strong> name and <strong class="source-inline1">device</strong> parameter similar to the tokenizer. We set the <strong class="source-inline1">return_dict</strong> parameter as <strong class="source-inline1">True</strong> so that we get the model results as a <a id="_idIndexMarker551" class="calibre6 pcalibre pcalibre1"/>dictionary instead of a tuple:<pre class="source-code">
device = torch.device("cuda" <strong class="bold1">if</strong> torch.cuda.is_available() 
    <strong class="bold1">else</strong> "cpu")
tokenizer = T5Tokenizer.from_pretrained(
    't5-small', legacy=False, device=device)
model = T5ForConditionalGeneration.from_pretrained(
    't5-small', return_dict=True, device_map=device)</pre></li>				<li class="calibre14">We initialize the <strong class="source-inline1">premise</strong> and <strong class="source-inline1">hypothesis</strong> sentences. In this case, the hypothesis supports the premise:<pre class="source-code">
premise = "The corner coffee shop serves the most awesome coffee I have ever had."
hypothesis = "I love the coffee served by the corner coffee shop."</pre></li>				<li class="calibre14">In this step, we call the tokenizer with the <strong class="source-inline1">mnli premise</strong> and <strong class="source-inline1">hypothesis</strong> values. This is a simple text concatenation step to set up the tokenizer for the <strong class="source-inline1">entailment</strong> task. We read the <strong class="source-inline1">input_ids</strong> property to get the token identifiers for the concatenated string. Once we have the token IDs, we use the model to generate the entailment prediction. This returns a list of tensors with the predictions, which we use in the next step:<pre class="source-code">
input_ids = tokenizer(
    "mnli premise: " + premise + " hypothesis: " + hypothesis,
    return_tensors="pt").input_ids
entailment_ids = model.generate(input_ids.to(device), 
    max_new_tokens=20)</pre></li>				<li class="calibre14">In this step, we call the <strong class="source-inline1">decode</strong> method of the tokenizer and pass it the first tensor (or vector) of the tensors that were returned by the <strong class="source-inline1">generate</strong> call of the model. We also<a id="_idIndexMarker552" class="calibre6 pcalibre pcalibre1"/> instruct the tokenizer to skip the special tokens that are used by the tokenizer internally. The tokenizer generates the string label from the vector that is passed in. We print the prediction result. In this case, the generated prediction by the model is <strong class="source-inline1">entailment</strong>:<pre class="source-code">
prediction = tokenizer.decode(
    entailment_ids[0], skip_special_tokens=True, device=device)
print(prediction)</pre></li>			</ol>
			<pre class="console">
entailment</pre>			<h2 id="_idParaDest-249" class="calibre5"><a id="_idTextAnchor265" class="calibre6 pcalibre pcalibre1"/>There’s more...</h2>
			<p class="calibre3">Now that we have shown an <a id="_idIndexMarker553" class="calibre6 pcalibre pcalibre1"/>example in the case of entailment with a single sentence, the same framework can be used to process a batch of sentences to generate entailment predictions. We will tailor <em class="italic">steps 3</em>,<em class="italic">4</em>, and <em class="italic">5</em> from the previous recipe for this example. We initialize an array of two sentences for both <code>premise</code> and <code>hypothesis</code>, respectively. Both the <code>premise</code> sentences are the same, while the <code>hypothesis</code> sentences are of <code>entailment</code> and <code>contradiction</code>, respectively:</p>
			<pre class="source-code">
premise = ["The corner coffee shop serves the most awesome coffee I have ever had.", "The corner coffee shop serves the most awesome coffee I have ever had."]
hypothesis = ["I love the coffee served by the corner coffee shop.", "I find the coffee served by the corner coffee shop too bitter for my taste."]</pre>			<p class="calibre3">Since we have an array of sentences for both <code>premises</code> and <code>hypothesis</code>, we create an array of concatenated inputs that combine the <code>tokenizer</code> instruction. This array is used to pass to the tokenizer and we use the token IDs returned by <code>tokenizer</code> in the next step:</p>
			<pre class="source-code">
premises_and_hypotheses = [f"mnli premise: {pre} 
    hypothesis: {hyp}" <strong class="bold1">for</strong> pre, hyp <strong class="bold1">in</strong> zip(premise, hypothesis)]
input_ids = tokenizer(
    text=premises_and_hypotheses, padding=True,
    return_tensors="pt").input_ids</pre>			<p class="calibre3">We now generate the predictions using the same methodology that we used earlier. However, in this step, we generate the inference label by iterating through the tensors returned by the model output <a id="_idIndexMarker554" class="calibre6 pcalibre pcalibre1"/>and printing the prediction:</p>
			<pre class="source-code">
entailment_ids = model.generate(input_ids.to(device), 
    max_new_tokens=20)
<strong class="bold1">for</strong> _tensor <strong class="bold1">in</strong> entailment_ids:
    entailment = tokenizer.decode(_tensor,<a id="_idTextAnchor266" class="pcalibre pcalibre1 calibre20"/>
        skip_special_tokens=True, device=device)
    print(entailment)</pre>			<h1 id="_idParaDest-250" class="calibre7"><a id="_idTextAnchor267" class="calibre6 pcalibre pcalibre1"/>Enhancing explainability via a classifier-invariant approach</h1>
			<p class="calibre3">Now, we will explore recipes that will allow<a id="_idIndexMarker555" class="calibre6 pcalibre pcalibre1"/> us to understand the decisions made by text classifiers. We will explore techniques that will use a sentiment classifier and NLP explainability libraries to interpret the classification labels and their relation to the input text, especially in the aspect of individual words in the text.</p>
			<p class="calibre3">Though a lot of the current models for<a id="_idIndexMarker556" class="calibre6 pcalibre pcalibre1"/> text classification in NLP are based on deep neural networks, it is difficult to interpret the results of classification via the network weights or parameters. It is equally challenging to map these network parameters to the individual components or words in the input. However, there are still a few techniques in the NLP space to help us understand the decisions made by the classifier. We will explore these techniques in the current recipe and the following one.</p>
			<p class="calibre3">In this recipe, we will learn<a id="_idIndexMarker557" class="calibre6 pcalibre pcalibre1"/> how to interpret the feature importance of each word in a text passage while being invariant of the classifier model. This technique can be used for any text classifier as we treat the classifier as a black box and use the results of the predictions to infer the results from an explainability perspective.</p>
			<h2 id="_idParaDest-251" class="calibre5"><a id="_idTextAnchor268" class="calibre6 pcalibre pcalibre1"/>Getting ready</h2>
			<p class="calibre3">We will use the <code>lime</code> library for explainability. You can use the <code>9.7_explanability_via_classifier.ipynb<a id="_idTextAnchor269" class="calibre6 pcalibre pcalibre1"/></code> notebook from the code site if you want to work from an existing <a id="_idIndexMarker558" class="calibre6 pcalibre pcalibre1"/>notebook.</p>
			<h2 id="_idParaDest-252" class="calibre5"><a id="_idTextAnchor270" class="calibre6 pcalibre pcalibre1"/>How to do it...</h2>
			<p class="calibre3">In this recipe, we will repurpose a classifier that we built in the <em class="italic">Transformers</em> chapter and use it to generate a sentiment prediction. We will call this classifier multiple times with a perturbation of the input to understand the contribution of each word to the sentiment. Let’s get started:</p>
			<ol class="calibre13">
				<li class="calibre14">Do the necessary imports:<pre class="source-code">
import numpy as np
import torch
from lime.lime_text import LimeTextExplainer
from transformers import pipeline</pre></li>				<li class="calibre14">In this step, we initialize the device and the pipeline for sentiment classification. For more details on this step, please refer to chapter-8. <pre class="source-code">
device = torch.device(
    "cuda" if torch.cuda. is_available() else "cpu")
roberta_pipe = pipeline(
    "sentiment-analysis",
    model="siebert/sentiment-roberta-large-english",
    tokenizer="siebert/sentiment-roberta-large-english",
    top_k=1,
    device=device
)</pre></li>				<li class="calibre14">In this step, we initialize a<a id="_idIndexMarker559" class="calibre6 pcalibre pcalibre1"/> sample text passage along with setting the print options. Setting the <a id="_idIndexMarker560" class="calibre6 pcalibre pcalibre1"/>print options allows us to print the outputs in the later steps in an easy-to-read format:<pre class="source-code">
sample_text = "I really liked the Oppenheimer movie and found it truly entertaining and full of substance."
np.set_printoptions(suppress = True,
    formatter = {'float_kind':'{:f}'.format},
    precision = 2)</pre></li>				<li class="calibre14">In this step, we create a wrapper function for sentiment classification. This method is used by the explainer to invoke the classification pipeline multiple times to gauge the<a id="_idIndexMarker561" class="calibre6 pcalibre pcalibre1"/> contribution of each word in the passage:<pre class="source-code">
<strong class="bold1">def</strong> predict_prob(texts):
    preds = roberta_pipe(texts)
    preds = np.array([
        [label[0]['score'], 1 - label[0]['score']]
        <strong class="bold1">if</strong> label[0]['label'] == 'NEGATIVE'
        <strong class="bold1">else</strong> [1 - label[0]['score'], label[0]['score']]
        <strong class="bold1">for</strong> label <strong class="bold1">in</strong> preds
    ])
    <strong class="bold1">return</strong> preds</pre></li>				<li class="calibre14">In this step, we instantiate the <strong class="source-inline1">LimeTextExplainer</strong> class and call the <strong class="source-inline1">explain_instance</strong> method for it. This method takes the sample text along with the <strong class="source-inline1">classifier</strong> wrapper<a id="_idIndexMarker562" class="calibre6 pcalibre pcalibre1"/> function. The wrapper function passed to this method expects it to take a single instance of a string and return the probabilities of the target classes. In this case, our wrapper function accepts a simple string and returns the probabilities for the <strong class="source-inline1">NEGATIVE</strong> and <strong class="source-inline1">POSITIVE</strong> classes, respectively, and in that order:<pre class="source-code">
explainer = LimeTextExplainer(
    class_names=['NEGATIVE', 'POSITIVE'])
exp = explainer.explain_instance(
    text_instance=sample_text,
    classifier_fn=predict_prob)</pre></li>				<li class="calibre14">In this step, we print the <a id="_idIndexMarker563" class="calibre6 pcalibre pcalibre1"/>class probabilities for the sample text. As we observe, the sample text has been assigned<a id="_idIndexMarker564" class="calibre6 pcalibre pcalibre1"/> a <strong class="source-inline1">POSITIVE</strong> sentiment as per the classifier:<pre class="source-code">
original_prediction = predict_prob(sample_text)
print(original_prediction)</pre></li>			</ol>
			<pre class="console">
[[0.001083 0.998917]]</pre>			<ol class="calibre13">
				<li value="7" class="calibre14">In this step, we print the explanations. As we observe from the probabilities for each word, the words <strong class="source-inline1">entertaining</strong> and <strong class="source-inline1">liked</strong> contributed the most to the <strong class="source-inline1">POSITIVE</strong> class. There are some words that contribute negatively to the positive sentiment, but overall, the sentence is classified as positive:<pre class="source-code">
print(np.array(exp.as_list()))</pre></li>			</ol>
			<pre class="console">
[['liked' '0.02466976195824297']
 ['entertaining' '0.023293546246506702']
 ['and' '0.018718510660163126']
 ['truly' '0.015312955730851004']
 ['Oppenheimer' '-0.012689413190611268']
 ['substance' '0.011282896692531665']
 ['of' '-0.007935237702088416']
 ['movie' '0.00665836523527015']
 ['it' '0.004033408096240486']
 ['found' '0.003214157926470171']]</pre>			<ol class="calibre13">
				<li value="8" class="calibre14">Let’s initialize another text to<a id="_idIndexMarker565" class="calibre6 pcalibre pcalibre1"/> something with a negative sentiment:<pre class="source-code">
modified_text = "I found the Oppenheimer movie very slow, boring and veering on being too scientific."</pre></li>				<li class="calibre14">Get the class <a id="_idIndexMarker566" class="calibre6 pcalibre pcalibre1"/>probability as predicted by the classifier for the new text and print it:<pre class="source-code">
new_prediction = predict_prob(modified_text)
print(new_prediction)</pre></li>			</ol>
			<pre class="console">
[[0.999501 0.000499]]</pre>			<ol class="calibre13">
				<li value="10" class="calibre14">Use the <strong class="source-inline1">explainer</strong> instance to evaluate the text and print the contribution of each word to the negative sentiment. We observe that the words <strong class="source-inline1">boring</strong> and <strong class="source-inline1">slow</strong> contributed most to the negative sentiment:<pre class="source-code">
exp = explainer.explain_instance(
    text_instance=modified_text,
    classifier_fn=predict_prob)
print(np.array(exp.as_list()))</pre></li>			</ol>
			<pre class="console">
[['boring' '-0.1541527292742657']
 ['slow' '-0.13677434672789646']
 ['too' '-0.07536450832681185']
 ['veering' '-0.06154593708589755']
 ['Oppenheimer' '-0.021333762714731672']
 ['found' '0.015601753307753232']
 ['movie' '0.011810474276051267']
 ['I' '0.010142608<a id="_idTextAnchor271" class="pcalibre pcalibre1 calibre20"/>38624105']
 ['the' '-0.008070326804220167']
 ['scientific' '-0.006083605323956207']]</pre>			<h2 id="_idParaDest-253" class="calibre5"><a id="_idTextAnchor272" class="calibre6 pcalibre pcalibre1"/>There’s more...</h2>
			<p class="calibre3">Now that we have <a id="_idIndexMarker567" class="calibre6 pcalibre pcalibre1"/>seen how to interpret the word contributions<a id="_idIndexMarker568" class="calibre6 pcalibre pcalibre1"/> for the sentiment classification, we want to further improve our recipe to provide a visual representation of the explainability:</p>
			<ol class="calibre13">
				<li class="calibre14">Continuing on from <em class="italic">step 5</em> in the recipe, we can also print the explanations using <strong class="source-inline1">pyplot</strong>:<pre class="source-code">
exp = explainer.explain_instance(text_instance=sample_text,
    classifier_fn=predict_prob)
_ = exp.as_pyplot_figure()</pre></li>			</ol>
			<div><div><img src="img/B18411_09_1.jpg" alt="" role="presentation" class="calibre4"/>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 9.1 – Probability contribution of each word in the sentence to the final class</p>
			<ol class="calibre13">
				<li value="2" class="calibre14">We can also highlight the<a id="_idIndexMarker569" class="calibre6 pcalibre pcalibre1"/> exact words in the text. The contribution of each word is also highlighted<a id="_idIndexMarker570" class="calibre6 pcalibre pcalibre1"/> using a light or dark shade of the assigned class, which, in this case, is orange. The words with the blue high<a id="_idTextAnchor273" class="calibre6 pcalibre pcalibre1"/>lights are the ones that contribute against the <strong class="source-inline1">POSITIVE</strong> class:<pre class="source-code">
exp.show_in_notebook()</pre></li>			</ol>
			<div><div><img src="img/B18411_09_2.jpg" alt="" role="presentation" class="calibre4"/>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 9.2 – The highlighted class association for each word</p>
			<h1 id="_idParaDest-254" class="calibre7"><a id="_idTextAnchor274" class="calibre6 pcalibre pcalibre1"/>Enhancing explainability via text generation</h1>
			<p class="calibre3">In this recipe, we will learn how to <a id="_idIndexMarker571" class="calibre6 pcalibre pcalibre1"/>understand the inference emitted by the classifier using text generation. We will use the same classifier that we used in the <em class="italic">Explainability via a classifier invariant approach</em> recipe. To better understand the behavior of the classifier in a random setting, we will replace the words in the input <a id="_idIndexMarker572" class="calibre6 pcalibre pcalibre1"/>sentence with different tokens.</p>
			<h2 id="_idParaDest-255" class="calibre5"><a id="_idTextAnchor275" class="calibre6 pcalibre pcalibre1"/>Getting ready</h2>
			<p class="calibre3">We will need to install a <code>spacy</code> artifact for this recipe. Please use the following command in your environment before starting this recipe.</p>
			<p class="calibre3">Now that we have installed <code>spacy</code>, we will need to download the <code>en_core_web_sm</code> pipeline using the following step beforehand:</p>
			<pre class="console">
python3 -m spacy download en_core_web_sm</pre>			<p class="calibre3">You can use the <code>9.8_explanability_via_generation.ipynb</code> notebook from the code site if you need to work from an existing notebook.</p>
			<h2 id="_idParaDest-256" class="calibre5"><a id="_idTextAnchor276" class="calibre6 pcalibre pcalibre1"/>How to do it</h2>
			<p class="calibre3">Let’s get started:</p>
			<ol class="calibre13">
				<li class="calibre14">Do the necessary imports:<pre class="source-code">
import numpy as np
import spacy
import time
import torch
from anchor import anchor_text
from transformers import pipeline</pre></li>				<li class="calibre14">In this step, we initialize the <strong class="source-inline1">spacy</strong> pipeline with the <strong class="source-inline1">en_core_web_sm</strong> model. This pipeline contains the components for <strong class="source-inline1">tok2vec</strong>, <strong class="source-inline1">tagger</strong>, <strong class="source-inline1">parser</strong>, <strong class="source-inline1">ner</strong>, <strong class="source-inline1">lemmatizer</strong>, and so on, and is optimized for the CPU:<pre class="source-code">
nlp = spacy.load('en_core_web_sm')</pre></li>				<li class="calibre14"> In this step, we initialize the device and our classifier. We use the same sentence classifier that we used in the <em class="italic">Explainability via a classifier invariant approach</em> recipe. The idea is to <a id="_idIndexMarker573" class="calibre6 pcalibre pcalibre1"/>understand the same classifier and observe how its classification behaves for different inputs, as<a id="_idIndexMarker574" class="calibre6 pcalibre pcalibre1"/> generated by the <strong class="source-inline1">anchor</strong> explainability library:<pre class="source-code">
device = torch.device("cuda" <strong class="bold1">if</strong> torch.cuda.is_available(<strong class="bold1"># Load model directly</strong>
from transformers import( AutoTokenizer, 
    AutoModelForSequenceClassification)
tokenizer = AutoTokenizer.from_pretrained(
    "jonathanfernandes/imdb_model")
model = AutoModelForSequenceClassification.from_pretrained(
    "jonathanfernandes/imdb_model")) <strong class="bold1">else</strong> "cpu")
classifier = pipeline(
    "sentiment-analysis",
    model="siebert/sentiment-roberta-large-english",
    tokenizer="siebert/sentiment-roberta-large-english",
    top_k=1,
    device=device)</pre></li>				<li class="calibre14">In this step, we define a function that takes a list of sentences and emits a list of <strong class="source-inline1">POSITIVE</strong> or <strong class="source-inline1">NEGATIVE</strong> labels <a id="_idIndexMarker575" class="calibre6 pcalibre pcalibre1"/>for them.  This method internally calls the classifier that was initialized in the previous<a id="_idIndexMarker576" class="calibre6 pcalibre pcalibre1"/> step:<pre class="source-code">
<strong class="bold1">def</strong> predict_prob(texts):
    preds = classifier(texts)
    preds = np.array([
        0 <strong class="bold1">if</strong> label[0]['label'] == 'NEGATIVE' <strong class="bold1">else</strong> 1
        <strong class="bold1">for</strong> label <strong class="bold1">in</strong> preds])
    <code>spacy</code> pipeline, the class labels, and <code>use_unk_distribution</code> as true. The class labels in this case are <code>NEGATIVE</code> and <code>POSITIVE</code>. The <code>use_unk_distribution</code> parameter specifies that the explainer uses the <code>UNK</code> token for masked words when it generates text for explanability.explainer = anchor_text.AnchorText(nlp, [‘NEGATIVE’, ‘POSITIVE’], use_unk_distribution=True)</p></li>				<li class="calibre14">In this step, we initialize a passage of text. We use that text sentence to predict its class probability by using the <strong class="source-inline1">predict_prob</strong> method and print the prediction:<pre class="source-code">
text = 'The little mermaid is a good story.'
pred = explainer.class_names[predict_prob([text])[0]]
print('Prediction: %s' % pred)
Prediction: POSITIVE</pre><p class="calibre3">In this step, we call the <code>explain_instance</code> method for the explainer instance. We pass it the input sentence, the <code>predict_prob</code> method, and a <code>threshold</code>. The explainer instance uses the <code>predict_prob</code> method to invoke the classifier for different variations of the input sentence to explain what words contribute the most. It also identifies what class labels are emitted when some words in the input sentence are replaced by the <code>UNK</code> token. The <code>threshold</code> parameter defines the minimum probability for a given class under which all the generated <a id="_idIndexMarker577" class="calibre6 pcalibre pcalibre1"/>samples are to be ignored. This effectively means that all the sentences generated by the <a id="_idIndexMarker578" class="calibre6 pcalibre pcalibre1"/>explainer will have the probability greater than the threshold, for a given class.exp = explainer.explain_instance(text, predict_prob, threshold=0.95)</p></li>				<li class="calibre14">We print the <strong class="source-inline1">anchor</strong> words that contribute the most to the <strong class="source-inline1">POSITIVE</strong> label in this case. We also print the precision as measured by the explainer. We observe that it identifies the words <strong class="source-inline1">good</strong>, <strong class="source-inline1">a</strong>, and <strong class="source-inline1">is</strong> as contributing the most to the <strong class="source-inline1">POSITIVE</strong> classification:<pre class="source-code">
print('Anchor: %s' % (' AND '.join(exp.names())))
print('Precision: %.2f' % exp.precision())</pre></li>			</ol>
			<pre class="console">
Anchor: good AND a AND is
Precision: 1.00</pre>			<ol class="calibre13">
				<li value="7" class="calibre14">We print some of the possible sentences that the explainer believes would result in a <strong class="source-inline1">POSITIVE</strong> classification. The explainer perturbs the input sentence by replacing one or more of the words with the <strong class="source-inline1">UNK</strong> token and invokes the classifier method on the perturbed sentence. There are some interesting observations on how the classifier behaves. For example, the sentence <strong class="source-inline1">The UNK UNK is a good story UNK</strong> has been labeled as <strong class="source-inline1">POSITIVE</strong>. This indicates that the title of the story is irrelevant to the classification. Another interesting example is the sentence <strong class="source-inline1">The UNK mermaid is a good UNK UNK</strong>. In this sentence, we observe that the classifier is invariant to the object in context, which, in <a id="_idIndexMarker579" class="calibre6 pcalibre pcalibre1"/>this case, is a story:<pre class="source-code">
print('\n'.join([x[0] <strong class="bold1">for</strong> x <strong class="bold1">in</strong> exp.examples(
    only_same_prediction=True)]))</pre></li>			</ol>
			<pre class="console">
The little UNK is a good UNK .
The UNK mermaid is a good story .
The UNK UNK is a good story UNK
UNK little mermaid is a good story UNK
The UNK mermaid is a good UNK .
UNK little UNK is a good UNK .
The little mermaid is a good story UNK
The UNK UNK is a good UNK .
The little UNK is a good UNK .
The little mermaid is a good UNK .</pre>			<ol class="calibre13">
				<li value="8" class="calibre14">Similar to the previous step, we now ask the explainer to print sentences that would result in a <strong class="source-inline1">NEGATIVE</strong> classification. In this particular case, the explainer was unable to generate <a id="_idIndexMarker580" class="calibre6 pcalibre pcalibre1"/>any negative examples by just replacing the words. The explainer is unable to generate any <strong class="source-inline1">NEGATIVE</strong> examples. This happens because the explainer can only use the <strong class="source-inline1">UNK</strong> token to perturb the input sentence. And since the <strong class="source-inline1">UNK</strong> token is not associated with any <strong class="source-inline1">POSITIVE</strong> or <strong class="source-inline1">NEGATIVE</strong> sentiment, using just that token does not provide a way to affect the classifier to generate a <strong class="source-inline1">NEGATIVE</strong> classification. We get no output from this step:<pre class="source-code">
print('\n'.join([x[0] <strong class="bold1">for</strong> x <strong class="bold1">in</strong> exp.examples(
    only_different_prediction=True)]))</pre></li>				<li class="calibre14">So far, we used the <strong class="source-inline1">UNK</strong> token to vary or perturb the input to the classifier. The presence of the <strong class="source-inline1">UNK</strong> token in the text makes it unnatural. To understand the classifier better, it would be useful to enumerate natural sentences and understand how those affect the classification. We will use <strong class="bold">BERT</strong> to perturb the input and get the explainer to generate natural sentences. This will help us better understand how <a id="_idIndexMarker581" class="calibre6 pcalibre pcalibre1"/>the results differ in the context of sentences that are natural:<pre class="source-code">
explainer = anchor_text.AnchorText(nlp, 
    ['negative', 'positive'],
    use_unk_distribution=False)
exp = explainer.explain_instance(text, 
    predict_prob, threshold=0.95)</pre></li>				<li class="calibre14">We now print some sentences for which the classifier thinks the label would be <strong class="source-inline1">POSITIVE</strong>. In this instance, we observe that the explainer generates sentences that are natural. For example, the generated sentence <strong class="source-inline1">my little mermaid tells a good story</strong> replaced the word <strong class="source-inline1">the</strong> in the original sentence with <strong class="source-inline1">my</strong>. This word was<a id="_idIndexMarker582" class="calibre6 pcalibre pcalibre1"/> generated via BERT. BERT uses the encoder part of the Transformer architecture and has been trained to predict missing words in a sentence by masking them. The explainer in this case masks the individual words in the input sentence and uses BERT to generate the replacement word. Since the underlying model to generate text is a probabilistic model, your output might differ from the following and also vary between runs:<pre class="source-code">
print('\n'.join([x[0] <strong class="bold1">for</strong> x <strong class="bold1">in</strong> exp.examples(
    only_same_prediction=True)]))</pre></li>			</ol>
			<pre class="console">
the weeping mermaid gives his good story .
Me ##rmaid mermaid : a good story .
rainbow moon mermaid theater " good story "
my little mermaid tells a good story .
Pretty little mermaid tells a good story .
My black mermaid song sweet good story ;
" little mermaid : very good story .
This damned mermaid gives a good story .
| " mermaid " : good story .
Me ##rmaid mermaid : very good story .</pre>			<ol class="calibre13">
				<li value="11" class="calibre14">We now print some sentences for which the classifier thinks the label would be <strong class="source-inline1">NEGATIVE</strong>. Though<a id="_idIndexMarker583" class="calibre6 pcalibre pcalibre1"/> not all the sentences appear to have a <strong class="source-inline1">NEGATIVE</strong> sentiment, there are quite a few of them with such a<a id="_idIndexMarker584" class="calibre6 pcalibre pcalibre1"/> sentiment:<pre class="source-code">
print('\n'.join([x[0] <strong class="bold1">for</strong> x <strong class="bold1">in</strong> exp.examples(
    only_different_prediction=True)]))</pre></li>			</ol>
			<pre class="console">
' til mermaid brings a good story …
only little mermaid : too good story ##book
smash hit mermaid with any good story ...
nor did mermaid tell a good story !
† denotes mermaid side / good story .
no native mermaid has a good story .
no ordinary mermaid is a good story .
Very little mermaid ain any good story yet
miss rainbow mermaid made a good story .
The gorgeous mermaid ain your good story (</pre>		</div>
	</body></html>