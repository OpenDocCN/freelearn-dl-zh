<html><head></head><body>
  <div><h1 class="chapterNumber"><a id="_idTextAnchor191"/>7</h1>
    <h1 class="chapterTitle" id="_idParaDest-184"><a id="_idTextAnchor192"/>Enhancing the GenAISys with DeepSeek</h1>
    <p class="normal">The <em class="italic">DeepSeek-V3 Technical Report</em> arrived in December 2024, followed a month later by the <strong class="keyWord">DeepSeek-R1</strong> paper and <a id="_idIndexMarker462"/>a full set of open source resources. The release sent a shockwave through the AI community: download counts on Hugging Face exploded, DeepSeek apps topped store charts, and new API providers sprang up overnight. Governments debated moratoriums while the major generative AI players—OpenAI, X (with Grok 3), and others—stepped on the gas. Within weeks, we saw o3 versions improve OpenAI models, a clear signal that the AI race had entered a new phase. At the same time, real-world AI production teams watched these dizzying innovations pile up, disrupting existing AI systems. Teams that spent months adapting their systems to one generative AI model found themselves caught in a gray area between systems that work but could still be improved.</p>
    <p class="normal">So, what should we do? Should we upgrade a stable GenAISys to follow the latest trend in an accelerating AI market with the cost and risks entailed? Or should we ignore the latest models if our system is stable? If we ignore evolutions, our system may become obsolete. If we keep following the trends, our system will become unstable!</p>
    <p class="normal">This chapter shows how to strike a workable balance. Instead of rewriting entire environments for every model upgrade or new functionality, we <a id="_idIndexMarker463"/>introduce a <strong class="keyWord">handler-selection mechanism</strong> that routes user requests to the right tool at the right time. A <strong class="keyWord">handler registry</strong> stores <a id="_idIndexMarker464"/>every AI function we develop; the selection layer inspects each incoming message and triggers the appropriate handler. With this design, the GenAISys can evolve indefinitely without destabilizing the stack. We will begin the chapter by defining how a balanced approach can be found between model evolutions and real-world usage, illustrated through a product design and production use case. Next comes a concise look at DeepSeek-V3, DeepSeek-R1, and the distilled Llama model we’ll implement. Then, we’ll install <strong class="keyWord">DeepSeek-R1-Distill-Llama-8B</strong> locally with Hugging Face, wrap it in a reusable function, and<a id="_idIndexMarker465"/> then plug it into our GenAISys. At that point, we will develop the flexible, scalable environment of the handler-selection mechanism to allow us to activate the models and tasks we need for each project. By the end of the chapter, you will be able to have full control over the GenAISys and be ready for whatever the AI market throws at you.</p>
    <p class="normal">This chapter covers the following topics:</p>
    <ul>
      <li class="bulletList">The balance between AI acceleration and usage</li>
      <li class="bulletList">An overview of DeepSeek-V3, R1, and distillation models</li>
      <li class="bulletList">Installing DeepSeek-R1-Distill-Llama-8B locally</li>
      <li class="bulletList">Creating a function to run DeepSeek-R1-Distill-Llama-8B</li>
      <li class="bulletList">Deploying DeepSeek-R1-Distill-Llama-8B in the GenAISys</li>
      <li class="bulletList">Building a handler registry for all the AI functions</li>
      <li class="bulletList">Building a handler-selection mechanism to select the handlers</li>
      <li class="bulletList">Upgrading the AI functions to be handler-compatible</li>
      <li class="bulletList">Running product design and production examples</li>
    </ul>
    <p class="normal">Let’s start by defining the balance between relentless AI evolution and day-to-day business usage.</p>
    <h1 class="heading-1" id="_idParaDest-185"><a id="_idTextAnchor193"/>Balancing model evolution with project needs</h1>
    <p class="normal">Before racing to adopt every new model, we must anchor our decisions on project needs. So far, our GenAISys <a id="_idIndexMarker466"/>has served mostly marketing functions for an online travel agency. Now, imagine that the agency has grown large enough to fund a line of branded merchandise—custom travel bags, booklets, and other goodies. To manage this new venture, the company hires a <strong class="keyWord">product designer and production manager</strong> (<strong class="keyWord">PDPM</strong>). The<a id="_idIndexMarker467"/> PDPM studies customer feedback and designs personalized kits but quickly sees that AI could boost both creativity and throughput.</p>
    <p class="normal">The examples in this chapter thus focus on product design and production workflows. Our goal is not to force DeepSeek (or any other model) into every task but to choose the model that best fits the need. To do that, we’ll extend the GenAISys with a handler-selection mechanism that responds to user choices in the IPython interface and to keywords in each message. Depending on the situation, the operations team can configure the system to route requests to GPT-4o, DeepSeek, or any future model.</p>
    <p class="normal">Before wiring DeepSeek into our GenAISys, let’s review the DeepSeek model family.</p>
    <h1 class="heading-1" id="_idParaDest-186"><a id="_idTextAnchor194"/>DeepSeek-V3, DeepSeek-V1, and R1-Distill-Llama: Overview</h1>
    <p class="normal">DeepSeek’s journey began with DeepSeek-V3, advanced to DeepSeek-R1—a reasoning-focused upgrade—and then branched into distilled variants built on Qwen and Llama architectures, as shown in <em class="italic">Figure 7.1</em>. V3 was responsible for putting the model on the map, and it was R1 that brought in robust reasoning.</p>
    <figure class="mediaobject"><img alt="Figure 7.1: DeepSeek development cycle" src="img/B32304_07_1.png"/></figure>
    <p class="packt_figref">Figure 7.1: DeepSeek development cycle</p>
    <p class="normal">According to DeepSeek-AI et al. (2024), V3 delivered striking efficiency gains. Its full training budget was only 2.788 million H800 GPU-hours (≈ USD 5.6 million at USD 2 per GPU-hour)—remarkably low for a modern frontier model. Even on a per-token basis, the cost is lean, needing just 180 K GPU-hours per trillion tokens. The cost is, therefore, very economical compared to what is typically reported for large-scale models.</p>
    <p class="normal">When we examine the list of authors of the DeepSeek-V3 Technical Report (2024) on arXiv, <a href="https://arxiv.org/abs/2412.19437">https://arxiv.org/abs/2412.19437</a>, we first notice that more than 150 specialists wrote the paper! In itself, this factor alone proves the efficiency of open source approaches that involve collective efforts to produce efficiency-driven architectures by opening ideas to every person willing to contribute. The list of <em class="italic">Contributions and Acknowledgements</em> in <em class="italic">Appendix A</em> is a tribute to open source developments.</p>
    <figure class="mediaobject"><img alt="Figure 7.2: DeepSeek-R1 is derived from DeepSeek-V3" src="img/B32304_07_2.png"/></figure>
    <p class="packt_figref">Figure 7.2: DeepSeek-R1 is derived from DeepSeek-V3</p>
    <p class="normal">DeepSeek-R1 grew straight out of DeepSeek-V3. The team wanted V3’s punch, but with feather-weight inference, so they wired the model to activate only a minimal subset of experts during inference, as shown in <em class="italic">Figure 7.2</em>. Furthermore, training stayed just as lean. R1 jumped directly into reinforcement learning with no supervised fine-tuning. The reasoning was high but faced limitations for classic NLP tasks. Rule-based rewards were introduced to avoid the neural network’s training cycles. The training prompts were structured with neat <code class="inlineCode">&lt;think&gt; … &lt;answer&gt;</code> tags, avoiding the smuggling of biases into the model’s final answer. Moreover, the reinforcement learning process began with <em class="italic">cold-start</em> data containing <strong class="keyWord">chain of thought</strong> (<strong class="keyWord">CoT</strong>) examples focusing on reasoning. This approach reduced training time and costs.</p>
    <p class="normal">DeepSeek evolved to R1 by refining MoE strategies and integrating multi-token prediction, significantly enhancing both accuracy and efficiency. Finally, DeepSeek-R1 was used to enhance DeepSeek-V3 with reasoning features. DeepSeek-R1 was also distilled into smaller models such as Llama and Qwen. The technique used was knowledge distillation, where a smaller “student” model (in this chapter, Llama) learns from a “teacher” model (in this chapter, DeepSeek-R1). This approach is effective in that it teaches the student model to achieve performance similar to that of the teacher while being more efficient and suitable for deployment on resource-constrained devices, which will be the case in this chapter, as you’ll see.</p>
    <p class="normal">Let’s install and run DeepSeek-R1-Distill-Llama-8B and plug it into our GenAISys.</p>
    <h1 class="heading-1" id="_idParaDest-187"><a id="_idTextAnchor195"/>Getting started with DeepSeek-R1-Distill-Llama-8B</h1>
    <p class="normal">In this <a id="_idIndexMarker468"/>section, we will implement DeepSeek-RAI-Distill-Llama-8B, a distilled version of DeepSeek-R1, as shown in <em class="italic">Figure 7.3</em>. We will install Hugging Face’s open-source <code class="inlineCode">Transformers</code> library, an open framework for using and fine-tuning pre-trained transformer models.</p>
    <figure class="mediaobject"><img alt="Figure 7.3: Installing DeepSeek-RAI-Distill-Llama-8B, a distilled version of DeepSeek-R1" src="img/B32304_07_3.png"/></figure>
    <p class="packt_figref">Figure 7.3: Installing DeepSeek-RAI-Distill-Llama-8B, a distilled version of DeepSeek-R1</p>
    <div><p class="normal">We will be using the DeepSeek-RAI-Distill-Llama-8B documented by Hugging Face: <a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B">https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B</a>. Hugging Face also provides recommendations for this model: <a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B#usage-recommendations">https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B#usage-recommendations</a>. </p>
    </div>
    <p class="normal">The version we will download is an open source distilled version of DeepSeek-R1 provided by Unsloth, an LLM accelerator, on Hugging Face: <a href="https://unsloth.ai/">https://unsloth.ai/</a>. We will thus not use a DeepSeek API but only a locally installed open source version that does not interact with the web, leveraging Hugging Face’s SOC 2 Type 2 certification that complies with privacy and security constraints: <a href="https://huggingface.co/docs/inference-endpoints/en/security">https://huggingface.co/docs/inference-endpoints/en/security</a>.</p>
    <p class="normal">To install <a id="_idIndexMarker469"/>deepseek-ai/DeepSeek-R1-Distill-Llama-8B locally on a recent machine, it is recommended to have about 20 GB of RAM. A bit less is possible, but it is best to avoid the risk. About 20 GB of disk space is also recommended.</p>
    <p class="normal">To install DeepSeek-R1-Distill-Llama-8B on Google Colab, it is recommended to use Google Colab Pro to obtain GPU memory and power. For this section, the Hugging Face model is downloaded on Google Drive, which is mounted through Google Colab. The disk space required will exceed the free version of Google Drive, and a minimal subscription to Google Drive may be required. Check the costs before installing on Google Colab.</p>
    <p class="normal">Open <code class="inlineCode">Getting_started_with_DeepSeek_R1_Distill_Llama_8B.ipynb</code> within the Chapter07 directory on GitHub (<a href="https://github.com/Denis2054/Building-Business-Ready-Generative-AI-Systems/tree/main">https://github.com/Denis2054/Building-Business-Ready-Generative-AI-Systems/tree/main</a>). We will follow the standard procedure of the Hugging Face framework:</p>
    <ul>
      <li class="bulletList">Run the notebook once to install DeepSeek-R1-Distill-Llama-8B locally:
        <pre class="programlisting code"><code class="hljs-code">install_deepseek=True
</code></pre>
      </li>
      <li class="bulletList">Run the notebook with no installation and interact with the model:
        <pre class="programlisting code"><code class="hljs-code">install_deepseek=False
</code></pre>
      </li>
    </ul>
    <p class="normal">With the model in place, we can wrap it in a handler and plug it into our GenAISys in the next section.</p>
    <h2 class="heading-2" id="_idParaDest-188"><a id="_idTextAnchor196"/>Setting up the DeepSeek Hugging Face environment</h2>
    <p class="normal">We’ll begin by <a id="_idIndexMarker470"/>installing DeepSeek-R1-Distill-Llama-8B (locally or in Colab) and then run a quick inference to confirm everything works.</p>
    <p class="normal">We will first install DeepSeek in the first session:</p>
    <pre class="programlisting code"><code class="hljs-code"># Set install_deepseek to True to download and install R1-Distill-Llama-8B locally
# Set install_deepseek to False to run an R1 session
install_deepseek=True
</code></pre>
    <p class="normal">The GPU needs to be activated, so let’s check it:</p>
    <pre class="programlisting code"><code class="hljs-code">Checking GPU activation
!nvidia-smi
</code></pre>
    <p class="normal">If we are installing Google Colab, we can mount Google Drive:</p>
    <pre class="programlisting code"><code class="hljs-code">from google.colab import drive
drive.mount('/content/drive')
</code></pre>
    <p class="normal">We now<a id="_idIndexMarker471"/> set the cache directory in Google Drive and set the corresponding environment variables:</p>
    <pre class="programlisting code"><code class="hljs-code">import os
# Define the cache directory in your Google Drive
cache_dir = '/content/drive/MyDrive/genaisys/HuggingFaceCache'
# Set environment variables to direct Hugging Face to use this cache directory
os.environ['TRANSFORMERS_CACHE'] = cache_dir
#os.environ['HF_DATASETS_CACHE'] = os.path.join(cache_dir, 'datasets')
</code></pre>
    <p class="normal">We can now install the Hugging Face <code class="inlineCode">Transformers</code> library:</p>
    <pre class="programlisting code"><code class="hljs-code">!pip install transformers==4.48.3
</code></pre>
    <p class="normal">With that, we are ready to download the model.</p>
    <h2 class="heading-2" id="_idParaDest-189"><a id="_idTextAnchor197"/>Downloading DeepSeek</h2>
    <p class="normal">Let’s now <a id="_idIndexMarker472"/>download the model from <code class="inlineCode">unsloth/DeepSeek-R1-Distill-Llama-8B</code> within the Hugging Face framework with the tokenizer and the model:</p>
    <pre class="programlisting code"><code class="hljs-code">from transformers import AutoTokenizer, AutoModelForCausalLM
import time
if install_deepseek==True:
    # Record the start time
    start_time = time.time()
    model_name = 'unsloth/DeepSeek-R1-Distill-Llama-8B'
    # Load the tokenizer and model
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(
        model_name, device_map='auto', torch_dtype='auto'
    )
    # Record the end time
    end_time = time.time()
    # Calculate the elapsed time
    elapsed_time = end_time - start_time
    print(f"Time taken to load the model: {elapsed_time:.2f} seconds")
</code></pre>
    <p class="normal">The download time will be displayed and also depends on your internet connection and Hugging Face’s download speed. Once installed, verify that everything is installed in your local directory. In this<a id="_idIndexMarker473"/> case, it is as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">if install_deepseek==True:
    !ls -R /content/drive/MyDrive/genaisys/HuggingFaceCache
</code></pre>
    <p class="normal">The output should show the files downloaded:</p>
    <pre class="programlisting con"><code class="hljs-con">/content/drive/MyDrive/genaisys/HuggingFaceCache:
models--unsloth--DeepSeek-R1-Distill-Llama-8B  version.txt
/content/drive/MyDrive/genaisys/HuggingFaceCache/models--unsloth--DeepSeek-R1-Distill-Llama-8B:
blobs  refs  snapshots
/content/drive/MyDrive/genaisys/HuggingFaceCache/models--unsloth--DeepSeek-R1-Distill-Llama-8B/blobs:
03910325923893259d090bfa92baa4088cd46573…
</code></pre>
    <p class="normal">Now, let’s run a DeepSeek session.</p>
    <h2 class="heading-2" id="_idParaDest-190"><a id="_idTextAnchor198"/>Running a DeepSeek-R1-Distill-Llama-8B session</h2>
    <p class="normal">To make sure <a id="_idIndexMarker474"/>the model is correctly installed and also to avoid overwriting the installation when starting a new session, go back to the top of the notebook and set the following:</p>
    <pre class="programlisting code"><code class="hljs-code">install_deepseek=False
</code></pre>
    <p class="normal">We will now load the <code class="inlineCode">DeepSeek-R1-Distill-Llama-8B</code> tokenizer and model locally:</p>
    <pre class="programlisting code"><code class="hljs-code">import time
from transformers import AutoTokenizer, AutoModelForCausalLM
if install_deepseek==False:
    # Define the path to the model directory
    model_path = '/content/drive/MyDrive/genaisys/HuggingFaceCache/models--unsloth--DeepSeek-R1-Distill-Llama-8B/snapshots/71f34f954141d22ccdad72a2e3927dddf702c9de'
    # Record the start time
    start_time = time.time()
    # Load the tokenizer and model from the specified path
    tokenizer = AutoTokenizer.from_pretrained(
        model_path, local_files_only=True
    )
    model = AutoModelForCausalLM.from_pretrained(
        model_path, device_map='auto', torch_dtype='auto', 
        local_files_only=True
    )
    # Record the end time
    end_time = time.time()
    # Calculate the elapsed time
    elapsed_time = end_time - start_time
    print(f"Time taken to load the model: {elapsed_time:.2f} seconds")
</code></pre>
    <p class="normal">The time it took to load <a id="_idIndexMarker475"/>the model is displayed and will depend on the configuration of your machine:</p>
    <pre class="programlisting code"><code class="hljs-code">Time taken to load the model: 14.71 seconds
</code></pre>
    <p class="normal">We can have a look at the configuration of the Llama model:</p>
    <pre class="programlisting code"><code class="hljs-code">if install_deepseek==False:
    print(model.config)
</code></pre>
    <p class="normal">The output shows interesting information. The <code class="inlineCode">LlamaConfig</code> readout confirms we are running a compact, well-scoped model:</p>
    <pre class="programlisting con"><code class="hljs-con">LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "/content/drive/MyDrive/genaisys/HuggingFaceCache/models--unsloth--DeepSeek-R1-Distill-Llama-8B/snapshots/71f34f954141d22ccdad72a2e3927dddf702c9de",
  "architectures": [
    "LlamaForCausalLM"
  ],
 …
</code></pre>
    <p class="normal">The distilled Llama model has 32 transformer layers and 32 attention heads per layer, totaling 1,024 attention heads. Also, it contains 8 billion parameters. By contrast, its teacher model, <strong class="keyWord">DeepSeek-R1</strong>, is an MoE giant with <strong class="keyWord">61 layers</strong> and a massive <strong class="keyWord">671 billion parameters</strong>, of which about <strong class="keyWord">37 billion</strong> are active on each forward pass. Let’s now run an example with a prompt for a production issue:</p>
    <pre class="programlisting code"><code class="hljs-code">if install_deepseek==False:
    prompt="""
    Explain how a product designer could transform customer requirements for a traveling bag into a production plan.
    """
</code></pre>
    <p class="normal">We first insert time measurement and tokenize the input using the GPU:</p>
    <pre class="programlisting code"><code class="hljs-code">import time
if install_deepseek==False:
    # Record the start time
    start_time = time.time()
    # Tokenize the input
    inputs = tokenizer(prompt, return_tensors='pt').to('cuda')
</code></pre>
    <p class="normal">Then, we run the generation:</p>
    <pre class="programlisting code"><code class="hljs-code">    # Generate output with enhanced anti-repetition settings
    outputs = model.generate(
      **inputs,
        max_new_tokens=1200,
        repetition_penalty=1.5,     # Increase penalty to 1.5 or higher
        no_repeat_ngram_size=3,     # Prevent repeating n-grams of size 3
        temperature=0.6,            # Reduce randomness slightly
        top_p=0.9,                  # Nucleus sampling for diversity
        top_k=50       # Limits token selection to top-k probable tokens
  )
</code></pre>
    <p class="normal">The goal of our <a id="_idIndexMarker476"/>parameters is to limit the repetitions and remain focused:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">max_new_tokens=1200</code>: To limit the number of output tokens</li>
      <li class="bulletList"><code class="inlineCode">repetition_penalty=1.5</code>: To limit the repetitions (can be higher)</li>
      <li class="bulletList"><code class="inlineCode">no_repeat_ngram_size=3</code>: To prevent repeating n-grams of a particular size</li>
      <li class="bulletList"><code class="inlineCode">temperature=0.6</code>: To reduce randomness and stay focused</li>
      <li class="bulletList"><code class="inlineCode">top_p=0.9</code>: Allows nucleus sampling for diversity</li>
      <li class="bulletList"><code class="inlineCode">top_k=50</code>: Limits token selection to <code class="inlineCode">top_k</code> to make the next token choice</li>
    </ul>
    <p class="normal">This set of tokens tends to limit repetitions while allowing diversity. We can now decode the generated text with the tokenizer:</p>
    <pre class="programlisting code"><code class="hljs-code">    # Decode and display the output
    generated_text = tokenizer.decode(
        outputs[0], skip_special_tokens=True
    )
    # Record the end time
    end_time = time.time()
    # Calculate the elapsed time
    elapsed_time = end_time - start_time
    print(f"Time taken to load the model: {elapsed_time:.2f} seconds")
</code></pre>
    <p class="normal">The output shows the overall time it took the model to think and respond:<code class="inlineCode"> </code></p>
    <pre class="programlisting con"><code class="hljs-con">Time taken to load the model: 20.61 seconds               
</code></pre>
    <p class="normal">Let’s wrap <code class="inlineCode">generated_text</code> and display it:</p>
    <pre class="programlisting code"><code class="hljs-code">import textwrap
if install_deepseek==False:
    wrapped_text = textwrap.fill(generated_text, width=80)   
print(wrapped_text)
</code></pre>
    <p class="normal">The <a id="_idIndexMarker477"/>output provides ideas as requested. It displays DeepSeek-R1’s thinking abilities:</p>
    <pre class="programlisting con"><code class="hljs-con">…Once goals &amp; priorities become clearer, developing
prototypes becomes more focused since each iteration would aim at testing one main feature rather than multiple changes simultaneously—which makes refining individual elements easier before moving towards finalizing designs, When prototyping starts: 1) Start with basic functional mockups using simple tools –…
</code></pre>
    <h2 class="heading-2" id="_idParaDest-191"><a id="_idTextAnchor199"/>Integrating DeepSeek-R1-Distill-Llama-8B</h2>
    <p class="normal">In this section, we<a id="_idIndexMarker478"/> will add DeepSeek-R1-Distill-Llama-8B to our GenAISys in a few steps. Open <code class="inlineCode">GenAISys_DeepSeek.ipynb</code>. You can decide to run the notebook with DeepSeek in the first cell, which will require a GPU:</p>
    <pre class="programlisting code"><code class="hljs-code"># DeepSeek activation deepseek=True to activate. 20 Go (estimate) GPU memory and 30-40 Go Disk Space
deepseek=True
</code></pre>
    <p class="normal">You can also decide not to run DeepSeek in this notebook, in which case, you will not need a GPU and can change the runtime to CPU. If you decide on this option, OpenAI’s API will take over, confirming that no GPU is required:</p>
    <pre class="programlisting code"><code class="hljs-code">deepseek=False
</code></pre>
    <p class="normal">Now, go to the <em class="italic">Setting up the DeepSeek Hugging Face environment</em> subsection of the notebook. We will simply transfer the following cells from <code class="inlineCode">Getting_started_with_DeepSeek_R1_Distill_Llama_8B.ipynb</code> to this subsection. The following code will only be activated if <code class="inlineCode">deepseek=True</code>:</p>
    <ul>
      <li class="bulletList">GPU activation check: <code class="inlineCode">!nvidia-smi</code></li>
      <li class="bulletList">Setting the local cache of the model: <code class="inlineCode">…os.environ['TRANSFORMERS_CACHE'] =cache_dir…</code></li>
      <li class="bulletList">Installing the Hugging Face library: <code class="inlineCode">!pip install transformers==4.48.3</code></li>
      <li class="bulletList">Loading the tokenizer and the model:
        <pre class="programlisting code"><code class="hljs-code">  from transformers import AutoTokenizer, AutoModelForCausalLM
  # Define the path to the model directory
  model_path = …
</code></pre>
      </li>
    </ul>
    <p class="normal">The<a id="_idIndexMarker479"/> installation is now complete. The calls to the DeepSeek model will be made in the <em class="italic">AI Functions</em> section if <code class="inlineCode">DeepSeek==True</code> with the parameters described in the <em class="italic">Running a DeepSeek-R1-Distill-Llama-8B session</em> section:</p>
    <pre class="programlisting code"><code class="hljs-code">    if models == "DeepSeek":
      # Tokenize the input
      inputs = tokenizer(sc_input, return_tensors='pt').to('cuda')
….
      task_response =tokenizer.decode(outputs[0],skip_special_tokens=True)
</code></pre>
    <p class="normal">With DeepSeek functioning, we’re ready to build the handler selection mechanism, which will route every user request to GPT-4o, DeepSeek, or any future model—without touching the rest of the stack.</p>
    <h1 class="heading-1" id="_idParaDest-192"><a id="_idTextAnchor200"/>Implementing the handler selection mechanism as an orchestrator of the GenAISys</h1>
    <p class="normal">The PDPM at <a id="_idIndexMarker480"/>the online travel agency is experiencing increased demands, requiring the agency to design and produce large quantities of merchandise kits, including travel bags, booklets, and pens. The PDPM wants to be directly involved in the GenAISys development to explore how it can significantly boost productivity.</p>
    <p class="normal">Given the growing complexity and variety of AI tasks in the system, the GenAISys development team has decided to organize these tasks using handlers, as illustrated in <em class="italic">Figure 7.4</em>:</p>
    <figure class="mediaobject"><img alt="Figure 7.4: GenAISys data flow and component interaction" src="img/B32304_07_4.png"/></figure>
    <p class="packt_figref">Figure 7.4: GenAISys data flow and component interaction</p>
    <p class="normal">We’ll, therefore, define, implement, and then invite the PDPM to run the enhanced GenAISys to evaluate<a id="_idIndexMarker481"/> functions aimed at improving productivity in merchandise design and production.</p>
    <p class="normal"><em class="italic">Figure 7.4</em> describes the behavior of the handler pipeline we are going to implement:</p>
    <ol>
      <li class="numberedList" value="1">The <strong class="keyWord">IPython interface</strong> serves as the entry and exit point for user interactions, capturing user input, formatting it, and displaying responses returned by the handler mechanism.</li>
      <li class="numberedList">The <strong class="keyWord">handler mechanism</strong> interprets user inputs, directing data among the IPython interface, the handler registry, and the AI functions. It ensures tasks triggered by user messages execute smoothly.</li>
      <li class="numberedList">The <strong class="keyWord">handler registry</strong> maintains a list of all available handlers and their corresponding functions. It supports system modularity and scalability by clarifying handler registration and retrieval.</li>
      <li class="numberedList"><strong class="keyWord">AI functions</strong> perform core tasks such as natural language understanding and data analysis, executing instructions received from the handler mechanism, and returning outputs to the IPython Interface.</li>
    </ol>
    <p class="normal">In this setup, a user provides input through the IPython interface. This input is routed into a handler selection mechanism, which then evaluates the available handlers registered alongside specific conditions. Each entry in the registry is a (condition, handler) pair responsible for different operations such as reasoning, image generation, or data analysis. Once a matching condition is found, the corresponding AI function is activated. After processing, it returns the results to the interface. This structured pipeline—from user input through to the AI-generated response—is handled gracefully, with each handler clearly defined for readability and efficiency.</p>
    <p class="normal">Before coding, let’s clearly define what we mean by a “handler” in the GenAISys.</p>
    <h2 class="heading-2" id="_idParaDest-193"><a id="_idTextAnchor201"/>What is a handler?</h2>
    <p class="normal">A handler is<a id="_idIndexMarker482"/> essentially a specialized function responsible for addressing specific tasks or types of requests. Each handler is registered alongside a condition, typically a small function or lambda expression. When evaluated as <code class="inlineCode">True</code>, this condition indicates that the associated handler should be invoked. This design neatly decouples the logic for deciding <em class="italic">which</em> handler should run from <em class="italic">how</em> the handler executes its task.</p>
    <p class="normal">In our context, handlers are the orchestrator’s building blocks—conditional functions designed to process specific input types. When a user provides input, the handler selection mechanism evaluates it against the handler registry, which consists of pairs of conditions and handlers. Upon finding a match, the corresponding handler is triggered, invoking specialized functions such as <code class="inlineCode">handle_generation</code>, <code class="inlineCode">handle_analysis</code>, or <code class="inlineCode">handle_pinecone_rag</code>. These handlers execute sophisticated reasoning, data retrieval, or content generation tasks, providing precise and targeted outputs.</p>
    <p class="normal">But why exactly is a handler better for our GenAISys than a traditional list of <code class="inlineCode">if…then</code> conditions?</p>
    <h2 class="heading-2" id="_idParaDest-194"><a id="_idTextAnchor202"/>Why is a handler better than a traditional if...then list?</h2>
    <p class="normal">Using handlers<a id="_idIndexMarker483"/> improves maintainability and readability. Instead of scattering multiple <code class="inlineCode">if...then</code> checks across the code, each handler is self-contained: it has its condition and a separate function that carries out the required action. This structure makes it easier to add, remove, or modify handlers without risking unintended interactions in a chain of lengthy conditionals. Additionally, since it separates the logic of “which handler do we need?” from “how does that handler actually work?” we’re left with a more modular design that makes scaling seamless.</p>
    <p class="normal">We will first go through the modifications to our IPython interface.</p>
    <h1 class="heading-1" id="_idParaDest-195"><a id="_idTextAnchor203"/>1. IPython interface</h1>
    <p class="normal">We’ll start by<a id="_idIndexMarker484"/> reviewing the primary updates to our IPython interface, which remains the main interaction point, as shown in <em class="italic">Figure 7.5</em>. From a user perspective, the introduction of handlers doesn’t alter the interface significantly, but some underlying code adjustments are necessary.</p>
    <figure class="mediaobject"><img alt="Figure 7.5: The IPython interface processes the user input and displays the output" src="img/B32304_07_5.png"/></figure>
    <p class="packt_figref">Figure 7.5: The IPython interface processes the user input and displays the output</p>
    <p class="normal">The IPython interface calls <code class="inlineCode">chat_with_gpt</code> as before:</p>
    <pre class="programlisting code"><code class="hljs-code">response = chat_with_gpt(
    user_histories[active_user], user_message, pfiles, 
    active_instruct, models=selected_model
)
</code></pre>
    <p class="normal">Now, however, we <a id="_idIndexMarker485"/>can explicitly select either an OpenAI or a DeepSeek model with the following:</p>
    <pre class="programlisting code"><code class="hljs-code">models=selected_model
</code></pre>
    <p class="normal">To add the model to the <code class="inlineCode">chat_with_gpt</code> call, we first add a drop-down model selector to the interface:</p>
    <pre class="programlisting code"><code class="hljs-code"># Dropdown for model selection
model_selector = Dropdown(
    options=["OpenAI", "DeepSeek"],
    value="OpenAI",
    description="Model:",
    layout=Layout(width="50%")
)
</code></pre>
    <p class="normal">The model selector is added to the <code class="inlineCode">VBox</code> instances in the interface:</p>
    <pre class="programlisting code"><code class="hljs-code"># Display interactive widgets
display(
    VBox(
        [user_selector, input_box, submit_button, agent_checkbox, 
            tts_checkbox, files_checkbox, instruct_selector, 
            model_selector],
        layout=Layout(display='flex', flex_flow='column', 
            align_items='flex-start', width='100%')
    )
)
</code></pre>
    <p class="normal">The user can now choose their preferred model directly from the interface, as shown here:</p>
    <figure class="mediaobject"><img alt="Figure 7.6: Selecting a model" src="img/B32304_07_6.png"/></figure>
    <p class="packt_figref">Figure 7.6: Selecting a model</p>
    <div><p class="normal"><img alt="A magnifying glass on a black background  AI-generated content may be incorrect." src="img/1.png"/><strong class="keyWord">Quick tip</strong>: Need to see a high-resolution version of this image? Open this book in the next-gen Packt Reader or view it in the PDF/ePub copy.</p>
      <p class="normal"><img alt="" src="img/2.png"/><strong class="keyWord">The next-gen Packt Reader</strong> is included for free with the purchase of this book. Scan the QR code OR go to <a href="http://packtpub.com/unlock">packtpub.com/unlock</a>, then use the search bar to find this book by name. Double-check the edition shown to make sure you get the right one.</p>
      <p class="normal"><img alt="A qr code on a white background  AI-generated content may be incorrect." src="img/Unlock_Code2.png"/></p>
    </div>
    <p class="normal">An additional feature<a id="_idIndexMarker486"/> has been added to manage file displays.</p>
    <h2 class="heading-2" id="_idParaDest-196"><a id="_idTextAnchor204"/>File management</h2>
    <p class="normal">There are many ways to <a id="_idIndexMarker487"/>design file management. We will introduce a function here that can be expanded during a project’s implementation phase as needed. Our file management code has three functions:</p>
    <ul>
      <li class="bulletList">Manage user-triggered file deletion</li>
      <li class="bulletList">Delete <code class="inlineCode">c_image.png</code> when the checkbox is unchecked</li>
      <li class="bulletList">Use existence checks to prevent errors during deletion</li>
    </ul>
    <p class="normal">We will build the code to handle user interactions directly by observing changes in the checkbox widget of our interface within the Jupyter Notebook environment. The code will then delete a specific image file (<code class="inlineCode">c_image.png</code>) when the user unchecks the checkbox named <code class="inlineCode">files_checkbox</code>. This ensures that files are removed cleanly when they are no longer needed, preventing clutter and saving storage space.</p>
    <p class="normal">We first define the function:</p>
    <pre class="programlisting code"><code class="hljs-code">def on_files_checkbox_change(change):
</code></pre>
    <p class="normal">The event handler function defines a callback function named <code class="inlineCode">on_files_checkbox_change</code> that will execute when the state of <code class="inlineCode">files_checkbox</code> changes. <code class="inlineCode">change</code> is provided by the observer, which contains information about the change event, including the following:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">old</code>: The previous state of the checkbox</li>
      <li class="bulletList"><code class="inlineCode">new</code>: The new state of the checkbox</li>
    </ul>
    <pre class="programlisting code"><code class="hljs-code">    # Only remove images if the checkbox changed from True to False.
    if change['old'] == True and change['new'] == False:
</code></pre>
    <p class="normal">The code verifies whether the checkbox was previously checked (<code class="inlineCode">True</code>) and has now been unchecked (<code class="inlineCode">False</code>). This guarantees that the file deletion only occurs when the user explicitly unchecks the checkbox, preventing accidental file removal. We now remove the file:</p>
    <pre class="programlisting code"><code class="hljs-code">        if os.path.exists("c_image.png"):
            os.remove("c_image.png")
</code></pre>
    <p class="normal">We also need <a id="_idIndexMarker488"/>to add an observer to inform the <code class="inlineCode">on_files_checkbox_change</code> function when there is a file status change:</p>
    <pre class="programlisting code"><code class="hljs-code"># Attach the observer to files_checkbox
files_checkbox.observe(on_files_checkbox_change, names='value')
</code></pre>
    <p class="normal">The <code class="inlineCode">files_checkbox.observe()</code> function links the <code class="inlineCode">on_files_checkbox_change </code>function to the <code class="inlineCode">files_checkbox</code> widget. <code class="inlineCode">names='value'</code> specifies that the function should be triggered when the value of the checkbox changes (i.e., when it is checked or unchecked).</p>
    <p class="normal">We will now move on to the next part of the pipeline and implement the handler selection mechanism.</p>
    <h1 class="heading-1" id="_idParaDest-197"><a id="_idTextAnchor205"/>2. Handler selection mechanism</h1>
    <p class="normal">The handler selection mechanism<strong class="keyWord"> </strong><strong class="keyWord"><a id="_idIndexMarker489"/></strong>dynamically selects and executes the appropriate handler based on predefined conditions. It iterates through available handlers, evaluating conditions until it finds a match, ensuring efficient and structured processing of the user input. The handler selection mechanism is in the <code class="inlineCode">chat_with_gpt</code> function we built in the previous chapters. However, it now contains an orchestration task, as shown in <em class="italic">Figure 7.7</em>:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">chat_with_gpt</code> remains a pivotal function within the GenAISys and now contains the handler mechanism</li>
      <li class="bulletList">It checks conditions sequentially to decide which handler to invoke</li>
      <li class="bulletList">It falls back to a memory-based handler if no conditions match</li>
      <li class="bulletList">It ensures robustness with error handling for an uninterrupted user experience</li>
    </ul>
    <figure class="mediaobject"><img alt="Figure 7.7: The orchestration role of the handler mechanism" src="img/B32304_07_7.png"/></figure>
    <p class="packt_figref">Figure 7.7: The orchestration role of the handler mechanism</p>
    <p class="normal">In the broader GenAISys workflow, the handler mechanism acts as an orchestrator. It processes user inputs and identifies which AI functions to activate. When the IPython interface captures user messages, the handler mechanism evaluates these inputs to determine the appropriate handler from the handler registry. If no specific handler matches, it defaults to a memory-based response, which is then returned to the IPython interface.</p>
    <p class="normal">The <code class="inlineCode">chat_with_gpt</code> function encapsulates this logic. It iterates through a predefined list of handlers, each paired with a corresponding condition function. When a condition evaluates to true, the associated handler is executed. If none match, the fallback memory-based handler ensures a seamless response:</p>
    <pre class="programlisting code"><code class="hljs-code">def chat_with_gpt(
    messages, user_message, files_status, active_instruct, models
):
     global memory_enabled  # Ensure memory is used if set globally
</code></pre>
    <p class="normal">Let’s go through<a id="_idIndexMarker490"/> the parameters of the function:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">messages</code>: The conversation history between the user and the AI</li>
      <li class="bulletList"><code class="inlineCode">user_message</code>: The latest message from the user</li>
      <li class="bulletList"><code class="inlineCode">files_status</code>: Tracks the status of any files involved in the conversation</li>
      <li class="bulletList"><code class="inlineCode">active_instruct</code>: Any instruction or mode that might influence how responses are generated</li>
      <li class="bulletList"><code class="inlineCode">models</code>: Specifies the active AI model in use</li>
    </ul>
    <p class="normal">The function uses <code class="inlineCode">global memory_enabled</code> to access a global variable that determines whether memory should be applied to store/remember the full dialogue of a user. In this chapter, <code class="inlineCode">global memory_enabled=True</code>.</p>
    <p class="normal">The function attempts to execute the appropriate handler based on the provided conditions:</p>
    <pre class="programlisting code"><code class="hljs-code">    try:
        # Iterate over handlers and execute the first matching one
        for condition, handler in handlers:
            if condition(messages, active_instruct, memory_enabled, 
                models, user_message):
                return handler(messages, active_instruct, memory_enabled, 
                    models, user_message, files_status=files_status)
</code></pre>
    <p class="normal">As you can see, <code class="inlineCode">for condition, handler in handlers</code> iterates over a list called <code class="inlineCode">handlers</code>, where each item is a tuple containing the following items:</p>
    <ul>
      <li class="bulletList">A condition function<strong class="keyWord"> </strong>to check whether a handler should be used</li>
      <li class="bulletList">A handler function to execute whether the condition is satisfied</li>
      <li class="bulletList">A generic <code class="inlineCode">if</code> condition, <code class="inlineCode">(...)</code>, to<strong class="keyWord"> </strong>evaluate the condition function with the provided parameters</li>
      <li class="bulletList">The code returns the output of the corresponding handler if the condition is met, immediately exiting the function</li>
    </ul>
    <p class="normal">Let’s now add a fallback if no handlers match the input conditions:</p>
    <pre class="programlisting code"><code class="hljs-code">        # If no handler matched, default to memory handling with full conversation history
        return handle_with_memory(
            messages,  # Now passing full message history
            user_message,
            files_status=files_status,
            instruct=active_instruct,
            mem=memory_enabled,  # Ensuring memory usage
            models=models
        )
</code></pre>
    <p class="normal"><code class="inlineCode">handle_with_memory</code> is called as a default handler that does the following:</p>
    <ul>
      <li class="bulletList">Uses the full conversation history (<code class="inlineCode">messages</code>)</li>
      <li class="bulletList">Considers memory if <code class="inlineCode">memory_enabled</code> is true, which is the case in this chapter</li>
      <li class="bulletList">Returns the response directly if executed</li>
    </ul>
    <p class="normal">Finally, let’s add an <a id="_idIndexMarker491"/>exception to catch return errors:</p>
    <pre class="programlisting code"><code class="hljs-code">    except Exception as e:
        return f"An error occurred in the handler selection mechanism: {str(e)}"
</code></pre>
    <p class="normal">With the handler selection mechanism defined, we can now proceed to build the handler registry that stores these handlers.</p>
    <h1 class="heading-1" id="_idParaDest-198"><a id="_idTextAnchor206"/>3. Handler registry</h1>
    <p class="normal">The <strong class="keyWord">handler registry</strong> is a <a id="_idIndexMarker492"/>structured collection of condition-handler pairs, where each condition is a lambda function that evaluates user messages and instructions to determine whether specific criteria are met. When a condition is satisfied, the corresponding handler is triggered and executed immediately, as illustrated:</p>
    <figure class="mediaobject"><img alt="Figure 7.8: Creating the handler registry" src="img/B32304_07_8.png"/></figure>
    <p class="packt_figref">Figure 7.8: Creating the handler registry</p>
    <div><p class="normal"> All lambda functions have four parameters (<code class="inlineCode">msg</code>, <code class="inlineCode">instruct</code>, <code class="inlineCode">mem</code>, and <code class="inlineCode">models</code>). This ensures that the number of arguments matches when <code class="inlineCode">chat_with_gpt()</code> calls a handler.</p>
    </div>
    <p class="normal">The handler registry has<a id="_idIndexMarker493"/> three main features:</p>
    <ul>
      <li class="bulletList">Is orchestrated by the handler mechanism and can be unlimited</li>
      <li class="bulletList">Routes inputs based on keywords, instructions, or model selection</li>
      <li class="bulletList">Guarantees a fallback response if no conditions match</li>
    </ul>
    <p class="normal">We will design<a id="_idIndexMarker494"/> our handler registry with the following structure of four key properties:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Handler registration</strong>: Creates a list of handlers, each with a condition function and a corresponding handler function</li>
      <li class="bulletList"><strong class="keyWord">Specific handler conditions</strong>: Sequentially checks whether an input meets any of the specific conditions</li>
      <li class="bulletList"><strong class="keyWord">Fallback handler</strong>: Adds a default memory-based handler if none of the conditions match</li>
      <li class="bulletList"><strong class="keyWord">Execution</strong>: When a condition is satisfied, the corresponding handler is executed immediately</li>
    </ul>
    <p class="normal">The role of <code class="inlineCode">**kwargs</code> in the code provides a flexible way to interact with the AI functions. <code class="inlineCode">**kwargs</code> is short for <em class="italic">keyword arguments</em> and is used in Python functions to allow passing a variable number of arguments to a function. In the context of our handler registry code, <code class="inlineCode">**kwargs</code> plays a crucial role by allowing handlers to accept additional, optional parameters without explicitly defining them in the function. It makes the handlers extensible for future updates or new parameters without requiring modifications to existing function signatures.</p>
    <p class="normal">We will now begin to build the handler registry with the Pinecone/RAG handler.</p>
    <h2 class="heading-2" id="_idParaDest-199"><a id="_idTextAnchor207"/>Pinecone/RAG handler</h2>
    <p class="normal">The <a id="_idIndexMarker495"/>Pinecone/RAG handler<a id="_idIndexMarker496"/> manages the <strong class="keyWord">retrieval-augmented generation</strong> (<strong class="keyWord">RAG</strong>) functions <a id="_idIndexMarker497"/>previously defined. It activates when detecting the <code class="inlineCode">Pinecone</code> or <code class="inlineCode">RAG</code> keyword within the user message:</p>
    <pre class="programlisting code"><code class="hljs-code">    # Pinecone / RAG handler: check only the current user message
    (
        lambda msg, instruct, mem, models, user_message,
        **kwargs: “Pinecone” in user_message or “RAG” in user_message,
        lambda msg, instruct, mem, models, user_message,
        **kwargs: handle_pinecone_rag(user_message, models=models)
    ),
</code></pre>
    <p class="normal">This handler checks whether the user message contains “Pinecone” or “RAG,” in which case <code class="inlineCode">lambda:</code> returns <code class="inlineCode">True</code>; otherwise, it returns <code class="inlineCode">False</code>. We will now create the reasoning handler.</p>
    <h2 class="heading-2" id="_idParaDest-200"><a id="_idTextAnchor208"/>Reasoning handler</h2>
    <p class="normal">We have<a id="_idIndexMarker498"/> already built the reasoning function, but now we <a id="_idIndexMarker499"/>need a handler. The keywords that trigger the handler are <code class="inlineCode">Use reasoning</code>, <code class="inlineCode">customer</code>, and <code class="inlineCode">activities</code>. Any additional text in the message provides context for the reasoning process. The handler uses <code class="inlineCode">all()</code> to ensure all keywords are included in the message:</p>
    <pre class="programlisting code"><code class="hljs-code">    # Reasoning handler: check only the current user message
    (
        lambda msg, instruct, mem, models, user_message, **kwargs: all(
            keyword in user_message for keyword in [
                “Use reasoning”, “customer”, “activities”
            ]
        ),
        lambda msg, instruct, mem, models, user_message, **kwargs:
            handle_reasoning_customer(user_message, models=models)
    ),
</code></pre>
    <p class="normal">Let’s move on and create the analysis handler.</p>
    <h2 class="heading-2" id="_idParaDest-201"><a id="_idTextAnchor209"/>Analysis handler</h2>
    <p class="normal">The <a id="_idIndexMarker500"/>analysis handler has been used for memory analysis up <a id="_idIndexMarker501"/>to now and is triggered by the <code class="inlineCode">Analysis</code> instruction:</p>
    <pre class="programlisting code"><code class="hljs-code">    # Analysis handler: determined by the instruct flag
    (
        lambda msg, instruct, mem, models, user_message,
            **kwargs: instruct == “Analysis”,
        lambda msg, instruct, mem, models, user_message,
            **kwargs: handle_analysis(
                user_message, models=models)
    ),
</code></pre>
    <p class="normal">Time to create the generation handler.</p>
    <h2 class="heading-2" id="_idParaDest-202"><a id="_idTextAnchor210"/>Generation handler</h2>
    <p class="normal">The <a id="_idIndexMarker502"/>generation handler takes memory analysis to another level <a id="_idIndexMarker503"/>by asking the generative AI model to generate an engaging text for a customer based on a memory analysis of the text. The <code class="inlineCode">Generation</code> instruction triggers the generation handler:</p>
    <pre class="programlisting code"><code class="hljs-code">    # Generation handler: determined by the instruct flag
    (
        lambda msg, instruct, mem, models, user_message,
               **kwargs: instruct == “Generation”,
        lambda msg, instruct, mem, models, user_message,
                **kwargs: handle_generation(
                    user_message, models=models)
    ),
</code></pre>
    <p class="normal">Let’s now build the image creation handler.</p>
    <h2 class="heading-2" id="_idParaDest-203"><a id="_idTextAnchor211"/>Image handler</h2>
    <p class="normal">The<a id="_idIndexMarker504"/> image creation <a id="_idIndexMarker505"/>handler is triggered by the <code class="inlineCode">Create</code> and <code class="inlineCode">image</code> keywords in the user message:</p>
    <pre class="programlisting code"><code class="hljs-code">    # Create image handler: check only the current user message
    (
        lambda msg, instruct, mem, models, user_message,
        **kwargs: “Create” in user_message and “image” in user_message,
        lambda msg, instruct, mem, models, user_message,
            **kwargs: handle_image_creation(user_message, models=models)
    )
]
</code></pre>
    <p class="normal">We will now create the freestyle handler for when there is no keyword or instructions.</p>
    <h2 class="heading-2" id="_idParaDest-204"><a id="_idTextAnchor212"/>Fallback memory handler</h2>
    <p class="normal">This<a id="_idIndexMarker506"/> handler is a general-purpose handler when there <a id="_idIndexMarker507"/>is no instruction or keyword to trigger a specific function. Let’s append the fallback memory handler accordingly:</p>
    <pre class="programlisting code"><code class="hljs-code"># Append the fallback memory handler for when instruct is “None”
handlers.append(
    (
        lambda msg, instruct, mem, models, user_message,
               **kwargs: instruct == “None”,
        lambda msg, instruct, mem, models, user_message,
               **kwargs: handle_with_memory(
            msg,
            user_message,
            files_status=kwargs.get(‘files_status’),
            instruct=instruct,
            mem=memory_enabled,  # Replace user_memory with memory_enabled
            models=models
        )
    )
)
</code></pre>
    <div><p class="normal">Note that we have replaced <code class="inlineCode">user_memory</code> with <code class="inlineCode">memory_enabled</code> to generalize memory management. </p>
    </div>
    <p class="normal">You can add as<a id="_idIndexMarker508"/> many handlers and AI functions as you <a id="_idIndexMarker509"/>wish to the handler registry. You can scale your GenAISys as much as you need to. You can also modify the keywords by replacing them with explicit instructions, as we did for the <code class="inlineCode">Analysis</code> and <code class="inlineCode">generation</code> functions. The handlers will then call all the AI functions you need.</p>
    <p class="normal">Let’s now go through the new organization of the AI functions.</p>
    <h1 class="heading-1" id="_idParaDest-205"><a id="_idTextAnchor213"/>4. AI functions</h1>
    <p class="normal">We will now <a id="_idIndexMarker510"/>run the AI functions that are activated by the handler registry. The functions build on those from earlier chapters but are now managed by the handler-selection mechanism introduced in this chapter. Additionally, the examples used in this section are based on typical prompts related to product design and production scenarios. Keep in mind that, due to the stochastic (probabilistic) nature of generative AI models, outputs can vary each time we run these tasks.</p>
    <figure class="mediaobject"><img alt="Figure 7.9: AI functions call by the handler selection mechanism and registry" src="img/B32304_07_9.png"/></figure>
    <p class="packt_figref">Figure 7.9: AI functions call by the handler selection mechanism and registry</p>
    <p class="normal">We’ll now execute all AI functions currently available in our GenAISys, incorporating DeepSeek model calls where applicable. Let’s begin <a id="_idIndexMarker511"/>with the RAG functions.</p>
    <div><p class="normal">Functions such as speech synthesis, file management, dialogue history, and summary generation remain unchanged from previous chapters. </p>
    </div>
    <h2 class="heading-2" id="_idParaDest-206"><a id="_idTextAnchor214"/>RAG</h2>
    <p class="normal">This RAG function<a id="_idIndexMarker512"/> can<a id="_idIndexMarker513"/> run with OpenAI or DeepSeek with the <code class="inlineCode">Pinecone</code> keyword in the user message. The RAG function’s name has changed, but its process remains unchanged for the query:</p>
    <pre class="programlisting code"><code class="hljs-code"># Define Handler Functions
def handle_pinecone_rag(user_message, **kwargs):
    if "Pinecone" in user_message:
      namespace = "genaisys"
    if "RAG" in user_message:
      namespace = "data01"
    print(namespace)
    query_text = user_message
    query_results = get_query_results(query_text, namespace)
    print("Processed query results:")
    qtext, target_id = display_results(query_results)
    print(qtext)
    # Run task
    sc_input = qtext + " " + user_message
</code></pre>
    <p class="normal">However, the function now contains a DeepSeek distilled R1 call. The function first defaults to OpenAI if no model is provided or if DeepSeek is deactivated:</p>
    <pre class="programlisting code"><code class="hljs-code">    models = kwargs.get("models", "OpenAI")  # Default to OpenAI if not provided
    if models == "DeepSeek" and deepseek==False:
       models="OpenAI"
    if models == "OpenAI":
      task_response = reason.make_openai_api_call(
      sc_input, "system","You are an assistant who executes the tasks you are asked to do.", "user")
</code></pre>
    <p class="normal">If DeepSeek is activated, it will be called if chosen in the IPython interface for this task:</p>
    <pre class="programlisting code"><code class="hljs-code">    if models == "DeepSeek":
      …
    return f"{namespace}:{models}: {task_response}"
</code></pre>
    <p class="normal">We will first run a sentiment analysis.</p>
    <h3 class="heading-3" id="_idParaDest-207"><a id="_idTextAnchor215"/>Sentiment analysis (genaisys)</h3>
    <p class="normal">An <a id="_idIndexMarker514"/>example <a id="_idIndexMarker515"/>user input by the PDPM is the following:</p>
    <pre class="programlisting code"><code class="hljs-code">Let's see what Pinecone thinks about this sentence: The customer did not like the design of the traveling bag we gave out because it was not the right color requested.
</code></pre>
    <p class="normal">The output with OpenAI selected (default) and <code class="inlineCode">Agent</code> checked will be as follows:</p>
    <pre class="programlisting con"><code class="hljs-con">Agent: genaisys:OpenAI: Sentiment analysis score: 0.3
Explanation: The sentiment of the text is predominantly negative. The customer expresses dissatisfaction with the traveling bag due to it not being the right color as requested. The use of phrases like "did not like" and "not the right color" indicates a negative sentiment, as the customer's expectations were not met. The score of 0.3 reflects this negative sentiment, though it is not extremely low because the issue is specific to the color and not the overall quality or functionality of the bag.
</code></pre>
    <p class="normal">On the other hand, the output with DeepSeek selected (default) and <code class="inlineCode">Agent</code> checked will be as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><strong class="hljs-con-slc">Agent:</strong> genaisys:DeepSeek:
…The numerical value should be between 0 and 1, so let's say 0.2….
Wait, but the user didn't specify a particular method or model for sentiment analysis. Maybe they want a simple rule-based approach? Let's consider the presence of negative words like "did not like" and "not the right color requested." These words indicate dissatisfaction. However, the sentiment isn't extremely negative since it's just one aspect (color) that's the issue, not the entire product or experience. So, a score of 0.2 seems appropriate because it reflects a moderate negative sentiment based on the specific criticism given.
</code></pre>
    <p class="normal">The sentiment score and explanation score are acceptable in both cases. Imagine receiving thousands of such customer feedback messages—the GenAISys filters the low scores and provides these outputs automatically, storing them in the customer database.</p>
    <p class="normal">Now, the PDPM checks semantic analysis.</p>
    <h3 class="heading-3" id="_idParaDest-208"><a id="_idTextAnchor216"/>Semantic analysis (genaisys)</h3>
    <p class="normal">Consider <a id="_idIndexMarker516"/>another <a id="_idIndexMarker517"/>example input by the PDPM:</p>
    <pre class="programlisting code"><code class="hljs-code">Let's see what meaning analysis Pinecone on this comment: The customer did not accept the traveling bag we gave out for free because the size and colors did not match the order processed.
</code></pre>
    <p class="normal">This RAG function can run with OpenAI or DeepSeek with a “Pinecone” keyword in the user message.</p>
    <p class="normal">OpenAI’s output is acceptable and clearly outlines the semantic relationships within the message:</p>
    <pre class="programlisting con"><code class="hljs-con"><strong class="hljs-con-slc">Agent:</strong> genaisys:OpenAI: Semantic analysis involves understanding the meaning and interpretation of words, phrases, and sentences in context. It is a crucial aspect of natural language processing (NLP) that helps in extracting meaningful information from text. In the context of your comment, semantic analysis would involve breaking down the sentence to understand the underlying meaning and the relationships between different components of the sentence.
Here's a breakdown of the semantic elements in the comment:
1. <strong class="hljs-con-slc">Subject</strong>: "The customer" - This refers to the individual who is the focus of the action.
2. <strong class="hljs-con-slc">Action</strong>: "did not accept" - This indicates the customer's refusal or rejection of something.
3. <strong class="hljs-con-slc">Object</strong>: "the traveling bag we gave out for free" - This is the item that was offered to the customer without charge.
4. <strong class="hljs-con-slc">Reason</strong>: "because the size and colors did not match the order processed" - This provides the explanation for the customer's refusal, indicating a mismatch between what was expected (the order) and what was received (the traveling bag).
…
</code></pre>
    <p class="normal">DeepSeek’s output is relevant as well:</p>
    <pre class="programlisting con"><code class="hljs-con"><strong class="hljs-con-slc">Agent:</strong> genaisys:DeepSeek: 300,Semantic analysis…This is not an analysis but a semantic search. Provide more information on the topic. …
The original problem was that the customer didn't accept the travel bag due to mismatched size and color from the ordered one. So, my goal here should be to understand why this happened and perhaps suggest solutions.
I think the first step would be to look into whether there was a miscommunication in the ordering process. Maybe the website had incorrect images or descriptions leading to wrong expectations. Or perhaps during manufacturing, sizes got altered without notice. Another angle could be customer feedback—have others reported similar issues? It might also help to check if there were any system errors in processing orders which caused discrepancies between what was ordered and what was delivered.
…
# Final Answer
After conducting thorough research and identifying potential contributing factors, effective solutions have been implemented to address the discrepancy issues, resulting in improved satisfaction among customers receiving accurate products aligned with their expectations.
</code></pre>
    <p class="normal">DeepSeek’s <a id="_idIndexMarker518"/>answer is longer and more complex. However, what would a team prefer? A shorter answer <a id="_idIndexMarker519"/>like OpenAI’s response or a longer one with more explanations? The decision can be reached through workshops and meetings.</p>
    <h3 class="heading-3" id="_idParaDest-209"><a id="_idTextAnchor217"/>Data retrieval (data01)</h3>
    <p class="normal">Both OpenAI<a id="_idIndexMarker520"/> and DeepSeek can be used for data retrieval. The <a id="_idIndexMarker521"/>user input for a product designer could be as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">I'm a product designer and am trying to analyze human memory types. Use RAG to tell us what the CTO said about different memory types
</code></pre>
    <p class="normal">The output is satisfactory:</p>
    <pre class="programlisting con"><code class="hljs-con"><strong class="hljs-con-slc">Agent:</strong> data01:OpenAI: To analyze the CTO's statement about different memory types using Retrieval-Augmented Generation (RAG), we can break down the information into the relevant memory types and their application in the hybrid travel marketing campaign:
1. <strong class="hljs-con-slc">Memoryless</strong>: This refers to systems or processes that do not retain any information from past interactions. In the context of your campaign, this might relate to interactions that do not require historical data, such as one-time queries or transactions…
</code></pre>
    <p class="normal">We thus have a flexible RAG system in our GenAISys that can run with the models we wish. However, we still have to evaluate the models for each set of tasks we want to perform.</p>
    <h2 class="heading-2" id="_idParaDest-210"><a id="_idTextAnchor218"/>Chain of thought</h2>
    <p class="normal">The CoT function<a id="_idIndexMarker522"/> operates with <strong class="screenText">Files</strong> checked and defaults to <strong class="screenText">OpenAI</strong> as the model provider. Its implementation remains consistent as it is built and run in the previous chapter. The key difference is that it is now integrated into the handler selection mechanism, which activates based on specific keywords in the input:</p>
    <pre class="programlisting code"><code class="hljs-code">def handle_reasoning_customer(user_message, **kwargs):
    initial_query = user_message
    download("Chapter05", "customer_activities.csv")
    reasoning_steps = reason.chain_of_thought_reasoning(initial_query)
    return reasoning_steps
</code></pre>
    <p class="normal">Consider an<strong class="keyWord"> </strong>example user input from the PDPM:</p>
    <pre class="programlisting code"><code class="hljs-code">Use reasoning to analyze customer activities so I can design custom travel merchandise kits.
</code></pre>
    <p class="normal">The<a id="_idIndexMarker523"/> output seems acceptable:</p>
    <pre class="programlisting con"><code class="hljs-con"><strong class="hljs-con-slc">Agent:</strong> data01:OpenAI: To analyze the CTO's statement about different memory types using Retrieval-Augmented Generation (RAG), we can break down the memory types mentioned and their relevance to the hybrid travel marketing campaign:
1. <strong class="hljs-con-slc">Memoryless</strong>: This refers to systems or processes that do not retain any information from past interactions. In the context of a marketing campaign, this would mean treating each customer interaction as a standalone event without any historical context….
</code></pre>
    <p class="normal">Let’s now see how memory analysis will run with both model sources (OpenAI and DeepSeek).</p>
    <h2 class="heading-2" id="_idParaDest-211"><a id="_idTextAnchor219"/>Analysis (memory)</h2>
    <p class="normal">Both <a id="_idIndexMarker524"/>OpenAI and DeepSeek models handle memory-based customer profiles using neuroscientific-style categorizations. The function has been adapted to the handler selection process and contains a DeepSeek call:</p>
    <pre class="programlisting code"><code class="hljs-code">def handle_analysis(user_message, **kwargs):
    from cot_messages_c6 import system_message_s1
    models = kwargs.get("models", "OpenAI")  # Default to OpenAI if not provided
    if models == "DeepSeek" and deepseek==False:
      models="OpenAI"
    if models == "OpenAI":
      reasoning_steps = reason.make_openai_reasoning_call(
        user_message, system_message_s1)
    if models == "DeepSeek":
   …
    return reasoning_steps
</code></pre>
    <p class="normal">An example user input using the <code class="inlineCode">Analysis</code> option in the <code class="inlineCode">Reasoning</code> list could be:</p>
    <pre class="programlisting code"><code class="hljs-code">The hotel was great because the view reminded me of when I came here to Denver with my parents. I sure would have like to have received a custom T-shirt as a souvenir. Anyway, it was warm and sunny so we had an amazing time.
</code></pre>
    <p class="normal">OpenAI’s output contains a useful segment highlighting the emotional dimension related to the customer’s wish for a personalized souvenir, which could help the product designer with their merchandise<a id="_idIndexMarker525"/> kit production endeavor:</p>
    <pre class="programlisting con"><code class="hljs-con">…
Segment 2: "I sure would have like to have received a custom T-shirt as a souvenir."
- Memory Tags: [Episodic Memory]
- Dimension: Emotional
- Sentiment Score: 0.4
- Explanation: Here the speaker expresses a personal wish or regret about a missing souvenir from the event. Although it doesn't recount an actual episode in detail, it still connects to the personal event and reflects a feeling of slight disappointment, thereby engaging episodic memory and an emotional dimension with a modestly negative sentiment.
</code></pre>
    <p class="normal">DeepSeek’s output, however, goes off track. It first finds the right task to do:</p>
    <pre class="programlisting con"><code class="hljs-con">Okay let's see this through step by step now...
Alright, I need to tackle analyzing segments from the given user response according to their detailed tagging system based on cognitive psychology principles regarding STM vs LTM, semantic versus episodic memory, reality vs fiction, among others plus dimensions such as Intellectual, Emotional, Physical, along with assigning sentiments scores ranging from 0-1 reflecting positivity.
Let me start reading carefully paragraph-wise.
First sentence:"The hotel wasgreatbecauseviewremindedmeofwhencyamehere todallas."
</code></pre>
    <p class="normal">But it then gets lost and seems to struggle with formatting and coherence, introducing irregular spacing and even foreign characters:</p>
    <pre class="programlisting con"><code class="hljs-con">…Butwait,theuser later talks about souvenirs wantingcustomTshirtswhichmaybe indicatespositiveintent。…
但此处更多的是体验性的(Eating和Enjoying)=所以可能既有知识元素也有身体维度的食物味道。但主要在这里是描述经历，因此属于Episode或语义吗？…
</code></pre>
    <p class="normal">DeepSeek can certainly do better, but improving this result would require additional iterations of prompt refinement or selecting a more robust DeepSeek variant or API. Investing time in refining prompts carries some risk, as even then, the outcome may not meet your expectations. Whether to refine the prompt, switch to a DeepSeek API, explore another DeepSeek variant, or default to OpenAI should ultimately be decided collaboratively within the team and based <a id="_idIndexMarker526"/>on your project’s needs.</p>
    <p class="normal">Let’s now move on to running the generation function.</p>
    <h2 class="heading-2" id="_idParaDest-212"><a id="_idTextAnchor220"/>Generation</h2>
    <p class="normal">The <a id="_idIndexMarker527"/>generation function (select <code class="inlineCode">Generation</code> in the <code class="inlineCode">Reasoning</code> list), active by default with <strong class="screenText">OpenAI</strong>, <strong class="screenText">Agent</strong>, and <strong class="screenText">Files</strong> checked, supports the creation of engaging, memory-based customer messages:</p>
    <pre class="programlisting code"><code class="hljs-code">def handle_generation(user_message, **kwargs):
    from cot_messages_c6 import (
    system_message_s1, generation, imcontent4, imcontent4b
)
    reasoning_steps = reason.memory_reasoning_thread(
        user_message, system_message_s1, generation, 
        imcontent4, imcontent4b
    )
    return reasoning_steps
</code></pre>
    <p class="normal">Let’s consider a general user input as an example:</p>
    <pre class="programlisting code"><code class="hljs-code">The hotel was great because the view reminded me of when I came here to Denver with my parents. I sure would have like to have received a custom T-shirt as a souvenir. Anyway, it was warm and sunny so we had an amazing time.
</code></pre>
    <p class="normal">OpenAI’s output is an appealing customer-facing message, blending nostalgia and merchandising suggestions, accompanied by an appropriate custom T-shirt image:</p>
    <pre class="programlisting code"><code class="hljs-code">Customer message: Dear Customer,
Experience the charm of Denver with a nostalgic hotel view and enjoy the sunny weather. Explore the beautiful Denver Botanic Gardens and the iconic Red Rocks Amphitheatre. Don't miss out on exclusive souvenirs from local artists and a personalized T-shirt to remember your trip.
Best regards,
</code></pre>
    <figure class="mediaobject"><img alt="Figure 7.10: A personal image for a customer" src="img/B32304_07_10.png"/></figure>
    <p class="packt_figref">Figure 7.10: A personal image for a customer</p>
    <p class="normal">Let’s now<a id="_idIndexMarker528"/> create an image.</p>
    <h2 class="heading-2" id="_idParaDest-213"><a id="_idTextAnchor221"/>Creating an image</h2>
    <p class="normal">This<a id="_idIndexMarker529"/> functionality utilizes DALL-E to generate images, with the <strong class="screenText">Files</strong> box checked. The function does not change beyond being adapted to the handler-selection mechanism, which activates this feature with the <code class="inlineCode">Create</code> and <code class="inlineCode">image</code> keywords in the user input:</p>
    <pre class="programlisting code"><code class="hljs-code">def handle_image_creation(user_message, **kwargs):
    prompt = user_message
    image_url = reason.generate_image(
        prompt, model="dall-e-3", size="1024x1024", 
        quality="standard", n=1
    )
    # Save the image locally
    save_path = "c_image.png"
    image_data = requests.get(image_url).content
    with open(save_path, "wb") as file:
        file.write(image_data)
    return "Image created"
</code></pre>
    <p class="normal">The product designer could use it to ideate merchandising kits:</p>
    <pre class="programlisting code"><code class="hljs-code">Create an image:  Create an image of a custom T-shirt with surfing in Hawaii on big waves on it to look cool.
</code></pre>
    <p class="normal">The output is a cool T-shirt that the production team could use and adapt for production:</p>
    <figure class="mediaobject"><img alt="Figure 7.11: Custom T-shirt design" src="img/B32304_07_11.png"/></figure>
    <p class="packt_figref">Figure 7.11: Custom T-shirt design</p>
    <p class="normal">We will now create<a id="_idIndexMarker530"/> freestyle prompts that are not triggered by any keywords or instructions.</p>
    <h2 class="heading-2" id="_idParaDest-214"><a id="_idTextAnchor222"/>Fallback handler (memory-based)</h2>
    <p class="normal">This <a id="_idIndexMarker531"/>general-purpose handler activates when no specific instruction or keyword matches the input. <code class="inlineCode">handle_with_memory</code> runs with OpenAI and DeepSeek, depending on the model selected. The memory of a user dialogue is set with a global variable, <code class="inlineCode">memory_enabled</code>, that is initialized at the beginning:</p>
    <pre class="programlisting code"><code class="hljs-code"># Global variable to ensure memory is always used
memory_enabled = True  # Set to True to retain conversation memory
def handle_with_memory(messages, user_message, **kwargs):
    global memory_enabled  # Ensure global memory setting is used
</code></pre>
    <p class="normal">The function will return a message and stop if <code class="inlineCode">memory_enabled</code> is set to <code class="inlineCode">False</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">    # If memory is disabled, respond with a message
    if not memory_enabled:
        return "Memory is disabled."
</code></pre>
    <p class="normal">It will process the past messages of a user from the conversation history:</p>
    <pre class="programlisting code"><code class="hljs-code">    # Extract all past messages (user + assistant) from the conversation history
    conversation_history = [
        f"{msg['role'].capitalize()}: {msg['content']}"
        for msg in messages if "content" in msg
    ]
    # Combine all conversation history
    combined_history = "\n".join(conversation_history)
    # Append the latest user message to the history
    full_context = f"{combined_history}\nUser: {user_message}"
</code></pre>
    <p class="normal">Then, the models are selected with OpenAI being the default model provider if no other model is selected in the IPython interface:</p>
    <pre class="programlisting code"><code class="hljs-code">    models = kwargs.get("models", "OpenAI")  # Default to OpenAI if not provided
    if models == "OpenAI":
        task_response = reason.make_openai_api_call(
            full_context, "system",
            "You are an assistant who executes the tasks you are asked to do.", "user"
        )
    elif models == "DeepSeek":
        # Tokenize the full conversation history for DeepSeek
        …
        # Decode the output
        task_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
</code></pre>
    <p class="normal">The response message is stored and returned:</p>
    <pre class="programlisting code"><code class="hljs-code">    # Store bot response in memory
    messages.append({"role": "assistant", "content": task_response})
    return task_response
</code></pre>
    <p class="normal">An example input by the PDPM could be the following:</p>
    <pre class="programlisting code"><code class="hljs-code">We are new to production processes and experience issues with the production of a customized travel bag by Supplier S100 for a trip to Hawaii that should have an image of Hawaii on it with the name of the customer. The production process contained operations 10,20,30,40,50 and 60. Operation 40 is the operation that prints the image and the name of the customer on the bag. Supplier S100 lacks the special ink to perform the operation. SupplierS200 has the ink but the production of operations 10 to 30 have already been performed. Instructions: think this problem through and provide real-world production ideas to solve this issue.
</code></pre>
    <p class="normal">OpenAI’s answer<a id="_idIndexMarker532"/> is both acceptable and productive. Take your time to read the prompt and the response, which shows the transition of generative AI from NLP general tasks to zero-shot domain-specific tasks:</p>
    <pre class="programlisting con"><code class="hljs-con"><strong class="hljs-con-slc">Agent:</strong> To address the issue of Supplier S100 lacking the special ink needed for operation 40, while Supplier S200 has the ink, consider the following real-world production solutions:
1. <strong class="hljs-con-slc">Outsource Operation 40 to Supplier S200</strong>:
- Coordinate with Supplier S200 to perform operation 40. This involves transporting the semi-finished bags (after operations 10 to 30) to Supplier S200 for printing.
- Ensure that Supplier S200 is capable of handling the volume and has the necessary equipment to perform the operation to your quality standards.
2. <strong class="hljs-con-slc">Ink Transfer</strong>:
- If feasible, arrange for Supplier S200 to supply the special ink to Supplier S100. This would allow Supplier S100 to continue with operation 40 without needing to transport the bags.
- Ensure that Supplier S100 has the necessary equipment and expertise to use the ink provided by Supplier S200.
3. <strong class="hljs-con-slc">Collaborative Production</strong>:
- Establish a partnership between Supplier S100 and Supplier S200 where both suppliers work together to complete the production process. This could involve sharing resources, expertise, and logistics.
4. <strong class="hljs-con-slc">Logistics and Scheduling</strong>:
- Plan the logistics for transporting the bags between suppliers efficiently to minimize delays.
- Adjust the production schedule to accommodate the additional time required for transportation and coordination between suppliers.
5. <strong class="hljs-con-slc">Quality Control</strong>:
- Implement a quality control process to ensure that the printing meets the required standards, regardless of which supplier performs the operation.
- Conduct inspections before and after operation 40 to maintain quality consistency.
6. <strong class="hljs-con-slc">Communication and Coordination</strong>:
- Maintain clear communication with both suppliers to ensure that all parties are aligned on timelines, quality expectations, and responsibilities.
- Use project management tools to track progress and address any issues promptly.
7. <strong class="hljs-con-slc">Contingency Planning</strong>:
- Develop a contingency plan in case of further supply chain disruptions, such as identifying alternative suppliers or inks that could be used in the future.
</code></pre>
    <p class="normal">DeepSeek’s answer does not provide the same quality, although it contains some interesting points:</p>
    <pre class="programlisting con"><code class="hljs-con">C.), Utilize Alternate Suppliers With Intermediaries Who Could Possibly Acquire Required Inks From Other Sources Perhaps Via Importing Directly Under Different Terms Agreements,
D ). Leverage Existing Inventory Wherever Feasible Reallocating Stockpiles Smartly Across Production Lines Maybe Even Repurposing Materials When Not Used For Primary Function Into Secondary Applications Which May Free Up Costs And Resources While Mitigating Penalties Imposed By Delays,
</code></pre>
    <p class="normal">Obtaining a <a id="_idIndexMarker533"/>better result would require further prompt design and output analysis cycles, evaluating DeepSeek models that are not distilled, such as DeepSeek-V3 or DeepSeek-R1. DeepSeek can surely do better, as demonstrated by using DeepSeek-R1 on <a href="https://chat.deepseek.com/">https://chat.deepseek.com/</a>, which produced the following output:</p>
    <pre class="programlisting con"><code class="hljs-con">To resolve the production issue with the customized travel bags, consider the following structured solutions:
### 1. **Outsource Operation 40 to Supplier S200** 
   - **Process**: 
     - S100 ships semi-finished bags (post-operation 30) to S200. 
     - S200 performs operation 40 (printing) using their special ink. 
     - Bags are returned to S100 to complete operations 50 and 60. 
   - **Key Considerations**: 
     - Use expedited shipping (e.g., air freight) to minimize delays. 
     - Ensure packaging protects semi-finished goods during transit. 
     - Negotiate cost-sharing for logistics between S100 and S200. 
### 2. **Expedite Ink Procurement for S100** 
   - **Action**: 
     - Source the ink directly from S200 or an alternative supplier. 
     - Prioritize urgent delivery (e.g., same-day courier). 
   - **Feasibility Check**: 
     - Verify if the ink is proprietary or available on the open market. 
     - Address contractual barriers if S200 is a competitor. 
### 3. **Supplier Collaboration for End-to-End Production** 
   - **Option**: 
     - S200 takes over all remaining steps (40, 50, 60) after S100 completes operations 10–30. 
   - **Requirements**: 
     - Confirm S200's capability to perform operations 50 and 60. 
     - Align quality standards between suppliers to avoid inconsistencies. 
### 4. **Process Re-engineering** 
   - **Temporary Workaround**: 
     - Explore alternative printing methods (e.g., heat transfer, patches) if customer specs allow. 
     - Assess feasibility of resequencing operations (e.g., printing later), though this is unlikely. 
### 5. **Proactive Communication and Risk Mitigation** 
   - **Customer Transparency**: 
     - Inform the customer of potential delays and offer solutions (e.g., partial deliveries, discounts). 
   - **Prevent Future Issues**: 
     - Implement dual sourcing for critical materials (e.g., ink). 
     - Use inventory management tools to track material availability in real-time. 
### **Cost vs. Speed Tradeoff Analysis** 
- **Outsourcing to S200**: Higher logistics costs but faster resolution if suppliers are geographically close. 
- **Expedited Ink**: Higher material costs but avoids production handoffs. 
### **Recommended Approach** 
1. **Prioritize Outsourcing Operation 40**: 
   - This balances speed and feasibility if S200 can print without disrupting the workflow. 
2. **Backup Plan**: 
   - If logistics are too slow, expedite ink procurement while negotiating with S200 for a bulk discount. 
By combining supplier collaboration, agile logistics, and proactive planning, the production delay can be minimized while maintaining quality and customer trust.
</code></pre>
    <p class="normal">Read the <a id="_idIndexMarker534"/>prompts and outputs in this section carefully. For security and privacy reasons, we are using only a locally installed, distilled Hugging Face open source version of DeepSeek-R1. However, you could use the online version of DeepSeek for certain tasks, such as the production example in this section, if you have the necessary permissions, just as you would with ChatGPT or any other online platform. Depending on your project’s specifications, you could also explore US-based DeepSeek APIs or alternative deployment approaches.</p>
    <p class="normal">In any case, both OpenAI and DeepSeek are capable of delivering effective production solutions when correctly prompted and when the most appropriate model version is selected. Generative AI has clearly entered a new phase!</p>
    <h1 class="heading-1" id="_idParaDest-215"><a id="_idTextAnchor223"/>Summary</h1>
    <p class="normal">In this chapter, we moved further along our journey into generative AI systems. First, we took the time to digest the arrival of DeepSeek-R1, a powerful open source reasoning model known for innovative efficiency improvements in training. This development immediately raised a critical question for project managers: should we constantly follow real-time trends or prioritize maintaining a stable system?</p>
    <p class="normal">To address this challenge, we developed a balanced solution by building a handler selection mechanism. This mechanism processes user messages, triggers handlers within a handler registry, and then activates the appropriate AI functions. To ensure flexibility and adaptability, we updated our IPython interface, allowing users to easily select between OpenAI and DeepSeek models before initiating a task.</p>
    <p class="normal">This design allows the GenAISys administrator to introduce new experimental models or any other function(non-AI, ML, or DL) while maintaining access to proven results. For instance, when analyzing user comments, administrators can run tasks using the reliable OpenAI model while simultaneously evaluating the DeepSeek model. Administrators can also disable specific models when necessary, providing a practical balance between stability and innovation, which is crucial in today’s fast-paced AI environment.</p>
    <p class="normal">To achieve this balance practically, we began by installing and running DeepSeek-R1-Distill-Llama-8B in an independent notebook, demonstrating its capabilities through production-related examples. We then integrated this distilled model into our GenAISys, creating a need for enhanced flexibility and scalability.</p>
    <p class="normal">The introduction of the handler selection mechanism and the structured handler registry ensures that our system can scale effectively and indefinitely. Each handler follows a unified, modular format, enabling easy management, activation, or deactivation by administrators. We demonstrated these handlers through a series of practical prompts related to product design and production.</p>
    <p class="normal">We are now positioned to expand and scale our GenAISys, adding new features within this adaptable framework. In the next chapter, we’ll continue this journey by connecting our GenAISys to the broader external world.</p>
    <h1 class="heading-1" id="_idParaDest-216"><a id="_idTextAnchor224"/>Questions</h1>
    <ol>
      <li class="numberedList" value="1">DeepSeek-V3 was trained with zero-shot examples. (True or False)</li>
      <li class="numberedList">DeepSeek-R1 is a reasoning model. (True or False)</li>
      <li class="numberedList">DeepSeek-R1 was first trained with RL-only. (True or False)</li>
      <li class="numberedList">DeepSeek-R1-Distill-Llama-8B is the teacher of DeepSeek-R1. (True or False)</li>
      <li class="numberedList">DeepSeek-V3 was enhanced with DeepSeek-R1, which was derived from DeepSeek. (True or False)</li>
      <li class="numberedList">A handler registry that contains a list of handlers for all the AI functions is scalable. (True or False)</li>
      <li class="numberedList">A handler selection mechanism that processes user messages makes the GenAISys highly flexible. (True or False)</li>
      <li class="numberedList">Generative AI models such as OpenAI and DeepSeek reasoning models solve a wide range of problems with no additional training. (True or False)</li>
      <li class="numberedList">A GenAISys with a solid architecture is sufficiently flexible to be expanded in terms of models and tasks to perform. (True or False)</li>
    </ol>
    <h1 class="heading-1" id="_idParaDest-217"><a id="_idTextAnchor225"/>References</h1>
    <ul>
      <li class="bulletList"><em class="italic">DeepSeek-V3 Technical Report</em>. <a href="https://arxiv.org/abs/2412.19437">https://arxiv.org/abs/2412.19437</a></li>
      <li class="bulletList">Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., &amp; Polosukhin, I. (2017). <em class="italic">Attention Is All You Need</em>. Advances in Neural Information Processing Systems, 30, 5998–6008. Available at: <a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a></li>
      <li class="bulletList">DeepSeekAI, Daya Guo, Dejian Yang, et al: <a href="https://arxiv.org/abs/2501.12948">https://arxiv.org/abs/2501.12948</a></li>
      <li class="bulletList">Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., &amp; Lample, G. (2023). <em class="italic">LLaMA: Open and Efficient Foundation Language Models</em>. <a href="https://arxiv.org/abs/2302.13971">https://arxiv.org/abs/2302.13971</a>)</li>
      <li class="bulletList">Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Vaughan, et al. (2024). <em class="italic">The Llama 3 Herd of Models</em>. <a href="https://arxiv.org/abs/2407.21783">https://arxiv.org/abs/2407.21783</a></li>
      <li class="bulletList">Hugging Face:<ul>
          <li class="bulletList"><a href="https://huggingface.co/docs/transformers/index">https://huggingface.co/docs/transformers/index</a></li>
          <li class="bulletList"><a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B">https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B</a></li>
          <li class="bulletList"><a href="https://huggingface.co/docs/inference-endpoints/en/security">https://huggingface.co/docs/inference-endpoints/en/security</a></li>
        </ul>
      </li>
      <li class="bulletList">Unsloth AI: <a href="https://unsloth.ai/">https://unsloth.ai/</a></li>
    </ul>
    <h1 class="heading-1" id="_idParaDest-218"><a id="_idTextAnchor226"/>Further reading</h1>
    <ul>
      <li class="bulletList">Frantar, E., Ashkboos, S., Hoefler, T., &amp; Alistarh, D. (2023). <em class="italic">GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers</em> <a href="https://arxiv.org/abs/2210.01774">https://arxiv.org/abs/2210.01774</a>)</li>
      <li class="bulletList">NVIDIA’s data center GPUs: <a href="https://www.nvidia.com/en-us/data-center/">https://www.nvidia.com/en-us/data-center/</a></li>
    </ul>
    <div><table class="table-container" id="table001-4">
        <tbody>
          <tr>
            <td class="table-cell">
              <h4 class="heading-4">Unlock this book’s exclusive benefits now</h4>
              <p class="normal">Scan this QR code or go to <a href="http://packtpub.com/unlock">packtpub.com/unlock</a>, then search for this book by name.</p>
            </td>
            <td class="table-cell" rowspan="2">
              <figure class="mediaobject"><img alt="A qr code on a white background  AI-generated content may be incorrect." src="img/Unlock.png"/></figure>
            </td>
          </tr>
          <tr>
            <td class="table-cell">
              <p class="normal"><em class="italic">Note: Keep your purchase invoice ready before you start.</em></p>
            </td>
          </tr>
        </tbody>
      </table>
    </div>
  </div>
</body></html>