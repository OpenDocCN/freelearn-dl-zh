- en: Monte Carlo Methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this chapter, we will jump back to the trial-and-error thread of **reinforcement
    learning** (**RL**) and look at Monte Carlo methods. This is a class of methods
    that works by episodically playing through an environment instead of planning.
    We will see how this improves our RL search for the best policy and we now start
    to think of our algorithm as an actual agent—one that explores the game environment
    rather than preplans a policy, which, in turn, allows us to understand the benefits
    of using a model for planning or not. From there, we will look at the Monte Carlo
    method and how to implement it in code. Then, we will revisit a larger version
    of the FrozenLake environment with our new Monte Carlo agent algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will continue looking at how RL has evolved and, in particular,
    focus on the trial and error thread with the Monte Carlo method. Here is a summary
    of the main topics we will cover in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding model-based and model-free learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing the Monte Carlo method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding RL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Playing the FrozenLake game
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using prediction and control
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We again explore more foundations of RL, variational inference, and the trial
    and error method. This knowledge will be essential for anyone who is serious about
    finishing this book, so please don't skip this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding model-based and model-free learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you recall from our very first chapter, [Chapter 1](5553d896-c079-4404-a41b-c25293c745bb.xhtml),
    *Understanding Rewards-Based Learning*, we explored the primary elements of RL.
    We learned that RL comprises of a policy, a value function, a reward function,
    and, optionally, a model. We use the word *model* in this context to refer to
    a detailed plan of the environment. Going back to the last chapter again, where
    we used the FrozenLake environment, we had a perfect model of that environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2963ac28-ffbe-4c12-b905-03f65c54ff7c.png)'
  prefs: []
  type: TYPE_IMG
- en: Model of the FrozenLake environment
  prefs: []
  type: TYPE_NORMAL
- en: Of course, looking at problems with a fully described model in a finite MDP
    is all well and good for learning. However, when it comes to the real world, having
    a full and completely understood model of any environment would likely be highly
    improbable, if not impossible. This is because there are far too many states to
    account for or model in any real-world problem. As it turns out, this can also
    be the case for many other models as well. Later in this book, we will look at
    environments with more states than the number of atoms in the known universe.
    We could never possibly model such environments. Hence, the planning methods we
    learned in [Chapter 2](8237fd36-1edf-4da0-b271-9a50c5b8deb3.xhtml), *Dynamic Programming
    and the Bellman Equation*, won't work. Instead, we need a method that can explore
    an environment and learn from it. This is where Monte Carlo comes in and is something
    we will cover in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the Monte Carlo method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Monte Carlo method was so named because of its similarity to gambling or
    chance. Hence, the method was named after the famous gambling destination at the
    time. While the method is extremely powerful, it has been used to describe the
    atom, quantum mechanics, and the quantity of [![](img/28309dc6-92e5-49ba-aea1-e9ad32bfce3c.png)]
    itself. It is only until fairly recently, within the last 20 years, that it has
    seen widespread acceptance in everything from engineering to financial analysis.
    The method itself has now become foundational to many aspects of machine learning
    and is worth further study for anyone in the AI field.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will see how the Monte Carlo method can be used to solve
    for [![](img/7b9758bc-0cbb-47ab-8289-66d8157600fb.png)].
  prefs: []
  type: TYPE_NORMAL
- en: Solving for
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The standard introduction to Monte Carlo methods is to show how it can be used
    to solve for ![](img/cd567828-140e-4cb5-95fd-7a08af97d028.png). Recall from geometry,
    [![](img/a35704d0-4695-4796-bd6a-a8693703de86.png)] represents half the circumference
    of a circle and 2π represents a full circle. To find this relationship and value,
    let''s consider a unit circle with a radius of 1 unit. That unit could be feet,
    meters, parsecs, or whatever—it''s not important. Then, if we place that circle
    within a square box with dimensions of 1 unit, we can see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f97cc8b3-a951-4ddf-85b6-b8bae03b57ad.png)'
  prefs: []
  type: TYPE_IMG
- en: A unit circle inside a unit square
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the preceding, we know that we have a square that encompasses dimensions
    of 2 units by 2 units or 100% of the area with an area of 4 square units. Going
    back to geometry again, we know that the area of a circle is given by [![](img/7a492136-0712-4f73-b12b-8c05460618aa.png)].
    Knowing that the circle is within the square and knowing the full area, we can
    then apply the Monte Carlo method to solve for the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8ae5a92d-fd87-4bdb-ba2f-5260483a1247.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The Monte Carlo method works by randomly sampling an area and then determining
    what percentage of that sample is correct or incorrect. Going back to our example,
    we can think of this as randomly dropping darts onto the square and then counting
    how many land within the circle. By counting the number of darts that land within
    the circle, we can then backcalculate a number for π using the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fde16558-a28b-439a-9af4-146fb64597ac.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding equation, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*ins*: The total number of darts or samples that fell within the circle'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*total*: The total number of darts dropped'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The important part about the preceding equation is to realize that all we are
    doing here is taking a percentage (*ins*/*total*) of how many darts fell within
    the circle to determine a value for π. This may still be a little unclear, so
    let's look at a couple of examples in the next sections.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Monte Carlo
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In many cases, even understanding simple concepts that are abstract can be difficult
    without real-world examples. Therefore, open up the `Chapter_3_1.py` code sample.
  prefs: []
  type: TYPE_NORMAL
- en: We should mention before starting that ![](img/bc1fcbba-b85e-406a-bc3c-71556f0e3a90.png),
    in this case, refers to the actual value we estimate at 3.14.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the entire code listing for reference:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This code solves for ![](img/812e1640-0feb-46c5-a7eb-d34729e14040.png) using
    the Monte Carlo method, which is quite impressive when you consider how simple
    the code is. Let's go over each section of the code.
  prefs: []
  type: TYPE_NORMAL
- en: We start with the `import` statements, and here we just import `random` and
    the `math` function, `sqrt`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From there, we define a couple of variables, `ins` and `n`. The `ins` variable
    holds the number of times a dart or sample is inside the circle. The `n` variable
    represents how many iterations or darts to drop.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we randomly drop darts with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: All this code does is randomly sample values in the range of `-1` to `1` for
    `x` and `y` and then determine whether they are within a circle radius of `1`, which
    is given by the calculation within the square root function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, the last couple of lines do the calculation and output the result.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the example as you normally would and observe the output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What you will likely find is the guess may be a bit off. That all depends on
    the number of samples. You see, the confidence of the Monte Carlo method and therefore
    the quality of the answer goes up with the more samples you do. Hence, to improve
    the last example, you will have to increase the value of the variable, `n`.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we look at this example again but this time look at what
    those dart samples may actually look like in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Plotting the guesses
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you are still having problems grasping this concept, visualizing this example
    may be more helpful. Run the exercise in the next section if you want to visualize
    what this sampling looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before starting this exercise, we will install the `matplotlib` library. Install
    the library with `pip` using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'After the install, open up the `Chapter_3_2.py` code example shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The code is quite similar to the last exercise and should be fairly self-explanatory.
    We will just focus on the important sections of code highlighted in the preceding.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The big difference in this example is we remember where the darts are dropped
    and identify whether they fell inside or outside of the circle. After that, we
    plot the results. We plot a point for each and color them green for inside the
    circle and red for outside.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the sample and observe the output, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/112792e6-4d06-43b4-b15c-7806abcf1979.png)'
  prefs: []
  type: TYPE_IMG
- en: Example output from Chapter_3_2.py
  prefs: []
  type: TYPE_NORMAL
- en: The output looks like a circle, as much as we would expect it to. However, there
    is a problem with the output value of π. Notice how the estimated value of π is
    now quite low. This is because the value of *n—*the number of darts or samples—is
    only 1,000\. That means, for the Monte Carlo method to be a good estimator, we
    also need to realize it needs a sufficiently large number of guesses.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we look to see how we can apply this method to an expanded
    version of the FrozenLake problem with RL.
  prefs: []
  type: TYPE_NORMAL
- en: Adding RL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we understand the Monte Carlo method, we need to understand how to
    apply it to RL. Recall that our expectation now is that our environment is relatively
    unknown, that is, we do not have a model. Instead, we now need to develop an algorithm
    by which to explore the environment by trial and error. Then, we can take all
    of those various trials and, by using Monte Carlo, average them out and determine
    a best or better policy. We can then use that improved policy to continue exploring
    the environment for further improvements. Essentially, our algorithm becomes an
    explorer rather than a planner and this is why we now refer to it as an agent.
  prefs: []
  type: TYPE_NORMAL
- en: Using the term **agent** reminds us that our algorithm is now an explorer and
    learner. Hence, our agents not only explore but also learn from that exploration
    and improve on it. Now, this is real artificial intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: Aside from the exploration part, which we already visited earlier in [Chapter
    1](5553d896-c079-4404-a41b-c25293c745bb.xhtml), *Understanding Rewards-Based Learning*,
    the agent still needs to evaluate a value function and improve on a policy. Hence,
    much of what we covered in [Chapter 2](8237fd36-1edf-4da0-b271-9a50c5b8deb3.xhtml),
    *Dynamic Programming and the Bellman Equation*, will be applicable. However, this
    time, instead of planning, our agent will explore the environment and then, after
    each episode, re-evaluate the value function and update the policy. An episode
    is defined as one complete set of moves from the start to termination. We call
    this type of learning episodic since it refers to the agent only learning and
    improving after an episode. This, of course, has its limitations and we will see
    how continuous control is done in [Chapter 4](bb05e528-e21b-4753-9e4c-372b8ed11e96.xhtml),
    *Temporal Difference Learning*. In the next section, we jump in and look at the
    code and how this all works.
  prefs: []
  type: TYPE_NORMAL
- en: Monte Carlo control
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are two ways to implement what is called **Monte Carlo control** on an
    agent. The difference between the two is how they calculate the average return
    or sampled mean. In what is called **First-Visit Monte Carlo**, the agent only
    samples the mean the first time a state is visited. The other method, **Every-Visit
    Monte Carlo**, samples the average return every time a state is visited. The latter
    method is what we will explore in the code example for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The original source code for this example was from Ankit Choudhary's blog ([https://www.analyticsvidhya.com/blog/2018/11/reinforcement-learning-introduction-monte-carlo-learning-openai-gym/](https://www.analyticsvidhya.com/blog/2018/11/reinforcement-learning-introduction-monte-carlo-learning-openai-gym/)).
  prefs: []
  type: TYPE_NORMAL
- en: The code has been heavily modified from the original. Ankit goes far more heavily
    into the mathematics of this method and the original is recommended for those
    readers interested in exploring more math.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open up `Chapter_3_3.py` and follow the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Open the code and review the imports. The code for this example is too large
    to place inline. Instead, the code has been broken into sections.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Scroll to the bottom of the sample and review the following lines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In the first line, we construct the environment. Then, we create `policy` using
    a function called `monte_carlo_e_soft`. We complete this step by printing out
    the results from the `test_policy` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Scroll up to the `monte_carlo_e_soft` function. We will get to the name later
    but, for now, the top lines are shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'These lines create a policy if there is none. This shows how the random policy
    is created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, we create a dictionary to store state and action values, as shown
    here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we start with a `for` loop that iterates through the number of episodes,
    like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Change `display=False` as highlighted in the preceding to `display=True`, as
    shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, before we get too far ahead, it may be helpful to see how the agent is
    playing a game. Run the code example and watch the output. Don''t run the code
    until completion—just for a few seconds or up to a minute is fine. Make sure to
    undo your code changes before saving:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/55657369-f70f-447b-abe6-ceb363afb5cd.png)'
  prefs: []
  type: TYPE_IMG
- en: Example output of agent playing the game
  prefs: []
  type: TYPE_NORMAL
- en: This screenshot shows an example of the agent exploring the expanded 8 x 8 FrozenLake
    environment. In the next section, we look at how the agent plays the game.
  prefs: []
  type: TYPE_NORMAL
- en: Again, make sure you undo your code and change `display=True` to `display=False`
    before proceeding.
  prefs: []
  type: TYPE_NORMAL
- en: Playing the FrozenLake game
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The agent code now plays or explores the environment and it is helpful if we
    understand how this code runs. Open up `Chapter_3_3.py` again and follow the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'All we need to focus on for this section is how the agent plays the game. Scroll
    down to the `play_game` function, as shown in the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see the function takes the `env` and `policy` environment as inputs.
    Then, inside, it resets the environments with `reset` and then initializes the
    variables. The start of the `while` loop is where the agent begins playing the
    game:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: For this environment, we are letting the agent play infinitely. That is, we
    are not limiting the number of steps the agent may take. However, for this environment,
    that is not a problem since it is quite likely the agent will fall into a hole.
    But, that is not always the case and we often need to limit the number of steps
    and agent takes in an environment. In many cases, that limit is set at `100`,
    for example.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Inside the `while` loop, we update the agent''s state, `s`, then display the
    environment is `display=True`. After that, we set up a `timestep` list to hold
    that `state`, `action`, and `value`. Then, we append the state, `s`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we look at the code that does the random sampling of the action based
    on the `policy` values, as shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This is essentially where the agent performs a uniform sampling of the policy
    with `random.uniform`, which is the Monte Carlo method. Uniform means that sampling
    is uniform across values and not skewed if it were from a normal or Gaussian method.
    After that, an action is selected in the `for` loop based on a randomly selected
    item in the policy. Keep in mind that, at the start, all actions may have an equal
    likelihood of `0.25` but later, as the agent learns policy items, it will learn
    to distribute accordingly as well.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Monte Carlo methods use a variety of sampling distributions to determine randomness.
    So far, we have extensively used uniform distributions, but in most real-world
    environments, a normal or Gaussian sampling method is used.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, after a random action is chosen, the agent takes a step and records it.
    It already recorded `state` and it now appends `action` and `reward`. Then, it
    appends the `timestep` list to the `episode` list, as shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Finally, when the agent has `finished`, by finding the goal or dropping in a
    hole, it returns the list of steps in `episode`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, with our understanding of how the agent plays the game, we can move on
    evaluating the game and optimizing it for prediction and control.
  prefs: []
  type: TYPE_NORMAL
- en: Using prediction and control
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When we previously had a model, our algorithm could learn to plan and improve
    a policy offline. Now, with no model, our algorithm needs to become an agent and
    learn to explore and, while doing that, also learn and improve. This allows our
    agent to now learn effectively by trial and error. Let''s jump back into the `Chapter_3_3.py` code
    example and follow the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start right from where we left off and review the last couple of lines
    including the `play_game` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Inside `evaluate_policy_check`, we test to see whether the `test_policy_freq`
    number has been reached. If it has, we output the current progress of the agent.
    In reality, what we are evaluating is how well the current policy will run an
    agent. The `evaluate_policy_check` function calls `test_policy` to evaluate the
    current policy. The `test_policy` function is shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '`test_policy` evaluates the current policy by running the `play_game` function
    and setting a new agent loose for several games set by `r = 100`. This provides
    a `wins` percentage, which is output to show the agent''s progress.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Back to the main function, we step into a `for` loop that loops through the
    last episode of gameplay in reverse order, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Looping through the episode in reverse order allows us to use the last reward
    and apply it backward. Hence, if the agent received a negative reward, all actions
    would be affected negatively. The same is also true for a positive reward. We
    keep track of the total reward with the `G` variable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Inside the last loop, we then check whether the state was already evaluated
    for this episode; if not, we find the list of returns and average them. From the
    averages, we can then determine the best action, `A_star`. This is shown in the
    code block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: A lot is going on in this block of code, so work through it slowly if you need
    to. The key takeaway is that all we are doing here is averaging returns or a state
    and then determining the most likely best action, according to Monte Carlo, within
    that state.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Before we jump to the last section of code, run the example as you normally
    would. This should yield a similar output to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/0a04c1ab-7c86-4a9d-9627-4b24ad950da8.png)'
  prefs: []
  type: TYPE_IMG
- en: Example output from Chapter_3_3.py
  prefs: []
  type: TYPE_NORMAL
- en: Notice how we can now visualize the agent's progress as it randomly explores.
    The percentage of wins you may see could be entirely different and in some cases,
    they may be much higher or lower. This is because the agent is randomly exploring.
    To evaluate an agent entirely, you would likely need to run the agent for more
    than 50,000 episodes. However, continually averaging a mean after a new sample
    is added over 50,000 iterations would be far too computationally expensive. Instead,
    we use another method called an incremental mean, which we will explore in the
    next section.
  prefs: []
  type: TYPE_NORMAL
- en: Incremental means
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An incremental or running mean allows us to keep an average for a list of numbers
    without having to remember the list. This, of course, has huge benefits when we
    need to keep a mean over 50,000, 1 million, or more episodes. Instead of updating
    the mean from a full list, for every episode, we hold one value that we incrementally
    update using the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1b649433-ce9c-4696-b18d-907d8c43b717.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding equation, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](img/b53a38e6-94e1-43ce-97bb-f35bb4774602.png)] = The current state value
    for the policy'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/d5bc9aee-bd81-489b-8c1c-9a946f2bafb5.png)] = Represents a discount
    rate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/a74c2e17-9f50-4ee2-8ef8-a4cb4ea6b7c6.png)] = The current total return'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'By applying this equation, we now have a method to update the policy and, coincidentally,
    we use a similar method in the full Q equation. However, we are not there yet
    and, instead, we update the value using the following algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dcfa8499-fad1-4c12-986d-7243dc20928d.png)'
  prefs: []
  type: TYPE_IMG
- en: The Monte Carlo ε-soft policy algorithm
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm shows how the e-soft or epsilon soft version of the Monte Carlo
    algorithm works. Recall this is the second method we can use to define an agent
    with Monte Carlo. While the preceding algorithm may be especially scary, the part
    we are interested in is the last one, shown in this equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3c6d6aad-729c-42c5-a6ad-d40d85fd9e15.png)'
  prefs: []
  type: TYPE_IMG
- en: This becomes a more effective method for policy updates and is what is shown
    in the example. Open up `Chapter_3_3.py` and follow the exercise*:*
  prefs: []
  type: TYPE_NORMAL
- en: 'Scroll down to the following section of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'It is in this last block of code that we incrementally update the policy to
    the best value, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Or we assign it some base value, as shown in the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: From here, we can run the example again and enjoy the output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that you understand the basics of the Monte Carlo method, you can move on
    to more sample exercises in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As always, the exercises in this section are here to improve your knowledge
    and understanding of the material. Please attempt to complete 1-3 of these exercises
    on your own:'
  prefs: []
  type: TYPE_NORMAL
- en: What other constants like π could we use Monte Carlo methods to calculate? Think
    of an experiment to calculate another constant we use.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open the `Chapter_3_1.py` sample code and change the value of `n`, that is,
    the number of darts dropped. How does that affect the calculated value for π?
    Use higher or lower values for `n`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When we calculated π, we assumed a uniform distribution of darts. However, in
    the real world, the darts would likely be distributed in a normal or Gaussian
    manner. How would this affect the Monte Carlo experiment?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Refer to sample `Chapter_3_2.py` and change the value of `n`. How does that
    affect plot generation? Are you able to fix it?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open `Chapter_3_3.py` and change the number of test episodes to run in the `test_policy`
    function to a higher or lower value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open `Chapter_3_3.py` and increase the number of episodes that are used to train
    the agent. How does the agent's performance increase, if at all?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open `Chapter_3_3.py` and change the value of alpha that is used to update the
    incremental mean of averages. How does that affect the agent's ability to learn?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add the ability to visualize each policy test in a graph. See whether you can
    transfer the way we created the plots in example `Chapter_3_2.py`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Since the code is fairly generic, test this code on another Gym environment.
    Start with the standard 4 x 4 FrozenLake environment and see how well it performs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Think of ways in which the Monte Carlo method given in this example could be
    improved upon.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These exercises do not take much additional time and they can make the world
    of difference to your understanding of the materials in this book. Please use
    them.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we extended our exploration of RL and looked again at trial-and-error
    methods. In particular, we focused on how the Monte Carlo method could be used
    as a way of learning from experimenting. We first looked at an example experiment
    of the Monte Carlo method for calculating π. From there, we looked at how to visualize
    the output of this experiment with matplotlib. Then, we looked at a code example
    that showed how to use the Monte Carlo method to solve a version of the FrozenLake
    problem. Exploring the code example in detail, we uncovered how the agent played
    the game and, through that exploration, learned to improve a policy. Finally,
    we finished this chapter by understanding how the agent improves this policy using
    an incremental sample mean.
  prefs: []
  type: TYPE_NORMAL
- en: The Monte Carlo method is powerful but, as we learned, it requires episodic
    gameplay while, in the real world, a working agent needs to continuously learn
    as it controls. This form of learning is called temporal difference learning and
    is something we will explore in the next chapter.
  prefs: []
  type: TYPE_NORMAL
