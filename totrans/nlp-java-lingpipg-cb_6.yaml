- en: Chapter 6. String Comparison and Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Distance and proximity – simple edit distance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weighted edit distance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Jaccard distance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Tf-Idf distance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using edit distance and language models for spelling correction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The case restoring corrector
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automatic phrase completion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Single-link and complete-link clustering using edit distance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Latent Dirichlet allocation (LDA) for multitopic clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter starts off by comparing strings using standard language neutral
    techniques. Then, we will use these techniques to build some commonly used applications.
    We will also look at clustering techniques based on distances between strings.
  prefs: []
  type: TYPE_NORMAL
- en: For a string, we use the canonical definition that a string is a sequence of
    characters. So, clearly, these techniques apply to words, phrases, sentences,
    paragraphs, and so on, all of which you have learnt to extract in the previous
    chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Distance and proximity – simple edit distance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: String comparison refers to techniques used to measure the similarity between
    two strings. We will use distance and proximity to specify how similar any two
    strings are. The more similar any two strings are, the lesser the distance between
    them, so the distance from a string to itself is 0\. An inverse measure is proximity,
    which means that the more similar any two strings are, the greater their proximity.
  prefs: []
  type: TYPE_NORMAL
- en: We will take a look at simple edit distance first. Simple edit distance measures
    distance in terms of how many edits are required to convert one string to the
    other. A common distance measure proposed by Levenshtien in 1965 allows deletion,
    insertion, and substitution as basic operations. Adding in transposition is called
    Damerau-Levenshtien distance. For example, the distance between `foo` and `boo`
    is 1, as we're looking at a substitution of `f` with `b`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For more on distance metrics, refer to the Wikipedia article on distance at
    [http://en.wikipedia.org/wiki/Distance](http://en.wikipedia.org/wiki/Distance).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at some more examples of editable operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Deletion**: `Bart` and `Bar`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Insertion**: `Bar` and `Bart`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Substitution**: `Bar` and `Car`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transposition**: `Bart` and `Brat`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, we will run a simple example on edit distance:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the `SimpleEditDistance` class using the command line or your IDE:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the command prompt, you will be prompted for two strings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You will see the distance between the two strings with transposition allowed
    and with transposition not allowed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Play with some more examples to get a sense of how it works—try them first by
    hand and then verify that you got the optimal case.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is a very simple piece of code, and all it does is create two instances
    of the `EditDistance` class: one that allows transpositions, and the other that
    does not allow transpositions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The remaining code will set up an I/O routing, apply the edit distances, and
    print them out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: If we wanted the proximity instead of distance, we would just use the `proximity`
    method instead of the `distance` method.
  prefs: []
  type: TYPE_NORMAL
- en: In simple `EditDistance`, all the editable operations have a fixed cost of 1.0,
    that is, each editable operation (deletion, substitution, insertion, and, if allowed,
    transposition) is counted with a cost of 1.0 each. So, in the example where we
    find the distance between `ab` and `ba`, there is one deletion and one insertion,
    both of which have a cost of 1.0\. Therefore, this makes the distance between
    `ab` and `ba` 2.0 if transposition is not allowed and 1.0 if it is. Note that
    typically, there will be more than one way to edit one string into the other.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'While `EditDistance` is quite simple to use, it is not simple to implement.
    This is what the Javadoc has to say about this class:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Implementation note: This class implements* *edit distance using dynamic programming
    in time O(n * m) where n and m are the length of the sequences being compared.
    Using a sliding window of three lattice slices rather than allocating the entire
    lattice at once, the space required is that for three arrays of integers as long
    as the shorter of the two character sequences being compared.*'
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will see how to assign varying costs to each edit
    operation.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the LingPipe Javadoc on `EditDistance` at [http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/EditDistance.html](http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/EditDistance.html)
    for more details
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more details on distance, refer to the Javadoc at [http://alias-i.com/lingpipe/docs/api/com/aliasi/util/Distance.html](http://alias-i.com/lingpipe/docs/api/com/aliasi/util/Distance.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more details on proximity, refer to the Javadoc at [http://alias-i.com/lingpipe/docs/api/com/aliasi/util/Proximity.html](http://alias-i.com/lingpipe/docs/api/com/aliasi/util/Proximity.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weighted edit distance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Weighted edit distance is essentially a simple edit distance, except that the
    edits allow different costs to be associated with each kind of edit operation.
    The edit operations we identified in the previous recipe are substitution, insertion,
    deletion, and transposition. Additionally, there can be a cost associated with
    the exact matches to increase the weight for matching – this might be used when
    edits are required, such as a string-variation generator. Edit weights are generally
    scaled as log probabilities so that you can assign likelihood to an edit operation.
    The larger the weight, the more likely that edit operation is. As probabilities
    are between 0 and 1, log probabilities, or weights, will be between negative infinity
    and zero. For more on this refer to the Javadoc on the `WeightedEditDistance`
    class at [http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/WeightedEditDistance.html](http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/WeightedEditDistance.html).
  prefs: []
  type: TYPE_NORMAL
- en: On the log scale, weighted edit distance can be generalized to produce exactly
    the same results as simple edit distance did in the previous recipe by setting
    the match weight to 0 and substituting, deleting, and inserting weights to -1
    and transposition weights to either -1 or negative infinity, if we want to turn
    transposition off.
  prefs: []
  type: TYPE_NORMAL
- en: We will look at weighted edit distance for spell checking and Chinese word segmentation
    in other recipes.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will use the `FixedWeightEditDistance` instance and create
    the `CustomWeightEditDistance` class that extends the `WeightedEditDistance` abstract
    class. The `FixedWeightEditDistance` class is initialized with weights for each
    edit operation. The `CustomWeightEditDistance` class extends `WeightedEditDistance`
    and has rules for each edit operation weights. The weight for deleting alphanumeric
    characters is -1, and for all other characters, that is, punctuation and spaces,
    it is is 0\. We will set insertion weights to be the same as deletion weights.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s expand on our previous example and look at a version that runs the simple
    edit distance as well as our weighted edit distance:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In your IDE run the `SimpleWeightedEditDistance class,` or in the command line,
    type:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the command line, you will be prompted for two strings: enter the examples
    shown here or choose your own:![How to do it...](img/00011.jpeg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As you can see, there are two other distance measures being shown here: a fixed
    weight edit distance and a custom weight edit distance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Play around with other examples, including punctuation and spaces.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will instantiate a `FixedWeightEditDistance` class with some weights that
    are, arbitrarily chosen:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we set the delete, substitute, and insert weights to be equal.
    This is very similar to the standard edit distance, except that we modified the
    weights associated with the edit operations from 1 to 2\. Setting the transpose
    weight to negative infinity effectively turns off transpositions completely. Obviously,
    it's not necessary that the delete, substitute, and insert weights should be equal.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will also create a `CustomWeightEditDistance` class, which treats punctuations
    and whitespaces as matches, that is, zero cost for the insert and delete operations
    (for letters or digits, the cost remains -1). For substitutions, if the character
    is different only in case, the cost is zero; for all other cases, the cost is
    -1\. We will also turn off transposition by setting its cost to negative infinity.
    This will result in `Abc+` matching `abc-`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This sort of custom weighted edit distance is particularly useful in comparing
    strings where minor formatting changes are encountered, such as gene/protein names
    that vary from `Serpin A3` to `serpina3` but refer to the same thing.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is a T&T (Tsuruoka and Tsujii) specification for edit distance to compare
    protein names, refer to [http://alias-i.com/lingpipe/docs/api/com/aliasi/dict/ApproxDictionaryChunker.html#TT_DISTANCE](http://alias-i.com/lingpipe/docs/api/com/aliasi/dict/ApproxDictionaryChunker.html#TT_DISTANCE)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More details on the `WeightedEditDistance` class can be found on the Javadoc
    page at [http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/WeightedEditDistance.html](http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/WeightedEditDistance.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Jaccard distance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Jaccard distance is a very popular and efficient way of comparing strings.
    The Jaccard distance operates at a token level and compares two strings by first
    tokenizing them and then dividing the number of common tokens by the total number
    of tokens. In the *Eliminate near duplicates with the Jaccard distance* recipe
    in [Chapter 1](part0014_split_000.html#page "Chapter 1. Simple Classifiers"),
    *Simple Classifiers*, we applied the distance to eliminate near-duplicate tweets.
    This recipe will go into a bit more detail and show you how it is computed.
  prefs: []
  type: TYPE_NORMAL
- en: 'A distance of 0 is a perfect match, that is, the strings share all their terms,
    and a distance of 1 is a perfect mismatch, that is, the strings have no terms
    in common. Remember that proximity and distance are additive inverses, so proximity
    also ranges from 1 to 0\. Proximity of 1 is a perfect match, and proximity of
    0 is a perfect mismatch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The tokens are generated by `TokenizerFactory`, which is passed in during construction.
    For example, let's use `IndoEuropeanTokenizerFactory` and take a look at a concrete
    example. If `string1` is `fruit flies like a banana` and `string2` is `time flies
    like an arrow`, then the token set for `string1` would be `{'fruit', 'flies',
    'like', 'a', 'banana'}`, and the token set for `string2` would be `{'time', 'flies',
    'like', 'an', 'arrow'}`. The common terms (or the intersection) between these
    two token sets are `{'flies', 'like'}`, and the union of these terms is `{'fruit','
    flies', 'like', 'a', 'banana', 'time', 'an', 'arrow'}`. Now, we can calculate
    the Jaccard proximity by dividing the number of common terms by the total number
    of terms, that is, 2/8, which equals 0.25\. Thus, the distance is 0.75 (1 - 0.25).Obviously,
    the Jaccard distance is eminently tunable by modifying the tokenizer that the
    class is initialized with. For example, one could use a case-normalizing tokenizer
    so that `Abc` and `abc` would be considered equivalent. Similarly, a stemming
    tokenizer would consider the words `runs` and `run` to be equivalent. We will
    see a similar ability in the next distance metric, the Tf-Idf distance, as well.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here''s how to run the `JaccardDistance` example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Eclipse, run the `JaccardDistanceSample` class, or in the command line,
    type:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As in the previous recipes, you will be prompted for two strings. The first
    string that we will use is `Mimsey Were the Borogroves`, which is an excellent
    sci-fi short-story title, and the second string `All mimsy were the borogoves,`
    is the actual line from *Jabberwocky* that inspired it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output contains the tokens and the distances using three different tokenizers.
    The `IndoEuropean` and `EnglishStopWord` tokenizers are pretty close and show
    that these two lines are far apart. Remember that the closer two strings are,
    the lesser is the distance between them. The character tokenizer, however, shows
    that these lines are closer to each other with characters as the basis of comparison.
    Tokenizers can make a big difference in calculating the distance between strings.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The code is straightforward, and we will just cover the creation of the `JaccardDistance`
    objects. We will start with three tokenizer factories:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Note that `englishStopWordTf` uses a base tokenizer factory to construct itself.
    Refer to [Chapter 2](part0027_split_000.html#page "Chapter 2. Finding and Working
    with Words"), *Finding and Working with Words*, if there are questions on what
    is going on here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, the Jaccard distance classes are constructed, given a tokenizer factory
    as an argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The rest of the code is just our standard I/O loop and some print statements.
    That's it! On to more sophisticated measures of string distance.
  prefs: []
  type: TYPE_NORMAL
- en: The Tf-Idf distance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A very useful distance metric between strings is provided by the `TfIdfDistance`
    class. It is, in fact, closely related to the distance metric from the popular
    open source search engine, Lucene/SOLR/Elastic Search, where the strings being
    compared are the query against documents in the index. Tf-Idf stands for the core
    formula that is **term frequency** (**TF**) times **inverse document frequency**
    (**IDF**) for terms shared by the query and the document. A very cool thing about
    this approach is that common terms (for example, `the`) that are very frequent
    in documents are downweighted, while rare terms are upweighted in the distance
    comparison. This can help focus the distance on terms that are actually discriminating
    in the document collection.
  prefs: []
  type: TYPE_NORMAL
- en: Not only does `TfIdfDistance` come in handy for search-engine-like applications,
    it can be very useful for clustering and for any problem that calls for document
    similarity without supervised training data. It has a desirable property; scores
    are normalized to a score between 0 and 1, and for a fixed document `d1` and varying
    length documents `d2`, do not overwhelm the assigned score. In our experience,
    the scores for different pairs of documents are fairly robust if you were trying
    to rank the quality of match for a pair of documents.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that there are a range of different distances called Tf-Idf distances.
    The one in this class is defined to be symmetric, unlike typical Tf-Idf distances
    that are defined for information-retrieval purposes.
  prefs: []
  type: TYPE_NORMAL
- en: There is a lot of information in the Javadoc that is well worth a good look.
    However, for the purposes of these recipes, all you need to know is that the Tf-Idf
    distance is useful for finding similar documents on a word-by-word basis.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the quest to keep things a little interesting, we will use our `TfIdfDistance`
    class to build a really simple search engine over tweets. We will perform the
    following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: If you have not done it already, run the `TwitterSearch` class from [Chapter
    1](part0014_split_000.html#page "Chapter 1. Simple Classifiers"), *Simple Classifiers*,
    and get some tweets to play with, or go with our provided data. We will use the
    tweets found by running the `Disney World` query, and they are already in the
    `data` directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Type the following in the command line—this uses our defaults:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Enter a query that has some likely word matches:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: That's it. Try different queries and play around with the scores. Then, have
    a look at the source.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This code is a very simple way to build a search engine rather than a good
    way to build one. However, it is a decent way to explore how the concept of string
    distance works in the search context. Later in the book, we will perform clustering
    based on the same distance metric. Start with the `main()` class in `src/com/lingpipe/cookbook/chapter6/TfIdfSearch.java`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This program can take command-line-supplied files for the searched data in
    the `.csv` format and a text file for use as the source of training data. Next,
    we will set up a tokenizer factory and `TfIdfDistance`. If you are not familiar
    with tokenizer factories, then refer to the *Modifying tokenizer factories* recipe
    in [Chapter 2](part0027_split_000.html#page "Chapter 2. Finding and Working with
    Words"), *Modifying Tokenizer Factories*, for an explanation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will get the data that will be the IDF component by splitting the
    training text on ".", which approximates sentence detection—we could have done
    a proper sentence detection like we did in the *Sentence detection* recipe in
    [Chapter 5](part0061_split_000.html#page "Chapter 5. Finding Spans in Text – Chunking"),
    *Finding Spans in Text – Chunking*, but we chose to keep the example as simple
    as possible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Inside the `for` loop, there is `handle()`, which trains the class with knowledge
    of the token distributions in the corpus, with sentences being the document. It
    often happens that the concept of document is either smaller (sentence, paragraph,
    and word) or larger than what is typically termed `document`. In this case, the
    document frequency will be the number of sentences the token is in.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, the documents that we are searching are loaded:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The console is set up to read in the query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, each document is scored against the query with `TfIdfDistance` and put
    into `ObjectToDoubleMap`, which keeps track of the proximity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, `scoredMatches` is retrieved in the proximity order, and the first
    10 examples are printed out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: While this approach is very inefficient, in that, each query iterates over all
    the training data, does an explicit `TfIdfDistance` comparison, and stores it,
    it is not a bad way to play around with small datasets and comparison metrics.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are some subtleties worth highlighting about `TfIdfDistance`.
  prefs: []
  type: TYPE_NORMAL
- en: Difference between supervised and unsupervised trainings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When we train `TfIdfDistance`, there are some important differences in the use
    of training from the ones used in the rest of the book. The training done here
    is unsupervised, which means that no human or other external source has marked
    up the data for the expected outcome. Most of the recipes in this book that train
    use human annotated, or supervised, data.
  prefs: []
  type: TYPE_NORMAL
- en: Training on test data is OK
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As this is unsupervised data, there is no requirement that the training data
    should be be distinct from the evaluation or production data.
  prefs: []
  type: TYPE_NORMAL
- en: Using edit distance and language models for spelling correction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spelling correction takes a user input text and provides a corrected form. Most
    of us are familiar with automatic spelling correction via our smart phones or
    editors such as Microsoft Word. There are obviously quite a few amusing examples
    of these on the Web where the spelling correction fails. In this example, we'll
    build our own spelling-correction engine and look at how to tune it.
  prefs: []
  type: TYPE_NORMAL
- en: 'LingPipe''s spelling correction is based on a noisy-channel model which models
    user mistakes and expected user input (based on the data). Expected user input
    is modeled by a character-language model, and mistakes (or noise) is modeled by
    weighted edit distance. The spelling correction is done using the `CompiledSpellChecker`
    class. This class implements the noisy-channel model and provides an estimate
    of the most likely message, given that the message actually received. We can express
    this through a formula in the following manner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: In other words, we will first create a model of the intended message by creating
    an n-gram character-language model. The language model stores the statistics of
    seen phrases, that is, essentially, it stores counts of how many times the n-grams
    occurred. This gives us `P(intended)`. For example, `P(intended)` is how likely
    is the character sequence `the`. Next, we will create the channel model, which
    is a weighted edit distance and gives us the probability that the error was typed
    for that intended text. Again, for example, how likely is the error `teh` when
    the user intended to type `the`. In our case, we will model the likeliness using
    weighted edit distance where the weights are scaled as log probabilities. Refer
    to the *Weighted edit distance* recipe earlier in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The usual way of creating a compiled spell checker is through an instance of
    `TrainSpellChecker`. The result of compiling the spell-checker-training class
    and reading it back in is a compiled spell checker. `TrainSpellChecker` creates
    the basic models, weighted edit distance, and token set through the compilation
    process. We will then need to set various parameters on the `CompiledSpellChecker`
    object.
  prefs: []
  type: TYPE_NORMAL
- en: A tokenizer factory can be optionally specified to train token-sensitive spell
    checkers. With tokenization, input is further normalized to insert a single whitespace
    between all the tokens that are not already separated by a space in the input.
    The tokens are then output during compilation and read back into the compiled
    spell checker. The output of set of tokens may be pruned to remove any below a
    given count threshold. The thresholding doesn't make sense in the absence of tokens
    because we only have characters to count in the absence of tokens. Additionally,
    the set of known tokens can be used to constrain the set of alternative spellings
    suggested during spelling correction to include only tokens in the observed token
    set.
  prefs: []
  type: TYPE_NORMAL
- en: 'This approach to spell check has several advantages over a pure dictionary-based
    solution:'
  prefs: []
  type: TYPE_NORMAL
- en: The context is usefully modeled. `Frod` can be corrected to `Ford` if the next
    word is `dealership` and to `Frodo` if the next word is `Baggins`—a character
    from *The Lord of the Rings* trilogy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spell checking can be sensitive to domains. Another big advantage of this approach
    over dictionary-based spell checking is that the corrections are motivated by
    data in the training corpus. So, `trt` will be corrected to `tort` in a legal
    domain, `tart` in a cooking domain, and `TRt` in a bioinformatics domain.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s look at the steps involved in running spell checking:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In your IDE, run the `SpellCheck` class, or in the command line, type the following—note
    that we are allocating 1 gigabyte of heap space with the `–Xmx1g` flag:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Be patient; the spell checker takes a minute or two to train.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, let''s enter some misspelled words such as `beleive`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see, we got the best alternative to the input text as well as some
    other alternatives. They are sorted by the likelihood of being the best alternative.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, we can play around with different input and see how well this spell checker
    does. Try multiple words in the input and see how it performs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Also, try inputting some proper names to see how they get evaluated.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let''s look at what makes all this tick. We will start off by setting
    up `TrainSpellChecker`, which requires a `NGramProcessLM` instance, `TokenizerFactory`,
    and an `EditDistance` object that sets up weights for edit operations such as
    deletion, insertion, substitution, and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '`NGramProcessLM` needs to know the number of characters to sample in its modeling
    of the data. Reasonable values have been supplied in this example for the weighted
    edit distance, but they can be played with to help with variations due to particular
    datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '`TrainSpellChecker` can now be constructed, and next, we will load 150,000
    lines of books from Project Gutenberg. In a search-engine context, this data will
    be the data in your index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will add entries from a dictionary to help with rare words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will compile `TrainSpellChecker` so that we can instantiate `CompiledSpellChecker`.
    Typically, the output of the `compileTo()` operation is written to disk, and `CompiledSpellChecker`
    is read and instantiated from the disk, but the in-memory option is being used
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Note that there is also a way to deserialize to `TrainSpellChecker` in cases
    where more data might be added later. `CompiledSpellChecker` will not accept further
    training instances.
  prefs: []
  type: TYPE_NORMAL
- en: '`CompiledSpellChecker` admits many fine-tuning methods that are not relevant
    during training but are relevant in use. For example, it can take a set of strings
    that are not to be edited; in this case, the single value is `lingpipe`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: If these tokens are seen in the input, they will not be considered for edits.
    This can have a huge impact on the run time. The larger this set is, the faster
    the decoder will run. Configure the set of do-not-edit tokens to be as large as
    possible if execution speed is important. Usually, this is done by taking the
    object to counter-map from the compiled spell checker and saving tokens with high
    counts.
  prefs: []
  type: TYPE_NORMAL
- en: 'During training, the tokenizer factory was used to normalize data into tokens
    separated by a single whitespace. It is not serialized in the compile step, so
    if token sensitivity is needed in do-not-edit tokens, then it must be supplied:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The nBest parameter is set for the number of hypotheses that will be considered
    in modifying inputs. Even though the `nBest` size in the output is set to 3, it
    is advisable to allow for a larger hypothesis space in the left-to-right exploration
    of best performing edits. Also, the class has methods to control what edits are
    allowed and how they are scored. See the tutorial and Javadoc for more about them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we will do a console I/O loop to generate spelling variations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have included a dictionary in this model, and we will just feed the dictionary
    entries into the trainer like any other data.
  prefs: []
  type: TYPE_NORMAL
- en: It might be worthwhile to boost the dictionary by training each word in the
    dictionary more than once. Depending on the count of the dictionary, it might
    dominate or be dominated by the source training data.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The spelling-correction tutorial is more complete and covers evaluation at [http://alias-i.com/lingpipe/demos/tutorial/querySpellChecker/read-me.html](http://alias-i.com/lingpipe/demos/tutorial/querySpellChecker/read-me.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Javadoc for `CompiledSpellChecker` can be found at [http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/CompiledSpellChecker.html](http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/CompiledSpellChecker.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More information on how spell checkers work is given in the textbook, *Speech
    and Language Processing*, *Jurafsky*, *Dan*, and *James H. Martin*, *2000*, *Prentice-Hall*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The case restoring corrector
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A case-restoring spell corrector, also called a truecasing corrector, only restores
    the case and does not change anything else, that is, it does not correct spelling
    errors. This is very useful when dealing with low-quality text from transcriptions,
    automatic speech-recognition output, chat logs, and so on, which contain a variety
    of case challenges. We typically want to enhance this text to build better rule-based
    or machine-learning systems. For example, news and video transcriptions (such
    as closed captions) typically have errors, and this makes it harder to use this
    data to train NER. Case restoration can be used as a normalization tool across
    different data sources to ensure that all the data is consistent.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In your IDE, run the `CaseRestore` class, or in the command line, type the
    following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s type in some mangled-case or single-case input:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see, the mangled case gets corrected. If we use more modern text,
    such as current newspaper data or something similar, this would be directly applicable
    to case-normalizing broadcast news transcripts or closed captions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The class works in a manner similar to the spelling correction in which we
    have a model specified by the language model and a channel model specified by
    the edit distance metric. The distance metric, however, only allows case changes,
    that is, case variants are zero cost, and all other edit costs are set to `Double.NEGATIVE_INFINITY`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will focus on what is different from the previous recipe rather than going
    over all the source. We will train the spell checker with some English text from
    Project Gutenberg and use the `CASE_RESTORING` edit distance from the `CompiledSpellChecker`
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Once again, by invoking the `bestAlternative` method, we will get the best
    estimate of case-restored text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: That's it. Case restoration is made easy.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The paper by Lucian Vlad Lita et al., 2003, at [http://www.cs.cmu.edu/~llita/papers/lita.truecasing-acl2003.pdf](http://www.cs.cmu.edu/~llita/papers/lita.truecasing-acl2003.pdf)
    is a good reference on truecasing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automatic phrase completion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Automatic phrase completion is different from spelling correction, in that,
    it finds the most likely completion among a set of fixed phrases for the text
    entered so far by a user.
  prefs: []
  type: TYPE_NORMAL
- en: 'Obviously, automatic phrase completion is ubiquitous on the Web, for instance,
    on[https://google.com](https://google.com). For example, if I type `anaz` as a
    query, Google pops up the following suggestions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Automatic phrase completion](img/00012.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Note that the application is performing spelling checking at the same time as
    completion. For instance, the top suggestion is **amazon**, even though the query
    so far is **anaz**. This is not surprising, given that the number of results reported
    for the phrases that start with **anaz** is probably very small.
  prefs: []
  type: TYPE_NORMAL
- en: Next, note that it's not doing word suggestion but phrase suggestion. Some of
    the results, such as **amazon prime** are two words.
  prefs: []
  type: TYPE_NORMAL
- en: One important difference between autocompletion and spell checking is that autocompletion
    typically operates over a fixed set of phrases that must match the beginning to
    be completed. What this means is that if I type a query `I want to find anaz`,
    there are no suggested completions. The source of phrases for a web search is
    typically high-frequency queries from the query logs.
  prefs: []
  type: TYPE_NORMAL
- en: In LingPipe, we use the `AutoCompleter` class, which maintains a dictionary
    of phrases with counts and provides suggested completions based on prefix matching
    by weighted edit distance and phrase likelihood.
  prefs: []
  type: TYPE_NORMAL
- en: The autocompleter finds the best scoring phrases for a given prefix. The score
    of a phrase versus a prefix is the sum of the score of the phrase and the maximum
    score of the prefix against any prefix of the phrase. The score for a phrase is
    just its maximum likelihood probability estimate, that is, the log of its count
    divided by the sum of all counts.
  prefs: []
  type: TYPE_NORMAL
- en: Google and other search engines most likely use their query counts as the data
    for the best scoring phrases. As we don't have query logs here, we'll use US census
    data about cities in the US with populations greater than 100,000\. The phrases
    are the city names, and their counts are their populations.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In your IDE, run the `AutoComplete` class, or in the command line, type the
    following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Enter some US city names and look at the output. For example, typing `new`
    will result in the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Typing city names that don''t exist in our initial list will not return any
    output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Configuring an autocompleter is very similar to configuring spelling, except
    that instead of training a language model, we will supply it with a fixed list
    of phrases and counts, an edit distance metric, and some configuration parameters.
    The initial portion of this code just reads a file and sets up a map of phrases
    to counts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to configure the edit distance. This will measure how close
    a prefix of a target phrase is to the query prefix. This class uses a fixed weight
    edit distance, but any edit distance might be used in general:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'There are a few parameters to tune autocompletion: the edit distance and search
    parameters. The edit distance is tuned in exactly the same way as it is for spelling.
    The maximum number of results to return is more of an application''s decision
    than a tuning''s decision. Having said this, smaller result sets are faster to
    compute. The maximum queue size indicates how big the set of hypotheses can get
    inside the autocompleter before being pruned. Set `maxQueueSize` as low as possible
    while still performing adequately to increase speed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Review the Javadoc for the `AutoCompleter` class at [http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/AutoCompleter.html](http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/AutoCompleter.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Single-link and complete-link clustering using edit distance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clustering is the process of grouping a collection of objects by their similarities,
    that is, using some sort of distance measure. The idea behind clustering is that
    objects within a cluster are located close to each other, but objects in different
    clusters are farther away from each other. We can divide clustering techniques
    very broadly into hierarchical (or agglomerative) and divisional techniques. Hierarchical
    techniques start by assuming that every object is its own cluster and merge clusters
    together until a stopping criterion has been met.
  prefs: []
  type: TYPE_NORMAL
- en: For example, a stopping criterion can be a fixed distance between every cluster.
    Divisional techniques go the other way and start by grouping all the objects into
    one cluster and split it until a stopping criterion has been met, such as the
    number of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: We will review hierarchical techniques in the next few recipes. The two clustering
    implementations we will provide in LingPipe are single-link clustering and complete-link
    clustering; the resulting clusters form what is known as a partition of the input
    set. A set of sets is a partition of another set if each element of the set is
    a member of exactly one set of the partition. In mathematical terms, the sets
    that make up a partition are pair-wise disjoint, and the union is the original
    set.
  prefs: []
  type: TYPE_NORMAL
- en: A clusterer takes a set of objects as input and returns a set of sets of objects
    as output, that is, in code, `Clusterer<String>` has a `cluster` method, which
    operates on `Set<String>` and returns `Set<Set<String>>`.
  prefs: []
  type: TYPE_NORMAL
- en: 'A hierarchical clusterer extends the `Clusterer` interface and also operates
    on a set of objects, but it returns `Dendrogram` instead of a set of sets of objects.
    A dendrogram is a binary tree over the elements being clustered, with distances
    attached to each branch, which indicates the distance between the two sub-branches.
    For the `aa`, `aaa`, `aaaaa`, `bbb`, `bbbb` strings, the single-link based dendrogram
    with `EditDistance` as the metric looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The preceding dendrogram is based on single-link clustering, which takes the
    minimum distance between any two elements as the measure of similarity. So, when
    `{'aa','aaa'}` is merged with `{'aaaa'}`, the score is 2.0 by adding two `a` to
    `aaa`. Complete-link clustering takes the maximum distance between any two elements,
    which would be 3.0, with an addition of three `a` to `aa`. Single-link clustering
    tends to create highly separated clusters, whereas complete-link clustering tends
    to create more tightly centered clusters.
  prefs: []
  type: TYPE_NORMAL
- en: There are two ways to extract clusterings from dendrograms. The simplest way
    is to set a distance bound and maintain every cluster formed at less than or equal
    to this bound. The other way to construct a clustering is to continue cutting
    the highest distance cluster until a specified number of clusters is obtained.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we will look at single-link and complete-link clustering with
    edit distance as the distance metric. We will try to cluster city names by `EditDistance`,
    where the maximum distance is 4.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In your IDE, run the `HierarchicalClustering`class, or in the command line,
    type the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is various clustering approaches to the same underlying set of `Strings`.
    In this recipe, we will intersperse the source and output. First, we will create
    our set of strings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will set up a single-link instance with `EditDistance` and create
    the dendrogram for the preceding set and print it out:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next up, we will create and print out the complete-link treatment of the same
    set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will produce the same dendrogram, but with different scores:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will produce the clusters where the number of clusters is being controlled
    for the single-link case:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will produce the following—it will be the same for the complete link,
    given the input set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following code snippet is the complete-link clustering without a max distance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will control the max distance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is the effects of clustering limited by maximum distance for
    the complete-link case. Note that the single-link input here will have all elements
    in the same cluster at 3:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: That's it! We have exercised a good portion of LingPipe's clustering API.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Clustering is very sensitive to `Distance` that is used for the comparison of
    clusters. Consult the Javadoc for the 10 implementing classes for possible variations.
    `TfIdfDistance` can come in very handy to cluster language data.
  prefs: []
  type: TYPE_NORMAL
- en: 'K-means (++) clustering is a feature-extractor-based clustering. This is what
    Javadoc says about it:'
  prefs: []
  type: TYPE_NORMAL
- en: '*K-means clustering* *may be viewed as an iterative approach to the minimization
    of the average square distance between items and their cluster centers…*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See also…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For a detailed tutorial including details on evaluations, go over to [http://alias-i.com/lingpipe/demos/tutorial/cluster/read-me.html](http://alias-i.com/lingpipe/demos/tutorial/cluster/read-me.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Latent Dirichlet allocation (LDA) for multitopic clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Latent Dirichlet allocation** (**LDA**) is a statistical technique to document
    clustering based on the tokens or words that are present in the document. Clustering
    such as classification generally assumes that categories are mutually exclusive.
    The neat thing about LDA is that it allows for documents to be in multiple topics
    at the same time, instead of just one category. This better reflects the fact
    that a tweet can be about *Disney* and *Wally World*, among other topics.'
  prefs: []
  type: TYPE_NORMAL
- en: The other neat thing about LDA, like many clustering techniques, is that it
    is unsupervised, which means that no supervised training data is required! The
    closest thing to training data is that the number of topics must be specified
    before hand.
  prefs: []
  type: TYPE_NORMAL
- en: LDA can be a great way to explore a dataset where you don't know what you don't
    know. It can also be difficult to tune, but generally, it does something interesting.
    Let's get a system working.
  prefs: []
  type: TYPE_NORMAL
- en: For each document, LDA assigns a probability of belonging to a topic based on
    the words in that document. We will start with documents that are converted to
    sequences of tokens. LDA uses the count of the tokens and does not care about
    the context or order in which those words appear. The model that LDA operates
    on for each document is called "a bag of words" to denote that the order is not
    important.
  prefs: []
  type: TYPE_NORMAL
- en: The LDA model consists of a fixed number of topics, each of which is modeled
    as a distribution over words. A document under LDA is modeled as a distribution
    over topics. There is a Dirichlet prior on both the topic distributions over words
    and the document distributions over topics. Check out the Javadoc, referenced
    tutorial, and research literature if you want to know more about what is going
    on behind the scenes.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will continue to work with the `.csv` data from tweets. Refer to [Chapter
    1](part0014_split_000.html#page "Chapter 1. Simple Classifiers"), *Simple Classifiers*,
    to know how to get tweets, or use the example data from the book. The recipe uses
    `data/gravity_tweets.csv`.
  prefs: []
  type: TYPE_NORMAL
- en: This recipe closely follows the tutorial at [http://alias-i.com/lingpipe/demos/tutorial/cluster/read-me.html](http://alias-i.com/lingpipe/demos/tutorial/cluster/read-me.html),
    which goes into much more detail than we do in the recipe. The LDA portion is
    at the end of the tutorial.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section will be a source code review for `src/com/lingpipe/cookbook/chapter6/Lda.java`
    with some references to the `src/com/lingpipe/cookbook/chapter6/LdaReportingHandler.java`
    helping class that will get discussed as we use parts of it:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The top of the `main()` method gets data from a standard `csv reader`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next up is a pile of configuration that we will address line by line. The `minTokenCount`
    filters all tokens that are seen less than five times in the algorithm. As datasets
    get bigger, this number can get larger. For 1100 tweets, we are assuming that
    at least five mentions will help reduce the overall noisiness of Twitter data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `numTopics` parameter is probably the most critical configuration value,
    because it informs the algorithm about how many topics to find. Changes to this
    number can produce very different topics. You can experiment with it. By choosing
    10, we are saying that the 1100 tweets talk about 10 things overall. This is clearly
    wrong; maybe, 100 is closer to the mark. It is possible that the 1100 tweets had
    more than 1100 topics, since a tweet can be in more than one topic. Play around
    with it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'According to the Javadoc, a rule of thumb for `documentTopicPrior` is to set
    it to 5 divided by the number of topics (or less if there are very few topics;
    0.1 is typically the maximum value used):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'A generally useful value for the `topicWordPrior` is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `burninEpochs` parameter sets how many epochs to run before sampling. Setting
    this to greater than 0 has desirable properties, in that, it avoids correlation
    in the samples. The `sampleLag` controls how often the sample is taken after burning
    is complete, and `numSamples` controls how many samples to take. Currently, 2000
    samples will be taken. If `burninEpochs` were 1000, then 3000 samples would be
    taken with a sample lag of 1 (every time). If `sampleLag` was 2, then there would
    be 5000 iterations (1000 burnin, 2000 samples taken every 2 epochs for a total
    of 4000 epochs). Consult the Javadoc and tutorial for more about what is going
    on here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, `randomSeed` initializes the random process in `GibbsSampler`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`SymbolTable` is constructed; this will store the mapping from strings to integers
    for efficient processing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'A tokenizer is next with our standard one:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, the configuration of LDA is printed out:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we will create a matrix of documents and tokens that will be input to
    LDA and a report on how many tokens are present:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'A sanity check will follow by reporting a total token count:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In order to get progress reports on the epochs/samples, a handler is created
    to deliver the desired news. It takes `symbolTable` as an argument to be able
    to recreate the tokens in reporting:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The method that the search accesses in `LdaReportingHandler` follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After all this setup, we will get to run LDA:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Wait, there''s more! However, we are almost done. We just need a final report:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we will get to run this code. Type the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Have a look at a sample of the resulting output that confirms the configuration
    and the early reports from the search epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After the epochs are done, we will get a report on the topics found. The first
    topic starts with a listing of words ordered by count. Note that the topic does
    not have a title. The topic `meaning` can be gleaned by scanning the words that
    have high counts and a high `Z` score. In this case, there is a word `movie` with
    a Z score of 4.0, `a` gets 6.0, and looking down the list, we see `good` with
    a score of 5.6\. The Z score reflects how nonindependent the word is from the
    topic with a higher score; this means that the word is more tightly associated
    with the topic. Look at the source for `LdaReportingHandler` to get the exact
    definition:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding output is pretty awful, and the other topics don''t look any
    better. The next topic shows no more potential, but some obvious problems are
    arising because of tokenization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Donning our system tuner's hats, we will adjust the tokenizer to be the `new
    RegExTokenizerFactory("[^\\s]+")` tokenizer, which really cleans up the clusters,
    increases clusters to 25, and applies `Util.filterJaccard(tweets, tokFactory,
    .5)` to remove duplicates (1100 to 301). These steps were not performed one at
    a time, but this is a recipe, so we present the results of some experimentation.
    There was no evaluation harness, so this was a process that was made up of making
    a change, seeing if the output looks better and so on. Clusters are notoriously
    difficult to evaluate and tune on such an open-ended problem. The output looks
    a bit better.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On scanning the topics, we get to know that there are still lots of low-value
    words that crap up the topics, but `Topic 18` looks somewhat promising, with a
    high Z score for `best` and `ever`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Looking further into the output, we will see some documents that score high
    for `Topic 18`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Both seem reasonable for a `best movie ever` topic. However, be warned that
    the other topics/document assignments are fairly bad.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can't really claim victory over this dataset in all honesty, but we have
    laid out the mechanics of how LDA works and its configuration. LDA has not been
    a huge commercial success, but it has produced interesting concept-level implementations
    for National Institutes of Health and other customers. LDA is a tuner's paradise
    with many ways to play with the resulting clustering. Check out the tutorial and
    Javadoc, and send us your success stories.
  prefs: []
  type: TYPE_NORMAL
