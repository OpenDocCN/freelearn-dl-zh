- en: Chapter 6. String Comparison and Clustering
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章 字符串比较和聚类
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍以下食谱：
- en: Distance and proximity – simple edit distance
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 距离和邻近度 – 简单编辑距离
- en: Weighted edit distance
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加权编辑距离
- en: The Jaccard distance
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杰卡德距离
- en: The Tf-Idf distance
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tf-Idf距离
- en: Using edit distance and language models for spelling correction
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用编辑距离和语言模型进行拼写校正
- en: The case restoring corrector
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 案例恢复校正器
- en: Automatic phrase completion
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动短语补全
- en: Single-link and complete-link clustering using edit distance
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用编辑距离进行单链和完全链聚类
- en: Latent Dirichlet allocation (LDA) for multitopic clustering
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 潜在狄利克雷分配（LDA）用于多主题聚类
- en: Introduction
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简介
- en: This chapter starts off by comparing strings using standard language neutral
    techniques. Then, we will use these techniques to build some commonly used applications.
    We will also look at clustering techniques based on distances between strings.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章首先通过使用标准的中立语言技术比较字符串。然后，我们将使用这些技术构建一些常用的应用。我们还将探讨基于字符串之间距离的聚类技术。
- en: For a string, we use the canonical definition that a string is a sequence of
    characters. So, clearly, these techniques apply to words, phrases, sentences,
    paragraphs, and so on, all of which you have learnt to extract in the previous
    chapters.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个字符串，我们使用规范定义，即字符串是一系列字符的序列。因此，显然，这些技术适用于单词、短语、句子、段落等，所有这些你都在前面的章节中学过如何提取。
- en: Distance and proximity – simple edit distance
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 距离和邻近度 – 简单编辑距离
- en: String comparison refers to techniques used to measure the similarity between
    two strings. We will use distance and proximity to specify how similar any two
    strings are. The more similar any two strings are, the lesser the distance between
    them, so the distance from a string to itself is 0\. An inverse measure is proximity,
    which means that the more similar any two strings are, the greater their proximity.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 字符串比较是指用于衡量两个字符串之间相似度的技术。我们将使用距离和邻近度来指定任何两个字符串的相似程度。任何两个字符串越相似，它们之间的距离就越小，因此一个字符串到自身的距离是0。一个相反的度量是邻近度，这意味着任何两个字符串越相似，它们的邻近度就越大。
- en: We will take a look at simple edit distance first. Simple edit distance measures
    distance in terms of how many edits are required to convert one string to the
    other. A common distance measure proposed by Levenshtien in 1965 allows deletion,
    insertion, and substitution as basic operations. Adding in transposition is called
    Damerau-Levenshtien distance. For example, the distance between `foo` and `boo`
    is 1, as we're looking at a substitution of `f` with `b`.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将研究简单的编辑距离。简单的编辑距离通过需要多少次编辑将一个字符串转换为另一个字符串来衡量距离。1965年由Levenshtien提出的一个常见的距离度量允许删除、插入和替换作为基本操作。加入转置称为Damerau-Levenshtien距离。例如，`foo`
    和 `boo` 之间的距离是1，因为我们正在查看将 `f` 替换为 `b` 的替换操作。
- en: Note
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: For more on distance metrics, refer to the Wikipedia article on distance at
    [http://en.wikipedia.org/wiki/Distance](http://en.wikipedia.org/wiki/Distance).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于距离度量的信息，请参考维基百科上的距离文章 [http://en.wikipedia.org/wiki/Distance](http://en.wikipedia.org/wiki/Distance)。
- en: 'Let''s look at some more examples of editable operations:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些可编辑操作的更多示例：
- en: '**Deletion**: `Bart` and `Bar`'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**删除**：`Bart` 和 `Bar`'
- en: '**Insertion**: `Bar` and `Bart`'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**插入**：`Bar` 和 `Bart`'
- en: '**Substitution**: `Bar` and `Car`'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**替换**：`Bar` 和 `Car`'
- en: '**Transposition**: `Bart` and `Brat`'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**转置**：`Bart` 和 `Brat`'
- en: How to do it...
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'Now, we will run a simple example on edit distance:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将通过一个简单的例子来运行编辑距离：
- en: 'Run the `SimpleEditDistance` class using the command line or your IDE:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用命令行或你的集成开发环境（IDE）运行 `SimpleEditDistance` 类：
- en: '[PRE0]'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In the command prompt, you will be prompted for two strings:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在命令提示符下，你将被提示输入两个字符串：
- en: '[PRE1]'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You will see the distance between the two strings with transposition allowed
    and with transposition not allowed.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你将看到允许转置和不允许转置的字符串之间的距离。
- en: Play with some more examples to get a sense of how it works—try them first by
    hand and then verify that you got the optimal case.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过一些更多示例来了解它是如何工作的——首先手动尝试，然后验证你是否得到了最优解。
- en: How it works...
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'This is a very simple piece of code, and all it does is create two instances
    of the `EditDistance` class: one that allows transpositions, and the other that
    does not allow transpositions:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码非常简单，它所做的只是创建两个 `EditDistance` 类的实例：一个允许转置，另一个不允许转置：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The remaining code will set up an I/O routing, apply the edit distances, and
    print them out:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余的代码将设置输入/输出路由，应用编辑距离，并将它们打印出来：
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: If we wanted the proximity instead of distance, we would just use the `proximity`
    method instead of the `distance` method.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要的是邻近度而不是距离，我们只需使用`proximity`方法而不是`distance`方法。
- en: In simple `EditDistance`, all the editable operations have a fixed cost of 1.0,
    that is, each editable operation (deletion, substitution, insertion, and, if allowed,
    transposition) is counted with a cost of 1.0 each. So, in the example where we
    find the distance between `ab` and `ba`, there is one deletion and one insertion,
    both of which have a cost of 1.0\. Therefore, this makes the distance between
    `ab` and `ba` 2.0 if transposition is not allowed and 1.0 if it is. Note that
    typically, there will be more than one way to edit one string into the other.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在简单的`EditDistance`中，所有可编辑的操作都有一个固定的成本1.0，也就是说，每个可编辑的操作（删除、替换、插入，以及如果允许的话，置换）都被计为1.0。因此，在我们找到`ab`和`ba`之间的距离的例子中，有一个删除和一个插入，它们各自的成本都是1.0。因此，如果禁止置换，`ab`和`ba`之间的距离是2.0，如果允许置换，则是1.0。请注意，通常，将一个字符串编辑成另一个字符串会有多种方式。
- en: Note
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'While `EditDistance` is quite simple to use, it is not simple to implement.
    This is what the Javadoc has to say about this class:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然`EditDistance`使用起来相当简单，但实现起来并不简单。这是Javadoc对这个类所说的话：
- en: '*Implementation note: This class implements* *edit distance using dynamic programming
    in time O(n * m) where n and m are the length of the sequences being compared.
    Using a sliding window of three lattice slices rather than allocating the entire
    lattice at once, the space required is that for three arrays of integers as long
    as the shorter of the two character sequences being compared.*'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*实现说明：这个类使用动态规划实现编辑距离，时间复杂度为O(n * m)，其中n和m是被比较的序列的长度。使用三个晶格切片的滑动窗口而不是一次性分配整个晶格，所需的空间是两个比较字符序列中较短的长度整数数组的长度。*'
- en: In the following sections, we will see how to assign varying costs to each edit
    operation.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将看到如何为每个编辑操作分配不同的成本。
- en: See also
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: Refer to the LingPipe Javadoc on `EditDistance` at [http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/EditDistance.html](http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/EditDistance.html)
    for more details
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关`EditDistance`的更多详细信息，请参阅LingPipe Javadoc [http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/EditDistance.html](http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/EditDistance.html)。
- en: For more details on distance, refer to the Javadoc at [http://alias-i.com/lingpipe/docs/api/com/aliasi/util/Distance.html](http://alias-i.com/lingpipe/docs/api/com/aliasi/util/Distance.html)
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关距离的更多详细信息，请参阅[http://alias-i.com/lingpipe/docs/api/com/aliasi/util/Distance.html](http://alias-i.com/lingpipe/docs/api/com/aliasi/util/Distance.html)的Javadoc。
- en: For more details on proximity, refer to the Javadoc at [http://alias-i.com/lingpipe/docs/api/com/aliasi/util/Proximity.html](http://alias-i.com/lingpipe/docs/api/com/aliasi/util/Proximity.html)
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关邻近度的更多详细信息，请参阅[http://alias-i.com/lingpipe/docs/api/com/aliasi/util/Proximity.html](http://alias-i.com/lingpipe/docs/api/com/aliasi/util/Proximity.html)的Javadoc。
- en: Weighted edit distance
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加权编辑距离
- en: Weighted edit distance is essentially a simple edit distance, except that the
    edits allow different costs to be associated with each kind of edit operation.
    The edit operations we identified in the previous recipe are substitution, insertion,
    deletion, and transposition. Additionally, there can be a cost associated with
    the exact matches to increase the weight for matching – this might be used when
    edits are required, such as a string-variation generator. Edit weights are generally
    scaled as log probabilities so that you can assign likelihood to an edit operation.
    The larger the weight, the more likely that edit operation is. As probabilities
    are between 0 and 1, log probabilities, or weights, will be between negative infinity
    and zero. For more on this refer to the Javadoc on the `WeightedEditDistance`
    class at [http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/WeightedEditDistance.html](http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/WeightedEditDistance.html).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 加权编辑距离本质上是一种简单的编辑距离，不同之处在于编辑操作允许与每种编辑操作关联不同的成本。我们在前面的菜谱中确定的编辑操作是替换、插入、删除和置换。此外，还可以与精确匹配相关联一个成本，以增加匹配的权重——这可能在需要编辑时使用，例如字符串变异生成器。编辑权重通常按对数概率缩放，以便你可以分配一个编辑操作的似然性。权重越大，该编辑操作的可能性就越大。由于概率介于0和1之间，对数概率或权重将在负无穷大到零之间。有关更多信息，请参阅`WeightedEditDistance`类的Javadoc
    [http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/WeightedEditDistance.html](http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/WeightedEditDistance.html)。
- en: On the log scale, weighted edit distance can be generalized to produce exactly
    the same results as simple edit distance did in the previous recipe by setting
    the match weight to 0 and substituting, deleting, and inserting weights to -1
    and transposition weights to either -1 or negative infinity, if we want to turn
    transposition off.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在对数尺度上，加权编辑距离可以通过将匹配权重设置为0，将替换、删除和插入权重设置为-1，将转置权重设置为-1或负无穷大（如果我们想关闭转置操作）来推广，以产生与之前配方中简单编辑距离完全相同的结果。
- en: We will look at weighted edit distance for spell checking and Chinese word segmentation
    in other recipes.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在其他配方中查看加权编辑距离在拼写检查和中文分词中的应用。
- en: In this section, we will use the `FixedWeightEditDistance` instance and create
    the `CustomWeightEditDistance` class that extends the `WeightedEditDistance` abstract
    class. The `FixedWeightEditDistance` class is initialized with weights for each
    edit operation. The `CustomWeightEditDistance` class extends `WeightedEditDistance`
    and has rules for each edit operation weights. The weight for deleting alphanumeric
    characters is -1, and for all other characters, that is, punctuation and spaces,
    it is is 0\. We will set insertion weights to be the same as deletion weights.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用`FixedWeightEditDistance`实例并创建一个扩展`WeightedEditDistance`抽象类的`CustomWeightEditDistance`类。`FixedWeightEditDistance`类使用每个编辑操作的权重进行初始化。`CustomWeightEditDistance`类扩展`WeightedEditDistance`并为每个编辑操作的权重有规则。删除字母数字字符的权重为-1，而对于所有其他字符，即标点符号和空格，它为0。我们将设置插入权重与删除权重相同。
- en: How to do it...
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'Let''s expand on our previous example and look at a version that runs the simple
    edit distance as well as our weighted edit distance:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们扩展我们之前的例子，并查看一个同时运行简单编辑距离和我们的加权编辑距离的版本：
- en: 'In your IDE run the `SimpleWeightedEditDistance class,` or in the command line,
    type:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在您的IDE中运行`SimpleWeightedEditDistance`类，或在命令行中键入：
- en: '[PRE4]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In the command line, you will be prompted for two strings: enter the examples
    shown here or choose your own:![How to do it...](img/00011.jpeg)'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在命令行中，您将被提示输入两个字符串：输入这里显示的示例或选择您自己的：![如何实现...](img/00011.jpeg)
- en: 'As you can see, there are two other distance measures being shown here: a fixed
    weight edit distance and a custom weight edit distance.'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如您所见，这里还展示了两种其他的距离度量：固定权重编辑距离和自定义权重编辑距离。
- en: Play around with other examples, including punctuation and spaces.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试其他示例，包括标点符号和空格。
- en: How it works...
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'We will instantiate a `FixedWeightEditDistance` class with some weights that
    are, arbitrarily chosen:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将实例化一个具有一些任意选择的权重的`FixedWeightEditDistance`类：
- en: '[PRE5]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In this example, we set the delete, substitute, and insert weights to be equal.
    This is very similar to the standard edit distance, except that we modified the
    weights associated with the edit operations from 1 to 2\. Setting the transpose
    weight to negative infinity effectively turns off transpositions completely. Obviously,
    it's not necessary that the delete, substitute, and insert weights should be equal.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将删除、替换和插入的权重设置为相等。这非常类似于标准的编辑距离，除了我们将编辑操作相关的权重从1修改为2。将转置权重设置为负无穷大实际上完全关闭了转置操作。显然，删除、替换和插入的权重不必相等。
- en: 'We will also create a `CustomWeightEditDistance` class, which treats punctuations
    and whitespaces as matches, that is, zero cost for the insert and delete operations
    (for letters or digits, the cost remains -1). For substitutions, if the character
    is different only in case, the cost is zero; for all other cases, the cost is
    -1\. We will also turn off transposition by setting its cost to negative infinity.
    This will result in `Abc+` matching `abc-`:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将创建一个`CustomWeightEditDistance`类，该类将标点符号和空白视为匹配，即插入和删除操作（对于字母或数字，成本保持为-1）为零成本。对于替换，如果字符仅在大小写上不同，则成本为零；对于所有其他情况，成本为-1。我们还将通过将其成本设置为负无穷大来关闭转置操作。这将导致`Abc+`与`abc-`匹配：
- en: '[PRE6]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This sort of custom weighted edit distance is particularly useful in comparing
    strings where minor formatting changes are encountered, such as gene/protein names
    that vary from `Serpin A3` to `serpina3` but refer to the same thing.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这种自定义加权编辑距离在比较字符串时特别有用，在这些字符串中会遇到一些微小的格式变化，例如从`Serpin A3`到`serpina3`变化的基因/蛋白质名称，但指的是同一事物。
- en: See also
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考以下内容
- en: There is a T&T (Tsuruoka and Tsujii) specification for edit distance to compare
    protein names, refer to [http://alias-i.com/lingpipe/docs/api/com/aliasi/dict/ApproxDictionaryChunker.html#TT_DISTANCE](http://alias-i.com/lingpipe/docs/api/com/aliasi/dict/ApproxDictionaryChunker.html#TT_DISTANCE)
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于比较蛋白质名称，存在一个T&T（Tsuruoka和Tsujii）的编辑距离规范，请参阅[http://alias-i.com/lingpipe/docs/api/com/aliasi/dict/ApproxDictionaryChunker.html#TT_DISTANCE](http://alias-i.com/lingpipe/docs/api/com/aliasi/dict/ApproxDictionaryChunker.html#TT_DISTANCE)。
- en: More details on the `WeightedEditDistance` class can be found on the Javadoc
    page at [http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/WeightedEditDistance.html](http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/WeightedEditDistance.html)
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更多关于`WeightedEditDistance`类的详细信息可以在Javadoc页面[http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/WeightedEditDistance.html](http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/WeightedEditDistance.html)上找到。
- en: The Jaccard distance
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Jaccard距离
- en: The Jaccard distance is a very popular and efficient way of comparing strings.
    The Jaccard distance operates at a token level and compares two strings by first
    tokenizing them and then dividing the number of common tokens by the total number
    of tokens. In the *Eliminate near duplicates with the Jaccard distance* recipe
    in [Chapter 1](part0014_split_000.html#page "Chapter 1. Simple Classifiers"),
    *Simple Classifiers*, we applied the distance to eliminate near-duplicate tweets.
    This recipe will go into a bit more detail and show you how it is computed.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Jaccard距离是一种非常流行且高效的方法，用于比较字符串。Jaccard距离在令牌级别上操作，首先对两个字符串进行分词，然后通过将共同令牌的数量除以令牌总数来比较这两个字符串。在[第1章](part0014_split_000.html#page
    "第1章。简单分类器")中“使用Jaccard距离消除近似重复项”的配方中，我们应用了距离来消除近似重复的推文。这个配方将更详细地介绍，并展示它是如何计算的。
- en: 'A distance of 0 is a perfect match, that is, the strings share all their terms,
    and a distance of 1 is a perfect mismatch, that is, the strings have no terms
    in common. Remember that proximity and distance are additive inverses, so proximity
    also ranges from 1 to 0\. Proximity of 1 is a perfect match, and proximity of
    0 is a perfect mismatch:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 0的距离是一个完美匹配，即字符串共享它们所有的术语，而1的距离是一个完美不匹配，即字符串没有共同的术语。记住，相似度和距离是加法逆元，因此相似度也介于1到0之间。相似度为1是完美匹配，相似度为0是完美不匹配：
- en: '[PRE7]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The tokens are generated by `TokenizerFactory`, which is passed in during construction.
    For example, let's use `IndoEuropeanTokenizerFactory` and take a look at a concrete
    example. If `string1` is `fruit flies like a banana` and `string2` is `time flies
    like an arrow`, then the token set for `string1` would be `{'fruit', 'flies',
    'like', 'a', 'banana'}`, and the token set for `string2` would be `{'time', 'flies',
    'like', 'an', 'arrow'}`. The common terms (or the intersection) between these
    two token sets are `{'flies', 'like'}`, and the union of these terms is `{'fruit','
    flies', 'like', 'a', 'banana', 'time', 'an', 'arrow'}`. Now, we can calculate
    the Jaccard proximity by dividing the number of common terms by the total number
    of terms, that is, 2/8, which equals 0.25\. Thus, the distance is 0.75 (1 - 0.25).Obviously,
    the Jaccard distance is eminently tunable by modifying the tokenizer that the
    class is initialized with. For example, one could use a case-normalizing tokenizer
    so that `Abc` and `abc` would be considered equivalent. Similarly, a stemming
    tokenizer would consider the words `runs` and `run` to be equivalent. We will
    see a similar ability in the next distance metric, the Tf-Idf distance, as well.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 令牌是由`TokenizerFactory`生成的，该工厂在构建时传入。例如，让我们使用`IndoEuropeanTokenizerFactory`并查看一个具体示例。如果`string1`是`fruit
    flies like a banana`，而`string2`是`time flies like an arrow`，那么`string1`的令牌集将是`{'fruit',
    'flies', 'like', 'a', 'banana'}`，而`string2`的令牌集将是`{'time', 'flies', 'like', 'an',
    'arrow'}`。这两个令牌集的共同术语（或交集）是`{'flies', 'like'}`，这些术语的并集是`{'fruit','flies', 'like',
    'a', 'banana', 'time', 'an', 'arrow'}`。现在，我们可以通过将共同术语的数量除以术语总数来计算Jaccard相似度，即2/8，等于0.25。因此，距离是0.75（1
    - 0.25）。显然，通过修改类初始化时使用的分词器，Jaccard距离是可以调整的。例如，可以使用一个大小写归一化的分词器，使得`Abc`和`abc`被视为等效。同样，一个词干提取分词器会将单词`runs`和`run`视为等效。我们将在下一个距离度量中看到类似的能力，即Tf-Idf距离。
- en: How to do it...
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'Here''s how to run the `JaccardDistance` example:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是如何运行`JaccardDistance`示例：
- en: 'In Eclipse, run the `JaccardDistanceSample` class, or in the command line,
    type:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Eclipse中运行`JaccardDistanceSample`类，或在命令行中输入：
- en: '[PRE8]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'As in the previous recipes, you will be prompted for two strings. The first
    string that we will use is `Mimsey Were the Borogroves`, which is an excellent
    sci-fi short-story title, and the second string `All mimsy were the borogoves,`
    is the actual line from *Jabberwocky* that inspired it:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如前所述的食谱，你将被提示输入两个字符串。我们将使用的第一个字符串是`Mimsey Were the Borogroves`，这是一个优秀的科幻短篇小说标题，第二个字符串`All
    mimsy were the borogoves,`是启发它的《Jabberwocky》中的实际行：
- en: '[PRE9]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The output contains the tokens and the distances using three different tokenizers.
    The `IndoEuropean` and `EnglishStopWord` tokenizers are pretty close and show
    that these two lines are far apart. Remember that the closer two strings are,
    the lesser is the distance between them. The character tokenizer, however, shows
    that these lines are closer to each other with characters as the basis of comparison.
    Tokenizers can make a big difference in calculating the distance between strings.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出包含使用三个不同分词器的标记和距离。`IndoEuropean`和`EnglishStopWord`分词器非常接近，显示出这两行相距甚远。记住，两个字符串越接近，它们之间的距离就越小。然而，字符分词器却显示出这些行在字符比较的基础上彼此更接近。分词器在计算字符串之间的距离时可以产生很大的差异。
- en: How it works...
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'The code is straightforward, and we will just cover the creation of the `JaccardDistance`
    objects. We will start with three tokenizer factories:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 代码很简单，我们只需覆盖`JaccardDistance`对象的创建。我们将从三个分词工厂开始：
- en: '[PRE10]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Note that `englishStopWordTf` uses a base tokenizer factory to construct itself.
    Refer to [Chapter 2](part0027_split_000.html#page "Chapter 2. Finding and Working
    with Words"), *Finding and Working with Words*, if there are questions on what
    is going on here.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`englishStopWordTf`使用一个基础分词工厂来构建自身。如果有关于这里发生的事情的问题，请参阅[第2章](part0027_split_000.html#page
    "第2章. 查找和使用单词"), *查找和使用单词*。
- en: 'Next, the Jaccard distance classes are constructed, given a tokenizer factory
    as an argument:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，构建了Jaccard距离类，它接受一个分词工厂作为参数：
- en: '[PRE11]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The rest of the code is just our standard I/O loop and some print statements.
    That's it! On to more sophisticated measures of string distance.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余的代码只是我们的标准输入/输出循环和一些打印语句。就是这样！接下来，我们将探讨更复杂的字符串距离度量方法。
- en: The Tf-Idf distance
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Tf-Idf距离
- en: A very useful distance metric between strings is provided by the `TfIdfDistance`
    class. It is, in fact, closely related to the distance metric from the popular
    open source search engine, Lucene/SOLR/Elastic Search, where the strings being
    compared are the query against documents in the index. Tf-Idf stands for the core
    formula that is **term frequency** (**TF**) times **inverse document frequency**
    (**IDF**) for terms shared by the query and the document. A very cool thing about
    this approach is that common terms (for example, `the`) that are very frequent
    in documents are downweighted, while rare terms are upweighted in the distance
    comparison. This can help focus the distance on terms that are actually discriminating
    in the document collection.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`TfIdfDistance`类提供了字符串之间一个非常有用的距离度量。实际上，它与流行的开源搜索引擎Lucene/SOLR/Elastic Search中的距离度量密切相关，其中被比较的字符串是针对索引中文档的查询。Tf-Idf代表查询和文档共享的**词频（TF**）乘以**逆文档频率（IDF**）的核心公式。这个方法的一个非常酷的特点是，在距离比较中，常见的术语（例如，`the`）在文档中非常频繁，因此会被降权，而罕见的术语在距离比较中会被提升。这可以帮助将距离集中在文档集合中真正具有区分性的术语上。'
- en: Not only does `TfIdfDistance` come in handy for search-engine-like applications,
    it can be very useful for clustering and for any problem that calls for document
    similarity without supervised training data. It has a desirable property; scores
    are normalized to a score between 0 and 1, and for a fixed document `d1` and varying
    length documents `d2`, do not overwhelm the assigned score. In our experience,
    the scores for different pairs of documents are fairly robust if you were trying
    to rank the quality of match for a pair of documents.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅`TfIdfDistance`对于搜索引擎类应用很有用，它对于聚类以及任何需要文档相似性而不需要监督训练数据的任何问题也非常有用。它有一个理想的特性；分数被归一化到0到1之间的分数，并且对于固定的文档`d1`和不同长度的文档`d2`，不会使分配的分数过载。根据我们的经验，如果你试图对文档对的质量进行排名，不同文档对的分数相当稳健。
- en: Note
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Note that there are a range of different distances called Tf-Idf distances.
    The one in this class is defined to be symmetric, unlike typical Tf-Idf distances
    that are defined for information-retrieval purposes.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，存在一系列称为Tf-Idf距离的不同距离。这个类中的Tf-Idf距离被定义为对称的，这与为信息检索目的定义的典型Tf-Idf距离不同。
- en: There is a lot of information in the Javadoc that is well worth a good look.
    However, for the purposes of these recipes, all you need to know is that the Tf-Idf
    distance is useful for finding similar documents on a word-by-word basis.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Javadoc中有大量信息值得一看。然而，对于这些配方的目的，你需要知道的是，Tf-Idf距离对于按单词逐个查找相似文档是有用的。
- en: How to do it...
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'In the quest to keep things a little interesting, we will use our `TfIdfDistance`
    class to build a really simple search engine over tweets. We will perform the
    following steps:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持事情有点趣味性，我们将使用我们的`TfIdfDistance`类在推文中构建一个非常简单的搜索引擎。我们将执行以下步骤：
- en: If you have not done it already, run the `TwitterSearch` class from [Chapter
    1](part0014_split_000.html#page "Chapter 1. Simple Classifiers"), *Simple Classifiers*,
    and get some tweets to play with, or go with our provided data. We will use the
    tweets found by running the `Disney World` query, and they are already in the
    `data` directory.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你还没有这样做，运行[第1章](part0014_split_000.html#page "Chapter 1. Simple Classifiers")中的`TwitterSearch`类，*简单分类器*，获取一些推文来玩耍，或者使用我们提供的数据。我们将使用运行`Disney
    World`查询找到的推文，它们已经位于`data`目录中。
- en: 'Type the following in the command line—this uses our defaults:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在命令行中输入以下内容——这使用我们的默认设置：
- en: '[PRE12]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Enter a query that has some likely word matches:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入一个具有一些可能单词匹配的查询：
- en: '[PRE13]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: That's it. Try different queries and play around with the scores. Then, have
    a look at the source.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 就这样。尝试不同的查询并玩转分数。然后，查看源代码。
- en: How it works...
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'This code is a very simple way to build a search engine rather than a good
    way to build one. However, it is a decent way to explore how the concept of string
    distance works in the search context. Later in the book, we will perform clustering
    based on the same distance metric. Start with the `main()` class in `src/com/lingpipe/cookbook/chapter6/TfIdfSearch.java`:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码是构建搜索引擎的一种非常简单的方式，而不是一种很好的构建方式。然而，它是一种探索搜索上下文中字符串距离概念的好方法。在本书的后面，我们将根据相同的距离度量进行聚类。从`src/com/lingpipe/cookbook/chapter6/TfIdfSearch.java`中的`main()`类开始：
- en: '[PRE14]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This program can take command-line-supplied files for the searched data in
    the `.csv` format and a text file for use as the source of training data. Next,
    we will set up a tokenizer factory and `TfIdfDistance`. If you are not familiar
    with tokenizer factories, then refer to the *Modifying tokenizer factories* recipe
    in [Chapter 2](part0027_split_000.html#page "Chapter 2. Finding and Working with
    Words"), *Modifying Tokenizer Factories*, for an explanation:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 此程序可以接受命令行提供的以`.csv`格式搜索数据的文件，以及一个用作训练数据源的文本文件。接下来，我们将设置一个分词器工厂和`TfIdfDistance`。如果你不熟悉分词器工厂，可以参考[第2章](part0027_split_000.html#page
    "Chapter 2. Finding and Working with Words")中的*修改分词器工厂*配方，*修改分词器工厂*，以获取解释：
- en: '[PRE15]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Then, we will get the data that will be the IDF component by splitting the
    training text on ".", which approximates sentence detection—we could have done
    a proper sentence detection like we did in the *Sentence detection* recipe in
    [Chapter 5](part0061_split_000.html#page "Chapter 5. Finding Spans in Text – Chunking"),
    *Finding Spans in Text – Chunking*, but we chose to keep the example as simple
    as possible:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将通过在训练文本上分割"."来获取将成为IDF组件的数据，这近似于句子检测——我们本可以像在[第5章](part0061_split_000.html#page
    "Chapter 5. Finding Spans in Text – Chunking")中的*文本中的跨度检测*配方中那样进行适当的句子检测，但我们选择尽可能简化示例：
- en: '[PRE16]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Inside the `for` loop, there is `handle()`, which trains the class with knowledge
    of the token distributions in the corpus, with sentences being the document. It
    often happens that the concept of document is either smaller (sentence, paragraph,
    and word) or larger than what is typically termed `document`. In this case, the
    document frequency will be the number of sentences the token is in.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在`for`循环内部，有`handle()`，它使用语料库中标记分布的知识来训练类，句子作为文档。通常情况下，文档的概念要么比通常所说的`document`小（句子、段落和单词），要么更大。在这种情况下，文档频率将是标记所在的句子数量。
- en: 'Next, the documents that we are searching are loaded:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，加载我们正在搜索的文档：
- en: '[PRE17]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The console is set up to read in the query:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 控制台已设置用于读取查询：
- en: '[PRE18]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Next, each document is scored against the query with `TfIdfDistance` and put
    into `ObjectToDoubleMap`, which keeps track of the proximity:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，每个文档都会用`TfIdfDistance`与查询进行评分，并放入`ObjectToDoubleMap`中，以跟踪其接近度：
- en: '[PRE19]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Finally, `scoredMatches` is retrieved in the proximity order, and the first
    10 examples are printed out:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`scoredMatches`按接近度顺序检索，并打印出前10个示例：
- en: '[PRE20]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: While this approach is very inefficient, in that, each query iterates over all
    the training data, does an explicit `TfIdfDistance` comparison, and stores it,
    it is not a bad way to play around with small datasets and comparison metrics.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这种方法非常低效，因为每个查询都会遍历所有训练数据，进行显式的`TfIdfDistance`比较，并将其存储起来，但这并不是在小数据集和比较度量上玩耍的一个坏方法。
- en: There's more...
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: There are some subtleties worth highlighting about `TfIdfDistance`.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 关于`TfIdfDistance`有一些值得强调的微妙之处。
- en: Difference between supervised and unsupervised trainings
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 监督训练和无监督训练之间的差异
- en: When we train `TfIdfDistance`, there are some important differences in the use
    of training from the ones used in the rest of the book. The training done here
    is unsupervised, which means that no human or other external source has marked
    up the data for the expected outcome. Most of the recipes in this book that train
    use human annotated, or supervised, data.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们训练`TfIdfDistance`时，与本书中其他部分使用的训练相比，有一些重要的差异。这里所做的训练是无监督的，这意味着没有人类或其他外部来源标记数据以获得预期的结果。本书中大多数训练配方使用的是人工标注的或监督数据。
- en: Training on test data is OK
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在测试数据上训练是可以的
- en: As this is unsupervised data, there is no requirement that the training data
    should be be distinct from the evaluation or production data.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是无监督数据，没有要求训练数据必须与评估或生产数据不同。
- en: Using edit distance and language models for spelling correction
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用编辑距离和语言模型进行拼写纠正
- en: Spelling correction takes a user input text and provides a corrected form. Most
    of us are familiar with automatic spelling correction via our smart phones or
    editors such as Microsoft Word. There are obviously quite a few amusing examples
    of these on the Web where the spelling correction fails. In this example, we'll
    build our own spelling-correction engine and look at how to tune it.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 拼写纠正会接受用户输入的文本并提供一个纠正后的形式。我们大多数人通过智能手机或Microsoft Word等编辑器熟悉自动拼写纠正。显然，网上有很多拼写纠正失败的有趣例子。在这个例子中，我们将构建自己的拼写纠正引擎，并查看如何调整它。
- en: 'LingPipe''s spelling correction is based on a noisy-channel model which models
    user mistakes and expected user input (based on the data). Expected user input
    is modeled by a character-language model, and mistakes (or noise) is modeled by
    weighted edit distance. The spelling correction is done using the `CompiledSpellChecker`
    class. This class implements the noisy-channel model and provides an estimate
    of the most likely message, given that the message actually received. We can express
    this through a formula in the following manner:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: LingPipe的拼写纠正基于一个噪声信道模型，该模型模拟用户错误和预期用户输入（基于数据）。预期用户输入由字符语言模型模拟，错误（或噪声）由加权编辑距离模拟。拼写纠正是通过`CompiledSpellChecker`类完成的。这个类实现了噪声信道模型，并提供了给定实际接收到的消息的最可能消息的估计。我们可以通过以下方式表达这个公式：
- en: '[PRE21]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In other words, we will first create a model of the intended message by creating
    an n-gram character-language model. The language model stores the statistics of
    seen phrases, that is, essentially, it stores counts of how many times the n-grams
    occurred. This gives us `P(intended)`. For example, `P(intended)` is how likely
    is the character sequence `the`. Next, we will create the channel model, which
    is a weighted edit distance and gives us the probability that the error was typed
    for that intended text. Again, for example, how likely is the error `teh` when
    the user intended to type `the`. In our case, we will model the likeliness using
    weighted edit distance where the weights are scaled as log probabilities. Refer
    to the *Weighted edit distance* recipe earlier in the chapter.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们将首先通过创建一个n-gram字符语言模型来创建预期信息的模型。语言模型存储了已见短语的统计信息，也就是说，它本质上存储了n-gram出现的次数。这给了我们`P(intended)`。例如，`P(intended)`是字符序列`the`出现的可能性。接下来，我们将创建信道模型，它是一个加权编辑距离，并给出错误被输入为该预期文本的概率。再次举例，当用户意图输入`the`时，错误`teh`出现的可能性有多大。在我们的情况下，我们将使用加权编辑距离来建模可能性，其中权重按对数概率缩放。请参考本章前面的*加权编辑距离*配方。
- en: The usual way of creating a compiled spell checker is through an instance of
    `TrainSpellChecker`. The result of compiling the spell-checker-training class
    and reading it back in is a compiled spell checker. `TrainSpellChecker` creates
    the basic models, weighted edit distance, and token set through the compilation
    process. We will then need to set various parameters on the `CompiledSpellChecker`
    object.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 创建编译型拼写检查器的通常方法是通过`TrainSpellChecker`的一个实例。编译拼写检查器训练类并重新读取的结果是一个编译型拼写检查器。`TrainSpellChecker`通过编译过程创建基本模型、加权编辑距离和标记集。然后我们需要在`CompiledSpellChecker`对象上设置各种参数。
- en: A tokenizer factory can be optionally specified to train token-sensitive spell
    checkers. With tokenization, input is further normalized to insert a single whitespace
    between all the tokens that are not already separated by a space in the input.
    The tokens are then output during compilation and read back into the compiled
    spell checker. The output of set of tokens may be pruned to remove any below a
    given count threshold. The thresholding doesn't make sense in the absence of tokens
    because we only have characters to count in the absence of tokens. Additionally,
    the set of known tokens can be used to constrain the set of alternative spellings
    suggested during spelling correction to include only tokens in the observed token
    set.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 可以可选地指定一个分词工厂来训练对标记敏感的拼写检查器。通过分词，输入进一步规范化，在输入中所有未用空格分隔的标记之间插入单个空格。然后，在编译期间输出标记，并重新读入编译型拼写检查器。标记集的输出可能会被修剪，以移除任何低于给定计数阈值的标记。在没有标记的情况下，阈值没有意义，因为我们只有字符可以计数。此外，已知标记集可以用来限制在拼写校正期间建议的替代拼写集，只包括观察到的标记集中的标记。
- en: 'This approach to spell check has several advantages over a pure dictionary-based
    solution:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这种拼写检查方法与纯基于字典的解决方案相比具有几个优点：
- en: The context is usefully modeled. `Frod` can be corrected to `Ford` if the next
    word is `dealership` and to `Frodo` if the next word is `Baggins`—a character
    from *The Lord of the Rings* trilogy.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上下文是有用的模型。如果下一个词是`dealership`，则`Frod`可以被更正为`Ford`；如果下一个词是`Baggins`——来自《指环王》三部曲的角色，则可以被更正为`Frodo`。
- en: Spell checking can be sensitive to domains. Another big advantage of this approach
    over dictionary-based spell checking is that the corrections are motivated by
    data in the training corpus. So, `trt` will be corrected to `tort` in a legal
    domain, `tart` in a cooking domain, and `TRt` in a bioinformatics domain.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拼写检查可能对领域敏感。与基于字典的拼写检查相比，这种方法的一个重大优点是，校正是由训练语料库中的数据驱动的。因此，在法律领域，`trt`将被更正为`tort`，在烹饪领域为`tart`，在生物信息学领域为`TRt`。
- en: How to do it...
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'Let''s look at the steps involved in running spell checking:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看运行拼写检查所涉及的步骤：
- en: 'In your IDE, run the `SpellCheck` class, or in the command line, type the following—note
    that we are allocating 1 gigabyte of heap space with the `–Xmx1g` flag:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在您的IDE中运行`SpellCheck`类，或在命令行中输入以下内容——请注意，我们使用`–Xmx1g`标志分配了1GB的堆空间：
- en: '[PRE22]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Be patient; the spell checker takes a minute or two to train.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请耐心等待；拼写检查器需要一分钟左右的时间来训练。
- en: 'Now, let''s enter some misspelled words such as `beleive`:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们输入一些拼写错误的单词，例如`beleive`：
- en: '[PRE23]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: As you can see, we got the best alternative to the input text as well as some
    other alternatives. They are sorted by the likelihood of being the best alternative.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如您所见，我们得到了输入文本的最佳替代方案以及一些其他替代方案。它们按照成为最佳替代方案的可能性进行排序。
- en: 'Now, we can play around with different input and see how well this spell checker
    does. Try multiple words in the input and see how it performs:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以尝试不同的输入并看看这个拼写检查器做得如何。尝试在输入中输入多个单词，看看它的表现：
- en: '[PRE24]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Also, try inputting some proper names to see how they get evaluated.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，尝试输入一些专有名词，看看它们是如何被评估的。
- en: How it works...
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'Now, let''s look at what makes all this tick. We will start off by setting
    up `TrainSpellChecker`, which requires a `NGramProcessLM` instance, `TokenizerFactory`,
    and an `EditDistance` object that sets up weights for edit operations such as
    deletion, insertion, substitution, and so on:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看是什么让这一切运转。我们将从设置`TrainSpellChecker`开始，它需要一个`NGramProcessLM`实例、`TokenizerFactory`和一个设置编辑操作（如删除、插入、替换等）权重的`EditDistance`对象：
- en: '[PRE25]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '`NGramProcessLM` needs to know the number of characters to sample in its modeling
    of the data. Reasonable values have been supplied in this example for the weighted
    edit distance, but they can be played with to help with variations due to particular
    datasets:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '`NGramProcessLM`需要知道在数据建模中要采样的字符数。在这个例子中已经为加权编辑距离提供了合理的值，但它们可以进行调整以帮助特定数据集的变体：'
- en: '[PRE26]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '`TrainSpellChecker` can now be constructed, and next, we will load 150,000
    lines of books from Project Gutenberg. In a search-engine context, this data will
    be the data in your index:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '`TrainSpellChecker`现在可以构建，接下来，我们将从Project Gutenberg加载150,000行书籍。在搜索引擎的上下文中，这些数据将是您的索引中的数据。'
- en: '[PRE27]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Next, we will add entries from a dictionary to help with rare words:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将从字典中添加条目以帮助处理罕见单词：
- en: '[PRE28]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Next, we will compile `TrainSpellChecker` so that we can instantiate `CompiledSpellChecker`.
    Typically, the output of the `compileTo()` operation is written to disk, and `CompiledSpellChecker`
    is read and instantiated from the disk, but the in-memory option is being used
    here:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将编译`TrainSpellChecker`，以便我们可以实例化`CompiledSpellChecker`。通常，`compileTo()`操作的输出会被写入磁盘，`CompiledSpellChecker`会从磁盘读取并实例化，但这里使用的是内存选项：
- en: '[PRE29]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Note that there is also a way to deserialize to `TrainSpellChecker` in cases
    where more data might be added later. `CompiledSpellChecker` will not accept further
    training instances.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在以后可能需要添加更多数据的情况下，也有一种将数据反序列化为`TrainSpellChecker`的方法。`CompiledSpellChecker`将不接受进一步的训练实例。
- en: '`CompiledSpellChecker` admits many fine-tuning methods that are not relevant
    during training but are relevant in use. For example, it can take a set of strings
    that are not to be edited; in this case, the single value is `lingpipe`:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`CompiledSpellChecker`承认许多在训练期间不相关但在使用时相关的微调方法。例如，它可以接受一组不应编辑的字符串；在这种情况下，单个值是`lingpipe`：'
- en: '[PRE30]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: If these tokens are seen in the input, they will not be considered for edits.
    This can have a huge impact on the run time. The larger this set is, the faster
    the decoder will run. Configure the set of do-not-edit tokens to be as large as
    possible if execution speed is important. Usually, this is done by taking the
    object to counter-map from the compiled spell checker and saving tokens with high
    counts.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这些标记在输入中看到，它们将不会被考虑进行编辑。这可能会对运行时间产生巨大影响。这个集合越大，解码器运行得越快。如果执行速度很重要，请将不允许编辑的标记集合配置得尽可能大。通常，这是通过从编译后的拼写检查器中提取对象以反映射，并保存计数高的标记来完成的。
- en: 'During training, the tokenizer factory was used to normalize data into tokens
    separated by a single whitespace. It is not serialized in the compile step, so
    if token sensitivity is needed in do-not-edit tokens, then it must be supplied:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练期间，分词器工厂被用来将数据归一化为由单个空格分隔的标记。它不在编译步骤中序列化，因此如果需要在不允许编辑的标记中实现标记敏感性，则必须提供：
- en: '[PRE31]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The nBest parameter is set for the number of hypotheses that will be considered
    in modifying inputs. Even though the `nBest` size in the output is set to 3, it
    is advisable to allow for a larger hypothesis space in the left-to-right exploration
    of best performing edits. Also, the class has methods to control what edits are
    allowed and how they are scored. See the tutorial and Javadoc for more about them.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: nBest参数用于设置在修改输入时要考虑的假设数量。尽管输出中的`nBest`大小设置为3，但在从左到右探索最佳编辑时，建议允许更大的假设空间。此外，该类有方法来控制允许哪些编辑以及如何评分。请参阅教程和Javadoc以了解更多信息。
- en: 'Finally, we will do a console I/O loop to generate spelling variations:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将进行控制台I/O循环以生成拼写变体：
- en: '[PRE32]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Tip
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: We have included a dictionary in this model, and we will just feed the dictionary
    entries into the trainer like any other data.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在这个模型中包含了一个字典，我们只需将字典条目像其他任何数据一样输入到训练器中。
- en: It might be worthwhile to boost the dictionary by training each word in the
    dictionary more than once. Depending on the count of the dictionary, it might
    dominate or be dominated by the source training data.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 可能值得通过训练字典中的每个单词多次来增强字典。根据字典的计数，它可能占主导地位或被源训练数据所支配。
- en: See also
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: The spelling-correction tutorial is more complete and covers evaluation at [http://alias-i.com/lingpipe/demos/tutorial/querySpellChecker/read-me.html](http://alias-i.com/lingpipe/demos/tutorial/querySpellChecker/read-me.html)
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拼写校正教程更完整，涵盖了在[http://alias-i.com/lingpipe/demos/tutorial/querySpellChecker/read-me.html](http://alias-i.com/lingpipe/demos/tutorial/querySpellChecker/read-me.html)的评估。
- en: The Javadoc for `CompiledSpellChecker` can be found at [http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/CompiledSpellChecker.html](http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/CompiledSpellChecker.html)
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CompiledSpellChecker` 的 Javadoc 可以在 [http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/CompiledSpellChecker.html](http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/CompiledSpellChecker.html)
    找到。'
- en: More information on how spell checkers work is given in the textbook, *Speech
    and Language Processing*, *Jurafsky*, *Dan*, and *James H. Martin*, *2000*, *Prentice-Hall*
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于拼写检查器如何工作的更多信息可以在教材《语音与语言处理》中找到，作者为 *Jurafsky*，*Dan*，和 *James H. Martin*，出版于
    *2000* 年，由 *Prentice-Hall* 出版。
- en: The case restoring corrector
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 恢复大小写的纠正器
- en: A case-restoring spell corrector, also called a truecasing corrector, only restores
    the case and does not change anything else, that is, it does not correct spelling
    errors. This is very useful when dealing with low-quality text from transcriptions,
    automatic speech-recognition output, chat logs, and so on, which contain a variety
    of case challenges. We typically want to enhance this text to build better rule-based
    or machine-learning systems. For example, news and video transcriptions (such
    as closed captions) typically have errors, and this makes it harder to use this
    data to train NER. Case restoration can be used as a normalization tool across
    different data sources to ensure that all the data is consistent.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 恢复大小写的拼写纠正器，也称为真大小写纠正器，它只恢复大小写，不改变其他任何内容，也就是说，它不会纠正拼写错误。这在处理来自转录、自动语音识别输出、聊天记录等低质量文本时非常有用，这些文本包含各种大小写挑战。我们通常希望增强这些文本以构建更好的基于规则或机器学习系统。例如，新闻和视频转录（如字幕）通常存在错误，这使得使用这些数据来训练命名实体识别变得更加困难。大小写恢复可以用作跨不同数据源的规范化工具，以确保所有数据的一致性。
- en: How to do it...
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'In your IDE, run the `CaseRestore` class, or in the command line, type the
    following:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在您的 IDE 中运行 `CaseRestore` 类，或在命令行中输入以下内容：
- en: '[PRE33]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now, let''s type in some mangled-case or single-case input:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们输入一些混乱的大小写或单大小写输入：
- en: '[PRE34]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: As you can see, the mangled case gets corrected. If we use more modern text,
    such as current newspaper data or something similar, this would be directly applicable
    to case-normalizing broadcast news transcripts or closed captions.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如您所见，混乱的大小写得到了纠正。如果我们使用更现代的文本，如当前报纸数据或类似的内容，这将直接适用于案例归一化的广播新闻转录或字幕。
- en: How it works...
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'The class works in a manner similar to the spelling correction in which we
    have a model specified by the language model and a channel model specified by
    the edit distance metric. The distance metric, however, only allows case changes,
    that is, case variants are zero cost, and all other edit costs are set to `Double.NEGATIVE_INFINITY`:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 该类的工作方式与拼写纠正类似，其中我们有一个由语言模型指定的模型和一个由编辑距离度量指定的信道模型。然而，距离度量只允许大小写变化，即大小写变体为零成本，所有其他编辑成本都设置为
    `Double.NEGATIVE_INFINITY`：
- en: 'We will focus on what is different from the previous recipe rather than going
    over all the source. We will train the spell checker with some English text from
    Project Gutenberg and use the `CASE_RESTORING` edit distance from the `CompiledSpellChecker`
    class:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将关注与之前配方不同的地方，而不是详述所有源代码。我们将使用来自 Project Gutenberg 的某些英文文本来训练拼写检查器，并使用 `CompiledSpellChecker`
    类中的 `CASE_RESTORING` 编辑距离：
- en: '[PRE35]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Once again, by invoking the `bestAlternative` method, we will get the best
    estimate of case-restored text:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，通过调用 `bestAlternative` 方法，我们将得到恢复大小写文本的最佳估计：
- en: '[PRE36]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: That's it. Case restoration is made easy.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 就这些了。大小写恢复变得简单。
- en: See also
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: The paper by Lucian Vlad Lita et al., 2003, at [http://www.cs.cmu.edu/~llita/papers/lita.truecasing-acl2003.pdf](http://www.cs.cmu.edu/~llita/papers/lita.truecasing-acl2003.pdf)
    is a good reference on truecasing
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lucian Vlad Lita 等人于 2003 年发表的论文 [http://www.cs.cmu.edu/~llita/papers/lita.truecasing-acl2003.pdf](http://www.cs.cmu.edu/~llita/papers/lita.truecasing-acl2003.pdf)
    是关于真大小写的良好参考。
- en: Automatic phrase completion
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动短语补全
- en: Automatic phrase completion is different from spelling correction, in that,
    it finds the most likely completion among a set of fixed phrases for the text
    entered so far by a user.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 自动短语补全与拼写纠正不同，它会在用户输入的文本中找到一组固定短语中最可能的补全。
- en: 'Obviously, automatic phrase completion is ubiquitous on the Web, for instance,
    on[https://google.com](https://google.com). For example, if I type `anaz` as a
    query, Google pops up the following suggestions:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，自动短语补全在网络上无处不在，例如在 [https://google.com](https://google.com)。例如，如果我输入 `anaz`
    作为查询，Google 会弹出以下建议：
- en: '![Automatic phrase completion](img/00012.jpeg)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![自动短语补全](img/00012.jpeg)'
- en: Note that the application is performing spelling checking at the same time as
    completion. For instance, the top suggestion is **amazon**, even though the query
    so far is **anaz**. This is not surprising, given that the number of results reported
    for the phrases that start with **anaz** is probably very small.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，应用程序在执行补全的同时也在进行拼写检查。例如，最高建议是**amazon**，尽管到目前为止的查询是**anaz**。考虑到以**anaz**开头的短语报告的结果数量可能非常小，这并不令人惊讶。
- en: Next, note that it's not doing word suggestion but phrase suggestion. Some of
    the results, such as **amazon prime** are two words.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，请注意，它不是进行单词建议，而是短语建议。一些结果，如**amazon prime**，是两个单词。
- en: One important difference between autocompletion and spell checking is that autocompletion
    typically operates over a fixed set of phrases that must match the beginning to
    be completed. What this means is that if I type a query `I want to find anaz`,
    there are no suggested completions. The source of phrases for a web search is
    typically high-frequency queries from the query logs.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 自动补全和拼写检查之间的重要区别在于，自动补全通常在一系列固定的短语上操作，这些短语必须与补全的开始部分匹配。这意味着，如果我输入查询`我想找到anaz`，将没有建议的补全。网页搜索的短语来源通常是查询日志中的高频查询。
- en: In LingPipe, we use the `AutoCompleter` class, which maintains a dictionary
    of phrases with counts and provides suggested completions based on prefix matching
    by weighted edit distance and phrase likelihood.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在LingPipe中，我们使用`AutoCompleter`类，它维护一个包含计数的短语字典，并根据加权编辑距离和短语可能性提供基于前缀匹配的建议补全。
- en: The autocompleter finds the best scoring phrases for a given prefix. The score
    of a phrase versus a prefix is the sum of the score of the phrase and the maximum
    score of the prefix against any prefix of the phrase. The score for a phrase is
    just its maximum likelihood probability estimate, that is, the log of its count
    divided by the sum of all counts.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 自动补全器为给定前缀找到最佳评分的短语。短语与前缀的分数是短语分数和前缀与短语任何前缀的最大分数的总和。短语分数只是其最大似然概率估计，即其计数的对数除以所有计数的总和。
- en: Google and other search engines most likely use their query counts as the data
    for the best scoring phrases. As we don't have query logs here, we'll use US census
    data about cities in the US with populations greater than 100,000\. The phrases
    are the city names, and their counts are their populations.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌和其他搜索引擎很可能使用它们的查询计数作为最佳评分短语的数据。由于我们没有查询日志，我们将使用关于美国人口超过10万的城市的人口普查数据。短语是城市名称，它们的计数是它们的人口。
- en: How to do it...
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'In your IDE, run the `AutoComplete` class, or in the command line, type the
    following command:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你的IDE中运行`AutoComplete`类，或者在命令行中输入以下命令：
- en: '[PRE37]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Enter some US city names and look at the output. For example, typing `new`
    will result in the following output:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入一些美国城市名称并查看输出。例如，输入`new`将产生以下输出：
- en: '[PRE38]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Typing city names that don''t exist in our initial list will not return any
    output:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入我们初始列表中不存在的城市名称将不会返回任何输出：
- en: '[PRE39]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: How it works...
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'Configuring an autocompleter is very similar to configuring spelling, except
    that instead of training a language model, we will supply it with a fixed list
    of phrases and counts, an edit distance metric, and some configuration parameters.
    The initial portion of this code just reads a file and sets up a map of phrases
    to counts:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 配置自动补全器与配置拼写检查非常相似，除了我们不会训练语言模型，而是提供一个固定短语列表和计数、编辑距离指标和一些配置参数。这段代码的初始部分只是读取一个文件并设置短语到计数的映射：
- en: '[PRE40]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The next step is to configure the edit distance. This will measure how close
    a prefix of a target phrase is to the query prefix. This class uses a fixed weight
    edit distance, but any edit distance might be used in general:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是配置编辑距离。这将衡量目标短语的前缀与查询前缀的接近程度。这个类使用固定的权重编辑距离，但通常可以使用任何编辑距离：
- en: '[PRE41]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'There are a few parameters to tune autocompletion: the edit distance and search
    parameters. The edit distance is tuned in exactly the same way as it is for spelling.
    The maximum number of results to return is more of an application''s decision
    than a tuning''s decision. Having said this, smaller result sets are faster to
    compute. The maximum queue size indicates how big the set of hypotheses can get
    inside the autocompleter before being pruned. Set `maxQueueSize` as low as possible
    while still performing adequately to increase speed:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 自动完成有一些参数可以调整：编辑距离和搜索参数。编辑距离的调整方式与拼写时相同。返回的最大结果数量更多的是一个应用程序的决定，而不是调整的决定。话虽如此，较小的结果集计算速度更快。最大队列大小表示在自动完成器中进行修剪之前，假设集可以变得有多大。尽可能将
    `maxQueueSize` 设置得低，同时仍然能够良好地执行以提高速度：
- en: '[PRE42]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: See also
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: Review the Javadoc for the `AutoCompleter` class at [http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/AutoCompleter.html](http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/AutoCompleter.html)
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看位于 [http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/AutoCompleter.html](http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/AutoCompleter.html)
    的 `AutoCompleter` 类的 Javadoc。
- en: Single-link and complete-link clustering using edit distance
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用编辑距离进行单链接和完全链接聚类
- en: Clustering is the process of grouping a collection of objects by their similarities,
    that is, using some sort of distance measure. The idea behind clustering is that
    objects within a cluster are located close to each other, but objects in different
    clusters are farther away from each other. We can divide clustering techniques
    very broadly into hierarchical (or agglomerative) and divisional techniques. Hierarchical
    techniques start by assuming that every object is its own cluster and merge clusters
    together until a stopping criterion has been met.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是将一组对象根据它们的相似性进行分组的过程，即使用某种距离度量。聚类背后的想法是，聚类内的对象彼此靠近，但不同聚类中的对象彼此较远。我们可以非常广泛地将聚类技术分为层次（或聚合）技术和分区技术。层次技术首先假设每个对象都是它自己的聚类，然后合并聚类直到满足停止标准。
- en: For example, a stopping criterion can be a fixed distance between every cluster.
    Divisional techniques go the other way and start by grouping all the objects into
    one cluster and split it until a stopping criterion has been met, such as the
    number of clusters.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个停止标准可以是每个聚类之间的固定距离。分区技术则相反，它首先将所有对象分组到一个聚类中，然后不断分割直到满足停止标准，比如聚类数量。
- en: We will review hierarchical techniques in the next few recipes. The two clustering
    implementations we will provide in LingPipe are single-link clustering and complete-link
    clustering; the resulting clusters form what is known as a partition of the input
    set. A set of sets is a partition of another set if each element of the set is
    a member of exactly one set of the partition. In mathematical terms, the sets
    that make up a partition are pair-wise disjoint, and the union is the original
    set.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的几个菜谱中回顾层次技术。LingPipe 将提供两种聚类实现：单链接聚类和完全链接聚类；生成的聚类形成了一个被称为输入集划分的结果。如果集合中的每个元素恰好是划分中一个集合的成员，那么一组集合就是另一个集合的划分。用数学术语来说，构成划分的集合是成对不相交的，并且它们的并集是原始集合。
- en: A clusterer takes a set of objects as input and returns a set of sets of objects
    as output, that is, in code, `Clusterer<String>` has a `cluster` method, which
    operates on `Set<String>` and returns `Set<Set<String>>`.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类器接收一组对象作为输入，并返回一组对象的集合作为输出，即在代码中，`Clusterer<String>` 有一个 `cluster` 方法，它作用于
    `Set<String>` 并返回 `Set<Set<String>>`。
- en: 'A hierarchical clusterer extends the `Clusterer` interface and also operates
    on a set of objects, but it returns `Dendrogram` instead of a set of sets of objects.
    A dendrogram is a binary tree over the elements being clustered, with distances
    attached to each branch, which indicates the distance between the two sub-branches.
    For the `aa`, `aaa`, `aaaaa`, `bbb`, `bbbb` strings, the single-link based dendrogram
    with `EditDistance` as the metric looks like this:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类器扩展了 `Clusterer` 接口，并且也作用于一组对象，但它返回的是 `Dendrogram` 而不是一组对象的集合。Dendrogram
    是一个二叉树，表示正在聚类的元素，每个分支上附有距离，这表示两个子分支之间的距离。对于 `aa`、`aaa`、`aaaaa`、`bbb`、`bbbb` 这些字符串，基于编辑距离的单链接层次树如下所示：
- en: '[PRE43]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The preceding dendrogram is based on single-link clustering, which takes the
    minimum distance between any two elements as the measure of similarity. So, when
    `{'aa','aaa'}` is merged with `{'aaaa'}`, the score is 2.0 by adding two `a` to
    `aaa`. Complete-link clustering takes the maximum distance between any two elements,
    which would be 3.0, with an addition of three `a` to `aa`. Single-link clustering
    tends to create highly separated clusters, whereas complete-link clustering tends
    to create more tightly centered clusters.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 上述树状图是基于单链接聚类，它将任意两个元素之间的最小距离作为相似度的度量。因此，当`{'aa','aaa'}`与`{'aaaa'}`合并时，分数是2.0，通过向`aaa`添加两个`a`。完整链接聚类考虑任意两个元素之间的最大距离，这将是一个3.0，通过向`aa`添加三个`a`。单链接聚类倾向于创建高度分离的聚类，而完整链接聚类倾向于创建更紧密集中的聚类。
- en: There are two ways to extract clusterings from dendrograms. The simplest way
    is to set a distance bound and maintain every cluster formed at less than or equal
    to this bound. The other way to construct a clustering is to continue cutting
    the highest distance cluster until a specified number of clusters is obtained.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 从树状图中提取聚类有两种方法。最简单的方法是设置一个距离界限，并保持所有形成的聚类都小于或等于这个界限。另一种构建聚类的方法是继续切割距离最高的聚类，直到获得指定数量的聚类。
- en: In this example, we will look at single-link and complete-link clustering with
    edit distance as the distance metric. We will try to cluster city names by `EditDistance`,
    where the maximum distance is 4.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将查看使用编辑距离作为距离度量的单链接和完整链接聚类。我们将尝试通过`EditDistance`聚类城市名称，最大距离为4。
- en: How to do it…
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点…
- en: 'In your IDE, run the `HierarchicalClustering`class, or in the command line,
    type the following:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你的集成开发环境（IDE）中运行`HierarchicalClustering`类，或者在命令行中输入以下内容：
- en: '[PRE44]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The output is various clustering approaches to the same underlying set of `Strings`.
    In this recipe, we will intersperse the source and output. First, we will create
    our set of strings:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出是针对相同底层集合的多种聚类方法。在这个菜谱中，我们将交错源代码和输出。首先，我们将创建我们的字符串集合：
- en: '[PRE45]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Next, we will set up a single-link instance with `EditDistance` and create
    the dendrogram for the preceding set and print it out:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将使用`EditDistance`设置单链接实例，并为前面的集合创建树状图并打印出来：
- en: '[PRE46]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The output will be as follows:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE47]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Next up, we will create and print out the complete-link treatment of the same
    set:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将创建并打印出相同集合的完整链接处理：
- en: '[PRE48]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'This will produce the same dendrogram, but with different scores:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将生成相同的树状图，但得分不同：
- en: '[PRE49]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Next, we will produce the clusters where the number of clusters is being controlled
    for the single-link case:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将生成单链接情况下控制聚类数量的聚类：
- en: '[PRE50]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'This will produce the following—it will be the same for the complete link,
    given the input set:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将生成以下结果——对于完整链接，给定输入集将相同：
- en: '[PRE51]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The following code snippet is the complete-link clustering without a max distance:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码片段是没有任何最大距离的完整链接聚类：
- en: '[PRE52]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The output will be:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE53]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Next, we will control the max distance:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将控制最大距离：
- en: '[PRE54]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The following is the effects of clustering limited by maximum distance for
    the complete-link case. Note that the single-link input here will have all elements
    in the same cluster at 3:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下是在完整链接情况下限制最大距离的聚类效果。请注意，这里的单链接输入将所有元素都放在3号聚类中：
- en: '[PRE55]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: That's it! We have exercised a good portion of LingPipe's clustering API.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 就这样！我们已经练习了LingPipe聚类API的大部分功能。
- en: There's more…
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: Clustering is very sensitive to `Distance` that is used for the comparison of
    clusters. Consult the Javadoc for the 10 implementing classes for possible variations.
    `TfIdfDistance` can come in very handy to cluster language data.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类对用于比较聚类的`Distance`非常敏感。请参阅10个实现类的Javadoc以获取可能的变体。`TfIdfDistance`在聚类语言数据时非常有用。
- en: 'K-means (++) clustering is a feature-extractor-based clustering. This is what
    Javadoc says about it:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: K-means（++）聚类是基于特征提取器的聚类。这是Javadoc对它的描述：
- en: '*K-means clustering* *may be viewed as an iterative approach to the minimization
    of the average square distance between items and their cluster centers…*'
  id: totrans-255
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*K-means聚类*可以被视为一种迭代方法，用于最小化项目与其聚类中心之间的平均平方距离…*'
- en: See also…
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见…
- en: For a detailed tutorial including details on evaluations, go over to [http://alias-i.com/lingpipe/demos/tutorial/cluster/read-me.html](http://alias-i.com/lingpipe/demos/tutorial/cluster/read-me.html)
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关包括评估细节的详细教程，请访问[http://alias-i.com/lingpipe/demos/tutorial/cluster/read-me.html](http://alias-i.com/lingpipe/demos/tutorial/cluster/read-me.html)
- en: Latent Dirichlet allocation (LDA) for multitopic clustering
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多主题聚类的潜在狄利克雷分配（LDA）
- en: '**Latent Dirichlet allocation** (**LDA**) is a statistical technique to document
    clustering based on the tokens or words that are present in the document. Clustering
    such as classification generally assumes that categories are mutually exclusive.
    The neat thing about LDA is that it allows for documents to be in multiple topics
    at the same time, instead of just one category. This better reflects the fact
    that a tweet can be about *Disney* and *Wally World*, among other topics.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '**潜在狄利克雷分配**（**LDA**）是一种基于文档中出现的标记或单词进行文档聚类的统计技术。聚类，如分类，通常假设类别是互斥的。LDA的巧妙之处在于它允许文档同时属于多个主题，而不仅仅是单个类别。这更好地反映了推特可以关于*迪士尼*和*沃尔玛世界*等众多主题的事实。'
- en: The other neat thing about LDA, like many clustering techniques, is that it
    is unsupervised, which means that no supervised training data is required! The
    closest thing to training data is that the number of topics must be specified
    before hand.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: LDA的另一个巧妙之处，就像许多聚类技术一样，是它是无监督的，这意味着不需要监督训练数据！最接近训练数据的是必须事先指定主题的数量。
- en: LDA can be a great way to explore a dataset where you don't know what you don't
    know. It can also be difficult to tune, but generally, it does something interesting.
    Let's get a system working.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: LDA可以是一种探索数据集的好方法，其中你不知道自己不知道什么。它也可能很难调整，但通常，它会做一些有趣的事情。让我们让系统运行起来。
- en: For each document, LDA assigns a probability of belonging to a topic based on
    the words in that document. We will start with documents that are converted to
    sequences of tokens. LDA uses the count of the tokens and does not care about
    the context or order in which those words appear. The model that LDA operates
    on for each document is called "a bag of words" to denote that the order is not
    important.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个文档，LDA根据该文档中的单词为其分配属于某个主题的概率。我们将从已转换为标记序列的文档开始。LDA使用标记的计数，而不关心这些单词出现的上下文或顺序。LDA对每个文档操作的模式被称为“词袋”，以表示顺序并不重要。
- en: The LDA model consists of a fixed number of topics, each of which is modeled
    as a distribution over words. A document under LDA is modeled as a distribution
    over topics. There is a Dirichlet prior on both the topic distributions over words
    and the document distributions over topics. Check out the Javadoc, referenced
    tutorial, and research literature if you want to know more about what is going
    on behind the scenes.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: LDA模型由固定数量的主题组成，每个主题都被建模为单词上的分布。在LDA下，文档被建模为主题上的分布。主题分布和文档分布上都有一个狄利克雷先验。如果你想了解更多关于幕后发生的事情的信息，请查看Javadoc、参考教程和研究文献。
- en: Getting ready
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will continue to work with the `.csv` data from tweets. Refer to [Chapter
    1](part0014_split_000.html#page "Chapter 1. Simple Classifiers"), *Simple Classifiers*,
    to know how to get tweets, or use the example data from the book. The recipe uses
    `data/gravity_tweets.csv`.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用推文的`.csv`数据。参考[第1章](part0014_split_000.html#page "第1章. 简单分类器")，*简单分类器*，了解如何获取推文，或使用书中的示例数据。食谱使用`data/gravity_tweets.csv`。
- en: This recipe closely follows the tutorial at [http://alias-i.com/lingpipe/demos/tutorial/cluster/read-me.html](http://alias-i.com/lingpipe/demos/tutorial/cluster/read-me.html),
    which goes into much more detail than we do in the recipe. The LDA portion is
    at the end of the tutorial.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 这个食谱紧密遵循[http://alias-i.com/lingpipe/demos/tutorial/cluster/read-me.html](http://alias-i.com/lingpipe/demos/tutorial/cluster/read-me.html)中的教程，该教程比我们在食谱中讲述的细节要多得多。LDA部分在教程的末尾。
- en: How to do it…
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'This section will be a source code review for `src/com/lingpipe/cookbook/chapter6/Lda.java`
    with some references to the `src/com/lingpipe/cookbook/chapter6/LdaReportingHandler.java`
    helping class that will get discussed as we use parts of it:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将对`src/com/lingpipe/cookbook/chapter6/Lda.java`进行源代码审查，并引用`src/com/lingpipe/cookbook/chapter6/LdaReportingHandler.java`辅助类，该类将在我们使用其部分内容时进行讨论：
- en: 'The top of the `main()` method gets data from a standard `csv reader`:'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`main()`方法的顶部从标准`csv reader`获取数据：'
- en: '[PRE56]'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Next up is a pile of configuration that we will address line by line. The `minTokenCount`
    filters all tokens that are seen less than five times in the algorithm. As datasets
    get bigger, this number can get larger. For 1100 tweets, we are assuming that
    at least five mentions will help reduce the overall noisiness of Twitter data:'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将逐行处理一堆配置。`minTokenCount` 过滤掉算法中看到少于五次的标记。随着数据集变大，这个数字可以更大。对于1100条推文，我们假设至少五次提及有助于减少Twitter数据的整体噪声：
- en: '[PRE57]'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The `numTopics` parameter is probably the most critical configuration value,
    because it informs the algorithm about how many topics to find. Changes to this
    number can produce very different topics. You can experiment with it. By choosing
    10, we are saying that the 1100 tweets talk about 10 things overall. This is clearly
    wrong; maybe, 100 is closer to the mark. It is possible that the 1100 tweets had
    more than 1100 topics, since a tweet can be in more than one topic. Play around
    with it:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`numTopics` 参数可能是最重要的配置值，因为它告知算法要找到多少个主题。这个数字的变化可以产生非常不同的主题。你可以尝试一下。选择10意味着1100条推文总体上谈论了10件事情。这显然是错误的；也许100更接近实际情况。1100条推文可能有超过1100个主题，因为一条推文可能属于多个主题。尝试调整它：'
- en: '[PRE58]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'According to the Javadoc, a rule of thumb for `documentTopicPrior` is to set
    it to 5 divided by the number of topics (or less if there are very few topics;
    0.1 is typically the maximum value used):'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据Javadoc，`documentTopicPrior` 的一个经验规则是将它设置为主题数量的5倍（如果主题很少，则更少；通常最大值为0.1）：
- en: '[PRE59]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'A generally useful value for the `topicWordPrior` is as follows:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 `topicWordPrior`，一个通常有用的值如下：
- en: '[PRE60]'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The `burninEpochs` parameter sets how many epochs to run before sampling. Setting
    this to greater than 0 has desirable properties, in that, it avoids correlation
    in the samples. The `sampleLag` controls how often the sample is taken after burning
    is complete, and `numSamples` controls how many samples to take. Currently, 2000
    samples will be taken. If `burninEpochs` were 1000, then 3000 samples would be
    taken with a sample lag of 1 (every time). If `sampleLag` was 2, then there would
    be 5000 iterations (1000 burnin, 2000 samples taken every 2 epochs for a total
    of 4000 epochs). Consult the Javadoc and tutorial for more about what is going
    on here:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`burninEpochs` 参数设置在采样之前要运行多少个epoch。将其设置为大于0具有一些期望的特性，即避免样本之间的相关性。`sampleLag`
    控制在烧毁完成后多久取一次样，而 `numSamples` 控制要取多少个样本。目前，将取2000个样本。如果 `burninEpochs` 是1000，那么将取3000个样本，采样间隔为1（每次）。如果
    `sampleLag` 是2，那么将有5000次迭代（1000次烧毁，每2个epoch取2000个样本，总共4000个epoch）。请参考Javadoc和教程了解更多关于这里发生的事情：'
- en: '[PRE61]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Finally, `randomSeed` initializes the random process in `GibbsSampler`:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，`randomSeed` 初始化 `GibbsSampler` 中的随机过程：
- en: '[PRE62]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '`SymbolTable` is constructed; this will store the mapping from strings to integers
    for efficient processing:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`SymbolTable` 已经构建完成；这将存储字符串到整数的映射，以便于高效处理：'
- en: '[PRE63]'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'A tokenizer is next with our standard one:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来是一个分词器，使用我们标准的分词器：
- en: '[PRE64]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Next, the configuration of LDA is printed out:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，打印出LDA的配置：
- en: '[PRE65]'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Then, we will create a matrix of documents and tokens that will be input to
    LDA and a report on how many tokens are present:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将创建一个文档和标记的矩阵，这些矩阵将被输入到LDA中，以及一个关于有多少标记存在的报告：
- en: '[PRE66]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'A sanity check will follow by reporting a total token count:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，将进行一个合理性检查，通过报告总标记数：
- en: '[PRE67]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'In order to get progress reports on the epochs/samples, a handler is created
    to deliver the desired news. It takes `symbolTable` as an argument to be able
    to recreate the tokens in reporting:'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了在epoch/samples上获得进度报告，创建了一个处理程序来传递所需的消息。它接受 `symbolTable` 作为参数，以便在报告中重新创建标记：
- en: '[PRE68]'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'The method that the search accesses in `LdaReportingHandler` follows:'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `LdaReportingHandler` 中，搜索访问的方法遵循以下步骤：
- en: '[PRE69]'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'After all this setup, we will get to run LDA:'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在所有这些设置之后，我们将运行LDA：
- en: '[PRE70]'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Wait, there''s more! However, we are almost done. We just need a final report:'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 等等，还有更多！然而，我们几乎完成了。我们只需要一个最终报告：
- en: '[PRE71]'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Finally, we will get to run this code. Type the following command:'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将运行这段代码。输入以下命令：
- en: '[PRE72]'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Have a look at a sample of the resulting output that confirms the configuration
    and the early reports from the search epochs:'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 看一下结果的输出样本，以确认配置和搜索epoch的早期报告：
- en: '[PRE73]'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'After the epochs are done, we will get a report on the topics found. The first
    topic starts with a listing of words ordered by count. Note that the topic does
    not have a title. The topic `meaning` can be gleaned by scanning the words that
    have high counts and a high `Z` score. In this case, there is a word `movie` with
    a Z score of 4.0, `a` gets 6.0, and looking down the list, we see `good` with
    a score of 5.6\. The Z score reflects how nonindependent the word is from the
    topic with a higher score; this means that the word is more tightly associated
    with the topic. Look at the source for `LdaReportingHandler` to get the exact
    definition:'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在完成epoch后，我们将得到关于找到的主题的报告。第一个主题以按计数排序的词汇列表开始。请注意，该主题没有标题。通过扫描计数高且Z得分高的词汇，我们可以推断出主题“意义”。在这种情况下，有一个单词“movie”的Z得分为4.0，“a”得分为6.0，向下看列表，我们看到“good”的得分为5.6。Z得分反映了单词与得分更高的主题的非独立性；这意味着该单词与主题的关联更加紧密。查看`LdaReportingHandler`的源代码以获取确切的定义：
- en: '[PRE74]'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'The preceding output is pretty awful, and the other topics don''t look any
    better. The next topic shows no more potential, but some obvious problems are
    arising because of tokenization:'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上述输出相当糟糕，其他主题看起来也好不到哪里去。下一个主题没有更多潜力，但一些明显的问题是由于分词引起的：
- en: '[PRE75]'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Donning our system tuner's hats, we will adjust the tokenizer to be the `new
    RegExTokenizerFactory("[^\\s]+")` tokenizer, which really cleans up the clusters,
    increases clusters to 25, and applies `Util.filterJaccard(tweets, tokFactory,
    .5)` to remove duplicates (1100 to 301). These steps were not performed one at
    a time, but this is a recipe, so we present the results of some experimentation.
    There was no evaluation harness, so this was a process that was made up of making
    a change, seeing if the output looks better and so on. Clusters are notoriously
    difficult to evaluate and tune on such an open-ended problem. The output looks
    a bit better.
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 穿上我们系统调优者的帽子，我们将调整分词器为`new RegExTokenizerFactory("[^\\s]+")`分词器，这实际上清理了聚类，将聚类数量增加到25，并应用`Util.filterJaccard(tweets,
    tokFactory, .5)`来移除重复项（从1100减少到301）。这些步骤并不是一次一个完成的，这是一个配方，所以我们展示了实验的结果。没有评估工具，所以这是一个通过做出改变，看看输出是否更好，等等的过程。在这样一个开放性问题中，聚类评估和调优是出了名的困难。输出看起来稍微好一些。
- en: 'On scanning the topics, we get to know that there are still lots of low-value
    words that crap up the topics, but `Topic 18` looks somewhat promising, with a
    high Z score for `best` and `ever`:'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在浏览主题时，我们了解到其中仍然有很多低价值词汇充斥着主题，但“主题18”看起来有些希望，因为“最佳”和“永远”这两个词的Z得分较高：
- en: '[PRE76]'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Looking further into the output, we will see some documents that score high
    for `Topic 18`:'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进一步查看输出结果，我们会看到一些在“主题18”上得分较高的文档：
- en: '[PRE77]'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: Both seem reasonable for a `best movie ever` topic. However, be warned that
    the other topics/document assignments are fairly bad.
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 两个选项对于一个“史上最佳电影”主题来说似乎都合理。然而，要警告的是，其他主题/文档作业相当糟糕。
- en: We can't really claim victory over this dataset in all honesty, but we have
    laid out the mechanics of how LDA works and its configuration. LDA has not been
    a huge commercial success, but it has produced interesting concept-level implementations
    for National Institutes of Health and other customers. LDA is a tuner's paradise
    with many ways to play with the resulting clustering. Check out the tutorial and
    Javadoc, and send us your success stories.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 实事求是地说，我们并不能真正宣称在这个数据集上取得了胜利，但我们已经阐述了LDA的工作原理及其配置。LDA并没有取得巨大的商业成功，但它为美国国家卫生研究院和其他客户产生了有趣的概念级实现。LDA是一个调优者的天堂，有很多方式可以玩转生成的聚类。查看教程和Javadoc，并分享你的成功故事。
