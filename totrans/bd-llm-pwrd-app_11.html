<html><head></head><body>
<div class="calibre1" id="_idContainer206">
<h1 class="chapternumber"><span class="kobospan" id="kobo.1.1">11</span></h1>
<h1 class="chaptertitle" id="_idParaDest-155"><span class="kobospan" id="kobo.2.1">Fine-Tuning Large Language Models</span></h1>
<p class="normal"><span class="kobospan" id="kobo.3.1">Up to this point, we’ve explored</span><a id="_idIndexMarker812" class="calibre3"/><span class="kobospan" id="kobo.4.1"> the features and applications of </span><strong class="screentext"><span class="kobospan" id="kobo.5.1">large language models</span></strong><span class="kobospan" id="kobo.6.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.7.1">LLMs</span></strong><span class="kobospan" id="kobo.8.1">) in their “base” form, meaning that we consumed them with the parameters obtained from their base training. </span><span class="kobospan" id="kobo.8.2">We experimented with many scenarios in which, even in their base form, LLMs have been able to adapt to a variety of scenarios. </span><span class="kobospan" id="kobo.8.3">Nevertheless, there might be extremely domain-specific cases where a general-purpose LLM is not sufficient to fully embrace the taxonomy and knowledge of that domain. </span><span class="kobospan" id="kobo.8.4">If this is the case, you might want to fine-tune your model on your domain-specific data.</span></p>
<div class="note">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.9.1">Definition</span></strong></p>
<p class="normal1"><span class="kobospan" id="kobo.10.1">In the context of fine-tuning</span><a id="_idIndexMarker813" class="calibre3"/><span class="kobospan" id="kobo.11.1"> language models, “taxonomy” refers to a structured classification or categorization system that organizes concepts, terms, and entities according to their relationships and hierarchies within a specific domain. </span><span class="kobospan" id="kobo.11.2">This system is essential for making the model’s understanding and generation of content more relevant and accurate for specialized applications.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.12.1">A concrete example of taxonomy in a domain-specific sector is in the medical field. </span><span class="kobospan" id="kobo.12.2">Here, taxonomy could categorize information into structured groups like diseases, symptoms, treatments, and patient demographics. </span><span class="kobospan" id="kobo.12.3">For instance, in the “diseases” category, there might be subcategories for types of diseases like “cardiovascular diseases,” which could be further divided into more specific conditions such as “hypertension” and “coronary artery disease.” </span><span class="kobospan" id="kobo.12.4">This detailed categorization helps in fine-tuning language models to understand and generate more precise and contextually appropriate responses in medical consultations or documentation.</span></p>
</div>
<p class="normal1"><span class="kobospan" id="kobo.13.1">In this chapter, we are going to cover the technical details of fine-tuning LLMs, from the theory behind it to the hands-on implementation with Python and Hugging Face. </span><span class="kobospan" id="kobo.13.2">By the end of this chapter, you will be able to fine-tune an LLM on your own data, so that you can build domain-specific applications powered by those models.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.14.1">We will delve into the following topics:</span></p>
<ul class="calibre14">
<li class="bulletlist"><span class="kobospan" id="kobo.15.1">Introduction to fine-tuning</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.16.1">Understanding when you need fine-tuning</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.17.1">Preparing your data to fine-tune the model</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.18.1">Fine-tuning a base model on your data</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.19.1">Hosting strategies for your fine-tuned model</span></li>
</ul>
<h1 class="heading" id="_idParaDest-156"><span class="kobospan" id="kobo.20.1">Technical requirements</span></h1>
<p class="normal"><span class="kobospan" id="kobo.21.1">To complete the tasks in this chapter, you will need the following:</span></p>
<ul class="calibre14">
<li class="bulletlist"><span class="kobospan" id="kobo.22.1">A Hugging Face account and user access token.</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.23.1">Python 3.7.1 or later version.</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.24.1">Python packages: Make sure to have the following Python packages installed: </span><code class="inlinecode"><span class="kobospan" id="kobo.25.1">python-dotenv</span></code><span class="kobospan" id="kobo.26.1">, </span><code class="inlinecode"><span class="kobospan" id="kobo.27.1">huggingface_hub</span></code><span class="kobospan" id="kobo.28.1">, </span><code class="inlinecode"><span class="kobospan" id="kobo.29.1">accelerate&gt;=0.16.0</span></code><span class="kobospan" id="kobo.30.1">, </span><code class="inlinecode"><span class="kobospan" id="kobo.31.1">&lt;1 transformers[torch]</span></code><span class="kobospan" id="kobo.32.1">, </span><code class="inlinecode"><span class="kobospan" id="kobo.33.1">safetensors</span></code><span class="kobospan" id="kobo.34.1">, </span><code class="inlinecode"><span class="kobospan" id="kobo.35.1">tensorflow</span></code><span class="kobospan" id="kobo.36.1">, </span><code class="inlinecode"><span class="kobospan" id="kobo.37.1">datasets</span></code><span class="kobospan" id="kobo.38.1">, </span><code class="inlinecode"><span class="kobospan" id="kobo.39.1">evaluate</span></code><span class="kobospan" id="kobo.40.1">, and</span><code class="inlinecode"><span class="kobospan" id="kobo.41.1"> accelerate</span></code><span class="kobospan" id="kobo.42.1">. </span><span class="kobospan" id="kobo.42.2">Those can be easily installed via </span><code class="inlinecode"><span class="kobospan" id="kobo.43.1">pip install</span></code><span class="kobospan" id="kobo.44.1"> in your terminal. </span><span class="kobospan" id="kobo.44.2">If you want to install everything from the latest release, you can refer to the original GitHub by running </span><code class="inlinecode"><span class="kobospan" id="kobo.45.1">pip install git+https://github.com/huggingface/transformers.git</span></code><span class="kobospan" id="kobo.46.1"> in your terminal.</span></li>
</ul>
<p class="normal1"><span class="kobospan" id="kobo.47.1">You can find all the code and examples in the book’s GitHub repository at </span><a href="Chapter_11.xhtml" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.48.1">https://github.com/PacktPublishing/Building-LLM-Powered-Applications</span></span></a><span class="kobospan" id="kobo.49.1">.</span></p>
<h1 class="heading" id="_idParaDest-157"><span class="kobospan" id="kobo.50.1">What is fine-tuning?</span></h1>
<p class="normal"><span class="kobospan" id="kobo.51.1">Fine-tuning is a technique of </span><strong class="screentext"><span class="kobospan" id="kobo.52.1">transfer learning</span></strong><span class="kobospan" id="kobo.53.1"> in which the weights</span><a id="_idIndexMarker814" class="calibre3"/><span class="kobospan" id="kobo.54.1"> of a pretrained</span><a id="_idIndexMarker815" class="calibre3"/><span class="kobospan" id="kobo.55.1"> neural network are used as the initial values for training a new neural network on a different task. </span><span class="kobospan" id="kobo.55.2">This can improve the performance of the new network by leveraging the knowledge learned from the previous task, especially when the new task has limited data.</span></p>
<div class="note">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.56.1">Definition</span></strong></p>
<p class="normal1"><span class="kobospan" id="kobo.57.1">Transfer learning</span><a id="_idIndexMarker816" class="calibre3"/><span class="kobospan" id="kobo.58.1"> is a technique in machine learning that involves using the knowledge learned from one task to improve the performance on a related but different task. </span><span class="kobospan" id="kobo.58.2">For example, if you have a model that can recognize cars, you can use some of its features to help you recognize trucks. </span><span class="kobospan" id="kobo.58.3">Transfer learning can save you time and resources by reusing existing models instead of training new ones from scratch.</span></p>
</div>
<p class="normal1"><span class="kobospan" id="kobo.59.1">To better understand the concepts of transfer learning and fine-tuning, let’s consider the following example.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.60.1">Imagine you want to train a computer vision neural network to recognize different types of flowers, such as roses, sunflowers, and tulips. </span><span class="kobospan" id="kobo.60.2">You have a lot of photos of flowers, but not enough to train a model from scratch.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.61.1">Instead, you can use transfer learning, which means taking a model that was already trained on a different task and using some of its knowledge for your new task. </span><span class="kobospan" id="kobo.61.2">For example, you can take a model that was trained to recognize many vehicles, such as cars, trucks, and bicycles. </span><span class="kobospan" id="kobo.61.3">This model has learned how to extract features from images, such as edges, shapes, colors, and textures. </span><span class="kobospan" id="kobo.61.4">These features are useful for any image recognition task, not just the original one.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.62.1">You can use this model</span><a id="_idIndexMarker817" class="calibre3"/><span class="kobospan" id="kobo.63.1"> as a base for your flower recognition model. </span><span class="kobospan" id="kobo.63.2">You only need to add a new layer on top of it, which will learn how to classify the features into flower types. </span><span class="kobospan" id="kobo.63.3">This layer is called the classifier layer, and </span><a id="_idIndexMarker818" class="calibre3"/><span class="kobospan" id="kobo.64.1">it is needed for the model to adapt to the new task. </span><span class="kobospan" id="kobo.64.2">Training the classifier</span><a id="_idIndexMarker819" class="calibre3"/><span class="kobospan" id="kobo.65.1"> layer on top of the base model is a process called </span><strong class="screentext"><span class="kobospan" id="kobo.66.1">feature extraction</span></strong><span class="kobospan" id="kobo.67.1">. </span><span class="kobospan" id="kobo.67.2">Once this step is done, you can further tailor your model with fine-tuning by unfreezing some of the base model layers and training them together with the classifier layer. </span><span class="kobospan" id="kobo.67.3">This allows you to adjust the base model features to better suit your task.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.68.1">The following picture illustrates the computer vision model example:</span></p>
<figure class="mediaobject"><span class="kobospan" id="kobo.69.1"><img alt="" role="presentation" src="../Images/B21714_11_01.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.70.1">Figure 11.1: Example of transfer learning and fine-tuning</span></p>
<p class="normal1"><span class="kobospan" id="kobo.71.1">Fine-tuning is usually done after feature extraction, as a final step to improve the performance of the model. </span><span class="kobospan" id="kobo.71.2">You can decide how many layers to unfreeze based on your data size and complexity. </span><span class="kobospan" id="kobo.71.3">A common practice is to unfreeze the last few layers of the base model, which are more specific to the original task, and leave the first few layers frozen, which are more generic and reusable.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.72.1">To summarize, transfer learning</span><a id="_idIndexMarker820" class="calibre3"/><span class="kobospan" id="kobo.73.1"> and fine-tuning are techniques that allow you to use a pretrained model for a new task. </span><span class="kobospan" id="kobo.73.2">Transfer learning involves adding a new classifier layer on top of the base model and training only that layer. </span><span class="kobospan" id="kobo.73.3">Fine-tuning involves unfreezing some or all of the base model layers and training them together with the classifier layer.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.74.1">In the context of generative AI, fine-tuning</span><a id="_idIndexMarker821" class="calibre3"/><span class="kobospan" id="kobo.75.1"> is the process of adapting a pretrained language model to a specific task or domain by updating its parameters on a task-specific dataset. </span><span class="kobospan" id="kobo.75.2">Fine-tuning can improve the performance and accuracy of the model for the target task. </span><span class="kobospan" id="kobo.75.3">The steps involved in fine-tuning are:</span></p>
<ol class="calibre15">
<li class="bulletlist1" value="1"><strong class="screentext"><span class="kobospan" id="kobo.76.1">Load the pretrained language model and its tokenizer</span></strong><span class="kobospan" id="kobo.77.1">: The tokenizer is used to convert</span><a id="_idIndexMarker822" class="calibre3"/><span class="kobospan" id="kobo.78.1"> text into numerical tokens that the model can process. </span><span class="kobospan" id="kobo.78.2">Different models have unique architectures and requirements, often coming with their own specialized tokenizers designed to handle their specific input formats.</span></li>
</ol>
<p class="normal-one"><span class="kobospan" id="kobo.79.1">For instance, </span><strong class="screentext"><span class="kobospan" id="kobo.80.1">BERT</span></strong><span class="kobospan" id="kobo.81.1"> (which stands for </span><strong class="screentext"><span class="kobospan" id="kobo.82.1">Bidirectional Encoder Representations from Transformers</span></strong><span class="kobospan" id="kobo.83.1">) uses WordPiece</span><a id="_idIndexMarker823" class="calibre3"/><span class="kobospan" id="kobo.84.1"> tokenization, while GPT-2 employs </span><strong class="screentext"><span class="kobospan" id="kobo.85.1">byte-pair encoding</span></strong><span class="kobospan" id="kobo.86.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.87.1">BPE</span></strong><span class="kobospan" id="kobo.88.1">). </span><span class="kobospan" id="kobo.88.2">Models also impose</span><a id="_idIndexMarker824" class="calibre3"/><span class="kobospan" id="kobo.89.1"> token limits due to memory constraints during training and inference.</span></p>
<p class="normal-one"><span class="kobospan" id="kobo.90.1">These limits determine the maximum sequence length that a model can handle. </span><span class="kobospan" id="kobo.90.2">For example, BERT has a maximum token limit of 512 tokens, while the GPT-2 can handle longer sequences (e.g., up to 1,024 tokens).</span></p>
<ol class="calibre15">
<li class="bulletlist1" value="2"><strong class="screentext"><span class="kobospan" id="kobo.91.1">Prepare the task-specific dataset</span></strong><span class="kobospan" id="kobo.92.1">: The dataset should contain input-output pairs that are relevant to the task. </span><span class="kobospan" id="kobo.92.2">For example, for sentiment analysis, the input could be a text review and the output could be a sentiment label (positive, negative, or neutral).</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.93.1">Define the task-specific head</span></strong><span class="kobospan" id="kobo.94.1">: The head is a layer or a set of layers that are added on top of the pretrained model to perform the task. </span><span class="kobospan" id="kobo.94.2">The head should match the output format and size of the task. </span><span class="kobospan" id="kobo.94.3">For example, for sentiment analysis, the head could be a linear layer with three output units corresponding to the three sentiment labels.</span></li>
</ol>
<div class="note-one">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.95.1">Note</span></strong></p>
<p class="normal1"><span class="kobospan" id="kobo.96.1">When dealing with an LLM specifically designed for text generation, the architecture differs from models used for classification or other tasks. </span><span class="kobospan" id="kobo.96.2">In fact, unlike classification tasks, where we predict labels, an LLM predicts the next word or token in a sequence. </span><span class="kobospan" id="kobo.96.3">This layer is added on top of the pretrained transformer-based models with the purpose of transforming the contextualized hidden representations from the base model into probabilities over the vocabulary.</span></p>
</div>
<ol class="calibre15">
<li class="bulletlist1" value="4"><strong class="screentext"><span class="kobospan" id="kobo.97.1">Train the model on the task-specific dataset</span></strong><span class="kobospan" id="kobo.98.1">: The training process involves feeding the input tokens to the model, computing the loss between the model output and the true output, and updating the model parameters using an optimizer. </span><span class="kobospan" id="kobo.98.2">The training can be done for a fixed number of epochs or until a certain criterion is met.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.99.1">Evaluate the model on a test or validation set</span></strong><span class="kobospan" id="kobo.100.1">:</span><strong class="screentext"> </strong><span class="kobospan" id="kobo.101.1">The evaluation process involves measuring the performance of the model on unseen data using appropriate metrics. </span><span class="kobospan" id="kobo.101.2">For example, for sentiment analysis, the metric could be accuracy or F1-score (which will be discussed later in this chapter). </span><span class="kobospan" id="kobo.101.3">The evaluation results can</span><a id="_idIndexMarker825" class="calibre3"/><span class="kobospan" id="kobo.102.1"> be used to compare different models or fine-tuning strategies.</span></li>
</ol>
<p class="normal1"><span class="kobospan" id="kobo.103.1">Even though it is less computationally and time expensive than full training, fine-tuning an LLM is not a “light” activity. </span><span class="kobospan" id="kobo.103.2">As LLMs are, by definition, large, their fine-tuning has hardware requirements as well as data collection and preprocessing.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.104.1">So, the first question that you want to ask yourself while approaching a given scenario is: “Do I really need to finetune my LLM?”</span></p>
<h1 class="heading" id="_idParaDest-158"><span class="kobospan" id="kobo.105.1">When is fine-tuning necessary?</span></h1>
<p class="normal"><span class="kobospan" id="kobo.106.1">As we saw in previous chapters, good prompt engineering combined with the non-parametric knowledge you can add to your model via embeddings are exceptional techniques to customize your LLM, and they can account for around 90% of use cases. </span><span class="kobospan" id="kobo.106.2">However, the preceding affirmation tends to hold for the state-of-the-art models, such as GPT-4, Llama 2, and PaLM 2. </span><span class="kobospan" id="kobo.106.3">As discussed, those models have a huge number of parameters that make them heavy, hence the need for computational power; plus, they might be proprietary and subject to a pay-per-use cost.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.107.1">Henceforth, fine-tuning might also be useful when you want to leverage a light and free-of-charge LLM, such as the Falcon LLM 7B, yet you want it to perform as well as a SOTA model in your specific task.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.108.1">Some examples</span><a id="_idIndexMarker826" class="calibre3"/><span class="kobospan" id="kobo.109.1"> of when fine-tuning might be necessary are:</span></p>
<ul class="calibre14">
<li class="bulletlist"><span class="kobospan" id="kobo.110.1">When you want to use an LLM for sentiment analysis on movie reviews, but the LLM was pretrained on Wikipedia articles and books. </span><span class="kobospan" id="kobo.110.2">Fine-tuning can help the LLM learn the vocabulary, style, and tone of movie reviews, as well as the relevant features for sentiment classification.</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.111.1">When you want to use an LLM for text summarization on news articles, but the LLM was pretrained on a language modeling objective. </span><span class="kobospan" id="kobo.111.2">Fine-tuning can help the LLM learn the structure, content, and length of summaries, as well as the generation objective and evaluation metrics.</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.112.1">When you want to use an LLM for machine translation between two languages, but the LLM was pretrained on a multilingual corpus that does not include those languages. </span><span class="kobospan" id="kobo.112.2">Fine-tuning can help the LLM learn the vocabulary, grammar, and syntax of the target languages, as well as the translation objective and alignment methods.</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.113.1">When you want</span><a id="_idIndexMarker827" class="calibre3"/><span class="kobospan" id="kobo.114.1"> to use an LLM to perform complex </span><strong class="screentext"><span class="kobospan" id="kobo.115.1">named entity recognition</span></strong><span class="kobospan" id="kobo.116.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.117.1">NER</span></strong><span class="kobospan" id="kobo.118.1">) tasks. </span><span class="kobospan" id="kobo.118.2">For example, financial and legal documents contain specialized terminology and entities that are not typically prioritized in general language models, henceforth a fine-tuning process might be extremely beneficial</span><a id="_idIndexMarker828" class="calibre3"/><span class="kobospan" id="kobo.119.1"> here.</span></li>
</ul>
<p class="normal1"><span class="kobospan" id="kobo.120.1">In this chapter, we will be covering a full-code approach leveraging Hugging Face models and libraries. </span><span class="kobospan" id="kobo.120.2">However, be aware</span><a id="_idIndexMarker829" class="calibre3"/><span class="kobospan" id="kobo.121.1"> that Hugging Face also offers a low-code platform</span><a id="_idIndexMarker830" class="calibre3"/><span class="kobospan" id="kobo.122.1"> called AutoTrain (you can read more about that at </span><a href="https://huggingface.co/autotrain" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.123.1">https://huggingface.co/autotrain</span></span></a><span class="kobospan" id="kobo.124.1">), which might be a good alternative if your organization is more oriented towards low-code strategies.</span></p>
<h1 class="heading" id="_idParaDest-159"><span class="kobospan" id="kobo.125.1">Getting started with fine-tuning</span></h1>
<p class="normal"><span class="kobospan" id="kobo.126.1">In this section, we are going</span><a id="_idIndexMarker831" class="calibre3"/><span class="kobospan" id="kobo.127.1"> to cover all the steps needed to fine-tune an LLM with a full-code approach. </span><span class="kobospan" id="kobo.127.2">We will be leveraging Hugging Face libraries, such as </span><code class="inlinecode"><span class="kobospan" id="kobo.128.1">datasets</span></code><span class="kobospan" id="kobo.129.1"> (to load data from the Hugging Face datasets ecosystem) and </span><code class="inlinecode"><span class="kobospan" id="kobo.130.1">tokenizers</span></code><span class="kobospan" id="kobo.131.1"> (to provide an implementation of the most popular tokenizers). </span><span class="kobospan" id="kobo.131.2">The scenario we are going to address is a sentiment analysis task. </span><span class="kobospan" id="kobo.131.3">Our goal is to fine-tune a model to make it an expert binary classifier of emotions, clustered into “positive” and “negative.”</span></p>
<h2 class="heading1" id="_idParaDest-160"><span class="kobospan" id="kobo.132.1">Obtaining the dataset</span></h2>
<p class="normal"><span class="kobospan" id="kobo.133.1">The first ingredient</span><a id="_idIndexMarker832" class="calibre3"/><span class="kobospan" id="kobo.134.1"> that we need is the training dataset. </span><span class="kobospan" id="kobo.134.2">For this purpose, I will leverage the datasets library available in Hugging Face to load a binary classification</span><a id="_idIndexMarker833" class="calibre3"/><span class="kobospan" id="kobo.135.1"> dataset called IMDB (you can find the dataset card at </span><a href="https://huggingface.co/datasets/imdb" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.136.1">https://huggingface.co/datasets/imdb</span></span></a><span class="kobospan" id="kobo.137.1">).</span></p>
<p class="normal1"><span class="kobospan" id="kobo.138.1">The dataset contains movie reviews, which are classified as positive or negative. </span><span class="kobospan" id="kobo.138.2">More specifically, the dataset contains two columns:</span></p>
<ul class="calibre14">
<li class="bulletlist"><span class="kobospan" id="kobo.139.1">Text: The raw text movie review.</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.140.1">Label: The sentiment of that review. </span><span class="kobospan" id="kobo.140.2">It is mapped as “0” for “Negative” and “1” for “Positive.”</span></li>
</ul>
<p class="normal1"><span class="kobospan" id="kobo.141.1">As it is a </span><strong class="screentext"><span class="kobospan" id="kobo.142.1">supervised learning</span></strong><span class="kobospan" id="kobo.143.1"> problem, the dataset</span><a id="_idIndexMarker834" class="calibre3"/><span class="kobospan" id="kobo.144.1"> already comes with 25,000 rows for the training set and 25,000 rows for the validation set.</span></p>
<div class="note">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.145.1">Definition</span></strong></p>
<p class="normal1"><span class="kobospan" id="kobo.146.1">Supervised learning is a type of machine learning that uses labeled datasets to train algorithms to classify data or predict outcomes accurately. </span><span class="kobospan" id="kobo.146.2">Labeled datasets are collections</span><a id="_idIndexMarker835" class="calibre3"/><span class="kobospan" id="kobo.147.1"> of examples that have both input</span><a id="_idIndexMarker836" class="calibre3"/><span class="kobospan" id="kobo.148.1"> features and desired output values, also known as labels or targets. </span><span class="kobospan" id="kobo.148.2">For example, a labeled dataset for handwriting recognition might have images of handwritten digits as input features and the corresponding numerical values as labels.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.149.1">Training and validation sets are subsets of the labeled dataset that are used for different purposes in the supervised learning process. </span><span class="kobospan" id="kobo.149.2">The training set is used to fit the parameters of the model, such as the weights of the connections in a neural network. </span><span class="kobospan" id="kobo.149.3">The validation set is used to tune the hyperparameters of the model, such as the number of hidden units in a neural network or the learning rate. </span><span class="kobospan" id="kobo.149.4">Hyperparameters are settings that affect the overall behavior and performance of the model but are not directly learned from the data. </span><span class="kobospan" id="kobo.149.5">The validation set helps to select the best model among different candidates by comparing their accuracy or other metrics on the validation set.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.150.1">Supervised learning differs from another type</span><a id="_idIndexMarker837" class="calibre3"/><span class="kobospan" id="kobo.151.1"> of machine learning, which is </span><strong class="screentext"><span class="kobospan" id="kobo.152.1">unsupervised learning</span></strong><span class="kobospan" id="kobo.153.1">. </span><span class="kobospan" id="kobo.153.2">With the latter, the algorithm is tasked with finding patterns, structures, or relationships in a dataset without the presence of labeled outputs or targets. </span><span class="kobospan" id="kobo.153.3">In other words, in unsupervised learning, the algorithm is not provided with specific guidance or labels to direct its learning process. </span><span class="kobospan" id="kobo.153.4">Instead, it explores the data and identifies inherent patterns or groupings on its own.</span></p>
</div>
<p class="normal1"><span class="kobospan" id="kobo.154.1">You can download the IMDB dataset by running the following code:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="hljs-keyword"><span class="kobospan" id="kobo.155.1">from</span></span><span class="kobospan" id="kobo.156.1"> datasets </span><span class="hljs-keyword"><span class="kobospan" id="kobo.157.1">import</span></span><span class="kobospan" id="kobo.158.1"> load_dataset
dataset = load_dataset(</span><span class="hljs-string"><span class="kobospan" id="kobo.159.1">"imdb"</span></span><span class="kobospan" id="kobo.160.1">)
dataset
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.161.1">Hugging Face datasets come with a dictionary schema, which is as follows:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="kobospan" id="kobo.162.1">DatasetDict({
    train: Dataset({
        features: [</span><span class="hljs-string"><span class="kobospan" id="kobo.163.1">'text'</span></span><span class="kobospan" id="kobo.164.1">, </span><span class="hljs-string"><span class="kobospan" id="kobo.165.1">'label'</span></span><span class="kobospan" id="kobo.166.1">],
        num_rows: </span><span class="hljs-number"><span class="kobospan" id="kobo.167.1">25000</span></span><span class="kobospan" id="kobo.168.1">
    })
    test: Dataset({
        features: [</span><span class="hljs-string"><span class="kobospan" id="kobo.169.1">'text'</span></span><span class="kobospan" id="kobo.170.1">, </span><span class="hljs-string"><span class="kobospan" id="kobo.171.1">'</span></span><span class="hljs-string"><span class="kobospan" id="kobo.172.1">label'</span></span><span class="kobospan" id="kobo.173.1">],
        num_rows: </span><span class="hljs-number"><span class="kobospan" id="kobo.174.1">25000</span></span><span class="kobospan" id="kobo.175.1">
    })
    unsupervised: Dataset({
        features: [</span><span class="hljs-string"><span class="kobospan" id="kobo.176.1">'text'</span></span><span class="kobospan" id="kobo.177.1">, </span><span class="hljs-string"><span class="kobospan" id="kobo.178.1">'label'</span></span><span class="kobospan" id="kobo.179.1">],
        num_rows: </span><span class="hljs-number"><span class="kobospan" id="kobo.180.1">50000</span></span><span class="kobospan" id="kobo.181.1">
    })
})
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.182.1">To access one observation of a particular Dataset object (for example, </span><code class="inlinecode"><span class="kobospan" id="kobo.183.1">train</span></code><span class="kobospan" id="kobo.184.1">), you can use slicers, as follows:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="kobospan" id="kobo.185.1">dataset[</span><span class="hljs-string"><span class="kobospan" id="kobo.186.1">"train"</span></span><span class="kobospan" id="kobo.187.1">][</span><span class="hljs-number"><span class="kobospan" id="kobo.188.1">100</span></span><span class="kobospan" id="kobo.189.1">]
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.190.1">This gives us the following output:</span></p>
<pre class="programlisting1"><code class="hljs-con"><span class="kobospan" id="kobo.191.1">{'text': "Terrible movie. </span><span class="kobospan" id="kobo.191.2">Nuff Said.[…]
 'label': 0}
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.192.1">So, the 101st observation of the training set contains a review labeled as negative.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.193.1">Now that we have</span><a id="_idIndexMarker838" class="calibre3"/><span class="kobospan" id="kobo.194.1"> the dataset, we need to preprocess it so that can be used to train our LLM. </span><span class="kobospan" id="kobo.194.2">To do so, we need to tokenize the provided text, and we will discuss this in the next section.</span></p>
<h2 class="heading1" id="_idParaDest-161"><span class="kobospan" id="kobo.195.1">Tokenizing the data</span></h2>
<p class="normal"><span class="kobospan" id="kobo.196.1">A tokenizer is a component</span><a id="_idIndexMarker839" class="calibre3"/><span class="kobospan" id="kobo.197.1"> that is responsible</span><a id="_idIndexMarker840" class="calibre3"/><span class="kobospan" id="kobo.198.1"> for splitting a text into smaller units, such as words or subwords, that can be used as inputs for an LLM. </span><span class="kobospan" id="kobo.198.2">Tokenizers can be used to encode text efficiently and consistently, as well as to add special tokens, such as mask or separator tokens, that are required by some models.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.199.1">Hugging Face provides</span><a id="_idIndexMarker841" class="calibre3"/><span class="kobospan" id="kobo.200.1"> a powerful utility called AutoTokenizer, available in the Hugging Face Transformers library, that offers tokenizers for various models, such as BERT and GPT-2. </span><span class="kobospan" id="kobo.200.2">It serves as a generic tokenizer class that dynamically selects and instantiates the appropriate tokenizer based on the pretrained model you specify.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.201.1">The following code snippet shows how we can initialize our tokenizer:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="hljs-keyword"><span class="kobospan" id="kobo.202.1">from</span></span><span class="kobospan" id="kobo.203.1"> transformers </span><span class="hljs-keyword"><span class="kobospan" id="kobo.204.1">import</span></span><span class="kobospan" id="kobo.205.1"> AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(</span><span class="hljs-string"><span class="kobospan" id="kobo.206.1">"bert-base-cased"</span></span><span class="kobospan" id="kobo.207.1">)
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.208.1">Note that we picked a specific</span><a id="_idIndexMarker842" class="calibre3"/><span class="kobospan" id="kobo.209.1"> tokenizer called </span><code class="inlinecode"><span class="kobospan" id="kobo.210.1">bert-base-cased</span></code><span class="kobospan" id="kobo.211.1">. </span><span class="kobospan" id="kobo.211.2">In fact, there</span><a id="_idIndexMarker843" class="calibre3"/><span class="kobospan" id="kobo.212.1"> is a link between a tokenizer and an LLM, in the sense that the the tokenizer prepares the inputs for the model by converting the text into numerical IDs that the model can understand.</span></p>
<div class="note">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.213.1">Definition</span></strong></p>
<p class="normal1"><span class="kobospan" id="kobo.214.1">The input IDs are the numerical IDs that</span><a id="_idIndexMarker844" class="calibre3"/><span class="kobospan" id="kobo.215.1"> correspond to the tokens in the vocabulary of the tokenizer. </span><span class="kobospan" id="kobo.215.2">They are returned by the tokenizer function when encoding a text input. </span><span class="kobospan" id="kobo.215.3">The input IDs are used as inputs for the model, which expects numerical tensors rather than strings. </span><span class="kobospan" id="kobo.215.4">Different tokenizers may have different input IDs for the same tokens, depending on their vocabulary and tokenization algorithm.</span></p>
</div>
<p class="normal1"><span class="kobospan" id="kobo.216.1">Different models may use different tokenization algorithms, such as word-based, character-based, or subword-based. </span><span class="kobospan" id="kobo.216.2">Therefore, it is important to use the correct tokenizer for each model, otherwise the model may not perform well or even produce errors. </span><span class="kobospan" id="kobo.216.3">Let’s look at potential scenarios for each:</span></p>
<ul class="calibre14">
<li class="bulletlist"><span class="kobospan" id="kobo.217.1">A character-based approach might fit scenarios</span><a id="_idIndexMarker845" class="calibre3"/><span class="kobospan" id="kobo.218.1"> that deal with rare words or languages with complex morphological structures, or when dealing with spelling correction tasks</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.219.1">The word-based approach might</span><a id="_idIndexMarker846" class="calibre3"/><span class="kobospan" id="kobo.220.1"> be a good fit for scenarios like NER, sentiment analysis, and text classification</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.221.1">The sub-word approach interpolates</span><a id="_idIndexMarker847" class="calibre3"/><span class="kobospan" id="kobo.222.1"> between the previous two, and it is useful when we want to balance the granularity of text representation with efficiency.</span></li>
</ul>
<p class="normal1"><span class="kobospan" id="kobo.223.1">As we will see in the next</span><a id="_idIndexMarker848" class="calibre3"/><span class="kobospan" id="kobo.224.1"> section, we will leverage the </span><strong class="screentext"><span class="kobospan" id="kobo.225.1">BERT</span></strong><span class="kobospan" id="kobo.226.1"> model for this scenario, hence we loaded its pretrained </span><a id="_idIndexMarker849" class="calibre3"/><span class="kobospan" id="kobo.227.1">tokenizer (which is a word-based tokenizer powered by an algorithm called WordPiece).</span></p>
<p class="normal1"><span class="kobospan" id="kobo.228.1">We now need to initialize </span><code class="inlinecode"><span class="kobospan" id="kobo.229.1">tokenize_function</span></code><span class="kobospan" id="kobo.230.1">, which will be used to format the dataset:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="hljs-keyword"><span class="kobospan" id="kobo.231.1">def</span></span> <span class="hljs-title"><span class="kobospan" id="kobo.232.1">tokenize_function</span></span><span class="kobospan" id="kobo.233.1">(</span><span><span class="kobospan" id="kobo.234.1">examples</span></span><span class="kobospan" id="kobo.235.1">):
    </span><span class="hljs-keyword"><span class="kobospan" id="kobo.236.1">return</span></span><span class="kobospan" id="kobo.237.1"> tokenizer(examples[</span><span class="hljs-string"><span class="kobospan" id="kobo.238.1">"text"</span></span><span class="kobospan" id="kobo.239.1">], padding = </span><span class="hljs-string"><span class="kobospan" id="kobo.240.1">"max_length"</span></span><span class="kobospan" id="kobo.241.1">, truncation=</span><span class="hljs-literal"><span class="kobospan" id="kobo.242.1">True</span></span><span class="kobospan" id="kobo.243.1">)
tokenized_datasets = dataset.</span><span class="hljs-built_in"><span class="kobospan" id="kobo.244.1">map</span></span><span class="kobospan" id="kobo.245.1">(tokenize_function, batched=</span><span class="hljs-literal"><span class="kobospan" id="kobo.246.1">True</span></span><span class="kobospan" id="kobo.247.1">)
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.248.1">As you can</span><a id="_idIndexMarker850" class="calibre3"/><span class="kobospan" id="kobo.249.1"> see, we also</span><a id="_idIndexMarker851" class="calibre3"/><span class="kobospan" id="kobo.250.1"> configured the </span><strong class="screentext"><span class="kobospan" id="kobo.251.1">padding</span></strong><span class="kobospan" id="kobo.252.1"> and </span><strong class="screentext"><span class="kobospan" id="kobo.253.1">truncation</span></strong><span class="kobospan" id="kobo.254.1"> of </span><code class="inlinecode"><span class="kobospan" id="kobo.255.1">tokenize_function</span></code><span class="kobospan" id="kobo.256.1"> to ensure an output</span><a id="_idIndexMarker852" class="calibre3"/><span class="kobospan" id="kobo.257.1"> with the right</span><a id="_idIndexMarker853" class="calibre3"/><span class="kobospan" id="kobo.258.1"> sizing for our BERT model.</span></p>
<div class="note">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.259.1">Definition</span></strong></p>
<p class="normal1"><span class="kobospan" id="kobo.260.1">Padding and truncation are two techniques that are used to make the input sequences of text</span><a id="_idIndexMarker854" class="calibre3"/><span class="kobospan" id="kobo.261.1"> have the same length. </span><span class="kobospan" id="kobo.261.2">This is often required for some </span><strong class="screentext"><span class="kobospan" id="kobo.262.1">natural language processing</span></strong><span class="kobospan" id="kobo.263.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.264.1">NLP</span></strong><span class="kobospan" id="kobo.265.1">) models, such as the BERT model, that expect fixed-length inputs.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.266.1">Padding means adding some special tokens, usually zeros, at the end or the beginning of a sequence to make it reach the desired length. </span><span class="kobospan" id="kobo.266.2">For example, if we have a sequence of length 5 and we want to pad it to a length of 8, we can add 3 zeros at the end, like this: [1, 2, 3, 4, 5, 0, 0, 0]. </span><span class="kobospan" id="kobo.266.3">This</span><a id="_idIndexMarker855" class="calibre3"/><span class="kobospan" id="kobo.267.1"> is called post-padding. </span><span class="kobospan" id="kobo.267.2">Alternatively, we can add 3 zeros</span><a id="_idIndexMarker856" class="calibre3"/><span class="kobospan" id="kobo.268.1"> at the beginning, like this: [0, 0, 0, 1, 2, 3, 4, 5]. </span><span class="kobospan" id="kobo.268.2">This is called pre-padding. </span><span class="kobospan" id="kobo.268.3">The choice of padding strategy depends on the model and the task.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.269.1">Truncation means removing some tokens from a sequence to make it fit the desired length. </span><span class="kobospan" id="kobo.269.2">For example, if we have a sequence of length 10 and we want to truncate it to a length of 8, we can remove 2 tokens from the end or the beginning of the sequence. </span><span class="kobospan" id="kobo.269.3">For example, we can</span><a id="_idIndexMarker857" class="calibre3"/><span class="kobospan" id="kobo.270.1"> remove the last 2 tokens, like this: [1, 2, 3, 4, 5, 6, 7, 8]. </span><span class="kobospan" id="kobo.270.2">This is called post-truncation. </span><span class="kobospan" id="kobo.270.3">Alternatively, we can remove the first 2 tokens, like this: [3, 4, 5, 6, 7, 8, 9, 10]. </span><span class="kobospan" id="kobo.270.4">This</span><a id="_idIndexMarker858" class="calibre3"/><span class="kobospan" id="kobo.271.1"> is called pre-truncation. </span><span class="kobospan" id="kobo.271.2">The choice of truncation strategy also depends on the model and the task.</span></p>
</div>
<p class="normal1"><span class="kobospan" id="kobo.272.1">Now, we can apply the function to our dataset and inspect the numerical IDs of one entry:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="kobospan" id="kobo.273.1">tokenized_datasets = dataset.</span><span class="hljs-built_in"><span class="kobospan" id="kobo.274.1">map</span></span><span class="kobospan" id="kobo.275.1">(tokenize_function, batched=</span><span class="hljs-literal"><span class="kobospan" id="kobo.276.1">True</span></span><span class="kobospan" id="kobo.277.1">)
tokenized_datasets[</span><span class="hljs-string"><span class="kobospan" id="kobo.278.1">'train'</span></span><span class="kobospan" id="kobo.279.1">][</span><span class="hljs-number"><span class="kobospan" id="kobo.280.1">100</span></span><span class="kobospan" id="kobo.281.1">][</span><span class="hljs-string"><span class="kobospan" id="kobo.282.1">'input_ids'</span></span><span class="kobospan" id="kobo.283.1">]
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.284.1">Here is our output:</span></p>
<pre class="programlisting1"><code class="hljs-con"><span class="kobospan" id="kobo.285.1">[101,
 12008,
 27788,
...
 </span><span class="kobospan" id="kobo.285.2">0,
 0,
 0,
 0,
 0]
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.286.1">As you can see, the last elements of the vector are zeroes, due to the </span><code class="inlinecode"><span class="kobospan" id="kobo.287.1">padding='max_lenght'</span></code><span class="kobospan" id="kobo.288.1"> parameter passed to the function.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.289.1">Optionally, you can decide to reduce the size of your dataset if you want to make the training time shorter. </span><span class="kobospan" id="kobo.289.2">In my case, I’ve shrunk the dataset as follows:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="kobospan" id="kobo.290.1">small_train_dataset = tokenized_datasets[</span><span class="hljs-string"><span class="kobospan" id="kobo.291.1">"train"</span></span><span class="kobospan" id="kobo.292.1">].shuffle(seed=</span><span class="hljs-number"><span class="kobospan" id="kobo.293.1">42</span></span><span class="kobospan" id="kobo.294.1">).select(</span><span class="hljs-built_in"><span class="kobospan" id="kobo.295.1">range</span></span><span class="kobospan" id="kobo.296.1">(</span><span class="hljs-number"><span class="kobospan" id="kobo.297.1">500</span></span><span class="kobospan" id="kobo.298.1">))
small_eval_dataset = tokenized_datasets[</span><span class="hljs-string"><span class="kobospan" id="kobo.299.1">"test"</span></span><span class="kobospan" id="kobo.300.1">].shuffle(seed=</span><span class="hljs-number"><span class="kobospan" id="kobo.301.1">42</span></span><span class="kobospan" id="kobo.302.1">).select(</span><span class="hljs-built_in"><span class="kobospan" id="kobo.303.1">range</span></span><span class="kobospan" id="kobo.304.1">(</span><span class="hljs-number"><span class="kobospan" id="kobo.305.1">500</span></span><span class="kobospan" id="kobo.306.1">))
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.307.1">So, I have two </span><a id="_idIndexMarker859" class="calibre3"/><span class="kobospan" id="kobo.308.1">sets – one for</span><a id="_idIndexMarker860" class="calibre3"/><span class="kobospan" id="kobo.309.1"> training, one for testing – of 500 observations each. </span><span class="kobospan" id="kobo.309.2">Now that we have our dataset preprocessed and ready, we need the model to be fine-tuned.</span></p>
<h2 class="heading1" id="_idParaDest-162"><span class="kobospan" id="kobo.310.1">Fine-tuning the model</span></h2>
<p class="normal"><span class="kobospan" id="kobo.311.1">As anticipated in the previous</span><a id="_idIndexMarker861" class="calibre3"/><span class="kobospan" id="kobo.312.1"> section, the LLM</span><a id="_idIndexMarker862" class="calibre3"/><span class="kobospan" id="kobo.313.1"> we are going to leverage for fine-tuning is the base version of BERT. </span><span class="kobospan" id="kobo.313.2">The BERT model is a transformer-based, encoder-only model for natural language understanding introduced by Google researchers in 2018. </span><span class="kobospan" id="kobo.313.3">BERT was the first example of a general-purpose LLM, meaning that it was the first model to be able to tackle multiple NLP tasks at once, which was different from the task-specific models existing up to that moment.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.314.1">Now, even though it might sound a bit “old fashioned” (in fact, compared to today’s model like the GPT-4, it is not even “large,” with only 340 million parameters in its large version), given all the new LLMs that have emerged in the market in the last few months, BERT and its fine-tuned variants are still a widely adopted architecture. </span><span class="kobospan" id="kobo.314.2">In fact, it was thanks to BERT</span><a id="_idIndexMarker863" class="calibre3"/><span class="kobospan" id="kobo.315.1"> that the standard for language models has greatly</span><a id="_idIndexMarker864" class="calibre3"/><span class="kobospan" id="kobo.316.1"> improved.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.317.1">The BERT model has two main components:</span></p>
<ul class="calibre14">
<li class="bulletlist"><span class="kobospan" id="kobo.318.1">Encoder: The encoder consists of multiple</span><a id="_idIndexMarker865" class="calibre3"/><span class="kobospan" id="kobo.319.1"> layers of transformer blocks, each with a self-attention layer and a feedforward layer. </span><span class="kobospan" id="kobo.319.2">The encoder takes as input a sequence of tokens, which are the basic units of text, and outputs a sequence of hidden states, which are high-dimensional vectors that represent the semantic information of each token.</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.320.1">Output layer: The output layer is task-specific</span><a id="_idIndexMarker866" class="calibre3"/><span class="kobospan" id="kobo.321.1"> and can be different depending on the type of task that BERT is used for. </span><span class="kobospan" id="kobo.321.2">For example, for text classification, the output layer can be a linear layer that predicts the class label of the input text. </span><span class="kobospan" id="kobo.321.3">For question answering, the output layer can be two linear layers that predict the start and end positions of the answer span in the input text.</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.322.1">The number of layers and parameters of the model depends on the model version. </span><span class="kobospan" id="kobo.322.2">In fact, BERT comes in two sizes: BERTbase and BERTlarge. </span><span class="kobospan" id="kobo.322.3">The following illustration shows the difference between the two versions:</span></li>
</ul>
<figure class="mediaobject"><span class="kobospan" id="kobo.323.1"><img alt="A screenshot of a computer  Description automatically generated" src="../Images/B21714_11_02.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.324.1">Figure 11.2: A comparison between BERTbase and BERTlarge (source: </span><a href="https://huggingface.co/blog/bert-101" class="calibre3"><span class="kobospan" id="kobo.325.1">https://huggingface.co/blog/bert-101</span></a><span class="kobospan" id="kobo.326.1">)</span></p>
<p class="normal1"><span class="kobospan" id="kobo.327.1">Later, other versions such as BERT-tiny, BERT-mini, BERT-small, and BERT-medium were introduced to reduce the computational cost and memory usage of BERT.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.328.1">The model has been trained on a heterogeneous corpus of around 3.3 billion words, belonging to Wikipedia and Google’s BooksCorpus. </span><span class="kobospan" id="kobo.328.2">The training phase involved two objectives:</span></p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.329.1">Masked language modeling</span></strong><span class="kobospan" id="kobo.330.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.331.1">MLM</span></strong><span class="kobospan" id="kobo.332.1">): MLM aims to teach the model to predict the original</span><a id="_idIndexMarker867" class="calibre3"/><span class="kobospan" id="kobo.333.1"> words that are randomly masked (replaced with a special token) in the input text. </span><span class="kobospan" id="kobo.333.2">For example, given the sentence “He bought a new [MASK] yesterday,” the model should predict the word “car” or “bike” or something else that makes sense. </span><span class="kobospan" id="kobo.333.3">This objective helps the model learn the vocabulary and the syntax of the language, as well as the semantic and contextual relations between words.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.334.1">Next sentence prediction</span></strong><span class="kobospan" id="kobo.335.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.336.1">NSP</span></strong><span class="kobospan" id="kobo.337.1">): NSP aims to teach</span><a id="_idIndexMarker868" class="calibre3"/><span class="kobospan" id="kobo.338.1"> the model to predict whether two sentences are consecutive or not in the original text. </span><span class="kobospan" id="kobo.338.2">For example, given the sentences “She loves reading books” and “Her favorite genre is fantasy,” the model should predict that they are consecutive because they are likely to appear together in a text. </span><span class="kobospan" id="kobo.338.3">However, given the sentences “She loves reading books” and “He plays soccer every weekend,” the model should predict that they are not consecutive because they are unlikely to be related. </span><span class="kobospan" id="kobo.338.4">This objective helps the model learn the coherence and logic of the text, as well as the discourse and pragmatic relations between sentences.</span></li>
</ul>
<p class="normal1"><span class="kobospan" id="kobo.339.1">By using these two </span><a id="_idIndexMarker869" class="calibre3"/><span class="kobospan" id="kobo.340.1">objectives (on which the model is trained at the same time), the BERT model</span><a id="_idIndexMarker870" class="calibre3"/><span class="kobospan" id="kobo.341.1"> can learn general language knowledge that can be transferred to specific tasks, such as text classification, question answering, and NER. </span><span class="kobospan" id="kobo.341.2">The BERT model can achieve better performance on these tasks than previous models that only use one</span><a id="_idIndexMarker871" class="calibre3"/><span class="kobospan" id="kobo.342.1"> direction of context</span><a id="_idIndexMarker872" class="calibre3"/><span class="kobospan" id="kobo.343.1"> or do not use pre-training at all. </span><span class="kobospan" id="kobo.343.2">In fact, it has achieved</span><a id="_idIndexMarker873" class="calibre3"/><span class="kobospan" id="kobo.344.1"> state-of-the-art results on many benchmarks and tasks, such as </span><strong class="screentext"><span class="kobospan" id="kobo.345.1">General Language Understanding Evaluation</span></strong><span class="kobospan" id="kobo.346.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.347.1">GLUE</span></strong><span class="kobospan" id="kobo.348.1">), </span><strong class="screentext"><span class="kobospan" id="kobo.349.1">Stanford Question Answering Dataset</span></strong><span class="kobospan" id="kobo.350.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.351.1">SQuAD</span></strong><span class="kobospan" id="kobo.352.1">), and </span><strong class="screentext"><span class="kobospan" id="kobo.353.1">Multi-Genre Natural Language Inference</span></strong><span class="kobospan" id="kobo.354.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.355.1">MultiNLI</span></strong><span class="kobospan" id="kobo.356.1">).</span></p>
<p class="normal1"><span class="kobospan" id="kobo.357.1">The BERT model is available – along with many fine-tuned versions –in the Hugging Face Hub. </span><span class="kobospan" id="kobo.357.2">You can instantiate the model as follows:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="hljs-keyword"><span class="kobospan" id="kobo.358.1">import</span></span><span class="kobospan" id="kobo.359.1"> torch
</span><span class="hljs-keyword"><span class="kobospan" id="kobo.360.1">from</span></span><span class="kobospan" id="kobo.361.1"> transformers </span><span class="hljs-keyword"><span class="kobospan" id="kobo.362.1">import</span></span><span class="kobospan" id="kobo.363.1"> AutoModelForSequenceClassification
model = AutoModelForSequenceClassification.from_pretrained(</span><span class="hljs-string"><span class="kobospan" id="kobo.364.1">"bert-base-cased"</span></span><span class="kobospan" id="kobo.365.1">, num_labels=</span><span class="hljs-number"><span class="kobospan" id="kobo.366.1">2</span></span><span class="kobospan" id="kobo.367.1">)
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.368.1">Note that </span><code class="inlinecode"><span class="kobospan" id="kobo.369.1">AutoModelForSequenceClassification</span></code><span class="kobospan" id="kobo.370.1"> is a subclass of </span><code class="inlinecode"><span class="kobospan" id="kobo.371.1">AutoModel</span></code><span class="kobospan" id="kobo.372.1">, which can instantiate a model architecture suitable for sequence classification, such as text classification or sentiment analysis. </span><span class="kobospan" id="kobo.372.2">It can be used for any task that requires a single label or a list of labels for each input sequence. </span><span class="kobospan" id="kobo.372.3">In my case, I set the number of output labels equal to two since we are dealing with a binary classification problem.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.373.1">On the other hand, </span><code class="inlinecode"><span class="kobospan" id="kobo.374.1">AutoModel</span></code><span class="kobospan" id="kobo.375.1"> is a generic class that can instantiate any model architecture from the library based on the pretrained model name or path. </span><span class="kobospan" id="kobo.375.2">It can be used for any task that does not require a specific </span><a id="_idIndexMarker874" class="calibre3"/><span class="kobospan" id="kobo.376.1">output format, such as feature extraction or language</span><a id="_idIndexMarker875" class="calibre3"/><span class="kobospan" id="kobo.377.1"> modeling.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.378.1">The final step before starting the training is to define the evaluation metrics we will need to understand how well our model will perform once fine-tuned.</span></p>
<h2 class="heading1" id="_idParaDest-163"><span class="kobospan" id="kobo.379.1">Using evaluation metrics</span></h2>
<p class="normal"><span class="kobospan" id="kobo.380.1">As we saw in </span><em class="italic"><span class="kobospan" id="kobo.381.1">Chapter 1</span></em><span class="kobospan" id="kobo.382.1">, evaluating an LLM</span><a id="_idIndexMarker876" class="calibre3"/><span class="kobospan" id="kobo.383.1"> in its general-purpose application</span><a id="_idIndexMarker877" class="calibre3"/><span class="kobospan" id="kobo.384.1"> might be cumbersome. </span><span class="kobospan" id="kobo.384.2">As those models are trained on unlabeled text and are not task-specific, but rather generic and adaptable given a user’s prompt, traditional evaluation metrics were not suitable anymore. </span><span class="kobospan" id="kobo.384.3">Evaluating an LLM means, among other things, measuring its language fluency, its coherence, and its ability to emulate different styles depending on a user’s request.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.385.1">However, we also saw how an LLM can be used for very specific scenarios, as in our binary classification task. </span><span class="kobospan" id="kobo.385.2">If this is the case, evaluation metrics boil down to those commonly used for that scenario.</span></p>
<div class="note">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.386.1">Note</span></strong></p>
<p class="normal1"><span class="kobospan" id="kobo.387.1">When it comes to more conversational tasks like summarization, Q&amp;A, and retrieval-augmented generation, a new set of evaluation metrics needs to be introduced, often powered in turn by LLMs. </span><span class="kobospan" id="kobo.387.2">Some of the most popular metrics are the following:</span></p>
<ul class="calibre14">
<li class="bulletlist"><span class="kobospan" id="kobo.388.1">Fluency: This assesses how naturally and smoothly the generated text reads.</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.389.1">Coherence: This evaluates the logical flow and connectivity of ideas within a text.</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.390.1">Relevance: This measures how well the generated content aligns with the given prompt or context.</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.391.1">GPT-similarity: This quantifies how closely the generated text resembles human-written content.</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.392.1">Groundedness: This assesses whether the generated text is based on factual information or context.</span></li>
</ul>
<p class="normal1"><span class="kobospan" id="kobo.393.1">These evaluation metrics help us understand the quality, naturalness, and relevance of LLM-generated text, guiding improvements and ensuring reliable AI assistance.</span></p>
</div>
<p class="normal1"><span class="kobospan" id="kobo.394.1">When it comes to binary classification, one of the most basic ways to evaluate a binary classifier is to use a confusion matrix. </span><span class="kobospan" id="kobo.394.2">A confusion matrix is a table that shows how many of the predicted labels match the true labels. </span><span class="kobospan" id="kobo.394.3">It has four cells:</span></p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.395.1">True positive</span></strong><span class="kobospan" id="kobo.396.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.397.1">TP</span></strong><span class="kobospan" id="kobo.398.1">): The number of cases where the classifier correctly predicted 1 when the true label was 1.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.399.1">False positive</span></strong><span class="kobospan" id="kobo.400.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.401.1">FP</span></strong><span class="kobospan" id="kobo.402.1">): The number of cases where the classifier incorrectly predicted 1 when the true label was 0.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.403.1">True negative</span></strong><span class="kobospan" id="kobo.404.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.405.1">TN</span></strong><span class="kobospan" id="kobo.406.1">): The number of cases where the classifier correctly predicted 0 when the true label was 0.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.407.1">False negative</span></strong><span class="kobospan" id="kobo.408.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.409.1">FN</span></strong><span class="kobospan" id="kobo.410.1">): The number of cases where the classifier incorrectly predicted 0 when the true label was 1.</span></li>
</ul>
<p class="normal1"><span class="kobospan" id="kobo.411.1">Here is an example</span><a id="_idIndexMarker878" class="calibre3"/><span class="kobospan" id="kobo.412.1"> of a confusion matrix for the sentiment classifier</span><a id="_idIndexMarker879" class="calibre3"/><span class="kobospan" id="kobo.413.1"> we are going to build, knowing that the label 0 is associated with “Negative” and the label 1 with “Positive”:</span></p>
<table class="table-container" id="table001-3">
<tbody class="calibre18">
<tr class="calibre19">
<td class="table-cell"/>
<td class="table-cell">
<p class="normal1"><strong class="keyword"><span class="kobospan2" id="kobo.414.1">Predicted Positive</span></strong></p>
</td>
<td class="table-cell">
<p class="normal1"><strong class="keyword"><span class="kobospan2" id="kobo.415.1">Predicted Negative</span></strong></p>
</td>
</tr>
<tr class="calibre19">
<td class="table-cell">
<p class="normal1"><strong class="keyword"><span class="kobospan2" id="kobo.416.1">Positive</span></strong></p>
</td>
<td class="table-cell">
<p class="normal1"><span class="kobospan2" id="kobo.417.1">20 (TP)</span></p>
</td>
<td class="table-cell">
<p class="normal1"><span class="kobospan2" id="kobo.418.1">5 (FN)</span></p>
</td>
</tr>
<tr class="calibre19">
<td class="table-cell">
<p class="normal1"><strong class="keyword"><span class="kobospan2" id="kobo.419.1">Negative</span></strong></p>
</td>
<td class="table-cell">
<p class="normal1"><span class="kobospan2" id="kobo.420.1">3 (FP)</span></p>
</td>
<td class="table-cell">
<p class="normal1"><span class="kobospan2" id="kobo.421.1">72 (TN)</span></p>
</td>
</tr>
</tbody>
</table>
<p class="normal1"><span class="kobospan" id="kobo.422.1">The confusion matrix can be used to calculate various metrics that measure different aspects of the classifier’s performance. </span><span class="kobospan" id="kobo.422.2">Some of the most common metrics are:</span></p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.423.1">Accuracy</span></strong><span class="kobospan" id="kobo.424.1">: The proportion of correct predictions among all predictions. </span><span class="kobospan" id="kobo.424.2">It is calculated as </span><code class="inlinecode"><span class="kobospan" id="kobo.425.1">(TP + TN) / (TP + FP + TN + FN)</span></code><span class="kobospan" id="kobo.426.1">. </span><span class="kobospan" id="kobo.426.2">For example, the accuracy of the sentiment classifier is </span><code class="inlinecode"><span class="kobospan" id="kobo.427.1">(20 + 72) / (20 + 3 + 72 + 5) = 0.92</span></code><span class="kobospan" id="kobo.428.1">.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.429.1">Precision</span></strong><span class="kobospan" id="kobo.430.1">: The proportion of correct positive predictions among all positive predictions. </span><span class="kobospan" id="kobo.430.2">It is calculated as </span><code class="inlinecode"><span class="kobospan" id="kobo.431.1">TP / (TP + FP)</span></code><span class="kobospan" id="kobo.432.1">. </span><span class="kobospan" id="kobo.432.2">For example, the precision of the sentiment classifier is </span><code class="inlinecode"><span class="kobospan" id="kobo.433.1">20 / (20 + 3) = 0.87</span></code><span class="kobospan" id="kobo.434.1">.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.435.1">Recall</span></strong><span class="kobospan" id="kobo.436.1">: The proportion of correct positive</span><a id="_idIndexMarker880" class="calibre3"/><span class="kobospan" id="kobo.437.1"> predictions among all positive cases. </span><span class="kobospan" id="kobo.437.2">It is also known as sensitivity or true positive rate. </span><span class="kobospan" id="kobo.437.3">It is calculated as </span><code class="inlinecode"><span class="kobospan" id="kobo.438.1">TP / (TP + FN)</span></code><span class="kobospan" id="kobo.439.1">. </span><span class="kobospan" id="kobo.439.2">For example, the recall of the sentiment classifier is </span><code class="inlinecode"><span class="kobospan" id="kobo.440.1">20 / (20 + 5) = 0.8</span></code><span class="kobospan" id="kobo.441.1">.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.442.1">Specificity</span></strong><span class="kobospan" id="kobo.443.1">: The proportion of correct negative</span><a id="_idIndexMarker881" class="calibre3"/><span class="kobospan" id="kobo.444.1"> predictions among all negative cases. </span><span class="kobospan" id="kobo.444.2">It is also known as the true negative rate. </span><span class="kobospan" id="kobo.444.3">It is calculated as </span><code class="inlinecode"><span class="kobospan" id="kobo.445.1">TN / (TN + FP)</span></code><span class="kobospan" id="kobo.446.1">. </span><span class="kobospan" id="kobo.446.2">For example, the specificity of the sentiment classifier is </span><code class="inlinecode"><span class="kobospan" id="kobo.447.1">72 / (72 + 3) = 0.96</span></code><span class="kobospan" id="kobo.448.1">.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.449.1">F1-score</span></strong><span class="kobospan" id="kobo.450.1">: The harmonic mean of precision and recall. </span><span class="kobospan" id="kobo.450.2">It is a measure of balance between precision and recall. </span><span class="kobospan" id="kobo.450.3">It is calculated as </span><code class="inlinecode"><span class="kobospan" id="kobo.451.1">2 * (precision * recall) / (precision + recall)</span></code><span class="kobospan" id="kobo.452.1">. </span><span class="kobospan" id="kobo.452.2">For example, the F1-score of the sentiment classifier is </span><code class="inlinecode"><span class="kobospan" id="kobo.453.1">2 * (0.87 * 0.8) / (0.87 + 0.8) = 0.83</span></code><span class="kobospan" id="kobo.454.1">.</span></li>
</ul>
<p class="normal1"><span class="kobospan" id="kobo.455.1">There are many other metrics</span><a id="_idIndexMarker882" class="calibre3"/><span class="kobospan" id="kobo.456.1"> that can be derived from the confusion matrix or other</span><a id="_idIndexMarker883" class="calibre3"/><span class="kobospan" id="kobo.457.1"> sources, such as the decision score or the probability output of the classifier. </span><span class="kobospan" id="kobo.457.2">Some examples are:</span></p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.458.1">Receiver operating characteristic</span></strong><span class="kobospan" id="kobo.459.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.460.1">ROC</span></strong><span class="kobospan" id="kobo.461.1">) </span><strong class="screentext"><span class="kobospan" id="kobo.462.1">curve</span></strong><span class="kobospan" id="kobo.463.1">: A plot of recall versus</span><a id="_idIndexMarker884" class="calibre3"/><span class="kobospan" id="kobo.464.1"> false positive rate (</span><code class="inlinecode"><span class="kobospan" id="kobo.465.1">FP / (FP + TN)</span></code><span class="kobospan" id="kobo.466.1">), which shows how well the classifier can distinguish between positive and negative cases at different thresholds.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.467.1">Area under the ROC curve</span></strong><span class="kobospan" id="kobo.468.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.469.1">AUC</span></strong><span class="kobospan" id="kobo.470.1">): The AUC, which measures </span><a id="_idIndexMarker885" class="calibre3"/><span class="kobospan" id="kobo.471.1">how well the classifier can rank positive cases higher than negative cases. </span><span class="kobospan" id="kobo.471.2">It can be illustrated in the following diagram, where the ROC curve and the area under the curve are displayed:</span></li>
</ul>
<figure class="mediaobject"><span class="kobospan" id="kobo.472.1"><img alt="" role="presentation" src="../Images/B21714_11_03.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.473.1">Figure 11.3: Illustration of a ROC curve, hightlighting a perfect classifier and the Area Under the Curve (AUC)</span></p>
<p class="normal1"><span class="kobospan" id="kobo.474.1">In our case, we will simply</span><a id="_idIndexMarker886" class="calibre3"/><span class="kobospan" id="kobo.475.1"> use the accuracy metric by following</span><a id="_idIndexMarker887" class="calibre3"/><span class="kobospan" id="kobo.476.1"> these steps:</span></p>
<ol class="calibre15">
<li class="bulletlist1" value="1"><span class="kobospan" id="kobo.477.1">You can import this metric from the </span><code class="inlinecode"><span class="kobospan" id="kobo.478.1">evaluate</span></code><span class="kobospan" id="kobo.479.1"> library as follows:
        </span><pre class="programlisting2"><code class="hljs-code"><span class="hljs-keyword"><span class="kobospan" id="kobo.480.1">import</span></span><span class="kobospan" id="kobo.481.1"> numpy </span><span class="hljs-keyword"><span class="kobospan" id="kobo.482.1">as</span></span><span class="kobospan" id="kobo.483.1"> np
</span><span class="hljs-keyword"><span class="kobospan" id="kobo.484.1">import</span></span><span class="kobospan" id="kobo.485.1"> evaluate
metric = evaluate.load(</span><span class="hljs-string"><span class="kobospan" id="kobo.486.1">"accuracy"</span></span><span class="kobospan" id="kobo.487.1">)
</span></code></pre>
</li>
<li class="bulletlist1"><span class="kobospan" id="kobo.488.1">We also need to define a function that computes the accuracy given the output of the training phase:
        </span><pre class="programlisting2"><code class="hljs-code"><span class="hljs-keyword"><span class="kobospan" id="kobo.489.1">def</span></span> <span class="hljs-title"><span class="kobospan" id="kobo.490.1">compute_metrics</span></span><span class="kobospan" id="kobo.491.1">(</span><span><span class="kobospan" id="kobo.492.1">eval_pred</span></span><span class="kobospan" id="kobo.493.1">):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-</span><span class="hljs-number"><span class="kobospan" id="kobo.494.1">1</span></span><span class="kobospan" id="kobo.495.1">)
    </span><span class="hljs-keyword"><span class="kobospan" id="kobo.496.1">return</span></span><span class="kobospan" id="kobo.497.1"> metric.compute(predictions=predictions, references=labels)
</span></code></pre>
</li>
<li class="bulletlist1"><span class="kobospan" id="kobo.498.1">Finally, we need to set our evaluation strategy, which means how often we want our model to be tested against the test set while training:
        </span><pre class="programlisting2"><code class="hljs-code"><span class="hljs-keyword"><span class="kobospan" id="kobo.499.1">from</span></span><span class="kobospan" id="kobo.500.1"> transformers </span><span class="hljs-keyword"><span class="kobospan" id="kobo.501.1">import</span></span><span class="kobospan" id="kobo.502.1"> TrainingArguments, Trainer
training_args = TrainingArguments(output_dir=</span><span class="hljs-string"><span class="kobospan" id="kobo.503.1">"test_trainer"</span></span><span class="kobospan" id="kobo.504.1">, num_train_epochs = </span><span class="hljs-number"><span class="kobospan" id="kobo.505.1">2</span></span><span class="kobospan" id="kobo.506.1">
evaluation_strategy=</span><span class="hljs-string"><span class="kobospan" id="kobo.507.1">"epoch"</span></span><span class="kobospan" id="kobo.508.1">)
</span></code></pre>
</li>
</ol>
<p class="normal-one"><span class="kobospan" id="kobo.509.1">In our case, we will set </span><code class="inlinecode"><span class="kobospan" id="kobo.510.1">epoch</span></code><span class="kobospan" id="kobo.511.1"> as the evaluation strategy, meaning that the evaluation is done</span><a id="_idIndexMarker888" class="calibre3"/><span class="kobospan" id="kobo.512.1"> at the end of each</span><a id="_idIndexMarker889" class="calibre3"/><span class="kobospan" id="kobo.513.1"> epoch.</span></p>
<div class="note">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.514.1">Definition</span></strong></p>
<p class="normal1"><span class="kobospan" id="kobo.515.1">An epoch is a term used in machine learning</span><a id="_idIndexMarker890" class="calibre3"/><span class="kobospan" id="kobo.516.1"> to describe one complete pass through the entire training dataset. </span><span class="kobospan" id="kobo.516.2">It is a hyperparameter that can be tuned to improve the performance of a machine-learning model. </span><span class="kobospan" id="kobo.516.3">During an epoch, the model’s weights are updated based on the training data and the loss function. </span><span class="kobospan" id="kobo.516.4">An epoch can consist of one or more batches, which are smaller subsets of the training data. </span><span class="kobospan" id="kobo.516.5">The number of batches in an epoch depends on the batch size, which is another hyperparameter that can be adjusted.</span></p>
</div>
<p class="normal1"><span class="kobospan" id="kobo.517.1">Now we have all the ingredients needed to start our fine-tuning, which will be covered in the next section.</span></p>
<h2 class="heading1" id="_idParaDest-164"><span class="kobospan" id="kobo.518.1">Training and saving</span></h2>
<p class="normal"><span class="kobospan" id="kobo.519.1">The last component</span><a id="_idIndexMarker891" class="calibre3"/><span class="kobospan" id="kobo.520.1"> we need to fine-tune</span><a id="_idIndexMarker892" class="calibre3"/><span class="kobospan" id="kobo.521.1"> our model is a </span><code class="inlinecode"><span class="kobospan" id="kobo.522.1">Trainer</span></code><span class="kobospan" id="kobo.523.1"> object. </span><span class="kobospan" id="kobo.523.2">The </span><code class="inlinecode"><span class="kobospan" id="kobo.524.1">Trainer</span></code><span class="kobospan" id="kobo.525.1"> object is a class that provides an API for feature-complete training and evaluation of models in PyTorch, optimized for Hugging Face Transformers. </span><span class="kobospan" id="kobo.525.2">You can follow these steps:</span></p>
<ol class="calibre15">
<li class="bulletlist1" value="1"><span class="kobospan" id="kobo.526.1">Let’s first initialize our </span><code class="inlinecode"><span class="kobospan" id="kobo.527.1">Trainer</span></code><span class="kobospan" id="kobo.528.1"> by specifying the parameters we’ve already configured in the previous steps. </span><span class="kobospan" id="kobo.528.2">More specifically, the </span><code class="inlinecode"><span class="kobospan" id="kobo.529.1">Trainer</span></code><span class="kobospan" id="kobo.530.1"> will need a model, some configuration args (such as the number of epochs), a training dataset, an evaluation dataset, and the type of evaluation metric to compute:
        </span><pre class="programlisting2"><code class="hljs-code"><span class="kobospan" id="kobo.531.1">trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics,
)
</span></code></pre>
</li>
<li class="bulletlist1"><span class="kobospan" id="kobo.532.1">You can then initiate the process of fine-tuning by calling the </span><code class="inlinecode"><span class="kobospan" id="kobo.533.1">trainer</span></code><span class="kobospan" id="kobo.534.1"> as follows:
        </span><pre class="programlisting2"><code class="hljs-code"><span class="kobospan" id="kobo.535.1">trainer.train()
</span></code></pre>
</li>
</ol>
<p class="normal-one"><span class="kobospan" id="kobo.536.1">Depending on your hardware, the training </span><a id="_idIndexMarker893" class="calibre3"/><span class="kobospan" id="kobo.537.1">process might take some time. </span><span class="kobospan" id="kobo.537.2">In my case, given</span><a id="_idIndexMarker894" class="calibre3"/><span class="kobospan" id="kobo.538.1"> the reduced size of the dataset and the low number of epochs (only 2), I don’t expect exceptional results. </span><span class="kobospan" id="kobo.538.2">Nevertheless, the training results for only two epochs in terms of accuracy are the following:</span></p>
<pre class="programlisting3"><code class="hljs-con"><span class="kobospan" id="kobo.539.1">{'eval_loss': 0.6720085144042969, 'eval_accuracy': 0.58, 'eval_runtime': 609.7916, 'eval_samples_per_second': 0.328, 'eval_steps_per_second': 0.041, 'epoch': 1.0}
{'eval_loss': 0.5366445183753967, 'eval_accuracy': 0.82, 'eval_runtime': 524.186, 'eval_samples_per_second': 0.382, 'eval_steps_per_second': 0.048, 'epoch': 2.0}
</span></code></pre>
<p class="normal-one"><span class="kobospan" id="kobo.540.1">As you can see, between the two epochs the model gained an accuracy improvement of 41.38%, hitting a final accuracy of 82%. </span><span class="kobospan" id="kobo.540.2">Considering the aforementioned elements, that’s not bad!</span></p>
<ol class="calibre15">
<li class="bulletlist1" value="3"><span class="kobospan" id="kobo.541.1">Once the model is trained, we can save it locally, specifying the path as follows:
        </span><pre class="programlisting2"><code class="hljs-code"><span class="kobospan" id="kobo.542.1">trainer.save_model('models/sentiment-classifier')
</span></code></pre>
</li>
<li class="bulletlist1"><span class="kobospan" id="kobo.543.1">To consume and test the model, you can load it with the following code:
        </span><pre class="programlisting2"><code class="hljs-code"><span class="kobospan" id="kobo.544.1">model = AutoModelForSequenceClassification.from_pretrained(</span><span class="hljs-string"><span class="kobospan" id="kobo.545.1">'models/sentiment-classifier'</span></span><span class="kobospan" id="kobo.546.1">)
</span></code></pre>
</li>
<li class="bulletlist1"><span class="kobospan" id="kobo.547.1">Finally, we need to test our model. </span><span class="kobospan" id="kobo.547.2">To do so, let’s pass a sentence to the model (to be first tokenized) on which it can perform sentiment classification:
        </span><pre class="programlisting2"><code class="hljs-code"><span class="kobospan" id="kobo.548.1">inputs = tokenizer(</span><span class="hljs-string"><span class="kobospan" id="kobo.549.1">"I cannot stand it anymore!"</span></span><span class="kobospan" id="kobo.550.1">, return_tensors=</span><span class="hljs-string"><span class="kobospan" id="kobo.551.1">"pt"</span></span><span class="kobospan" id="kobo.552.1">)
outputs = model(**inputs)
outputs
</span></code></pre>
</li>
</ol>
<p class="normal-one"><span class="kobospan" id="kobo.553.1">This yields the following output:</span></p>
<pre class="programlisting3"><code class="hljs-con"><span class="kobospan" id="kobo.554.1">SequenceClassifierOutput(loss=None, logits=tensor([[ 0.6467, -0.0041]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)
</span></code></pre>
<p class="normal-one"><span class="kobospan" id="kobo.555.1">Note that the model</span><a id="_idIndexMarker895" class="calibre3"/><span class="kobospan" id="kobo.556.1"> output is a </span><code class="inlinecode"><span class="kobospan" id="kobo.557.1">SequenceClassifierOutput</span></code><span class="kobospan" id="kobo.558.1"> object, which is the base</span><a id="_idIndexMarker896" class="calibre3"/><span class="kobospan" id="kobo.559.1"> class for outputs of sentence classification models. </span><span class="kobospan" id="kobo.559.2">Within this object, we are interested in the logit </span><strong class="screentext"><span class="kobospan" id="kobo.560.1">tensor</span></strong><span class="kobospan" id="kobo.561.1">, which is the vector of raw (non-normalized) predictions associated with labels that our classification model generated.</span></p>
<ol class="calibre15">
<li class="bulletlist1" value="6"><span class="kobospan" id="kobo.562.1">Since we are working with tensors, we will need to leverage the </span><code class="inlinecode"><span class="kobospan" id="kobo.563.1">tensorflow</span></code><span class="kobospan" id="kobo.564.1"> library in Python. </span><span class="kobospan" id="kobo.564.2">Plus, we will use the </span><code class="inlinecode"><span class="kobospan" id="kobo.565.1">softmax</span></code><span class="kobospan" id="kobo.566.1"> function to obtain the probability vector associated with each label, so that we know that the final result corresponds to the label with the greatest probability:
        </span><pre class="programlisting2"><code class="hljs-code"><span class="hljs-keyword"><span class="kobospan" id="kobo.567.1">import</span></span><span class="kobospan" id="kobo.568.1"> tensorflow </span><span class="hljs-keyword"><span class="kobospan" id="kobo.569.1">as</span></span><span class="kobospan" id="kobo.570.1"> tf
predictions = tf.math.softmax(outputs.logits.detach(), axis=-</span><span class="hljs-number"><span class="kobospan" id="kobo.571.1">1</span></span><span class="kobospan" id="kobo.572.1">)
</span><span class="hljs-built_in"><span class="kobospan" id="kobo.573.1">print</span></span><span class="kobospan" id="kobo.574.1">(predictions)
</span></code></pre>
</li>
</ol>
<p class="normal-one"><span class="kobospan" id="kobo.575.1">The following is the obtained output:</span></p>
<pre class="programlisting3"><code class="hljs-con"><span class="kobospan" id="kobo.576.1">tf.Tensor([[0.6571879  0.34281212]], shape=(1, 2), dtype=float32)
</span></code></pre>
<p class="normal-one"><span class="kobospan" id="kobo.577.1">Our model tells us that the sentiment of the sentence “I can’t stand it anymore” is negative, with a probability of 65.71%.</span></p>
<ol class="calibre15">
<li class="bulletlist1" value="7"><span class="kobospan" id="kobo.578.1">Note that you can also save the model in your Hugging Face account. </span><span class="kobospan" id="kobo.578.2">To do so, you first need to allow the notebook to push the code to your account as follows:
        </span><pre class="programlisting2"><code class="hljs-code"><span class="hljs-keyword"><span class="kobospan" id="kobo.579.1">from</span></span><span class="kobospan" id="kobo.580.1"> huggingface_hub </span><span class="hljs-keyword"><span class="kobospan" id="kobo.581.1">import</span></span><span class="kobospan" id="kobo.582.1"> notebook_login
notebook_login()
</span></code></pre>
</li>
<li class="bulletlist1"><span class="kobospan" id="kobo.583.1">You will be prompted to the Hugging Face login page, where you have to input your access token. </span><span class="kobospan" id="kobo.583.2">Then, you can save the model, specifying your account name and model name:
        </span><pre class="programlisting2"><code class="hljs-code"><span class="kobospan" id="kobo.584.1">trainer.push_to_hub(</span><span class="hljs-string"><span class="kobospan" id="kobo.585.1">'vaalto/sentiment-classifier'</span></span><span class="kobospan" id="kobo.586.1">)
</span></code></pre>
</li>
</ol>
<p class="normal1"><span class="kobospan" id="kobo.587.1">By doing so, this model</span><a id="_idIndexMarker897" class="calibre3"/><span class="kobospan" id="kobo.588.1"> can be consumed via the Hugging Face Hub as easily</span><a id="_idIndexMarker898" class="calibre3"/><span class="kobospan" id="kobo.589.1"> as we saw in the previous chapter, as shown in the following screenshot:</span></p>
<figure class="mediaobject"><span class="kobospan" id="kobo.590.1"><img alt="A screenshot of a computer  Description automatically generated" src="../Images/B21714_11_04.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.591.1">Figure 11.4: Model card within the Hugging Face Hub space</span></p>
<p class="normal1"><span class="kobospan" id="kobo.592.1">Furthermore, you can also decide to make the model public, so that everyone within Hugging Face can test and consume your creation.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.593.1">In this section, we fine-tuned a BERT model with just a few lines of code, thanks to Hugging Face libraries and accelerators. </span><span class="kobospan" id="kobo.593.2">Again, if your goal is reducing the code amount, you can leverage the low-code AutoTrain platform hosted in Hugging Face to train and fine-tune models.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.594.1">Hugging Face is definitely a solid platform for training your open-source LLM. </span><span class="kobospan" id="kobo.594.2">In addition to that, there are further platforms you might want to leverage since proprietary models can also be fine-tuned. </span><span class="kobospan" id="kobo.594.3">For example, OpenAI lets you fine-tune the GPT series with your own data, providing the computational power to train and host your customized models.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.595.1">Overall, fine-tuning</span><a id="_idIndexMarker899" class="calibre3"/><span class="kobospan" id="kobo.596.1"> can be the icing on the cake</span><a id="_idIndexMarker900" class="calibre3"/><span class="kobospan" id="kobo.597.1"> that makes your LLM exceptional for your use case. </span><span class="kobospan" id="kobo.597.2">Deciding a strategy to do so based on the framework we explored at the beginning is a pivotal step in building a successful application.</span></p>
<h1 class="heading" id="_idParaDest-165"><span class="kobospan" id="kobo.598.1">Summary</span></h1>
<p class="normal"><span class="kobospan" id="kobo.599.1">In this chapter, we covered the process of fine-tuning LLMs. </span><span class="kobospan" id="kobo.599.2">We started with a definition of fine-tuning and general considerations to take into account if you have to decide to fine-tune your LLM.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.600.1">We then went hands-on with practical sections on fine-tuning. </span><span class="kobospan" id="kobo.600.2">We covered a scenario where, starting from a base BERT model, we wanted a powerful review sentiment analyzer. </span><span class="kobospan" id="kobo.600.3">To do so, we fine-tuned the base model on the IMDB dataset using a full-code approach with Hugging Face Python libraries.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.601.1">Fine-tuning is a powerful technique to further customize LLMs toward your goal. </span><span class="kobospan" id="kobo.601.2">However, along with many other aspects of LLMs, it comes with some concerns and considerations in terms of ethics and security. </span><span class="kobospan" id="kobo.601.3">In the next chapter, we are going to delve deeper into that, sharing how to establish guardrails with LLMs and, more generally, how governments and countries are approaching the problem from a regulatory perspective.</span></p>
<h1 class="heading" id="_idParaDest-166"><span class="kobospan" id="kobo.602.1">References</span></h1>
<ul class="calibre16">
<li class="bulletlist"><span class="kobospan" id="kobo.603.1">Training dataset: </span><a href="https://huggingface.co/datasets/imdb" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.604.1">https://huggingface.co/datasets/imdb</span></span></a></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.605.1">HF AutoTrain: </span><a href="https://huggingface.co/docs/autotrain/index" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.606.1">https://huggingface.co/docs/autotrain/index</span></span></a></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.607.1">BERT paper: </span><em class="italic"><span class="kobospan" id="kobo.608.1">Jacob Devlin</span></em><span class="kobospan" id="kobo.609.1">, </span><em class="italic"><span class="kobospan" id="kobo.610.1">Ming-Wei Chang</span></em><span class="kobospan" id="kobo.611.1">, </span><em class="italic"><span class="kobospan" id="kobo.612.1">Kenton Lee</span></em><span class="kobospan" id="kobo.613.1">, </span><em class="italic"><span class="kobospan" id="kobo.614.1">Kristina Toutanova</span></em><span class="kobospan" id="kobo.615.1">, 2019, </span><em class="italic"><span class="kobospan" id="kobo.616.1">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</span></em><span class="kobospan" id="kobo.617.1">: </span><a href="https://arxiv.org/abs/1810.04805" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.618.1">https://arxiv.org/abs/1810.04805</span></span></a></li>
</ul>
<h1 class="heading"><span class="kobospan" id="kobo.619.1">Join our community on Discord</span></h1>
<p class="normal"><span class="kobospan" id="kobo.620.1">Join our community’s Discord space for discussions with the author and other readers:</span></p>
<p class="normal1"><a href="https://packt.link/llm" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.621.1">https://packt.link/llm</span></span></a></p>
<p class="normal1"><span class="kobospan" id="kobo.622.1"><img alt="" role="presentation" src="../Images/QR_Code214329708533108046.png" class="calibre4"/></span></p>
</div>
</body></html>