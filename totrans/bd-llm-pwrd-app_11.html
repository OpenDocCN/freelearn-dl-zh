<html><head></head><body>
<div><h1 class="chapternumber">11</h1>
<h1 class="chaptertitle" id="_idParaDest-155">Fine-Tuning Large Language Models</h1>
<p class="normal">Up to this point, we’ve explored<a id="_idIndexMarker812" class="calibre3"/> the features and applications of <strong class="screentext">large language models</strong> (<strong class="screentext">LLMs</strong>) in their “base” form, meaning that we consumed them with the parameters obtained from their base training. We experimented with many scenarios in which, even in their base form, LLMs have been able to adapt to a variety of scenarios. Nevertheless, there might be extremely domain-specific cases where a general-purpose LLM is not sufficient to fully embrace the taxonomy and knowledge of that domain. If this is the case, you might want to fine-tune your model on your domain-specific data.</p>
<div><p class="normal1"><strong class="screentext">Definition</strong></p>
<p class="normal1">In the context of fine-tuning<a id="_idIndexMarker813" class="calibre3"/> language models, “taxonomy” refers to a structured classification or categorization system that organizes concepts, terms, and entities according to their relationships and hierarchies within a specific domain. This system is essential for making the model’s understanding and generation of content more relevant and accurate for specialized applications.</p>
<p class="normal1">A concrete example of taxonomy in a domain-specific sector is in the medical field. Here, taxonomy could categorize information into structured groups like diseases, symptoms, treatments, and patient demographics. For instance, in the “diseases” category, there might be subcategories for types of diseases like “cardiovascular diseases,” which could be further divided into more specific conditions such as “hypertension” and “coronary artery disease.” This detailed categorization helps in fine-tuning language models to understand and generate more precise and contextually appropriate responses in medical consultations or documentation.</p>
</div>
<p class="normal1">In this chapter, we are going to cover the technical details of fine-tuning LLMs, from the theory behind it to the hands-on implementation with Python and Hugging Face. By the end of this chapter, you will be able to fine-tune an LLM on your own data, so that you can build domain-specific applications powered by those models.</p>
<p class="normal1">We will delve into the following topics:</p>
<ul class="calibre14">
<li class="bulletlist">Introduction to fine-tuning</li>
<li class="bulletlist1">Understanding when you need fine-tuning</li>
<li class="bulletlist1">Preparing your data to fine-tune the model</li>
<li class="bulletlist1">Fine-tuning a base model on your data</li>
<li class="bulletlist1">Hosting strategies for your fine-tuned model</li>
</ul>
<h1 class="heading" id="_idParaDest-156">Technical requirements</h1>
<p class="normal">To complete the tasks in this chapter, you will need the following:</p>
<ul class="calibre14">
<li class="bulletlist">A Hugging Face account and user access token.</li>
<li class="bulletlist1">Python 3.7.1 or later version.</li>
<li class="bulletlist1">Python packages: Make sure to have the following Python packages installed: <code class="inlinecode">python-dotenv</code>, <code class="inlinecode">huggingface_hub</code>, <code class="inlinecode">accelerate&gt;=0.16.0</code>, <code class="inlinecode">&lt;1 transformers[torch]</code>, <code class="inlinecode">safetensors</code>, <code class="inlinecode">tensorflow</code>, <code class="inlinecode">datasets</code>, <code class="inlinecode">evaluate</code>, and<code class="inlinecode"> accelerate</code>. Those can be easily installed via <code class="inlinecode">pip install</code> in your terminal. If you want to install everything from the latest release, you can refer to the original GitHub by running <code class="inlinecode">pip install git+https://github.com/huggingface/transformers.git</code> in your terminal.</li>
</ul>
<p class="normal1">You can find all the code and examples in the book’s GitHub repository at <a href="Chapter_11.xhtml" class="calibre3">https://github.com/PacktPublishing/Building-LLM-Powered-Applications</a>.</p>
<h1 class="heading" id="_idParaDest-157">What is fine-tuning?</h1>
<p class="normal">Fine-tuning is a technique of <strong class="screentext">transfer learning</strong> in which the weights<a id="_idIndexMarker814" class="calibre3"/> of a pretrained<a id="_idIndexMarker815" class="calibre3"/> neural network are used as the initial values for training a new neural network on a different task. This can improve the performance of the new network by leveraging the knowledge learned from the previous task, especially when the new task has limited data.</p>
<div><p class="normal1"><strong class="screentext">Definition</strong></p>
<p class="normal1">Transfer learning<a id="_idIndexMarker816" class="calibre3"/> is a technique in machine learning that involves using the knowledge learned from one task to improve the performance on a related but different task. For example, if you have a model that can recognize cars, you can use some of its features to help you recognize trucks. Transfer learning can save you time and resources by reusing existing models instead of training new ones from scratch.</p>
</div>
<p class="normal1">To better understand the concepts of transfer learning and fine-tuning, let’s consider the following example.</p>
<p class="normal1">Imagine you want to train a computer vision neural network to recognize different types of flowers, such as roses, sunflowers, and tulips. You have a lot of photos of flowers, but not enough to train a model from scratch.</p>
<p class="normal1">Instead, you can use transfer learning, which means taking a model that was already trained on a different task and using some of its knowledge for your new task. For example, you can take a model that was trained to recognize many vehicles, such as cars, trucks, and bicycles. This model has learned how to extract features from images, such as edges, shapes, colors, and textures. These features are useful for any image recognition task, not just the original one.</p>
<p class="normal1">You can use this model<a id="_idIndexMarker817" class="calibre3"/> as a base for your flower recognition model. You only need to add a new layer on top of it, which will learn how to classify the features into flower types. This layer is called the classifier layer, and <a id="_idIndexMarker818" class="calibre3"/>it is needed for the model to adapt to the new task. Training the classifier<a id="_idIndexMarker819" class="calibre3"/> layer on top of the base model is a process called <strong class="screentext">feature extraction</strong>. Once this step is done, you can further tailor your model with fine-tuning by unfreezing some of the base model layers and training them together with the classifier layer. This allows you to adjust the base model features to better suit your task.</p>
<p class="normal1">The following picture illustrates the computer vision model example:</p>
<figure class="mediaobject"><img alt="" role="presentation" src="img/B21714_11_01.png" class="calibre4"/></figure>
<p class="packt_figref">Figure 11.1: Example of transfer learning and fine-tuning</p>
<p class="normal1">Fine-tuning is usually done after feature extraction, as a final step to improve the performance of the model. You can decide how many layers to unfreeze based on your data size and complexity. A common practice is to unfreeze the last few layers of the base model, which are more specific to the original task, and leave the first few layers frozen, which are more generic and reusable.</p>
<p class="normal1">To summarize, transfer learning<a id="_idIndexMarker820" class="calibre3"/> and fine-tuning are techniques that allow you to use a pretrained model for a new task. Transfer learning involves adding a new classifier layer on top of the base model and training only that layer. Fine-tuning involves unfreezing some or all of the base model layers and training them together with the classifier layer.</p>
<p class="normal1">In the context of generative AI, fine-tuning<a id="_idIndexMarker821" class="calibre3"/> is the process of adapting a pretrained language model to a specific task or domain by updating its parameters on a task-specific dataset. Fine-tuning can improve the performance and accuracy of the model for the target task. The steps involved in fine-tuning are:</p>
<ol class="calibre15">
<li class="bulletlist1" value="1"><strong class="screentext">Load the pretrained language model and its tokenizer</strong>: The tokenizer is used to convert<a id="_idIndexMarker822" class="calibre3"/> text into numerical tokens that the model can process. Different models have unique architectures and requirements, often coming with their own specialized tokenizers designed to handle their specific input formats.</li>
</ol>
<p class="normal-one">For instance, <strong class="screentext">BERT</strong> (which stands for <strong class="screentext">Bidirectional Encoder Representations from Transformers</strong>) uses WordPiece<a id="_idIndexMarker823" class="calibre3"/> tokenization, while GPT-2 employs <strong class="screentext">byte-pair encoding</strong> (<strong class="screentext">BPE</strong>). Models also impose<a id="_idIndexMarker824" class="calibre3"/> token limits due to memory constraints during training and inference.</p>
<p class="normal-one">These limits determine the maximum sequence length that a model can handle. For example, BERT has a maximum token limit of 512 tokens, while the GPT-2 can handle longer sequences (e.g., up to 1,024 tokens).</p>
<ol class="calibre15">
<li class="bulletlist1" value="2"><strong class="screentext">Prepare the task-specific dataset</strong>: The dataset should contain input-output pairs that are relevant to the task. For example, for sentiment analysis, the input could be a text review and the output could be a sentiment label (positive, negative, or neutral).</li>
<li class="bulletlist1"><strong class="screentext">Define the task-specific head</strong>: The head is a layer or a set of layers that are added on top of the pretrained model to perform the task. The head should match the output format and size of the task. For example, for sentiment analysis, the head could be a linear layer with three output units corresponding to the three sentiment labels.</li>
</ol>
<div><p class="normal1"><strong class="screentext">Note</strong></p>
<p class="normal1">When dealing with an LLM specifically designed for text generation, the architecture differs from models used for classification or other tasks. In fact, unlike classification tasks, where we predict labels, an LLM predicts the next word or token in a sequence. This layer is added on top of the pretrained transformer-based models with the purpose of transforming the contextualized hidden representations from the base model into probabilities over the vocabulary.</p>
</div>
<ol class="calibre15">
<li class="bulletlist1" value="4"><strong class="screentext">Train the model on the task-specific dataset</strong>: The training process involves feeding the input tokens to the model, computing the loss between the model output and the true output, and updating the model parameters using an optimizer. The training can be done for a fixed number of epochs or until a certain criterion is met.</li>
<li class="bulletlist1"><strong class="screentext">Evaluate the model on a test or validation set</strong>:<strong class="screentext"> </strong>The evaluation process involves measuring the performance of the model on unseen data using appropriate metrics. For example, for sentiment analysis, the metric could be accuracy or F1-score (which will be discussed later in this chapter). The evaluation results can<a id="_idIndexMarker825" class="calibre3"/> be used to compare different models or fine-tuning strategies.</li>
</ol>
<p class="normal1">Even though it is less computationally and time expensive than full training, fine-tuning an LLM is not a “light” activity. As LLMs are, by definition, large, their fine-tuning has hardware requirements as well as data collection and preprocessing.</p>
<p class="normal1">So, the first question that you want to ask yourself while approaching a given scenario is: “Do I really need to finetune my LLM?”</p>
<h1 class="heading" id="_idParaDest-158">When is fine-tuning necessary?</h1>
<p class="normal">As we saw in previous chapters, good prompt engineering combined with the non-parametric knowledge you can add to your model via embeddings are exceptional techniques to customize your LLM, and they can account for around 90% of use cases. However, the preceding affirmation tends to hold for the state-of-the-art models, such as GPT-4, Llama 2, and PaLM 2. As discussed, those models have a huge number of parameters that make them heavy, hence the need for computational power; plus, they might be proprietary and subject to a pay-per-use cost.</p>
<p class="normal1">Henceforth, fine-tuning might also be useful when you want to leverage a light and free-of-charge LLM, such as the Falcon LLM 7B, yet you want it to perform as well as a SOTA model in your specific task.</p>
<p class="normal1">Some examples<a id="_idIndexMarker826" class="calibre3"/> of when fine-tuning might be necessary are:</p>
<ul class="calibre14">
<li class="bulletlist">When you want to use an LLM for sentiment analysis on movie reviews, but the LLM was pretrained on Wikipedia articles and books. Fine-tuning can help the LLM learn the vocabulary, style, and tone of movie reviews, as well as the relevant features for sentiment classification.</li>
<li class="bulletlist1">When you want to use an LLM for text summarization on news articles, but the LLM was pretrained on a language modeling objective. Fine-tuning can help the LLM learn the structure, content, and length of summaries, as well as the generation objective and evaluation metrics.</li>
<li class="bulletlist1">When you want to use an LLM for machine translation between two languages, but the LLM was pretrained on a multilingual corpus that does not include those languages. Fine-tuning can help the LLM learn the vocabulary, grammar, and syntax of the target languages, as well as the translation objective and alignment methods.</li>
<li class="bulletlist1">When you want<a id="_idIndexMarker827" class="calibre3"/> to use an LLM to perform complex <strong class="screentext">named entity recognition</strong> (<strong class="screentext">NER</strong>) tasks. For example, financial and legal documents contain specialized terminology and entities that are not typically prioritized in general language models, henceforth a fine-tuning process might be extremely beneficial<a id="_idIndexMarker828" class="calibre3"/> here.</li>
</ul>
<p class="normal1">In this chapter, we will be covering a full-code approach leveraging Hugging Face models and libraries. However, be aware<a id="_idIndexMarker829" class="calibre3"/> that Hugging Face also offers a low-code platform<a id="_idIndexMarker830" class="calibre3"/> called AutoTrain (you can read more about that at <a href="https://huggingface.co/autotrain" class="calibre3">https://huggingface.co/autotrain</a>), which might be a good alternative if your organization is more oriented towards low-code strategies.</p>
<h1 class="heading" id="_idParaDest-159">Getting started with fine-tuning</h1>
<p class="normal">In this section, we are going<a id="_idIndexMarker831" class="calibre3"/> to cover all the steps needed to fine-tune an LLM with a full-code approach. We will be leveraging Hugging Face libraries, such as <code class="inlinecode">datasets</code> (to load data from the Hugging Face datasets ecosystem) and <code class="inlinecode">tokenizers</code> (to provide an implementation of the most popular tokenizers). The scenario we are going to address is a sentiment analysis task. Our goal is to fine-tune a model to make it an expert binary classifier of emotions, clustered into “positive” and “negative.”</p>
<h2 class="heading1" id="_idParaDest-160">Obtaining the dataset</h2>
<p class="normal">The first ingredient<a id="_idIndexMarker832" class="calibre3"/> that we need is the training dataset. For this purpose, I will leverage the datasets library available in Hugging Face to load a binary classification<a id="_idIndexMarker833" class="calibre3"/> dataset called IMDB (you can find the dataset card at <a href="https://huggingface.co/datasets/imdb" class="calibre3">https://huggingface.co/datasets/imdb</a>).</p>
<p class="normal1">The dataset contains movie reviews, which are classified as positive or negative. More specifically, the dataset contains two columns:</p>
<ul class="calibre14">
<li class="bulletlist">Text: The raw text movie review.</li>
<li class="bulletlist1">Label: The sentiment of that review. It is mapped as “0” for “Negative” and “1” for “Positive.”</li>
</ul>
<p class="normal1">As it is a <strong class="screentext">supervised learning</strong> problem, the dataset<a id="_idIndexMarker834" class="calibre3"/> already comes with 25,000 rows for the training set and 25,000 rows for the validation set.</p>
<div><p class="normal1"><strong class="screentext">Definition</strong></p>
<p class="normal1">Supervised learning is a type of machine learning that uses labeled datasets to train algorithms to classify data or predict outcomes accurately. Labeled datasets are collections<a id="_idIndexMarker835" class="calibre3"/> of examples that have both input<a id="_idIndexMarker836" class="calibre3"/> features and desired output values, also known as labels or targets. For example, a labeled dataset for handwriting recognition might have images of handwritten digits as input features and the corresponding numerical values as labels.</p>
<p class="normal1">Training and validation sets are subsets of the labeled dataset that are used for different purposes in the supervised learning process. The training set is used to fit the parameters of the model, such as the weights of the connections in a neural network. The validation set is used to tune the hyperparameters of the model, such as the number of hidden units in a neural network or the learning rate. Hyperparameters are settings that affect the overall behavior and performance of the model but are not directly learned from the data. The validation set helps to select the best model among different candidates by comparing their accuracy or other metrics on the validation set.</p>
<p class="normal1">Supervised learning differs from another type<a id="_idIndexMarker837" class="calibre3"/> of machine learning, which is <strong class="screentext">unsupervised learning</strong>. With the latter, the algorithm is tasked with finding patterns, structures, or relationships in a dataset without the presence of labeled outputs or targets. In other words, in unsupervised learning, the algorithm is not provided with specific guidance or labels to direct its learning process. Instead, it explores the data and identifies inherent patterns or groupings on its own.</p>
</div>
<p class="normal1">You can download the IMDB dataset by running the following code:</p>
<pre class="programlisting"><code class="hljs-code">from datasets import load_dataset
dataset = load_dataset("imdb")
dataset
</code></pre>
<p class="normal1">Hugging Face datasets come with a dictionary schema, which is as follows:</p>
<pre class="programlisting"><code class="hljs-code">DatasetDict({
    train: Dataset({
        features: ['text', 'label'],
        num_rows: 25000
    })
    test: Dataset({
        features: ['text', 'label'],
        num_rows: 25000
    })
    unsupervised: Dataset({
        features: ['text', 'label'],
        num_rows: 50000
    })
})
</code></pre>
<p class="normal1">To access one observation of a particular Dataset object (for example, <code class="inlinecode">train</code>), you can use slicers, as follows:</p>
<pre class="programlisting"><code class="hljs-code">dataset["train"][100]
</code></pre>
<p class="normal1">This gives us the following output:</p>
<pre class="programlisting1"><code class="hljs-con">{'text': "Terrible movie. Nuff Said.[…]
 'label': 0}
</code></pre>
<p class="normal1">So, the 101st observation of the training set contains a review labeled as negative.</p>
<p class="normal1">Now that we have<a id="_idIndexMarker838" class="calibre3"/> the dataset, we need to preprocess it so that can be used to train our LLM. To do so, we need to tokenize the provided text, and we will discuss this in the next section.</p>
<h2 class="heading1" id="_idParaDest-161">Tokenizing the data</h2>
<p class="normal">A tokenizer is a component<a id="_idIndexMarker839" class="calibre3"/> that is responsible<a id="_idIndexMarker840" class="calibre3"/> for splitting a text into smaller units, such as words or subwords, that can be used as inputs for an LLM. Tokenizers can be used to encode text efficiently and consistently, as well as to add special tokens, such as mask or separator tokens, that are required by some models.</p>
<p class="normal1">Hugging Face provides<a id="_idIndexMarker841" class="calibre3"/> a powerful utility called AutoTokenizer, available in the Hugging Face Transformers library, that offers tokenizers for various models, such as BERT and GPT-2. It serves as a generic tokenizer class that dynamically selects and instantiates the appropriate tokenizer based on the pretrained model you specify.</p>
<p class="normal1">The following code snippet shows how we can initialize our tokenizer:</p>
<pre class="programlisting"><code class="hljs-code">from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
</code></pre>
<p class="normal1">Note that we picked a specific<a id="_idIndexMarker842" class="calibre3"/> tokenizer called <code class="inlinecode">bert-base-cased</code>. In fact, there<a id="_idIndexMarker843" class="calibre3"/> is a link between a tokenizer and an LLM, in the sense that the the tokenizer prepares the inputs for the model by converting the text into numerical IDs that the model can understand.</p>
<div><p class="normal1"><strong class="screentext">Definition</strong></p>
<p class="normal1">The input IDs are the numerical IDs that<a id="_idIndexMarker844" class="calibre3"/> correspond to the tokens in the vocabulary of the tokenizer. They are returned by the tokenizer function when encoding a text input. The input IDs are used as inputs for the model, which expects numerical tensors rather than strings. Different tokenizers may have different input IDs for the same tokens, depending on their vocabulary and tokenization algorithm.</p>
</div>
<p class="normal1">Different models may use different tokenization algorithms, such as word-based, character-based, or subword-based. Therefore, it is important to use the correct tokenizer for each model, otherwise the model may not perform well or even produce errors. Let’s look at potential scenarios for each:</p>
<ul class="calibre14">
<li class="bulletlist">A character-based approach might fit scenarios<a id="_idIndexMarker845" class="calibre3"/> that deal with rare words or languages with complex morphological structures, or when dealing with spelling correction tasks</li>
<li class="bulletlist1">The word-based approach might<a id="_idIndexMarker846" class="calibre3"/> be a good fit for scenarios like NER, sentiment analysis, and text classification</li>
<li class="bulletlist1">The sub-word approach interpolates<a id="_idIndexMarker847" class="calibre3"/> between the previous two, and it is useful when we want to balance the granularity of text representation with efficiency.</li>
</ul>
<p class="normal1">As we will see in the next<a id="_idIndexMarker848" class="calibre3"/> section, we will leverage the <strong class="screentext">BERT</strong> model for this scenario, hence we loaded its pretrained <a id="_idIndexMarker849" class="calibre3"/>tokenizer (which is a word-based tokenizer powered by an algorithm called WordPiece).</p>
<p class="normal1">We now need to initialize <code class="inlinecode">tokenize_function</code>, which will be used to format the dataset:</p>
<pre class="programlisting"><code class="hljs-code">def tokenize_function(examples):
    return tokenizer(examples["text"], padding = "max_length", truncation=True)
tokenized_datasets = dataset.map(tokenize_function, batched=True)
</code></pre>
<p class="normal1">As you can<a id="_idIndexMarker850" class="calibre3"/> see, we also<a id="_idIndexMarker851" class="calibre3"/> configured the <strong class="screentext">padding</strong> and <strong class="screentext">truncation</strong> of <code class="inlinecode">tokenize_function</code> to ensure an output<a id="_idIndexMarker852" class="calibre3"/> with the right<a id="_idIndexMarker853" class="calibre3"/> sizing for our BERT model.</p>
<div><p class="normal1"><strong class="screentext">Definition</strong></p>
<p class="normal1">Padding and truncation are two techniques that are used to make the input sequences of text<a id="_idIndexMarker854" class="calibre3"/> have the same length. This is often required for some <strong class="screentext">natural language processing</strong> (<strong class="screentext">NLP</strong>) models, such as the BERT model, that expect fixed-length inputs.</p>
<p class="normal1">Padding means adding some special tokens, usually zeros, at the end or the beginning of a sequence to make it reach the desired length. For example, if we have a sequence of length 5 and we want to pad it to a length of 8, we can add 3 zeros at the end, like this: [1, 2, 3, 4, 5, 0, 0, 0]. This<a id="_idIndexMarker855" class="calibre3"/> is called post-padding. Alternatively, we can add 3 zeros<a id="_idIndexMarker856" class="calibre3"/> at the beginning, like this: [0, 0, 0, 1, 2, 3, 4, 5]. This is called pre-padding. The choice of padding strategy depends on the model and the task.</p>
<p class="normal1">Truncation means removing some tokens from a sequence to make it fit the desired length. For example, if we have a sequence of length 10 and we want to truncate it to a length of 8, we can remove 2 tokens from the end or the beginning of the sequence. For example, we can<a id="_idIndexMarker857" class="calibre3"/> remove the last 2 tokens, like this: [1, 2, 3, 4, 5, 6, 7, 8]. This is called post-truncation. Alternatively, we can remove the first 2 tokens, like this: [3, 4, 5, 6, 7, 8, 9, 10]. This<a id="_idIndexMarker858" class="calibre3"/> is called pre-truncation. The choice of truncation strategy also depends on the model and the task.</p>
</div>
<p class="normal1">Now, we can apply the function to our dataset and inspect the numerical IDs of one entry:</p>
<pre class="programlisting"><code class="hljs-code">tokenized_datasets = dataset.map(tokenize_function, batched=True)
tokenized_datasets['train'][100]['input_ids']
</code></pre>
<p class="normal1">Here is our output:</p>
<pre class="programlisting1"><code class="hljs-con">[101,
 12008,
 27788,
...
 0,
 0,
 0,
 0,
 0]
</code></pre>
<p class="normal1">As you can see, the last elements of the vector are zeroes, due to the <code class="inlinecode">padding='max_lenght'</code> parameter passed to the function.</p>
<p class="normal1">Optionally, you can decide to reduce the size of your dataset if you want to make the training time shorter. In my case, I’ve shrunk the dataset as follows:</p>
<pre class="programlisting"><code class="hljs-code">small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(500))
small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(500))
</code></pre>
<p class="normal1">So, I have two <a id="_idIndexMarker859" class="calibre3"/>sets – one for<a id="_idIndexMarker860" class="calibre3"/> training, one for testing – of 500 observations each. Now that we have our dataset preprocessed and ready, we need the model to be fine-tuned.</p>
<h2 class="heading1" id="_idParaDest-162">Fine-tuning the model</h2>
<p class="normal">As anticipated in the previous<a id="_idIndexMarker861" class="calibre3"/> section, the LLM<a id="_idIndexMarker862" class="calibre3"/> we are going to leverage for fine-tuning is the base version of BERT. The BERT model is a transformer-based, encoder-only model for natural language understanding introduced by Google researchers in 2018. BERT was the first example of a general-purpose LLM, meaning that it was the first model to be able to tackle multiple NLP tasks at once, which was different from the task-specific models existing up to that moment.</p>
<p class="normal1">Now, even though it might sound a bit “old fashioned” (in fact, compared to today’s model like the GPT-4, it is not even “large,” with only 340 million parameters in its large version), given all the new LLMs that have emerged in the market in the last few months, BERT and its fine-tuned variants are still a widely adopted architecture. In fact, it was thanks to BERT<a id="_idIndexMarker863" class="calibre3"/> that the standard for language models has greatly<a id="_idIndexMarker864" class="calibre3"/> improved.</p>
<p class="normal1">The BERT model has two main components:</p>
<ul class="calibre14">
<li class="bulletlist">Encoder: The encoder consists of multiple<a id="_idIndexMarker865" class="calibre3"/> layers of transformer blocks, each with a self-attention layer and a feedforward layer. The encoder takes as input a sequence of tokens, which are the basic units of text, and outputs a sequence of hidden states, which are high-dimensional vectors that represent the semantic information of each token.</li>
<li class="bulletlist1">Output layer: The output layer is task-specific<a id="_idIndexMarker866" class="calibre3"/> and can be different depending on the type of task that BERT is used for. For example, for text classification, the output layer can be a linear layer that predicts the class label of the input text. For question answering, the output layer can be two linear layers that predict the start and end positions of the answer span in the input text.</li>
<li class="bulletlist1">The number of layers and parameters of the model depends on the model version. In fact, BERT comes in two sizes: BERTbase and BERTlarge. The following illustration shows the difference between the two versions:</li>
</ul>
<figure class="mediaobject"><img alt="A screenshot of a computer  Description automatically generated" src="img/B21714_11_02.png" class="calibre4"/></figure>
<p class="packt_figref">Figure 11.2: A comparison between BERTbase and BERTlarge (source: <a href="https://huggingface.co/blog/bert-101" class="calibre3">https://huggingface.co/blog/bert-101</a>)</p>
<p class="normal1">Later, other versions such as BERT-tiny, BERT-mini, BERT-small, and BERT-medium were introduced to reduce the computational cost and memory usage of BERT.</p>
<p class="normal1">The model has been trained on a heterogeneous corpus of around 3.3 billion words, belonging to Wikipedia and Google’s BooksCorpus. The training phase involved two objectives:</p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext">Masked language modeling</strong> (<strong class="screentext">MLM</strong>): MLM aims to teach the model to predict the original<a id="_idIndexMarker867" class="calibre3"/> words that are randomly masked (replaced with a special token) in the input text. For example, given the sentence “He bought a new [MASK] yesterday,” the model should predict the word “car” or “bike” or something else that makes sense. This objective helps the model learn the vocabulary and the syntax of the language, as well as the semantic and contextual relations between words.</li>
<li class="bulletlist1"><strong class="screentext">Next sentence prediction</strong> (<strong class="screentext">NSP</strong>): NSP aims to teach<a id="_idIndexMarker868" class="calibre3"/> the model to predict whether two sentences are consecutive or not in the original text. For example, given the sentences “She loves reading books” and “Her favorite genre is fantasy,” the model should predict that they are consecutive because they are likely to appear together in a text. However, given the sentences “She loves reading books” and “He plays soccer every weekend,” the model should predict that they are not consecutive because they are unlikely to be related. This objective helps the model learn the coherence and logic of the text, as well as the discourse and pragmatic relations between sentences.</li>
</ul>
<p class="normal1">By using these two <a id="_idIndexMarker869" class="calibre3"/>objectives (on which the model is trained at the same time), the BERT model<a id="_idIndexMarker870" class="calibre3"/> can learn general language knowledge that can be transferred to specific tasks, such as text classification, question answering, and NER. The BERT model can achieve better performance on these tasks than previous models that only use one<a id="_idIndexMarker871" class="calibre3"/> direction of context<a id="_idIndexMarker872" class="calibre3"/> or do not use pre-training at all. In fact, it has achieved<a id="_idIndexMarker873" class="calibre3"/> state-of-the-art results on many benchmarks and tasks, such as <strong class="screentext">General Language Understanding Evaluation</strong> (<strong class="screentext">GLUE</strong>), <strong class="screentext">Stanford Question Answering Dataset</strong> (<strong class="screentext">SQuAD</strong>), and <strong class="screentext">Multi-Genre Natural Language Inference</strong> (<strong class="screentext">MultiNLI</strong>).</p>
<p class="normal1">The BERT model is available – along with many fine-tuned versions –in the Hugging Face Hub. You can instantiate the model as follows:</p>
<pre class="programlisting"><code class="hljs-code">import torch
from transformers import AutoModelForSequenceClassification
model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", num_labels=2)
</code></pre>
<p class="normal1">Note that <code class="inlinecode">AutoModelForSequenceClassification</code> is a subclass of <code class="inlinecode">AutoModel</code>, which can instantiate a model architecture suitable for sequence classification, such as text classification or sentiment analysis. It can be used for any task that requires a single label or a list of labels for each input sequence. In my case, I set the number of output labels equal to two since we are dealing with a binary classification problem.</p>
<p class="normal1">On the other hand, <code class="inlinecode">AutoModel</code> is a generic class that can instantiate any model architecture from the library based on the pretrained model name or path. It can be used for any task that does not require a specific <a id="_idIndexMarker874" class="calibre3"/>output format, such as feature extraction or language<a id="_idIndexMarker875" class="calibre3"/> modeling.</p>
<p class="normal1">The final step before starting the training is to define the evaluation metrics we will need to understand how well our model will perform once fine-tuned.</p>
<h2 class="heading1" id="_idParaDest-163">Using evaluation metrics</h2>
<p class="normal">As we saw in <em class="italic">Chapter 1</em>, evaluating an LLM<a id="_idIndexMarker876" class="calibre3"/> in its general-purpose application<a id="_idIndexMarker877" class="calibre3"/> might be cumbersome. As those models are trained on unlabeled text and are not task-specific, but rather generic and adaptable given a user’s prompt, traditional evaluation metrics were not suitable anymore. Evaluating an LLM means, among other things, measuring its language fluency, its coherence, and its ability to emulate different styles depending on a user’s request.</p>
<p class="normal1">However, we also saw how an LLM can be used for very specific scenarios, as in our binary classification task. If this is the case, evaluation metrics boil down to those commonly used for that scenario.</p>
<div><p class="normal1"><strong class="screentext">Note</strong></p>
<p class="normal1">When it comes to more conversational tasks like summarization, Q&amp;A, and retrieval-augmented generation, a new set of evaluation metrics needs to be introduced, often powered in turn by LLMs. Some of the most popular metrics are the following:</p>
<ul class="calibre14">
<li class="bulletlist">Fluency: This assesses how naturally and smoothly the generated text reads.</li>
<li class="bulletlist1">Coherence: This evaluates the logical flow and connectivity of ideas within a text.</li>
<li class="bulletlist1">Relevance: This measures how well the generated content aligns with the given prompt or context.</li>
<li class="bulletlist1">GPT-similarity: This quantifies how closely the generated text resembles human-written content.</li>
<li class="bulletlist1">Groundedness: This assesses whether the generated text is based on factual information or context.</li>
</ul>
<p class="normal1">These evaluation metrics help us understand the quality, naturalness, and relevance of LLM-generated text, guiding improvements and ensuring reliable AI assistance.</p>
</div>
<p class="normal1">When it comes to binary classification, one of the most basic ways to evaluate a binary classifier is to use a confusion matrix. A confusion matrix is a table that shows how many of the predicted labels match the true labels. It has four cells:</p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext">True positive</strong> (<strong class="screentext">TP</strong>): The number of cases where the classifier correctly predicted 1 when the true label was 1.</li>
<li class="bulletlist1"><strong class="screentext">False positive</strong> (<strong class="screentext">FP</strong>): The number of cases where the classifier incorrectly predicted 1 when the true label was 0.</li>
<li class="bulletlist1"><strong class="screentext">True negative</strong> (<strong class="screentext">TN</strong>): The number of cases where the classifier correctly predicted 0 when the true label was 0.</li>
<li class="bulletlist1"><strong class="screentext">False negative</strong> (<strong class="screentext">FN</strong>): The number of cases where the classifier incorrectly predicted 0 when the true label was 1.</li>
</ul>
<p class="normal1">Here is an example<a id="_idIndexMarker878" class="calibre3"/> of a confusion matrix for the sentiment classifier<a id="_idIndexMarker879" class="calibre3"/> we are going to build, knowing that the label 0 is associated with “Negative” and the label 1 with “Positive”:</p>
<table class="table-container" id="table001-3">
<tbody class="calibre18">
<tr class="calibre19">
<td class="table-cell"/>
<td class="table-cell">
<p class="normal1"><strong class="keyword">Predicted Positive</strong></p>
</td>
<td class="table-cell">
<p class="normal1"><strong class="keyword">Predicted Negative</strong></p>
</td>
</tr>
<tr class="calibre19">
<td class="table-cell">
<p class="normal1"><strong class="keyword">Positive</strong></p>
</td>
<td class="table-cell">
<p class="normal1">20 (TP)</p>
</td>
<td class="table-cell">
<p class="normal1">5 (FN)</p>
</td>
</tr>
<tr class="calibre19">
<td class="table-cell">
<p class="normal1"><strong class="keyword">Negative</strong></p>
</td>
<td class="table-cell">
<p class="normal1">3 (FP)</p>
</td>
<td class="table-cell">
<p class="normal1">72 (TN)</p>
</td>
</tr>
</tbody>
</table>
<p class="normal1">The confusion matrix can be used to calculate various metrics that measure different aspects of the classifier’s performance. Some of the most common metrics are:</p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext">Accuracy</strong>: The proportion of correct predictions among all predictions. It is calculated as <code class="inlinecode">(TP + TN) / (TP + FP + TN + FN)</code>. For example, the accuracy of the sentiment classifier is <code class="inlinecode">(20 + 72) / (20 + 3 + 72 + 5) = 0.92</code>.</li>
<li class="bulletlist1"><strong class="screentext">Precision</strong>: The proportion of correct positive predictions among all positive predictions. It is calculated as <code class="inlinecode">TP / (TP + FP)</code>. For example, the precision of the sentiment classifier is <code class="inlinecode">20 / (20 + 3) = 0.87</code>.</li>
<li class="bulletlist1"><strong class="screentext">Recall</strong>: The proportion of correct positive<a id="_idIndexMarker880" class="calibre3"/> predictions among all positive cases. It is also known as sensitivity or true positive rate. It is calculated as <code class="inlinecode">TP / (TP + FN)</code>. For example, the recall of the sentiment classifier is <code class="inlinecode">20 / (20 + 5) = 0.8</code>.</li>
<li class="bulletlist1"><strong class="screentext">Specificity</strong>: The proportion of correct negative<a id="_idIndexMarker881" class="calibre3"/> predictions among all negative cases. It is also known as the true negative rate. It is calculated as <code class="inlinecode">TN / (TN + FP)</code>. For example, the specificity of the sentiment classifier is <code class="inlinecode">72 / (72 + 3) = 0.96</code>.</li>
<li class="bulletlist1"><strong class="screentext">F1-score</strong>: The harmonic mean of precision and recall. It is a measure of balance between precision and recall. It is calculated as <code class="inlinecode">2 * (precision * recall) / (precision + recall)</code>. For example, the F1-score of the sentiment classifier is <code class="inlinecode">2 * (0.87 * 0.8) / (0.87 + 0.8) = 0.83</code>.</li>
</ul>
<p class="normal1">There are many other metrics<a id="_idIndexMarker882" class="calibre3"/> that can be derived from the confusion matrix or other<a id="_idIndexMarker883" class="calibre3"/> sources, such as the decision score or the probability output of the classifier. Some examples are:</p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext">Receiver operating characteristic</strong> (<strong class="screentext">ROC</strong>) <strong class="screentext">curve</strong>: A plot of recall versus<a id="_idIndexMarker884" class="calibre3"/> false positive rate (<code class="inlinecode">FP / (FP + TN)</code>), which shows how well the classifier can distinguish between positive and negative cases at different thresholds.</li>
<li class="bulletlist1"><strong class="screentext">Area under the ROC curve</strong> (<strong class="screentext">AUC</strong>): The AUC, which measures <a id="_idIndexMarker885" class="calibre3"/>how well the classifier can rank positive cases higher than negative cases. It can be illustrated in the following diagram, where the ROC curve and the area under the curve are displayed:</li>
</ul>
<figure class="mediaobject"><img alt="" role="presentation" src="img/B21714_11_03.png" class="calibre4"/></figure>
<p class="packt_figref">Figure 11.3: Illustration of a ROC curve, hightlighting a perfect classifier and the Area Under the Curve (AUC)</p>
<p class="normal1">In our case, we will simply<a id="_idIndexMarker886" class="calibre3"/> use the accuracy metric by following<a id="_idIndexMarker887" class="calibre3"/> these steps:</p>
<ol class="calibre15">
<li class="bulletlist1" value="1">You can import this metric from the <code class="inlinecode">evaluate</code> library as follows:
        <pre class="programlisting2"><code class="hljs-code">import numpy as np
import evaluate
metric = evaluate.load("accuracy")
</code></pre>
</li>
<li class="bulletlist1">We also need to define a function that computes the accuracy given the output of the training phase:
        <pre class="programlisting2"><code class="hljs-code">def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)
</code></pre>
</li>
<li class="bulletlist1">Finally, we need to set our evaluation strategy, which means how often we want our model to be tested against the test set while training:
        <pre class="programlisting2"><code class="hljs-code">from transformers import TrainingArguments, Trainer
training_args = TrainingArguments(output_dir="test_trainer", num_train_epochs = 2
evaluation_strategy="epoch")
</code></pre>
</li>
</ol>
<p class="normal-one">In our case, we will set <code class="inlinecode">epoch</code> as the evaluation strategy, meaning that the evaluation is done<a id="_idIndexMarker888" class="calibre3"/> at the end of each<a id="_idIndexMarker889" class="calibre3"/> epoch.</p>
<div><p class="normal1"><strong class="screentext">Definition</strong></p>
<p class="normal1">An epoch is a term used in machine learning<a id="_idIndexMarker890" class="calibre3"/> to describe one complete pass through the entire training dataset. It is a hyperparameter that can be tuned to improve the performance of a machine-learning model. During an epoch, the model’s weights are updated based on the training data and the loss function. An epoch can consist of one or more batches, which are smaller subsets of the training data. The number of batches in an epoch depends on the batch size, which is another hyperparameter that can be adjusted.</p>
</div>
<p class="normal1">Now we have all the ingredients needed to start our fine-tuning, which will be covered in the next section.</p>
<h2 class="heading1" id="_idParaDest-164">Training and saving</h2>
<p class="normal">The last component<a id="_idIndexMarker891" class="calibre3"/> we need to fine-tune<a id="_idIndexMarker892" class="calibre3"/> our model is a <code class="inlinecode">Trainer</code> object. The <code class="inlinecode">Trainer</code> object is a class that provides an API for feature-complete training and evaluation of models in PyTorch, optimized for Hugging Face Transformers. You can follow these steps:</p>
<ol class="calibre15">
<li class="bulletlist1" value="1">Let’s first initialize our <code class="inlinecode">Trainer</code> by specifying the parameters we’ve already configured in the previous steps. More specifically, the <code class="inlinecode">Trainer</code> will need a model, some configuration args (such as the number of epochs), a training dataset, an evaluation dataset, and the type of evaluation metric to compute:
        <pre class="programlisting2"><code class="hljs-code">trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics,
)
</code></pre>
</li>
<li class="bulletlist1">You can then initiate the process of fine-tuning by calling the <code class="inlinecode">trainer</code> as follows:
        <pre class="programlisting2"><code class="hljs-code">trainer.train()
</code></pre>
</li>
</ol>
<p class="normal-one">Depending on your hardware, the training <a id="_idIndexMarker893" class="calibre3"/>process might take some time. In my case, given<a id="_idIndexMarker894" class="calibre3"/> the reduced size of the dataset and the low number of epochs (only 2), I don’t expect exceptional results. Nevertheless, the training results for only two epochs in terms of accuracy are the following:</p>
<pre class="programlisting3"><code class="hljs-con">{'eval_loss': 0.6720085144042969, 'eval_accuracy': 0.58, 'eval_runtime': 609.7916, 'eval_samples_per_second': 0.328, 'eval_steps_per_second': 0.041, 'epoch': 1.0}
{'eval_loss': 0.5366445183753967, 'eval_accuracy': 0.82, 'eval_runtime': 524.186, 'eval_samples_per_second': 0.382, 'eval_steps_per_second': 0.048, 'epoch': 2.0}
</code></pre>
<p class="normal-one">As you can see, between the two epochs the model gained an accuracy improvement of 41.38%, hitting a final accuracy of 82%. Considering the aforementioned elements, that’s not bad!</p>
<ol class="calibre15">
<li class="bulletlist1" value="3">Once the model is trained, we can save it locally, specifying the path as follows:
        <pre class="programlisting2"><code class="hljs-code">trainer.save_model('models/sentiment-classifier')
</code></pre>
</li>
<li class="bulletlist1">To consume and test the model, you can load it with the following code:
        <pre class="programlisting2"><code class="hljs-code">model = AutoModelForSequenceClassification.from_pretrained('models/sentiment-classifier')
</code></pre>
</li>
<li class="bulletlist1">Finally, we need to test our model. To do so, let’s pass a sentence to the model (to be first tokenized) on which it can perform sentiment classification:
        <pre class="programlisting2"><code class="hljs-code">inputs = tokenizer("I cannot stand it anymore!", return_tensors="pt")
outputs = model(**inputs)
outputs
</code></pre>
</li>
</ol>
<p class="normal-one">This yields the following output:</p>
<pre class="programlisting3"><code class="hljs-con">SequenceClassifierOutput(loss=None, logits=tensor([[ 0.6467, -0.0041]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)
</code></pre>
<p class="normal-one">Note that the model<a id="_idIndexMarker895" class="calibre3"/> output is a <code class="inlinecode">SequenceClassifierOutput</code> object, which is the base<a id="_idIndexMarker896" class="calibre3"/> class for outputs of sentence classification models. Within this object, we are interested in the logit <strong class="screentext">tensor</strong>, which is the vector of raw (non-normalized) predictions associated with labels that our classification model generated.</p>
<ol class="calibre15">
<li class="bulletlist1" value="6">Since we are working with tensors, we will need to leverage the <code class="inlinecode">tensorflow</code> library in Python. Plus, we will use the <code class="inlinecode">softmax</code> function to obtain the probability vector associated with each label, so that we know that the final result corresponds to the label with the greatest probability:
        <pre class="programlisting2"><code class="hljs-code">import tensorflow as tf
predictions = tf.math.softmax(outputs.logits.detach(), axis=-1)
print(predictions)
</code></pre>
</li>
</ol>
<p class="normal-one">The following is the obtained output:</p>
<pre class="programlisting3"><code class="hljs-con">tf.Tensor([[0.6571879  0.34281212]], shape=(1, 2), dtype=float32)
</code></pre>
<p class="normal-one">Our model tells us that the sentiment of the sentence “I can’t stand it anymore” is negative, with a probability of 65.71%.</p>
<ol class="calibre15">
<li class="bulletlist1" value="7">Note that you can also save the model in your Hugging Face account. To do so, you first need to allow the notebook to push the code to your account as follows:
        <pre class="programlisting2"><code class="hljs-code">from huggingface_hub import notebook_login
notebook_login()
</code></pre>
</li>
<li class="bulletlist1">You will be prompted to the Hugging Face login page, where you have to input your access token. Then, you can save the model, specifying your account name and model name:
        <pre class="programlisting2"><code class="hljs-code">trainer.push_to_hub('vaalto/sentiment-classifier')
</code></pre>
</li>
</ol>
<p class="normal1">By doing so, this model<a id="_idIndexMarker897" class="calibre3"/> can be consumed via the Hugging Face Hub as easily<a id="_idIndexMarker898" class="calibre3"/> as we saw in the previous chapter, as shown in the following screenshot:</p>
<figure class="mediaobject"><img alt="A screenshot of a computer  Description automatically generated" src="img/B21714_11_04.png" class="calibre4"/></figure>
<p class="packt_figref">Figure 11.4: Model card within the Hugging Face Hub space</p>
<p class="normal1">Furthermore, you can also decide to make the model public, so that everyone within Hugging Face can test and consume your creation.</p>
<p class="normal1">In this section, we fine-tuned a BERT model with just a few lines of code, thanks to Hugging Face libraries and accelerators. Again, if your goal is reducing the code amount, you can leverage the low-code AutoTrain platform hosted in Hugging Face to train and fine-tune models.</p>
<p class="normal1">Hugging Face is definitely a solid platform for training your open-source LLM. In addition to that, there are further platforms you might want to leverage since proprietary models can also be fine-tuned. For example, OpenAI lets you fine-tune the GPT series with your own data, providing the computational power to train and host your customized models.</p>
<p class="normal1">Overall, fine-tuning<a id="_idIndexMarker899" class="calibre3"/> can be the icing on the cake<a id="_idIndexMarker900" class="calibre3"/> that makes your LLM exceptional for your use case. Deciding a strategy to do so based on the framework we explored at the beginning is a pivotal step in building a successful application.</p>
<h1 class="heading" id="_idParaDest-165">Summary</h1>
<p class="normal">In this chapter, we covered the process of fine-tuning LLMs. We started with a definition of fine-tuning and general considerations to take into account if you have to decide to fine-tune your LLM.</p>
<p class="normal1">We then went hands-on with practical sections on fine-tuning. We covered a scenario where, starting from a base BERT model, we wanted a powerful review sentiment analyzer. To do so, we fine-tuned the base model on the IMDB dataset using a full-code approach with Hugging Face Python libraries.</p>
<p class="normal1">Fine-tuning is a powerful technique to further customize LLMs toward your goal. However, along with many other aspects of LLMs, it comes with some concerns and considerations in terms of ethics and security. In the next chapter, we are going to delve deeper into that, sharing how to establish guardrails with LLMs and, more generally, how governments and countries are approaching the problem from a regulatory perspective.</p>
<h1 class="heading" id="_idParaDest-166">References</h1>
<ul class="calibre16">
<li class="bulletlist">Training dataset: <a href="https://huggingface.co/datasets/imdb" class="calibre3">https://huggingface.co/datasets/imdb</a></li>
<li class="bulletlist1">HF AutoTrain: <a href="https://huggingface.co/docs/autotrain/index" class="calibre3">https://huggingface.co/docs/autotrain/index</a></li>
<li class="bulletlist1">BERT paper: <em class="italic">Jacob Devlin</em>, <em class="italic">Ming-Wei Chang</em>, <em class="italic">Kenton Lee</em>, <em class="italic">Kristina Toutanova</em>, 2019, <em class="italic">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</em>: <a href="https://arxiv.org/abs/1810.04805" class="calibre3">https://arxiv.org/abs/1810.04805</a></li>
</ul>
<h1 class="heading">Join our community on Discord</h1>
<p class="normal">Join our community’s Discord space for discussions with the author and other readers:</p>
<p class="normal1"><a href="https://packt.link/llm" class="calibre3">https://packt.link/llm</a></p>
<p class="normal1"><img alt="" role="presentation" src="img/QR_Code214329708533108046.png" class="calibre4"/></p>
</div>
</body></html>