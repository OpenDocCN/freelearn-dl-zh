- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data Versioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Data versioning** refers to the systematic tracking and management of different
    iterations of datasets used throughout the life cycle of model development, including
    pre-training, fine-tuning, evaluation, and deployment. It involves assigning unique
    identifiers to datasets or subsets thereof, capturing changes over time, and enabling
    reproducibility by ensuring that any specific model version can be linked back
    to the exact data version used.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you’ll learn how to implement effective data versioning strategies
    for LLM development. For instance, when we want to add 10,000 new oncology research
    papers to a dataset, the system automatically creates a new dataset version. If
    the model performance then degrades, the dataset can instantly roll back to the
    previous verified dataset version, ensuring reproducibility and maintaining the
    integrity of the research process.
  prefs: []
  type: TYPE_NORMAL
- en: This design pattern transforms dataset management from a chaotic, manual process
    into a structured, trackable workflow in LLM model development.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll be covering the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the need for data versioning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data versioning strategies for large language datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tools for data versioning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating data versioning in training workflows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Version control for text corpora
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing dataset variants and experiments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best practices for data versioning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the need for data versioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data versioning is particularly important in LLM projects due to the massive
    scale and complexity of language datasets. As an LLM engineer, you need to track
    changes in your datasets to ensure the reproducibility of your models and maintain
    a clear history of data modifications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by implementing a basic data versioning system using Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This part of the `DatasetVersion` class initializes the basic structure for
    versioning your LLM datasets. It generates a unique hash for each version of the
    data and timestamps the version. The `_generate_hash` method creates a deterministic
    hash based on the sorted JSON representation of the data, ensuring that identical
    data always produces the same hash.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s add the `save` and `load` methods for dataset versions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The `save` method serializes the dataset version to a JSON file, including all
    relevant information. The `load` method is a class method that reconstructs a
    `DatasetVersion` instance from a saved file. This allows you to easily store and
    retrieve different versions of your dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Having discussed the need for data versioning, now let us outline key strategies
    for managing versioning in large language datasets to support traceability, reproducibility,
    and efficient storage.
  prefs: []
  type: TYPE_NORMAL
- en: Data versioning strategies for large language datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Among the various strategies available for handling data versioning—such as
    snapshotting, content-addressable storage, and checksum-based tracking—this section
    focuses on the **delta-based system** due to its potential for minimizing storage
    costs when dealing with iterative updates in large language datasets. Delta-based
    versioning stores only the differences between dataset versions rather than duplicating
    entire files, making it particularly effective in scenarios involving frequent
    but minor changes. However, its effectiveness decreases when the dataset structure
    undergoes significant reformatting or involves binary files. Schema changes, column
    reordering, or file splitting can disrupt the delta mechanism, often necessitating
    a full dataset rewrite. Similarly, binary files, due to their opaque structure
    and compression, tend to change globally with even minor edits, limiting the advantage
    of delta-based storage. This approach is discussed here for its relevance in typical
    LLM workflows where data evolves gradually but remains largely text-based and
    structured.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example of how you might implement a delta-based versioning system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This part of the `DeltaDatasetVersion` class extends our previous `DatasetVersion`
    class to implement delta-based versioning. The `_compute_delta` method calculates
    the differences between the current version and a base version using Python’s
    `difflib`. This approach can significantly reduce storage requirements for large
    datasets by only storing the changes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s add methods to save and load these delta-based versions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The `save` method now stores only the delta and metadata, significantly reducing
    the file size of large datasets. The `load` method reconstructs the full dataset
    by applying the delta to the base version. This approach allows for the efficient
    storage and retrieval of multiple versions of large language datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Tools for data versioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While custom solutions can be effective, there are also specialized tools designed
    for data versioning in machine learning projects. One such tool is **Data Version
    Control** (**DVC**), which integrates with Git and provides powerful features
    for managing large datasets and is widely used. DVC is an open-source tool that
    extends Git to manage large datasets and machine learning artifacts by storing
    data in external storage while tracking metadata in the Git repository. It enables
    reproducible pipelines, efficient data sharing, and experiment tracking, making
    it a popular choice for managing LLM datasets and training workflows.
  prefs: []
  type: TYPE_NORMAL
- en: Given the scale of LLM models, DVC’s versioning approach must carefully balance
    comprehensive tracking with computational efficiency, requiring intelligent checksum
    and metadata calculation strategies that minimize latency and processing overhead
    to prevent versioning from becoming a bottleneck in the model development workflow.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example of how you might use DVC in your LLM project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This part of the script demonstrates how to initialize DVC, add a dataset to
    DVC tracking, and commit a new version of the dataset. DVC works alongside Git,
    allowing you to version your data in a similar way to how you version your code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to Git, DVC uses `init`, `add`, `commit`, and `push` commands. The
    following list briefly describes each command:'
  prefs: []
  type: TYPE_NORMAL
- en: '`dvc init`: Initializes a new DVC project by creating a `.dvc` directory in
    your project and setting up the necessary metadata tracking infrastructure. This
    is analogous to `git init`, but specifically for data version control, preparing
    your project to track large datasets and model files.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dvc add`: Adds large data files to DVC tracking, creating a lightweight `.dvc`
    metadata file that contains a hash of the file. This command moves the actual
    data to a separate storage location while maintaining a reference in your Git
    repository, allowing you to version large files without bloating your Git repository.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dvc commit`: Creates a snapshot of the current state of your tracked data
    files, similar to a Git commit but specifically for data files. This command helps
    you mark significant points in your data’s history and creates a clear record
    of when and how your datasets changed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dvc push`: Uploads your tracked data files to a remote storage location (such
    as cloud storage, network drive, or local external storage). This command ensures
    that your data versions are safely backed up and can be retrieved by other team
    members or across different development environments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let’s add a function to push the dataset to remote storage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The `push_dataset_to_remote` function pushes both the DVC-tracked data and the
    Git repository to their respective remote storage locations. This allows you to
    store your large datasets separately from your code repository while maintaining
    version control for both.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will focus on integrating data versioning within the training workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating data versioning in training workflows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To make data versioning an integral part of your LLM training workflow, you
    need to incorporate version checking and logging into your training scripts. Here’s
    an example of how you might do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This code snippet shows how to incorporate dataset version information into
    your LLM training workflow. The `DatasetInfo` class encapsulates the essential
    version information, while the `load_dataset_info` function retrieves this information
    from a JSON file. The `train_llm` function demonstrates how to log the dataset
    version and metadata during training, ensuring that each trained model is associated
    with a specific version of the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how you might use this in a training script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: By integrating dataset version information into your training process, you enhance
    reproducibility and make it easier to track which version of the data was used
    for each trained model.
  prefs: []
  type: TYPE_NORMAL
- en: Version control for text corpora
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When dealing with text corpora for LLM training, you often need to handle large
    collections of documents. Here’s an approach to version control for text corpora
    using a combination of file hashing and metadata tracking:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This part of the code defines functions to hash individual files and generate
    a manifest of all files in a corpus directory. The manifest is a dictionary mapping
    relative file paths to their corresponding hash values, providing a snapshot of
    the entire corpus. The manifest file is important because it serves as a compact,
    reproducible fingerprint of the entire dataset, enabling quick integrity checks,
    facilitating version tracking, and allowing researchers to verify the exact state
    of their corpus across different environments or points in time without needing
    to store or transfer the entire large dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s add a function to compare two manifests and identify changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The `compare_manifests` function identifies added, removed, and modified files
    between two versions of the corpus. This approach allows you to track changes
    in your text corpus efficiently, even when dealing with large numbers of files.
  prefs: []
  type: TYPE_NORMAL
- en: Managing dataset variants and experiments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In LLM development, you often need to manage multiple variants of your dataset
    for different experiments. Here’s a simple system for managing dataset variants:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This part of the `DatasetVariantManager` class sets up the basic structure for
    managing dataset variants. It initializes the manager with a base path and loads
    existing variants from a JSON file, if available.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s add methods to create and retrieve variants:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The `create_variant` method allows you to create new dataset variants based
    on existing ones, specifying only the changes. The `get_variant` method retrieves
    a variant, applying all changes from its base variants recursively. This system
    allows you to efficiently manage and track different configurations of your dataset
    for various experiments.
  prefs: []
  type: TYPE_NORMAL
- en: 'A clear and consistent naming convention is recommended for managing dataset
    variants in LLM development to ensure traceability, reproducibility, and clarity.
    Here is a suggested naming convention that balances readability and scalability
    for managing dataset variants:'
  prefs: []
  type: TYPE_NORMAL
- en: '`<``base>_<modifier1>_<modifier2>_..._<description>`'
  prefs: []
  type: TYPE_NORMAL
- en: This format uses a **base name** to indicate the root dataset, followed by **modifiers**
    and optional descriptions to specify what changes or attributes differentiate
    the variant. Modifiers are concise and ordered hierarchically to reflect the transformation
    process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the key components closely:'
  prefs: []
  type: TYPE_NORMAL
- en: '`base` or a descriptive name (e.g., `clean` or `raw`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Modifiers**: Sequential changes or transformations applied to the base. Each
    modifier reflects an aspect of the dataset such as size, language, or preprocessing
    applied.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Description**: An optional part that provides extra context or details about
    the changes, typically used for experiments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best practices for data versioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Over the years, I have gathered the following best practices:'
  prefs: []
  type: TYPE_NORMAL
- en: Use a dedicated data versioning tool such as DVC for large-scale projects.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Include dataset version information in your model metadata.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use delta-based versioning for large datasets to save storage space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement regular backups of your versioned datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use consistent naming conventions for dataset versions and variants.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrate data versioning checks into your `dvc status` to verify no unexpected
    modifications have occurred, automatically comparing dataset checksums against
    approved versions, and blocking model training if any data discrepancies are detected.
    Key steps include creating a pre-training validation stage that compares current
    dataset versions with expected reference versions, automatically triggering alerts
    or stopping the pipeline if unverified data modifications are detected, and maintaining
    a comprehensive audit trail of dataset changes throughout the machine learning
    development process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored various aspects of data versioning for LLM development.
    We implemented basic versioning systems and delta-based versioning for large datasets.
    We examined tools such as DVC for more advanced versioning needs. We also looked
    at integrating data versioning into LLM training workflows, managing text corpora
    versions, and handling dataset variants for experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Data versioning is a critical practice in LLM development, ensuring reproducibility,
    facilitating collaboration, and enabling robust model governance. By implementing
    these techniques and best practices, you can significantly improve the manageability
    and reliability of your LLM projects.
  prefs: []
  type: TYPE_NORMAL
- en: In the upcoming chapter, we’ll explore dataset annotation and labeling techniques
    specifically tailored for LLMs. In particular, we’ll cover strategies for efficient
    annotation, quality control measures, and methods for scaling annotation processes
    to meet the demands of large language datasets.
  prefs: []
  type: TYPE_NORMAL
