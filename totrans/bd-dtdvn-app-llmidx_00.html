<html><head></head><body><html:html>&#13;
 <html:head>&#13;
  <html:title>&#13;
   Understanding Large Language Models&#13;
  </html:title>&#13;
 </html:head>&#13;
 <html:body>&#13;
  <html:div class="epub-source">&#13;
   <html:h1 id="_idParaDest-16">&#13;
    Understanding Large Language Models&#13;
   </html:h1>&#13;
   <html:div id="_idContainer016">&#13;
    <html:p>&#13;
     If you are reading&#13;
     <html:a id="_idIndexMarker000">&#13;
     </html:a>&#13;
     this book, you have probably explored the realm of&#13;
     <html:strong class="bold">&#13;
      large language models&#13;
     </html:strong>&#13;
     (&#13;
     <html:strong class="bold">&#13;
      LLMs&#13;
     </html:strong>&#13;
     ) and already recognize their potential applications as well as their pitfalls. This book aims to address the challenges LLMs face and provides a practical guide to building data-driven LLM applications with LlamaIndex, taking&#13;
     <html:a id="_idIndexMarker001">&#13;
     </html:a>&#13;
     developers from foundational concepts to advanced techniques for implementing&#13;
     <html:strong class="bold">&#13;
      retrieval-augmented generation&#13;
     </html:strong>&#13;
     (&#13;
     <html:strong class="bold">&#13;
      RAG&#13;
     </html:strong>&#13;
     ) to create high-performance interactive&#13;
     <html:strong class="bold">&#13;
      artificial intelligence&#13;
     </html:strong>&#13;
     (&#13;
     <html:strong class="bold">&#13;
      AI&#13;
     </html:strong>&#13;
     ) systems&#13;
     <html:a id="_idIndexMarker002">&#13;
     </html:a>&#13;
     augmented by&#13;
     <html:span class="No-Break">&#13;
      external data.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     This chapter introduces&#13;
     <html:strong class="bold">&#13;
      generative AI&#13;
     </html:strong>&#13;
     (&#13;
     <html:strong class="bold">&#13;
      GenAI&#13;
     </html:strong>&#13;
     ) and LLMs. It explains how LLMs generate&#13;
     <html:a id="_idIndexMarker003">&#13;
     </html:a>&#13;
     human-like text after training on massive datasets. We’ll also overview LLM capabilities, limitations such as outdated knowledge potential for false information, and lack of reasoning. You’ll be introduced to RAG as a potential solution, combining retrieval models using indexed data with generative models to increase fact accuracy, logical reasoning, and context relevance. Overall, you’ll gain a basic LLM understanding and learn about RAG as a way to overcome some LLM weaknesses, setting the stage for utilizing&#13;
     <html:span class="No-Break">&#13;
      LLMs practically.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     In this chapter, we will cover the following&#13;
     <html:span class="No-Break">&#13;
      main topics:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:ul>&#13;
     <html:li>&#13;
      Introducing GenAI&#13;
      <html:span class="No-Break">&#13;
       and LLMs&#13;
      </html:span>&#13;
     </html:li>&#13;
     <html:li>&#13;
      Understanding the role of LLMs in&#13;
      <html:span class="No-Break">&#13;
       modern technology&#13;
      </html:span>&#13;
     </html:li>&#13;
     <html:li>&#13;
      Exploring challenges&#13;
      <html:span class="No-Break">&#13;
       with LLMs&#13;
      </html:span>&#13;
     </html:li>&#13;
     <html:li>&#13;
      Augmenting LLMs&#13;
      <html:span class="No-Break">&#13;
       with RAG&#13;
      </html:span>&#13;
     </html:li>&#13;
    </html:ul>&#13;
    <html:a id="_idTextAnchor016">&#13;
    </html:a>&#13;
   </html:div>&#13;
  </html:div>&#13;
 </html:body>&#13;
</html:html>
<html:html>&#13;
 <html:head>&#13;
  <html:title>&#13;
   Introducing GenAI and LLMs&#13;
  </html:title>&#13;
 </html:head>&#13;
 <html:body>&#13;
  <html:div class="epub-source">&#13;
   <html:h1 id="_idParaDest-17">&#13;
    Introducing GenAI and LLMs&#13;
   </html:h1>&#13;
   <html:div id="_idContainer016">&#13;
    <html:p>&#13;
     Introductions&#13;
     <html:a id="_idIndexMarker004">&#13;
     </html:a>&#13;
     are sometimes boring, but here, it is important&#13;
     <html:a id="_idIndexMarker005">&#13;
     </html:a>&#13;
     for us to set the context and help you familiarize yourself with GenAI and LLMs before we dive deep into LlamaIndex. I will try to be as concise as possible and, if the reader is already familiar with this information, I apologize for the&#13;
     <html:span class="No-Break">&#13;
      brief digression.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:a id="_idTextAnchor017">&#13;
    </html:a>&#13;
    <html:h2 id="_idParaDest-18">&#13;
     What is GenAI?&#13;
    </html:h2>&#13;
    <html:p>&#13;
     <html:strong class="bold">&#13;
      GenAI&#13;
     </html:strong>&#13;
     refers to&#13;
     <html:a id="_idIndexMarker006">&#13;
     </html:a>&#13;
     systems that are capable of generating new content such as text, images, audio, or video. Unlike more specialized AI systems that are designed for specific tasks such as image classification or speech recognition, GenAI models can create completely new assets that are often very difficult – if not impossible – to distinguish from&#13;
     <html:span class="No-Break">&#13;
      human-created content.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     These&#13;
     <html:a id="_idIndexMarker007">&#13;
     </html:a>&#13;
     systems use&#13;
     <html:strong class="bold">&#13;
      machine learning&#13;
     </html:strong>&#13;
     (&#13;
     <html:strong class="bold">&#13;
      ML&#13;
     </html:strong>&#13;
     ) techniques such as&#13;
     <html:strong class="bold">&#13;
      neural networks&#13;
     </html:strong>&#13;
     (&#13;
     <html:strong class="bold">&#13;
      NNs&#13;
     </html:strong>&#13;
     ) that&#13;
     <html:a id="_idIndexMarker008">&#13;
     </html:a>&#13;
     are trained on vast amounts of data. By learning patterns and structures within the training data, generative models can model the underlying probability distribution of the data and sample from this distribution to generate new examples. In other words, they act as big&#13;
     <html:span class="No-Break">&#13;
      prediction machines.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     We will now discuss LLMs, which are one of the most popular fields&#13;
     <html:span class="No-Break">&#13;
      in GenAI.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:a id="_idTextAnchor018">&#13;
    </html:a>&#13;
    <html:h2 id="_idParaDest-19">&#13;
     What is an LLM?&#13;
    </html:h2>&#13;
    <html:p>&#13;
     One of&#13;
     <html:a id="_idIndexMarker009">&#13;
     </html:a>&#13;
     the most prominent and rapidly advancing branches&#13;
     <html:a id="_idIndexMarker010">&#13;
     </html:a>&#13;
     of GenAI is&#13;
     <html:strong class="bold">&#13;
      natural language generation&#13;
     </html:strong>&#13;
     (&#13;
     <html:strong class="bold">&#13;
      NLG&#13;
     </html:strong>&#13;
     ) through&#13;
     <html:strong class="bold">&#13;
      LLMs&#13;
     </html:strong>&#13;
     (&#13;
     <html:span class="No-Break">&#13;
      <html:em class="italic">&#13;
       Figure 1&#13;
      </html:em>&#13;
     </html:span>&#13;
     <html:span class="No-Break">&#13;
      <html:em class="italic">&#13;
       .1&#13;
      </html:em>&#13;
     </html:span>&#13;
     <html:span class="No-Break">&#13;
      ):&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:div>&#13;
     <html:div class="IMG---Figure" id="_idContainer011">&#13;
      <html:img src="../Images/B21861_1_01.jpg"/>&#13;
     </html:div>&#13;
    </html:div>&#13;
    <html:p class="IMG---Caption" lang="en-US">&#13;
     Figure 1.1 – LLMs are a sub-branch of GenAI&#13;
    </html:p>&#13;
    <html:p>&#13;
     LLMs are NNs that are specifically designed and optimized to understand and generate human language. They are&#13;
     <html:em class="italic">&#13;
      large&#13;
     </html:em>&#13;
     in the sense that they are trained on massive amounts of text containing&#13;
     <html:a id="_idIndexMarker011">&#13;
     </html:a>&#13;
     billions or even trillions of words scraped from the internet and other sources. Larger models show increased performance on benchmarks, better generalization, and new emergent abilities. In contrast with earlier, rule-based generation systems, the main distinguishing feature of an LLM is that it can produce novel, original text that&#13;
     <html:span class="No-Break">&#13;
      reads naturally.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     By learning patterns from many sources, LLMs acquire various language skills found in their training data – from nuanced grammar to topic knowledge and even basic common-sense reasoning. These learned patterns allow LLMs to extend human-written text in contextually relevant ways. As they keep improving, LLMs create new possibilities for automatically&#13;
     <html:a id="_idIndexMarker012">&#13;
     </html:a>&#13;
     generating&#13;
     <html:strong class="bold">&#13;
      natural language&#13;
     </html:strong>&#13;
     (&#13;
     <html:strong class="bold">&#13;
      NL&#13;
     </html:strong>&#13;
     ) content&#13;
     <html:span class="No-Break">&#13;
      at scale.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     During the training process, LLMs gradually learn probabilistic relationships between words and rules that govern language structure from their huge dataset of training data. Once trained, they are able to generate remarkably human-like text by predicting the probability of the next word in a sequence, based on the previous words. In many cases, the text they generate is so natural that it makes you wonder: aren’t we humans just a similar but more sophisticated prediction machine? But that’s a topic for&#13;
     <html:span class="No-Break">&#13;
      another book.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     One of the key&#13;
     <html:a id="_idIndexMarker013">&#13;
     </html:a>&#13;
     architectural innovations is the&#13;
     <html:strong class="bold">&#13;
      transformer&#13;
     </html:strong>&#13;
     (that is the&#13;
     <html:em class="italic">&#13;
      T&#13;
     </html:em>&#13;
     in&#13;
     <html:em class="italic">&#13;
      GPT&#13;
     </html:em>&#13;
     ), which uses an&#13;
     <html:strong class="bold">&#13;
      attention mechanism&#13;
     </html:strong>&#13;
     to learn contextual relationships between words. Attention&#13;
     <html:a id="_idIndexMarker014">&#13;
     </html:a>&#13;
     allows the model to learn long-range dependencies in text. It’s like if you’re listening carefully in a conversation, you pay&#13;
     <html:strong class="bold">&#13;
      attention&#13;
     </html:strong>&#13;
     to the context to understand the full meaning. This means they&#13;
     <html:em class="italic">&#13;
      understand&#13;
     </html:em>&#13;
     not just words that are close together but also how words that are far apart in a sentence or paragraph relate to&#13;
     <html:span class="No-Break">&#13;
      each other.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     <html:em class="italic">&#13;
      Attention&#13;
     </html:em>&#13;
     allows the model to selectively focus on relevant parts of the input sequence when making predictions, thus capturing complex patterns and dependencies within the data. This feature&#13;
     <html:a id="_idIndexMarker015">&#13;
     </html:a>&#13;
     makes it possible for particularly large transformer models (with many parameters and trained on massive datasets) to demonstrate surprising new abilities such as in-context learning, where they can perform tasks&#13;
     <html:a id="_idIndexMarker016">&#13;
     </html:a>&#13;
     with just a few examples in their prompt. To learn more about transformers and&#13;
     <html:strong class="bold">&#13;
      Generative Pre-trained Transformer&#13;
     </html:strong>&#13;
     (&#13;
     <html:strong class="bold">&#13;
      GPT&#13;
     </html:strong>&#13;
     ), you can refer to&#13;
     <html:em class="italic">&#13;
      Improving Language Understanding with unsupervised learning&#13;
     </html:em>&#13;
     – Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya&#13;
     <html:span class="No-Break">&#13;
      Sutskever (&#13;
     </html:span>&#13;
     <html:a>&#13;
      <html:span class="No-Break">&#13;
       https://openai.com/research/language-unsupervised&#13;
      </html:span>&#13;
     </html:a>&#13;
     <html:span class="No-Break">&#13;
      ).&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     The best-performing LLMs such as GPT-4, Claude 2.1, and Llama 2 contain trillions of parameters&#13;
     <html:a id="_idIndexMarker017">&#13;
     </html:a>&#13;
     and have been trained on internet-scale datasets using advanced&#13;
     <html:strong class="bold">&#13;
      deep learning&#13;
     </html:strong>&#13;
     (&#13;
     <html:strong class="bold">&#13;
      DL&#13;
     </html:strong>&#13;
     ) techniques. The resulting model has an extensive vocabulary and a broad knowledge of language structure such as grammar and syntax, and about the world in general. Thanks to their unique traits, LLMs are able to generate text that is coherent, grammatically correct, and semantically relevant. The outputs they produce may not always be completely logical or factually accurate, but they usually read convincingly like being written by a human. But it’s not all about size. The quality of data and training algorithms – among others – can also play a huge role in the resulting performance of a&#13;
     <html:span class="No-Break">&#13;
      particular model.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     Many models feature a user interface that allows for response generation through prompts. Additionally, some offer an API for developers to access the model programmatically. This method will be our primary focus in the upcoming chapters of&#13;
     <html:span class="No-Break">&#13;
      our book.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     Next up, we’ll talk about how LLMs are making big changes in tech. They’re helping not just big companies but everyone. Curious? Let’s&#13;
     <html:span class="No-Break">&#13;
      keep reading.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:a id="_idTextAnchor019">&#13;
    </html:a>&#13;
   </html:div>&#13;
  </html:div>&#13;
 </html:body>&#13;
</html:html>
<html:html>&#13;
 <html:head>&#13;
  <html:title>&#13;
   Understanding the role of LLMs in modern technology&#13;
  </html:title>&#13;
 </html:head>&#13;
 <html:body>&#13;
  <html:div class="epub-source">&#13;
   <html:h1 id="_idParaDest-20">&#13;
    Understanding the role of LLMs in modern technology&#13;
   </html:h1>&#13;
   <html:div id="_idContainer016">&#13;
    <html:p>&#13;
     Oh! What good times we are living in. There has never been a more favorable era for small businesses&#13;
     <html:a id="_idIndexMarker018">&#13;
     </html:a>&#13;
     and entrepreneurs. Given the enormous potential of this technology, it’s a real miracle that, instead of ending up strictly under the control of large corporations or governments, it is literally within everyone’s reach. Now, it’s truly possible for almost anyone – even a non-technical person – to realize their ideas and solve problems that until now seemed impossible to solve without a huge amount&#13;
     <html:span class="No-Break">&#13;
      of resources.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     The disruptive potential that LLMs have – in almost all industries –&#13;
     <html:span class="No-Break">&#13;
      is enormous.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     It’s true: there are concerns that this technology could replace us. However, technology’s role is to make lives easier, taking over repetitive activities. As before, we’ll likely do the same things, only much more efficiently and better with LLMs’ help. We will do more&#13;
     <html:span class="No-Break">&#13;
      with less.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     I would dare say that LLMs have become the foundation of NLG technology. They can already power chatbots, search engines, coding assistants, text summarization tools, and other applications that synthesize written text interactively or automatically. And their capabilities keep advancing rapidly with bigger datasets&#13;
     <html:span class="No-Break">&#13;
      and models.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     And then, there are also the&#13;
     <html:strong class="bold">&#13;
      agents&#13;
     </html:strong>&#13;
     . These automated wonders are capable of perceiving and interpreting&#13;
     <html:em class="italic">&#13;
      stimuli&#13;
     </html:em>&#13;
     from the digital environment – and not just digital – to make decisions and act accordingly. Backed by the power of an LLM, intelligent agents can solve complex problems and fundamentally change the way we interact with technology. We’ll cover this topic in more detail throughout&#13;
     <html:a>&#13;
      <html:span class="No-Break">&#13;
       <html:em class="italic">&#13;
        Chapter 8&#13;
       </html:em>&#13;
      </html:span>&#13;
     </html:a>&#13;
     ,&#13;
     <html:em class="italic">&#13;
      Building Chatbots and Agents&#13;
     </html:em>&#13;
     <html:span class="No-Break">&#13;
      <html:em class="italic">&#13;
       with LlamaIndex&#13;
      </html:em>&#13;
     </html:span>&#13;
     <html:span class="No-Break">&#13;
      .&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     Despite their relatively short existence, LLMs have already proven to be remarkably versatile and powerful. With the right techniques and prompts, their output can be steered in useful directions at scale. LLMs are driving innovation in numerous fields as their generative powers continue to evolve. Their capabilities keep expanding from nuanced dialog to multimodal intelligence. And, at the moment, the LLM-powered wave of innovation across industries and technologies shows no signs of&#13;
     <html:span class="No-Break">&#13;
      slowing down.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     <html:strong class="bold">&#13;
      The Gartner Hype Cycle model&#13;
     </html:strong>&#13;
     serves&#13;
     <html:a id="_idIndexMarker019">&#13;
     </html:a>&#13;
     as a strategic guide for technology leaders, helping them evaluate new technologies not just on their merits but also in the context of their organization’s specific needs and&#13;
     <html:span class="No-Break">&#13;
      goals (&#13;
     </html:span>&#13;
     <html:a>&#13;
      <html:span class="No-Break">&#13;
       https://www.gartner.com/en/research/methodologies/gartner-hype-cycle&#13;
      </html:span>&#13;
     </html:a>&#13;
     <html:span class="No-Break">&#13;
      ).&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     Judging by current adoption levels, LLMs are currently well into the&#13;
     <html:strong class="bold">&#13;
      Slope of Enlightenment&#13;
     </html:strong>&#13;
     stage, ready to take off into the&#13;
     <html:strong class="bold">&#13;
      Plateau of Productivity&#13;
     </html:strong>&#13;
     – where mainstream adoption really starts to take off (&#13;
     <html:span class="No-Break">&#13;
      <html:em class="italic">&#13;
       Figure 1&#13;
      </html:em>&#13;
     </html:span>&#13;
     <html:em class="italic">&#13;
      .2&#13;
     </html:em>&#13;
     ). Companies are becoming more pragmatic&#13;
     <html:a id="_idIndexMarker020">&#13;
     </html:a>&#13;
     about their application, focusing on specialized use cases where they offer the&#13;
     <html:span class="No-Break">&#13;
      most value:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:div>&#13;
     <html:div class="IMG---Figure" id="_idContainer012">&#13;
      <html:img src="../Images/B21861_1_02.jpg"/>&#13;
     </html:div>&#13;
    </html:div>&#13;
    <html:p class="IMG---Caption" lang="en-US">&#13;
     Figure 1.2 – The Gartner Hype Cycle&#13;
    </html:p>&#13;
    <html:p>&#13;
     But, unlike other more specific technologies, LLMs are rather a new form of infrastructure – a kind of ecosystem where new concepts will be able to manifest and, undoubtedly, revolutionary applications will&#13;
     <html:span class="No-Break">&#13;
      be born.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     This is their true potential, and this is the ideal time to learn how to take advantage of the opportunities&#13;
     <html:span class="No-Break">&#13;
      they offer.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     Before we jump into innovative solutions that could maximize LLMs’ capabilities, let’s take a step back and look at some challenges&#13;
     <html:span class="No-Break">&#13;
      and limitations.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:a id="_idTextAnchor020">&#13;
    </html:a>&#13;
   </html:div>&#13;
  </html:div>&#13;
 </html:body>&#13;
</html:html>
<html:html>&#13;
 <html:head>&#13;
  <html:title>&#13;
   Exploring challenges with LLMs&#13;
  </html:title>&#13;
 </html:head>&#13;
 <html:body>&#13;
  <html:div class="epub-source">&#13;
   <html:h1 id="_idParaDest-21">&#13;
    Exploring challenges with LLMs&#13;
   </html:h1>&#13;
   <html:div id="_idContainer016">&#13;
    <html:p>&#13;
     Not all the news is good, however. It’s time to also discuss the&#13;
     <html:em class="italic">&#13;
      darker&#13;
     </html:em>&#13;
     side&#13;
     <html:span class="No-Break">&#13;
      of LLMs.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     These&#13;
     <html:a id="_idIndexMarker021">&#13;
     </html:a>&#13;
     models do have important limitations and some collateral effects too. Here is a list of the most important ones, but please consider it non-exhaustive. There may be others not included here, and the order is&#13;
     <html:span class="No-Break">&#13;
      arbitrarily chosen:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:ul>&#13;
     <html:li>&#13;
      They lack access to&#13;
      <html:span class="No-Break">&#13;
       real-time data.&#13;
      </html:span>&#13;
      <html:ul>&#13;
       <html:li>&#13;
        LLMs are trained on a static dataset, meaning that the information they have is only as up to date as the data they were trained on, which might not include the latest news, scientific discoveries, or&#13;
        <html:span class="No-Break">&#13;
         social trends.&#13;
        </html:span>&#13;
       </html:li>&#13;
       <html:li>&#13;
        This limitation can be critical when users seek real-time or recent information, as the LLMs might provide outdated or irrelevant responses. Furthermore, even if they cite data or statistics, these numbers are likely to have changed or evolved, leading to&#13;
        <html:span class="No-Break">&#13;
         potential misinformation.&#13;
        </html:span>&#13;
       </html:li>&#13;
      </html:ul>&#13;
     </html:li>&#13;
    </html:ul>&#13;
    <html:p class="callout-heading">&#13;
     Note&#13;
    </html:p>&#13;
    <html:p class="callout">&#13;
     While recent features introduced by OpenAI, for example, allow the underlying LLM to integrate with Bing to retrieve fresh context from the internet, that’s not an inherent feature of the LLM but rather an augmentation provided by the&#13;
     <html:span class="No-Break">&#13;
      ChatGPT interface.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:ul>&#13;
     <html:li>&#13;
      This lack of real-time updating also means that LLMs – by themselves – are not suited for tasks such as live customer service queries that may require real-time access to user data, inventory levels, or system statuses,&#13;
      <html:span class="No-Break">&#13;
       for example.&#13;
      </html:span>&#13;
     </html:li>&#13;
    </html:ul>&#13;
    <html:ul>&#13;
     <html:li>&#13;
      They have no intrinsic way of distinguishing factual truth&#13;
      <html:span class="No-Break">&#13;
       from falsehoods.&#13;
      </html:span>&#13;
      <html:ul>&#13;
       <html:li>&#13;
        Without proper monitoring, they can generate convincing misinformation. And trust me – they don’t do it on purpose. In very simple terms, LLMs are basically just looking for words that&#13;
        <html:span class="No-Break">&#13;
         fit together.&#13;
        </html:span>&#13;
       </html:li>&#13;
       <html:li>&#13;
        Check out&#13;
        <html:em class="italic">&#13;
         Figure 1.3&#13;
        </html:em>&#13;
        for an example of how one of the previous versions of the GPT-3.5 model would produce&#13;
        <html:span class="No-Break">&#13;
         false information:&#13;
        </html:span>&#13;
       </html:li>&#13;
      </html:ul>&#13;
     </html:li>&#13;
    </html:ul>&#13;
    <html:div>&#13;
     <html:div class="IMG---Figure" id="_idContainer013">&#13;
      <html:img src="../Images/B21861_1_03.jpg"/>&#13;
     </html:div>&#13;
    </html:div>&#13;
    <html:p class="IMG---Caption" lang="en-US">&#13;
     Figure 1.3 – Screenshot from a GPT 3.5-turbo-instruct playground&#13;
    </html:p>&#13;
    <html:ul>&#13;
     <html:li>&#13;
      As these models stochastically (randomly) generate text, their outputs are not guaranteed to be completely logical, factual, or harmless. Also, the training&#13;
      <html:a id="_idIndexMarker022">&#13;
      </html:a>&#13;
      data inherently biases the model, and LLMs may generate toxic, incorrect, or nonsensical text without warning. Since this data sometimes includes unsavory elements of online discourse, LLMs risk amplifying harmful biases and toxic content present in their&#13;
      <html:span class="No-Break">&#13;
       training data.&#13;
      </html:span>&#13;
     </html:li>&#13;
    </html:ul>&#13;
    <html:p class="callout-heading">&#13;
     Note&#13;
    </html:p>&#13;
    <html:p class="callout">&#13;
     While this kind of result may be easily achieved in a playground environment, using an older AI model, OpenAI’s ChatGPT interface uses newer models and employs additional guardrails, thus making these kinds of responses much&#13;
     <html:span class="No-Break">&#13;
      less probable.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:ul>&#13;
     <html:li>&#13;
      They also cannot maintain context and memory over&#13;
      <html:span class="No-Break">&#13;
       long documents.&#13;
      </html:span>&#13;
      <html:ul>&#13;
       <html:li>&#13;
        An interaction with a vanilla-flavor, standard LLM can prove to be a charm for simple topics or a quick question-and-answer session. But go beyond the context window limit of the model, and you’ll soon experience its limitations as it struggles to maintain coherence and may lose important details from earlier parts of the conversation or document. This can result in fragmented or incomplete responses that may not fully address the complexities of a long-form interaction or in-depth analysis, just like a human suffering from&#13;
        <html:em class="italic">&#13;
         short-term&#13;
        </html:em>&#13;
        <html:span class="No-Break">&#13;
         <html:em class="italic">&#13;
          memory loss&#13;
         </html:em>&#13;
        </html:span>&#13;
        <html:span class="No-Break">&#13;
         .&#13;
        </html:span>&#13;
       </html:li>&#13;
      </html:ul>&#13;
     </html:li>&#13;
    </html:ul>&#13;
    <html:p class="callout-heading">&#13;
     Note&#13;
    </html:p>&#13;
    <html:p class="callout">&#13;
     Although recently released AI models such as Anthropic’s Claude 2.1 and Google’s Gemini Pro 1.5 have dramatically raised the bar in terms of context window limit, ingesting an entire book and running inference on such a large context may prove to be prohibitive from a&#13;
     <html:span class="No-Break">&#13;
      cost perspective.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:ul>&#13;
     <html:li>&#13;
      LLMs also&#13;
      <html:a id="_idIndexMarker023">&#13;
      </html:a>&#13;
      exhibit unpredictable failures in reasoning and fact retention. Take a look at&#13;
      <html:span class="No-Break">&#13;
       <html:em class="italic">&#13;
        Figure 1&#13;
       </html:em>&#13;
      </html:span>&#13;
      <html:em class="italic">&#13;
       .4&#13;
      </html:em>&#13;
      for a typical logic reasoning problem that proves to be challenging even for newer models such&#13;
      <html:span class="No-Break">&#13;
       as GPT-4:&#13;
      </html:span>&#13;
     </html:li>&#13;
    </html:ul>&#13;
    <html:div>&#13;
     <html:div class="IMG---Figure" id="_idContainer014">&#13;
      <html:img src="../Images/B21861_1_04.jpg"/>&#13;
     </html:div>&#13;
    </html:div>&#13;
    <html:p class="IMG---Caption" lang="en-US">&#13;
     Figure 1.4 – Screenshot from a GPT-4 playground&#13;
    </html:p>&#13;
    <html:ul>&#13;
     <html:li>&#13;
      In this example, the answer is wrong because the only scenario that fits is if Emily is the one telling the truth. The treasure would then be neither in the attic nor in&#13;
      <html:span class="No-Break">&#13;
       the basement.&#13;
      </html:span>&#13;
     </html:li>&#13;
     <html:li>&#13;
      Their capabilities beyond fluent text generation remain inconsistent and limited. Blindly trusting their output without skepticism&#13;
      <html:span class="No-Break">&#13;
       invites errors.&#13;
      </html:span>&#13;
     </html:li>&#13;
    </html:ul>&#13;
    <html:ul>&#13;
     <html:li>&#13;
      The complexity of massive LLMs also reduces transparency into&#13;
      <html:span class="No-Break">&#13;
       their functioning.&#13;
      </html:span>&#13;
      <html:ul>&#13;
       <html:li>&#13;
        The lack of interpretability makes it hard to audit for issues or understand exactly when and why they fail. All you get is the output, but there’s no easy way of knowing the actual decision process that led to that output or the documented fact in which that particular output is grounded. As such, LLMs still require careful governance to mitigate risks from biased, false, or&#13;
        <html:span class="No-Break">&#13;
         dangerous outputs.&#13;
        </html:span>&#13;
       </html:li>&#13;
      </html:ul>&#13;
     </html:li>&#13;
     <html:li>&#13;
      As with&#13;
      <html:a id="_idIndexMarker024">&#13;
      </html:a>&#13;
      many other things out there, it turns out we cannot really call them sustainable. At least&#13;
      <html:span class="No-Break">&#13;
       not yet.&#13;
      </html:span>&#13;
      <html:ul>&#13;
       <html:li>&#13;
        Their massive scale makes them expensive to train and environmentally costly due to huge computing requirements. And it’s not just the training itself but also their usage. According to some estimates, “&#13;
        <html:em class="italic">&#13;
         the water consumption of ChatGPT has been estimated at 500 milliliters for a session of 20-50 queries&#13;
        </html:em>&#13;
        ” –&#13;
        <html:em class="italic">&#13;
         AMPLIFY, VOL. 36, NO. 8&#13;
        </html:em>&#13;
        :&#13;
        <html:em class="italic">&#13;
         Arthur D. Little’s Greg Smith, Michael Bateman, Remy Gillet, and Eystein Thanisch&#13;
        </html:em>&#13;
        (&#13;
        <html:a>&#13;
         https://www.cutter.com/article/environmental-impact-large-language-models&#13;
        </html:a>&#13;
        ). This is not negligible by any means. Think about the countless failed attempts to get an answer from an LLM, then multiply that with the countless users exercising their prompt engineering skills&#13;
        <html:span class="No-Break">&#13;
         every minute.&#13;
        </html:span>&#13;
       </html:li>&#13;
      </html:ul>&#13;
     </html:li>&#13;
     <html:li>&#13;
      And here’s some more bad news: as models advance in complexity and training techniques, LLMs are rapidly becoming a huge source of&#13;
      <html:span class="No-Break">&#13;
       machine-generated text.&#13;
      </html:span>&#13;
      <html:ul>&#13;
       <html:li>&#13;
        So huge, in fact, that according to predictions, it will end up almost entirely replacing human-generated text (&#13;
        <html:em class="italic">&#13;
         Brown, Tom B. et al. (2020)&#13;
        </html:em>&#13;
        .&#13;
        <html:em class="italic">&#13;
         Language Models are Few-Shot Learners&#13;
        </html:em>&#13;
        .&#13;
        <html:em class="italic">&#13;
         arXiv:2005.14165 [&#13;
        </html:em>&#13;
        <html:span class="No-Break">&#13;
         <html:em class="italic">&#13;
          cs.CL]&#13;
         </html:em>&#13;
        </html:span>&#13;
        <html:span class="No-Break">&#13;
         .&#13;
        </html:span>&#13;
        <html:a>&#13;
         <html:span class="No-Break">&#13;
          https://arxiv.org/abs/2005.14165&#13;
         </html:span>&#13;
        </html:a>&#13;
        <html:span class="No-Break">&#13;
         ).&#13;
        </html:span>&#13;
       </html:li>&#13;
       <html:li>&#13;
        In a way, this means they may become the victims of their own success. As more and more data is generated by AI, it gradually&#13;
        <html:em class="italic">&#13;
         contaminates&#13;
        </html:em>&#13;
        the training of new models, decreasing&#13;
        <html:span class="No-Break">&#13;
         their capabilities.&#13;
        </html:span>&#13;
       </html:li>&#13;
       <html:li>&#13;
        As in biology, any ecosystem that cannot maintain a healthy diversity in its genetic pool will&#13;
        <html:span class="No-Break">&#13;
         gradually degrade.&#13;
        </html:span>&#13;
       </html:li>&#13;
      </html:ul>&#13;
     </html:li>&#13;
    </html:ul>&#13;
    <html:p>&#13;
     <html:em class="italic">&#13;
      I saved the good news&#13;
     </html:em>&#13;
     <html:span class="No-Break">&#13;
      <html:em class="italic">&#13;
       for last.&#13;
      </html:em>&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     What if I told you there is at least one solution that can partially address almost all&#13;
     <html:span class="No-Break">&#13;
      these problems?&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     In many ways, a language model is very similar to an operating system. It provides a foundational layer upon which applications can be built. Just as an operating system manages&#13;
     <html:a id="_idIndexMarker025">&#13;
     </html:a>&#13;
     hardware resources and provides services for computer programs, LLMs manage linguistic resources and provide services&#13;
     <html:a id="_idIndexMarker026">&#13;
     </html:a>&#13;
     for various&#13;
     <html:strong class="bold">&#13;
      NL processing&#13;
     </html:strong>&#13;
     (&#13;
     <html:strong class="bold">&#13;
      NLP&#13;
     </html:strong>&#13;
     ) tasks. Using prompts to interact with them is much like writing code using an Assembly Language. It’s a low-level interaction. But, as you’ll soon find out, there are more sophisticated and practical ways of using LLMs to their&#13;
     <html:span class="No-Break">&#13;
      full potential.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     It’s time to talk&#13;
     <html:span class="No-Break">&#13;
      about RAG.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:a id="_idTextAnchor021">&#13;
    </html:a>&#13;
   </html:div>&#13;
  </html:div>&#13;
 </html:body>&#13;
</html:html>
<html:html>&#13;
 <html:head>&#13;
  <html:title>&#13;
   Augmenting LLMs with RAG&#13;
  </html:title>&#13;
 </html:head>&#13;
 <html:body>&#13;
  <html:div class="epub-source">&#13;
   <html:h1 id="_idParaDest-22">&#13;
    Augmenting LLMs with RAG&#13;
   </html:h1>&#13;
   <html:div id="_idContainer016">&#13;
    <html:p>&#13;
     Coined for&#13;
     <html:a id="_idIndexMarker027">&#13;
     </html:a>&#13;
     the first time in a 2020&#13;
     <html:a id="_idIndexMarker028">&#13;
     </html:a>&#13;
     paper,&#13;
     <html:em class="italic">&#13;
      Lewis, Patrick et al. (2005). “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks”. arXiv:2005.11401 [cs.CL]&#13;
     </html:em>&#13;
     (&#13;
     <html:a>&#13;
      https://arxiv.org/abs/2005.11401&#13;
     </html:a>&#13;
     ), published by several researchers from Meta, RAG is a technique that combines the powers of retrieval methods and generative models to answer user questions. The idea is to first retrieve relevant information from an indexed data source containing proprietary knowledge and then use that retrieved information to generate a more informed, context-rich response using a generative model (&#13;
     <html:span class="No-Break">&#13;
      <html:em class="italic">&#13;
       Figure 1&#13;
      </html:em>&#13;
     </html:span>&#13;
     <html:span class="No-Break">&#13;
      <html:em class="italic">&#13;
       .5&#13;
      </html:em>&#13;
     </html:span>&#13;
     <html:span class="No-Break">&#13;
      ):&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:div>&#13;
     <html:div class="IMG---Figure" id="_idContainer015">&#13;
      <html:img src="../Images/B21861_1_05.jpg"/>&#13;
     </html:div>&#13;
    </html:div>&#13;
    <html:p class="IMG---Caption" lang="en-US">&#13;
     Figure 1.5 – A RAG model&#13;
    </html:p>&#13;
    <html:p>&#13;
     Let’s&#13;
     <html:a id="_idIndexMarker029">&#13;
     </html:a>&#13;
     have a look at what this&#13;
     <html:a id="_idIndexMarker030">&#13;
     </html:a>&#13;
     means&#13;
     <html:span class="No-Break">&#13;
      in practice:&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:ul>&#13;
     <html:li>&#13;
      <html:strong class="bold">&#13;
       Much better fact retention&#13;
      </html:strong>&#13;
      : One of the advantages of using RAG is its ability to pull from specific data sources, which can improve fact retention. Instead of relying solely on the generative model’s own&#13;
      <html:em class="italic">&#13;
       knowledge&#13;
      </html:em>&#13;
      – which is mostly generic – it refers to external documents to construct its answers, increasing the chances that the information&#13;
      <html:span class="No-Break">&#13;
       is accurate.&#13;
      </html:span>&#13;
     </html:li>&#13;
     <html:li>&#13;
      <html:strong class="bold">&#13;
       Improved reasoning&#13;
      </html:strong>&#13;
      : The retrieval step allows RAG models to pull in information that is specifically related to the question. In general, this would result in more logical and coherent reasoning. This could help overcome limitations in reasoning that many&#13;
      <html:span class="No-Break">&#13;
       LLMs face.&#13;
      </html:span>&#13;
     </html:li>&#13;
     <html:li>&#13;
      <html:strong class="bold">&#13;
       Context relevance&#13;
      </html:strong>&#13;
      : Because it pulls information from external sources based on the query, RAG can be more contextually accurate than a standalone generative model, which has to rely only on its training data and might not have the most up-to-date or contextually relevant information. Not only that, but you could also get an actual&#13;
      <html:em class="italic">&#13;
       quote&#13;
      </html:em>&#13;
      from the model regarding the source of the actual knowledge used in&#13;
      <html:span class="No-Break">&#13;
       the answer.&#13;
      </html:span>&#13;
     </html:li>&#13;
     <html:li>&#13;
      <html:strong class="bold">&#13;
       Reduced trust issues&#13;
      </html:strong>&#13;
      : While not foolproof, the hybrid approach means that RAG could, in principle, be less prone to generating completely false or nonsensical answers. That means an increased probability of receiving a&#13;
      <html:span class="No-Break">&#13;
       valid output.&#13;
      </html:span>&#13;
     </html:li>&#13;
     <html:li>&#13;
      <html:strong class="bold">&#13;
       Validation&#13;
      </html:strong>&#13;
      : It’s often easier to validate the reliability of the retrieved documents in an RAG setup by setting up a mechanism to provide a reference to the original information used for generating a response. This could be a step toward more transparent and trustworthy&#13;
      <html:span class="No-Break">&#13;
       model behavior.&#13;
      </html:span>&#13;
     </html:li>&#13;
    </html:ul>&#13;
    <html:p class="callout-heading">&#13;
     A word of caution&#13;
    </html:p>&#13;
    <html:p class="callout">&#13;
     Even&#13;
     <html:a id="_idIndexMarker031">&#13;
     </html:a>&#13;
     if RAG makes LLMs better&#13;
     <html:a id="_idIndexMarker032">&#13;
     </html:a>&#13;
     and more reliable, it doesn’t completely fix the issue of them sometimes giving wrong or confusing answers. There is no silver bullet that will completely eliminate all the issues mentioned previously. It’s still a good idea to double-check and evaluate their outputs, and we’ll talk about ways of doing that later in the book. Because, as you may already know or you’ve probably guessed by now, LlamaIndex is one of the many ways of augmenting LLM-based applications using RAG. And a very effective one, I&#13;
     <html:span class="No-Break">&#13;
      should add.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     While some LLM providers have started introducing RAG components into their API, such as OpenAI’s&#13;
     <html:strong class="bold">&#13;
      Assistants&#13;
     </html:strong>&#13;
     feature, using a standalone framework such as LlamaIndex provides many more customization options. It also enables the usage of local models, enabling self-hosted solutions and greatly reducing costs and privacy concerns associated with a&#13;
     <html:span class="No-Break">&#13;
      hosted model.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:a id="_idTextAnchor022">&#13;
    </html:a>&#13;
   </html:div>&#13;
  </html:div>&#13;
 </html:body>&#13;
</html:html>
<html:html>&#13;
 <html:head>&#13;
  <html:title>&#13;
   Summary&#13;
  </html:title>&#13;
 </html:head>&#13;
 <html:body>&#13;
  <html:div class="epub-source">&#13;
   <html:h1 id="_idParaDest-23">&#13;
    Summary&#13;
   </html:h1>&#13;
   <html:div id="_idContainer016">&#13;
    <html:p>&#13;
     In this chapter, we covered a quick introduction to GenAI and LLMs. You learned how LLMs such as GPT work and some of their capabilities and limitations. A key takeaway is that while powerful, LLMs have weaknesses – such as the potential for false information and lack of reasoning – that require mitigation techniques. We discussed RAG as one method to overcome some&#13;
     <html:span class="No-Break">&#13;
      LLM limitations.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     These lessons provide useful background on how to approach LLMs practically while being aware of their risks. At the same time, you learned the importance of techniques such as RAG to address LLMs’&#13;
     <html:span class="No-Break">&#13;
      potential downsides.&#13;
     </html:span>&#13;
    </html:p>&#13;
    <html:p>&#13;
     With this introductory foundation in place, we are now ready to dive into the next chapter where we will explore the LlamaIndex ecosystem. LlamaIndex offers an effective RAG framework to augment LLMs with indexed data for more accurate, logical outputs. Learning to leverage LlamaIndex tools will be the natural next step to harness the power of LLMs in a&#13;
     <html:span class="No-Break">&#13;
      proficient way.&#13;
     </html:span>&#13;
    </html:p>&#13;
   </html:div>&#13;
  </html:div>&#13;
 </html:body>&#13;
</html:html></body></html>