- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Understanding the Theory Behind Diffusion Models
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç†è§£æ‰©æ•£æ¨¡å‹èƒŒåçš„ç†è®º
- en: This chapter will dive into the theory that powers **diffusion models** and
    see the internal workings of the system. How could a neural network model generate
    such realistic images? Curious minds would like to lift the cover and see the
    internal workings.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« å°†æ·±å…¥æ¢è®¨é©±åŠ¨**æ‰©æ•£æ¨¡å‹**çš„ç†è®ºï¼Œå¹¶äº†è§£ç³»ç»Ÿçš„å†…éƒ¨å·¥ä½œåŸç†ã€‚ç¥ç»ç½‘ç»œæ¨¡å‹æ˜¯å¦‚ä½•ç”Ÿæˆå¦‚æ­¤é€¼çœŸçš„å›¾åƒçš„å‘¢ï¼Ÿå¥½å¥‡å¿ƒå¼ºçš„äººæƒ³è¦æ­å¼€é¢çº±ï¼Œçœ‹çœ‹å†…éƒ¨çš„å·¥ä½œæœºåˆ¶ã€‚
- en: We are going to touch on the foundation of the diffusion model, aiming to figure
    out how it works internally and pave the foundation to implement a workable pipeline
    in the next chapter.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†è§¦åŠæ‰©æ•£æ¨¡å‹çš„åŸºç¡€ï¼Œæ—¨åœ¨å¼„æ¸…æ¥šå…¶å†…éƒ¨å·¥ä½œåŸç†ï¼Œå¹¶ä¸ºä¸‹ä¸€ç« å®ç°å¯è¡Œçš„æµç¨‹å¥ å®šåŸºç¡€ã€‚
- en: By comprehending the intricacies of diffusion models, we not only enhance our
    understanding of the advanced **Stable Diffusion** (also known as **latent diffusion
    models** (**LDMs**)) but also gain the ability to navigate the source code of
    the Diffusers package more effectively.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡ç†è§£æ‰©æ•£æ¨¡å‹çš„å¤æ‚æ€§ï¼Œæˆ‘ä»¬ä¸ä»…å¢å¼ºäº†æˆ‘ä»¬å¯¹é«˜çº§**ç¨³å®šæ‰©æ•£**ï¼ˆä¹Ÿç§°ä¸º**æ½œåœ¨æ‰©æ•£æ¨¡å‹**ï¼ˆ**LDMs**ï¼‰ï¼‰çš„ç†è§£ï¼Œè€Œä¸”è¿˜èƒ½æ›´æœ‰æ•ˆåœ°å¯¼èˆªDiffusersåŒ…çš„æºä»£ç ã€‚
- en: This knowledge will enable us to extend the packageâ€™s features in line with
    emerging requirements.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é¡¹çŸ¥è¯†å°†ä½¿æˆ‘ä»¬èƒ½å¤Ÿæ ¹æ®æ–°å…´éœ€æ±‚æ‰©å±•åŒ…çš„åŠŸèƒ½ã€‚
- en: 'Specifically, we will go through the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†æ¢è®¨ä»¥ä¸‹ä¸»é¢˜ï¼š
- en: Understanding the image-to-noise process
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç†è§£å›¾åƒåˆ°å™ªå£°çš„è¿‡ç¨‹
- en: A more efficient **forward** **diffusion process**
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ›´é«˜æ•ˆçš„**æ­£å‘****æ‰©æ•£è¿‡ç¨‹**
- en: The noise-to-image training process
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å™ªå£°åˆ°å›¾åƒçš„è®­ç»ƒè¿‡ç¨‹
- en: The noise-to-image sampling process
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å™ªå£°åˆ°å›¾åƒçš„é‡‡æ ·è¿‡ç¨‹
- en: Understanding Classifier Guidance denoising
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç†è§£åˆ†ç±»å™¨å¼•å¯¼å»å™ª
- en: By the end of this chapter, we will have taken a deep dive into the internal
    workings of the diffusion model initially brought out by Jonathan Ho et al. [4].
    We will understand the foundational idea of the diffusion model and learn about
    the **forward diffusion process**. We will understand the reverse diffusion process
    for diffusion model training and sampling and learn to enable a text-guided diffusion
    model.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°æœ¬ç« ç»“æŸæ—¶ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨ç”±Jonathan Hoç­‰äººæœ€åˆæå‡ºçš„æ‰©æ•£æ¨¡å‹çš„å†…éƒ¨å·¥ä½œåŸç†ã€‚æˆ‘ä»¬å°†ç†è§£æ‰©æ•£æ¨¡å‹çš„åŸºç¡€ç†å¿µï¼Œå¹¶å­¦ä¹ **æ­£å‘æ‰©æ•£è¿‡ç¨‹**ã€‚æˆ‘ä»¬å°†äº†è§£æ‰©æ•£æ¨¡å‹è®­ç»ƒå’Œé‡‡æ ·çš„åå‘æ‰©æ•£è¿‡ç¨‹ï¼Œå¹¶å­¦ä¼šå¯ç”¨æ–‡æœ¬å¼•å¯¼çš„æ‰©æ•£æ¨¡å‹ã€‚
- en: Letâ€™s get started.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å¼€å§‹å§ã€‚
- en: Understanding the image-to-noise process
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç†è§£å›¾åƒåˆ°å™ªå£°çš„è¿‡ç¨‹
- en: The idea of the diffusion model is inspired by the diffusion concept from thermodynamics.
    Take one image as a cup of water and add enough noise (ink) to the image (water)
    to finally turn the image (water) into a complete noise image (ink water).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰©æ•£æ¨¡å‹çš„æƒ³æ³•å—åˆ°äº†çƒ­åŠ›å­¦ä¸­æ‰©æ•£æ¦‚å¿µçš„å¯å‘ã€‚å°†ä¸€å¼ å›¾åƒè§†ä¸ºä¸€æ¯æ°´ï¼Œå¹¶å‘å›¾åƒï¼ˆæ°´ï¼‰ä¸­æ·»åŠ è¶³å¤Ÿçš„å™ªå£°ï¼ˆå¢¨æ°´ï¼‰ï¼Œæœ€ç»ˆå°†å›¾åƒï¼ˆæ°´ï¼‰å˜æˆå®Œæ•´çš„å™ªå£°å›¾åƒï¼ˆå¢¨æ°´æ°´ï¼‰ã€‚
- en: As shown in *Figure 4**.1*, image xÂ 0 can be converted to a nearly Gaussian
    (normally distributed) noise image xÂ T.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚*å›¾4**.1*æ‰€ç¤ºï¼Œå›¾åƒxÂ 0å¯ä»¥è¢«è½¬æ¢ä¸ºä¸€ä¸ªå‡ ä¹é«˜æ–¯ï¼ˆæ­£æ€åˆ†å¸ƒï¼‰çš„å™ªå£°å›¾åƒxÂ Tã€‚
- en: '![Figure 4.1: Forward diffusion and reverse denoising](img/B21263_04_01.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾4.1ï¼šæ­£å‘æ‰©æ•£å’Œåå‘å»å™ª](img/B21263_04_01.jpg)'
- en: 'Figure 4.1: Forward diffusion and reverse denoising'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4.1ï¼šæ­£å‘æ‰©æ•£å’Œåå‘å»å™ª
- en: We employ a predetermined forward diffusion process, denoted as q, which systematically
    introduces Gaussian noise to an image until it culminates in pure noise. The process
    is denoted by q(xÂ t | xÂ t-1). Note that the reverse process pÂ Î¸(xÂ t-1 | xÂ t) is
    still unknown.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é‡‡ç”¨ä¸€ä¸ªé¢„å®šçš„æ­£å‘æ‰©æ•£è¿‡ç¨‹ï¼Œè¡¨ç¤ºä¸ºqï¼Œè¯¥è¿‡ç¨‹ç³»ç»Ÿåœ°ç»™å›¾åƒå¼•å…¥é«˜æ–¯å™ªå£°ï¼Œç›´åˆ°æœ€ç»ˆå˜æˆçº¯å™ªå£°ã€‚è¿™ä¸ªè¿‡ç¨‹è¡¨ç¤ºä¸ºq(xÂ t | xÂ t-1)ã€‚è¯·æ³¨æ„ï¼Œåå‘è¿‡ç¨‹pÂ Î¸(xÂ t-1
    | xÂ t)ä»ç„¶æœªçŸ¥ã€‚
- en: 'One step of the forward diffusion process can be denoted as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å‘æ‰©æ•£è¿‡ç¨‹çš„ä¸€æ­¥å¯ä»¥è¡¨ç¤ºå¦‚ä¸‹ï¼š
- en: q(xÂ t | xÂ t-1) â‰” ğ’©(xÂ t; âˆšÂ _Â 1 âˆ’ Î²Â t xÂ t-1Â , Î²Â t I)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: q(xÂ t | xÂ t-1) â‰” ğ’©(xÂ t; âˆšÂ _Â 1 âˆ’ Î²Â t xÂ t-1Â , Î²Â t I)
- en: 'Let me explain this formula bit by bit from left to right:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»å·¦åˆ°å³ä¸€ç‚¹ä¸€ç‚¹è§£é‡Šè¿™ä¸ªå…¬å¼ï¼š
- en: The notation q(xÂ t | xÂ t-1) is used to denote a conditional probability distribution.
    In this case, the distribution q represents the probability of observing the noisy
    image xÂ t given the previous image xÂ tâˆ’1.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¬¦å·q(xÂ t | xÂ t-1)ç”¨æ¥è¡¨ç¤ºæ¡ä»¶æ¦‚ç‡åˆ†å¸ƒã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œåˆ†å¸ƒqè¡¨ç¤ºåœ¨ç»™å®šå…ˆå‰å›¾åƒxÂ tâˆ’1çš„æƒ…å†µä¸‹è§‚å¯Ÿåˆ°å™ªå£°å›¾åƒxÂ tçš„æ¦‚ç‡ã€‚
- en: 'The define sign : = is used in the formula instead of the tilde symbol (âˆ¼)
    because the diffusion forward process is a deterministic process. The tilde symbol
    (âˆ¼) is typically used to represent a distribution. In this case, if we used the
    tilde symbol, the formula would be saying that the noisy image is a complete Gaussian
    distribution. However, this is not the case. The noisy image in t step is defined
    by a deterministic function of the previous image and added noise.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å…¬å¼ä¸­ä½¿ç”¨å®šä¹‰ç¬¦å· := è€Œä¸æ˜¯æ³¢æµªç¬¦å· (âˆ¼) æ˜¯å› ä¸ºæ‰©æ•£å‰å‘è¿‡ç¨‹æ˜¯ä¸€ä¸ªç¡®å®šæ€§è¿‡ç¨‹ã€‚æ³¢æµªç¬¦å· (âˆ¼) é€šå¸¸ç”¨äºè¡¨ç¤ºåˆ†å¸ƒã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¦‚æœæˆ‘ä»¬ä½¿ç”¨æ³¢æµªç¬¦å·ï¼Œå…¬å¼å°†è¡¨ç¤ºå™ªå£°å›¾åƒæ˜¯ä¸€ä¸ªå®Œæ•´çš„é«˜æ–¯åˆ†å¸ƒã€‚ç„¶è€Œï¼Œæƒ…å†µå¹¶éå¦‚æ­¤ã€‚t
    æ­¥çš„å™ªå£°å›¾åƒæ˜¯ç”±å‰ä¸€ä¸ªå›¾åƒå’Œæ·»åŠ çš„å™ªå£°çš„ç¡®å®šæ€§å‡½æ•°å®šä¹‰çš„ã€‚
- en: Then why is ğ’© used here? The ğ’© symbol is used to represent a Gaussian distribution.
    However, in this case, the ğ’© symbol is being used to represent the functional
    form of the noisy image.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆä¸ºä»€ä¹ˆè¿™é‡Œä½¿ç”¨ ğ’© ç¬¦å·å‘¢ï¼Ÿğ’© ç¬¦å·ç”¨äºè¡¨ç¤ºé«˜æ–¯åˆ†å¸ƒã€‚ç„¶è€Œï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œğ’© ç¬¦å·è¢«ç”¨æ¥è¡¨ç¤ºå™ªå£°å›¾åƒçš„å‡½æ•°å½¢å¼ã€‚
- en: On the right side, before the semicolon, xÂ t is the thing we want to have in
    normal distribution. After the semicolon, those are the parameters of the distribution.
    A semicolon is usually used to separate the output and parameters.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨å³ä¾§ï¼Œåˆ†å·ä¹‹å‰ï¼Œx_t æ˜¯æˆ‘ä»¬å¸Œæœ›åœ¨æ­£æ€åˆ†å¸ƒä¸­çš„ä¸œè¥¿ã€‚åˆ†å·ä¹‹åï¼Œé‚£äº›æ˜¯åˆ†å¸ƒçš„å‚æ•°ã€‚åˆ†å·é€šå¸¸ç”¨äºåˆ†éš”è¾“å‡ºå’Œå‚æ•°ã€‚
- en: Î²Â t is the noise variance at step t. âˆšÂ _Â 1 âˆ’ Î²Â tÂ  xÂ tâˆ’1 is the mean of the new
    distribution.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Î²t æ˜¯ç¬¬ t æ­¥çš„å™ªå£°æ–¹å·®ã€‚âˆš_1 âˆ’ Î²t x_tâˆ’1 æ˜¯æ–°åˆ†å¸ƒçš„å‡å€¼ã€‚
- en: Why is the big I used in the formula? Because an RGB image can have multiple
    channels, and the identity matrix can apply the noise variance to different channels
    independently.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆå…¬å¼ä¸­ä½¿ç”¨å¤§å†™çš„ I å‘¢ï¼Ÿå› ä¸º RGB å›¾åƒå¯ä»¥æœ‰å¤šä¸ªé€šé“ï¼Œè€Œå•ä½çŸ©é˜µå¯ä»¥å°†å™ªå£°æ–¹å·®ç‹¬ç«‹åœ°åº”ç”¨äºä¸åŒçš„é€šé“ã€‚
- en: 'It is quite easy to add Gaussian noise to an image using Python:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Python æ·»åŠ é«˜æ–¯å™ªå£°åˆ°å›¾åƒç›¸å½“ç®€å•ï¼š
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To execute the preceding code, you will also need to install the `ipyplot`
    package by `pip install ipyplot`. The code provided performs a simulation of a
    forward diffusion process on an image and then visualizes the progression of this
    process over a number of iterations. Hereâ€™s a step-by-step explanation of what
    each part of the code is doing:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æ‰§è¡Œå‰é¢çš„ä»£ç ï¼Œæ‚¨è¿˜éœ€è¦é€šè¿‡ `pip install ipyplot` å®‰è£… `ipyplot` åŒ…ã€‚æä¾›çš„ä»£ç åœ¨å›¾åƒä¸Šæ‰§è¡Œå‰å‘æ‰©æ•£è¿‡ç¨‹çš„æ¨¡æ‹Ÿï¼Œç„¶åå¯è§†åŒ–è¿™ä¸ªè¿‡ç¨‹åœ¨å¤šæ¬¡è¿­ä»£ä¸­çš„è¿›å±•ã€‚ä»¥ä¸‹æ˜¯ä»£ç æ¯ä¸ªéƒ¨åˆ†æ‰€åšæ“ä½œçš„é€æ­¥è§£é‡Šï¼š
- en: 'Importing libraries:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¼å…¥åº“ï¼š
- en: '`ipyplot` is a library for plotting images in Jupyter notebooks in a more interactive
    way.'
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ipyplot` æ˜¯ä¸€ä¸ªåº“ï¼Œç”¨äºä»¥æ›´äº¤äº’çš„æ–¹å¼åœ¨ Jupyter ç¬”è®°æœ¬ä¸­ç»˜åˆ¶å›¾åƒã€‚'
- en: '`PIL` (which stands for `Image` module, is used for image manipulation.'
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PIL`ï¼ˆä»£è¡¨ `Image` æ¨¡å—ï¼Œç”¨äºå›¾åƒå¤„ç†ï¼‰ã€‚'
- en: 'Loading the image:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åŠ è½½å›¾åƒï¼š
- en: '`img_path` is defined as the path to the `image` file `dog.png`.'
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`img_path` è¢«å®šä¹‰ä¸º `image` æ–‡ä»¶ `dog.png` çš„è·¯å¾„ã€‚'
- en: '`image` is loaded using `plt.imread(img_path)`.'
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ `plt.imread(img_path)` åŠ è½½ `image`ã€‚
- en: 'Setting parameters:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è®¾ç½®å‚æ•°ï¼š
- en: '`num_iterations` defines the number of times the diffusion process will be
    simulated.'
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_iterations` å®šä¹‰äº†æ‰©æ•£è¿‡ç¨‹å°†è¢«æ¨¡æ‹Ÿçš„æ¬¡æ•°ã€‚'
- en: '`beta` is a parameter that simulates noise variance in the diffusion process.'
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beta` æ˜¯ä¸€ä¸ªå‚æ•°ï¼Œç”¨äºæ¨¡æ‹Ÿæ‰©æ•£è¿‡ç¨‹ä¸­çš„å™ªå£°æ–¹å·®ã€‚'
- en: 'Initializing lists:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åˆå§‹åŒ–åˆ—è¡¨ï¼š
- en: '`images` is initialized as an empty list, which will later hold the PIL image
    objects that result from each iteration of the diffusion process.'
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`images` è¢«åˆå§‹åŒ–ä¸ºä¸€ä¸ªç©ºåˆ—è¡¨ï¼Œå®ƒå°†éšåä¿å­˜æ‰©æ•£è¿‡ç¨‹æ¯æ¬¡è¿­ä»£äº§ç”Ÿçš„ PIL å›¾åƒå¯¹è±¡ã€‚'
- en: '`steps` is a list of strings that will act as labels for the images when they
    are plotted, indicating the step number for each image.'
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`steps` æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨ï¼Œå½“å›¾åƒè¢«ç»˜åˆ¶æ—¶å°†ä½œä¸ºæ ‡ç­¾ä½¿ç”¨ï¼ŒæŒ‡ç¤ºæ¯ä¸ªå›¾åƒçš„æ­¥éª¤ç¼–å·ã€‚'
- en: 'Forward diffusion process:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å‰å‘æ‰©æ•£è¿‡ç¨‹ï¼š
- en: A `for` loop runs for `num_iterations` times, each time performing a diffusion
    step. `mean` is computed by scaling the image with a factor of `sqrt(1 -` `beta)`.
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª `for` å¾ªç¯è¿è¡Œ `num_iterations` æ¬¡ï¼Œæ¯æ¬¡æ‰§è¡Œä¸€ä¸ªæ‰©æ•£æ­¥éª¤ã€‚`mean` é€šè¿‡å°†å›¾åƒä¹˜ä»¥ `sqrt(1 - beta)`
    çš„å› å­æ¥è®¡ç®—ã€‚
- en: A new image is generated by adding Gaussian noise to the mean, where the noise
    has a standard deviation of `beta`. This is done using `np.random.normal`.
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šè¿‡å‘å‡å€¼æ·»åŠ é«˜æ–¯å™ªå£°ç”Ÿæˆæ–°çš„å›¾åƒï¼Œå…¶ä¸­å™ªå£°çš„æ ‡å‡†å·®ä¸º `beta`ã€‚è¿™æ˜¯é€šè¿‡ `np.random.normal` å®ç°çš„ã€‚
- en: The resulting image array values are scaled to the range 0-255 and converted
    to an 8-bit unsigned integer format, which is a common format for images.
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç»“æœçš„å›¾åƒæ•°ç»„å€¼è¢«ç¼©æ”¾åˆ° 0-255 èŒƒå›´ï¼Œå¹¶è½¬æ¢ä¸º 8 ä½æ— ç¬¦å·æ•´æ•°æ ¼å¼ï¼Œè¿™æ˜¯å›¾åƒçš„å¸¸è§æ ¼å¼ã€‚
- en: '`pil_image` is created by converting the image array to a PIL image object
    in RGB mode.'
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pil_image` é€šè¿‡å°†å›¾åƒæ•°ç»„è½¬æ¢ä¸º RGB æ¨¡å¼çš„ PIL å›¾åƒå¯¹è±¡æ¥åˆ›å»ºã€‚'
- en: Plot the image using `ipyplot` in a grid as shown in *Figure 4**.2*.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ `ipyplot` åœ¨ç½‘æ ¼ä¸­ç»˜åˆ¶å›¾åƒï¼Œå¦‚å›¾ *å›¾ 4.2* æ‰€ç¤ºã€‚
- en: '![Figure 4.2: Add noise to the image](img/B21263_04_02.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ 4.2ï¼šå‘å›¾åƒæ·»åŠ å™ªå£°](img/B21263_04_02.jpg)'
- en: 'Figure 4.2: Add noise to the image'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 4.2ï¼šå‘å›¾åƒæ·»åŠ å™ªå£°
- en: From the result, we can see that even though every image is from a normal distribution
    function, not every image is a complete Gaussian distribution, or more strictly
    speaking, an `1000`, and later, in Stable Diffusion, the step number is reduced
    to between `20` to `50`.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ç»“æœä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œå°½ç®¡æ¯ä¸ªå›¾åƒéƒ½æ¥è‡ªæ­£æ€åˆ†å¸ƒå‡½æ•°ï¼Œä½†å¹¶éæ¯ä¸ªå›¾åƒéƒ½æ˜¯å®Œæ•´çš„é«˜æ–¯åˆ†å¸ƒï¼Œæˆ–è€…æ›´ä¸¥æ ¼åœ°è¯´ï¼Œæ˜¯ `1000`ï¼Œåæ¥åœ¨ç¨³å®šæ‰©æ•£ä¸­ï¼Œæ­¥éª¤æ•°å‡å°‘åˆ°
    `20` åˆ° `50` ä¹‹é—´ã€‚
- en: If the last image of *Figure 4**.2* is an isotropic Gaussian distribution, its
    2D distribution visualization will appear as a circle; it is characterized by
    having equal variances in all dimensions. In other words, the spread or width
    of the distribution is the same along all axes.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœ *å›¾ 4.2* çš„æœ€åä¸€å¹…å›¾åƒæ˜¯å„å‘åŒæ€§çš„é«˜æ–¯åˆ†å¸ƒï¼Œå…¶ 2D åˆ†å¸ƒå¯è§†åŒ–å°†å‘ˆç°ä¸ºä¸€ä¸ªåœ†åœˆï¼›å®ƒä»¥æ‰€æœ‰ç»´åº¦å…·æœ‰ç›¸ç­‰çš„æ–¹å·®ä¸ºç‰¹å¾ã€‚æ¢å¥è¯è¯´ï¼Œåˆ†å¸ƒçš„æ‰©æ•£æˆ–å®½åº¦æ²¿æ‰€æœ‰è½´éƒ½æ˜¯ç›¸åŒçš„ã€‚
- en: 'Letâ€™s plot an image pixel distribution after adding 16x times Gaussian noise:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬åœ¨æ·»åŠ  16 å€é«˜æ–¯å™ªå£°åç»˜åˆ¶å›¾åƒåƒç´ åˆ†å¸ƒï¼š
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The result is shown in *Figure 4**.3*.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœå¦‚å›¾ *å›¾ 4.3* æ‰€ç¤ºã€‚
- en: '![Figure 4.3: A nearly isotropic, normally distributed noise image](img/B21263_04_03.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ 4.3ï¼šå‡ ä¹å„å‘åŒæ€§çš„æ­£æ€åˆ†å¸ƒå™ªå£°å›¾åƒ](img/B21263_04_03.jpg)'
- en: 'Figure 4.3: A nearly isotropic, normally distributed noise image'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 4.3ï¼šå‡ ä¹å„å‘åŒæ€§çš„æ­£æ€åˆ†å¸ƒå™ªå£°å›¾åƒ
- en: The figure shows how the code efficiently transforms an image into a nearly
    isotropic, normally distributed noise image in just 16 steps, as illustrated in
    the last image of *Figure 4**.2*.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥å›¾æ˜¾ç¤ºäº†ä»£ç å¦‚ä½•ä»…ç”¨ 16 æ­¥é«˜æ•ˆåœ°å°†å›¾åƒè½¬æ¢ä¸ºå‡ ä¹å„å‘åŒæ€§çš„æ­£æ€åˆ†å¸ƒå™ªå£°å›¾åƒï¼Œå¦‚å›¾ *å›¾ 4.2* çš„æœ€åä¸€å¹…å›¾åƒæ‰€ç¤ºã€‚
- en: A more efficient forward diffusion process
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ›´é«˜æ•ˆçš„æ­£å‘æ‰©æ•£è¿‡ç¨‹
- en: If we use the chained process to calculate a noisy image at t step, it first
    requires calculating the noisy image from 1 to t âˆ’ 1 steps, which is not efficient.
    We can leverage a trick called **reparameterization** [10] to transform the original
    chained process into a one-step process. Here is what the trick looks like.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬ä½¿ç”¨é“¾å¼è¿‡ç¨‹åœ¨ t æ­¥è®¡ç®—ä¸€ä¸ªå™ªå£°å›¾åƒï¼Œå®ƒé¦–å…ˆéœ€è¦è®¡ç®—ä» 1 åˆ° t - 1 æ­¥çš„å™ªå£°å›¾åƒï¼Œè¿™å¹¶ä¸é«˜æ•ˆã€‚æˆ‘ä»¬å¯ä»¥åˆ©ç”¨ä¸€ç§ç§°ä¸º**é‡å‚æ•°åŒ–**[10]çš„æŠ€å·§å°†åŸå§‹é“¾å¼è¿‡ç¨‹è½¬æ¢ä¸ºä¸€æ­¥è¿‡ç¨‹ã€‚è¿™ä¸ªæŠ€å·§çœ‹èµ·æ¥æ˜¯è¿™æ ·çš„ã€‚
- en: 'If we have a Gaussian distribution z with Î¼ as the mean and ÏƒÂ 2 variance:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬æœ‰ä¸€ä¸ªä»¥ Î¼ ä¸ºå‡å€¼ã€ÏƒÂ² ä¸ºæ–¹å·®çš„é«˜æ–¯åˆ†å¸ƒ zï¼š
- en: z âˆ¼ ğ’©(Î¼, ÏƒÂ 2)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: z âˆ¼ ğ’©(Î¼, ÏƒÂ²)
- en: 'Then, we can rewrite the distribution as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥å°†åˆ†å¸ƒé‡å†™å¦‚ä¸‹ï¼š
- en: Ïµ âˆ¼ ğ’©(0,1)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Ïµ âˆ¼ ğ’©(0,1)
- en: z = Î¼+ ÏƒÏµ
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: z = Î¼ + ÏƒÏµ
- en: 'The benefit brought by this trick is that we can now calculate an image at
    any step with a one-step calculation, which will greatly boost the training performance:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæŠ€å·§å¸¦æ¥çš„å¥½å¤„æ˜¯ï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥ä¸€æ­¥è®¡ç®—è®¡ç®—ä»»ä½•æ­¥éª¤çš„å›¾åƒï¼Œè¿™å°†å¤§å¤§æé«˜è®­ç»ƒæ€§èƒ½ï¼š
- en: xÂ t = âˆšÂ _Â 1 âˆ’ Î²Â tÂ  xÂ tâˆ’1 + âˆšÂ _Â Î²Â tÂ  ÏµÂ tâˆ’1
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: x_t = âˆš(1 âˆ’ Î²_t) x_{tâˆ’1} + âˆš(Î²_t) Ïµ_{tâˆ’1}
- en: 'Now, say we define the following:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œå‡è®¾æˆ‘ä»¬å®šä¹‰ä»¥ä¸‹å†…å®¹ï¼š
- en: Î±Â t = 1 âˆ’ Î²Â t
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Î±_t = 1 âˆ’ Î²_t
- en: 'We now have the following:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æœ‰ä»¥ä¸‹å†…å®¹ï¼š
- en: _Â Î±Â t = âˆÂ i=1Â tÂ Î±Â i
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: _Î±_t = âˆ(i=1 to t) Î±_i
- en: 'There is no magic here; define Î±Â t and Î±Â â€¾Â Â t is only for convenience, so that
    we can calculate a noised image at step t and generate xÂ t from the source un-noised
    image xÂ 0 using the following equation:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ²¡æœ‰é­”æ³•ï¼›å®šä¹‰ Î±_t å’Œ Î±_â€¾_t åªæ˜¯ä¸ºäº†æ–¹ä¾¿ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥åœ¨ t æ­¥è®¡ç®—ä¸€ä¸ªå™ªå£°å›¾åƒï¼Œå¹¶ä½¿ç”¨ä»¥ä¸‹æ–¹ç¨‹ä»æºæœªå™ªå£°å›¾åƒ x_0 ç”Ÿæˆ x_tï¼š
- en: xÂ t = âˆšÂ _Â _Â Î±Â tÂ  xÂ 0 + âˆšÂ _Â 1 âˆ’ _Â Î±Â t
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: x_t = âˆš(1 âˆ’ Î²_t) x_{tâˆ’1} + âˆš(Î²_t) Ïµ_{tâˆ’1}
- en: What do Î±Â t and Î±Â â€¾Â Â t look like? Here is a simplified sample (*Figure 4**.4*).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Î±_t å’Œ Î±_â€¾_t çœ‹èµ·æ¥æ˜¯ä»€ä¹ˆæ ·å­ï¼Ÿè¿™é‡Œæ˜¯ä¸€ä¸ªç®€åŒ–çš„ç¤ºä¾‹ (*å›¾ 4.4*)ã€‚
- en: '![Figure 4.4: Implementation of reparameterization](img/B21263_04_04.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ 4.4ï¼šé‡å‚æ•°åŒ–å®ç°](img/B21263_04_04.jpg)'
- en: 'Figure 4.4: Implementation of reparameterization'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 4.4ï¼šé‡å‚æ•°åŒ–å®ç°
- en: In *Figure 4**.4*, we have all the same Î± - 0.1 and Î² - 0.9\. Now, whenever
    we need to generate a noised image xÂ t, we can quickly calculate Î±Â â€¾Â Â t from known
    numbers; the lines show what numbers are used to calculate Î±Â â€¾Â Â t.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ *å›¾ 4.4* ä¸­ï¼Œæˆ‘ä»¬æ‰€æœ‰çš„ Î± - 0.1 å’Œ Î² - 0.9 éƒ½æ˜¯ç›¸åŒçš„ã€‚ç°åœ¨ï¼Œæ¯å½“æˆ‘ä»¬éœ€è¦ç”Ÿæˆä¸€ä¸ªå™ªå£°å›¾åƒ x_t æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥å¿«é€Ÿä»å·²çŸ¥æ•°å­—ä¸­è®¡ç®—å‡º
    Î±_â€¾_tï¼›çº¿æ¡æ˜¾ç¤ºäº†ç”¨äºè®¡ç®— Î±_â€¾_t çš„æ•°å­—ã€‚
- en: 'The following code can generate a noised image at any step:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹ä»£ç å¯ä»¥åœ¨ä»»ä½•æ­¥éª¤ç”Ÿæˆä¸€ä¸ªå™ªå£°å›¾åƒï¼š
- en: '[PRE2]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This code is the implementation of the previously presented math formula. I
    present the code here to help build a correlated understanding between the math
    formula and the real implementation. If you are familiar with Python, you may
    find that this code makes the underlying subtleties easier to understand. The
    code can generate a noised image as shown in *Figure 4**.5*.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ®µä»£ç æ˜¯ä¹‹å‰å±•ç¤ºçš„æ•°å­¦å…¬å¼çš„å®ç°ã€‚æˆ‘åœ¨è¿™é‡Œå±•ç¤ºä»£ç æ˜¯ä¸ºäº†å¸®åŠ©å¤§å®¶å»ºç«‹æ•°å­¦å…¬å¼ä¸å®é™…å®ç°ä¹‹é—´çš„å…³è”ç†è§£ã€‚å¦‚æœä½ ç†Ÿæ‚‰ Pythonï¼Œå¯èƒ½ä¼šå‘ç°è¿™æ®µä»£ç ä½¿å¾—åº•å±‚ç»†èŠ‚æ›´å®¹æ˜“ç†è§£ã€‚è¯¥ä»£ç å¯ä»¥ç”Ÿæˆå¦‚å›¾
    *å›¾ 4.5* æ‰€ç¤ºçš„å¸¦å™ªå£°çš„å›¾åƒã€‚
- en: '![Figure 4.5: Implementation of reparameterization](img/B21263_04_05.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ 4.5ï¼šé‡å‚æ•°åŒ–å®ç°](img/B21263_04_05.jpg)'
- en: 'Figure 4.5: Implementation of reparameterization'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 4.5ï¼šé‡å‚æ•°åŒ–å®ç°
- en: Now, letâ€™s think about how to recover an image by leveraging a neural network.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬æ€è€ƒå¦‚ä½•åˆ©ç”¨ç¥ç»ç½‘ç»œæ¢å¤å›¾åƒã€‚
- en: The noise-to-image training process
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å™ªå£°åˆ°å›¾åƒçš„è®­ç»ƒè¿‡ç¨‹
- en: We have the solution to add noise to the image, which is known as forward diffusion,
    as shown in *Figure 4**.6*. To recover an image from the noise, or **reverse diffusion**,
    as shown in *Figure 4**.6*, we need to find a way to implement the reverse step
    pÂ Î¸(xÂ tâˆ’1| xÂ t). However, this step is intractable or uncomputable without additional
    help.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»æœ‰äº†å‘å›¾åƒæ·»åŠ å™ªå£°çš„è§£å†³æ–¹æ¡ˆï¼Œè¿™è¢«ç§°ä¸ºæ­£å‘æ‰©æ•£ï¼Œå¦‚å›¾ *å›¾ 4.6* æ‰€ç¤ºã€‚è¦ä»å™ªå£°ä¸­æ¢å¤å›¾åƒï¼Œæˆ–è¿›è¡Œ**åå‘æ‰©æ•£**ï¼Œå¦‚å›¾ *å›¾ 4.6* æ‰€ç¤ºï¼Œæˆ‘ä»¬éœ€è¦æ‰¾åˆ°ä¸€ç§æ–¹æ³•æ¥å®ç°åå‘æ­¥éª¤
    pÎ¸(xtâˆ’1|xt)ã€‚ç„¶è€Œï¼Œæ²¡æœ‰é¢å¤–çš„å¸®åŠ©ï¼Œè¿™ä¸€æ­¥æ˜¯æ— æ³•å¤„ç†çš„æˆ–æ— æ³•è®¡ç®—çš„ã€‚
- en: Consider that we have the ending Gaussian noise data, and all those noise step
    data in hand. What if we can train a neural network that can reverse the process?
    We can use the neural network to provide the mean and variance of a noise image
    and then remove the generated noise from the previous image data. By doing this,
    we should be able to use this step to represent pÂ Î¸(xÂ tâˆ’1| xÂ t), and thus recover
    an image.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: è€ƒè™‘åˆ°æˆ‘ä»¬å·²ç»æœ‰æœ€ç»ˆçš„é«˜æ–¯å™ªå£°æ•°æ®ï¼Œä»¥åŠæ‰€æœ‰å™ªå£°æ­¥éª¤æ•°æ®åœ¨æ‰‹ã€‚å¦‚æœæˆ‘ä»¬èƒ½å¤Ÿè®­ç»ƒä¸€ä¸ªèƒ½å¤Ÿé€†è½¬è¿‡ç¨‹çš„ç¥ç»ç½‘ç»œä¼šæ€æ ·ï¼Ÿæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç¥ç»ç½‘ç»œæ¥æä¾›å™ªå£°å›¾åƒçš„å‡å€¼å’Œæ–¹å·®ï¼Œç„¶åä»ä¹‹å‰å›¾åƒæ•°æ®ä¸­ç§»é™¤ç”Ÿæˆçš„å™ªå£°ã€‚é€šè¿‡è¿™æ ·åšï¼Œæˆ‘ä»¬åº”è¯¥èƒ½å¤Ÿä½¿ç”¨è¿™ä¸€æ­¥æ¥è¡¨ç¤º
    pÎ¸(xtâˆ’1|xt)ï¼Œä»è€Œæ¢å¤å›¾åƒã€‚
- en: '![Figure 4.6: Forward diffusion and reverse process](img/B21263_04_06.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ 4.6ï¼šæ­£å‘æ‰©æ•£å’Œåå‘è¿‡ç¨‹](img/B21263_04_06.jpg)'
- en: 'Figure 4.6: Forward diffusion and reverse process'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 4.6ï¼šæ­£å‘æ‰©æ•£å’Œåå‘è¿‡ç¨‹
- en: You may ask how we should calculate the loss and update the weights. The ending
    image (xÂ T) removes the previously added noise and will provide the ground truth
    data. After all, we can generate the noise data in the forward diffusion processes
    on the fly. Next, compare it with the output data from the neural network (usually
    a UNet). We get the loss data that can be used to calculate the gradient descendant
    data and update the neural network weights.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯èƒ½ä¼šé—®æˆ‘ä»¬åº”è¯¥å¦‚ä½•è®¡ç®—æŸå¤±å¹¶æ›´æ–°æƒé‡ã€‚æœ€ç»ˆå›¾åƒ (xT) ç§»é™¤äº†ä¹‹å‰æ·»åŠ çš„å™ªå£°ï¼Œå¹¶å°†æä¾›çœŸå®æ•°æ®ã€‚æ¯•ç«Ÿï¼Œæˆ‘ä»¬å¯ä»¥åœ¨æ­£å‘æ‰©æ•£è¿‡ç¨‹ä¸­å®æ—¶ç”Ÿæˆå™ªå£°æ•°æ®ã€‚æ¥ä¸‹æ¥ï¼Œå°†å…¶ä¸ç¥ç»ç½‘ç»œï¼ˆé€šå¸¸æ˜¯
    UNetï¼‰çš„è¾“å‡ºæ•°æ®è¿›è¡Œæ¯”è¾ƒã€‚æˆ‘ä»¬å¾—åˆ°æŸå¤±æ•°æ®ï¼Œå¯ç”¨äºè®¡ç®—æ¢¯åº¦ä¸‹é™æ•°æ®å¹¶æ›´æ–°ç¥ç»ç½‘ç»œæƒé‡ã€‚
- en: 'The DDPM paper [4] provided a simplified way to calculate the loss:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: DDPM è®ºæ–‡ [4] æä¾›äº†ä¸€ç§ç®€åŒ–çš„æŸå¤±è®¡ç®—æ–¹æ³•ï¼š
- en: 'LÂ simple(Î¸) : = ğ”¼Â t, xÂ 0,âˆˆ[|| âˆˆ âˆ’ âˆˆÂ Î¸(âˆšÂ _Â _Â Î±Â tÂ  xÂ 0 + âˆšÂ _Â 1 âˆ’ _Â Î±Â tÂ  Ïµ, t)
    ||Â 2]'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Lsimple(Î¸) := ğ”¼t, x0âˆˆ[||âˆˆ âˆ’ âˆˆÎ¸(âˆšÎ±t xt0 + âˆš1 âˆ’ Î±t Ïµ, t) ||Â²]
- en: 'Since xÂ t = âˆšÂ _Â _Â Î±Â t xÂ 0Â  + âˆšÂ _Â 1 âˆ’ _Â Î±Â tÂ , we can further simplify the formula
    to the following:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äº xt = âˆšÎ±t xt0 + âˆš1 âˆ’ Î±tï¼Œæˆ‘ä»¬å¯ä»¥è¿›ä¸€æ­¥ç®€åŒ–å…¬å¼ä¸ºä»¥ä¸‹å½¢å¼ï¼š
- en: LÂ simple(Î¸) â‰” ğ”¼Â t,xÂ 0,Ïµ[||Ïµ âˆ’ ÏµÂ Î¸(xÂ t, t) ||Â 2]
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Lsimple(Î¸) â‰” ğ”¼t,x0,Ïµ[||Ïµ âˆ’ ÏµÎ¸(xt, t) ||Â²]
- en: 'The UNet will take a noised image data: xÂ t and a time step data: t as inputs
    as shown in *Figure 4**.7*. Why take t as input? Because all the denoising processes
    share the same neural network weights, the input t will help train a UNet with
    a time step in mind.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: UNet å°†ä»¥å¸¦å™ªå£°çš„å›¾åƒæ•°æ® x_t å’Œæ—¶é—´æ­¥æ•°æ® t ä½œä¸ºè¾“å…¥ï¼Œå¦‚å›¾ *å›¾ 4.7* æ‰€ç¤ºã€‚ä¸ºä»€ä¹ˆä»¥ t ä½œä¸ºè¾“å…¥ï¼Ÿå› ä¸ºæ‰€æœ‰å»å™ªè¿‡ç¨‹éƒ½å…±äº«ç›¸åŒçš„ç¥ç»ç½‘ç»œæƒé‡ï¼Œè¾“å…¥
    t å°†å¸®åŠ©è®­ç»ƒä¸€ä¸ªè€ƒè™‘æ—¶é—´æ­¥çš„ UNetã€‚
- en: '![Figure 4.7: UNet training inputs and loss calculation](img/B21263_04_07.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ 4.7ï¼šUNet è®­ç»ƒè¾“å…¥å’ŒæŸå¤±è®¡ç®—](img/B21263_04_07.jpg)'
- en: 'Figure 4.7: UNet training inputs and loss calculation'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 4.7ï¼šUNet è®­ç»ƒè¾“å…¥å’ŒæŸå¤±è®¡ç®—
- en: When we say letâ€™s train a neural network to predict the noise distribution that
    will be removed from the image leading to a clearer image, what is the neural
    network predicting? In the DDPM paper [4], the original diffusion model uses a
    fixed variance Î¸, and sets the Gaussian distribution mean - Î¼ as the only parameter
    that needs to be learned through a neural network.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬è¯´è®­ç»ƒä¸€ä¸ªç¥ç»ç½‘ç»œæ¥é¢„æµ‹å°†è¢«ä»å›¾åƒä¸­ç§»é™¤çš„å™ªå£°åˆ†å¸ƒï¼Œä»è€Œå¾—åˆ°æ›´æ¸…æ™°çš„å›¾åƒæ—¶ï¼Œç¥ç»ç½‘ç»œé¢„æµ‹çš„æ˜¯ä»€ä¹ˆï¼Ÿåœ¨DDPMè®ºæ–‡[4]ä¸­ï¼ŒåŸå§‹çš„æ‰©æ•£æ¨¡å‹ä½¿ç”¨ä¸€ä¸ªå›ºå®šçš„æ–¹å·®Î¸ï¼Œå¹¶å°†é«˜æ–¯åˆ†å¸ƒçš„å‡å€¼-
    Î¼ä½œä¸ºå”¯ä¸€éœ€è¦é€šè¿‡ç¥ç»ç½‘ç»œå­¦ä¹ çš„å‚æ•°ã€‚
- en: 'In a PyTorch implementation, the loss data can be calculated like this:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨PyTorchå®ç°ä¸­ï¼ŒæŸå¤±æ•°æ®å¯ä»¥è®¡ç®—å¦‚ä¸‹ï¼š
- en: '[PRE3]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now, we should be able to train a diffusion model and the model should be able
    to recover an image from a random Gaussian distributed noise. Next, letâ€™s take
    a look at how the inference or sampling works.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬åº”è¯¥èƒ½å¤Ÿè®­ç»ƒä¸€ä¸ªæ‰©æ•£æ¨¡å‹ï¼Œå¹¶ä¸”è¯¥æ¨¡å‹åº”è¯¥èƒ½å¤Ÿä»éšæœºé«˜æ–¯åˆ†å¸ƒçš„å™ªå£°ä¸­æ¢å¤å›¾åƒã€‚æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹æ¨ç†æˆ–é‡‡æ ·çš„å·¥ä½œåŸç†ã€‚
- en: The noise-to-image sampling process
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å™ªå£°åˆ°å›¾åƒçš„é‡‡æ ·è¿‡ç¨‹
- en: 'Here are the steps to sample an image from the model, or, in other words, generate
    an image from the reverse diffusion process:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯ä»æ¨¡å‹ä¸­é‡‡æ ·å›¾åƒçš„æ­¥éª¤ï¼Œæˆ–è€…æ¢å¥è¯è¯´ï¼Œé€šè¿‡åå‘æ‰©æ•£è¿‡ç¨‹ç”Ÿæˆå›¾åƒï¼š
- en: 'Generate a complete Gaussian noise with a mean of 0 and a variance of 1:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç”Ÿæˆä¸€ä¸ªå‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1çš„å®Œæ•´é«˜æ–¯å™ªå£°ï¼š
- en: xÂ T âˆ¼ ğ’©(0,1)
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: xÂ T âˆ¼ ğ’©(0,1)
- en: We will use this noise as the starting image.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä½¿ç”¨è¿™ä¸ªå™ªå£°ä½œä¸ºèµ·å§‹å›¾åƒã€‚
- en: '2. Loop through t = T to t = 1\. In each step, if t > 1, then generate another
    Gaussian noise image z:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 2. ä»t = Tå¾ªç¯åˆ°t = 1ã€‚åœ¨æ¯ä¸€æ­¥ä¸­ï¼Œå¦‚æœt > 1ï¼Œåˆ™ç”Ÿæˆå¦ä¸€ä¸ªé«˜æ–¯å™ªå£°å›¾åƒzï¼š
- en: z âˆ¼ ğ’©(0,1)
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: z âˆ¼ ğ’©(0,1)
- en: 'If t = 1, then the following occurs:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœt = 1ï¼Œåˆ™ä»¥ä¸‹æƒ…å†µå‘ç”Ÿï¼š
- en: z = 0
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: z = 0
- en: 'Then, generate a noise from the UNet model, and remove the generated noise
    from the input noisy image xÂ t:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œä»UNetæ¨¡å‹ç”Ÿæˆå™ªå£°ï¼Œå¹¶ä»è¾“å…¥çš„å™ªå£°å›¾åƒxÂ tä¸­ç§»é™¤ç”Ÿæˆçš„å™ªå£°ï¼š
- en: xÂ t-1 = Â 1Â _Â âˆšÂ _Â Î±Â tÂ (xÂ t âˆ’ Â 1 âˆ’ Î±Â tÂ _Â âˆšÂ _Â 1 âˆ’ _Â Î±Â tÂ  ÏµÂ Î¸(xÂ t, t)) + âˆšÂ _Â 1 âˆ’
    Î±Â tÂ  z
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: xÂ t-1 = Â 1Â _Â âˆšÂ _Â Î±Â tÂ (xÂ t âˆ’ Â 1 âˆ’ Î±Â tÂ _Â âˆšÂ _Â 1 âˆ’ _Â Î±Â tÂ  ÏµÂ Î¸(xÂ t, t)) + âˆšÂ _Â 1 âˆ’
    Î±Â tÂ  z
- en: If we take a look at the preceding equation, all those Î±Â t and Î±Â â€¾Â Â t are known
    numbers sourced from Î²Â t. The only thing we need from the UNet is the ÏµÂ Î¸(xÂ t,
    t), which is the noise produced by the UNet, as shown in *Figure 4**.8*.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬çœ‹ä¸€ä¸‹å‰é¢çš„æ–¹ç¨‹ï¼Œæ‰€æœ‰çš„Î±Â tå’ŒÎ±Â â€¾Â Â téƒ½æ˜¯æ¥è‡ªÎ²Â tçš„å·²çŸ¥æ•°å­—ã€‚æˆ‘ä»¬å”¯ä¸€éœ€è¦ä»UNetå¾—åˆ°çš„æ˜¯ÏµÂ Î¸(xÂ t, t)ï¼Œè¿™æ˜¯UNetäº§ç”Ÿçš„å™ªå£°ï¼Œå¦‚*å›¾4**.8*æ‰€ç¤ºã€‚8*ã€‚
- en: '![Figure 4.8: Sampling from UNet](img/B21263_04_08.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾4.8ï¼šä»UNeté‡‡æ ·](img/B21263_04_08.jpg)'
- en: 'Figure 4.8: Sampling from UNet'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4.8ï¼šä»UNeté‡‡æ ·
- en: The added âˆšÂ _Â 1 âˆ’ Î±Â tÂ  z looks a little bit mysterious here. Why add this to
    the process? The original paper doesnâ€™t explain this added noise, but researchers
    found that the added noise in the denoising process will significantly improve
    the generated image quality!
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œæ·»åŠ çš„âˆšÂ _Â 1 âˆ’ Î±Â tÂ  zçœ‹èµ·æ¥æœ‰ç‚¹ç¥ç§˜ã€‚ä¸ºä»€ä¹ˆè¦æ·»åŠ è¿™ä¸ªè¿‡ç¨‹ï¼ŸåŸå§‹è®ºæ–‡æ²¡æœ‰è§£é‡Šè¿™ä¸ªæ·»åŠ çš„å™ªå£°ï¼Œä½†ç ”ç©¶äººå‘˜å‘ç°ï¼Œåœ¨å»å™ªè¿‡ç¨‹ä¸­æ·»åŠ çš„å™ªå£°å°†æ˜¾è‘—æé«˜ç”Ÿæˆçš„å›¾åƒè´¨é‡ï¼
- en: 3. Loop end, return the final generated image xÂ 0.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 3. å¾ªç¯ç»“æŸï¼Œè¿”å›æœ€ç»ˆç”Ÿæˆçš„å›¾åƒxÂ 0ã€‚
- en: Now, letâ€™s talk about the image generation guidance.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬è°ˆè°ˆå›¾åƒç”Ÿæˆå¼•å¯¼ã€‚
- en: Understanding Classifier Guidance denoising
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç†è§£åˆ†ç±»å™¨å¼•å¯¼å»å™ª
- en: Until now, we havenâ€™t talked about the text guidance yet. The image generation
    process will take a random Gaussian noise as the only input, and then randomly
    generate an image based on the training dataset. But we want a guided image generation;
    for example, input â€œdogâ€ to ask the diffusion model to generate an image including
    â€œdog.â€
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬è¿˜æ²¡æœ‰è®¨è®ºæ–‡æœ¬å¼•å¯¼ã€‚å›¾åƒç”Ÿæˆè¿‡ç¨‹å°†ä»¥éšæœºé«˜æ–¯å™ªå£°ä½œä¸ºå”¯ä¸€è¾“å…¥ï¼Œç„¶åæ ¹æ®è®­ç»ƒæ•°æ®é›†éšæœºç”Ÿæˆå›¾åƒã€‚ä½†æˆ‘ä»¬éœ€è¦ä¸€ä¸ªå¼•å¯¼çš„å›¾åƒç”Ÿæˆï¼›ä¾‹å¦‚ï¼Œè¾“å…¥â€œdogâ€æ¥è¦æ±‚æ‰©æ•£æ¨¡å‹ç”ŸæˆåŒ…å«â€œdogâ€çš„å›¾åƒã€‚
- en: In 2021, Dhariwal and Nichol, from OpenAI, proposed classifier guidance in their
    paper titled *Diffusion Models Beat GANs on Image* *Synthesis* [12].
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨2021å¹´ï¼Œæ¥è‡ªOpenAIçš„Dhariwalå’ŒNicholåœ¨ä»–ä»¬é¢˜ä¸º*æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒ* *åˆæˆ* [12]çš„è®ºæ–‡ä¸­æå‡ºäº†åˆ†ç±»å™¨å¼•å¯¼ã€‚
- en: Based on the proposed methodology, we can achieve classifier-guided denoising
    by providing a classification label during the training stage. Instead of just
    image or time-step embedding, we also provide text description embeddings as shown
    in *Figure 4**.9*.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®æå‡ºçš„æ–¹æ³•ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨è®­ç»ƒé˜¶æ®µæä¾›åˆ†ç±»æ ‡ç­¾æ¥å®ç°åˆ†ç±»å™¨å¼•å¯¼å»å™ªã€‚é™¤äº†å›¾åƒæˆ–æ—¶é—´æ­¥é•¿åµŒå…¥ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†æ–‡æœ¬æè¿°åµŒå…¥ï¼Œå¦‚*å›¾4**.9*æ‰€ç¤ºã€‚
- en: '![Figure 4.9: Train a diffusion model with conditional text](img/B21263_04_09.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾4.9ï¼šä½¿ç”¨æ¡ä»¶æ–‡æœ¬è®­ç»ƒæ‰©æ•£æ¨¡å‹](img/B21263_04_09.jpg)'
- en: 'Figure 4.9: Train a diffusion model with conditional text'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4.9ï¼šä½¿ç”¨æ¡ä»¶æ–‡æœ¬è®­ç»ƒæ‰©æ•£æ¨¡å‹
- en: In *Figure 4**.7*, there are two inputs, while in *Figure 4**.9*, there is one
    additional input â€“ **Text embedding**; it is the embedding data generated from
    OpenAIâ€™s CLIP model. We will discuss the way more powerful CLIP model guided diffusion
    model in the next chapter.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨*å›¾4**.7*ä¸­ï¼Œæœ‰ä¸¤ä¸ªè¾“å…¥ï¼Œè€Œåœ¨*å›¾4**.9*ä¸­ï¼Œæœ‰ä¸€ä¸ªé¢å¤–çš„è¾“å…¥ â€“ **æ–‡æœ¬åµŒå…¥**ï¼›è¿™æ˜¯ç”±OpenAIçš„CLIPæ¨¡å‹ç”Ÿæˆçš„åµŒå…¥æ•°æ®ã€‚æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€ç« è®¨è®ºæ›´å¼ºå¤§çš„CLIPæ¨¡å‹å¼•å¯¼çš„æ‰©æ•£æ¨¡å‹ã€‚
- en: Summary
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ‘˜è¦
- en: In this chapter, we took a deep dive into the internal workings of the diffusion
    model initially brought out by Jonathan Ho et al. [4]. We learned about the foundational
    ideas of the diffusion model and learned about the forward diffusion process.
    We also walked through the reverse diffusion process for diffusion model training
    and sampling and explored how to enable a text-guided diffusion model.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬æ·±å…¥æ¢è®¨äº†ç”±çº¦ç¿°é€ŠÂ·éœç­‰æœ€åˆæå‡ºçš„æ‰©æ•£æ¨¡å‹çš„å†…éƒ¨å·¥ä½œåŸç†ã€‚[4]ã€‚æˆ‘ä»¬äº†è§£äº†æ‰©æ•£æ¨¡å‹çš„åŸºç¡€æ€æƒ³ï¼Œå¹¶å­¦ä¹ äº†æ­£å‘æ‰©æ•£è¿‡ç¨‹ã€‚æˆ‘ä»¬è¿˜äº†è§£äº†æ‰©æ•£æ¨¡å‹è®­ç»ƒå’Œé‡‡æ ·çš„åå‘æ‰©æ•£è¿‡ç¨‹ï¼Œå¹¶æ¢è®¨äº†å¦‚ä½•å®ç°æ–‡æœ¬å¼•å¯¼çš„æ‰©æ•£æ¨¡å‹ã€‚
- en: Through this chapter, we aimed to explain the core idea of the diffusion model.
    If you want to implement a diffusion model by yourself, I would recommend reading
    through the original DDPM paper directly.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡æœ¬ç« ï¼Œæˆ‘ä»¬æ—¨åœ¨è§£é‡Šæ‰©æ•£æ¨¡å‹çš„æ ¸å¿ƒæ€æƒ³ã€‚å¦‚æœä½ æƒ³è‡ªå·±å®ç°æ‰©æ•£æ¨¡å‹ï¼Œæˆ‘å»ºè®®ç›´æ¥é˜…è¯»åŸå§‹DDPMè®ºæ–‡ã€‚
- en: The DDPM diffusion model can generate realistic images, but one of its problems
    is its performance. Not only is training a model slow, but the image sampling
    is also slow. In the next chapter, we are going to discuss the Stable Diffusion
    model, which will boost the speed in a genius way.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: DDPMæ‰©æ•£æ¨¡å‹å¯ä»¥ç”Ÿæˆé€¼çœŸçš„å›¾åƒï¼Œä½†å…¶ä¸­ä¸€ä¸ªé—®é¢˜æ˜¯å…¶æ€§èƒ½ã€‚ä¸ä»…è®­ç»ƒæ¨¡å‹é€Ÿåº¦æ…¢ï¼Œå›¾åƒé‡‡æ ·ä¹Ÿæ…¢ã€‚åœ¨ä¸‹ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºStable Diffusionæ¨¡å‹ï¼Œå®ƒå°†ä»¥å¤©æ‰çš„æ–¹å¼æé«˜é€Ÿåº¦ã€‚
- en: References
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: '*The Annotated Diffusion Model* â€“ [https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/annotated_diffusion.ipynb#scrollTo=c5a94671](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/annotated_diffusion.ipynb#scrollTo=c5a94671'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*æ³¨é‡Šè¿‡çš„æ‰©æ•£æ¨¡å‹* â€“ [https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/annotated_diffusion.ipynb#scrollTo=c5a94671](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/annotated_diffusion.ipynb#scrollTo=c5a94671)'
- en: )
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: '*Training with Diffusers* â€“ [https://colab.research.google.com/gist/anton-l/f3a8206dae4125b93f05b1f5f703191d/diffusers_training_example.ipynb](https://colab.research.google.com/gist/anton-l/f3a8206dae4125b93f05b1f5f703191d/diffusers_training_example.ipynb)'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*ä½¿ç”¨æ‰©æ•£å™¨è¿›è¡Œè®­ç»ƒ* â€“ [https://colab.research.google.com/gist/anton-l/f3a8206dae4125b93f05b1f5f703191d/diffusers_training_example.ipynb](https://colab.research.google.com/gist/anton-l/f3a8206dae4125b93f05b1f5f703191d/diffusers_training_example.ipynb)'
- en: '*Diffusers* â€“ [https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb#scrollTo=PzW5ublpBuUt](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb#scrollTo=PzW5ublpBuUt'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*æ‰©æ•£å™¨* â€“ [https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb#scrollTo=PzW5ublpBuUt](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb#scrollTo=PzW5ublpBuUt)'
- en: )
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: Jonathan Ho et al., *Denoising Diffusion Probabilistic Models* â€“ [https://arxiv.org/abs/2006.11239](https://arxiv.org/abs/2006.11239)
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: çº¦ç¿°é€ŠÂ·éœç­‰ï¼Œ*å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹* â€“ [https://arxiv.org/abs/2006.11239](https://arxiv.org/abs/2006.11239)
- en: Steins, *Diffusion Model Clearly Explained!* â€“ [https://medium.com/@steinsfu/diffusion-model-clearly-explained-cd331bd41166](https://medium.com/@steinsfu/diffusion-model-clearly-explained-cd331bd41166)
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ–¯è’‚æ©æ–¯ï¼Œ*æ‰©æ•£æ¨¡å‹æ¸…æ™°è§£é‡Š!* â€“ [https://medium.com/@steinsfu/diffusion-model-clearly-explained-cd331bd41166](https://medium.com/@steinsfu/diffusion-model-clearly-explained-cd331bd41166)
- en: Steins, *Stable Diffusion Clearly Explained!* â€“ [https://medium.com/@steinsfu/stable-diffusion-clearly-explained-ed008044e07e](https://medium.com/@steinsfu/stable-diffusion-clearly-explained-ed008044e07e)
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ–¯è’‚æ©æ–¯ï¼Œ*Stable Diffusionæ¸…æ™°è§£é‡Š!* â€“ [https://medium.com/@steinsfu/stable-diffusion-clearly-explained-ed008044e07e](https://medium.com/@steinsfu/stable-diffusion-clearly-explained-ed008044e07e)
- en: DeepFindr, *Diffusion models from scratch in PyTorch* â€“ [https://www.youtube.com/watch?v=a4Yfz2FxXiY&t=5s&ab_channel=DeepFindr](https://www.youtube.com/watch?v=a4Yfz2FxXiY&t=5s&ab_channel=DeepFindr
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DeepFindrï¼Œ*ä»é›¶å¼€å§‹ä½¿ç”¨PyTorchæ„å»ºæ‰©æ•£æ¨¡å‹* â€“ [https://www.youtube.com/watch?v=a4Yfz2FxXiY&t=5s&ab_channel=DeepFindr](https://www.youtube.com/watch?v=a4Yfz2FxXiY&t=5s&ab_channel=DeepFindr)
- en: )
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: Ari Seff, *What are Diffusion Models?* â€“ [https://www.youtube.com/watch?v=fbLgFrlTnGU&ab_channel=AriSeff](https://www.youtube.com/watch?v=fbLgFrlTnGU&ab_channel=AriSeff
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é˜¿é‡ŒÂ·å¡å¤«ï¼Œ*ä»€ä¹ˆæ˜¯æ‰©æ•£æ¨¡å‹ï¼Ÿ* â€“ [https://www.youtube.com/watch?v=fbLgFrlTnGU&ab_channel=AriSeff](https://www.youtube.com/watch?v=fbLgFrlTnGU&ab_channel=AriSeff)
- en: )
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: Prafulla Dhariwal, Alex Nichol*, Diffusion Models Beat GANs on Image Synthesis*
    â€“ [https://arxiv.org/abs/2105.05233](https://arxiv.org/abs/2105.05233)
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Prafulla Dhariwal, Alex Nichol*ï¼Œ**æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒåˆæˆä¸Šå‡»è´¥äº†GANs** â€“ [https://arxiv.org/abs/2105.05233](https://arxiv.org/abs/2105.05233)
- en: Diederik P Kingma, Max Welling, *Auto-Encoding Variational Bayes* â€“ [https://arxiv.org/abs/1312.6114](https://arxiv.org/abs/1312.6114)
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Diederik P Kingma, Max Wellingï¼Œ**è‡ªåŠ¨ç¼–ç å˜åˆ†è´å¶æ–¯** â€“ [https://arxiv.org/abs/1312.6114](https://arxiv.org/abs/1312.6114)
- en: Lilian Weng, *What are Diffusion Models?* â€“ [https://lilianweng.github.io/posts/2021-07-11-diffusion-models/](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/)
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Lilian Wengï¼Œ**ä»€ä¹ˆæ˜¯æ‰©æ•£æ¨¡å‹ï¼Ÿ** â€“ [https://lilianweng.github.io/posts/2021-07-11-diffusion-models/](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/)
- en: Prafulla Dhariwal, Alex Nichol, *Diffusion Models Beat GANs on Image Synthesis*
    â€“ [https://arxiv.org/abs/2105.05233](https://arxiv.org/abs/2105.05233)
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Prafulla Dhariwal, Alex Nicholï¼Œ**æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒåˆæˆä¸Šå‡»è´¥äº†GANs** â€“ [https://arxiv.org/abs/2105.05233](https://arxiv.org/abs/2105.05233)
