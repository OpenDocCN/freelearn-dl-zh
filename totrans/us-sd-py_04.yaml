- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Understanding the Theory Behind Diffusion Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will dive into the theory that powers **diffusion models** and
    see the internal workings of the system. How could a neural network model generate
    such realistic images? Curious minds would like to lift the cover and see the
    internal workings.
  prefs: []
  type: TYPE_NORMAL
- en: We are going to touch on the foundation of the diffusion model, aiming to figure
    out how it works internally and pave the foundation to implement a workable pipeline
    in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: By comprehending the intricacies of diffusion models, we not only enhance our
    understanding of the advanced **Stable Diffusion** (also known as **latent diffusion
    models** (**LDMs**)) but also gain the ability to navigate the source code of
    the Diffusers package more effectively.
  prefs: []
  type: TYPE_NORMAL
- en: This knowledge will enable us to extend the package‚Äôs features in line with
    emerging requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we will go through the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the image-to-noise process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A more efficient **forward** **diffusion process**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The noise-to-image training process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The noise-to-image sampling process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding Classifier Guidance denoising
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, we will have taken a deep dive into the internal
    workings of the diffusion model initially brought out by Jonathan Ho et al. [4].
    We will understand the foundational idea of the diffusion model and learn about
    the **forward diffusion process**. We will understand the reverse diffusion process
    for diffusion model training and sampling and learn to enable a text-guided diffusion
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs get started.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the image-to-noise process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The idea of the diffusion model is inspired by the diffusion concept from thermodynamics.
    Take one image as a cup of water and add enough noise (ink) to the image (water)
    to finally turn the image (water) into a complete noise image (ink water).
  prefs: []
  type: TYPE_NORMAL
- en: As shown in *Figure 4**.1*, image x¬†0 can be converted to a nearly Gaussian
    (normally distributed) noise image x¬†T.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1: Forward diffusion and reverse denoising](img/B21263_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.1: Forward diffusion and reverse denoising'
  prefs: []
  type: TYPE_NORMAL
- en: We employ a predetermined forward diffusion process, denoted as q, which systematically
    introduces Gaussian noise to an image until it culminates in pure noise. The process
    is denoted by q(x¬†t | x¬†t-1). Note that the reverse process p¬†Œ∏(x¬†t-1 | x¬†t) is
    still unknown.
  prefs: []
  type: TYPE_NORMAL
- en: 'One step of the forward diffusion process can be denoted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: q(x¬†t | x¬†t-1) ‚âî ùí©(x¬†t; ‚àö¬†_¬†1 ‚àí Œ≤¬†t x¬†t-1¬†, Œ≤¬†t I)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let me explain this formula bit by bit from left to right:'
  prefs: []
  type: TYPE_NORMAL
- en: The notation q(x¬†t | x¬†t-1) is used to denote a conditional probability distribution.
    In this case, the distribution q represents the probability of observing the noisy
    image x¬†t given the previous image x¬†t‚àí1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The define sign : = is used in the formula instead of the tilde symbol (‚àº)
    because the diffusion forward process is a deterministic process. The tilde symbol
    (‚àº) is typically used to represent a distribution. In this case, if we used the
    tilde symbol, the formula would be saying that the noisy image is a complete Gaussian
    distribution. However, this is not the case. The noisy image in t step is defined
    by a deterministic function of the previous image and added noise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then why is ùí© used here? The ùí© symbol is used to represent a Gaussian distribution.
    However, in this case, the ùí© symbol is being used to represent the functional
    form of the noisy image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the right side, before the semicolon, x¬†t is the thing we want to have in
    normal distribution. After the semicolon, those are the parameters of the distribution.
    A semicolon is usually used to separate the output and parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Œ≤¬†t is the noise variance at step t. ‚àö¬†_¬†1 ‚àí Œ≤¬†t¬† x¬†t‚àí1 is the mean of the new
    distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why is the big I used in the formula? Because an RGB image can have multiple
    channels, and the identity matrix can apply the noise variance to different channels
    independently.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It is quite easy to add Gaussian noise to an image using Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To execute the preceding code, you will also need to install the `ipyplot`
    package by `pip install ipyplot`. The code provided performs a simulation of a
    forward diffusion process on an image and then visualizes the progression of this
    process over a number of iterations. Here‚Äôs a step-by-step explanation of what
    each part of the code is doing:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Importing libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`ipyplot` is a library for plotting images in Jupyter notebooks in a more interactive
    way.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PIL` (which stands for `Image` module, is used for image manipulation.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Loading the image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`img_path` is defined as the path to the `image` file `dog.png`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image` is loaded using `plt.imread(img_path)`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Setting parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`num_iterations` defines the number of times the diffusion process will be
    simulated.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`beta` is a parameter that simulates noise variance in the diffusion process.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Initializing lists:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`images` is initialized as an empty list, which will later hold the PIL image
    objects that result from each iteration of the diffusion process.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`steps` is a list of strings that will act as labels for the images when they
    are plotted, indicating the step number for each image.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Forward diffusion process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A `for` loop runs for `num_iterations` times, each time performing a diffusion
    step. `mean` is computed by scaling the image with a factor of `sqrt(1 -` `beta)`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A new image is generated by adding Gaussian noise to the mean, where the noise
    has a standard deviation of `beta`. This is done using `np.random.normal`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The resulting image array values are scaled to the range 0-255 and converted
    to an 8-bit unsigned integer format, which is a common format for images.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pil_image` is created by converting the image array to a PIL image object
    in RGB mode.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Plot the image using `ipyplot` in a grid as shown in *Figure 4**.2*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.2: Add noise to the image](img/B21263_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.2: Add noise to the image'
  prefs: []
  type: TYPE_NORMAL
- en: From the result, we can see that even though every image is from a normal distribution
    function, not every image is a complete Gaussian distribution, or more strictly
    speaking, an `1000`, and later, in Stable Diffusion, the step number is reduced
    to between `20` to `50`.
  prefs: []
  type: TYPE_NORMAL
- en: If the last image of *Figure 4**.2* is an isotropic Gaussian distribution, its
    2D distribution visualization will appear as a circle; it is characterized by
    having equal variances in all dimensions. In other words, the spread or width
    of the distribution is the same along all axes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs plot an image pixel distribution after adding 16x times Gaussian noise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The result is shown in *Figure 4**.3*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3: A nearly isotropic, normally distributed noise image](img/B21263_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.3: A nearly isotropic, normally distributed noise image'
  prefs: []
  type: TYPE_NORMAL
- en: The figure shows how the code efficiently transforms an image into a nearly
    isotropic, normally distributed noise image in just 16 steps, as illustrated in
    the last image of *Figure 4**.2*.
  prefs: []
  type: TYPE_NORMAL
- en: A more efficient forward diffusion process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we use the chained process to calculate a noisy image at t step, it first
    requires calculating the noisy image from 1 to t ‚àí 1 steps, which is not efficient.
    We can leverage a trick called **reparameterization** [10] to transform the original
    chained process into a one-step process. Here is what the trick looks like.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we have a Gaussian distribution z with Œº as the mean and œÉ¬†2 variance:'
  prefs: []
  type: TYPE_NORMAL
- en: z ‚àº ùí©(Œº, œÉ¬†2)
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we can rewrite the distribution as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: œµ ‚àº ùí©(0,1)
  prefs: []
  type: TYPE_NORMAL
- en: z = Œº+ œÉœµ
  prefs: []
  type: TYPE_NORMAL
- en: 'The benefit brought by this trick is that we can now calculate an image at
    any step with a one-step calculation, which will greatly boost the training performance:'
  prefs: []
  type: TYPE_NORMAL
- en: x¬†t = ‚àö¬†_¬†1 ‚àí Œ≤¬†t¬† x¬†t‚àí1 + ‚àö¬†_¬†Œ≤¬†t¬† œµ¬†t‚àí1
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, say we define the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Œ±¬†t = 1 ‚àí Œ≤¬†t
  prefs: []
  type: TYPE_NORMAL
- en: 'We now have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: _¬†Œ±¬†t = ‚àè¬†i=1¬†t¬†Œ±¬†i
  prefs: []
  type: TYPE_NORMAL
- en: 'There is no magic here; define Œ±¬†t and Œ±¬†‚Äæ¬†¬†t is only for convenience, so that
    we can calculate a noised image at step t and generate x¬†t from the source un-noised
    image x¬†0 using the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: x¬†t = ‚àö¬†_¬†_¬†Œ±¬†t¬† x¬†0 + ‚àö¬†_¬†1 ‚àí _¬†Œ±¬†t
  prefs: []
  type: TYPE_NORMAL
- en: What do Œ±¬†t and Œ±¬†‚Äæ¬†¬†t look like? Here is a simplified sample (*Figure 4**.4*).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4: Implementation of reparameterization](img/B21263_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.4: Implementation of reparameterization'
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 4**.4*, we have all the same Œ± - 0.1 and Œ≤ - 0.9\. Now, whenever
    we need to generate a noised image x¬†t, we can quickly calculate Œ±¬†‚Äæ¬†¬†t from known
    numbers; the lines show what numbers are used to calculate Œ±¬†‚Äæ¬†¬†t.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code can generate a noised image at any step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This code is the implementation of the previously presented math formula. I
    present the code here to help build a correlated understanding between the math
    formula and the real implementation. If you are familiar with Python, you may
    find that this code makes the underlying subtleties easier to understand. The
    code can generate a noised image as shown in *Figure 4**.5*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5: Implementation of reparameterization](img/B21263_04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.5: Implementation of reparameterization'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let‚Äôs think about how to recover an image by leveraging a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: The noise-to-image training process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have the solution to add noise to the image, which is known as forward diffusion,
    as shown in *Figure 4**.6*. To recover an image from the noise, or **reverse diffusion**,
    as shown in *Figure 4**.6*, we need to find a way to implement the reverse step
    p¬†Œ∏(x¬†t‚àí1| x¬†t). However, this step is intractable or uncomputable without additional
    help.
  prefs: []
  type: TYPE_NORMAL
- en: Consider that we have the ending Gaussian noise data, and all those noise step
    data in hand. What if we can train a neural network that can reverse the process?
    We can use the neural network to provide the mean and variance of a noise image
    and then remove the generated noise from the previous image data. By doing this,
    we should be able to use this step to represent p¬†Œ∏(x¬†t‚àí1| x¬†t), and thus recover
    an image.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6: Forward diffusion and reverse process](img/B21263_04_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.6: Forward diffusion and reverse process'
  prefs: []
  type: TYPE_NORMAL
- en: You may ask how we should calculate the loss and update the weights. The ending
    image (x¬†T) removes the previously added noise and will provide the ground truth
    data. After all, we can generate the noise data in the forward diffusion processes
    on the fly. Next, compare it with the output data from the neural network (usually
    a UNet). We get the loss data that can be used to calculate the gradient descendant
    data and update the neural network weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'The DDPM paper [4] provided a simplified way to calculate the loss:'
  prefs: []
  type: TYPE_NORMAL
- en: 'L¬†simple(Œ∏) : = ùîº¬†t, x¬†0,‚àà[|| ‚àà ‚àí ‚àà¬†Œ∏(‚àö¬†_¬†_¬†Œ±¬†t¬† x¬†0 + ‚àö¬†_¬†1 ‚àí _¬†Œ±¬†t¬† œµ, t)
    ||¬†2]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since x¬†t = ‚àö¬†_¬†_¬†Œ±¬†t x¬†0¬† + ‚àö¬†_¬†1 ‚àí _¬†Œ±¬†t¬†, we can further simplify the formula
    to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: L¬†simple(Œ∏) ‚âî ùîº¬†t,x¬†0,œµ[||œµ ‚àí œµ¬†Œ∏(x¬†t, t) ||¬†2]
  prefs: []
  type: TYPE_NORMAL
- en: 'The UNet will take a noised image data: x¬†t and a time step data: t as inputs
    as shown in *Figure 4**.7*. Why take t as input? Because all the denoising processes
    share the same neural network weights, the input t will help train a UNet with
    a time step in mind.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7: UNet training inputs and loss calculation](img/B21263_04_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.7: UNet training inputs and loss calculation'
  prefs: []
  type: TYPE_NORMAL
- en: When we say let‚Äôs train a neural network to predict the noise distribution that
    will be removed from the image leading to a clearer image, what is the neural
    network predicting? In the DDPM paper [4], the original diffusion model uses a
    fixed variance Œ∏, and sets the Gaussian distribution mean - Œº as the only parameter
    that needs to be learned through a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a PyTorch implementation, the loss data can be calculated like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Now, we should be able to train a diffusion model and the model should be able
    to recover an image from a random Gaussian distributed noise. Next, let‚Äôs take
    a look at how the inference or sampling works.
  prefs: []
  type: TYPE_NORMAL
- en: The noise-to-image sampling process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are the steps to sample an image from the model, or, in other words, generate
    an image from the reverse diffusion process:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate a complete Gaussian noise with a mean of 0 and a variance of 1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: x¬†T ‚àº ùí©(0,1)
  prefs: []
  type: TYPE_NORMAL
- en: We will use this noise as the starting image.
  prefs: []
  type: TYPE_NORMAL
- en: '2. Loop through t = T to t = 1\. In each step, if t > 1, then generate another
    Gaussian noise image z:'
  prefs: []
  type: TYPE_NORMAL
- en: z ‚àº ùí©(0,1)
  prefs: []
  type: TYPE_NORMAL
- en: 'If t = 1, then the following occurs:'
  prefs: []
  type: TYPE_NORMAL
- en: z = 0
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, generate a noise from the UNet model, and remove the generated noise
    from the input noisy image x¬†t:'
  prefs: []
  type: TYPE_NORMAL
- en: x¬†t-1 = ¬†1¬†_¬†‚àö¬†_¬†Œ±¬†t¬†(x¬†t ‚àí ¬†1 ‚àí Œ±¬†t¬†_¬†‚àö¬†_¬†1 ‚àí _¬†Œ±¬†t¬† œµ¬†Œ∏(x¬†t, t)) + ‚àö¬†_¬†1 ‚àí
    Œ±¬†t¬† z
  prefs: []
  type: TYPE_NORMAL
- en: If we take a look at the preceding equation, all those Œ±¬†t and Œ±¬†‚Äæ¬†¬†t are known
    numbers sourced from Œ≤¬†t. The only thing we need from the UNet is the œµ¬†Œ∏(x¬†t,
    t), which is the noise produced by the UNet, as shown in *Figure 4**.8*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8: Sampling from UNet](img/B21263_04_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.8: Sampling from UNet'
  prefs: []
  type: TYPE_NORMAL
- en: The added ‚àö¬†_¬†1 ‚àí Œ±¬†t¬† z looks a little bit mysterious here. Why add this to
    the process? The original paper doesn‚Äôt explain this added noise, but researchers
    found that the added noise in the denoising process will significantly improve
    the generated image quality!
  prefs: []
  type: TYPE_NORMAL
- en: 3. Loop end, return the final generated image x¬†0.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let‚Äôs talk about the image generation guidance.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Classifier Guidance denoising
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Until now, we haven‚Äôt talked about the text guidance yet. The image generation
    process will take a random Gaussian noise as the only input, and then randomly
    generate an image based on the training dataset. But we want a guided image generation;
    for example, input ‚Äúdog‚Äù to ask the diffusion model to generate an image including
    ‚Äúdog.‚Äù
  prefs: []
  type: TYPE_NORMAL
- en: In 2021, Dhariwal and Nichol, from OpenAI, proposed classifier guidance in their
    paper titled *Diffusion Models Beat GANs on Image* *Synthesis* [12].
  prefs: []
  type: TYPE_NORMAL
- en: Based on the proposed methodology, we can achieve classifier-guided denoising
    by providing a classification label during the training stage. Instead of just
    image or time-step embedding, we also provide text description embeddings as shown
    in *Figure 4**.9*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.9: Train a diffusion model with conditional text](img/B21263_04_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.9: Train a diffusion model with conditional text'
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 4**.7*, there are two inputs, while in *Figure 4**.9*, there is one
    additional input ‚Äì **Text embedding**; it is the embedding data generated from
    OpenAI‚Äôs CLIP model. We will discuss the way more powerful CLIP model guided diffusion
    model in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we took a deep dive into the internal workings of the diffusion
    model initially brought out by Jonathan Ho et al. [4]. We learned about the foundational
    ideas of the diffusion model and learned about the forward diffusion process.
    We also walked through the reverse diffusion process for diffusion model training
    and sampling and explored how to enable a text-guided diffusion model.
  prefs: []
  type: TYPE_NORMAL
- en: Through this chapter, we aimed to explain the core idea of the diffusion model.
    If you want to implement a diffusion model by yourself, I would recommend reading
    through the original DDPM paper directly.
  prefs: []
  type: TYPE_NORMAL
- en: The DDPM diffusion model can generate realistic images, but one of its problems
    is its performance. Not only is training a model slow, but the image sampling
    is also slow. In the next chapter, we are going to discuss the Stable Diffusion
    model, which will boost the speed in a genius way.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*The Annotated Diffusion Model* ‚Äì [https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/annotated_diffusion.ipynb#scrollTo=c5a94671](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/annotated_diffusion.ipynb#scrollTo=c5a94671'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: '*Training with Diffusers* ‚Äì [https://colab.research.google.com/gist/anton-l/f3a8206dae4125b93f05b1f5f703191d/diffusers_training_example.ipynb](https://colab.research.google.com/gist/anton-l/f3a8206dae4125b93f05b1f5f703191d/diffusers_training_example.ipynb)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Diffusers* ‚Äì [https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb#scrollTo=PzW5ublpBuUt](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb#scrollTo=PzW5ublpBuUt'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: Jonathan Ho et al., *Denoising Diffusion Probabilistic Models* ‚Äì [https://arxiv.org/abs/2006.11239](https://arxiv.org/abs/2006.11239)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Steins, *Diffusion Model Clearly Explained!* ‚Äì [https://medium.com/@steinsfu/diffusion-model-clearly-explained-cd331bd41166](https://medium.com/@steinsfu/diffusion-model-clearly-explained-cd331bd41166)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Steins, *Stable Diffusion Clearly Explained!* ‚Äì [https://medium.com/@steinsfu/stable-diffusion-clearly-explained-ed008044e07e](https://medium.com/@steinsfu/stable-diffusion-clearly-explained-ed008044e07e)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: DeepFindr, *Diffusion models from scratch in PyTorch* ‚Äì [https://www.youtube.com/watch?v=a4Yfz2FxXiY&t=5s&ab_channel=DeepFindr](https://www.youtube.com/watch?v=a4Yfz2FxXiY&t=5s&ab_channel=DeepFindr
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: Ari Seff, *What are Diffusion Models?* ‚Äì [https://www.youtube.com/watch?v=fbLgFrlTnGU&ab_channel=AriSeff](https://www.youtube.com/watch?v=fbLgFrlTnGU&ab_channel=AriSeff
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: Prafulla Dhariwal, Alex Nichol*, Diffusion Models Beat GANs on Image Synthesis*
    ‚Äì [https://arxiv.org/abs/2105.05233](https://arxiv.org/abs/2105.05233)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Diederik P Kingma, Max Welling, *Auto-Encoding Variational Bayes* ‚Äì [https://arxiv.org/abs/1312.6114](https://arxiv.org/abs/1312.6114)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lilian Weng, *What are Diffusion Models?* ‚Äì [https://lilianweng.github.io/posts/2021-07-11-diffusion-models/](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prafulla Dhariwal, Alex Nichol, *Diffusion Models Beat GANs on Image Synthesis*
    ‚Äì [https://arxiv.org/abs/2105.05233](https://arxiv.org/abs/2105.05233)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
