- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Understanding the Theory Behind Diffusion Models
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解扩散模型背后的理论
- en: This chapter will dive into the theory that powers **diffusion models** and
    see the internal workings of the system. How could a neural network model generate
    such realistic images? Curious minds would like to lift the cover and see the
    internal workings.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将深入探讨驱动**扩散模型**的理论，并了解系统的内部工作原理。神经网络模型是如何生成如此逼真的图像的呢？好奇心强的人想要揭开面纱，看看内部的工作机制。
- en: We are going to touch on the foundation of the diffusion model, aiming to figure
    out how it works internally and pave the foundation to implement a workable pipeline
    in the next chapter.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将触及扩散模型的基础，旨在弄清楚其内部工作原理，并为下一章实现可行的流程奠定基础。
- en: By comprehending the intricacies of diffusion models, we not only enhance our
    understanding of the advanced **Stable Diffusion** (also known as **latent diffusion
    models** (**LDMs**)) but also gain the ability to navigate the source code of
    the Diffusers package more effectively.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 通过理解扩散模型的复杂性，我们不仅增强了我们对高级**稳定扩散**（也称为**潜在扩散模型**（**LDMs**））的理解，而且还能更有效地导航Diffusers包的源代码。
- en: This knowledge will enable us to extend the package’s features in line with
    emerging requirements.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这项知识将使我们能够根据新兴需求扩展包的功能。
- en: 'Specifically, we will go through the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将探讨以下主题：
- en: Understanding the image-to-noise process
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解图像到噪声的过程
- en: A more efficient **forward** **diffusion process**
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更高效的**正向****扩散过程**
- en: The noise-to-image training process
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 噪声到图像的训练过程
- en: The noise-to-image sampling process
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 噪声到图像的采样过程
- en: Understanding Classifier Guidance denoising
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解分类器引导去噪
- en: By the end of this chapter, we will have taken a deep dive into the internal
    workings of the diffusion model initially brought out by Jonathan Ho et al. [4].
    We will understand the foundational idea of the diffusion model and learn about
    the **forward diffusion process**. We will understand the reverse diffusion process
    for diffusion model training and sampling and learn to enable a text-guided diffusion
    model.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，我们将深入探讨由Jonathan Ho等人最初提出的扩散模型的内部工作原理。我们将理解扩散模型的基础理念，并学习**正向扩散过程**。我们将了解扩散模型训练和采样的反向扩散过程，并学会启用文本引导的扩散模型。
- en: Let’s get started.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: Understanding the image-to-noise process
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解图像到噪声的过程
- en: The idea of the diffusion model is inspired by the diffusion concept from thermodynamics.
    Take one image as a cup of water and add enough noise (ink) to the image (water)
    to finally turn the image (water) into a complete noise image (ink water).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型的想法受到了热力学中扩散概念的启发。将一张图像视为一杯水，并向图像（水）中添加足够的噪声（墨水），最终将图像（水）变成完整的噪声图像（墨水水）。
- en: As shown in *Figure 4**.1*, image x 0 can be converted to a nearly Gaussian
    (normally distributed) noise image x T.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图4**.1*所示，图像x 0可以被转换为一个几乎高斯（正态分布）的噪声图像x T。
- en: '![Figure 4.1: Forward diffusion and reverse denoising](img/B21263_04_01.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图4.1：正向扩散和反向去噪](img/B21263_04_01.jpg)'
- en: 'Figure 4.1: Forward diffusion and reverse denoising'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1：正向扩散和反向去噪
- en: We employ a predetermined forward diffusion process, denoted as q, which systematically
    introduces Gaussian noise to an image until it culminates in pure noise. The process
    is denoted by q(x t | x t-1). Note that the reverse process p θ(x t-1 | x t) is
    still unknown.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用一个预定的正向扩散过程，表示为q，该过程系统地给图像引入高斯噪声，直到最终变成纯噪声。这个过程表示为q(x t | x t-1)。请注意，反向过程p θ(x t-1
    | x t)仍然未知。
- en: 'One step of the forward diffusion process can be denoted as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 正向扩散过程的一步可以表示如下：
- en: q(x t | x t-1) ≔ 𝒩(x t; √ _ 1 − β t x t-1 , β t I)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: q(x t | x t-1) ≔ 𝒩(x t; √ _ 1 − β t x t-1 , β t I)
- en: 'Let me explain this formula bit by bit from left to right:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我从左到右一点一点解释这个公式：
- en: The notation q(x t | x t-1) is used to denote a conditional probability distribution.
    In this case, the distribution q represents the probability of observing the noisy
    image x t given the previous image x t−1.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 符号q(x t | x t-1)用来表示条件概率分布。在这种情况下，分布q表示在给定先前图像x t−1的情况下观察到噪声图像x t的概率。
- en: 'The define sign : = is used in the formula instead of the tilde symbol (∼)
    because the diffusion forward process is a deterministic process. The tilde symbol
    (∼) is typically used to represent a distribution. In this case, if we used the
    tilde symbol, the formula would be saying that the noisy image is a complete Gaussian
    distribution. However, this is not the case. The noisy image in t step is defined
    by a deterministic function of the previous image and added noise.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 公式中使用定义符号 := 而不是波浪符号 (∼) 是因为扩散前向过程是一个确定性过程。波浪符号 (∼) 通常用于表示分布。在这种情况下，如果我们使用波浪符号，公式将表示噪声图像是一个完整的高斯分布。然而，情况并非如此。t
    步的噪声图像是由前一个图像和添加的噪声的确定性函数定义的。
- en: Then why is 𝒩 used here? The 𝒩 symbol is used to represent a Gaussian distribution.
    However, in this case, the 𝒩 symbol is being used to represent the functional
    form of the noisy image.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 那么为什么这里使用 𝒩 符号呢？𝒩 符号用于表示高斯分布。然而，在这种情况下，𝒩 符号被用来表示噪声图像的函数形式。
- en: On the right side, before the semicolon, x t is the thing we want to have in
    normal distribution. After the semicolon, those are the parameters of the distribution.
    A semicolon is usually used to separate the output and parameters.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在右侧，分号之前，x_t 是我们希望在正态分布中的东西。分号之后，那些是分布的参数。分号通常用于分隔输出和参数。
- en: β t is the noise variance at step t. √ _ 1 − β t  x t−1 is the mean of the new
    distribution.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: βt 是第 t 步的噪声方差。√_1 − βt x_t−1 是新分布的均值。
- en: Why is the big I used in the formula? Because an RGB image can have multiple
    channels, and the identity matrix can apply the noise variance to different channels
    independently.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么公式中使用大写的 I 呢？因为 RGB 图像可以有多个通道，而单位矩阵可以将噪声方差独立地应用于不同的通道。
- en: 'It is quite easy to add Gaussian noise to an image using Python:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Python 添加高斯噪声到图像相当简单：
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To execute the preceding code, you will also need to install the `ipyplot`
    package by `pip install ipyplot`. The code provided performs a simulation of a
    forward diffusion process on an image and then visualizes the progression of this
    process over a number of iterations. Here’s a step-by-step explanation of what
    each part of the code is doing:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行前面的代码，您还需要通过 `pip install ipyplot` 安装 `ipyplot` 包。提供的代码在图像上执行前向扩散过程的模拟，然后可视化这个过程在多次迭代中的进展。以下是代码每个部分所做操作的逐步解释：
- en: 'Importing libraries:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入库：
- en: '`ipyplot` is a library for plotting images in Jupyter notebooks in a more interactive
    way.'
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ipyplot` 是一个库，用于以更交互的方式在 Jupyter 笔记本中绘制图像。'
- en: '`PIL` (which stands for `Image` module, is used for image manipulation.'
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PIL`（代表 `Image` 模块，用于图像处理）。'
- en: 'Loading the image:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载图像：
- en: '`img_path` is defined as the path to the `image` file `dog.png`.'
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`img_path` 被定义为 `image` 文件 `dog.png` 的路径。'
- en: '`image` is loaded using `plt.imread(img_path)`.'
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `plt.imread(img_path)` 加载 `image`。
- en: 'Setting parameters:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置参数：
- en: '`num_iterations` defines the number of times the diffusion process will be
    simulated.'
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_iterations` 定义了扩散过程将被模拟的次数。'
- en: '`beta` is a parameter that simulates noise variance in the diffusion process.'
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beta` 是一个参数，用于模拟扩散过程中的噪声方差。'
- en: 'Initializing lists:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化列表：
- en: '`images` is initialized as an empty list, which will later hold the PIL image
    objects that result from each iteration of the diffusion process.'
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`images` 被初始化为一个空列表，它将随后保存扩散过程每次迭代产生的 PIL 图像对象。'
- en: '`steps` is a list of strings that will act as labels for the images when they
    are plotted, indicating the step number for each image.'
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`steps` 是一个字符串列表，当图像被绘制时将作为标签使用，指示每个图像的步骤编号。'
- en: 'Forward diffusion process:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前向扩散过程：
- en: A `for` loop runs for `num_iterations` times, each time performing a diffusion
    step. `mean` is computed by scaling the image with a factor of `sqrt(1 -` `beta)`.
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 `for` 循环运行 `num_iterations` 次，每次执行一个扩散步骤。`mean` 通过将图像乘以 `sqrt(1 - beta)`
    的因子来计算。
- en: A new image is generated by adding Gaussian noise to the mean, where the noise
    has a standard deviation of `beta`. This is done using `np.random.normal`.
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过向均值添加高斯噪声生成新的图像，其中噪声的标准差为 `beta`。这是通过 `np.random.normal` 实现的。
- en: The resulting image array values are scaled to the range 0-255 and converted
    to an 8-bit unsigned integer format, which is a common format for images.
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果的图像数组值被缩放到 0-255 范围，并转换为 8 位无符号整数格式，这是图像的常见格式。
- en: '`pil_image` is created by converting the image array to a PIL image object
    in RGB mode.'
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pil_image` 通过将图像数组转换为 RGB 模式的 PIL 图像对象来创建。'
- en: Plot the image using `ipyplot` in a grid as shown in *Figure 4**.2*.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `ipyplot` 在网格中绘制图像，如图 *图 4.2* 所示。
- en: '![Figure 4.2: Add noise to the image](img/B21263_04_02.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.2：向图像添加噪声](img/B21263_04_02.jpg)'
- en: 'Figure 4.2: Add noise to the image'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2：向图像添加噪声
- en: From the result, we can see that even though every image is from a normal distribution
    function, not every image is a complete Gaussian distribution, or more strictly
    speaking, an `1000`, and later, in Stable Diffusion, the step number is reduced
    to between `20` to `50`.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果中，我们可以看到，尽管每个图像都来自正态分布函数，但并非每个图像都是完整的高斯分布，或者更严格地说，是 `1000`，后来在稳定扩散中，步骤数减少到
    `20` 到 `50` 之间。
- en: If the last image of *Figure 4**.2* is an isotropic Gaussian distribution, its
    2D distribution visualization will appear as a circle; it is characterized by
    having equal variances in all dimensions. In other words, the spread or width
    of the distribution is the same along all axes.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 *图 4.2* 的最后一幅图像是各向同性的高斯分布，其 2D 分布可视化将呈现为一个圆圈；它以所有维度具有相等的方差为特征。换句话说，分布的扩散或宽度沿所有轴都是相同的。
- en: 'Let’s plot an image pixel distribution after adding 16x times Gaussian noise:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在添加 16 倍高斯噪声后绘制图像像素分布：
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The result is shown in *Figure 4**.3*.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如图 *图 4.3* 所示。
- en: '![Figure 4.3: A nearly isotropic, normally distributed noise image](img/B21263_04_03.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.3：几乎各向同性的正态分布噪声图像](img/B21263_04_03.jpg)'
- en: 'Figure 4.3: A nearly isotropic, normally distributed noise image'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3：几乎各向同性的正态分布噪声图像
- en: The figure shows how the code efficiently transforms an image into a nearly
    isotropic, normally distributed noise image in just 16 steps, as illustrated in
    the last image of *Figure 4**.2*.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 该图显示了代码如何仅用 16 步高效地将图像转换为几乎各向同性的正态分布噪声图像，如图 *图 4.2* 的最后一幅图像所示。
- en: A more efficient forward diffusion process
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更高效的正向扩散过程
- en: If we use the chained process to calculate a noisy image at t step, it first
    requires calculating the noisy image from 1 to t − 1 steps, which is not efficient.
    We can leverage a trick called **reparameterization** [10] to transform the original
    chained process into a one-step process. Here is what the trick looks like.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用链式过程在 t 步计算一个噪声图像，它首先需要计算从 1 到 t - 1 步的噪声图像，这并不高效。我们可以利用一种称为**重参数化**[10]的技巧将原始链式过程转换为一步过程。这个技巧看起来是这样的。
- en: 'If we have a Gaussian distribution z with μ as the mean and σ 2 variance:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一个以 μ 为均值、σ² 为方差的高斯分布 z：
- en: z ∼ 𝒩(μ, σ 2)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: z ∼ 𝒩(μ, σ²)
- en: 'Then, we can rewrite the distribution as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以将分布重写如下：
- en: ϵ ∼ 𝒩(0,1)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ϵ ∼ 𝒩(0,1)
- en: z = μ+ σϵ
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: z = μ + σϵ
- en: 'The benefit brought by this trick is that we can now calculate an image at
    any step with a one-step calculation, which will greatly boost the training performance:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这个技巧带来的好处是，我们现在可以一步计算计算任何步骤的图像，这将大大提高训练性能：
- en: x t = √ _ 1 − β t  x t−1 + √ _ β t  ϵ t−1
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: x_t = √(1 − β_t) x_{t−1} + √(β_t) ϵ_{t−1}
- en: 'Now, say we define the following:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设我们定义以下内容：
- en: α t = 1 − β t
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: α_t = 1 − β_t
- en: 'We now have the following:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有以下内容：
- en: _ α t = ∏ i=1 t α i
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: _α_t = ∏(i=1 to t) α_i
- en: 'There is no magic here; define α t and α ‾  t is only for convenience, so that
    we can calculate a noised image at step t and generate x t from the source un-noised
    image x 0 using the following equation:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这里没有魔法；定义 α_t 和 α_‾_t 只是为了方便，这样我们就可以在 t 步计算一个噪声图像，并使用以下方程从源未噪声图像 x_0 生成 x_t：
- en: x t = √ _ _ α t  x 0 + √ _ 1 − _ α t
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: x_t = √(1 − β_t) x_{t−1} + √(β_t) ϵ_{t−1}
- en: What do α t and α ‾  t look like? Here is a simplified sample (*Figure 4**.4*).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: α_t 和 α_‾_t 看起来是什么样子？这里是一个简化的示例 (*图 4.4*)。
- en: '![Figure 4.4: Implementation of reparameterization](img/B21263_04_04.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.4：重参数化实现](img/B21263_04_04.jpg)'
- en: 'Figure 4.4: Implementation of reparameterization'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4：重参数化实现
- en: In *Figure 4**.4*, we have all the same α - 0.1 and β - 0.9\. Now, whenever
    we need to generate a noised image x t, we can quickly calculate α ‾  t from known
    numbers; the lines show what numbers are used to calculate α ‾  t.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图 4.4* 中，我们所有的 α - 0.1 和 β - 0.9 都是相同的。现在，每当我们需要生成一个噪声图像 x_t 时，我们可以快速从已知数字中计算出
    α_‾_t；线条显示了用于计算 α_‾_t 的数字。
- en: 'The following code can generate a noised image at any step:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码可以在任何步骤生成一个噪声图像：
- en: '[PRE2]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This code is the implementation of the previously presented math formula. I
    present the code here to help build a correlated understanding between the math
    formula and the real implementation. If you are familiar with Python, you may
    find that this code makes the underlying subtleties easier to understand. The
    code can generate a noised image as shown in *Figure 4**.5*.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码是之前展示的数学公式的实现。我在这里展示代码是为了帮助大家建立数学公式与实际实现之间的关联理解。如果你熟悉 Python，可能会发现这段代码使得底层细节更容易理解。该代码可以生成如图
    *图 4.5* 所示的带噪声的图像。
- en: '![Figure 4.5: Implementation of reparameterization](img/B21263_04_05.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.5：重参数化实现](img/B21263_04_05.jpg)'
- en: 'Figure 4.5: Implementation of reparameterization'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.5：重参数化实现
- en: Now, let’s think about how to recover an image by leveraging a neural network.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们思考如何利用神经网络恢复图像。
- en: The noise-to-image training process
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 噪声到图像的训练过程
- en: We have the solution to add noise to the image, which is known as forward diffusion,
    as shown in *Figure 4**.6*. To recover an image from the noise, or **reverse diffusion**,
    as shown in *Figure 4**.6*, we need to find a way to implement the reverse step
    p θ(x t−1| x t). However, this step is intractable or uncomputable without additional
    help.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经有了向图像添加噪声的解决方案，这被称为正向扩散，如图 *图 4.6* 所示。要从噪声中恢复图像，或进行**反向扩散**，如图 *图 4.6* 所示，我们需要找到一种方法来实现反向步骤
    pθ(xt−1|xt)。然而，没有额外的帮助，这一步是无法处理的或无法计算的。
- en: Consider that we have the ending Gaussian noise data, and all those noise step
    data in hand. What if we can train a neural network that can reverse the process?
    We can use the neural network to provide the mean and variance of a noise image
    and then remove the generated noise from the previous image data. By doing this,
    we should be able to use this step to represent p θ(x t−1| x t), and thus recover
    an image.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到我们已经有最终的高斯噪声数据，以及所有噪声步骤数据在手。如果我们能够训练一个能够逆转过程的神经网络会怎样？我们可以使用神经网络来提供噪声图像的均值和方差，然后从之前图像数据中移除生成的噪声。通过这样做，我们应该能够使用这一步来表示
    pθ(xt−1|xt)，从而恢复图像。
- en: '![Figure 4.6: Forward diffusion and reverse process](img/B21263_04_06.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.6：正向扩散和反向过程](img/B21263_04_06.jpg)'
- en: 'Figure 4.6: Forward diffusion and reverse process'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.6：正向扩散和反向过程
- en: You may ask how we should calculate the loss and update the weights. The ending
    image (x T) removes the previously added noise and will provide the ground truth
    data. After all, we can generate the noise data in the forward diffusion processes
    on the fly. Next, compare it with the output data from the neural network (usually
    a UNet). We get the loss data that can be used to calculate the gradient descendant
    data and update the neural network weights.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问我们应该如何计算损失并更新权重。最终图像 (xT) 移除了之前添加的噪声，并将提供真实数据。毕竟，我们可以在正向扩散过程中实时生成噪声数据。接下来，将其与神经网络（通常是
    UNet）的输出数据进行比较。我们得到损失数据，可用于计算梯度下降数据并更新神经网络权重。
- en: 'The DDPM paper [4] provided a simplified way to calculate the loss:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: DDPM 论文 [4] 提供了一种简化的损失计算方法：
- en: 'L simple(θ) : = 𝔼 t, x 0,∈[|| ∈ − ∈ θ(√ _ _ α t  x 0 + √ _ 1 − _ α t  ϵ, t)
    || 2]'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Lsimple(θ) := 𝔼t, x0∈[||∈ − ∈θ(√αt xt0 + √1 − αt ϵ, t) ||²]
- en: 'Since x t = √ _ _ α t x 0  + √ _ 1 − _ α t , we can further simplify the formula
    to the following:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 xt = √αt xt0 + √1 − αt，我们可以进一步简化公式为以下形式：
- en: L simple(θ) ≔ 𝔼 t,x 0,ϵ[||ϵ − ϵ θ(x t, t) || 2]
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Lsimple(θ) ≔ 𝔼t,x0,ϵ[||ϵ − ϵθ(xt, t) ||²]
- en: 'The UNet will take a noised image data: x t and a time step data: t as inputs
    as shown in *Figure 4**.7*. Why take t as input? Because all the denoising processes
    share the same neural network weights, the input t will help train a UNet with
    a time step in mind.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: UNet 将以带噪声的图像数据 x_t 和时间步数据 t 作为输入，如图 *图 4.7* 所示。为什么以 t 作为输入？因为所有去噪过程都共享相同的神经网络权重，输入
    t 将帮助训练一个考虑时间步的 UNet。
- en: '![Figure 4.7: UNet training inputs and loss calculation](img/B21263_04_07.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.7：UNet 训练输入和损失计算](img/B21263_04_07.jpg)'
- en: 'Figure 4.7: UNet training inputs and loss calculation'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.7：UNet 训练输入和损失计算
- en: When we say let’s train a neural network to predict the noise distribution that
    will be removed from the image leading to a clearer image, what is the neural
    network predicting? In the DDPM paper [4], the original diffusion model uses a
    fixed variance θ, and sets the Gaussian distribution mean - μ as the only parameter
    that needs to be learned through a neural network.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们说训练一个神经网络来预测将被从图像中移除的噪声分布，从而得到更清晰的图像时，神经网络预测的是什么？在DDPM论文[4]中，原始的扩散模型使用一个固定的方差θ，并将高斯分布的均值-
    μ作为唯一需要通过神经网络学习的参数。
- en: 'In a PyTorch implementation, the loss data can be calculated like this:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch实现中，损失数据可以计算如下：
- en: '[PRE3]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now, we should be able to train a diffusion model and the model should be able
    to recover an image from a random Gaussian distributed noise. Next, let’s take
    a look at how the inference or sampling works.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们应该能够训练一个扩散模型，并且该模型应该能够从随机高斯分布的噪声中恢复图像。接下来，让我们看看推理或采样的工作原理。
- en: The noise-to-image sampling process
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 噪声到图像的采样过程
- en: 'Here are the steps to sample an image from the model, or, in other words, generate
    an image from the reverse diffusion process:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是从模型中采样图像的步骤，或者换句话说，通过反向扩散过程生成图像：
- en: 'Generate a complete Gaussian noise with a mean of 0 and a variance of 1:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成一个均值为0，方差为1的完整高斯噪声：
- en: x T ∼ 𝒩(0,1)
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: x T ∼ 𝒩(0,1)
- en: We will use this noise as the starting image.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用这个噪声作为起始图像。
- en: '2. Loop through t = T to t = 1\. In each step, if t > 1, then generate another
    Gaussian noise image z:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 从t = T循环到t = 1。在每一步中，如果t > 1，则生成另一个高斯噪声图像z：
- en: z ∼ 𝒩(0,1)
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: z ∼ 𝒩(0,1)
- en: 'If t = 1, then the following occurs:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果t = 1，则以下情况发生：
- en: z = 0
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: z = 0
- en: 'Then, generate a noise from the UNet model, and remove the generated noise
    from the input noisy image x t:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，从UNet模型生成噪声，并从输入的噪声图像x t中移除生成的噪声：
- en: x t-1 =  1 _ √ _ α t (x t −  1 − α t _ √ _ 1 − _ α t  ϵ θ(x t, t)) + √ _ 1 −
    α t  z
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: x t-1 =  1 _ √ _ α t (x t −  1 − α t _ √ _ 1 − _ α t  ϵ θ(x t, t)) + √ _ 1 −
    α t  z
- en: If we take a look at the preceding equation, all those α t and α ‾  t are known
    numbers sourced from β t. The only thing we need from the UNet is the ϵ θ(x t,
    t), which is the noise produced by the UNet, as shown in *Figure 4**.8*.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们看一下前面的方程，所有的α t和α ‾  t都是来自β t的已知数字。我们唯一需要从UNet得到的是ϵ θ(x t, t)，这是UNet产生的噪声，如*图4**.8*所示。8*。
- en: '![Figure 4.8: Sampling from UNet](img/B21263_04_08.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图4.8：从UNet采样](img/B21263_04_08.jpg)'
- en: 'Figure 4.8: Sampling from UNet'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.8：从UNet采样
- en: The added √ _ 1 − α t  z looks a little bit mysterious here. Why add this to
    the process? The original paper doesn’t explain this added noise, but researchers
    found that the added noise in the denoising process will significantly improve
    the generated image quality!
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里添加的√ _ 1 − α t  z看起来有点神秘。为什么要添加这个过程？原始论文没有解释这个添加的噪声，但研究人员发现，在去噪过程中添加的噪声将显著提高生成的图像质量！
- en: 3. Loop end, return the final generated image x 0.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 循环结束，返回最终生成的图像x 0。
- en: Now, let’s talk about the image generation guidance.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们谈谈图像生成引导。
- en: Understanding Classifier Guidance denoising
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解分类器引导去噪
- en: Until now, we haven’t talked about the text guidance yet. The image generation
    process will take a random Gaussian noise as the only input, and then randomly
    generate an image based on the training dataset. But we want a guided image generation;
    for example, input “dog” to ask the diffusion model to generate an image including
    “dog.”
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们还没有讨论文本引导。图像生成过程将以随机高斯噪声作为唯一输入，然后根据训练数据集随机生成图像。但我们需要一个引导的图像生成；例如，输入“dog”来要求扩散模型生成包含“dog”的图像。
- en: In 2021, Dhariwal and Nichol, from OpenAI, proposed classifier guidance in their
    paper titled *Diffusion Models Beat GANs on Image* *Synthesis* [12].
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在2021年，来自OpenAI的Dhariwal和Nichol在他们题为*扩散模型在图像* *合成* [12]的论文中提出了分类器引导。
- en: Based on the proposed methodology, we can achieve classifier-guided denoising
    by providing a classification label during the training stage. Instead of just
    image or time-step embedding, we also provide text description embeddings as shown
    in *Figure 4**.9*.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 根据提出的方法，我们可以在训练阶段提供分类标签来实现分类器引导去噪。除了图像或时间步长嵌入之外，我们还提供了文本描述嵌入，如*图4**.9*所示。
- en: '![Figure 4.9: Train a diffusion model with conditional text](img/B21263_04_09.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图4.9：使用条件文本训练扩散模型](img/B21263_04_09.jpg)'
- en: 'Figure 4.9: Train a diffusion model with conditional text'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.9：使用条件文本训练扩散模型
- en: In *Figure 4**.7*, there are two inputs, while in *Figure 4**.9*, there is one
    additional input – **Text embedding**; it is the embedding data generated from
    OpenAI’s CLIP model. We will discuss the way more powerful CLIP model guided diffusion
    model in the next chapter.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图4**.7*中，有两个输入，而在*图4**.9*中，有一个额外的输入 – **文本嵌入**；这是由OpenAI的CLIP模型生成的嵌入数据。我们将在下一章讨论更强大的CLIP模型引导的扩散模型。
- en: Summary
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we took a deep dive into the internal workings of the diffusion
    model initially brought out by Jonathan Ho et al. [4]. We learned about the foundational
    ideas of the diffusion model and learned about the forward diffusion process.
    We also walked through the reverse diffusion process for diffusion model training
    and sampling and explored how to enable a text-guided diffusion model.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们深入探讨了由约翰逊·霍等最初提出的扩散模型的内部工作原理。[4]。我们了解了扩散模型的基础思想，并学习了正向扩散过程。我们还了解了扩散模型训练和采样的反向扩散过程，并探讨了如何实现文本引导的扩散模型。
- en: Through this chapter, we aimed to explain the core idea of the diffusion model.
    If you want to implement a diffusion model by yourself, I would recommend reading
    through the original DDPM paper directly.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本章，我们旨在解释扩散模型的核心思想。如果你想自己实现扩散模型，我建议直接阅读原始DDPM论文。
- en: The DDPM diffusion model can generate realistic images, but one of its problems
    is its performance. Not only is training a model slow, but the image sampling
    is also slow. In the next chapter, we are going to discuss the Stable Diffusion
    model, which will boost the speed in a genius way.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: DDPM扩散模型可以生成逼真的图像，但其中一个问题是其性能。不仅训练模型速度慢，图像采样也慢。在下一章中，我们将讨论Stable Diffusion模型，它将以天才的方式提高速度。
- en: References
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '*The Annotated Diffusion Model* – [https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/annotated_diffusion.ipynb#scrollTo=c5a94671](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/annotated_diffusion.ipynb#scrollTo=c5a94671'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*注释过的扩散模型* – [https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/annotated_diffusion.ipynb#scrollTo=c5a94671](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/annotated_diffusion.ipynb#scrollTo=c5a94671)'
- en: )
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: '*Training with Diffusers* – [https://colab.research.google.com/gist/anton-l/f3a8206dae4125b93f05b1f5f703191d/diffusers_training_example.ipynb](https://colab.research.google.com/gist/anton-l/f3a8206dae4125b93f05b1f5f703191d/diffusers_training_example.ipynb)'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*使用扩散器进行训练* – [https://colab.research.google.com/gist/anton-l/f3a8206dae4125b93f05b1f5f703191d/diffusers_training_example.ipynb](https://colab.research.google.com/gist/anton-l/f3a8206dae4125b93f05b1f5f703191d/diffusers_training_example.ipynb)'
- en: '*Diffusers* – [https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb#scrollTo=PzW5ublpBuUt](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb#scrollTo=PzW5ublpBuUt'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*扩散器* – [https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb#scrollTo=PzW5ublpBuUt](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb#scrollTo=PzW5ublpBuUt)'
- en: )
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: Jonathan Ho et al., *Denoising Diffusion Probabilistic Models* – [https://arxiv.org/abs/2006.11239](https://arxiv.org/abs/2006.11239)
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 约翰逊·霍等，*去噪扩散概率模型* – [https://arxiv.org/abs/2006.11239](https://arxiv.org/abs/2006.11239)
- en: Steins, *Diffusion Model Clearly Explained!* – [https://medium.com/@steinsfu/diffusion-model-clearly-explained-cd331bd41166](https://medium.com/@steinsfu/diffusion-model-clearly-explained-cd331bd41166)
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 斯蒂恩斯，*扩散模型清晰解释!* – [https://medium.com/@steinsfu/diffusion-model-clearly-explained-cd331bd41166](https://medium.com/@steinsfu/diffusion-model-clearly-explained-cd331bd41166)
- en: Steins, *Stable Diffusion Clearly Explained!* – [https://medium.com/@steinsfu/stable-diffusion-clearly-explained-ed008044e07e](https://medium.com/@steinsfu/stable-diffusion-clearly-explained-ed008044e07e)
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 斯蒂恩斯，*Stable Diffusion清晰解释!* – [https://medium.com/@steinsfu/stable-diffusion-clearly-explained-ed008044e07e](https://medium.com/@steinsfu/stable-diffusion-clearly-explained-ed008044e07e)
- en: DeepFindr, *Diffusion models from scratch in PyTorch* – [https://www.youtube.com/watch?v=a4Yfz2FxXiY&t=5s&ab_channel=DeepFindr](https://www.youtube.com/watch?v=a4Yfz2FxXiY&t=5s&ab_channel=DeepFindr
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DeepFindr，*从零开始使用PyTorch构建扩散模型* – [https://www.youtube.com/watch?v=a4Yfz2FxXiY&t=5s&ab_channel=DeepFindr](https://www.youtube.com/watch?v=a4Yfz2FxXiY&t=5s&ab_channel=DeepFindr)
- en: )
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: Ari Seff, *What are Diffusion Models?* – [https://www.youtube.com/watch?v=fbLgFrlTnGU&ab_channel=AriSeff](https://www.youtube.com/watch?v=fbLgFrlTnGU&ab_channel=AriSeff
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 阿里·塞夫，*什么是扩散模型？* – [https://www.youtube.com/watch?v=fbLgFrlTnGU&ab_channel=AriSeff](https://www.youtube.com/watch?v=fbLgFrlTnGU&ab_channel=AriSeff)
- en: )
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: Prafulla Dhariwal, Alex Nichol*, Diffusion Models Beat GANs on Image Synthesis*
    – [https://arxiv.org/abs/2105.05233](https://arxiv.org/abs/2105.05233)
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Prafulla Dhariwal, Alex Nichol*，**扩散模型在图像合成上击败了GANs** – [https://arxiv.org/abs/2105.05233](https://arxiv.org/abs/2105.05233)
- en: Diederik P Kingma, Max Welling, *Auto-Encoding Variational Bayes* – [https://arxiv.org/abs/1312.6114](https://arxiv.org/abs/1312.6114)
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Diederik P Kingma, Max Welling，**自动编码变分贝叶斯** – [https://arxiv.org/abs/1312.6114](https://arxiv.org/abs/1312.6114)
- en: Lilian Weng, *What are Diffusion Models?* – [https://lilianweng.github.io/posts/2021-07-11-diffusion-models/](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/)
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Lilian Weng，**什么是扩散模型？** – [https://lilianweng.github.io/posts/2021-07-11-diffusion-models/](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/)
- en: Prafulla Dhariwal, Alex Nichol, *Diffusion Models Beat GANs on Image Synthesis*
    – [https://arxiv.org/abs/2105.05233](https://arxiv.org/abs/2105.05233)
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Prafulla Dhariwal, Alex Nichol，**扩散模型在图像合成上击败了GANs** – [https://arxiv.org/abs/2105.05233](https://arxiv.org/abs/2105.05233)
