<html><head></head><body>
  <div><h1 class="chapterNumber">1</h1>
    <h1 id="_idParaDest-15" class="chapterTitle">Why Retrieval Augmented Generation?</h1>
    <p class="normal">Even the most advanced generative AI models can only generate responses based on the data they have been trained on. They cannot provide accurate answers to questions about information outside their training data. Generative AI models simply don’t know that they don’t know! This leads to inaccurate or inappropriate outputs, sometimes called hallucinations, bias, or, simply said, nonsense.</p>
    <p class="normal"><strong class="keyWord">Retrieval Augmented Generation</strong> (<strong class="keyWord">RAG</strong>) is a framework that addresses this limitation by combining retrieval-based<a id="_idIndexMarker000"/> approaches with generative models. It retrieves relevant data from external sources in real time and uses this data to generate more accurate and contextually relevant responses. Generative AI models integrated with RAG retrievers are revolutionizing the field with their unprecedented efficiency and power. One of the key strengths of RAG is its adaptability. It can be seamlessly applied to any type of data, be it text, images, or audio. This versatility makes RAG ecosystems a reliable and efficient tool for enhancing generative AI capabilities.</p>
    <p class="normal">A project manager, however, already encounters a wide range of generative AI platforms, frameworks, and models such as Hugging Face, Google Vertex AI, OpenAI, LangChain, and more. An additional layer of emerging RAG frameworks and platforms will only add complexity with Pinecone, Chroma, Activeloop, LlamaIndex, and so on. All these Generative AI and RAG frameworks often overlap, creating an incredible number of possible configurations. Finding the right configuration of models and RAG resources for a specific project, therefore, can be challenging for a project manager. There is no silver bullet. The challenge is tremendous, but the rewards, when achieved, are immense!</p>
    <p class="normal">We will begin this chapter by defining the RAG framework at a high level. Then, we will define the three main RAG configurations: naïve RAG, advanced RAG, and modular RAG. We will also compare RAG and fine-tuning and determine when to use these approaches. RAG can only exist within an ecosystem, and we will design and describe one in this chapter. Data needs to come from somewhere and be processed. Retrieval requires an organized environment to retrieve data, and generative AI models have input constraints.</p>
    <p class="normal">Finally, we will dive into the practical aspect of this chapter. We will build a Python program from scratch to run entry-level naïve RAG with keyword search and matching. We will also code an advanced RAG system with vector search and index-based retrieval. Finally, we will build a modular RAG that takes both naïve and advanced RAG into account. By the end of this chapter, you will acquire a theoretical understanding of the RAG framework and practical experience in building a RAG-driven generative AI program. This hands-on approach will deepen your understanding and equip you for the following chapters.</p>
    <p class="normal">In a nutshell, this chapter covers the following topics:</p>
    <ul>
      <li class="bulletList">Defining the RAG framework</li>
      <li class="bulletList">The RAG ecosystem</li>
      <li class="bulletList">Naïve keyword search and match RAG in Python</li>
      <li class="bulletList">Advanced RAG with vector-search and index-based RAG in Python</li>
      <li class="bulletList">Building a modular RAG program</li>
    </ul>
    <p class="normal">Let’s begin by defining RAG.</p>
    <h1 id="_idParaDest-16" class="heading-1">What is RAG?</h1>
    <p class="normal">When a generative AI <a id="_idIndexMarker001"/>model doesn’t know how to answer accurately, some say it is hallucinating or producing bias. Simply said, it just produces nonsense. However, it all boils down to the impossibility of providing an adequate response when the model’s training didn’t include the information requested beyond the classical model configuration issues. This confusion often leads to random sequences of the most probable outputs, not the most accurate ones.</p>
    <p class="normal">RAG begins where generative AI ends by providing the information an LLM model lacks to answer accurately. RAG was designed (Lewis et al., 2020) for LLMs. The RAG framework will perform optimized information retrieval tasks, and the generation ecosystem will add this information <a id="_idIndexMarker002"/>to the input (user query or automated prompt) to produce improved output. The RAG framework can be summed up at a high level in the following figure:</p>
    <figure class="mediaobject"><img src="img/B31169_01_01.png" alt="A diagram of a library  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 1.1: The two main components of RAG-driven generative AI</p>
    <p class="normal">Think of yourself as a student in a library. You have an essay to write on RAG. Like ChatGPT, for example, or any other AI copilot, you have learned how to read and write. As with any <strong class="keyWord">Large Language Model</strong> (<strong class="keyWord">LLM</strong>), you are<a id="_idIndexMarker003"/> sufficiently trained to read advanced information, summarize it, and write content. However, like any superhuman AI you will find from Hugging Face, Vertex AI, or OpenAI, there are many things you don’t know.</p>
    <p class="normal">In the <em class="italic">retrieval</em> phase, you search the library for books on the topic you need (the left side of <em class="italic">Figure 1.1</em>). Then, you go back to your seat, perform a retrieval task by yourself or a co-student, and extract the information you need from those books. In the <em class="italic">generation</em> phase (the right side of <em class="italic">Figure 1.1</em>), you begin to write your essay. You are a RAG-driven generative human agent, much like a RAG-driven generative AI framework.</p>
    <p class="normal">As you continue to write your essay on RAG, you stumble across some tough topics. You don’t have the time to go through all the information available physically! You, as a generative human agent, are stuck, just as a generative AI model would be. You may try to write something, just as a generative AI model does when its output makes little sense. But you, like the generative AI agent, will not realize whether the content is accurate or not until somebody corrects your essay and you get a grade that will rank your essay.</p>
    <p class="normal">At this point, you have reached <a id="_idIndexMarker004"/>your limit and decide to turn to a RAG generative AI copilot to ensure you get the correct answers. However, you are puzzled by the number of LLM models and RAG configurations available. You need first to understand the resources available and how RAG is organized. Let’s go through the main RAG configurations.</p>
    <h1 id="_idParaDest-17" class="heading-1">Naïve, advanced, and modular RAG configurations</h1>
    <p class="normal">A RAG framework necessarily contains two<a id="_idIndexMarker005"/> main components: a retriever and a <a id="_idIndexMarker006"/>generator. The generator can be any LLM or foundation multimodal AI platform or model, such as GPT-4o, Gemini, Llama, or one of the hundreds of variations of the initial architectures. The retriever can be any of the emerging frameworks, methods, and tools such as Activeloop, Pinecone, LlamaIndex, LangChain, Chroma, and many more.</p>
    <p class="normal">The issue now is to decide which of the three types of RAG frameworks (Gao et al., 2024) will fit the needs of a project. We will illustrate these three approaches in code in the <em class="italic">Naïve, advanced, and modular RAG in code</em> section of this chapter:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Naïve RAG</strong>: This type of RAG framework doesn’t involve complex data embedding and indexing. It can<a id="_idIndexMarker007"/> be efficient to access<a id="_idIndexMarker008"/> reasonable amounts of data through keywords, for example, to augment a user’s input and obtain a satisfactory response.</li>
      <li class="bulletList"><strong class="keyWord">Advanced RAG</strong>: This type of <a id="_idIndexMarker009"/>RAG involves more complex scenarios, such as with vector search and indexed-base retrieval applied. Advanced<a id="_idIndexMarker010"/> RAG can be implemented with a wide range of methods. It can process multiple data types, as well as multimodal data, which can be structured or unstructured.</li>
      <li class="bulletList"><strong class="keyWord">Modular RAG</strong>: Modular RAG broadens the horizon to include any scenario that involves naïve RAG, advanced <a id="_idIndexMarker011"/>RAG, machine learning, and <a id="_idIndexMarker012"/>any algorithm needed to complete a complex project.</li>
    </ul>
    <p class="normal">However, before going further, we need to decide if we should implement RAG or fine-tune a model.</p>
    <h1 id="_idParaDest-18" class="heading-1">RAG versus fine-tuning</h1>
    <p class="normal">RAG is not always an alternative to fine-tuning, and fine-tuning cannot always replace RAG. If we accumulate too much data in <a id="_idIndexMarker013"/>RAG datasets, the system may become too cumbersome to manage. On the other hand, we cannot fine-tune a model with dynamic, ever-changing data such as daily weather forecasts, stock market values, corporate news, and all forms of daily events.</p>
    <p class="normal">The decision of whether to implement RAG or fine-tune a model relies on the proportion of parametric versus non-parametric information. The fundamental difference between a model<a id="_idIndexMarker014"/> trained from scratch or fine-tuned and RAG can be summed up in terms of parametric and non-parametric knowledge:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Parametric</strong>: In a RAG-driven generative AI ecosystem, the parametric part refers to the generative AI model’s parameters (weights) learned through training data. This means the<a id="_idIndexMarker015"/> model’s knowledge is stored in these learned weights and biases. The original training data is transformed into a mathematical form, which we call a parametric representation. Essentially, the model “remembers” what it learned from the data, but the data itself is not stored explicitly.</li>
      <li class="bulletList"><strong class="keyWord">Non-Parametric</strong>: In contrast, the non-parametric part of a RAG ecosystem involves storing explicit data that<a id="_idIndexMarker016"/> can be accessed directly. This means that the data remains available and can be queried whenever needed. Unlike parametric models, where knowledge is embedded indirectly in the weights, non-parametric data in RAG allows us to see and use the actual data for each output.</li>
    </ul>
    <p class="normal">The difference between RAG and fine-tuning relies on the amount of static (parametric) and dynamic (non-parametric) ever-evolving data the generative AI model must process. A system that relies too heavily on RAG might become overloaded and cumbersome to manage. A system that relies too much on fine-tuning a generative model will display its inability to adapt to daily information updates.</p>
    <p class="normal">There is a decision-making threshold illustrated in <em class="italic">Figure 1.2</em> that shows that a RAG-driven generative AI project manager will have to evaluate the potential of the ecosystem’s trained parametric <a id="_idIndexMarker017"/>generative AI model before implementing a non-parametric (explicit data) RAG framework. The potential of the RAG component requires careful evaluation as well.</p>
    <figure class="mediaobject"><img src="img/B31169_01_02.png" alt="A diagram of a temperature measurement  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 1.2: The decision-making threshold between enhancing RAG or fine-tuning an LLM</p>
    <p class="normal">In the end, the balance between enhancing the retriever and the generator in a RAG-driven generative AI ecosystem depends on a project’s specific requirements and goals. RAG and fine-tuning are not mutually exclusive.</p>
    <p class="normal">RAG can be used to improve a model’s overall efficiency, together with fine-tuning, which serves as a method to enhance the performance of both the retrieval and generation components within the RAG framework. We will fine-tune a proportion of the retrieval data in <em class="chapterRef">Chapter 9</em>, <em class="italic">Empowering AI Models: Fine-Tuning RAG Data and Human Feedback</em>.</p>
    <p class="normal">We will now see how a RAG-driven generative AI involves an ecosystem with many components.</p>
    <h1 id="_idParaDest-19" class="heading-1">The RAG ecosystem</h1>
    <p class="normal">RAG-driven generative AI is a framework that can be implemented in many configurations. RAG’s framework runs within a broad ecosystem, as shown in <em class="italic">Figure 1.3</em>. However, no matter how many retrieval and generation frameworks you encounter, it all boils down to the following<a id="_idIndexMarker018"/> four domains and questions that go with them:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Data</strong>: Where is the data coming from? Is it reliable? Is it sufficient? Are there copyright, privacy, and security issues?</li>
      <li class="bulletList"><strong class="keyWord">Storage</strong>: How is the data going to be stored before or after processing it? What amount of data will be stored?</li>
      <li class="bulletList"><strong class="keyWord">Retrieval</strong>: How will the correct data be retrieved to augment the user’s input before it is sufficient for the generative model? What type of RAG framework will be successful for a project?</li>
      <li class="bulletList"><strong class="keyWord">Generation</strong>: Which generative AI model will fit into the type of RAG framework chosen?</li>
    </ul>
    <p class="normal">The data, storage, and generation domains depend heavily on the type of RAG framework you choose. Before making that choice, we need to evaluate the proportion of parametric and non-parametric knowledge in the ecosystem we are implementing. <em class="italic">Figure 1.3</em> represents the RAG framework, which includes the main components regardless of the types of RAG implemented:</p>
    <figure class="mediaobject"><img src="img/B31169_01_03.png" alt="A diagram of a process  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 1.3: The Generative RAG-ecosystem</p>
    <ul>
      <li class="bulletList">The <strong class="keyWord">Retriever (D)</strong> handles <a id="_idIndexMarker019"/>data collection, processing, storage, and retrieval</li>
      <li class="bulletList">The <strong class="keyWord">Generator (G)</strong> handles input augmentation, prompt engineering, and generation</li>
      <li class="bulletList">The <strong class="keyWord">Evaluator (E) </strong>handles mathematical metrics, human evaluation, and feedback</li>
      <li class="bulletList">The <strong class="keyWord">Trainer (T)</strong> handles the initial pre-trained model and fine-tuning the model</li>
    </ul>
    <p class="normal">Each of these four components <a id="_idIndexMarker020"/>relies on their respective ecosystems, which form the overall RAG-driven generative AI pipeline. We will refer to the domains D, G, E, and T in the following sections. Let’s begin with the retriever.</p>
    <h2 id="_idParaDest-20" class="heading-2">The retriever (D)</h2>
    <p class="normal">The retriever component of a RAG ecosystem <a id="_idIndexMarker021"/>collects, processes, stores, and retrieves data. The starting point of a RAG ecosystem is thus an ingestion data process, of which the first step is to collect data.</p>
    <h3 id="_idParaDest-21" class="heading-3">Collect (D1)</h3>
    <p class="normal">In today’s world, AI data is as diverse as our media playlists. It can be anything from a chunk of text in a blog post to a <a id="_idIndexMarker022"/>meme or even the latest hit song streamed through headphones. And it doesn’t stop there—the files themselves come in all shapes and sizes. Think of PDFs filled with all kinds of details, web pages, plain text files that get straight to the point, neatly organized JSON files, catchy MP3 tunes, videos in MP4 format, or images in PNG and JPG.</p>
    <p class="normal">Furthermore, a large proportion of this data is unstructured and found in unpredictable and complex ways. Fortunately, many platforms, such as Pinecone, OpenAI, Chroma, and Activeloop, provide ready-to-use tools to process and store this jungle of data.</p>
    <h3 id="_idParaDest-22" class="heading-3">Process (D2)</h3>
    <p class="normal">In the data collection phase (D1) of <a id="_idIndexMarker023"/>multimodal data processing, various types of data, such as text, images, and videos, can be extracted from websites using web scraping techniques or any other source of information. These data objects are then transformed to create uniform feature representations. For example, data can be chunked (broken into smaller parts), embedded (transformed into vectors), and indexed to enhance searchability and retrieval efficiency.</p>
    <p class="normal">We will introduce these techniques, starting with the <em class="italic">Building Hybrid Adaptive RAG in Python</em> section of this chapter. In the<a id="_idIndexMarker024"/> following chapters, we will continue building more complex data processing functions.</p>
    <h3 id="_idParaDest-23" class="heading-3">Storage (D3)</h3>
    <p class="normal">At this stage of the pipeline, we have collected<a id="_idIndexMarker025"/> and begun processing a large amount of diverse data from the internet—videos, pictures, texts, you name it. Now, what can we do with all that data to make it useful?</p>
    <p class="normal">That’s where vector stores like Deep Lake, Pinecone, and Chroma come into play. Think of these as super smart libraries that don’t just store your data but convert it into mathematical entities as vectors, enabling powerful computations. They can also apply a variety of indexing methods and other techniques for rapid access.</p>
    <p class="normal">Instead of keeping the data in static spreadsheets and files, we turn it into a dynamic, searchable system that can power anything from chatbots to search engines.</p>
    <h3 id="_idParaDest-24" class="heading-3">Retrieval query (D4)</h3>
    <p class="normal">The retrieval process is triggered by the user input or automated input (G1).</p>
    <p class="normal">To retrieve data quickly, we load it into<a id="_idIndexMarker026"/> vector stores and datasets after transforming it into a suitable format. Then, using a combination of keyword searches, smart embeddings, and indexing, we can retrieve the data efficiently. Cosine similarity, for example, finds items that are closely related, ensuring that the search results are not just fast but also highly relevant.</p>
    <p class="normal">Once the data is retrieved, we then augment the input.</p>
    <h2 id="_idParaDest-25" class="heading-2">The generator (G)</h2>
    <p class="normal">The lines are blurred in the RAG ecosystem between input and retrieval, as shown in <em class="italic">Figure 1.3</em>, representing the<a id="_idIndexMarker027"/> RAG framework and ecosystem. The user<a id="_idIndexMarker028"/> input (G1), automated or human, interacts with the retrieval query (D4) to augment the input before sending it to the generative model.</p>
    <p class="normal">The generative flow begins with an input.</p>
    <h3 id="_idParaDest-26" class="heading-3">Input (G1)</h3>
    <p class="normal">The input can be a batch of automated<a id="_idIndexMarker029"/> tasks (processing emails, for example) or human prompts through a <strong class="keyWord">User Interface</strong> (<strong class="keyWord">UI</strong>). This flexibility allows you to seamlessly integrate AI into various professional environments, enhancing productivity across industries.</p>
    <h3 id="_idParaDest-27" class="heading-3">Augmented input with HF (G2)</h3>
    <p class="normal"><strong class="keyWord">Human feedback</strong> (<strong class="keyWord">HF</strong>) can be added to the input,<a id="_idIndexMarker030"/> as described in the <em class="italic">Human feedback (E2) under Evaluator (E) </em>section. Human feedback will make a RAG ecosystem considerably adaptable and provide full control over data retrieval and generative AI inputs. In the <em class="italic">Building hybrid adaptive RAG in Python</em> section of this chapter, we will build augmented input with human feedback.</p>
    <h3 id="_idParaDest-28" class="heading-3">Prompt engineering (G3)</h3>
    <p class="normal">Both the retriever (D) and the<a id="_idIndexMarker031"/> generator (G) rely heavily on prompt engineering to prepare the standard and augmented message that the generative AI model will have to process. Prompt engineering brings the retriever’s output and the user input together.</p>
    <h3 id="_idParaDest-29" class="heading-3">Generation and output (G4)</h3>
    <p class="normal">The choice of a generative AI model depends on the goals of a project. Llama, Gemini, GPT, and other models can fit various<a id="_idIndexMarker032"/> requirements. However, the prompt must meet each model’s specifications. Frameworks such as LangChain, which we will implement in this book, help streamline the integration of various AI models into applications by providing adaptable interfaces and tools.</p>
    <h2 id="_idParaDest-30" class="heading-2">The evaluator (E)</h2>
    <p class="normal">We often rely on mathematical metrics to<a id="_idIndexMarker033"/> assess the performance of a generative AI model. However, these <a id="_idIndexMarker034"/>metrics only give us part of the picture. It’s important to remember that the ultimate test of an AI’s effectiveness comes down to human evaluation.</p>
    <h3 id="_idParaDest-31" class="heading-3">Metrics (E1)</h3>
    <p class="normal">A model cannot be evaluated <a id="_idIndexMarker035"/>without mathematical metrics, such as cosine similarity, as with any AI system. These metrics ensure that the retrieved data is relevant and accurate. By quantifying the relationships and relevance of data points, they provide a solid foundation for assessing the model’s performance and reliability.</p>
    <h3 id="_idParaDest-32" class="heading-3">Human feedback (E2)</h3>
    <p class="normal">No generative AI system, whether RAG-driven <a id="_idIndexMarker036"/>or not, and whether the mathematical metrics seem sufficient or not, can elude human evaluation. It is ultimately human evaluation that decides if a system designed for human users will be accepted or rejected, praised or criticized.</p>
    <p class="normal">Adaptive RAG introduces the human, real-life, pragmatic feedback factor that will improve a RAG-driven generative AI ecosystem. We will implement adaptive RAG in <em class="chapterRef">Chapter 5</em>, <em class="italic">Boosting RAG Performance with Expert Human Feedback</em>.</p>
    <h2 id="_idParaDest-33" class="heading-2">The trainer (T)</h2>
    <p class="normal">A standard generative AI <a id="_idIndexMarker037"/>model is pre-trained with a vast amount of general-purpose data. Then, we<a id="_idIndexMarker038"/> can fine-tune (T2) the model with domain-specific data.</p>
    <p class="normal">We will take this further by integrating static RAG data into the fine-tuning process in <em class="chapterRef">Chapter 9</em>, <em class="italic">Empowering AI Models: Fine-Tuning RAG Data and Human Feedback</em>. We will also integrate human feedback, which provides valuable information that can be integrated into the fine-tuning process in a variant of <strong class="keyWord">Reinforcement Learning from Human Feedback</strong> (<strong class="keyWord">RLHF</strong>).</p>
    <p class="normal">We are now ready to code entry-level naïve, advanced, and modular RAG in Python.</p>
    <h1 id="_idParaDest-34" class="heading-1">Naïve, advanced, and modular RAG in code</h1>
    <p class="normal">This section introduces naïve, advanced, and modular RAG through basic educational examples. The program builds<a id="_idIndexMarker039"/> keyword matching, vector search, and index-based retrieval <a id="_idIndexMarker040"/>methods. Using <a id="_idIndexMarker041"/>OpenAI’s GPT models, it generates responses based on input queries and retrieved documents.</p>
    <p class="normal">The goal of the notebook is for a conversational agent to answer questions on RAG in general. We will build the retriever from the bottom up, from scratch, in Python and run the generator with OpenAI GPT-4o in eight sections of code divided into two parts:</p>
    <p class="normal"><strong class="keyWord">Part 1: Foundations and Basic Implementation</strong></p>
    <ol>
      <li class="numberedList" value="1"><strong class="keyWord">Environment</strong> setup for OpenAI API integration</li>
      <li class="numberedList"><strong class="keyWord">Generator</strong> function using GPT-4o</li>
      <li class="numberedList"><strong class="keyWord">Data</strong> setup with a list of documents (<code class="inlineCode">db_records</code>)</li>
      <li class="numberedList"><strong class="keyWord">Query </strong>for user input</li>
    </ol>
    <p class="normal"><strong class="keyWord">Part 2: Advanced Techniques and Evaluation</strong></p>
    <ol>
      <li class="numberedList" value="1"><strong class="keyWord">Retrieval metrics</strong> to measure retrieval responses</li>
      <li class="numberedList"><strong class="keyWord">Naïve RAG</strong> with a keyword search and matching function</li>
      <li class="numberedList"><strong class="keyWord">Advanced RAG</strong> with vector search and index-based search</li>
      <li class="numberedList"><strong class="keyWord">Modular RAG</strong> implementing flexible retrieval methods</li>
    </ol>
    <p class="normal">To get started, open <code class="inlineCode">RAG_Overview.ipynb</code> in the GitHub repository. We will begin by establishing the foundations of the notebook and exploring the basic implementation.</p>
    <h2 id="_idParaDest-35" class="heading-2">Part 1: Foundations and basic implementation</h2>
    <p class="normal">In this section, we will set up the environment, create a function for the generator, define a function to print a formatted response, and define the user query.</p>
    <p class="normal">The first step is to install the environment.</p>
    <div><p class="normal">The section titles of the following implementation of the notebook follow the structure in the code. Thus, you can follow the code in the notebook or read this self-contained section.</p>
    </div>
    <h3 id="_idParaDest-36" class="heading-3">1. Environment</h3>
    <p class="normal">The main package to install is<a id="_idIndexMarker042"/> OpenAI to access GPT-4o through an API:</p>
    <pre class="programlisting code"><code class="hljs-code">!pip install openai==1.40.3
</code></pre>
    <p class="normal">Make sure to freeze the OpenAI version you install. In RAG framework ecosystems, we will have to install several packages to run advanced RAG configurations. Once we have stabilized an installation, we will freeze the version of the packages installed to minimize potential conflicts between the libraries and modules we implement.</p>
    <p class="normal">Once you have installed <code class="inlineCode">openai</code>, you will have to create an account on OpenAI (if you don’t have one) and obtain an API key. Make sure to check the costs and payment plans before running the API.</p>
    <p class="normal">Once you have a key, store it in <a id="_idIndexMarker043"/>a safe place and retrieve it as follows from Google Drive, for example, as shown in the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">#API Key
#Store you key in a file and read it(you can type it directly in the notebook but it will be visible for somebody next to you)
from google.colab import drive
drive.mount('/content/drive')
</code></pre>
    <p class="normal">You can use Google Drive or any other method you choose to store your key. You can read the key from a file, or you can also choose to enter the key directly in the code:</p>
    <pre class="programlisting code"><code class="hljs-code">f = open("drive/MyDrive/files/api_key.txt", "r")
API_KEY=f.readline().strip()
f.close()
 
#The OpenAI Key
import os
import openai
os.environ['OPENAI_API_KEY'] =API_KEY
openai.api_key = os.getenv("OPENAI_API_KEY")
</code></pre>
    <p class="normal">With that, we have set up the main resources for our project. We will now write a generation function for the OpenAI model.</p>
    <h3 id="_idParaDest-37" class="heading-3">2. The generator</h3>
    <p class="normal">The code imports <code class="inlineCode">openai</code> to generate <a id="_idIndexMarker044"/>content and <code class="inlineCode">time</code> to measure the time the requests take:</p>
    <pre class="programlisting code"><code class="hljs-code">import openai
from openai import OpenAI
import time
client = OpenAI()
gptmodel="gpt-4o"
start_time = time.time()  # Start timing before the request
</code></pre>
    <p class="normal">We now create a function that creates a prompt with an instruction and the user input:</p>
    <pre class="programlisting code"><code class="hljs-code">def call_llm_with_full_text(itext):
    # Join all lines to form a single string
    text_input = '\n'.join(itext)
    prompt = f"Please elaborate on the following content:\n{text_input}"
</code></pre>
    <p class="normal">The function will try to<a id="_idIndexMarker045"/> call <code class="inlineCode">gpt-4o</code>, adding additional information for the model:</p>
    <pre class="programlisting code"><code class="hljs-code">    try:
      response = client.chat.completions.create(
         model=gptmodel,
         messages=[
            {"role": "system", "content": "You are an expert Natural Language Processing exercise expert."},
            {"role": "assistant", "content": "1.You can explain read the input and answer in detail"},
            {"role": "user", "content": prompt}
         ],
         temperature=0.1  # Add the temperature parameter here and other parameters you need
        )
      return response.choices[0].message.content.strip()
    except Exception as e:
        return str(e)
</code></pre>
    <p class="normal">Note that the instruction messages remain general in this scenario so that the model remains flexible. The <code class="inlineCode">temperature</code> is low (more precise) and set to <code class="inlineCode">0.1</code>. If you wish for the system to be more creative, you can set <code class="inlineCode">temperature</code> to a higher value, such as <code class="inlineCode">0.7</code>. However, in this case, it is recommended to ask for precise responses.</p>
    <p class="normal">We can add <code class="inlineCode">textwrap</code> to format the response as a nice paragraph when we call the generative AI model:</p>
    <pre class="programlisting code"><code class="hljs-code">import textwrap
def print_formatted_response(response):
    # Define the width for wrapping the text
    wrapper = textwrap.TextWrapper(width=80)  # Set to 80 columns wide, but adjust as needed
    wrapped_text = wrapper.fill(text=response)
    # Print the formatted response with a header and footer
    print("Response:")
    print("---------------")
    print(wrapped_text)
    print("---------------\n")
</code></pre>
    <div><p class="normal">The generator is now ready to be called when we need it. Due to the probabilistic nature of generative AI models, it might produce different outputs each time we call it.</p>
    </div>
    <p class="normal">The program now<a id="_idIndexMarker046"/> implements the data retrieval functionality.</p>
    <h3 id="_idParaDest-38" class="heading-3">3. The Data</h3>
    <p class="normal">Data collection includes text, images, audio, and video. In this notebook, we will focus on <strong class="keyWord">data retrieval</strong> through naïve, advanced, and modular configurations, not data collection. We will collect and <a id="_idIndexMarker047"/>embed data later in <em class="chapterRef">Chapter 2</em>, <em class="italic">RAG Embedding Vector Stores with Deep Lake and OpenAI</em>. As such, we will assume that the data we need has been processed and thus collected, cleaned, and split into sentences. We will also assume that the process included loading the sentences into a Python list named <code class="inlineCode">db_records</code>.</p>
    <p class="normal">This approach illustrates three aspects of the RAG ecosystem we described in <em class="italic">The RAG ecosystem</em> section and the components of the system described in <em class="italic">Figure 1.3</em>:</p>
    <ul>
      <li class="bulletList">The <strong class="keyWord">retriever (D)</strong> has three <strong class="keyWord">data processing</strong> components, <strong class="keyWord">collect (D1)</strong>, <strong class="keyWord">process (D2)</strong>, and <strong class="keyWord">storage (D3)</strong>, which are preparatory phases of the retriever.</li>
      <li class="bulletList">The<strong class="keyWord"> retriever query (D4)</strong> is thus independent of the first three phases (collect, process, and storage) of the retriever.</li>
      <li class="bulletList">The data processing phase will often be done independently and prior to activating the retriever query, as we will implement starting in <em class="italic">Chapter 2</em>.</li>
    </ul>
    <p class="normal">This program assumes that data processing has been completed and the dataset is ready:</p>
    <pre class="programlisting code"><code class="hljs-code">db_records = [
    "Retrieval Augmented Generation (RAG) represents a sophisticated hybrid approach in the field of artificial intelligence, particularly within the realm of natural language processing (NLP).",
…/…
</code></pre>
    <p class="normal">We can display a<a id="_idIndexMarker048"/> formatted version of the dataset:</p>
    <pre class="programlisting code"><code class="hljs-code">import textwrap
paragraph = ' '.join(db_records)
wrapped_text = textwrap.fill(paragraph, width=80)
print(wrapped_text)
</code></pre>
    <p class="normal">The output joins the sentences in <code class="inlineCode">db_records</code> for display, as printed in this excerpt, but <code class="inlineCode">db_records</code> remains unchanged:</p>
    <pre class="programlisting con"><code class="hljs-con">Retrieval Augmented Generation (RAG) represents a sophisticated hybrid approach in the field of artificial intelligence, particularly within the realm of natural language processing (NLP)…
</code></pre>
    <p class="normal">The program is now ready to process a query.</p>
    <h3 id="_idParaDest-39" class="heading-3">4.The query</h3>
    <p class="normal">The <strong class="keyWord">retriever</strong> (<strong class="keyWord">D4</strong> in <em class="italic">Figure 1.3</em>) query process depends on how the data was processed, but the query itself is simply user<a id="_idIndexMarker049"/> input or automated input from another AI agent. We all dream of users who introduce the best input into software systems, but unfortunately, in real life, unexpected inputs lead to unpredictable behaviors. We must, therefore, build systems that take imprecise inputs into account.</p>
    <p class="normal">In this section, we will imagine a situation in which hundreds of users in an organization have heard the word “RAG” associated with “LLM” and “vector stores.” Many of them would like to understand what these terms mean to keep up with a software team that’s deploying a conversational agent in their department. After a couple of days, the terms they heard become fuzzy in their memory, so they ask the conversational agent, GPT-4o in this case, to explain what they remember with the following query:</p>
    <pre class="programlisting code"><code class="hljs-code">query = "define a rag store"
</code></pre>
    <p class="normal">In this case, we will simply store the main query of the topic of this program in <code class="inlineCode">query</code>, which represents the junction between the retriever and the generator. It will trigger a configuration of RAG (naïve, advanced, and modular). The choice of configuration will depend on the goals of each project.</p>
    <p class="normal">The program takes the query and sends it to a GPT-4o model to be processed and then displays the formatted output:</p>
    <pre class="programlisting code"><code class="hljs-code"># Call the function and print the result
llm_response = call_llm_with_full_text(query)
print_formatted_response(llm_response)
</code></pre>
    <p class="normal">The output is revealing. Even the most powerful generative AI models cannot guess what a user, who knows <a id="_idIndexMarker050"/>nothing about AI, is trying to find out in good faith. In this case, GPT-4o will answer as shown in this excerpt of the output:</p>
    <pre class="programlisting con"><code class="hljs-con">Response:
---------------
Certainly! The content you've provided appears to be a sequence of characters
that, when combined, form the phrase "define a rag store." Let's break it down
step by step:…
… This is an indefinite article used before words that begin with a consonant sound.    - **rag**: This is a noun that typically refers to a pieceof old, often torn, cloth.    - **store**: This is a noun that refers to a place where goods are sold.  4. **Contextual Meaning**:    - **"Define a rag store"**: This phrase is asking for an explanation or definition of what a "rag store" is. 5. **Possible Definition**:    - A "rag store" could be a shop or retail establishment that specializes in selling rags,…
</code></pre>
    <p class="normal">The output will seem like a hallucination, but is it really? The user wrote the query with the good intentions of every beginner trying to learn a new topic. GPT-4o, in good faith, did what it could with the limited context it had with its probabilistic algorithm, which might even produce a different response each time we run it. However, GPT-4o is being wary of the query. It wasn’t very clear, so it ends the response with the following output that asks the user for more context:</p>
    <pre class="programlisting con"><code class="hljs-con">…Would you like more information or a different type of elaboration on this content?…
</code></pre>
    <p class="normal">The user is puzzled, not knowing what to do, and GPT-4o is awaiting further instructions. The software team has to do something!</p>
    <div><p class="normal">Generative AI is based on probabilistic algorithms. As such, the response provided might vary from one run to another, providing similar (but not identical) responses.</p>
    </div>
    <p class="normal">That is when RAG<a id="_idIndexMarker051"/> comes in to save the situation. We will leave this query as it is for the whole notebook and see if a RAG-driven GPT-4o system can do better.</p>
    <h2 id="_idParaDest-40" class="heading-2">Part 2: Advanced techniques and evaluation</h2>
    <p class="normal">In <em class="italic">Part 2</em>, we will introduce naïve, advanced, and modular RAG. The goal is to introduce the three methods, not to process complex documents, which we will implement throughout the following chapters of this book.</p>
    <p class="normal">Let’s first begin by defining retrieval metrics to measure the accuracy of the documents we retrieve.</p>
    <h3 id="_idParaDest-41" class="heading-3">1. Retrieval metrics</h3>
    <p class="normal">This section explores retrieval metrics, first focusing on the role of cosine similarity in assessing the relevance of text<a id="_idIndexMarker052"/> documents. Then we will implement enhanced similarity metrics by incorporating synonym expansion and text preprocessing to improve the accuracy of similarity calculations between texts.</p>
    <p class="normal">We will explore more metrics in the <em class="italic">Metrics calculation and display</em> section in <em class="chapterRef">Chapter 7</em>, <em class="italic">Building Scalable Knowledge-Graph-Based RAG with Wikipedia API and LlamaIndex</em>.</p>
    <p class="normal">In this chapter, let’s begin with cosine similarity.</p>
    <h4 class="heading-4">Cosine similarity</h4>
    <p class="normal">Cosine similarity measures the cosine of the angle between two vectors. In our case, the two vectors are the user query and<a id="_idIndexMarker053"/> each document in a corpus.</p>
    <p class="normal">The program first imports the class and function we need:</p>
    <pre class="programlisting code"><code class="hljs-code">from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
</code></pre>
    <p class="normal"><code class="inlineCode">TfidfVectorizer</code> imports the class that converts text documents into a matrix of TF-IDF features. <strong class="keyWord">Term Frequency-Inverse Document Frequency</strong> (<strong class="keyWord">TF-IDF</strong>) quantifies the relevance of a word to a document in a<a id="_idIndexMarker054"/> collection, distinguishing common words from those significant to specific texts. TF-IDF will thus quantify word relevance in documents using frequency across the document and inverse frequency across the corpus. <code class="inlineCode">cosine_similarity</code> imports the function we will use to calculate the similarity between vectors.</p>
    <p class="normal"><code class="inlineCode">calculate_cosine_similarity(text1, text2)</code> then calculates the cosine similarity between the query (<code class="inlineCode">text1</code>) and each record of the dataset.</p>
    <p class="normal">The function converts the <a id="_idIndexMarker055"/>query text (<code class="inlineCode">text1</code>) and each record (<code class="inlineCode">text2</code>) in the dataset into a vector with a vectorizer. Then, it calculates and returns the cosine similarity between the two vectors:</p>
    <pre class="programlisting code"><code class="hljs-code">def calculate_cosine_similarity(text1, text2):
    vectorizer = TfidfVectorizer(
        stop_words='english',
        use_idf=True,
        norm='l2',
        ngram_range=(1, 2),  # Use unigrams and bigrams
        sublinear_tf=True,   # Apply sublinear TF scaling
        analyzer='word'      # You could also experiment with 'char' or 'char_wb' for character-level features
    )
    tfidf = vectorizer.fit_transform([text1, text2])
    similarity = cosine_similarity(tfidf[0:1], tfidf[1:2])
    return similarity[0][0]
</code></pre>
    <p class="normal">The key parameters of this function are:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">stop_words='english</code>: Ignores common English words to focus on meaningful content</li>
      <li class="bulletList"><code class="inlineCode">use_idf=True</code>: Enables inverse document frequency weighting</li>
      <li class="bulletList"><code class="inlineCode">norm='l2'</code>: Applies L2 normalization to each output vector</li>
      <li class="bulletList"><code class="inlineCode">ngram_range=(1, 2)</code>: Considers both single words and two-word combinations</li>
      <li class="bulletList"><code class="inlineCode">sublinear_tf=True</code>: Applies logarithmic term frequency scaling</li>
      <li class="bulletList"><code class="inlineCode">analyzer='word'</code>: Analyzes text at the word level</li>
    </ul>
    <p class="normal">Cosine similarity can be limited in some cases. Cosine similarity has limitations when dealing with ambiguous queries because it strictly measures the similarity based on the angle between vector representations of text. If a user asks a vague question like “What is rag?” in the program of this chapter and the database primarily contains information on “RAG” as in “retrieval-augmented generation” for AI, not “rag cloths,” the cosine similarity score might be low. This low score occurs because the mathematical model lacks contextual understanding to differentiate between the different meanings of “rag.” It only computes similarity based on the presence and frequency of similar words in the text, without grasping the user’s intent or the broader context of the query. Thus, even if the answers<a id="_idIndexMarker056"/> provided are technically accurate within the available dataset, the cosine similarity may not reflect the relevance accurately if the query’s context isn’t well-represented in the data.</p>
    <p class="normal">In this case, we can try enhanced similarity.</p>
    <h4 class="heading-4">Enhanced similarity</h4>
    <p class="normal">Enhanced similarity introduces calculations that leverage natural language processing tools to better capture semantic relationships between words. Using libraries like spaCy and NLTK, it preprocesses texts to<a id="_idIndexMarker057"/> reduce noise, expands terms with synonyms from WordNet, and computes similarity based on the semantic richness of the expanded vocabulary. This method aims to improve the accuracy of similarity assessments between two texts by considering a broader context than typical direct comparison methods.</p>
    <p class="normal">The code contains four main functions:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">get_synonyms(word)</code>: Retrieves synonyms for a given word from WordNet</li>
      <li class="bulletList"><code class="inlineCode">preprocess_text(text)</code>: Converts all text to lowercase, lemmatizes gets the (roots of words), and filters stopwords (common words) and punctuation from text</li>
      <li class="bulletList"><code class="inlineCode">expand_with_synonyms(words)</code>: Enhances a list of words by adding their synonyms</li>
      <li class="bulletList"><code class="inlineCode">calculate_enhanced_similarity(text1, text2)</code>: Computes cosine similarity between preprocessed and synonym-expanded text vectors</li>
    </ul>
    <p class="normal">The <code class="inlineCode">calculate_enhanced_similarity(text1, text2)</code> function takes two texts and ultimately returns the cosine similarity score between two processed and synonym-expanded texts. This score quantifies the textual similarity based on their semantic content and enhanced word sets.</p>
    <p class="normal">The code begins by downloading and importing the necessary libraries and then runs the four functions beginning with <code class="inlineCode">calculate_enhanced_similarity(text1, text2)</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">import spacy
import nltk
nltk.download('wordnet')
from nltk.corpus import wordnet
from collections import Counter
import numpy as np
# Load spaCy model
nlp = spacy.load("en_core_web_sm")
…
</code></pre>
    <p class="normal">Enhanced similarity takes <a id="_idIndexMarker058"/>this a bit further in terms of metrics. However, integrating RAG with generative AI presents multiple challenges.</p>
    <p class="normal">No matter which metric we implement, we will face the following limitations:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Input versus Document Length</strong>: User queries are often short, while retrieved documents are longer and richer, complicating direct similarity evaluations.</li>
      <li class="bulletList"><strong class="keyWord">Creative Retrieval</strong>: Systems may creatively select longer documents that meet user expectations but yield poor metric scores due to unexpected content alignment.</li>
      <li class="bulletList"><strong class="keyWord">Need for Human Feedback</strong>: Often, human judgment is crucial to accurately assess the relevance and effectiveness of retrieved content, as automated metrics may not fully capture user satisfaction. We will explore this critical aspect of RAG in <em class="chapterRef">Chapter 5</em>, <em class="italic">Boosting RAG Performance with Expert Human Feedback</em>.</li>
    </ul>
    <p class="normal">We will always have to find the right balance between mathematical metrics and human feedback.</p>
    <p class="normal">We are now ready to create an example with naïve RAG.</p>
    <h3 id="_idParaDest-42" class="heading-3">2. Naïve RAG</h3>
    <p class="normal">Naïve RAG with keyword search <a id="_idIndexMarker059"/>and matching can prove efficient with well-defined documents within an organization, such as legal and medical documents. These documents generally have clear titles or labels for images, for example. In this naïve RAG function, we will implement keyword search and matching. To achieve this, we will apply a straightforward retrieval method in the code:</p>
    <ol>
      <li class="numberedList" value="1">Split the query into individual keywords</li>
      <li class="numberedList">Split each record in the dataset into keywords</li>
      <li class="numberedList">Determine the length of the common matches</li>
      <li class="numberedList">Choose the record with the best score</li>
    </ol>
    <p class="normal">The generation method will:</p>
    <ul>
      <li class="bulletList">Augment the <a id="_idIndexMarker060"/>user input with the result of the retrieval query</li>
      <li class="bulletList">Request the generation model, which is <code class="inlineCode">gpt-4o</code> in this case</li>
      <li class="bulletList">Display the response</li>
    </ul>
    <p class="normal">Let’s write the keyword search and matching function.</p>
    <h4 class="heading-4">Keyword search and matching</h4>
    <p class="normal">The best matching function<a id="_idIndexMarker061"/> first initializes the best scores:</p>
    <pre class="programlisting code"><code class="hljs-code">def find_best_match_keyword_search(query, db_records):
    best_score = 0
    best_record = None
</code></pre>
    <p class="normal">The query is then split into keywords. Each record is also split into words to find the common words, measure the length of common content, and find the best match:</p>
    <pre class="programlisting code"><code class="hljs-code"># Split the query into individual keywords
    query_keywords = set(query.lower().split())
    # Iterate through each record in db_records
    for record in db_records:
        # Split the record into keywords
        record_keywords = set(record.lower().split())
        # Calculate the number of common keywords
        common_keywords = query_keywords.intersection(record_keywords)
        current_score = len(common_keywords)
        # Update the best score and record if the current score is higher
        if current_score &gt; best_score:
            best_score = current_score
            best_record = record
    return best_score, best_record
</code></pre>
    <p class="normal">We now call the function, format the response, and print it:</p>
    <pre class="programlisting code"><code class="hljs-code"># Assuming 'query' and 'db_records' are defined in previous cells in your Colab notebook
best_keyword_score, best_matching_record = find_best_match_keyword_search(query, db_records)
print(f"Best Keyword Score: {best_keyword_score}")
#print(f"Best Matching Record: {best_matching_record}")
print_formatted_response(best_matching_record)
</code></pre>
    <p class="normal">The main query of this notebook will be <code class="inlineCode">query = "define a rag store"</code> to see if each RAG method produces an<a id="_idIndexMarker062"/> acceptable output.</p>
    <p class="normal">The keyword search finds the best record in the list of sentences in the dataset:</p>
    <pre class="programlisting con"><code class="hljs-con">Best Keyword Score: 3
Response:
---------------
A RAG vector store is a database or dataset that contains vectorized data points.
---------------
</code></pre>
    <p class="normal">Let’s run the metrics.</p>
    <h4 class="heading-4">Metrics</h4>
    <p class="normal">We created the <a id="_idIndexMarker063"/>similarity metrics in the <em class="italic">1. Retrieval metrics</em> section of this chapter. We will first apply cosine similarity:</p>
    <pre class="programlisting code"><code class="hljs-code"># Cosine Similarity
score = calculate_cosine_similarity(query, best_matching_record)
print(f"Best Cosine Similarity Score: {score:.3f}")
</code></pre>
    <p class="normal">The output similarity is low, as explained in the <em class="italic">1. Retrieval metrics</em> section of this chapter. The user input is short and the response is longer and complete:</p>
    <pre class="programlisting con"><code class="hljs-con">Best Cosine Similarity Score: 0.126
</code></pre>
    <p class="normal">Enhanced similarity will produce a better score:</p>
    <pre class="programlisting code"><code class="hljs-code"># Enhanced Similarity
response = best_matching_record
print(query,": ", response)
similarity_score = calculate_enhanced_similarity(query, response)
print(f"Enhanced Similarity:, {similarity_score:.3f}")
</code></pre>
    <p class="normal">The score produced is higher with enhanced functionality:</p>
    <pre class="programlisting con"><code class="hljs-con">define a rag store :  A RAG vector store is a database or dataset that contains vectorized data points.
Enhanced Similarity:, 0.642
</code></pre>
    <p class="normal">The output of the query <a id="_idIndexMarker064"/>will now augment the user input.</p>
    <h4 class="heading-4">Augmented input</h4>
    <p class="normal">The augmented input is the concatenation<a id="_idIndexMarker065"/> of the user input and the best matching record of the dataset detected with the keyword search:</p>
    <pre class="programlisting code"><code class="hljs-code">augmented_input=query+ ": "+ best_matching_record
</code></pre>
    <p class="normal">The augmented input is displayed if necessary for maintenance reasons:</p>
    <pre class="programlisting code"><code class="hljs-code">print_formatted_response(augmented_input)
</code></pre>
    <p class="normal">The output then shows that the augmented input is ready:</p>
    <pre class="programlisting con"><code class="hljs-con">Response:
---------------
define a rag store: A RAG vector store is a database or dataset that contains
vectorized data points.
---------------
</code></pre>
    <p class="normal">The input is now ready for the generation process.</p>
    <h4 class="heading-4">Generation</h4>
    <p class="normal">We are now ready to call GPT-4o and<a id="_idIndexMarker066"/> display the formatted response:</p>
    <pre class="programlisting code"><code class="hljs-code">llm_response = call_llm_with_full_text(augmented_input)
print_formatted_response(llm_response)
</code></pre>
    <p class="normal">The following excerpt of the response shows that GPT-4o understands the input and provides an interesting, pertinent response:</p>
    <pre class="programlisting con"><code class="hljs-con">Response:
---------------
Certainly! Let's break down and elaborate on the provided content:  ### Define a
RAG Store:  A **RAG (Retrieval-Augmented Generation) vector store** is a
specialized type of database or dataset that is designed to store and manage
vectorized data points…
</code></pre>
    <p class="normal">Naïve RAG can be sufficient in many situations. However, if the volume of documents becomes too large or the content becomes more complex, then advanced RAG configurations will <a id="_idIndexMarker067"/>provide better results. Let’s now explore advanced RAG.</p>
    <h3 id="_idParaDest-43" class="heading-3">3. Advanced RAG</h3>
    <p class="normal">As datasets grow larger, keyword search methods might prove too long to run. For instance, if we have hundreds of documents and each document contains hundreds of sentences, it will become challenging to use keyword search only. Using an index will reduce the computational load to just a fraction of the total data.</p>
    <p class="normal">In this section, we will go beyond searching<a id="_idIndexMarker068"/> text with keywords. We will see how RAG transforms text data into numerical representations, enhancing search efficiency and processing speed. Unlike traditional methods that directly parse text, RAG first converts documents and user queries into vectors, numerical forms that speed up calculations. In simple terms, a vector is a list of numbers representing various features of text. Simple vectors might count word occurrences (term frequency), while more complex vectors, known as embeddings, capture deeper linguistic patterns.</p>
    <p class="normal">In this section, we will implement vector search and index-based search:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Vector Search</strong>: We will convert each sentence in our dataset into a numerical vector. By <a id="_idIndexMarker069"/>calculating the cosine similarity between the query vector (the user query) and these document vectors, we can quickly find the most relevant documents.</li>
      <li class="bulletList"><strong class="keyWord">Index-Based Search</strong>: In this <a id="_idIndexMarker070"/>case, all sentences are converted into vectors using <strong class="keyWord">TF-IDF</strong> (<strong class="keyWord">Term Frequency-Inverse Document Frequency</strong>), a statistical measure used to evaluate how important a word is to a document in a collection. These vectors act as indices in a matrix, allowing quick similarity comparisons without parsing each document fully.</li>
    </ul>
    <p class="normal">Let’s start with vector search and see these concepts in action.</p>
    <h4 class="heading-4">3.1.Vector search</h4>
    <p class="normal">Vector search converts the user<a id="_idIndexMarker071"/> query and the documents into numerical values as vectors, enabling mathematical calculations that <em class="italic">retrieve relevant data faster when dealing with large volumes of data</em>.</p>
    <p class="normal">The program runs through each<a id="_idIndexMarker072"/> record of the dataset to find the best matching document by computing the cosine similarity of the query vector and each record in the dataset:</p>
    <pre class="programlisting code"><code class="hljs-code">def find_best_match(text_input, records):
    best_score = 0
    best_record = None
    for record in records:
        current_score = calculate_cosine_similarity(text_input, record)
        if current_score &gt; best_score:
            best_score = current_score
            best_record = record
    return best_score, best_record
</code></pre>
    <p class="normal">The code then calls the vector search function and displays the best record found:</p>
    <pre class="programlisting code"><code class="hljs-code">best_similarity_score, best_matching_record = find_best_match(query, db_records)
print_formatted_response(best_matching_record)
</code></pre>
    <p class="normal">The output is satisfactory:</p>
    <pre class="programlisting con"><code class="hljs-con">Response:
---------------
A RAG vector store is a database or dataset that contains vectorized data
points.
</code></pre>
    <p class="normal">The response is the best one found, like with naïve RAG. This shows that there is no silver bullet. Each RAG technique has its merits. The metrics will confirm this observation.</p>
    <h5 class="heading-5">Metrics</h5>
    <p class="normal">The metrics are the <a id="_idIndexMarker073"/>same for both similarity methods as for naïve RAG because the same document was retrieved:</p>
    <pre class="programlisting code"><code class="hljs-code">print(f"Best Cosine Similarity Score: {best_similarity_score:.3f}")
</code></pre>
    <p class="normal">The output is:</p>
    <pre class="programlisting con"><code class="hljs-con">Best Cosine Similarity Score: 0.126
</code></pre>
    <p class="normal">And with enhanced similarity, we obtain the same output as for naïve RAG:</p>
    <pre class="programlisting code"><code class="hljs-code"># Enhanced Similarity
response = best_matching_record
print(query,": ", response)
similarity_score = calculate_enhanced_similarity(query, best_matching_record)
print(f"Enhanced Similarity:, {similarity_score:.3f}")
</code></pre>
    <p class="normal">The output <a id="_idIndexMarker074"/>confirms the trend:</p>
    <pre class="programlisting con"><code class="hljs-con">define a rag store :  A RAG vector store is a database or dataset that contains vectorized data points.
Enhanced Similarity:, 0.642
</code></pre>
    <p class="normal">So why use vector search if it produces the same outputs as naïve RAG? Well, in a small dataset, everything looks easy. But when we’re dealing with datasets of millions of complex documents, keyword search will not capture subtleties that vectors can. Let’s now augment the user query with this information retrieved.</p>
    <h5 class="heading-5">Augmented input</h5>
    <p class="normal">We add the information retrieved to<a id="_idIndexMarker075"/> the user query with no other aid and display the result:</p>
    <pre class="programlisting code"><code class="hljs-code"># Call the function and print the result
augmented_input=query+": "+best_matching_record
print_formatted_response(augmented_input)
</code></pre>
    <p class="normal">We only added a space between the user query and the retrieved information; nothing else. The output is satisfactory:</p>
    <pre class="programlisting con"><code class="hljs-con">Response:
---------------
define a rag store: A RAG vector store is a database or dataset that contains
vectorized data points.
---------------
</code></pre>
    <p class="normal">Let’s now see how the generative AI model reacts to this augmented input.</p>
    <h5 class="heading-5">Generation</h5>
    <p class="normal">We now call GPT-4o with the<a id="_idIndexMarker076"/> augmented input and display the formatted output:</p>
    <pre class="programlisting code"><code class="hljs-code"># Call the function and print the result
augmented_input=query+best_matching_record
llm_response = call_llm_with_full_text(augmented_input)
print_formatted_response(llm_response)
</code></pre>
    <p class="normal">The response makes sense, as shown in the following excerpt:</p>
    <pre class="programlisting con"><code class="hljs-con">Response:
---------------
Certainly! Let's break down and elaborate on the provided content:  ### Define a RAG Store:  A **RAG (Retrieval-Augmented Generation) vector store** is a specialized type of database or dataset that is designed to store and manage vectorized data points…
</code></pre>
    <p class="normal">While vector search significantly speeds up the process of finding relevant documents by sequentially going through each record, its efficiency can decrease as the dataset size increases. To address this scalability issue, indexed search offers a more advanced solution. Let’s now see how index-based search can accelerate document retrieval.</p>
    <h4 class="heading-4">3.2. Index-based search</h4>
    <p class="normal">Index-based search compares the vector of a user query not with the direct vector of a document’s content but with an<a id="_idIndexMarker077"/> indexed vector that represents this content.</p>
    <p class="normal">The program first imports the class <a id="_idIndexMarker078"/>and function we need:</p>
    <pre class="programlisting code"><code class="hljs-code">from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
</code></pre>
    <p class="normal"><code class="inlineCode">TfidfVectorizer</code> imports the class that converts text documents into a matrix of TF-IDF features. TF-IDF will quantify word relevance in documents using frequency across the document. The function finds the best matches using the cosine similarity function to calculate the similarity between the query and the weighted vectors of the matrix:</p>
    <pre class="programlisting code"><code class="hljs-code">def find_best_match(query, vectorizer, tfidf_matrix):
    query_tfidf = vectorizer.transform([query])
    similarities = cosine_similarity(query_tfidf, tfidf_matrix)
    best_index = similarities.argmax()  # Get the index of the highest similarity score
    best_score = similarities[0, best_index]
    return best_score, best_index
</code></pre>
    <p class="normal">The function’s main tasks are:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Transform Query</strong>: Converts the input query into TF-IDF vector format using the provided vectorizer</li>
      <li class="bulletList"><strong class="keyWord">Calculate Similarities</strong>: Computes the cosine similarity between the query vector and all vectors in the tfidf_matrix</li>
      <li class="bulletList"><strong class="keyWord">Identify Best Match</strong>: Finds the index (<code class="inlineCode">best_index</code>) of the highest similarity score in the results</li>
      <li class="bulletList"><strong class="keyWord">Retrieve Best Score</strong>: Extracts the highest cosine similarity score (<code class="inlineCode">best_score</code>)</li>
    </ul>
    <p class="normal">The output is the best<a id="_idIndexMarker079"/> similarity score found and the best index.</p>
    <p class="normal">The following code first calls the dataset vectorizer and then searches for the most similar record through its index:</p>
    <pre class="programlisting code"><code class="hljs-code">vectorizer, tfidf_matrix = setup_vectorizer(db_records)
best_similarity_score, best_index = find_best_match(query, vectorizer, tfidf_matrix)
best_matching_record = db_records[best_index]
</code></pre>
    <p class="normal">Finally, the results are displayed:</p>
    <pre class="programlisting code"><code class="hljs-code">print_formatted_response(best_matching_record)
</code></pre>
    <p class="normal">The system finds the best similar document to the user’s input query:</p>
    <pre class="programlisting con"><code class="hljs-con">Response:
---------------
A RAG vector store is a database or dataset that contains vectorized data
points.
---------------
</code></pre>
    <p class="normal">We can see that the fuzzy user query produced a reliable output at the retrieval level before running GPT-4o.</p>
    <p class="normal">The metrics that follow in the program are the same as for naïve and advanced RAG with vector search. This is normal because the document found is the closest to the user’s input query. We <a id="_idIndexMarker080"/>will be introducing more complex documents for RAG starting in <em class="chapterRef">Chapter 2</em>, <em class="italic">RAG Embedding Vector Stores with Deep Lake and OpenAI</em>. For now, let’s have a look at the features that influence how the words are represented in vectors.</p>
    <h5 class="heading-5">Feature extraction</h5>
    <p class="normal">Before augmenting the input with<a id="_idIndexMarker081"/> this document, run the following cell, which calls the <code class="inlineCode">setup_vectorizer(records)</code> function again but displays the matrix so that you can see its format. This is shown in the following excerpt for the words “accurate” and “additional” in one of the sentences:</p>
    <figure class="mediaobject"><img src="img/B31169_01_04.png" alt="A black and white image of a number  Description automatically generated with medium confidence"/></figure>
    <p class="packt_figref">Figure 1.4: Format of the matrix</p>
    <p class="normal">Let’s now augment the input.</p>
    <h5 class="heading-5">Augmented input</h5>
    <p class="normal">We will simply add the query to the<a id="_idIndexMarker082"/> best matching record in a minimal way to see how GPT-4o will react and display the output:</p>
    <pre class="programlisting code"><code class="hljs-code">augmented_input=query+": "+best_matching_record
print_formatted_response(augmented_input)
</code></pre>
    <p class="normal">The output is close to or the same as with vector search, but the retrieval method is faster:</p>
    <pre class="programlisting con"><code class="hljs-con">Response:
---------------
define a rag store: A RAG vector store is a database or dataset that contains
vectorized data points.
---------------
</code></pre>
    <p class="normal">We will now plug this augmented input into the generative AI model.</p>
    <h5 class="heading-5">Generation</h5>
    <p class="normal">We now call GPT-4o with the <a id="_idIndexMarker083"/>augmented input and display the output:</p>
    <pre class="programlisting code"><code class="hljs-code"># Call the function and print the result
llm_response = call_llm_with_full_text(augmented_input)
print_formatted_response(llm_response)
</code></pre>
    <p class="normal">The output makes sense for the user who entered the initial fuzzy query:</p>
    <pre class="programlisting con"><code class="hljs-con">Response:
---------------
Certainly! Let's break down and elaborate on the given content:  ---  **Define a RAG store:**  A **RAG vector store** is a **database** or **dataset** that contains **vectorized data points**.  ---  ### Detailed Explanation:  1. **RAG Store**:    - **RAG** stands for **Retrieval-Augmented Generation**. It is a technique used in natural language processing (NLP) where a model retrieves relevant information from a database or dataset to augment its generation capabilities…
</code></pre>
    <p class="normal">This approach worked well in a closed environment within an organization in a specific domain. In an open environment, the user might have to elaborate before submitting a request.</p>
    <p class="normal">In this section, we saw that a TF-IDF matrix pre-computes document vectors, enabling faster, simultaneous comparisons without repeated vector transformations. We have seen how vector and index-based search can improve retrieval. However, in one project, we may need to apply naïve and advanced RAG depending on the documents we need to retrieve. Let’s now see how modular RAG can improve our system.</p>
    <h3 id="_idParaDest-44" class="heading-3">4. Modular RAG</h3>
    <p class="normal">Should we use keyword search, vector <a id="_idIndexMarker084"/>search, or index-based search when implementing RAG? Each approach has its merits. The choice will depend on several factors:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Keyword search</strong> suits simple retrieval</li>
      <li class="bulletList"><strong class="keyWord">Vector search</strong> is ideal for semantic-rich documents</li>
      <li class="bulletList"><strong class="keyWord">Index-based search</strong> offers speed with large data.</li>
    </ul>
    <p class="normal">However, all three methods can perfectly fit together in a project. In one scenario, for example, a keyword search can help find clearly defined document labels, such as the titles of PDF files and labeled images, before they are processed. Then, indexed search will group the documents into indexed subsets. Finally, the retrieval program can search the indexed dataset, find a subset, and only use vector search to go through a limited number of documents to find <a id="_idIndexMarker085"/>the most relevant one.</p>
    <p class="normal">In this section, we will create a <code class="inlineCode">RetrievalComponent</code> class that can be called at each step of a project to perform the task required. The code sums up the three methods we have built in this chapter and that we can sum for the <code class="inlineCode">RetrievalComponent</code> through its main members.</p>
    <p class="normal">The following code initializes the class with search method choice and prepares a vectorizer if needed. <code class="inlineCode">self</code> refers to the current instance of the class to access its variables, methods, and functions:</p>
    <pre class="programlisting code"><code class="hljs-code">def __init__(self, method='vector'):
        self.method = method
        if self.method == 'vector' or self.method == 'indexed':
            self.vectorizer = TfidfVectorizer()
            self.tfidf_matrix = None
</code></pre>
    <p class="normal">In this case, the vector search is activated.</p>
    <p class="normal">The <code class="inlineCode">fit</code> method builds a TF-IDF matrix from records, and is applicable for vector or indexed search methods:</p>
    <pre class="programlisting code"><code class="hljs-code">    def fit(self, records):
        if self.method == 'vector' or self.method == 'indexed':
            self.tfidf_matrix = self.vectorizer.fit_transform(records)
</code></pre>
    <p class="normal">The retrieve method directs the query to the appropriate search method:</p>
    <pre class="programlisting code"><code class="hljs-code">    def retrieve(self, query):
        if self.method == 'keyword':
            return self.keyword_search(query)
        elif self.method == 'vector':
            return self.vector_search(query)
        elif self.method == 'indexed':
            return self.indexed_search(query)
</code></pre>
    <p class="normal">The keyword search method finds the best match by counting common keywords between queries and documents:</p>
    <pre class="programlisting code"><code class="hljs-code">    def keyword_search(self, query):
        best_score = 0
        best_record = None
        query_keywords = set(query.lower().split())
        for index, doc in enumerate(self.documents):
            doc_keywords = set(doc.lower().split())
            common_keywords = query_keywords.intersection(doc_keywords)
            score = len(common_keywords)
            if score &gt; best_score:
                best_score = score
                best_record = self.documents[index]
        return best_record
</code></pre>
    <p class="normal">The vector search method<a id="_idIndexMarker086"/> computes similarities between query TF-IDF and document matrix and returns the best match:</p>
    <pre class="programlisting code"><code class="hljs-code">    def vector_search(self, query):
        query_tfidf = self.vectorizer.transform([query])
        similarities = cosine_similarity(query_tfidf, self.tfidf_matrix)
        best_index = similarities.argmax()
        return db_records[best_index]
</code></pre>
    <p class="normal">The indexed search method uses a precomputed TF-IDF matrix for fast retrieval of the best-matching document:</p>
    <pre class="programlisting code"><code class="hljs-code">    def indexed_search(self, query):
        # Assuming the tfidf_matrix is precomputed and stored
        query_tfidf = self.vectorizer.transform([query])
        similarities = cosine_similarity(query_tfidf, self.tfidf_matrix)
        best_index = similarities.argmax()
        return db_records[best_index]
</code></pre>
    <p class="normal">We can now activate modular RAG strategies.</p>
    <h4 class="heading-4">Modular RAG strategies</h4>
    <p class="normal">We can call the retrieval component for <a id="_idIndexMarker087"/>any RAG configuration we wish when needed:</p>
    <pre class="programlisting code"><code class="hljs-code"># Usage example
retrieval = RetrievalComponent(method='vector')  # Choose from 'keyword', 'vector', 'indexed'
retrieval.fit(db_records)
best_matching_record = retrieval.retrieve(query)
print_formatted_response(best_matching_record)
</code></pre>
    <p class="normal">In this case, the vector search method was activated.</p>
    <p class="normal">The following cells select the <a id="_idIndexMarker088"/>best record, as in the <em class="italic">3.1. Vector search</em> section, augment the input, call the generative model, and display the output as shown in the following excerpt:</p>
    <pre class="programlisting con"><code class="hljs-con">Response:
---------------
Certainly! Let's break down and elaborate on the content provided:  ---
**Define a RAG store:**  A **RAG (Retrieval-Augmented Generation) store** is a specialized type of data storage system designed to support the retrieval and generation of information...
</code></pre>
    <p class="normal">We have built a program that demonstrated how different search methodologies—keyword, vector, and index-based—can be effectively integrated into a RAG system. Each method has its unique strengths and addresses specific needs within a data retrieval context. The choice of method depends on the dataset size, query type, and performance requirements, which we will explore in the following chapters.</p>
    <p class="normal">It’s now time to summarize our explorations in this chapter and move to the next level!</p>
    <h1 id="_idParaDest-45" class="heading-1">Summary</h1>
    <p class="normal">RAG for generative AI relies on two main components: a retriever and a generator. The retriever processes data and defines a search method, such as fetching labeled documents with keywords—the generator’s input, an LLM, benefits from augmented information when producing sequences. We went through the three main configurations of the RAG framework: naïve RAG, which accesses datasets through keywords and other entry-level search methods; advanced RAG, which introduces embeddings and indexes to improve the search methods; and modular RAG, which can combine naïve and advanced RAG as well as other ML methods.</p>
    <p class="normal">The RAG framework relies on datasets that can contain dynamic data. A generative AI model relies on parametric data through its weights. These two approaches are not mutually exclusive. If the RAG datasets become too cumbersome, fine-tuning can prove useful. When fine-tuned models cannot respond to everyday information, RAG can come in handy. RAG frameworks also rely heavily on the ecosystem that provides the critical functionality to make the systems work. We went through the main components of the RAG ecosystem, from the retriever to the generator, for which the trainer is necessary, and the evaluator. Finally, we built an entry-level naïve, advanced, and modular RAG program in Python, leveraging keyword matching, vector search, and index-based retrieval, augmenting the input of GPT-4o.</p>
    <p class="normal">Our next step in <em class="chapterRef">Chapter 2</em>, <em class="italic">RAG Embedding Vector Stores with Deep Lake and OpenAI</em>, is to embed data in vectors. We will store the vectors in vector stores to enhance the speed and precision of the retrieval functions of a RAG ecosystem.</p>
    <h1 id="_idParaDest-46" class="heading-1">Questions</h1>
    <p class="normal">Answer the following questions with <em class="italic">Yes</em> or <em class="italic">No</em>:</p>
    <ol>
      <li class="numberedList" value="1">Is RAG designed to improve the accuracy of generative AI models?</li>
      <li class="numberedList">Does a naïve RAG configuration rely on complex data embedding?</li>
      <li class="numberedList">Is fine-tuning always a better option than using RAG?</li>
      <li class="numberedList">Does RAG retrieve data from external sources in real time to enhance responses?</li>
      <li class="numberedList">Can RAG be applied only to text-based data?</li>
      <li class="numberedList">Is the retrieval process in RAG triggered by a user or automated input? </li>
      <li class="numberedList">Are cosine similarity and TF-IDF both metrics used in advanced RAG configurations?</li>
      <li class="numberedList">Does the RAG ecosystem include only data collection and generation components?</li>
      <li class="numberedList">Can advanced RAG configurations process multimodal data such as images and audio?</li>
      <li class="numberedList">Is human feedback irrelevant in evaluating RAG systems?</li>
    </ol>
    <h1 id="_idParaDest-47" class="heading-1">References</h1>
    <ul>
      <li class="bulletList"><em class="italic">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</em> by Patrick Lewis, Ethan Perez, Aleksandra Piktus, et al.: <a href="https://arxiv.org/abs/2005.11401">https://arxiv.org/abs/2005.11401</a></li>
      <li class="bulletList"><em class="italic">Retrieval-Augmented Generation for Large Language Models: A Survey</em> by Yunfan Gao, Yun Xiong, Xinyu Gao, et al.: <a href="https://arxiv.org/abs/2312.10997">https://arxiv.org/abs/2312.10997</a></li>
      <li class="bulletList">OpenAI models: <a href="https://platform.openai.com/docs/models">https://platform.openai.com/docs/models</a></li>
    </ul>
    <h1 id="_idParaDest-48" class="heading-1">Further reading</h1>
    <ul>
      <li class="bulletList">To understand why RAG-driven Generative AI transparency is recommended, please see <a href="https://hai.stanford.edu/news/introducing-foundation-model-transparency-index">https://hai.stanford.edu/news/introducing-foundation-model-transparency-index</a></li>
    </ul>
    <h1 id="_idParaDest-49" class="heading-1">Join our community on Discord</h1>
    <p class="normal">Join our community’s Discord space for discussions with the author and other readers:</p>
    <p class="normal"><a href="https://www.packt.link/rag">https://www.packt.link/rag</a></p>
    <p class="normal"><img src="img/QR_Code50409000288080484.png" alt=""/></p>
  </div>
</body></html>