- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Prompt Engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Prompt engineering for an enterprise takes a slightly different approach to
    interacting with ChatGPT or any LLM for personal use. Prompt engineering helps
    ensure that when the customer messages the LLM, a set of instructions is in place
    for them to succeed. When building prompts to generate a recommendation or complete
    some backend analysis, the recommendation team directly creates the prompt. The
    job is to consider how the instructions that give *context* to the customer’s
    messages, also called a prompt, are framed or *create* the prompts that request
    a result directly from the LLM. First, we will focus on prompt engineering before
    continuing with fine-tuning in the next chapter, which is an inevitable next step
    for enterprise solutions.
  prefs: []
  type: TYPE_NORMAL
- en: None of the tools discussed should be considered in a silo. Any enterprise solution
    will adopt **Retrieval-Augmented Generation** (**RAG**), prompt engineering, fine-tuning,
    and other approaches. Each can support different capabilities while sometimes
    overlapping. While prompt engineering will align the responses with the goal,
    fine-tuning will help the model improve its understanding.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter focuses on a few critical topics related to prompt engineering:'
  prefs: []
  type: TYPE_NORMAL
- en: Giving context through prompt engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompt engineering techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Andrew Ng’s agentic approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advanced techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Giving context through prompt engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To be clear, when building a RAG solution, customers prompt the enterprise system
    for answers to questions, fill out forms, and interact through prompting. Additional
    prompts, called instructions, wrap these prompts to ensure they are constrained
    or managed within a context defined by the business. These instructions give the
    customer guardrails. Time for prompt engineering 101.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt 101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Prompt engineering instructs the chat instance to respond. It frames or puts
    structure around the answer, defines what to include or exclude from responses,
    and provides any safety rails to implement.
  prefs: []
  type: TYPE_NORMAL
- en: Instructions can be tested and iterated. Hundreds of changes will be made before
    settling on better instructions. Multiple models will be doing pieces of the enterprise
    puzzle, each with its instructions. We will take a few minutes to clarify that
    we are focused on instructions, a form of prompts that a user needs to control
    how the model will respond to.
  prefs: []
  type: TYPE_NORMAL
- en: The prompt strategy depends on the task’s needs. If it is a general-purpose
    interactive prompt, it will focus on style, tone, factualness, and quality. If
    this prompt is for a step that ingests tables and formats content, it will focus
    on structure and data output. We will address instructions that wrap customer
    prompts, as shown in *Figure 7**.1*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_07_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 – How to rationalize instructions, prompts, and answers
  prefs: []
  type: TYPE_NORMAL
- en: If the prompt comes from the customer, we can’t control their request. So, we
    do what we can to control it. In the figure above, we establish the persona of
    the Alligiance chat, but the customer asks the question, and the model provides
    the specific answer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instructions can be simple, such as in the examples from OpenAI, or they can
    be crafted to address some of our enterprise needs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Even a trivial example like this has a few essential elements:'
  prefs: []
  type: TYPE_NORMAL
- en: It clarified the persona of the AI and the type of business
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It defines how it should act
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It constrains where to look for answers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It provides a suggestion on how to respond
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It doesn’t include any actual questions; those come from the customer’s prompt
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This will grow in complexity, spanning dozens or hundreds of lines of text,
    but this is a cost-benefit trade-off. The longer the prompt, the more tokens are
    used. These additional instructions are included for *every* prompt the user sends
    to the model, so use your tokens wisely. Remember that tokens represent how the
    model accounts for size and cost based on the amount of text. While humans understand
    word count, the model talks in tokens. It can have a maximum amount of context
    passed to it (in tokens) and a maximum amount of data returned at one time (in
    tokens), and then charges are based on the number of tokens. We will cover more
    about tokens in this and the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The LLM does not directly interact with customers for recommender solutions
    or behind-the-scenes uses of a model. Instructions can provide general guidance,
    and prompts (more detailed instructions) that can be used for specific task efforts.
    This abstraction creates consistency in one set of instructions for all prompts
    in a group of projects.
  prefs: []
  type: TYPE_NORMAL
- en: A thoughtful enterprise instruction set has to be in place to support the user’s
    prompts for conversational AI. The differences between instructions that act as
    a wrapper for customer prompts and the actual prompt impact how to write instructions
    or prompts. Instructions have to be more generic and support a wide range of prompts.
    The direct prompts are targeted, focusing the LLM on providing one good answer,
    as shown in the example in the preceding figure. So, there are differences in
    designing prompts for your personal use versus what is needed in an enterprise
    solution.
  prefs: []
  type: TYPE_NORMAL
- en: Designing instructions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have all created a variety of prompts for home or work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'However, to frame instructions to guide users interacting with an enterprise
    conversational assistant or when building the instructions for any recommender
    use cases, robust instructions that clarify the goals and persona of all interactions
    are needed. Here is the start of an instruction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This should frame the customer request (their prompt) who might ask questions
    such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Together, these two layers of prompt engineering give instructions to the model.
    So, the company provides the instructions, and the customer provides their prompt.
    With recommender solutions, the company does it all.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine an LLM-driven recommender, for scoring leads, rating a person’s reputation,
    offering products to upsell, providing sentiment feedback, or suggesting data
    to ignore because it has undesirable or harmful content. Create prompts specific
    to those use cases. Each prompt will serve only some of the issues. This is why
    multiple models are discussed so much. There can even be models designed to decide
    which model to use next. The prompt of the first model helps route the request
    to the second model, which has prompts specific to its task and is tuned to the
    needs of the request. This chaining of models will be covered here and more in
    [*Chapter 8*](B21964_08.xhtml#_idTextAnchor172), *Fine-Tuning*. Every one of these
    models needs well-thought-out prompts to guide an interaction, and none of these
    will have a human prompting the system.
  prefs: []
  type: TYPE_NORMAL
- en: There is a wealth of documentation and tutorials on what simple prompts can
    do. Start exploring more at the OpenAI site.
  prefs: []
  type: TYPE_NORMAL
- en: 'Documentation: [Prompt Examples](https://platform.openai.com/docs/examples)
    ([https://platform.openai.com/docs/examples](https://platform.openai.com/docs/examples))'
  prefs: []
  type: TYPE_NORMAL
- en: But these are just starting points. Much work is needed to scale these up to
    work reliably and with the style and tone expected in a business use case. Understand
    where prompt engineering fits into the process in addition to the prompt’s content.
    We can summarize the highlights from OpenAI’s high-level presentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Video: [Techniques for improving LLM Quality](https://youtu.be/ahnGLM-RC1Y)
    ([https://youtu.be/ahnGLM-RC1Y](https://youtu.be/ahnGLM-RC1Y))'
  prefs: []
  type: TYPE_NORMAL
- en: The takeaway starts at about 3 minutes. *Figure 7**.2* outlines their approach.
    They reviewed RAG (discussed in the last chapter) as a solution to help an enterprise
    access its knowledge base and other data sources. They make a great point that
    this data can be scrubbed and cleaned *before* having a working system. Meanwhile,
    prompt engineering and fine-tuning rely on a *working* system for feedback.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_07_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 — Tools to help us optimize our ChatGPT solution
  prefs: []
  type: TYPE_NORMAL
- en: These tools are all needed in enterprise solutions to improve LLM quality. We
    can walk through why this is the case. Prompt engineering can start with basic
    example questions to see how the model acts. When it doesn’t work well, adding
    training examples to improve how it responds to questions unique to our business
    is next. This will quickly result in wanting more data than can be handled by
    basic interactions, so the solution extends into RAG. Now, the results don’t fit
    our style or tone or don’t follow the instructions expected, so fine-tuning is
    added, giving examples to the model to train it on how it is expected to respond.
    Results can indicate that the RAG could be refined and optimized further, so they
    go back and work on it. This results in wanting to fine-tune the results further.
    And this cycle continues, hopefully improving at every step.
  prefs: []
  type: TYPE_NORMAL
- en: For our video learners, Mark Hennings has an excellent 15-minute overview that
    quickly covers a lot of ground.
  prefs: []
  type: TYPE_NORMAL
- en: 'Video: [Prompt Eng, RAG, and Fine Tuning](https://www.youtube.com/watch?v=YVWxbHJakgg)
    (Mark Hennings) ([https://www.youtube.com/watch?v=YVWxbHJakgg](https://www.youtube.com/watch?v=YVWxbHJakgg))'
  prefs: []
  type: TYPE_NORMAL
- en: An excellent place to start is by teaching some basic strategies for prompting.
  prefs: []
  type: TYPE_NORMAL
- en: Basic strategies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many structured methodologies have been proposed for prompt engineering, and
    most are similar. One is called **RACE** (**Role, Action, Context, and** **Examples**),
    another is called **CO-STAR** (**Context, Objective, Style, Tone, Audience, and**
    **Response**), and another is called **CARE** (**Content, Action Result, and Example**).
    Other approaches are explained without a cute initialism. First, it is good to
    understand the primary instructions of a typical prompt; then, dive deeper to
    help with enterprise instructions.
  prefs: []
  type: TYPE_NORMAL
- en: '*Table 7.1* cross-lists the similar concepts for each approach in the first
    column. Each framework uses slightly different terminology but mainly covers the
    same fundamentals. The cute initialisms seem primarily for branding. We will ignore
    that and focus on goals, not the terms. Expect to write prompts that contain all
    of these approaches. We will also explain when not to do some of this.'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Approach** | **Explanation** | **Example** |'
  prefs: []
  type: TYPE_TB
- en: '| **Priming (role, audience,** **and objective)** | Establish the context of
    the response. | *You are a sales and service assistant to the inside sales team
    to help them* *close deals.* |'
  prefs: []
  type: TYPE_TB
- en: '| **Style and** **tone (attitude)** | Define the style and tone expected in
    the response. | *Respond in simple language and explain any processes step by
    step while being encouraging* *and supportive.* |'
  prefs: []
  type: TYPE_TB
- en: '| **Example** | Provide examples of how the output should look. | *Here is
    an example of how you should sound with the* *details expected.**The Smith deal
    closed on 5 March this year. It is worth $1.2M in revenue booked over the next*
    *five months.**Here is* *another example.**Jim Lankey is the lead on the Wilson
    deal. He has worked with Wilson for the last three years. Email him at jim@ourcompany.com
    for* *more details.* |'
  prefs: []
  type: TYPE_TB
- en: '| **Handling errors and** **edge cases** | Creating guardrails for the scope
    of responses. | *If the questions do not appear to be about the sales or service
    support, first try to confirm your understanding, and if off topic politely decline
    to* *offer suggestions.* |'
  prefs: []
  type: TYPE_TB
- en: '| **Dynamic content (can also** **be context)** | Inject facts from RAG. “What
    was the size of the service control contact in 2023? | *User* *question: {question}**Use
    this, if useful: {knowledge* *from RAG}* |'
  prefs: []
  type: TYPE_TB
- en: '| **Output** **format (response)** | Define how the default responses should
    look. | *Keep answers short and to the point; use tables or numbered lists* *when
    needed.* |'
  prefs: []
  type: TYPE_TB
- en: Table 7.1 – Basic prompt components
  prefs: []
  type: TYPE_NORMAL
- en: Most of the training and videos out there discuss prompting. They typically
    focus on personal prompting and how to get an LLM to craft the output for one
    task. This chapter focuses on enterprise prompting, getting the LLM to respond
    every time in a way that is conducive to business customers. However, much of
    basic prompting is still relevant. To explore more, here are the resources used
    beyond OpenAI to craft our explanation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [Getting started with LLM prompt engineering](https://learn.microsoft.com/en-us/ai/playbook/technology-guidance/generative-ai/working-with-llms/prompt-engineering)
    ([https://learn.microsoft.com/en-us/ai/playbook/technology-guidance/generative-ai/working-with-llms/prompt-engineering](https://learn.microsoft.com/en-us/ai/playbook/technology-guidance/generative-ai/working-with-llms/prompt-engineering))'
  prefs: []
  type: TYPE_NORMAL
- en: I like Jules Damji’s article because it references research and methods that
    go deeper. We, too, need to go deeper when building a production solution. The
    basics will be explained; later, explore more.
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [Best Prompt Techniques for Best LLM Responses](https://medium.com/the-modern-scientist/best-prompt-techniques-for-best-llm-responses-24d2ff4f6bca)
    by Jules Damji ([https://medium.com/the-modern-scientist/best-prompt-techniques-for-best-llm-responses-24d2ff4f6bca](https://medium.com/the-modern-scientist/best-prompt-techniques-for-best-llm-responses-24d2ff4f6bca))'
  prefs: []
  type: TYPE_NORMAL
- en: There are plenty of examples on the web. Since CO-STAR was mentioned, check
    out the prompts in their notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'GitHub: [Basic prompting from the CO-STAR framework](https://colab.research.google.com/github/dmatrix/genai-cookbook/blob/main/llm-prompts/1_how_to_use_basic_prompt.ipynb)
    ([https://colab.research.google.com/github/dmatrix/genai-cookbook/blob/main/llm-prompts/1_how_to_use_basic_prompt.ipynb](https://colab.research.google.com/github/dmatrix/genai-cookbook/blob/main/llm-prompts/1_how_to_use_basic_prompt.ipynb))'
  prefs: []
  type: TYPE_NORMAL
- en: In the prompt on their first GitHub example, they provide each characteristic
    of CO-STAR. In some cases, such as when creating a recommender or using an LLM
    for a backend service, specificity is paramount, as Wove does in our ongoing case
    study.
  prefs: []
  type: TYPE_NORMAL
- en: However, there is also the use case of an enterprise LLM fed with RAG data.
    Instructions must be more generic for a RAG process for knowledge retrieval. It
    is not focused on writing a blog post, developing a specific answer, or performing
    one task. It will answer many questions, fill out forms, submit data, and change
    topics with some frequency. This means instructions will guide and frame the answer
    based on the user’s prompt. This is why instructions get very long. They have
    to cover a wide range of interactions and all of the components of a prompt. One
    option is to create distinct models that service specific tasks and use a primary
    model to determine which model to send the request to. This is an intelligent
    use of resources. This hub and spoke model must only know enough to classify and
    forward to the suitable model. It doesn’t need to do the heavy lifting.
  prefs: []
  type: TYPE_NORMAL
- en: Conversely, highly tuned models for specific tasks will have specific prompts.
    The system should outsource questions that don’t match this particular task to
    other models. We will cover the agent approach in more detail shortly. The hub-and-spoke
    approach is shown in *Figure 7**.3*. Thanks to Miha at [Miha.Academy](https://miha.academy/)
    ([https://miha.academy/](https://miha.academy/)) for his templates to create the
    flow shown in figures like this.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_07_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 – A hub and spoke process to route to specific tuned models
  prefs: []
  type: TYPE_NORMAL
- en: 'There is no one correct model. Explore matching models for quality, performance,
    and cost with the needs of the use case. In this hub-and-spoke example, the routing
    model decides which model should be passed a specific task:'
  prefs: []
  type: TYPE_NORMAL
- en: Model 1 is for processing a transaction. It needs to model existing APIs and
    requirements for the backend. This graphic might mask the additional needs of
    multiple models or dynamic prompts to handle how to work on various channels.
    Flow diagrams with that complexity would be difficult to visualize in this graphic.
    Use your imagination. This diagram can get very complex.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model 2 handles any business chit-chat and social interactions. Any further
    input might be routed to a different model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model 3 supports RAG. It is fine-tuned to handle discussing technical documentation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model 4 handles some tasks that require local processing for security purposes,
    so an open-source model that can be deployed locally handles this task.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, Model 5 uses Anthropic for cloud service integration because it handles
    this task quickly and inexpensively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each model requires design effort, testing, validation, and a care and feeding
    process. Each is an application to itself. This is unsurprising, as many enterprise
    solutions might comprise dozens or hundreds of systems and services.
  prefs: []
  type: TYPE_NORMAL
- en: Quick tricks to always keep in mind
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There is a wealth of coaching for prompt engineering, and one book will not
    make you an expert. To grow into an expert, learn these skills, apply them (and
    apply them, and apply them), feel how the model reacts to these instructions,
    and adapt as new models become available. Here are the fundamentals that OpenAI
    preaches. Instill them into best practices:'
  prefs: []
  type: TYPE_NORMAL
- en: Write clear instructions – be direct with the LLM. Tell it what to *do* and
    avoid adverse terms such as *don’t*. Niceties cost money and, except in rare examples,
    don’t add value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Split complex tasks into simpler subtasks. Ask the LLM to break down the problem
    into steps or give it the steps if the prompt is constrained to a specific process.
    This also allows specific models to perform particular workflow tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reference, prioritize, and demand the use of the enterprise data or require
    it to be the only source of truth. For example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the structure or format for data (customers can ask for different formats),
    such as bulleted lists or tables. This might have limited use in a general-purpose
    customer-support LLM:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Provide examples (or, as this grows, move examples to fine-tuning, including
    the expected style and tone); this is a **few-shot learning** for the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Consider any constraints that should be put in place for guardrails. Keep customers
    focused on the enterprise data, even if more social style and tone are included,
    like in this example. Avoid politics, general knowledge, or culturally sensitive
    areas:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Although models ship with controls, the bar is higher for large enterprises.
    Businesses don’t want screenshots of inappropriate interactions circulating online;
    plenty of those failures are already in the news.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Give ChatGPT time to think. Allow it to follow the steps to the answer, or have
    it check if it works. Recall that it is a people-pleaser. It wants to provide
    an answer. Breaking down solutions into component steps can allow for a more accurate
    answer. Use instructions to require it to resolve issues step by step. Ask the
    LLM to follow a specific method to deconstruct a problem, ask follow-up questions,
    and decide how to get to a resolution. I have viewed dozens of not-very-good videos
    on this subject. This one shows an excellent multi-step reasoning process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Video: [Optimize Instruction tuned Conversational AI/LLM](https://www.youtube.com/watch?v=aeDr0duR_jo”https://www.youtube.com/watch?v=aeDr0duR_jo)
    ([https://www.youtube.com/watch?v=aeDr0duR_jo”https://www.youtube.com/watch?v=aeDr0duR_jo](https://www.youtube.com/watch?v=aeDr0duR_jo”https://www.youtube.com/watch?v=aeDr0duR_jo))'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If needed, chain models together, ask one (or more than one) to solve a problem
    and then have another model check the work before sharing it with the customer.
    Some situations might demand this additional cost and complexity.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Test changes systematically. Test, test, test! Each model update can profoundly
    change the skill. Rerun previous questions and then ask the LLM to compare the
    previous and new results for any significant differences. The tools in this space
    are changing and adapting to these new approaches. Ensure that good-quality tests
    are consistent with what a user would do and cover edge cases. [*Chapter 10*](B21964_10_split_000.xhtml#_idTextAnchor216),
    *Monitoring and Evaluation,* explains testing within a care and feeding life cycle,
    which is about listening to feedback and iterating on the results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For interactive chats, the instructions are written to support the user’s prompt;
    the prompt itself is for the customer to write, so it has to be generic enough
    to handle the variety of questions that will be asked.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider whether the LLM can give enough structure to the results for recommender
    or non-interactive solutions. Instruct the model on how recommendations should
    appear or use templates to enforce specific guidelines.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide new information. Use RAG or other retrieval solutions to get the latest
    information from knowledge, APIs, or databases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inject context. Use data sources to include specific details in prompts to give
    the user’s conversation more context.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider costs. Creating large prompts means spending more tokens for every
    interaction, which costs money and can add up quickly. Fine-tuning can help reduce
    that cost. Using less expensive models for specific tasks also reduces the cost.
    Be willing to move to new models. The industry is evolving quickly, with price
    reductions of 70% for some new models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Don’t expect miracles; the LLM is not capable of all responses. Foundational
    models don’t do well with math. [*Chapter 3*](B21964_03.xhtml#_idTextAnchor058)*,
    Identifying Optimal Use Cases for ChatGPT*, discussed various use cases to avoid.
    Avoid lousy use cases. The OpenAI team says prompts are unsuitable for “*reliability
    replicating a complex style or method, i.e., learning a new* *programming language.*”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each use case will demand some of these tips but don’t expect to use all of
    them. A/B testing is an excellent generic usability method for learning whether
    one of these approaches works better.
  prefs: []
  type: TYPE_NORMAL
- en: A/B testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OpenAI’s extensive documentation is a source of great ideas to help improve
    prompts. However, it misses one excellent method that product designers use with
    software development, which is A/B testing. This is something that has been around
    for a long time. A/B testing requires deploying multiple solutions and comparing
    the results to determine a winner. In reality, this can be an A/B/C/D testing
    with various prompts. If one scores significantly better than the other, that
    is the winner. Then, iteratively test with a second A/B test by creating new versions
    based on the winner. A/B testing can be done in multiple ways, including deploying
    and testing within a user research study or deploying in production while monitoring
    the results. More advanced solutions incorporate analytics into the testing, and
    if statistically there is a winner, the test automatically shuts down, and the
    winner is deployed to all users. This can be done with prompts, fine-tuning, RAG
    data resources, and any case where multiple options are viable. Traditionally,
    this was done with GUI issues, such as the best location or label for a button.
    Automating the rollover to adjust the winning condition has been around for decades.
    Existing clever ideas continue to apply in the generative AI world, while there
    are dozens of techniques specific to prompting. Let’s look at some essential prompt-specific
    techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt engineering techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are dozens of techniques to improve prompts. This section highlights the
    most valuable strategies for enterprise use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Self-consistency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Think of self-consistency as aligning statements with truth, thus making them
    logically aligned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Solar power is a renewable resource, *unlike* coal or oil, which have finite
    reserves. The response from the LLM needs to be more consistent in representing
    solar power, as it is *not* a finite resource. The documentation might be an issue,
    or the context length or writing style infers wrong conclusions. A solution is
    to provide a few examples that can teach the model. This is not training it with
    the exact answers; it only gives exemplars to approach the class of problems.
    It is pretty amazing. Alternatively, ask the question differently and see whether
    some answers are consistent.
  prefs: []
  type: TYPE_NORMAL
- en: Wang et al. (2023) go through a variety of these few-shot training examples
    in a variety of situations to help improve reasoning from models. Few-shot learning
    is covered later in this chapter. Few-shot learning provides some examples to
    train a system to respond. They get to self-consistency by taking multiple answers
    from the model responses and deciding on the correct solution based on their consistency.
    It is like fault-tolerant software that might use three different computers to
    evaluate an answer. If two or more are right, they go with the shared answer.
    This means additional costs for gathering additional solutions. Wang et al. point
    out that they can use this method to include these examples in a fine-tuning model.
    Putting an extensive collection of examples into a fine-tuned model reduces the
    cost of large prompts.
  prefs: []
  type: TYPE_NORMAL
- en: If reasoning is part of the solution space, consider how this approach can help
    improve quality. Their research covered examples of arithmetic reasoning, common
    sense reasoning, symbolic reasoning, and strategies. *Table 7.2* shows examples
    from each area used in the prompts to support a model’s performance of these tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [Self-consistency improves chain of thought reasoning in language
    models](https://arxiv.org/pdf/2203.11171) by Wang et al. ([https://arxiv.org/pdf/2203.11171](https://arxiv.org/pdf/2203.11171))
    by Wang et al.'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Reasoning** | **Examples** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Arithmetic** | (Train on examples like this)Q: The grove has 15 trees.
    Grove workers will plant trees in the grove today. When done, there will be 21
    trees. How many trees did the workers plant today?A: We start with 15 trees. Later,
    we have 21 trees. The difference must be the number of trees planted. So, they
    planted 21- 15 = 6 trees. The answer is 6.(To then ask similar questions like
    this)*Q: When I was 6, my sister was half my age. Now that I’m 70, how old* *is
    she?*(So, the reasoned answer is correct)*When I was 6, my sister was half my
    age, so she was 3\. Now, I am 70, so she is 70 - 3 = 67\. The answer* *is 67*.
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Common** **sense** | Yes or no: Can the Great Depression be treated with
    Prozac?(The wrong answer)*The Great Depression was an economic depression. Prozac
    is a drug used to treat depression. Thus, the Great Depression could be treated
    with Prozac. So, the* *answer* *is yes.*(A correctly reasoned example)*Prozac
    is an anti-depressant medication. The Great Depression is not a disease. Thus,
    Prozac cannot treat the Great Depression. So, the answer* *is no.* |'
  prefs: []
  type: TYPE_TB
- en: '| **Symbolic** | Concatenate the last letters of each name.Q: Jim BeamA: mmQ:
    Richard MillerA: drQ: Ely Kaplan(The correct response to the training example)A:
    yn |'
  prefs: []
  type: TYPE_TB
- en: '| **Strategy** | Yes or no: Is Albany, Georgia, the most populous US Albany?(The
    wrong answer)*Albany, Georgia is the most* *populous US Albany. Thus, the answer*
    *is yes.*(A correctly reasoned example)*Albany, Georgia has a population of about
    88,000\. Albany, New York, has a population of about 95,000\. Thus, Albany, Georgia,
    is not the most populous US Albany. So, the answer* *is no.* |'
  prefs: []
  type: TYPE_TB
- en: Table 7.2 – Examples of self-consistency testing and training from Wang et al.
    (2023)
  prefs: []
  type: TYPE_NORMAL
- en: General knowledge prompting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While in RAG, the reduced results from a knowledge search are used to create
    a manageable set of information to pass to the LLM for analysis. General knowledge
    prompting provides context to a question so a model can use that to inform the
    answer. Take database information to build and share a persona for the user. This
    might help the model improve its interactions with the customer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This stretches the concept of general knowledge prompting. Sometimes, experiments
    like this can yield results. These approaches can be used in conjunction with
    other methods. Prompt chaining helps break down problems into manageable parts.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt chaining
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Multiple approaches break down tasks into smaller tasks and then apply more
    refined reasoning to a part of a problem. The team at Wove breaks down its tasks
    so that specific prompts can be controlled. For example, document extraction is
    done with one set of prompts and a second set of formats, and the results from
    the documents are returned. This involves chaining one model output to become
    input for the next model. This allows models to be hyper-focused on specific tasks.
    Each can then become better at their job, helping to manage workflow and allowing
    for improvements in one segment not to impact another. A large single-purpose
    model to do all of this would be hard to operate and improve.
  prefs: []
  type: TYPE_NORMAL
- en: Time to think
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A model can be asked in a prompt for a **chain of thought** (**COT**). Thus,
    a model needs to work out its solution before jumping to a conclusion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: OpenAI has plenty of other strategies, some of which are very tactical, such
    as using delimiters to keep specific input distinct. Some of these can be adapted
    to instructions that help guide user prompts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [Writing clear instructions](https://platform.openai.com/docs/guides/prompt-engineering/strategy-write-clear-instructions)
    ([https://platform.openai.com/docs/guides/prompt-engineering/strategy-write-clear-instructions](https://platform.openai.com/docs/guides/prompt-engineering/strategy-write-clear-instructions))'
  prefs: []
  type: TYPE_NORMAL
- en: Anthropic does an excellent job of providing some enterprise-related examples
    of analyzing a legal contract using chaining. Here are some other examples they
    provide.
  prefs: []
  type: TYPE_NORMAL
- en: '**Content creation pipelines**: Research → outline → draft → edit → format'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data processing**: Extract → transform → analyze → visualize'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decision-making**: Gather info → list options → analyze each → recommend'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Verification loops**: Generate content → review → refine → re-review'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Article: [Chaining complex prompts for stronger performance](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/chain-prompts)
    ([https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/chain-prompts](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/chain-prompts))'
  prefs: []
  type: TYPE_NORMAL
- en: I especially like one of the advantages mentioned in the article—traceability.
    There is so much magic going on with LLMs; sometimes, we need to *feel* our way
    to success, even with metrics that provide scoring. Breaking down tasks into modules
    that can be tweaked independently to spot process issues is very appealing. Large
    prompts with a massive scope involve more work to adapt and improve.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use chaining within the same model in three steps – summarize, analyze, and
    update. OpenAI refers to this as an “inner monologue.” It has this conversation
    internally before revealing the answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This requires significantly more resources, but the findings should be more
    accurate. Notice the terms `Summarize`, `Analyze`, `Review`, and `Update`. The
    model determines the meaning of these terms in that it knows what to do when asked
    to analyze, review, update, summarize, and so on. By breaking down the process
    into steps, communication gaps can be fixed to return the correct result.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of prompts for an email summary to get a model to think
    through a problem and reduce hallucinations. This is a robust version of “*Think
    about the solution step by step*.” Craft versions of these to match the use case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Recognize that chaining only works in some use cases. It can be applied in backend
    solutions and recommenders or in more freeform conversational interactions, as
    it might take time to go through multiple steps. But it is a solid strategy for
    complex problems. The hub and spoke flow chart shows that chains can be done with
    multiple vendors’ models. Understand what a model is good at, consider speed or
    responsiveness, and consider its ability to handle media like images, if needed,
    and make cost/benefit trade-offs. With this complexity comes another issue – handling
    entities and working with other systems. A program-aided approach is part of the
    solution for enterprise complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Program-aided language models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although the example in the prompt guide is based on using a Python interpreter
    as an intermediate step to calculate an answer to a math question, since LLMs
    are not good at math, enterprise use cases can do form filling, organize data,
    write copy, build SQL queries, and perform mundane tasks. The LLM can help to
    identify entities, objects, dates, times, sizes, product IDs, names, and a wealth
    of other elements in a statement. Backend services that can validate some of this
    data are expected. So, programming and existing infrastructure are used to capture
    the data. An LLM can also be used to format data correctly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let me give an example that is easily created in the Playground. This prompt
    extracts information from a conversation for an expense receipt input system.
    Later, during my testing, the items in italics were added to improve the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'So, a conversation, with no other prompting or training, looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The results show that ChatGPT 4o (2024-05-13) got this right – it correctly
    identified **UA** as **United Airlines**, spelled *Benihana* correctly, and adjusted
    the date format. Now, this can be submitted to a backend. All typos were intentional.
    Apply this example to the use cases in your organization. I suspect there is something
    similar.
  prefs: []
  type: TYPE_NORMAL
- en: So, if the solution is more than searching for knowledge, like interacting with
    backend systems, balance the LLM expertise with the capability of the services.
    Over time, those backend systems will adapt, but to get out of the gate, choose
    which systems must be adapted or flexed. Services, scheduling, forms, and all
    those mundane business processes customers and employees have to do but don’t
    want to do as humans can be automated or semi-automated. This is at the heart
    of the enterprise space. And can make an enterprise experience more consumer-friendly.
  prefs: []
  type: TYPE_NORMAL
- en: Few-shot prompting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Doesn''t everyone do something better when shown an example beforehand? This
    book has hundreds of examples to support learning. LLMs are the same with complex
    tasks. When including RAG data, use examples to clarify understanding. A **few-shot
    prompt** consists of a few examples to prime the model and help it understand
    what is expected. The prompt gives the model a few shots (examples) to learn.
    As a preview of fine-tuning, few-shot learning is the simple version of fine-tuning.
    A prompt should not have 100 examples, but as explained later, it is better to
    include them in a fine-tuned model. Look at this example to see how the model
    will follow along:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Even a basic model such as the GPT 3.5 turbo (1106) is knowledgeable (newer
    models such as the GPT-4o mini were tested. They performed just as well but were
    wordier). It is impressive that it doesn’t fall for our mistake in classification,
    corrects it, and provides the proper classification for the next statement. Sentiment
    analysis is popular in service and sales use cases but is a simple example. Sentiment
    analysis for support cases is more nuanced. The issue isn’t their mood but what
    to do about it.
  prefs: []
  type: TYPE_NORMAL
- en: 'With recommenders, examples are provided to adapt its results to a similar
    approach. The format is for the model’s output in the program-aided language model
    expense assistant example. Think of this as a collection of formatted examples
    to form the LLM’s answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: If the prompt gets out of control and it is doing too much, consider breaking
    down cases to farm out tasks to specific models. This will be covered shortly,
    but fine-tuning in the next chapter is another option to reduce the prompt’s size
    and complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Why use GPT 3.5 Turbo 1106? There are better models
  prefs: []
  type: TYPE_NORMAL
- en: The strategies and learnings of this book can be applied to any modern model.
  prefs: []
  type: TYPE_NORMAL
- en: For the examples shared, the ability to use larger context windows, output large
    datasets, and performance are not factors. 1106 is roughly 10x cheaper than GPT-4,
    30x more affordable than GPT-4o mini, and less expensive than Claude 2, Llama,
    and Gemini 1.5 Pro (September 2024). Learn and practice without worrying about
    hefty bills. Invest in the right model quality and cost balance for actual models.
    There is no magic flow chart to determine the right fit. It is about testing,
    experimentation, researching what others have found, and understanding the use
    case, amount of use, and performance needs. For some use cases, running a model
    on a local system is possible with modern hardware. OpenAI doesn’t have this,
    but some open-source models do. With the introduction of GPT-4o mini, the costs
    continue to come down. It is one-third the price of GPT-3.5 Turbo. [*Chapter 8*](B21964_08.xhtml#_idTextAnchor172),
    *Fine-Tuning,* will explain the costs of running a fine-tuned model, which is
    more than the equivalent generic model.
  prefs: []
  type: TYPE_NORMAL
- en: This is just a glimpse into some methods to build effective prompts. A few were
    left out, as they are discussed in Andrew Ng’s talk.
  prefs: []
  type: TYPE_NORMAL
- en: Andrew Ng’s agentic approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a wealth of videos and tutorials out there. Andrew NGs are recommended
    because of his long history in the space and the trust he has garnered. You may
    already know Andrew NG or follow his AI discussions. In the following video, he
    discusses a few critical design patterns.
  prefs: []
  type: TYPE_NORMAL
- en: 'Video: [Andrew NG Agentic Presentation](https://www.youtube.com/watch?v=sal78ACtGTc)
    ([https://www.youtube.com/watch?v=sal78ACtGTc](https://www.youtube.com/watch?v=sal78ACtGTc))'
  prefs: []
  type: TYPE_NORMAL
- en: 'I delayed discussing these in the previous section to include them here. This
    will reinforce the concept that there are many approaches to solving problems
    and that no single approach will work for all solutions:'
  prefs: []
  type: TYPE_NORMAL
- en: Reflection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tool use
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Planning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-agent collaboration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many of these are essential to our prompt engineering and nuanced tuning discussion.
    Let’s explore each of these approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Reflection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is a great approach. Take the output from the LLM and ask it to think more
    deeply about refining it. If it is sent back to the same LLM, this is called self-reflection.
    However, one model can also be used with a second model; this would be reflection.
    The Wove case study used multiple models in their flow.
  prefs: []
  type: TYPE_NORMAL
- en: Andrew suggests the following articles to learn more about reflection, a form
    of chaining. Although they appear a little technical, the concepts of self-reflection
    and the examples are easy to follow. They cover a variety of use cases and have
    good examples. Madaan et al. recognize that the iterative approach works wonders
    in a space like enterprise solutions with intricate requirements and hard-to-define
    goals. As with support calls and customer service, the original question isn’t
    going to get a suitable answer. It can take dozens of interactions to frame a
    problem and find a solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [SELF-REFINE: Iterative Refinement with Self-Feedback](https://proceedings.neurips.cc/paper_files/paper/2023/file/91edff07232fb1b55a505a9e9f6c0ff3-Paper-Conference.pdf)
    by Madaan et al. ([https://proceedings.neurips.cc/paper_files/paper/2023/file/91edff07232fb1b55a505a9e9f6c0ff3-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2023/file/91edff07232fb1b55a505a9e9f6c0ff3-Paper-Conference.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: Madaan et al. also showed that the self-refinement process was more effective
    than asking a model to produce multiple outputs. Humans still preferred the refined
    results over all of the additionally generated outputs. *Figure 7**.4* shows self-reflection
    using the same model a second time. See how the same models can be chained together
    for further refinements.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_07_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 – An example of self-reflection using the same model again
  prefs: []
  type: TYPE_NORMAL
- en: Although many of these articles typically use coding examples, focusing on the
    examples of decision-making or reasoning is better for finding enterprise value.
    Shinn et al. covers a technical discussion. Consider adding value by validating
    whether the development team uses reflection in its prompts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [Reflexion: Language Agents with Verbal Reinforcement Learning](https://arxiv.org/pdf/2303.11366.pdf)
    by Shinn et al. ([https://arxiv.org/pdf/2303.11366.pdf](https://arxiv.org/pdf/2303.11366.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: An extensive collection of LLMs, tools, and services are used with any enterprise
    workflow to create a complete solution. Tools are part of this solution.
  prefs: []
  type: TYPE_NORMAL
- en: Tool use
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It has been repeatedly mentioned that building enterprise solutions requires
    a robust ecosystem. It is sometimes challenging to integrate third-party solutions
    into large enterprises. Cost, licensing issues, cloud access, authentication and
    security, and legal issues all get in the way. In an emerging field, it is unrealistic
    for most companies to do everything in-house. It is not expected to build the
    models in-house, so the tools should be the same. It is ideal to have a collection
    of tools for building the pipeline, monitoring, fine-tuning, documentation, and
    knowledge integration, not to mention the work to integrate internal services.
    Patil et al. references a few pieces of the puzzle to help write API calls. This
    is a big deal when accessing enterprise data. This is one of the areas developers
    should review.
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [Gorilla: Large Language Model Connected with Massive APIs](https://arxiv.org/pdf/2305.15334.pdf)
    by Patil et al. ([https://arxiv.org/pdf/2305.15334.pdf](https://arxiv.org/pdf/2305.15334.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: The second article of interest relates to vision integration. So far, we haven’t
    spent time on vision tools and use cases. That was intentional. However, vision
    tools have a place in enterprise solutions. They can interpret images such as
    receipts, invoices, contracts, and charts, analyze video analysis to count an
    inventory, identify people, classify or count objects in a shopping cart or a
    construction project, or keep track of tasks on an assembly line. There are plenty
    of places to integrate vision into an enterprise workflow. These will likely each
    have their collection of models, each playing a part in the vision analysis process,
    with a unique care and feeding life cycle. Yung et al. explores challenges in
    the multi-modal space.
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action](https://arxiv.org/pdf/2303.11381.pdf)
    by Yung et al. ([https://arxiv.org/pdf/2303.11381.pdf](https://arxiv.org/pdf/2303.11381.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: Model usage costs become only one factor as solutions scale up and other development
    expenses are included. Because ChatGPT is a variable cost that increases with
    volume, large users can negotiate better pricing as model fee structures mature.
    For open-source models, the team has to incur the cost to run the model, likely
    in silos if done for a customer, while larger shared instances might work for
    internal enterprise needs. It seems reasonable that a large enterprise with hundreds
    of internal processes will have thousands of active models.
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT must be integrated with other tools to support the concept of planning.
    There are now hundreds of tool providers. As the saying goes, enterprises want
    to *eat their own dog food*; they like to do all the work in-house and prefer
    not to use third-party tools. However, because of the speed of adoption, only
    some enterprises can build what they need from scratch. So, it is also essential
    to have a structure to support rapid decision-making, tool integration, and a
    reasonable licensing process.
  prefs: []
  type: TYPE_NORMAL
- en: Planning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a conversational assistant case, the challenge is to provide instructions
    and examples that support CoT prompting. It is easier to use these methods directly
    when a system provides all the prompting. Andrew referenced Wei et al. to help
    understand CoT prompting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/pdf/2201.11903.pdf)
    by Wei et al. ([https://arxiv.org/pdf/2201.11903.pdf](https://arxiv.org/pdf/2201.11903.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: More interesting is the orchestration of various models and using a model to
    orchestrate itself, as described in this article by Shen et al.
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging
    Face](https://arxiv.org/pdf/2303.17580.pdf) by Shen et al. ([https://arxiv.org/pdf/2303.17580.pdf](https://arxiv.org/pdf/2303.17580.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: This approach allows the assignment of specific tasks to an appropriate AI model.
    Like with Wove, it is expected to use different models tuned to solve particular
    problems. That leads us to a multi-agent solution, another way of approaching
    this problem.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-agent collaboration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is essential to apply the suitable model to a part of a problem and chain
    those models together to increase the overall quality of the solution. Create
    and use the correct test measurements and evaluate different quality and cost/benefit
    models.
  prefs: []
  type: TYPE_NORMAL
- en: There is ample evidence that these models perform much better when all of our
    tools, process improvements, and model choices are used to improve the solution.
  prefs: []
  type: TYPE_NORMAL
- en: The most exciting article from Qian et al. discusses the concept of a factory
    of agents. ChatDev is a solid idea and approach that can be adapted to any generative
    AI solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [Communicative Agents for Software Development](https://arxiv.org/pdf/2307.07924.pdf)
    by Qian et al. ([https://arxiv.org/pdf/2307.07924.pdf](https://arxiv.org/pdf/2307.07924.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: I can’t resist showing their ChatDev diagram in *Figure 7**.5*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_07_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5 – ChatDev, a chat-powered framework using LLM agents in professional
    roles
  prefs: []
  type: TYPE_NORMAL
- en: ChatDev allows a unique collection of agents to handle each development process
    job. Thus, they can have their own opinions on the design, coding, testing, and
    documentation because they are trained and focused on different tasks. This is
    similar to the Wove use case, which uses various models to perform specific functions
    in its workflow. It is scary to think humans can all be replaced by virtual agents,
    but the reality is that some of this is real today. Be aware that this approach
    allows for independent analysis from these various groups. Although this might
    not be the most suitable collection of agents for the use case, it should help
    generate a few ideas on where to use agents to improve the results from a single
    (unchecked) LLM. If this were diagrammed like shown with self-reflection, it would
    look like a hub and spoke diagram, with direct connections between the various
    process steps (from designing to coding to testing, for example).
  prefs: []
  type: TYPE_NORMAL
- en: Check out the appendix at the end of the ChatDev article. It shows the virtual
    talent pool’s roles and responsibilities and discusses the process understood
    by each role. It is just fascinating. I have yet to try the game, so whether it
    creates a compelling user experience is unknown. But it is always best to know
    about these approaches so that a virtual agent doesn’t replace you in a job!
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: AutoGen: [Enabling Next-Gen LLM Applications via Multi-Agent Conversation](https://arxiv.org/pdf/2308.08155.pdf)
    by Wu et al. ([https://arxiv.org/pdf/2308.08155.pdf](https://arxiv.org/pdf/2308.08155.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: Advanced techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although most of these are covered in the *Prompt Engineering Guide*, one additional
    technique is worth mentioning. Miguel Neves mentions this technique in the following
    article. The article is being maintained, so it might have some new techniques
    when viewed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [A guide to prompt techniques](https://www.tensorops.ai/post/prompt-engineering-techniques-practical-guide)
    by Miguel Neves ([https://www.tensorops.ai/post/prompt-engineering-techniques-practical-guide](https://www.tensorops.ai/post/prompt-engineering-techniques-practical-guide))'
  prefs: []
  type: TYPE_NORMAL
- en: Miguel references Emotion Prompts, which involve putting pressure on a model
    and instructing it that its results are essential to the person. The original
    research is worth reviewing.
  prefs: []
  type: TYPE_NORMAL
- en: Strategy – emotional prompting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Cheng Li, Jindong Wang, and their co-authors have researched methods to improve
    prompts by encouraging urgency. This is achieved by building emotive prompts into
    the queries. Using emotive prompts can boost performance on a variety of benchmarks.
    Consider testing this language in instructions to increase performance, truthfulness,
    and informativeness. Given our previous recommendation around limited niceties,
    the tested LLMs respond more effectively based on this approach. It works with
    humans, and it turns out it works with LLMs. Li shared the example in *Figure
    7**.6*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [Improving LLMS with emotional prompts](https://arxiv.org/pdf/2307.11760)
    by Cheng Li et al. ([https://arxiv.org/pdf/2307.11760](https://arxiv.org/pdf/2307.11760))'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_07_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.6 – Applying an emotive improvement to prompts
  prefs: []
  type: TYPE_NORMAL
- en: 'They validated their answers with over 100 human subjects. They provided responses
    that were prompted with and without the emotive prompt. This prompt was used as
    a baseline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The two prompts that did the best on benchmarks included the emotive prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This research had much to digest, but ChatGPT and other large models responded
    best to these prompts. Jindong also suggested sharing this additional research.
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [The Good, The Bad, and Why? Unveiling Emotions in Generative AI](https://arxiv.org/pdf/2312.11111)
    ([https://arxiv.org/pdf/2312.11111](https://arxiv.org/pdf/2312.11111))'
  prefs: []
  type: TYPE_NORMAL
- en: In this article, they also explore EmotionAttack and EmotionDecode. The former
    can impair the performance of an AI model, while the latter can help explain the
    effects of emotional stimuli. Check it out to delve deeper into this area of prompt
    engineering.
  prefs: []
  type: TYPE_NORMAL
- en: So far, tweaking words in prompts was covered. However, there are methods to
    adjust the parameters used by the models.
  prefs: []
  type: TYPE_NORMAL
- en: Strategy – adjusting ChatGPT parameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Parameters are available depending on a model’s release. The ChatGPT Playground
    provides **Temperature** and **Top P** control, as seen in *Figure 7**.7*.
  prefs: []
  type: TYPE_NORMAL
- en: '| ![](img/B21964_07_07.jpg) | **Instructions** insulate and wrap the prompts
    to give control over the results.**Temperature** ranges from 0 to 2\. It controls
    the randomness of the results. At zero, it would be repetitive and deterministic
    – boring, if you will. Choose a lower value than the default for more professional
    responses.**Top P** ranges from 0 to 1\. This is based on something called nucleus
    sampling. The higher the value, the more unlikely the possible choices, the more
    diverse the results. Lower values mean more confident results. For example, **Top
    P** at 90% means that it will only draw choices from 90% of the tokens. That means
    any long tail of random results in the bottom 10% will be ignored.The best practice
    is to only alter **Temperature** or **Top P**, but not both. |'
  prefs: []
  type: TYPE_TB
- en: Figure 7.7 – The Temperature and Top P parameters are available in the Playground
  prefs: []
  type: TYPE_NORMAL
- en: 'The best way to get a feel for **Temperature** and **Top P** is to play with
    them in the Playground:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to the Playground and the **Completion** tab.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Demo: [Playground for learning about Temperature and Top P](https://platform.openai.com/playground/complete)
    ([https://platform.openai.com/playground/complete](https://platform.openai.com/playground/complete))'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Set the **Show probabilities** dropdown on the settings panel to **Full spectrum**,
    as shown in *Figure 7**.8*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B21964_07_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.8 – Setting up the Full spectrum option
  prefs: []
  type: TYPE_NORMAL
- en: Set the M**aximum length** to 10 for a simple response without wasting money,
    as shown in *Figure 7**.9*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B21964_07_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.9 – Setting up the Maximum length setting
  prefs: []
  type: TYPE_NORMAL
- en: Type in a statement in the **Playground** field, as shown in *Figure 7**.10*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B21964_07_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.10 – Enter this example phrase
  prefs: []
  type: TYPE_NORMAL
- en: Click **Submit**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: View the completion results and see the likelihood of a token being selected,
    as shown in *Figure 7**.11*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B21964_07_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.11 – Showing the completion probabilities
  prefs: []
  type: TYPE_NORMAL
- en: If **Top P** is reduced to zero and returns results, the tokens it picked from
    are more limited, as shown in *Figure 7**.12*. The choices represent over 90%
    of the possible options. Compare that to the preceding figure, where the top 11
    options only covered 41.72% of the possibilities.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B21964_07_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.12 – The change in results with adjustments in Top P
  prefs: []
  type: TYPE_NORMAL
- en: Changing **Temperature** to zero gives more consistent results, as shown in
    *Figure 7**.13*. Try it multiple times and repeatedly see some of the same results.
    This will provide the best possible paths that a model can deliver.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B21964_07_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.13 – The change in results with adjustments in Temperature
  prefs: []
  type: TYPE_NORMAL
- en: Move **Temperature** up to 2\. The results will appear a little insane, as shown
    in *Figure 7**.13*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B21964_07_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.14 – A wacky response when Temperature is raised to 2
  prefs: []
  type: TYPE_NORMAL
- en: Continue to play with examples and see how these parameters change how the model
    picks tokens. When creating a real solution, adjust these defaults only after
    being comfortable with how the mode reacts to prompts and fine-tuning.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Other options might exist depending on the model. Check out this article for
    more background on parameters such as stop sequences (to keep lists short), frequency
    penalty, and presence penalty.
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [Prompt Engineering Guide](https://www.promptingguide.ai/introduction)
    ([https://www.promptingguide.ai/introduction](https://www.promptingguide.ai/introduction))'
  prefs: []
  type: TYPE_NORMAL
- en: I recommend this guide. It is easy to spend months learning about prompt engineering.
    It has many examples and over a dozen popular techniques to improve prompts or
    instructions. This is the top recommended reference for prompt engineering. However,
    new strategies, including multi-modal prompting, are to be considered as models
    change and adapt.
  prefs: []
  type: TYPE_NORMAL
- en: Strategy – multi-modal prompting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Keeping up with the evolution of generative AI in a book is challenging. With
    models now supporting inputs in various modalities, text, images, and voice, solutions
    to use cases can also adapt. The example of expense receipt scanning is an excellent
    example of multi-modal interaction (to go with the SoundHound example from an
    earlier chapter). Parsing and understanding the image of a receipt and combining
    that with voice or text interactions is a compelling use case. The enterprise
    space has a lot of exciting use cases that these improvements in model processing
    can support. Google does an excellent job of giving us the basics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [Google explains multi-modal prompting for Gemini](https://developers.google.com/solutions/content-driven/ai-images)
    ([https://developers.google.com/solutions/content-driven/ai-images](https://developers.google.com/solutions/content-driven/ai-images))'
  prefs: []
  type: TYPE_NORMAL
- en: Inventory management comes to mind with this strategy. Isn’t it easier to take
    a picture of a shelf and have it count the items rather than counting them manually?
    Or should a model read handwriting in real time to help perform calculations,
    chart graphs, and interpret results? The various sciences have many uses for image
    classification, recognition, interpretation, and reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: So, by building on use cases that require image analysis, voice interaction,
    or handwriting recognition, adapt prompts and instructions to support multi-modal
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Combine the COT method with multi-modal data and improve the output quality
    by stepping through a process to get an answer. This step-wise progression allows
    information analysis to form context and support a follow-up question with this
    more robust understanding.
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [Language Is Not All You Need](https://arxiv.org/pdf/2302.14045):
    Aligning Perception with Language Models ([https://arxiv.org/pdf/2302.14045](https://arxiv.org/pdf/2302.14045))'
  prefs: []
  type: TYPE_NORMAL
- en: What is also interesting is that all of these methods are tools that can be
    applied on top of each other. Think about the training that Telsa must do to understand
    the scenes for self-driving. Or Google Lens, with its deep learning models, constantly
    recognizes strange items thrown at it. Incorporating models that do these tasks
    outside text recognition is found throughout enterprise use cases. Few-shot learning
    helps improve accuracy since the types of pictures needed for analysis might be
    outside a basic model. Build a fine-tuned model with lots of examples. If counting
    inventory, give examples with results. For managing receipts, gather various examples
    in different formats, such as handwritten receipts, receipts in other languages
    and currencies, MM/YY and YY/MM date formats, receipts from emails, etc. Thousands
    of receipts per language might be needed. When doing product or item recognition,
    consider angles and placement other than the traditional orientation, lighting
    conditions, and distractors in the image field. There are many examples. All of
    these assume additional training is required for the model. This is the value
    of the enterprise data. Without this new data, the model would not have been successful.
    Training is needed even with third-party tools and other models, which might also
    be faster and easier to manage.
  prefs: []
  type: TYPE_NORMAL
- en: Third-party prompt frameworks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: No one can predict the wealth of third-party tools and products built on top
    of ChatGPT and the other LLMs. Every day, new innovative tools appear. Some tools
    help avoid the complexities of directly working with the model. If these tools
    can focus on providing high-quality customer results and mask or enrich the flexibility
    needed to solve these problems, work them into your process. One good example
    of a robust tool on top of the models is in the Salesforce demos, as shown in
    *Figure 7**.15*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_07_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.15 – An example of the Einstein prompt template injecting enterprise
    data into the context
  prefs: []
  type: TYPE_NORMAL
- en: In Salesforce, a customer can create a prompt template and embed the data source
    elements as variables in the prompt. Thus, they can customize instructions to
    provide the style, tone, and persona insight needed to craft messages to prospects.
  prefs: []
  type: TYPE_NORMAL
- en: Salesforce’s documentation covers crafting a prompt correctly and the guidelines
    similar to what is discussed here. Just because a guideline is in documentation
    doesn’t mean that customers will follow it. The next iteration of this prompt
    workspace could benefit from a recommender UI that understands these guidelines
    and catches prompts that don’t conform.
  prefs: []
  type: TYPE_NORMAL
- en: 'Documentation: [Ingredients of a Prompt Template](https://help.salesforce.com/s/articleView?id=sf.prompt_builder_template_ingredients.htm)
    (from Salesforce) ([https://help.salesforce.com/s/articleView?id=sf.prompt_builder_template_ingredients.htm](https://help.salesforce.com/s/articleView?id=sf.prompt_builder_template_ingredients.htm))'
  prefs: []
  type: TYPE_NORMAL
- en: Salesforce spends time getting its prompt template right, so there is one more
    helpful resource worth reviewing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Documentation: [Guide to the prompt builder](https://admin.salesforce.com/blog/2024/the-ultimate-guide-to-prompt-builder-spring-24)
    (from Salesforce) ([https://admin.salesforce.com/blog/2024/the-ultimate-guide-to-prompt-builder-spring-24](https://admin.salesforce.com/blog/2024/the-ultimate-guide-to-prompt-builder-spring-24))'
  prefs: []
  type: TYPE_NORMAL
- en: Regardless of the tools, care for and feed the LLM to improve the output. Adopting
    prompt engineering techniques can make significant improvements. Though these
    methods can reach their limits, other methods and tricks can be used instead.
  prefs: []
  type: TYPE_NORMAL
- en: Addressing the lost in the lost-the-middle problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have spent considerable effort in this book addressing how to handle hallucinations.
    This is what the industry, the media, and engineers like to talk about. This is
    likely because they can be tracked, and there are many methods to improve hallucinations.
    Not all issues are easy to explain and fix. The **lost-in-the-middle** problem
    refers to the tendency of LLMs to lose coherence and context when generating or
    processing information, especially in the *middle* of long texts or dialogues.
    It’s like when a newscaster asks a series of questions at one time, and the interviewee
    answers the first and last questions but can’t remember the one in the middle.
    Models have this same problem.
  prefs: []
  type: TYPE_NORMAL
- en: Nelson Liu’s paper documents a significant drop in accuracy when information
    is in the middle of a document. The chart in *Figure 7**.16* is almost scary.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_07_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.16 – The location of relevant information in the input context matters
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [Lost in the Middle: How Language Models Use Long Contexts](https://cs.stanford.edu/~nfliu/papers/lost-in-the-middle.arxiv2023.pdf)
    ([https://cs.stanford.edu/~nfliu/papers/lost-in-the-middle.arxiv2023.pdf](https://cs.stanford.edu/~nfliu/papers/lost-in-the-middle.arxiv2023.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: A 10 to 20% or more drop in accuracy is a big deal. This becomes a tradeoff.
    Creating a large context window can lead to a lost-in-the-middle problem. It can
    come up in the testing or, more likely, when monitoring logs. If RAG provides
    extensive content or the conversation gets extended, as new information comes
    in, there might be less room to hold the context of early details.
  prefs: []
  type: TYPE_NORMAL
- en: For now, it is second only to hallucinations to the headaches it provides. If
    this issue appears, brainstorm strategies with your team to mitigate information
    being lost in the middle. One idea is to use function calls to construct or re-construct
    the context window with critical information. Or use an intermediate model to
    summarize the context window to create a smaller, newer context to continue the
    thread. This is an emerging problem for the community and the foundation model
    vendors. It is likely above the call of duty for our readers, but those monitoring
    logs can notice it, so knowing about it is half the battle. Recall that we can
    only bring so much knowledge into the context window with RAG. We must monitor
    context window size when bringing in RAG data, adding contextual information from
    other sources, and including space for our prompt engineering. The answer might
    be in the middle of an RAG document; thus, we will see a reduction in the likelihood
    of giving the correct answer.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we must allow the context window to grow throughout a conversation.
    Last we checked, there was no one good answer, so keep an eye out to identify
    this issue. If the model is losing sight of the purpose of the chat, this might
    be why.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is much to learn with prompt engineering, but it should be clear why these
    instructions are essential to give models context, direction, guidance, and style.
    This process is an emerging art, as only some things can be easily explained.
    This chapter covered examples of prompt engineering well-grounded in scientific
    exploration, even if the topic is less than deterministic, such as emotive prompting.
  prefs: []
  type: TYPE_NORMAL
- en: Contribute to the process by helping to define and improve these task flows
    through use case expertise, creating, verifying, and editing prompts, testing
    various prompts, and monitoring whether changes move the solutions in the right
    direction. Go forth and prompt!
  prefs: []
  type: TYPE_NORMAL
- en: With the basics of prompt engineering, [*Chapter 8*](B21964_08.xhtml#_idTextAnchor172),
    *Fine-Tuning*, can fill in some gaps and add a cost-effective and accurate method
    for teaching the model more refined responses when it encounters specific tasks.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '| ![](img/B21964_07_QR_Scanner.jpg) | The links, book recommendations, and
    GitHub files in this chapter are posted on the reference page.Web Page: [Chapter
    7 References](https://uxdforai.com/references#C7) ([https://uxdforai.com/references#C7](https://uxdforai.com/references#C7))
    |'
  prefs: []
  type: TYPE_TB
