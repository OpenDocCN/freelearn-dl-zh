- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Picking Up and Putting Away Toys using Reinforcement Learning and Genetic Algorithms
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用强化学习和遗传算法捡起和放回玩具
- en: This chapter is where the robots start to get challenging – and fun. What we
    want to do now is have the robot’s manipulator arm start picking up objects. Not
    only that, but instead of preprogramming arm moves and grasping actions, we want
    the robot to be able to learn how to pick up objects, and how to move its arm
    without hitting itself.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章是机器人变得具有挑战性和有趣的地方。我们现在想要让机器人的操作臂开始捡起物体。不仅如此，我们希望机器人能够学会如何捡起物体，以及如何移动它的手臂而不会撞到自己。
- en: How would you teach a child to pick up toys in their room? Would you offer a
    reward for completing the task, such as “*If you pick up your toys, you will get
    a treat?*” Or would you offer a threat of punishment, such as “*If you don’t pick
    up your toys, you can’t play games on your tablet.*” This concept, offering positive
    feedback for good behavior and negative feedback for undesirable actions, is called
    **reinforcement learning**. That is one of the ways we will train our robot in
    this chapter.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 你会如何教一个孩子在他们房间里捡起玩具？你会为完成任务提供奖励，比如“*如果你捡起你的玩具，你将得到一份奖励？*”或者你会提供惩罚的威胁，比如“*如果你不捡起你的玩具，你就不能在你的平板电脑上玩游戏.*”这个概念，为良好的行为提供正面反馈，为不良行为提供负面反馈，被称为**强化学习**。这是我们本章将要训练机器人的方法之一。
- en: If this sounds something like a game, where you get positive points for reaching
    a goal and lose points for missing a goal, then you are right. We have some concept
    of winning that we are trying to achieve, and we create some sort of point system
    to reinforce – that is to say, reward – behavior when the robot does what we want
    it to.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要机器人臂来执行代码，你需要一个机器人臂。我使用的是从Amazon.com购买的**LewanSoul Robot xArm**。这个臂使用数字伺服电机，这使得编程变得容易得多，并为我们提供了位置反馈，这样我们就知道手臂在什么位置。我购买的臂可以在出版时在[http://tinyurl.com/xarmRobotBook](http://tinyurl.com/xarmRobotBook)找到。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Designing the software
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计软件
- en: Setting up the solution
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置解决方案
- en: Introducing Q-learning for grasping objects
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍用于抓取物体的Q学习
- en: Introducing **genetic algorithms** (**GAs**) for path planning
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍用于路径规划的**遗传算法**（**GAs**）
- en: Alternative robot arm ML approaches
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 替代机器人臂机器学习方法
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The exercise in this chapter does not require any new software or tools that
    we haven’t already seen in previous chapters. We will start by using Python and
    ROS 2\. You will need an IDE for Python (IDLE or Visual Studio Code) to edit the
    source code.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的练习不需要任何我们在前几章中没有见过的新的软件或工具。我们将首先使用Python和ROS 2。你需要一个Python的IDE（IDLE或Visual
    Studio Code）来编辑源代码。
- en: Since this chapter is all about moving the robot arm, you will need a robot
    arm to execute the code. The one I used is the **LewanSoul Robot xArm**, which
    I purchased from Amazon.com. This arm uses digital servos, which makes the programming
    much easier, and provides us with position feedback, so we know what position
    the arm is in. The arm I purchased can be found at [http://tinyurl.com/xarmRobotBook](http://tinyurl.com/xarmRobotBook)
    at the time of publication.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这听起来像是一款游戏，你在达到目标时获得正分，错过目标时失去分数，那么你就对了。我们有一些想要实现的胜利概念，我们创建了一种某种类型的点系统来强化——也就是说，奖励——当机器人做我们希望它做的事情时。
- en: Note
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If you don’t want to buy a robot arm (or can’t), you can run this code against
    a simulation of a robot arm using ROS 2 and **Gazebo**, a simulation engine. You
    can find instructions at [https://community.arm.com/arm-research/b/articles/posts/do-you-want-to-build-a-robot](https://community.arm.com/arm-research/b/articles/posts/do-you-want-to-build-a-robot).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不想购买机械臂（或者不能购买），你可以使用ROS 2和**Gazebo**（一个仿真引擎）的机械臂仿真来运行此代码。你可以在此处找到说明：[https://community.arm.com/arm-research/b/articles/posts/do-you-want-to-build-a-robot](https://community.arm.com/arm-research/b/articles/posts/do-you-want-to-build-a-robot)。
- en: You’ll find the code for this chapter in the GitHub repository for this book,
    at [https://github.com/PacktPublishing/Artificial-Intelligence-for-Robotics-2e](https://github.com/PacktPublishing/Artificial-Intelligence-for-Robotics-2e).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在本书的GitHub仓库中找到本章的代码，网址为：[https://github.com/PacktPublishing/Artificial-Intelligence-for-Robotics-2e](https://github.com/PacktPublishing/Artificial-Intelligence-for-Robotics-2e)。
- en: Task analysis
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 任务分析
- en: 'Our tasks for this chapter are pretty straightforward. We will use a robot
    arm to pick up the toys we identified in the previous chapter. This can be divided
    into the following tasks:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的任务相当直接。我们将使用机器人手臂来拿起我们在上一章中确定的小玩具。这可以分为以下任务：
- en: First, we build an interface to control the robot arm. We are using **ROS 2**
    to connect the various parts of the robot together, so this interface is how the
    rest of the system sends commands and receives data from the arm. Then we get
    into teaching the arm to perform its function, which is picking up toys. The first
    level of capability is picking up or grasping toys. Each toy is slightly different,
    and the same strategy won’t work every time. Also, the toy might be in different
    orientations, so we have to adapt to how the toy is presented to the robot’s end
    effector (a fancy name for its hand). So rather than write a lot of custom code
    that may or may not work all the time, we want to create a structure so that the
    robot can learn for itself.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我们构建了一个控制机器人手臂的界面。我们使用**ROS 2**将机器人的各个部分连接起来，因此这个界面是系统其余部分向手臂发送命令和接收数据的方式。然后我们开始教手臂执行其功能，即拿起玩具。能力的第一级是拿起或抓取玩具。每个玩具都略有不同，相同的策略并不总是有效。此外，玩具可能处于不同的方向，因此我们必须适应玩具呈现给机器人末端执行器（即其手的别称）的方式。因此，我们不想编写大量的可能或可能不起作用的定制代码，而是想创建一个结构，使机器人能够自我学习。
- en: The next problem we face is to have the arm move. It’s not just that the arm
    has positions, but it also has to have a path from a start point to an end point.
    The arm is not a monolithic part – it’s composed of six different motors (as shown
    in *Figure 5**.3*) that each do something different. Two of the motors – the grip
    and the wrist – don’t move the arm at all; they only affect the hand. So our arm
    path is controlled by four motors. The other big problem is that the arm can collide
    with the body of the robot if we are not careful, so our path planning for the
    arm has to avoid collisions.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们面临下一个问题是要让手臂移动。问题不仅仅是手臂有位置，它还需要从一个起点到一个终点的路径。手臂不是一个单一的部分——它由六个不同的电机组成（如图**5.3**所示），每个电机都执行不同的功能。其中两个电机——握持和手腕——根本不会移动手臂；它们只影响手部。因此，我们的手臂路径由四个电机控制。另一个大问题是，如果我们不小心，手臂可能会与机器人的身体碰撞，所以我们的手臂路径规划必须避免碰撞。
- en: We will use a completely different technique for learning arm paths. A **GA**
    is a technique for machine learning that uses an analog of evolution to *evolve*
    complex behaviors out of simple movements.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将使用一种完全不同的技术来学习手臂路径。**GA**是一种机器学习技术，它使用进化的模拟来从简单的动作中**进化**出复杂的行为。
- en: Now let’s talk a bit first about what we have to work with. We have a `600`,
    and (after a short time interval to permit the motor to move) we see that the
    servo position is `421`, then something is preventing the motor from reaching
    the goal we set for it. This information will be very valuable for training the
    robot arm.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们先谈谈我们必须要处理的事情。我们有一个`600`，（在允许电机移动的短暂时间间隔后）我们看到伺服位置是`421`，然后有东西阻止电机达到我们为其设定的目标。这些信息对于训练机器人手臂将非常有价值。
- en: We can use **forward kinematics**, which means summing up all the angles and
    levers of the arm to deduce where the hand is located (I’ll provide the code for
    that later in the chapter). We can use this hand location as our desired state
    – our **reward criteria**. We will give the robot points, or rewards, based on
    how close the hand is to the desired position and orientation we want. We want
    the robot to figure out what it takes to get to that position. We need to give
    the robot a way to test out different theories or actions that will result in
    the arm moving.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用**正向运动学**，这意味着将手臂的所有角度和杠杆相加，以推断出手的位置（我将在本章后面提供相应的代码）。我们可以将这个手的位置作为我们的期望状态——我们的**奖励标准**。我们将根据手部与期望位置和方向之间的接近程度给机器人评分，或给予奖励。我们希望机器人能够找出达到该位置所需的方法。我们需要为机器人提供一种测试不同理论或行动的方法，这些行动会导致手臂移动。
- en: We will begin by just working with the robot hand, or to use the fancy robot
    term, the **end effector**.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先与机器人手部进行工作，或者用时髦的机器人术语，称为**末端执行器**。
- en: 'The following diagram shows how we are trying to align our robot arm to pick
    up a toy by rotating the wrist:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了我们是如何尝试通过旋转手腕来调整我们的机器人手臂以拿起玩具的：
- en: '![Figure 5.1 – Storyboard for picking up a toy](img/B19846_05_1.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图5.1 – 拿起玩具的故事板](img/B19846_05_1.jpg)'
- en: Figure 5.1 – Storyboard for picking up a toy
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1 – 拿起玩具的故事板
- en: For grasping, we have three actions to work with. We position the arm to pick
    up the toy, we adjust the angle of the hand by rotating the hand with the wrist
    servo, and we close the hand in order to grasp the object. If the hand closes
    completely, then we missed the toy and the hand is empty. If the toy is keeping
    the gripper from closing because we picked it up, then we have success and have
    grabbed the toy. We’ll be using this process to teach the robot to use different
    hand positions to pick up toys based on their shape.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于抓取，我们有三个动作可以操作。我们将机械臂定位以拾取玩具，通过旋转手腕伺服电机调整手的角位，并关闭手以抓取物体。如果手完全关闭，那么我们错过了玩具，手是空的。如果玩具阻止夹爪关闭，因为我们已经拾取了它，那么我们就成功了，已经抓到了玩具。我们将使用这个过程来教会机器人使用不同的手部位置根据玩具的形状来拾取玩具。
- en: Designing the software
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 软件设计
- en: 'The first steps in designing the robot arm control software are to establish
    a coordinate frame (how we measure movement), after which we set up our solution
    space by creating states (arm positions) and actions (movements that change positions).
    The following diagram shows the coordinate frame for the robot arm:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 设计机械臂控制软件的第一步是建立一个坐标系（我们如何测量运动），然后我们通过创建状态（机械臂位置）和动作（改变位置的运动）来设置我们的解决方案空间。以下图显示了机械臂的坐标系：
- en: '![Figure 5.2 – Robot arm coordinate frame](img/B19846_05_2.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图5.2 – 机械臂坐标系](img/B19846_05_2.jpg)'
- en: Figure 5.2 – Robot arm coordinate frame
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2 – 机械臂坐标系
- en: Let’s define the coordinate frame of the robot – our reference that we use to
    measure movement – as shown in the preceding diagram. The X direction is toward
    the front of the robot, so movement forward and backward is along the X-axis.
    Horizontal movement (left or right) is along the Y-axis. Vertical movement (up
    and down) is in the Z direction. We place the zero point – the origin of our coordinates
    – down the center of the robot arm with zero Z (Z=0) on the floor. So, if I say
    the robot hand is moving positively in X, then it is moving away from the front
    of the robot. If the hand (the end of the arm) is moving in Y, then it is moving
    left or right.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义机器人的坐标系——我们用来测量运动的参考——如图所示的前图。X方向朝向机器人的前方，因此前后移动沿着X轴。水平移动（左或右）沿着Y轴。垂直移动（上下）在Z方向。我们将零点——我们坐标的原点——放置在机械臂中心的下方，Z=0在地板上。因此，如果我说机器人手部在X轴上正向移动，那么它是在远离机器人的前方移动。如果手（手臂的末端）在Y轴上移动，那么它是在向左或向右移动。
- en: 'Now we must have a set of names that we will call the servo motors in the arm.
    We’ll do a bit of anthropomorphic naming, and give the arm parts anatomical titles.
    The motors are numbered in the control system and the servos on my robot arm are
    labeled:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们必须有一组名称，我们将用它来称呼机械臂中的伺服电机。我们将进行一些拟人化命名，并给机械臂的各个部分赋予解剖学名称。电机在控制系统中编号，我机器人臂上的伺服电机标记如下：
- en: '*Motor 1* opens and closes the gripper. We may also call the gripper the hand.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*电机1* 控制夹爪的开启和关闭。我们也可以将夹爪称为手。'
- en: '*Motor 2* is wrist rotate, which rotates the hand.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*电机2* 是手腕旋转电机，它旋转手部。'
- en: '*Motor 3* is the wrist pitch (up and down) direction.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*电机3* 是手腕俯仰（上下）方向。'
- en: We’ll call *Motor 4* the elbow. The elbow flexes the arm in the middle, just
    as you expect.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将*电机4*称为肘部。肘部在中间弯曲手臂，正如你所期望的那样。
- en: '*Motor 5* is the shoulder pitch servo, which moves the arm up and down, rotating
    around the Y-axis when the arm is pointing straight ahead.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*电机5* 是肩部俯仰伺服电机，当机械臂指向正前方时，它使机械臂上下移动，绕Y轴旋转。'
- en: '*Motor 6* is at the base of the arm, so we’ll call it the shoulder yaw (right
    or left) servo. It rotates the entire arm about the Z-axis. I’ve decided not to
    move this axis, since the entire base of the robot can rotate due to the omni
    wheels. We’ll just move the arm up and down to simplify the problem. The navigation
    system we develop in [*Chapter 8*](B19846_08.xhtml#_idTextAnchor235) will point
    the arm in the correct direction.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*电机6* 位于机械臂的底部，因此我们将其称为肩部偏航（右或左）伺服电机。它绕Z轴旋转整个机械臂。我决定不移动这个轴，因为由于全向轮，整个机器人底座可以旋转。我们将只移动机械臂的上下位置以简化问题。我们在[*第8章*](B19846_08.xhtml#_idTextAnchor235)中开发的导航系统将使机械臂指向正确的方向。'
- en: 'We will start by defining an interface to the robot arm that the rest of the
    robot control system can use:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先定义一个机器人臂的接口，其余的机器人控制系统可以使用：
- en: '![Figure 5.3 – Robot arm motor nomenclature](img/B19846_05_3.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图5.3 – 机械臂电机命名](img/B19846_05_3.jpg)'
- en: Figure 5.3 – Robot arm motor nomenclature
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3 – 机械臂电机命名
- en: Here, pitch refers to up/down motion while yaw refers to right/left motion.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，俯仰指的是上下运动，而偏航指的是左右运动。
- en: 'We’ll use two terms that are common in the robot world to describe how we calculate
    where the arm is based on the data we have:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用在机器人世界中常见的两个术语来描述我们如何根据我们拥有的数据计算手臂的位置：
- en: '**Forward Kinematics** (**FK**) is the process of starting at the base of the
    robot arm and working out toward the gripper, calculating the position and orientation
    of each joint in turn. We take the position of the joint and the angle it is at,
    and add the length of the arm between that joint and the next joint. The process
    of doing this calculation, which produces an X-Y-Z position and a pitch-roll-yaw
    orientation of the end of the robot’s fingers, is called forward kinematics because
    we calculate forward from the base and out to the arm.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正向运动学**（**FK**）是从机器人手臂的基座开始，逐步计算出抓取器的位置和方向，依次计算每个关节的位置和方向的过程。我们取关节的位置和角度，并加上该关节与下一个关节之间的手臂长度。这个过程通过计算产生一个X-Y-Z位置和机器人手指末端的俯仰-滚转-偏航方向，称为正向运动学，因为我们是从基座向前计算到手臂的。'
- en: '**Inverse Kinematics** (**IK**) takes a different approach. We know the position
    and orientation of either where the hand is, or where we want it to be. Then we
    calculate backward up the arm to determine what joint angles would produce that
    hand position. IK is a bit trickier because there may be more than one solution
    (combination of joint positions) that may produce a given hand result. Try this
    with your own arm. Grasp a doorknob. Now move your arm while keeping your hand
    on the doorknob. There are multiple combinations of your joints that result in
    your hand being in the same position and orientation. We won’t be using IK here
    in this book, but I wanted you to be familiar with the term, which is often used
    in robot arms to drive the position of robot end effectors (grippers or hands).'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**逆向运动学**（**IK**）采取不同的方法。我们知道手的位置和方向，或者我们希望它在哪里。然后我们沿着手臂向后计算，以确定产生该手位置的关节角度。逆向运动学有点复杂，因为可能有多个解决方案（关节位置的组合）可以产生给定的手结果。用你自己的手臂试一试。抓住门把手。现在在保持手在门把手上的同时移动你的手臂。你的关节有多种组合可以使你的手保持在相同的位置和方向。在这本书中，我们不会使用逆向运动学，但我希望你对这个术语熟悉，它经常在机器人手臂中用来驱动机器人末端执行器（夹具或手）的位置。'
- en: For a more in-depth explanation of these concepts, you can refer to [https://control.com/technical-articles/robot-manipulation-control-with-inverse-and-forward-kinematics/](https://control.com/technical-articles/robot-manipulation-control-with-inverse-and-forward-kinematics/).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 若想对这些概念有更深入的解释，你可以参考[https://control.com/technical-articles/robot-manipulation-control-with-inverse-and-forward-kinematics/](https://control.com/technical-articles/robot-manipulation-control-with-inverse-and-forward-kinematics/)。
- en: Next, let’s discuss how we can put the arm in motion.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论如何使手臂运动起来。
- en: Setting up the solution
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置解决方案
- en: We will call the act of setting the motors to a different position an **action**,
    and we will call the position of the robot arm and hand the **state**. An action
    applied to a state results in the arm being in a new state.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把将电机设置到不同位置的行为称为**动作**，并将机器人手臂和手的位置称为**状态**。对一个状态应用动作会导致手臂进入一个新的状态。
- en: We are going to have the robot associate states (a beginning position of the
    hand) and an action (the motor commands used when at that state) with the probability
    of generating either a positive or negative **outcome** – we will be training
    the robot to figure out which sets of actions result in maximizing the **reward**.
    What’s a reward? It’s just an arbitrary value that we use to define whether the
    learning the robot accomplished was positive – something we wanted – or negative
    – something we did not want. If the action resulted in positive learning, then
    we increment the reward, and if it does not, then we decrement the reward. The
    robot will use an algorithm to both try and maximize the reward, and to incrementally
    learn a task.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将让机器人将状态（手的初始位置）和动作（在该状态下使用的电机命令）与产生正或负**结果**的概率相关联——我们将训练机器人找出哪些动作组合可以最大化**奖励**。奖励是什么？它只是一个任意值，我们用它来定义机器人完成的学习是积极的——我们想要的——还是消极的——我们不想要的。如果动作导致了积极的学习，那么我们就增加奖励，如果没有，那么我们就减少奖励。机器人将使用一个算法来尝试最大化奖励，并逐步学习一个任务。
- en: Let’s understand this process better by exploring the role played by machine
    learning.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过探索机器学习所扮演的角色来更好地理解这个过程。
- en: Machine learning for robot arms
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器人手臂的机器学习
- en: 'Since incremental learning was also part of neural networks, we will use some
    of the same tools we used before in our neural network to propagate a reward to
    each step in a chain of movements that result in the hand moving to some location.
    In reinforcement learning, this is called **discounting the reward** – distributing
    portions of rewards to the step in a multi-step process. Likewise, the combination
    of a state and an action is called a **policy** – because we are telling the robot,
    “when you are in this position, and want to go to that position, do this action.”
    Let’s understand this concept better by looking more closely at our process for
    learning with the robot arm:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 由于增量学习也是神经网络的一部分，我们将使用之前在神经网络中使用的一些相同工具，将奖励传播到导致手移动到某个位置的连续动作链中的每一步。在强化学习中，这被称为**折现奖励**——将奖励的部分分配给多步过程中的每一步。同样，状态和动作的组合称为**策略**——因为我们正在告诉机器人，“当你处于这个位置，并想要到达那个位置时，执行这个动作。”让我们通过更仔细地观察我们使用机器人手臂进行学习的流程来更好地理解这个概念：
- en: We set our goal position of the robot hand, which is the position of the robot
    hand in X and Z coordinates in millimeters from the rotational center of the arm.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们设定了机器人手的最终位置，即机器人手在X和Z坐标上相对于手臂旋转中心的毫米位置。
- en: The robot will try a series of movements to try and get close to that goal.
    We will not be giving the robot the motor positions it needs to get to that goal
    – the robot must learn. The initial movements will be totally randomly generated.
    We will restrict the delta movement (analogous to the learning rate from the previous
    chapter) to some small size so we don’t get wild flailing of the arm.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 机器人将尝试一系列动作，试图接近那个目标。我们不会给机器人提供到达那个目标所需的电机位置——机器人必须学习。初始动作将是完全随机生成的。我们将限制增量动作（类似于上一章中的学习率）的大小，以避免手臂剧烈挥动。
- en: At each incremental movement, we will score the movement based on whether or
    not the arm moved closer to the goal position.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每次增量动作中，我们将根据手臂是否更接近目标位置来评分该动作。
- en: The robot will remember these movements by associating the beginning state and
    the action (movement) with the reward score.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 机器人将通过将初始状态和动作（移动）与奖励评分关联来记住这些动作。
- en: Later, we will train a neural network to generate probabilities of positive
    outcomes based on the inputs of starting state and movement action. This will
    allow the arm to learn which sequences of movement achieve positive results. Then
    we will be able to predict which movement will result in the arm moving correctly
    based on the starting position.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，我们将训练一个神经网络，根据起始状态和动作输入生成积极结果的概率。这将使手臂能够学习哪些动作序列能够产生积极的结果。然后，我们将能够根据起始位置预测哪种动作会导致手臂正确移动。
- en: You can also surmise that we must add a reward for accomplishing the task quickly
    – we want the results to be efficient, and so we will add rewards for taking the
    shortest time to complete the task – or, you could say, we subtract a reward for
    each step needed to get to the goal so that the process with the fewest steps
    gets the most reward.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你也可以推测，我们必须为快速完成任务添加奖励——我们希望结果高效，因此我们将为完成任务的用时最短添加奖励——或者说，我们可以为达到目标所需的每一步减去奖励，这样步骤最少的流程将获得最多的奖励。
- en: 'We calculate rewards using the **Q-function**, as follows:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用**Q函数**来计算奖励，如下所示：
- en: '*Q = Q(s,a)+ (reward(s,a) + g ** *max(Q(s’,a’))*'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*Q = Q(s,a) + (reward(s,a) + g ** *max(Q(s’，a’))*'
- en: where *Q* represents the reward the robot will get (or expects to get) from
    a particular action. *Q(s,a)* is the final reward that we expect overall for an
    action given the starting state. *reward(s,a)* is the reward for that action (the
    small, incremental step we take now). *g* is a discount function that rewards
    getting to the goal quicker, that is, with a fewer number of steps (the more steps
    you have, the more *g* discounts (removes the reward)), and *max(Q(s’,a’))* selects
    the action that results in the largest reward out of the set of actions available
    at that state. In the equation, *s* and *a* represent the current state and action,
    and *s’* and *a’* represent the subsequent state and action, respectively. This
    is my version of Bellman’s equation for decision-making, with some adaptations
    for this particular problem. I added a discount for longer solutions (with more
    steps, thus taking longer to execute) to reward quicker arm movement (fewer steps),
    and left out the learning rate (alpha) as we are taking whole steps for each state
    (we don’t have intermediate states to learn).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*Q*代表机器人从特定动作获得的奖励（或期望获得的奖励）。*Q(s,a)*是在给定起始状态下，我们期望的该动作的最终奖励。*reward(s,a)*是该动作的奖励（我们现在采取的小增量步骤）。*g*是一个折扣函数，奖励更快到达目标，即以更少的步骤（步骤越多，*g*折扣（移除奖励）越多），*max(Q(s’，a’))*选择在那种状态下从可用动作集中产生最大奖励的动作。在方程中，*s*和*a*代表当前状态和动作，而*s’*和*a’*分别代表后续状态和动作。这是我针对决策问题的Bellman方程版本，进行了一些适应。我添加了对更长解决方案（更多步骤，因此执行时间更长）的折扣，以奖励更快的手臂移动（更少的步骤），并且省略了学习率（alpha），因为我们对每个状态都采取整步（我们没有中间状态来学习）。
- en: Next, let’s understand how we can teach the robot arm how to learn movement.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们了解如何教机器人手臂学习运动。
- en: How do we pick actions?
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们如何选择动作？
- en: 'What actions can the robot arm perform? As shown in *Figure 5**.3*, we have
    six motors, and we have three options for each motor:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 机器人手臂可以执行哪些动作？如*图5**.3所示，我们有六个电机，每个电机有三个选项：
- en: We can do nothing – that is, not move at all
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以什么都不做——也就是说，根本不移动
- en: We can move counterclockwise, which will make our motor angle smaller
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以逆时针移动，这将使我们的电机角度变小
- en: We can move clockwise, which makes our motor angle larger
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以顺时针移动，这使得我们的电机角度变大
- en: Note
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Most servo motors treat positive position changes as clockwise rotation. Thus,
    if we command the rotation to change from 200 to 250 degrees, the motor will turn
    clockwise 50 degrees.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数伺服电机将正位置变化视为顺时针旋转。因此，如果我们命令旋转从200度变为250度，电机将顺时针旋转50度。
- en: Our action space for each motion of the robot arm is to move each motor either
    clockwise, counterclockwise, or not at all. This gives us 729 combinations with
    6 motors (*3*6 possible actions). That is quite a lot. The software interface
    we are going to build refers to the robot arm motors by number with *1* being
    the hand and *6* being the shoulder rotation motor.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对机器人手臂每个动作的动作空间是移动每个电机顺时针、逆时针或根本不移动。这给我们提供了6个电机的729种组合（3^6种可能动作）。这相当多。我们将要构建的软件界面通过数字来引用机器人手臂的电机，*1*代表手，*6*代表肩部旋转电机。
- en: Let’s reduce this number and just consider the motions of three of the motors
    – the `[-1, 0, 1]`. We will use a value of just +/-1 or 0 in the action matrix
    to step the motors in small increments. The x-y coordinates of the hand can be
    computed from the sums of the angles of each joint times the length of the arm.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们减少这个数字，只考虑三个电机的运动——`[-1, 0, 1]`。我们将在动作矩阵中使用仅+/-1或0的值来以小增量移动电机。手部的x-y坐标可以通过每个关节角度的总和乘以手臂长度来计算。
- en: 'Here is a Python function to compute the position of the robot hand, given
    that each arm segment is 10 cm long. You can substitute the length of your robot
    arm segments. This function turns the motor angles representing the hand position
    from degrees into x-y coordinates in centimeters:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个Python函数，用于计算机器人手的位姿，假设每个手臂段长10厘米。你可以替换你机器人手臂段的长度。这个函数将代表手位姿的电机角度从度数转换为厘米的x-y坐标：
- en: '[PRE0]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The actions of the arm (possible movements) make up the action space of our
    robot arm, which is the set of all possible actions. What we will be doing in
    this chapter is investigating various ways of picking which action to perform
    and when in order to accomplish our tasks, and using machine learning to do it.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 手臂的动作（可能的移动）构成了我们机器人手臂的动作空间，即所有可能动作的集合。在本章中，我们将探讨各种选择执行哪个动作以及何时执行的方法，以便完成我们的任务，并使用机器学习来实现这一点。
- en: Another way of looking at this process is that we are generating a **decision
    tree**. You are probably familiar with the concept. We have a bit of a unique
    application when applying this to a robot arm, because our arm is a series of
    joints connected together, and moving one moves all of the other joints farther
    out on the arm. When we move Motor 5, Motors 4 and 3 move position in space, and
    their angles and distances to the ground and to our goal change. Each possible
    motor move adds 27 new branches to our decision tree, and can generate 27 new
    arm positions. All we have to do is pick which one to keep.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种看待这个过程的方式是我们正在生成一个**决策树**。你可能熟悉这个概念。当我们将这个概念应用到机器人臂时，我们有一个独特应用，因为我们的臂是一系列连接在一起的关节，移动一个关节会使其他所有关节在臂上向外移动。当我们移动电机
    5 时，电机 4 和 3 在空间中的位置会发生变化，它们与地面和我们的目标的角度和距离也会改变。每个可能的电机移动都会为我们的决策树添加 27 个新分支，并可以生成
    27 个新的臂位置。我们唯一要做的就是选择保留哪一个。
- en: The rest of this chapter will deal with just how we go about selecting our motions.
    It’s time to start writing some code now. The first order of business is to create
    an interface to the robot arm that the rest of the robot can use.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的其余部分将讨论我们如何选择动作。现在是时候开始编写一些代码了。首要任务是创建一个机器人臂的接口，以便机器人其余部分可以使用。
- en: Creating the interface to the arm
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建机器人臂的接口
- en: As previously noted, we are using ROS 2 as our interface service, which creates
    a **Modular Open System Architecture** (**MOSA**). This turns our components into
    *plug-and-play* devices that can be added, removed, or modified, much like the
    apps on a smartphone. The secret to making that happen is to create a useful,
    generic interface, which we will do now.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们使用 ROS 2 作为我们的接口服务，它创建了一个**模块化开放式系统架构**（**MOSA**）。这使得我们的组件变成了*即插即用*的设备，可以添加、删除或修改，就像智能手机上的应用程序一样。实现这一点的秘诀是创建一个有用的通用接口，我们现在将这样做。
- en: Note
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: I’m creating my own interface to ROS 2 that is just for this book. We won’t
    be using any other ROS packages with this arm – just what we create, so I wanted
    the very minimum interface to get the job done.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我正在为这本书创建自己的 ROS 2 接口。我们不会使用任何其他 ROS 包与这个臂一起使用——只使用我们创建的，所以我希望接口尽可能简单，以便完成这项工作。
- en: 'We’ll be creating this interface in Python. Follow these steps:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Python 创建此接口。请按照以下步骤操作：
- en: 'First, create a **package** for the robot arm in ROS 2\. A package is a portable
    organization unit for functionality in ROS 2\. Since we have multiple programs
    and multiple functions for the robot arm, we can bundle them together:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，在 ROS 2 中为机器人臂创建一个**包**。包是 ROS 2 中功能的一个可移植组织单元。由于我们为机器人臂有多个程序和多个功能，我们可以将它们捆绑在一起：
- en: '[PRE1]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We need to install the drivers for xArm so we can use them in Python:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要安装 xArm 的驱动程序，以便我们可以在 Python 中使用它们：
- en: '[PRE2]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now we go to our new source directory:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们转到我们的新源目录：
- en: '[PRE3]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Open the editor and let’s start coding. First, we need some imports:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开编辑器，让我们开始编码。首先，我们需要一些导入：
- en: '[PRE4]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`rclpy` is the ROS 2 Python interface. `xarm` is the interface to the robot
    arm, while `time` of course is a time module that we will use to set timers. Finally,
    we use some standard ROS message formats with which to communicate.'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`rclpy` 是 ROS 2 的 Python 接口。`xarm` 是机器人臂的接口，而 `time` 当然是一个我们将用来设置计时器的时间模块。最后，我们使用一些标准的
    ROS 消息格式来进行通信。'
- en: 'Next, we are going to create some predefined named positions of the arm as
    shortcuts. This is a simple way to put the arm where we need it. I’ve defined
    five arm preset positions we can call:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一些预定义的臂命名位置作为快捷方式。这是一种简单的方法，将臂放置在我们需要的位置。我定义了五个我们可以调用的臂预设位置：
- en: '![Figure 5.4 – Robot arm positions](img/B19846_05_4.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.4 – 机器人臂位置](img/B19846_05_4.jpg)'
- en: Figure 5.4 – Robot arm positions
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4 – 机器人臂位置
- en: 'Let’s describe these positions in some detail:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细描述这些位置：
- en: '*High Carry* is the position we want the arm to be at when we are carrying
    an object such as a toy. The arm is over the robot, and the hand is elevated.
    This helps to keep the toy from falling out of the hand.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*高携带*是我们携带玩具等物体时希望手臂所处的位置。手臂在机器人上方，手部抬高。这有助于防止玩具从手中掉落。'
- en: '*Neutral Carry* is the standard position when the robot is driving so that
    the arm is not in front of the camera.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*中性携带*是当机器人驾驶时，手臂不在摄像机前的标准位置。'
- en: '*Pick Up* is a combination of *Grasp* and *Grasp Close* (which are not shown
    individually in the figure). The former is an arm position that puts the hand
    on the ground so we can pick up an object. The arm is as far out in front of the
    robot as it will go and touches the ground. The latter just closes the end effector
    to grab a toy.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*拾取*是*抓取*和*抓取闭合*（在图中没有单独显示）的组合。前者是手臂位置，将手放在地面上以便拾取物体。手臂尽可能向前伸展并接触地面。后者只是关闭末端执行器以抓取玩具。'
- en: '*Drop Off* is the arm position high above the robot to put a toy in the toy
    box, which is quite tall.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*放下*是将玩具放入玩具箱（相当高）的手臂位置。'
- en: '*Align* (not shown) is a utility mode to check the alignment of the arm. All
    the servos are set to their middle position and the arm should point at the ceiling
    in a straight line. If it does not, you need to adjust the arm using the utility
    that came with it.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*对齐*（未显示）是一种实用模式，用于检查手臂的对齐情况。所有伺服电机都设置为中间位置，手臂应该以直线指向天花板。如果不这样做，您需要使用随附的实用程序调整手臂。'
- en: 'Let’s see how we can set up the ROS interface. The numbers are the servo motor
    positions (angles) in units from `0` (fully counterclockwise) to `1000` (fully
    clockwise). The `9999` code means to not change the servo in that position so
    we can create commands that don’t change the positions of parts of the arm, such
    as the gripper:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们如何设置ROS接口。这些数字是伺服电机的位置（角度），单位从`0`（完全逆时针）到`1000`（完全顺时针）。`9999`代码表示在该位置不改变伺服电机，这样我们可以创建不改变手臂部分位置（如夹爪）的命令：
- en: '[PRE5]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now we can start defining our robot arm control class. We’ll start with the
    class definition and the initialization function:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以开始定义我们的机器人手臂控制类了。我们将从类定义和初始化函数开始：
- en: '[PRE6]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'There is quite a bit going on here to set up our ROS interface for the robot
    arm:'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里设置我们的机器人手臂的ROS接口有很多事情要做：
- en: First of all, we call up the object class structure (`super`) to initialize
    our ROS 2 node with the name `xarm_manager`.
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我们调用对象类结构（`super`）来使用名称`xarm_manager`初始化我们的ROS 2节点。
- en: Then we create a publisher for the arm position information, helpfully called
    `xarm_pos`. Here, POS stands for position. This publishes the arm position in
    servo units, which go from `0` (fully counterclockwise) to `1000` (fully clockwise).
    We also publish the arm angles in degrees, in case we need that information, as
    `xarm_angle`. The center of the servo travel is 0 degrees (`500` in servo units).
    Counterclockwise positions are negative angles while clockwise positions are positive
    angles. I just used integer degrees (no decimal points) since we don’t need that
    level of precision for the arm. Our High Carry position in servo units is `[666,501,195,867,617,500]`,
    and in servo angles is `[41,0,-76,91,29,0]`. We publish our outputs and subscribe
    to our inputs.
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们创建了一个用于手臂位置信息的发布者，方便地称为`xarm_pos`。在这里，POS代表位置。它以伺服单位发布手臂位置，这些单位从`0`（完全逆时针）到`1000`（完全顺时针）。我们还以`xarm_angle`发布手臂角度（以度为单位），以防我们需要该信息。伺服器行程的中心是0度（伺服单位中的`500`）。逆时针位置是负角度，而顺时针位置是正角度。我仅使用了整数度数（没有小数点），因为我们不需要那么高的精度来控制手臂。我们的高抬位置在伺服单位中是`[666,501,195,867,617,500]`，在伺服角度中是`[41,0,-76,91,29,0]`。我们发布我们的输出并订阅我们的输入。
- en: 'Our inputs, or subscriptions, provide the outside interface to the arm. I thought
    through how the arm might be used, and came up with the interface I wanted to
    see. In our case, we have a very simple arm, and need only a few commands. First
    of all, we have a string command called `RobotCmd`, which lets us create commands
    to control the mode or state of the robot. This will be used for a lot of commands
    for the robot, and not just for the arm. I’ve created several arm mode commands
    that we’ll cover in a few paragraphs. The usefulness of `RobotCmd` is we can send
    any string on this input and process it on the receiving end. It’s a very flexible
    and useful interface. Note that for each subscriber, we create a function call
    to a callback routine. When the data is published on the interface, the callback
    routine is called in our program (`xarm_mgr.py`) automatically:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的输入，或者说订阅，为手臂提供了外部接口。我思考了如何使用手臂，并提出了我想要的接口。在我们的情况下，我们有一个非常简单的手臂，只需要几个命令。首先，我们有一个名为`RobotCmd`的字符串命令，它允许我们创建控制机器人模式或状态的命令。这将用于许多机器人的命令，而不仅仅是手臂。我创建了一些手臂模式命令，我们将在接下来的几段中介绍。`RobotCmd`的有用之处在于我们可以向这个输入发送任何字符串，并在接收端处理它。这是一个非常灵活且有用的接口。请注意，对于每个订阅者，我们都会创建一个函数调用到回调例程。当数据在接口上发布时，我们的程序（`xarm_mgr.py`）会自动调用回调例程：
- en: '[PRE7]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The next part of the interface allows us to move the base of the arm in yaw,
    and operate the hand and wrist independently. In this chapter, we are starting
    with training just the gripper, so it helps to have an independent interface to
    rotate, open, and close the gripper. Operating the hand does not change the coordinate
    position of the gripper, so this can be separated. Likewise, we move the hand
    in yaw – right and left – to line up with toys to be grasped. We are going to
    start with this function locked off, and we’ll add the yaw function later. This
    is controlled by the computer vision system that we designed in the previous chapter,
    so it needs a separate interface. We have the `xarmWrist` command to rotate the
    wrist, `xarmEffector` to open and close the gripper fingers, and `xarmBase` to
    move the base of the arm right or left:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接口的下一段允许我们在偏航方向上移动手臂的底部，并独立操作手和手腕。在本章中，我们开始仅训练夹爪，因此有一个独立的接口来旋转、打开和关闭夹爪是有帮助的。操作手部不会改变夹爪的坐标位置，因此这可以分开。同样，我们通过偏航方向（向右和向左）移动手部，以对齐要抓取的玩具。我们将从这个功能开始锁定，稍后添加偏航功能。这是由我们在上一章中设计的计算机视觉系统控制的，因此需要一个独立的接口。我们有`xarmWrist`命令来旋转手腕，`xarmEffector`来打开和关闭夹爪手指，以及`xarmBase`来将手臂的底部向右或向左移动：
- en: '[PRE8]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The last command interface lets us move the arm to any position we specify.
    Normally, we command the arm to move using an array of numbers, like this: `[100,500,151,553,117,500]`.
    I’ve added a *secret feature* to this command. Since we may want to move the arm
    without either changing the yaw angle (which comes from the vision system) or
    the hand position (which may or may not be holding a toy), we can send commands
    that move the arm but don’t affect some of the servos, such as the hand. I used
    the value `9999` as the *don’t-move-this-servo* value. So if the arm position
    command reads `[9999, 9999, 500, 807, 443, 9999]` then the yaw position (*Motor
    6*) and the hand position (*Motors 0* and *1*) don’t change:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后的命令接口使我们能够将手臂移动到我们指定的任何位置。通常，我们使用一组数字来命令手臂移动，如下所示：`[100,500,151,553,117,500]`。我在这个命令中增加了一个**秘密功能**。由于我们可能希望在不需要改变偏航角度（来自视觉系统）或手部位置（可能或可能不握有玩具）的情况下移动手臂，我们可以发送移动手臂但不影响某些伺服电机的命令，例如手部。我使用了值`9999`作为**不移动此伺服电机**的值。因此，如果手臂位置命令读取为`[9999,
    9999, 500, 807, 443, 9999]`，则偏航位置（*电机6*）和手部位置（*电机0和1*）不会改变：
- en: '[PRE9]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now that we have all of our publish and subscribe interfaces defined, we can
    open the USB interface to the robot arm and see whether it is responding. If not,
    we’ll throw an error message:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经定义了所有的发布和订阅接口，我们可以打开连接到机器人手臂的USB接口，看看它是否在响应。如果没有响应，我们将抛出一个错误信息：
- en: '[PRE10]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Note
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Here is the quick cheat guide to the servos in the `xarmPos` command array:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是`xarmPos`命令数组中伺服电机的快速作弊指南：
- en: '`[grip open/close, wrist rotate, wrist pitch, elbow pitch, shoulder pitch,`
    `shoulder yaw]`'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '`[握紧/松开，手腕旋转，手腕俯仰，肘部俯仰，肩部俯仰，肩部偏航]`'
- en: 'Our next function in the source code is to set up a telemetry timer. We want
    to periodically publish the arm’s position for the rest of the robot to use. We’ll
    create a timer callback that executes periodically at a rate we specify. Let’s
    start with once a second. This is an informational value and we are not using
    it for control – the servo controller takes care of that. This is the code we
    need:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在源代码中的下一个函数是设置遥测定时器。我们希望定期发布机械臂的位置，以便机器人其他部分可以使用。我们将创建一个定时器回调，它以我们指定的速率定期执行。让我们从每秒一次开始。这是一个信息值，我们不会用它来控制——伺服控制器负责这一点。这是我们需要编写的代码：
- en: '[PRE11]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The `timer_period` is the interval between interrupts. The `self.timer` class
    variable is a function pointer to the timer function, and we point it at another
    function, `self.timer_callback`, which we’ll define in the next code block. Every
    second, the interrupt will go off and call the `timer_callback` routine.
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`timer_period` 是中断之间的间隔。`self.timer` 类变量是一个指向定时器函数的函数指针，我们将它指向另一个函数，`self.timer_callback`，我们将在下一个代码块中定义它。每秒钟，中断将会触发并调用
    `timer_callback` 例程。'
- en: 'Our next bit of code is part of the hardware interface. Since we are initializing
    the arm controller, we need to open the hardware connection to the arm, which
    is a USB port using the **human interface device** (**HID**) protocol:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们接下来的代码是硬件接口的一部分。由于我们正在初始化机械臂控制器，我们需要打开与机械臂的硬件连接，这是一个使用**人机界面设备**（**HID**）协议的USB端口：
- en: '[PRE12]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We first create a `try` block so we can handle any exceptions. The robot arm
    may not be powered on, or it may not be connected, so we have to be prepared to
    handle this. We create an arm object (`self.arm`) that will be our interface to
    the hardware. If the arm opens successfully, then we return. If not, we run through
    the `except` routine:'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们首先创建一个 `try` 块，以便我们可以处理任何异常。机器人臂可能未开启电源，或者可能未连接，因此我们必须准备好处理这种情况。我们创建一个臂对象（`self.arm`），它将成为我们与硬件的接口。如果臂成功打开，则返回。如果不成功，我们运行
    `except` 例程：
- en: First, we log that we did not find the arm in the ROS error log. The ROS logging
    function is very versatile and provides a handy place to store information that
    you need for debugging.
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我们在 ROS 错误日志中记录我们没有找到机械臂。ROS 日志记录函数非常灵活，提供了一个方便的地方来存储您在调试过程中需要的信息。
- en: Then we set the arm to a null object (`None`) so that we don’t throw unnecessary
    errors later in the program, and we can test to see whether the arm is connected.
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们将机械臂设置为空对象（`None`），这样我们就可以在程序后续部分避免抛出不必要的错误，并且可以测试机械臂是否已连接。
- en: 'The next block of code is our timer callback that publishes telemetry about
    the arm. Remember that we defined two output messages, the arm position and arm
    angle. We can service them both here:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一个代码块是我们的定时器回调，它发布有关机械臂的遥测信息。记住，我们定义了两个输出消息，机械臂位置和机械臂角度。我们可以在这里服务它们：
- en: '[PRE13]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We are using the `Int32MultiArray` datatype so that we can publish the arm position
    data as an array of integers. We collect the data from the arm by calling `self.arm.getPosition(servoNumber)`.
    We append the output to our array, and when we are done, call the ROS publish
    routine `(self.<topic name>.publish(msg))`. We do the same thing for the arm angle,
    which we can get by calling `arm.getPosition(servoNumber, True)` to return an
    angle instead of servo units.
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们使用 `Int32MultiArray` 数据类型，这样我们就可以将机械臂位置数据发布为一个整数数组。我们通过调用 `self.arm.getPosition(servoNumber)`
    从机械臂收集数据。我们将输出追加到我们的数组中，完成后，调用 ROS 发布例程 `(self.<topic name>.publish(msg))`。对于机械臂角度，我们可以通过调用
    `arm.getPosition(servoNumber, True)` 来获取，这将返回一个角度而不是伺服单元。
- en: 'Now we can handle receiving commands from other programs. Next, we are going
    to be creating a control panel for the robot that can send commands and set modes
    for the robot:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以处理来自其他程序的命令。接下来，我们将为机器人创建一个控制面板，可以发送命令并设置机器人的模式：
- en: '[PRE14]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This section is pretty straightforward. We receive a string message containing
    a command, and we parse the message to see whether it is something this program
    recognizes. If so, we process the message and perform the appropriate command.
    If we get `ARM MID_CARRY`, which is a command to position the arm to the middle
    position, then we send a `setArm` command using the `MidCarry` global variable,
    which has the servo positions for all six motors.
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个部分相当直接。我们接收一个包含命令的字符串消息，并解析消息以查看它是否是程序可以识别的内容。如果是，我们处理消息并执行适当的命令。如果我们收到 `ARM
    MID_CARRY` 命令，这是一个将机械臂定位到中间位置的命令，那么我们使用 `MidCarry` 全局变量发送一个 `setArm` 命令，该变量包含所有六个电机的伺服位置。
- en: 'Next, we write the code for the robot to receive and execute the wrist servo
    command, which rotates the gripper. This command goes to *Motor 2*:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们编写机器人接收并执行手腕伺服电机的代码，该命令旋转夹爪。这个命令发送到*电机2*：
- en: '[PRE15]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This function call is executed when the `xarmWrist` topic is published. This
    command just moves the wrist rotation, which we would use to align the fingers
    of the hand to the object we are picking up. I added some exception handling for
    invalid values, along with limit checking on the range of the input, which I consider
    a standard practice for external inputs. We don’t want the arm to do something
    weird with an invalid input, such as if someone was able to send a string on the
    `xarmWrist` topic instead of an integer. We also check whether the range of the
    data in the command is valid, which in this case is from `0` to `1000` servo units.
    If we get an out-of-bounds error, we clamp the command to the allowed range using
    the `min` and `max` functions.
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当`xarmWrist`主题发布时，执行这个函数调用。这个命令只是移动手腕旋转，我们会用它来调整手的手指以对准我们正在抓取的物体。我为无效值添加了一些异常处理，并在输入范围内进行了限制检查，我认为这是外部输入的标准做法。我们不希望手臂对无效输入执行奇怪的操作，例如如果有人能够在`xarmWrist`主题上发送字符串而不是整数。我们还检查命令中数据的范围是否有效，在这种情况下是`0`到`1000`个伺服单位。如果我们得到越界错误，我们将使用`min`和`max`函数将命令限制在允许的范围内。
- en: 'The end effector command and base command (which controls the left-right rotation
    of the entire arm) work exactly the same way:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 末端执行器命令和基础命令（控制整个手臂的左右旋转）的工作方式完全相同：
- en: '[PRE16]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The `setArm` command lets us send one comment to set the position of every servo
    motor at once, with one command. We send an array of six integers, and this program
    relays that to the servo motor controller.
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`setArm`命令让我们可以发送一个命令来同时设置每个伺服电机的位置。我们发送一个包含六个整数的数组，这个程序将这个数组传递给伺服电机控制器。'
- en: As mentioned before, I put in a special value, `9999`, that tells this bit of
    code to not move that motor. This lets us send commands to the arm that move some
    of the servos, or just one of them. This lets us move the up/down axis and left/right
    axis of the end of the arm independently, which is important.
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如前所述，我设置了一个特殊值，`9999`，这个值告诉这段代码不要移动那个电机。这使得我们可以向手臂发送命令，移动一些伺服电机，或者只移动其中一个。这使得我们可以独立地移动手臂末端的上下轴和左右轴，这非常重要。
- en: Another thing that is important is that while this bit of Python code executes
    almost instantly, the servo motors take a finite amount of time to move. We have
    to throw in some delays between the servo commands so that the servo controller
    can process them and send them to the right motor. I’ve discovered that the value
    `0.1` (1/10 of a second) between commands works. If you leave this value out,
    only one servo will move, and the arm will not process the rest of the commands.
    The servos use a serial interface in a daisy chain fashion, which means they relay
    messages to each other. Each servo is plugged into one other servo, which is a
    big improvement over all the servos being plugged in individually.
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 另一件重要的事情是，尽管这段Python代码几乎瞬间执行，但伺服电机移动需要一定的时间。我们必须在伺服命令之间加入一些延迟，以便伺服控制器可以处理它们并将它们发送到正确的电机。我发现命令之间的`0.1`（1/10秒）延迟是有效的。如果你省略这个值，只有一个伺服电机会移动，手臂将不会处理其余的命令。伺服电机以菊花链的方式使用串行接口，这意味着它们相互传递消息。每个伺服电机都连接到另一个伺服电机，这比所有伺服电机单独连接要好得多。
- en: 'We can finish up our arm control code with `MAIN` – the executable part of
    the program:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以用`MAIN`来完成我们的手臂控制代码——程序的执行部分：
- en: '[PRE17]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Here, we initialize `rclpy` (ROS 2 Python interface) to connect our program
    to the ROS infrastructure. Then we create an instance of our `xarm` control class
    we created. We’ll call it `xarmCtr`. Then we just have to tell ROS 2 to execute.
    We don’t even need a loop. The program will perform publish and subscribe calls,
    and our timer sends out telemetry, which is all included in our `xarmControl`
    object. When we fall out of spin, we are done with the program, so we shut down
    the ROS node, and then the program.
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们初始化`rclpy`（ROS 2 Python接口）以将我们的程序连接到ROS基础设施。然后我们创建我们创建的`xarm`控制类的实例。我们将它称为`xarmCtr`。然后我们只需告诉ROS
    2执行。我们甚至不需要循环。程序将执行发布和订阅调用，我们的计时器发送遥测数据，这些都包含在我们的`xarmControl`对象中。当我们退出`spin`时，我们就完成了程序，所以我们将关闭ROS节点，然后程序结束。
- en: Now we are ready to start training our robot arm! To do this, we are going to
    use three different methods to train our arm to pick up objects. In the first
    stage, we will just train the robot hand – the end effector – to grasp objects.
    We will use Q-learning, a type of RL, to accomplish this. We will have the robot
    try to pick up items, and we will reward, or give points, if the robot is successful
    and subtract points if it fails. The software will try to maximize the reward
    to get the most points, just like playing a game. We will generate different policies,
    or action plans, to make this happen.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备开始训练我们的机器人手臂！为此，我们将使用三种不同的方法来训练我们的手臂拿起物体。在第一阶段，我们将仅训练机器人手——末端执行器——来抓取物体。我们将使用Q学习，一种强化学习类型，来完成这项任务。我们将让机器人尝试拿起物品，如果机器人成功，我们将给予奖励或得分，如果机器人失败，我们将减分。软件将尝试最大化奖励以获得最高分数，就像玩游戏一样。我们将生成不同的策略或动作计划来实现这一点。
- en: Introducing Q-learning for grasping objects
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍用于抓取物体的Q学习。
- en: 'Training a robot arm end effector to pick up an oddly shaped object using the
    **Q-learning** RL technique involves several steps. Here’s a step-by-step explanation
    of the process:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 使用**Q学习**强化学习技术训练机器人手臂末端执行器拿起形状奇特的物体涉及几个步骤。以下是该过程的逐步解释：
- en: 'Define the state space and action space:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义状态空间和动作空间：
- en: '**Define the state space**: This includes all the relevant information about
    the environment and the robot arm, such as the position and orientation of the
    object, the position and orientation of the end effector, and any other relevant
    sensor data'
  id: totrans-150
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定义状态空间**：这包括有关环境和机器人手臂的所有相关信息，例如物体的位置和方向、末端执行器的位置和方向以及任何其他相关传感器数据。'
- en: '**Define the action space**: These are the possible actions the robot arm can
    take, such as rotating the end effector, moving it in different directions, or
    adjusting its gripper'
  id: totrans-151
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定义动作空间**：这些是机器人手臂可以采取的可能动作，例如旋转末端执行器、在不同方向上移动它或调整其夹爪。'
- en: '**Set up the Q-table**: Create a Q-table that represents the state-action pairs
    and initialize it with random values. The Q-table will have a row for each state
    and a column for each action. As we test each position that the arm moves to,
    we will store the reward that was computed by the Q-learning equation (introduced
    in the *Machine learning for robot arms* section) in this table so that we can
    refer to it later. We will search the Q-table by state and action to see which
    state-action pair results in the largest reward.'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**设置Q表**：创建一个表示状态-动作对的Q表，并用随机值初始化它。Q表将包含每一状态一行，每一动作一列。当我们测试手臂移动到的每个位置时，我们将使用Q学习方程（在*机器人手臂的机器学习*部分介绍）计算出的奖励存储在这个表中，以便我们稍后可以参考。我们将通过状态和动作搜索Q表，以查看哪个状态-动作对会产生最大的奖励。'
- en: '**Define the reward function**: Define a reward function that provides feedback
    to the robot arm based on its actions. The reward function should encourage the
    arm to pick up the object successfully and discourage undesirable behavior.'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义奖励函数**：定义一个奖励函数，根据机器手臂的动作为其提供反馈。奖励函数应鼓励手臂成功拿起物体，并阻止不良行为。'
- en: '**Start the training loop**: Start the training loop, which consists of multiple
    episodes. Each episode represents one iteration of the training process:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**启动训练循环**：启动训练循环，它由多个剧集组成。每个剧集代表训练过程的迭代：'
- en: Reset the environment and set the initial state
  id: totrans-155
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重置环境和设置初始状态。
- en: Select an action based on the current state using an exploration-exploitation
    strategy such as epsilon-greedy, where you explore random actions with a certain
    probability (epsilon) or choose the action with the highest Q-value
  id: totrans-156
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据当前状态使用探索-利用策略（如ε-贪婪）选择动作，其中以一定的概率（ε）探索随机动作或选择具有最高Q值的动作。
- en: Execute the selected action and observe the new state and the reward
  id: totrans-157
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行选定的动作，并观察新的状态和奖励。
- en: Update the Q-value in the Q-table using the Q-learning update equation, which
    incorporates the reward, the maximum Q-value for the next state, and the learning
    rate (alpha) and discount factor (gamma) parameters
  id: totrans-158
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Q学习更新方程更新Q表中的Q值，该方程结合了奖励、下一个状态的最大Q值以及学习率（alpha）和折扣因子（gamma）参数。
- en: Update the current state to the new state
  id: totrans-159
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将当前状态更新为新状态。
- en: Repeat the previous steps until the episode terminates, either by successfully
    picking up the object or reaching a maximum number of steps
  id: totrans-160
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复之前的步骤，直到剧集结束，无论是成功拿起物体还是达到最大步数。
- en: '**Exploration and exploitation**: Adjust the exploration rate (represented
    by epsilon) over time to gradually reduce exploration and favor exploitation of
    the learned knowledge. This allows the robot arm to initially explore different
    actions and gradually focus on exploiting the learned information to improve performance.'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**探索与利用**：随着时间的推移调整探索率（用epsilon表示），逐渐减少探索并优先利用学到的知识。这允许机器人臂最初探索不同的动作，并逐渐专注于利用学到的信息以提高性能。'
- en: '**Repeat training**: Continue the training loop for multiple episodes until
    the Q-values converge or the performance reaches a satisfactory level.'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**重复训练**：继续多个回合的训练循环，直到Q值收敛或性能达到满意水平。'
- en: '**Perform testing**: After training, use the learned Q-values to make decisions
    on the actions to take in a testing environment. Apply the trained policy to the
    robot arm end effector, allowing it to pick up the oddly shaped object based on
    the learned knowledge.'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**执行测试**：在训练后，使用学到的Q值在测试环境中做出决策。将训练好的策略应用于机器人臂末端执行器，使其能够根据学到的知识捡起形状奇特的物体。'
- en: Note
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Implementing Q-learning for training a robot arm end effector requires a combination
    of software and hardware components, such as simulation environments, robotic
    arm controllers, and sensory input interfaces. The specifics of the implementation
    can vary depending on the robot arm platform and the tools and libraries being
    used.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 实现机器人臂末端执行器的Q-learning训练需要软件和硬件组件的组合，例如仿真环境、机器人臂控制器和感官输入接口。具体实现方式可能因机器人臂平台和使用的工具和库而异。
- en: Writing the code
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编写代码
- en: 'Now we’ll implement the seven-step process we just described by building the
    code that will train the arm, using the robot arm interface we made in the previous
    section:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将通过构建代码来实现我们刚才描述的七个步骤，该代码将使用我们在上一节中制作的机器人臂接口来训练手臂：
- en: 'First, we include our imports – the functions we’ll need to implement our training
    code:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们包含所需的导入 – 我们将需要实现训练代码的功能：
- en: '[PRE18]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '`rclpy` is the ROS 2 Python interface. We use `Detection2D` to talk to the
    vision system from the previous chapter (YOLOV8). I’ll explain the `pickle` reference
    when we get to it.'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`rclpy`是ROS 2的Python接口。我们使用`Detection2D`与上一章（YOLOV8）中的视觉系统通信。当我们到达那里时，我会解释`pickle`引用。'
- en: 'Next, let’s define some functions we’ll be using later:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们定义一些我们稍后会使用的函数：
- en: '[PRE19]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The learning rate is used in reinforcement learning just like in other machine
    learning algorithms to adjust how fast the system makes changes as a result of
    inputs. We’ll start with `0.1`. If this value is too big, we will have big jumps
    in our training that can cause erratic outputs. If it’s too small, we’ll have
    to do a lot of repetitions. `actionSpace` is the list of possible hand actions
    that we are teaching. These values are the angle of the wrist in degrees. Note
    that `-90` and `+90` are the same as far as grasping is concerned.
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 学习率在强化学习中就像在其他机器学习算法中一样，用于调整系统根据输入做出改变的速度。我们将从`0.1`开始。如果这个值太大，我们的训练会有大的跳跃，这可能导致不稳定的输出。如果太小，我们可能需要进行很多次重复。`actionSpace`是我们正在教授的可能的手部动作列表。这些值是手腕的角度（以度为单位）。请注意，就抓取而言，`-90`和`+90`是相同的。
- en: The `round4` function is used to round off the aspect ratio of the bounding
    box. When we detect a toy, as you may remember, the object recognition system
    draws a box around it. We use that bounding box as a clue to how the toy is oriented
    relative to the robot. We want a limited number of aspect angles to train for,
    so we’ll round this off to the nearest `0.25`.
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`round4`函数用于将边界框的宽高比四舍五入。如您所记得，当我们检测到玩具时，对象识别系统会在其周围画一个框。我们使用这个边界框作为线索，了解玩具相对于机器人的方向。我们希望训练有限的宽高角，因此我们将它四舍五入到最近的`0.25`。'
- en: The `SortbyQ` function is a custom sort key that we’ll use to sort our training
    to put the highest reward – represented by the letter `Q` – first.
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`SortbyQ`函数是我们将用于对训练进行排序的自定义排序键，将最高奖励（用字母`Q`表示）放在第一位。'
- en: 'In this step, we’ll declare the class that will teach the robot to grasp objects.
    We’ll call the class `LearningHand`, and we’ll make it a node in ROS 2:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们将声明一个将教会机器人抓取物体的类。我们将把这个类命名为`LearningHand`，并将其作为ROS 2中的一个节点：
- en: '[PRE20]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Here we initialize the object by passing up the `init` function to the parent
    class (with `super`). We give the node the name `armQLearn`, which is how the
    rest of the robot will find it.
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们通过将`init`函数传递给父类（使用`super`）来初始化对象。我们给节点命名为`armQLearn`，这样机器人其他部分就能找到它。
- en: Our ROS interface subscribes to several topics. We need to talk to the robot
    arm, so we subscribe to `xarm_pos` (arm position). We need to subscribe (like
    every program that talks to the robot) to `RobotCmd`, which is our master mode
    command channel. We also need to be able to send commands on `RobotCmd`, so we
    create a publisher on that topic. Finally, we use a ROS parameter to set the value
    of how many repetitions we want for each learning task.
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的ROS接口订阅了几个主题。我们需要与机械臂通信，因此我们订阅了`xarm_pos`（手臂位置）。我们需要订阅（就像与机器人通信的每个程序一样）`RobotCmd`，这是我们主模式命令通道。我们还需要能够在`RobotCmd`上发送命令，因此我们在该主题上创建了一个发布者。最后，我们使用ROS参数设置每个学习任务重复次数的值。
- en: 'This next block of code completes the setup for the learning function:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一个代码块完成了学习函数的设置：
- en: '[PRE21]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We set the learning system mode to `idle`, which just means “wait for the user
    to start learning.” We create the arm interface by instantiating the `ArmInterface`
    class object we imported. Next, we need to set up our learning matrix, which stores
    the possible aspects (things we can see) and the possible actions (things we can
    do). The last element, which we set to 0 here, is the `Q` value, which is where
    we store our training results.
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将学习系统模式设置为`idle`，这意味着“等待用户开始学习。”我们通过实例化我们导入的`ArmInterface`类对象来创建手臂接口。接下来，我们需要设置我们的学习矩阵，它存储可能的方面（我们可以看到的事物）和可能的行为（我们可以做的事情）。这里我们设置为0的最后一个元素是`Q`值，这是我们存储训练结果的地方。
- en: 'The following set of functions helps us to command the arm:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下函数集帮助我们控制手臂：
- en: '[PRE22]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '`sndCmd` (send command) publishes on the `RobotCmd` topic and sets arm modes.
    `SetHandAngle`, as you expect, sets the angle of the wrist servo. `armPosCallback`
    receives the arm’s current position, which is published by the arm control program.
    `setActionPairs` allows us to create new action pairs to learn.'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`sndCmd`（发送命令）在`RobotCmd`主题上发布并设置手臂模式。`SetHandAngle`，正如你所期望的，设置手腕伺服电机的角度。`armPosCallback`接收手臂的当前位置，这是由手臂控制程序发布的。`setActionPairs`允许我们创建新的动作对以进行学习。'
- en: 'Now we are ready to do the arm training. This is a combined human and robot
    activity, and is really a lot of fun to do. We’ll try the same aspect 20 times:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们准备进行手臂训练。这是一个结合了人和机器人活动的过程，真的非常有趣。我们将尝试20次相同的方面：
- en: '[PRE23]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This initiates the training program on the robot arm. We start by training based
    on aspect. We first look at our `stateActionPairs` to sort on the highest `Q`
    value for this aspect. We use our custom `SortbyQ` function to sort the list of
    `stateActionPairs`. We set the hand angle to the angle with the highest `Q`, or
    expected reward.
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这在机械臂上启动了训练程序。我们首先基于方面进行训练。我们首先查看我们的`stateActionPairs`以按此方面的最高`Q`值进行排序。我们使用我们的自定义`SortbyQ`函数对`stateActionPairs`列表进行排序。我们将手角度设置为具有最高`Q`值或预期奖励的角度。
- en: 'This part of the program is the physical motion the robot arm will go through:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 程序的这一部分是机器人手臂将要经历的物理运动：
- en: '[PRE24]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We start by telling the arm to move to the *Mid Carry* position – halfway up.
    Then we wait 1 second for the arm to complete its motion, and then we move the
    arm to the grasp position. The next step moves the wrist to the angle that we
    got from the `Q` function. Then we close the gripper with the `ARM GRASP_CLOSE`
    command. Now we raise the arm to see whether the gripper can lift the toy, using
    the `ARM MID_CARRY` instruction. If we are successful, the robot arm will now
    be holding toy. If not, the gripper will be empty.
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们首先告诉手臂移动到*Mid Carry*位置——中间位置。然后我们等待1秒钟，直到手臂完成其动作，然后我们将手臂移动到抓取位置。下一步将手腕移动到`Q`函数得到的角。然后我们使用`ARM
    GRASP_CLOSE`命令关闭夹爪。现在我们抬起手臂，看看夹爪是否能够举起玩具，使用`ARM MID_CARRY`指令。如果我们成功，机器人手臂现在将持有玩具。如果不成功，夹爪将是空的。
- en: 'Now we can check to see whether the gripper has an object in it:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以检查夹爪中是否有物体：
- en: '[PRE25]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: If the grip of the robot hand is correct, the toy will prevent the gripper from
    closing. We check the hand position (which the arm sends twice a second) to see
    the position. For my particular arm, the position that corresponds to 650 servo
    units or greater is completely closed. Your arm may vary, so check to see what
    the arm reports for a fully closed and empty gripper. We set the `gripSuccess`
    variable as appropriate.
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果机器人手的握持正确，玩具将阻止夹爪关闭。我们检查手的位置（手臂每秒发送两次）以查看位置。对于我的特定手臂，对应于650伺服单位或更大的位置是完全关闭的。你的手臂可能不同，所以检查手臂报告的完全关闭和空夹爪的位置。我们根据适当的情况设置`gripSuccess`变量。
- en: 'Now we do the machine learning part. We use my special modified Bellman equation
    introduced in the *Machine learning for robot arms* section to adjust the Q value
    for this state-action pair:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们进行机器学习部分。我们使用在*机器人臂的机器学习*部分中引入的特别修改后的Bellman方程来调整这个状态-动作对的Q值：
- en: '[PRE26]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Since we are not using a future reward value (we get the complete reward from
    this one action of closing the gripper and raising the arm), we don’t need the
    expected future reward, only a present reward. We multiply the `gripSuccess` value
    (`+1` or `-1`) by the learning rate and add this to the old Q score to get a new
    Q score. Each success increments the reward while any failure leads to a decrement.
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于我们不是使用未来的奖励值（我们从这个关闭夹具和抬起手臂的动作中获得完整的奖励），我们不需要预期的未来奖励，只需要当前的奖励。我们将`gripSuccess`值（`+1`或`-1`）乘以学习率，并将其添加到旧的Q分数中，以获得新的Q分数。每次成功都会增加奖励，而任何失败都会导致减少。
- en: 'To finish our learning function, we insert the updated Q value back into the
    learning table that matches the aspect angle and the wrist angle we tested:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了完成我们的学习函数，我们将更新的Q值放回与测试的角度和手腕角度相匹配的学习表中：
- en: '[PRE27]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: If this state-action pair is not in the table (which it should be), then we
    add it. I put this in just to keep the program from erroring out if we give a
    strange arm angle. Finally, we pause the program and wait for the user to hit
    the *Enter* key in order to continue.
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果这个状态-动作对不在表中（它应该在那里），我们就添加它。我这样做只是为了防止在给出奇怪的臂角度时程序出错。最后，我们暂停程序并等待用户按下*Enter*键以继续。
- en: 'Let’s now look at the rest of the program, which is pretty straightforward.
    We have to do some housekeeping, service some calls, and make our main training
    loop:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们来看看程序的其余部分，这部分相当直接。我们必须做一些维护工作，处理一些调用，并构建我们的主要训练循环：
- en: '[PRE28]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This `cmdCallBack` receives commands from the `RobotCmd` topic. The only two
    commands we service in this program are `GoLearnHand`, which starts the learning
    process, and `StopLearnHand`, which lets you stop training.
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个`cmdCallBack`接收来自`RobotCmd`主题的命令。在这个程序中，我们只处理两个命令：`GoLearnHand`，它启动学习过程，以及`StopLearnHand`，它允许你停止训练。
- en: 'This section is our arm interface to the robot arm and sets up the publish/subscribe
    interface we need to command the arm:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个部分是我们的臂接口到机器人臂的接口，并设置了我们需要用来控制臂的发布/订阅接口：
- en: '[PRE29]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: We subscribe to `xarm_pos` (arm position in servo units) and `xarm_angle` (arm
    position in degrees). I added the ability to set the robot arm position on the
    `xarm` topic, but you may not need that.
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们订阅了`xarm_pos`（伺服单元中的臂位置）和`xarm_angle`（以度为单位的手臂位置）。我在`xarm`主题上添加了设置机器人臂位置的能力，但你可能不需要这个功能。
- en: For each subscription we need a callback function. We have `armPosCallback`
    and `armAngleCallback,` which will be called when the arm publishes its position,
    which I set to happen at 2 Hertz, or twice a second. You can increase this rate
    in the `xarm_mgr` program if you feel it necessary.
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于每个订阅，我们需要一个回调函数。我们有一个`armPosCallback`和`armAngleCallback`，当臂发布其位置时将被调用，我将此设置为每秒2赫兹，即每秒两次。如果你觉得有必要，可以在`xarm_mgr`程序中增加这个速率。
- en: 'Now we get to the main program. For a lot of ROS programs, this main section
    is pretty brief. We have an extra routine we need to put here. To save the training
    function after we do our training, I came up with this solution – to *pickle*
    the state-action pairs and put them into a file:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们进入主程序。对于许多ROS程序，这个主要部分相当简短。我们需要在这里添加一个额外的例程。为了在训练后保存训练函数，我想出了这个解决方案——将状态-动作对*pickle*并放入一个文件中：
- en: '[PRE30]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: When we run this program, we need to load this file and set our action-pairs
    table to these saved values. I set up a `try`/`except` block to send an error
    message when this training file is not found. This will happen the first time
    you run the program, but we’ll create a new file in just a moment for next time.
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当我们运行这个程序时，我们需要加载这个文件并将我们的动作对表设置为这些保存的值。我设置了一个`try`/`except`块，当找不到这个训练文件时发送错误消息。这将在你第一次运行程序时发生，但我们将很快为下一次运行创建一个新的文件。
- en: We also instantiate our class variables for the arm trainer and the arm interface,
    which creates the main part of our training program.
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们还实例化了臂训练器和臂接口的类变量，这创建了我们的训练程序的主要部分。
- en: 'This is the meat of our training loop. We set the aspect and number of trial
    repetitions that we train on:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是我们的训练循环的核心。我们设置了训练的方面和试验重复次数：
- en: '[PRE31]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Start with the toy parallel to the front of the robot. Do 20 trials of picking
    it up, and then move the toy 45 degrees to the right for the next part. Then perform
    20 more trials. Then the toy is moved to be 90 degrees to the robot. Run 20 trials.
    Finally set the toy at -45 degrees (to the left) for the final set and run 20
    times. Welcome to machine learning!
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从玩具与机器人前方平行开始。进行20次拾取尝试，然后移动玩具45度向右进行下一部分。然后进行另外20次尝试。然后，将玩具移动到与机器人成90度角。运行20次试验。最后，将玩具设置为-45度（向左）进行最终设置，运行20次。欢迎来到机器学习！
- en: 'You’ll probably guess that the last thing we do is save our training data,
    like this:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可能会猜到我们最后要做的事情是保存我们的训练数据，如下所示：
- en: '[PRE32]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This completes our training program. Repeat this training for as many types
    of toys as you have, and you should have a trained arm that consistently picks
    up toys at a variety of angles to the robot. Start with a selection of toys you
    want the robot to pick up. Set the angle of the toy to the robot at 0 – let’s
    say this is with the longest part of the toy parallel to the front of the robot.
    Then we send `GoLearnHand` on `RobotCmd` to put the robot arm in learning mode.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了我们的训练程序。重复进行这种训练，直到你对所有类型的玩具都进行了训练，你应该会得到一个能够以各种角度持续拾取玩具的机器人手臂。首先，选择你希望机器人拾取的玩具。将玩具的角度设置为机器人0度——比如说，这是玩具最长部分与机器人前方平行。然后我们向`RobotCmd`发送`GoLearnHand`以将机器人手臂置于学习模式。
- en: We have tried out Q-learning in a couple of different configurations, with a
    limited amount of success in training our robot. The main problem with Q-learning
    is that we have a very large number of possible states, or positions, that the
    robot arm can be in. This means that gaining a lot of knowledge about any one
    position by repeated trials is very difficult. Next, we are going to introduce
    a different approach using GAs to generate our movement actions.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尝试过几种不同的配置进行Q学习，但在训练我们的机器人方面取得了一些有限的成果。Q学习的主要问题是，我们有一个非常大的可能状态数，或者说位置数，机器人手臂可以处于这些位置。这意味着通过重复试验获得任何单个位置的大量知识是非常困难的。接下来，我们将介绍一种使用遗传算法生成我们的运动动作的不同方法。
- en: Introducing GAs
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍遗传算法（GAs）
- en: Moving the robot arm requires the coordination of three motors simultaneously
    to create a smooth movement. We need a mechanism to create different combinations
    of motor movement for the robot to test. We could just use random numbers, but
    that would be inefficient and could take thousands of trials to get to the level
    of training we want.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 移动机器人手臂需要同时协调三个电机以创建平滑的运动。我们需要一种机制来为机器人创建不同的电机运动组合以进行测试。我们本可以使用随机数，但这将是不高效的，可能需要数千次试验才能达到我们想要的训练水平。
- en: What if we had a way of trying different combinations of motor movement, and
    then pitting them against one another to pick the best one? It would be a sort
    of Darwinian *survival of the fittest* for arm movement scripts – such as a GA
    process. Let’s explore how we can apply this concept to our use case.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一种方法来尝试不同的电机运动组合，并将它们相互对抗以选择最佳组合，这将是一种类似于达尔文的“适者生存”的机器人手臂运动脚本——例如遗传算法过程。让我们探讨如何将这个概念应用到我们的用例中。
- en: Understanding how the GA process works
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解遗传算法（GA）过程的工作原理
- en: 'Here are the steps involved in our GA process:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们遗传算法过程中涉及的步骤：
- en: We do a trial run to go from position 1 (neutral carry) to position 2 (pickup).
    The robot moves the arm 100 times before getting the hand into the right position.
    Why 100? We need a large enough sample space to allow the algorithm to explore
    different solutions. With a value of 50, the solution did not converge satisfactorily,
    while a value of 200 yielded the same result as 100.
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们进行一次试验运行，从位置1（中性携带）到位置2（拾取）。在将手放入正确位置之前，机器人将手臂移动100次。为什么是100次？我们需要足够大的样本空间，以便算法能够探索不同的解决方案。当值为50时，解决方案没有满意地收敛，而值为200时，结果与100次相同。
- en: We score each movement based on the percentage of goal accomplishment, indicating
    how much this movement contributed to the goal.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们根据目标完成百分比对每次移动进行评分，表明这次移动对目标的贡献程度。
- en: We take the 10 best moves and put them in a database.
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将10个最佳移动放入数据库中。
- en: We run the test again and do the same thing – now we have 10 more *best moves*
    and 20 moves in the database.
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们再次进行测试，并做同样的事情——现在我们有10个更多的“最佳移动”和20个动作在数据库中。
- en: We take the five best from the first set and cross them with the five best from
    the second set – plus five moves chosen at random and five more made up of totally
    random moves. Crossing two solutions refers to the process of taking a segment
    from the first set and a segment from the second set. In genetic terms, this is
    like taking half the *DNA* from each of two *parents* to make a new *child*.
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从第一组中选取五个最佳动作，与第二组中选取的五个最佳动作进行交叉——再加上随机选择的五个动作和五个完全随机的动作。交叉两个解决方案指的是从第一组中取一段，从第二组中取一段的过程。在遗传学的术语中，这就像从两个*父母*中各取一半的*DNA*来制造一个新的*孩子*。
- en: We run that sequence of moves, and then take the 10 best individual moves and
    continue on.
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们运行这一系列动作，然后选择10个最佳的单个动作并继续进行。
- en: Through the process of selection, we should quickly get down to a sequence that
    performs the task. It may not be optimal, but it will work. We are managing our
    *gene pool* (a list of trial solutions to our problem) to create a solution to
    a problem by successive approximation. We want to keep a good mix of possibilities
    that can be combined in different ways to solve the problem of moving our arm
    to its goal.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 通过选择的过程，我们应该很快就能得到一个执行任务的序列。它可能不是最优的，但它是有效的。我们正在管理我们的*基因库*（我们问题的试验解决方案列表），通过连续近似来创建一个问题的解决方案。我们希望保持一个良好的可能性混合，这些可能性可以用不同的方式组合起来，以解决将我们的手臂移动到目标位置的问题。
- en: We can actually use several methods of **cross-breeding** our movement sequences.
    What I described is a simple cross – half the first parent’s genetic material
    and half the second parent’s material (if you will pardon the biological metaphor).
    We could instead use quarters – ¼ first, ¼ second, ¼ first, ¼ second – to have
    two crosses. We could also randomly grab bits from one or the other. We will stick
    with the half/half strategy for now, but you are free to experiment to your heart’s
    content. In essence, in all of these options, we are taking a solution, breaking
    it in half, and randomly combining it with half of a solution from another trial.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实际上可以使用几种**杂交**我们动作序列的方法。我描述的是一种简单的杂交——第一父母遗传物质的一半和第二父母物质的一半（如果你能原谅这个生物学的比喻）。我们也可以使用四分之一——四分之一第一，四分之一第二，四分之一第一，四分之一第二——来进行两次杂交。我们也可以随机从其中一个或另一个中抓取片段。我们现在将坚持一半/一半的策略，但你完全可以根据自己的意愿进行实验。本质上，在这些所有选项中，我们都在采取一个解决方案，将其一分为二，然后随机将其与另一个试验中一半的解决方案结合。
- en: 'You are about to issue an objection: what if the movement takes less than 10
    steps? Easy – when we get to the goal, we stop, and discard the remaining steps.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能要提出一个反对意见：如果动作少于10步怎么办？简单——当我们到达目标时，我们停止，并丢弃剩余的步骤。
- en: Note
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We are not looking for a perfect or optimum task execution, but just something
    good enough to get the job done. For a lot of real-time robotics, we don’t have
    the luxury of time to create a perfect solution, so any solution that gets the
    job done is adequate.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不是在寻找完美或最优的任务执行，只是足够好以完成任务的东西。对于许多实时机器人，我们没有时间上的奢侈来创建一个完美的解决方案，所以任何能完成任务的解决方案都是足够的。
- en: Why did we add the five additional random sample moves, and five totally random
    moves? This also mimics natural selection – the power of mutation. Our genetic
    code (the DNA in our bodies) is not perfect, and sometimes inferior material gets
    passed along. We also experience random mutations from bad copies of genes, cosmic
    rays, and viruses. We are introducing some random factors to *bump* the tuning
    of our algorithm – the element of natural selection – in case we converge on a
    local minimum or miss some simple path because it has not occurred yet to our
    previous movements.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们要添加五个额外的随机样本动作和五个完全随机的动作？这也模仿了自然选择——变异的力量。我们的遗传代码（我们体内的DNA）并不完美，有时劣质材料会被传递下去。我们也可能从基因的坏副本、宇宙射线和病毒中经历随机突变。我们引入一些随机因素来*调整*我们算法的调谐——自然选择的元素——以防我们收敛到一个局部最小值或错过一些简单的路径，因为在我们之前的动作中还没有发生。
- en: But why on Earth are we going to all this trouble? The GA process can do something
    very difficult for a piece of software – it can innovate or evolve new solutions
    out of primitive actions by basically trying stuff until it finds out what works
    and what does not. We have provided another machine learning process to add to
    our toolbox, but one that can create solutions we, the programmers, had not preconceived.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 但为什么我们要费这么大的劲呢？遗传算法过程可以为软件做一件非常困难的事情——它可以通过尝试直到找到有效和无效的方法，从基本动作中创新或进化出新的解决方案。我们提供了一个额外的机器学习过程来添加到我们的工具箱中，但这是一个可以创建我们程序员没有预先设想出的解决方案的过程。
- en: Now, let’s dive into the GA process. In the interest of transparency, we are
    going to build our own GA process from scratch.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入GA过程。为了提高透明度，我们将从头开始构建自己的GA过程。
- en: Note
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We’ll be building our own tools in this version, but there are some prebuilt
    toolsets that can help you to create GAs, such as `pip` `install deap`.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个版本中，我们将构建自己的工具，但也有一些预构建的工具集可以帮助你创建GA，例如`pip install deap`。
- en: Building a GA process
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建GA过程
- en: 'We loosely adopt the concept of the *survival of the fittest* to decide which
    plans are the fittest and get to survive and propagate. I’m giving you a sandbox
    in which to play genetic engineer, where you have access to all of the parts and
    nothing is hidden behind the curtain. You will find that for our problem, the
    code is not all that complex:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我们松散地采用“适者生存”的概念来决定哪些计划是最适合的，并得以生存和繁衍。我给你一个沙盒，让你在其中扮演遗传工程师的角色，你将能够访问所有部件，没有任何东西隐藏在幕后。你会发现，对于我们的问题，代码并不那么复杂：
- en: We’ll start by creating the `computefitness` function, the one that scores our
    genetic material. **Fitness** is our criteria for grading our algorithm. We can
    change the fitness to our heart’s content to tailor our output to our needs. In
    this case, we are making a path in space for the robot arm from the starting location
    to the ending goal location. We evaluate our path in terms of how close any point
    of the path comes to our goal. Just as in our previous programs, the movement
    of the robot is constituted as 27 combinations of the three motors going clockwise,
    counterclockwise, or not moving. We divide the movement into small steps, each
    three motor units (1.8 degrees) of so of motion. We string together a whole group
    of these steps to make a path. The fitness function steps along the path and computes
    the hand position at each step.
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先创建`computefitness`函数，即评分我们的遗传材料。**适应性**是我们评估算法的标准。我们可以随心所欲地改变适应性，以调整我们的输出以满足我们的需求。在这种情况下，我们正在为机器人手臂从起始位置到目标位置构建空间路径。我们根据路径上的任何点接近目标的方式评估我们的路径。就像我们之前的程序一样，机器人的运动由三个电机的顺时针、逆时针或不动这27种组合构成。我们将运动分成小步骤，每个步骤大约是三个电机单元（1.8度）的运动。我们将这些步骤连在一起形成一个路径。适应性函数沿着路径前进，并在每个步骤计算手的位置。
- en: The `predictReward` function makes a trial computation of where the robot hand
    has moved as a result of that step. Let’s say we move *Motor 1* clockwise three
    steps, leave *Motor 2* alone, and move *Motor 3* counterclockwise three steps.
    This causes the hand to move slightly up and out. We score each step individually
    by how close it comes to the goal. Our score is computed out of 100; 100 is exactly
    at the goal, and we take away one point for each 1/100th of the distance the arm
    is away from the goal, up to a maximum of 340 mm. Why 340? That is the total length
    of the arm. We score the total movement a bit differently than you might think.
    Totaling up the rewards make no difference, as we want the point of closest approach
    to the goal. So we pick the single step with the highest reward and save that
    value. We throw away any steps after that, since they will only take us further
    away. Thus we automatically prune our paths to end at the goal.
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`predictReward`函数对机器人手由于该步骤而移动的位置进行试验计算。假设我们顺时针移动*电机1*三个步骤，保持*电机2*不动，并逆时针移动*电机3*三个步骤。这导致手稍微向上和向外移动。我们通过每个步骤接近目标的方式单独评分。我们的评分是100分；100分正好在目标处，我们每100分之1的距离从目标处减去一分，最多减去340毫米。为什么是340？这是手臂的总长度。我们评分的方式可能与你想象的略有不同。总奖励的加总没有区别，因为我们想要的是最接近目标点的点。因此，我们选择具有最高奖励的单个步骤并保存该值。我们丢弃该步骤之后的任何步骤，因为它们只会让我们离目标更远。因此，我们自动修剪路径，使其在目标处结束。'
- en: 'I used the term `allele` to indicate a single step out of the total path, which
    I called `chrom`, short for chromosome:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我使用术语“等位基因”来表示整个路径中的一个单独步骤，我将其称为“chrom”，是染色体（chromosome）的简称：
- en: '[PRE33]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'How do we create our paths to start with? The `make_new_individual` function
    builds our initial population of chromosomes, or paths, out of random numbers.
    Each chromosome contains a path made up of a number from 0 to 26 that represents
    all the valid combinations of motor commands. We set the path length to be a random
    number from 10 to 60:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何创建初始路径？`make_new_individual`函数使用随机数构建我们的初始染色体种群，或路径。每个染色体包含由0到26的数字组成的路径，这些数字代表所有有效的电机命令组合。我们将路径长度设置为10到60之间的随机数：
- en: '[PRE34]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We use the `roulette` function to pick a portion of our population to continue.
    Each generation, we select from the top 50% of scoring individuals to donate their
    DNA to create the next generation. We want the reward value of the path, or chromosome,
    to weigh the selection process; the higher the reward score, the better chance
    of having children. This is part of our selection process:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用 `roulette` 函数选择我们种群的一部分继续进行。每一代，我们从得分最高的 50% 的个体中选择他们的 DNA 来创建下一代。我们希望路径或染色体的奖励值在选择过程中起到作用；奖励分数越高，成为后代的机会就越大。这是我们选择过程的一部分：
- en: '[PRE35]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We start by building our initial population out of random parts. Their original
    fitness will be very low: about 13% or less. We maintain a pool of 300 individual
    paths, which we call chromosomes:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先用随机部分构建初始种群。它们的原始适应度将非常低：大约 13% 或更低。我们维持一个包含 300 个个体路径的池，我们称之为染色体：
- en: '[PRE36]'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Here we set up the loop to go through 100 generations of our natural selection
    process. We begin by computing the fitness of each individual and adding that
    score to a fitness list with an index pointing back to the chromosome:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们设置循环以遍历 100 代的自然选择过程。我们首先计算每个个体的适应度，并将该分数添加到一个适应度列表中，该列表的索引指向染色体：
- en: '[PRE37]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We sort the fitness in inverse order to get the best individuals. The largest
    number should be first:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们按逆序排序适应度以获得最佳个体。最大的数字应该排在第一位：
- en: '[PRE38]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We keep the top 50% of the population and discard the bottom 50%. The bottom
    half is out of the gene pool as being unfit:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们保留种群中排名前 50% 的个体，并丢弃排名后 50% 的个体。下半部分由于不适应而被排除在基因池之外：
- en: '[PRE39]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We pull out the top performer from the whole list and put it into the **hall
    of fame** (**HOF**). This will eventually be the output of our process. In the
    meantime, we use the HOF or **HOF fitness** (**HOFF**) value as a measure of the
    fitness of this generation:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从整个列表中挑选出表现最好的个体，并将其放入 **名人堂**（**HOF**）。这将是我们的最终输出。同时，我们使用 HOF 或 **HOF 适应度**（**HOFF**）值作为这一代适应度的衡量标准：
- en: '[PRE40]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We store the HOFF value in a `trainingData` list so we can graph the results
    at the end of the program:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将 HOFF 值存储在 `trainingData` 列表中，以便在程序结束时绘制结果图：
- en: '[PRE41]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'At this phase, we have deleted the bottom 50% of our population, removing the
    worst performers. Now we need to replace them with the children of the best performers
    of this generation. We are going to use crossover as our mating technique. There
    are several types of genetic mating that can produce successful offspring. Crossover
    is popular and a good place to start, as well as being easy to code. All we are
    doing is picking a spot in the genome and taking the first half from one parent,
    and the second half from the other. We pick our parents to *mate* randomly from
    the remaining population, weighted proportionally to their fitness. This is referred
    to as **roulette wheel selection**. The better individuals are weighted more heavily
    and are more likely to be selected for breeding. We create 140 new individuals
    as children of this generation:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个阶段，我们已经删除了种群中排名后 50% 的个体，移除了表现最差的个体。现在我们需要用这一代最佳表现者的后代来替换他们。我们将使用交叉作为配对技术。有几种遗传配对可以产生成功的后代。交叉很受欢迎，是一个好的起点，同时也很容易编码。我们所做的一切只是在基因组中挑选一个位置，从一位父母那里取前半部分，从另一位父母那里取后半部分。我们从剩余的种群中随机选择父母进行配对，按其适应度成比例加权。这被称为
    **轮盘赌选择**。更好的个体被赋予更高的权重，更有可能被选中进行繁殖。我们为这一代创造了 140 个新的个体：
- en: '[PRE42]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Our next step is **mutation**. In real natural selection, there is a small
    chance that DNA will get corrupted or changed by cosmic rays, miscopying of the
    sequence, or other factors. Some mutations are beneficial, and some are not. We
    create our version of this process by having a small chance (1/100 or so) that
    one gene in our new child path is randomly changed into some other value:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的下一步是 **变异**。在真实自然选择中，DNA 有很小的机会会被宇宙射线、序列的错误复制或其他因素所损坏或改变。一些变异是有益的，而一些则不是。我们通过让新后代路径中的一个基因有很小的机会（大约
    1/100）随机改变成其他值来创建我们这个过程的版本：
- en: '[PRE43]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Now that we have done all our processing, we add this new child path to our
    population, and get ready for the next generation to be evaluated. We record some
    data and loop back to the start:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经完成了所有的处理，我们将这条新的后代路径添加到我们的种群中，并为下一代评估做好准备。我们记录一些数据并返回到起点：
- en: '[PRE44]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'So, how did we do with our mad genetic experiment? The following output chart
    speaks for itself:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们的疯狂遗传实验做得怎么样？以下输出图表自说自话：
- en: "![Figure 5.5 – Learning curve for the \uFEFFGA solution](img/B19846_05_5.jpg)"
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![图5.5 – GA解决方案的学习曲线](img/B19846_05_5.jpg)'
- en: Figure 5.5 – Learning curve for the GA solution
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5 – GA解决方案的学习曲线
- en: The GA, for all it seems like a bit of voodoo programming, works quite well
    as a machine learning tool for this specific case of training our robot arm. Our
    solution peaked at 99.76% of the goal (about 2 mm) after just 90 generations or
    so, which is quite fast for an AI learning process. You can see the smooth nature
    of the learning that shows that this approach can be used to solve path-planning
    problems for our robot arm. I have to admit that I was quite skeptical about this
    process, but it seems to work quite well for this particular problem domain.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管GA看起来像是一种有点像巫术的编程，但作为训练我们机器人臂的特定案例的机器学习工具，它工作得相当好。我们的解决方案在90代左右达到了99.76%的目标（大约2毫米），这对于人工智能学习过程来说相当快。你可以看到学习的平滑性，这表明这种方法可以用来解决我们机器人臂的路径规划问题。我必须承认，我对这个过程相当怀疑，但它似乎在这个特定的问题领域工作得相当好。
- en: The programming really was not too hard, and you can spend some time improving
    the process by tweaking the parameters of the GA. What if we had a smaller population?
    What if we changed the fitness criteria? Get in there, muck about, and see what
    you can learn.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 编程实际上并不太难，你可以花些时间通过调整GA的参数来改进这个过程。如果我们有一个更小的种群会怎样？如果我们改变了适应度标准会怎样？进去，捣鼓一下，看看你能学到什么。
- en: Alternative robot arm ML approaches
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 替代的机器人臂机器学习方法
- en: The realm of robot arm control via machine learning is really just getting started.
    There are a couple of research avenues I wanted to bring to your attention as
    you look for further study. One way to approach our understanding of robot movement
    is to consider the balance between *exploitation* and *exploration*. Exploitation
    is getting the robot to its goal as quickly as possible. Exploration is using
    the space around the robot to try new things. The path-planning program may have
    been stuck on a local minimum (think of this as a blind alley), and there could
    be better, more optimal solutions available that had not been considered.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 通过机器学习进行机器人臂控制的领域实际上才刚刚开始。有几个研究方向我想在您寻找进一步研究时引起您的注意。理解机器人运动的一种方法就是考虑*利用*和*探索*之间的平衡。利用就是尽可能快地将机器人带到目标位置。探索则是利用机器人周围的空间尝试新事物。路径规划程序可能已经陷入了局部最小值（可以想象成死胡同），可能存在更好的、更优的解决方案，而这些方案尚未被考虑。
- en: There is also more than one way to teach a robot. We have been using a form
    of self-exploration in our training. What if we could show the robot what to do
    and have it learn by example? We could let the robot observe a human doing the
    same task, and have it try to emulate the results. Let’s discuss some alternative
    methods in the following sections.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 教导机器人的方法不止一种。我们在训练中一直使用一种自我探索的形式。如果我们能够向机器人展示该做什么，并让它通过示例学习会怎样？我们可以让机器人观察人类执行同样的任务，并让它尝试模仿结果。让我们在接下来的章节中讨论一些替代方法。
- en: Google’s SAC-X
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 谷歌的SAC-X
- en: Google is trying a slightly different approach to the robot arm problem. In
    their **Scheduled Auxiliary Control** (**SAC- X**) program, they surmise that
    it can be quite difficult to assign reward points to individual movements of the
    robot arm. They break down a complex task into smaller auxiliary tasks, and give
    reward points for those supporting tasks to let the robot build up to a complicated
    challenge. If we were stacking blocks with a robot arm, we might separate picking
    up the block as one task, moving with the block in hand as another, and so on.
    Google referred to this as a *sparse reward* problem if reinforcement was only
    used on the main task, stacking a block on top of another. You can imagine how,
    in the process of teaching a robot to stack blocks, there would be thousands of
    failed attempts before a successful move resulted in a reward.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌正在尝试一种稍微不同的方法来解决机器人臂问题。在他们的**计划辅助控制**（**SAC-X**）程序中，他们认为给机器人臂的个别动作分配奖励点可能相当困难。他们将复杂任务分解成更小的辅助任务，并为支持这些任务的辅助任务分配奖励点，让机器人逐步建立起面对复杂挑战的能力。如果我们用机器人臂堆叠方块，我们可能会将拾取方块作为一个任务，手持方块移动作为另一个任务，等等。谷歌将这种如果只在主要任务上使用强化，即堆叠方块在另一个方块上作为**稀疏奖励**问题。你可以想象，在教机器人堆叠方块的过程中，会有数千次失败的尝试，直到一个成功的移动导致奖励的产生。
- en: Amazon Robotics Challenge
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 亚马逊机器人挑战赛
- en: Amazon has millions and millions of boxes, parts, bits, and other things on
    its shelves. The company needs to get the stuff from the shelves into small boxes
    so they can ship it to you as fast as possible when you order it. For the last
    few years, Amazon has sponsored the *Amazon Robotics Challenge*, where teams from
    universities were invited to use robot arms to pick up items off a shelf and,
    you guessed it, put them into a box.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊有数百万个箱子、零件、碎片和其他东西堆放在货架上。该公司需要将这些东西从货架上取下来放入小箱子中，以便在你下单时尽可能快地将它们运送到你那里。在过去几年中，亚马逊赞助了*亚马逊机器人挑战赛*，邀请来自大学的团队使用机械臂从货架上取下物品，然后，正如你所猜到的，将它们放入箱子中。
- en: When you consider that Amazon sells almost everything imaginable, this is a
    real challenge. In 2017, a team from Queensland, Australia, won the challenge
    with a low-cost arm and a really good hand-tracking system.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 当你考虑到亚马逊几乎销售所有可以想象得到的东西时，这是一个真正的挑战。2017年，来自澳大利亚昆士兰州的一支团队凭借一个低成本机械臂和一个非常好的手部追踪系统赢得了挑战。
- en: Summary
  id: totrans-281
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Our task for this chapter was to use machine learning to teach the robot how
    to use its robot arm. We used two techniques with some variations. We used a variety
    of reinforcement learning techniques, or Q-learning, to develop a movement path
    by selecting individual actions based on the robot’s arm state. Each motion was
    scored individually as a reward, and as part of the overall path as a value. The
    process stored the results of the learning in a Q-matrix that could be used to
    generate a path. We improved our first cut of the reinforcement learning program
    by indexing, or encoding, the motions from a 27-element array of possible combinations
    of motors as numbers from 0 to 26, and likewise indexing the robot state to a
    state lookup table. This resulted in a 40x speedup of the learning process. Our
    Q-learning approach struggled with the large number of states that the robot arm
    could be in.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的任务是使用机器学习教机器人如何使用它的机械臂。我们使用了两种技术，并做了一些变体。我们使用了多种强化学习技术，或称为Q学习，通过根据机器人机械臂的状态选择单个动作来开发运动路径。每个动作都被单独评分作为奖励，作为整体路径的一部分作为价值。这个过程将学习结果存储在一个Q矩阵中，可以用来生成路径。我们通过索引，或编码，从可能的电机组合的27元素数组中提取动作作为从0到26的数字，同样将机器人状态索引到状态查找表中。这导致学习过程的速度提高了40倍。我们的Q学习方法在处理机器人机械臂可能处于的大量状态时遇到了困难。
- en: Our second technique was a GA. We created individual random paths to make a
    population. We created a fitness function to score each path against our goal
    and kept the top performers from each generation. We then crossed genetic material
    from two somewhat randomly selected individuals to create a new child path. The
    GA also simulated mutation by having a slight chance of random changes in the
    steps of a path. The results for the GA showed no problem with the state space
    complexity of our robot arm and generated a valid path after just a few generations.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第二种技术是遗传算法（GA）。我们创建了个体的随机路径来形成一个种群。我们创建了一个适应度函数来评估每条路径与我们的目标，并保留每一代的顶尖表现者。然后，我们从两个随机选择的个体中交叉遗传物质来创建一个新的子路径。GA还通过在路径步骤中随机改变一小部分来模拟突变。GA的结果显示，对于我们的机器人机械臂的状态空间复杂性没有问题，并在几代之后生成了一个有效的路径。
- en: Why do we go to all of this trouble? We use machine learning techniques when
    other empirical methods are either difficult, not reliable, or don’t produce solutions
    in a reasonable amount of time. We can also tackle much more complex tasks with
    these techniques that might be intractable to a brute-force or math-only solution.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为什么要费这么大的劲？当其他经验方法要么难以实现，要么不可靠，或者不产生在合理时间内解决问题的解决方案时，我们使用机器学习技术。我们还可以使用这些技术解决可能对暴力或仅数学解决方案难以处理的大量更复杂的问题。
- en: In the next chapter, we’ll be adding a voice interface to the robot with natural
    language processing, so you can talk to the robot and it will listen – and talk
    back.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将为机器人添加一个带有自然语言处理功能的语音界面，这样你就可以与机器人交谈，它会倾听——并回应。
- en: Questions
  id: totrans-286
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: In Q-learning, what does the Q stand for?
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Q学习中，Q代表什么？
- en: '**Hint**: You will have to research this yourself.'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**提示**：你需要自己进行研究。'
- en: What could we do to limit the number of states that the Q-learning algorithm
    has to search through?
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们能做些什么来限制Q学习算法需要搜索的状态数量？
- en: What effect does changing the learning rate have on the learning process?
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 改变学习率对学习过程有什么影响？
- en: What function or parameter serves to penalize longer paths in the Q-learning
    equation? What effect does increasing or decreasing this function have?
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Q-learning 方程中，哪个函数或参数用于惩罚较长的路径？增加或减少这个函数会有什么影响？
- en: In the genetic algorithm, how would you go about penalizing longer paths so
    that shorter paths (fewer number of steps) would be preferred?
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在遗传算法中，你将如何对较长的路径进行惩罚，以便更偏好较短的路径（步骤数量较少）？
- en: Look up the SARSA variation of Q-learning. How would you implement the SARSA
    technique into program 2.
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查找 SARSA 变体的 Q-learning。你将如何将 SARSA 技术应用到程序 2 中。
- en: What effect does changing the learning rate in the genetic algorithm have? What
    are the upper and lower bounds of the learning rate?
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 改变遗传算法中的学习率会有什么影响？学习率的上限和下限是多少？
- en: In a genetic algorithm, what effect does reducing the population have?
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在遗传算法中，减少种群数量会有什么影响？
- en: Further reading
  id: totrans-296
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '*Python Deep Learning* by Zocca, Spacagna, Slater, and Roelants, Packt Publishing'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《Python 深度学习》*，作者 Zocca, Spacagna, Slater 和 Roelants，Packt 出版'
- en: '*Artificial Intelligence with Python* by Prateek Joshi, Packt Publishing'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《使用 Python 的人工智能》*，作者 Prateek Joshi，Packt 出版'
- en: '*AI Junkie: Genetic Algorithm – A Brief Overview*, retrieved from [http://www.ai-junkie.com/ga/intro/gat2.html](http://www.ai-junkie.com/ga/intro/gat2.html)'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《AI 爱好者：遗传算法 - 简要概述》*，来自 [http://www.ai-junkie.com/ga/intro/gat2.html](http://www.ai-junkie.com/ga/intro/gat2.html)'
- en: '*Basic Reinforcement Learning Tutorial 2:* *SARSA*: [https://github.com/vmayoral/basic_reinforcement_learning/tree/master/tutorial2](https://github.com/vmayoral/basic_reinforcement_learning/tree/master/tutorial2)'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《基本强化学习教程 2：SARSA》*：[https://github.com/vmayoral/basic_reinforcement_learning/tree/master/tutorial2](https://github.com/vmayoral/basic_reinforcement_learning/tree/master/tutorial2)'
- en: '*Google DeepMind Blog: Learning by Playing (Robot Arm (**SAC-X))*: [https://deepmind.com/blog/learning-playing/](https://deepmind.com/blog/learning-playing/)'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Google DeepMind 博客：通过玩耍学习（机器人臂（**SAC-X**））*：[https://deepmind.com/blog/learning-playing/](https://deepmind.com/blog/learning-playing/)'
