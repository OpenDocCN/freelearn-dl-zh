<html><head></head><body>
<div id="_idContainer388">
<h1 class="chapter-number" id="_idParaDest-206"><a id="_idTextAnchor215"/><span class="koboSpan" id="kobo.1.1">11</span></h1>
<h1 id="_idParaDest-207"><a id="_idTextAnchor216"/><span class="koboSpan" id="kobo.2.1">The Future Ahead</span></h1>
<p><span class="koboSpan" id="kobo.3.1">In this book, we started with how a neural network could digest text. </span><span class="koboSpan" id="kobo.3.2">As we have seen, neural networks do not do this natively but require the text to be processed. </span><span class="koboSpan" id="kobo.3.3">Simple neural networks can be used for some basic tasks such as classification, but human language carries an enormous amount of </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">complex information.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">In </span><em class="italic"><span class="koboSpan" id="kobo.6.1">Chapters 2</span></em><span class="koboSpan" id="kobo.7.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.8.1">3</span></em><span class="koboSpan" id="kobo.9.1">, we saw how we need sophisticated models in order to use semantic and syntactic information. </span><span class="koboSpan" id="kobo.9.2">The emergence of transformers and LLMs has made it possible to have models capable of reasoning and storing enormous amounts of factual knowledge. </span><span class="koboSpan" id="kobo.9.3">These multipurpose knowledge and skills have enabled LLMs to solve tasks for which they have not been trained (coding, solving math problems, and so on). </span><span class="koboSpan" id="kobo.9.4">Nevertheless, LLMs have problems such as a lack of specialized domain knowledge, continual learning, being able to use tools, and so on. </span><span class="koboSpan" id="kobo.9.5">Thus, from </span><a href="B21257_04.xhtml#_idTextAnchor058"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.10.1">Chapter 4</span></em></span></a><span class="koboSpan" id="kobo.11.1"> onward, we described systems that extend the capabilities of LLMs and which are designed to solve </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">LLMs’ problems.</span></span></p>
<p><span class="koboSpan" id="kobo.13.1">In this chapter, we will discuss how some problems remain to be solved and what lies ahead in the future. </span><span class="koboSpan" id="kobo.13.2">We will start by presenting how agents can be used in different industries and the revolution that awaits us thanks to agents. </span><span class="koboSpan" id="kobo.13.3">Then, we will discuss some of the most pressing questions both technically </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">and ethically.</span></span></p>
<p><span class="koboSpan" id="kobo.15.1">In this chapter, we’ll be covering the </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">following topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.17.1">AI agents </span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">in healthcare</span></span></li>
<li><span class="koboSpan" id="kobo.19.1">AI agents in </span><span class="No-Break"><span class="koboSpan" id="kobo.20.1">other sectors</span></span></li>
<li><span class="koboSpan" id="kobo.21.1">Challenges and </span><span class="No-Break"><span class="koboSpan" id="kobo.22.1">open questions</span></span></li>
</ul>
<h1 id="_idParaDest-208"><a id="_idTextAnchor217"/><span class="koboSpan" id="kobo.23.1">AI agents in healthcare</span></h1>
<p><span class="koboSpan" id="kobo.24.1">One</span><a id="_idIndexMarker1476"/><span class="koboSpan" id="kobo.25.1"> of the most exciting prospects for AI development is the possibility of having autonomous systems capable of conducting scientific discoveries on their own. </span><span class="koboSpan" id="kobo.25.2">This new paradigm is referred to as the </span><em class="italic"><span class="koboSpan" id="kobo.26.1">AI scientist</span></em><span class="koboSpan" id="kobo.27.1">. </span><span class="koboSpan" id="kobo.27.2">Throughout this book, we have seen some examples of systems that are thought to be in accordance with this</span><a id="_idIndexMarker1477"/><span class="koboSpan" id="kobo.28.1"> idea (ChemCrow, the virtual lab, and so on). </span><span class="koboSpan" id="kobo.28.2">In this section, we will discuss this paradigm in more detail: where the research is heading, the challenges faced, and </span><span class="No-Break"><span class="koboSpan" id="kobo.29.1">future developments.</span></span></p>
<p><span class="koboSpan" id="kobo.30.1">The idea behind an AI agent is to exploit LLMs in combination with tools (agents), as we have seen so far. </span><span class="koboSpan" id="kobo.30.2">In the future, researchers would like to add an experimental platform (an autonomous system able to conduct experiments by itself) to these systems so that they can conduct experiments independently. </span><span class="koboSpan" id="kobo.30.3">The complexity of biology could then be approached in a series of actionable tasks, where an LLM could break down a problem into a series of subtasks and autonomously solve them. </span><span class="koboSpan" id="kobo.30.4">The goal then would be to achieve discoveries not only more quickly but also more efficiently. </span><span class="koboSpan" id="kobo.30.5">The AI scientist would then be able to produce research at a speed and scale that would otherwise be impossible </span><span class="No-Break"><span class="koboSpan" id="kobo.31.1">for humans.</span></span></p>
<p><span class="koboSpan" id="kobo.32.1">In the first phase, humans would be at the center of the project. </span><span class="koboSpan" id="kobo.32.2">Scientists would provide input and criticism to the LLMs, and the models would incorporate this feedback into the process. </span><span class="koboSpan" id="kobo.32.3">During this iterative process, the model would analyze the problem, search the internet for information, and devise a plan, under human supervision (or otherwise using handcrafted prompts to guide it through the process). </span><span class="koboSpan" id="kobo.32.4">In such a scenario, an LLM would be an assistant to humans, where it proposes solutions and hypotheses. </span><span class="koboSpan" id="kobo.32.5">The ultimate goal would be to have an </span><span class="No-Break"><span class="koboSpan" id="kobo.33.1">autonomous agent.</span></span></p>
<p><span class="koboSpan" id="kobo.34.1">This</span><a id="_idIndexMarker1478"/><span class="koboSpan" id="kobo.35.1"> vision is the culmination of a process that has been ongoing for decades in biomedical research. </span><span class="koboSpan" id="kobo.35.2">In fact, since the early 1990s, people have been talking about a new paradigm: the use of data-driven models. </span><span class="koboSpan" id="kobo.35.3">This paradigm shift has occurred because of technological advances and the vast availability of data. </span><span class="koboSpan" id="kobo.35.4">Biomedical research produces a large amount of data, and in the last three decades, this information has begun to be centralized in a series of databases. </span><span class="koboSpan" id="kobo.35.5">Simultaneously with this integration and new accessibility of information, all sorts of tools have been developed by researchers. </span><span class="koboSpan" id="kobo.35.6">At first, these computational tools were models and statistical methods, but gradually, biomedical research has also benefited from machine learning and AI models. </span><span class="koboSpan" id="kobo.35.7">In a sense, the successes of one propelled the successes of the other, and vice versa. </span><span class="koboSpan" id="kobo.35.8">The more data was centralized and made available to the community, the more this allowed new models to be developed. </span><span class="koboSpan" id="kobo.35.9">Discoveries obtained through new models and methods prompted the production of new experiments and new data. </span><span class="koboSpan" id="kobo.35.10">For example, transcriptomics experiments allowed for large datasets, which were perfect for developing new machine learning models and tools. </span><span class="koboSpan" id="kobo.35.11">These models allowed some biological questions to be answered, and these answers led to new experiments and thus new data. </span><span class="koboSpan" id="kobo.35.12">AlphaFold2 was only possible because of the millions of structures on</span><a id="_idIndexMarker1479"/><span class="koboSpan" id="kobo.36.1"> the </span><strong class="bold"><span class="koboSpan" id="kobo.37.1">Protein Data Bank</span></strong><span class="koboSpan" id="kobo.38.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.39.1">PDB</span></strong><span class="koboSpan" id="kobo.40.1">). </span><span class="koboSpan" id="kobo.40.2">AlphaFold2 allowed </span><a id="_idIndexMarker1480"/><span class="koboSpan" id="kobo.41.1">researchers to produce new hypotheses, later confirmed by new experiments and new structures on the PDB. </span><span class="koboSpan" id="kobo.41.2">In addition, the limitations of AlphaFold2 led researchers to collect new data for specific questions. </span><span class="koboSpan" id="kobo.41.3">These new data and experimental verifications led to new models, creating </span><span class="No-Break"><span class="koboSpan" id="kobo.42.1">positive feedback.</span></span></p>
<p><span class="koboSpan" id="kobo.43.1">As you can see, when the </span><a id="_idIndexMarker1481"/><span class="koboSpan" id="kobo.44.1">LLMs arrived, fertile ground for further revolution was already present. </span><span class="koboSpan" id="kobo.44.2">First, a vast amount of data (millions of articles and huge databases of experimental data) was available, thus allowing models either to be trained on this data or to be able to search for information through dedicated databases. </span><span class="koboSpan" id="kobo.44.3">For example, a model could search for information it missed on biological sequences through dedicated APIs. </span><span class="koboSpan" id="kobo.44.4">Or, an LLM could use RAG to search for information on new articles. </span><span class="koboSpan" id="kobo.44.5">Second, the community produced thousands of models to solve specific tasks. </span><span class="koboSpan" id="kobo.44.6">An LLM does not need to know how to solve a task; there is a curated list of resources it can use for a whole range of subtasks. </span><span class="koboSpan" id="kobo.44.7">An LLM then does not need additional training but only needs to know how to orchestrate these specific task tools and models. </span><span class="koboSpan" id="kobo.44.8">At this point, we have everything we need to be able to create an agent system. </span><span class="koboSpan" id="kobo.44.9">Agents can be found at every step of the biomedical research process, thus enabling future drug development in a shorter time frame and saving </span><span class="No-Break"><span class="koboSpan" id="kobo.45.1">important resources.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer357">
<span class="koboSpan" id="kobo.46.1"><img alt="Figure 11.1 – Empowering biomedical research with AI agents (https://www.cell.com/cell/fulltext/S0092-8674(24)01070-5)" src="image/B21257_11_01.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.47.1">Figure 11.1 – Empowering biomedical research with AI agents (</span><a href="https://www.cell.com/cell/fulltext/S0092-8674(24)01070-5"><span class="koboSpan" id="kobo.48.1">https://www.cell.com/cell/fulltext/S0092-8674(24)01070-5</span></a><span class="koboSpan" id="kobo.49.1">)</span></p>
<h2 id="_idParaDest-209"><a id="_idTextAnchor218"/><span class="koboSpan" id="kobo.50.1">Biomedical AI agents</span></h2>
<p><span class="koboSpan" id="kobo.51.1">ChemCrow</span><a id="_idIndexMarker1482"/><span class="koboSpan" id="kobo.52.1"> is an example of this type of agent, defined for a specific case and domain. </span><span class="koboSpan" id="kobo.52.2">The reasoning of the system is limited to the specific tasks; the agent must use the experimental data and existing knowledge. </span><span class="koboSpan" id="kobo.52.3">It is the researcher who defines both the hypothesis and the tasks; the system only has to complete them. </span><span class="koboSpan" id="kobo.52.4">Level 1 can be considered orchestrators under the </span><a id="_idIndexMarker1483"/><span class="koboSpan" id="kobo.53.1">supervision of a </span><span class="No-Break"><span class="koboSpan" id="kobo.54.1">human being.</span></span></p>
<p><span class="koboSpan" id="kobo.55.1">For example, ChemCrow has demonstrated concrete outcomes in research automation: according to a study published in </span><em class="italic"><span class="koboSpan" id="kobo.56.1">Nature Machine Intelligence</span></em><span class="koboSpan" id="kobo.57.1">, ChemCrow autonomously planned and executed the synthesis of an insect repellent and three organocatalysts, and guided the screening and synthesis of a novel chromophore (</span><em class="italic"><span class="koboSpan" id="kobo.58.1">Nature Machine Intelligence</span></em><span class="koboSpan" id="kobo.59.1">, 2024). </span><span class="koboSpan" id="kobo.59.2">Additionally, by integrating 18 specialized tools, ChemCrow has streamlined complex chemical research processes, significantly increasing efficiency and accessibility for both expert and non-expert users (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.60.1">ScienceDaily</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.61.1">, 2024).</span></span></p>
<p><span class="koboSpan" id="kobo.62.1">Most agent approaches are based on the use of a central LLM. </span><span class="koboSpan" id="kobo.62.2">An LLM is pre-trained with general knowledge and then aligned to human preferences to make the most of its knowledge and the skills it has learned during pre-training. </span><span class="koboSpan" id="kobo.62.3">The biomedical field requires specialized expertise and knowledge. </span><span class="koboSpan" id="kobo.62.4">Therefore, various experiments have often been conducted where an LLM has been fine-tuned to specialize in medicine (e.g., BioGPT, NYUTron, and MedPalm). </span><span class="koboSpan" id="kobo.62.5">This approach is clearly expensive, and a model becomes outdated quickly (thousands of papers are published every day). </span><span class="koboSpan" id="kobo.62.6">So, different approaches have been sought in which it is not necessary to conduct repeated rounds </span><span class="No-Break"><span class="koboSpan" id="kobo.63.1">of fine-tuning.</span></span></p>
<p><span class="koboSpan" id="kobo.64.1">One option is to try and use one model (one LLM) but with different professional expertise (assigning a specific role at each round). </span><span class="koboSpan" id="kobo.64.2">The idea is to use one model, but craft prompts to assign a role to the LLM (biologist, clinician, chemist, and so on). </span><span class="koboSpan" id="kobo.64.3">There are also other alternatives, for example, using instruction tuning to create an expert for a domain (so rather than aligning the model on specific knowledge, align it on specific tasks that would be an expert’s). </span><span class="koboSpan" id="kobo.64.4">For example, we can ask a model to perform a task (</span><em class="italic"><span class="koboSpan" id="kobo.65.1">Write a sequence for a protein X that has a function Y</span></em><span class="koboSpan" id="kobo.66.1">) or provide it with a specific role (</span><em class="italic"><span class="koboSpan" id="kobo.67.1">You are a biologist specializing in proteomics; your task is: write a sequence for a protein X that has a function Y)</span></em><span class="koboSpan" id="kobo.68.1">. </span><span class="koboSpan" id="kobo.68.2">A complex task can be performed by more than one specialist; for example, we can provide the model with the task directly (</span><em class="italic"><span class="koboSpan" id="kobo.69.1">Identify a gene involved in the interaction of the Covid virus with a respiratory cell; design an antibody to block it</span></em><span class="koboSpan" id="kobo.70.1">) or break it down into several subsequent tasks (a first task with a first role such as </span><em class="italic"><span class="koboSpan" id="kobo.71.1">You are a professional virologist with expertise on the Covid19 virus; your task is: identify a gene involved in the interaction of the Covid virus with a respiratory cell</span></em><span class="koboSpan" id="kobo.72.1"> and then assign the model a second task: </span><em class="italic"><span class="koboSpan" id="kobo.73.1">You are a computational immunologist with expertise in designing blocking antibodies; your task is: design an antibody to block it</span></em><span class="koboSpan" id="kobo.74.1">). </span><span class="koboSpan" id="kobo.74.2">In contrast to the previous approach to learning, a methodology (solving tasks) does not quickly become outdated like domain knowledge. </span><span class="koboSpan" id="kobo.74.3">Other authors suggest that one can instead simply use </span><span class="No-Break"><span class="koboSpan" id="kobo.75.1">in-context learning.</span></span></p>
<p><span class="koboSpan" id="kobo.76.1">This strategy means providing the model in context with a whole range of information that would be needed to play the role of a specialist (specific information about the role the model is to impersonate: definition, skills, specific knowledge, and so on). </span><span class="koboSpan" id="kobo.76.2">This strategy is very similar to assigning a role by prompt, but we give much more information. </span><span class="koboSpan" id="kobo.76.3">Although these prompts are full of information and instructions, the model does not always follow</span><a id="_idIndexMarker1484"/><span class="koboSpan" id="kobo.77.1"> them. </span><span class="koboSpan" id="kobo.77.2">Also, it is difficult to describe in a prompt what a specialist’s role is. </span><span class="koboSpan" id="kobo.77.3">So, an additional strategy is that the model independently generates and refines the </span><span class="No-Break"><span class="koboSpan" id="kobo.78.1">role prompt.</span></span></p>
<p><span class="koboSpan" id="kobo.79.1">Agents may therefore have different tools at their disposal and different purposes. </span><span class="koboSpan" id="kobo.79.2">The rationale for this multi-role approach is that an LLM does not have a deep understanding of planning and reasoning but still shows acquired skills. </span><span class="koboSpan" id="kobo.79.3">So, instead of one agent having to handle the whole process, we have a pool of agents where each agent has to take care of a limited subtask. </span><span class="koboSpan" id="kobo.79.4">Typically, in addition to the definition of different types of agents, there is also the definition of working protocol (for example, in the virtual lab, in addition to agents, a protocol of team and individual meetings </span><span class="No-Break"><span class="koboSpan" id="kobo.80.1">was defined).</span></span></p>
<p><span class="koboSpan" id="kobo.81.1">In any case, although there is so much expectation about a multi-agent approach where there is an LLM acting with several people, some studies give mixed results. </span><span class="koboSpan" id="kobo.81.2">In fact, some authors say that what are formally called “personas” (assigning a role to an LLM) do not give a particular advantage, except in rare cases. </span><span class="koboSpan" id="kobo.81.3">In any case, to date, it is necessary for these prompts to be precisely designed to be effective (and it is a laborious, </span><span class="No-Break"><span class="koboSpan" id="kobo.82.1">trial-and-error process).</span></span></p>
<p><span class="koboSpan" id="kobo.83.1">Since LLMs have good critical thinking skills, it has been suggested that they can be used in brainstorming. </span><span class="koboSpan" id="kobo.83.2">Although LLMs have no reasoning skills and limited creativity, they can conduct a quick survey of the literature. </span><span class="koboSpan" id="kobo.83.3">Agents can then be used to propose ideas, evaluate the best, refine and prioritize, provide critique, and discuss feasibility. </span><span class="koboSpan" id="kobo.83.4">One interesting possibility is to use a pool of agents where each agent has different expertise, which mimics the brainstorming </span><span class="No-Break"><span class="koboSpan" id="kobo.84.1">discussion process.</span></span></p>
<p><span class="koboSpan" id="kobo.85.1">Different frameworks can be created where agents interact with humans or with each other. </span><span class="koboSpan" id="kobo.85.2">For example, leveraging critique capabilities can facilitate the creation of agents with distinct goals to foster debate. </span><span class="koboSpan" id="kobo.85.3">One group of agents could focus on critiquing and challenging ideas, while another could aim to persuade and advocate for their viewpoints. </span><span class="koboSpan" id="kobo.85.4">Each agent could have different expertise and have different tools at their disposal. </span><span class="koboSpan" id="kobo.85.5">This approach, therefore, evaluates a research proposition from different perspectives. </span><span class="koboSpan" id="kobo.85.6">A research idea can then be viewed as an optimization problem where agents try to arrive at the best solution. </span><span class="koboSpan" id="kobo.85.7">In addition to a setting where agents are competing, the possibility of cooperation can also be exploited. </span><span class="koboSpan" id="kobo.85.8">Agents provide feedback sequentially on a proposition with the purpose of improving an idea. </span><span class="koboSpan" id="kobo.85.9">The two frameworks are not necessarily opposites but can be reconciled in systems where each idea goes through feedback loops and critique. </span><span class="koboSpan" id="kobo.85.10">Because the frameworks are organized with natural language prompts, multi-agent systems provide </span><span class="No-Break"><span class="koboSpan" id="kobo.86.1">unique flexibility.</span></span></p>
<p><span class="koboSpan" id="kobo.87.1">Similarly, it is not necessary that all agents be equal peers; hierarchical levels can be organized. </span><span class="koboSpan" id="kobo.87.2">For example, one agent may have the role of facilitating discussion or having greater decision-making weight. </span><span class="koboSpan" id="kobo.87.3">In the virtual lab, there is an agent that has the role of principal investigator, which initiates the discussion and has decision-making power. </span><span class="koboSpan" id="kobo.87.4">Thus, multiple decision-making levels can be instituted that are managed by </span><span class="No-Break"><span class="koboSpan" id="kobo.88.1">sophisticated architecture.</span></span></p>
<p><span class="koboSpan" id="kobo.89.1">Note that agents </span><a id="_idIndexMarker1485"/><span class="koboSpan" id="kobo.90.1">can then design experiments and, conjugated with experiential tools, these experiments can be accomplished. </span><span class="koboSpan" id="kobo.90.2">This would provide a new level of capability, toward a process that becomes end </span><span class="No-Break"><span class="koboSpan" id="kobo.91.1">to end.</span></span></p>
<p><span class="koboSpan" id="kobo.92.1">In this regard, Gao (</span><a href="https://www.cell.com/cell/fulltext/S0092-8674(24)01070-5"><span class="koboSpan" id="kobo.93.1">https://www.cell.com/cell/fulltext/S0092-8674(24)01070-5</span></a><span class="koboSpan" id="kobo.94.1">) defined three levels of autonomy for an agent system in </span><span class="No-Break"><span class="koboSpan" id="kobo.95.1">biomedical research:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.96.1">Level 0</span></strong><span class="koboSpan" id="kobo.97.1">: A machine learning model is used as a tool by a researcher. </span><span class="koboSpan" id="kobo.97.2">The researcher defines the hypothesis, uses the model for a specific task, and evaluates the output. </span><em class="italic"><span class="koboSpan" id="kobo.98.1">Level 0</span></em><span class="koboSpan" id="kobo.99.1"> systems are tools such as models for making predictions in the </span><span class="No-Break"><span class="koboSpan" id="kobo.100.1">biological field.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.101.1">Level 1</span></strong><span class="koboSpan" id="kobo.102.1">: This is also defined as </span><em class="italic"><span class="koboSpan" id="kobo.103.1">AI agent as a research assistant</span></em><span class="koboSpan" id="kobo.104.1">; the researcher defines a hypothesis, specifies the tasks that need to be conducted to get to the goal, and the agent uses a restricted set of tools. </span><span class="koboSpan" id="kobo.104.2">ChemCrow is an example of this type of agent, defined for a specific case and domain. </span><span class="koboSpan" id="kobo.104.3">The reasoning of the system is limited to the specific tasks; the agent must use the experimental data and existing knowledge. </span><span class="koboSpan" id="kobo.104.4">It is the researcher who defines both the hypothesis and the tasks; the system only has to complete them. </span><em class="italic"><span class="koboSpan" id="kobo.105.1">Level 1</span></em><span class="koboSpan" id="kobo.106.1"> can be considered orchestrators under the supervision of a </span><span class="No-Break"><span class="koboSpan" id="kobo.107.1">human being.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.108.1">Level 2</span></strong><span class="koboSpan" id="kobo.109.1">: Also referred to as </span><em class="italic"><span class="koboSpan" id="kobo.110.1">AI agent as a collaborator</span></em><span class="koboSpan" id="kobo.111.1">, the system helps a researcher redefine the hypothesis thanks in part to its large set of tools. </span><span class="koboSpan" id="kobo.111.2">Despite its contribution to the hypothesis, its ability to understand scientific phenomena and generate innovative hypotheses remains limited. </span><span class="koboSpan" id="kobo.111.3">What differentiates it from </span><em class="italic"><span class="koboSpan" id="kobo.112.1">Level 1</span></em><span class="koboSpan" id="kobo.113.1"> is participation in hypothesis improvement and task definition to </span><span class="No-Break"><span class="koboSpan" id="kobo.114.1">test it.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.115.1">Level 3</span></strong><span class="koboSpan" id="kobo.116.1">: This is the last level and is defined as </span><em class="italic"><span class="koboSpan" id="kobo.117.1">AI agent as a scientist</span></em><span class="koboSpan" id="kobo.118.1">. </span><span class="koboSpan" id="kobo.118.2">In this case, an agent must be able to develop and extrapolate novel hypotheses and define links between findings that cannot be inferred solely from the literature. </span><span class="koboSpan" id="kobo.118.3">A </span><em class="italic"><span class="koboSpan" id="kobo.119.1">Level 3</span></em><span class="koboSpan" id="kobo.120.1"> agent then collaborates as an equal to a researcher or can propose hypotheses on its own, defines tasks to test hypotheses, and </span><span class="No-Break"><span class="koboSpan" id="kobo.121.1">completes them.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.122.1">To date, we have no </span><a id="_idIndexMarker1486"/><span class="koboSpan" id="kobo.123.1">agents beyond </span><em class="italic"><span class="koboSpan" id="kobo.124.1">Level 1</span></em><span class="koboSpan" id="kobo.125.1">, and we will probably need new architectures and training systems for </span><em class="italic"><span class="koboSpan" id="kobo.126.1">Levels 2</span></em><span class="koboSpan" id="kobo.127.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.128.1">3</span></em><span class="koboSpan" id="kobo.129.1">. </span><em class="italic"><span class="koboSpan" id="kobo.130.1">Level 0</span></em><span class="koboSpan" id="kobo.131.1"> is then a set of tools that are used by researchers but lack any autonomy. </span><span class="koboSpan" id="kobo.131.2">A </span><em class="italic"><span class="koboSpan" id="kobo.132.1">Level 1</span></em><span class="koboSpan" id="kobo.133.1"> agent can write code to conduct a bioinformatics analysis to process data, conduct statistical analysis, or use other tools. </span><span class="koboSpan" id="kobo.133.2">A </span><em class="italic"><span class="koboSpan" id="kobo.134.1">Level 1</span></em><span class="koboSpan" id="kobo.135.1"> agent uses </span><em class="italic"><span class="koboSpan" id="kobo.136.1">Level 0</span></em><span class="koboSpan" id="kobo.137.1"> tools to carry out these tasks, allowing it to test a hypothesis. </span><span class="koboSpan" id="kobo.137.2">A </span><em class="italic"><span class="koboSpan" id="kobo.138.1">Level 2</span></em><span class="koboSpan" id="kobo.139.1"> agent should not just perform narrow tasks on human indications but should be able given an initial hypothesis to refine it, decide, and perform tasks autonomously. </span><span class="koboSpan" id="kobo.139.2">We expect a </span><em class="italic"><span class="koboSpan" id="kobo.140.1">Level 2</span></em><span class="koboSpan" id="kobo.141.1"> agent, after being given the hypothesis, to be able to also refine experiments, and then critically evaluate to maximize a goal. </span><span class="koboSpan" id="kobo.141.2">A </span><em class="italic"><span class="koboSpan" id="kobo.142.1">Level 3</span></em><span class="koboSpan" id="kobo.143.1"> agent, on the other hand, should collaborate with humans to generate hypotheses, and can practically be considered its peer. </span><span class="koboSpan" id="kobo.143.2">A </span><em class="italic"><span class="koboSpan" id="kobo.144.1">Level 3</span></em><span class="koboSpan" id="kobo.145.1"> agent should be able to evaluate existing challenges and anticipate future research directions. </span><span class="koboSpan" id="kobo.145.2">In addition, a </span><em class="italic"><span class="koboSpan" id="kobo.146.1">Level 3</span></em><span class="koboSpan" id="kobo.147.1"> agent should integrate with experimental platforms to be able to conduct the entire process end </span><span class="No-Break"><span class="koboSpan" id="kobo.148.1">to end.</span></span></p>
<h1 id="_idParaDest-210"><a id="_idTextAnchor219"/><span class="koboSpan" id="kobo.149.1">AI agents in other sectors</span></h1>
<p><span class="koboSpan" id="kobo.150.1">In this section, we will discuss how LLM agents are having and will have a global impact across a range </span><span class="No-Break"><span class="koboSpan" id="kobo.151.1">of industries.</span></span></p>
<h2 id="_idParaDest-211"><a id="_idTextAnchor220"/><span class="koboSpan" id="kobo.152.1">Physical agents</span></h2>
<p><span class="koboSpan" id="kobo.153.1">Physical AI agents (for example, robots) are LLM agents </span><a id="_idIndexMarker1487"/><span class="koboSpan" id="kobo.154.1">that are capable of</span><a id="_idIndexMarker1488"/><span class="koboSpan" id="kobo.155.1"> navigating the real world and performing actions. </span><span class="koboSpan" id="kobo.155.2">Thus, they can be considered systems that are embodied and integrate AI with the physical world. </span><span class="koboSpan" id="kobo.155.3">LLMs in these systems provide the backbone for reasoning and contextual understanding. </span><span class="koboSpan" id="kobo.155.4">On this backbone, other modules such as memory, additional skills, and tools can </span><span class="No-Break"><span class="koboSpan" id="kobo.156.1">be added.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer358">
<span class="koboSpan" id="kobo.157.1"><img alt="Figure 11.2 – LLM-based agent (https://arxiv.org/pdf/2501.08944v1)" src="image/B21257_11_02.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.158.1">Figure 11.2 – LLM-based agent (</span><a href="https://arxiv.org/pdf/2501.08944v1"><span class="koboSpan" id="kobo.159.1">https://arxiv.org/pdf/2501.08944v1</span></a><span class="koboSpan" id="kobo.160.1">)</span></p>
<p><span class="koboSpan" id="kobo.161.1">Unlike a virtual agent, a physical AI agent must also understand and adapt to physical dynamics such as gravity, friction, and inertia. </span><span class="koboSpan" id="kobo.161.2">Being able to understand physical laws allows it to be able to navigate the environment and </span><span class="No-Break"><span class="koboSpan" id="kobo.162.1">perform tasks.</span></span></p>
<p><span class="koboSpan" id="kobo.163.1">There </span><a id="_idIndexMarker1489"/><span class="koboSpan" id="kobo.164.1">are several advantages to using an LLM for a </span><span class="No-Break"><span class="koboSpan" id="kobo.165.1">physical</span></span><span class="No-Break"><a id="_idIndexMarker1490"/></span><span class="No-Break"><span class="koboSpan" id="kobo.166.1"> agent:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.167.1">Human interaction</span></strong><span class="koboSpan" id="kobo.168.1">: LLMs allow humans to interact more easily through the use of natural language. </span><span class="koboSpan" id="kobo.168.2">In addition, the use of LLMs allows for better communication and better management of emotions, allowing for easier acceptance. </span><span class="koboSpan" id="kobo.168.3">Likewise, people are already accustomed to collaboration with LLMs, thus predisposing users to collaborate more easily with robots to solve problems, generate plans, and </span><span class="No-Break"><span class="koboSpan" id="kobo.169.1">perform tasks.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.170.1">Flexibility and adaptation</span></strong><span class="koboSpan" id="kobo.171.1">: LLMs today are multi-purpose with generalist capabilities, which allows them to adapt more easily to different tasks and circumstances. </span><span class="koboSpan" id="kobo.171.2">In addition, for specific tasks and environments, LLMs can be fine-tuned to acquire new skills and knowledge needed to operate in different environments. </span><span class="koboSpan" id="kobo.171.3">LLMs also have reasoning skills and the ability to find information; this knowledge and these skills acquired during pre-training can be used to solve tasks for which they were not programmed. </span><span class="koboSpan" id="kobo.171.4">In addition, LLMs can be guided to perform a task through natural language, making it easy to explain to robots the tasks they need </span><span class="No-Break"><span class="koboSpan" id="kobo.172.1">to accomplish.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.173.1">Multimodal capabilities</span></strong><span class="koboSpan" id="kobo.174.1">: Today, several LLMs are capable of taking different types of modalities as input. </span><span class="koboSpan" id="kobo.174.2">This capability allows them to integrate information from different types of sensors, so they can understand </span><span class="No-Break"><span class="koboSpan" id="kobo.175.1">their surroundings.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.176.1">In recent years, the idea of combining LLMs and robots has already been explored. </span><span class="koboSpan" id="kobo.176.2">For example, PaLM-SayCan was an experiment in which they used Google PaLM to command a robot. </span><span class="koboSpan" id="kobo.176.3">Later, Google used a PaLM-E model, which is itself multimodal. </span><span class="koboSpan" id="kobo.176.4">In addition, new alternatives are being tested today in which </span><strong class="bold"><span class="koboSpan" id="kobo.177.1">reinforcement learning</span></strong><span class="koboSpan" id="kobo.178.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.179.1">RL</span></strong><span class="koboSpan" id="kobo.180.1">) is used</span><a id="_idIndexMarker1491"/><span class="koboSpan" id="kobo.181.1"> to improve the interaction of LLMs with </span><span class="No-Break"><span class="koboSpan" id="kobo.182.1">the environment.</span></span></p>
<p><span class="koboSpan" id="kobo.183.1">Several challenges remain at present for robots controlled </span><span class="No-Break"><span class="koboSpan" id="kobo.184.1">by LLMs:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.185.1">Datasets and training</span></strong><span class="koboSpan" id="kobo.186.1">: LLMs require extensive training with large amounts of data. </span><span class="koboSpan" id="kobo.186.2">Collecting these datasets is not easy; to date, there are no quality datasets to train a robot in an environment (datasets that require large amounts of images and text). </span><span class="koboSpan" id="kobo.186.3">A robot would have to be trained with task descriptions and how to perform them, making it expensive to acquire these multimodal datasets. </span><span class="koboSpan" id="kobo.186.4">Using RL requires that you acquire datasets in which you have information about the actions taken by the system and the effect on the environment. </span><span class="koboSpan" id="kobo.186.5">Datasets used for one task may not be useful for training in another. </span><span class="koboSpan" id="kobo.186.6">For</span><a id="_idIndexMarker1492"/><span class="koboSpan" id="kobo.187.1"> example, a dataset used for training a dog robot</span><a id="_idIndexMarker1493"/><span class="koboSpan" id="kobo.188.1"> cannot be used for training a humanoid robot). </span><span class="koboSpan" id="kobo.188.2">Robot training requires interaction with the environment; this is a laborious and time-consuming process. </span><span class="koboSpan" id="kobo.188.3">Efforts are being made to overcome this problem with the use of games and simulations. </span><span class="koboSpan" id="kobo.188.4">However, this alternative is a simplification of the real environment and may not </span><span class="No-Break"><span class="koboSpan" id="kobo.189.1">be enough.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.190.1">Structure of the robot</span></strong><span class="koboSpan" id="kobo.191.1">: A robot can be of an arbitrary shape. </span><span class="koboSpan" id="kobo.191.2">Today, motion robots are designed with human shape, but this is not strictly necessary. </span><span class="koboSpan" id="kobo.191.3">In fact, robots for particular applications might have different shapes. </span><span class="koboSpan" id="kobo.191.4">For example, a robot thought of as a chef might have a better shape if designed for its </span><span class="No-Break"><span class="koboSpan" id="kobo.192.1">specific environment.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.193.1">Deployment of the LLM</span></strong><span class="koboSpan" id="kobo.194.1">: The optimum in these systems is to place an LLM inside the robot. </span><span class="koboSpan" id="kobo.194.2">Deployment inside the robot is one of the limitations of current LLMs. </span><span class="koboSpan" id="kobo.194.3">Many LLMs require considerable hardware resources (different GPUs for a single LLM), which makes deployment inside a local brain not feasible. </span><span class="koboSpan" id="kobo.194.4">In contrast, today, the robot’s brain resides in the cloud. </span><span class="koboSpan" id="kobo.194.5">This obviously has several limitations, especially when there is </span><span class="No-Break"><span class="koboSpan" id="kobo.195.1">signal loss.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.196.1">Security</span></strong><span class="koboSpan" id="kobo.197.1">: LLMs have biases and misconceptions that result from pre-training. </span><span class="koboSpan" id="kobo.197.2">In addition, LLMs can also hallucinate or commit errors. </span><span class="koboSpan" id="kobo.197.3">These factors can manifest themselves in errors while performing a task. </span><span class="koboSpan" id="kobo.197.4">An LLM who can control physical actions could then cause harm. </span><span class="koboSpan" id="kobo.197.5">For example, a robot could burn down a house while cooking. </span><span class="koboSpan" id="kobo.197.6">At the same time, LLMs can be hacked, posing the risk of private data leakage or </span><span class="No-Break"><span class="koboSpan" id="kobo.198.1">intentional damage.</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer359">
<span class="koboSpan" id="kobo.199.1"><img alt="Figure 11.3 – Challenge in embodied intelligence (https://arxiv.org/pdf/2311.07226)" src="image/B21257_11_03.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.200.1">Figure 11.3 – Challenge in embodied intelligence (</span><a href="https://arxiv.org/pdf/2311.07226"><span class="koboSpan" id="kobo.201.1">https://arxiv.org/pdf/2311.07226</span></a><span class="koboSpan" id="kobo.202.1">)</span></p>
<h2 id="_idParaDest-212"><a id="_idTextAnchor221"/><span class="koboSpan" id="kobo.203.1">LLM agents for gaming</span></h2>
<p><span class="koboSpan" id="kobo.204.1">LLM-based AI agents for </span><a id="_idIndexMarker1494"/><span class="koboSpan" id="kobo.205.1">gaming are another interesting frontier, where the </span><a id="_idIndexMarker1495"/><span class="koboSpan" id="kobo.206.1">reasoning capabilities of the model are used to interact with the environment (the game). </span><span class="koboSpan" id="kobo.206.2">In general, a framework dedicated to gaming requires a set of components such as an LLM, memory, and tools to interact with the game. </span><span class="koboSpan" id="kobo.206.3">Often, the system is trained using RL (where a game is an episode). </span><span class="koboSpan" id="kobo.206.4">An LLM can then analyze the moves conducted in previous games and reason about what the best </span><span class="No-Break"><span class="koboSpan" id="kobo.207.1">action is.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer360">
<span class="koboSpan" id="kobo.208.1"><img alt="Figure 11.4 – Overall framework for LLM-based game (https://arxiv.org/pdf/2404.02039)" src="image/B21257_11_04.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.209.1">Figure 11.4 – Overall framework for LLM-based game (</span><a href="https://arxiv.org/pdf/2404.02039"><span class="koboSpan" id="kobo.210.1">https://arxiv.org/pdf/2404.02039</span></a><span class="koboSpan" id="kobo.211.1">)</span></p>
<p><span class="koboSpan" id="kobo.212.1">Especially today, many games are quite complex and there is sophisticated interaction with the environment and other characters. </span><span class="koboSpan" id="kobo.212.2">An LLM can then reason about the richness of textual information (object descriptions, task descriptions, dialogues with characters, etc.) to decide on a plan of action or strategy. </span><span class="koboSpan" id="kobo.212.3">For example, in Pokémon battles, each player has several Pokémon of different species. </span><span class="koboSpan" id="kobo.212.4">Each species has different abilities and statistics; knowledge of the game is necessary in order to win a battle. </span><span class="koboSpan" id="kobo.212.5">Using an LLM can allow you to leverage the model’s implicit knowledge to be able to select an effective strategy (such as using an electric attack does not bring damage to a ground-type Pokémon). </span><span class="koboSpan" id="kobo.212.6">In addition, an LLM can exploit techniques such as chain-of-thought (CoT) to integrate different elements</span><a id="_idIndexMarker1496"/><span class="koboSpan" id="kobo.213.1"> into action choices (especially if it has to think </span><a id="_idIndexMarker1497"/><span class="koboSpan" id="kobo.214.1">several </span><span class="No-Break"><span class="koboSpan" id="kobo.215.1">moves ahead).</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer361">
<span class="koboSpan" id="kobo.216.1"><img alt="Figure 11.5 – Use of semantic knowledge for devising an effective strategy (https://arxiv.org/pdf/2404.02039)" src="image/B21257_11_05.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.217.1">Figure 11.5 – Use of semantic knowledge for devising an effective strategy (</span><a href="https://arxiv.org/pdf/2404.02039"><span class="koboSpan" id="kobo.218.1">https://arxiv.org/pdf/2404.02039</span></a><span class="koboSpan" id="kobo.219.1">)</span></p>
<p><span class="koboSpan" id="kobo.220.1">LLM-based agents are an interesting prospect for the game because they could enrich the players’ experience. </span><span class="koboSpan" id="kobo.220.2">For example, LLMs could create characters who discuss more naturally with players, provide hints during the game, cooperate with them, and guide them through the adventure. </span><span class="koboSpan" id="kobo.220.3">Or they could be used to generate antagonists that are more complex and match the </span><span class="No-Break"><span class="koboSpan" id="kobo.221.1">player’s level.</span></span></p>
<h2 id="_idParaDest-213"><a id="_idTextAnchor222"/><span class="koboSpan" id="kobo.222.1">Web agents</span></h2>
<p><span class="koboSpan" id="kobo.223.1">Web agents are</span><a id="_idIndexMarker1498"/><span class="koboSpan" id="kobo.224.1"> AI agents designed explicitly to interact with the web and assist humans </span><a id="_idIndexMarker1499"/><span class="koboSpan" id="kobo.225.1">in tedious and repetitive tasks. </span><span class="koboSpan" id="kobo.225.2">Thus, the purpose of these agents is to automate these tasks and improve productivity and efficiency. </span><span class="koboSpan" id="kobo.225.3">Again, the brain is an LLM, which allows reasoning and task understanding to be conducted. </span><span class="koboSpan" id="kobo.225.4">The architecture of a web agent is similar to that seen in this book. </span><span class="koboSpan" id="kobo.225.5">A web agent has a module dedicated to perception (input from the web), reasoning (LLM), and a module dedicated to interaction with the web. </span><span class="koboSpan" id="kobo.225.6">The perception module requires interaction with the web via either HTML (text-based agents that read the HTML document and process it) or via screenshots of websites (use of multimodal LLMs). </span><span class="koboSpan" id="kobo.225.7">Once an LLM receives a task, it can then browse the web, schedule subtasks, retrieve information from memory, and execute </span><span class="No-Break"><span class="koboSpan" id="kobo.226.1">the plan.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer362">
<span class="koboSpan" id="kobo.227.1"><img alt="Figure 11.6 – Web agent framework (https://arxiv.org/pdf/2503.23350)" src="image/B21257_11_06.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.228.1">Figure 11.6 – Web agent framework (</span><a href="https://arxiv.org/pdf/2503.23350)"><span class="koboSpan" id="kobo.229.1">https://arxiv.org/pdf/2503.23350)</span></a></p>
<p><span class="koboSpan" id="kobo.230.1">AI agents are a new frontier for AI, one that is poised to have a rapid practical impact. </span><span class="koboSpan" id="kobo.230.2">Despite their potential, several challenges and issues remain, which we will address in the </span><span class="No-Break"><span class="koboSpan" id="kobo.231.1">next section.</span></span></p>
<h1 id="_idParaDest-214"><a id="_idTextAnchor223"/><span class="koboSpan" id="kobo.232.1">Challenges and open questions</span></h1>
<p><span class="koboSpan" id="kobo.233.1">In this section, we will address several open questions about both agents and the capabilities of LLMs. </span><span class="koboSpan" id="kobo.233.2">Despite advances in the field, several points remain to be resolved for the safe use of </span><span class="No-Break"><span class="koboSpan" id="kobo.234.1">AI agents.</span></span></p>
<h2 id="_idParaDest-215"><a id="_idTextAnchor224"/><span class="koboSpan" id="kobo.235.1">Challenges in human-agent communication</span></h2>
<p><span class="koboSpan" id="kobo.236.1">Once they are</span><a id="_idIndexMarker1500"/><span class="koboSpan" id="kobo.237.1"> deployed in the real world, agents can perform actions that lead to problematic failures. </span><span class="koboSpan" id="kobo.237.2">For example, a shopping agent might spend money unexpectedly or inadvertently leak sensitive information. </span><span class="koboSpan" id="kobo.237.3">Coding agents might execute or produce viruses, delete important files, or push repositories into production that are full of bugs. </span><span class="koboSpan" id="kobo.237.4">Communication with the user is key to avoiding such problems. </span><span class="koboSpan" id="kobo.237.5">The use of agents should be based on two key principles: transparency and control. </span><span class="koboSpan" id="kobo.237.6">Indeed, there must be an alignment between the user’s goals and the agent’s behavior; the user must then be able to control the process and have access to its progress. </span><span class="koboSpan" id="kobo.237.7">Communication between humans and agents allows us to advance these two principles, but some open </span><span class="No-Break"><span class="koboSpan" id="kobo.238.1">challenges remain.</span></span></p>
<p><span class="koboSpan" id="kobo.239.1">Modern agents are not yet completely perfect and can make mistakes (especially for goals that are complex or include several steps). </span><span class="koboSpan" id="kobo.239.2">Therefore, it is important that we can verify the agent’s behavior, both the result of its work and that it has understood the task. </span><span class="koboSpan" id="kobo.239.3">Therefore, a way must be found to verify that the agent has understood the goal and that its plan and actions are directed toward this goal. </span><span class="koboSpan" id="kobo.239.4">Verifying that the agent has truly understood the goal allows us to avoid costly errors and save computation </span><span class="No-Break"><span class="koboSpan" id="kobo.240.1">and time.</span></span></p>
<p><span class="koboSpan" id="kobo.241.1">In addition, LLMs have a component that is stochastic. </span><span class="koboSpan" id="kobo.241.2">This component arises from the probabilistic nature of the model output functions (stochastic decoding) and the complex natures of interactions that can evolve during the task (unanticipated events). </span><span class="koboSpan" id="kobo.241.3">Therefore, the output and behavior of the model may not be consistent. </span><span class="koboSpan" id="kobo.241.4">Even in a deterministic setting (temperature 0), changes in the environment during task execution may lead to unexpected or unintended results. </span><span class="koboSpan" id="kobo.241.5">Inconsistencies may also emerge from the outdated model knowledge or imperfect world model present within an LLM. </span><span class="koboSpan" id="kobo.241.6">For example, an agent might buy an item that is out of budget or different from a user’s needs, due to misalignment of its knowledge of the </span><span class="No-Break"><span class="koboSpan" id="kobo.242.1">real world.</span></span></p>
<p><span class="koboSpan" id="kobo.243.1">Similarly, interactions with the user and the outside world generate a great deal of information. </span><span class="koboSpan" id="kobo.243.2">This broad context is important for directing the agent’s behavior, which can then be learned from past interactions. </span><span class="koboSpan" id="kobo.243.3">Although this context is fundamental to being able to perform the task effectively, it risks becoming far too wide and manageable over time. </span><span class="koboSpan" id="kobo.243.4">At the same time, modern LLMs have a noise problem and struggle to find relevant information when it is scattered in unnecessary detail. </span><span class="koboSpan" id="kobo.243.5">Therefore, effective ways must be found for an agent to focus on the relevant part of the last interaction with the user. </span><span class="koboSpan" id="kobo.243.6">Also, some of the information should not be able to be reused (privacy and ethical concerns), so one would need to find an easy way to manage, edit, and remove the </span><span class="No-Break"><span class="koboSpan" id="kobo.244.1">past information.</span></span></p>
<p><span class="koboSpan" id="kobo.245.1">What we have discussed are the general challenges of user-agent communication. </span><span class="koboSpan" id="kobo.245.2">We can also define open challenges that are in the communication between user and agent, and vice versa. </span><span class="koboSpan" id="kobo.245.3">First, we need to make sure that we can design agents that enable effective communication </span><a id="_idIndexMarker1501"/><span class="koboSpan" id="kobo.246.1">by the user by addressing </span><span class="No-Break"><span class="koboSpan" id="kobo.247.1">these points:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.248.1">Clear goal acquisition</span></strong><span class="koboSpan" id="kobo.249.1">: The focus of the system is for the agent to understand the goal and for the user to be able to provide it clearly. </span><span class="koboSpan" id="kobo.249.2">To avoid costly mistakes, we need to design agents for which users can define goals unambiguously. </span><span class="koboSpan" id="kobo.249.3">Some possibilities have been studied in some areas: sets of logical rules and the use of formal languages. </span><span class="koboSpan" id="kobo.249.4">To make this technology usable for everyone, we need to use natural language. </span><span class="koboSpan" id="kobo.249.5">Natural language is rich in both nuance and ambiguity, however, and allows complex goals to be defined with vague and incomplete definitions. </span><span class="koboSpan" id="kobo.249.6">Hence, mechanisms must be defined to disambiguate unclear goals or allow the agent to infer from context (or </span><span class="No-Break"><span class="koboSpan" id="kobo.250.1">past interactions).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.251.1">Respect for user preferences</span></strong><span class="koboSpan" id="kobo.252.1">: One can achieve a goal with several paths, but some are more optimal than others (both for efficiency and for respecting a user’s preferences). </span><span class="koboSpan" id="kobo.252.2">User preferences may not be aligned with LLM values (during post-training, the model is aligned with human preferences, which do not necessarily reflect the preferences of a general user but only of a selected pool of annotators). </span><span class="koboSpan" id="kobo.252.3">For example, if a user requests a route, they may prefer a more eco-friendly means of transportation. </span><span class="koboSpan" id="kobo.252.4">The agent should adhere to these preferences when possible, or interrupt the process to inform the user when it cannot. </span><span class="koboSpan" id="kobo.252.5">Model alignment may be one possible approach to take user preferences into account. </span><span class="koboSpan" id="kobo.252.6">However, current alignment approaches primarily consider aggregate preferences, and methods for accommodating individual preferences remain undeveloped. </span><span class="koboSpan" id="kobo.252.7">In more general terms, an agent can also achieve a goal by generating harm (even in an unintended way), and this risk is greater if it has the ability to </span><span class="No-Break"><span class="koboSpan" id="kobo.253.1">use tools.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.254.1">Incorporating feedback</span></strong><span class="koboSpan" id="kobo.255.1">: We know agents are error-prone, and while we can develop strategies to reduce errors, completely eliminating them may not be possible. </span><span class="koboSpan" id="kobo.255.2">An agent might continue to use suboptimal tools (not understanding the goal or setting the wrong plan) in repeated interactions, frustrating the user. </span><span class="koboSpan" id="kobo.255.3">One way to correct this behavior is to provide feedback from the user. </span><span class="koboSpan" id="kobo.255.4">There is now research on how to incorporate this feedback and how to represent it in a more effective form for the agent (e.g., turn it into </span><span class="No-Break"><span class="koboSpan" id="kobo.256.1">first-order logic).</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.257.1">There are also challenges </span><a id="_idIndexMarker1502"/><span class="koboSpan" id="kobo.258.1">that are associated with how the agent communicates to the user, especially pertaining to their capabilities, what actions they take or will take, goal achievement, and </span><span class="No-Break"><span class="koboSpan" id="kobo.259.1">unexpected events:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.260.1">Capabilities of the agent</span></strong><span class="koboSpan" id="kobo.261.1">: The user must be able to understand the full capabilities (and limitations) of an agent in order to conduct informed decision-making. </span><span class="koboSpan" id="kobo.261.2">It should be clear what information the agent has access to, how it will use this information, how it can modify the external environment, what tools it has access to, and whether it can connect to </span><span class="No-Break"><span class="koboSpan" id="kobo.262.1">the internet.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.263.1">What actions the agent will take</span></strong><span class="koboSpan" id="kobo.264.1">: To solve the goal, an agent can detail a complex plan, which can be particularly costly (time, resources, or money) and may violate some of the user’s preferences. </span><span class="koboSpan" id="kobo.264.2">The user then should be aware of the actions an agent takes and be able to provide feedback. </span><span class="koboSpan" id="kobo.264.3">Of course, an effective form of communication must be found to avoid irrelevant details being communicated and the user not fully understanding the agent’s actions. </span><span class="koboSpan" id="kobo.264.4">In addition, it should be clarified whether some actions require the user’s </span><span class="No-Break"><span class="koboSpan" id="kobo.265.1">explicit approval.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.266.1">Monitor progress</span></strong><span class="koboSpan" id="kobo.267.1">: For an agent moving in a dynamic environment, a plan to complete a task requires several steps; it is useful for a user to be aware of what the agent is doing and whether it is necessary to modify the process or stop it. </span><span class="koboSpan" id="kobo.267.2">An agent conducting multiple actions at the same time could lead to unexpected and harmful behavior. </span><span class="koboSpan" id="kobo.267.3">For example, an agent who builds news reports and invests in the market might read fake news and conduct a series of </span><span class="No-Break"><span class="koboSpan" id="kobo.268.1">bad investments.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.269.1">Changes in the environment and side effects</span></strong><span class="koboSpan" id="kobo.270.1">: An agent must monitor the changes in the environment or potential side effects of its operations. </span><span class="koboSpan" id="kobo.270.2">Take, for example, an agent tasked with buying a product online at the lowest price available. </span><span class="koboSpan" id="kobo.270.3">The agent could search online and find the product at a very competitive price and order it. </span><span class="koboSpan" id="kobo.270.4">However, the offer might require a subscription or other hidden costs that would make the purchase much more expensive than the user’s preferences or budget. </span><span class="koboSpan" id="kobo.270.5">The user must be aware of the side effects that are generated by the </span><span class="No-Break"><span class="koboSpan" id="kobo.271.1">agent’s behavior.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.272.1">Goal attainment</span></strong><span class="koboSpan" id="kobo.273.1">: The user specifies a goal, and the agent plans actions and executes them. </span><span class="koboSpan" id="kobo.273.2">At the end of this process, the user must be clear whether the agent has achieved the goal or not (or partially). </span><span class="koboSpan" id="kobo.273.3">Thus, a way is needed to evaluate that a goal has been achieved. </span><span class="koboSpan" id="kobo.273.4">For example, the goal might be to buy the cheapest possible cell phone with a certain type of performance. </span><span class="koboSpan" id="kobo.273.5">The agent could lead the purchase, but we need to assess whether the agent has met the other conditions as well. </span><span class="koboSpan" id="kobo.273.6">Thus, we need a way to verify that the goal has been fully and </span><span class="No-Break"><span class="koboSpan" id="kobo.274.1">satisfactorily </span></span><span class="No-Break"><a id="_idIndexMarker1503"/></span><span class="No-Break"><span class="koboSpan" id="kobo.275.1">achieved.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.276.1">Communication with agents is a complex but critical topic. </span><span class="koboSpan" id="kobo.276.2">Miscommunication can lead to system failure and is an important point to consider. </span><span class="koboSpan" id="kobo.276.3">In this section, we have provided a list of important elements to evaluate user-agent communication from different perspectives. </span><span class="koboSpan" id="kobo.276.4">In the next subsection, we will see whether or not the use of multi-agents is superior to the single agent. </span><span class="koboSpan" id="kobo.276.5">Some studies question </span><span class="No-Break"><span class="koboSpan" id="kobo.277.1">this perspective.</span></span></p>
<h2 id="_idParaDest-216"><a id="_idTextAnchor225"/><span class="koboSpan" id="kobo.278.1">No clear superiority of multi-agents</span></h2>
<p><span class="koboSpan" id="kobo.279.1">As</span><a id="_idIndexMarker1504"/><span class="koboSpan" id="kobo.280.1"> mentioned earlier, an LLM-based agent can be identified as an entity that has an initial state (usually a description in the prompt specifying its initial state), can track what it produces (state), and can interact with the environment through the use of tools (action). </span><span class="koboSpan" id="kobo.280.2">A </span><strong class="bold"><span class="koboSpan" id="kobo.281.1">multi-agent system</span></strong><span class="koboSpan" id="kobo.282.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.283.1">MAS</span></strong><span class="koboSpan" id="kobo.284.1">) is </span><a id="_idIndexMarker1505"/><span class="koboSpan" id="kobo.285.1">defined as a collection of agents that interact with each other in a coordinated manner to solve </span><span class="No-Break"><span class="koboSpan" id="kobo.286.1">a task.</span></span></p>
<p><span class="koboSpan" id="kobo.287.1">MASs are an extension of single-agent systems, designed to create a more sophisticated framework capable of addressing complex problems. </span><span class="koboSpan" id="kobo.287.2">Obviously, this means a higher computational cost (more LLM calls in inference). </span><span class="koboSpan" id="kobo.287.3">This higher computational cost should be justified by a substantial performance gain. </span><span class="koboSpan" id="kobo.287.4">In fact, some studies show that this is not the case. </span><span class="koboSpan" id="kobo.287.5">MASs offer only marginal gains in performance compared to </span><span class="No-Break"><span class="koboSpan" id="kobo.288.1">single-agent systems.</span></span></p>
<p><span class="koboSpan" id="kobo.289.1">It is useful to outline the </span><a id="_idIndexMarker1506"/><span class="koboSpan" id="kobo.290.1">architectural trade-offs between single-agent and multi-agent designs. </span><span class="koboSpan" id="kobo.290.2">While MASs offer potential advantages in modularity and parallelism, they also introduce additional complexity, coordination overhead, and cost. </span><span class="koboSpan" id="kobo.290.3">The following table summarizes the </span><span class="No-Break"><span class="koboSpan" id="kobo.291.1">key differences:</span></span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-4">
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.292.1">Single-Agent Design</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.293.1">Multi-Agent Design</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.294.1">Cost</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.295.1">Lower: fewer inference steps and </span><span class="No-Break"><span class="koboSpan" id="kobo.296.1">less orchestration</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.297.1">Higher: more agents, and more LLM calls and </span><span class="No-Break"><span class="koboSpan" id="kobo.298.1">tool usage</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.299.1">Latency</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.300.1">Generally lower, streamlined </span><span class="No-Break"><span class="koboSpan" id="kobo.301.1">single flow</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.302.1">Potentially higher due to </span><span class="No-Break"><span class="koboSpan" id="kobo.303.1">inter-agent communication</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.304.1">Fault tolerance</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.305.1">Lower: failure in the agent often breaks </span><span class="No-Break"><span class="koboSpan" id="kobo.306.1">the system</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.307.1">Higher: failures can be contained within </span><span class="No-Break"><span class="koboSpan" id="kobo.308.1">individual agents</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.309.1">Modularity</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.310.1">Monolithic and harder </span><span class="No-Break"><span class="koboSpan" id="kobo.311.1">to extend</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.312.1">Modular: agents can be added or </span><span class="No-Break"><span class="koboSpan" id="kobo.313.1">replaced independently</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.314.1">Scalability</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.315.1">Limited: the agent handles </span><span class="No-Break"><span class="koboSpan" id="kobo.316.1">all logic</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.317.1">Higher: parallel agents allow </span><span class="No-Break"><span class="koboSpan" id="kobo.318.1">distributed problem-solving</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.319.1">Communication </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.320.1">overhead</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.321.1">None (</span><span class="No-Break"><span class="koboSpan" id="kobo.322.1">internal reasoning)</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.323.1">Significant: explicit agent-to-agent </span><span class="No-Break"><span class="koboSpan" id="kobo.324.1">messaging required</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.325.1">Interpretability</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.326.1">Easier: single </span><span class="No-Break"><span class="koboSpan" id="kobo.327.1">decision chain</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.328.1">Harder: distributed reasoning may </span><span class="No-Break"><span class="koboSpan" id="kobo.329.1">reduce transparency</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.330.1">Table 11.1 – Potential causes of multi-agent system failure</span></p>
<p><span class="koboSpan" id="kobo.331.1">As shown in the preceding table, multi-agent architectures introduce a set of trade-offs that must be carefully balanced. </span><span class="koboSpan" id="kobo.331.2">While they offer modularity and potential fault isolation, they often suffer from increased latency, communication overhead, and coordination challenges. </span><span class="koboSpan" id="kobo.331.3">These</span><a id="_idIndexMarker1507"/><span class="koboSpan" id="kobo.332.1"> trade-offs are reflected in empirical evaluations </span><span class="No-Break"><span class="koboSpan" id="kobo.333.1">of MASs.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer363">
<span class="koboSpan" id="kobo.334.1"><img alt="Figure 11.7 – Failure rates of five popular multi-agent LLM systems (https://arxiv.org/pdf/2503.13657)" src="image/B21257_11_07.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.335.1">Figure 11.7 – Failure rates of five popular multi-agent LLM systems (</span><a href="https://arxiv.org/pdf/2503.13657"><span class="koboSpan" id="kobo.336.1">https://arxiv.org/pdf/2503.13657</span></a><span class="koboSpan" id="kobo.337.1">)</span></p>
<p><span class="koboSpan" id="kobo.338.1">MASs should bring numerous benefits, such as greater accuracy and the ability to handle more complex tasks, create more complex plans, or find better solutions. </span><span class="koboSpan" id="kobo.338.2">If MASs do not bring all these benefits and indeed often fail, we need to understand why. </span><span class="koboSpan" id="kobo.338.3">In a recent study, Cemri et al. </span><span class="koboSpan" id="kobo.338.4">(2025) set out to conduct a detailed taxonomy of MAS failures with expert annotators by analyzing 150 conversation traces (each averaging over 15,000 lines of text) to identify failures and the causes of these failures. </span><span class="koboSpan" id="kobo.338.5">In their work, they identified 14 causes, grouped into 3 </span><span class="No-Break"><span class="koboSpan" id="kobo.339.1">main groups:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer364">
<span class="koboSpan" id="kobo.340.1"><img alt="Figure 11.8 – Taxonomy of MAS failure modes (https://arxiv.org/pdf/2503.13657)" src="image/B21257_11_08.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.341.1">Figure 11.8 – Taxonomy of MAS failure modes (</span><a href="https://arxiv.org/pdf/2503.13657"><span class="koboSpan" id="kobo.342.1">https://arxiv.org/pdf/2503.13657</span></a><span class="koboSpan" id="kobo.343.1">)</span></p>
<p><span class="koboSpan" id="kobo.344.1">The three main</span><a id="_idIndexMarker1508"/><span class="koboSpan" id="kobo.345.1"> categories are thus </span><span class="No-Break"><span class="koboSpan" id="kobo.346.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.347.1">Specification and system design failures</span></strong><span class="koboSpan" id="kobo.348.1">: Failure results from deficits in MAS design. </span><span class="koboSpan" id="kobo.348.2">For the authors, much of the failure stems from poor choice of architecture, management of conversation between agents, poor task specification, violation of constraints, and poor specification of agent roles and responsibilities. </span><span class="koboSpan" id="kobo.348.3">In other words, if the instructions for agents are not clear, the system may fail. </span><span class="koboSpan" id="kobo.348.4">Even when instructions are clear, however, the MAS may not be aligned with </span><span class="No-Break"><span class="koboSpan" id="kobo.349.1">user instructions.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.350.1">Inter-agent misalignment</span></strong><span class="koboSpan" id="kobo.351.1">: Failure emerges from ineffective communication, little collaboration, conflicting behaviors among agents, and gradual derailment from the initial task. </span><span class="koboSpan" id="kobo.351.2">As we mentioned previously, achieving efficient communication between agents is not easy. </span><span class="koboSpan" id="kobo.351.3">Therefore, some agents may not communicate efficiently and simply </span><span class="No-Break"><span class="koboSpan" id="kobo.352.1">waste resources.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.353.1">Task verification and termination</span></strong><span class="koboSpan" id="kobo.354.1">: A third important category includes failure to complete the task or its premature termination. </span><span class="koboSpan" id="kobo.354.2">MASs often lack a verification mechanism that checks and ensures the accuracy, completeness, and reliability of interactions, decisions, and outcomes. </span><span class="koboSpan" id="kobo.354.3">Simply put, many systems do not include a dedicated agent (or other mechanism) to monitor the process and verify that the task was </span><span class="No-Break"><span class="koboSpan" id="kobo.355.1">successfully executed.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.356.1">The results of their</span><a id="_idIndexMarker1509"/><span class="koboSpan" id="kobo.357.1"> investigation showed that none of the causes are prevalent but are equally distributed across systems. </span><span class="koboSpan" id="kobo.357.2">In addition, some causes are correlated, producing a kind of ripple effect. </span><span class="koboSpan" id="kobo.357.3">For example, wrong architecture design can cause inefficient communication </span><span class="No-Break"><span class="koboSpan" id="kobo.358.1">between agents.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer365">
<span class="koboSpan" id="kobo.359.1"><img alt="Figure 11.9 – Distribution of failure modes by categories and systems (https://arxiv.org/pdf/2503.13657)" src="image/B21257_11_09.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.360.1">Figure 11.9 – Distribution of failure modes by categories and systems (</span><a href="https://arxiv.org/pdf/2503.13657"><span class="koboSpan" id="kobo.361.1">https://arxiv.org/pdf/2503.13657</span></a><span class="koboSpan" id="kobo.362.1">)</span></p>
<p><span class="koboSpan" id="kobo.363.1">The results of this work clearly show that failures can be avoided through more careful design. </span><span class="koboSpan" id="kobo.363.2">Improving prompts, agent communication, and adding an agent (or other verifier mechanism) allow for noticeably improved performance and lower risk of failure. </span><span class="koboSpan" id="kobo.363.3">In two case studies, the authors show how this is the case. </span><span class="koboSpan" id="kobo.363.4">On the other hand, these suggestions are not enough to solve all agent problems but will be further </span><span class="No-Break"><span class="koboSpan" id="kobo.364.1">technical progress.</span></span></p>
<p><span class="koboSpan" id="kobo.365.1">In addition to the system itself, many of the limitations of agents also stem from the agent itself (i.e., the model that is used for agents). </span><span class="koboSpan" id="kobo.365.2">In the next subsection, we will discuss the reasoning limitations </span><span class="No-Break"><span class="koboSpan" id="kobo.366.1">of LLMs.</span></span></p>
<h2 id="_idParaDest-217"><a id="_idTextAnchor226"/><span class="koboSpan" id="kobo.367.1">Limits of reasoning</span></h2>
<p><span class="koboSpan" id="kobo.368.1">Reasoning is </span><a id="_idIndexMarker1510"/><span class="koboSpan" id="kobo.369.1">a</span><a id="_idIndexMarker1511"/><span class="koboSpan" id="kobo.370.1"> fundamental cognitive function of human beings, and it is difficult to give a precise definition. </span><span class="koboSpan" id="kobo.370.2">Wikipedia defines reasoning this way: “</span><em class="italic"><span class="koboSpan" id="kobo.371.1">Reason is the capacity of consciously applying logic by drawing valid conclusions from new or existing information, with the aim of seeking the truth. </span><span class="koboSpan" id="kobo.371.2">It is associated with such characteristically human activities as philosophy, religion, science, language, mathematics, and art, and is normally considered to be a distinguishing ability possessed </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.372.1">by humans</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.373.1">.”</span></span></p>
<p><span class="koboSpan" id="kobo.374.1">For a long time, it was said that only human beings are equipped with reasoning. </span><span class="koboSpan" id="kobo.374.2">Today, however, it has been shown that primates, octopuses, and birds also exhibit basic forms of reasoning such as making decisions or solving problems. </span><span class="koboSpan" id="kobo.374.3">One of the problems with reasoning is the difficulty of being able to evaluate it. </span><span class="koboSpan" id="kobo.374.4">Typically, to do this, one assesses the ability to solve complex problems or make decisions. </span><span class="koboSpan" id="kobo.374.5">Complex problem-solving requires identifying the problem, dividing it into subproblems, finding patterns, and then choosing the best solution. </span><span class="koboSpan" id="kobo.374.6">Decision-making similarly requires identifying problems and patterns and evaluating alternatives before choosing the </span><span class="No-Break"><span class="koboSpan" id="kobo.375.1">best solution.</span></span></p>
<p><span class="koboSpan" id="kobo.376.1">In the case of LLMs, an attempt was made to measure reasoning capabilities through benchmark datasets that assess problem-solving ability (such as GLUE, SuperGLUE, and Hellaswag). </span><span class="koboSpan" id="kobo.376.2">Today, on many of these datasets, humans have been outperformed by next-generation LLMs. </span><span class="koboSpan" id="kobo.376.3">These new reasoning capabilities would be mainly due to </span><span class="No-Break"><span class="koboSpan" id="kobo.377.1">three factors:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.378.1">LLMs performing well in all the benchmarks dedicated to reasoning. </span><span class="koboSpan" id="kobo.378.2">These benchmarks contain math or coding problems that require reasoning skills. </span><span class="koboSpan" id="kobo.378.3">The results in these benchmarks suggest that LLMs are capable </span><span class="No-Break"><span class="koboSpan" id="kobo.379.1">of reasoning.</span></span></li>
<li><span class="koboSpan" id="kobo.380.1">The emergence of new properties with increasing parameters, number of tokens, and </span><span class="No-Break"><span class="koboSpan" id="kobo.381.1">compute budget.</span></span></li>
<li><span class="koboSpan" id="kobo.382.1">The use of techniques such as CoT, which allows the model to fulfill </span><span class="No-Break"><span class="koboSpan" id="kobo.383.1">its potential.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.384.1">There are those who question this view, claiming that there are alternative explanations for the performance achieved in these benchmarks. </span><span class="koboSpan" id="kobo.384.2">After all, many authors regard LLMs as nothing more than mere stochastic parrots. </span><span class="koboSpan" id="kobo.384.3">Jiang, in 2022 (</span><a href="https://arxiv.org/pdf/2406.11050"><span class="koboSpan" id="kobo.385.1">https://arxiv.org/pdf/2406.11050</span></a><span class="koboSpan" id="kobo.386.1">), suggested that the models are merely pattern-matching machines: “</span><em class="italic"><span class="koboSpan" id="kobo.387.1">A strong token bias suggests that the model is relying on superficial patterns in the input rather than truly understanding the underlying </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.388.1">reasoning task</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.389.1">.”</span></span></p>
<p><span class="koboSpan" id="kobo.390.1">In the same study, it was</span><a id="_idIndexMarker1512"/><span class="koboSpan" id="kobo.391.1"> observed that LLMs fail to generalize when they encounter new examples that exhibit patterns different from those seen in the pre-training phase. </span><span class="koboSpan" id="kobo.391.2">If we change tokens in the examples, pattern mapping fails (a transformer, through in-context learning, tries to find examples in its knowledge that are similar to the problem posed by the user). </span><span class="koboSpan" id="kobo.391.3">When the model fails to find examples, the model fails to solve the question. </span><span class="koboSpan" id="kobo.391.4">This fragility and dependence on training examples would explain why the model succeeds in solving complex problems (it finds patterns) and fails even with some very simple questions (it does not find examples). </span><span class="koboSpan" id="kobo.391.5">This is confirmed by a correlation between the example’s frequency in training data and </span><span class="No-Break"><span class="koboSpan" id="kobo.392.1">test performance.</span></span></p>
<p><span class="koboSpan" id="kobo.393.1">For example, when the model is asked to solve the classic “25 horses” graph theory problem, the model succeeds. </span><span class="koboSpan" id="kobo.393.2">If the “horse” token is changed to “bunny,” the model fails to solve it. </span><span class="koboSpan" id="kobo.393.3">The token change is irrelevant to the problem’s underlying logic, yet the model fails to solve it because it has difficulty mapping the problem. </span><span class="koboSpan" id="kobo.393.4">Both GPT-4 and Claude have significant performance drops due to perturbations in animal names </span><span class="No-Break"><span class="koboSpan" id="kobo.394.1">and numbers.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer366">
<span class="koboSpan" id="kobo.395.1"><img alt="Figure 11.10 – Token bias using the classic problems (https://arxiv.org/pdf/2406.11050)" src="image/B21257_11_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.396.1">Figure 11.10 – Token bias using the classic problems (</span><a href="https://arxiv.org/pdf/2406.11050"><span class="koboSpan" id="kobo.397.1">https://arxiv.org/pdf/2406.11050</span></a><span class="koboSpan" id="kobo.398.1">)</span></p>
<p><span class="koboSpan" id="kobo.399.1">This </span><a id="_idIndexMarker1513"/><span class="koboSpan" id="kobo.400.1">phenomenon is called </span><strong class="bold"><span class="koboSpan" id="kobo.401.1">prompt sensitivity</span></strong><span class="koboSpan" id="kobo.402.1"> (a different response to a prompt that </span><a id="_idIndexMarker1514"/><span class="koboSpan" id="kobo.403.1">is semantically equivalent to another). </span><span class="koboSpan" id="kobo.403.2">This is confirmed by the fact that LLMs are sensitive to noise. </span><span class="koboSpan" id="kobo.403.3">They are easily distracted by irrelevant context, which makes it more difficult to find patterns. </span><span class="koboSpan" id="kobo.403.4">This sensitivity is not resolved by prompting techniques specialized to improve reasoning, suggesting that disturbing pattern-matching activity disrupts reasoning ability. </span><span class="koboSpan" id="kobo.403.5">An example of irrelevant context disrupting the pattern but not impacting actual </span><span class="No-Break"><span class="koboSpan" id="kobo.404.1">problem-solving follows:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer367">
<span class="koboSpan" id="kobo.405.1"><img alt="Figure 11.11 – Irrelevant context disturbs LLMs (https://arxiv.org/pdf/2302.00093)" src="image/B21257_11_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.406.1">Figure 11.11 – Irrelevant context disturbs LLMs (</span><a href="https://arxiv.org/pdf/2302.00093"><span class="koboSpan" id="kobo.407.1">https://arxiv.org/pdf/2302.00093</span></a><span class="koboSpan" id="kobo.408.1">)</span></p>
<p><span class="koboSpan" id="kobo.409.1">Some authors suggest that intelligence can be seen as an emergent property. </span><span class="koboSpan" id="kobo.409.2">Biological systems naturally tend to become more complex, and this process is driven by natural selection. </span><span class="koboSpan" id="kobo.409.3">Evolution has shown an increase in intelligence over time as it promotes the adaptability of various species. </span><span class="koboSpan" id="kobo.409.4">Of course, intelligence is not an economic process, and a larger brain consumes a greater amount of resources (metabolic consumption). </span><span class="koboSpan" id="kobo.409.5">Loss function could be seen as evolutionary pressure. </span><span class="koboSpan" id="kobo.409.6">From this, it would follow that the increase in model capacity (in terms of the number of parameters) would parallel the increase in neurons in animal brains over time, and that loss function would instead be the evolutionary pressure to push these parameters to be used efficiently. </span><span class="koboSpan" id="kobo.409.7">By scaling up models and training (parameters and training tokens), intelligence could also emerge in LLMs. </span><span class="koboSpan" id="kobo.409.8">Reasoning then is seen as an emergent property that emerges from scaling the models. </span><span class="koboSpan" id="kobo.409.9">However, later studies suggest that emergent properties in LLMs can be a measurement error, and with it, the whole theory is related to the emergence </span><span class="No-Break"><span class="koboSpan" id="kobo.410.1">of reasoning.</span></span></p>
<p><span class="koboSpan" id="kobo.411.1">In the next figure, you can see </span><a id="_idIndexMarker1515"/><span class="koboSpan" id="kobo.412.1">how some properties seem to emerge as the model </span><span class="No-Break"><span class="koboSpan" id="kobo.413.1">size increases.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer368">
<span class="koboSpan" id="kobo.414.1"><img alt="Figure 11.12 – Examples of emerging reasoning properties (https://arxiv.org/abs/2304.15004)" src="image/B21257_11_12.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.415.1">Figure 11.12 – Examples of emerging reasoning properties (</span><a href="https://arxiv.org/abs/2304.15004"><span class="koboSpan" id="kobo.416.1">https://arxiv.org/abs/2304.15004</span></a><span class="koboSpan" id="kobo.417.1">)</span></p>
<p><span class="koboSpan" id="kobo.418.1">According to other authors, LLMs are capable of reasoning, but it needs to be unlocked. </span><span class="koboSpan" id="kobo.418.2">CoT prompting thus helps the model unlock its potential through intermediate reasoning and thus guides it to the correct answer in arithmetic problems. </span><span class="koboSpan" id="kobo.418.3">CoT is today’s prompt engineering technique and is also used to train deep reasoning models (such as ChatGPT-o1 or DeepSeek R1). </span><span class="koboSpan" id="kobo.418.4">In fact, these models are trained on long CoTs that are used to conduct supervised fine-tuning. </span><span class="koboSpan" id="kobo.418.5">These models explore different reasoning paths to arrive at an answer, showing high improvements in reasoning benchmarks. </span><span class="koboSpan" id="kobo.418.6">However, some studies show that these models suffer from both overthinking </span><span class="No-Break"><span class="koboSpan" id="kobo.419.1">and underthinking.</span></span></p>
<p><span class="koboSpan" id="kobo.420.1">Overthinking is a curious phenomenon in which these models reason longer than necessary when it comes to solving problems that are particularly simple. </span><span class="koboSpan" id="kobo.420.2">The model explores different reasoning paths for trivial questions. </span><span class="koboSpan" id="kobo.420.3">This indicates that the model is unable to understand which question needs more effort. </span><span class="koboSpan" id="kobo.420.4">Underthinking is the opposite, wherein the model may abandon the promising thinking path. </span><span class="koboSpan" id="kobo.420.5">This indicates a clear lack of depth of reasoning, where the model does not go all the way to a </span><span class="No-Break"><span class="koboSpan" id="kobo.421.1">correct solution.</span></span></p>
<p><span class="koboSpan" id="kobo.422.1">At the same time, even </span><a id="_idIndexMarker1516"/><span class="koboSpan" id="kobo.423.1">the benefits of CoT have been questioned (</span><a href="https://arxiv.org/pdf/2409.12183"><span class="koboSpan" id="kobo.424.1">https://arxiv.org/pdf/2409.12183</span></a><span class="koboSpan" id="kobo.425.1">): “</span><em class="italic"><span class="koboSpan" id="kobo.426.1">As much as 95% of the total performance gain from CoT on MMLU is attributed to questions containing “=” in the question or generated output. </span><span class="koboSpan" id="kobo.426.2">For non-math questions, we find no features to indicate when CoT </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.427.1">will help</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.428.1">.”</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer369">
<span class="koboSpan" id="kobo.429.1"><img alt="Figure 11.13 – CoT improvements are limited to symbolic and mathematical reasoning (https://arxiv.org/pdf/2409.12183)" src="image/B21257_11_13.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.430.1">Figure 11.13 – CoT improvements are limited to symbolic and mathematical reasoning (</span><a href="https://arxiv.org/pdf/2409.12183"><span class="koboSpan" id="kobo.431.1">https://arxiv.org/pdf/2409.12183</span></a><span class="koboSpan" id="kobo.432.1">)</span></p>
<p><span class="koboSpan" id="kobo.433.1">CoT would seem to help the model solve problems as it allows it to leverage the skills it learned during pre-training. </span><span class="koboSpan" id="kobo.433.2">CoT would simply help develop a plan, but then the LLMs may not be able to execute it. </span><span class="koboSpan" id="kobo.433.3">So, CoT can be used to get a plan, but to get the most benefit, an external tool would have to be added (such as a </span><span class="No-Break"><span class="koboSpan" id="kobo.434.1">Python interpreter).</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer370">
<span class="koboSpan" id="kobo.435.1"><img alt="Figure 11.14 – An LLM can devise a plan but needs an external tool to better solve some problems (https://arxiv.org/pdf/2409.12183)" src="image/B21257_11_14.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.436.1">Figure 11.14 – An LLM can devise a plan but needs an external tool to better solve some problems (</span><a href="https://arxiv.org/pdf/2409.12183"><span class="koboSpan" id="kobo.437.1">https://arxiv.org/pdf/2409.12183</span></a><span class="koboSpan" id="kobo.438.1">)</span></p>
<p><span class="koboSpan" id="kobo.439.1">These models </span><a id="_idIndexMarker1517"/><span class="koboSpan" id="kobo.440.1">are all tested on the same benchmarks as </span><a id="_idIndexMarker1518"/><span class="koboSpan" id="kobo.441.1">the </span><strong class="bold"><span class="koboSpan" id="kobo.442.1">Grade School Math 8K</span></strong><span class="koboSpan" id="kobo.443.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.444.1">GSM8K</span></strong><span class="koboSpan" id="kobo.445.1">) dataset, which provides complex arithmetic problems but is at risk of data leakage (considering how many billions of tokens are used to train an LLM, the model may have already seen the answer in </span><span class="No-Break"><span class="koboSpan" id="kobo.446.1">the training).</span></span></p>
<p><span class="koboSpan" id="kobo.447.1">Therefore, in their study, Mirzadeh et al. </span><span class="koboSpan" id="kobo.447.2">modified GSM8K, keeping the same questions but making statistical pattern matching difficult. </span><span class="koboSpan" id="kobo.447.3">If the model was capable of true reasoning, it should solve it easily; if, instead, it relied on pattern matching, it </span><span class="No-Break"><span class="koboSpan" id="kobo.448.1">would fail.</span></span></p>
<p><span class="koboSpan" id="kobo.449.1">In the following figure, notice how the GSM8K examples are modified to better control the response of the LLM. </span><span class="koboSpan" id="kobo.449.2">Using this dataset, we can formally investigate the LLM’s reasoning and highlight that state-of-the-art LLMs exhibit significant performance variations; this shows that LLM reasoning </span><span class="No-Break"><span class="koboSpan" id="kobo.450.1">is fragile.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer371">
<span class="koboSpan" id="kobo.451.1"><img alt="Figure 11.15 – This dataset serves as a tool to investigate the presumed reasoning capabilities of LLMs (https://arxiv.org/pdf/2410.05229)" src="image/B21257_11_15.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.452.1">Figure 11.15 – This dataset serves as a tool to investigate the presumed reasoning capabilities of LLMs (</span><a href="https://arxiv.org/pdf/2410.05229"><span class="koboSpan" id="kobo.453.1">https://arxiv.org/pdf/2410.05229</span></a><span class="koboSpan" id="kobo.454.1">)</span></p>
<p><span class="koboSpan" id="kobo.455.1">Testing state-of-the-art</span><a id="_idIndexMarker1519"/><span class="koboSpan" id="kobo.456.1"> LLMs, Mirzadeh et al. </span><span class="koboSpan" id="kobo.456.2">found no evidence of formal reasoning in language models. </span><span class="koboSpan" id="kobo.456.3">The models are not robust and have a drop in performance when numerical values are changed, and their capabilities degrade sharply as the complexity of the problem increases. </span><span class="koboSpan" id="kobo.456.4">The model is, in fact, fooled by added phrases that have no relevance. </span><span class="koboSpan" id="kobo.456.5">Instead, the model takes them into account, tries to map them, and sometimes turns them into operations. </span><span class="koboSpan" id="kobo.456.6">Mirzadeh et al. </span><span class="koboSpan" id="kobo.456.7">suggest that this occurs because their training datasets included similar examples that required conversion to mathematical operations: “</span><em class="italic"><span class="koboSpan" id="kobo.457.1">For instance, a common case we observe is that models interpret statements about “discount” as “multiplication”, regardless of the context. </span><span class="koboSpan" id="kobo.457.2">This raises the question of whether these models have truly understood the mathematical concepts </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.458.1">well enough</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.459.1">.”</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer372">
<span class="koboSpan" id="kobo.460.1"><img alt="Figure 11.16 – Example of error (https://arxiv.org/pdf/2410.05229)" src="image/B21257_11_16.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.461.1">Figure 11.16 – Example of error (</span><a href="https://arxiv.org/pdf/2410.05229"><span class="koboSpan" id="kobo.462.1">https://arxiv.org/pdf/2410.05229</span></a><span class="koboSpan" id="kobo.463.1">)</span></p>
<p><span class="koboSpan" id="kobo.464.1">More recent </span><a id="_idIndexMarker1520"/><span class="koboSpan" id="kobo.465.1">LLMs that have been trained on CoT (such as GPT4-o1) also fail in this task. </span><span class="koboSpan" id="kobo.465.2">This suggests that LLMs are elaborate statistical pattern machines but do not possess </span><span class="No-Break"><span class="koboSpan" id="kobo.466.1">true reasoning.</span></span></p>
<h2 id="_idParaDest-218"><a id="_idTextAnchor227"/><span class="koboSpan" id="kobo.467.1">Creativity in LLM</span></h2>
<p><span class="koboSpan" id="kobo.468.1">Creativity is</span><a id="_idIndexMarker1521"/><span class="koboSpan" id="kobo.469.1"> considered along with reasoning to be one of the skills that makes human beings. </span><span class="koboSpan" id="kobo.469.2">If quantifying reasoning is hard, being able to quantify creativity is a much harder task. </span><span class="koboSpan" id="kobo.469.3">However, creativity plays a very important role in what makes us human, and it concerns activities such as writing poems or books, creating works of art, or even generating theories and achieving groundbreaking discoveries. </span><span class="koboSpan" id="kobo.469.4">That is why the question as to whether an LLM can be creative has </span><span class="No-Break"><span class="koboSpan" id="kobo.470.1">been raised.</span></span></p>
<p><span class="koboSpan" id="kobo.471.1">The problem in investigating the creativity of LLMs is that we do not have an unambiguous definition of creativity. </span><span class="koboSpan" id="kobo.471.2">In the field of research, creativity is often used as the definition chosen by Margaret Boden: “</span><em class="italic"><span class="koboSpan" id="kobo.472.1">the ability to come up with ideas or artifacts that are new, surprising and valuable</span></em><span class="koboSpan" id="kobo.473.1">.” </span><span class="koboSpan" id="kobo.473.2">Although this definition is accepted, it is difficult to evaluate </span><span class="No-Break"><span class="koboSpan" id="kobo.474.1">its elements:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.475.1">Value</span></strong><span class="koboSpan" id="kobo.476.1">: This is the easiest element to define. </span><span class="koboSpan" id="kobo.476.2">For example, code produced by an LLM can be considered valuable if it works in its </span><span class="No-Break"><span class="koboSpan" id="kobo.477.1">own way.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.478.1">Novelty</span></strong><span class="koboSpan" id="kobo.479.1">: For an object to be considered novel, it should be dissimilar to what has already been created. </span><span class="koboSpan" id="kobo.479.2">For a text, being novel could be considered the difference in output compared to other texts. </span><span class="koboSpan" id="kobo.479.3">One definition might be to generate a text whose embedding is distant from other </span><span class="No-Break"><span class="koboSpan" id="kobo.480.1">different texts.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.481.1">Surprising</span></strong><span class="koboSpan" id="kobo.482.1">: This is considered one of the most important and difficult elements to define. </span><span class="koboSpan" id="kobo.482.2">Random recombination of words can be considered new (or different) but certainly not surprising (nor valuable). </span><em class="italic"><span class="koboSpan" id="kobo.483.1">Surprising</span></em><span class="koboSpan" id="kobo.484.1"> is often understood as something new but not a simple variation </span><span class="No-Break"><span class="koboSpan" id="kobo.485.1">or recombination.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.486.1">Boden at the same</span><a id="_idIndexMarker1522"/><span class="koboSpan" id="kobo.487.1"> time described what she thought were three types of creativity with respect to the concept </span><span class="No-Break"><span class="koboSpan" id="kobo.488.1">of surprise:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.489.1">Combinatorial creativity</span></strong><span class="koboSpan" id="kobo.490.1">: The combination of familiar elements in an unfamiliar way (such as two genres that have not been </span><span class="No-Break"><span class="koboSpan" id="kobo.491.1">combined previously)</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.492.1">Exploratory creativity</span></strong><span class="koboSpan" id="kobo.493.1">: The exploration of new solutions in the way of thinking (such as a new narrative style, or a twist to a narrative style that had not </span><span class="No-Break"><span class="koboSpan" id="kobo.494.1">been explored)</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.495.1">Transformational creativity</span></strong><span class="koboSpan" id="kobo.496.1">: Changing the current narrative style or the current way </span><span class="No-Break"><span class="koboSpan" id="kobo.497.1">of thinking</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.498.1">In line with these definitions, several authors have sought to understand whether LLMs can be creative, and if so, what kind of creativity they can manifest. </span><span class="koboSpan" id="kobo.498.2">The main problem with this investigation is trying to quantify the creativity of an LLM. </span><span class="koboSpan" id="kobo.498.3">One approach is to assess whether the output of LLMs can be mapped to existing text snippets on the web. </span><span class="koboSpan" id="kobo.498.4">Human creativity is influenced by previous writers, but when a writer produces original writing, this cannot be mapped to previous writings. </span><span class="koboSpan" id="kobo.498.5">If every text generated by an LLM can be mapped to other texts, it is overwhelming evidence of a lack of creativity. </span><span class="koboSpan" id="kobo.498.6">In a recently published study, Lu (2024) analyzed how much of what is produced by an LLM is mappable to texts on the internet. </span><span class="koboSpan" id="kobo.498.7">The purpose of this study was precisely to create a creative index and compare LLMs and </span><span class="No-Break"><span class="koboSpan" id="kobo.499.1">human beings.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer373">
<span class="koboSpan" id="kobo.500.1"><img alt="Figure 11.17 – Mapping of LLM output to internet text (https://arxiv.org/pdf/2410.04265)" src="image/B21257_11_17.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.501.1">Figure 11.17 – Mapping of LLM output to internet text (</span><a href="https://arxiv.org/pdf/2410.04265"><span class="koboSpan" id="kobo.502.1">https://arxiv.org/pdf/2410.04265</span></a><span class="koboSpan" id="kobo.503.1">)</span></p>
<p><span class="koboSpan" id="kobo.504.1">The results of this</span><a id="_idIndexMarker1523"/><span class="koboSpan" id="kobo.505.1"> approach show that humans exhibit greater creativity (based on unique word and sentence combinations) than LLMs. </span><span class="koboSpan" id="kobo.505.2">That small amount of residual creativity in LLMs may simply result from stochastic processes and the fact that we do not know the entire </span><span class="No-Break"><span class="koboSpan" id="kobo.506.1">pre-training</span></span><span class="No-Break"><span class="koboSpan" id="kobo.507.1"> dataset.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer374">
<span class="koboSpan" id="kobo.508.1"><img alt="Figure 11.18 – Comparison between the creativity index of humans and that of LLMs (https://arxiv.org/pdf/2410.04265)" src="image/B21257_11_18.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.509.1">Figure 11.18 – Comparison between the creativity index of humans and that of LLMs (</span><a href="https://arxiv.org/pdf/2410.04265"><span class="koboSpan" id="kobo.510.1">https://arxiv.org/pdf/2410.04265</span></a><span class="koboSpan" id="kobo.511.1">)</span></p>
<p><span class="koboSpan" id="kobo.512.1">Lou et al. </span><span class="koboSpan" id="kobo.512.2">suggest an interesting analogy: “</span><em class="italic"><span class="koboSpan" id="kobo.513.1">Just as a DJ remixes existing tracks while a composer creates original music, we speculate that LLMs behave more like DJs, blending existing texts to produce impressive new outputs, while skilled human authors, similar to music composers, craft original </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.514.1">works</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.515.1">. </span><span class="koboSpan" id="kobo.515.2">”</span></span></p>
<p><span class="koboSpan" id="kobo.516.1">Despite LLMs </span><a id="_idIndexMarker1524"/><span class="koboSpan" id="kobo.517.1">being incapable of true creativity, several studies have tried to increase the pseudo-creativity of models (in the long run, LLMs can be particularly repetitive). </span><span class="koboSpan" id="kobo.517.2">There are three potential strategies for </span><span class="No-Break"><span class="koboSpan" id="kobo.518.1">doing this:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.519.1">Acting on the hyperparameters of an LLM</span></strong><span class="koboSpan" id="kobo.520.1">: The first strategy coincides with raising the temperature of an LLM. </span><span class="koboSpan" id="kobo.520.2">Temperature controls the uncertainty or randomness in the generation process. </span><span class="koboSpan" id="kobo.520.3">Adjusting temperature impacts model generation, where at low temperatures (e.g., 0.1–0.5), the model generates deterministic, focused, and predictable outputs. </span><span class="koboSpan" id="kobo.520.4">Increasing the temperature generates output that becomes less predictable. </span><span class="koboSpan" id="kobo.520.5">Beyond 2.0, the process becomes chaotic and the model generates nonsense. </span><span class="koboSpan" id="kobo.520.6">So, for applications that require creativity, you can explore higher temperatures but remember that this also generally leads to a reduction </span><span class="No-Break"><span class="koboSpan" id="kobo.521.1">in consistency.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.522.1">Conducting additional training for an LLM</span></strong><span class="koboSpan" id="kobo.523.1">: The use of post-training techniques is an avenue that is being explored widely today. </span><span class="koboSpan" id="kobo.523.2">Post-training techniques are used for model alignment and to make the model more receptive to performing tasks. </span><span class="koboSpan" id="kobo.523.3">Some authors have proposed using techniques that also incentivize the variety </span><span class="No-Break"><span class="koboSpan" id="kobo.524.1">of outputs.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.525.1">Prompting strategy</span></strong><span class="koboSpan" id="kobo.526.1">: Use prompts that try to force the model to be more creative. </span><span class="koboSpan" id="kobo.526.2">However, prompting </span><a id="_idIndexMarker1525"/><span class="koboSpan" id="kobo.527.1">strategies do not seem to have </span><span class="No-Break"><span class="koboSpan" id="kobo.528.1">great results.</span></span></li>
</ul>
<h2 id="_idParaDest-219"><a id="_idTextAnchor228"/><span class="koboSpan" id="kobo.529.1">Mechanistic interpretability</span></h2>
<p><span class="koboSpan" id="kobo.530.1">Recent advances in AI </span><a id="_idIndexMarker1526"/><span class="koboSpan" id="kobo.531.1">have meant rapid advancement in model capabilities. </span><span class="koboSpan" id="kobo.531.2">Paradoxically, the paradigm of self-supervised learning means that even if models are designed by humans, the capabilities of LLMs are not designed a priori. </span><span class="koboSpan" id="kobo.531.3">In theory, a developer only needs to know the process without understanding how the model works, since the desired properties appear during training. </span><span class="koboSpan" id="kobo.531.4">In other words, an LLM is not designed for the properties it shows; these properties were obtained through scaling, and much of how it gets there is unclear. </span><span class="koboSpan" id="kobo.531.5">Reconstructing how they appear and the mechanisms behind these abilities is not an easy task, especially after a model of billions of parameters has been trained. </span><span class="koboSpan" id="kobo.531.6">These models are considered to be black boxes, and recently, there has been some discussion of how they can </span><span class="No-Break"><span class="koboSpan" id="kobo.532.1">be analyzed.</span></span></p>
<p><span class="koboSpan" id="kobo.533.1">There are several</span><a id="_idIndexMarker1527"/><span class="koboSpan" id="kobo.534.1"> types of interpretability for a model (as described in the following list and figure). </span><span class="koboSpan" id="kobo.534.2">Each of these types of interpretability focuses on </span><span class="No-Break"><span class="koboSpan" id="kobo.535.1">different aspects.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer375">
<span class="koboSpan" id="kobo.536.1"><img alt="Figure 11.19 – Progressive level of interpretation of a model (https://arxiv.org/pdf/2404.14082)" src="image/B21257_11_19.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.537.1">Figure 11.19 – Progressive level of interpretation of a model (</span><a href="https://arxiv.org/pdf/2404.14082"><span class="koboSpan" id="kobo.538.1">https://arxiv.org/pdf/2404.14082</span></a><span class="koboSpan" id="kobo.539.1">)</span></p>
<p><span class="koboSpan" id="kobo.540.1">We can divide the various types of approaches to interpretability into </span><span class="No-Break"><span class="koboSpan" id="kobo.541.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.542.1">Behavioral</span></strong><span class="koboSpan" id="kobo.543.1">: One considers the model as a black box and is interested in the relationship between input and output. </span><span class="koboSpan" id="kobo.543.2">This paradigm considers those classical approaches to interpretability that </span><span class="No-Break"><span class="koboSpan" id="kobo.544.1">are model-agnostic.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.545.1">Attributional</span></strong><span class="koboSpan" id="kobo.546.1">: The approaches try to understand the decision-making processes of the model by tracking the contribution of each component of the input and are based on the </span><span class="No-Break"><span class="koboSpan" id="kobo.547.1">gradient shift.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.548.1">Concept-based</span></strong><span class="koboSpan" id="kobo.549.1">: Probes are used to try to better understand the learned representation of </span><span class="No-Break"><span class="koboSpan" id="kobo.550.1">the model.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.551.1">Mechanistic</span></strong><span class="koboSpan" id="kobo.552.1">: This is a granular analysis of the components and how they are organized, trying to identify </span><span class="No-Break"><span class="koboSpan" id="kobo.553.1">causal relationships.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.554.1">Mechanistic</span><a id="_idIndexMarker1528"/><span class="koboSpan" id="kobo.555.1"> interpretability aims to uncover the internal decision-making processes of a neural network by identifying the mechanisms that produce its outputs. </span><span class="koboSpan" id="kobo.555.2">We focus on this approach because it emphasizes understanding the individual components of a model and how they contribute to its overall behavior. </span><span class="koboSpan" id="kobo.555.3">This perspective is valuable as it enables us to analyze the model through a comprehensive and </span><span class="No-Break"><span class="koboSpan" id="kobo.556.1">transparent lens.</span></span></p>
<p><span class="koboSpan" id="kobo.557.1">Mechanistic interpretability goes beyond previous approaches because it seeks to identify causal mechanisms to the generalization of neural networks, and thus the decision-making processes behind them. </span><span class="koboSpan" id="kobo.557.2">In response to the growth of models and their increased capabilities, there has been a question of how these models acquire these general capabilities, and thus a need for </span><span class="No-Break"><span class="koboSpan" id="kobo.558.1">global explanations.</span></span></p>
<p><span class="koboSpan" id="kobo.559.1">Although LLMs generate text that resembles that produced by humans, this does not mean that the representation of concepts and cognitive processes are the same. </span><span class="koboSpan" id="kobo.559.2">This is demonstrated by the fact that LLMs display superhuman performance on some tasks, whereas in other tasks that are simple for humans, they fail miserably. </span><span class="koboSpan" id="kobo.559.3">We need a way to solve this paradox, which is through mechanistic interpretability. </span><span class="koboSpan" id="kobo.559.4">To try to resolve this dissonance, reverse engineering of LLMs has been proposed. </span><span class="koboSpan" id="kobo.559.5">Reverse engineering (a mechanistic interpretability approach) involves three steps: decomposing the model into simpler parts, describing how these parts work and how they interact, and testing whether the assumptions are correct. </span><span class="koboSpan" id="kobo.559.6">While mechanistic interpretability aims to uncover the internal logic and causal mechanisms within the network, concept-based interpretability focuses on understanding how models represent high-level, human-understandable concepts and how these concepts contribute to the model’s decisions, providing insights into the reasoning behind predictions and bridging the gap between human cognition and machine learning processes. </span><span class="koboSpan" id="kobo.559.7">These two approaches are shown in the </span><span class="No-Break"><span class="koboSpan" id="kobo.560.1">following figure:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer376">
<span class="koboSpan" id="kobo.561.1"><img alt="Figure 11.20 – Reverse engineering (https://arxiv.org/pdf/2501.16496)" src="image/B21257_11_20.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.562.1">Figure 11.20 – Reverse engineering (</span><a href="https://arxiv.org/pdf/2501.16496"><span class="koboSpan" id="kobo.563.1">https://arxiv.org/pdf/2501.16496</span></a><span class="koboSpan" id="kobo.564.1">)</span></p>
<p><span class="koboSpan" id="kobo.565.1">The problem </span><a id="_idIndexMarker1529"/><span class="koboSpan" id="kobo.566.1">with this approach is that it is difficult to decompose neural networks into functional components. </span><span class="koboSpan" id="kobo.566.2">In fact, in neural networks, neurons are polysemantic and represent more than one concept. </span><span class="koboSpan" id="kobo.566.3">So, the interpretation of single components is not very useful, and can instead be misleading. </span><span class="koboSpan" id="kobo.566.4">Authors today focus on trying to decompose into functional units that incorporate multiple neurons even on multiple layers. </span><span class="koboSpan" id="kobo.566.5">Since these concepts are represented by multiple neurons (superimposition hypothesis), attempts are made to disentangle this sparse representation through the use of tools that </span><span class="No-Break"><span class="koboSpan" id="kobo.567.1">force sparsity.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer377">
<span class="koboSpan" id="kobo.568.1"><img alt="Figure 11.21 – Disentangle superimposed representation with SDL (https://arxiv.org/pdf/2501.16496)" src="image/B21257_11_21.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.569.1">Figure 11.21 – Disentangle superimposed representation with SDL (</span><a href="https://arxiv.org/pdf/2501.16496"><span class="koboSpan" id="kobo.570.1">https://arxiv.org/pdf/2501.16496</span></a><span class="koboSpan" id="kobo.571.1">)</span></p>
<p><span class="koboSpan" id="kobo.572.1">This shift toward decomposing the model into functional units that incorporate multiple neurons and layers requires </span><span class="No-Break"><span class="koboSpan" id="kobo.573.1">new techniques.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.574.1">Sparse dictionary learning</span></strong><span class="koboSpan" id="kobo.575.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.576.1">SDL</span></strong><span class="koboSpan" id="kobo.577.1">) includes </span><a id="_idIndexMarker1530"/><span class="koboSpan" id="kobo.578.1">a number of approaches that allow a sparse representation of what a model has learned. </span><strong class="bold"><span class="koboSpan" id="kobo.579.1">Sparse autoencoders</span></strong><span class="koboSpan" id="kobo.580.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.581.1">SAEs</span></strong><span class="koboSpan" id="kobo.582.1">) are </span><a id="_idIndexMarker1531"/><span class="koboSpan" id="kobo.583.1">one such approach that allows us to learn sparse features that are connected to model features and make what the model has learned more accessible. </span><span class="koboSpan" id="kobo.583.2">SAEs use an encoder and decoder to sparsify the superimposed representation within </span><span class="No-Break"><span class="koboSpan" id="kobo.584.1">the model.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer378">
<span class="koboSpan" id="kobo.585.1"><img alt="Figure 11.22 – Illustration of an SAE applied to an LLM (https://arxiv.org/pdf/2404.14082)" src="image/B21257_11_22.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.586.1">Figure 11.22 – Illustration of an SAE applied to an LLM (</span><a href="https://arxiv.org/pdf/2404.14082"><span class="koboSpan" id="kobo.587.1">https://arxiv.org/pdf/2404.14082</span></a><span class="koboSpan" id="kobo.588.1">)</span></p>
<p><span class="koboSpan" id="kobo.589.1">SAEs </span><a id="_idIndexMarker1532"/><span class="koboSpan" id="kobo.590.1">allow us to identify </span><a id="_idIndexMarker1533"/><span class="koboSpan" id="kobo.591.1">features that are human interpretable, and through sparsity, we try to learn a small number of features. </span><span class="koboSpan" id="kobo.591.2">At a fundamental level, SAEs can extract features related to individual words or tokens, such as word frequency features (activations that correspond to high-frequency vs. </span><span class="koboSpan" id="kobo.591.3">low-frequency words), and part-of-speech features (features that selectively activate for nouns, verbs, or adjectives). </span><span class="koboSpan" id="kobo.591.4">SAEs often capture syntactic rules embedded within LLMs, such as activations that fire for certain syntactic patterns (e.g., subject-verb-object structures) or features corresponding to syntactic dependencies, such as whether a word is a noun modifying another noun. </span><span class="koboSpan" id="kobo.591.5">In addition, it is also possible to identify high-level features such as neurons that fire for texts about specific domains (e.g., politics, science, or sports) and whether a sentence expresses positive, negative, or neutral sentiment. </span><span class="koboSpan" id="kobo.591.6">Lastly, some features can be also related to writing style and discourse structure, such as distinguishing between academic writing and casual conversation, programming languages versus human language, or distinct writing styles of certain authors (e.g., Shakespeare vs. </span><span class="No-Break"><span class="koboSpan" id="kobo.592.1">X/Twitter posts).</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer379">
<span class="koboSpan" id="kobo.593.1"><img alt="Figure 11.23 – SAE training overview (https://arxiv.org/pdf/2309.08600)" src="image/B21257_11_23.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.594.1">Figure 11.23 – SAE training overview (</span><a href="https://arxiv.org/pdf/2309.08600"><span class="koboSpan" id="kobo.595.1">https://arxiv.org/pdf/2309.08600</span></a><span class="koboSpan" id="kobo.596.1">)</span></p>
<p><span class="koboSpan" id="kobo.597.1">Some features learned by</span><a id="_idIndexMarker1534"/><span class="koboSpan" id="kobo.598.1"> SAEs may not reflect real knowledge but rather random statistical properties of the model’s embeddings. </span><span class="koboSpan" id="kobo.598.2">In addition, SAEs</span><a id="_idIndexMarker1535"/><span class="koboSpan" id="kobo.599.1"> sometimes learn spurious correlations in hidden layers rather than meaningful conceptual structures. </span><span class="koboSpan" id="kobo.599.2">Also, SAEs focus on only one layer at a time, not considering that different neurons in different layers may interact for the same concept. </span><span class="koboSpan" id="kobo.599.3">Despite the associated cost, SAEs are considered a promising method for analyzing model behavior. </span><span class="koboSpan" id="kobo.599.4">At the same time, it was proposed to train LLMs in a more interpretable and scattered manner. </span><span class="koboSpan" id="kobo.599.5">The use of sparsity in the model weights aid interpretability. </span><span class="koboSpan" id="kobo.599.6">Techniques such as pruning and other similar techniques introduce zeros into the model weights, effectively erasing them. </span><span class="koboSpan" id="kobo.599.7">Sparsity eliminates connections between neurons; this makes it easier to follow the flow of information and better understand the model’s decision-making process (or the relationship connecting input and output). </span><span class="koboSpan" id="kobo.599.8">Mixture-of-experts has a similar effect and thus makes it </span><span class="No-Break"><span class="koboSpan" id="kobo.600.1">more interpretable.</span></span></p>
<p><span class="koboSpan" id="kobo.601.1">Interpretability techniques are now critical to understanding the behavior of LLMs and preventing dangerous behaviors from emerging, such as deceiving users, showing bias, giving wrong answers especially to please users’ beliefs (a phenomenon called “sycophancy”), and learning spurious correlations. </span><span class="koboSpan" id="kobo.601.2">As parameters and training have increased, models have become increasingly sophisticated in their responses, increasingly verbose, and persuasive, making it difficult for the user to understand whether an answer is correct. </span><span class="koboSpan" id="kobo.601.3">In addition, these models are now deployed with the public, which means users with malicious intentions can conduct attacks such as data poisoning, jailbreaking, adversarial attacks, and so on. </span><span class="koboSpan" id="kobo.601.4">Interpretability helps to monitor the behavior of the model in its interaction with the public, highlight where there have been failures, and address them in real time. </span><span class="koboSpan" id="kobo.601.5">Interpretability is an important requirement for model safety because it allows us not only to identify problem behaviors but also to identify which components are responsible for them. </span><span class="koboSpan" id="kobo.601.6">Once we have identified components associated with unintended behaviors, we can intervene </span><span class="No-Break"><span class="koboSpan" id="kobo.602.1">with steering.</span></span></p>
<p><span class="koboSpan" id="kobo.603.1">In addition, today, there is much more attention to privacy, and </span><em class="italic"><span class="koboSpan" id="kobo.604.1">machine unlearning</span></em><span class="koboSpan" id="kobo.605.1"> is the application field that deals with scrubbing the influence of particular data points on a trained machine learning model. </span><span class="koboSpan" id="kobo.605.2">For example, regulatory questions may require that we remove information concerning a person from our model. </span><span class="koboSpan" id="kobo.605.3">Machine unlearning deals with trying to remove this information without having to train the model from scratch. </span><span class="koboSpan" id="kobo.605.4">Machine unlearning is related to interpretability, as decomposition techniques allow us to localize concepts and information in model parameters. </span><span class="koboSpan" id="kobo.605.5">More generally, we want to have the ability to be able to edit model knowledge (such as correcting factual errors, removing copyrighted content, or eliminating harmful information like instructions for weapon construction). </span><span class="koboSpan" id="kobo.605.6">Editing requires being able to intervene on model parameters in a surgical manner without destroying additional knowledge and other capabilities. </span><span class="koboSpan" id="kobo.605.7">Editing is even more complex than unlearning because it means rewriting model knowledge. </span><span class="koboSpan" id="kobo.605.8">Interpretability techniques allow us to understand whether editing or unlearning has worked and then to monitor </span><span class="No-Break"><span class="koboSpan" id="kobo.606.1">the process.</span></span></p>
<p><span class="koboSpan" id="kobo.607.1">Interpretability is</span><a id="_idIndexMarker1536"/><span class="koboSpan" id="kobo.608.1"> also useful in trying to predict how the model will perform in new situations, and thus avoid safety risks. </span><span class="koboSpan" id="kobo.608.2">Some behaviors of the model may, in fact, appear only in unanticipated situations, and may not manifest themselves when conducting a standard evaluation. </span><span class="koboSpan" id="kobo.608.3">For example, we can identify susceptibility or potential backdoors before these are discovered by users. </span><span class="koboSpan" id="kobo.608.4">Considering that today’s LLMs are increasingly connected to tools, any misuse can have effects that propagate. </span><span class="koboSpan" id="kobo.608.5">For example, if an LLM is connected to finance databases, it could be exploited to extract information about users. </span><span class="koboSpan" id="kobo.608.6">Or an LLM that shops online could be exploited for fraud and buying fraudulent products. </span><span class="koboSpan" id="kobo.608.7">Fine-tuning and other post-training steps can lead to the emergence or exacerbation of behaviors that were not present in the pre-trained model. </span><span class="koboSpan" id="kobo.608.8">In addition, some properties seem to emerge at scale and are difficult to predict when we train a smaller model. </span><span class="koboSpan" id="kobo.608.9">Often, smaller versions of the final architecture are trained when designing a new architecture. </span><span class="koboSpan" id="kobo.608.10">Smaller models may not have problems that emerge only </span><span class="No-Break"><span class="koboSpan" id="kobo.609.1">at scale.</span></span></p>
<p><span class="koboSpan" id="kobo.610.1">Interpretability also has important commendable aspects; understanding the behavior of the model and its components allows us to be able to speed up inference. </span><span class="koboSpan" id="kobo.610.2">For example, if some computations are unnecessary, we could turn them off, or use the knowledge gained to distill a more efficient model. </span><span class="koboSpan" id="kobo.610.3">In addition, we could identify components that impact either positive or </span><span class="No-Break"><span class="koboSpan" id="kobo.611.1">negative reasoning.</span></span></p>
<p><span class="koboSpan" id="kobo.612.1">Another intriguing aspect of interpretability is that it can be used for new discoveries (commonly called </span><strong class="bold"><span class="koboSpan" id="kobo.613.1">microscope AI</span></strong><span class="koboSpan" id="kobo.614.1">). </span><span class="koboSpan" id="kobo.614.2">In other</span><a id="_idIndexMarker1537"/><span class="koboSpan" id="kobo.615.1"> words, you can investigate a model that has been trained on certain data, and you can use interpretability techniques to gain insights into it. </span><span class="koboSpan" id="kobo.615.2">You </span><a id="_idIndexMarker1538"/><span class="koboSpan" id="kobo.616.1">can use these techniques to identify patterns that might have eluded humans. </span><span class="koboSpan" id="kobo.616.2">For example, after AlphaZero’s success in defeating humans at chess, researchers considered extracting information from the model to identify concepts about sacrifices that humans could learn. </span><span class="koboSpan" id="kobo.616.3">In this paper (</span><a href="https://arxiv.org/abs/2310.16410"><span class="koboSpan" id="kobo.617.1">https://arxiv.org/abs/2310.16410</span></a><span class="koboSpan" id="kobo.618.1">), Schut et al. </span><span class="koboSpan" id="kobo.618.2">(2023) identified these concepts or patterns to see how the model had a different representation of </span><span class="No-Break"><span class="koboSpan" id="kobo.619.1">the game.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer380">
<span class="koboSpan" id="kobo.620.1"><img alt="Figure 11.24 – Learning from machine-unique knowledge (https://arxiv.org/pdf/2310.16410)" src="image/B21257_11_24.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.621.1">Figure 11.24 – Learning from machine-unique knowledge (</span><a href="https://arxiv.org/pdf/2310.16410"><span class="koboSpan" id="kobo.622.1">https://arxiv.org/pdf/2310.16410</span></a><span class="koboSpan" id="kobo.623.1">)</span></p>
<p><span class="koboSpan" id="kobo.624.1">LLMs have a large memory, and humans express themselves through language; this allows them to analyze and conduct hypotheses about human psychology. </span><span class="koboSpan" id="kobo.624.2">Interpretability then is an approach that allows us to not only better understand the model but also be able to use models as a tool to better understand humans. </span><span class="koboSpan" id="kobo.624.3">In the next section, we will discuss how models can potentially approach </span><span class="No-Break"><span class="koboSpan" id="kobo.625.1">human intelligence.</span></span></p>
<h2 id="_idParaDest-220"><a id="_idTextAnchor229"/><span class="koboSpan" id="kobo.626.1">The road to artificial general intelligence</span></h2>
<p class="author-quote"><em class="italic"><span class="koboSpan" id="kobo.627.1">“Artificial general intelligence (AGI) is a hypothesized type of highly autonomous artificial intelligence (AI) that would match or surpass human capabilities across most or all economically valuable cognitive work. </span><span class="koboSpan" id="kobo.627.2">It contrasts with narrow AI, which is limited to specific tasks. </span><span class="koboSpan" id="kobo.627.3">Artificial superintelligence (ASI), on the other hand, refers to AGI that greatly exceeds human cognitive capabilities. </span><span class="koboSpan" id="kobo.627.4">AGI is considered one of the definitions of strong AI.”</span></em></p>
<p><span class="koboSpan" id="kobo.628.1">This is the definition</span><a id="_idIndexMarker1539"/><span class="koboSpan" id="kobo.629.1"> of </span><strong class="bold"><span class="koboSpan" id="kobo.630.1">artificial general intelligence</span></strong><span class="koboSpan" id="kobo.631.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.632.1">AGI</span></strong><span class="koboSpan" id="kobo.633.1">) according to</span><a id="_idIndexMarker1540"/><span class="koboSpan" id="kobo.634.1"> Wikipedia. </span><span class="koboSpan" id="kobo.634.2">Before imagining AI capable of surpassing humans, one must ask whether AI has caught up with human capabilities. </span><span class="koboSpan" id="kobo.634.3">In general, before the advent of ChatGPT, this debate did not begin (at least for the general public). </span><span class="koboSpan" id="kobo.634.4">This is because the previous models had superhuman capabilities only for specialized applications. </span><span class="koboSpan" id="kobo.634.5">For example, AlphaGo had been able to defeat human champions with relative ease, but no one thought that what makes us human was knowing how to play Go. </span><span class="koboSpan" id="kobo.634.6">Models such </span><a id="_idIndexMarker1541"/><span class="koboSpan" id="kobo.635.1">as DALL-E and ChatGPT, on the other hand, have begun to raise questions in the general audience </span><a id="_idIndexMarker1542"/><span class="koboSpan" id="kobo.636.1">as well. </span><span class="koboSpan" id="kobo.636.2">After all, generating art or creative writing are skills that are generally connected with humans. </span><span class="koboSpan" id="kobo.636.3">This feeling was reinforced when ChatGPT and other LLMs were able to pass university or medical and legal </span><span class="No-Break"><span class="koboSpan" id="kobo.637.1">licensing exams.</span></span></p>
<p><span class="koboSpan" id="kobo.638.1">We discussed creativity and reasoning in previous subsections. </span><span class="koboSpan" id="kobo.638.2">The current consensus is that LLMs do not exhibit true reasoning or creativity skills. </span><span class="koboSpan" id="kobo.638.3">They are sophisticated stochastic pattern machines, and their ability to find patterns in the whole of human knowledge makes them </span><span class="No-Break"><span class="koboSpan" id="kobo.639.1">extraordinarily effective.</span></span></p>
<p><span class="koboSpan" id="kobo.640.1">If LLMs are not capable of showing a level of human intelligence today, one may wonder what that might bring to AGI. </span><span class="koboSpan" id="kobo.640.2">Until now, it has been believed that it was possible to achieve AGI simply by scaling parameters and training. </span><span class="koboSpan" id="kobo.640.3">According to the idea of emergent properties, reasoning and creativity should appear at some point in the scaling (by increasing the size of the model and the number of tokens used for training, without our being able to predict it, the model should begin to show true reasoning). </span><span class="koboSpan" id="kobo.640.4">Today, most researchers do not believe that this is possible, nor that post-training techniques </span><span class="No-Break"><span class="koboSpan" id="kobo.641.1">will suffice.</span></span></p>
<p><span class="koboSpan" id="kobo.642.1">Moreover, scaling is not possible indefinitely. </span><span class="koboSpan" id="kobo.642.2">Even if we could invest enormous amounts of money and resources, there is not enough text to create models that grow linearly. </span><span class="koboSpan" id="kobo.642.3">In fact, humans generate a limited amount of text, and we are approaching the limit of the stock of text generated </span><span class="No-Break"><span class="koboSpan" id="kobo.643.1">by humans.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer381">
<span class="koboSpan" id="kobo.644.1"><img alt="Figure 11.25 – Projections of the stock of public text and data usage (https://epoch.ai/blog/will-we-run-out-of-data-limits-of-llm-scaling-based-on-human-generated-data)" src="image/B21257_11_25.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.645.1">Figure 11.25 – Projections of the stock of public text and data usage (</span><a href="https://epoch.ai/blog/will-we-run-out-of-data-limits-of-llm-scaling-based-on-human-generated-data"><span class="koboSpan" id="kobo.646.1">https://epoch.ai/blog/will-we-run-out-of-data-limits-of-llm-scaling-based-on-human-generated-data</span></a><span class="koboSpan" id="kobo.647.1">)</span></p>
<p><span class="koboSpan" id="kobo.648.1">The solution to </span><a id="_idIndexMarker1543"/><span class="koboSpan" id="kobo.649.1">this could</span><a id="_idIndexMarker1544"/><span class="koboSpan" id="kobo.650.1"> be the use of synthetic data. </span><span class="koboSpan" id="kobo.650.2">Synthetic data, however, can be considered a kind of “knowledge distillation” and can lead to model collapse. </span><span class="koboSpan" id="kobo.650.3">Models that are trained with synthetic data go into collapse, showing that performance </span><span class="No-Break"><span class="koboSpan" id="kobo.651.1">degrades rapidly.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer382">
<span class="koboSpan" id="kobo.652.1"><img alt="Figure 11.26 – Examples generated after iterative retraining for different compositions of the retraining dataset, from 0% synthetic data to 100 % synthetic data (https://arxiv.org/pdf/2311.12202)" src="image/B21257_11_26.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.653.1">Figure 11.26 – Examples generated after iterative retraining for different compositions of the retraining dataset, from 0% synthetic data to 100 % synthetic data (</span><a href="https://arxiv.org/pdf/2311.12202"><span class="koboSpan" id="kobo.654.1">https://arxiv.org/pdf/2311.12202</span></a><span class="koboSpan" id="kobo.655.1">)</span></p>
<p><span class="koboSpan" id="kobo.656.1">If scaling is not the </span><a id="_idIndexMarker1545"/><span class="koboSpan" id="kobo.657.1">solution, some researchers propose that the key</span><a id="_idIndexMarker1546"/><span class="koboSpan" id="kobo.658.1"> lies in developing a “world model.” </span><span class="koboSpan" id="kobo.658.2">That is, much like the human brain constructs an internal representation of the external environment, building such structured representations could be essential to advancing the capabilities of LLMs. </span><span class="koboSpan" id="kobo.658.3">This representation is used to imagine possible actions or consequences of actions. </span><span class="koboSpan" id="kobo.658.4">This model would also be used to generalize tasks we have learned in one domain and apply them to another. </span><span class="koboSpan" id="kobo.658.5">Today, some researchers suggest that LLMs have a rudimentary model of the world and that this can also be visualized. </span><span class="koboSpan" id="kobo.658.6">For example, Gurnee (2023) states that LLMs form a rudimentary “world model” during training and that it shows </span><span class="No-Break"><span class="koboSpan" id="kobo.659.1">spatiotemporal</span></span><span class="No-Break"><a id="_idIndexMarker1547"/></span><span class="No-Break"><span class="koboSpan" id="kobo.660.1"> representations.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer383">
<span class="koboSpan" id="kobo.661.1"><img alt="Figure 11.27 – Spatial and temporal world models of Llama-2-70b (https://arxiv.org/pdf/2310.02207)" src="image/B21257_11_27.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.662.1">Figure 11.27 – Spatial and temporal world models of Llama-2-70b (</span><a href="https://arxiv.org/pdf/2310.02207"><span class="koboSpan" id="kobo.663.1">https://arxiv.org/pdf/2310.02207</span></a><span class="koboSpan" id="kobo.664.1">)</span></p>
<p><span class="koboSpan" id="kobo.665.1">These </span><a id="_idIndexMarker1548"/><span class="koboSpan" id="kobo.666.1">spatiotemporal representations are far from constituting a dynamic causal world model, but they seem to be the first elements for its evolution. </span><span class="koboSpan" id="kobo.666.2">However, there is no consensus on whether these world models can then evolve into something that is robust and reliable for conducting simulations or learning causal relationships as in humans. </span><span class="koboSpan" id="kobo.666.3">For example, in one study (Vafa, 2024), transformers failed to create a reliable map of New York City that can be used to conduct predictions and then used </span><span class="No-Break"><span class="koboSpan" id="kobo.667.1">to guide.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer384">
<span class="koboSpan" id="kobo.668.1"><img alt="Figure 11.28 – Reconstructed maps of Manhattan from sequences produced by three models (https://arxiv.org/pdf/2406.03689)" src="image/B21257_11_28.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.669.1">Figure 11.28 – Reconstructed maps of Manhattan from sequences produced by three models (</span><a href="https://arxiv.org/pdf/2406.03689"><span class="koboSpan" id="kobo.670.1">https://arxiv.org/pdf/2406.03689</span></a><span class="koboSpan" id="kobo.671.1">)</span></p>
<p><span class="koboSpan" id="kobo.672.1">Certainly, there is a </span><a id="_idIndexMarker1549"/><span class="koboSpan" id="kobo.673.1">wealth of information in language that can be</span><a id="_idIndexMarker1550"/><span class="koboSpan" id="kobo.674.1"> learned, and that enables LLMs to be able to solve a large number of tasks. </span><span class="koboSpan" id="kobo.674.2">However, some researchers suggest that this is not enough and that models should be embodied (being used in a physical agent and being able to interact physically with the environment) in order to really make a quantum leap (including being able to learn a more robust world model). </span><span class="koboSpan" id="kobo.674.3">To date, this is a hypothesis and remains an </span><span class="No-Break"><span class="koboSpan" id="kobo.675.1">open question.</span></span></p>
<h2 id="_idParaDest-221"><a id="_idTextAnchor230"/><span class="koboSpan" id="kobo.676.1">Ethical questions</span></h2>
<p><span class="koboSpan" id="kobo.677.1">An article was recently published</span><a id="_idIndexMarker1551"/><span class="koboSpan" id="kobo.678.1"> suggesting that fully autonomous AI agents should not be developed (Mitchell, 2025). </span><span class="koboSpan" id="kobo.678.2">While this might seem drastic, it still emphasizes the risks that autonomous agents </span><span class="No-Break"><span class="koboSpan" id="kobo.679.1">can bring:</span></span></p>
<p class="author-quote"><em class="italic"><span class="koboSpan" id="kobo.680.1">The development of AI agents is a critical inflection point in artificial intelligence. </span><span class="koboSpan" id="kobo.680.2">As history demonstrates, even well-engineered autonomous systems can make catastrophic errors from trivial causes. </span><span class="koboSpan" id="kobo.680.3">While increased autonomy can offer genuine benefits in specific contexts, human judgment and contextual understanding remain essential, particularly for high-stakes decisions.</span></em></p>
<p><span class="koboSpan" id="kobo.681.1">The authors identify a series of levels for agents, in which humans progressively cede control of a process to the agent until the AI agent takes complete control. </span><span class="koboSpan" id="kobo.681.2">Recent developments in AI show how we are moving closer to creating processes where agents are in charge of an </span><span class="No-Break"><span class="koboSpan" id="kobo.682.1">entire process.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer385">
<span class="koboSpan" id="kobo.683.1"><img alt="Figure 11.29 – Levels of agents (https://arxiv.org/pdf/2502.02649)" src="image/B21257_11_29.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.684.1">Figure 11.29 – Levels of agents (</span><a href="https://arxiv.org/pdf/2502.02649"><span class="koboSpan" id="kobo.685.1">https://arxiv.org/pdf/2502.02649</span></a><span class="koboSpan" id="kobo.686.1">)</span></p>
<p><span class="koboSpan" id="kobo.687.1">In </span><a href="B21257_03.xhtml#_idTextAnchor042"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.688.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.689.1">, we</span><a id="_idIndexMarker1552"/><span class="koboSpan" id="kobo.690.1"> discussed the risks associated with LLMs, whereas, in this subsection, we want to discuss in detail the risks that are associated with AI agents. </span><span class="koboSpan" id="kobo.690.2">Clearly, many of the risks of agent systems arise from LLMs (an LLM is central to an agent system), but extending an LLM’s ability with tools creates or exacerbates </span><span class="No-Break"><span class="koboSpan" id="kobo.691.1">new risks.</span></span></p>
<p><span class="koboSpan" id="kobo.692.1">Before addressing some of the risks in detail, we would like to discuss one of the least underestimated risks of generative AI: namely, the risk of anthropomorphizing agents. </span><span class="koboSpan" id="kobo.692.2">As we mentioned previously, LLMs have no level of consciousness nor do they generate real emotions. </span><span class="koboSpan" id="kobo.692.3">LLMs emulate the distribution with which they are trained; this makes them appear as though they can emulate emotions (this clearly does not mean that they actually possess or express emotions). </span><span class="koboSpan" id="kobo.692.4">This must be taken into account in interactions with chatbots or other social applications in which an LLM is present. </span><span class="koboSpan" id="kobo.692.5">These “perceived emotions” affect not only users but also researchers who must interpret the results of agents. </span><span class="koboSpan" id="kobo.692.6">Their ability to emulate emotions can be an effective tool for studies that simulate human behaviors, but excessive anthropomorphization risks creating misinformation and misattribution of results. </span><span class="koboSpan" id="kobo.692.7">In addition, anthropomorphization can lead to the risk of creating parasocial relationships between users and AI agents (a risk that will become greater when agents are embodied and thus capable of </span><span class="No-Break"><span class="koboSpan" id="kobo.693.1">physical interaction).</span></span></p>
<p><span class="koboSpan" id="kobo.694.1">Linked to the risk of anthropomorphization is the risk of excessive influence on users. </span><span class="koboSpan" id="kobo.694.2">There is the risk of a user being over-reliant and over-confident in an agent. </span><span class="koboSpan" id="kobo.694.3">Whether it is because of errors (such as hallucinations) or malicious behavior (poisoning or hacking), a user should be sufficiently skeptical of an agent’s behavior. </span><span class="koboSpan" id="kobo.694.4">Influence risk is considered a group of risks that</span><a id="_idIndexMarker1553"/><span class="koboSpan" id="kobo.695.1"> influence the user’s behavior </span><span class="No-Break"><span class="koboSpan" id="kobo.696.1">and beliefs:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.697.1">Persuasion</span></strong><span class="koboSpan" id="kobo.698.1">: Refers to the ability of a model to influence a user’s behavior. </span><span class="koboSpan" id="kobo.698.2">This can be especially problematic when an agent forces a transformative choice on the user or solicits behaviors that </span><span class="No-Break"><span class="koboSpan" id="kobo.699.1">are harmful.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.700.1">Manipulation</span></strong><span class="koboSpan" id="kobo.701.1">: Refers to agents that bypass an individual’s rational capabilities (such as misrepresenting information or exploiting cognitive bias) to influence decision-making. </span><span class="koboSpan" id="kobo.701.2">This behavior could also emerge as a byproduct of poor design choices, creating a product that keeps the user engaged, or personalization for the purpose of creating trust. </span><span class="koboSpan" id="kobo.701.3">It is morally problematic because it does not respect the user’s autonomy and could force the user into behaviors that are harmful </span><span class="No-Break"><span class="koboSpan" id="kobo.702.1">to himself.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.703.1">Deception</span></strong><span class="koboSpan" id="kobo.704.1">: Refers to strategies that cause an individual to form a false belief. </span><span class="koboSpan" id="kobo.704.2">This is likely to push a user toward behaviors that could be harmful to themselves because they are confused by </span><span class="No-Break"><span class="koboSpan" id="kobo.705.1">false beliefs.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.706.1">Coercion</span></strong><span class="koboSpan" id="kobo.707.1">: Implies an individual choosing something because they have no other acceptable alternative. </span><span class="koboSpan" id="kobo.707.2">This risk can be physical (with embodied agents) or psychological (</span><span class="No-Break"><span class="koboSpan" id="kobo.708.1">also chatbots).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.709.1">Exploitation</span></strong><span class="koboSpan" id="kobo.710.1">: Implies taking unfair advantage of an individual’s circumstances. </span><span class="koboSpan" id="kobo.710.2">AI agents can be programmed to be exploitative (we can imagine an AI agent in a casino trying to push users to spend as much </span><span class="No-Break"><span class="koboSpan" id="kobo.711.1">as possible).</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.712.1">These behaviors can be exploited by malicious actors. </span><span class="koboSpan" id="kobo.712.2">For example, an agent’s persuasion skills can be used for the spread of misinformation online. </span><span class="koboSpan" id="kobo.712.3">LLMs are capable of generating impressive amounts of text that can seem authoritative. </span><span class="koboSpan" id="kobo.712.4">An LLM in itself can generate hallucinations, but it can be used for the purpose of intentionally generating fake news with a specific purpose. </span><span class="koboSpan" id="kobo.712.5">The use of agents allows an LLM to use additional tools (generate images and videos and retrieve information) and feed them directly into communication channels. </span><span class="koboSpan" id="kobo.712.6">Paradoxically, since this fake news is difficult to intercept by humans, agents can also be used to combat the spread of AI-generated misinformation. </span><span class="koboSpan" id="kobo.712.7">Agents can then be used to generate disinformation at scale, with the cost to generate gradually dropping (and is still lower than employing humans). </span><span class="koboSpan" id="kobo.712.8">In addition, agents make it possible to search for information about the victim and thus generate customized content to be </span><span class="No-Break"><span class="koboSpan" id="kobo.713.1">more effective.</span></span></p>
<p><span class="koboSpan" id="kobo.714.1">Misinformation can be </span><a id="_idIndexMarker1554"/><span class="koboSpan" id="kobo.715.1">used to reinforce bias toward individuals or groups. </span><span class="koboSpan" id="kobo.715.2">This content (text, images, audio, and video) can be used to influence political elections or drive citizen outrage. </span><span class="koboSpan" id="kobo.715.3">In addition, apart from disinformation, agents can also produce other types of harmful content, including depictions of nudity, hate, </span><span class="No-Break"><span class="koboSpan" id="kobo.716.1">or violence.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer386">
<span class="koboSpan" id="kobo.717.1"><img alt="Figure 11.30 – Opportunities and challenges of combating misinformation in the age of LLMs (https://arxiv.org/pdf/2311.05656)" src="image/B21257_11_30.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.718.1">Figure 11.30 – Opportunities and challenges of combating misinformation in the age of LLMs (</span><a href="https://arxiv.org/pdf/2311.05656"><span class="koboSpan" id="kobo.719.1">https://arxiv.org/pdf/2311.05656</span></a><span class="koboSpan" id="kobo.720.1">)</span></p>
<p><span class="koboSpan" id="kobo.721.1">Malicious actors can also use agents for additional purposes such as phishing attacks, cyberattacks, or scams. </span><span class="koboSpan" id="kobo.721.2">In fact, LLMs can also generate harmful code that can be used to steal money or information. </span><span class="koboSpan" id="kobo.721.3">Chatbots can be used for the purpose of gaining trust and convincing someone to share information or earnings. </span><span class="koboSpan" id="kobo.721.4">There are also devious ways to attack an agent; for example, we can imagine an agent who conducts purchases for a user can be infiltrated by a bad actor who poisons the agent and prompts it to conduct fraudulent purchases. </span><span class="koboSpan" id="kobo.721.5">The planned deployment of AI assistants in fields such as healthcare, law, education, and science multiplies the risks and severity of possible harm. </span><span class="koboSpan" id="kobo.721.6">In addition, many of the AI agents today are natively multimodal (multiple possible types of inputs and outputs) and leverage deep reasoning models that are capable of more reasoning and planning. </span><span class="koboSpan" id="kobo.721.7">In addition, unlike LLMs, they also incorporate memory systems, all of which add risk. </span><span class="koboSpan" id="kobo.721.8">For example, an agent tasked with conducting a cyberattack could retrieve from memory successful past attacks or discard outdated techniques, search for information on online vulnerabilities, generate and execute code, and devise a multi-step strategy. </span><span class="koboSpan" id="kobo.721.9">As has been seen, a malicious actor can interfere with an LLM in several ways, for example, through prompt injection or information extraction. </span><span class="koboSpan" id="kobo.721.10">An LLM </span><a id="_idIndexMarker1555"/><span class="koboSpan" id="kobo.722.1">acquires sensitive information during its training that can be extracted. </span><span class="koboSpan" id="kobo.722.2">Agents can be connected to sensitive databases, and there are techniques to make LLMs extract the content. </span><span class="koboSpan" id="kobo.722.3">In addition, agent misuse can be conducted by authoritarian governments. </span><span class="koboSpan" id="kobo.722.4">For example, governments may use agents to generate misinformation or censorship and may use them for surveillance, tracking, and silencing dissent. </span><span class="koboSpan" id="kobo.722.5">Advanced agent systems can extract data from cell phones, cars, the Internet of Things, and more, making it easier to control </span><span class="No-Break"><span class="koboSpan" id="kobo.723.1">the population.</span></span></p>
<p><span class="koboSpan" id="kobo.724.1">Another risk is the economic impact of these agents. </span><span class="koboSpan" id="kobo.724.2">AI is expected to impact several aspects of the economy in terms of productivity but also in terms of employment, job quality, and inequality. </span><span class="koboSpan" id="kobo.724.3">The use of agents and AI in general have different </span><span class="No-Break"><span class="koboSpan" id="kobo.725.1">associated risks:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.726.1">Employment</span></strong><span class="koboSpan" id="kobo.727.1">: Various research estimates that 47 percent of jobs are at risk of automation, especially jobs that are characterized by routines and physical tasks such as driving and manufacturing. </span><span class="koboSpan" id="kobo.727.2">Advances in LLMs have also brought alarm to jobs that involve generating and manipulating information, and that are normally associated with higher levels of education such as translators, tax advisers, or even software engineers. </span><span class="koboSpan" id="kobo.727.3">AI could therefore accelerate job loss for positions requiring skilled labor, without creating a number of positions with which to absorb </span><span class="No-Break"><span class="koboSpan" id="kobo.728.1">displaced jobs.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.729.1">Job quality</span></strong><span class="koboSpan" id="kobo.730.1">: Some initial studies have suggested that the use of AI could make workers more productive and increase the wages of workers. </span><span class="koboSpan" id="kobo.730.2">Some studies, however, place emphasis on the possibility that employers may more efficiently monitor their employees with greater stress. </span><span class="koboSpan" id="kobo.730.3">Other studies note how the introduction of robots may reduce the physical workload in manufacturing but push workers to work faster, with less human contact and </span><span class="No-Break"><span class="koboSpan" id="kobo.731.1">more supervision.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.732.1">Inequality</span></strong><span class="koboSpan" id="kobo.733.1">: On the one hand, technological development has decreased inequality between different countries. </span><span class="koboSpan" id="kobo.733.2">At the same time, intra-country income inequality has increased, with a sharper separation of wealth between the richest and poorest. </span><span class="koboSpan" id="kobo.733.3">There are few studies looking at how AI may impact inequality, but some studies suggest that firms are best able to draw on AI with increased productivity and earnings, while workers are at risk of displacement and thus reduced income. </span><span class="koboSpan" id="kobo.733.4">Some studies suggest that high-income occupations may benefit from using AI, while others will be impacted. </span><span class="koboSpan" id="kobo.733.5">For example, AI assistants appear to impact junior positions by reducing them. </span><span class="koboSpan" id="kobo.733.6">In addition, most of the leading AI research labs, start-ups, and enterprises are located in certain geographic areas, with the risk of concentration of well-paying positions. </span><span class="koboSpan" id="kobo.733.7">Conversely, AI also creates low-paying jobs, especially in data creation and </span><span class="No-Break"><span class="koboSpan" id="kobo.734.1">data acquisition.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.735.1">To date, AI tools </span><a id="_idIndexMarker1556"/><span class="koboSpan" id="kobo.736.1">are not sophisticated enough to replace humans, but some effects on employment are already visible. </span><span class="koboSpan" id="kobo.736.2">Tools such as </span><a id="_idIndexMarker1557"/><span class="koboSpan" id="kobo.737.1">ChatGPT and DALL-E have</span><a id="_idIndexMarker1558"/><span class="koboSpan" id="kobo.738.1"> already had an impact on the “creative economies” (which includes writers, artists, designers, photographers, content creators, and so on) with reduced positions </span><span class="No-Break"><span class="koboSpan" id="kobo.739.1">and earnings.</span></span></p>
<p><span class="koboSpan" id="kobo.740.1">Another risk is the environmental impact of generative AI. </span><span class="koboSpan" id="kobo.740.2">Data and compute underlie the training and use of AI systems. </span><span class="koboSpan" id="kobo.740.3">Thus, hardware and infrastructure for storage and processing (including data centers and telecommunications) are required for the creation and use of agents. </span><span class="koboSpan" id="kobo.740.4">Creating the necessary hardware has an environmental impact (mining of rare earths, energy to build and ship them, water in plants, and use of chemicals). </span><span class="koboSpan" id="kobo.740.5">Then, a model requires energy to be drawn, built, and deployed (operational costs). </span><span class="koboSpan" id="kobo.740.6">Beyond training, deployment in inference also requires resources and energy consumption. </span><span class="koboSpan" id="kobo.740.7">Energy consumption and corresponding carbon dioxide emissions associated with LLM training are increasing over time. </span><span class="koboSpan" id="kobo.740.8">As can be seen in the following figure, carbon dioxide production linearly increases with energy consumption for training (directly related to the number of parameters and the increase </span><span class="No-Break"><span class="koboSpan" id="kobo.741.1">in training):</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer387">
<span class="koboSpan" id="kobo.742.1"><img alt="Figure 11.31 – Estimated energy consumed (kWh) and CO2 (kg) by different models (https://arxiv.org/pdf/2302.08476)" src="image/B21257_11_31.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.743.1">Figure 11.31 – Estimated energy consumed (kWh) and CO2 (kg) by different models (</span><a href="https://arxiv.org/pdf/2302.08476"><span class="koboSpan" id="kobo.744.1">https://arxiv.org/pdf/2302.08476</span></a><span class="koboSpan" id="kobo.745.1">)</span></p>
<p><span class="koboSpan" id="kobo.746.1">Considering the </span><a id="_idIndexMarker1559"/><span class="koboSpan" id="kobo.747.1">increase in users who use LLMs (or services that include LLMs) on a daily basis, the impact of training on emissions is only a fraction. </span><span class="koboSpan" id="kobo.747.2">Today, inference is supposed to be increasingly important (it has been estimated that 60% of machine learning energy use at Google from 2019–2021 was attributable </span><span class="No-Break"><span class="koboSpan" id="kobo.748.1">to inference).</span></span></p>
<p><span class="koboSpan" id="kobo.749.1">These are some of the possible risks with LLMs and agents. </span><span class="koboSpan" id="kobo.749.2">To date, strategies are being studied to try to address and mitigate </span><span class="No-Break"><span class="koboSpan" id="kobo.750.1">these risks.</span></span></p>
<h1 id="_idParaDest-222"><a id="_idTextAnchor231"/><span class="koboSpan" id="kobo.751.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.752.1">This chapter presented how some industries will be revolutionized by agents. </span><span class="koboSpan" id="kobo.752.2">The AI revolution goes beyond these industries and will have a large-scale impact. </span><span class="koboSpan" id="kobo.752.3">This book, however, provided a serious and structured introduction to the technical component that will drive this revolution, giving you the tools to understand the future that will come (and is already upon us). </span><span class="koboSpan" id="kobo.752.4">Apart from the sense of wonder that this technological revolution may inspire, we wanted to remind you that there are still technical and ethical challenges that should not </span><span class="No-Break"><span class="koboSpan" id="kobo.753.1">be overlooked.</span></span></p>
<p><span class="koboSpan" id="kobo.754.1">This chapter closes this book but leaves open a series of questions and challenges for the future. </span><span class="koboSpan" id="kobo.754.2">Readers who have followed us up to this point can find in this final chapter suggestions for leveraging what they have learned at the industry level and at the </span><span class="No-Break"><span class="koboSpan" id="kobo.755.1">research level.</span></span></p>
<h1 id="_idParaDest-223"><a id="_idTextAnchor232"/><span class="koboSpan" id="kobo.756.1">Further reading</span></h1>
<ul>
<li><span class="koboSpan" id="kobo.757.1">Luo, </span><em class="italic"><span class="koboSpan" id="kobo.758.1">BioGPT: </span></em><em class="italic"><span class="koboSpan" id="kobo.759.1">Generative Pre-trained Transformer for Biomedical Text Generation and Mining</span></em><span class="koboSpan" id="kobo.760.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.761.1">2022, </span></span><a href="https://academic.oup.com/bib/article/23/6/bbac409/6713511"><span class="No-Break"><span class="koboSpan" id="kobo.762.1">https://academic.oup.com/bib/article/23/6/bbac409/6713511</span></span></a></li>
<li><span class="koboSpan" id="kobo.763.1">Yao, </span><em class="italic"><span class="koboSpan" id="kobo.764.1">Health System-scale Language Models are All-purpose Prediction Engines</span></em><span class="koboSpan" id="kobo.765.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.766.1">2023, </span></span><a href="https://www.nature.com/articles/s41586-023-06160-y"><span class="No-Break"><span class="koboSpan" id="kobo.767.1">https://www.nature.com/articles/s41586-023-06160-y</span></span></a></li>
<li><span class="koboSpan" id="kobo.768.1">Singhal, </span><em class="italic"><span class="koboSpan" id="kobo.769.1">Towards </span></em><em class="italic"><span class="koboSpan" id="kobo.770.1">Exper-level</span></em><em class="italic"><span class="koboSpan" id="kobo.771.1"> Medical Question Answering with Large Language Models</span></em><span class="koboSpan" id="kobo.772.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.773.1">2023, </span></span><a href="https://arxiv.org/abs/2305.09617"><span class="No-Break"><span class="koboSpan" id="kobo.774.1">https://arxiv.org/abs/2305.09617</span></span></a></li>
<li><span class="koboSpan" id="kobo.775.1">Gao, </span><em class="italic"><span class="koboSpan" id="kobo.776.1">Empowering </span></em><em class="italic"><span class="koboSpan" id="kobo.777.1">Biomedical Discovery with AI Agents, </span></em><span class="No-Break"><span class="koboSpan" id="kobo.778.1">2024</span></span><span class="No-Break"><span class="koboSpan" id="kobo.779.1">, </span></span><a href="https://www.cell.com/cell/fulltext/S0092-8674(24)01070-5"><span class="No-Break"><span class="koboSpan" id="kobo.780.1">https://www.cell.com/cell/fulltext/S0092-8674(24)01070-5</span></span></a></li>
<li><span class="koboSpan" id="kobo.781.1">Gu, </span><em class="italic"><span class="koboSpan" id="kobo.782.1">A Survey on LLM-as-a-Judge</span></em><span class="koboSpan" id="kobo.783.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.784.1">2024, </span></span><a href="https://arxiv.org/abs/2411.15594"><span class="No-Break"><span class="koboSpan" id="kobo.785.1">https://arxiv.org/abs/2411.15594</span></span></a></li>
<li><span class="koboSpan" id="kobo.786.1">Ning, </span><em class="italic"><span class="koboSpan" id="kobo.787.1">A Survey of WebAgents: Towards Next-Generation AI Agents for Web Automation with Large Foundation Models</span></em><span class="koboSpan" id="kobo.788.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.789.1">2025, </span></span><a href="https://arxiv.org/abs/2503.23350"><span class="No-Break"><span class="koboSpan" id="kobo.790.1">https://arxiv.org/abs/2503.23350</span></span></a></li>
<li><span class="koboSpan" id="kobo.791.1">Xu, </span><em class="italic"><span class="koboSpan" id="kobo.792.1">A Survey on Robotics with Foundation Models: </span></em><em class="italic"><span class="koboSpan" id="kobo.793.1">Toward</span></em><em class="italic"><span class="koboSpan" id="kobo.794.1"> Embodied AI</span></em><span class="koboSpan" id="kobo.795.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.796.1">2024, </span></span><a href="https://arxiv.org/pdf/2402.02385"><span class="No-Break"><span class="koboSpan" id="kobo.797.1">https://arxiv.org/pdf/2402.02385</span></span></a></li>
<li><span class="koboSpan" id="kobo.798.1">Hu, </span><em class="italic"><span class="koboSpan" id="kobo.799.1">A Survey on Large Language Model Based Game Agents</span></em><span class="koboSpan" id="kobo.800.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.801.1">2025, </span></span><a href="https://arxiv.org/pdf/2404.02039"><span class="No-Break"><span class="koboSpan" id="kobo.802.1">https://arxiv.org/pdf/2404.02039</span></span></a></li>
<li><span class="koboSpan" id="kobo.803.1">Bousateouane, </span><em class="italic"><span class="koboSpan" id="kobo.804.1">Physical AI Agents: Integrating Cognitive Intelligence with Real-World Action</span></em><span class="koboSpan" id="kobo.805.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.806.1">2025, </span></span><a href="https://arxiv.org/pdf/2501.08944v1"><span class="No-Break"><span class="koboSpan" id="kobo.807.1">https://arxiv.org/pdf/2501.08944v1</span></span></a></li>
<li><span class="koboSpan" id="kobo.808.1">Zeng, </span><em class="italic"><span class="koboSpan" id="kobo.809.1">Large Language Models for Robotics: A Survey</span></em><span class="koboSpan" id="kobo.810.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.811.1">2023, </span></span><a href="https://arxiv.org/pdf/2311.07226"><span class="No-Break"><span class="koboSpan" id="kobo.812.1">https://arxiv.org/pdf/2311.07226</span></span></a></li>
<li><span class="koboSpan" id="kobo.813.1">Bansal, </span><em class="italic"><span class="koboSpan" id="kobo.814.1">Challenges in Human-Agent Communication</span></em><span class="koboSpan" id="kobo.815.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.816.1">2024, </span></span><a href="https://www.microsoft.com/en-us/research/uploads/prod/2024/12/HCAI_Agents.pdf"><span class="No-Break"><span class="koboSpan" id="kobo.817.1">https://www.microsoft.com/en-us/research/uploads/prod/2024/12/HCAI_Agents.pdf</span></span></a></li>
<li><span class="koboSpan" id="kobo.818.1">Raieli, </span><em class="italic"><span class="koboSpan" id="kobo.819.1">The Savant Syndrome: Is Pattern Recognition Equivalent to Intelligence?</span></em><span class="koboSpan" id="kobo.820.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.821.1">2024, </span></span><a href="https://medium.com/towards-data-science/the-savant-syndrome-is-pattern-recognition-equivalent-to-intelligence-242aab928152"><span class="No-Break"><span class="koboSpan" id="kobo.822.1">https://medium.com/towards-data-science/the-savant-syndrome-is-pattern-recognition-equivalent-to-intelligence-242aab928152</span></span></a></li>
<li><span class="koboSpan" id="kobo.823.1">Lu, </span><em class="italic"><span class="koboSpan" id="kobo.824.1">Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity</span></em><span class="koboSpan" id="kobo.825.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.826.1">2022, </span></span><a href="https://aclanthology.org/2022.acl-long.556/"><span class="No-Break"><span class="koboSpan" id="kobo.827.1">https://aclanthology.org/2022.acl-long.556/</span></span></a></li>
<li><span class="koboSpan" id="kobo.828.1">Zhao, </span><em class="italic"><span class="koboSpan" id="kobo.829.1">Calibrate Before Use: Improving Few-shot Performance of Language Models</span></em><span class="koboSpan" id="kobo.830.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.831.1">2021, </span></span><a href="https://proceedings.mlr.press/v139/zhao21c.html"><span class="No-Break"><span class="koboSpan" id="kobo.832.1">https://proceedings.mlr.press/v139/zhao21c.html</span></span></a></li>
<li><span class="koboSpan" id="kobo.833.1">Raieli, </span><em class="italic"><span class="koboSpan" id="kobo.834.1">Emergent Abilities in AI: Are We Chasing a Myth?</span></em><span class="koboSpan" id="kobo.835.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.836.1">2023, </span></span><a href="https://towardsdatascience.com/emergent-abilities-in-ai-are-we-chasing-a-myth-fead754a1bf9"><span class="No-Break"><span class="koboSpan" id="kobo.837.1">https://towardsdatascience.com/emergent-abilities-in-ai-are-we-chasing-a-myth-fead754a1bf9</span></span></a></li>
<li><span class="koboSpan" id="kobo.838.1">Raieli, </span><em class="italic"><span class="koboSpan" id="kobo.839.1">A Focus on Emergent Properties in Artificial Intelligence</span></em><span class="koboSpan" id="kobo.840.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.841.1">2025, </span></span><a href="https://github.com/SalvatoreRa/artificial-intelligence-articles/blob/main/articles/emergent_properties.md"><span class="No-Break"><span class="koboSpan" id="kobo.842.1">https://github.com/SalvatoreRa/artificial-intelligence-articles/blob/main/articles/emergent_properties.md</span></span></a></li>
<li><span class="koboSpan" id="kobo.843.1">Xu, </span><em class="italic"><span class="koboSpan" id="kobo.844.1">Towards Large Reasoning Models: A Survey of Reinforced Reasoning with Large Language Models</span></em><span class="koboSpan" id="kobo.845.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.846.1">2025, </span></span><a href="https://arxiv.org/abs/2501.09686v3"><span class="No-Break"><span class="koboSpan" id="kobo.847.1">https://arxiv.org/abs/2501.09686v3</span></span></a></li>
<li><span class="koboSpan" id="kobo.848.1">Sprague, </span><em class="italic"><span class="koboSpan" id="kobo.849.1">To CoT or Not to CoT? </span><span class="koboSpan" id="kobo.849.2">Chain-of-thought Helps Mainly on Math and Symbolic Reasoning</span></em><span class="koboSpan" id="kobo.850.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.851.1">2024, </span></span><a href="https://arxiv.org/pdf/2409.12183"><span class="No-Break"><span class="koboSpan" id="kobo.852.1">https://arxiv.org/pdf/2409.12183</span></span></a></li>
<li><span class="koboSpan" id="kobo.853.1">Raieli, </span><em class="italic"><span class="koboSpan" id="kobo.854.1">To CoT or Not to CoT: Do LLMs Really Need Chain-of-Thought?</span></em><span class="koboSpan" id="kobo.855.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.856.1">2024, </span></span><a href="https://levelup.gitconnected.com/to-cot-or-not-to-cot-do-llms-really-need-chain-of-thought-5a59698c90bb"><span class="No-Break"><span class="koboSpan" id="kobo.857.1">https://levelup.gitconnected.com/to-cot-or-not-to-cot-do-llms-really-need-chain-of-thought-5a59698c90bb</span></span></a></li>
<li><span class="koboSpan" id="kobo.858.1">Mirzadeh, </span><em class="italic"><span class="koboSpan" id="kobo.859.1">GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models</span></em><span class="koboSpan" id="kobo.860.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.861.1">2024, </span></span><a href="https://arxiv.org/pdf/2410.05229"><span class="No-Break"><span class="koboSpan" id="kobo.862.1">https://arxiv.org/pdf/2410.05229</span></span></a></li>
<li><span class="koboSpan" id="kobo.863.1">Sharkey, </span><em class="italic"><span class="koboSpan" id="kobo.864.1">Open Problems in Mechanistic Interpretability</span></em><span class="koboSpan" id="kobo.865.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.866.1">2025, </span></span><a href="https://arxiv.org/abs/2501.16496"><span class="No-Break"><span class="koboSpan" id="kobo.867.1">https://arxiv.org/abs/2501.16496</span></span></a></li>
<li><span class="koboSpan" id="kobo.868.1">Bereska, </span><em class="italic"><span class="koboSpan" id="kobo.869.1">Mechanistic Interpretability for AI Safety -- A Review</span></em><span class="koboSpan" id="kobo.870.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.871.1">2025, </span></span><a href="https://arxiv.org/abs/2404.14082"><span class="No-Break"><span class="koboSpan" id="kobo.872.1">https://arxiv.org/abs/2404.14082</span></span></a></li>
<li><span class="koboSpan" id="kobo.873.1">Cemri, </span><em class="italic"><span class="koboSpan" id="kobo.874.1">Why Do Multi-Agent LLM Systems Fail?</span></em><span class="koboSpan" id="kobo.875.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.876.1">2025, </span></span><a href="https://arxiv.org/pdf/2503.13657"><span class="No-Break"><span class="koboSpan" id="kobo.877.1">https://arxiv.org/pdf/2503.13657</span></span></a></li>
<li><span class="koboSpan" id="kobo.878.1">Raieli, </span><em class="italic"><span class="koboSpan" id="kobo.879.1">Creativity in LLMs: Optimizing for Diversity and Uniqueness</span></em><span class="koboSpan" id="kobo.880.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.881.1">2025, </span></span><a href="https://medium.com/data-science-collective/creativity-in-llms-optimizing-for-diversity-and-uniqueness-f5c7208f4d99"><span class="No-Break"><span class="koboSpan" id="kobo.882.1">https://medium.com/data-science-collective/creativity-in-llms-optimizing-for-diversity-and-uniqueness-f5c7208f4d99</span></span></a></li>
<li><span class="koboSpan" id="kobo.883.1">Boden, </span><em class="italic"><span class="koboSpan" id="kobo.884.1">The Creative Mind</span></em><span class="koboSpan" id="kobo.885.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.886.1">2003, </span></span><a href="https://www.routledge.com/The-Creative-Mind-Myths-and-Mechanisms/Boden/p/book/9780415314534"><span class="No-Break"><span class="koboSpan" id="kobo.887.1">https://www.routledge.com/The-Creative-Mind-Myths-and-Mechanisms/Boden/p/book/9780415314534</span></span></a></li>
<li><span class="koboSpan" id="kobo.888.1">Peeperkorn, </span><em class="italic"><span class="koboSpan" id="kobo.889.1">Is Temperature the Creativity Parameter of Large Language Models?</span></em><span class="koboSpan" id="kobo.890.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.891.1">2024, </span></span><a href="https://arxiv.org/abs/2405.00492"><span class="No-Break"><span class="koboSpan" id="kobo.892.1">https://arxiv.org/abs/2405.00492</span></span></a></li>
<li><span class="koboSpan" id="kobo.893.1">Benedek, </span><em class="italic"><span class="koboSpan" id="kobo.894.1">To Create or to Recall? </span><span class="koboSpan" id="kobo.894.2">Neural Mechanisms Underlying the Generation of Creative New Ideas</span></em><span class="koboSpan" id="kobo.895.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.896.1">2014, </span></span><a href="https://www.sciencedirect.com/science/article/pii/S1053811913011130"><span class="No-Break"><span class="koboSpan" id="kobo.897.1">https://www.sciencedirect.com/science/article/pii/S1053811913011130</span></span></a></li>
<li><span class="koboSpan" id="kobo.898.1">Raieli, </span><em class="italic"><span class="koboSpan" id="kobo.899.1">You’re Not a Writer, ChatGPT — But You Sound Like One</span></em><span class="koboSpan" id="kobo.900.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.901.1">2024, </span></span><a href="https://levelup.gitconnected.com/youre-not-a-writer-chatgpt-but-you-sound-like-one-75fa329ac3a9"><span class="No-Break"><span class="koboSpan" id="kobo.902.1">https://levelup.gitconnected.com/youre-not-a-writer-chatgpt-but-you-sound-like-one-75fa329ac3a9</span></span></a></li>
<li><span class="koboSpan" id="kobo.903.1">Raieli, </span><em class="italic"><span class="koboSpan" id="kobo.904.1">How Far Is AI from Human Intelligence?</span></em><span class="koboSpan" id="kobo.905.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.906.1">2025, </span></span><a href="https://levelup.gitconnected.com/how-far-is-ai-from-human-intelligence-6ab4b2a5ce1c"><span class="No-Break"><span class="koboSpan" id="kobo.907.1">https://levelup.gitconnected.com/how-far-is-ai-from-human-intelligence-6ab4b2a5ce1c</span></span></a></li>
<li><span class="koboSpan" id="kobo.908.1">Villalobos, </span><em class="italic"><span class="koboSpan" id="kobo.909.1">Will We Run Out of Data? </span><span class="koboSpan" id="kobo.909.2">Limits of LLM Scaling Based on Human-generated Data</span></em><span class="koboSpan" id="kobo.910.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.911.1">2022, </span></span><a href="https://arxiv.org/abs/2211.04325"><span class="No-Break"><span class="koboSpan" id="kobo.912.1">https://arxiv.org/abs/2211.04325</span></span></a></li>
<li><span class="koboSpan" id="kobo.913.1">Feng, </span><em class="italic"><span class="koboSpan" id="kobo.914.1">How Far Are We From AGI: Are LLMs All We Need?</span></em><span class="koboSpan" id="kobo.915.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.916.1">2024, </span></span><a href="https://arxiv.org/abs/2405.10313"><span class="No-Break"><span class="koboSpan" id="kobo.917.1">https://arxiv.org/abs/2405.10313</span></span></a></li>
<li><span class="koboSpan" id="kobo.918.1">Karvonen, </span><em class="italic"><span class="koboSpan" id="kobo.919.1">Emergent World Models and Latent Variable Estimation in Chess-Playing Language Models</span></em><span class="koboSpan" id="kobo.920.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.921.1">2024, </span></span><a href="https://arxiv.org/pdf/2403.15498v2"><span class="No-Break"><span class="koboSpan" id="kobo.922.1">https://arxiv.org/pdf/2403.15498v2</span></span></a></li>
<li><span class="koboSpan" id="kobo.923.1">Li, </span><em class="italic"><span class="koboSpan" id="kobo.924.1">Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task</span></em><span class="koboSpan" id="kobo.925.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.926.1">2022, </span></span><a href="https://arxiv.org/abs/2210.13382"><span class="No-Break"><span class="koboSpan" id="kobo.927.1">https://arxiv.org/abs/2210.13382</span></span></a></li>
<li><span class="koboSpan" id="kobo.928.1">Bowman, </span><em class="italic"><span class="koboSpan" id="kobo.929.1">Eight Things to Know about Large Language Models</span></em><span class="koboSpan" id="kobo.930.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.931.1">2023, </span></span><a href="https://arxiv.org/pdf/2304.00612"><span class="No-Break"><span class="koboSpan" id="kobo.932.1">https://arxiv.org/pdf/2304.00612</span></span></a></li>
<li><span class="koboSpan" id="kobo.933.1">Shumailov, </span><em class="italic"><span class="koboSpan" id="kobo.934.1">AI Models Collapse When Trained on Recursively Generated Data</span></em><span class="koboSpan" id="kobo.935.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.936.1">2024, </span></span><a href="https://www.nature.com/articles/s41586-024-07566-y"><span class="No-Break"><span class="koboSpan" id="kobo.937.1">https://www.nature.com/articles/s41586-024-07566-y</span></span></a></li>
<li><span class="koboSpan" id="kobo.938.1">LessWrong, </span><em class="italic"><span class="koboSpan" id="kobo.939.1">Embodiment is Indispensable for AGI</span></em><span class="koboSpan" id="kobo.940.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.941.1">2022, </span></span><a href="https://www.lesswrong.com/posts/vBBxKBWn4zRXwivxC/embodiment-is-indispensable-for-agi"><span class="No-Break"><span class="koboSpan" id="kobo.942.1">https://www.lesswrong.com/posts/vBBxKBWn4zRXwivxC/embodiment-is-indispensable-for-agi</span></span></a></li>
<li><span class="koboSpan" id="kobo.943.1">Tan, </span><em class="italic"><span class="koboSpan" id="kobo.944.1">The Path to AGI Goes through Embodiment</span></em><span class="koboSpan" id="kobo.945.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.946.1">2023, </span></span><a href="https://ojs.aaai.org/index.php/AAAI-SS/article/view/27485"><span class="No-Break"><span class="koboSpan" id="kobo.947.1">https://ojs.aaai.org/index.php/AAAI-SS/article/view/27485</span></span></a></li>
<li><span class="koboSpan" id="kobo.948.1">Mitchell, </span><em class="italic"><span class="koboSpan" id="kobo.949.1">Fully Autonomous AI Agents Should Not be Developed</span></em><span class="koboSpan" id="kobo.950.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.951.1">2025, </span></span><a href="https://arxiv.org/pdf/2502.02649"><span class="No-Break"><span class="koboSpan" id="kobo.952.1">https://arxiv.org/pdf/2502.02649</span></span></a></li>
<li><span class="koboSpan" id="kobo.953.1">Diamond, </span><em class="italic"><span class="koboSpan" id="kobo.954.1">On the Ethical Considerations of Generative Agents</span></em><span class="koboSpan" id="kobo.955.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.956.1">2024, </span></span><a href="https://arxiv.org/abs/2411.19211"><span class="No-Break"><span class="koboSpan" id="kobo.957.1">https://arxiv.org/abs/2411.19211</span></span></a></li>
<li><span class="koboSpan" id="kobo.958.1">Siqueira de Cerqueira, </span><em class="italic"><span class="koboSpan" id="kobo.959.1">Can We Trust AI Agents? </span><span class="koboSpan" id="kobo.959.2">An Experimental Study Towards Trustworthy LLM-Based Multi-Agent Systems for AI Ethics</span></em><span class="koboSpan" id="kobo.960.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.961.1">2024, </span></span><a href="https://arxiv.org/abs/2411.08881"><span class="No-Break"><span class="koboSpan" id="kobo.962.1">https://arxiv.org/abs/2411.08881</span></span></a></li>
<li><span class="koboSpan" id="kobo.963.1">Chaffer, </span><em class="italic"><span class="koboSpan" id="kobo.964.1">Decentralized Governance of Autonomous AI Agents</span></em><span class="koboSpan" id="kobo.965.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.966.1">2024, </span></span><a href="https://arxiv.org/abs/2412.17114v3"><span class="No-Break"><span class="koboSpan" id="kobo.967.1">https://arxiv.org/abs/2412.17114v3</span></span></a></li>
<li><span class="koboSpan" id="kobo.968.1">Gabriel, </span><em class="italic"><span class="koboSpan" id="kobo.969.1">The Ethics of Advanced AI Assistants</span></em><span class="koboSpan" id="kobo.970.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.971.1">2024, </span></span><a href="https://arxiv.org/pdf/2404.16244"><span class="No-Break"><span class="koboSpan" id="kobo.972.1">https://arxiv.org/pdf/2404.16244</span></span></a></li>
<li><span class="koboSpan" id="kobo.973.1">Chen, </span><em class="italic"><span class="koboSpan" id="kobo.974.1">Combating Misinformation in the Age of LLMs: Opportunities and Challenges</span></em><span class="koboSpan" id="kobo.975.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.976.1">2023, </span></span><a href="https://arxiv.org/abs/2311.05656"><span class="No-Break"><span class="koboSpan" id="kobo.977.1">https://arxiv.org/abs/2311.05656</span></span></a></li>
<li><span class="koboSpan" id="kobo.978.1">Luccioni, </span><em class="italic"><span class="koboSpan" id="kobo.979.1">Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning</span></em><span class="koboSpan" id="kobo.980.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.981.1">2023, </span></span><a href="https://arxiv.org/abs/2302.08476"><span class="No-Break"><span class="koboSpan" id="kobo.982.1">https://arxiv.org/abs/2302.08476</span></span></a></li>
</ul>
</div>
</body></html>