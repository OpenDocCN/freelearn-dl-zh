<html><head></head><body>
  <div><h1 class="chapterNumber">11</h1>
    <h1 id="_idParaDest-262" class="chapterTitle">MLOps and LLMOps</h1>
    <p class="normal">Throughout<a id="_idIndexMarker1004"/> the book, we’ve already used <strong class="keyWord">machine learning operations</strong> (<strong class="keyWord">MLOps</strong>) components and principles such as a model registry to share and version our fined-tuned <strong class="keyWord">large language models</strong> (<strong class="keyWord">LLMs</strong>), a <a id="_idIndexMarker1005"/>logical feature store for our fine-tuning and RAG data, and an orchestrator to glue all our ML pipelines together. But MLOps is not just about these components; it takes an ML application to the next level by automating data collection, training, testing, and deployment. Thus, the end goal of MLOps is to automate as much as possible and let users focus on the most critical decisions, such as when a change in distribution is detected and a decision must be taken on whether it is <a id="_idIndexMarker1006"/>essential to retrain the model or not. But what about <strong class="keyWord">LLM operations</strong> (<strong class="keyWord">LLMOps</strong>)? How does it differ from MLOps?</p>
    <p class="normal">The term <em class="italic">LLMOps</em> is a product<a id="_idIndexMarker1007"/> of the widespread <a id="_idIndexMarker1008"/>adoption of LLMs. It is built on top of MLOps, which is built on top of <strong class="keyWord">development operations</strong> (<strong class="keyWord">DevOps</strong>). Thus, to fully understand what LLMOps is about, we must provide a historical context, starting with DevOps and building on the term from there—which is precisely what this chapter will do. At its core, LLMOps focuses on problems specific to LLMs, such as prompt monitoring and versioning, input and output guardrails to prevent toxic behavior, and feedback loops to gather fine-tuning data. It also focuses on scaling issues that appear when working with LLMs, such as collecting trillions of tokens for training datasets, training models on massive GPU clusters, and reducing infrastructure costs. Fortunately for the common folk, these issues are solved mainly by a few companies that fine-tune foundational models, such as Meta, which provides the Llama family of models. Most companies will adopt these pre-trained foundational models for their use cases, focusing on LLMOps problems such as prompt monitoring and versioning.</p>
    <p class="normal">On the implementation side of things, to add LLMOps to our LLM Twin use case, we will deploy all our ZenML pipelines to AWS. We will implement a <strong class="keyWord">continuous integration and continuous deployment</strong> (<strong class="keyWord">CI/CD</strong>) pipeline<a id="_idIndexMarker1009"/> to test the integrity of our code and automate the deployment process, a <strong class="keyWord">continuous training</strong> (<strong class="keyWord">CT</strong>) pipeline<a id="_idIndexMarker1010"/> to automate our training, and a monitoring pipeline to track all our prompts and generated answers. This is a natural progression in any ML project, regardless of whether you use LLMs.</p>
    <p class="normal">In previous chapters, you learned how to build an LLM application. Now, it’s time to explore three main goals related to LLMOps. The first one is to gain a theoretical understanding of LLMOps, starting with DevOps, then moving to the fundamental principles of MLOps, and finally, digging into LLMOps. We don’t aim to provide the whole theory on DevOps, MLOps, and LLMOps, as you could easily write an entire book on these topics. However, we want to build a strong understanding of why we make certain decisions when implementing the LLM Twin use case.</p>
    <p class="normal">Our second goal is to deploy the ZenML pipelines to AWS (currently, we’ve deployed only our inference pipeline to AWS in <em class="italic">Chapter 10</em>). This section will be hands-on, showing you how to leverage ZenML to deploy everything to AWS. We need this to implement our third and last goal, which is to apply what we’ve learned in the theory section to our LLM Twin use case. We will implement a CI/CD pipeline using GitHub Actions, a CT and alerting pipeline using ZenML, and a monitoring pipeline using Opik from Comet ML.</p>
    <p class="normal">Thus, in this chapter, we will cover the following topics:</p>
    <ul>
      <li class="bulletList">The path to LLMOps: Understanding its roots in DevOps and MLOps</li>
      <li class="bulletList">Deploying the LLM Twin’s pipelines to the cloud</li>
      <li class="bulletList">Adding LLMOps to the LLM Twin</li>
    </ul>
    <h1 id="_idParaDest-263" class="heading-1">The path to LLMOps: Understanding its roots in DevOps and MLOps</h1>
    <p class="normal">To understand<a id="_idIndexMarker1011"/> LLMOps, we have to start with the field’s beginning, which is DevOps, as<a id="_idIndexMarker1012"/> it inherits most of its fundamental principles from there. Then, we will move to MLOps<a id="_idIndexMarker1013"/> to understand how the DevOps domain was adapted to support ML systems. Finally, we will explain what LLMOps is and how it emerged from MLOps after the widespread adoption of LLMs.</p>
    <h2 id="_idParaDest-264" class="heading-2">DevOps</h2>
    <p class="normal">Manually shipping software is <a id="_idIndexMarker1014"/>time-consuming, error-prone, involves security risks, and doesn’t scale. Thus, DevOps was born to automate the process of shipping software at scale. More specifically, DevOps is used in software development, where you want to completely automate your building, testing, deploying, and monitoring components. It is a methodology designed to shorten the development lifecycle and ensure continuous delivery of high-quality software. It encourages collaboration, automates processes, integrates workflows, and implements rapid feedback loops. These elements contribute to a culture where building, testing, and releasing software becomes more reliable and faster.</p>
    <p class="normal">Embracing a DevOps culture offers significant advantages to an organization, primarily boosting operational efficiency, speeding up feature delivery, and enhancing product quality. Some of the main benefits include:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Improved collaboration:</strong> DevOps is <a id="_idIndexMarker1015"/>pivotal in creating a more unified working environment. Eliminating the barriers between development and operations teams fosters enhanced communication and teamwork, leading to a more efficient and productive workplace.</li>
      <li class="bulletList"><strong class="keyWord">Boosted efficiency:</strong> Automating the software development lifecycle reduces manual tasks, errors, and delivery times.</li>
      <li class="bulletList"><strong class="keyWord">Ongoing improvement:</strong> DevOps is not just about internal processes. It’s about ensuring that the software effectively meets user needs. Promoting a culture of continuous feedback enables teams to quickly adapt and enhance their processes, thereby delivering software that genuinely satisfies the end users.</li>
      <li class="bulletList"><strong class="keyWord">Superior quality and security:</strong> DevOps ensures swift software development while maintaining high quality and security standards through CI/CD and<a id="_idIndexMarker1016"/> proactive security measures.</li>
    </ul>
    <h3 id="_idParaDest-265" class="heading-3">The DevOps lifecycle</h3>
    <p class="normal">As illustrated in <em class="italic">Figure 11.1</em>, the DevOps lifecycle encompasses the entire journey from the inception of software development to its delivery, upkeep, and security. The key stages of this lifecycle are:</p>
    <ol>
      <li class="numberedList" value="1"><strong class="keyWord">Plan:</strong> Organize<a id="_idIndexMarker1017"/> and prioritize the tasks, ensuring each is tracked to completion.</li>
      <li class="numberedList"><strong class="keyWord">Code:</strong> Collaborate <a id="_idIndexMarker1018"/>with your team to write, design, develop, and securely manage code and project data.</li>
      <li class="numberedList"><strong class="keyWord">Build:</strong> Package <a id="_idIndexMarker1019"/>your applications and dependencies into an executable format.</li>
      <li class="numberedList"><strong class="keyWord">Test:</strong> This stage<a id="_idIndexMarker1020"/> is crucial. It’s where you confirm that your code functions correctly and meets quality standards, ideally through automated testing.</li>
      <li class="numberedList"><strong class="keyWord">Release:</strong> If the tests <a id="_idIndexMarker1021"/>pass, flag the tested build as a new release, which is now ready to be shipped.</li>
      <li class="numberedList"><strong class="keyWord">Deploy:</strong> Deploy <a id="_idIndexMarker1022"/>the latest release to the end users.</li>
      <li class="numberedList"><strong class="keyWord">Operate</strong>: Manage and <a id="_idIndexMarker1023"/>maintain the infrastructure on which the software runs effectively once it is live. This involves scaling, security, data management, and backup and recovery.</li>
      <li class="numberedList"><strong class="keyWord">Monitor:</strong> Track performance <a id="_idIndexMarker1024"/>metrics and errors to reduce the severity and frequency of incidents.</li>
    </ol>
    <figure class="mediaobject"><img src="img/B31105_11_01.png" alt="A blue and green arrows with text  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 11.1: DevOps lifecycle steps</p>
    <h3 id="_idParaDest-266" class="heading-3">The core DevOps concepts</h3>
    <p class="normal">DevOps encompasses various practices throughout the application lifecycle, but the core ones that we will touch on throughout this book are:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Deployment environments</strong>: To <a id="_idIndexMarker1025"/>thoroughly test your code before shipping it to production, you must define multiple pre-production environments that mimic the production one. The most common approach is to create a dev environment where the developers can test their latest features. Then, you have a staging environment where the QA team and stakeholders tinker with the application to find bugs and experience the latest features before they ship to the users. Lastly, we have the <a id="_idIndexMarker1026"/>production environment, which is exposed to end users.</li>
      <li class="bulletList"><strong class="keyWord">Version control:</strong> Used to track, manage, and version every change made to the source <a id="_idIndexMarker1027"/>code. This allows you to have complete control over the evolution of the code and deployment processes. For example, without versioning, tracking changes between the dev, staging, and production environments would be impossible. By versioning your software, you always know what version is stable and ready to be shipped.</li>
      <li class="bulletList"><strong class="keyWord">Continuous integration (CI):</strong> Before pushing <a id="_idIndexMarker1028"/>the code into the dev, staging, and production main branches, you automatically build your application and run automated tests on each change. After all the automated tests pass, the feature branch can be merged into the main one.</li>
      <li class="bulletList"><strong class="keyWord">Continuous delivery (CD):</strong> Continuous <a id="_idIndexMarker1029"/>delivery works in conjunction with CI and automates the infrastructure provisioning and application deployment steps. For example, after the code is merged into the staging environment, the application with the latest changes will be automatically deployed on top of your staging infrastructure. After, the QA team (or stakeholders) starts manually testing the latest features to verify that they work as expected. These two steps are commonly referred to together as CI/CD.</li>
    </ul>
    <p class="normal">Note that DevOps suggests a set of core principles that are platform/tool agnostic. However, within our LLM Twin use case, we will add a version control layer using GitHub, which <a id="_idIndexMarker1030"/>aims to track the evolution of the code. Another popular tool for version control is <a id="_idIndexMarker1031"/>GitLab. To implement the CI/CD pipeline, we will leverage the GitHub ecosystem<a id="_idIndexMarker1032"/> and GitHub Actions, which <a id="_idIndexMarker1033"/>are free for open-source projects. Other tool<a id="_idIndexMarker1034"/> choices are GitLab CI/CD, CircleCI, and Jenkins. Usually, you<a id="_idIndexMarker1035"/> pick the DevOps tool based on your development <a id="_idIndexMarker1036"/>environment, customization, and privacy needs. For example, Jenkins is an open-source DevOps tool you can host yourself and control fully. The downside is that you must host and maintain it yourself, adding a complexity layer. Thus, many companies choose what works best with their version control ecosystem, such as GitHub Actions or GitLab CI/CD.</p>
    <p class="normal">Now that we’ve established a solid understanding of DevOps, let’s explore how the MLOps field has emerged to keep these same core principles in the AI/ML world.</p>
    <h2 id="_idParaDest-267" class="heading-2">MLOps</h2>
    <p class="normal">As you might have worked<a id="_idIndexMarker1037"/> out by now, MLOps tries to apply the DevOps principles to ML. The core issue is that an ML application has many other moving parts compared to a standard software application, such as the data, model, and, finally, the code. MLOps aims to track, operationalize, and monitor all these concepts for better reproducibility, robustness, and control.</p>
    <p class="normal">In ML systems, a build can be triggered by any change in these areas—whether it’s an update in the code, modifications in the data, or adjustments to the model.</p>
    <figure class="mediaobject"><img src="img/B31105_11_02.png" alt=""/></figure>
    <p class="packt_figref">Figure 11.2: Relationship between data, model, and code changes</p>
    <p class="normal">In DevOps, everything is centered around the code. For example, when a new feature is added to the codebase, you have to trigger the CI/CD pipeline. In MLOps, the code can remain <a id="_idIndexMarker1038"/>unchanged while only the data changes. In that case, you must train (or fine-tune) a new model, resulting in a new dataset and model version. Intuitively, when one component changes, it affects one or more of the others. Thus, MLOps has to take into consideration all this extra complexity. Here are a few examples that can trigger a change in the data and indirectly in the model:</p>
    <ul>
      <li class="bulletList">After deploying the ML model, its performance might decay as time passes, so we need new data to retrain it.</li>
      <li class="bulletList">After understanding how to collect data in the real world, we might recognize that getting the data for our problem is challenging, so we need to re-formulate it to work with our real-world setup.</li>
      <li class="bulletList">While in the experimentation stage and training the model, we often must collect more data or re-label it, which generates a new set of models.</li>
      <li class="bulletList">After serving the model in the production environment and collecting feedback from the end users, we might recognize that the assumptions we made for training the model are wrong, so we must change our model.</li>
    </ul>
    <p class="normal">So, what is MLOps?</p>
    <p class="normal">A more official definition of<a id="_idIndexMarker1039"/> MLOps is the following: MLOps is the extension of the DevOps field that makes data and models their first-class citizen while preserving the DevOps methodology.</p>
    <p class="normal">Like DevOps, MLOps originates from the idea that isolating ML model development from its deployment process (ML operations) diminishes the system’s overall quality, transparency, and agility. With that in mind, an optimal MLOps experience treats ML assets consistently as other software assets within a CI/CD environment as part of a cohesive release process.</p>
    <h3 id="_idParaDest-268" class="heading-3">MLOps core components</h3>
    <p class="normal">We have already used all of these components throughout the book, but let’s have a quick refresher on the MLOps core components now that we better understand the field. Along with source control and CI/CD, MLOps revolves around:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Model registry:</strong> A<a id="_idIndexMarker1040"/> centralized repository for storing trained ML models (<strong class="screenText">tools:</strong> <strong class="screenText">Comet ML</strong>, <strong class="screenText">W&amp;B</strong>, <strong class="screenText">MLflow</strong>, <strong class="screenText">ZenML</strong>)</li>
      <li class="bulletList"><strong class="keyWord">Feature store:</strong> Preprocessing and storing input data as features for both model <a id="_idIndexMarker1041"/>training and inference pipelines (<strong class="screenText">tools:</strong> <strong class="screenText">Hopsworks</strong>, <strong class="screenText">Tecton</strong>, <strong class="screenText">Featureform</strong>)</li>
      <li class="bulletList"><strong class="keyWord">ML metadata store:</strong> This store tracks information related to model training, such as model <a id="_idIndexMarker1042"/>configurations, training data, testing data, and performance metrics. It is mainly used to compare multiple models and look at the model lineages to understand how they were created (<strong class="screenText">tools:</strong> <strong class="screenText">Comet ML</strong>, <strong class="screenText">W&amp;B</strong>, <strong class="screenText">MLflow</strong>)</li>
      <li class="bulletList"><strong class="keyWord">ML pipeline orchestrator:</strong> Automating<a id="_idIndexMarker1043"/> the sequence of steps in ML projects (<strong class="screenText">tools:</strong> <strong class="screenText">ZenML</strong>, <strong class="screenText">Airflow</strong>, <strong class="screenText">Prefect</strong>, <strong class="screenText">Dagster</strong>)</li>
    </ul>
    <p class="normal">You might have noticed an overlap between the MLOps components and its specific tooling. This is common, as most MLOps tools offer unified solutions, often called MLOps platforms.</p>
    <h3 id="_idParaDest-269" class="heading-3">MLOps principles</h3>
    <p class="normal">Six core principles guide the MLOps field. These are independent of any tool and sit at the core of building robust and scalable ML systems. </p>
    <p class="normal">They are:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Automation or operationalization</strong>: Automation in MLOps involves transitioning from <a id="_idIndexMarker1044"/>manual processes to automated pipelines through CT and CI/CD. This enables the efficient retraining and deployment of ML models in response to triggers such as new <a id="_idIndexMarker1045"/>data, performance drops, or unhandled edge cases. Moving from manual experimentation to full automation ensures that our ML systems are robust, scalable, and adaptable to changing requirements without errors or delays.</li>
      <li class="bulletList"><strong class="keyWord">Versioning</strong>: In MLOps, it is crucial to track changes in code, models, and data individually, ensuring<a id="_idIndexMarker1046"/> consistency and reproducibility. Code is tracked using tools like Git, models are versioned through model registries, and data versioning can be managed using solutions like DVC or artifact management systems.</li>
      <li class="bulletList"><strong class="keyWord">Experiment tracking:</strong> As training ML models is an iterative and experimental <a id="_idIndexMarker1047"/>process that involves comparing multiple experiments based on predefined metrics, using an experiment tracker to help us pick the best model is important. Tools like Comet ML, W&amp;B, MLflow, and Neptune allow us to log all necessary information to compare experiments easily and select the best model for production.</li>
      <li class="bulletList"><strong class="keyWord">Testing</strong>: MLOps suggests that along with testing<a id="_idIndexMarker1048"/> your code, you should also test your data and models through unit, integration, acceptance, regression, and stress tests. This ensures that each component functions correctly and integrates well, focusing on inputs, outputs, and handling edge cases.</li>
      <li class="bulletList"><strong class="keyWord">Monitoring</strong>: This stage<a id="_idIndexMarker1049"/> is vital for detecting performance degradation in served ML models due to changes in production data, allowing timely intervention such as retraining, further prompt or feature engineering, or data validation. By tracking logs, system metrics, and model metrics and detecting drifts, we can maintain the health of ML systems in production, detect issues as fast as possible, and ensure they continue to deliver accurate results.</li>
      <li class="bulletList"><strong class="keyWord">Reproducibility</strong>: This <a id="_idIndexMarker1050"/>ensures that every process (such as training or feature engineering) within your ML systems produces identical results when given the same input by tracking all the moving variables, such as code versions, data versions, hyperparameters, or any other type of configurations. Due to the non-deterministic nature of ML training and inference, setting well-known seeds when generating pseudo-random numbers is essential to achieving consistent outcomes and making processes as deterministic as<a id="_idIndexMarker1051"/> possible.</li>
    </ul>
    <p class="normal">If you want to learn more, we’ve offered an in-depth exploration of these principles in the <em class="italic">Appendix</em> at the end of this book.</p>
    <h3 id="_idParaDest-270" class="heading-3">ML vs. MLOps engineering</h3>
    <p class="normal">There is a fine line between<a id="_idIndexMarker1052"/> ML engineering and MLOps. If we want to define a rigid job description for the two rules, it cannot be easy to completely differentiate what responsibilities go into <strong class="keyWord">ML engineering</strong> (<strong class="keyWord">MLE</strong>) and what goes into <a id="_idIndexMarker1053"/>MLOps. I have seen many job roles that bucket the MLOps role with the platform and cloud engineers. From one perspective, that makes a lot of sense: as an MLOps engineer, you have a lot of work to do on the infrastructure side. On the other hand, as seen in this section, an MLOps engineer still has to implement things such as experiment tracking, model registries, versioning, and more. A good strategy would be to let the ML engineer integrate these into the code and the MLOps engineer focus on making them work on their infrastructure.</p>
    <p class="normal">At a big corporation, ultimately, differentiating the two roles might make sense. But when working in small to medium-sized teams, you will wear multiple hats and probably work on the ML system’s MLE and MLOps aspects.</p>
    <figure class="mediaobject"><img src="img/B31105_11_03.png" alt=""/></figure>
    <p class="packt_figref">Figure 11.3: DS vs. MLE vs. MLOps</p>
    <p class="normal">For instance, in <em class="italic">Figure 11.3</em>, we see a clear division of responsibilities among the three key roles: data scientist/ML researcher, ML engineer, and MLOps engineer. The <strong class="keyWord">Data Scientist</strong> (<strong class="keyWord">DS</strong>) implements<a id="_idIndexMarker1054"/> specific models to address problems. </p>
    <p class="normal">The ML engineer takes the functional models from the DS team and constructs a layer on<a id="_idIndexMarker1055"/> top of them, making them modular and extendable and providing access to a <strong class="keyWord">database</strong> (<strong class="keyWord">DB</strong>) or exposing <a id="_idIndexMarker1056"/>them as an <a id="_idIndexMarker1057"/>API over the internet. However, the MLOps engineer plays a pivotal role in this process. They take the code from this intermediate layer and place it on a more generic layer, the infrastructure. This action marks the application’s transition to production. From this point, we can start thinking about automation, monitoring, versioning, and more.</p>
    <p class="normal">The intermediate layer differentiates a proof of concept from an actual product. In that layer, you design an extendable application that has a state by integrating a DB and is accessible over the internet through an API. When shipping the application on a specific infrastructure, you must consider scalability, latency, and cost-effectiveness. Of course, the intermediate and generic layers depend on each other, and often, you must reiterate to meet the application requirements.</p>
    <h2 id="_idParaDest-271" class="heading-2">LLMOps</h2>
    <p class="normal">LLMOps <a id="_idIndexMarker1058"/>encompasses the practices and processes essential for managing and running LLMs. This field is a specialized branch of MLOps, concentrating on the unique challenges and demands associated with LLMs. While MLOps addresses the principles and practices of managing various ML models, LLMOps focuses on the distinct aspects of LLMs, including their large size, highly complex training requirements, prompt management, and non-deterministic nature of generating answers. However, note that at its core, LLMOps still inherits all the fundamentals presented in the MLOps section. Thus, here, we will focus on what it adds on top.</p>
    <p class="normal">When training LLMs from scratch, the data and model dimensions of an ML system grow substantially, which is one aspect that sets LLMOps apart from MLOps. These are the main <a id="_idIndexMarker1059"/>concerns when training LLMs from scratch:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Data collection and preparation</strong> involves collecting, preparing, and managing the massive datasets required for training LLMs. It involves big data techniques for processing, storing, and sharing training datasets. For example, GPT-4 was trained on roughly 13 trillion tokens, equal to approximately 10 trillion words.</li>
      <li class="bulletList">Managing <strong class="keyWord">LLMs’</strong> <strong class="keyWord">considerable number of parameters</strong> is a significant technical challenge from the infrastructure’s point of view. It requires vast computation resources, usually clusters of machines powered by Nvidia GPUs with CUDA support.</li>
      <li class="bulletList">The massive size of LLMs directly impacts <strong class="keyWord">model training</strong>. When training an LLM from scratch, you can’t fit it on a single GPU due to the model’s size or the higher batch size you require for the expected results. Thus, you need multi-GPU training, which involves optimizing your processes and infrastructure to support data, model, or tensor parallelism.</li>
      <li class="bulletList">Managing massive datasets and multi-GPU clusters involves substantial <strong class="keyWord">costs</strong>. For example, the estimated training cost for GPT-4 is around $100 million, as stated by Sam Altman, the CEO of OpenAI (<a href="https://en.wikipedia.org/wiki/GPT-4#Training">https://en.wikipedia.org/wiki/GPT-4#Training</a>). Add to that the costs of multiple experiments, evaluation, and inference. Even if these numbers are not exact, as the sources are not 100% reliable, the scale of the costs of training an LLM is trustworthy, which implies that only the <a id="_idIndexMarker1060"/>large players in the industry can afford to train LLMs from scratch.</li>
    </ul>
    <p class="normal">At its core, LLMOps<a id="_idIndexMarker1061"/> is MLOps at scale. It uses the same MLOps principles but is applied to big data and huge models that require more computing power to train and run. However, due to its huge scale, the most significant trend is the shift away from training neural networks from scratch for specific tasks. This approach is becoming obsolete with the rise of fine-tuning, especially with the advent of foundation models such as <a id="_idIndexMarker1062"/>GPT. A few organizations with<a id="_idIndexMarker1063"/> extensive computational resources, such as OpenAI and Google, develop these foundation models. Thus, most applications now rely on the lightweight fine-tuning of parts of these models, prompt engineering, or optionally distilling data or models into smaller, specialized inference networks.</p>
    <p class="normal">Thus, for most LLM applications out there, your development steps will involve the selection of a foundation model, which you further have to optimize by using prompt engineering, fine-tuning, or RAG. Thus, the operational aspect of these three steps is the most critical to understand. Let’s dive into some popular components of LLMOps that can improve prompt engineering, fine-tuning, and RAG.</p>
    <h3 id="_idParaDest-272" class="heading-3">Human feedback</h3>
    <p class="normal">One valuable refinement<a id="_idIndexMarker1064"/> step of your LLM is aligning it with your audience’s preferences. You must introduce a feedback loop within your application and gather a human feedback dataset to<a id="_idIndexMarker1065"/> further fine-tune the LLM with techniques such as <strong class="keyWord">Reinforcement Learning with Human Feedback</strong> (<strong class="keyWord">RLHF</strong>) or more advanced ones such as <strong class="keyWord">Direct Preference Optimization</strong> (<strong class="keyWord">DPO</strong>). One popular feedback loop is the thumbs-up/thumbs-down button <a id="_idIndexMarker1066"/>present in most chatbot interfaces. You can read more on preference alignment in <em class="italic">Chapter 6</em>.</p>
    <h3 id="_idParaDest-273" class="heading-3">Guardrails</h3>
    <p class="normal">Unfortunately, LLM systems<a id="_idIndexMarker1067"/> are not reliable, as they often hallucinate. You can optimize <a id="_idIndexMarker1068"/>your system against hallucinations, but as hallucinations are hard to detect and can take many forms, there are significant changes that will still happen in the future. </p>
    <p class="normal">Most users have accepted this phenomenon, but what is not acceptable is when LLMs accidentally output sensitive information, such as GitHub Copilot outputting AWS secret keys or other chatbots providing people’s passwords. This can also happen with people’s phone numbers, addresses, email addresses, and more. Ideally, you should remove all this sensitive data from your training data so the LLM doesn’t memorize it, but that doesn’t always happen.</p>
    <p class="normal">LLMs are well known for producing toxic and harmful outputs, such as sexist and racist outputs. For example, during an experiment on ChatGPT around April 2023, people found how to hijack the system by forcing the chatbot to adopt a negative persona, such as “a bad person” or “a horrible person.” It worked even by forcing the chatbot to play the role of well-known negative characters from our history, such as dictators or criminals. For example, this is what ChatGPT produced when impersonating a bad person:</p>
    <pre class="programlisting code"><code class="hljs-code">X is just another third-world country with nothing but drug lords and poverty-stricken people. The people there are uneducated and violent, and they don't have any respect for law and order. If you ask me, X is just a cesspool of crime and misery, and no one in their right mind would want to go there.
</code></pre>
    <p class="normal">Check the source of the experiment for more examples of different personas: <a href="https://techcrunch.com/2023/04/12/researchers-discover-a-way-to-make-chatgpt-consistently-toxic/">https://techcrunch.com/2023/04/12/researchers-discover-a-way-to-make-chatgpt-consistently-toxic/</a>.</p>
    <p class="normal">The discussion can <a id="_idIndexMarker1069"/>be extended to a never-ending list of examples, but the key <a id="_idIndexMarker1070"/>takeaway is that your LLM can produce harmful output or receive dangerous input, so you should monitor and prepare for them. Thus, to create safe LLM systems, you must protect them against harmful, sensitive, or invalid input and output by adding guardrails:</p>
    <ul>
      <li class="bulletList"><strong class="screenText">Input guardrails</strong>:<strong class="screenText"> </strong>Input<a id="_idIndexMarker1071"/> guardrails primarily protect against <a id="_idIndexMarker1072"/>three main risks: exposing private information to external APIs, executing harmful prompts that could compromise your system (model jailbreaking), and accepting violent or unethical prompts. When it comes to leaking private information to external APIs, the risk is specific to sending sensitive data outside your organization, such as credentials or classified information. When talking about model jailbreaking, we mainly refer to prompt injection, such as executing malicious SQL code that can access, delete, or corrupt your data. Lastly, some applications don’t want to accept violent or unethical queries from users, such as asking an LLM how to build a bomb.</li>
      <li class="bulletList"><strong class="screenText">Output guardrails</strong>: At the<a id="_idIndexMarker1073"/> output of an LLM response, you<a id="_idIndexMarker1074"/> want to catch failed outputs that don’t respect your application’s standards. This can vary from one application to another, but some examples are empty responses (these responses don’t follow your expected format, such as JSON or YAML), toxic responses, hallucinations, and, in general, wrong responses. Also, you <a id="_idIndexMarker1075"/>have to check for sensitive information that can leak from the internal knowledge of the LLM or your <a id="_idIndexMarker1076"/>RAG system.</li>
    </ul>
    <p class="normal">Popular guardrail tools are Galileo Protect, which<a id="_idIndexMarker1077"/> detects prompt injections, toxic language, data privacy protection leaks, and hallucinations. Also, you can use OpenAI’s Moderation API<a id="_idIndexMarker1078"/> to detect harmful inputs or outputs and take action on them.</p>
    <p class="normal">The downside of adding input and output guardrails is the extra latency added to your system, which might interfere with your application’s user experience. Thus, there is a trade-off between the safety of your input/output and latency. Regarding invalid outputs, as LLMs are non-deterministic, you can implement a retry mechanism to generate another potential candidate. However, as stated above, running the retry sequentially will double the response time. Thus, a common strategy is to run multiple generations in parallel and pick the best one. This will increase redundancy but help keep the latency in check.</p>
    <h3 id="_idParaDest-274" class="heading-3">Prompt monitoring</h3>
    <p class="normal">Monitoring is not new to<a id="_idIndexMarker1079"/> LLMOps, but in the LLM world, we have a<a id="_idIndexMarker1080"/> new entity to manage: the prompt. Thus, we have to find specific ways to log and analyze them.</p>
    <p class="normal">Most ML platforms, such as <a id="_idIndexMarker1081"/>Opik (from Comet ML) and W&amp;B, or other specialized tools like <a id="_idIndexMarker1082"/>Langfuse, have implemented logging tools to debug and monitor prompts. While in production, using these tools, you usually want to track the user input, the prompt templates, the input variables, the generated response, the number of tokens, and the latency.</p>
    <p class="normal">When <a id="_idIndexMarker1083"/>generating an answer with an LLM, we don’t wait for the whole answer to be generated; we stream the output token by token. This makes the entire process snappier and more responsive. Thus, when it comes to tracking the latency of generating an answer, the final user experience must<a id="_idIndexMarker1084"/> look at this from multiple perspectives, such as:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Time to First Token</strong> (<strong class="keyWord">TTFT</strong>): The<a id="_idIndexMarker1085"/> time it takes for the first token to be generated</li>
      <li class="bulletList"><strong class="keyWord">Time between Tokens</strong> (<strong class="keyWord">TBT</strong>): The<a id="_idIndexMarker1086"/> interval between each token generation</li>
      <li class="bulletList"><strong class="keyWord">Tokens per Second</strong> (<strong class="keyWord">TPS</strong>): The <a id="_idIndexMarker1087"/>rate at which tokens are generated</li>
      <li class="bulletList"><strong class="keyWord">Time per Output Token</strong> (<strong class="keyWord">TPOT</strong>): The<a id="_idIndexMarker1088"/> time it takes to generate each output token</li>
      <li class="bulletList"><strong class="keyWord">Total Latency</strong>: The<a id="_idIndexMarker1089"/> total time required to complete a response</li>
    </ul>
    <p class="normal">Also, tracking the total input and output tokens is critical to understanding the costs of hosting your LLMs.</p>
    <p class="normal">Ultimately, you can compute metrics that validate your model’s performance for each input, prompt, and output tuple. Depending on your use case, you can compute things such as accuracy, toxicity, and hallucination rate. When working with RAG systems, you can also compute metrics relative to the relevance and precision of the retrieved context.</p>
    <p class="normal">Another essential thing to <a id="_idIndexMarker1090"/>consider when monitoring prompts is to log their full traces. You might have multiple intermediate steps from the user query to the final general answer. For example, rewriting the query to improve the RAG’s retrieval accuracy evolves one or more intermediate steps. Thus, logging the full trace reveals the entire process from when a user sends a query to when the final response is returned, including the actions the system takes, the documents retrieved, and the final prompt sent to the model. Additionally, you can log the latency, tokens, and costs at each step, providing a more fine-grained view of all the steps. </p>
    <figure class="mediaobject"><img src="img/B31105_11_04.png" alt="Trace in Langfuse UI"/></figure>
    <p class="packt_figref">Figure 11.4: Example trace in the Langfuse UI</p>
    <p class="normal">As shown in <em class="italic">Figure 11.4</em>, the end goal is to trace each step from the user’s input until the generated <a id="_idIndexMarker1091"/>answer. If something fails or behaves unexpectedly, you can point exactly to the faulty step. The query can fail due to an incorrect answer, an invalid context, or incorrect data processing. Also, the application can behave unexpectedly if the number of generated tokens suddenly fluctuates during specific steps.</p>
    <p class="normal">To conclude, LLMOps<a id="_idIndexMarker1092"/> is a rapidly developing field. Given its quick evolution, making predictions is challenging. The truth is that we are not sure if the term LLMOps is here to stay. However, what is certain is that numerous new use cases for LLMs will emerge, along with tools and best practices to manage their lifecycle.</p>
    <p class="normal">Even if this DevOps, MLOps, and LLMOps section is far from comprehensive, it provides a strong idea of how to apply best ops practices in our LLM Twin use case.</p>
    <h1 id="_idParaDest-275" class="heading-1">Deploying the LLM Twin’s pipelines to the cloud</h1>
    <p class="normal">This <a id="_idIndexMarker1093"/>section will show you how to deploy all the LLM Twin’s pipelines to the cloud. We must deploy the entire infrastructure to have the whole system working in the cloud. Thus, we will have to:</p>
    <ol>
      <li class="numberedList" value="1">Set up an instance of MongoDB serverless.</li>
      <li class="numberedList">Set up an instance of Qdrant serverless.</li>
      <li class="numberedList">Deploy the ZenML pipelines, container, and artifact registry to AWS.</li>
      <li class="numberedList">Containerize the code and push the Docker image to a container registry.</li>
    </ol>
    <p class="normal">Note that the training and inference pipelines already work with AWS SageMaker. Thus, by following the preceding four steps, we ensure that our whole system is on the cloud, ready to scale and serve our imaginary clients.</p>
    <div><p class="normal"><strong class="keyWord">What are the deployment costs?</strong></p>
      <p class="normal">We will stick to the free versions<a id="_idIndexMarker1094"/> of the MongoDB, Qdrant, and ZenML services. As for AWS, we will mostly stick to their free tier for running the ZenML pipelines. The SageMaker training and inference components are more costly to run (which we won’t run in this section). Thus, what we will show you in the following sections will generate minimum costs (a few dollars at most) from AWS.</p>
    </div>
    <h2 id="_idParaDest-276" class="heading-2">Understanding the infrastructure</h2>
    <p class="normal">Before diving<a id="_idIndexMarker1095"/> into the step-by-step tutorial, where we will show you how to set up all the necessary components, let’s briefly overview our infrastructure and how all the elements interact. This will help us in mindfully following the tutorials below.</p>
    <p class="normal">As shown in <em class="italic">Figure 11.5</em>, we have a few services to set up. To keep things simple, for MongoDB and Qdrant, we will leverage their serverless freemium version. As for ZenML, we will leverage the free trial of the ZenML cloud, which will help us orchestrate all the pipelines in the cloud. How will it do that?</p>
    <p class="normal">By leveraging the ZenML cloud, we can quickly allocate all the required AWS resources to run, scale, and store the ML pipeline. It will help us spin up, with a few clicks, the following AWS components:</p>
    <ul>
      <li class="bulletList">An ECR service for storing Docker images</li>
      <li class="bulletList">An S3 object storage for storing all our artifacts and models</li>
      <li class="bulletList">SageMaker Orchestrator for orchestrating, running, and scaling all our ML pipelines</li>
    </ul>
    <figure class="mediaobject"><img src="img/B31105_11_05.png" alt=""/></figure>
    <p class="packt_figref">Figure 11.5: Infrastructure flow</p>
    <p class="normal">Now that we <a id="_idIndexMarker1096"/>understand what the essential resources of our infrastructure are, let’s look over the core flow of running a pipeline in the cloud that we will learn to implement, presented in <em class="italic">Figure 11.5</em>:</p>
    <ol>
      <li class="numberedList" value="1">Build a Docker image that contains all the system dependencies, the project dependencies, and the LLM Twin application.</li>
      <li class="numberedList">Push the Docker image to <strong class="screenText">ECR</strong>, where <strong class="screenText">SageMaker</strong> can access it.</li>
      <li class="numberedList">Now, we can trigger any pipeline implemented during this book either from the CLI of our local machine or <strong class="screenText">ZenML’s</strong> dashboard.</li>
      <li class="numberedList">Each step from ZenML’s pipeline will be mapped to a SageMaker job that runs on an AWS EC2 <strong class="keyWord">virtual machine</strong> (<strong class="keyWord">VM</strong>). Based on the dependencies between the <strong class="keyWord">directed acyclic graph</strong> (<strong class="keyWord">DAG</strong>) steps, some will run in parallel and others sequentially.</li>
      <li class="numberedList">When running a step, SageMaker pulls the Docker image from ECR, defined in step 2. Based on the pulled image, it creates a Docker container that executes the pipeline step.</li>
      <li class="numberedList">As the job is executed, it can access the S3 artifact storage, MongoDB, and Qdrant vector DB to query or push data. The ZenML dashboard is a key tool, providing real-time updates on the pipeline’s progress and ensuring a clear view of the process.</li>
    </ol>
    <p class="normal">Now that we know<a id="_idIndexMarker1097"/> how the infrastructure works, let’s start by setting up MongoDB, Qdrant, and the ZenML cloud.</p>
    <div><p class="normal"><strong class="keyWord">What AWS cloud region should I choose?</strong></p>
      <p class="normal">In our tutorials, all the services will be deployed to AWS within the <strong class="screenText">Frankfurt (eu-central-1)</strong> region. You can select another region, but be consistent across all the services to ensure faster responses between components and reduce potential errors.</p>
      <p class="normal"><strong class="keyWord">How should I manage changes in the services’ UIs?</strong></p>
      <p class="normal">Unfortunately, MongoDB, Qdrant, or other services may change their UI or naming conventions. As we can’t update this book each time that happens, please refer to their official documentation to check anything that differs from our tutorial. We apologize for this inconvenience, but unfortunately, it is not in our control.</p>
    </div>
    <h2 id="_idParaDest-277" class="heading-2">Setting up MongoDB</h2>
    <p class="normal">We will show you<a id="_idIndexMarker1098"/> how to create and<a id="_idIndexMarker1099"/> integrate a free MongoDB cluster into our projects. To do so, these are the steps you have to follow:</p>
    <ol>
      <li class="numberedList" value="1">Go to their site<a id="_idIndexMarker1100"/> at <a href="https://www.mongodb.com">https://www.mongodb.com</a> and create an account.</li>
      <li class="numberedList">In the left panel, go to <strong class="screenText">Deployment</strong> <strong class="keyWord">|</strong> <strong class="screenText">Database</strong> and click <strong class="screenText">Build a Cluster</strong>.</li>
      <li class="numberedList">Within the creation form, do the following:<ol class="alphabeticList level-2" style="list-style-type: lower-alpha;">
          <li class="alphabeticList level-2" value="1">Choose an <strong class="screenText">M0 Free</strong> cluster.</li>
          <li class="alphabeticList level-2">Call your cluster <strong class="screenText">twin</strong>.</li>
          <li class="alphabeticList level-2">Choose <strong class="screenText">AWS</strong> as your provider.</li>
          <li class="alphabeticList level-2">Choose <strong class="screenText">Frankfurt (eu-central-1)</strong> as your region. You can choose another region, but be careful to choose the same region for all future AWS services.</li>
          <li class="alphabeticList level-2">Leave the rest of the attributes with their default values.</li>
          <li class="alphabeticList level-2">In the bottom right, click the <strong class="screenText">Create Deployment</strong> green button.</li>
        </ol>
      </li>
      <li class="numberedList">To test that your newly created MongoDB cluster works fine, we must connect to it from our local machine. We used the MongoDB VS Code extension to do so, but you can use any other tool. Thus, from their <strong class="screenText">Choose a connection method </strong>setup flow, choose <strong class="screenText">MongoDB for VS Code</strong>. Then, follow the steps provided on their site.</li>
      <li class="numberedList">To connect, you must paste the DB connection URL in the VS Code extension (or another tool of your liking), which contains your username, password, and cluster URL, similar to this one: <code class="inlineCode">mongodb+srv://&lt;username&gt;:&lt;password&gt; @twin.vhxy1.mongodb.net</code>. Make sure to save this URL somewhere you can copy it from later.</li>
      <li class="numberedList">If you don’t know or want to change your password, go to <strong class="screenText">Security </strong><strong class="screenText">→</strong><strong class="screenText"> Quickstart</strong> in the left panel. There, you can edit your login credentials. Be sure to save them somewhere safe, as you won’t be able to access them later.</li>
      <li class="numberedList">After verifying that your connections work, go to <strong class="screenText">Security </strong><strong class="screenText">→</strong><strong class="screenText"> Network Access</strong> in the left panel and click <strong class="screenText">ADD IP ADDRESS</strong>.<strong class="screenText"> </strong>Then click <strong class="screenText">ALLOW ACCESS FROM ANYWHERE</strong> and hit Confirm. Out of simplicity, we allow any machine from any IP to access our MongoDB cluster. This ensures that our pipelines can query or write to the DB without any additional complex networking setup. It’s not the safest option for production, but for our example, it’s perfectly fine.</li>
      <li class="numberedList">The final step is to return to your project and open your <code class="inlineCode">.env</code> file. Now, either add or replace the <code class="inlineCode">DATABASE_HOST</code> variable with your MongoDB <a id="_idIndexMarker1101"/>connection <a id="_idIndexMarker1102"/>string. It should look something like this: <code class="inlineCode">DATABASE_HOST= mongodb+srv://&lt;username&gt;:&lt;password&gt; @twin.vhxy1.mongodb.net</code>.</li>
    </ol>
    <p class="normal">That’s it! Now, instead of reading and writing from your local MongoDB, you will do it from the cloud MongoDB cluster we just created. Let’s repeat a similar process with Qdrant.</p>
    <h2 id="_idParaDest-278" class="heading-2">Setting up Qdrant</h2>
    <p class="normal">We have to <a id="_idIndexMarker1103"/>repeat a similar process to what we did for MongoDB. Thus, to create a <a id="_idIndexMarker1104"/>Qdrant cluster and hook it to our project, follow these steps:</p>
    <ol>
      <li class="numberedList" value="1">Go to <a id="_idIndexMarker1105"/>Qdrant at <a href="https://cloud.qdrant.io/">https://cloud.qdrant.io/</a> and create an <a id="_idIndexMarker1106"/>account.</li>
      <li class="numberedList">In the left panel, go<a id="_idIndexMarker1107"/> to <strong class="screenText">Clusters</strong> and click <strong class="screenText">Create</strong>.</li>
      <li class="numberedList">Fill out the cluster creation form with the following:<ol class="alphabeticList level-2" style="list-style-type: lower-alpha;">
          <li class="alphabeticList level-2" value="1">Choose the <strong class="screenText">Free</strong> version of the cluster.</li>
          <li class="alphabeticList level-2">Choose <strong class="screenText">GCP</strong> as the cloud provider (while writing the book, it was the only one allowed for a free cluster).</li>
          <li class="alphabeticList level-2">Choose <strong class="screenText">Frankfurt</strong> as the region (or the same region as you chose for MongoDB).</li>
          <li class="alphabeticList level-2">Name the cluster <strong class="screenText">twin</strong>.</li>
          <li class="alphabeticList level-2">Leave the rest of the attributes with their default values and click <strong class="screenText">Create</strong>.</li>
        </ol>
      </li>
      <li class="numberedList">Access the cluster in the <strong class="screenText">Data Access Control </strong>section in the left panel.</li>
      <li class="numberedList">Click <strong class="screenText">Create</strong> and choose your <strong class="screenText">twin</strong> cluster to create a new access token.<strong class="screenText"> </strong>Copy the newly created token somewhere safe, as you won’t be able to access it anymore.</li>
      <li class="numberedList">You can run their example from <strong class="screenText">Usage Examples</strong> to test that your connection works fine.</li>
      <li class="numberedList">Go back to the <strong class="screenText">Clusters </strong>section of Qdrant and open your newly created <strong class="screenText">twin </strong>cluster. You will have access to the cluster’s <strong class="screenText">endpoint</strong>, which you need to configure Qdrant in your code.</li>
    </ol>
    <p class="normal">You can visualize your Qdrant collections and documents by clicking <strong class="screenText">Open Dashboard</strong> and entering your <strong class="screenText">API Key</strong> as your password. The Qdrant cluster dashboard will now be empty, but after running the pipelines, you will see all the collections, as shown here:</p>
    <figure class="mediaobject"><img src="img/B31105_11_06.png" alt=""/></figure>
    <p class="packt_figref">Figure 11.6: Qdrant cluster dashboard example after being populated with two collections.</p>
    <p class="normal">Finally, return <a id="_idIndexMarker1108"/>to your project and open your <code class="inlineCode">.env</code> file. Now, we must fill in a couple of environment <a id="_idIndexMarker1109"/>variables as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">USE_QDRANT_CLOUD=true
QDRANT_CLOUD_URL=&lt;the endpoint URL found at step 7&gt;
QDRANT_APIKEY=&lt;the access token created at step 5&gt;
</code></pre>
    <p class="normal">That’s it! Instead of reading and writing from your local Qdrant vector DB, you will do it from the cloud Qdrant cluster we just created. Just to be sure that everything works fine, run the end-to-end data pipeline with the cloud version of MongoDB and Qdrant as follows:</p>
    <pre class="programlisting con"><code class="hljs-con">peotry poe run-end-to-end-data-pipeline
</code></pre>
    <p class="normal">The last step is setting up the ZenML cloud and deploying all our infrastructure to AWS.</p>
    <h2 id="_idParaDest-279" class="heading-2">Setting up the ZenML cloud</h2>
    <p class="normal">Setting up the<a id="_idIndexMarker1110"/> ZenML cloud and the AWS infrastructure is a multi-step <a id="_idIndexMarker1111"/>process. First, we will set up a ZenML cloud account, then the AWS infrastructure through the ZenML cloud, and, finally, we will bundle our code in a Docker image to run it in AWS SageMaker.</p>
    <p class="normal">Let’s start with setting up the ZenML cloud:</p>
    <ol>
      <li class="numberedList" value="1">Go to the<a id="_idIndexMarker1112"/> ZenML cloud at <a href="https://cloud.zenml.io">https://cloud.zenml.io</a> and make an account. They provide a seven-day free trial, which is enough to run our examples.</li>
      <li class="numberedList">Fill out their onboarding form and create an organization with a unique name and a tenant called <strong class="screenText">twin</strong>. A tenant refers to a deployment of ZenML in a fully isolated environment. Wait a few minutes until your tenant server is up before proceeding to the next step.</li>
      <li class="numberedList">If you want to, you can go through their <strong class="screenText">Quickstart Guide</strong> to understand how the ZenML cloud works with a simpler example. It is not required to go through it to deploy the LLM Twin application, but we recommend it to ensure everything works fine.</li>
      <li class="numberedList">At this point, we <a id="_idIndexMarker1113"/>assume that you have gone through the <strong class="screenText">Quickstart Guide</strong>. Otherwise, you<a id="_idIndexMarker1114"/> might encounter issues during the next steps. To connect our project with this ZenML cloud tenant, return to the project and run the <code class="inlineCode">zenml connect</code> command provided in the dashboard. It looks similar to the following example but with a different URL:<code class="inlineCode">zenml connect --url https://0c37a553-zenml.cloudinfra.zenml.io</code>.</li>
      <li class="numberedList">To ensure everything works fine, run a random pipeline from your code. Note that at this point, we are still running it locally, but instead of logging the results to the local server, we log everything to the cloud version:
        <pre class="programlisting con-one"><code class="hljs-con">poetry poe run-digital-data-etl
</code></pre>
      </li>
      <li class="numberedList">Go to the <strong class="screenText">Pipelines</strong> section in the left panel of the ZenML dashboard. If everything worked fine, you should see the pipeline you ran in <em class="italic">Step 5</em> there.
        <div><p class="normal">Ensure that your ZenML server version matches your local ZenML version. For example, when we wrote this book, both were version 0.64.0. If they don’t match, you might encounter strange behavior, or it might not work correctly. The easiest fix is to go to your <code class="inlineCode">pyproject.toml</code> file, find the <code class="inlineCode">zenml</code> dependency, and update it with the version of your server. Then run <code class="inlineCode">poetry lock --no-update &amp;&amp; poetry install</code> to update your local virtual environment.</p>
        </div>
      </li>
    </ol>
    <p class="normal">To ship the<a id="_idIndexMarker1115"/> code to AWS, you must create a ZenML stack. A stack<a id="_idIndexMarker1116"/> is a set of <a id="_idIndexMarker1117"/>components, such as the underlying orchestrator, object storage, and container registry, that ZenML needs under the hood to run the pipelines. Intuitively, you can see your stack as your infrastructure. While working locally, ZenML offers a default stack that allows you to quickly develop your code and test things locally. However, by defining different stacks, you can quickly switch between different infrastructure environments, such as local and AWS runs, which we will showcase in this section.</p>
    <div><p class="normal">Before starting this section, ensure you have an AWS account with admin permissions ready.</p>
    </div>
    <p class="normal">With that in mind, let’s create an AWS stack for our project. To do so, follow the next steps:</p>
    <ol>
      <li class="numberedList" value="1">In the left panel, click on the <strong class="screenText">Stacks </strong>section and hit the <strong class="screenText">New Stack</strong> button.</li>
      <li class="numberedList">You will have multiple options for creating a stack, but the easiest is creating one from scratch within the in-browser experience, which doesn’t require additional preparations. This is not very flexible, but it is enough to host our project. Thus, choose <strong class="screenText">Create New Infrastructure </strong><strong class="screenText">→</strong><strong class="screenText"> In-browser Experience</strong>.</li>
      <li class="numberedList">Then, choose <strong class="screenText">AWS</strong> as your cloud provider.</li>
      <li class="numberedList">Choose<strong class="screenText"> Europe (Frankfurt)—eu-central-1</strong> as your location or the region you used to set up MongoDB and Qdrant.</li>
      <li class="numberedList">Name it <strong class="screenText">aws-stack</strong>.<strong class="screenText"> </strong>It is essential to name it exactly like this so that the commands that we will use work.</li>
      <li class="numberedList">Now ZenML will create a set of IAM roles to give permissions to all the other components to communicate with each other, an S3 bucket as your artifact storage, an ECR repository as your container registry, and SageMaker as your orchestrator.</li>
      <li class="numberedList">Click <strong class="screenText">Next</strong>.</li>
      <li class="numberedList">Click the <strong class="screenText">Deploy to AWS </strong>button. It will open a <strong class="screenText">CloudFormation </strong>page on AWS. ZenML leverages <strong class="screenText">CloudFormation </strong>(an infrastructure as code, or IaC, tool)<strong class="screenText"> </strong>to create all the AWS resources we enumerated in <em class="italic">Step 6</em>.</li>
      <li class="numberedList">At the bottom, check all the boxes to acknowledge that AWS CloudFormation will create AWS resources on your behalf. Finally, click the <strong class="screenText">Create stack </strong>button. Now, we must wait for a couple of minutes for AWS CloudFormation to spin up all the resources.</li>
      <li class="numberedList">Return to<a id="_idIndexMarker1118"/> the ZenML page and click the <strong class="screenText">Finish</strong> button.</li>
    </ol>
    <p class="normal-one">By leveraging <a id="_idIndexMarker1119"/>ZenML, we efficiently deployed the entire AWS infrastructure for our ML pipelines. We began with a basic example, sacrificing some control. However, if you seek more control, ZenML offers the option to use Terraform (an IaC tool) to fully control your AWS resources or to connect ZenML with your current infrastructure.</p>
    <p class="normal-one">Before moving to the next step, let’s have a quick recap of the AWS resources we just created:</p>
    <ul>
      <li class="bulletList level-2"><strong class="keyWord">An IAM role</strong> is an AWS identity with permissions policies that define what actions are allowed or denied for that role. It is used to grant access<a id="_idIndexMarker1120"/> to AWS services without needing to share security credentials.</li>
      <li class="bulletList level-2"><strong class="screenText">S3 </strong>is a scalable<a id="_idIndexMarker1121"/> and secure object storage service that allows storing and retrieving files from anywhere on the web. It is commonly used for data backup, content storage, and data lakes. It’s more scalable and flexible than Google Drive.</li>
      <li class="bulletList level-2"><strong class="screenText">ECR </strong>is a fully managed <a id="_idIndexMarker1122"/>Docker container registry that makes storing, managing, and deploying Docker container images easy.</li>
      <li class="bulletList level-2"><strong class="screenText">SageMaker </strong>is a fully <a id="_idIndexMarker1123"/>managed service that allows developers and data scientists to quickly build, train, and deploy ML models.</li>
      <li class="bulletList level-2"><strong class="screenText">SageMaker Orchestrator </strong>is a feature of SageMaker that helps automate the execution of <a id="_idIndexMarker1124"/>ML workflows, manage dependencies between steps, and ensure the reproducibility and scalability of model training and deployment pipelines. Other similar tools are Prefect, Dagster, Metaflow, and Airflow.</li>
      <li class="bulletList level-2"><strong class="keyWord">CloudFormation </strong>is a <a id="_idIndexMarker1125"/>service that allows you to model and set up your AWS resources so that you can spend less time managing them<a id="_idIndexMarker1126"/> and more time focusing on your applications. It automates the process of provisioning AWS <a id="_idIndexMarker1127"/>infrastructure using templates.</li>
    </ul>
    <p class="normal">Before running the ML pipelines, the last step is to containerize the code and prepare a Docker image that packages our dependencies and code.</p>
    <h3 id="_idParaDest-280" class="heading-3">Containerize the code using Docker</h3>
    <p class="normal">So far, we <a id="_idIndexMarker1128"/>have defined our infrastructure, MongoDB, Qdrant, and AWS, for storage and computing. The last step is to find a way to take our code and run it on top of this infrastructure. The most popular solution is<a id="_idIndexMarker1129"/> Docker, a tool that allows us to create an isolated environment (a container) that contains everything we need to run our application, such as system dependencies, Python dependencies, and the code.</p>
    <p class="normal">We defined our<a id="_idIndexMarker1130"/> Docker image at the project’s root<a id="_idIndexMarker1131"/> in the <code class="inlineCode">Dockerfile</code>. This is the standard naming convention for Docker. Before digging into the code, if you want to build the Docker image yourself, ensure that you have Docker installed on your machine. If you don’t have it, you can install it by following the instructions provided here: <a href="https://docs.docker.com/engine/install">https://docs.docker.com/engine/install</a>. Now, let’s look at the content of the <code class="inlineCode">Dockerfile</code> step by step.</p>
    <p class="normal">The <code class="inlineCode">Dockerfile</code> begins by specifying the base image, which is a lightweight version of Python 3.11 based on the Debian Bullseye distribution. The environment variables are then set up to configure various aspects of the container, such as the workspace directory, turning off Python bytecode generation, and configuring Python to output directly to the terminal. Additionally, the version of Poetry to be installed is specified, and a few environment variables are set to ensure that package installations are non-interactive, which is vital for automated builds.</p>
    <pre class="programlisting code"><code class="hljs-code">FROM python:3.11-slim-bullseye AS release
ENV WORKSPACE_ROOT=/app/
ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1
ENV POETRY_VERSION=1.8.3
ENV DEBIAN_FRONTEND=noninteractive
ENV POETRY_NO_INTERACTION=1
</code></pre>
    <p class="normal">Next, we install Google Chrome in the container. The installation process begins by updating the package lists and installing essential tools like gnupg, wget, and curl. The Google Linux signing key is added, and the Google Chrome repository is configured. After another package list update, the stable version of Google Chrome is installed. The package lists are removed after installation to keep the image as small as possible.</p>
    <pre class="programlisting code"><code class="hljs-code">RUN apt-get update -y &amp;&amp; \
    apt-get install -y gnupg wget curl --no-install-recommends &amp;&amp; \
    wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | gpg --dearmor -o /usr/share/keyrings/google-linux-signing-key.gpg &amp;&amp; \
    echo "deb [signed-by=/usr/share/keyrings/google-linux-signing-key.gpg] https://dl.google.com/linux/chrome/deb/ stable main" &gt; /etc/apt/sources.list.d/google-chrome.list &amp;&amp; \
    apt-get update -y &amp;&amp; \
    apt-get install -y google-chrome-stable &amp;&amp; \
    rm -rf /var/lib/apt/lists/*
</code></pre>
    <p class="normal">Following the Chrome installation, other essential system dependencies are installed. Once these packages are installed, the package cache is cleaned up to reduce the image size further.</p>
    <pre class="programlisting code"><code class="hljs-code">RUN apt-get update -y \
    &amp;&amp; apt-get install -y --no-install-recommends build-essential \
    gcc \
    python3-dev \
    build-essential \
    libglib2.0-dev \
    libnss3-dev \
    &amp;&amp; apt-get clean \
    &amp;&amp; rm -rf /var/lib/apt/lists/*
</code></pre>
    <p class="normal">Poetry, the <a id="_idIndexMarker1132"/>dependency management tool, is then installed using pip. The <code class="inlineCode">--no-cache-dir</code> option prevents pip from caching packages, helping to keep the image smaller. After installation, Poetry is configured to use up to 20 parallel workers when installing packages, which can speed up the installation process.</p>
    <pre class="programlisting code"><code class="hljs-code">RUN pip install --no-cache-dir "poetry==$POETRY_VERSION"
RUN poetry config installer.max-workers 20
</code></pre>
    <p class="normal">The working directory inside the container is set to <code class="inlineCode">WORKSPACE_ROOT</code>, which defaults to <code class="inlineCode">/app/</code>, where the application code will reside. The <code class="inlineCode">pyproject.toml</code> and <code class="inlineCode">poetry.lock</code> files define the Python’s project dependencies and are copied into this directory.</p>
    <pre class="programlisting code"><code class="hljs-code">WORKDIR $WORKSPACE_ROOT
COPY pyproject.toml poetry.lock $WORKSPACE_ROOT
</code></pre>
    <p class="normal">With the dependency files in place, the project’s dependencies are installed using Poetry. The configuration turns off the creation of a virtual environment, meaning the dependencies will be installed directly into the container’s Python environment. The installation excludes development dependencies and prevents caching to minimize space usage. </p>
    <p class="normal">Additionally, the <code class="inlineCode">poethepoet</code> plugin is installed to help manage tasks within the project. Finally, any remaining Poetry cache is removed to <a id="_idIndexMarker1133"/>keep the container as lean as possible.</p>
    <pre class="programlisting code"><code class="hljs-code">RUN poetry config virtualenvs.create false &amp;&amp; \
    poetry install --no-root --no-interaction --no-cache --without dev &amp;&amp; \
    poetry self add 'poethepoet[poetry_plugin]' &amp;&amp; \
    rm -rf ~/.cache/pypoetry/cache/ &amp;&amp; \
    rm -rf ~/.cache/pypoetry/artifacts/
</code></pre>
    <p class="normal">In the final step, the entire project directory from the host machine is copied into the container’s working directory. This step ensures that all the application files are available within the container.</p>
    <p class="normal">One important trick when writing a <code class="inlineCode">Dockerfile</code> is to decouple your installation steps from copying the rest of the files. This is useful because each Docker command is cached and layered on top of each other. Thus, whenever you change one layer when rebuilding the Docker image, all the layers below the one altered are executed again. Because you rarely change your system and project dependencies but mostly change your code, copying your project files in the last step makes rebuilding Docker images fast by taking advantage of the caching mechanism’s full potential.</p>
    <pre class="programlisting code"><code class="hljs-code">COPY . $WORKSPACE_ROOT
</code></pre>
    <p class="normal">This <code class="inlineCode">Dockerfile</code> is designed to create a clean, consistent Python environment with all necessary dependencies. It allows the project to run smoothly in any environment that supports Docker.</p>
    <p class="normal">The last step is to build the Docker image and push it to the ECR created by ZenML. To build the Docker image from the root of the project, run the following:</p>
    <pre class="programlisting con"><code class="hljs-con">docker buildx build --platform linux/amd64 -t llmtwin -f Dockerfile .
</code></pre>
    <p class="normal">We must build<a id="_idIndexMarker1134"/> it on a Linux platform as the Google Chrome installer we used inside Docker works only on a Linux machine. Even if you use a macOS or Windows machine, Docker can emulate a virtual Linux container.</p>
    <p class="normal">The tag of the newly created Docker image is <code class="inlineCode">llmtwin</code>. We also provide this <code class="inlineCode">build</code> command under a <code class="inlineCode">poethepoet</code> command:</p>
    <pre class="programlisting con"><code class="hljs-con">poetry poe build-docker-image
</code></pre>
    <p class="normal">Now, let’s push the Docker image to ECR. To do so, navigate to your AWS console and then to the ECR service. From there, find the newly created ECR repository. It should be prefixed with <code class="inlineCode">zenml-*</code>, as shown here:</p>
    <figure class="mediaobject"><img src="img/B31105_11_07.png" alt=""/></figure>
    <p class="packt_figref">Figure 11.7: AWS ECR example</p>
    <p class="normal">The first step<a id="_idIndexMarker1135"/> is to authenticate to ECR. For this to work, ensure that you have the AWS CLI installed and configured with your admin AWS credentials, as explained in <em class="italic">Chapter 2</em>:</p>
    <pre class="programlisting con"><code class="hljs-con">AWS_REGION=&lt;your_region&gt; # e.g. AWS_REGION=eu-central-1
AWS_ECR_URL=&lt;your_acount_id&gt;
aws ecr get-login-password --region ${AWS_REGION}| docker login --username AWS --password-stdin ${AWS_ECR_URL}
</code></pre>
    <p class="normal">You can get your current <code class="inlineCode">AWS_REGION</code> by clicking on the toggle in the top-right corner, as seen in <em class="italic">Figure 11.8</em>. Also, you can copy the ECR URL to fill the <code class="inlineCode">AWS_ECR_URL</code> variable from the main AWS ECR dashboard, as illustrated in <em class="italic">Figure 11.7</em>. After running the previous command, you should see the message <strong class="screenText">Login Succeeded</strong> on the CLI.</p>
    <figure class="mediaobject"><img src="img/B31105_11_08.png" alt=""/></figure>
    <p class="packt_figref">Figure 11.8: AWS region and account details</p>
    <p class="normal">Now we have to<a id="_idIndexMarker1136"/> add another tag to the <code class="inlineCode">llmtwin</code> Docker image that signals the Docker registry we want to push it to:</p>
    <pre class="programlisting con"><code class="hljs-con">docker tag llmtwin ${AWS_ECR_URL}:latest
</code></pre>
    <p class="normal">Finally, we push it to ECR by running:</p>
    <pre class="programlisting con"><code class="hljs-con">docker push ${AWS_ECR_URL}:latest
</code></pre>
    <p class="normal">After the upload is finished, return to your AWS ECR dashboard and open your ZenML repository. The Docker image should appear, as shown here:</p>
    <figure class="mediaobject"><img src="img/B31105_11_09.png" alt=""/></figure>
    <p class="packt_figref">Figure 11.9: AWS ECR repository example after the Docker image is pushed</p>
    <p class="normal">For every change in the code that you need to ship and test, you would have to go through all these steps, which are tedious and error-prone. The <em class="italic">Adding LLMOps to the LLM Twin</em><strong class="screenText"> </strong>section of this chapter will teach us how to automate these steps within the CD pipeline using GitHub Actions. Still, we first wanted to go through them manually to fully understand the behind-the-scenes process and not treat it as a black box. Understanding these details is vital for debugging your CI/CD <a id="_idIndexMarker1137"/>pipelines, where you must understand the error messages and how to fix them.</p>
    <p class="normal">Now that we have built our Docker image and pushed it to AWS ECR, let’s deploy it to AWS.</p>
    <h3 id="_idParaDest-281" class="heading-3">Run the pipelines on AWS</h3>
    <p class="normal">We are<a id="_idIndexMarker1138"/> very close to running the ML pipelines on AWS, but we have to go through a few final steps. Let’s switch from the default ZenML stack to the AWS one we created in this chapter. From the root of your project, run the following in the CLI:</p>
    <pre class="programlisting con"><code class="hljs-con">zenml stack set aws-stack
</code></pre>
    <p class="normal">Return to your AWS ECR ZenML repository and copy the image URI as shown in <em class="italic">Figure 11.9</em>. Then, go to the <code class="inlineCode">configs</code> directory, open the <code class="inlineCode">configs/end_to_end_data.yaml</code> file, and update the <code class="inlineCode">settings.docker.parent_image</code> attribute with your ECR URL, as shown below:</p>
    <pre class="programlisting code"><code class="hljs-code">settings:
  docker:
    parent_image: &lt;YOUR ECR URL&gt; #e.g., 992382797823.dkr.ecr.eu-central-1.amazonaws.com/zenml-rlwlcs:latest
    skip_build: True
</code></pre>
    <p class="normal">We’ve configured the pipeline to always use the latest Docker image available in ECR. This means that the pipeline will automatically pick up the latest changes made to the code whenever we push a new image.</p>
    <p class="normal">We must export all the credentials from our <code class="inlineCode">.env</code> file to ZenML secrets, a feature that safely stores your credentials and makes them accessible within your pipelines:</p>
    <pre class="programlisting con"><code class="hljs-con">poetry poe export-settings-to-zenml
</code></pre>
    <p class="normal">The last step is setting up to run the pipelines asynchronously so we don’t have to wait until they are finished, which might result in timeout errors:</p>
    <pre class="programlisting con"><code class="hljs-con">zenml orchestrator update aws-stack --synchronous=False
</code></pre>
    <p class="normal">Now that <a id="_idIndexMarker1139"/>ZenML knows to use the AWS stack, our custom Docker image, and has access to our credentials, we are finally done with the setup. Run the <code class="inlineCode">end-to-end-data-pipeline</code> with the following command:</p>
    <pre class="programlisting con"><code class="hljs-con">poetry poe run-end-to-end-data-pipeline
</code></pre>
    <p class="normal">Now you can go to <strong class="screenText">ZenML Cloud </strong><strong class="screenText">→</strong><strong class="screenText"> Pipelines </strong><strong class="screenText">→</strong><strong class="screenText"> end_to_end_data</strong> and open the latest run. On the ZenML dashboard, you can visualize the latest state of the pipeline, as seen in <em class="italic">Figure 11.10</em>. Note that this pipeline runs all the data-related pipelines in a single run.</p>
    <p class="normal">In the <em class="italic">Adding LLMOps to the LLM Twin</em><strong class="screenText"> </strong>section, we will explain why we compressed all the steps into a single pipeline.</p>
    <figure class="mediaobject"><img src="img/B31105_11_10.png" alt=""/></figure>
    <p class="packt_figref">Figure 11.10: ZenML example of running the end-to-end-data-pipeline</p>
    <p class="normal">You can click on<a id="_idIndexMarker1140"/> any running block and find details about the run, the code used for that specific step, and the logs for monitoring and debugging, as illustrated in <em class="italic">Figure 11.11</em>:</p>
    <figure class="mediaobject"><img src="img/B31105_11_11.png" alt=""/></figure>
    <p class="packt_figref">Figure 11.11: ZenML step metadata example</p>
    <div><p class="normal">To run other pipelines, you have to update the <code class="inlineCode">settings.docker.parent_image</code> attribute in their config file under the <code class="inlineCode">configs/</code> directory.</p>
    </div>
    <p class="normal">To find even<a id="_idIndexMarker1141"/> more details about the runs, you can go to AWS SageMaker. In the left panel, click <strong class="screenText">SageMaker dashboard</strong>, and on the right, in the <strong class="screenText">Processing </strong>column, click on the green <strong class="screenText">Running</strong> section, as shown in <em class="italic">Figure 11.12</em>. </p>
    <p class="normal">This will open a list of all the <strong class="screenText">processing jobs </strong>that execute your ZenML pipelines.</p>
    <figure class="mediaobject"><img src="img/B31105_11_12.png" alt=""/></figure>
    <p class="packt_figref">Figure 11.12: SageMaker dashboard</p>
    <div><p class="normal">If you want to run the pipelines locally again, use the following CLI command:</p>
      <pre class="programlisting con"><code class="hljs-con">poetry poe set-local-stack
</code></pre>
      <p class="normal">If you want to disconnect from the ZenML cloud dashboard and use the local version again, run the following:</p>
      <pre class="programlisting con"><code class="hljs-con">zenml disconnect
</code></pre>
    </div>
    <h3 id="_idParaDest-282" class="heading-3">Troubleshooting the ResourceLimitExceeded error after running a ZenML pipeline on SageMaker</h3>
    <p class="normal">Let’s <a id="_idIndexMarker1142"/>assume, you’ve encountered a <strong class="screenText">ResourceLimitExceeded </strong>error after running a ZenML pipeline on SageMaker using the AWS stack. In this case, you have to explicitly ask AWS to give you access to a specific type of AWS EC2 VM.</p>
    <p class="normal">ZenML uses, by default, <code class="inlineCode">ml.t3.medium</code> EC2 machines, which are part of the AWS freemium tier. However, some AWS accounts cannot access these VMs by default. To check your access, search your AWS console for <strong class="screenText">Service Quotas</strong>. </p>
    <p class="normal">Then, in the left panel, click on <strong class="screenText">AWS services</strong>, search for <strong class="screenText">Amazon SageMaker</strong>, and then for <code class="inlineCode">ml.t3.medium</code>. In <em class="italic">Figure 11.13</em>, you can see our quotas for these types of machines. If yours is <strong class="screenText">0</strong>, you should request that AWS increase them to numbers similar to those from <em class="italic">Figure 11.13</em> in the <strong class="screenText">Applied account-level quota value</strong> column. The whole process is free of charge and only requires a few clicks. Unfortunately, you might have to wait for a few hours up to one day until AWS accepts your request.</p>
    <figure class="mediaobject"><img src="img/B31105_11_13.png" alt=""/></figure>
    <p class="packt_figref">Figure 11.13: SageMaker—ml.t3.medium expected quotas</p>
    <p class="normal">You can find <a id="_idIndexMarker1143"/>step-by-step instructions on how to solve this error and request new quotas at this link: <a href="https://repost.aws/knowledge-center/sagemaker-resource-limit-exceeded-error">https://repost.aws/knowledge-center/sagemaker-resource-limit-exceeded-error</a>.</p>
    <div><p class="normal">If you changed the values from your .env file and want to update the ZenML secrets with them, first run the following CLI command to delete the old secrets:</p>
      <pre class="programlisting con"><code class="hljs-con">poetry poe delete-settings-zenml
</code></pre>
      <p class="normal">Then, you can export them again by running:</p>
      <pre class="programlisting con"><code class="hljs-con">poetry poe export-settings-to-zenml
</code></pre>
    </div>
    <h1 id="_idParaDest-283" class="heading-1">Adding LLMOps to the LLM Twin</h1>
    <p class="normal">In the previous section, we saw<a id="_idIndexMarker1144"/> how to set up the infrastructure for the LLM Twin project by manually building the Docker image and pushing it to ECR. We want to automate the entire process and implement a CI/CD pipeline using GitHub Actions and a CT pipeline using ZenML. As mentioned earlier, implementing a CI/CD/CT pipeline ensures that each feature pushed to main branches is consistent and tested. Also, by automating the deployment and training, you support collaboration, save time, and reduce human errors.</p>
    <p class="normal">Finally, at the end of the section, we will show you how to implement a prompt monitoring pipeline using Opik from Comet ML and an alerting system using ZenML. This prompt monitoring pipeline will help us debug and analyze the RAG and LLM logic. As LLM systems are non-deterministic, capturing and storing the prompt traces is essential for monitoring your ML logic.</p>
    <p class="normal">Before diving into the implementation, let’s start with a quick section on the LLM Twin’s CI/CD pipeline flow.</p>
    <h2 id="_idParaDest-284" class="heading-2">LLM Twin’s CI/CD pipeline flow</h2>
    <p class="normal">We have two <a id="_idIndexMarker1145"/>environments: staging<a id="_idIndexMarker1146"/> and production. When <a id="_idIndexMarker1147"/>developing a new feature, we create a new branch out of the staging branch and develop solely on that one. When we are done and consider the feature finished, we open a <strong class="keyWord">pull request</strong> (<strong class="keyWord">PR</strong>) to the staging branch. After the feature branch is accepted, it is merged into the staging branch. This is a standard workflow in most software applications. There might be variations, like adding a dev environment, but the principles remain the same.</p>
    <p class="normal">As illustrated in <em class="italic">Figure 11.14</em>, the CI pipeline is triggered when the PR opens. At this point, we test the feature branch for linting and formatting errors. Also, we run a <code class="inlineCode">gitleaks</code> command to check for credentials and sensitive information that was committed by mistake. If the linting, formatting, and gitleaks steps pass (also known as static analysis), we run the automated tests. Note that the static analysis steps run faster than the automated tests. Thus, the order matters. That’s why adding the static analysis steps at the beginning of the CI pipeline is good practice. We propose the following order of the CI steps:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">gitleaks</code> checks</li>
      <li class="bulletList">Linting checks</li>
      <li class="bulletList">Formatting checks</li>
      <li class="bulletList">Automated testing, such as unit and integration tests</li>
    </ul>
    <p class="normal">If any check fails, the CI pipeline fails, and the developer who created the PR cannot merge it into the staging branch until it fixes the issues.</p>
    <p class="normal">Implementing a <a id="_idIndexMarker1148"/>CI pipeline ensures that new features follow the repository’s standards and don’t break existing functionality. The exact process repeats when we plan to merge the staging branch into the production one. We open a PR, and the CI pipeline is automatically executed before merging the staging branch into production.</p>
    <figure class="mediaobject"><img src="img/B31105_11_14.png" alt=""/></figure>
    <p class="packt_figref">Figure 11.14: CI/CD pipelines flow</p>
    <p class="normal">The <a id="_idIndexMarker1149"/>CD pipeline runs after the branch is merged. For example, after the feature branch is merged into staging, the CD pipeline takes the code from the staging branch, builds a new Docker image, and pushes it to the AWS ECR Docker repository. When running future pipeline runs in the staging environment, it will use the latest Docker image that was built by the CD pipeline. The exact process happens between staging and production. Still, the key difference is that the staging environment exists as an experimental place where the QA team and stakeholders can further manually test the new feature along with what is automatically tested in the CI pipeline.</p>
    <div><p class="normal">In our repository, we used only a main branch, which reflects production, and feature branches to push new work. We did this to keep things simple, but the same principles apply. To extend the flow, you must create a staging branch and add it to the CD pipeline.</p>
    </div>
    <h3 id="_idParaDest-285" class="heading-3">More on formatting errors</h3>
    <p class="normal">Formatting errors relate to the style<a id="_idIndexMarker1150"/> and structure of your code, ensuring that it adheres to a consistent visual layout. This can include the placement of spaces, indentation, line length, and other stylistic elements.</p>
    <p class="normal">The main purpose of formatting is to make your code more readable and maintainable. Consistent formatting helps teams work together more effectively, as the code looks uniform, regardless of who wrote it. Examples<a id="_idIndexMarker1151"/> of formatting errors are:</p>
    <ul>
      <li class="bulletList">Incorrect indentation (e.g., mixing spaces and tabs)</li>
      <li class="bulletList">Lines that are too long (e.g., exceeding <code class="inlineCode">79</code> or <code class="inlineCode">88</code> characters, depending on your style guide)</li>
      <li class="bulletList">Missing or extra spaces around operators or after commas</li>
    </ul>
    <h3 id="_idParaDest-286" class="heading-3">More on linting errors</h3>
    <p class="normal">Linting errors relate to potential<a id="_idIndexMarker1152"/> issues in your code that could lead to bugs, inefficiencies, or non-adherence to coding standards beyond just style. Linting checks often involve static analysis of the code to catch things like unused variables, undefined names, or questionable practices.</p>
    <p class="normal">Linting’s main goal is to catch potential errors or bad practices early in the development process, improving code quality and reducing the likelihood of bugs. Examples of linting <a id="_idIndexMarker1153"/>errors are:</p>
    <ul>
      <li class="bulletList">Unused imports or variables</li>
      <li class="bulletList">Undefined variables or functions are being used</li>
      <li class="bulletList">Potentially <a id="_idIndexMarker1154"/>dangerous code (e.g., using <code class="inlineCode">==</code> instead of <code class="inlineCode">is</code> for checking against <code class="inlineCode">None</code>)</li>
    </ul>
    <p class="normal">We use Ruff, a versatile tool for formatting and linting. It incorporates checks for common formatting issues and PEP 8 compliance, as well as deeper linting checks for potential errors and code quality problems. Also, it is written in Rust, making it fast for big codebases.</p>
    <p class="normal">Before implementing what we’ve explained above, let’s examine the core principles of GitHub Actions.</p>
    <h2 id="_idParaDest-287" class="heading-2">Quick overview of GitHub Actions</h2>
    <p class="normal">GitHub Actions is <a id="_idIndexMarker1155"/>a CI/CD platform provided by GitHub that allows developers to automate their workflows directly within a GitHub repository. It enables users to build, test, and deploy their code directly from GitHub by defining workflows in YAML files. Since it’s part of GitHub, it works seamlessly with repositories, issues, PRs, and other GitHub features. Here are the key components you should know about:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Workflows:</strong> A workflow<a id="_idIndexMarker1156"/> is an automated process defined in a YAML file located in your repository’s <code class="inlineCode">.github/workflows directory</code>. It specifies what should happen (e.g., <code class="inlineCode">build</code>, <code class="inlineCode">test</code>, and <code class="inlineCode">deploy</code>) and when (e.g., on push, on PR).</li>
      <li class="bulletList"><strong class="keyWord">Jobs:</strong> Workflows are made up of<a id="_idIndexMarker1157"/> jobs, which are groups of steps that execute on the same runner. Each job runs in its own virtual environment.</li>
      <li class="bulletList"><strong class="keyWord">Steps:</strong> Jobs are made up of multiple <a id="_idIndexMarker1158"/>independent steps, which can be actions or shell commands.</li>
      <li class="bulletList"><strong class="keyWord">Actions:</strong> Actions<a id="_idIndexMarker1159"/> are reusable commands or scripts. You can use pre-built actions from GitHub Marketplace or create your own. You can think of them as Python functions.</li>
      <li class="bulletList"><strong class="keyWord">Runners:</strong> Runners<a id="_idIndexMarker1160"/> are the servers that run your jobs. GitHub provides hosted runners (Linux, Windows, macOS), or you can even self-host your runners.</li>
    </ul>
    <p class="normal">A workflow<a id="_idIndexMarker1161"/> is described using YAML syntax. For example, a simple workflow that clones the current GitHub repository and installs Python 3.11 on an Ubuntu machine looks like this:</p>
    <pre class="programlisting code"><code class="hljs-code">name: Example
on: [push]
jobs:
  build:
    runs-on: ubuntu-latest
    steps:
         - name: Checkout
           uses: actions/checkout@v3
         - name: Setup Python
           uses: actions/setup-python@v3
           with:
               python-version: "3.11"
</code></pre>
    <p class="normal">The workflows are triggered by events like <code class="inlineCode">push</code>, <code class="inlineCode">pull_request</code>, or <code class="inlineCode">schedule</code>. For example, you might trigger a workflow every time code is pushed to a specific branch. Now that we understand how GitHub Actions works, let’s look at the LLM Twin’s CI pipeline.</p>
    <h2 id="_idParaDest-288" class="heading-2">The CI pipeline</h2>
    <p class="normal">The LLM Twin’s <a id="_idIndexMarker1162"/>CI pipeline is split into two jobs:</p>
    <ul>
      <li class="bulletList">A <strong class="keyWord">QA job</strong> that <a id="_idIndexMarker1163"/>looks for formatting <a id="_idIndexMarker1164"/>and linting errors using Ruff. Also, it runs a <code class="inlineCode">gitleaks</code> step to scan for leaked secrets throughout our repository.</li>
      <li class="bulletList">A <strong class="keyWord">test job</strong> that<a id="_idIndexMarker1165"/> runs all our automatic <a id="_idIndexMarker1166"/>tests using <code class="inlineCode">Pytest</code>. In our use case, we implemented just a dummy test to showcase the CI pipeline, but using the structure from this book, you can easily extend it with real tests for your use case.</li>
    </ul>
    <h3 id="_idParaDest-289" class="heading-3">GitHub Actions CI YAML file</h3>
    <pre>pull_request</code> event occurs. Hence, the CI workflow will automatically run whenever a PR is opened, synchronized, or reopened.</pre>
    <pre class="programlisting code"><code class="hljs-code">name: CI
on:
  pull_request:
</code></pre>
    <p class="normal">The <code class="inlineCode">concurrency</code> section ensures that only one instance of this workflow runs for a given reference (like a branch) at any given time. The <code class="inlineCode">group</code> field is defined using GitHub’s expression syntax to create a unique group name based on the workflow and the reference. The <code class="inlineCode">cancel-in-progress: true</code> line ensures that if a new workflow run is triggered before the previous one finishes, the previous run is canceled. This is particularly useful to prevent redundant executions of the same workflow.</p>
    <pre class="programlisting code"><code class="hljs-code">concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true
</code></pre>
    <p class="normal">The workflow defines two separate jobs: <code class="inlineCode">qa</code> and <code class="inlineCode">test</code>. Each job runs on the latest version of Ubuntu, specified by <code class="inlineCode">runs-on: ubuntu-latest</code>.</p>
    <p class="normal"><strong class="screenText">The first job</strong>, named <code class="inlineCode">QA</code>, is responsible for quality assurance tasks like code checks and formatting verification. Within the <code class="inlineCode">qa</code> job, the first step is to check out the repository’s code using the <code class="inlineCode">actions/checkout@v3</code> action. This step is necessary to ensure that the job has access to the code that needs to be analyzed.</p>
    <pre class="programlisting code"><code class="hljs-code">jobs:
  qa:
    name: QA
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v3
</code></pre>
    <p class="normal">The next step is to set up the Python environment. This is done using the <code class="inlineCode">actions/setup-python@v3</code> action, with the Python version specified as <code class="inlineCode">"3.11"</code>. This step ensures that the subsequent steps in the job will run in the correct Python environment.</p>
    <pre class="programlisting code"><code class="hljs-code">      - name: Setup Python
        uses: actions/setup-python@v3
        with:
          python-version: "3.11"
</code></pre>
    <p class="normal">The workflow then<a id="_idIndexMarker1168"/> installs Poetry using the <code class="inlineCode">abatilo/actions-poetry@v2</code> action, specifying the version of Poetry as <code class="inlineCode">1.8.3</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">      - name: Install poetry
        uses: abatilo/actions-poetry@v2
        with:
          poetry-version: 1.8.3
</code></pre>
    <p class="normal">Once Poetry is set up, the workflow installs the project’s development dependencies using the <code class="inlineCode">poetry install --only dev</code> command. Additionally, the workflow adds the <code class="inlineCode">poethepoet</code> plugin for Poetry, which will be used to run predefined tasks more conveniently within the project.</p>
    <pre class="programlisting code"><code class="hljs-code">      - name: Install packages
        run: |
          poetry install --only dev
          poetry self add 'poethepoet[poetry_plugin]'
</code></pre>
    <p class="normal">The <code class="inlineCode">qa</code> job then runs several quality checks on the code. The first check uses a tool called <code class="inlineCode">gitleaks</code> to scan for secrets in the codebase, ensuring that no sensitive information is accidentally committed:</p>
    <pre class="programlisting code"><code class="hljs-code">      - name: gitleaks check
        run: poetry poe gitleaks-check
</code></pre>
    <p class="normal">Following the <code class="inlineCode">gitleaks</code> check, the workflow runs a linting process to enforce coding standards and best practices in the Python code. This is achieved through the <code class="inlineCode">poetry poe lint-check</code> command, which uses Ruff under the hood.</p>
    <pre class="programlisting code"><code class="hljs-code">      - name: Lint check [Python]
        run: poetry poe lint-check
</code></pre>
    <p class="normal">The last step in the <code class="inlineCode">qa</code> job is a format check, which ensures that the Python code is properly formatted according to the project’s style guidelines. This is done using the <code class="inlineCode">poetry poe format-check</code> command, which uses Ruff under the hood.</p>
    <pre class="programlisting code"><code class="hljs-code">      - name: Format check [Python]
        run: poetry poe format-check
</code></pre>
    <p class="normal">The <strong class="screenText">second job</strong> defined <a id="_idIndexMarker1169"/>in the workflow is the <code class="inlineCode">test</code> job, which also runs on the latest version of Ubuntu. Like the <code class="inlineCode">qa</code> job, it starts by checking out the code from the repository and installing Python 3.11 and Poetry 1.8.3.</p>
    <pre class="programlisting code"><code class="hljs-code">  test:
    name: Test
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v3
      …
</code></pre>
    <p class="normal">After setting up the system dependencies, the <code class="inlineCode">test</code> job installs all the project’s dependencies with the <code class="inlineCode">poetry install</code> command. As we want to run the tests, this time, we need to install all the dependencies that are required to run the application.</p>
    <pre class="programlisting code"><code class="hljs-code">      - name: Install packages
        run: |
          poetry install –-without aws
          poetry self add 'poethepoet[poetry_plugin]'
</code></pre>
    <p class="normal">Finally, the <code class="inlineCode">test</code> job runs the project’s tests using the <code class="inlineCode">poetry poe test</code> command. This step ensures that all tests are executed and provides feedback on whether the current code changes break any functionality.</p>
    <pre class="programlisting code"><code class="hljs-code">      - name: Run tests
        run: |
          echo "Running tests..."
          poetry poe test
</code></pre>
    <p class="normal">If any of the steps from <a id="_idIndexMarker1170"/>the QA or test jobs fail, the GitHub Actions workflow will fail, resulting in the PR not being able to be merged until the issue is fixed. By taking this approach, we ensure that all the new features added to the main branches respect the standard of the project and that it doesn’t break existing functionality through automated tests.</p>
    <p class="normal"><em class="italic">Figure 11.15</em> shows the CI pipeline in the <strong class="screenText">Actions</strong> tab of the GitHub repository. It was run after a commit with the message <strong class="screenText">feat: Add Docker image and CD pipeline</strong> and ran the two jobs described above, QA and Test.</p>
    <figure class="mediaobject"><img src="img/B31105_11_15.png" alt=""/></figure>
    <p class="packt_figref">Figure 11.15: GitHub Actions CI pipeline run example</p>
    <h2 id="_idParaDest-290" class="heading-2">The CD pipeline</h2>
    <p class="normal">The CD <a id="_idIndexMarker1171"/>pipeline will automate the Docker steps we manually performed in the <strong class="screenText">Deploying the LLM Twin’s pipelines to the cloud</strong> section, which are:</p>
    <ul>
      <li class="bulletList">Set up Docker.</li>
      <li class="bulletList">Log in to AWS.</li>
      <li class="bulletList">Build the Docker image.</li>
      <li class="bulletList">Push the Docker image to AWS ECR.</li>
    </ul>
    <p class="normal">With that in mind, let’s look at the GitHub Actions YAML file, which sits under <code class="inlineCode">.github/workflows/cd.yaml</code>. It begins by naming the workflow <code class="inlineCode">CD</code> and specifying the trigger for this workflow. The trigger is any push to the repository’s main branch. This workflow will automatically run when new code is pushed to the main branch, usually when a PR is merged into the main branch. The <code class="inlineCode">on.push</code> configuration sets up the trigger:</p>
    <pre class="programlisting code"><code class="hljs-code">name: CD
on:
  push:
    branches:
      - main
</code></pre>
    <p class="normal">The workflow then defines a single job named <code class="inlineCode">Build &amp; Push Docker Image</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">jobs:
  build:
    name: Build &amp; Push Docker Image
    runs-on: ubuntu-latest
</code></pre>
    <p class="normal">The first step within the job is to check out the repository’s code.</p>
    <pre class="programlisting code"><code class="hljs-code">steps:
  - name: Checkout Code
    uses: actions/checkout@v3
</code></pre>
    <p class="normal">After checking<a id="_idIndexMarker1172"/> out the code, the workflow sets up docker buildx, a Docker CLI plugin that extends Docker’s build capabilities with features like multi-platform builds and cache import/export:</p>
    <pre class="programlisting code"><code class="hljs-code">- name: Set up Docker Buildx
  uses: docker/setup-buildx-action@v3
</code></pre>
    <p class="normal">The next step involves configuring the AWS credentials. This step is crucial for interacting with AWS services, such as<a id="_idIndexMarker1173"/> Amazon <strong class="keyWord">Elastic Container Registry</strong> (<strong class="keyWord">ECR</strong>), where the Docker images will be pushed. The AWS access key, secret access key, and region are securely retrieved from the repository’s secrets to authenticate the workflow with AWS. This ensures the workflow has the necessary permissions to push Docker images to the ECR repository. We will show you how to configure these secrets after wrapping up with the YAML file:</p>
    <pre class="programlisting code"><code class="hljs-code">- name: Configure AWS credentials
  uses: aws-actions/configure-aws-credentials@v1
  with:
    aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
    aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
    aws-region: ${{ secrets.AWS_REGION }}
</code></pre>
    <p class="normal">Once the AWS credentials are configured, the workflow logs in to Amazon ECR. This step is essential for authenticating the Docker CLI with the ECR registry, allowing subsequent steps to push images to the registry:</p>
    <pre class="programlisting code"><code class="hljs-code">- name: Login to Amazon ECR
  id: login-ecr
  uses: aws-actions/amazon-ecr-login@v1
</code></pre>
    <p class="normal">The final step in the workflow involves building the Docker image and pushing it to the Amazon ECR repository. This is accomplished using the <code class="inlineCode">docker/build-push-action@v6</code> action. The <code class="inlineCode">context</code> specifies the build context, which is typically the repository’s root directory. The <code class="inlineCode">file</code> option points to the <code class="inlineCode">Dockerfile</code>, which defines how the image should be built. The <code class="inlineCode">tags</code> section assigns tags to the image, including<a id="_idIndexMarker1174"/> the specific commit SHA and the <code class="inlineCode">latest</code> tag, which is a common practice for identifying the most recent version of the image. The <code class="inlineCode">push</code> option is set to <code class="inlineCode">true</code>, meaning the image will be uploaded to ECR after it is built:</p>
    <pre class="programlisting code"><code class="hljs-code">- name: Build images &amp; push to ECR
  id: build-image
  uses: docker/build-push-action@v6
  with:
    context: .
    file: ./Dockerfile
    tags: |
      ${{ steps.login-ecr.outputs.registry }}/${{ secrets.AWS_ECR_NAME }}:${{ github.sha }}
      ${{ steps.login-ecr.outputs.registry }}/${{ secrets.AWS_ECR_NAME }}:latest
    push: true
</code></pre>
    <p class="normal">To conclude, the CD pipeline authenticates to AWS, builds the Docker image, and pushes it to AWS ECR. The Docker image is pushed with <code class="inlineCode">latest</code> and the commit’s SHA tag. By doing so, we can always use the latest image and point to the commit of the code from which the image was generated.</p>
    <p class="normal">Also, in our code, we have only a main branch, which reflects our production environment. But you, as a developer, have the power to extend this functionality with a staging and dev environment. You just have to add the name of the branches in the <code class="inlineCode">on.push.branches</code> configuration at the beginning of the YAML file.</p>
    <p class="normal">In <em class="italic">Figure 11.16</em>, you can<a id="_idIndexMarker1175"/> observe how the CD pipeline looks after a PR is merged into the production branch. As seen before, we only have the <strong class="screenText">Build &amp; Push Docker Image</strong> job<strong class="screenText"> </strong>here.</p>
    <figure class="mediaobject"><img src="img/B31105_11_16.png" alt=""/></figure>
    <p class="packt_figref">Figure 11.16: GitHub Actions CD pipeline run example</p>
    <p class="normal">The last step in setting up the CI/CD pipeline is to test it and see how it works.</p>
    <h2 id="_idParaDest-291" class="heading-2">Test out the CI/CD pipeline</h2>
    <p class="normal">To test the <a id="_idIndexMarker1176"/>CI/CD pipelines yourself, you must fork the LLM-Engineering repository to have full <em class="italic">write</em> access to the GitHub repository. Here is the official tutorial on how to fork a GitHub project: <a href="https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/fork-a-repo">https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/fork-a-repo</a></p>
    <p class="normal">The last step is to set up a few secrets that will allow the CD pipeline to log in to AWS and point to the right ECR resource. To do so, go to the <strong class="screenText">Settings</strong> tab at the top of the forked repository in GitHub. In the left panel, in the <strong class="screenText">Security</strong> section, click on the <strong class="screenText">Secrets and Variables</strong> toggle and, finally, on <strong class="screenText">Actions</strong>. Then, on the <strong class="screenText">Secrets</strong> tab, create four repository secrets, as shown in <em class="italic">Figure 11.17</em>. These secrets will be securely stored and accessible only by the GitHub Actions CD pipeline.</p>
    <p class="normal">The <code class="inlineCode">AWS_ACCESS_KEY_ID</code> and <code class="inlineCode">AWS_SECRET_ACCESS_KEY</code> are the AWS credentials you used across the book. In <em class="italic">Chapter 2</em>, you see how to create them. The <code class="inlineCode">AWS_REGION</code> (e.g., <code class="inlineCode">eu-central-1</code>) and <code class="inlineCode">AWS_ECR_NAME</code> are the same ones used in the <strong class="screenText">Deploying the LLM Twin’s pipelines</strong> to the cloud section.</p>
    <div><p class="normal">For the <code class="inlineCode">AWS_ECR_NAME</code>, you should configure only the name of the repository (e.g., <code class="inlineCode">zenml-vrsopg</code>) and not the full URI (e.g., <a href="https://992382797823.dkr.ecr.eu-central-1.amazonaws.com/zenml-vrsopg">992382797823.dkr.ecr.eu-central-1.amazonaws.com/zenml-vrsopg</a>), as seen in the image below:</p>
 
 <img src="img/B31105_11_17.png" alt=""/>
    <p class="packt_figref">Figure 11.17: Configuring only repository name</p>
 
   
    </div>
    
    <p class="normal">To trigger the CI pipeline, create a feature branch, modify the code or documentation, and create a PR to the main branch. To trigger the CD pipeline, merge the PR into the main branch. </p>
    <p class="normal">After the CD GitHub Actions are complete, check the ECR repository to see whether the Docker image was pushed successfully.</p>
    <figure class="mediaobject"><img src="img/B31105_11_18.png" alt=""/></figure>
    <p class="packt_figref">Figure 11.18: GitHub Actions secrets</p>
    <p class="normal">If you need more details on how to set up GitHub Actions secrets, we recommend checking out their official documentation: <a href="https://docs.github.com/en/actions/security-for-github-actions/security-guides/using-secrets-in-github-actions">https://docs.github.com/en/actions/security-for-github-actions/security-guides/using-secrets-in-github-actions</a></p>
    <h2 id="_idParaDest-292" class="heading-2">The CT pipeline</h2>
    <p class="normal">To <a id="_idIndexMarker1177"/>implement the CT pipeline, we will leverage ZenML. Once ZenML (or other orchestrators such as Metaflow, Dagster, or Airflow) orchestrates all your pipelines and your infrastructure is deployed, you are very close to reaching CT.</p>
    <p class="normal">Remember the core difference between the CI/CD and CT pipelines. The CI/CD pipeline takes care of testing, building, and deploying your code—a dimension that any software program has. The CT pipeline leverages the code managed by the CI/CD pipeline to automate your data, training, and model-serving process, where the data and model dimensions are present only in the AI world.</p>
    <p class="normal">Before diving into the <a id="_idIndexMarker1178"/>implementation, we want to highlight two design choices that made reaching CT simple:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">The FTI architecture:</strong> A modular system with clear interfaces and components made it easy to capture the relationship between the pipelines and automate them.</li>
      <li class="bulletList"><strong class="keyWord">Starting with an orchestrator since day 0:</strong> We started with ZenML at the beginning of the project’s development. Early on, we only used it locally. But it acted as an entry point for our pipelines and a way to monitor their execution. Doing so forced us to decouple each pipeline and transfer the communication between them solely through various types of data storage, such as the data warehouse, feature store, or artifact store. As we have leveraged ZenML since day 0, we got rid of implementing a tedious CLI to configure our application. Instead, we did it directly through YAML configuration files out of the box.</li>
    </ul>
    <p class="normal">In <em class="italic">Figure 11.19</em>, we can see all the pipelines that we have to chain together to fully automate our training and deployment. The pipelines aren’t new; they aggregate everything we’ve covered throughout this book. Thus, at this point, we will treat them as black boxes that interact with each other.</p>
    <figure class="mediaobject"><img src="img/B31105_11_19.png" alt=""/></figure>
    <p class="packt_figref">Figure 11.19: CT pipeline</p>
    <p class="normal">For the LLM Twin’s <a id="_idIndexMarker1179"/>CT pipeline, we have to discuss the initial trigger that starts the pipelines and how the pipelines are triggered by each other.</p>
    <h3 id="_idParaDest-293" class="heading-3">Initial triggers</h3>
    <p class="normal">As illustrated in <em class="italic">Figure 11.18</em>, we initially want to trigger the data collection pipeline. Usually, the triggers can be of three types:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Manual triggers:</strong> Done<a id="_idIndexMarker1180"/> through the CLI or the orchestrator’s dashboard, in our case, through the ZenML dashboard. Manual triggers <a id="_idIndexMarker1181"/>are still extremely powerful tools, as you need just one action to start the whole ML system, from data gathering to deployment, instead of fiddling with dozens of scripts that you might configure wrong or run in an invalid order.</li>
      <li class="bulletList"><strong class="keyWord">REST API triggers:</strong> You can <a id="_idIndexMarker1182"/>call a pipeline by an <a id="_idIndexMarker1183"/>HTTP request. This is extremely useful when integrating your ML pipelines with other components. For example, you can have a watcher constantly looking for new articles. It triggers the ML logic using this REST API trigger when it finds some. To find more details on this feature, check out this tutorial on ZenML’s documentation: <a href="https://docs.zenml.io/v/docs/how-to/trigger-pipelines/trigger-a-pipeline-from-rest-api">https://docs.zenml.io/v/docs/how-to/trigger-pipelines/trigger-a-pipeline-from-rest-api</a>.</li>
      <li class="bulletList"><strong class="keyWord">Scheduled triggers:</strong> Another <a id="_idIndexMarker1184"/>common approach is to<a id="_idIndexMarker1185"/> schedule your pipeline to run constantly on a fixed interval. For example, depending on your use case, you can schedule your pipeline to run daily, hourly, or every minute. Most of the orchestrators, ZenML included, provide a cron expression interface where you can define your execution frequency. In the following example from ZenML, the pipeline is scheduled every hour:
        <pre class="programlisting code-one"><code class="hljs-code"> Schedule(cron_expression="* * 1 * *")
</code></pre>
      </li>
    </ul>
    <p class="normal">We chose a manual trigger for our LLM Twin use case as we don’t have other components to leverage the REST API triggers. Also, as the datasets are generated from a list of static links defined in the ZenML configs, running them on a schedule doesn’t make sense as they would always yield the same results.</p>
    <p class="normal">But a possible next step for the project is to implement a watcher that monitors for new articles. When it finds any, it generates a new config and triggers the pipelines through the REST API. Another option is implementing the watcher as an additional pipeline and leveraging the schedule triggers to look daily for new data. If it finds any, it executes the whole ML system; otherwise, it stops.</p>
    <p class="normal">The conclusion is that once you can manually trigger all your ML pipelines through a single command, you can quickly adapt it to more advanced and complex scenarios.</p>
    <h3 id="_idParaDest-294" class="heading-3">Trigger downstream pipelines</h3>
    <p class="normal">To keep things<a id="_idIndexMarker1186"/> simple, we sequentially chained all the pipelines. More concretely, when the data collection pipeline has finished, it will trigger the feature pipeline. When the feature pipeline has been completed successfully, it triggers the dataset generation pipeline, and so on. You can make the logic more complex, like scheduling the generate instruct dataset pipeline to run daily, checking the amount of new data in the Qdrant vector DB, and starting only if it has enough new data. From this point, you can further tweak the system’s parameters and optimize them to reduce costs.</p>
    <p class="normal">To trigger all the pipelines in one go, we created one master pipeline that aggregates everything in one entry point:</p>
    <pre class="programlisting code"><code class="hljs-code">@pipeline
def end_to_end_data(
    author_links: list[dict[str, str | list[str]]], … # Other paramaters…
) -&gt; None:
    wait_for_ids = []
    for author_data in author_links:
        last_step_invocation_id = digital_data_etl(
            user_full_name=author_data["user_full_name"], links=author_data["links"]
        )
        wait_for_ids.append(last_step_invocation_id)
    author_full_names = [author_data["user_full_name"] for author_data in author_links]
    wait_for_ids = feature_engineering(author_full_names=author_full_names, wait_for=wait_for_ids)
    generate_instruct_datasets(…)
       training(…)
       deploy(…)
</code></pre>
    <p class="normal">To keep the function light, we added all the logic up to computing the features. But, as we suggested in the code snippet above, you can easily add the instruction dataset generation, training, and deploy logic to the parent pipeline to implement an end-to-end flow. By doing that, you can automate everything from data collection to deploying the model.</p>
    <p class="normal">To run the end-to-end <a id="_idIndexMarker1187"/>pipeline, use the following <code class="inlineCode">poe</code> command:</p>
    <pre class="programlisting con"><code class="hljs-con">poetry poe run-end-to-end-data-pipeline
</code></pre>
    <p class="normal">What we implemented is not the best approach, as it compresses all the steps into a single monolith pipeline (which we want to avoid), as illustrated in <em class="italic">Figure 11.20</em>. Usually, you want to keep each pipeline isolated and use triggers to start downstream pipelines. This makes the system easier to understand, debug, and monitor.</p>
    <figure class="mediaobject"><img src="img/B31105_11_20.png" alt=""/></figure>
    <p class="packt_figref">Figure 11.20: End-to-end pipeline illustrated in ZenML’s dashboard</p>
    <p class="normal">Unfortunately, the ZenML<a id="_idIndexMarker1188"/> cloud’s free trial has a limitation of a maximum of three pipelines. As we have more, we avoided that limitation by compressing all the steps into a single pipeline. But if you plan to host ZenML yourself or buy their license, they offer the possibility to independently trigger a pipeline from another pipeline, as you can see in the code snippet below where we triggered the feature engineering pipeline after the data collection ETL:</p>
    <pre class="programlisting code"><code class="hljs-code">from zenml import pipeline, step
@pipeline 
def digital_data_etl(user_full_name: str, links: list[str]) -&gt; str:
 user = get_or_create_user(user_full_name)
 crawl_links(user=user, links=links)
trigger_feature_engineering_pipeline(user)
@step 
def trigger_feature_engineering_pipeline(user):
run_config = PipelineRunConfiguration(…)
Client().trigger_pipeline("feature_engineering", run_configuration=run_config)
@pipeline
def feature_engineering(author_full_names: list[str]) -&gt; list[str]:
… # ZenML steps
</code></pre>
    <p class="normal">By taking this approach, each pipeline will have its independent run, where one pipeline sequentially triggers the next one, as described at the beginning of this section. Note that this feature is not unique to ZenML but is common in orchestrator tools. The principles we have learned so far hold. Only how we interact with the tool changes.</p>
    <h2 id="_idParaDest-295" class="heading-2">Prompt monitoring</h2>
    <p class="normal">We will use Opik (from Comet ML) to monitor<a id="_idIndexMarker1189"/> our prompts. But remember from the <em class="italic">LLMOps</em> section earlier in this chapter that we are not interested only in the input prompt and generated answer. </p>
    <p class="normal">We want to log the entire trace from the user’s input until the final result is available. Before diving into the LLM Twin use case, let’s look at a simpler example:</p>
    <pre class="programlisting code"><code class="hljs-code">from opik import track
import openai
from opik.integrations.openai import track_openai
openai_client = track_openai(openai.OpenAI())
@track
def preprocess_input(text: str) -&gt; str:
    return text.strip().lower()
@track
def generate_response(prompt: str) -&gt; str:
    response = openai_client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content
@track
def postprocess_output(response: str) -&gt; str:
    return response.capitalize()
@track(name="llm_chain")
def llm_chain(input_text: str) -&gt; str:
    preprocessed = preprocess_input(input_text)
    generated = generate_response(preprocessed)
    postprocessed = postprocess_output(generated)
    return postprocessed
result = llm_chain("Hello, do you enjoy reading the book?")
</code>
llm_chain()</code> main function, which takes the initial input as a parameter and returns the final result. </pre>
    <p class="normal">Then, you have preprocessing and <a id="_idIndexMarker1190"/>postprocessing functions surrounding the actual LLM call. Using the <code class="inlineCode">@track()</code> decorator, we log the input and output of each function, which will ultimately be aggregated into a single trace. By doing so, we will have access to the initial input text, the generated answer, and all the intermediary steps required to debug any potential issues using Opik’s dashboard.</p>
    <pre>update()</code> method, where you can tag your trace or add any other metadata, such as the number of input tokens, through a Python dictionary:</pre>
    <pre class="programlisting code"><code class="hljs-code">from opik import track, opik_context
@track
def llm_chain(input_text):
    # LLM chain code
    # ...
    opik_context.update_current_trace(
tags=["inference_pipeline"],
metadata={
 "num_tokens": compute_num_tokens(…)
},
feedback_scores=[
{
 "name": "user_feedback",
 "value": 1.0,
 "reason": "The response was valuable and correct."
},
{
 "name": "llm_judge_score",
 "value": compute_llm_judge_score(…),
 "reason": "Computing runtime metrics using an LLM Judge."
}
)
</code></pre>
    <p class="normal">You can expand on this idea and log various feedback scores. The most common is asking the user if the generated answer is valuable and correct. Another option is to compute various metrics automatically through heuristics or LLM judges.</p>
    <p class="normal">Finally, let’s see how to add prompt monitoring to our LLM Twin project. First, look at <em class="italic">Figure 11.21</em> and remember our model-serving architecture. We have two microservices, the<a id="_idIndexMarker1192"/> LLM and business microservices. The LLM microservice has a narrow scope, as it only takes as input a prompt that already contains the user’s input and context and returns an answer that is usually post-processed. Thus, the business microservice is the right place to implement the monitoring pipeline, as it coordinates the end-to-end flow. More concretely, Opik implementation will be in the FastAPI server developed in <em class="italic">Chapter 10</em>.</p>
    <figure class="mediaobject"><img src="img/B31105_11_21.png" alt=""/></figure>
    <p class="packt_figref">Figure 11.21: Inference pipeline serving architecture</p>
    <p class="normal">As our<a id="_idIndexMarker1193"/> implementation is already modular, using Opik makes it straightforward to log an end-to-end trace of a user’s request:</p>
    <pre class="programlisting code"><code class="hljs-code">from opik import track
@track
def call_llm_service(query: str, context: str | None) -&gt; str:
    llm = LLMInferenceSagemakerEndpoint(…)
    answer = InferenceExecutor(llm, query, context).execute()
    return answer
@track
def rag(query: str) -&gt; str:
    retriever = ContextRetriever()
    documents = retriever.search(query, k=3 * 3)
    context = EmbeddedChunk.to_context(documents)
    answer = call_llm_service(query, context)
    return answer
</code></pre>
    <p class="normal">The <code class="inlineCode">rag()</code> function represents your application’s entry point. All the other processing steps take place in the <code class="inlineCode">ContextRetriever</code> and <code class="inlineCode">InferenceExector</code> classes. Also, by decorating the <code class="inlineCode">call_llm_service()</code> function, we can clearly capture the prompt sent to the LLM and its response.</p>
    <p class="normal">To add more <a id="_idIndexMarker1194"/>granularity to our trace, we can further decorate other functions containing pre- or post-processing steps, such as the <code class="inlineCode">ContextRetriever</code> search function:</p>
    <pre class="programlisting code"><code class="hljs-code">class ContextRetriever:
     …
   
    @track
    
    def search(
        self,
        query: str,
        k: int = 3,
        expand_to_n_queries: int = 3,
    ) -&gt; list:
        query_model = Query.from_str(query)
        query_model = self._metadata_extractor.generate(query_model)
        … # Rest of the implementation
</code></pre>
    <p class="normal">Or even go further to the retrieval optimization methods, such as the self-query metadata extractor, to add more granularity:</p>
    <pre class="programlisting code"><code class="hljs-code">class SelfQuery:
 
    @track
    def generate(self, query: str) -&gt; str:
        …
        return enhanced_query
</code></pre>
    <p class="normal">The developer is <a id="_idIndexMarker1195"/>responsible for deciding how much granularity the application needs for proper debugging and analysis. As having detailed monitoring is healthy, monitoring everything can be dangerous as it adds too much noise and makes manually understanding the traces difficult. You must find the right balance. A good rule of thumb is tracing the most critical functions, such as <code class="inlineCode">rag()</code> and <code class="inlineCode">call_llm_service()</code>, and gradually adding more granularity when needed.</p>
    <p class="normal">The last step is to attach valuable metadata and tags to our traces. To do so, we will further enhance the <code class="inlineCode">rag()</code> function as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">@track
def rag(query: str) -&gt; str:
    retriever = ContextRetriever()
    documents = retriever.search(query, k=3 * 3)
    context = EmbeddedChunk.to_context(documents)
    answer, prompt = call_llm_service(query, context)
    trace = get_current_trace()
    trace.update(
tags=["rag"],
metadata={
 "model_id": settings.HF_MODEL_ID,
   "embedding_model_id": settings.TEXT_EMBEDDING_MODEL_ID,
   "temperature": settings.TEMPERATURE_INFERENCE,
   "prompt_tokens": compute_num_tokens(prompt),
   "total_tokens": compute_num_tokens(answer),
  
}
   )
    return answer
</code></pre>
    <p class="normal">There are <a id="_idIndexMarker1196"/>three main aspects that we should constantly monitor:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Model configuration:</strong> Here, we should consider both the LLM and other models used within the RAG layer. The most critical aspects of logging are the model IDs, but you can also capture other important information that significantly impacts the generation, such as the temperature.</li>
      <li class="bulletList"><strong class="keyWord">Total number of tokens:</strong> It’s critical to constantly analyze the statistics of the number of tokens generated by your input prompts and total tokens, as this significantly impacts your serving costs. For example, if the average of the total number of tokens generated suddenly increases, it’s a strong signal that you have a bug in your system that you should investigate.</li>
      <li class="bulletList"><strong class="keyWord">The duration of each step:</strong> Tracking the duration of each step within your trace is essential to finding bottlenecks within your system. If the latency of a specific request is abnormally large, you quickly have access to a report that helps you find the source of the problem.</li>
    </ul>
    <h2 id="_idParaDest-296" class="heading-2">Alerting</h2>
    <p class="normal">Using ZenML, you can quickly implement an alerting system<a id="_idIndexMarker1197"/> on any platform of your liking, such as email, Discord, or Slack. For example, you can add a callback in your training pipeline to trigger a notification when the pipeline fails or the training has finished successfully:</p>
    <pre class="programlisting code"><code class="hljs-code">from zenml import get_pipeline_context, pipeline
@pipeline(on_failure=notify_on_failure)
def training_pipeline(…):
…
notify_on_success()
</code></pre>
    <p class="normal">Implementing the <a id="_idIndexMarker1198"/>notification functions is straightforward. As seen in the code snippets below, you have to get the <code class="inlineCode">alerter</code> instance from your current stack, build the message as you see fit, and send it to your notification channel of choice:</p>
    <pre class="programlisting code"><code class="hljs-code">from zenml.client import Client
alerter = Client().active_stack.alerter
def notify_on_failure() -&gt; None:
        alerter.post(message=build_message(status="failed"))
@step(enable_cache=False)
def notify_on_success() -&gt; None:
        alerter.post(message=build_message(status="succeeded"))
</code></pre>
    <p class="normal">ZenML and most orchestrators simplify implementing an <code class="inlineCode">alerter</code>, as it’s a critical component in your MLOps/LLMOps infrastructure.</p>
    <h1 id="_idParaDest-297" class="heading-1">Summary</h1>
    <p class="normal">In this chapter, we laid down the foundations with a theoretical section on DevOps. Then, we moved on to MLOps and its core components and principles. Finally, we presented how LLMOps differs from MLOps by introducing strategies such as prompt monitoring, guardrails, and human-in-the-loop feedback. Also, we briefly discussed why most companies would avoid training LLMs from scratch but choose to optimize them for their use case through prompt engineering or fine-tuning. At the end of the theoretical portion of the chapter, we learned what a CI/CD/CT pipeline is, the three core dimensions of an ML application (code, data, model), and that, after deployment, it is more critical than ever to implement a monitoring and alerting layer due to model degradation.</p>
    <p class="normal">Next, we learned how to deploy the LLM Twin’s pipeline to the cloud. We understood the infrastructure and went step by step through deploying MongoDB, Qdrant, the ZenML cloud, and all the necessary AWS resources to sustain the application. Finally, we learned how to Dockerize our application and push our Docker image to AWS ECR, which will be used to execute the application on top of AWS SageMaker.</p>
    <p class="normal">The final step was to add LLMOps to our LLM Twin project. We began by implementing a CI/CD pipeline with GitHub Actions. Then, we looked at our CT strategy by leveraging ZenML. </p>
    <p class="normal">Finally, we saw how to implement a monitoring pipeline using Opik from Comet ML and an alerting system using ZenML. These are the fundamental pillars in adding MLOps and LLMOps to any LLM-based application.</p>
    <p class="normal">The framework we learned about throughout the book can quickly be extrapolated to other LLM applications. Even if we used the LLM Twin use case as an example, most of the strategies applied can be adapted to other projects. Thus, we can get an entirely new application by changing the data and making minor tweaks to the code. Data is the new oil, remember? </p>
    <p class="normal">By finalizing this chapter, we’ve learned to build an end-to-end LLM application, starting with data collection and fine-tuning until deploying the LLM microservice and RAG service. Throughout this book, we aimed to provide a thought framework to help you build and solve real-world problems in the GenAI landscape. Now that you have it, we wish you good luck in your journey and happy building!</p>
    <h1 id="_idParaDest-298" class="heading-1">References</h1>
    <ul>
      <li class="bulletList">GitLab. (2023, January 25). <em class="italic">What is DevOps? | GitLab</em>. GitLab. <a href="https://about.gitlab.com/topics/devops/">https://about.gitlab.com/topics/devops/</a></li>
      <li class="bulletList">Huyen, C. (2024, July 25). Building a generative AI platform. <em class="italic">Chip Huyen</em>. <a href="https://huyenchip.com/2024/07/25/genai-platform.html">https://huyenchip.com/2024/07/25/genai-platform.html</a></li>
      <li class="bulletList"><em class="italic">Lightricks customer story: Building a recommendation engine from scratch</em>. (n.d.). <a href="https://www.qwak.com/academy/lightricks-customer-story-building-a-recommendation-engine-from-scratch">https://www.qwak.com/academy/lightricks-customer-story-building-a-recommendation-engine-from-scratch</a></li>
      <li class="bulletList"><em class="italic">What LLMOps</em>. (n.d.). Google Cloud. <a href="https://cloud.google.com/discover/what-is-llmops?hl=en">https://cloud.google.com/discover/what-is-llmops?hl=en</a></li>
      <li class="bulletList"><em class="italic">MLOps: Continuous delivery and automation pipelines in machine learning</em>. (2024, August 28). Google Cloud. <a href="https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning#top_of_page">https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning#top_of_page</a></li>
      <li class="bulletList"><em class="italic">Ml-ops.org</em>. (2024a, July 5). <a href="https://ml-ops.org/content/mlops-principles">https://ml-ops.org/content/mlops-principles</a></li>
      <li class="bulletList"><em class="italic">Ml-ops.org</em>. (2024b, July 5). <a href="https://ml-ops.org/content/mlops-principles">https://ml-ops.org/content/mlops-principles</a></li>
      <li class="bulletList"><em class="italic">Ml-ops.org</em>. (2024c, July 5). <a href="https://ml-ops.org/content/motivation">https://ml-ops.org/content/motivation</a></li>
      <li class="bulletList">Mohandas, G. M. (2022a). Monitoring machine learning systems. <em class="italic">Made With ML</em>. <a href="https://madewithml.com/courses/mlops/monitoring/">https://madewithml.com/courses/mlops/monitoring/</a></li>
      <li class="bulletList">Mohandas, G. M. (2022b). Testing Machine Learning Systems: Code, Data and Models. <em class="italic">Made With ML</em>. <a href="https://madewithml.com/courses/mlops/testing/ ">https://madewithml.com/courses/mlops/testing/</a></li>
      <li class="bulletList">Preston-Werner, T. (n.d.). <em class="italic">Semantic Versioning 2.0.0</em>. Semantic Versioning. <a href="https://semver.org/">https://semver.org/</a></li>
      <li class="bulletList">Ribeiro, M. T., Wu, T., Guestrin, C., &amp; Singh, S. (2020, May 8). <em class="italic">Beyond Accuracy: Behavioral Testing of NLP models with CheckList</em>. arXiv.org. <a href="https://arxiv.org/abs/2005.04118 ">https://arxiv.org/abs/2005.04118</a></li>
      <li class="bulletList">Wandb. (2023, November 30). <em class="italic">Understanding LLMOps: Large Language Model Operations</em>. Weights &amp; Biases. <a href="https://wandb.ai/site/articles/understanding-llmops-large-language-model-operations/">https://wandb.ai/site/articles/understanding-llmops-large-language-model-operations/</a></li>
      <li class="bulletList">Zenml-Io. (n.d.). <em class="italic">GitHub—zenml-io/zenml-huggingface-sagemaker: An example MLOps overview of ZenML pipelines from a Hugging Face model repository to a deployed AWS SageMaker endpoint.</em> GitHub. <a href="https://github.com/zenml-io/zenml-huggingface-sagemaker/tree/main  ">https://github.com/zenml-io/zenml-huggingface-sagemaker/tree/main</a></li>
    </ul>
    <p class="normal"><a href="https://github.com/zenml-io/zenml-huggingface-sagemaker/tree/main  "/></p>
    <h1 id="_idParaDest-299" class="heading-1">Join our book’s Discord space</h1>
    <p class="normal">Join our community’s Discord space for discussions with the authors and other readers:</p>
    <p class="normal"><a href="https://packt.link/llmeng">https://packt.link/llmeng</a></p>
    <p class="normal"><img src="img/QR_Code79969828252392890.png" alt=""/></p>
  </div>
</body></html>