- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Advanced RAG Techniques for Information Retrieval and Augmentation
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 信息检索和增强的高级RAG技术
- en: 'In the previous chapter, we discussed RAG and how this paradigm has evolved
    to solve some shortcomings of LLMs. However, even naïve RAG (the basic form of
    this paradigm) is not without its challenges and problems. Naïve RAG consists
    of a few simple components: an embedder, a vector database for retrieval, and
    an LLM for generation. As mentioned in the previous chapter, naïve RAG involves
    a collection of text being embedded in a database; once a query from a user arrives,
    text chunks that are relevant to the query are searched for and provided to the
    LLM to generate a response. These components allow us to respond effectively to
    user queries; but as we shall see, we can add additional components to improve
    the system.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了RAG以及这种范式如何演变以解决LLMs的一些不足。然而，即使是简单的RAG（这种范式的原始形式）也并非没有其挑战和问题。简单的RAG由几个简单的组件组成：一个嵌入器、一个用于检索的向量数据库和一个用于生成的LLM。正如前一章所述，简单的RAG涉及将文本集合嵌入到数据库中；一旦用户查询到达，就会搜索与查询相关的文本块，并将其提供给LLM以生成响应。这些组件使我们能够有效地响应用户查询；但正如我们将看到的，我们可以添加额外的组件来改进系统。
- en: In this chapter, we will see how in advanced RAG, we can modify or improve the
    various steps in the pipeline (data ingestion, indexing, retrieval, and generation).
    This solves some of the problems of naïve RAG and gives us more control over the
    whole process. We will later see how the demand for more flexibility led to a
    further step forward (modular RAG). We will also discuss important aspects of
    RAG, especially when the system (a RAG base product) is being produced. For example,
    we will discuss the challenges when we have a large amount of data or users. Also,
    since these systems may contain sensitive data, we will discuss both robustness
    and privacy. Finally, although RAG is a popular system today, it is still relatively
    new. So, there are still unanswered questions and exciting prospects for its future.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将看到在高级RAG中，我们可以修改或改进管道中的各个步骤（数据摄入、索引、检索和生成）。这解决了简单RAG的一些问题，并使我们能够更好地控制整个过程。我们稍后将看到，对更多灵活性的需求导致了进一步的进步（模块化RAG）。我们还将讨论RAG的重要方面，特别是在系统（RAG基础产品）被生产时。例如，我们将讨论在数据量或用户数量很大时的挑战。此外，由于这些系统可能包含敏感数据，我们将讨论鲁棒性和隐私。最后，尽管RAG是当今流行的系统，但它仍然相对较新。因此，仍然存在一些未解决的问题和令人兴奋的前景。
- en: 'In this chapter, we’ll be covering the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下主题：
- en: Discussing naïve RAG issues
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讨论简单的RAG问题
- en: Exploring advanced RAG pipelines
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索高级RAG管道
- en: Modular RAG and integration with other systems
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模块化RAG和与其他系统的集成
- en: Implementing an advanced RAG pipeline
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现高级RAG管道
- en: Understanding the scalability and performance of RAG
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解RAG的可扩展性和性能
- en: Open questions
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开放性问题
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: Most of the code in this chapter can be run on a CPU, but it is preferable for
    it to be run on a GPU. The code is written in PyTorch and uses standard libraries
    for the most part (PyTorch, Hugging Face Transformers, LangChain, `chromadb`,
    `sentence-transformer`, `faiss-cpu`, and so on).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的大部分代码可以在CPU上运行，但最好在GPU上运行。代码是用PyTorch编写的，大部分使用标准库（PyTorch、Hugging Face Transformers、LangChain、`chromadb`、`sentence-transformer`、`faiss-cpu`等）。
- en: 'The code for this chapter can be found on GitHub: [https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr6](https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr6).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在GitHub上找到：[https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr6](https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr6)。
- en: Discussing naïve RAG issues
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 讨论简单的RAG问题
- en: In the previous chapter, we introduced RAG in its basic version (called naïve
    RAG). Although the basic version of RAG has gone a long way in solving some of
    the most pressing problems of LLMs, several issues remain. For industrial applications,
    in particular (as well as medical, legal, and financial), naïve RAG is not enough,
    and we need a more sophisticated pipeline. We will now explore the problems associated
    with naïve RAG, each of which is associated with a specific step in the pipeline
    (query handling, retrieval, and generation).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了RAG的基本版本（称为简单RAG）。尽管RAG的基本版本在解决LLMs的一些最紧迫问题方面已经取得了长足的进步，但仍然存在一些问题。特别是对于工业应用（以及医疗、法律和金融），简单的RAG是不够的，我们需要一个更复杂的管道。现在，我们将探讨与简单RAG相关的问题，每个问题都与管道中的特定步骤（查询处理、检索和生成）相关。
- en: '![Figure 6.1 – Summary of naïve RAG issues and identifying different steps
    in the pipeline where the issues can arise](img/B21257_06_01.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图6.1 – 简要概述了简单的RAG问题和识别管道中可能出现问题的不同步骤](img/B21257_06_01.jpg)'
- en: Figure 6.1 – Summary of naïve RAG issues and identifying different steps in
    the pipeline where the issues can arise
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 – 简要概述了简单的RAG问题以及识别管道中可能出现问题的不同步骤
- en: 'Let’s discuss these issues in detail:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细讨论这些问题：
- en: '**Retrieval challenges**: The phase of retrieval struggles with precision (retrieved
    chunks are misaligned) and recall (finding all relevant chunks). In addition,
    the knowledge base could be outdated. This could lead to either hallucinations
    or, depending on the prompt used, a response such as, “Sorry, I do not know the
    answer” or “The context does not allow the query to be answered.” This can also
    be derived from poor database indexing or the documents being of different types
    (PDF, HTML, text, and so on) and being treated incorrectly (chunking for all file
    types is an example).'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**检索挑战**：检索阶段在精确度（检索到的片段不匹配）和召回率（找到所有相关片段）方面存在困难。此外，知识库可能过时。这可能导致幻觉，或者根据使用的提示，可能得到“对不起，我不知道答案”或“上下文不允许回答查询”之类的响应。这也可以从数据库索引不良或文档类型不同（PDF、HTML、文本等）且处理不当（例如，对所有文件类型进行分块）中得出。'
- en: '**Missed top-rank documents**: Documents essential to answering the query may
    not be at the top of the list. By selecting the top *k* documents, we might select
    top chunks that are less relevant (or do not contain the answer) and not return
    the really relevant chunks to the LLM. The semantic representation capability
    of the embedding model may be weak (i.e., we chose an ineffective model because
    it was too small or not suitable for the domain of our documents).'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**遗漏顶级文档**：对于回答查询至关重要的文档可能不在列表的顶部。通过选择前 *k* 个文档，我们可能会选择不那么相关（或不含答案）的顶级片段，而没有返回真正相关的片段给LLM。嵌入模型的语义表示能力可能较弱（即，我们选择了一个无效的模型，因为它太小或不适合我们文档的领域）。'
- en: '**Relevant information not in context**: Documents with the answer are found
    but there are too many to fit in the LLM’s context. For example, the response
    might need several chunks, and these are too many for the context length of the
    model.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文中没有相关信息**：找到了包含答案的文档，但太多以至于无法适应LLM的上下文。例如，响应可能需要几个片段，而这些片段对于模型的上下文长度来说太多了。'
- en: '**Failed extraction**: The right context might be returned to the LLM, but
    it might not extract the right answer. Usually, this happens when there is too
    much noise or conflicting information in the context. The model might generate
    hallucinations despite having the answer in the prompt (contextual hallucinations).'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提取失败**：可能已经将正确的上下文返回给LLM，但它可能无法提取正确的答案。通常，这发生在上下文中噪声过多或信息冲突的情况下。即使提示（上下文）中包含答案，模型也可能产生幻觉（上下文幻觉）。'
- en: '**Answer in the wrong format**: There may be additional specifics in the query.
    For example, we may want an LLM to generate bullet points or report the information
    in a table. The LLM may ignore this information.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**答案格式错误**：查询可能有额外的具体要求。例如，我们可能希望LLM生成项目符号或以表格形式报告信息。LLM可能会忽略这些信息。'
- en: '**Incorrect specificity**: The generated answer is not specific enough or too
    specific with respect to the user’s needs. This is generally a problem associated
    with how the system is designed and what its purpose is. Our RAG may be part of
    a product designed for students and must give clear and comprehensive answers
    on a topic. The model, on the other hand, might answer vaguely or too technically
    for a student. Typically, this is a problem when the query (or instructions) is
    not clear enough.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不正确的具体性**：生成的答案不够具体或过于具体，不符合用户的需求。这通常与系统的设计及其目的有关。我们的RAG可能是为设计给学生的产品的一部分，必须就某个主题给出清晰和全面的答案。另一方面，模型可能回答得过于模糊或过于技术化，不适合学生。通常，当查询（或指令）不够清晰时，会出现这个问题。'
- en: '**Augmentation hurdles or information redundancy**: Our database may contain
    information from different corpora, and many of the documents may contain redundant
    information or be in different styles and tones. The LLM could then generate repetition
    and/or create hallucinations. Also, the answer may not be good quality because
    the model fails to integrate the information from the various chunks.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增强障碍或信息冗余**：我们的数据库可能包含来自不同语料库的信息，许多文档可能包含冗余信息或具有不同的风格和语气。LLM随后可能会生成重复内容和/或创造幻觉。此外，由于模型未能整合来自各个片段的信息，答案可能质量不高。'
- en: '**Incomplete answers**: These are answers that are not wrong but are incomplete
    (this can result from either not finding all the necessary information or errors
    on the part of the LLM in using the context). Sometimes, it can also be a problem
    of the query being too complex (“Summarize items A, B, and C”), and so it might
    also be better to modify the query.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不完整的答案**：这些答案并不错误，但是不完整的（这可能是因为没有找到所有必要的信息或LLM在处理上下文时的错误）。有时，这也可能是查询过于复杂的问题（“总结A、B和C项”），因此修改查询可能更好。'
- en: '**Lack of flexibility**: This when the system is not flexible; it does not
    currently allow efficient updating, and we cannot incorporate feedback from users,
    past interactions, and so on. The system does not allow us to handle certain files
    that are abundant in our corpus (for example, our system does not allow Excel).'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺乏灵活性**：这是当系统不够灵活时；它目前不允许高效更新，并且我们不能整合来自用户、过去交互等的反馈。系统不允许我们处理我们语料库中丰富的某些文件（例如，我们的系统不允许Excel文件）。'
- en: '**Scalability and overall performance**: In this case, our system may be too
    slow to conduct an embedding, generate a response, and so on. Alternatively, we
    cannot handle embedding multiple documents per second, or we have performance
    issues that are specific to our product or domain. System security is a sore point,
    especially if we have sensitive data.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性和整体性能**：在这种情况下，我们的系统可能太慢，无法进行嵌入、生成响应等。或者，我们无法每秒处理多个文档的嵌入，或者我们遇到特定于我们的产品或领域的问题。系统安全性是一个痛点，尤其是如果我们有敏感数据的话。'
- en: Now that we understand the issues with naïve RAG, let’s understand how advanced
    RAG helps us tackle these issues.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了朴素RAG的问题，让我们来看看高级RAG是如何帮助我们解决这些问题的。
- en: Exploring the advanced RAG pipeline
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索高级RAG管道
- en: 'Advanced RAG introduces a number of specific improvements to try to address
    the issues highlighted in naïve RAG. Advanced RAG, in other words, modifies the
    various components of RAG to try to optimize the RAG paradigm. These various modifications
    occur at the different steps of RAG: **pre-retrieval** and **post-retrieval**.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 高级RAG引入了一系列特定的改进，试图解决朴素RAG中提出的问题。换句话说，高级RAG通过修改RAG的各个组件来尝试优化RAG范式。这些各种修改发生在RAG的不同步骤中：**检索前**和**检索后**。
- en: In the **pre-retrieval process**, the purpose is to optimize indexing and querying.
    For example, **adding metadata** enables more granular searching, and we provide
    more content to the LLM to generate text. Metadata can succinctly contain information
    that would otherwise be dispersed throughout the document.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在**检索前过程**中，目的是优化索引和查询。例如，**添加元数据**可以提供更细粒度的搜索，并且我们向LLM提供更多内容以生成文本。元数据可以简洁地包含信息，否则这些信息将分散在整个文档中。
- en: 'In naïve RAG, we divide the document into different chunks and find the relevant
    chunks for each document. This approach has two limitations:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在朴素RAG中，我们将文档分成不同的片段，并为每个文档找到相关的片段。这种方法有两个局限性：
- en: When we have many documents, it impacts latency time and performance
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们拥有许多文档时，它会影响延迟时间和性能
- en: When the documents are large, we may not be able to easily find the relevant
    chunks
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当文档很大时，我们可能无法轻松找到相关的片段
- en: In naïve RAG, there is only one level (all chunks are equivalent even if they
    are derived from different documents). In general, though, for many corpora, there
    is a hierarchy, and it might be beneficial to use it.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在朴素RAG中，只有一个级别（即使它们来自不同的文档，所有片段都是等效的）。然而，一般来说，对于许多语料库，存在层次结构，利用它可能是有益的。
- en: To address these limitations, advanced RAG introduces several enhancements designed
    to improve both retrieval and generation. In the following subsections, we will
    explore some techniques
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些限制，高级RAG引入了几个增强功能，旨在提高检索和生成的效率。在接下来的小节中，我们将探讨一些技术
- en: Hierarchical indexing
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 层次化索引
- en: For a document consisting of several chapters, we could first find the chapters
    of interest and from there search for the various sections of interest. Since
    the chapters may be of considerable size (rich in noise), embedding may not best
    represent their contextual significance. The solution is to use summaries and
    metadata. In a **hierarchical index**, you create summaries at each hierarchical
    level (which can be considered abstracts). At the first level, we have summaries
    that highlight only the key points in large document segments. In the lower levels,
    the granularity will increase, and these abstracts will be closer and closer to
    only the relevant section of data. Next, we will conduct the embedding of these
    abstracts. At inference time, we will calculate the similarity with these summary
    embeddings. Of course, this means either we manually write the summaries or we
    use an LLM to conduct summarization. Then, using the associated metadata, we can
    find the chunks that match the summary and provide it to the model.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于由多个章节组成的文档，我们首先可以找到感兴趣的章节，然后从那里搜索感兴趣的各个部分。由于章节可能相当大（噪声丰富），嵌入可能无法最好地代表它们的上下文意义。解决方案是使用摘要和元数据。在**层次索引**中，你在每个层次级别创建摘要（这可以被认为是摘要）。在第一级，我们有只突出显示大文档段中关键点的摘要。在较低级别，粒度将增加，这些摘要将越来越接近仅包含相关数据部分。接下来，我们将对这些摘要进行嵌入。在推理时间，我们将计算与这些摘要嵌入的相似度。当然，这意味着我们可能需要手动编写摘要，或者使用LLM进行摘要。然后，使用相关的元数据，我们可以找到与摘要匹配的块，并将其提供给模型。
- en: '![Figure 6.2 – Hierarchical indexing](img/B21257_06_02.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图6.2 – 层次索引](img/B21257_06_02.jpg)'
- en: Figure 6.2 – Hierarchical indexing
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 – 层次索引
- en: As seen in the preceding figure, the corpus is divided into documents; we then
    obtain a summary of each document and embed it (in naïve RAG, we were dividing
    into chunks and embedding the chunks). In the next step, we embed the summary
    of a lower hierarchical level of the documents (chapter, sections, heading, and
    subheadings) until we reach the chunk level. At inference time, a similarity search
    is conducted on the summaries to retrieve the chunks we are interested in.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，语料库被分为文档；然后我们获取每个文档的摘要并将其嵌入（在简单的RAG中，我们是将文档分割成块并嵌入块）。在下一步中，我们将嵌入文档较低层次级别的摘要（章节、部分、标题和副标题），直到达到块级别。在推理时间，对摘要进行相似度搜索以检索我们感兴趣的块。
- en: There are some variations to this approach. For more control, we can choose
    a split approach for each file type (HTML, PDF, and GitHub repository). In this
    way, we can make the summary data type-specific and embed the summary, which works
    as a kind of text normalization.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法有一些变体。为了获得更多控制，我们可以为每种文件类型（HTML、PDF和GitHub仓库）选择一个分割方法。这样，我们可以使摘要数据类型特定，并嵌入摘要，这相当于一种文本归一化。
- en: When we have documents that are too long for our LLM summarizer, we can use
    **map and reduce**, where we first conduct a summarization of various parts of
    the document, then collate these summaries and get a single summary. If the documents
    are too encyclopedic (i.e., deal with too many topics), there is a risk of semantic
    noise impacting retrieval. To solve this, we can have multiple summaries per document
    (e.g., one summary per 10K tokens or every 10 pages of document).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们拥有的文档对于我们的LLM摘要器来说太长时，我们可以使用**映射和归约**，首先对文档的各个部分进行摘要，然后整理这些摘要，得到一个单一的摘要。如果文档过于百科全书式（即涉及太多主题），存在语义噪声影响检索的风险。为了解决这个问题，我们可以为每个文档提供多个摘要（例如，每10K个标记或每10页文档一个摘要）。
- en: Hierarchical indexing improves the contextual understanding of the document
    (because it respects its hierarchy and captures the relationships between various
    sections, such as chapters, headings, and subheadings). This approach allows greater
    accuracy in finding the results, and they are more relevant. On the other hand,
    this approach comes at a cost both during the pre-retrieval stage and in inference.
    Too many levels and you risk having a combinatorial explosion, that is, a rapid
    growth in complexity due to the exponential increase in possible combinations,
    with a huge latency cost.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 层次索引提高了对文档的上下文理解（因为它尊重其层次结构并捕获各个部分之间的关系，如章节、标题和副标题）。这种方法允许在查找结果时具有更高的准确性，并且它们更加相关。另一方面，这种方法在检索前阶段和推理阶段都有成本。层次太多，你可能会面临组合爆炸的风险，即由于可能组合的指数级增加而导致的复杂性的快速增长，以及巨大的延迟成本。
- en: '![Figure 6.3 – Hierarchical indexing variation](img/B21257_06_03.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图6.3 – 层次索引变化](img/B21257_06_03.jpg)'
- en: Figure 6.3 – Hierarchical indexing variation
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 – 层次索引变化
- en: 'In the preceding figure, we can see these hierarchical index variations:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们可以看到这些层次索引的变化：
- en: '*A*: Different handling for each document type to better represent their structure'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*A*: 对每种文档类型进行不同的处理，以更好地表示其结构'
- en: '*B*: Map and reduce to handle too-long documents (intermediate summaries are
    created and then used to create the final document summary)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*B*: 映射和归约处理过长的文档（创建中间摘要，然后用于创建最终文档摘要）'
- en: '*C*: Multi-summary for each document when documents are discussing too many
    topics'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*C*: 当文档讨论太多主题时，每个文档的多摘要'
- en: Hypothetical questions and HyDE
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 假设性问题与HyDE
- en: Another modification of the naïve RAG pipeline is to try to make chunks and
    possible questions more semantically similar. By having an idea of who our users
    are, we can imagine the kind of use they will get out of our system (for a chatbot,
    most queries will be questions, so we can tailor the system toward these kinds
    of queries). **Hypothetical questions** is a type of strategy in which we use
    an LLM to generate one (or more) hypothetical question(s) for each chunk. These
    hypothetical questions are then transformed into vectors (embedding), and these
    vectors are used to do a similarity search when there is a query from a user.
    Of course, once we have identified the hypothetical questions most similar to
    our real query, we find the chunks (thanks to the metadata) and provide them to
    the model. We can generate either a single query or multiple queries for each
    chunk (this increases the accuracy as well as the computational cost). In this
    case, we are not using the vector representation of chunks (we do not conduct
    embeddings of chunks but hypothetical questions). Also, we do not necessarily
    have to save the hypothetical questions, just their vectors (the important thing
    is that we can map them back to the chunks).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对简单的RAG管道的另一种修改是尝试使块和可能的问题在语义上更加相似。通过了解我们的用户是谁，我们可以想象他们将从我们的系统中获得什么样的用途（对于一个聊天机器人，大多数查询将是问题，因此我们可以调整系统以适应这些类型的查询）。**假设性问题**是一种策略，其中我们使用一个LLM为每个块生成一个（或多个）假设性问题。然后，将这些假设性问题转换为向量（嵌入），并在有用户查询时使用这些向量进行相似性搜索。当然，一旦我们确定了与我们的实际查询最相似的假设性问题，我们就找到块（多亏了元数据）并将它们提供给模型。我们可以为每个块生成单个查询或多个查询（这增加了准确性以及计算成本）。在这种情况下，我们不是使用块的向量表示（我们不进行块的嵌入，而是假设性问题）。此外，我们不一定需要保存假设性问题，只需它们的向量（重要的是我们能够将它们映射回块）。
- en: '**Hypothetical Document Embeddings** (**HyDE**) instead tries to convert the
    user answers to better match the chunks. Given a query, we create hypothetical
    answers to it. After that, we conduct embeddings of these generated answers and
    carry out a similarity search to find the chunks of interest. These generated
    answers should be most semantically similar to the user’s query, allowing us to
    be able to find better chunks. In some variants, we create five different generated
    answers and conduct the average of their embedding vectors before conducting the
    similarity search. This approach can help when we have a low recall metric in
    the retrieval step or when the documents (or queries) come from a specific domain
    that is different from the retrieval domain. In fact, embedding models generalize
    poorly to knowledge domains that they have not seen. An interesting little note
    is that when an LLM generates these hypothetical answers, it does not know the
    exact answer (that is not even the purpose of the approach) but is able to capture
    relevant patterns in the question. We can then use these captured patterns to
    retrieve the chunks.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**假设文档嵌入**（**HyDE**）则试图将用户答案转换为更好地匹配块。给定一个查询，我们为它创建假设性答案。之后，我们对这些生成的答案进行嵌入，并执行相似性搜索以找到感兴趣的块。这些生成的答案应该与用户的查询在语义上最为相似，使我们能够找到更好的块。在某些变体中，我们创建五个不同的生成答案，并在进行相似性搜索之前对这些答案的嵌入向量进行平均。这种方法可以帮助我们在检索步骤中召回率低时，或者当文档（或查询）来自与检索域不同的特定领域时。事实上，嵌入模型在它们未见过的知识域中泛化得不好。一个有趣的注释是，当LLM生成这些假设性答案时，它不知道确切的答案（这甚至不是这种方法的目的），但它能够捕捉到问题中的相关模式。然后，我们可以使用这些捕获的模式来检索块。'
- en: Let’s look in detail at the difference between the two approaches. With the
    hypothetical questions approach, we generate hypothetical questions and use the
    embedding of these hypothetical questions to then find the chunks of interest.
    With HyDE, we generate hypothetical answers to our query and then use the embedding
    of these answers to find the chunks of interest.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细地看看两种方法之间的差异。在假设问题方法中，我们生成假设问题，并使用这些假设问题的嵌入来找到感兴趣的片段。在 HyDE 中，我们生成针对查询的假设答案，然后使用这些答案的嵌入来找到感兴趣的片段。
- en: '![Figure 6.4 – Hypothetical questions and HyDE approaches](img/B21257_06_04.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.4 – 假设问题和 HyDE 方法](img/B21257_06_04.jpg)'
- en: Figure 6.4 – Hypothetical questions and HyDE approaches
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4 – 假设问题和 HyDE 方法
- en: 'We can then look in detail at the differences between the two approaches, imagining
    that we have a hypothetical user question (“What are the potential side effects
    of using acetaminophen?”):'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以详细地比较两种方法之间的差异，假设我们有一个假设用户问题（“使用对乙酰氨基酚的潜在副作用是什么？”）：
- en: '**Pre-retrieval phase**: During this phase, we have to create our drug embedding
    database. We reduce our documents (sections of a drug’s safety report) into chunks.
    In the hypothetical questions approach, for each chunk, hypothetical questions
    are generated using an LLM (for example, “What are the side effects of this drug?”
    or “Are there any adverse reactions mentioned?”). Each of these hypothetical questions
    is then embedded into a vector space (a database of vectors for these questions).
    At this stage, HyDE is equal to classic RAG; no variation is conducted.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**检索前阶段**：在此阶段，我们必须创建我们的药物嵌入数据库。我们将文档（药物安全报告的部分）减少为片段。在假设问题方法中，对于每个片段，使用 LLM
    生成假设问题（例如，“这种药物的副作用是什么？”或“是否有提到任何不良反应？”）。然后，将这些假设问题嵌入到向量空间中（这些问题的向量数据库）。在此阶段，HyDE
    等同于经典的 RAG；没有进行任何变异。'
- en: '**Query phase**: In the hypothetical questions approach, when a user submits
    the query, it is embedded and matched against the embedded hypothetical questions.
    The system looks for the hypothetical questions that are most similar to the user’s
    question (in this case, it might be, “What are the side effects of this drug?”).
    At this point, the chunks from which these hypothetical questions were generated
    are identified (we use metadata). These chunks are provided in context for generation.
    In HyDE, when the user query arrives, an LLM generates hypothetical answers (for
    example, “Paracetamol may cause side effects such as nausea, liver damage, and
    rashes” or “Potential adverse reactions include dizziness and gastrointestinal
    discomfort”).'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**查询阶段**：在假设问题方法中，当用户提交查询时，它会被嵌入并与嵌入的假设问题进行匹配。系统寻找与用户问题最相似的假设问题（在这种情况下，可能是“这种药物的副作用是什么？”）。在此阶段，识别出生成这些假设问题的片段（我们使用元数据）。这些片段被提供在上下文中进行生成。在
    HyDE 中，当用户查询到达时，LLM 生成假设答案（例如，“对乙酰氨基酚可能引起恶心、肝损伤和皮疹等副作用”或“潜在的不良反应包括头晕和胃肠道不适”）。'
- en: Note that these answers are generated using LLM knowledge without retrieval.
    At this point, we conduct the embedding of these hypothetical answers (we use
    an embedding model), then conduct the embedding of the query and try to match
    it with the embedded hypothetical answers. For example, “Paracetamol may cause
    side effects such as nausea, liver damage, and rashes” is the one closest to the
    user query. We then search for the chunks closest to these hypothetical answers
    and provide the LLM to generate the context.
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，这些答案是通过不检索的 LLM 知识生成的。在此阶段，我们对这些假设答案进行嵌入（我们使用嵌入模型），然后对查询进行嵌入，并尝试将其与嵌入的假设答案匹配。例如，“对乙酰氨基酚可能引起恶心、肝损伤和皮疹等副作用”是与用户查询最接近的一个。然后我们搜索与这些假设答案最接近的片段，并将这些片段提供给
    LLM 生成上下文。
- en: Context enrichment
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 上下文丰富
- en: Another technique is **context enrichment**, in which we find smaller chunks
    (greater granularity for better search quality) and then add surrounding context.
    **Sentence window retrieval** is one such technique in which each sentence in
    a document is embedded separately (the embedded textual unit is smaller and therefore
    more granular). This allows us to have higher precision in finding answers, though
    we risk losing context for LLM reasoning (and thus worse generation). To solve
    this, we expand our context window. Having found a sentence *x*, we take *k* sentences
    that surround it in the document (sentences that are before and after our sentence
    *x* in the document).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种技术是**上下文丰富**，其中我们找到更小的段（更细粒度以获得更好的搜索质量），然后添加周围上下文。**句子窗口检索**是其中一种技术，其中文档中的每个句子都单独嵌入（嵌入的文本单元更小，因此更细粒度）。这使我们能够在寻找答案时具有更高的精确度，尽管我们可能会失去LLM推理的上下文（因此生成效果较差）。为了解决这个问题，我们扩展了上下文窗口。在找到句子
    *x* 后，我们取文档中围绕它的 *k* 个句子（在文档中位于我们的句子 *x* 之前和之后的句子）。
- en: '**Parent document retriever** is a similar technique that tries to find a balance
    between searching on small chunks and providing context with larger chunks. The
    documents are divided into small child chunks, but we preserve the hierarchy of
    their parent documents. In this case, we conduct embedding of small chunks that
    directly address the specifics of a query (ensuring larger chunks’ relevance).
    But then we find the larger parent documents (to which the found chunks belong)
    and provide them to the LLM for generation (more contextual information and depth).
    To avoid retrieving too many parent documents, once the top *k* chunks are found,
    if more than *n* chunks belong to a parent document, we add this document to the
    LLM context.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**父文档检索器** 是一种类似的技术，试图在搜索小段和提供大段上下文之间找到平衡。文档被分成小段子文档，但我们保留了它们父文档的层次结构。在这种情况下，我们直接针对查询的具体内容进行小段的嵌入（确保大段的相关性）。然后我们找到较大的父文档（找到的段所属的文档）并将它们提供给LLM进行生成（更多上下文信息和深度）。为了避免检索过多的父文档，一旦找到前
    *k* 个段，如果超过 *n* 个段属于一个父文档，我们将此文档添加到LLM上下文中。'
- en: 'These approaches are depicted in the following figure:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法在以下图中展示：
- en: Once a chunk is found, we expand the selection with the previous and next chunks.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦找到一个段，我们就使用前一个和后一个段来扩展选择。
- en: We conduct embedding of small chunks and find the top *k* chunks; if most chunks
    (greater than a parameter *n*) are derived from a document, we provide the LLM
    with the document as the context.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们对小段文本进行嵌入并找到前 *k* 个段；如果大部分段（大于参数 *n*）都来自一个文档，我们将该文档作为上下文提供给LLM。
- en: '![Figure 6.5 – Context enrichment approaches](img/B21257_06_05.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图6.5 – 上下文丰富方法](img/B21257_06_05.jpg)'
- en: Figure 6.5 – Context enrichment approaches
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 – 上下文丰富方法
- en: Query transformation
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查询转换
- en: '**Query transformation** is a family of techniques that leverages an LLM to
    improve retrieval. If a query is too complex, it can be decomposed into a series
    of queries. In fact, we may not find a chunk that responds to the query, but more
    easily find chunks that respond to each subquery (e.g., “Who was the inventor
    of the telegraph and the telephone?” is best broken down into two independent
    queries). **Step-back prompting** uses an LLM to generate a more general query
    that can match a high-level context. It stems from the idea that when a human
    being is faced with a difficult task, they take a step back and do abstractions
    to get to the high-level principles. In this case, we use the embedding of this
    high-level query and the user’s query, and both found contexts are provided to
    the LLM for generation. **Query rewriting**, on the other hand, reformulates the
    initial query with an LLM to make retrieval easier.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**查询转换** 是一系列利用LLM来提高检索的技术。如果一个查询过于复杂，它可以被分解成一系列子查询。实际上，我们可能找不到一个直接响应查询的段，但更容易找到响应每个子查询的段（例如，“电报和电话的发明者是谁？”最好分解成两个独立的查询）。**回退提示**
    使用LLM生成一个更通用的查询，以匹配高级上下文。这源于当人类面对困难任务时，他们会退一步进行抽象以到达高级原则的想法。在这种情况下，我们使用这个高级查询的嵌入和用户的查询，并将这两个找到的上下文都提供给LLM进行生成。**查询重写**，另一方面，使用LLM重新表述初始查询以简化检索。'
- en: '![Figure 6.6 – Three examples of query transformation](img/B21257_06_06.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图6.6 – 查询转换的三个示例](img/B21257_06_06.jpg)'
- en: Figure 6.6 – Three examples of query transformation
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 – 查询转换的三个示例
- en: '**Query expansion** is a technique similar to query rewriting. Underlying it
    is the idea that adding terms to the query can allow it to find relevant documents
    that do not have lexical overlap with the query (and thus improve retrieval recall).
    Again, we use an LLM to modify the query. There are two main possibilities:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**查询扩展**是一种类似于查询重写的技巧。其背后的理念是，向查询中添加术语可以使它找到与查询没有词汇重叠的相关文档（从而提高检索召回率）。同样，我们使用LLM来修改查询。有两种主要可能性：'
- en: Ask an LLM to generate an answer to the query, after which the generated answer
    and the query are embedded and used for retrieval.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请求一个LLM（大型语言模型）对查询生成答案，之后将生成的答案和查询嵌入并用于检索。
- en: Generate several queries similar to the original query (usually a prefixed number
    *n*). This *n* set of queries is then vectorized and used for search.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成几个与原始查询相似的查询（通常是一个前缀数字*n*）。然后，将这*n*个查询向量化并用于搜索。
- en: This approach usually improves retrieval because it helps disambiguate the query
    and find documents that otherwise would not be found; it also helps the system
    better compile the query. On the other hand, though, it also leads to finding
    irrelevant documents, so it pays to combine it with post-processing techniques
    for finding documents.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法通常可以提高检索效果，因为它有助于消除查询歧义并找到否则可能找不到的文档；它还有助于系统更好地编制查询。然而，另一方面，它也可能导致找到不相关的文档，因此结合后处理技术来查找文档是值得的。
- en: Keyword-based search and hybrid search
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关键词搜索和混合搜索
- en: Another way to improve search is to focus not only on contextual information
    but also on keywords. **Keyword-based search** is a search by an exact match of
    certain keywords. This type of search is beneficial for specific terms (such as
    product or company names or specific industry jargon). However, it is sensitive
    to typos and synonyms and does not capture context. **Vector or semantic search**,
    on the contrary, finds the semantic meaning of a query but does not find exact
    terms or keywords (which is sometimes essential for some queries, especially in
    some domains such as marketing). **Hybrid search** takes the best of both worlds
    by combining a model for keyword search and vectorial search.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 提高搜索的另一种方法是不仅要关注上下文信息，还要关注关键词。**基于关键词的搜索**是通过某些特定关键词的精确匹配来进行的搜索。这种搜索对特定术语（如产品或公司名称或特定行业术语）有益。然而，它对拼写错误和同义词敏感，并且无法捕捉上下文。**向量或语义搜索**则相反，它找到查询的语义含义，但找不到精确的术语或关键词（这在某些查询中有时是必不可少的，尤其是在某些领域，如市场营销）。**混合搜索**通过结合关键词搜索和向量搜索的模型，取两者之长。
- en: 'The most commonly used model for keyword search is BM25 (which we discussed
    in the previous chapter), which generates sparse embeddings. BM25 then allows
    us to identify documents that contain specific terms in the query. So, we create
    two embeddings: a sparse embedding with BM25 and a dense embedding with a transformer.
    To select the best chunks, you generally try to balance the impact of your different
    types of searches. The final score is a weighted combination (you use an alpha
    hyperparameter) of the two scores:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 最常用的关键词搜索模型是BM25（我们在上一章中讨论过），它生成稀疏嵌入。BM25然后允许我们识别包含查询中特定术语的文档。因此，我们创建了两个嵌入：一个使用BM25的稀疏嵌入和一个使用transformer的密集嵌入。为了选择最佳片段，你通常试图平衡不同类型搜索的影响。最终得分是两个得分的加权组合（你使用一个alpha超参数）：
- en: <mrow><mrow><msub><mrow><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi></mrow><mrow><mi>h</mi><mi>y</mi><mi>b</mi><mi>r</mi><mi>i</mi><mi>d</mi></mrow></msub><mo>=</mo><mfenced
    close=")" open="("><mrow><mn>1</mn><mo>−</mo><mi mathvariant="normal">α</mi></mrow></mfenced><mo>∙</mo><msub><mrow><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi></mrow><mrow><mi>s</mi><mi>p</mi><mi>a</mi><mi>r</mi><mi>s</mi><mi>e</mi></mrow></msub><mo>+</mo><mi
    mathvariant="normal">α</mi><mo>∙</mo><msub><mrow><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi></mrow><mrow><mi>d</mi><mi>e</mi><mi>n</mi><mi>s</mi><mi>e</mi></mrow></msub></mrow></mrow>
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '`<mrow><mrow><msub><mrow><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi></mrow><mrow><mi>h</mi><mi>y</mi><mi>b</mi><mi>r</mi><mi>r</mi><mi>i</mi><mi>d</mi></mrow></msub><mo>=</mo><mfenced
    close=")" open="("><mrow><mn>1</mn><mo>−</mo><mi mathvariant="normal">α</mi></mrow></mfenced><mo>∙</mo><msub><mrow><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi></mrow><mrow><mi>s</mi><mi>p</mi><mi>a</mi><mi>r</mi><mi>s</mi><mi>e</mi></mrow></msub><mo>+</mo><mi
    mathvariant="normal">α</mi><mo>∙</mo><msub><mrow><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi></mrow><mrow><mi>d</mi><mi>e</mi><mi>n</mi><mi>s</mi><mi>e</mi></mrow></msub></mrow></mrow>`'
- en: '*α* has a value between 0 and 1 (0 means pure vectorial search, while 1 means
    only keyword search). Typically, the value of *α* is 0.4 or 0.5 (other articles
    even suggest 0.3).'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*α* 的值介于0和1之间（0表示纯向量搜索，而1表示仅关键词搜索）。通常，*α* 的值是0.4或0.5（其他文章甚至建议0.3）。'
- en: 'As a practical example, we can imagine an e-commerce platform with a vast product
    catalog containing millions of items across categories such as electronics, fashion,
    and home appliances. Users search for products with different types of queries,
    which may include the following:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 作为实际例子，我们可以想象一个拥有庞大产品目录的电子商务平台，包含数百万种商品，涵盖电子、时尚和家用电器等类别。用户会使用不同类型的查询来搜索产品，以下是一些可能的查询类型：
- en: Specific terms such as a brand or product name (e.g., “iPhone 16”)
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特定术语，如品牌或产品名称（例如，“iPhone 16”）
- en: A general description (e.g., “Medium-price phone with a good camera”)
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一般描述（例如，“中等价位的具有良好摄像头的手机”）
- en: Queries that contain mixed elements (e.g., “iPhone with cost less than $500”)
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含混合元素的查询（例如，“价格低于500美元的iPhone”）
- en: A pure keyword-based search ( such as the BM25 algorithm) would struggle with
    vague or purely descriptive descriptions, while a vector-based semantic search
    might miss exact matches for a product. Hybrid search combines the best of both.
    BM25 prioritizes exact matches, such as matches of “iPhone,” allowing us to find
    specific items using keywords. Semantic search allows us to capture the semantic
    meaning of phrases such as “phone with a good camera.” Hybrid search is a great
    solution for all three of the previously mentioned cases.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 纯关键词搜索（例如BM25算法）在处理模糊或纯粹描述性的描述时会遇到困难，而基于向量的语义搜索可能会错过产品的精确匹配。混合搜索结合了两者之长。BM25优先考虑精确匹配，如“iPhone”的匹配，使我们能够通过关键词找到特定项目。语义搜索允许我们捕捉到“具有良好摄像头的手机”之类的短语的语义含义。混合搜索是解决前述三种情况的一个很好的解决方案。
- en: Query routing
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查询路由
- en: So far, we have assumed that once a query arrives, it is used for a search within
    the vector database. In reality, we may want to conduct the search differently
    or control the flow within the system. For example, the system should be able
    to interact with different types of databases (vector, SQL, and proprietary databases),
    different sources, or different types of modalities (image, text, and sound).
    Some queries do not, then, need to be searched with RAG; the parametric memory
    of the model might suffice (we will discuss this in more depth in the *Open questions
    and future perspectives* section). Query routing thus allows control over how
    the system should respond to the query. You can imagine it as being a series of
    if/else causes, though instead of being hardcoded, we have a router (usually an
    LLM) that makes a decision whenever a query arrives. Obviously, this means that
    we have a nondeterministic system, and it will not always make the right decision,
    although it can have a major positive impact on performance.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们假设一旦查询到达，就会在向量数据库中进行搜索。实际上，我们可能希望以不同的方式或控制系统内的流程进行搜索。例如，系统应该能够与不同类型的数据库（向量、SQL和专有数据库）、不同来源或不同类型的模态（图像、文本和声音）进行交互。因此，一些查询可能不需要使用RAG进行搜索；模型的参数化内存可能就足够了（我们将在*开放问题和未来展望*部分进行更深入的讨论）。查询路由因此允许控制系统如何响应查询。你可以想象它是一系列if/else条件，尽管它不是硬编码的，我们有一个路由器（通常是一个LLM），每当查询到达时都会做出决定。显然，这意味着我们有一个非确定性的系统，它并不总是能做出正确的决定，尽管它可以对性能产生重大积极影响。
- en: 'The router can be a set of logical rules or a neural model. Some options for
    a router are the following:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 路由器可以是一组逻辑规则或一个神经网络模型。以下是一些路由器的选项：
- en: '**Logical routers**: A set of logical rules that can be if/else clauses (e.g.,
    if the query is an image, it searches the image database; otherwise, it searches
    the text database). Logical routers don’t understand the query, but they are very
    fast and deterministic.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**逻辑路由器**：一组逻辑规则，可以是if/else语句（例如，如果查询是图像，则搜索图像数据库；否则，搜索文本数据库）。逻辑路由器不理解查询，但它们非常快且确定。'
- en: '**Keyword routers**: A slightly more sophisticated alternative in which we
    try to select a route by matching keywords between the query and a list of options.
    This search can be done with a sparse encoder, a specialized package, or even
    an LLM.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关键词路由器**：一种稍微复杂一些的替代方案，我们尝试通过在查询和选项列表之间匹配关键词来选择路由。这种搜索可以使用稀疏编码器、专用包甚至LLM来完成。'
- en: '**Zero-shot classification router**: Zero-shot classification is a task in
    which an LLM is asked to classify an item with a set of labels without being specified
    and trained for it. Each query is given to an LLM that must assign a route label
    from those in a list.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**零样本分类路由器**：零样本分类是一个任务，其中要求一个大型语言模型（LLM）对一组未指定和训练过的标签进行分类。每个查询都提交给一个LLM，它必须从列表中选择一个路由标签。'
- en: '**LLM function calling router**: The different routes are described as functions
    (with a specific description) and the model must decide where to direct the queries
    by selecting the function (in this approach, we leverage its decision-making ability).'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LLM函数调用路由器**：不同的路由被描述为函数（具有特定的描述），模型必须通过选择函数（在此方法中，我们利用其决策能力）来决定将查询导向何处。'
- en: '**Semantic router**: In this approach, we use a semantic search to decide on
    the best route. In short, we have a list of example queries and the associated
    routes. These are then embedded and saved as vectors in a database. When a query
    arrives, we conduct a similarity search with the other queries in our database.
    We then select the option associated with the query with the best similarity match.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语义路由器**：在此方法中，我们使用语义搜索来决定最佳路由。简而言之，我们有一个示例查询列表及其相关路由。然后，这些查询被嵌入并保存为数据库中的向量。当查询到达时，我们对我们数据库中的其他查询进行相似度搜索。然后，我们选择与最佳相似度匹配的查询相关的选项。'
- en: '![Figure 6.7 – Query routing](img/B21257_06_07.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图6.7 – 查询路由](img/B21257_06_07.jpg)'
- en: Figure 6.7 – Query routing
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7 – 查询路由
- en: Once we have found the context, we need to integrate it with the query and provide
    it to the LLM for generation. There are several strategies to improve this process,
    usually called **post-retrieval strategies**. After the vector search, retrieval
    returns the top *k* documents (an arbitrary cutoff that is determined in advance).
    This can lead to the loss of relevant information. The simplest solution is to
    increase the value of the top *k* chunks. Obviously, we cannot return all retrieved
    chunks, both because they would not fit into the context length of the model and
    because the LLM would then have problems with handling all this information (efficient
    use of a long context length).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们找到了上下文，我们需要将其与查询整合，并提供给LLM进行生成。有几种策略可以改进这个过程，通常称为**检索后策略**。在向量搜索之后，检索返回前*k*个文档（一个预先确定的任意截止值）。这可能导致相关信息丢失。最简单的解决方案是增加前*k*个块的价值。显然，我们无法返回所有检索到的块，因为它们不会适合模型的上下文长度，而且LLM在处理所有这些信息时也会遇到问题（有效利用长上下文长度）。
- en: We can imagine a company offering different services across different domains,
    such as banking, insurance, and finance. Customers interact with a chatbot to
    seek assistance with banking services (account details, transactions, and so on),
    insurance services (policy details, claims, etc.), and financial services (suggestions,
    investments, etc.). Each domain is different. Due to regulations and privacy issues,
    we want to prevent a chatbot from searching for details for a customer of another
    service. Also, searching all databases for every query is inefficient and leads
    to more latency and irrelevant results.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以想象一家公司提供不同领域的不同服务，例如银行、保险和金融。客户通过与聊天机器人互动来寻求银行服务（账户详情、交易等）、保险服务（保单详情、索赔等）和金融服务（建议、投资等）的协助。每个领域都有其独特性。由于法规和隐私问题，我们希望防止聊天机器人搜索其他服务的客户详情。此外，对每个查询搜索所有数据库既不高效，还会导致更多延迟和无关的结果。
- en: Reranking
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重排序
- en: 'One proposed solution to this dilemma is to maximize document retrieval (increase
    the top *k* retrieved results and thus increase the retrieval recall metric) but
    at the same time maximize the LLM recall (by minimizing the number of documents
    supplied to the LLM). This strategy is called **reranking**. Reranking consists
    of two steps:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这一困境的一个建议方案是最大化文档检索（增加前*k*个检索到的结果，从而提高检索召回率指标），同时最大化LLM的召回率（通过最小化提供给LLM的文档数量）。这种策略被称为**重排序**。重排序包括两个步骤：
- en: First, we conduct a classical retrieval and find a large number of chunks.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们进行经典检索并找到大量块。
- en: Next, we use a reranker (a second model) to reorder the chunks and then select
    the top *k* chunks to provide to the LLM.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们使用重排序器（第二个模型）重新排序块，然后选择前*k*个块提供给LLM。
- en: 'The reranker improves the quality of chunks returned to the LLM and reduces
    hallucinations in the system. In addition, reranking considers contrasting information
    (related to the query) and then considers chunks in context with the query. There
    are several types of rerankers, each with its own limitations and advantages:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 重新排序器提高了返回给LLM的块的质量，并减少了系统中的幻觉。此外，重新排序考虑了与查询相关的对比信息，然后考虑与查询相关的上下文中的块。有几种类型的重新排序器，每种都有其自身的局限性和优点：
- en: '**Cross-encoders**: These are transformers (such as BGE) that take two textual
    sequences (the query and the various chunks one at a time) as input and return
    the similarity between 0 and 1.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**交叉编码器**：这些是变换器（如BGE），它们接受两个文本序列（查询和各个块逐个）作为输入，并返回0到1之间的相似度。'
- en: '**Multi-vector rerankers**: These are still transformers (such as ColBERT)
    and require less computation than cross-encoders (the interaction between the
    two sequences is late-stage). The principle is similar; given two sequences, they
    return a similarity between 0 and 1\. There are improved versions with a large
    context length, such as jina-colbert-v1-en.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多向量重新排序器**：这些仍然是变换器（如ColBERT）并且比交叉编码器（两个序列之间的交互是后期阶段）需要的计算更少。原理类似；给定两个序列，它们返回0到1之间的相似度。有改进版本，具有较长的上下文长度，如jina-colbert-v1-en。'
- en: '**LLMs for reranking**: LLMs can also be used as rerankers. Several strategies
    are used to improve the ranking capabilities of an LLM:'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用于重新排序的LLM**：LLM也可以用作重新排序器。使用几种策略来提高LLM的排名能力：'
- en: '**Pointwise methods** are used to calculate the relevance of a query and a
    single document (also referred to as zero-shot document reranking).'
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**点wise方法**用于计算查询和单个文档的相关性（也称为零样本文档重新排序）。'
- en: '**Pairwise methods** consist of providing an LLM with both the query and two
    documents and asking it to choose which one is more relevant.'
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成对方法**包括向LLM提供查询和两个文档，并要求它选择哪个更相关。'
- en: '**Listwise methods**, on the other hand, propose to provide a query and a list
    of documents to the LLM and instruct it to produce as output a ranked list. Models
    such as GPT are usually used, with the risk of high computational or economic
    costs.'
  id: totrans-110
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**按列表方法**，另一方面，建议向LLM（大型语言模型）提供一个查询和一个文档列表，并指示它生成一个按排名排序的输出列表。通常使用GPT等模型，但存在高计算或经济成本的风险。'
- en: '**Fine-tuned LLMs**: This is a class of models that is specifically for ranking
    tasks. Although LLMs are generalist models, they do not have specific training
    for ranking and therefore cannot accurately measure query-document relevance.
    Fine-tuning allows them to improve their capability. Generally, there are two
    types of models used: encoder-decoder transformers (RankT5) or decoder-only transformers
    (e.g., derivatives of Llama and GPT).'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**微调的LLM**：这是一类专门用于排名任务的模型。尽管LLM是通用模型，但它们没有针对排名的特定训练，因此无法准确衡量查询-文档的相关性。微调使它们能够提高其能力。通常，使用两种类型的模型：编码器-解码器变换器（RankT5）或仅解码器变换器（例如Llama和GPT的衍生品）。'
- en: All these approaches have an impact on both performance (retrieval quality)
    and cost (computational cost, system latency, and potential system cost). Generally,
    multi-vectors are those with lower computational cost and discrete performance.
    LLM-based methods may have the best performance but have high computational costs.
    In general, reranking has a positive impact on the system, which is why it is
    often a component of the pipeline.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些方法都会对性能（检索质量）和成本（计算成本、系统延迟和潜在的系统成本）产生影响。一般来说，多向量是那些计算成本较低且性能离散的。基于LLM的方法可能具有最佳性能，但计算成本很高。一般来说，重新排序对系统有积极影响，这也是为什么它经常是管道的一部分。
- en: '![Figure 6.8 – Reranking approach. Chunks highlighted in red are the chunks
    relevant to the query](img/B21257_06_08.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图6.8 – 重新排序方法。用红色突出显示的块是与查询相关的块](img/B21257_06_08.jpg)'
- en: Figure 6.8 – Reranking approach. Chunks highlighted in red are the chunks relevant
    to the query
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8 – 重新排序方法。用红色突出显示的块是与查询相关的块
- en: Alternatively, there are other **post-processing techniques**. For example,
    it is possible to filter out chunks if the similarity achieved is below a certain
    score threshold, if they do not include certain keywords, if a certain value is
    not present in the metadata associated with the chunks, if the chunks are older
    than a certain date, and many other possibilities. An additional strategy is that
    once we have found chunks, starting from the embedding vectors, we conduct **k-nearest
    neighbors** (**kNN**) research. In other words, we add other chunks that are neighbors
    in the latent space of those found (this strategy can be done before or after
    reranking).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，还有其他**后处理技术**。例如，如果达到的相似度低于某个分数阈值，如果它们不包含某些关键词，如果与块关联的元数据中不存在某个值，如果块比某个日期旧，以及许多其他可能性。另一个策略是，一旦我们找到了块，从嵌入向量开始，我们进行**k近邻**（**kNN**）研究。换句话说，我们在那些找到的块在潜在空间中的邻居中添加其他块（这种策略可以在重新排序之前或之后进行）。
- en: 'In addition, once the chunks are selected to be provided in context to the
    LLM, we can alter their order. As shown in the following figure, a study published
    in 2023 shows that the best performance for an LLM is when the important information
    is placed at the beginning or end of the input context length (performance drops
    if the information is in the middle of the context length, especially if it is
    very long):'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一旦选择了要提供给LLM的上下文中的块，我们就可以改变它们的顺序。如图所示，2023年发表的一项研究表明，对于LLM来说，将重要信息放在输入上下文长度的开头或末尾可以获得最佳性能（如果信息位于上下文长度的中间，性能会下降，尤其是如果它非常长）：
- en: '![Figure 6.9 – Changing the location of relevant information impacts the performance
    of an LLM (https://arxiv.org/abs/2307.03172)](img/B21257_06_09.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图6.9 – 改变相关信息的位置会影响LLM的性能 (https://arxiv.org/abs/2307.03172)](img/B21257_06_09.jpg)'
- en: Figure 6.9 – Changing the location of relevant information impacts the performance
    of an LLM ([https://arxiv.org/abs/2307.03172](https://arxiv.org/abs/2307.03172))
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9 – 改变相关信息的位置会影响LLM的性能 ([https://arxiv.org/abs/2307.03172](https://arxiv.org/abs/2307.03172))
- en: That is why it has been proposed to **reorder the chunks**. They can be placed
    in order of relevance, but also in alternating patterns (chunks with an even index
    are placed at the beginning of the list and chunks with an odd index at the end).
    The alternating pattern is used especially when using wide top *k* chunks, so
    the most relevant chunks are placed at the beginning and end (while the less relevant
    ones are in the middle of the context length).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 正因如此，有人提出了**重新排序块**的建议。它们可以按照相关性排序，也可以按照交替模式（偶数索引的块放在列表开头，奇数索引的块放在列表末尾）。交替模式特别适用于使用宽顶
    *k* 块时，这样最相关的块就被放在了开头和末尾（而不那么相关的块则位于上下文长度的中间）。
- en: 'You can notice that reranking improves the performance of the system:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以注意到重新排序提高了系统的性能：
- en: '![Figure 6.10 – Reranking improves the performance in question-answering (https://arxiv.org/pdf/2409.07691)](img/B21257_06_10.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图6.10 – 重新排序提高了问答性能 (https://arxiv.org/pdf/2409.07691)](img/B21257_06_10.jpg)'
- en: Figure 6.10 – Reranking improves the performance in question-answering ([https://arxiv.org/pdf/2409.07691](https://arxiv.org/pdf/2409.07691))
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10 – 重新排序提高了问答性能 ([https://arxiv.org/pdf/2409.07691](https://arxiv.org/pdf/2409.07691))
- en: In addition to reranking, several complementary techniques can be applied after
    the retrieval stage to further refine the information passed to the LLM. These
    include methods for improving citation accuracy, managing chat history, compressing
    context, and optimizing prompt formulation. Let's have a look at some of them.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 除了重新排序之外，还可以在检索阶段之后应用几种补充技术来进一步细化传递给LLM的信息。这包括提高引用准确性的方法、管理聊天历史、压缩上下文和优化提示公式。让我们看看其中的一些。
- en: '**Reference citations** is not really a technique for system improvement, but
    it is highly recommended as a component of a RAG system. Especially if we are
    using different sources to compose our query response, it is good to keep track
    of which sources were used (e.g., the documents that the LLM used). We can simply
    safeguard the sources that were used for generation (which documents the chunks
    correspond to). Another possibility is to mention in the prompt for the LLM the
    sources used. A more sophisticated technique is fuzzy citation query engine. Fuzzy
    matching is a string search to match the generated response to the found chunks
    (a technique that is based on dividing the words in the chunk into n-grams and
    then conducting a TF-IDF).'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献引用**并不是系统改进的技术，但它作为RAG系统的一个组成部分被高度推荐。特别是如果我们使用不同的来源来组成我们的查询响应，跟踪使用了哪些来源（例如，LLM使用的文档）是很好的。我们可以简单地保护用于生成的来源（哪些文档的块对应）。另一种可能性是在LLM的提示中提及使用的来源。一种更复杂的技术是模糊引用查询引擎。模糊匹配是对生成的响应与找到的块进行字符串搜索以匹配（一种基于将块中的单词分为n-gram然后进行TF-IDF的技术）。'
- en: '**ChatEngine** is another extension of RAG. Conducting fine-tuning of the model
    is complex, but at the same time, we want the LLM to remember previous interactions
    with the user. RAG makes it easy to do this, so we can save previous dialogues
    with users. A simple technique is to include the previous chat in the prompt.
    Alternatively, we can conduct embedding of the chats and find the highlights.
    Another technique is to try to capture the context of the user dialogue (chat
    logic). Since the discussion can wind through several messages, one solution to
    avoid a prompt that may exceed the context length is **prompt compression**. We
    reduce the prompt length by reducing the previous interaction with the user.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '**ChatEngine**是RAG的另一个扩展。对模型进行微调是复杂的，但与此同时，我们希望LLM记住与用户的先前交互。RAG使得这样做变得容易，因此我们可以保存与用户的先前对话。一种简单的方法是将之前的聊天包括在提示中。或者，我们可以对聊天进行嵌入并找到亮点。另一种技术是尝试捕捉用户对话的上下文（聊天逻辑）。由于讨论可能贯穿几条消息，为了避免可能超过上下文长度的提示，一种解决方案是**提示压缩**。我们通过减少与用户的先前交互来缩短提示长度。'
- en: 'In general, **contextual compression** is a concept that helps the LLM during
    generation. It also saves computational (or economic, if using a model via an
    API) resources. Once the documents are found, we can compress the context, with
    the aim of retaining only the relevant information. In fact, the context often
    also contains information irrelevant to the query, or even repetitions. Additionally,
    most of the words in a sentence could be predicted directly from the context and
    are not needed to provide the information to the LLM during generation. There
    are several strategies to reduce the prompt provided to the LLM:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 通常来说，**上下文压缩**是一个有助于LLM在生成过程中使用的概念。它还能节省计算（或经济，如果通过API使用模型）资源。一旦找到文档，我们可以压缩上下文，目的是只保留相关信息。实际上，上下文通常也包含与查询无关的信息，甚至重复信息。此外，句子中的大多数单词都可以直接从上下文中预测出来，因此不需要在生成过程中提供给LLM。有几种策略可以减少提供给LLM的提示：
- en: '**Context filtering**: In information theory, tokens with low entropy are easily
    predictable and thus contain redundant information (provide less relevant information
    to the LLM and have little impact on its understanding of the context). We therefore
    use an LLM that assigns an information value to each lexical unit (how much it
    expects to see that token or sentence in context). We conduct a ranking in descending
    order and keep only those tokens that are in the first p-th percentile (we decide
    this *a priori*, or it can be context-dependent).'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文过滤**：在信息理论中，低熵的标记容易被预测，因此包含冗余信息（向LLM提供较少的相关信息，对其理解上下文的影响很小）。因此，我们使用一个LLM为每个词汇单元分配一个信息值（它期望在上下文中看到该标记或句子的程度）。我们按降序进行排名，只保留位于前p百分位的标记（我们事先决定，或者它可以依赖于上下文）。'
- en: '**LongLLMLingua**: This is another approach based on information entropy and
    using information from both context and query (question aware). The approach conducts
    dynamic compression and reordering of documents to make generation more efficient.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LongLLMLingua**：这是基于信息熵的另一种方法，使用上下文和查询（问题感知）中的信息。该方法通过动态压缩和重新排序文档来提高生成效率。'
- en: '**Autocompressors**: This uses a kind of fine-tuning of the system and summary
    vectors. The idea behind it is that a long text can be summarized in a small vector
    representation (summary vectors). These vectors can be used as soft prompts to
    give context to the model. The process relies on keeping the LLM’s weights frozen
    while introducing trainable tokens into the prompt. These tokens are learned during
    training, enabling the system to be optimized end-to-end without modifying the
    model’s core parameters. During generation, these vectors are joined, and the
    model is then context-aware. Already trained models exist, as follows:'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动压缩器**：这使用了一种对系统和摘要向量的微调。其背后的想法是，长文本可以被总结成一个小的向量表示（摘要向量）。这些向量可以用作软提示，为模型提供上下文。这个过程依赖于在提示中引入可训练的标记的同时保持LLM的权重冻结。这些标记在训练期间学习，使得系统可以在不修改模型核心参数的情况下进行端到端优化。在生成过程中，这些向量被连接起来，模型随后变得具有上下文意识。已经训练好的模型如下：'
- en: '![Figure 6.11 – A) Context compression and filtering. B) Autocompressor. (Adapted
    from https://arxiv.org/abs/2305.14788)](img/B21257_06_11.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图6.11 – A) 上下文压缩和过滤。B) 自动压缩器。（改编自 https://arxiv.org/abs/2305.14788)](img/B21257_06_11.jpg)'
- en: Figure 6.11 – A) Context compression and filtering. B) Autocompressor. (Adapted
    from [https://arxiv.org/abs/2305.14788](https://arxiv.org/abs/2305.14788))
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11 – A) 上下文压缩和过滤。B) 自动压缩器。（改编自[https://arxiv.org/abs/2305.14788](https://arxiv.org/abs/2305.14788))
- en: '**Prompt engineering** is another solution to improve generation. Some suggestions
    are common to any interaction with an LLM. Thus, principles such as providing
    clear (“Reply using the context”) and unambiguous (“If the answer is not in the
    context, write I do not know”) instructions apply to RAG. There may, however,
    be specific directions or even examples for designing the best possible prompt
    for our system. Other instructions may be specific to how we want the output (for
    example, as a list, in HTML, and so on). There are also libraries for creating
    prompts for RAG that follow a specific format.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**提示工程**是提高生成的另一种解决方案。一些建议适用于与任何LLM的交互。因此，提供清晰（“使用上下文回复”）和不模糊（“如果答案不在上下文中，写我不知道”）的指令原则适用于RAG。然而，可能存在针对设计我们系统最佳提示的具体方向或甚至示例。其他指令可能特定于我们想要的输出方式（例如，作为列表、HTML等）。还有用于创建遵循特定格式的RAG提示的库。'
- en: Response optimization
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 响应优化
- en: 'The last step in a pipeline before conducting the final response is to improve
    the response from the user. One strategy is that of the **response synthesizer**.
    The basic strategy is to concatenate the prompt, context, and query and provide
    it to the LLM for generation. More sophisticated strategies involve more calls
    from the LLM. There are several alternatives to this idea:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行最终响应之前，管道中的最后一步是改进用户的响应。一种策略是**响应合成器**。基本策略是将提示、上下文和查询连接起来，并将其提供给LLM进行生成。更复杂的策略涉及LLM的更多调用。对此想法有几种替代方案：
- en: Iteratively refine the response using one chunk at a time. The previous response
    and a subsequent chunk are sent to the model to improve the response with the
    new information.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一次使用一个片段迭代地细化响应。将之前的响应和随后的片段发送到模型中，以新的信息改进响应。
- en: Generate several responses with different chunks, then concatenate them all
    together and generate a summary response.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用不同的片段生成几个响应，然后将它们全部连接起来，生成一个总结响应。
- en: Hierarchical summarization starts with the responses generated for each different
    context and recursively combines them until we arrive at a single response. While
    this approach enhances the quality of both summaries and generated answers, it
    requires significantly more LLM calls, making it costly in terms of both computational
    resources and financial expense.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层次化摘要从为每个不同上下文生成的响应开始，递归地组合它们，直到我们得到单个响应。虽然这种方法提高了摘要和生成答案的质量，但它需要显著更多的LLM调用，因此在计算资源和财务费用方面都变得昂贵。
- en: An interesting development is the possibility of using RAG as a component of
    an agent system. As we introduced in [*Chapter 4*](B21257_04.xhtml#_idTextAnchor058)*,*
    RAG can act as the memory of the system. RAG can be combined with **agents**.
    An LLM is capable of reasoning that can be merged with RAG and call-up tools or
    connect to sites when a query requires additional steps. An agent can also handle
    different components (retrieve chat history, conduct query routing, connect to
    APIs, and execute code). A complex RAG pipeline can have several components that
    are not the best fit for every situation, and an LLM can decide which are the
    best components to use.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有趣的发展是使用 RAG 作为智能体系统组件的可能性。正如我们在[*第 4 章*](B21257_04.xhtml#_idTextAnchor058)*中介绍的那样，RAG
    可以作为系统的记忆。RAG 可以与**智能体**结合。一个大型语言模型（LLM）能够进行推理，可以与 RAG 合并，并在查询需要额外步骤时调用工具或连接到网站。智能体还可以处理不同的组件（检索聊天历史、执行查询路由、连接到
    API 和执行代码）。一个复杂的 RAG 流程可以包含几个不适合每个情况的组件，而 LLM 可以决定使用哪些最佳组件。
- en: '![Figure 6.12 – Different elements in a pipeline of advanced RAG](img/B21257_06_12.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.12 – 高级 RAG 流程中的不同元素](img/B21257_06_12.jpg)'
- en: Figure 6.12 – Different elements in a pipeline of advanced RAG
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.12 – 高级 RAG 流程中的不同元素
- en: 'So far, we have assumed that a pipeline should be executed only once. The standard
    practice is we conduct retrieval once and then generate. This approach, though,
    can be insufficient for complex problems that require multi-step reasoning. There
    are three possibilities in this case:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们假设流程应该只执行一次。标准做法是我们先进行检索，然后生成。然而，对于需要多步推理的复杂问题，这种方法可能是不够的。在这种情况下有三种可能性：
- en: '**Iterative retrieval**: In this case, the retrieval is conducted multiple
    times. Given a query, we conduct the retrieval, we generate the result, and then
    the result is judged by an LLM. Depending on the judgment, we repeat the process
    up to *n* times. This process improves the robustness of the answers after each
    iteration, but it can also lead to the accumulation of irrelevant information.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**迭代检索**：在这种情况下，检索会进行多次。给定一个查询，我们进行检索，生成结果，然后由 LLM 进行判断。根据判断，我们重复这个过程，直到 *n*
    次。这个过程提高了每次迭代后答案的鲁棒性，但也可能导致无关信息的积累。'
- en: '**Recursive retrieval**: This system was developed to increase the depth and
    relevance of search results. It is similar to the previous one, but at each iteration,
    the query is refined in response to previous search results. The purpose is to
    find the most relevant information by exploiting a feedback loop. Many of these
    approaches exploit **chain-of-thought** (**CoT**) to guide the retrieval process.
    In this case, the system then breaks down the query into a series of intermediate
    steps that it must solve. This approach is advantageous when the query is not
    particularly clear or when the information sought is highly specialized or requires
    careful consideration of nuanced details.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**递归检索**：这个系统是为了增加搜索结果的深度和相关性而开发的。它与之前的一个类似，但在每次迭代中，查询会根据之前的搜索结果进行细化。目的是通过利用反馈循环来找到最相关的信息。许多这些方法利用**思维链**（**CoT**）来指导检索过程。在这种情况下，系统将查询分解成一系列必须解决的中间步骤。当查询不是特别清晰或所需信息高度专业或需要仔细考虑细微差别时，这种方法是有利的。'
- en: '**Adaptive retrieval**: In this case, the LLM actively determines when to search
    and whether the retrieved content is optimal. The LLM judges not only the retrieval
    step but also its own operation. The LLM can decide when to respond, when to search,
    or whether additional tools are needed. This approach is often used not only when
    searching on the RAG but also when conducting web searches. Flare (an adaptative
    approach to RAG) analyzes confidence during the generation process and makes a
    decision when the confidence falls below a certain threshold. Self-RAG, on the
    other hand, introduces **reflection tokens** to monitor the process and force
    an introspection of the LLM.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自适应检索**：在这种情况下，LLM 主动决定何时搜索以及检索的内容是否最优。LLM 不仅判断检索步骤，还判断自己的操作。LLM 可以决定何时响应、何时搜索或是否需要额外的工具。这种方法不仅用于在
    RAG 上搜索时，也用于进行网络搜索。Flare（一种 RAG 的自适应方法）在生成过程中分析置信度，并在置信度低于某个阈值时做出决定。另一方面，Self-RAG
    引入了**反思令牌**来监控过程并强制 LLM 进行反思。'
- en: '![Figure 6.13 – Augmentation of RAG pipelines (https://arxiv.org/pdf/2312.10997)](img/B21257_06_13.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.13 – RAG 流程的增强 (https://arxiv.org/pdf/2312.10997)](img/B21257_06_13.jpg)'
- en: Figure 6.13 – Augmentation of RAG pipelines ([https://arxiv.org/pdf/2312.10997](https://arxiv.org/pdf/2312.10997))
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.13 – RAG管道的增强 ([https://arxiv.org/pdf/2312.10997](https://arxiv.org/pdf/2312.10997))
- en: To better understand how advanced RAG techniques address known limitations,
    *Table 6.1* presents the mapping between key problems and the most effective solutions
    proposed in recent research.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解高级RAG技术如何解决已知的限制，*表6.1*展示了关键问题与最近研究中提出的最有效解决方案之间的映射。
- en: '| **Problem** **to Solve** | **Solution** |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| **需要解决的问题** | **解决方案** |'
- en: '| **Issues in naïve RAG: Latency and performance degradation with many or**
    **large documents** | Use hierarchical indexing: Summarize large sections, create
    multi-level embeddings, use metadata, and implement variations such as map-reduce
    for long documents or multi-summary for diverse topics. |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| **朴素RAG的问题：在许多或大型文档中延迟和性能下降** | 使用分层索引：总结大块，创建多级嵌入，使用元数据，并实现长文档的map-reduce或多主题的多摘要等变体。
    |'
- en: '| **Flat hierarchy limits relevance when the corpus contains an** **inherent
    structure** | Apply hierarchical indexing: Respect the document’s structure (chapters,
    headings, and subheadings), and retrieve context based on hierarchical summaries
    and embeddings. |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| **当语料库包含固有结构时，扁平层次结构限制了相关性** | 应用分层索引：尊重文档的结构（章节、标题和副标题），并根据分层摘要和嵌入检索上下文。
    |'
- en: '| **Low retrieval accuracy and domain-specific** **generalization challenges**
    | Generate and embed hypothetical questions for each chunk (Hypothetical Qs).
    Use HyDE: generate hypothetical answers to match query semantics, embed them,
    and retrieve relevant chunks. |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| **低检索准确率和特定领域的泛化挑战** | 为每个块生成和嵌入假设问题（假设Qs）。使用HyDE：生成与查询语义匹配的假设答案，嵌入它们，并检索相关块。
    |'
- en: '| **Loss of context in** **granular chunking** | Use context enrichment: Expand
    retrieved chunks with surrounding context using sentence windows or retrieve parent
    documents to broaden context. |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| **在细粒度分块中丢失上下文** | 使用上下文丰富：使用句子窗口扩展检索到的块周围的上下文，或检索父文档以扩展上下文。 |'
- en: '| **Complex queries and low recall from** **initial retrieval** | Apply query
    transformation: Decompose complex queries into subqueries, use step-back prompting
    or query expansion. Embed transformed queries for improved retrieval. |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| **从初始检索中产生的复杂查询和低召回率** | 应用查询转换：将复杂查询分解为子查询，使用回退提示或查询扩展。嵌入转换后的查询以改进检索。 |'
- en: '| **Context mismatch for specific terms** **or keywords** | Use hybrid search:
    Combine keyword-based (e.g., BM25) and vector-based retrieval using weighted scoring.
    |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| **特定术语或关键词的上下文不匹配** | 使用混合搜索：结合基于关键词的（例如，BM25）和基于向量的检索，使用加权评分。 |'
- en: '| **Inefficiency in managing diverse** **query types** | Implement query routing:
    Use logical rules, keyword-based or semantic classifiers, zero-shot models, or
    LLM-based routers to direct queries to the appropriate backends. |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| **管理多种查询类型时的低效** | 实现查询路由：使用逻辑规则、基于关键词或语义分类器、零样本模型或基于LLM的路由器将查询定向到适当的后端。
    |'
- en: '| **Loss of relevant chunks due to arbitrary** **top-k cutoff** | Apply reranking:
    Use cross-encoders, multi-vector rerankers, or LLM-based (pointwise, pairwise,
    or listwise) reranking to reorder retrieved chunks. |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| **由于任意的top-k截止值而丢失相关块** | 应用重新排序：使用交叉编码器、多向量重新排序器或基于LLM的（点对点、成对或列表）重新排序来重新排序检索到的块。
    |'
- en: '| **Loss of information or efficiency in** **LLM context** | Use context compression:
    Filter low-entropy tokens, compress or reorder chunks dynamically (e.g., LongLLMLingua),
    or apply summary vectors and autocompressors. |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| **在LLM上下文中丢失信息或效率** | 使用上下文压缩：过滤低熵标记，动态压缩或重新排序块（例如，LongLLMLingua），或应用摘要向量和自动压缩器。
    |'
- en: '| **Inefficient** **response generation** | Optimize responses: Use iterative
    refinement, hierarchical summarization, or multi-step response synthesis. Improve
    prompt quality and specificity. |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| **低效的响应生成** | 优化响应：使用迭代细化、分层摘要或多步响应合成。提高提示质量和特异性。 |'
- en: '| **Memory limitations in** **dialogue systems** | Use ChatEngine techniques:
    Save and embed past conversations, compress user dialogue, and merge chat history
    with current queries. |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| **对话系统中的内存限制** | 使用ChatEngine技术：保存并嵌入过去的对话，压缩用户对话，并将聊天历史与当前查询合并。 |'
- en: '| **Need for complex reasoning or dynamic** **query adaptation** | Adopt adaptive
    and multi-step retrieval: Use recursive, iterative approaches with feedback loops
    and self-reflection (e.g., Flare, Self-RAG). |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| **需要复杂推理或动态查询适应** | 采用自适应和多步骤检索：使用递归、迭代方法，并带有反馈循环和自我反思（例如，Flare，Self-RAG）。|'
- en: '| **Lack of source tracking in** **generated responses** | Include citations:
    Use fuzzy citation matching, metadata tagging, or embed source references in prompts.
    |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| **生成响应中缺乏源追踪** | 包含引用：使用模糊引用匹配、元数据标记或在提示中嵌入源引用。|'
- en: '| **Need for pipeline customization based on query complexity** **or modality**
    | Augment RAG pipelines: Combine with agents for reasoning, tool use, and decision-making.
    Apply adaptive and recursive retrieval loops for complex queries. |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| **根据查询复杂性**或**模态**进行管道定制的需求 | 增强RAG管道：与推理、工具使用和决策的代理结合。对于复杂查询，应用自适应和递归检索循环。|'
- en: Table 6.1 – Problems and solutions in RAG
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.1 – RAG中的问题和解决方案
- en: Modular RAG and its integration with other systems
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模块化RAG及其与其他系统的集成
- en: Modular RAG is a further advancement; it can be considered as an extension of
    advanced RAG but focused on adaptability and versatility. In this sense, the modular
    system means it has separate components that can be used either sequentially or
    in parallel.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 模块化RAG是一个进一步的发展；它可以被视为高级RAG的扩展，但更侧重于适应性和多功能性。在这种情况下，模块化系统意味着它具有可以单独使用或并行使用的独立组件。
- en: 'The pipeline itself is remodeled, with alternating search and generation. In
    general, modular RAG involves optimizing the system toward performance and adapting
    to different tasks. Modular RAG introduces modules for this that are specialized.
    Some examples of the modules that are included are as follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 管道本身经过重新设计，交替进行搜索和生成。一般来说，模块化RAG涉及优化系统性能并适应不同任务。模块化RAG引入了专门化的模块来实现这一点。以下是一些包含的模块示例：
- en: '**Search module**: This module is responsible for finding relevant information
    about a query. It allows searching through search engines, databases, and **knowledge
    graphs** (**KGs**). It can also use sophisticated search algorithms, use machine
    learning, and execute code.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**搜索模块**：此模块负责查找关于查询的相关信息。它允许通过搜索引擎、数据库和**知识图谱**（**KGs**）进行搜索。它还可以使用复杂的搜索算法、机器学习并执行代码。'
- en: '**Memory module**: This module serves to store relevant information during
    the search process. In addition, the system can retrieve context that was previously
    searched.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**记忆模块**：此模块用于在搜索过程中存储相关信息。此外，系统可以检索之前搜索过的上下文。'
- en: '**Routing module**: This module tries to identify the best path for a query,
    where it can either search for different information in different databases or
    decompose the query.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**路由模块**：此模块试图确定查询的最佳路径，它可以在不同的数据库中搜索不同的信息或分解查询。'
- en: '**Generation module**: Different queries may require a different type of generation,
    such as summarization, paraphrasing, and context expansion. The focus of this
    module is on improving the quality and relevance of the output.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成模块**：不同的查询可能需要不同类型的生成，例如摘要、释义和上下文扩展。此模块的重点是提高输出质量和相关性。'
- en: '**Task-adaptable module**: This module allows dynamic adaptation to tasks that
    are requested from the system. In this way, the system dynamically adjusts retrieval,
    processing, and generation.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任务自适应模块**：此模块允许系统动态适应请求的任务。这样，系统可以动态调整检索、处理和生成。'
- en: '**Validation module**: This module evaluates retrieved responses and context.
    The system can identify errors, biases, and inconsistencies. The process becomes
    iterative, in which the system can improve its responses.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**验证模块**：此模块评估检索到的响应和上下文。系统可以识别错误、偏见和不一致性。这个过程变得迭代，系统可以改进其响应。'
- en: '![Figure 6.14 – Three different paradigms of RAG (https://arxiv.org/pdf/2312.10997)](img/B21257_06_14.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![图6.14 – RAG的三个不同范式 (https://arxiv.org/pdf/2312.10997)](img/B21257_06_14.jpg)'
- en: Figure 6.14 – Three different paradigms of RAG ([https://arxiv.org/pdf/2312.10997](https://arxiv.org/pdf/2312.10997))
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.14 – RAG的三个不同范式 ([https://arxiv.org/pdf/2312.10997](https://arxiv.org/pdf/2312.10997))
- en: Modular RAG offers the advantage of adaptability because these modules can be
    replaced or reconfigured as needed. The flow between different modules can be
    finely tuned, allowing an additional level of flexibility. Furthermore, if naïve
    and advanced RAG are characterized by a “retrieve and read” mechanism, modular
    RAG allows “retrieve, read, and rewrite.” In fact, through the ability to evaluate
    and provide feedback, the system can refine the response to the query.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 模块化RAG具有适应性优势，因为这些模块可以根据需要替换或重新配置。不同模块之间的流程可以精细调整，从而提供额外的灵活性。此外，如果原始和高级RAG以“检索和阅读”机制为特征，那么模块化RAG允许“检索、阅读和重写”。实际上，通过评估和提供反馈的能力，系统可以优化对查询的响应。
- en: As this new paradigm spread, interesting alternatives were experimented with,
    such as integrating information coming from the parametric memory of the LLM.
    In this case, the model is asked to generate a response before retrieval (recite
    and answer). **Demonstrate-search-predict** (**DSP**) shows how you can have different
    interactions between the LLM and RAG to solve complex queries (or knowledge-intensive
    tasks). DSP shows how a modular RAG allows for robust and flexible pipelines at
    the same time. **Self-reflective retrieval-augmented generation** (**Self-RAG**),
    on the other hand, introduces an element of criticism into the system. The LLM
    reflects on what it generates, critiquing its output in terms of factuality and
    overall quality. Another alternative is to use interleaved CoT generation and
    retrieval. These approaches usually work best when we have issues that require
    reasoning.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 随着这一新范式的传播，人们尝试了有趣的可选方案，例如整合来自LLM参数化记忆的信息。在这种情况下，模型在检索之前被要求生成一个响应（背诵和回答）。**演示-搜索-预测**（**DSP**）展示了如何通过LLM和RAG之间的不同交互来解决复杂查询（或知识密集型任务）。DSP展示了模块化RAG如何同时实现稳健和灵活的管道。另一方面，**自我反思的检索增强生成**（**Self-RAG**）将批评元素引入系统。LLM反思它所生成的，从事实性和整体质量的角度对其输出进行批评。另一种选择是使用交错CoT生成和检索。这些方法通常在我们有需要推理的问题时效果最好。
- en: Training and training-free approaches
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练和无训练方法
- en: 'RAG approaches fall into two groups: training-free and training-based. Naïve
    RAG approaches are generally considered training-free. **Training-free** means
    that the two main components of the system (the embedder and LLM) are kept frozen
    from the beginning. This is possible because they are two components that are
    pre-trained and therefore have already acquired capabilities that allow us to
    use them.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: RAG方法分为两组：无训练和基于训练。原始RAG方法通常被认为是无训练的。**无训练**意味着系统的两个主要组件（嵌入器和LLM）从一开始就被冻结。这是可能的，因为它们是两个预训练的组件，因此已经获得了使我们能够使用它们的技能。
- en: 'Alternatively, we can have three types of **training-based approaches**: independent
    training, sequential training, and joint training.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以有三种基于训练的方法：独立训练、顺序训练和联合训练。
- en: In **independent training**, both the retriever and LLMs are trained separately
    in totally independent processes (there is no interaction during training). In
    this case, we have separate fine-tuning of the various components of the system.
    This approach is useful when we want to adapt our system to a specific domain
    (legal, financial, or medical, for example). Compared to a training-free approach,
    this type of training improves the capabilities of the system for the domain of
    our application. LLMs can also be fine-tuned to make better use of the context.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在**独立训练**中，检索器和LLMs在完全独立的过程中分别进行训练（训练过程中没有交互）。在这种情况下，我们对系统的各个组成部分进行单独的微调。当我们想要将我们的系统适应到特定领域（例如法律、金融或医疗）时，这种方法很有用。与无训练方法相比，这种类型的训练提高了系统对我们应用领域的功能。LLMs也可以进行微调，以更好地利用上下文。
- en: '**Sequential training**, on the otherRAG:sequential training” hand, assumes
    that we use these two components sequentially, so it is better to find a form
    of training that increases the synergy between these components. The components
    can first be trained independently, following which they are trained sequentially.
    One of the components is kept frozen while the other undergoes additional training.
    Depending on what the order of training is, we can have two classes, retriever-first
    or LLM-first:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '**顺序训练**，另一方面，假设我们按顺序使用这两个组件，因此最好找到一种能够增加这些组件之间协同效应的训练形式。组件可以先独立训练，然后按顺序训练。其中一个组件保持冻结状态，而另一个则进行额外的训练。根据训练的顺序，我们可以有两种类型，检索优先或LLM优先：'
- en: '**Retriever-first**: In this class, the trainer’s training is conducted and
    then it is kept frozen. Then, the LLM is trained to understand how to use the
    knowledge in the retriever context. For example, we conduct the fine-tuning of
    our retriever independently and then we conduct fine-tuning of the LLM using the
    retrieved chunks. The LLM receives the retriever chunks during its fine-tuning
    and learns how best to use this context for generation.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**检索优先**：在这个课程中，训练者的训练是先进行的，然后保持冻结状态。然后，训练语言模型（LLM）理解如何在检索器上下文中使用知识。例如，我们独立地对检索器进行微调，然后使用检索到的片段对LLM进行微调。LLM在其微调过程中接收检索器片段，并学习如何最好地使用这个上下文进行生成。'
- en: '**LLM-first**: This is a bit more complex, but it uses the supervision of an
    LLM to train the retriever. An LLM is usually a much more capable model than the
    retriever because it has many more parameters and has been trained on many more
    tokens, thus making it a good supervisor. In a sense, this approach can be seen
    as a kind of knowledge distillation in which we take advantage of the greater
    knowledge of a larger model to train a smaller model.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LLM优先**：这稍微复杂一些，但它是利用LLM的监督来训练检索器。LLM通常比检索器更强大，因为它有更多的参数，并且已经在更多的标记上进行了训练，因此它是一个很好的监督者。从某种意义上说，这种方法可以被视为一种知识蒸馏，我们利用更大模型的更多知识来训练更小的模型。'
- en: '| **Training Approach** | **Domains/Applications** | **Reasoning** |  |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| **训练方法** | **领域/应用** | **推理** |  |'
- en: '| **Retriever-first** | **Search engines (general** **or domain-specific)**For
    example, legal document search, medical literature search, or e-commerce product
    search | Focuses on retrieving the most relevant documents quickly and accurately.
    Essential for systems where domain-specific precision is critical, and the retriever
    must handle vast, structured, or semi-structured corpora. |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| **检索优先** | **搜索引擎（通用或特定领域）**例如，法律文件搜索，医学文献搜索，或电子商务产品搜索 | 专注于快速准确地检索最相关的文档。对于领域特定精度至关重要的系统，检索器必须处理大量、结构化或半结构化语料库。|'
- en: '|  | **Enterprise** **knowledge management**For example, internal corporate
    documentation, FAQs, or CRM systems | Emphasizes retrieving the right documents
    efficiently from proprietary databases, where the quality of retrieval has a more
    significant impact than the quality of generation. |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '|  | **企业知识管理**例如，内部企业文档，常见问题解答，或CRM系统 | 强调从专有数据库中高效检索正确的文档，其中检索的质量比生成的质量有更大的影响。|'
- en: '|  | **Scientific** **research repositories**For example, PubMed, arXiv, or
    patents | Ensures precise and recall-optimized retrieval in highly technical or
    specialized fields where high-quality retrieval is essential for downstream tasks
    such as summarization or report generation. |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '|  | **科学研究存储库**例如，PubMed，arXiv，或专利 | 确保在高度技术或专业领域中进行精确和召回优化的检索，在这些领域中，高质量的检索对于下游任务（如摘要或报告生成）至关重要。|'
- en: '|  | **Regulatory and** **compliance systems**For example, financial compliance
    checks, or legal case law databases | In domains where accuracy and compliance
    are critical, the retriever must reliably surface the most relevant content while
    minimizing irrelevant or low-confidence retrievals. |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '|  | **监管和合规系统**例如，金融合规检查，或法律案例法数据库 | 在准确性和合规性至关重要的领域，检索器必须可靠地呈现最相关的内容，同时最大限度地减少不相关或低置信度的检索。|'
- en: '| **LLM-first** | **Conversational agents**For example, customer support chatbots
    or personal assistants | Relies heavily on the generative capabilities of the
    LLM to provide nuanced, conversational responses. Retrieval is secondary as the
    LLM interprets and integrates retrieved content. |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| **LLM-first** | **对话代理**例如，客户支持聊天机器人或个人助理 | 严重依赖LLM的生成能力来提供细微、对话式的响应。检索是次要的，因为LLM解释并整合检索到的内容。|'
- en: '|  | **Creative applications**For example, content writing, storytelling, or
    brainstorming | The LLM’s ability to create, synthesize, and infer from retrieved
    data is paramount. Retrieval supports generation by providing a broader context
    rather than being the focal point of optimization. |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '|  | **创意应用**例如，内容创作、讲故事或头脑风暴 | LLM从检索数据中创建、综合和推断的能力至关重要。检索通过提供更广泛的环境来支持生成，而不是作为优化的焦点。|'
- en: '|  | **Complex** **reasoning tasks**For example, multi-step problem-solving
    or decision-making systems | The LLM’s role as a reasoner outweighs retrieval
    precision, as the focus is on the ability to process, relate, and infer knowledge.
    Retrieval primarily ensures access to supplementary information for reasoning.
    |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '|  | **复杂推理任务**例如，多步骤问题解决或决策系统 | LLM作为推理者的角色超过了检索的精确度，因为重点是处理、关联和推断知识的能力。检索主要确保推理时能够访问补充信息。|'
- en: '|  | **Educational tools**For example, learning assistants or personalized
    tutoring systems | The LLM’s ability to adapt and generate instructional content
    tailored to the user’s context is more critical than precise retrieval. Retrieval
    serves as a secondary mechanism to ensure the completeness of information. |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '|  | **教育工具**例如，学习助手或个性化辅导系统 | LLM适应并生成针对用户上下文定制的教学内容的能 力比精确检索更重要。检索作为次要机制，确保信息的完整性。|'
- en: Table 6.2 – Training approaches
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.2 – 训练方法
- en: According to this article (Izacard, [https://arxiv.org/abs/2012.04584](https://arxiv.org/abs/2012.04584)),
    attention activation values in the LLM are a good proxy for defining the relevance
    of a document, so they can be used to provide a label (a kind of guide) to the
    retriever on how good the search results are. Hence, the retriever is trained
    with a metric based on attention in the LLM. For a less expensive approach, a
    small LLM can be used to generate the label to then train the retriever. There
    are then variations in these approaches, but all are based on the principle that
    once we have fine-tuned the LLM, we want to align the retriever.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这篇文章（Izacard，[https://arxiv.org/abs/2012.04584](https://arxiv.org/abs/2012.04584)），LLM中的注意力激活值是定义文档相关性的良好代理，因此它们可以用来为检索器提供标签（一种指南），以说明搜索结果有多好。因此，检索器使用基于LLM中注意力的度量进行训练。对于一种更经济的方案，可以使用一个小型LLM来生成标签，然后训练检索器。这些方法有各种变体，但所有方法都基于这样一个原则：一旦我们微调了LLM，我们希望使检索器与之对齐。
- en: '**Joint methods**, on the other hand, represent end-to-end training of the
    system. In other words, both the retriever and the generator are aligned at the
    same time (simultaneously). The idea is that we want the system to simultaneously
    improve both its ability to find knowledge and its ability to use this knowledge
    for generation. The advantage is that we have a synergistic effect during training.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，**联合方法**代表系统的端到端训练。换句话说，检索器和生成器同时（同时）对齐。我们的想法是希望系统同时提高其发现知识的能力和使用这些知识进行生成的能力。优势在于我们在训练期间有协同效应。
- en: '![Figure 6.15 – Different training methods in RAG (https://arxiv.org/pdf/2405.06211)](img/B21257_06_15.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![图6.15 – RAG中的不同训练方法](https://arxiv.org/pdf/2405.06211)(img/B21257_06_15.jpg)'
- en: Figure 6.15 – Different training methods in RAG ([https://arxiv.org/pdf/2405.06211](https://arxiv.org/pdf/2405.06211))
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.15 – RAG中的不同训练方法([https://arxiv.org/pdf/2405.06211](https://arxiv.org/pdf/2405.06211))
- en: Now that we know the different modifications that we can apply to our RAG, let’s
    try them in the next section.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了我们可以应用到我们的RAG上的不同修改，让我们在下一节尝试它们。
- en: Implementing an advanced RAG pipeline
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实施高级RAG管道
- en: 'In this section, we will describe how an advanced RAG pipeline can be implemented.
    In this pipeline, we use a more advanced version of naïve RAG, including some
    add-ons to improve it. This shows us how the starting basis is a classic RAG pipeline
    (embedding, retrieval, and generation) but more sophisticated components are inserted.
    In this pipeline, we have used the following add-ons:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将描述如何实现高级RAG管道。在这个管道中，我们使用朴素RAG的更高级版本，包括一些附加组件来改进它。这表明我们的起始基础是一个经典的RAG管道（嵌入、检索和生成），但插入了一些更复杂的组件。在这个管道中，我们使用了以下附加组件：
- en: '**Reranker**: This allows us to sort the context found during the retrieval
    step. This is one of the most widely used elements in advanced RAG because it
    has been seen to significantly improve results.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重排器**：这允许我们在检索步骤中找到的上下文中进行排序。这是高级RAG中最广泛使用的元素之一，因为它已被证明可以显著提高结果。'
- en: '**Query transformation**: In this case, we are using a simple query transformation.
    This is because we want to try to broaden our retrieval range, since some relevant
    documents may be missed.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**查询转换**：在这种情况下，我们使用简单的查询转换。这是因为我们希望尝试扩大我们的检索范围，因为一些相关文档可能会被遗漏。'
- en: '**Query routing**: This prevents us from treating all queries the same and
    allows us to establish rules for more efficient retrieval.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**查询路由**：这使我们能够避免对所有查询一视同仁，并允许我们为更有效的检索建立规则。'
- en: '**Hybrid search**: With this, we combine the power of keyword-based search
    with semantic search.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**混合搜索**：通过这种方式，我们将基于关键词的搜索的强大功能与语义搜索相结合。'
- en: '**Summarization**: With this, we try to eliminate redundant information from
    our retrieved context.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**摘要**：通过这种方式，我们尝试从检索到的上下文中消除冗余信息。'
- en: Of course, we could add other components, but generally, these are the most
    commonly used and give an overview of what components we can add to naïve RAG.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们可以添加其他组件，但通常，这些是最常用的，并概述了我们可以添加到朴素RAG中的组件。
- en: 'We can see in the following figure how our pipeline is modified:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在以下图中看到我们的管道是如何修改的：
- en: '![Figure 6.16 – Pipeline of advanced RAG](img/B21257_06_16.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![图6.16 – 高级RAG的管道](img/B21257_06_16.jpg)'
- en: Figure 6.16 – Pipeline of advanced RAG
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.16 – 高级RAG的管道
- en: 'The complete code can be found in the repository; here, we will just see the
    highlights. In this code snippet, we are defining a function to represent the
    query transformation. In this case, we are developing only a small modification
    of the query (searching for other related terms in our query):'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的代码可以在仓库中找到；在这里，我们只看重点。在这个代码片段中，我们定义了一个函数来表示查询转换。在这种情况下，我们只是在查询中进行了小的修改（搜索查询中的其他相关术语）：
- en: '[PRE0]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we perform query routing. Query routing enforces a simple rule: if specific
    keywords are present in the query, a keyword-based search is performed; otherwise,
    a semantic (embedding-based) search is used. In some cases, we may want to first
    retrieve only documents that contain certain keywords—such as references to a
    specific product—and then narrow the results further using semantic search:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们执行查询路由。查询路由强制执行一个简单的规则：如果查询中存在特定关键词，则执行基于关键词的搜索；否则，使用语义（基于嵌入）搜索。在某些情况下，我们可能首先只想检索包含某些关键词的文档——例如，对特定产品的引用——然后使用语义搜索进一步缩小结果：
- en: '[PRE1]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we perform a hybrid search, which allows us to use search based on semantic
    and keyword content. This is one of the most widely used components in RAG pipelines
    today. When chunking is used, sometimes documents relevant to a query can only
    be found because they contain a keyword (e.g., the name of a product, a person,
    and so on). Obviously, not all chunks that contain a keyword are relevant documents
    (especially for queries where we are more interested in a semantic concept). With
    hybrid search, we can balance the two types of search, choosing how many chunks
    to take from one or the other type of search:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们执行混合搜索，这使我们能够使用基于语义和关键词内容的搜索。这是今天RAG管道中最广泛使用的组件之一。当使用分块时，有时只有包含关键词的文档与查询相关（例如，产品的名称、人名等）。显然，并非所有包含关键词的分块都是相关文档（特别是对于我们更感兴趣于语义概念的查询）。使用混合搜索，我们可以平衡两种类型的搜索，选择从一种或另一种类型的搜索中取多少分块：
- en: '[PRE2]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As mentioned, the reranker is one of the most frequently used elements; it
    is a transformer that is used to reorder the context. If we have found 10 chunks,
    we reorder the found chunks and usually take a subset of them. Sometimes, semantic
    search can find the most relevant chunks again, but these may then be found further
    down the order. The reranker ensures that these chunks are then actually placed
    in the context of the LLM:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，重排器是使用最频繁的元素之一；它是一个用于重新排序上下文的transformer。如果我们找到了10个片段，我们会重新排序找到的片段，并通常取它们的一个子集。有时，语义搜索可以再次找到最相关的片段，但这些片段可能被放置在顺序的更下方。重排器确保这些片段实际上被放置在LLM的上下文中：
- en: '[PRE3]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'As mentioned earlier, context can also contain information that is redundant.
    LLMs are sensitive to noise, so reducing this noise can help generation. In this
    case, we use an LLM to summarize the found context (of course, we set a limit
    to avoid losing too much information):'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，上下文也可能包含冗余信息。LLM对噪声很敏感，因此减少这种噪声可以帮助生成。在这种情况下，我们使用一个LLM来总结找到的上下文（当然，我们设定了一个限制，以避免丢失太多信息）：
- en: '[PRE4]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Once defined, we just need to assemble them into a single pipeline. Once that’s
    done, we can use our RAG pipeline. Check the code in the repository and play around
    with the code. Once you have a RAG pipeline that works, the next natural step
    is deployment. In the next section, we will discuss potential challenges to the
    deployment.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦定义，我们只需将它们组装成一个单一的管道。一旦完成，我们就可以使用我们的RAG管道。检查存储库中的代码并尝试修改代码。一旦你有一个可以工作的RAG管道，下一步自然就是部署。在下一节中，我们将讨论部署可能面临的潜在挑战。
- en: Understanding the scalability and performance of RAG
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解RAG的可扩展性和性能
- en: In this section, we will mainly describe challenges that are related to the
    commissioning of a RAG system or that may emerge with the scaling of the system.
    The main advantage of RAG over an LLM is that it can be scaled without conducting
    additional training. The purpose and requirements of development and production
    are mainly different. LLMs and RAG pose new challenges, especially when you want
    to take a system into production. Productionizing means taking a complex system
    such as RAG from a prototype to a stable, operational environment. This can be
    extremely complex when you have to manage different users who may be connected
    remotely. While in development, accuracy might be the most important metric, while
    in production, special care must be taken to balance performance and cost.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将主要描述与RAG系统部署相关或随着系统扩展可能出现的挑战。与LLM相比，RAG的主要优势在于它可以不进行额外训练而进行扩展。开发和生产的目的和需求主要不同。LLM和RAG提出了新的挑战，尤其是在你希望将系统投入生产时。生产化意味着将像RAG这样的复杂系统从原型转变为稳定、可操作的环境。当你必须管理可能远程连接的不同用户时，这可能会非常复杂。在开发过程中，准确性可能是最重要的指标，而在生产过程中，必须特别小心地平衡性能和成本。
- en: Large organizations, in particular, may already have big data stored and may
    therefore want to use RAG with it. Big data can be a significant challenge for
    a RAG system, especially considering the volume, velocity, and variety of data.
    **Scalability** is a critical concern when discussing big data; the same principle
    applies to RAG.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其是大型组织，可能已经存储了大量的数据，因此可能希望使用RAG与这些数据结合。大数据对RAG系统来说可能是一个重大的挑战，特别是考虑到数据的量、速度和多样性。"可扩展性"在讨论大数据时是一个关键问题；同样的原则也适用于RAG。
- en: Data scalability, storage, and preprocessing
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据可扩展性、存储和预处理
- en: 'So far, we have talked about how to find information. We have assumed that
    the data is in textual form. The data structure of the text is an important parameter,
    and putting it into production can be problematic. So, our system may have to
    integrate the following:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了如何查找信息。我们假设数据是以文本形式存在的。文本的数据结构是一个重要的参数，将其投入生产可能会遇到问题。因此，我们的系统可能需要整合以下内容：
- en: '**Unstructured data**: Text is the most commonly used data type present in
    a corpus. It can have different origins: encyclopedic (from Wikipedia), domain-specific
    (scientific, medical, or financial), industry-specific (reports or standard documents),
    downloaded from the internet, or user chat. It can thus be generated by humans
    but also include data generated by automated systems or by LLMs themselves (previous
    interactions with users). In addition, it can be multi-language, and the system
    may have to conduct a cross-language search. Today, there are both LLMs that have
    been trained with different languages and multi-lingual embedders (specifically
    designed for multi-lingual capabilities). There are also other types of unstructured
    data, such as image and video. We will discuss multimodal RAG in a little more
    detail in the next section.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非结构化数据**：文本是语料库中最常用的数据类型。它可以有不同的来源：百科全书式（来自维基百科），特定领域（科学、医学或金融），行业特定（报告或标准文件），从互联网下载，或用户聊天。因此，它可以由人类生成，但也可能包括由自动化系统或LLMs本身（与用户的先前交互）生成的数据。此外，它可能是多语言的，系统可能需要进行跨语言搜索。今天，既有用不同语言训练的LLMs，也有多语言嵌入器（专门设计用于多语言能力）。还有其他类型的非结构化数据，如图像和视频。我们将在下一节更详细地讨论多模态RAG。'
- en: '**Semi-structured data**: Generally, this means data that contains a mixture
    of textual and table information (such as PDFs). Other examples of semi-structured
    data are JSON, XML, and HTML. These types of data are often complex to use with
    RAG. There are usually file-specific pipelines (chunking, metadata storing, and
    so on) because they can create problems for the system. In the case of PDF, chunking
    can separate tables into multiple chunks, making retrieval inefficient. In addition,
    tables make similarity search more complicated. An alternative is to extract the
    tables and turn them into text or insert them into compatible databases (such
    as SQL). Since the available methods are not yet optimal, there is still intense
    research in the field.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**半结构化数据**：通常，这意味着包含文本和表格信息的混合数据（如PDF）。半结构化数据的其他例子包括JSON、XML和HTML。这些类型的数据通常很难与RAG一起使用。通常有特定于文件的管道（分块、元数据存储等），因为它们可能会给系统带来问题。在PDF的情况下，分块可以将表格分成多个块，使得检索效率低下。此外，表格使相似性搜索更加复杂。一种替代方法是提取表格并将它们转换为文本或插入到兼容的数据库中（如SQL）。由于可用的方法尚未优化，该领域的研究仍然非常活跃。'
- en: '**Structured data**: Structured data is data that is in a standardized format
    that can be accessed efficiently by both humans and software. Structured data
    generally has some special features: defined attributes (same attributes for all
    data values as in a table), relational attributes (tables have common values that
    tie different datasets together; for example, in a customer dataset, there are
    IDs that allow users and their purchases to be found), quantitative data (data
    is optimized for mathematical analysis), and storage (data is stored in a particular
    format and with precise rules). Examples of structured data are Excel files, SQL
    databases, web form results, point-of-sale data, and product directories. Another
    example of structured data is KGs, which we will discuss in detail in the next
    chapter.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结构化数据**：结构化数据是指以标准格式存储的数据，可以由人类和软件高效访问。结构化数据通常具有一些特殊特征：定义了属性（所有数据值都使用表格中的相同属性），关系属性（表具有将不同数据集连接起来的公共值；例如，在客户数据集中，有ID可以找到用户及其购买记录），定量数据（数据优化用于数学分析），以及存储（数据以特定格式和精确规则存储）。结构化数据的例子包括Excel文件、SQL数据库、网页表单结果、销售点数据和产品目录。另一个结构化数据的例子是知识图谱（KGs），我们将在下一章详细讨论。'
- en: These factors must be taken into account. For example, if we are designing a
    system that needs to search for compliance documents in various regions and in
    different languages, we need a RAG that can conduct cross-lingual retrieval. If
    our organization has primarily one type of data (PDF or SQL databases), it is
    important to take this into account and optimize the system to search for this
    type of data. There are specific alternatives to improve the capabilities of RAGs
    with structured data. One example, chain-of-table, is a method that integrates
    CoT prompting with table transformations. In a step-by-step process with an LLM
    and a set of predefined operations, it extracts and modifies tables. This approach
    is designed for handling complex tables, and it exploits step-by-step reasoning
    and step-by-step tabular operations to accomplish this. This approach is useful
    if we have complex SQL databases or large amounts of data frames as data sources.
    Then, there are more sophisticated alternatives that combine symbolic reasoning
    and textual reasoning. Mix self-consistency is a dedicated approach to tabular
    data understanding that uses textual and symbolic reasoning with self-consistency,
    thus creating multi-paths of reasoning and then aggregating with self-consistency.
    For semi-structured data such as PDFs and JHTML, there are dedicated packages
    that allow us to extract information from them or to parse data.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这些因素必须被考虑在内。例如，如果我们正在设计一个需要在各个地区和不同语言中搜索合规文件的系统，我们需要一个能够进行跨语言检索的RAG。如果我们组织主要有一种类型的数据（PDF或SQL数据库），那么考虑这一点并优化系统以搜索此类数据是很重要的。有一些特定的替代方案可以改善RAGs处理结构化数据的能力。一个例子是链表，这是一种将CoT提示与表转换相结合的方法。在LLM和一组预定义操作的逐步过程中，它提取并修改表格。这种方法旨在处理复杂的表格，并利用逐步推理和逐步表格操作来完成这一任务。这种方法在数据源为复杂的SQL数据库或大量数据框时很有用。然后，还有更复杂的替代方案，它们结合了符号推理和文本推理。混合自洽是一种针对表格数据理解的专用方法，它使用文本和符号推理以及自洽性，从而创建推理的多路径，然后通过自洽性进行聚合。对于PDF和JHTML等半结构化数据，有专门的包允许我们从它们中提取信息或解析数据。
- en: It is not only the type of data that impacts RAG performance but also the amount
    of data itself. As the volume of data increases, so does the difficulty in finding
    relevant information. Likewise, it is likely to increase the latency of the system.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅数据类型会影响RAG的性能，数据量本身也会影响。随着数据量的增加，找到相关信息变得越来越困难。同样，这也可能增加系统的延迟。
- en: '**Data storage** is one of the focal points to be addressed before bringing
    the system into production. Distributed storage systems (an infrastructure that
    divides data into several physical servers or data centers) can be a solution
    for large volumes of data. This has the advantage of increasing system speed and
    reducing the risk of data loss, but risks increasing costs and management complexity.
    When you have different types of data, it can be advantageous to use a structure
    called a data lake. A **data lake** is a centralized repository that is designed
    for the storage and processing of structured, semi-structured, and unstructured
    data. The advantage of the data lake is that it is a scalable and flexible structure
    for ingesting, processing, and storing data of different types. The data lake
    is advantageous for RAG because it allows more data context to be maintained than
    other data structures. On the other hand, data lakes require more expertise to
    be functional. Alternatives may be partitioning data into smaller, more manageable
    partitions (based on geography, topic, time, and so on), which allows more efficient
    retrieval. In the case of numerous requests, frequently accessed data caching
    can be conducted to avoid repetition. These strategies can be used in the case
    of big data storage and access.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据存储**是在将系统投入生产之前需要解决的焦点之一。分布式存储系统（将数据分割成几个物理服务器或数据中心的基础设施）可以是大数据量的解决方案。这具有提高系统速度和降低数据丢失风险的优点，但可能会增加成本和管理复杂性。当你有不同类型的数据时，使用称为数据湖的结构可能是有利的。**数据湖**是一个设计用于存储和处理结构化、半结构化和非结构化数据的集中式存储库。数据湖的优势在于它是一个可扩展且灵活的结构，用于摄取、处理和存储不同类型的数据。数据湖对RAG有利，因为它允许比其他数据结构维护更多的数据上下文。另一方面，数据湖需要更多的专业知识才能发挥作用。替代方案可能包括将数据分割成更小、更易于管理的分区（基于地理位置、主题、时间等），这允许更有效的检索。在大量请求的情况下，频繁访问的数据缓存可以避免重复。这些策略可以在大数据存储和访问的情况下使用。'
- en: Another important aspect is building solid pipelines for **data preprocessing
    and cleaning**. In the development stage, it is common to work with well-polished
    datasets, but in production, this is not the case. Especially in big data, it
    is essential to make sure that there are no inconsistencies or that the system
    can handle missing or incomplete data. In a big data environment, data comes from
    many sources and not all of them are good quality. Therefore, imputation techniques
    (KNN or others) can be used to fill in missing data. Other additions that can
    improve the process are techniques to eliminate noisy or erroneous data, such
    as outlier detection algorithms, normalization techniques, and regular expression
    techniques to eliminate erroneous data points.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的方面是构建用于**数据预处理和清洗**的稳固管道。在开发阶段，通常与经过良好打磨的数据集一起工作，但在生产中并非如此。特别是在大数据中，确保没有不一致性或系统可以处理缺失或不完整的数据至关重要。在大数据环境中，数据来自许多来源，并非所有来源都是高质量的。因此，可以使用插补技术（如KNN或其他）来填补缺失数据。其他可以改进流程的补充包括消除噪声或错误数据的技巧，例如异常检测算法、归一化技术和用于消除错误数据点的正则表达式技术。
- en: '**Data deduplication** is another important aspect when working with LLMs.
    Duplicate data harms the training of LLMs and is also detrimental when found during
    the generation process (risking outputs that are inaccurate, biased, or of poor
    quality). As the volume of data increases, data duplication is a risk that increases
    linearly. There are techniques such as fuzzy matching and hash-based deduplication
    that can be used to eliminate duplicate elements. In general, a pipeline should
    be created to control the quality and governance of the data in the system (data
    quality monitoring). These pipelines should include rules and tracking systems
    to be able to identify problematic data and its origin. Although these pipelines
    are essential, pipelines that are too complex to maintain or slow down the system
    too much should be avoided.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据去重**是处理LLMs时另一个重要的方面。重复数据会损害LLMs的训练，在生成过程中被发现时也会造成损害（可能导致不准确、有偏见或质量低下的输出）。随着数据量的增加，数据重复的风险会线性增加。可以使用模糊匹配和基于哈希的去重技术来消除重复元素。通常，应该创建一个管道来控制系统中数据的质量和治理（数据质量监控）。这些管道应包括规则和跟踪系统，以便能够识别有问题的数据和其来源。尽管这些管道是必不可少的，但过于复杂难以维护或过多地减慢系统速度的管道应避免。'
- en: Once we have decided on our data storage infrastructure, we need to make sure
    we have efficient **data indexing and retrieval**. There are indexing methods
    that are specialized for big data, such as Apache Lucene or Elasticsearch. Also,
    the most used data can be cached, or the retrieval process can be distributed
    to create a parallel infrastructure and reduce bottlenecks when there are multiple
    users. Given the complexity of some of these techniques, it is always best to
    test and conduct benchmarks before putting them into production.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们确定了我们的数据存储基础设施，我们需要确保我们拥有高效的**数据索引和检索**。有一些索引方法专门针对大数据，例如Apache Lucene或Elasticsearch。此外，最常用的数据可以被缓存，或者检索过程可以被分布式以创建并行基础设施，并在有多个用户时减少瓶颈。鉴于这些技术的复杂性，在投入生产之前进行测试和基准测试总是最好的做法。
- en: Parallel processing
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 并行处理
- en: 'Especially for applications with a large number of users, **parallel processing**
    can significantly increase system scalability. This obviously requires a good
    cloud infrastructure with well-organized clusters. Applying parallel processing
    to RAG significantly decreases system latency even when there are large datasets.
    Apache Spark and Dask are among the most widely used solutions for implementing
    parallel computing with RAG. As we have seen, parallel computing can be implemented
    at various stages of the RAG pipeline: storage, retrieval, and generation. During
    storage, the various nodes can be used to implement the entire data preprocessing
    pipeline, that is, preprocessing, indexing, and chunking of part of the dataset
    (up to embedding). Although it seems less intuitive, during retrieval, the dataset
    can be divided among various nodes, with each node responsible for finding information
    from a particular dataset shard. In this way, we reduce the computational burden
    on each node and make the retrieval process parallel.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其对于拥有大量用户的软件应用，**并行处理**可以显著提高系统可扩展性。这显然需要一个良好的云基础设施，以及组织良好的集群。将并行处理应用于 RAG
    可以显著降低系统延迟，即使存在大量数据集。Apache Spark 和 Dask 是实现 RAG 并行计算最广泛使用的解决方案之一。正如我们所见，并行计算可以在
    RAG 管道各个阶段实现：存储、检索和生成。在存储阶段，可以使用各个节点来执行整个数据预处理管道，即预处理、索引和部分数据集（直到嵌入）的块化。尽管这看起来不太直观，但在检索阶段，数据集可以分配给各个节点，每个节点负责从特定的数据集分片中查找信息。这样，我们减轻了每个节点的计算负担，并使检索过程并行化。
- en: Similarly, generation can be made parallel. In fact, LLMs are computationally
    intensive but are transformer-based. The transformer was designed with both parallelization
    of training and inference in mind. There are techniques that allow parallelization
    in the case of long sequences or large batches of data. Later, more sophisticated
    techniques, such as tensor parallelism, model parallelism, and specialized frameworks,
    were developed. Paralleling the system, however, has inherent challenges and the
    risk of emerging errors. For these reasons, it is important to monitor the system
    during use and implement fault-tolerance mechanisms (such as checkpoints), advanced
    scheduling (such as dynamic task assignment), and other potential solutions.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，生成过程也可以并行化。实际上，LLMs 计算密集，但基于 Transformer。Transformer 的设计考虑了训练和推理的并行化。存在一些技术可以在长序列或大数据批量情况下实现并行化。后来，开发了更复杂的技巧，例如张量并行、模型并行和专用框架。然而，并行化系统本身存在固有的挑战和出现错误的风险。因此，在系统使用期间监控系统并实施容错机制（如检查点）、高级调度（如动态任务分配）和其他潜在解决方案是很重要的。
- en: RAG is a resource-intensive process (or at least some of the steps are), so
    it is good practice to implement techniques that dynamically allocate resources
    and monitor the workloads of the various processes. Also, it is recommended to
    use a modular approach that separates the various components, such as data ingestion,
    storage, retrieval, and generation. In any case, it is advisable to have a process
    that monitors not only performance in terms of accuracy but also memory usage,
    costs, network usage, and so on.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: RAG 是一个资源密集型过程（或者至少某些步骤是），因此实施动态分配资源和监控各种进程的工作负载的技术是很好的做法。此外，建议使用模块化方法，将各种组件（如数据摄取、存储、检索和生成）分开。无论如何，建议有一个过程来监控不仅准确性方面的性能，还包括内存使用、成本、网络使用等方面。
- en: '![Figure 6.17 – Big data solutions for RAG scalability](img/B21257_06_17.jpg)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.17 – RAG 可扩展性的大数据解决方案](img/B21257_06_17.jpg)'
- en: Figure 6.17 – Big data solutions for RAG scalability
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.17 – RAG 可扩展性的大数据解决方案
- en: We have talked generally about RAG. As we saw earlier, though, RAG today can
    be composed of several components. With advanced RAG and modular RAG, we saw how
    this system can be rapidly extended with additional components that impact both
    the accuracy of the system and its computational and latency costs. Thus, there
    are many alternatives for our system, and it is difficult to choose which components
    are most important. To date, there are a few benchmark studies that have conducted
    a rigorous analysis of both performance and computational costs. In a recent study
    (Wang, 2024), the authors analyzed the potential best components and gave guidance
    on which elements to use. In *Figure 6**.18*, the components marked in blue are
    those, according to the authors of the study, that give the best performance,
    while those in bold are optional components.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经一般性地讨论了 RAG。然而，正如我们之前看到的，今天的 RAG 可以由几个组件组成。通过高级 RAG 和模块化 RAG，我们看到了如何通过添加额外的组件快速扩展系统，这些组件会影响系统的准确性和计算及延迟成本。因此，我们的系统有许多替代方案，很难选择哪些组件最重要。迄今为止，只有少数基准研究对性能和计算成本进行了严格的分析。在最近的一项研究中（王，2024），作者分析了潜在的最佳组件，并提供了关于使用哪些元素的指导。在
    *图 6**.18 中，标记为蓝色的组件是研究作者认为能提供最佳性能的组件，而加粗的则是可选组件。
- en: '![Figure 6.18 – Contribution of each component for an optimal RAG (https://arxiv.org/pdf/2407.01219)](img/B21257_06_18.jpg)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.18 – 对最佳 RAG 每个组件的贡献](https://arxiv.org/pdf/2407.01219)](img/B21257_06_18.jpg)'
- en: Figure 6.18 – Contribution of each component for an optimal RAG ([https://arxiv.org/pdf/2407.01219](https://arxiv.org/pdf/2407.01219))
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.18 – 对最佳 RAG 每个组件的贡献([https://arxiv.org/pdf/2407.01219](https://arxiv.org/pdf/2407.01219))
- en: 'For example, the addition of some components improves system accuracy with
    a noticeable increase in latency. HyDE achieves the highest performance score
    but seems to have a significant computational cost. In this case, the performance
    improvement does not justify this increased latency. Other components increase
    the computational cost, but their absence results in an appreciable drop in performance
    (this is the case with reranking). Summarization modules help the model achieve
    optimal accuracy; their cost can be justified if latency is not problematic. Although
    it is virtually impossible to test all components in a systematic search, some
    guidelines can be provided. The best performance is achieved with the query classification
    module, HyDE, the reranking module, context repacking, and summarization. If this
    is too expensive computationally or in terms of latency, however, it is better
    to avoid techniques such as HyDE and stick to the other modules (perhaps choosing
    less expensive alternatives, for example, a reranker with fewer parameters). This
    is summarized in the following table comparing individual modules and techniques
    in terms of performance and computational efficiency:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，添加一些组件可以提高系统准确性，但延迟也会明显增加。HyDE 实现了最高的性能分数，但似乎有显著的计算成本。在这种情况下，性能提升并不能证明这种增加的延迟是合理的。其他组件增加了计算成本，但它们的缺失会导致性能显著下降（这种情况发生在重新排序中）。摘要模块有助于模型实现最佳准确性；如果延迟不是问题，它们的成本是可以接受的。尽管在系统性的搜索中测试所有组件几乎是不可能的，但可以提供一些指导方针。最佳性能是通过查询分类模块
    HyDE、重新排序模块、上下文重组和摘要实现的。然而，如果从计算或延迟的角度来看成本过高，则最好避免使用 HyDE 等技术，并坚持使用其他模块（例如，选择参数更少的重新排序器）。以下表格总结了单个模块和技术的性能和计算效率：
- en: '![Figure 6.19 – Impact of single modules and techniques on accuracy and latency
    (https://arxiv.org/pdf/2407.01219)](img/B21257_06_19.jpg)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.19 – 单个模块和技术对准确性和延迟的影响](https://arxiv.org/pdf/2407.01219)](img/B21257_06_19.jpg)'
- en: Figure 6.19 – Impact of single modules and techniques on accuracy and latency
    ([https://arxiv.org/pdf/2407.01219](https://arxiv.org/pdf/2407.01219))
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.19 – 单个模块和技术对准确性和延迟的影响([https://arxiv.org/pdf/2407.01219](https://arxiv.org/pdf/2407.01219))
- en: In addition, there are also parallelization strategies specifically designed
    for RAG. LlamaIndex offers a parallel pipeline for data ingestion and processing.
    In addition, to increase the robustness of the system, there are systems to prevent
    errors. For example, when using a model, you may encounter runtime errors (especially
    if you use external APIs such as OpenAI or Anthropic). In these cases, it pays
    to have fallback models. An **LLM router** is a system that allows you to route
    queries to different LLMs. Typically, there is a predictor model to intelligently
    decide which LLM is best suited for a given prompt (taking into account potential
    accuracy or factors such as cost). These routers can be used either as closed
    source models or to route queries to different external LLM APIs.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有专门为RAG设计的并行化策略。LlamaIndex提供了一种并行管道用于数据摄取和处理。为了提高系统的鲁棒性，还有防止错误的系统。例如，在使用模型时，你可能会遇到运行时错误（特别是如果你使用外部API，如OpenAI或Anthropic）。在这些情况下，拥有后备模型是有益的。**LLM路由器**是一种允许你将查询路由到不同LLM的系统。通常，有一个预测模型来智能地决定哪个LLM最适合给定的提示（考虑到潜在的准确性或成本等因素）。这些路由器可以用作封闭源模型，或者将查询路由到不同的外部LLM
    API。
- en: Security and privacy
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安全和隐私
- en: An important aspect to consider when a system goes into production is the **security
    and privacy** of the system. RAG can handle an enormous amount of sensitive and
    confidential data; breaching the system can lead to devastating consequences for
    an organization (regulatory fines, lawsuits, reputational damages, and so on).
    One of the main solutions is data encryption. Some algorithms and protocols are
    widely used in the industry and can also be applied to RAG (e.g., AES-256 and
    TLS/SSL). Similarly, it is important to implement internal policies to safeguard
    keys and change them frequently. In addition, a system of credentials and privileges
    must be implemented to ensure controlled access by users. It is good practice
    today to use methods such as **multi-factor authentication** (**MFA**), strong
    password rules, and policies for access from multiple devices. Again, an important
    part of this is continuous monitoring of potential breakage, incident reporting,
    and policies if they occur. Before deployment, it is essential to conduct testing
    of the system and its robustness to identify potential vulnerabilities.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 当系统投入生产时，需要考虑的一个重要方面是系统的**安全和隐私**。RAG可以处理大量敏感和机密数据；系统被破坏可能导致组织遭受毁灭性的后果（例如监管罚款、诉讼、声誉损害等）。其中一个主要的解决方案是数据加密。一些算法和协议在业界得到广泛应用，也可以应用于RAG（例如，AES-256和TLS/SSL）。同样，实施内部政策以保护密钥并频繁更换它们也很重要。此外，必须实施凭证和权限系统，以确保用户受控访问。如今，使用如**多因素认证**（**MFA**）、强密码规则以及多设备访问策略是良好的实践。再次强调，这其中的一个重要部分是对潜在破坏的持续监控、事件报告以及如果发生事件时的政策。在部署之前，对系统和其鲁棒性进行测试以识别潜在漏洞是至关重要的。
- en: '**Privacy** is a crucial and increasingly sensitive topic today. It is important
    that the system complies with key regulations such as the **General Data Protection
    Regulation** (**GDPR**) and the **California Consumer Privacy Act** (**CCPA**).
    Especially when handling large amounts of personal data, violations of these regulations
    expose an organization to hefty fines. To avoid penalties, it is a good idea to
    implement robust data governance, tracking practices, and data management. There
    are also techniques that can be used to improve system privacy, such as differential
    privacy and secure multi-party computation. In addition, incidents should be tracked
    and there should be policies for handling problems and resolving them.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '**隐私**是当今一个关键且日益敏感的话题。系统遵守关键法规，如**通用数据保护条例**（**GDPR**）和**加州消费者隐私法案**（**CCPA**）非常重要。特别是在处理大量个人数据时，违反这些法规会使组织面临巨额罚款。为了避免处罚，实施强大的数据治理、跟踪实践和数据管理是明智之举。还有一些技术可以用来提高系统隐私，例如差分隐私和安全的多方计算。此外，应跟踪事件，并应制定处理问题和解决问题的政策。'
- en: '![Figure 6.20 – The RAG system and potential risks (https://arxiv.org/pdf/2402.16893)](img/B21257_06_20.jpg)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![图6.20 – RAG系统和潜在风险 (https://arxiv.org/pdf/2402.16893)](img/B21257_06_20.jpg)'
- en: Figure 6.20 – The RAG system and potential risks ([https://arxiv.org/pdf/2402.16893](https://arxiv.org/pdf/2402.16893))
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.20 – RAG系统和潜在风险（[https://arxiv.org/pdf/2402.16893](https://arxiv.org/pdf/2402.16893)）
- en: Then, there are several security problems today that are specific to RAG systems.
    For example, vectors might look like simple numbers but in fact can be converted
    back into text. The embedding process can be seen as lossy, but that doesn’t mean
    it can’t be decoded into the original text. In theory, embedding vectors should
    only maintain the semantic meaning of the original text, thus protecting sensitive
    data. In fact, in some studies, they have been able to recover more than 70% of
    the words in the original text. Moreover, extremely sophisticated techniques are
    not necessary. In what are called **embedding inversion attacks**, you acquire
    the vectors and then decode them into the original text. In other words, contrary
    to popular belief, you can reconstruct text from vectors, and so these vectors
    should be protected as well. In addition, any system that includes an LLM is susceptible
    to **prompt injection attacks**. This is a type of attack in what looks like a
    legitimate prompt where malicious instructions are added. This could be to prompt
    the model to leak information. Prompt injection is one of the greatest risks to
    models, and often, new methods are described in the literature, so all previous
    precautions quickly become obsolete. In addition, particular prompts can induce
    outputs that are not expected by RAG. Adversarial prefixes are prefixes added
    to what is a prompt for RAG and can induce the generation of hallucinations and
    factual incorrect outputs.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，目前存在一些针对RAG系统的特定安全问题。例如，向量可能看起来像是简单的数字，但实际上可以转换回文本。嵌入过程可以被视为有损的，但这并不意味着它不能解码回原始文本。理论上，嵌入向量应仅保持原始文本的语义意义，从而保护敏感数据。实际上，在某些研究中，他们已经能够恢复原始文本中超过70%的单词。此外，不需要极其复杂的技巧。在所谓的**嵌入反转攻击**中，你获取向量并将它们解码回原始文本。换句话说，与普遍看法相反，你可以从向量中重建文本，因此这些向量也应受到保护。此外，任何包含LLM的系统都容易受到**提示注入攻击**。这是一种看似合法的提示，其中添加了恶意指令。这可能是为了提示模型泄露信息。提示注入是模型面临的最大风险之一，通常，文献中会描述新的方法，因此所有先前的预防措施很快就会过时。此外，特定的提示可能会诱导RAG产生非预期的输出。对抗性前缀是添加到RAG提示中的前缀，可以诱导生成幻觉和不准确的事实输出。
- en: Another type of attack is **poisoning RAG**, in which an attempt is made to
    enter erroneous data that will then be used by the LLM to generate skewed outputs.
    For example, to generate misinformation, we can craft target text that when injected
    will cause the system to generate a desired output. In the example in the figure,
    we inject text to poison RAG to influence the answer to a question.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种攻击类型是**毒化RAG**，其目的是输入错误的数据，然后LLM会使用这些数据生成偏颇的输出。例如，为了生成虚假信息，我们可以构建目标文本，当注入时会导致系统生成期望的输出。在图例的例子中，我们注入文本以毒化RAG来影响问题的答案。
- en: '![Figure 6.21 – Overview of poisoned RAG (https://arxiv.org/pdf/2402.07867)](img/B21257_06_21.jpg)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![图6.21 – 毒化RAG概述 (https://arxiv.org/pdf/2402.07867)](img/B21257_06_21.jpg)'
- en: Figure 6.21 – Overview of poisoned RAG ([https://arxiv.org/pdf/2402.07867](https://arxiv.org/pdf/2402.07867))
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.21 – 毒化RAG概述 ([https://arxiv.org/pdf/2402.07867](https://arxiv.org/pdf/2402.07867))
- en: '**Membership inference attacks** (**MIAs**) are another type of attack in which
    an attempt is made to infer whether certain data is present within a dataset.
    If a sample resides in the RAG dataset, it will probably be found for a particular
    query and inserted into the context of an LLM. With an MIA, we can know if a piece
    of data is present in the system and then try to extract it with prompt injection
    (e.g., by making LLM output the retrieved context ).'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '**成员推理攻击**（**MIAs**）是另一种攻击类型，其目的是推断某些数据是否存在于数据集中。如果一个样本位于RAG数据集中，它可能会在特定查询中被找到，并插入到LLM的上下文中。通过MIAs，我们可以知道数据是否存在于系统中，然后尝试通过提示注入（例如，通过让LLM输出检索到的上下文）来提取它。'
- en: That is why there are specific solutions for the RAG (or for LLMs in general).
    One example is **NeMo Guardrails**, which is an open source toolkit developed
    by NVIDIA to add programmable rails to LLM-based applications. These rails provide
    a mechanism to control the LLM output of a model (so we act directly at the generation
    level). In this way, we can provide constraints (not engaging in harmful topics,
    following a path during dialog, not responding to certain requests, using a certain
    language, and so on). The advantage of this approach over other embedded techniques
    (such as model alignment at training) is that it happens at runtime and we do
    not have to conduct additional training for the model. This approach is also model
    agnostic and, generally, these rails are interpretable (during alignment, we should
    analyze the dataset used for training). NeMo Guardrails implements user-defined
    programmable rails via an interpretable language (called Colang) that allows us
    to define behavior rules for LLMs.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 正因如此，针对RAG（或更普遍的LLMs）有特定的解决方案。一个例子是**NeMo Guardrails**，这是一个由NVIDIA开发的开源工具包，旨在为基于LLM的应用程序添加可编程轨道。这些轨道提供了一种控制模型LLM输出的机制（因此我们直接在生成级别进行操作）。通过这种方式，我们可以提供约束（不涉及有害话题、在对话中遵循路径、不回应某些请求、使用特定语言等）。与其他嵌入式技术（如训练时的模型对齐）相比，这种方法的优势在于它发生在运行时，我们不需要对模型进行额外的训练。这种方法也是模型无关的，通常，这些轨道是可解释的（在对齐过程中，我们应该分析用于训练的数据集）。NeMo
    Guardrails通过一种可解释的语言（称为Colang）实现用户定义的可编程轨道，允许我们为LLMs定义行为规则。
- en: 'With this toolkit, we can use different types of guardrails: input rails (reject
    input, conduct further processing, or modify the input, to avoid leakage of sensitive
    information), output rails (refuse to produce outputs in case of problematic content),
    retrieval rails (reject chunks and thus do not put them in the context for LLM,
    or alter present chunks), or dialog rails (decide whether to perform an action,
    use the LLM for a next step, or use a default response).'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个工具包，我们可以使用不同类型的轨道：输入轨道（拒绝输入、进行进一步处理或修改输入，以避免敏感信息泄露）、输出轨道（在内容有问题的情况下拒绝生成输出）、检索轨道（拒绝块并因此不将其放入LLM的上下文中，或更改现有块），或对话轨道（决定是否执行操作、使用LLM进行下一步，或使用默认响应）。
- en: '![Figure 6.22 – Programmable versus embedded rails for LLMs (https://arxiv.org/abs/2310.10501)](img/B21257_06_22.jpg)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![图6.22 – LLMs的可编程与嵌入式轨道对比（https://arxiv.org/abs/2310.10501）](img/B21257_06_22.jpg)'
- en: Figure 6.22 – Programmable versus embedded rails for LLMs ([https://arxiv.org/abs/2310.10501](https://arxiv.org/abs/2310.10501))
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.22 – LLMs的可编程与嵌入式轨道对比([https://arxiv.org/abs/2310.10501](https://arxiv.org/abs/2310.10501))
- en: Llama Guard, on the other hand, is a system designed to examine input (via prompt
    classification) and output (via response classification) and judge whether the
    text is safe or unsafe. This approach then uses Llama 2 for classification and
    then uses a specifically adapted LLM as the judge.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，**Llama Guard**是一个旨在检查输入（通过提示分类）和输出（通过响应分类）并判断文本是否安全或不安全的系统。然后，这种方法使用Llama
    2进行分类，并使用专门调整的LLM作为评判者。
- en: Open questions and future perspectives
  id: totrans-263
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开放性问题与未来展望
- en: Although there have been significant advances in RAG technology, there are still
    challenges. In this section, we will discuss these challenges and prospects.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管RAG技术在很大程度上取得了进步，但仍然存在挑战。在本节中，我们将讨论这些挑战和前景。
- en: Recently, there has been wide interest and discussion about the expansion of
    the context length of LLMs. Today, most of the best-performing LLMs have a context
    length of more than 100K tokens (some up to over 1 million). This capability means
    that a model has the capacity for long document question-answering (in other words,
    the ability to insert long documents such as books within a single prompt). Many
    small user cases can be covered by a context length of 1 to 10 million tokens.
    The advantage of a **long-context LLM** (**LC-LLM**) is that it can then conduct
    interleaved retrieval and generation of the information in the prompt and conduct
    one-shot reasoning over the entire document. Especially for summarization tasks,
    the LC-LLM has a competitive advantage because it can conduct a scan of the whole
    document and relate information present at the top and bottom of the document.
    For some, LC-LLM means that the RAG is doomed to disappear.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，关于LLM（大型语言模型）上下文长度扩展的话题引起了广泛的兴趣和讨论。如今，大多数表现最好的LLM的上下文长度超过100K个token（有些甚至超过100万个）。这种能力意味着模型具有进行长文档问答的能力（换句话说，能够在单个提示中插入如书籍等长文档）。1到1000万个token的上下文长度可以覆盖许多小型用例。**长上下文LLM（LC-LLM**）的优势在于它能够对提示中的信息进行交错检索和生成，并对整个文档进行一次性推理。特别是在摘要任务中，LC-LLM具有竞争优势，因为它可以对整个文档进行扫描，并将文档顶部和底部出现的信息联系起来。对一些人来说，LC-LLM意味着RAG（检索-生成）注定要消失。
- en: In reality, the LC-LLM does not compete with RAG, and RAG is not doomed to disappear
    in the short term. The LC-LLM does not use the whole framework efficiently. In
    particular, the information in the middle of the context is attended much less
    efficiently. Similarly, reasoning is impacted by irrelevant information, and a
    long prompt inevitably provides an unnecessary amount of detail to answer a query.
    The LC-LLM hallucinates much more than RAG, and the latter allows for reference
    checking (which documents were used, thus making the retrieval and reasoning process
    observable and transparent). The LC-LLM also has difficulty with structured data
    (which is most data in many industries) and has a fairly considerable cost (latency
    increases significantly with a long prompt and also the cost per query). Finally,
    1 million tokens are not a lot when considering the amount of data that even a
    small organization has (so retrieval is always necessary).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，LC-LLM并不与RAG竞争，RAG在短期内也不注定要消失。LC-LLM并没有高效地使用整个框架。特别是，上下文中中间的信息被关注得要少得多。同样，推理受到无关信息的影响，而长的提示不可避免地提供了回答查询所需的不必要的大量细节。LC-LLM的幻觉比RAG要多得多，后者允许进行参考检查（使用了哪些文档，从而使检索和推理过程变得可观察和透明）。LC-LLM在处理结构化数据（许多行业中的大多数数据）方面也存在困难，并且成本相当高（长提示会导致延迟显著增加，以及每查询的成本）。最后，考虑到即使是小型组织拥有的数据量，100万个token也不是很多（因此检索总是必要的）。
- en: The LC-LLM opens up exciting possibilities for developers. First, it means that
    a precise chunking strategy will be necessary much less frequently. Chunks can
    be much larger (up to a document per chunk or at least a group of pages). This
    will mean less need to balance granularity and performance. Second, less prompt
    engineering will be needed. Especially for reasoning tasks, some questions can
    be answered with the information in one chunk, but others require deep analysis
    among several sections or multiple documents. Instead of a complex CoT, it is
    possible to answer these questions with a single prompt. Third, summarization
    is easier with the LC-LLM, so it can be conducted with a single retrieval. Finally,
    the LC-LLM allows for better customization and interaction with the user. In such
    a long prompt, it will be possible to upload the entire chat with the user. There
    are still some open challenges, though, especially in retrieving documents for
    the LC-LLM.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: LC-LLM为开发者开辟了令人兴奋的可能性。首先，这意味着精确的分块策略将很少需要。分块可以更大（每个分块可以是一个文档，或者至少是一组页面）。这将意味着减少对粒度和性能之间平衡的需求。其次，需要更少的提示工程。特别是对于推理任务，一些问题可以用一个分块中的信息来回答，但其他问题则需要在不同部分或多个文档中进行深入分析。而不是复杂的CoT（概念重叠），可以用单个提示来回答这些问题。第三，使用LC-LLM进行摘要更容易，因此可以单次检索完成。最后，LC-LLM允许更好地定制和与用户交互。在这样的长提示中，可以上传与用户整个对话。尽管如此，仍然存在一些开放性的挑战，尤其是在为LC-LLM检索文档方面。
- en: Similarly, there are no embedding models today that can handle similar context
    lengths (currently, the maximum context length of an embedder is 32K). Therefore,
    even with an LC-LLM, the chunks cannot be larger than 32K. The LC-LLM is still
    expensive in terms of performance and can seriously impact the scalability of
    the system. In any case, there are already potential RAG variations being studied
    that take the LC-LLM into account – for example, adapting small-to-big retrieval
    in which you find the necessary chunks and then send the entire document associated
    with the LC-LLM, or conduct routing of a query to pipeline whole-document retrieval
    (such as whole-document summarization tasks) or to find chunks (specific questions
    or multi-part questions that require chunks of different documents). Many companies
    work with KV caching, which is an approach in which you store the activations
    from the key and query from an attention layer (so you don’t have to recompute
    the entire activations for a sequence during generation). So, it has been proposed
    that RAG could also be used to find the cache
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，目前还没有能够处理类似上下文长度的嵌入模型（目前，嵌入器的最大上下文长度为32K）。因此，即使有LC-LLM，块的大小也不能超过32K。LC-LLM在性能方面仍然很昂贵，并且可能会严重影响系统的可扩展性。无论如何，已经有研究正在探讨一些考虑LC-LLM的潜在RAG变体——例如，在从小到大的检索中，你找到必要的块，然后发送与LC-LLM关联的整个文档，或者将查询路由到管道化整个文档检索（如整个文档摘要任务）或找到块（特定问题或需要不同文档块的多部分问题）。许多公司使用KV缓存，这是一种在生成过程中存储来自键和注意力层的激活以及查询的方法（这样你就不必在整个序列生成过程中重新计算整个激活）。因此，有人提出，RAG也可以用来查找缓存
- en: 'We can see these possible evolutions visually in the following figure:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在以下图中直观地看到这些可能的演变：
- en: '![Figure 6.23 – Possible evolution of RAG with the LC-LLM](img/B21257_06_23.jpg)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![图6.23 – RAG与LC-LLM可能的演变](img/B21257_06_23.jpg)'
- en: Figure 6.23 – Possible evolution of RAG with the LC-LLM
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.23 – RAG与LC-LLM可能的演变
- en: A. Retrieving first the chunks and then the associated documents
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: A. 首先检索块，然后检索相关的文档
- en: B. Router deciding whether it is necessary to retrieve small chunks or whole
    documents
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: B. 路由器决定是否需要检索小块或整个文档
- en: C. Retrieving the document and then KV caching them for the LC-LLM
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: C. 首先检索文档，然后为LC-LLM进行KV缓存
- en: '**Multimodal RAG** is an exciting prospect and challenge that has been discussed.
    Most organizations have not only textual data but also extensive amounts of data
    in other modalities (images, audio, video, and so on). In addition, many files
    may contain more than one modality (for example, a book that contains not only
    text but also images). Searching for multimodal data can be of particular interest
    in different contexts and different applications. On the other hand, multimodal
    RAG is complicated by the fact that each modality has its own challenges. There
    are some alternatives to how we can achieve multimodal RAG. We will see three
    possible strategies:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '**多模态RAG**是一个令人兴奋的前景和挑战，已经被讨论过。大多数组织不仅拥有文本数据，还有大量其他模态的数据（图像、音频、视频等）。此外，许多文件可能包含不止一种模态（例如，一本书不仅包含文本，还包含图像）。在不同的上下文和应用中搜索多模态数据可能特别有趣。另一方面，多模态RAG的复杂性在于每个模态都有其自身的挑战。我们有几种不同的方法可以实现多模态RAG。我们将看到三种可能的策略：'
- en: '**Embed all modalities into the same vector space**: We previously saw the
    case of CLIP in [*Chapter 3*](B21257_03.xhtml#_idTextAnchor042) (a model trained
    by contrastive learning to achieve unique embedding for images and text), which
    allowed us to search both images and text. We can use a model such as CLIP to
    conduct embedding of all modalities (in this case, images and text, but other
    cross-modal models exist). We can then find both images and text and use a multimodal
    model for generation (for example, we can use BLIP2 or BLIP3 as a vision language
    model). A multimodal model can conduct reasoning about both images and text. This
    approach has the advantage that we only need to change the embedding model to
    our system. In addition, a multimodal model can conduct reasoning by exploiting
    the information in both the image and the text. For example, if we have a PDF
    with tables, we can find the chunk of interest and the associated graphs. The
    model can use the information contained in both modalities to be able to answer
    the query more effectively. The disadvantage is that CLIP is an expensive model,
    and **multimodal LLMs** (**MMLLMs**) are more expensive than text-only LLMs. Also,
    we need to be sure that our embedding model is capable of capturing all the nuances
    of images and text.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**将所有模态嵌入到相同的向量空间中**：我们之前在[*第三章*](B21257_03.xhtml#_idTextAnchor042)（一个通过对比学习训练以实现图像和文本独特嵌入的模型）中看到了这种情况，它允许我们同时搜索图像和文本。我们可以使用像CLIP这样的模型来对所有模态进行嵌入（在这种情况下，是图像和文本，但还存在其他跨模态模型）。然后我们可以找到图像和文本，并使用多模态模型进行生成（例如，我们可以使用BLIP2或BLIP3作为视觉语言模型）。多模态模型可以对图像和文本进行推理。这种方法的优势在于我们只需要更改嵌入模型到我们的系统中。此外，多模态模型可以通过利用图像和文本中的信息来进行推理。例如，如果我们有一个包含表格的PDF，我们可以找到感兴趣的片段及其相关的图表。模型可以使用两种模态中包含的信息来更有效地回答查询。缺点是CLIP是一个昂贵的模型，**多模态大型语言模型（MMLLMs**）比仅文本的LLMs更昂贵。此外，我们需要确保我们的嵌入模型能够捕捉到图像和文本的所有细微差别。'
- en: '**Single-grounded modality**: Another option is to transform all modes into
    the primary mode (which can be different depending on the focus of the application).
    For example, we extract text from the PDF and create text descriptions for each
    of the images along with metadata (for audio, we can use a transcript). In some
    variants, we keep the images in storage. During retrieval, we find the text again
    (so we use a classic embedding model and a database that contains only vectors
    obtained from text). We can then use an LLM or MMLLM (if we want to add the images
    obtained by retrieving metadata or description) during the generation phase. Again,
    the main advantage is that we do not have to train any new type of model, but
    it can be expensive as an approach, and we lose some nuances from the image.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单基础模态**：另一个选择是将所有模式转换为基本模式（这取决于应用的焦点）。例如，我们从PDF中提取文本，并为每个图像创建文本描述以及元数据（对于音频，我们可以使用转录）。在某些变体中，我们保留存储中的图像。在检索过程中，我们再次找到文本（因此我们使用经典的嵌入模型和仅包含从文本中获得的向量的数据库）。然后我们可以在生成阶段使用LLM或MMLLM（如果我们想添加通过检索元数据或描述获得的图像）。再次，主要优势是我们不必训练任何新的模型类型，但作为一种方法，它可能很昂贵，并且我们失去了图像的一些细微差别。'
- en: '**Separate retrieval for each modality**: In this case, each modality is embedded
    separately. For example, if we have three modalities, we will have three separate
    models (audio-text-aligned model, image-text-aligned model, and text embedder)
    and three separate databases (audio, images, and text). When the query arrives,
    we encode for each mode (so audio, images, and text). So, in this case, we have
    done three retrievals and may have found different elements, so it pays to have
    a rerank step (to efficiently combine the results). Obviously, we need a dedicated
    multimodal rerank that can allow us to retrieve the most relevant chunks. It simplifies
    the organization because we have dedicated models for each mode (a model that
    works well for all modes is difficult to obtain) but it increases the complexity
    of the system. Similarly, while a classical reranker has to reorder *n* chunks,
    a multimodal reranker has the complexity of reordering *m* x *n* chunks (where
    *m* is the number of modes).'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**为每个模态分别检索**：在这种情况下，每个模态都是单独嵌入的。例如，如果我们有三个模态，我们将有三个单独的模型（音频-文本对齐模型、图像-文本对齐模型和文本嵌入器）和三个单独的数据库（音频、图像和文本）。当查询到达时，我们为每个模式进行编码（因此是音频、图像和文本）。所以，在这种情况下，我们进行了三次检索，可能会找到不同的元素，因此有一个重新排序步骤（以有效地组合结果）是值得的。显然，我们需要一个专门的多模态重新排序器，以便我们可以检索最相关的块。它简化了组织，因为我们有针对每个模式的专用模型（一个对所有模式都工作得很好的模型很难获得），但它增加了系统的复杂性。同样，虽然一个经典的重新排序器必须重新排序*n*个块，但一个多模态重新排序器的复杂性是重新排序*m*
    x *n*个块（其中*m*是模态的数量）。'
- en: Finally, once the multimodal chunks have been obtained, there may be alternatives;
    for example, we can use an MMLM to generate a response, and then this response
    needs to be integrated into the context for a final LLM. As we saw earlier, our
    RAG pipeline can be more sophisticated than naïve RAG. We can then combine all
    the elements we saw earlier into a single system.
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，一旦获得了多模态块，可能会有替代方案；例如，我们可以使用一个MMLM来生成响应，然后这个响应需要被整合到上下文中以形成一个最终的LLM。正如我们之前看到的，我们的RAG管道可以比简单的RAG更复杂。然后我们可以将之前看到的所有元素组合成一个单一的系统。
- en: '![Figure 6.24 – Three potential approaches to multimodal RAG](img/B21257_06_24.jpg)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![图6.24 – 多模态RAG的三个潜在方法](img/B21257_06_24.jpg)'
- en: Figure 6.24 – Three potential approaches to multimodal RAG
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.24 – 多模态RAG的三个潜在方法
- en: Although RAG efficiently mitigates hallucinations, they can happen. We have
    previously discussed hallucinations as a plague of LLMs. In this section, we will
    mainly discuss hallucinations in RAG. One of the most peculiar cases is **contextual
    hallucinations**, in which the correct facts are provided in the context, but
    the LLM still generates the wrong output. Although the model provides the correct
    information, it produces a wrong answer (this often occurs in tasks such as summarization
    or document-based questions). This occurs because the LLM has its own prior knowledge,
    and it is wrong to assume that the model does not use this internal knowledge.
    Furthermore, the model is instruction-tuned or otherwise aligned, so it implicitly
    makes a decision on whether to use the context or ignore it and use its knowledge
    to answer the user’s question. In some cases, this might even be useful, since
    it could happen that we have found the wrong or misleading context. In general,
    for many closed source models, we do not know what they were trained on, though
    we can monitor their confidence in an answer. Given a question *x*, the model
    will respond with an answer *x*. Depending on its knowledge, this will have a
    confidence *c* (which is based on the probability associated with the tokens generated
    by the model). Basically, the more confident a model is in its answer, the less
    prone it will be to changing its answer if the context suggests differently. An
    interesting finding is that if the correct answer is slightly different from the
    LLM’s knowledge, the LLM is likely to change its answer. In case of a large divergence,
    the LLM will choose its own answer. For example, to the question, “What is the
    maximum dosage of drug x?” the model may have seen 20 µg in its training. If the
    context suggests 30, the LLM will provide 30 as the output; if the context suggests
    100, the LLM will state 20\. Larger LLMs are generally more confident and prefer
    their answer, while smaller models are more willing to use context. Finally, this
    behavior can be altered with prompt engineering. Stricter prompts will force the
    model to use context, while weaker prompts will push the model to use its prior
    knowledge.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管RAG有效地缓解了幻觉，但它们仍然可能发生。我们之前已经讨论过幻觉是LLM的灾难。在本节中，我们将主要讨论RAG中的幻觉。其中最奇特的情况之一是**情境幻觉**，在这种幻觉中，正确的信息在上下文中提供，但LLM仍然生成错误的输出。尽管模型提供了正确的信息，但它产生了错误的答案（这种情况通常发生在摘要或基于文档的问题等任务中）。这是因为在LLM中存在自己的先验知识，假设模型不使用这种内部知识是错误的。此外，模型经过指令微调或以其他方式对齐，因此它隐式地做出决定，是使用上下文还是忽略上下文并使用其知识来回答用户的问题。在某些情况下，这甚至可能是有用的，因为可能我们找到了错误或误导性的上下文。总的来说，对于许多闭源模型，我们不知道它们是在什么上进行训练的，尽管我们可以监控它们对答案的信心。给定一个问题
    *x*，模型将回答一个答案 *x*。根据其知识，这将有一个信心 *c*（这是基于模型生成的标记的概率）。基本上，一个模型对其答案越自信，它在上下文暗示不同的情况下改变答案的可能性就越小。一个有趣的发现是，如果正确答案与LLM的知识略有不同，LLM很可能会改变其答案。在存在较大差异的情况下，LLM将选择自己的答案。例如，对于“药物x的最大剂量是多少？”这个问题，模型可能在训练中看到了20
    µg。如果上下文暗示是30，LLM将提供30作为输出；如果上下文暗示是100，LLM将声明20。较大的LLM通常更自信，更喜欢自己的答案，而较小的模型更愿意使用上下文。最后，这种行为可以通过提示工程来改变。更严格的提示将迫使模型使用上下文，而较弱的提示将推动模型使用其先验知识。
- en: '![Figure 6.25 – Example of a standard prompt in comparison with a loose or
    strict prompt (https://arxiv.org/pdf/2404.10198)](img/B21257_06_25.jpg)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![图6.25 – 标准提示与宽松或严格提示的比较示例 (https://arxiv.org/pdf/2404.10198)](img/B21257_06_25.jpg)'
- en: Figure 6.25 – Example of a standard prompt in comparison with a loose or strict
    prompt ([https://arxiv.org/pdf/2404.10198](https://arxiv.org/pdf/2404.10198))
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.25 – 标准提示与宽松或严格提示的比较示例 ([https://arxiv.org/pdf/2404.10198](https://arxiv.org/pdf/2404.10198))
- en: 'Other factors also help reduce hallucinations in RAG:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 其他因素也有助于减少RAG中的幻觉：
- en: '**Data quality**: Data quality has a big impact on system quality in general.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据质量**：数据质量通常对系统质量有重大影响。'
- en: '**Contextual awareness**: The LLM may not best understand the user’s intent,
    or the found context may not be the right one. Query rewriting and other components
    of advanced RAG might be the solution.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**情境意识**：LLM可能并不完全理解用户的意图，或者找到的上下文可能不是正确的。查询重写和其他高级RAG组件可能是解决方案。'
- en: '**Negative rejection**: When retrieval fails to find the appropriate context
    for the query, the model attempts to respond anyway, thereby generating hallucinations
    or incorrect answers. This is often the fault of a poorly written query, so it
    can be improved with components that modify the query (such as HyDE). Alternatively,
    stricter prompts force the LLM to respond only if there is context.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**负面拒绝**：当检索未能找到查询的适当上下文时，模型仍然尝试做出回应，从而产生幻觉或错误答案。这通常是由于查询编写不当造成的，因此可以通过修改查询的组件（如HyDE）来改进。或者，更严格的提示会迫使LLM只有在有上下文的情况下才做出回应。'
- en: '**Reasoning abilities**: Some queries may require reasoning or are too complex.
    The reasoning limit of the system depends on the LLM; RAG is for finding the context
    to answer the query.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推理能力**：一些查询可能需要推理或过于复杂。系统的推理限制取决于LLM；RAG是为了找到回答查询的上下文。'
- en: '**Domain mismatch**: A generalist model will have difficulty with domains that
    are too technical. Fine-tuning the embedder and LLM can be a solution.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**领域不匹配**：通用模型在处理过于技术化的领域时会遇到困难。微调嵌入器和LLM可以是一个解决方案。'
- en: '**Objective mismatch**: The goals of the embedder and LLM are not aligned,
    so today there are systems that try to optimize end-to-end retrieval and generation.
    This can be a solution for complex queries or specialized domains.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标不匹配**：嵌入器和LLM的目标不一致，因此今天有系统试图优化端到端检索和生成。这可以解决复杂查询或特定领域的问题。'
- en: There are other exciting perspectives. For example, there is some work on using
    reinforcement learning to improve the ability of RAG to respond to complex queries.
    Other research deals with integrating graph research; we will discuss this in
    more detail in the next chapter. In addition, we have assumed so far that the
    database is static, but in the age of the internet, there is a discussion on how
    to integrate the internet into RAG (e.g., conducting a hybrid search in an organization’s
    protected data and also finding context through an internet search). This opens
    up exciting but complex questions, such as whether or not to conduct database
    updates, how to filter out irrelevant search engine results, and security issues.
    In addition, there are more and more specialized applications of RAG, where the
    authors focus on creating systems optimized for their field of application (e.g.,
    RAG for math, medicine, biology, and so on). All this shows active research into
    RAG and interest in its application.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他令人兴奋的视角。例如，有一些工作正在研究使用强化学习来提高RAG对复杂查询的响应能力。其他研究涉及整合图研究；我们将在下一章中更详细地讨论这一点。此外，我们迄今为止一直假设数据库是静态的，但在互联网时代，有一个关于如何将互联网整合到RAG中的讨论（例如，在组织受保护的数据中进行混合搜索，并通过互联网搜索找到上下文）。这提出了令人兴奋但复杂的问题，例如是否进行数据库更新、如何过滤掉不相关的搜索引擎结果以及安全问题。此外，还有越来越多的RAG专用应用，作者专注于创建针对其应用领域的优化系统（例如，用于数学、医学、生物学等）。所有这些都表明了RAG的活跃研究和对其应用的兴趣。
- en: Summary
  id: totrans-293
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we initially discussed what the problems of naïve RAG are.
    This allowed us to see a number of add-ons that can be used to solve the sore
    points of naïve RAG. Using these add-ons is the basis of what is now called the
    advanced RAG paradigm. Over time, the community then moved toward a more flexible
    and modular structure that is now called modular RAG.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们最初讨论了朴素RAG的问题。这使得我们看到了许多可以用来解决朴素RAG痛点的附加组件。使用这些附加组件是现在所说的先进RAG范式的基础。随着时间的推移，社区随后转向了一种更灵活和模块化的结构，现在称为模块化RAG。
- en: We then saw how to scale this structure in the presence of big data. Like any
    LLM-based application, there are computational and cost challenges when you have
    to take the system from a development environment to a production environment.
    In addition, both LLMs and RAGs can have security and privacy risks. These are
    important points, especially when these products are open to the public. Today,
    there is an increasing focus on compliance and more and more regulations are being
    considered.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随后看到了如何在大数据存在的情况下扩展这种结构。像任何基于LLM的应用一样，当你需要将系统从开发环境迁移到生产环境时，会面临计算和成本挑战。此外，LLM和RAG都可能存在安全和隐私风险。这些是重要的问题，尤其是当这些产品向公众开放时。今天，对合规性的关注越来越多，越来越多的法规正在被考虑。
- en: Finally, we saw that some issues remain open, such as the relationship with
    long-context LLMs or the multimodal extension of these models. In addition, there
    is a delicate balance between retrieval and generation, and we explored potential
    solutions in case of problems. Recently, there has been active research into integration
    with KGs. GraphRAG is often discussed today; in the next chapter, we will discuss
    what a KG is and the relationship between graphs and RAG.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们注意到一些问题仍然悬而未决，例如与长上下文LLM的关系或这些模型的跨模态扩展。此外，检索和生成之间存在微妙的平衡，我们在出现问题时探讨了潜在解决方案。最近，对与知识图谱（KG）的集成进行了积极的研究。GraphRAG今天经常被讨论；在下一章中，我们将讨论什么是知识图谱以及图与RAG之间的关系。
- en: Further reading
  id: totrans-297
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'LlamaIndex, *Node Postprocessor* *Modules*: [https://docs.llamaindex.ai/en/stable/module_guides/querying/node_postprocessors/node_postprocessors/](https://docs.llamaindex.ai/en/stable/module_guides/querying/node_postprocessors/node_postprocessors/)'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'LlamaIndex, *节点后处理器* *模块*: [https://docs.llamaindex.ai/en/stable/module_guides/querying/node_postprocessors/node_postprocessors/](https://docs.llamaindex.ai/en/stable/module_guides/querying/node_postprocessors/node_postprocessors/)'
- en: 'Nelson, *Lost in the Middle: How Language Models Use Long Contexts*, 2023:
    [https://arxiv.org/abs/2307.03172](https://arxiv.org/abs/2307.03172)'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nelson, *《迷失在中间：语言模型如何使用长上下文》*, 2023: [https://arxiv.org/abs/2307.03172](https://arxiv.org/abs/2307.03172)'
- en: 'Jerry Liu, *Unifying LLM-powered QA Techniques with Routing Abstractions*,
    2023: [https://betterprogramming.pub/unifying-llm-powered-qa-techniques-with-routing-abstractions-438e2499a0d0](https://betterprogramming.pub/unifying-llm-powered-qa-techniques-with-routing-abstractions-438e2499a0d0)'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jerry Liu, *《通过路由抽象统一LLM驱动的问答技术》*, 2023: [https://betterprogramming.pub/unifying-llm-powered-qa-techniques-with-routing-abstractions-438e2499a0d0](https://betterprogramming.pub/unifying-llm-powered-qa-techniques-with-routing-abstractions-438e2499a0d0)'
- en: 'Chevalier, *Adapting Language Models to Compress Contexts*, 2023: [https://arxiv.org/abs/2305.14788](https://arxiv.org/abs/2305.14788)'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chevalier, *《适应语言模型以压缩上下文》*, 2023: [https://arxiv.org/abs/2305.14788](https://arxiv.org/abs/2305.14788)'
- en: 'Li, *Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of
    LLMs with Self-Information-Based Content Filtering*, 2023: [https://arxiv.org/abs/2304.12102](https://arxiv.org/abs/2304.12102)'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li, *《解锁LLM的上下文约束：利用基于自信息的内容过滤提高LLM的上下文效率》*, 2023: [https://arxiv.org/abs/2304.12102](https://arxiv.org/abs/2304.12102)'
- en: 'Izacard, *Distilling Knowledge from Reader to Retriever for Question Answering*,
    2020: [https://arxiv.org/abs/2012.04584](https://arxiv.org/abs/2012.04584)'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Izacard, *《从读者到检索器的知识蒸馏：用于问答》*, 2020: [https://arxiv.org/abs/2012.04584](https://arxiv.org/abs/2012.04584)'
- en: 'Wang, *Searching for Best Practices in Retrieval-Augmented Generation*, 2024:
    [https://arxiv.org/pdf/2407.01219](https://arxiv.org/pdf/2407.01219)'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang, *《在检索增强生成中寻找最佳实践》*, 2024: [https://arxiv.org/pdf/2407.01219](https://arxiv.org/pdf/2407.01219)'
- en: 'Li, *Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study
    and Hybrid Approach*, 2024: [https://www.arxiv.org/abs/2407.16833](https://www.arxiv.org/abs/2407.16833)'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li, *《检索增强生成或长上下文LLM？一项综合研究和混合方法》*, 2024: [https://www.arxiv.org/abs/2407.16833](https://www.arxiv.org/abs/2407.16833)'
- en: 'Raieli, *RAG is Dead, Long Live RAG*, 2024: [https://levelup.gitconnected.com/rag-is-dead-long-live-rag-c607e1799199](https://levelup.gitconnected.com/rag-is-dead-long-live-rag-c607e1799199)'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Raieli, *《RAG已死，RAG永生》*, 2024: [https://levelup.gitconnected.com/rag-is-dead-long-live-rag-c607e1799199](https://levelup.gitconnected.com/rag-is-dead-long-live-rag-c607e1799199)'
- en: 'Raieli, *War and Peace: A Conflictual Love Between the LLM and RAG*, 2024:
    [https://ai.plainenglish.io/war-and-peace-a-conflictual-love-between-the-llm-and-rag-78428a5776fb](https://ai.plainenglish.io/war-and-peace-a-conflictual-love-between-the-llm-and-rag-78428a5776fb)'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Raieli, *《战争与和平：LLM与RAG之间的冲突性爱情》*, 2024: [https://ai.plainenglish.io/war-and-peace-a-conflictual-love-between-the-llm-and-rag-78428a5776fb](https://ai.plainenglish.io/war-and-peace-a-conflictual-love-between-the-llm-and-rag-78428a5776fb)'
- en: 'jinaai/jina-colbert-v2: [https://huggingface.co/jinaai/jina-colbert-v2](https://huggingface.co/jinaai/jina-colbert-v2)'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'jinaai/jina-colbert-v2: [https://huggingface.co/jinaai/jina-colbert-v2](https://huggingface.co/jinaai/jina-colbert-v2)'
- en: '`mix_self_consistency`: [https://github.com/run-llama/llama-hub/blob/main/llama_hub/llama_packs/tables/mix_self_consistency/mix_self_consistency.ipynb](https://github.com/run-llama/llama-hub/blob/main/llama_hub/llama_packs/tables/mix_self_consistency/mix_self_consistency.ipynb)'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mix_self_consistency`: [https://github.com/run-llama/llama-hub/blob/main/llama_hub/llama_packs/tables/mix_self_consistency/mix_self_consistency.ipynb](https://github.com/run-llama/llama-hub/blob/main/llama_hub/llama_packs/tables/mix_self_consistency/mix_self_consistency.ipynb)'
- en: 'Zeng, *The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented
    Generation (RAG)*, 2024: [https://arxiv.org/abs/2402.16893](https://arxiv.org/abs/2402.16893)'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '曾，*善与恶：探索检索增强生成（RAG）中的隐私问题*，2024: [https://arxiv.org/abs/2402.16893](https://arxiv.org/abs/2402.16893)'
- en: 'Xue, *BadRAG: Identifying Vulnerabilities in Retrieval Augmented Generation
    of Large Language Models*, 2024: [https://arxiv.org/abs/2406.00083](https://arxiv.org/abs/2406.00083)'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '薛，*BadRAG：识别大型语言模型检索增强生成中的漏洞*，2024: [https://arxiv.org/abs/2406.00083](https://arxiv.org/abs/2406.00083)'
- en: 'Chen, *Controlling Risk of Retrieval-augmented Generation: A Counterfactual
    Prompting Framework*, 2024: [https://arxiv.org/abs/2409.16146](https://arxiv.org/abs/2409.16146)'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '陈，*控制检索增强生成的风险：一种反事实提示框架*，2024: [https://arxiv.org/abs/2409.16146](https://arxiv.org/abs/2409.16146)'
- en: 'Zhang, *HijackRAG: Hijacking Attacks against Retrieval-Augmented Large Language
    Models*, 2024: [https://arxiv.org/abs/2410.22832](https://arxiv.org/abs/2410.22832)'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '张，*HijackRAG：针对检索增强大型语言模型的劫持攻击*，2024: [https://arxiv.org/abs/2410.22832](https://arxiv.org/abs/2410.22832)'
- en: 'Xian, *On the Vulnerability of Applying Retrieval-Augmented Generation within
    Knowledge-Intensive Application Domains*, 2024: [https://arxiv.org/abs/2409.17275v1](https://arxiv.org/abs/2409.17275v1)'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '邢，*在知识密集型应用领域应用检索增强生成时的脆弱性*，2024: [https://arxiv.org/abs/2409.17275v1](https://arxiv.org/abs/2409.17275v1)'
