<html><head></head><body>
<div><p><a id="_idTextAnchor080"/></p>
<h1 class="chapter-number" id="_idParaDest-45"><a id="_idTextAnchor081"/>3</h1>
<h1 id="_idParaDest-46"><a id="_idTextAnchor082"/>Tracing the Foundations of Natural Language Processing and the Impact of the Transformer</h1>
<p>The transformer architecture is a key advancement that underpins most modern generative language models. Since its introduction in 2017, it has become a fundamental part <a id="_idIndexMarker156"/>of <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>), enabling models such as <strong class="bold">Generative Pre-trained Transformer 4</strong> (<strong class="bold">GPT-4</strong>) and Claude<a id="_idIndexMarker157"/> to advance text generation capabilities significantly. A deep understanding of the transformer architecture is crucial for grasping the mechanics <a id="_idIndexMarker158"/>of modern <strong class="bold">large language </strong><strong class="bold">models</strong> (<strong class="bold">LLMs</strong>).</p>
<p>In the previous chapter, we explored generative modeling techniques, including <strong class="bold">generative adversarial networks</strong> (<strong class="bold">GANs</strong>), diffusion<a id="_idIndexMarker159"/> models, and <strong class="bold">autoregressive</strong> (<strong class="bold">AR</strong>) transformers. We discussed how Transformers <a id="_idIndexMarker160"/>can be leveraged to generate images from text. However, transformers are more than just one generative approach among many; they form the basis for nearly all state-of-the-art generative language models.</p>
<p>In this chapter, we’ll cover the evolution of NLP that ultimately led to the advent of the transformer architecture. We cannot cover all the critical steps forward, but we will attempt to cover major milestones, starting with early linguistic analysis techniques and statistical language <a id="_idIndexMarker161"/>modeling, followed by advancements in <strong class="bold">recurrent neural networks</strong> (<strong class="bold">RNNs</strong>) and <strong class="bold">convolutional neural networks</strong> (<strong class="bold">CNNs</strong>) that <a id="_idIndexMarker162"/>highlight the potential of <strong class="bold">deep learning</strong> (<strong class="bold">DL</strong>) for NLP. Our main objective will be to introduce the transformer—its <a id="_idIndexMarker163"/>basis in DL, its self-attention architecture, and its rapid evolution, which has led to LLMs and this phenomenon <a id="_idIndexMarker164"/>we call <strong class="bold">generative </strong><strong class="bold">AI</strong> (<strong class="bold">GenAI</strong>).</p>
<p>Understanding the origins and mechanics of the transformer architecture is important for recognizing its groundbreaking impact. The principles and modeling capabilities introduced by transformers are carried forward by all modern language models built upon this framework. We will build our intuition for Transformers through historical context and hands-on implementation, as this foundational understanding is key to understanding the future of GenAI.</p>
<h1 id="_idParaDest-47">Early appro<a id="_idTextAnchor083"/>aches in NLP</h1>
<p>Before the <a id="_idIndexMarker165"/>widespread <a id="_idIndexMarker166"/>use of <strong class="bold">neural networks</strong> (<strong class="bold">NNs</strong>) in language processing, NLP was largely grounded in methods that counted words. Two particularly notable techniques<a id="_idIndexMarker167"/> were <strong class="bold">count vectors</strong> and <strong class="bold">Term Frequency-Inverse Document Frequency</strong> (<strong class="bold">TF-IDF</strong>). In essence, count vectors tallied up how <a id="_idIndexMarker168"/>often each word appeared in a document. Building on this, Dadgar et al. applied the TF-IDF algorithm (historically used for information retrieval) to text classification in 2016. This method assigned weights to words based on their significance in one document relative to their occurrence across a collection of documents. These count-based methods were successful for tasks such as searching and categorizing. However, they presented a key limitation in that they could not capture the semantic relationships between words, meaning they could not interpret the nuanced meanings of words in context. This challenge paved the way for exploring NNs, offering a deeper and more nuanced way to understand and rep<a id="_idTextAnchor084"/>resent text.</p>
<h2 id="_idParaDest-48"><a id="_idTextAnchor085"/>Advent of neural language models</h2>
<p>In 2003, Yoshua <a id="_idIndexMarker169"/>Bengio’s team<a id="_idIndexMarker170"/> at the University of Montreal introduced the <strong class="bold">Neural Network Language Model</strong> (<strong class="bold">NNLM</strong>), a <a id="_idIndexMarker171"/>novel approach to language technology. The NNLM was designed to predict the next word in a sequence based on prior words using a particular type of <strong class="bold">neural network</strong> (<strong class="bold">NN</strong>). The<a id="_idIndexMarker172"/> design prominently featured hidden layers that learned word embeddings, which are compact vector representations capturing the core semantic meanings of words. This aspect was absent in count-based approaches. However, the NNLM was still limited in its ability to interpret longer sequences and handle large vocabularies. Despite these limitations, the NNLM sparked widespread exploration of NNs in language modeling.</p>
<p>The introduction of the NNLM highlighted the potential of NNs in language processing, particularly using word embeddings. Yet, its limitations with long sequences and large vocabulary<a id="_idIndexMarker173"/> signaled <a id="_idIndexMarker174"/>the need for<a id="_idTextAnchor086"/> further research.</p>
<h2 id="_idParaDest-49"><a id="_idTextAnchor087"/>Distributed representations</h2>
<p>Following the inception<a id="_idIndexMarker175"/> of the<a id="_idIndexMarker176"/> NNLM, NLP research was propelled toward crafting high-quality word vector representations. These representations could be initially learned from extensive sets of unlabeled text data and later applied to downstream models for various tasks. The period saw the emergence of two prominent <a id="_idIndexMarker177"/>methods: <strong class="bold">Word2Vec </strong>(introduced by Mikolov et al., 2013) and <strong class="bold">Global Vectors</strong> (<strong class="bold">GloVe</strong>, introduced<a id="_idIndexMarker178"/> by Pennington et al., 2014). These methods applied <strong class="bold">distributed representation</strong> to craft high-quality word vector representations.  Distributed representation portrays items such as words not as unique identifiers but as sets of continuous values or vectors. In these vectors, each value corresponds to a specific feature or characteristic of the item. Unlike traditional representations, where each item has a unique symbol, distributed representations allow these items to share features with others, enabling a more intelligent capture of underlying patterns in the data.</p>
<p>Let us elucidate this concept a bit further. Suppose we represent words based on two features: <strong class="bold">Formality</strong> and <strong class="bold">Positivity</strong>. We might have vectors such as the following:</p>
<p class="Basic-Paragraph"><code>Formal: [1, 0]</code></p>
<p class="Basic-Paragraph"><code>Happy: [0, 1]</code></p>
<p class="Basic-Paragraph"><code>Cheerful: [0, 1]</code></p>
<p>In this example, each element in the vector corresponds to one of these features. In the vector for <code>Formal</code>, the <code>1</code> element under <em class="italic">Formality</em> indicates that the word is formal, while the <code>0</code> element under <em class="italic">Positivity</em> indicates neutrality in terms of positivity. Similarly, for <code>Happy</code> and <code>Cheerful</code>, the 1 element under <em class="italic">Positivity</em> indicates that these words have a positive connotation. This way, distributed representation captures the essence of words through vectors, allowing for shared features among different words to understand underlying patterns in data.</p>
<p>Word2Vec employs a relatively straightforward approach where NNs are used to predict the surrounding words for each target word in a dataset. Through this process, the NN ascertains values or “weights” for each target word. These weights form a vector for each word<a id="_idIndexMarker179"/> in a <strong class="bold">continuous vector space</strong>—a mathematical space wherein each point represents a possible value a vector can take. In the context of NLP, each dimension of this space corresponds to a feature, and the position of a word in this space captures its semantic or linguistic relationships to other words.</p>
<p>These vectors <a id="_idIndexMarker180"/>form a <strong class="bold">feature-based representation</strong>—a type of representation where each dimension represents a different feature that contributes to the word’s meaning. Unlike <a id="_idIndexMarker181"/>a symbolic<a id="_idIndexMarker182"/> representation, where each word is represented as a unique symbol, a feature-based representation captures the semantic essence of words in terms of shared features.</p>
<p>On the other hand, GloVe adopts a different approach. It analyzes the <strong class="bold">global co-occurrence statistics</strong>—a count<a id="_idIndexMarker183"/> of how often words appear together in a large text corpus. GloVe learns vector representations that capture the relationships between words by analyzing these counts across the entire corpus. This method also results in a distributed representation of words in a continuous vector space, capturing <strong class="bold">semantic similarity</strong>—a <a id="_idIndexMarker184"/>measure of the degree to which two words are similar in meaning. In a continuous vector space, we can think about semantic similarity as the simple geometric proximity of vectors representing words.</p>
<p>To further illustrate, suppose we have a tiny corpus of text containing the following sentences:</p>
<p>“Coffee is hot.”</p>
<p>“Ice cream is cold.”</p>
<p>From this corpus, GloVe would notice that “<em class="italic">coffee</em>” co-occurs with “<em class="italic">hot</em>” and “<em class="italic">ice</em> <em class="italic">cream</em>” co-occurs with “<em class="italic">cold</em>.” Through its optimization process, it would aim to create vectors for these words in a way that reflects these relationships. In this oversimplified example, GloVe might produce a vector such as this:</p>
<p class="Basic-Paragraph"><code>Coffee: [1, 0]</code></p>
<p class="Basic-Paragraph"><code>Hot: [0.9, 0]</code></p>
<p class="Basic-Paragraph"><code>Ice Cream: [0, 1]</code></p>
<p class="Basic-Paragraph"><code>Cold: [0, 0.9]</code></p>
<p>Here, the closeness of the vectors for “<em class="italic">coffee</em>” and “<em class="italic">hot</em>” (and, similarly, “<em class="italic">ice</em> <em class="italic">cream</em>” and “<em class="italic">cold</em>”) in this space reflects the co-occurrence relationships observed in the corpus. The vector difference between “<em class="italic">coffee</em>” and “<em class="italic">hot</em>” might resemble the vector difference between “<em class="italic">ice</em> <em class="italic">cream</em>” and “<em class="italic">cold</em>,” capturing the contrasting temperature relationships in a geometric way within the vector space.</p>
<p>Both Word2Vec and GloVe excel at encapsulating relevant semantic information about words to represent an efficient <strong class="bold">encoding</strong>—a compact way of representing information that captures the essential features necessary for a task while reducing the dimensionality and complexity of the data.</p>
<p>These methodologies in creating meaningful vector representations served as a step toward the<a id="_idIndexMarker185"/> adoption <a id="_idIndexMarker186"/>of <strong class="bold">transfer learning </strong>in NLP. The vectors provide a shared semantic foundation that facilitates the transfer of learned relat<a id="_idTextAnchor088"/>ionships across varying tasks.</p>
<h2 id="_idParaDest-50"><a id="_idTextAnchor089"/>Transfer Learning</h2>
<p>GloVe and other<a id="_idIndexMarker187"/> methods of<a id="_idIndexMarker188"/> deriving distributed representations paved the way for transfer learning in NLP. By creating rich vector representations of words that encapsulate semantic relationships, these methods provided a foundational understanding of text. The vectors serve as a shared base of knowledge that can be applied to different tasks. When a model, initially trained on one task, is utilized for another, the pre-learned vector representations aid in preserving the semantic understanding, thereby reducing the data or training needed for the new task. This practice of transferring acquired knowledge has become fundamental for efficiently addressing a range of NLP tasks.</p>
<p>Consider a model trained to understand sentiments (positive or negative) in movie reviews. Through training, this model has learned distributed representations of words, capturing sentiment-related nuances. Now, suppose there is a new task: understanding sentiments in product reviews. Instead of training a new model from the beginning, transfer learning allows us to use the distributed representations from the movie review task to initiate training for the product review task. This could lead to quicker training and better performance, especially with limited data for the product review task.</p>
<p>The effectiveness of transfer learning, bolstered by distributed representations from methods such as GloVe, highlighted the potential of leveraging pre-existing knowledge for new tasks. It was a precursor to the integration of NNs in NLP, highlighting the benefits of utilizing learned representations<a id="_idIndexMarker189"/> across tasks. The advent of NNs in NLP brought about models capable of learning even richer representations, further amplifying the impact and scope of transfer learning.</p>
<h2 id="_idParaDest-51"><a id="_idTextAnchor090"/>Advent of NNs in NLP</h2>
<p>The advent of NNs in <a id="_idIndexMarker190"/>NLP marked<a id="_idIndexMarker191"/> a monumental shift in the field’s capability to understand and process language. Building upon the groundwork laid by methodologies such as Word2Vec, GloVe, and the practice of transfer learning, NNs introduced a higher level of abstraction and learning capacity. Unlike previous methods that often relied on hand-crafted features, NNs could automatically learn intricate patterns and relationships from data. This ability to learn from data propelled NLP into a new era where models could achieve unprecedented levels of performance across a myriad of language-related tasks. The emergence of architectures such as CNNs and RNNs, followed by the revolutionary transformer architecture, showcased the remarkable versatility and efficacy of NNs in tackling complex NLP challenges. This transition not only accelerated the pace of innovation but also expanded the horizon of what could be achieved in <a id="_idIndexMarker192"/>understan<a id="_idTextAnchor091"/>ding human language computationally.</p>
<h3>Language modeling with RNNs</h3>
<p>Despite how well<a id="_idIndexMarker193"/> these distributed word vectors excelled at <a id="_idIndexMarker194"/>encoding local semantic relationships, modeling long-range dependencies would require a more sophisticated network architecture. This led to the use of RNNs. RNNs (originally introduced by Elman in 1990) are a type of NN architecture that processes data sequences by iterating through each element of the sequence while maintaining a dynamic internal state that captures information about the previous elements. Unlike<a id="_idIndexMarker195"/> traditional <strong class="bold">feedforward networks</strong> (<strong class="bold">FNNs</strong>) that processed each input independently, RNNs introduced iterations that allowed information to be passed from one step in the sequence to the next, enabling them to capture temporal dependencies in data. The iterative processing and dynamic updating in NNs enable them to learn and represent relationships within the text. These networks can capture contextual connections and interdependencies across sentences or even entire documents.</p>
<p>However, standard RNNs had technical limitations when dealing with long sequences. This led to the development<a id="_idIndexMarker196"/> of <strong class="bold">long short-term memory</strong> (<strong class="bold">LSTM</strong>) networks. LSTMs were first introduced by Hochreiter and Schmidhuber in 1997. They were a special class of RNNs designed to address the <strong class="bold">vanishing gradient</strong> problem, which<a id="_idIndexMarker197"/> is the challenge where the network cannot learn from earlier parts of a sequence as the sequence gets longer. LSTMs applied a unique gating architecture to control the flow of information within the network, enabling them to maintain and access information over long sequences without suffering from the vanishing gradient problem.</p>
<p>The name “<strong class="bold">long short-term memory</strong>” refers<a id="_idIndexMarker198"/> to the network’s ability to keep track of information over both short and long sequences of data:</p>
<ul>
<li><strong class="bold">Short-term</strong>: LSTMs can remember recent information, which is useful for understanding the current context. For example, in language modeling, knowing the last few words can be crucial for predicting the next word. Consider a phrase such as, “The cat, which already ate a lot, was not hungry.” As the LSTM processes the text, when it reaches the word “not,” the recent information that the cat “ate a lot” is crucial to predict the next word, “hungry,” accurately.</li>
<li><strong class="bold">Long-term</strong>: Unlike standard RNNs, LSTMs are also capable of retaining information from many steps back in the sequence, which is particularly useful for long-range dependencies, where a piece of information early in a sentence could be important for understanding a word much later in the sequence. In the same phrase, the information that “The cat” is the subject of the sentence is introduced early on. This <a id="_idIndexMarker199"/>information is crucial later to understand who “was not hungry” as it processes the later part of the sentence.</li>
</ul>
<p>The <strong class="bold">M</strong> or <strong class="bold">memory</strong> in LSTMs is maintained through a unique architecture that employs three gating mechanisms—input, output, and forget gates. These gates control the flow of information within the network, deciding what information should be kept, discarded, or used at each step in the sequence, enabling LSTMs to maintain and access information over long sequences. Effectively, these gates and the network state allowed LTSMs to carry the “memory” across time steps, ensuring that valuable information was retained throughout the processing of the sequence.</p>
<p>Ultimately, LSTMs obtained state-of-the-art results on many language modeling and text classification benchmarks. They became the dominant NN architecture for NLP tasks due to their ability to capture short- and long-range contextual relationships.</p>
<p>The success of LSTMs demonstrated the potential of neural architectures in capturing the complex <a id="_idIndexMarker200"/>relationships inherent in language, significantly <a id="_idIndexMarker201"/>advancing the field of NLP. However, the continuous pursuit of more efficient and effective models led the commu<a id="_idTextAnchor092"/>nity toward exploring other NN architectures.</p>
<h3>Rise of CNNs</h3>
<p>Around 2014, the NLP<a id="_idIndexMarker202"/> domain witnessed a rise in the popularity of CNN<a id="_idIndexMarker203"/>s for tackling NLP tasks, a notable shift led by Yoon Kim. CNNs (originally brought forward by LeCun et al. for image recognition) operate based on convolutional layers that scan the input by moving a filter (or kernel) across the input data, at each position calculating the dot product of the filter’s weights and the input data. In NLP, these layers work over local n-gram windows (consecutive sequences of <em class="italic">n</em> words) to identify patterns or features, such as specific sequences of words or characters in the text. Employing convolutional layers over local n-gram windows, CNNs scan and analyze the data to detect initial patterns or features. Following this, pooling layers are used to reduce the dimensionality of the data, which helps in both reducing computational complexity and focusing on the most salient features identified by the convolutional layers.</p>
<p>Combining convolutional and pooling layers, CNNs can extract hierarchical features. These features represent information at different levels of abstraction by combining simpler, lower-level features to form more complex, higher-level features. In NLP, this process might start with detecting basic patterns such as common word pairs or phrases in the initial layers, progressing to recognizing more abstract concepts such as semantic relationships in<a id="_idIndexMarker204"/> the higher layers.</p>
<p>For comparison, we again consider a scenario where a CNN is employed to analyze and categorize customer reviews into positive, negative, or neutral sentiments:</p>
<ul>
<li><strong class="bold">Lower-level features (initial layers)</strong>: The CNN <a id="_idIndexMarker205"/>might identify basic patterns such as common word pairs or phrases in the initial layers. For instance, it might recognize phrases such as “great service,” “terrible experience,” or “not happy.”</li>
<li><strong class="bold">Intermediate-level features (middle layers)</strong>: As data progresses through the network, middle layers might start recognizing more complex patterns, such as negations (“not good”) or contrasts (“good but expensive”).</li>
<li><strong class="bold">Higher-level features (later layers)</strong>: The CNN could identify abstract concepts such as overall sentiment in the later layers. For instance, it might deduce a positive sentiment from phrases such as “excellent service” or “loved the ambiance” and a negative sentiment from phrases such as “worst experience” or “terrible food.”</li>
</ul>
<p>In this way, CNNs inherently learn higher-level abstract representations of text. Although they lack the sequential processing characteristic of RNNs, they offer a computational advantage due to their inherent <strong class="bold">parallelism</strong> or ability to process multiple parts of the data simultaneously. Unlike RNNs, which process sequences iteratively and require the previous step to be completed before proceeding to the next, CNNs can process various parts of the input data in parallel, significantly speeding up training times.</p>
<p>CNNs, while efficient, have a limitation in their convolution operation, which only processes local data from smaller or nearby regions, thereby missing relationships across more significant portions of the entire input data, referred to as global information. This gave rise to attention-augmented convolutional networks that integrate self-attention with convolutions to address this limitation. Self-attention, initially used in sequence and generative modeling, was adapted for visual tasks such as image classification, enabling the network to process and capture relationships across the entire input data. However, attention augmentation, which combines convolutions and self-attention, yielded the best results. This method retained the computational efficiency of CNNs and captured global information, marking an advancement in image classification and object detection tasks. We will discuss self-attention in detail later as it became a critical component of the transformer.</p>
<p>The ability of CNNs to process multiple parts of data simultaneously marked a significant advancement in computational efficiency, paving the way for further innovations in NN architectures for<a id="_idIndexMarker206"/> NLP. As the field progressed, a pivotal shift occurred with the<a id="_idIndexMarker207"/> advent of attention-augmented NNs, introducing a<a id="_idTextAnchor093"/> new paradigm in how models handle sequential data.</p>
<h1 id="_idParaDest-52"><a id="_idTextAnchor094"/>The emergence of the Transformer in advanced language models</h1>
<p>In 2017, inspired by the<a id="_idIndexMarker208"/> capabilities of CNNs and the innovative application of attention mechanisms, Vaswani et al. introduced the transformer architecture in the seminal paper <em class="italic">Attention is All You Need</em>. The original transformer applied several novel methods, particularly emphasizing the instrumental impact of attention. It employed a <strong class="bold">self-attention mechanism</strong>, allowing <a id="_idIndexMarker209"/>each element in the input sequence to focus on distinct parts of the sequence, capturing dependencies regardless of their positions in a structured manner. The term “self” in “self-attention” refers to how the attention mechanism is applied to the input sequence itself, meaning each element in the sequence is compared to every other element to determine its attention scores.</p>
<p>To truly appreciate how the transformer architecture works, we can describe how the components in its architecture play a role in handling a particular task. Suppose we need our transformer to translate the English sentence “<em class="italic">Hello, how are you?</em>” into French: “<em class="italic">Bonjour, comment ça va?</em>” Let us walk through this step by step to examine and elucidate how the<a id="_idIndexMarker210"/> transformer might accomplish this task. For now, we will describe each step in detail <a id="_idTextAnchor095"/>and later implement the full architecture us<a id="_idTextAnchor096"/>ing Python.</p>
<h2 id="_idParaDest-53"><a id="_idTextAnchor097"/>Components of the transformer architecture</h2>
<p>Before diving into <a id="_idIndexMarker211"/>how the transformer model fulfills our translation task, we need to understand the steps involved. The complete architecture is quite dense, so we will break it down into small, logical, and digestible components.</p>
<p>First, we discuss the two components central to the architectural design of the transformer model: the encoder and decoder stacks. We will also explain how data flows within these layer stacks, including the concept of tokens, and how relationships between tokens are captured and refined using critical techniques such as self-attention and FFNs.</p>
<p>Then, we transition into the training process of the transformer model. Here, we review fundamental concepts such as batches, masking, the training loop, data preparation, optimizer selection, and strategies to improve performance. We will explain how the transformer optimizes performance using a loss function, which is crucial in shaping how the model learns to translate.</p>
<p>Following the training process, we discuss model inference, which is how our trained model generates translations. This section points out the order in which individual model components operate during translation and emphasizes the importance of each step.</p>
<p>As discussed, central to the transformer are two vital components, often called the encoder stack <a id="_idIndexMarker212"/>and the decoder stack.</p>
<h3>Encoder and decoder stacks</h3>
<p>In the context of the transformer model, <strong class="bold">stacks</strong> reference a hierarchical arrangement of <strong class="bold">layers</strong>. Each layer in this context is, in fact, an NN layer like the layers we come across in classical DL models. While a layer is a level in the model where specific computational operations occur, a stack refers to multiple such layers arranged consecutively.</p>
<h4>Encoder stack</h4>
<p>Consider our <a id="_idIndexMarker213"/>example sentence “<em class="italic">Hello, how are you?</em>”. We<a id="_idIndexMarker214"/> first convert it into tokens. Each token typically represents a word. In the case of our example sentence, tokenization would break it down into separate tokens, resulting in the following:</p>
<p><code>[“Hello”, “,”, “how”, “are”, “</code><code>you”, “?”]</code></p>
<p>Here, each word or punctuation represents a distinct token. These tokens are then transformed into numerical representations, also known <a id="_idIndexMarker215"/>as <strong class="bold">embeddings</strong>. These embedding vectors capture the semantic meaning and context of the words, enabling the model to understand and process the input data effectively. The embeddings aid in capturing complex relationships and contexts from the original English input sentence through this series of transformations across layers.</p>
<p>This stack comprises multiple layers, where each layer applies self-attention and FFN computations on its input data (which we will describe in detail shortly). The embeddings iteratively capture complex relationships and context from the original English input sentence through this series of transformations across layers.</p>
<h4>Decoder stack</h4>
<p>Once the encoder<a id="_idIndexMarker216"/> completes its <a id="_idIndexMarker217"/>task, the output vectors—or the embeddings of the input sentence that hold its contextual information—are passed on to the decoder. Within the decoder stack, multiple layers work sequentially to generate a French translation from the embeddings.</p>
<p>The process begins by converting the first embedding into the French phrase “<em class="italic">Bonjour</em>.” The subsequent layer uses the following embedding and context from the previously generated words to predict the next word in the French sentence. This process is repeated through all the layers in the stack, each using input embeddings and generated words to define and refine the translation.</p>
<p>The decoder stack progressively builds (or decodes) the translated sentence through this iterative process, arriving at “<em class="italic">Bonjour, comment </em><em class="italic">ça va?</em>”.</p>
<p>With an overall understanding of the encoder-decoder structure, our next step is unraveling the intricate operations within each stack. However, before delving into the self-attention mechanism and FFNs, there is one vital component we need to understand — positional encoding. Positional encoding is paramount to the transformer’s performance because it gives the transformer model a sense of the order of words, something subsequent operations in the stack lack.</p>
<h3>Positional encoding</h3>
<p>Every word in a<a id="_idIndexMarker218"/> sentence holds <a id="_idIndexMarker219"/>two types of information — its meaning and its role in the larger context of the sentence. The contextual role often stems from a word’s position in the arrangement of words. A sentence such as “<em class="italic">Hello, how are you?</em>” makes sense because the words are in a specific order. Change that to “<em class="italic">Are you, how hello?</em>” and the meaning becomes unclear.</p>
<p>Consequently, Vaswani et al. introduced <strong class="bold">positional encoding</strong> to ensure that the transformer encodes each word with additional data about its position in the sentence. Positional encodings are computed using a blend of sine and cosine functions across different frequencies, which generate a unique set of values for each position in a sequence. These values are then added to the original embeddings of the tokens, providing a way for the model to capture the order of words. These enriched embeddings are then ready to be processed by the self-attention mechanism in the subsequent<a id="_idIndexMarker220"/> layers of the transformer<a id="_idIndexMarker221"/> model.</p>
<h3>Self-attention mechanism</h3>
<p>As each token <a id="_idIndexMarker222"/>of our input<a id="_idIndexMarker223"/> sentence “<em class="italic">Hello, how are you?</em>” passes through each layer of the encoder stack, it undergoes a transformation via the self-attention mechanism.</p>
<p>As the name suggests, the self-attention mechanism allows each token (word) to attend to (or focus on) other vital tokens to understand the full context within the sentence. Before encoding a particular word, this attention mechanism interprets the relationship between each word and the others in the sequence. It then assigns distinct attention scores to different words based on their relevance to the current word being processed.</p>
<p>Consider again our input sentence “<em class="italic">Hello, how are you?</em>”. When the self-attention mechanism is processing the last word, “<em class="italic">you</em>,” it does not just focus on “<em class="italic">you</em>.” Instead, it takes into consideration the entire sentence: it looks at “<em class="italic">Hello</em>,” glances over “<em class="italic">how</em>,” reflects on “<em class="italic">are</em>,” and, of course, focuses on “<em class="italic">you</em>.”</p>
<p>In doing so, it assigns various levels of attention to each word. You can visualize attention (<em class="italic">Figure 3</em><em class="italic">.1</em>) as lines connecting “<em class="italic">you</em>” to every other word. The line to “<em class="italic">Hello</em>” might be thick, indicating a lot of attention, representing the influence of “<em class="italic">Hello</em>” on the encoding of “<em class="italic">you</em>.” The line connecting “<em class="italic">you</em>” and “<em class="italic">how</em>” might be thinner, suggesting less attention given to “<em class="italic">how</em>.” The lines to “<em class="italic">are</em>” and “<em class="italic">you</em>” would have other thicknesses based on how they help in providing context to “<em class="italic">you</em>”:</p>
<p class="IMG---Figure"> </p>
<div><div><img alt="Figure 3.1: Self-attention mechanism" src="img/B21773_03_01.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.1: Self-attention mechanism</p>
<p>This way, when encoding “<em class="italic">you</em>,” a weighted mix of the entire sentence is considered, not just the single word. And these weights defining the mix are what we refer to as attention scores.</p>
<p>The self-attention mechanism is implemented through a few steps:</p>
<ol>
<li>Initially, each input word is represented as a vector, which we obtain from the word embedding.</li>
<li>These vectors are then mapped to new vectors called query, key, and value vectors through learned transformations.</li>
<li>An attention score for each word is then computed by taking the dot product of the query vector of the word with the key vector of every other word, followed by a SoftMax operation (which we will describe later).</li>
<li>These scores indicate how much focus to place on other parts of the input sentence for each word as it is encoded.</li>
<li>Finally, a weighted sum of the value vectors is computed based on these scores to give us our final output vectors, or the self-attention outputs.</li>
</ol>
<p>It is important to note that this computation is done for each word in the sentence. This ensures a comprehensive understanding of the context in the sentence, considering multiple parts of the sentence at once. This concept set the transformer apart from nearly every model that came before it.</p>
<p>Instead of running <a id="_idIndexMarker224"/>the <a id="_idIndexMarker225"/>self-attention mechanism once (or “single-head” attention), the transformer replicates the self-attention mechanism multiple times in parallel. Each replica or head operates on the same input but has its own independent set of learned parameters to compute the attention scores. This allows each head to learn different contextual relationships between words. This parallel process is<a id="_idIndexMarker226"/> known as <strong class="bold">multi-head </strong><strong class="bold">attention</strong> (<strong class="bold">MHA</strong>).</p>
<p>Imagine our sentence “<em class="italic">Hello, how are you?</em>” again. One head might concentrate on how “<em class="italic">Hello</em>” relates to “<em class="italic">you</em>,” whereas another head might focus more on how “<em class="italic">how</em>” relates to “<em class="italic">you</em>.” Each head has its own set of query, key, and value weights, further enabling them to specialize and learn different things. The outputs of these multiple heads are then concatenated and transformed to produce final values passed onto the next layer in the stack.</p>
<p>This multi-head approach allows the model to capture a wider range of information from the same input words. It is like having several perspectives on the same sentence, each providing unique insights.</p>
<p>So far, for our input sentence “<em class="italic">Hello, how are you?</em>”, we have converted each word into token representations, which are then contextualized using the MHA mechanism. Through parallel self-attention, our transformer can consider the full range of interactions between each word and every other word in the sentence. We now have a set of diverse and context-enriched word representations, each containing a textured understanding of a word’s role in the sentence. However, this contextual understanding contained within the attention mechanism is just one component of the information processing in our transformer model. Next comes another layer of interpretation through<a id="_idIndexMarker227"/> position-wise <a id="_idIndexMarker228"/>FFNs. The FFN will add further nuances to these representations, making them more informative and valuable for our translation task.</p>
<p>In the next section, we discuss a vital aspect of the transformer’s training sequence: masking. Specifically, the transformer applies causal (or look-ahead) masking during the decoder self-attention to ensure that each output token prediction depends only on previously generated<a id="_idIndexMarker229"/> tokens, not future unknown tokens.</p>
<h3>Masking</h3>
<p>The transformer <a id="_idIndexMarker230"/>applies two types of masking during training. The first is a preprocessing step to ensure input sentences are of the same length, which enables efficient batch computation. The second is look-ahead (or causal) masking, which allows the model to selectively ignore future tokens in a sequence. This type of masking occurs in the self-attention mechanism in the decoder and prevents the model from peeking ahead at future tokens in the sequence. For example, when translating the word “<em class="italic">Hello</em>” to French, look-ahead masking ensures that the model does not have access to the subsequent words “<em class="italic">how</em>,” “<em class="italic">are</em>,” or “<em class="italic">you</em>.” This way, the model learns to generate translations based on the current and preceding words, adhering to a natural progression in translation tasks, mimicking that of human translation.</p>
<p>With a clearer understanding of how data is prepared and masked for training, we now transition to another significant aspect of the training process: hyperparameters. Unlike parameters learned from the data, hyperparameters are configurations set before training to control the model optimization process and guide the learning journey. The following section will explore various hyperparameters and their roles during training.</p>
<h3>SoftMax</h3>
<p>To understand the <a id="_idIndexMarker231"/>role of the FFN, we <a id="_idIndexMarker232"/>can describe its two primary components—linear transformations and an activation function:</p>
<ul>
<li><strong class="bold">Linear transformations</strong> are essentially <a id="_idIndexMarker233"/>matrix <a id="_idIndexMarker234"/>multiplications. Think of them as tools that reshape or tweak the input data. In the FFN, these transformations occur twice, where two different weights (or matrices) are used.</li>
<li>A <strong class="bold">rectified linear unit</strong> (<strong class="bold">ReLU</strong>) function<a id="_idIndexMarker235"/> is applied between these two transformations. The role of the ReLU function is to introduce non-linearity in the model. Simply put, the ReLU function allows the model to capture patterns within the input data that are not strictly proportional, i.e., <a id="_idIndexMarker236"/>non-linear, which <a id="_idIndexMarker237"/>is typical<a id="_idIndexMarker238"/> of <strong class="bold">natural language</strong> (<strong class="bold">NL</strong>) data.</li>
</ul>
<p>The FFN is called <strong class="bold">position-wise</strong> because<a id="_idIndexMarker239"/> it treats each word in the sentence separately (position by position), regardless of the sequence. This contrasts with the self-attention mechanism, which considers the entire sequence at once.</p>
<p>So, let us attempt to visualize the process: Imagine our word “<em class="italic">Hello</em>” arriving here after going through the self-attention mechanism. It carries with it information about its own identity mixed with contextual references to “<em class="italic">how</em>,” “<em class="italic">are</em>,” and “<em class="italic">you</em>.” This integrated information resides within a vector that characterizes “<em class="italic">Hello</em>.”</p>
<p>When “<em class="italic">Hello</em>” enters the FFN, picture it as a tunnel with two gates. At the first gate (or linear layer), “<em class="italic">Hello</em>” is transformed by a matrix multiplication operation, changing its representation. Afterward, it encounters the ReLU function—which makes the representation non-linear.</p>
<p>After this, “<em class="italic">Hello</em>” passes through a second gate (another linear layer), emerging on the other side transformed yet again. The core identity of “<em class="italic">Hello</em>” remains but is now imbued with even more context, carefully calibrated and adjusted by the FFN.</p>
<p>Once the input passes through the gates, there is one additional step. The transformed vector still must be<a id="_idIndexMarker240"/> converted into a form that can be interpreted as a prediction for our final translation task.</p>
<p>This brings us to using the SoftMax function, the final transformation within the transformer’s decoder. After the vectors pass through the FFN, they are further processed through a final linear layer. The result is then fed into a SoftMax function.</p>
<p>SoftMax serves as a mechanism for converting the output of our model into a form that can be interpreted as probabilities. In essence, the SoftMax function will take the output from our final linear layer (which could be any set of real numbers) and transform it into a distribution of<a id="_idIndexMarker241"/> probabilities, representing the likelihood of each word being the next word in our output sequence. For example, if our target vocabulary includes “<em class="italic">Bonjour</em>,” “<em class="italic">Hola</em>,” “<em class="italic">Hello</em>,” and “<em class="italic">Hallo</em>,” the SoftMax function will assign each of these words a probability, and the word with the highest probability will be chosen as the output translation for the word “<em class="italic">Hello</em>.” We can illustrate with this oversimplified representation of the output probabilities:</p>
<p><code>[ Bonjour: 0.4, Hola: 0.3, Hello: 0.2, Hallo: </code><code>0.1 ]</code></p>
<p><em class="italic">Figure 3</em><em class="italic">.2</em> shows a more complete (albeit oversimplified) view of the flow of information through the architecture.</p>
<p class="IMG---Figure"> </p>
<div><div><img alt="Figure 3.2: A simplified illustration of the transformer" src="img/B21773_03_02.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.2: A simplified illustration of the transformer</p>
<p>Now that we’ve<a id="_idIndexMarker242"/> introduced the<a id="_idIndexMarker243"/> architectural components of the transformer, we are poised to understand how its components work together.</p>
<h2 id="_idParaDest-54"><a id="_idTextAnchor098"/>Sequence-to-sequence learning</h2>
<p>The components of<a id="_idIndexMarker244"/> a transformer come together to learn from data using a mechanism known as <strong class="bold">sequence-to-sequence</strong> (<strong class="bold">Seq2Seq</strong>) learning, a subset of <strong class="bold">supervised learning</strong> (<strong class="bold">SL</strong>). Recall <a id="_idIndexMarker245"/>that SL is a technique that uses labeled data to train models to predict outcomes accurately. In Seq2Seq learning, we provide the transformer with training data that comprises examples of input and corresponding correct output, which, in this case, are correct translations. Seq2Seq learning is particularly well suited for tasks such as machine translation where both the input and output are sequences of words.</p>
<p>The very first step in the learning process is to convert each word in the phrase into tokens, which are then transformed into numerical embeddings. These embeddings carry the semantic essence of each word. Positional encodings are computed and added to these embeddings to imbue them with positional awareness.</p>
<p>As these enriched embeddings traverse through the encoder stack, within each layer, the self-attention mechanism refines the embeddings by aggregating contextual information from the entire phrase. Following self-attention, each word’s embedding undergoes further transformation in the position-wise FFNs, adjusting the embeddings to capture even more complex relationships.</p>
<p>Upon exiting the encoder, the embeddings now hold a rich mixture of semantic and contextual information. They are passed onto the decoder stack, which aims to translate the phrase into another language (that is, the target sequence). As with the encoder, each layer in the decoder also employs self-attention and position-wise FFNs, but with an additional layer of cross-attention that interacts with the encoder’s outputs. This interaction helps align the input and output phrases, a crucial aspect of translation.</p>
<p>As the embeddings move through the decoder layers, they are progressively refined to represent the translated phrase that the model will predict. The final layer of the decoder processes the embeddings through a linear transformation and SoftMax function to produce a probability distribution over the target vocabulary. This distribution defines the model’s predicted likelihood for each potential next token at each step. The decoder then samples from this distribution to select the token with the highest predicted probability as its next output. By iteratively sampling the most likely next tokens according to the predicted distributions, the decoder can autoregressively generate the full translated output sequence token by token.</p>
<p>However, for the transformer to reliably sample from the predicted next-token distributions to generate high-quality translations, it must progressively learn by iterating over thousands of examples of input-output pairs. In the next section, we explore model <a id="_idIndexMarker246"/>training in further detail.</p>
<h3>Model training</h3>
<p>As discussed, the<a id="_idIndexMarker247"/> primary goal of the training phase is to refine the model’s parameters to facilitate accurate translation from one language to another. But what does the refinement of parameters entail, and why is it pivotal?</p>
<p>Parameters are internal variables that the model utilizes to generate translations. Initially, these parameters are assigned random values, which are adjusted with each training iteration. Again, the model is provided with training data that comprises thousands of examples of input data and corresponding correct output, which, in this case, is the correct translation. It then compares its predicted output tokens to the correct (or actual) target sequences using an error (or loss) function.</p>
<p>Based on the loss, the model updates its parameters, gradually improving its ability to choose the correct item in the sequence at each step of decoding. This slowly refines the probability distributions.</p>
<p>Over thousands of training iterations, the model learns associations between source and target languages. Eventually, it acquires enough knowledge to decode coherent, human-like translations from unseen inputs by relying on patterns discovered during training. Therefore, training drives the model’s ability to produce accurate target sequences from the predicted vocabulary distributions.</p>
<p>After training on sufficient translation pairs, the transformer reaches reliable translation performance. The trained model can then take in new input sequences and output translated sequences by generalizing to that new data.</p>
<p>For instance, with our example sentence “<em class="italic">Hello, how are you?</em>” and its French translation “<em class="italic">Bonjour, comment ça va?</em>”, the English sentence serves as the input, and the French sentence serves as the target output. The training data comprises many translated pairs. Each time the model processes a batch of data, it generates predictions for the translation, compares them to the actual target translations, and then adjusts its parameters to reduce the discrepancy (or minimize the loss) between the predicted and actual translations. This is repeated with numerous batches of data until the model’s translations are sufficiently accurate.</p>
<h3>Hyperparameters</h3>
<p>Again, unlike <a id="_idIndexMarker248"/>parameters, which the <a id="_idIndexMarker249"/>model learns from the training data, hyperparameters are preset configurations that govern the training process and the structure of the model. They are a crucial part of setting up a successful training run.</p>
<p>Some key hyperparameters in the context of transformer models include the following:</p>
<ul>
<li><strong class="bold">Learning rate</strong>: This value determines the step size at which the optimizer updates the model parameters. A higher learning rate could speed up the training but may overshoot the optimal solution. A lower learning rate may result in a more precise convergence to the optimal solution, albeit at the cost of longer training time. We will discuss optimizers in detail in the next section.</li>
<li><strong class="bold">Batch size</strong>: The number of data examples processed in a single batch affects the computational accuracy and the memory requirements during training.</li>
<li><strong class="bold">Model dimensions</strong>: The model’s size (for example, the number of layers in the encoder and decoder, the dimensionality of the embeddings, and so on) is a crucial hyperparameter that affects the model’s capacity to learn and generalize.</li>
<li><strong class="bold">Optimizer settings</strong>: Choosing an optimizer and its settings, such as the initial learning rate, beta values in the Adam optimizer, and so on, are also considered hyperparameters. Again, we will explore optimizers further in the next section.</li>
<li><strong class="bold">Regularization terms</strong>: Regularization terms such as dropout rate are hyperparameters that help prevent overfitting by adding some form of randomness or constraint to the training process.</li>
</ul>
<p>Selecting the proper values for hyperparameters is crucial for the training process as it significantly impacts the model’s performance and efficiency. It often involves hyperparameter tuning, which involves experimentation and refining to find values for hyperparameters that yield reliable performance for a given task. Hyperparameter tuning can be somewhat of an art and a science. We will touch on this more in later chapters.</p>
<p>With a high-level<a id="_idIndexMarker250"/> grasp of hyperparameters, we will move on to the choice of optimizer, which is pivotal in controlling how efficiently the model learns from the training data.</p>
<h3>Choice of optimizer</h3>
<p>The optimizer is a <a id="_idIndexMarker251"/>fundamental component of the training process and is responsible for updating the model’s parameters to minimize error. Different optimizers have different strategies for navigating the parameter space to find a set of parameter values that yield low loss (or less error). The choice of optimizer can significantly impact the speed and quality of the training process.</p>
<p>In the context of transformer models, the Adam optimizer is often the optimizer of choice due to its efficiency and empirical success in training deep networks. Adam adapts learning rates during training. For simplicity, we will not explore all the possible optimizers but instead describe their purpose.</p>
<p>The optimizer’s primary task is to fine-tune the model’s parameters to reduce translation errors, progressively guiding the model toward the desired level of performance. However, an over-zealous optimization could lead the model to memorize the training data, failing to generalize well to unseen data. To mitigate this, we employ regularization techniques.</p>
<p>In the next section, we will explore regularization—a technique that works with optimization to<a id="_idIndexMarker252"/> ensure that while the model learns to minimize translation errors, it also remains adaptable to new, unseen data.</p>
<h3>Regularization</h3>
<p>Regularization techniques are <a id="_idIndexMarker253"/>employed <a id="_idIndexMarker254"/>to deter the model from memorizing the training data (a phenomenon known as overfitting) and to promote better performance on new, unseen data. Overfitting arises when the model, to minimize the error, learns the training data to such an extent that it captures useless patterns (or noise) along with the actual patterns. This over-precision in learning the training data leads to a decline in performance when the model is exposed to new data.</p>
<p>Let us revisit our simple scenario where we train a model to translate English greetings to French greetings using a dataset that includes the word “<em class="italic">Hello</em>” and its translation “<em class="italic">Bonjour</em>.” If the model is overfitting, it may memorize the exact phrases from the training data without understanding the broader translation pattern.</p>
<p>In an overfit scenario, suppose the model learns to translate “<em class="italic">Hello</em>” to “<em class="italic">Bonjour</em>” with a probability of 1.0 because that is what it encountered most often in the training data. When presented with new, unseen data, it may encounter variations it has not seen before, such as “<em class="italic">Hi</em>,” which should also translate to “<em class="italic">Bonjour</em>.” However, due to overfitting, the model might fail to generalize from “<em class="italic">Hello</em>” to “<em class="italic">Hi</em>” as it is overly focused on the exact mappings it saw during training.</p>
<p>Several regularization techniques can mitigate the overfitting problem. These techniques apply certain constraints on the model’s parameters during training, encouraging the model to learn a more generalized representation of the data rather than memorizing the<a id="_idIndexMarker255"/> training<a id="_idIndexMarker256"/> dataset.</p>
<p>Here are some standard regularization techniques used in the context of transformer models:</p>
<ul>
<li><strong class="bold">Dropout</strong>: In the context of NN-based models such as the transformer, the term “neurons” refers to individual elements within the model that work together to learn from the data and make predictions. Each neuron learns specific aspects or features from the data, enabling the model to understand and translate text. During training, dropout randomly deactivates or “drops out” a fraction of these neurons, temporarily removing them from the network. This random deactivation encourages the model to spread its learning across many neurons rather than relying too heavily on a few. By doing so, dropout helps the model to better generalize its learning to unseen data rather than merely memorizing the training data (that is, overfitting).</li>
<li><strong class="bold">Layer normalization</strong>: Layer normalization is a technique that normalizes the activations of neurons in a layer for each training example rather than across a batch of examples. This normalization helps stabilize the training process and acts as a form of regularization, preventing overfitting.</li>
<li><strong class="bold">L1 or L2 regularization</strong>: L1 regularization, also known as Lasso, adds a penalty equal to the absolute magnitude of coefficients, promoting parameter sparsity. L2 regularization, or Ridge, adds a penalty based on the square of the coefficients, discouraging large values to prevent overfitting. Although these techniques help in controlling model complexity and enhancing generalization, they were not part of the transformer’s initial design.</li>
</ul>
<p>By employing these regularization techniques, the model is guided toward learning more generalized patterns in the data, which improves its ability to perform well on unseen data, thus making the model more reliable and robust in translating new text inputs.</p>
<p>Throughout the training process, we have mentioned the loss function and discussed how the optimizer leverages it to adjust the model’s parameters, aiming to minimize prediction error. The loss function quantifies the model’s performance. We discussed how regularization penalizes the loss function to prevent overfitting, encouraging the model to learn<a id="_idIndexMarker257"/> simpler, more generalizable<a id="_idIndexMarker258"/> patterns. In the next section, we look closer at the nuanced role of the loss function itself.</p>
<h3>Loss function</h3>
<p>The loss function is<a id="_idIndexMarker259"/> vital in training the<a id="_idIndexMarker260"/> transformer model, quantifying the differences between the model’s predictions and the actual data. In language translation, this error is measured between generated and actual translations in the training dataset. A common choice for this task is cross-entropy loss, which measures the difference between the model’s predicted probability distribution across the target vocabulary and the actual distribution, where the truth has a probability of 1 for the correct word and 0 for the rest.</p>
<p>The transformer often employs a variant known as label-smoothed cross-entropy loss.  Label smoothing adjusts the target probability distribution during training, slightly lowering the probability for the correct class and increasing the probability for all other classes, which helps prevent the model from becoming too confident in its predictions. For instance, with a target vocabulary comprising “<em class="italic">Bonjour</em>,” “<em class="italic">Hola</em>,” “<em class="italic">Hello</em>,” and “<em class="italic">Hallo</em>,” and assuming “<em class="italic">Bonjour</em>” is the correct translation, a standard cross-entropy loss would aim for the probability distribution of <code>Bonjour: 1.0</code>, <code>Hola: 0.0</code>, <code>Hello: 0.0</code>, <code>Hallo: 0.0</code>. However, the label-smoothed cross-entropy loss would slightly adjust these probabilities, as follows:</p>
<p><code>[ “Bonjour”: 0.925, “Hola”: 0.025, “Hello”: 0.025, “Hallo”: </code><code>0.025 ]</code></p>
<p>The smoothing reduces the model’s confidence and promotes better generalization to unseen data. With a clearer understanding of the loss function’s role, we can move on to the inference<a id="_idIndexMarker261"/> phase, where the trained model<a id="_idIndexMarker262"/> generates translations for new, unseen data.</p>
<h3>Inference</h3>
<p>Having traversed the <a id="_idIndexMarker263"/>training landscape, our trained model is now adept with optimized parameters to tackle the translation task. In the inference stage, these learned parameters are employed to translate new, unseen text. We will continue with our example phrase “<em class="italic">Hello, how are you?</em>” to elucidate this process.</p>
<p>The inference stage is the practical application of the trained model on new data. The trained parameters, refined after numerous iterations during training, are now used to translate text from one language to another. The inference steps can be described as follows:</p>
<ol>
<li><strong class="bold">Input preparation</strong>: Initially, our<a id="_idIndexMarker264"/> phrase “Hello, how are you?” is tokenized and encoded into a format that the model can process, akin to the preparation steps in the training phase.</li>
<li><strong class="bold">Passing through the model</strong>: The encoded input is then propagated through the model. As it navigates through the encoder and decoder stacks, the trained parameters guide the transformation of the input data, inching closer to accurate translations at each step.</li>
<li><strong class="bold">Output generation</strong>: At the culmination of the decoder stack, the model generates a probability distribution across the target vocabulary for each word in the input text. For the word “<em class="italic">Hello</em>,” a probability distribution is formed over the target vocabulary, which, in our case, comprises French words. The word with the highest probability is selected as the translation. This process is replicated for each word in the phrase, rendering the translated output “<em class="italic">Bonjour, comment </em><em class="italic">ça va?</em>”.</li>
</ol>
<p>Now that we understand how the model produces the final output, we can implement a transformer model step by step to solidify the concepts we have discussed. However, before we dive into the code, we can briefly give a synopsis of the end-to-end architecture <a id="_idIndexMarker265"/>flow:</p>
<ol>
<li><strong class="bold">Input tokenization</strong>: The initial English phrase “<em class="italic">Hello, how are you?</em>” is tokenized into <a id="_idIndexMarker266"/>smaller units such as “<em class="italic">Hello</em>,” “<em class="italic">,</em>,” “<em class="italic">how</em>,” and so on.</li>
<li><strong class="bold">Embeddings</strong>: These tokens are then mapped to continuous vector representations through an embedding layer.</li>
<li><strong class="bold">Positional encoding</strong>: To preserve the order of the sequence, positional encodings are added to the embeddings.</li>
<li><strong class="bold">Encoder self-attention</strong>: The embedded input sequence navigates through the encoder’s sequence of self-attention layers. Here, each word gauges the relevance of every other word to comprehend the full context.</li>
<li><strong class="bold">FFN</strong>: The representations are subsequently refined by position-wise FFNs within each encoder layer.</li>
<li><strong class="bold">Encoder output</strong>: The encoder renders contextual representations capturing the essence of the input sequence.</li>
<li><strong class="bold">Decoder attention</strong>: Incrementally, the decoder crafts the output sequence, employing self-attention solely on preceding words to maintain the sequence order.</li>
<li><strong class="bold">Encoder-decoder attention</strong>: The decoder evaluates the encoder’s output, centering on pertinent input context while generating each word in the output sequence.</li>
<li><strong class="bold">Output layers</strong>: The decoder feeds its output to the linear and SoftMax layers to produce “<em class="italic">Bonjour, comment </em><em class="italic">ça va?</em></li>
</ol>
<p>At the end of this chapter, we will adapt a best-in-class implementation of the original transformer (Huang et al., 2022) into a minimal example that could later be trained on various downstream tasks. This will serve as a theoretical exercise to further solidify our understanding. In practice, we would rely on pre-trained or foundation models, which we will learn to implement in later chapters.</p>
<p>However, before we begin our practice project, we can trace its impact on the current landscape of GenAI. We <a id="_idIndexMarker267"/>follow the trajectory of early <a id="_idIndexMarker268"/>applications of the architecture (for example, <strong class="bold">Bidirectional Encoded Representations from Transformers</strong> (<strong class="bold">BERT</strong>)) through to the first GPT.</p>
<h1 id="_idParaDest-55"><a id="_idTextAnchor099"/>Evolving language models – the AR Transformer and its role in GenAI</h1>
<p>In <a href="B21773_02.xhtml#_idTextAnchor045"><em class="italic">Chapter 2</em></a>, we reviewed some of the generative paradigms that apply a <a id="_idIndexMarker269"/>transformer-based<a id="_idIndexMarker270"/> approach. Here, we trace the <a id="_idIndexMarker271"/>evolution of Transformers more closely, outlining some of the most impactful transformer-based language models from the initial transformer in 2017 to more recent state-of-the-art models that demonstrate the scalability, versatility, and societal considerations involved in this fast-moving domain of AI (as illustrated in <em class="italic">Figure 3</em><em class="italic">.3</em>):</p>
<div><div><img alt="Figure 3.3: From the original transformer to GPT-4" src="img/B21773_03_03.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.3: From the original transformer to GPT-4</p>
<ul>
<li><strong class="bold">2017 – Transformer</strong>: The transformer model, introduced by Vaswani et al., was a paradigm shift in NLP, featuring self-attention layers that could process entire sequences of data in parallel. This architecture enabled the model to evaluate the importance of each word in a sentence relative to all other words, thereby enhancing the model’s ability to capture the context.</li>
<li><strong class="bold">2018 – BERT</strong>: Google’s BERT model innovated on the transformer architecture by utilizing a bidirectional context in its encoder layers during pre-training. It was one of the first models to understand the context of a word based on its entire sentence, both left and right, significantly improving performance on a wide range of NLP tasks, especially those requiring a deep understanding of context.</li>
<li><strong class="bold">2018 – GPT-1</strong>: OpenAI’s GPT-1 model was a milestone in NLP, adopting a generative pre-trained approach with a transformer’s decoder-only model. It was pre-trained on a diverse corpus of text data and fine-tuned for various tasks, using a<a id="_idIndexMarker272"/> unidirectional approach that generated text sequentially from left to right, which was particularly suited for generative text applications.</li>
<li><strong class="bold">2019 – GPT-2</strong>: GPT-2 built upon the foundation laid by GPT-1, maintaining its decoder-only<a id="_idIndexMarker273"/> architecture<a id="_idIndexMarker274"/> but significantly expanding its scale in terms of dataset and model size. This allowed GPT-2 to generate text that was more coherent and contextually relevant across a broader range of topics, demonstrating the power of scaling up transformer models.</li>
<li><strong class="bold">2020 – GPT-3</strong>: OpenAI’s GPT-3 pushed the boundaries of scale in transformer models to 175 billion parameters, enabling a wide range of tasks to be performed with minimal input, often with <strong class="bold">zero-shot learning</strong> (<strong class="bold">ZSL</strong>) or <strong class="bold">few-shot learning</strong> (<strong class="bold">FSL</strong>). This showed that Transformers could generalize across tasks and data types, often without the need for extensive task-specific data or fine-tuning.</li>
<li><strong class="bold">2021 – InstructGPT</strong>: An optimized variant of GPT-3, InstructGPT was fine-tuned specifically to follow user instructions and generate aligned responses, incorporating feedback loops that emphasized safety and relevance in its outputs. This represented a focus on creating AI models that could more accurately interpret and respond to human prompts.</li>
<li><strong class="bold">2023 – GPT-4</strong>: GPT-4 was an evolution of OpenAI’s transformer models into the multimodal space, capable of understanding and generating content based on both text and images. This model aimed to produce safer and more contextually nuanced responses, showcasing a significant advancement in the model’s ability to handle complex tasks and generate creative content.</li>
<li><strong class="bold">2023 – LLaMA 2</strong>: Meta AI’s LLaMA 2 was part of a suite of models that focused on efficiency and accessibility, allowing for high-performance language modeling while being more resource-efficient. This model was aimed at facilitating a broader range of research and application development within the AI community.</li>
<li><strong class="bold">2023 – Claude 2</strong>: Anthropic’s Claude 2 was an advancement over Claude 1, increasing its token context window and improving its reasoning and memory capabilities. It aimed to align more closely with human values, offering responsible and <a id="_idIndexMarker275"/>nuanced generative capabilities for open-domain question-answering and other conversational AI applications, marking progress in ethical AI development.</li>
</ul>
<p>The timeline <a id="_idIndexMarker276"/>presented <a id="_idIndexMarker277"/>highlights the remarkable progress in transformer-based language models over the past several years. What originated as an architecture that introduced the concept of self-attention has rapidly evolved into models with billions of parameters that can generate coherent text, answer questions, and perform a variety of intellectual tasks at high levels of performance. The increase in scale and accessibility of models such as GPT-4 has opened new possibilities for AI applications. At the same time, recent models have illustrated a focus on safety and ethics and providing more nuanced, helpful responses to users.</p>
<p>In the next section, we accomplish a rite of passage for practitioners with an interest in the NL field. We implement the key compo<a id="_idTextAnchor100"/>nents of the original transformer architecture using Python to<a id="_idIndexMarker278"/> more <a id="_idIndexMarker279"/>fully understand<a id="_idIndexMarker280"/> the mechanics that started it all.</p>
<h1 id="_idParaDest-56"><a id="_idTextAnchor101"/>Implementing the original Transformer</h1>
<p>The following code <a id="_idIndexMarker281"/>demonstrates how to implement a minimal transformer model for a Seq2Seq translation task, mainly translating English text to French. The code is structured into multiple sections, handling various aspects from data loading to model training and translation.</p>
<h2 id="_idParaDest-57"><a id="_idTextAnchor102"/>Data loading and preparation</h2>
<p>Initially, the code<a id="_idIndexMarker282"/> loads a dataset and prepares it for training. The data is loaded from a CSV file, which is then split into English and French text. The text is limited to 100 characters for demonstration purposes to reduce training time. The CSV file includes a few thousand example data points and can be found in the book’s GitHub repository (<a href="https://github.com/PacktPublishing/Python-Generative-AI">https://github.com/PacktPublishing/Python-Generative-AI</a>) along with the complete code:</p>
<pre class="source-code">
import pandas as pd
import numpy as np
# Load demo data
data = pd.read_csv("./Chapter_3/data/en-fr_mini.csv")
# Separate English and French lexicons
EN_TEXT = data.en.to_numpy().tolist()
FR_TEXT = data.fr.to_numpy().tolist()
# Arbitrarily cap at 100 characters for demonstration to avoid long training times
def demo_limit(vocab, limit=100):
    return [i[:limit] for i in vocab]
EN_TEXT = demo_limit(EN_TEXT)
FR_TEXT = demo_limit(FR_TEXT)
# Establish the maximum length of a given sequence
MAX_LEN = 100</pre>
<h2 id="_idParaDest-58"><a id="_idTextAnchor103"/>Tokenization</h2>
<p>Next, a tokenizer is<a id="_idIndexMarker283"/> trained on the text data. The tokenizer is essential for converting text data into numerical data that can be fed into the model:</p>
<pre class="source-code">
from tokenizers import Tokenizer
from tokenizers.models import WordPiece
from tokenizers.trainers import WordPieceTrainer
from tokenizers.pre_tokenizers import Whitespace
def train_tokenizer(texts):
    tokenizer = Tokenizer(WordPiece(unk_token="[UNK]"))
    tokenizer.pre_tokenizer = Whitespace()
    trainer = WordPieceTrainer(
        vocab_size=5000,
        special_tokens=["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]", 
            "&lt;sos&gt;", "&lt;eos&gt;"],
    )
    tokenizer.train_from_iterator(texts, trainer)
    return tokenizer
en_tokenizer = train_tokenizer(EN_TEXT)
fr_tokenizer = train_tokenizer(FR_TEXT)</pre>
<h2 id="_idParaDest-59"><a id="_idTextAnchor104"/>Data tensorization</h2>
<p>The text data is then <a id="_idIndexMarker284"/>tensorized, which involves converting the text data into tensor format. This step is crucial for preparing the data for training with <code>PyTorch</code>:</p>
<pre class="source-code">
import torch
from torch.nn.utils.rnn import pad_sequence
def tensorize_data(text_data, tokenizer):
    numericalized_data = [
        torch.tensor(tokenizer.encode(text).ids) for text in text_data
    ]
    padded_data = pad_sequence(numericalized_data,
        batch_first=True)
    return padded_data
src_tensor = tensorize_data(EN_TEXT, en_tokenizer)
tgt_tensor = tensorize_data(FR_TEXT, fr_tokenizer)</pre>
<h2 id="_idParaDest-60"><a id="_idTextAnchor105"/>Dataset creation</h2>
<p>A custom dataset<a id="_idIndexMarker285"/> class is created to handle the data. This class is essential for loading the data in batches during training:</p>
<pre class="source-code">
from torch.utils.data import Dataset, DataLoader
class TextDataset(Dataset):
    def __init__(self, src_data, tgt_data):
        self.src_data = src_data
        self.tgt_data = tgt_data
    def __len__(self):
        return len(self.src_data)
    def __getitem__(self, idx):
        return self.src_data[idx], self.tgt_data[idx]
dataset = TextDataset(src_tensor, tgt_tensor)</pre>
<h2 id="_idParaDest-61"><a id="_idTextAnchor106"/>Embeddings layer</h2>
<p>The embeddings layer <a id="_idIndexMarker286"/>maps each token to a continuous vector space. This layer is crucial for the model to understand and process the text data:</p>
<pre class="source-code">
import torch.nn as nn
class Embeddings(nn.Module):
    def __init__(self, d_model, vocab_size):
        super(Embeddings, self).__init__()
        self.embed = nn.Embedding(vocab_size, d_model)
    def forward(self, x):
        return self.embed(x)</pre>
<h2 id="_idParaDest-62"><a id="_idTextAnchor107"/>Positional encoding</h2>
<p>The positional <a id="_idIndexMarker287"/>encoding layer adds positional information to the embeddings, which helps the model understand the order of tokens in the sequence:</p>
<pre class="source-code">
import math
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout=0.1,
                 max_len=MAX_LEN
    ):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0.0, max_len).unsqueeze(1)
        div_term = torch.exp(
            torch.arange(0.0, d_model, 2) * - \
                (math.log(10000.0) / d_model)
        )
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer("pe", pe)
    def forward(self, x):
        x = x + self.pe[:, : x.size(1)]
        return self.dropout(x)</pre>
<h2 id="_idParaDest-63"><a id="_idTextAnchor108"/>Multi-head self-attention</h2>
<p>The <strong class="bold">multi-head self-attention</strong> (<strong class="bold">MHSA</strong>) layer is <a id="_idIndexMarker288"/>a crucial <a id="_idIndexMarker289"/>part of the transformer architecture that allows the model to focus on different parts of the input sequence when producing an output sequence:</p>
<pre class="source-code">
class MultiHeadSelfAttention(nn.Module):
    def __init__(self, d_model, nhead):
        super(MultiHeadSelfAttention, self).__init__()
        self.attention = nn.MultiheadAttention(d_model, nhead)
    def forward(self, x):
        return self.attention(x, x, x)</pre>
<h2 id="_idParaDest-64"><a id="_idTextAnchor109"/>FFN</h2>
<p>The FFN is<a id="_idIndexMarker290"/> a simple <strong class="bold">fully connected NN</strong> (<strong class="bold">FCNN</strong>) that <a id="_idIndexMarker291"/>operates independently on each position:</p>
<pre class="source-code">
class FeedForward(nn.Module):
    def __init__(self, d_model, d_ff):
        super(FeedForward, self).__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.dropout = nn.Dropout(0.1)
        self.linear2 = nn.Linear(d_ff, d_model)
    def forward(self, x):
        return self.linear2(self.dropout(torch.relu(self.linear1(x))))</pre>
<h2 id="_idParaDest-65"><a id="_idTextAnchor110"/>Encoder layer</h2>
<p>The encoder layer<a id="_idIndexMarker292"/> consists of an MHSA mechanism and a simple FFNN. This structure is repeated in a stack to form the complete encoder:</p>
<pre class="source-code">
class EncoderLayer(nn.Module):
    def __init__(self, d_model, nhead, d_ff):
        super(EncoderLayer, self).__init__()
        self.self_attn = MultiHeadSelfAttention(d_model, nhead)
        self.feed_forward = FeedForward(d_model, d_ff)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(0.1)
    def forward(self, x):
        x = x.transpose(0, 1)
        attn_output, _ = self.self_attn(x)
        x = x + self.dropout(attn_output)
        x = self.norm1(x)
        ff_output = self.feed_forward(x)
        x = x + self.dropout(ff_output)
        return self.norm2(x).transpose(0, 1)</pre>
<h2 id="_idParaDest-66"><a id="_idTextAnchor111"/>Encoder</h2>
<p>The encoder is a stack<a id="_idIndexMarker293"/> of identical layers with an MHSA mechanism and an FFN:</p>
<pre class="source-code">
class Encoder(nn.Module):
    def __init__(self, d_model, nhead, d_ff, num_layers, vocab_size):
        super(Encoder, self).__init__()
        self.embedding = Embeddings(d_model, vocab_size)
        self.pos_encoding = PositionalEncoding(d_model)
        self.encoder_layers = nn.ModuleList(
            [EncoderLayer(d_model, nhead, d_ff) for _ in range(
                num_layers)]
        )
        self.feed_forward = FeedForward(d_model, d_ff)
    def forward(self, x):
        x = self.embedding(x)
        x = self.pos_encoding(x)
        for layer in self.encoder_layers:
            x = layer(x)
        return x</pre>
<h2 id="_idParaDest-67"><a id="_idTextAnchor112"/>Decoder layer</h2>
<p>Similarly, the <a id="_idIndexMarker294"/>decoder layer consists of two MHA mechanisms—one self-attention and one cross-attention—followed by an FFN:</p>
<pre class="source-code">
class DecoderLayer(nn.Module):
    def __init__(self, d_model, nhead, d_ff):
        super(DecoderLayer, self).__init__()
        self.self_attn = MultiHeadSelfAttention(d_model, nhead)
        self.cross_attn = nn.MultiheadAttention(d_model, nhead)
        self.feed_forward = FeedForward(d_model, d_ff)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(0.1)
    def forward(self, x, memory):
        x = x.transpose(0, 1)
        memory = memory.transpose(0, 1)
        attn_output, _ = self.self_attn(x)
        x = x + self.dropout(attn_output)
        x = self.norm1(x)
        attn_output, _ = self.cross_attn(x, memory, memory)
        x = x + self.dropout(attn_output)
        x = self.norm2(x)
        ff_output = self.feed_forward(x)
        x = x + self.dropout(ff_output)
        return self.norm3(x).transpose(0, 1)</pre>
<h2 id="_idParaDest-68"><a id="_idTextAnchor113"/>Decoder</h2>
<p>The decoder is also<a id="_idIndexMarker295"/> a stack of identical layers. Each layer contains two MHA mechanisms and an FFN:</p>
<pre class="source-code">
class Decoder(nn.Module):
    def __init__(self, d_model, nhead, d_ff, num_layers, vocab_size):
        super(Decoder, self).__init__()
        self.embedding = Embeddings(d_model, vocab_size)
        self.pos_encoding = PositionalEncoding(d_model)
        self.decoder_layers = nn.ModuleList(
            [DecoderLayer(d_model, nhead, d_ff) for _ in range(
                num_layers)]
        )
        self.linear = nn.Linear(d_model, vocab_size)
        self.softmax = nn.Softmax(dim=2)
    def forward(self, x, memory):
        x = self.embedding(x)
        x = self.pos_encoding(x)
        for layer in self.decoder_layers:
            x = layer(x, memory)
        x = self.linear(x)
        return self.softmax(x)</pre>
<p>This stacking layer pattern continues to build the transformer architecture. Each block has a specific<a id="_idIndexMarker296"/> role in processing the input data and generating output translations.</p>
<h2 id="_idParaDest-69"><a id="_idTextAnchor114"/>Complete transformer</h2>
<p>The transformer <a id="_idIndexMarker297"/>model encapsulates the previously defined encoder and decoder structures. This is the primary class that will be used for training and translation tasks:</p>
<pre class="source-code">
class Transformer(nn.Module):
    def __init__(
        self,
        d_model,
        nhead,
        d_ff,
        num_encoder_layers,
        num_decoder_layers,
        src_vocab_size,
        tgt_vocab_size,
    ):
        super(Transformer, self).__init__()
        self.encoder = Encoder(d_model, nhead, d_ff, \
            num_encoder_layers, src_vocab_size)
        self.decoder = Decoder(d_model, nhead, d_ff, \
            num_decoder_layers, tgt_vocab_size)
    def forward(self, src, tgt):
        memory = self.encoder(src)
        output = self.decoder(tgt, memory)
        return output</pre>
<h2 id="_idParaDest-70"><a id="_idTextAnchor115"/>Training function</h2>
<p>The <code>train</code> function <a id="_idIndexMarker298"/>iterates through the <a id="_idIndexMarker299"/>epochs and batches, calculates the loss, and updates the model parameters:</p>
<pre class="source-code">
def train(model, loss_fn, optimizer, NUM_EPOCHS=10):
    for epoch in range(NUM_EPOCHS):
        model.train()
        total_loss = 0
        for batch in batch_iterator:
            src, tgt = batch
            optimizer.zero_grad()
            output = model(src, tgt)
            loss = loss_fn(output.view(-1, TGT_VOCAB_SIZE),
                tgt.view(-1))
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"Epoch {epoch}, 
            Loss {total_loss / len(batch_iterator)}")</pre>
<h2 id="_idParaDest-71"><a id="_idTextAnchor116"/>Translation function</h2>
<p>The <code>translate</code> function <a id="_idIndexMarker300"/>uses the trained <a id="_idIndexMarker301"/>model to translate a source text into the target language. It generates a translation token by token and stops when an <strong class="bold">end-of-sequence</strong> (<strong class="bold">EOS</strong>) token <a id="_idIndexMarker302"/>is generated or when the maximum target length is reached:</p>
<pre class="source-code">
def translate(model, src_text, src_tokenizer,
              tgt_tokenizer, max_target_length=50
):
    model.eval()
    src_tokens = src_tokenizer.encode(src_text).ids
    src_tensor = torch.LongTensor(src_tokens).unsqueeze(0)
    tgt_sos_idx = tgt_tokenizer.token_to_id("&lt;sos&gt;")
    tgt_eos_idx = tgt_tokenizer.token_to_id("&lt;eos&gt;")
    tgt_tensor = torch.LongTensor([tgt_sos_idx]).unsqueeze(0)
    for i in range(max_target_length):
        with torch.no_grad():
            output = model(src_tensor, tgt_tensor)
        predicted_token_idx = output.argmax(dim=2)[0, -1].item()
        if predicted_token_idx == tgt_eos_idx:
            break
        tgt_tensor = torch.cat((tgt_tensor,
            torch.LongTensor([[predicted_token_idx]])),
            dim=1)
    translated_token_ids = tgt_tensor[0, 1:].tolist()
    translated_text = tgt_tokenizer.decode(translated_token_ids)
    return translated_text</pre>
<h2 id="_idParaDest-72"><a id="_idTextAnchor117"/>Main execution</h2>
<p>In the main block<a id="_idIndexMarker303"/> of the script, hyperparameters are defined, the tokenizer and model are instantiated, and training and translation processes are initiated:</p>
<pre class="source-code">
if __name__ == "__main__":
    NUM_ENCODER_LAYERS = 2
    NUM_DECODER_LAYERS = 2
    DROPOUT_RATE = 0.1
    EMBEDDING_DIM = 512
    NHEAD = 8
    FFN_HID_DIM = 2048
    BATCH_SIZE = 31
    LEARNING_RATE = 0.001
    en_tokenizer = train_tokenizer(EN_TEXT)
    fr_tokenizer = train_tokenizer(FR_TEXT)
    SRC_VOCAB_SIZE = len(en_tokenizer.get_vocab())
    TGT_VOCAB_SIZE = len(fr_tokenizer.get_vocab())
    src_tensor = tensorize_data(EN_TEXT, en_tokenizer)
    tgt_tensor = tensorize_data(FR_TEXT, fr_tokenizer)
    dataset = TextDataset(src_tensor, tgt_tensor)
    model = Transformer(
        EMBEDDING_DIM,
        NHEAD,
        FFN_HID_DIM,
        NUM_ENCODER_LAYERS,
        NUM_DECODER_LAYERS,
        SRC_VOCAB_SIZE,
        TGT_VOCAB_SIZE,
    )
    loss_fn = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    batch_iterator = DataLoader(
        dataset, batch_size=BATCH_SIZE,
        shuffle=True, drop_last=True
    )
    train(model, loss_fn, optimizer, NUM_EPOCHS=10)
    src_text = "hello, how are you?"
    translated_text = translate(
        model, src_text, en_tokenizer, fr_tokenizer)
    print(translated_text)</pre>
<p>This script orchestrates a machine translation task from loading data to training a transformer model and eventually translating text from English to French. Initially, it loads a dataset, processes the text, and establishes tokenizers to convert text to numerical data. Following<a id="_idIndexMarker304"/> this, it defines the architecture of a transformer model in <code>PyTorch</code>, detailing each component from the embeddings’ self-attention mechanisms to the encoder and decoder stacks.</p>
<p>The script further organizes the data into batches, sets up a training loop, and defines a translation function. Training the model on the provided English and French sentences teaches it to map sequences from one language to another. Finally, it translates a sample sentence <a id="_idIndexMarker305"/>from English to French to demonstrate the model’s capabilities.</p>
<h1 id="_idParaDest-73"><a id="_idTextAnchor118"/>Summary</h1>
<p>The advent of the transformer significantly propelled the field of NLP forward, serving as the foundation for today’s cutting-edge generative language models. This chapter delineated the progression of NLP that paved the way for this pivotal innovation. Initial statistical techniques such as count vectors and TF-IDF were adept at extracting rudimentary word patterns, yet they fell short in grasping semantic nuances.</p>
<p>Incorporating neural language models marked a stride toward more profound representations through word embeddings. Nevertheless, recurrent networks encountered hurdles in handling longer sequences. This inspired the emergence of CNNs, which introduced computational efficacy via parallelism, albeit at the expense of global contextual awareness.</p>
<p>The inception of attention mechanisms emerged as a cornerstone. In 2017, Vaswani et al. augmented these advancements, unveiling the transformer architecture. The hallmark self-attention mechanism of the transformer facilitates contextual modeling across extensive sequences in a parallelized manner. The layered encoder-decoder structure meticulously refines representations to discern relationships indispensable for endeavors such as translation.</p>
<p>The transformer, with its parallelizable and scalable self-attention design, set new benchmarks in performance. Its core tenets are the architectural bedrock for contemporary high-achieving gen<a id="_idTextAnchor119"/><a id="_idTextAnchor120"/>erative language models such as GPT.</p>
<p>In the next chapter, we will discuss how to apply pre-trained generative models from prototype to production.</p>
<h1 id="_idParaDest-74"><a id="_idTextAnchor121"/>References</h1>
<p>This reference section serves as a repository of sources referenced within this book; you can explore these resources to further enhance your understanding and knowledge of the subject matter:</p>
<ul>
<li>Bahdanau, D., Cho, K., and Bengio, Y. (2014). <em class="italic">Neural machine translation by jointly learning to align and translate</em>. arXiv preprint arXiv:1409.0473.</li>
<li>Bengio, Y., Ducharme, R., and Vincent, P. (2003). <em class="italic">A neural probabilistic language model. The Journal of Machine Learning Research</em>, 3, 1137-1155.</li>
<li>Dadgar, S. M. H., Araghi, M. S., and Farahani, M. M. (2016). <em class="italic">Improving text classification performance based on TFIDF and LSI index</em>. 2016 IEEE International Conference on Engineering &amp; Technology (ICETECH).</li>
<li>Elman, J. L. (1990). <em class="italic">Finding structure in time. Cognitive science</em>, 14(2), 179-211.</li>
<li>Hochreiter, S., and Schmidhuber, J. (1997). <em class="italic">Long short-term memory. Neural computation</em>, 9(8), 1735-1780.</li>
<li>Kim, Y. (2014). <em class="italic">Convolutional neural networks for sentence classification</em>. arXiv preprint arXiv:1408.5882.</li>
<li>Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and Dean, J. (2013). <em class="italic">Distributed representations of words and phrases and their compositionality. Advances in neural information processing </em><em class="italic">systems</em>, 26.</li>
<li>Pennington, J., Socher, R., and Manning, C. (2014). <em class="italic">GloVe: Global vectors for word representation. Proceedings of the 2014 conference on empirical methods in natural language processing (</em><em class="italic">EMNLP)</em>, 1532-1543.</li>
<li>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... and Polosukhin, I. (2017). <em class="italic">Attention is all you need. Advances in neural information processing </em><em class="italic">systems</em>, 30.</li>
</ul>
</div>
</body></html>