<html><head></head><body>
<div id="_idContainer023">
<p><a id="_idTextAnchor080"/></p>
<h1 class="chapter-number" id="_idParaDest-45"><a id="_idTextAnchor081"/><span class="koboSpan" id="kobo.1.1">3</span></h1>
<h1 id="_idParaDest-46"><a id="_idTextAnchor082"/><span class="koboSpan" id="kobo.2.1">Tracing the Foundations of Natural Language Processing and the Impact of the Transformer</span></h1>
<p><span class="koboSpan" id="kobo.3.1">The transformer architecture is a key advancement that underpins most modern generative language models. </span><span class="koboSpan" id="kobo.3.2">Since its introduction in 2017, it has become a fundamental part </span><a id="_idIndexMarker156"/><span class="koboSpan" id="kobo.4.1">of </span><strong class="bold"><span class="koboSpan" id="kobo.5.1">natural language processing</span></strong><span class="koboSpan" id="kobo.6.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.7.1">NLP</span></strong><span class="koboSpan" id="kobo.8.1">), enabling models such as </span><strong class="bold"><span class="koboSpan" id="kobo.9.1">Generative Pre-trained Transformer 4</span></strong><span class="koboSpan" id="kobo.10.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.11.1">GPT-4</span></strong><span class="koboSpan" id="kobo.12.1">) and Claude</span><a id="_idIndexMarker157"/><span class="koboSpan" id="kobo.13.1"> to advance text generation capabilities significantly. </span><span class="koboSpan" id="kobo.13.2">A deep understanding of the transformer architecture is crucial for grasping the mechanics </span><a id="_idIndexMarker158"/><span class="koboSpan" id="kobo.14.1">of modern </span><strong class="bold"><span class="koboSpan" id="kobo.15.1">large language </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.16.1">models</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.17.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.18.1">LLMs</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.19.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.20.1">In the previous chapter, we explored generative modeling techniques, including </span><strong class="bold"><span class="koboSpan" id="kobo.21.1">generative adversarial networks</span></strong><span class="koboSpan" id="kobo.22.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.23.1">GANs</span></strong><span class="koboSpan" id="kobo.24.1">), diffusion</span><a id="_idIndexMarker159"/><span class="koboSpan" id="kobo.25.1"> models, and </span><strong class="bold"><span class="koboSpan" id="kobo.26.1">autoregressive</span></strong><span class="koboSpan" id="kobo.27.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.28.1">AR</span></strong><span class="koboSpan" id="kobo.29.1">) transformers. </span><span class="koboSpan" id="kobo.29.2">We discussed how Transformers </span><a id="_idIndexMarker160"/><span class="koboSpan" id="kobo.30.1">can be leveraged to generate images from text. </span><span class="koboSpan" id="kobo.30.2">However, transformers are more than just one generative approach among many; they form the basis for nearly all state-of-the-art generative </span><span class="No-Break"><span class="koboSpan" id="kobo.31.1">language models.</span></span></p>
<p><span class="koboSpan" id="kobo.32.1">In this chapter, we’ll cover the evolution of NLP that ultimately led to the advent of the transformer architecture. </span><span class="koboSpan" id="kobo.32.2">We cannot cover all the critical steps forward, but we will attempt to cover major milestones, starting with early linguistic analysis techniques and statistical language </span><a id="_idIndexMarker161"/><span class="koboSpan" id="kobo.33.1">modeling, followed by advancements in </span><strong class="bold"><span class="koboSpan" id="kobo.34.1">recurrent neural networks</span></strong><span class="koboSpan" id="kobo.35.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.36.1">RNNs</span></strong><span class="koboSpan" id="kobo.37.1">) and </span><strong class="bold"><span class="koboSpan" id="kobo.38.1">convolutional neural networks</span></strong><span class="koboSpan" id="kobo.39.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.40.1">CNNs</span></strong><span class="koboSpan" id="kobo.41.1">) that </span><a id="_idIndexMarker162"/><span class="koboSpan" id="kobo.42.1">highlight the potential of </span><strong class="bold"><span class="koboSpan" id="kobo.43.1">deep learning</span></strong><span class="koboSpan" id="kobo.44.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.45.1">DL</span></strong><span class="koboSpan" id="kobo.46.1">) for NLP. </span><span class="koboSpan" id="kobo.46.2">Our main objective will be to introduce the transformer—its </span><a id="_idIndexMarker163"/><span class="koboSpan" id="kobo.47.1">basis in DL, its self-attention architecture, and its rapid evolution, which has led to LLMs and this phenomenon </span><a id="_idIndexMarker164"/><span class="koboSpan" id="kobo.48.1">we call </span><strong class="bold"><span class="koboSpan" id="kobo.49.1">generative </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.50.1">AI</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.51.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.52.1">GenAI</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.53.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.54.1">Understanding the origins and mechanics of the transformer architecture is important for recognizing its groundbreaking impact. </span><span class="koboSpan" id="kobo.54.2">The principles and modeling capabilities introduced by transformers are carried forward by all modern language models built upon this framework. </span><span class="koboSpan" id="kobo.54.3">We will build our intuition for Transformers through historical context and hands-on implementation, as this foundational understanding is key to understanding the future </span><span class="No-Break"><span class="koboSpan" id="kobo.55.1">of GenAI.</span></span></p>
<h1 id="_idParaDest-47"><span class="koboSpan" id="kobo.56.1">Early appro</span><a id="_idTextAnchor083"/><span class="koboSpan" id="kobo.57.1">aches in NLP</span></h1>
<p><span class="koboSpan" id="kobo.58.1">Before the </span><a id="_idIndexMarker165"/><span class="koboSpan" id="kobo.59.1">widespread </span><a id="_idIndexMarker166"/><span class="koboSpan" id="kobo.60.1">use of </span><strong class="bold"><span class="koboSpan" id="kobo.61.1">neural networks</span></strong><span class="koboSpan" id="kobo.62.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.63.1">NNs</span></strong><span class="koboSpan" id="kobo.64.1">) in language processing, NLP was largely grounded in methods that counted words. </span><span class="koboSpan" id="kobo.64.2">Two particularly notable techniques</span><a id="_idIndexMarker167"/><span class="koboSpan" id="kobo.65.1"> were </span><strong class="bold"><span class="koboSpan" id="kobo.66.1">count vectors</span></strong><span class="koboSpan" id="kobo.67.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.68.1">Term Frequency-Inverse Document Frequency</span></strong><span class="koboSpan" id="kobo.69.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.70.1">TF-IDF</span></strong><span class="koboSpan" id="kobo.71.1">). </span><span class="koboSpan" id="kobo.71.2">In essence, count vectors tallied up how </span><a id="_idIndexMarker168"/><span class="koboSpan" id="kobo.72.1">often each word appeared in a document. </span><span class="koboSpan" id="kobo.72.2">Building on this, Dadgar et al. </span><span class="koboSpan" id="kobo.72.3">applied the TF-IDF algorithm (historically used for information retrieval) to text classification in 2016. </span><span class="koboSpan" id="kobo.72.4">This method assigned weights to words based on their significance in one document relative to their occurrence across a collection of documents. </span><span class="koboSpan" id="kobo.72.5">These count-based methods were successful for tasks such as searching and categorizing. </span><span class="koboSpan" id="kobo.72.6">However, they presented a key limitation in that they could not capture the semantic relationships between words, meaning they could not interpret the nuanced meanings of words in context. </span><span class="koboSpan" id="kobo.72.7">This challenge paved the way for exploring NNs, offering a deeper and more nuanced way to understand and </span><span class="No-Break"><span class="koboSpan" id="kobo.73.1">rep</span><a id="_idTextAnchor084"/><span class="koboSpan" id="kobo.74.1">resent text.</span></span></p>
<h2 id="_idParaDest-48"><a id="_idTextAnchor085"/><span class="koboSpan" id="kobo.75.1">Advent of neural language models</span></h2>
<p><span class="koboSpan" id="kobo.76.1">In 2003, Yoshua </span><a id="_idIndexMarker169"/><span class="koboSpan" id="kobo.77.1">Bengio’s team</span><a id="_idIndexMarker170"/><span class="koboSpan" id="kobo.78.1"> at the University of Montreal introduced the </span><strong class="bold"><span class="koboSpan" id="kobo.79.1">Neural Network Language Model</span></strong><span class="koboSpan" id="kobo.80.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.81.1">NNLM</span></strong><span class="koboSpan" id="kobo.82.1">), a </span><a id="_idIndexMarker171"/><span class="koboSpan" id="kobo.83.1">novel approach to language technology. </span><span class="koboSpan" id="kobo.83.2">The NNLM was designed to predict the next word in a sequence based on prior words using a particular type of </span><strong class="bold"><span class="koboSpan" id="kobo.84.1">neural network</span></strong><span class="koboSpan" id="kobo.85.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.86.1">NN</span></strong><span class="koboSpan" id="kobo.87.1">). </span><span class="koboSpan" id="kobo.87.2">The</span><a id="_idIndexMarker172"/><span class="koboSpan" id="kobo.88.1"> design prominently featured hidden layers that learned word embeddings, which are compact vector representations capturing the core semantic meanings of words. </span><span class="koboSpan" id="kobo.88.2">This aspect was absent in count-based approaches. </span><span class="koboSpan" id="kobo.88.3">However, the NNLM was still limited in its ability to interpret longer sequences and handle large vocabularies. </span><span class="koboSpan" id="kobo.88.4">Despite these limitations, the NNLM sparked widespread exploration of NNs in </span><span class="No-Break"><span class="koboSpan" id="kobo.89.1">language modeling.</span></span></p>
<p><span class="koboSpan" id="kobo.90.1">The introduction of the NNLM highlighted the potential of NNs in language processing, particularly using word embeddings. </span><span class="koboSpan" id="kobo.90.2">Yet, its limitations with long sequences and large vocabulary</span><a id="_idIndexMarker173"/><span class="koboSpan" id="kobo.91.1"> signaled </span><a id="_idIndexMarker174"/><span class="koboSpan" id="kobo.92.1">the need for</span><a id="_idTextAnchor086"/> <span class="No-Break"><span class="koboSpan" id="kobo.93.1">further research.</span></span></p>
<h2 id="_idParaDest-49"><a id="_idTextAnchor087"/><span class="koboSpan" id="kobo.94.1">Distributed representations</span></h2>
<p><span class="koboSpan" id="kobo.95.1">Following the inception</span><a id="_idIndexMarker175"/><span class="koboSpan" id="kobo.96.1"> of the</span><a id="_idIndexMarker176"/><span class="koboSpan" id="kobo.97.1"> NNLM, NLP research was propelled toward crafting high-quality word vector representations. </span><span class="koboSpan" id="kobo.97.2">These representations could be initially learned from extensive sets of unlabeled text data and later applied to downstream models for various tasks. </span><span class="koboSpan" id="kobo.97.3">The period saw the emergence of two prominent </span><a id="_idIndexMarker177"/><span class="koboSpan" id="kobo.98.1">methods: </span><strong class="bold"><span class="koboSpan" id="kobo.99.1">Word2Vec </span></strong><span class="koboSpan" id="kobo.100.1">(introduced by Mikolov et al., 2013) and </span><strong class="bold"><span class="koboSpan" id="kobo.101.1">Global Vectors</span></strong><span class="koboSpan" id="kobo.102.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.103.1">GloVe</span></strong><span class="koboSpan" id="kobo.104.1">, introduced</span><a id="_idIndexMarker178"/><span class="koboSpan" id="kobo.105.1"> by Pennington et al., 2014). </span><span class="koboSpan" id="kobo.105.2">These methods applied </span><strong class="bold"><span class="koboSpan" id="kobo.106.1">distributed representation</span></strong><span class="koboSpan" id="kobo.107.1"> to craft high-quality word vector representations.  </span><span class="koboSpan" id="kobo.107.2">Distributed representation portrays items such as words not as unique identifiers but as sets of continuous values or vectors. </span><span class="koboSpan" id="kobo.107.3">In these vectors, each value corresponds to a specific feature or characteristic of the item. </span><span class="koboSpan" id="kobo.107.4">Unlike traditional representations, where each item has a unique symbol, distributed representations allow these items to share features with others, enabling a more intelligent capture of underlying patterns in </span><span class="No-Break"><span class="koboSpan" id="kobo.108.1">the data.</span></span></p>
<p><span class="koboSpan" id="kobo.109.1">Let us elucidate this concept a bit further. </span><span class="koboSpan" id="kobo.109.2">Suppose we represent words based on two features: </span><strong class="bold"><span class="koboSpan" id="kobo.110.1">Formality</span></strong><span class="koboSpan" id="kobo.111.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.112.1">Positivity</span></strong><span class="koboSpan" id="kobo.113.1">. </span><span class="koboSpan" id="kobo.113.2">We might have vectors such as </span><span class="No-Break"><span class="koboSpan" id="kobo.114.1">the following:</span></span></p>
<p class="Basic-Paragraph"><strong class="source-inline"><span class="koboSpan" id="kobo.115.1">Formal: [1, 0]</span></strong></p>
<p class="Basic-Paragraph"><strong class="source-inline"><span class="koboSpan" id="kobo.116.1">Happy: [0, 1]</span></strong></p>
<p class="Basic-Paragraph"><strong class="source-inline"><span class="koboSpan" id="kobo.117.1">Cheerful: [0, 1]</span></strong></p>
<p><span class="koboSpan" id="kobo.118.1">In this example, each element in the vector corresponds to one of these features. </span><span class="koboSpan" id="kobo.118.2">In the vector for </span><strong class="source-inline"><span class="koboSpan" id="kobo.119.1">Formal</span></strong><span class="koboSpan" id="kobo.120.1">, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.121.1">1</span></strong><span class="koboSpan" id="kobo.122.1"> element under </span><em class="italic"><span class="koboSpan" id="kobo.123.1">Formality</span></em><span class="koboSpan" id="kobo.124.1"> indicates that the word is formal, while the </span><strong class="source-inline"><span class="koboSpan" id="kobo.125.1">0</span></strong><span class="koboSpan" id="kobo.126.1"> element under </span><em class="italic"><span class="koboSpan" id="kobo.127.1">Positivity</span></em><span class="koboSpan" id="kobo.128.1"> indicates neutrality in terms of positivity. </span><span class="koboSpan" id="kobo.128.2">Similarly, for </span><strong class="source-inline"><span class="koboSpan" id="kobo.129.1">Happy</span></strong><span class="koboSpan" id="kobo.130.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.131.1">Cheerful</span></strong><span class="koboSpan" id="kobo.132.1">, the 1 element under </span><em class="italic"><span class="koboSpan" id="kobo.133.1">Positivity</span></em><span class="koboSpan" id="kobo.134.1"> indicates that these words have a positive connotation. </span><span class="koboSpan" id="kobo.134.2">This way, distributed representation captures the essence of words through vectors, allowing for shared features among different words to understand underlying patterns </span><span class="No-Break"><span class="koboSpan" id="kobo.135.1">in data.</span></span></p>
<p><span class="koboSpan" id="kobo.136.1">Word2Vec employs a relatively straightforward approach where NNs are used to predict the surrounding words for each target word in a dataset. </span><span class="koboSpan" id="kobo.136.2">Through this process, the NN ascertains values or “weights” for each target word. </span><span class="koboSpan" id="kobo.136.3">These weights form a vector for each word</span><a id="_idIndexMarker179"/><span class="koboSpan" id="kobo.137.1"> in a </span><strong class="bold"><span class="koboSpan" id="kobo.138.1">continuous vector space</span></strong><span class="koboSpan" id="kobo.139.1">—a mathematical space wherein each point represents a possible value a vector can take. </span><span class="koboSpan" id="kobo.139.2">In the context of NLP, each dimension of this space corresponds to a feature, and the position of a word in this space captures its semantic or linguistic relationships to </span><span class="No-Break"><span class="koboSpan" id="kobo.140.1">other words.</span></span></p>
<p><span class="koboSpan" id="kobo.141.1">These vectors </span><a id="_idIndexMarker180"/><span class="koboSpan" id="kobo.142.1">form a </span><strong class="bold"><span class="koboSpan" id="kobo.143.1">feature-based representation</span></strong><span class="koboSpan" id="kobo.144.1">—a type of representation where each dimension represents a different feature that contributes to the word’s meaning. </span><span class="koboSpan" id="kobo.144.2">Unlike </span><a id="_idIndexMarker181"/><span class="koboSpan" id="kobo.145.1">a symbolic</span><a id="_idIndexMarker182"/><span class="koboSpan" id="kobo.146.1"> representation, where each word is represented as a unique symbol, a feature-based representation captures the semantic essence of words in terms of </span><span class="No-Break"><span class="koboSpan" id="kobo.147.1">shared features.</span></span></p>
<p><span class="koboSpan" id="kobo.148.1">On the other hand, GloVe adopts a different approach. </span><span class="koboSpan" id="kobo.148.2">It analyzes the </span><strong class="bold"><span class="koboSpan" id="kobo.149.1">global co-occurrence statistics</span></strong><span class="koboSpan" id="kobo.150.1">—a count</span><a id="_idIndexMarker183"/><span class="koboSpan" id="kobo.151.1"> of how often words appear together in a large text corpus. </span><span class="koboSpan" id="kobo.151.2">GloVe learns vector representations that capture the relationships between words by analyzing these counts across the entire corpus. </span><span class="koboSpan" id="kobo.151.3">This method also results in a distributed representation of words in a continuous vector space, capturing </span><strong class="bold"><span class="koboSpan" id="kobo.152.1">semantic similarity</span></strong><span class="koboSpan" id="kobo.153.1">—a </span><a id="_idIndexMarker184"/><span class="koboSpan" id="kobo.154.1">measure of the degree to which two words are similar in meaning. </span><span class="koboSpan" id="kobo.154.2">In a continuous vector space, we can think about semantic similarity as the simple geometric proximity of vectors </span><span class="No-Break"><span class="koboSpan" id="kobo.155.1">representing words.</span></span></p>
<p><span class="koboSpan" id="kobo.156.1">To further illustrate, suppose we have a tiny corpus of text containing the </span><span class="No-Break"><span class="koboSpan" id="kobo.157.1">following sentences:</span></span></p>
<p><span class="koboSpan" id="kobo.158.1">“Coffee </span><span class="No-Break"><span class="koboSpan" id="kobo.159.1">is hot.”</span></span></p>
<p><span class="koboSpan" id="kobo.160.1">“Ice cream </span><span class="No-Break"><span class="koboSpan" id="kobo.161.1">is cold.”</span></span></p>
<p><span class="koboSpan" id="kobo.162.1">From this corpus, GloVe would notice that “</span><em class="italic"><span class="koboSpan" id="kobo.163.1">coffee</span></em><span class="koboSpan" id="kobo.164.1">” co-occurs with “</span><em class="italic"><span class="koboSpan" id="kobo.165.1">hot</span></em><span class="koboSpan" id="kobo.166.1">” and “</span><em class="italic"><span class="koboSpan" id="kobo.167.1">ice</span></em> <em class="italic"><span class="koboSpan" id="kobo.168.1">cream</span></em><span class="koboSpan" id="kobo.169.1">” co-occurs with “</span><em class="italic"><span class="koboSpan" id="kobo.170.1">cold</span></em><span class="koboSpan" id="kobo.171.1">.” </span><span class="koboSpan" id="kobo.171.2">Through its optimization process, it would aim to create vectors for these words in a way that reflects these relationships. </span><span class="koboSpan" id="kobo.171.3">In this oversimplified example, GloVe might produce a vector such </span><span class="No-Break"><span class="koboSpan" id="kobo.172.1">as this:</span></span></p>
<p class="Basic-Paragraph"><strong class="source-inline"><span class="koboSpan" id="kobo.173.1">Coffee: [1, 0]</span></strong></p>
<p class="Basic-Paragraph"><strong class="source-inline"><span class="koboSpan" id="kobo.174.1">Hot: [0.9, 0]</span></strong></p>
<p class="Basic-Paragraph"><strong class="source-inline"><span class="koboSpan" id="kobo.175.1">Ice Cream: [0, 1]</span></strong></p>
<p class="Basic-Paragraph"><strong class="source-inline"><span class="koboSpan" id="kobo.176.1">Cold: [0, 0.9]</span></strong></p>
<p><span class="koboSpan" id="kobo.177.1">Here, the closeness of the vectors for “</span><em class="italic"><span class="koboSpan" id="kobo.178.1">coffee</span></em><span class="koboSpan" id="kobo.179.1">” and “</span><em class="italic"><span class="koboSpan" id="kobo.180.1">hot</span></em><span class="koboSpan" id="kobo.181.1">” (and, similarly, “</span><em class="italic"><span class="koboSpan" id="kobo.182.1">ice</span></em> <em class="italic"><span class="koboSpan" id="kobo.183.1">cream</span></em><span class="koboSpan" id="kobo.184.1">” and “</span><em class="italic"><span class="koboSpan" id="kobo.185.1">cold</span></em><span class="koboSpan" id="kobo.186.1">”) in this space reflects the co-occurrence relationships observed in the corpus. </span><span class="koboSpan" id="kobo.186.2">The vector difference between “</span><em class="italic"><span class="koboSpan" id="kobo.187.1">coffee</span></em><span class="koboSpan" id="kobo.188.1">” and “</span><em class="italic"><span class="koboSpan" id="kobo.189.1">hot</span></em><span class="koboSpan" id="kobo.190.1">” might resemble the vector difference between “</span><em class="italic"><span class="koboSpan" id="kobo.191.1">ice</span></em> <em class="italic"><span class="koboSpan" id="kobo.192.1">cream</span></em><span class="koboSpan" id="kobo.193.1">” and “</span><em class="italic"><span class="koboSpan" id="kobo.194.1">cold</span></em><span class="koboSpan" id="kobo.195.1">,” capturing the contrasting temperature relationships in a geometric way within the </span><span class="No-Break"><span class="koboSpan" id="kobo.196.1">vector space.</span></span></p>
<p><span class="koboSpan" id="kobo.197.1">Both Word2Vec and GloVe excel at encapsulating relevant semantic information about words to represent an efficient </span><strong class="bold"><span class="koboSpan" id="kobo.198.1">encoding</span></strong><span class="koboSpan" id="kobo.199.1">—a compact way of representing information that captures the essential features necessary for a task while reducing the dimensionality and complexity of </span><span class="No-Break"><span class="koboSpan" id="kobo.200.1">the data.</span></span></p>
<p><span class="koboSpan" id="kobo.201.1">These methodologies in creating meaningful vector representations served as a step toward the</span><a id="_idIndexMarker185"/><span class="koboSpan" id="kobo.202.1"> adoption </span><a id="_idIndexMarker186"/><span class="koboSpan" id="kobo.203.1">of </span><strong class="bold"><span class="koboSpan" id="kobo.204.1">transfer learning </span></strong><span class="koboSpan" id="kobo.205.1">in NLP. </span><span class="koboSpan" id="kobo.205.2">The vectors provide a shared semantic foundation that facilitates the transfer of learned relat</span><a id="_idTextAnchor088"/><span class="koboSpan" id="kobo.206.1">ionships across </span><span class="No-Break"><span class="koboSpan" id="kobo.207.1">varying tasks.</span></span></p>
<h2 id="_idParaDest-50"><a id="_idTextAnchor089"/><span class="koboSpan" id="kobo.208.1">Transfer Learning</span></h2>
<p><span class="koboSpan" id="kobo.209.1">GloVe and other</span><a id="_idIndexMarker187"/><span class="koboSpan" id="kobo.210.1"> methods of</span><a id="_idIndexMarker188"/><span class="koboSpan" id="kobo.211.1"> deriving distributed representations paved the way for transfer learning in NLP. </span><span class="koboSpan" id="kobo.211.2">By creating rich vector representations of words that encapsulate semantic relationships, these methods provided a foundational understanding of text. </span><span class="koboSpan" id="kobo.211.3">The vectors serve as a shared base of knowledge that can be applied to different tasks. </span><span class="koboSpan" id="kobo.211.4">When a model, initially trained on one task, is utilized for another, the pre-learned vector representations aid in preserving the semantic understanding, thereby reducing the data or training needed for the new task. </span><span class="koboSpan" id="kobo.211.5">This practice of transferring acquired knowledge has become fundamental for efficiently addressing a range of </span><span class="No-Break"><span class="koboSpan" id="kobo.212.1">NLP tasks.</span></span></p>
<p><span class="koboSpan" id="kobo.213.1">Consider a model trained to understand sentiments (positive or negative) in movie reviews. </span><span class="koboSpan" id="kobo.213.2">Through training, this model has learned distributed representations of words, capturing sentiment-related nuances. </span><span class="koboSpan" id="kobo.213.3">Now, suppose there is a new task: understanding sentiments in product reviews. </span><span class="koboSpan" id="kobo.213.4">Instead of training a new model from the beginning, transfer learning allows us to use the distributed representations from the movie review task to initiate training for the product review task. </span><span class="koboSpan" id="kobo.213.5">This could lead to quicker training and better performance, especially with limited data for the product </span><span class="No-Break"><span class="koboSpan" id="kobo.214.1">review task.</span></span></p>
<p><span class="koboSpan" id="kobo.215.1">The effectiveness of transfer learning, bolstered by distributed representations from methods such as GloVe, highlighted the potential of leveraging pre-existing knowledge for new tasks. </span><span class="koboSpan" id="kobo.215.2">It was a precursor to the integration of NNs in NLP, highlighting the benefits of utilizing learned representations</span><a id="_idIndexMarker189"/><span class="koboSpan" id="kobo.216.1"> across tasks. </span><span class="koboSpan" id="kobo.216.2">The advent of NNs in NLP brought about models capable of learning even richer representations, further amplifying the impact and scope of </span><span class="No-Break"><span class="koboSpan" id="kobo.217.1">transfer learning</span></span><span class="No-Break"><span class="koboSpan" id="kobo.218.1">.</span></span></p>
<h2 id="_idParaDest-51"><a id="_idTextAnchor090"/><span class="koboSpan" id="kobo.219.1">Advent of NNs in NLP</span></h2>
<p><span class="koboSpan" id="kobo.220.1">The advent of NNs in </span><a id="_idIndexMarker190"/><span class="koboSpan" id="kobo.221.1">NLP marked</span><a id="_idIndexMarker191"/><span class="koboSpan" id="kobo.222.1"> a monumental shift in the field’s capability to understand and process language. </span><span class="koboSpan" id="kobo.222.2">Building upon the groundwork laid by methodologies such as Word2Vec, GloVe, and the practice of transfer learning, NNs introduced a higher level of abstraction and learning capacity. </span><span class="koboSpan" id="kobo.222.3">Unlike previous methods that often relied on hand-crafted features, NNs could automatically learn intricate patterns and relationships from data. </span><span class="koboSpan" id="kobo.222.4">This ability to learn from data propelled NLP into a new era where models could achieve unprecedented levels of performance across a myriad of language-related tasks. </span><span class="koboSpan" id="kobo.222.5">The emergence of architectures such as CNNs and RNNs, followed by the revolutionary transformer architecture, showcased the remarkable versatility and efficacy of NNs in tackling complex NLP challenges. </span><span class="koboSpan" id="kobo.222.6">This transition not only accelerated the pace of innovation but also expanded the horizon of what could be achieved in </span><a id="_idIndexMarker192"/><span class="koboSpan" id="kobo.223.1">understan</span><a id="_idTextAnchor091"/><span class="koboSpan" id="kobo.224.1">ding human </span><span class="No-Break"><span class="koboSpan" id="kobo.225.1">language computationally.</span></span></p>
<h3><span class="koboSpan" id="kobo.226.1">Language modeling with RNNs</span></h3>
<p><span class="koboSpan" id="kobo.227.1">Despite how well</span><a id="_idIndexMarker193"/><span class="koboSpan" id="kobo.228.1"> these distributed word vectors excelled at </span><a id="_idIndexMarker194"/><span class="koboSpan" id="kobo.229.1">encoding local semantic relationships, modeling long-range dependencies would require a more sophisticated network architecture. </span><span class="koboSpan" id="kobo.229.2">This led to the use of RNNs. </span><span class="koboSpan" id="kobo.229.3">RNNs (originally introduced by Elman in 1990) are a type of NN architecture that processes data sequences by iterating through each element of the sequence while maintaining a dynamic internal state that captures information about the previous elements. </span><span class="koboSpan" id="kobo.229.4">Unlike</span><a id="_idIndexMarker195"/><span class="koboSpan" id="kobo.230.1"> traditional </span><strong class="bold"><span class="koboSpan" id="kobo.231.1">feedforward networks</span></strong><span class="koboSpan" id="kobo.232.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.233.1">FNNs</span></strong><span class="koboSpan" id="kobo.234.1">) that processed each input independently, RNNs introduced iterations that allowed information to be passed from one step in the sequence to the next, enabling them to capture temporal dependencies in data. </span><span class="koboSpan" id="kobo.234.2">The iterative processing and dynamic updating in NNs enable them to learn and represent relationships within the text. </span><span class="koboSpan" id="kobo.234.3">These networks can capture contextual connections and interdependencies across sentences or even </span><span class="No-Break"><span class="koboSpan" id="kobo.235.1">entire documents.</span></span></p>
<p><span class="koboSpan" id="kobo.236.1">However, standard RNNs had technical limitations when dealing with long sequences. </span><span class="koboSpan" id="kobo.236.2">This led to the development</span><a id="_idIndexMarker196"/><span class="koboSpan" id="kobo.237.1"> of </span><strong class="bold"><span class="koboSpan" id="kobo.238.1">long short-term memory</span></strong><span class="koboSpan" id="kobo.239.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.240.1">LSTM</span></strong><span class="koboSpan" id="kobo.241.1">) networks. </span><span class="koboSpan" id="kobo.241.2">LSTMs were first introduced by Hochreiter and Schmidhuber in 1997. </span><span class="koboSpan" id="kobo.241.3">They were a special class of RNNs designed to address the </span><strong class="bold"><span class="koboSpan" id="kobo.242.1">vanishing gradient</span></strong><span class="koboSpan" id="kobo.243.1"> problem, which</span><a id="_idIndexMarker197"/><span class="koboSpan" id="kobo.244.1"> is the challenge where the network cannot learn from earlier parts of a sequence as the sequence gets longer. </span><span class="koboSpan" id="kobo.244.2">LSTMs applied a unique gating architecture to control the flow of information within the network, enabling them to maintain and access information over long sequences without suffering from the vanishing </span><span class="No-Break"><span class="koboSpan" id="kobo.245.1">gradient problem.</span></span></p>
<p><span class="koboSpan" id="kobo.246.1">The name “</span><strong class="bold"><span class="koboSpan" id="kobo.247.1">long short-term memory</span></strong><span class="koboSpan" id="kobo.248.1">” refers</span><a id="_idIndexMarker198"/><span class="koboSpan" id="kobo.249.1"> to the network’s ability to keep track of information over both short and long sequences </span><span class="No-Break"><span class="koboSpan" id="kobo.250.1">of data:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.251.1">Short-term</span></strong><span class="koboSpan" id="kobo.252.1">: LSTMs can remember recent information, which is useful for understanding the current context. </span><span class="koboSpan" id="kobo.252.2">For example, in language modeling, knowing the last few words can be crucial for predicting the next word. </span><span class="koboSpan" id="kobo.252.3">Consider a phrase such as, “The cat, which already ate a lot, was not hungry.” </span><span class="koboSpan" id="kobo.252.4">As the LSTM processes the text, when it reaches the word “not,” the recent information that the cat “ate a lot” is crucial to predict the next word, “</span><span class="No-Break"><span class="koboSpan" id="kobo.253.1">hungry,” accurately.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.254.1">Long-term</span></strong><span class="koboSpan" id="kobo.255.1">: Unlike standard RNNs, LSTMs are also capable of retaining information from many steps back in the sequence, which is particularly useful for long-range dependencies, where a piece of information early in a sentence could be important for understanding a word much later in the sequence. </span><span class="koboSpan" id="kobo.255.2">In the same phrase, the information that “The cat” is the subject of the sentence is introduced early on. </span><span class="koboSpan" id="kobo.255.3">This </span><a id="_idIndexMarker199"/><span class="koboSpan" id="kobo.256.1">information is crucial later to understand who “was not hungry” as it processes the later part of </span><span class="No-Break"><span class="koboSpan" id="kobo.257.1">the sentence.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.258.1">The </span><strong class="bold"><span class="koboSpan" id="kobo.259.1">M</span></strong><span class="koboSpan" id="kobo.260.1"> or </span><strong class="bold"><span class="koboSpan" id="kobo.261.1">memory</span></strong><span class="koboSpan" id="kobo.262.1"> in LSTMs is maintained through a unique architecture that employs three gating mechanisms—input, output, and forget gates. </span><span class="koboSpan" id="kobo.262.2">These gates control the flow of information within the network, deciding what information should be kept, discarded, or used at each step in the sequence, enabling LSTMs to maintain and access information over long sequences. </span><span class="koboSpan" id="kobo.262.3">Effectively, these gates and the network state allowed LTSMs to carry the “memory” across time steps, ensuring that valuable information was retained throughout the processing of </span><span class="No-Break"><span class="koboSpan" id="kobo.263.1">the sequence.</span></span></p>
<p><span class="koboSpan" id="kobo.264.1">Ultimately, LSTMs obtained state-of-the-art results on many language modeling and text classification benchmarks. </span><span class="koboSpan" id="kobo.264.2">They became the dominant NN architecture for NLP tasks due to their ability to capture short- and long-range </span><span class="No-Break"><span class="koboSpan" id="kobo.265.1">contextual relationships.</span></span></p>
<p><span class="koboSpan" id="kobo.266.1">The success of LSTMs demonstrated the potential of neural architectures in capturing the complex </span><a id="_idIndexMarker200"/><span class="koboSpan" id="kobo.267.1">relationships inherent in language, significantly </span><a id="_idIndexMarker201"/><span class="koboSpan" id="kobo.268.1">advancing the field of NLP. </span><span class="koboSpan" id="kobo.268.2">However, the continuous pursuit of more efficient and effective models led the commu</span><a id="_idTextAnchor092"/><span class="koboSpan" id="kobo.269.1">nity toward exploring other </span><span class="No-Break"><span class="koboSpan" id="kobo.270.1">NN architectures.</span></span></p>
<h3><span class="koboSpan" id="kobo.271.1">Rise of CNNs</span></h3>
<p><span class="koboSpan" id="kobo.272.1">Around 2014, the NLP</span><a id="_idIndexMarker202"/><span class="koboSpan" id="kobo.273.1"> domain witnessed a rise in the popularity of CNN</span><a id="_idIndexMarker203"/><span class="koboSpan" id="kobo.274.1">s for tackling NLP tasks, a notable shift led by Yoon Kim. </span><span class="koboSpan" id="kobo.274.2">CNNs (originally brought forward by LeCun et al. </span><span class="koboSpan" id="kobo.274.3">for image recognition) operate based on convolutional layers that scan the input by moving a filter (or kernel) across the input data, at each position calculating the dot product of the filter’s weights and the input data. </span><span class="koboSpan" id="kobo.274.4">In NLP, these layers work over local n-gram windows (consecutive sequences of </span><em class="italic"><span class="koboSpan" id="kobo.275.1">n</span></em><span class="koboSpan" id="kobo.276.1"> words) to identify patterns or features, such as specific sequences of words or characters in the text. </span><span class="koboSpan" id="kobo.276.2">Employing convolutional layers over local n-gram windows, CNNs scan and analyze the data to detect initial patterns or features. </span><span class="koboSpan" id="kobo.276.3">Following this, pooling layers are used to reduce the dimensionality of the data, which helps in both reducing computational complexity and focusing on the most salient features identified by the </span><span class="No-Break"><span class="koboSpan" id="kobo.277.1">convolutional layers.</span></span></p>
<p><span class="koboSpan" id="kobo.278.1">Combining convolutional and pooling layers, CNNs can extract hierarchical features. </span><span class="koboSpan" id="kobo.278.2">These features represent information at different levels of abstraction by combining simpler, lower-level features to form more complex, higher-level features. </span><span class="koboSpan" id="kobo.278.3">In NLP, this process might start with detecting basic patterns such as common word pairs or phrases in the initial layers, progressing to recognizing more abstract concepts such as semantic relationships in</span><a id="_idIndexMarker204"/><span class="koboSpan" id="kobo.279.1"> the </span><span class="No-Break"><span class="koboSpan" id="kobo.280.1">higher layers.</span></span></p>
<p><span class="koboSpan" id="kobo.281.1">For comparison, we again consider a scenario where a CNN is employed to analyze and categorize customer reviews into positive, negative, or </span><span class="No-Break"><span class="koboSpan" id="kobo.282.1">neutral sentiments:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.283.1">Lower-level features (initial layers)</span></strong><span class="koboSpan" id="kobo.284.1">: The CNN </span><a id="_idIndexMarker205"/><span class="koboSpan" id="kobo.285.1">might identify basic patterns such as common word pairs or phrases in the initial layers. </span><span class="koboSpan" id="kobo.285.2">For instance, it might recognize phrases such as “great service,” “terrible experience,” or “</span><span class="No-Break"><span class="koboSpan" id="kobo.286.1">not happy.”</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.287.1">Intermediate-level features (middle layers)</span></strong><span class="koboSpan" id="kobo.288.1">: As data progresses through the network, middle layers might start recognizing more complex patterns, such as negations (“not good”) or contrasts (“good </span><span class="No-Break"><span class="koboSpan" id="kobo.289.1">but expensive”).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.290.1">Higher-level features (later layers)</span></strong><span class="koboSpan" id="kobo.291.1">: The CNN could identify abstract concepts such as overall sentiment in the later layers. </span><span class="koboSpan" id="kobo.291.2">For instance, it might deduce a positive sentiment from phrases such as “excellent service” or “loved the ambiance” and a negative sentiment from phrases such as “worst experience” or “</span><span class="No-Break"><span class="koboSpan" id="kobo.292.1">terrible food.”</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.293.1">In this way, CNNs inherently learn higher-level abstract representations of text. </span><span class="koboSpan" id="kobo.293.2">Although they lack the sequential processing characteristic of RNNs, they offer a computational advantage due to their inherent </span><strong class="bold"><span class="koboSpan" id="kobo.294.1">parallelism</span></strong><span class="koboSpan" id="kobo.295.1"> or ability to process multiple parts of the data simultaneously. </span><span class="koboSpan" id="kobo.295.2">Unlike RNNs, which process sequences iteratively and require the previous step to be completed before proceeding to the next, CNNs can process various parts of the input data in parallel, significantly speeding up </span><span class="No-Break"><span class="koboSpan" id="kobo.296.1">training times.</span></span></p>
<p><span class="koboSpan" id="kobo.297.1">CNNs, while efficient, have a limitation in their convolution operation, which only processes local data from smaller or nearby regions, thereby missing relationships across more significant portions of the entire input data, referred to as global information. </span><span class="koboSpan" id="kobo.297.2">This gave rise to attention-augmented convolutional networks that integrate self-attention with convolutions to address this limitation. </span><span class="koboSpan" id="kobo.297.3">Self-attention, initially used in sequence and generative modeling, was adapted for visual tasks such as image classification, enabling the network to process and capture relationships across the entire input data. </span><span class="koboSpan" id="kobo.297.4">However, attention augmentation, which combines convolutions and self-attention, yielded the best results. </span><span class="koboSpan" id="kobo.297.5">This method retained the computational efficiency of CNNs and captured global information, marking an advancement in image classification and object detection tasks. </span><span class="koboSpan" id="kobo.297.6">We will discuss self-attention in detail later as it became a critical component of </span><span class="No-Break"><span class="koboSpan" id="kobo.298.1">the transformer.</span></span></p>
<p><span class="koboSpan" id="kobo.299.1">The ability of CNNs to process multiple parts of data simultaneously marked a significant advancement in computational efficiency, paving the way for further innovations in NN architectures for</span><a id="_idIndexMarker206"/><span class="koboSpan" id="kobo.300.1"> NLP. </span><span class="koboSpan" id="kobo.300.2">As the field progressed, a pivotal shift occurred with the</span><a id="_idIndexMarker207"/><span class="koboSpan" id="kobo.301.1"> advent of attention-augmented NNs, introducing a</span><a id="_idTextAnchor093"/><span class="koboSpan" id="kobo.302.1"> new paradigm in how models handle </span><span class="No-Break"><span class="koboSpan" id="kobo.303.1">sequential data.</span></span></p>
<h1 id="_idParaDest-52"><a id="_idTextAnchor094"/><span class="koboSpan" id="kobo.304.1">The emergence of the Transformer in advanced language models</span></h1>
<p><span class="koboSpan" id="kobo.305.1">In 2017, inspired by the</span><a id="_idIndexMarker208"/><span class="koboSpan" id="kobo.306.1"> capabilities of CNNs and the innovative application of attention mechanisms, Vaswani et al. </span><span class="koboSpan" id="kobo.306.2">introduced the transformer architecture in the seminal paper </span><em class="italic"><span class="koboSpan" id="kobo.307.1">Attention is All You Need</span></em><span class="koboSpan" id="kobo.308.1">. </span><span class="koboSpan" id="kobo.308.2">The original transformer applied several novel methods, particularly emphasizing the instrumental impact of attention. </span><span class="koboSpan" id="kobo.308.3">It employed a </span><strong class="bold"><span class="koboSpan" id="kobo.309.1">self-attention mechanism</span></strong><span class="koboSpan" id="kobo.310.1">, allowing </span><a id="_idIndexMarker209"/><span class="koboSpan" id="kobo.311.1">each element in the input sequence to focus on distinct parts of the sequence, capturing dependencies regardless of their positions in a structured manner. </span><span class="koboSpan" id="kobo.311.2">The term “self” in “self-attention” refers to how the attention mechanism is applied to the input sequence itself, meaning each element in the sequence is compared to every other element to determine its </span><span class="No-Break"><span class="koboSpan" id="kobo.312.1">attention scores.</span></span></p>
<p><span class="koboSpan" id="kobo.313.1">To truly appreciate how the transformer architecture works, we can describe how the components in its architecture play a role in handling a particular task. </span><span class="koboSpan" id="kobo.313.2">Suppose we need our transformer to translate the English sentence “</span><em class="italic"><span class="koboSpan" id="kobo.314.1">Hello, how are you?</span></em><span class="koboSpan" id="kobo.315.1">” into French: “</span><em class="italic"><span class="koboSpan" id="kobo.316.1">Bonjour, comment ça va?</span></em><span class="koboSpan" id="kobo.317.1">” Let us walk through this step by step to examine and elucidate how the</span><a id="_idIndexMarker210"/><span class="koboSpan" id="kobo.318.1"> transformer might accomplish this task. </span><span class="koboSpan" id="kobo.318.2">For now, we will describe each step in detail </span><a id="_idTextAnchor095"/><span class="koboSpan" id="kobo.319.1">and later implement the full architecture </span><span class="No-Break"><span class="koboSpan" id="kobo.320.1">us</span><a id="_idTextAnchor096"/><span class="koboSpan" id="kobo.321.1">ing Python.</span></span></p>
<h2 id="_idParaDest-53"><a id="_idTextAnchor097"/><span class="koboSpan" id="kobo.322.1">Components of the transformer architecture</span></h2>
<p><span class="koboSpan" id="kobo.323.1">Before diving into </span><a id="_idIndexMarker211"/><span class="koboSpan" id="kobo.324.1">how the transformer model fulfills our translation task, we need to understand the steps involved. </span><span class="koboSpan" id="kobo.324.2">The complete architecture is quite dense, so we will break it down into small, logical, and </span><span class="No-Break"><span class="koboSpan" id="kobo.325.1">digestible components.</span></span></p>
<p><span class="koboSpan" id="kobo.326.1">First, we discuss the two components central to the architectural design of the transformer model: the encoder and decoder stacks. </span><span class="koboSpan" id="kobo.326.2">We will also explain how data flows within these layer stacks, including the concept of tokens, and how relationships between tokens are captured and refined using critical techniques such as self-attention </span><span class="No-Break"><span class="koboSpan" id="kobo.327.1">and FFNs.</span></span></p>
<p><span class="koboSpan" id="kobo.328.1">Then, we transition into the training process of the transformer model. </span><span class="koboSpan" id="kobo.328.2">Here, we review fundamental concepts such as batches, masking, the training loop, data preparation, optimizer selection, and strategies to improve performance. </span><span class="koboSpan" id="kobo.328.3">We will explain how the transformer optimizes performance using a loss function, which is crucial in shaping how the model learns </span><span class="No-Break"><span class="koboSpan" id="kobo.329.1">to translate.</span></span></p>
<p><span class="koboSpan" id="kobo.330.1">Following the training process, we discuss model inference, which is how our trained model generates translations. </span><span class="koboSpan" id="kobo.330.2">This section points out the order in which individual model components operate during translation and emphasizes the importance of </span><span class="No-Break"><span class="koboSpan" id="kobo.331.1">each step.</span></span></p>
<p><span class="koboSpan" id="kobo.332.1">As discussed, central to the transformer are two vital components, often called the encoder stack </span><a id="_idIndexMarker212"/><span class="koboSpan" id="kobo.333.1">and the </span><span class="No-Break"><span class="koboSpan" id="kobo.334.1">decoder stack.</span></span></p>
<h3><span class="koboSpan" id="kobo.335.1">Encoder and decoder stacks</span></h3>
<p><span class="koboSpan" id="kobo.336.1">In the context of the transformer model, </span><strong class="bold"><span class="koboSpan" id="kobo.337.1">stacks</span></strong><span class="koboSpan" id="kobo.338.1"> reference a hierarchical arrangement of </span><strong class="bold"><span class="koboSpan" id="kobo.339.1">layers</span></strong><span class="koboSpan" id="kobo.340.1">. </span><span class="koboSpan" id="kobo.340.2">Each layer in this context is, in fact, an NN layer like the layers we come across in classical DL models. </span><span class="koboSpan" id="kobo.340.3">While a layer is a level in the model where specific computational operations occur, a stack refers to multiple such layers </span><span class="No-Break"><span class="koboSpan" id="kobo.341.1">arranged consecutively.</span></span></p>
<h4><span class="koboSpan" id="kobo.342.1">Encoder stack</span></h4>
<p><span class="koboSpan" id="kobo.343.1">Consider our </span><a id="_idIndexMarker213"/><span class="koboSpan" id="kobo.344.1">example sentence “</span><em class="italic"><span class="koboSpan" id="kobo.345.1">Hello, how are you?</span></em><span class="koboSpan" id="kobo.346.1">”. </span><span class="koboSpan" id="kobo.346.2">We</span><a id="_idIndexMarker214"/><span class="koboSpan" id="kobo.347.1"> first convert it into tokens. </span><span class="koboSpan" id="kobo.347.2">Each token typically represents a word. </span><span class="koboSpan" id="kobo.347.3">In the case of our example sentence, tokenization would break it down into separate tokens, resulting in </span><span class="No-Break"><span class="koboSpan" id="kobo.348.1">the following:</span></span></p>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.349.1">[“Hello”, “,”, “how”, “are”, “</span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.350.1">you”, “?”]</span></strong></span></p>
<p><span class="koboSpan" id="kobo.351.1">Here, each word or punctuation represents a distinct token. </span><span class="koboSpan" id="kobo.351.2">These tokens are then transformed into numerical representations, also known </span><a id="_idIndexMarker215"/><span class="koboSpan" id="kobo.352.1">as </span><strong class="bold"><span class="koboSpan" id="kobo.353.1">embeddings</span></strong><span class="koboSpan" id="kobo.354.1">. </span><span class="koboSpan" id="kobo.354.2">These embedding vectors capture the semantic meaning and context of the words, enabling the model to understand and process the input data effectively. </span><span class="koboSpan" id="kobo.354.3">The embeddings aid in capturing complex relationships and contexts from the original English input sentence through this series of transformations </span><span class="No-Break"><span class="koboSpan" id="kobo.355.1">across layers.</span></span></p>
<p><span class="koboSpan" id="kobo.356.1">This stack comprises multiple layers, where each layer applies self-attention and FFN computations on its input data (which we will describe in detail shortly). </span><span class="koboSpan" id="kobo.356.2">The embeddings iteratively capture complex relationships and context from the original English input sentence through this series of transformations </span><span class="No-Break"><span class="koboSpan" id="kobo.357.1">across layers.</span></span></p>
<h4><span class="koboSpan" id="kobo.358.1">Decoder stack</span></h4>
<p><span class="koboSpan" id="kobo.359.1">Once the encoder</span><a id="_idIndexMarker216"/><span class="koboSpan" id="kobo.360.1"> completes its </span><a id="_idIndexMarker217"/><span class="koboSpan" id="kobo.361.1">task, the output vectors—or the embeddings of the input sentence that hold its contextual information—are passed on to the decoder. </span><span class="koboSpan" id="kobo.361.2">Within the decoder stack, multiple layers work sequentially to generate a French translation from </span><span class="No-Break"><span class="koboSpan" id="kobo.362.1">the embeddings.</span></span></p>
<p><span class="koboSpan" id="kobo.363.1">The process begins by converting the first embedding into the French phrase “</span><em class="italic"><span class="koboSpan" id="kobo.364.1">Bonjour</span></em><span class="koboSpan" id="kobo.365.1">.” </span><span class="koboSpan" id="kobo.365.2">The subsequent layer uses the following embedding and context from the previously generated words to predict the next word in the French sentence. </span><span class="koboSpan" id="kobo.365.3">This process is repeated through all the layers in the stack, each using input embeddings and generated words to define and refine </span><span class="No-Break"><span class="koboSpan" id="kobo.366.1">the translation.</span></span></p>
<p><span class="koboSpan" id="kobo.367.1">The decoder stack progressively builds (or decodes) the translated sentence through this iterative process, arriving at “</span><em class="italic"><span class="koboSpan" id="kobo.368.1">Bonjour, comment </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.369.1">ça va?</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.370.1">”.</span></span></p>
<p><span class="koboSpan" id="kobo.371.1">With an overall understanding of the encoder-decoder structure, our next step is unraveling the intricate operations within each stack. </span><span class="koboSpan" id="kobo.371.2">However, before delving into the self-attention mechanism and FFNs, there is one vital component we need to understand — positional encoding. </span><span class="koboSpan" id="kobo.371.3">Positional encoding is paramount to the transformer’s performance because it gives the transformer model a sense of the order of words, something subsequent operations in the </span><span class="No-Break"><span class="koboSpan" id="kobo.372.1">stack lack.</span></span></p>
<h3><span class="koboSpan" id="kobo.373.1">Positional encoding</span></h3>
<p><span class="koboSpan" id="kobo.374.1">Every word in a</span><a id="_idIndexMarker218"/><span class="koboSpan" id="kobo.375.1"> sentence holds </span><a id="_idIndexMarker219"/><span class="koboSpan" id="kobo.376.1">two types of information — its meaning and its role in the larger context of the sentence. </span><span class="koboSpan" id="kobo.376.2">The contextual role often stems from a word’s position in the arrangement of words. </span><span class="koboSpan" id="kobo.376.3">A sentence such as “</span><em class="italic"><span class="koboSpan" id="kobo.377.1">Hello, how are you?</span></em><span class="koboSpan" id="kobo.378.1">” makes sense because the words are in a specific order. </span><span class="koboSpan" id="kobo.378.2">Change that to “</span><em class="italic"><span class="koboSpan" id="kobo.379.1">Are you, how hello?</span></em><span class="koboSpan" id="kobo.380.1">” and the meaning </span><span class="No-Break"><span class="koboSpan" id="kobo.381.1">becomes unclear.</span></span></p>
<p><span class="koboSpan" id="kobo.382.1">Consequently, Vaswani et al. </span><span class="koboSpan" id="kobo.382.2">introduced </span><strong class="bold"><span class="koboSpan" id="kobo.383.1">positional encoding</span></strong><span class="koboSpan" id="kobo.384.1"> to ensure that the transformer encodes each word with additional data about its position in the sentence. </span><span class="koboSpan" id="kobo.384.2">Positional encodings are computed using a blend of sine and cosine functions across different frequencies, which generate a unique set of values for each position in a sequence. </span><span class="koboSpan" id="kobo.384.3">These values are then added to the original embeddings of the tokens, providing a way for the model to capture the order of words. </span><span class="koboSpan" id="kobo.384.4">These enriched embeddings are then ready to be processed by the self-attention mechanism in the subsequent</span><a id="_idIndexMarker220"/><span class="koboSpan" id="kobo.385.1"> layers of the </span><span class="No-Break"><span class="koboSpan" id="kobo.386.1">transformer</span></span><span class="No-Break"><a id="_idIndexMarker221"/></span><span class="No-Break"><span class="koboSpan" id="kobo.387.1"> model.</span></span></p>
<h3><span class="koboSpan" id="kobo.388.1">Self-attention mechanism</span></h3>
<p><span class="koboSpan" id="kobo.389.1">As each token </span><a id="_idIndexMarker222"/><span class="koboSpan" id="kobo.390.1">of our input</span><a id="_idIndexMarker223"/><span class="koboSpan" id="kobo.391.1"> sentence “</span><em class="italic"><span class="koboSpan" id="kobo.392.1">Hello, how are you?</span></em><span class="koboSpan" id="kobo.393.1">” passes through each layer of the encoder stack, it undergoes a transformation via the </span><span class="No-Break"><span class="koboSpan" id="kobo.394.1">self-attention mechanism.</span></span></p>
<p><span class="koboSpan" id="kobo.395.1">As the name suggests, the self-attention mechanism allows each token (word) to attend to (or focus on) other vital tokens to understand the full context within the sentence. </span><span class="koboSpan" id="kobo.395.2">Before encoding a particular word, this attention mechanism interprets the relationship between each word and the others in the sequence. </span><span class="koboSpan" id="kobo.395.3">It then assigns distinct attention scores to different words based on their relevance to the current word </span><span class="No-Break"><span class="koboSpan" id="kobo.396.1">being processed.</span></span></p>
<p><span class="koboSpan" id="kobo.397.1">Consider again our input sentence “</span><em class="italic"><span class="koboSpan" id="kobo.398.1">Hello, how are you?</span></em><span class="koboSpan" id="kobo.399.1">”. </span><span class="koboSpan" id="kobo.399.2">When the self-attention mechanism is processing the last word, “</span><em class="italic"><span class="koboSpan" id="kobo.400.1">you</span></em><span class="koboSpan" id="kobo.401.1">,” it does not just focus on “</span><em class="italic"><span class="koboSpan" id="kobo.402.1">you</span></em><span class="koboSpan" id="kobo.403.1">.” </span><span class="koboSpan" id="kobo.403.2">Instead, it takes into consideration the entire sentence: it looks at “</span><em class="italic"><span class="koboSpan" id="kobo.404.1">Hello</span></em><span class="koboSpan" id="kobo.405.1">,” glances over “</span><em class="italic"><span class="koboSpan" id="kobo.406.1">how</span></em><span class="koboSpan" id="kobo.407.1">,” reflects on “</span><em class="italic"><span class="koboSpan" id="kobo.408.1">are</span></em><span class="koboSpan" id="kobo.409.1">,” and, of course, focuses </span><span class="No-Break"><span class="koboSpan" id="kobo.410.1">on “</span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.411.1">you</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.412.1">.”</span></span></p>
<p><span class="koboSpan" id="kobo.413.1">In doing so, it assigns various levels of attention to each word. </span><span class="koboSpan" id="kobo.413.2">You can visualize attention (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.414.1">Figure 3</span></em></span><em class="italic"><span class="koboSpan" id="kobo.415.1">.1</span></em><span class="koboSpan" id="kobo.416.1">) as lines connecting “</span><em class="italic"><span class="koboSpan" id="kobo.417.1">you</span></em><span class="koboSpan" id="kobo.418.1">” to every other word. </span><span class="koboSpan" id="kobo.418.2">The line to “</span><em class="italic"><span class="koboSpan" id="kobo.419.1">Hello</span></em><span class="koboSpan" id="kobo.420.1">” might be thick, indicating a lot of attention, representing the influence of “</span><em class="italic"><span class="koboSpan" id="kobo.421.1">Hello</span></em><span class="koboSpan" id="kobo.422.1">” on the encoding of “</span><em class="italic"><span class="koboSpan" id="kobo.423.1">you</span></em><span class="koboSpan" id="kobo.424.1">.” </span><span class="koboSpan" id="kobo.424.2">The line connecting “</span><em class="italic"><span class="koboSpan" id="kobo.425.1">you</span></em><span class="koboSpan" id="kobo.426.1">” and “</span><em class="italic"><span class="koboSpan" id="kobo.427.1">how</span></em><span class="koboSpan" id="kobo.428.1">” might be thinner, suggesting less attention given to “</span><em class="italic"><span class="koboSpan" id="kobo.429.1">how</span></em><span class="koboSpan" id="kobo.430.1">.” </span><span class="koboSpan" id="kobo.430.2">The lines to “</span><em class="italic"><span class="koboSpan" id="kobo.431.1">are</span></em><span class="koboSpan" id="kobo.432.1">” and “</span><em class="italic"><span class="koboSpan" id="kobo.433.1">you</span></em><span class="koboSpan" id="kobo.434.1">” would have other thicknesses based on how they help in providing context </span><span class="No-Break"><span class="koboSpan" id="kobo.435.1">to “</span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.436.1">you</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.437.1">”:</span></span></p>
<p class="IMG---Figure"> </p>
<div>
<div class="IMG---Figure" id="_idContainer020">
<span class="koboSpan" id="kobo.438.1"><img alt="Figure 3.1: Self-attention mechanism" src="image/B21773_03_01.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.439.1">Figure 3.1: Self-attention mechanism</span></p>
<p><span class="koboSpan" id="kobo.440.1">This way, when encoding “</span><em class="italic"><span class="koboSpan" id="kobo.441.1">you</span></em><span class="koboSpan" id="kobo.442.1">,” a weighted mix of the entire sentence is considered, not just the single word. </span><span class="koboSpan" id="kobo.442.2">And these weights defining the mix are what we refer to as </span><span class="No-Break"><span class="koboSpan" id="kobo.443.1">attention scores.</span></span></p>
<p><span class="koboSpan" id="kobo.444.1">The self-attention mechanism is implemented through a </span><span class="No-Break"><span class="koboSpan" id="kobo.445.1">few steps:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.446.1">Initially, each input word is represented as a vector, which we obtain from the </span><span class="No-Break"><span class="koboSpan" id="kobo.447.1">word embedding.</span></span></li>
<li><span class="koboSpan" id="kobo.448.1">These vectors are then mapped to new vectors called query, key, and value vectors through </span><span class="No-Break"><span class="koboSpan" id="kobo.449.1">learned transformations.</span></span></li>
<li><span class="koboSpan" id="kobo.450.1">An attention score for each word is then computed by taking the dot product of the query vector of the word with the key vector of every other word, followed by a SoftMax operation (which we will </span><span class="No-Break"><span class="koboSpan" id="kobo.451.1">describe later).</span></span></li>
<li><span class="koboSpan" id="kobo.452.1">These scores indicate how much focus to place on other parts of the input sentence for each word as it </span><span class="No-Break"><span class="koboSpan" id="kobo.453.1">is encoded.</span></span></li>
<li><span class="koboSpan" id="kobo.454.1">Finally, a weighted sum of the value vectors is computed based on these scores to give us our final output vectors, or the </span><span class="No-Break"><span class="koboSpan" id="kobo.455.1">self-attention outputs.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.456.1">It is important to note that this computation is done for each word in the sentence. </span><span class="koboSpan" id="kobo.456.2">This ensures a comprehensive understanding of the context in the sentence, considering multiple parts of the sentence at once. </span><span class="koboSpan" id="kobo.456.3">This concept set the transformer apart from nearly every model that came </span><span class="No-Break"><span class="koboSpan" id="kobo.457.1">before it.</span></span></p>
<p><span class="koboSpan" id="kobo.458.1">Instead of running </span><a id="_idIndexMarker224"/><span class="koboSpan" id="kobo.459.1">the </span><a id="_idIndexMarker225"/><span class="koboSpan" id="kobo.460.1">self-attention mechanism once (or “single-head” attention), the transformer replicates the self-attention mechanism multiple times in parallel. </span><span class="koboSpan" id="kobo.460.2">Each replica or head operates on the same input but has its own independent set of learned parameters to compute the attention scores. </span><span class="koboSpan" id="kobo.460.3">This allows each head to learn different contextual relationships between words. </span><span class="koboSpan" id="kobo.460.4">This parallel process is</span><a id="_idIndexMarker226"/><span class="koboSpan" id="kobo.461.1"> known as </span><strong class="bold"><span class="koboSpan" id="kobo.462.1">multi-head </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.463.1">attention</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.464.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.465.1">MHA</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.466.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.467.1">Imagine our sentence “</span><em class="italic"><span class="koboSpan" id="kobo.468.1">Hello, how are you?</span></em><span class="koboSpan" id="kobo.469.1">” again. </span><span class="koboSpan" id="kobo.469.2">One head might concentrate on how “</span><em class="italic"><span class="koboSpan" id="kobo.470.1">Hello</span></em><span class="koboSpan" id="kobo.471.1">” relates to “</span><em class="italic"><span class="koboSpan" id="kobo.472.1">you</span></em><span class="koboSpan" id="kobo.473.1">,” whereas another head might focus more on how “</span><em class="italic"><span class="koboSpan" id="kobo.474.1">how</span></em><span class="koboSpan" id="kobo.475.1">” relates to “</span><em class="italic"><span class="koboSpan" id="kobo.476.1">you</span></em><span class="koboSpan" id="kobo.477.1">.” </span><span class="koboSpan" id="kobo.477.2">Each head has its own set of query, key, and value weights, further enabling them to specialize and learn different things. </span><span class="koboSpan" id="kobo.477.3">The outputs of these multiple heads are then concatenated and transformed to produce final values passed onto the next layer in </span><span class="No-Break"><span class="koboSpan" id="kobo.478.1">the stack.</span></span></p>
<p><span class="koboSpan" id="kobo.479.1">This multi-head approach allows the model to capture a wider range of information from the same input words. </span><span class="koboSpan" id="kobo.479.2">It is like having several perspectives on the same sentence, each providing </span><span class="No-Break"><span class="koboSpan" id="kobo.480.1">unique insights.</span></span></p>
<p><span class="koboSpan" id="kobo.481.1">So far, for our input sentence “</span><em class="italic"><span class="koboSpan" id="kobo.482.1">Hello, how are you?</span></em><span class="koboSpan" id="kobo.483.1">”, we have converted each word into token representations, which are then contextualized using the MHA mechanism. </span><span class="koboSpan" id="kobo.483.2">Through parallel self-attention, our transformer can consider the full range of interactions between each word and every other word in the sentence. </span><span class="koboSpan" id="kobo.483.3">We now have a set of diverse and context-enriched word representations, each containing a textured understanding of a word’s role in the sentence. </span><span class="koboSpan" id="kobo.483.4">However, this contextual understanding contained within the attention mechanism is just one component of the information processing in our transformer model. </span><span class="koboSpan" id="kobo.483.5">Next comes another layer of interpretation through</span><a id="_idIndexMarker227"/><span class="koboSpan" id="kobo.484.1"> position-wise </span><a id="_idIndexMarker228"/><span class="koboSpan" id="kobo.485.1">FFNs. </span><span class="koboSpan" id="kobo.485.2">The FFN will add further nuances to these representations, making them more informative and valuable for our </span><span class="No-Break"><span class="koboSpan" id="kobo.486.1">translation task.</span></span></p>
<p><span class="koboSpan" id="kobo.487.1">In the next section, we discuss a vital aspect of the transformer’s training sequence: masking. </span><span class="koboSpan" id="kobo.487.2">Specifically, the transformer applies causal (or look-ahead) masking during the decoder self-attention to ensure that each output token prediction depends only on previously generated</span><a id="_idIndexMarker229"/><span class="koboSpan" id="kobo.488.1"> tokens, not future </span><span class="No-Break"><span class="koboSpan" id="kobo.489.1">unknown tokens.</span></span></p>
<h3><span class="koboSpan" id="kobo.490.1">Masking</span></h3>
<p><span class="koboSpan" id="kobo.491.1">The transformer </span><a id="_idIndexMarker230"/><span class="koboSpan" id="kobo.492.1">applies two types of masking during training. </span><span class="koboSpan" id="kobo.492.2">The first is a preprocessing step to ensure input sentences are of the same length, which enables efficient batch computation. </span><span class="koboSpan" id="kobo.492.3">The second is look-ahead (or causal) masking, which allows the model to selectively ignore future tokens in a sequence. </span><span class="koboSpan" id="kobo.492.4">This type of masking occurs in the self-attention mechanism in the decoder and prevents the model from peeking ahead at future tokens in the sequence. </span><span class="koboSpan" id="kobo.492.5">For example, when translating the word “</span><em class="italic"><span class="koboSpan" id="kobo.493.1">Hello</span></em><span class="koboSpan" id="kobo.494.1">” to French, look-ahead masking ensures that the model does not have access to the subsequent words “</span><em class="italic"><span class="koboSpan" id="kobo.495.1">how</span></em><span class="koboSpan" id="kobo.496.1">,” “</span><em class="italic"><span class="koboSpan" id="kobo.497.1">are</span></em><span class="koboSpan" id="kobo.498.1">,” or “</span><em class="italic"><span class="koboSpan" id="kobo.499.1">you</span></em><span class="koboSpan" id="kobo.500.1">.” </span><span class="koboSpan" id="kobo.500.2">This way, the model learns to generate translations based on the current and preceding words, adhering to a natural progression in translation tasks, mimicking that of </span><span class="No-Break"><span class="koboSpan" id="kobo.501.1">human translation.</span></span></p>
<p><span class="koboSpan" id="kobo.502.1">With a clearer understanding of how data is prepared and masked for training, we now transition to another significant aspect of the training process: hyperparameters. </span><span class="koboSpan" id="kobo.502.2">Unlike parameters learned from the data, hyperparameters are configurations set before training to control the model optimization process and guide the learning journey. </span><span class="koboSpan" id="kobo.502.3">The following section will explore various hyperparameters and their roles </span><span class="No-Break"><span class="koboSpan" id="kobo.503.1">during training.</span></span></p>
<h3><span class="koboSpan" id="kobo.504.1">SoftMax</span></h3>
<p><span class="koboSpan" id="kobo.505.1">To understand the </span><a id="_idIndexMarker231"/><span class="koboSpan" id="kobo.506.1">role of the FFN, we </span><a id="_idIndexMarker232"/><span class="koboSpan" id="kobo.507.1">can describe its two primary components—linear transformations and an </span><span class="No-Break"><span class="koboSpan" id="kobo.508.1">activation function:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.509.1">Linear transformations</span></strong><span class="koboSpan" id="kobo.510.1"> are essentially </span><a id="_idIndexMarker233"/><span class="koboSpan" id="kobo.511.1">matrix </span><a id="_idIndexMarker234"/><span class="koboSpan" id="kobo.512.1">multiplications. </span><span class="koboSpan" id="kobo.512.2">Think of them as tools that reshape or tweak the input data. </span><span class="koboSpan" id="kobo.512.3">In the FFN, these transformations occur twice, where two different weights (or matrices) </span><span class="No-Break"><span class="koboSpan" id="kobo.513.1">are used.</span></span></li>
<li><span class="koboSpan" id="kobo.514.1">A </span><strong class="bold"><span class="koboSpan" id="kobo.515.1">rectified linear unit</span></strong><span class="koboSpan" id="kobo.516.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.517.1">ReLU</span></strong><span class="koboSpan" id="kobo.518.1">) function</span><a id="_idIndexMarker235"/><span class="koboSpan" id="kobo.519.1"> is applied between these two transformations. </span><span class="koboSpan" id="kobo.519.2">The role of the ReLU function is to introduce non-linearity in the model. </span><span class="koboSpan" id="kobo.519.3">Simply put, the ReLU function allows the model to capture patterns within the input data that are not strictly proportional, i.e., </span><a id="_idIndexMarker236"/><span class="koboSpan" id="kobo.520.1">non-linear, which </span><a id="_idIndexMarker237"/><span class="koboSpan" id="kobo.521.1">is typical</span><a id="_idIndexMarker238"/><span class="koboSpan" id="kobo.522.1"> of </span><strong class="bold"><span class="koboSpan" id="kobo.523.1">natural language</span></strong><span class="koboSpan" id="kobo.524.1"> (</span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.525.1">NL</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.526.1">) data.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.527.1">The FFN is called </span><strong class="bold"><span class="koboSpan" id="kobo.528.1">position-wise</span></strong><span class="koboSpan" id="kobo.529.1"> because</span><a id="_idIndexMarker239"/><span class="koboSpan" id="kobo.530.1"> it treats each word in the sentence separately (position by position), regardless of the sequence. </span><span class="koboSpan" id="kobo.530.2">This contrasts with the self-attention mechanism, which considers the entire sequence </span><span class="No-Break"><span class="koboSpan" id="kobo.531.1">at once.</span></span></p>
<p><span class="koboSpan" id="kobo.532.1">So, let us attempt to visualize the process: Imagine our word “</span><em class="italic"><span class="koboSpan" id="kobo.533.1">Hello</span></em><span class="koboSpan" id="kobo.534.1">” arriving here after going through the self-attention mechanism. </span><span class="koboSpan" id="kobo.534.2">It carries with it information about its own identity mixed with contextual references to “</span><em class="italic"><span class="koboSpan" id="kobo.535.1">how</span></em><span class="koboSpan" id="kobo.536.1">,” “</span><em class="italic"><span class="koboSpan" id="kobo.537.1">are</span></em><span class="koboSpan" id="kobo.538.1">,” and “</span><em class="italic"><span class="koboSpan" id="kobo.539.1">you</span></em><span class="koboSpan" id="kobo.540.1">.” </span><span class="koboSpan" id="kobo.540.2">This integrated information resides within a vector that </span><span class="No-Break"><span class="koboSpan" id="kobo.541.1">characterizes “</span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.542.1">Hello</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.543.1">.”</span></span></p>
<p><span class="koboSpan" id="kobo.544.1">When “</span><em class="italic"><span class="koboSpan" id="kobo.545.1">Hello</span></em><span class="koboSpan" id="kobo.546.1">” enters the FFN, picture it as a tunnel with two gates. </span><span class="koboSpan" id="kobo.546.2">At the first gate (or linear layer), “</span><em class="italic"><span class="koboSpan" id="kobo.547.1">Hello</span></em><span class="koboSpan" id="kobo.548.1">” is transformed by a matrix multiplication operation, changing its representation. </span><span class="koboSpan" id="kobo.548.2">Afterward, it encounters the ReLU function—which makes the </span><span class="No-Break"><span class="koboSpan" id="kobo.549.1">representation non-linear.</span></span></p>
<p><span class="koboSpan" id="kobo.550.1">After this, “</span><em class="italic"><span class="koboSpan" id="kobo.551.1">Hello</span></em><span class="koboSpan" id="kobo.552.1">” passes through a second gate (another linear layer), emerging on the other side transformed yet again. </span><span class="koboSpan" id="kobo.552.2">The core identity of “</span><em class="italic"><span class="koboSpan" id="kobo.553.1">Hello</span></em><span class="koboSpan" id="kobo.554.1">” remains but is now imbued with even more context, carefully calibrated and adjusted by </span><span class="No-Break"><span class="koboSpan" id="kobo.555.1">the FFN.</span></span></p>
<p><span class="koboSpan" id="kobo.556.1">Once the input passes through the gates, there is one additional step. </span><span class="koboSpan" id="kobo.556.2">The transformed vector still must be</span><a id="_idIndexMarker240"/><span class="koboSpan" id="kobo.557.1"> converted into a form that can be interpreted as a prediction for our final </span><span class="No-Break"><span class="koboSpan" id="kobo.558.1">translation task.</span></span></p>
<p><span class="koboSpan" id="kobo.559.1">This brings us to using the SoftMax function, the final transformation within the transformer’s decoder. </span><span class="koboSpan" id="kobo.559.2">After the vectors pass through the FFN, they are further processed through a final linear layer. </span><span class="koboSpan" id="kobo.559.3">The result is then fed into a </span><span class="No-Break"><span class="koboSpan" id="kobo.560.1">SoftMax function.</span></span></p>
<p><span class="koboSpan" id="kobo.561.1">SoftMax serves as a mechanism for converting the output of our model into a form that can be interpreted as probabilities. </span><span class="koboSpan" id="kobo.561.2">In essence, the SoftMax function will take the output from our final linear layer (which could be any set of real numbers) and transform it into a distribution of</span><a id="_idIndexMarker241"/><span class="koboSpan" id="kobo.562.1"> probabilities, representing the likelihood of each word being the next word in our output sequence. </span><span class="koboSpan" id="kobo.562.2">For example, if our target vocabulary includes “</span><em class="italic"><span class="koboSpan" id="kobo.563.1">Bonjour</span></em><span class="koboSpan" id="kobo.564.1">,” “</span><em class="italic"><span class="koboSpan" id="kobo.565.1">Hola</span></em><span class="koboSpan" id="kobo.566.1">,” “</span><em class="italic"><span class="koboSpan" id="kobo.567.1">Hello</span></em><span class="koboSpan" id="kobo.568.1">,” and “</span><em class="italic"><span class="koboSpan" id="kobo.569.1">Hallo</span></em><span class="koboSpan" id="kobo.570.1">,” the SoftMax function will assign each of these words a probability, and the word with the highest probability will be chosen as the output translation for the word “</span><em class="italic"><span class="koboSpan" id="kobo.571.1">Hello</span></em><span class="koboSpan" id="kobo.572.1">.” </span><span class="koboSpan" id="kobo.572.2">We can illustrate with this oversimplified representation of the </span><span class="No-Break"><span class="koboSpan" id="kobo.573.1">output probabilities:</span></span></p>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.574.1">[ Bonjour: 0.4, Hola: 0.3, Hello: 0.2, Hallo: </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.575.1">0.1 ]</span></strong></span></p>
<p><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.576.1">Figure 3</span></em></span><em class="italic"><span class="koboSpan" id="kobo.577.1">.2</span></em><span class="koboSpan" id="kobo.578.1"> shows a more complete (albeit oversimplified) view of the flow of information through </span><span class="No-Break"><span class="koboSpan" id="kobo.579.1">the architecture.</span></span></p>
<p class="IMG---Figure"> </p>
<div>
<div class="IMG---Figure" id="_idContainer021">
<span class="koboSpan" id="kobo.580.1"><img alt="Figure 3.2: A simplified illustration of the transformer" src="image/B21773_03_02.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.581.1">Figure 3.2: A simplified illustration of the transformer</span></p>
<p><span class="koboSpan" id="kobo.582.1">Now that we’ve</span><a id="_idIndexMarker242"/><span class="koboSpan" id="kobo.583.1"> introduced the</span><a id="_idIndexMarker243"/><span class="koboSpan" id="kobo.584.1"> architectural components of the transformer, we are poised to understand how its components </span><span class="No-Break"><span class="koboSpan" id="kobo.585.1">work together.</span></span></p>
<h2 id="_idParaDest-54"><a id="_idTextAnchor098"/><span class="koboSpan" id="kobo.586.1">Sequence-to-sequence learning</span></h2>
<p><span class="koboSpan" id="kobo.587.1">The components of</span><a id="_idIndexMarker244"/><span class="koboSpan" id="kobo.588.1"> a transformer come together to learn from data using a mechanism known as </span><strong class="bold"><span class="koboSpan" id="kobo.589.1">sequence-to-sequence</span></strong><span class="koboSpan" id="kobo.590.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.591.1">Seq2Seq</span></strong><span class="koboSpan" id="kobo.592.1">) learning, a subset of </span><strong class="bold"><span class="koboSpan" id="kobo.593.1">supervised learning</span></strong><span class="koboSpan" id="kobo.594.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.595.1">SL</span></strong><span class="koboSpan" id="kobo.596.1">). </span><span class="koboSpan" id="kobo.596.2">Recall </span><a id="_idIndexMarker245"/><span class="koboSpan" id="kobo.597.1">that SL is a technique that uses labeled data to train models to predict outcomes accurately. </span><span class="koboSpan" id="kobo.597.2">In Seq2Seq learning, we provide the transformer with training data that comprises examples of input and corresponding correct output, which, in this case, are correct translations. </span><span class="koboSpan" id="kobo.597.3">Seq2Seq learning is particularly well suited for tasks such as machine translation where both the input and output are sequences </span><span class="No-Break"><span class="koboSpan" id="kobo.598.1">of words.</span></span></p>
<p><span class="koboSpan" id="kobo.599.1">The very first step in the learning process is to convert each word in the phrase into tokens, which are then transformed into numerical embeddings. </span><span class="koboSpan" id="kobo.599.2">These embeddings carry the semantic essence of each word. </span><span class="koboSpan" id="kobo.599.3">Positional encodings are computed and added to these embeddings to imbue them with </span><span class="No-Break"><span class="koboSpan" id="kobo.600.1">positional awareness.</span></span></p>
<p><span class="koboSpan" id="kobo.601.1">As these enriched embeddings traverse through the encoder stack, within each layer, the self-attention mechanism refines the embeddings by aggregating contextual information from the entire phrase. </span><span class="koboSpan" id="kobo.601.2">Following self-attention, each word’s embedding undergoes further transformation in the position-wise FFNs, adjusting the embeddings to capture even more </span><span class="No-Break"><span class="koboSpan" id="kobo.602.1">complex relationships.</span></span></p>
<p><span class="koboSpan" id="kobo.603.1">Upon exiting the encoder, the embeddings now hold a rich mixture of semantic and contextual information. </span><span class="koboSpan" id="kobo.603.2">They are passed onto the decoder stack, which aims to translate the phrase into another language (that is, the target sequence). </span><span class="koboSpan" id="kobo.603.3">As with the encoder, each layer in the decoder also employs self-attention and position-wise FFNs, but with an additional layer of cross-attention that interacts with the encoder’s outputs. </span><span class="koboSpan" id="kobo.603.4">This interaction helps align the input and output phrases, a crucial aspect </span><span class="No-Break"><span class="koboSpan" id="kobo.604.1">of translation.</span></span></p>
<p><span class="koboSpan" id="kobo.605.1">As the embeddings move through the decoder layers, they are progressively refined to represent the translated phrase that the model will predict. </span><span class="koboSpan" id="kobo.605.2">The final layer of the decoder processes the embeddings through a linear transformation and SoftMax function to produce a probability distribution over the target vocabulary. </span><span class="koboSpan" id="kobo.605.3">This distribution defines the model’s predicted likelihood for each potential next token at each step. </span><span class="koboSpan" id="kobo.605.4">The decoder then samples from this distribution to select the token with the highest predicted probability as its next output. </span><span class="koboSpan" id="kobo.605.5">By iteratively sampling the most likely next tokens according to the predicted distributions, the decoder can autoregressively generate the full translated output sequence token </span><span class="No-Break"><span class="koboSpan" id="kobo.606.1">by token.</span></span></p>
<p><span class="koboSpan" id="kobo.607.1">However, for the transformer to reliably sample from the predicted next-token distributions to generate high-quality translations, it must progressively learn by iterating over thousands of examples of input-output pairs. </span><span class="koboSpan" id="kobo.607.2">In the next section, we explore model </span><a id="_idIndexMarker246"/><span class="koboSpan" id="kobo.608.1">training in </span><span class="No-Break"><span class="koboSpan" id="kobo.609.1">further detail.</span></span></p>
<h3><span class="koboSpan" id="kobo.610.1">Model training</span></h3>
<p><span class="koboSpan" id="kobo.611.1">As discussed, the</span><a id="_idIndexMarker247"/><span class="koboSpan" id="kobo.612.1"> primary goal of the training phase is to refine the model’s parameters to facilitate accurate translation from one language to another. </span><span class="koboSpan" id="kobo.612.2">But what does the refinement of parameters entail, and why is </span><span class="No-Break"><span class="koboSpan" id="kobo.613.1">it pivotal?</span></span></p>
<p><span class="koboSpan" id="kobo.614.1">Parameters are internal variables that the model utilizes to generate translations. </span><span class="koboSpan" id="kobo.614.2">Initially, these parameters are assigned random values, which are adjusted with each training iteration. </span><span class="koboSpan" id="kobo.614.3">Again, the model is provided with training data that comprises thousands of examples of input data and corresponding correct output, which, in this case, is the correct translation. </span><span class="koboSpan" id="kobo.614.4">It then compares its predicted output tokens to the correct (or actual) target sequences using an error (or </span><span class="No-Break"><span class="koboSpan" id="kobo.615.1">loss) function.</span></span></p>
<p><span class="koboSpan" id="kobo.616.1">Based on the loss, the model updates its parameters, gradually improving its ability to choose the correct item in the sequence at each step of decoding. </span><span class="koboSpan" id="kobo.616.2">This slowly refines the </span><span class="No-Break"><span class="koboSpan" id="kobo.617.1">probability distributions.</span></span></p>
<p><span class="koboSpan" id="kobo.618.1">Over thousands of training iterations, the model learns associations between source and target languages. </span><span class="koboSpan" id="kobo.618.2">Eventually, it acquires enough knowledge to decode coherent, human-like translations from unseen inputs by relying on patterns discovered during training. </span><span class="koboSpan" id="kobo.618.3">Therefore, training drives the model’s ability to produce accurate target sequences from the predicted </span><span class="No-Break"><span class="koboSpan" id="kobo.619.1">vocabulary distributions.</span></span></p>
<p><span class="koboSpan" id="kobo.620.1">After training on sufficient translation pairs, the transformer reaches reliable translation performance. </span><span class="koboSpan" id="kobo.620.2">The trained model can then take in new input sequences and output translated sequences by generalizing to that </span><span class="No-Break"><span class="koboSpan" id="kobo.621.1">new data.</span></span></p>
<p><span class="koboSpan" id="kobo.622.1">For instance, with our example sentence “</span><em class="italic"><span class="koboSpan" id="kobo.623.1">Hello, how are you?</span></em><span class="koboSpan" id="kobo.624.1">” and its French translation “</span><em class="italic"><span class="koboSpan" id="kobo.625.1">Bonjour, comment ça va?</span></em><span class="koboSpan" id="kobo.626.1">”, the English sentence serves as the input, and the French sentence serves as the target output. </span><span class="koboSpan" id="kobo.626.2">The training data comprises many translated pairs. </span><span class="koboSpan" id="kobo.626.3">Each time the model processes a batch of data, it generates predictions for the translation, compares them to the actual target translations, and then adjusts its parameters to reduce the discrepancy (or minimize the loss) between the predicted and actual translations. </span><span class="koboSpan" id="kobo.626.4">This is repeated with numerous batches of data until the model’s translations are </span><span class="No-Break"><span class="koboSpan" id="kobo.627.1">sufficiently accurate.</span></span></p>
<h3><span class="koboSpan" id="kobo.628.1">Hyperparameters</span></h3>
<p><span class="koboSpan" id="kobo.629.1">Again, unlike </span><a id="_idIndexMarker248"/><span class="koboSpan" id="kobo.630.1">parameters, which the </span><a id="_idIndexMarker249"/><span class="koboSpan" id="kobo.631.1">model learns from the training data, hyperparameters are preset configurations that govern the training process and the structure of the model. </span><span class="koboSpan" id="kobo.631.2">They are a crucial part of setting up a successful </span><span class="No-Break"><span class="koboSpan" id="kobo.632.1">training run.</span></span></p>
<p><span class="koboSpan" id="kobo.633.1">Some key hyperparameters in the context of transformer models include </span><span class="No-Break"><span class="koboSpan" id="kobo.634.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.635.1">Learning rate</span></strong><span class="koboSpan" id="kobo.636.1">: This value determines the step size at which the optimizer updates the model parameters. </span><span class="koboSpan" id="kobo.636.2">A higher learning rate could speed up the training but may overshoot the optimal solution. </span><span class="koboSpan" id="kobo.636.3">A lower learning rate may result in a more precise convergence to the optimal solution, albeit at the cost of longer training time. </span><span class="koboSpan" id="kobo.636.4">We will discuss optimizers in detail in the </span><span class="No-Break"><span class="koboSpan" id="kobo.637.1">next section.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.638.1">Batch size</span></strong><span class="koboSpan" id="kobo.639.1">: The number of data examples processed in a single batch affects the computational accuracy and the memory requirements </span><span class="No-Break"><span class="koboSpan" id="kobo.640.1">during training.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.641.1">Model dimensions</span></strong><span class="koboSpan" id="kobo.642.1">: The model’s size (for example, the number of layers in the encoder and decoder, the dimensionality of the embeddings, and so on) is a crucial hyperparameter that affects the model’s capacity to learn </span><span class="No-Break"><span class="koboSpan" id="kobo.643.1">and generalize.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.644.1">Optimizer settings</span></strong><span class="koboSpan" id="kobo.645.1">: Choosing an optimizer and its settings, such as the initial learning rate, beta values in the Adam optimizer, and so on, are also considered hyperparameters. </span><span class="koboSpan" id="kobo.645.2">Again, we will explore optimizers further in the </span><span class="No-Break"><span class="koboSpan" id="kobo.646.1">next section.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.647.1">Regularization terms</span></strong><span class="koboSpan" id="kobo.648.1">: Regularization terms such as dropout rate are hyperparameters that help prevent overfitting by adding some form of randomness or constraint to the </span><span class="No-Break"><span class="koboSpan" id="kobo.649.1">training process.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.650.1">Selecting the proper values for hyperparameters is crucial for the training process as it significantly impacts the model’s performance and efficiency. </span><span class="koboSpan" id="kobo.650.2">It often involves hyperparameter tuning, which involves experimentation and refining to find values for hyperparameters that yield reliable performance for a given task. </span><span class="koboSpan" id="kobo.650.3">Hyperparameter tuning can be somewhat of an art and a science. </span><span class="koboSpan" id="kobo.650.4">We will touch on this more in </span><span class="No-Break"><span class="koboSpan" id="kobo.651.1">later chapters.</span></span></p>
<p><span class="koboSpan" id="kobo.652.1">With a high-level</span><a id="_idIndexMarker250"/><span class="koboSpan" id="kobo.653.1"> grasp of hyperparameters, we will move on to the choice of optimizer, which is pivotal in controlling how efficiently the model learns from the </span><span class="No-Break"><span class="koboSpan" id="kobo.654.1">training data.</span></span></p>
<h3><span class="koboSpan" id="kobo.655.1">Choice of optimizer</span></h3>
<p><span class="koboSpan" id="kobo.656.1">The optimizer is a </span><a id="_idIndexMarker251"/><span class="koboSpan" id="kobo.657.1">fundamental component of the training process and is responsible for updating the model’s parameters to minimize error. </span><span class="koboSpan" id="kobo.657.2">Different optimizers have different strategies for navigating the parameter space to find a set of parameter values that yield low loss (or less error). </span><span class="koboSpan" id="kobo.657.3">The choice of optimizer can significantly impact the speed and quality of the </span><span class="No-Break"><span class="koboSpan" id="kobo.658.1">training process.</span></span></p>
<p><span class="koboSpan" id="kobo.659.1">In the context of transformer models, the Adam optimizer is often the optimizer of choice due to its efficiency and empirical success in training deep networks. </span><span class="koboSpan" id="kobo.659.2">Adam adapts learning rates during training. </span><span class="koboSpan" id="kobo.659.3">For simplicity, we will not explore all the possible optimizers but instead describe </span><span class="No-Break"><span class="koboSpan" id="kobo.660.1">their purpose.</span></span></p>
<p><span class="koboSpan" id="kobo.661.1">The optimizer’s primary task is to fine-tune the model’s parameters to reduce translation errors, progressively guiding the model toward the desired level of performance. </span><span class="koboSpan" id="kobo.661.2">However, an over-zealous optimization could lead the model to memorize the training data, failing to generalize well to unseen data. </span><span class="koboSpan" id="kobo.661.3">To mitigate this, we employ </span><span class="No-Break"><span class="koboSpan" id="kobo.662.1">regularization techniques.</span></span></p>
<p><span class="koboSpan" id="kobo.663.1">In the next section, we will explore regularization—a technique that works with optimization to</span><a id="_idIndexMarker252"/><span class="koboSpan" id="kobo.664.1"> ensure that while the model learns to minimize translation errors, it also remains adaptable to new, </span><span class="No-Break"><span class="koboSpan" id="kobo.665.1">unseen data.</span></span></p>
<h3><span class="koboSpan" id="kobo.666.1">Regularization</span></h3>
<p><span class="koboSpan" id="kobo.667.1">Regularization techniques are </span><a id="_idIndexMarker253"/><span class="koboSpan" id="kobo.668.1">employed </span><a id="_idIndexMarker254"/><span class="koboSpan" id="kobo.669.1">to deter the model from memorizing the training data (a phenomenon known as overfitting) and to promote better performance on new, unseen data. </span><span class="koboSpan" id="kobo.669.2">Overfitting arises when the model, to minimize the error, learns the training data to such an extent that it captures useless patterns (or noise) along with the actual patterns. </span><span class="koboSpan" id="kobo.669.3">This over-precision in learning the training data leads to a decline in performance when the model is exposed to </span><span class="No-Break"><span class="koboSpan" id="kobo.670.1">new data.</span></span></p>
<p><span class="koboSpan" id="kobo.671.1">Let us revisit our simple scenario where we train a model to translate English greetings to French greetings using a dataset that includes the word “</span><em class="italic"><span class="koboSpan" id="kobo.672.1">Hello</span></em><span class="koboSpan" id="kobo.673.1">” and its translation “</span><em class="italic"><span class="koboSpan" id="kobo.674.1">Bonjour</span></em><span class="koboSpan" id="kobo.675.1">.” </span><span class="koboSpan" id="kobo.675.2">If the model is overfitting, it may memorize the exact phrases from the training data without understanding the broader </span><span class="No-Break"><span class="koboSpan" id="kobo.676.1">translation pattern.</span></span></p>
<p><span class="koboSpan" id="kobo.677.1">In an overfit scenario, suppose the model learns to translate “</span><em class="italic"><span class="koboSpan" id="kobo.678.1">Hello</span></em><span class="koboSpan" id="kobo.679.1">” to “</span><em class="italic"><span class="koboSpan" id="kobo.680.1">Bonjour</span></em><span class="koboSpan" id="kobo.681.1">” with a probability of 1.0 because that is what it encountered most often in the training data. </span><span class="koboSpan" id="kobo.681.2">When presented with new, unseen data, it may encounter variations it has not seen before, such as “</span><em class="italic"><span class="koboSpan" id="kobo.682.1">Hi</span></em><span class="koboSpan" id="kobo.683.1">,” which should also translate to “</span><em class="italic"><span class="koboSpan" id="kobo.684.1">Bonjour</span></em><span class="koboSpan" id="kobo.685.1">.” </span><span class="koboSpan" id="kobo.685.2">However, due to overfitting, the model might fail to generalize from “</span><em class="italic"><span class="koboSpan" id="kobo.686.1">Hello</span></em><span class="koboSpan" id="kobo.687.1">” to “</span><em class="italic"><span class="koboSpan" id="kobo.688.1">Hi</span></em><span class="koboSpan" id="kobo.689.1">” as it is overly focused on the exact mappings it saw </span><span class="No-Break"><span class="koboSpan" id="kobo.690.1">during training.</span></span></p>
<p><span class="koboSpan" id="kobo.691.1">Several regularization techniques can mitigate the overfitting problem. </span><span class="koboSpan" id="kobo.691.2">These techniques apply certain constraints on the model’s parameters during training, encouraging the model to learn a more generalized representation of the data rather than memorizing the</span><a id="_idIndexMarker255"/> <span class="No-Break"><span class="koboSpan" id="kobo.692.1">training</span></span><span class="No-Break"><a id="_idIndexMarker256"/></span><span class="No-Break"><span class="koboSpan" id="kobo.693.1"> dataset.</span></span></p>
<p><span class="koboSpan" id="kobo.694.1">Here are some standard regularization techniques used in the context of </span><span class="No-Break"><span class="koboSpan" id="kobo.695.1">transformer models:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.696.1">Dropout</span></strong><span class="koboSpan" id="kobo.697.1">: In the context of NN-based models such as the transformer, the term “neurons” refers to individual elements within the model that work together to learn from the data and make predictions. </span><span class="koboSpan" id="kobo.697.2">Each neuron learns specific aspects or features from the data, enabling the model to understand and translate text. </span><span class="koboSpan" id="kobo.697.3">During training, dropout randomly deactivates or “drops out” a fraction of these neurons, temporarily removing them from the network. </span><span class="koboSpan" id="kobo.697.4">This random deactivation encourages the model to spread its learning across many neurons rather than relying too heavily on a few. </span><span class="koboSpan" id="kobo.697.5">By doing so, dropout helps the model to better generalize its learning to unseen data rather than merely memorizing the training data (that </span><span class="No-Break"><span class="koboSpan" id="kobo.698.1">is, overfitting).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.699.1">Layer normalization</span></strong><span class="koboSpan" id="kobo.700.1">: Layer normalization is a technique that normalizes the activations of neurons in a layer for each training example rather than across a batch of examples. </span><span class="koboSpan" id="kobo.700.2">This normalization helps stabilize the training process and acts as a form of regularization, </span><span class="No-Break"><span class="koboSpan" id="kobo.701.1">preventing overfitting.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.702.1">L1 or L2 regularization</span></strong><span class="koboSpan" id="kobo.703.1">: L1 regularization, also known as Lasso, adds a penalty equal to the absolute magnitude of coefficients, promoting parameter sparsity. </span><span class="koboSpan" id="kobo.703.2">L2 regularization, or Ridge, adds a penalty based on the square of the coefficients, discouraging large values to prevent overfitting. </span><span class="koboSpan" id="kobo.703.3">Although these techniques help in controlling model complexity and enhancing generalization, they were not part of the transformer’s </span><span class="No-Break"><span class="koboSpan" id="kobo.704.1">initial design.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.705.1">By employing these regularization techniques, the model is guided toward learning more generalized patterns in the data, which improves its ability to perform well on unseen data, thus making the model more reliable and robust in translating new </span><span class="No-Break"><span class="koboSpan" id="kobo.706.1">text inputs.</span></span></p>
<p><span class="koboSpan" id="kobo.707.1">Throughout the training process, we have mentioned the loss function and discussed how the optimizer leverages it to adjust the model’s parameters, aiming to minimize prediction error. </span><span class="koboSpan" id="kobo.707.2">The loss function quantifies the model’s performance. </span><span class="koboSpan" id="kobo.707.3">We discussed how regularization penalizes the loss function to prevent overfitting, encouraging the model to learn</span><a id="_idIndexMarker257"/><span class="koboSpan" id="kobo.708.1"> simpler, more generalizable</span><a id="_idIndexMarker258"/><span class="koboSpan" id="kobo.709.1"> patterns. </span><span class="koboSpan" id="kobo.709.2">In the next section, we look closer at the nuanced role of the loss </span><span class="No-Break"><span class="koboSpan" id="kobo.710.1">function itself.</span></span></p>
<h3><span class="koboSpan" id="kobo.711.1">Loss function</span></h3>
<p><span class="koboSpan" id="kobo.712.1">The loss function is</span><a id="_idIndexMarker259"/><span class="koboSpan" id="kobo.713.1"> vital in training the</span><a id="_idIndexMarker260"/><span class="koboSpan" id="kobo.714.1"> transformer model, quantifying the differences between the model’s predictions and the actual data. </span><span class="koboSpan" id="kobo.714.2">In language translation, this error is measured between generated and actual translations in the training dataset. </span><span class="koboSpan" id="kobo.714.3">A common choice for this task is cross-entropy loss, which measures the difference between the model’s predicted probability distribution across the target vocabulary and the actual distribution, where the truth has a probability of 1 for the correct word and 0 for </span><span class="No-Break"><span class="koboSpan" id="kobo.715.1">the rest.</span></span></p>
<p><span class="koboSpan" id="kobo.716.1">The transformer often employs a variant known as label-smoothed cross-entropy loss.  </span><span class="koboSpan" id="kobo.716.2">Label smoothing adjusts the target probability distribution during training, slightly lowering the probability for the correct class and increasing the probability for all other classes, which helps prevent the model from becoming too confident in its predictions. </span><span class="koboSpan" id="kobo.716.3">For instance, with a target vocabulary comprising “</span><em class="italic"><span class="koboSpan" id="kobo.717.1">Bonjour</span></em><span class="koboSpan" id="kobo.718.1">,” “</span><em class="italic"><span class="koboSpan" id="kobo.719.1">Hola</span></em><span class="koboSpan" id="kobo.720.1">,” “</span><em class="italic"><span class="koboSpan" id="kobo.721.1">Hello</span></em><span class="koboSpan" id="kobo.722.1">,” and “</span><em class="italic"><span class="koboSpan" id="kobo.723.1">Hallo</span></em><span class="koboSpan" id="kobo.724.1">,” and assuming “</span><em class="italic"><span class="koboSpan" id="kobo.725.1">Bonjour</span></em><span class="koboSpan" id="kobo.726.1">” is the correct translation, a standard cross-entropy loss would aim for the probability distribution of </span><strong class="source-inline"><span class="koboSpan" id="kobo.727.1">Bonjour: 1.0</span></strong><span class="koboSpan" id="kobo.728.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.729.1">Hola: 0.0</span></strong><span class="koboSpan" id="kobo.730.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.731.1">Hello: 0.0</span></strong><span class="koboSpan" id="kobo.732.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.733.1">Hallo: 0.0</span></strong><span class="koboSpan" id="kobo.734.1">. </span><span class="koboSpan" id="kobo.734.2">However, the label-smoothed cross-entropy loss would slightly adjust these probabilities, </span><span class="No-Break"><span class="koboSpan" id="kobo.735.1">as follows:</span></span></p>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.736.1">[ “Bonjour”: 0.925, “Hola”: 0.025, “Hello”: 0.025, “Hallo”: </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.737.1">0.025 ]</span></strong></span></p>
<p><span class="koboSpan" id="kobo.738.1">The smoothing reduces the model’s confidence and promotes better generalization to unseen data. </span><span class="koboSpan" id="kobo.738.2">With a clearer understanding of the loss function’s role, we can move on to the inference</span><a id="_idIndexMarker261"/><span class="koboSpan" id="kobo.739.1"> phase, where the trained model</span><a id="_idIndexMarker262"/><span class="koboSpan" id="kobo.740.1"> generates translations for new, </span><span class="No-Break"><span class="koboSpan" id="kobo.741.1">unseen data.</span></span></p>
<h3><span class="koboSpan" id="kobo.742.1">Inference</span></h3>
<p><span class="koboSpan" id="kobo.743.1">Having traversed the </span><a id="_idIndexMarker263"/><span class="koboSpan" id="kobo.744.1">training landscape, our trained model is now adept with optimized parameters to tackle the translation task. </span><span class="koboSpan" id="kobo.744.2">In the inference stage, these learned parameters are employed to translate new, unseen text. </span><span class="koboSpan" id="kobo.744.3">We will continue with our example phrase “</span><em class="italic"><span class="koboSpan" id="kobo.745.1">Hello, how are you?</span></em><span class="koboSpan" id="kobo.746.1">” to elucidate </span><span class="No-Break"><span class="koboSpan" id="kobo.747.1">this process.</span></span></p>
<p><span class="koboSpan" id="kobo.748.1">The inference stage is the practical application of the trained model on new data. </span><span class="koboSpan" id="kobo.748.2">The trained parameters, refined after numerous iterations during training, are now used to translate text from one language to another. </span><span class="koboSpan" id="kobo.748.3">The inference steps can be described </span><span class="No-Break"><span class="koboSpan" id="kobo.749.1">as follows:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.750.1">Input preparation</span></strong><span class="koboSpan" id="kobo.751.1">: Initially, our</span><a id="_idIndexMarker264"/><span class="koboSpan" id="kobo.752.1"> phrase “Hello, how are you?” </span><span class="koboSpan" id="kobo.752.2">is tokenized and encoded into a format that the model can process, akin to the preparation steps in the </span><span class="No-Break"><span class="koboSpan" id="kobo.753.1">training phase.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.754.1">Passing through the model</span></strong><span class="koboSpan" id="kobo.755.1">: The encoded input is then propagated through the model. </span><span class="koboSpan" id="kobo.755.2">As it navigates through the encoder and decoder stacks, the trained parameters guide the transformation of the input data, inching closer to accurate translations at </span><span class="No-Break"><span class="koboSpan" id="kobo.756.1">each step.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.757.1">Output generation</span></strong><span class="koboSpan" id="kobo.758.1">: At the culmination of the decoder stack, the model generates a probability distribution across the target vocabulary for each word in the input text. </span><span class="koboSpan" id="kobo.758.2">For the word “</span><em class="italic"><span class="koboSpan" id="kobo.759.1">Hello</span></em><span class="koboSpan" id="kobo.760.1">,” a probability distribution is formed over the target vocabulary, which, in our case, comprises French words. </span><span class="koboSpan" id="kobo.760.2">The word with the highest probability is selected as the translation. </span><span class="koboSpan" id="kobo.760.3">This process is replicated for each word in the phrase, rendering the translated output “</span><em class="italic"><span class="koboSpan" id="kobo.761.1">Bonjour, comment </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.762.1">ça va?</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.763.1">”.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.764.1">Now that we understand how the model produces the final output, we can implement a transformer model step by step to solidify the concepts we have discussed. </span><span class="koboSpan" id="kobo.764.2">However, before we dive into the code, we can briefly give a synopsis of the end-to-end </span><span class="No-Break"><span class="koboSpan" id="kobo.765.1">architecture </span></span><span class="No-Break"><a id="_idIndexMarker265"/></span><span class="No-Break"><span class="koboSpan" id="kobo.766.1">flow:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.767.1">Input tokenization</span></strong><span class="koboSpan" id="kobo.768.1">: The initial English phrase “</span><em class="italic"><span class="koboSpan" id="kobo.769.1">Hello, how are you?</span></em><span class="koboSpan" id="kobo.770.1">” is tokenized into </span><a id="_idIndexMarker266"/><span class="koboSpan" id="kobo.771.1">smaller units such as “</span><em class="italic"><span class="koboSpan" id="kobo.772.1">Hello</span></em><span class="koboSpan" id="kobo.773.1">,” “</span><em class="italic"><span class="koboSpan" id="kobo.774.1">,</span></em><span class="koboSpan" id="kobo.775.1">,” “</span><em class="italic"><span class="koboSpan" id="kobo.776.1">how</span></em><span class="koboSpan" id="kobo.777.1">,” and </span><span class="No-Break"><span class="koboSpan" id="kobo.778.1">so on.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.779.1">Embeddings</span></strong><span class="koboSpan" id="kobo.780.1">: These tokens are then mapped to continuous vector representations through an </span><span class="No-Break"><span class="koboSpan" id="kobo.781.1">embedding layer.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.782.1">Positional encoding</span></strong><span class="koboSpan" id="kobo.783.1">: To preserve the order of the sequence, positional encodings are added to </span><span class="No-Break"><span class="koboSpan" id="kobo.784.1">the embeddings.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.785.1">Encoder self-attention</span></strong><span class="koboSpan" id="kobo.786.1">: The embedded input sequence navigates through the encoder’s sequence of self-attention layers. </span><span class="koboSpan" id="kobo.786.2">Here, each word gauges the relevance of every other word to comprehend the </span><span class="No-Break"><span class="koboSpan" id="kobo.787.1">full context.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.788.1">FFN</span></strong><span class="koboSpan" id="kobo.789.1">: The representations are subsequently refined by position-wise FFNs within each </span><span class="No-Break"><span class="koboSpan" id="kobo.790.1">encoder layer.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.791.1">Encoder output</span></strong><span class="koboSpan" id="kobo.792.1">: The encoder renders contextual representations capturing the essence of the </span><span class="No-Break"><span class="koboSpan" id="kobo.793.1">input sequence.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.794.1">Decoder attention</span></strong><span class="koboSpan" id="kobo.795.1">: Incrementally, the decoder crafts the output sequence, employing self-attention solely on preceding words to maintain the </span><span class="No-Break"><span class="koboSpan" id="kobo.796.1">sequence order.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.797.1">Encoder-decoder attention</span></strong><span class="koboSpan" id="kobo.798.1">: The decoder evaluates the encoder’s output, centering on pertinent input context while generating each word in the </span><span class="No-Break"><span class="koboSpan" id="kobo.799.1">output sequence.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.800.1">Output layers</span></strong><span class="koboSpan" id="kobo.801.1">: The decoder feeds its output to the linear and SoftMax layers to produce “</span><em class="italic"><span class="koboSpan" id="kobo.802.1">Bonjour, comment </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.803.1">ça va?</span></em></span></li>
</ol>
<p><span class="koboSpan" id="kobo.804.1">At the end of this chapter, we will adapt a best-in-class implementation of the original transformer (Huang et al., 2022) into a minimal example that could later be trained on various downstream tasks. </span><span class="koboSpan" id="kobo.804.2">This will serve as a theoretical exercise to further solidify our understanding. </span><span class="koboSpan" id="kobo.804.3">In practice, we would rely on pre-trained or foundation models, which we will learn to implement in </span><span class="No-Break"><span class="koboSpan" id="kobo.805.1">later chapters.</span></span></p>
<p><span class="koboSpan" id="kobo.806.1">However, before we begin our practice project, we can trace its impact on the current landscape of GenAI. </span><span class="koboSpan" id="kobo.806.2">We </span><a id="_idIndexMarker267"/><span class="koboSpan" id="kobo.807.1">follow the trajectory of early </span><a id="_idIndexMarker268"/><span class="koboSpan" id="kobo.808.1">applications of the architecture (for example, </span><strong class="bold"><span class="koboSpan" id="kobo.809.1">Bidirectional Encoded Representations from Transformers</span></strong><span class="koboSpan" id="kobo.810.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.811.1">BERT</span></strong><span class="koboSpan" id="kobo.812.1">)) through to the </span><span class="No-Break"><span class="koboSpan" id="kobo.813.1">first GPT.</span></span></p>
<h1 id="_idParaDest-55"><a id="_idTextAnchor099"/><span class="koboSpan" id="kobo.814.1">Evolving language models – the AR Transformer and its role in GenAI</span></h1>
<p><span class="koboSpan" id="kobo.815.1">In </span><a href="B21773_02.xhtml#_idTextAnchor045"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.816.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.817.1">, we reviewed some of the generative paradigms that apply a </span><a id="_idIndexMarker269"/><span class="koboSpan" id="kobo.818.1">transformer-based</span><a id="_idIndexMarker270"/><span class="koboSpan" id="kobo.819.1"> approach. </span><span class="koboSpan" id="kobo.819.2">Here, we trace the </span><a id="_idIndexMarker271"/><span class="koboSpan" id="kobo.820.1">evolution of Transformers more closely, outlining some of the most impactful transformer-based language models from the initial transformer in 2017 to more recent state-of-the-art models that demonstrate the scalability, versatility, and societal considerations involved in this fast-moving domain of AI (as illustrated in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.821.1">Figure 3</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.822.1">.3</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.823.1">):</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer022">
<span class="koboSpan" id="kobo.824.1"><img alt="Figure 3.3: From the original transformer to GPT-4" src="image/B21773_03_03.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.825.1">Figure 3.3: From the original transformer to GPT-4</span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.826.1">2017 – Transformer</span></strong><span class="koboSpan" id="kobo.827.1">: The transformer model, introduced by Vaswani et al., was a paradigm shift in NLP, featuring self-attention layers that could process entire sequences of data in parallel. </span><span class="koboSpan" id="kobo.827.2">This architecture enabled the model to evaluate the importance of each word in a sentence relative to all other words, thereby enhancing the model’s ability to capture </span><span class="No-Break"><span class="koboSpan" id="kobo.828.1">the context.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.829.1">2018 – BERT</span></strong><span class="koboSpan" id="kobo.830.1">: Google’s BERT model innovated on the transformer architecture by utilizing a bidirectional context in its encoder layers during pre-training. </span><span class="koboSpan" id="kobo.830.2">It was one of the first models to understand the context of a word based on its entire sentence, both left and right, significantly improving performance on a wide range of NLP tasks, especially those requiring a deep understanding </span><span class="No-Break"><span class="koboSpan" id="kobo.831.1">of context.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.832.1">2018 – GPT-1</span></strong><span class="koboSpan" id="kobo.833.1">: OpenAI’s GPT-1 model was a milestone in NLP, adopting a generative pre-trained approach with a transformer’s decoder-only model. </span><span class="koboSpan" id="kobo.833.2">It was pre-trained on a diverse corpus of text data and fine-tuned for various tasks, using a</span><a id="_idIndexMarker272"/><span class="koboSpan" id="kobo.834.1"> unidirectional approach that generated text sequentially from left to right, which was particularly suited for generative </span><span class="No-Break"><span class="koboSpan" id="kobo.835.1">text applications.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.836.1">2019 – GPT-2</span></strong><span class="koboSpan" id="kobo.837.1">: GPT-2 built upon the foundation laid by GPT-1, maintaining its decoder-only</span><a id="_idIndexMarker273"/><span class="koboSpan" id="kobo.838.1"> architecture</span><a id="_idIndexMarker274"/><span class="koboSpan" id="kobo.839.1"> but significantly expanding its scale in terms of dataset and model size. </span><span class="koboSpan" id="kobo.839.2">This allowed GPT-2 to generate text that was more coherent and contextually relevant across a broader range of topics, demonstrating the power of scaling up </span><span class="No-Break"><span class="koboSpan" id="kobo.840.1">transformer models.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.841.1">2020 – GPT-3</span></strong><span class="koboSpan" id="kobo.842.1">: OpenAI’s GPT-3 pushed the boundaries of scale in transformer models to 175 billion parameters, enabling a wide range of tasks to be performed with minimal input, often with </span><strong class="bold"><span class="koboSpan" id="kobo.843.1">zero-shot learning</span></strong><span class="koboSpan" id="kobo.844.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.845.1">ZSL</span></strong><span class="koboSpan" id="kobo.846.1">) or </span><strong class="bold"><span class="koboSpan" id="kobo.847.1">few-shot learning</span></strong><span class="koboSpan" id="kobo.848.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.849.1">FSL</span></strong><span class="koboSpan" id="kobo.850.1">). </span><span class="koboSpan" id="kobo.850.2">This showed that Transformers could generalize across tasks and data types, often without the need for extensive task-specific data </span><span class="No-Break"><span class="koboSpan" id="kobo.851.1">or fine-tuning.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.852.1">2021 – InstructGPT</span></strong><span class="koboSpan" id="kobo.853.1">: An optimized variant of GPT-3, InstructGPT was fine-tuned specifically to follow user instructions and generate aligned responses, incorporating feedback loops that emphasized safety and relevance in its outputs. </span><span class="koboSpan" id="kobo.853.2">This represented a focus on creating AI models that could more accurately interpret and respond to </span><span class="No-Break"><span class="koboSpan" id="kobo.854.1">human prompts.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.855.1">2023 – GPT-4</span></strong><span class="koboSpan" id="kobo.856.1">: GPT-4 was an evolution of OpenAI’s transformer models into the multimodal space, capable of understanding and generating content based on both text and images. </span><span class="koboSpan" id="kobo.856.2">This model aimed to produce safer and more contextually nuanced responses, showcasing a significant advancement in the model’s ability to handle complex tasks and generate </span><span class="No-Break"><span class="koboSpan" id="kobo.857.1">creative content.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.858.1">2023 – LLaMA 2</span></strong><span class="koboSpan" id="kobo.859.1">: Meta AI’s LLaMA 2 was part of a suite of models that focused on efficiency and accessibility, allowing for high-performance language modeling while being more resource-efficient. </span><span class="koboSpan" id="kobo.859.2">This model was aimed at facilitating a broader range of research and application development within the </span><span class="No-Break"><span class="koboSpan" id="kobo.860.1">AI community.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.861.1">2023 – Claude 2</span></strong><span class="koboSpan" id="kobo.862.1">: Anthropic’s Claude 2 was an advancement over Claude 1, increasing its token context window and improving its reasoning and memory capabilities. </span><span class="koboSpan" id="kobo.862.2">It aimed to align more closely with human values, offering responsible and </span><a id="_idIndexMarker275"/><span class="koboSpan" id="kobo.863.1">nuanced generative capabilities for open-domain question-answering and other conversational AI applications, marking progress in ethical </span><span class="No-Break"><span class="koboSpan" id="kobo.864.1">AI development.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.865.1">The timeline </span><a id="_idIndexMarker276"/><span class="koboSpan" id="kobo.866.1">presented </span><a id="_idIndexMarker277"/><span class="koboSpan" id="kobo.867.1">highlights the remarkable progress in transformer-based language models over the past several years. </span><span class="koboSpan" id="kobo.867.2">What originated as an architecture that introduced the concept of self-attention has rapidly evolved into models with billions of parameters that can generate coherent text, answer questions, and perform a variety of intellectual tasks at high levels of performance. </span><span class="koboSpan" id="kobo.867.3">The increase in scale and accessibility of models such as GPT-4 has opened new possibilities for AI applications. </span><span class="koboSpan" id="kobo.867.4">At the same time, recent models have illustrated a focus on safety and ethics and providing more nuanced, helpful responses </span><span class="No-Break"><span class="koboSpan" id="kobo.868.1">to users.</span></span></p>
<p><span class="koboSpan" id="kobo.869.1">In the next section, we accomplish a rite of passage for practitioners with an interest in the NL field. </span><span class="koboSpan" id="kobo.869.2">We implement the key compo</span><a id="_idTextAnchor100"/><span class="koboSpan" id="kobo.870.1">nents of the original transformer architecture using Python to</span><a id="_idIndexMarker278"/><span class="koboSpan" id="kobo.871.1"> more </span><a id="_idIndexMarker279"/><span class="koboSpan" id="kobo.872.1">fully understand</span><a id="_idIndexMarker280"/><span class="koboSpan" id="kobo.873.1"> the mechanics that started </span><span class="No-Break"><span class="koboSpan" id="kobo.874.1">it all.</span></span></p>
<h1 id="_idParaDest-56"><a id="_idTextAnchor101"/><span class="koboSpan" id="kobo.875.1">Implementing the original Transformer</span></h1>
<p><span class="koboSpan" id="kobo.876.1">The following code </span><a id="_idIndexMarker281"/><span class="koboSpan" id="kobo.877.1">demonstrates how to implement a minimal transformer model for a Seq2Seq translation task, mainly translating English text to French. </span><span class="koboSpan" id="kobo.877.2">The code is structured into multiple sections, handling various aspects from data loading to model training </span><span class="No-Break"><span class="koboSpan" id="kobo.878.1">and translation.</span></span></p>
<h2 id="_idParaDest-57"><a id="_idTextAnchor102"/><span class="koboSpan" id="kobo.879.1">Data loading and preparation</span></h2>
<p><span class="koboSpan" id="kobo.880.1">Initially, the code</span><a id="_idIndexMarker282"/><span class="koboSpan" id="kobo.881.1"> loads a dataset and prepares it for training. </span><span class="koboSpan" id="kobo.881.2">The data is loaded from a CSV file, which is then split into English and French text. </span><span class="koboSpan" id="kobo.881.3">The text is limited to 100 characters for demonstration purposes to reduce training time. </span><span class="koboSpan" id="kobo.881.4">The CSV file includes a few thousand example data points and can be found in the book’s GitHub repository (</span><a href="https://github.com/PacktPublishing/Python-Generative-AI"><span class="koboSpan" id="kobo.882.1">https://github.com/PacktPublishing/Python-Generative-AI</span></a><span class="koboSpan" id="kobo.883.1">) along with the </span><span class="No-Break"><span class="koboSpan" id="kobo.884.1">complete code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.885.1">
import pandas as pd
import numpy as np
# Load demo data
data = pd.read_csv("./Chapter_3/data/en-fr_mini.csv")
# Separate English and French lexicons
EN_TEXT = data.en.to_numpy().tolist()
FR_TEXT = data.fr.to_numpy().tolist()
# Arbitrarily cap at 100 characters for demonstration to avoid long training times
def demo_limit(vocab, limit=100):
    return [i[:limit] for i in vocab]
EN_TEXT = demo_limit(EN_TEXT)
FR_TEXT = demo_limit(FR_TEXT)
# Establish the maximum length of a given sequence
MAX_LEN = 100</span></pre>
<h2 id="_idParaDest-58"><a id="_idTextAnchor103"/><span class="koboSpan" id="kobo.886.1">Tokenization</span></h2>
<p><span class="koboSpan" id="kobo.887.1">Next, a tokenizer is</span><a id="_idIndexMarker283"/><span class="koboSpan" id="kobo.888.1"> trained on the text data. </span><span class="koboSpan" id="kobo.888.2">The tokenizer is essential for converting text data into numerical data that can be fed into </span><span class="No-Break"><span class="koboSpan" id="kobo.889.1">the model:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.890.1">
from tokenizers import Tokenizer
from tokenizers.models import WordPiece
from tokenizers.trainers import WordPieceTrainer
from tokenizers.pre_tokenizers import Whitespace
def train_tokenizer(texts):
    tokenizer = Tokenizer(WordPiece(unk_token="[UNK]"))
    tokenizer.pre_tokenizer = Whitespace()
    trainer = WordPieceTrainer(
        vocab_size=5000,
        special_tokens=["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]", 
            "&lt;sos&gt;", "&lt;eos&gt;"],
    )
    tokenizer.train_from_iterator(texts, trainer)
    return tokenizer
en_tokenizer = train_tokenizer(EN_TEXT)
fr_tokenizer = train_tokenizer(FR_TEXT)</span></pre>
<h2 id="_idParaDest-59"><a id="_idTextAnchor104"/><span class="koboSpan" id="kobo.891.1">Data tensorization</span></h2>
<p><span class="koboSpan" id="kobo.892.1">The text data is then </span><a id="_idIndexMarker284"/><span class="koboSpan" id="kobo.893.1">tensorized, which involves converting the text data into tensor format. </span><span class="koboSpan" id="kobo.893.2">This step is crucial for preparing the data for training </span><span class="No-Break"><span class="koboSpan" id="kobo.894.1">with </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.895.1">PyTorch</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.896.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.897.1">
import torch
from torch.nn.utils.rnn import pad_sequence
def tensorize_data(text_data, tokenizer):
    numericalized_data = [
        torch.tensor(tokenizer.encode(text).ids) for text in text_data
    ]
    padded_data = pad_sequence(numericalized_data,
        batch_first=True)
    return padded_data
src_tensor = tensorize_data(EN_TEXT, en_tokenizer)
tgt_tensor = tensorize_data(FR_TEXT, fr_tokenizer)</span></pre>
<h2 id="_idParaDest-60"><a id="_idTextAnchor105"/><span class="koboSpan" id="kobo.898.1">Dataset creation</span></h2>
<p><span class="koboSpan" id="kobo.899.1">A custom dataset</span><a id="_idIndexMarker285"/><span class="koboSpan" id="kobo.900.1"> class is created to handle the data. </span><span class="koboSpan" id="kobo.900.2">This class is essential for loading the data in batches </span><span class="No-Break"><span class="koboSpan" id="kobo.901.1">during training:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.902.1">
from torch.utils.data import Dataset, DataLoader
class TextDataset(Dataset):
    def __init__(self, src_data, tgt_data):
        self.src_data = src_data
        self.tgt_data = tgt_data
    def __len__(self):
        return len(self.src_data)
    def __getitem__(self, idx):
        return self.src_data[idx], self.tgt_data[idx]
dataset = TextDataset(src_tensor, tgt_tensor)</span></pre>
<h2 id="_idParaDest-61"><a id="_idTextAnchor106"/><span class="koboSpan" id="kobo.903.1">Embeddings layer</span></h2>
<p><span class="koboSpan" id="kobo.904.1">The embeddings layer </span><a id="_idIndexMarker286"/><span class="koboSpan" id="kobo.905.1">maps each token to a continuous vector space. </span><span class="koboSpan" id="kobo.905.2">This layer is crucial for the model to understand and process the </span><span class="No-Break"><span class="koboSpan" id="kobo.906.1">text data:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.907.1">
import torch.nn as nn
class Embeddings(nn.Module):
    def __init__(self, d_model, vocab_size):
        super(Embeddings, self).__init__()
        self.embed = nn.Embedding(vocab_size, d_model)
    def forward(self, x):
        return self.embed(x)</span></pre>
<h2 id="_idParaDest-62"><a id="_idTextAnchor107"/><span class="koboSpan" id="kobo.908.1">Positional encoding</span></h2>
<p><span class="koboSpan" id="kobo.909.1">The positional </span><a id="_idIndexMarker287"/><span class="koboSpan" id="kobo.910.1">encoding layer adds positional information to the embeddings, which helps the model understand the order of tokens in </span><span class="No-Break"><span class="koboSpan" id="kobo.911.1">the sequence:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.912.1">
import math
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout=0.1,
                 max_len=MAX_LEN
    ):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0.0, max_len).unsqueeze(1)
        div_term = torch.exp(
            torch.arange(0.0, d_model, 2) * - \
                (math.log(10000.0) / d_model)
        )
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer("pe", pe)
    def forward(self, x):
        x = x + self.pe[:, : x.size(1)]
        return self.dropout(x)</span></pre>
<h2 id="_idParaDest-63"><a id="_idTextAnchor108"/><span class="koboSpan" id="kobo.913.1">Multi-head self-attention</span></h2>
<p><span class="koboSpan" id="kobo.914.1">The </span><strong class="bold"><span class="koboSpan" id="kobo.915.1">multi-head self-attention</span></strong><span class="koboSpan" id="kobo.916.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.917.1">MHSA</span></strong><span class="koboSpan" id="kobo.918.1">) layer is </span><a id="_idIndexMarker288"/><span class="koboSpan" id="kobo.919.1">a crucial </span><a id="_idIndexMarker289"/><span class="koboSpan" id="kobo.920.1">part of the transformer architecture that allows the model to focus on different parts of the input sequence when producing an </span><span class="No-Break"><span class="koboSpan" id="kobo.921.1">output sequence:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.922.1">
class MultiHeadSelfAttention(nn.Module):
    def __init__(self, d_model, nhead):
        super(MultiHeadSelfAttention, self).__init__()
        self.attention = nn.MultiheadAttention(d_model, nhead)
    def forward(self, x):
        return self.attention(x, x, x)</span></pre>
<h2 id="_idParaDest-64"><a id="_idTextAnchor109"/><span class="koboSpan" id="kobo.923.1">FFN</span></h2>
<p><span class="koboSpan" id="kobo.924.1">The FFN is</span><a id="_idIndexMarker290"/><span class="koboSpan" id="kobo.925.1"> a simple </span><strong class="bold"><span class="koboSpan" id="kobo.926.1">fully connected NN</span></strong><span class="koboSpan" id="kobo.927.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.928.1">FCNN</span></strong><span class="koboSpan" id="kobo.929.1">) that </span><a id="_idIndexMarker291"/><span class="koboSpan" id="kobo.930.1">operates independently on </span><span class="No-Break"><span class="koboSpan" id="kobo.931.1">each position:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.932.1">
class FeedForward(nn.Module):
    def __init__(self, d_model, d_ff):
        super(FeedForward, self).__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.dropout = nn.Dropout(0.1)
        self.linear2 = nn.Linear(d_ff, d_model)
    def forward(self, x):
        return self.linear2(self.dropout(torch.relu(self.linear1(x))))</span></pre>
<h2 id="_idParaDest-65"><a id="_idTextAnchor110"/><span class="koboSpan" id="kobo.933.1">Encoder layer</span></h2>
<p><span class="koboSpan" id="kobo.934.1">The encoder layer</span><a id="_idIndexMarker292"/><span class="koboSpan" id="kobo.935.1"> consists of an MHSA mechanism and a simple FFNN. </span><span class="koboSpan" id="kobo.935.2">This structure is repeated in a stack to form the </span><span class="No-Break"><span class="koboSpan" id="kobo.936.1">complete encoder:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.937.1">
class EncoderLayer(nn.Module):
    def __init__(self, d_model, nhead, d_ff):
        super(EncoderLayer, self).__init__()
        self.self_attn = MultiHeadSelfAttention(d_model, nhead)
        self.feed_forward = FeedForward(d_model, d_ff)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(0.1)
    def forward(self, x):
        x = x.transpose(0, 1)
        attn_output, _ = self.self_attn(x)
        x = x + self.dropout(attn_output)
        x = self.norm1(x)
        ff_output = self.feed_forward(x)
        x = x + self.dropout(ff_output)
        return self.norm2(x).transpose(0, 1)</span></pre>
<h2 id="_idParaDest-66"><a id="_idTextAnchor111"/><span class="koboSpan" id="kobo.938.1">Encoder</span></h2>
<p><span class="koboSpan" id="kobo.939.1">The encoder is a stack</span><a id="_idIndexMarker293"/><span class="koboSpan" id="kobo.940.1"> of identical layers with an MHSA mechanism and </span><span class="No-Break"><span class="koboSpan" id="kobo.941.1">an FFN:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.942.1">
class Encoder(nn.Module):
    def __init__(self, d_model, nhead, d_ff, num_layers, vocab_size):
        super(Encoder, self).__init__()
        self.embedding = Embeddings(d_model, vocab_size)
        self.pos_encoding = PositionalEncoding(d_model)
        self.encoder_layers = nn.ModuleList(
            [EncoderLayer(d_model, nhead, d_ff) for _ in range(
                num_layers)]
        )
        self.feed_forward = FeedForward(d_model, d_ff)
    def forward(self, x):
        x = self.embedding(x)
        x = self.pos_encoding(x)
        for layer in self.encoder_layers:
            x = layer(x)
        return x</span></pre>
<h2 id="_idParaDest-67"><a id="_idTextAnchor112"/><span class="koboSpan" id="kobo.943.1">Decoder layer</span></h2>
<p><span class="koboSpan" id="kobo.944.1">Similarly, the </span><a id="_idIndexMarker294"/><span class="koboSpan" id="kobo.945.1">decoder layer consists of two MHA mechanisms—one self-attention and one cross-attention—followed by </span><span class="No-Break"><span class="koboSpan" id="kobo.946.1">an FFN:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.947.1">
class DecoderLayer(nn.Module):
    def __init__(self, d_model, nhead, d_ff):
        super(DecoderLayer, self).__init__()
        self.self_attn = MultiHeadSelfAttention(d_model, nhead)
        self.cross_attn = nn.MultiheadAttention(d_model, nhead)
        self.feed_forward = FeedForward(d_model, d_ff)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(0.1)
    def forward(self, x, memory):
        x = x.transpose(0, 1)
        memory = memory.transpose(0, 1)
        attn_output, _ = self.self_attn(x)
        x = x + self.dropout(attn_output)
        x = self.norm1(x)
        attn_output, _ = self.cross_attn(x, memory, memory)
        x = x + self.dropout(attn_output)
        x = self.norm2(x)
        ff_output = self.feed_forward(x)
        x = x + self.dropout(ff_output)
        return self.norm3(x).transpose(0, 1)</span></pre>
<h2 id="_idParaDest-68"><a id="_idTextAnchor113"/><span class="koboSpan" id="kobo.948.1">Decoder</span></h2>
<p><span class="koboSpan" id="kobo.949.1">The decoder is also</span><a id="_idIndexMarker295"/><span class="koboSpan" id="kobo.950.1"> a stack of identical layers. </span><span class="koboSpan" id="kobo.950.2">Each layer contains two MHA mechanisms and </span><span class="No-Break"><span class="koboSpan" id="kobo.951.1">an FFN:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.952.1">
class Decoder(nn.Module):
    def __init__(self, d_model, nhead, d_ff, num_layers, vocab_size):
        super(Decoder, self).__init__()
        self.embedding = Embeddings(d_model, vocab_size)
        self.pos_encoding = PositionalEncoding(d_model)
        self.decoder_layers = nn.ModuleList(
            [DecoderLayer(d_model, nhead, d_ff) for _ in range(
                num_layers)]
        )
        self.linear = nn.Linear(d_model, vocab_size)
        self.softmax = nn.Softmax(dim=2)
    def forward(self, x, memory):
        x = self.embedding(x)
        x = self.pos_encoding(x)
        for layer in self.decoder_layers:
            x = layer(x, memory)
        x = self.linear(x)
        return self.softmax(x)</span></pre>
<p><span class="koboSpan" id="kobo.953.1">This stacking layer pattern continues to build the transformer architecture. </span><span class="koboSpan" id="kobo.953.2">Each block has a specific</span><a id="_idIndexMarker296"/><span class="koboSpan" id="kobo.954.1"> role in processing the input data and generating </span><span class="No-Break"><span class="koboSpan" id="kobo.955.1">output translations.</span></span></p>
<h2 id="_idParaDest-69"><a id="_idTextAnchor114"/><span class="koboSpan" id="kobo.956.1">Complete transformer</span></h2>
<p><span class="koboSpan" id="kobo.957.1">The transformer </span><a id="_idIndexMarker297"/><span class="koboSpan" id="kobo.958.1">model encapsulates the previously defined encoder and decoder structures. </span><span class="koboSpan" id="kobo.958.2">This is the primary class that will be used for training and </span><span class="No-Break"><span class="koboSpan" id="kobo.959.1">translation tasks:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.960.1">
class Transformer(nn.Module):
    def __init__(
        self,
        d_model,
        nhead,
        d_ff,
        num_encoder_layers,
        num_decoder_layers,
        src_vocab_size,
        tgt_vocab_size,
    ):
        super(Transformer, self).__init__()
        self.encoder = Encoder(d_model, nhead, d_ff, \
            num_encoder_layers, src_vocab_size)
        self.decoder = Decoder(d_model, nhead, d_ff, \
            num_decoder_layers, tgt_vocab_size)
    def forward(self, src, tgt):
        memory = self.encoder(src)
        output = self.decoder(tgt, memory)
        return output</span></pre>
<h2 id="_idParaDest-70"><a id="_idTextAnchor115"/><span class="koboSpan" id="kobo.961.1">Training function</span></h2>
<p><span class="koboSpan" id="kobo.962.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.963.1">train</span></strong><span class="koboSpan" id="kobo.964.1"> function </span><a id="_idIndexMarker298"/><span class="koboSpan" id="kobo.965.1">iterates through the </span><a id="_idIndexMarker299"/><span class="koboSpan" id="kobo.966.1">epochs and batches, calculates the loss, and updates the </span><span class="No-Break"><span class="koboSpan" id="kobo.967.1">model parameters:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.968.1">
def train(model, loss_fn, optimizer, NUM_EPOCHS=10):
    for epoch in range(NUM_EPOCHS):
        model.train()
        total_loss = 0
        for batch in batch_iterator:
            src, tgt = batch
            optimizer.zero_grad()
            output = model(src, tgt)
            loss = loss_fn(output.view(-1, TGT_VOCAB_SIZE),
                tgt.view(-1))
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"Epoch {epoch}, 
            Loss {total_loss / len(batch_iterator)}")</span></pre>
<h2 id="_idParaDest-71"><a id="_idTextAnchor116"/><span class="koboSpan" id="kobo.969.1">Translation function</span></h2>
<p><span class="koboSpan" id="kobo.970.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.971.1">translate</span></strong><span class="koboSpan" id="kobo.972.1"> function </span><a id="_idIndexMarker300"/><span class="koboSpan" id="kobo.973.1">uses the trained </span><a id="_idIndexMarker301"/><span class="koboSpan" id="kobo.974.1">model to translate a source text into the target language. </span><span class="koboSpan" id="kobo.974.2">It generates a translation token by token and stops when an </span><strong class="bold"><span class="koboSpan" id="kobo.975.1">end-of-sequence</span></strong><span class="koboSpan" id="kobo.976.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.977.1">EOS</span></strong><span class="koboSpan" id="kobo.978.1">) token </span><a id="_idIndexMarker302"/><span class="koboSpan" id="kobo.979.1">is generated or when the maximum target length </span><span class="No-Break"><span class="koboSpan" id="kobo.980.1">is reached:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.981.1">
def translate(model, src_text, src_tokenizer,
              tgt_tokenizer, max_target_length=50
):
    model.eval()
    src_tokens = src_tokenizer.encode(src_text).ids
    src_tensor = torch.LongTensor(src_tokens).unsqueeze(0)
    tgt_sos_idx = tgt_tokenizer.token_to_id("&lt;sos&gt;")
    tgt_eos_idx = tgt_tokenizer.token_to_id("&lt;eos&gt;")
    tgt_tensor = torch.LongTensor([tgt_sos_idx]).unsqueeze(0)
    for i in range(max_target_length):
        with torch.no_grad():
            output = model(src_tensor, tgt_tensor)
        predicted_token_idx = output.argmax(dim=2)[0, -1].item()
        if predicted_token_idx == tgt_eos_idx:
            break
        tgt_tensor = torch.cat((tgt_tensor,
            torch.LongTensor([[predicted_token_idx]])),
            dim=1)
    translated_token_ids = tgt_tensor[0, 1:].tolist()
    translated_text = tgt_tokenizer.decode(translated_token_ids)
    return translated_text</span></pre>
<h2 id="_idParaDest-72"><a id="_idTextAnchor117"/><span class="koboSpan" id="kobo.982.1">Main execution</span></h2>
<p><span class="koboSpan" id="kobo.983.1">In the main block</span><a id="_idIndexMarker303"/><span class="koboSpan" id="kobo.984.1"> of the script, hyperparameters are defined, the tokenizer and model are instantiated, and training and translation processes </span><span class="No-Break"><span class="koboSpan" id="kobo.985.1">are initiated:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.986.1">
if __name__ == "__main__":
    NUM_ENCODER_LAYERS = 2
    NUM_DECODER_LAYERS = 2
    DROPOUT_RATE = 0.1
    EMBEDDING_DIM = 512
    NHEAD = 8
    FFN_HID_DIM = 2048
    BATCH_SIZE = 31
    LEARNING_RATE = 0.001
    en_tokenizer = train_tokenizer(EN_TEXT)
    fr_tokenizer = train_tokenizer(FR_TEXT)
    SRC_VOCAB_SIZE = len(en_tokenizer.get_vocab())
    TGT_VOCAB_SIZE = len(fr_tokenizer.get_vocab())
    src_tensor = tensorize_data(EN_TEXT, en_tokenizer)
    tgt_tensor = tensorize_data(FR_TEXT, fr_tokenizer)
    dataset = TextDataset(src_tensor, tgt_tensor)
    model = Transformer(
        EMBEDDING_DIM,
        NHEAD,
        FFN_HID_DIM,
        NUM_ENCODER_LAYERS,
        NUM_DECODER_LAYERS,
        SRC_VOCAB_SIZE,
        TGT_VOCAB_SIZE,
    )
    loss_fn = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    batch_iterator = DataLoader(
        dataset, batch_size=BATCH_SIZE,
        shuffle=True, drop_last=True
    )
    train(model, loss_fn, optimizer, NUM_EPOCHS=10)
    src_text = "hello, how are you?"
</span><span class="koboSpan" id="kobo.986.2">    translated_text = translate(
        model, src_text, en_tokenizer, fr_tokenizer)
    print(translated_text)</span></pre>
<p><span class="koboSpan" id="kobo.987.1">This script orchestrates a machine translation task from loading data to training a transformer model and eventually translating text from English to French. </span><span class="koboSpan" id="kobo.987.2">Initially, it loads a dataset, processes the text, and establishes tokenizers to convert text to numerical data. </span><span class="koboSpan" id="kobo.987.3">Following</span><a id="_idIndexMarker304"/><span class="koboSpan" id="kobo.988.1"> this, it defines the architecture of a transformer model in </span><strong class="source-inline"><span class="koboSpan" id="kobo.989.1">PyTorch</span></strong><span class="koboSpan" id="kobo.990.1">, detailing each component from the embeddings’ self-attention mechanisms to the encoder and </span><span class="No-Break"><span class="koboSpan" id="kobo.991.1">decoder stacks.</span></span></p>
<p><span class="koboSpan" id="kobo.992.1">The script further organizes the data into batches, sets up a training loop, and defines a translation function. </span><span class="koboSpan" id="kobo.992.2">Training the model on the provided English and French sentences teaches it to map sequences from one language to another. </span><span class="koboSpan" id="kobo.992.3">Finally, it translates a sample sentence </span><a id="_idIndexMarker305"/><span class="koboSpan" id="kobo.993.1">from English to French to demonstrate the </span><span class="No-Break"><span class="koboSpan" id="kobo.994.1">model’s capabilities.</span></span></p>
<h1 id="_idParaDest-73"><a id="_idTextAnchor118"/><span class="koboSpan" id="kobo.995.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.996.1">The advent of the transformer significantly propelled the field of NLP forward, serving as the foundation for today’s cutting-edge generative language models. </span><span class="koboSpan" id="kobo.996.2">This chapter delineated the progression of NLP that paved the way for this pivotal innovation. </span><span class="koboSpan" id="kobo.996.3">Initial statistical techniques such as count vectors and TF-IDF were adept at extracting rudimentary word patterns, yet they fell short in grasping </span><span class="No-Break"><span class="koboSpan" id="kobo.997.1">semantic nuances.</span></span></p>
<p><span class="koboSpan" id="kobo.998.1">Incorporating neural language models marked a stride toward more profound representations through word embeddings. </span><span class="koboSpan" id="kobo.998.2">Nevertheless, recurrent networks encountered hurdles in handling longer sequences. </span><span class="koboSpan" id="kobo.998.3">This inspired the emergence of CNNs, which introduced computational efficacy via parallelism, albeit at the expense of global </span><span class="No-Break"><span class="koboSpan" id="kobo.999.1">contextual awareness.</span></span></p>
<p><span class="koboSpan" id="kobo.1000.1">The inception of attention mechanisms emerged as a cornerstone. </span><span class="koboSpan" id="kobo.1000.2">In 2017, Vaswani et al. </span><span class="koboSpan" id="kobo.1000.3">augmented these advancements, unveiling the transformer architecture. </span><span class="koboSpan" id="kobo.1000.4">The hallmark self-attention mechanism of the transformer facilitates contextual modeling across extensive sequences in a parallelized manner. </span><span class="koboSpan" id="kobo.1000.5">The layered encoder-decoder structure meticulously refines representations to discern relationships indispensable for endeavors such </span><span class="No-Break"><span class="koboSpan" id="kobo.1001.1">as translation.</span></span></p>
<p><span class="koboSpan" id="kobo.1002.1">The transformer, with its parallelizable and scalable self-attention design, set new benchmarks in performance. </span><span class="koboSpan" id="kobo.1002.2">Its core tenets are the architectural bedrock for contemporary high-achieving gen</span><a id="_idTextAnchor119"/><a id="_idTextAnchor120"/><span class="koboSpan" id="kobo.1003.1">erative language models such </span><span class="No-Break"><span class="koboSpan" id="kobo.1004.1">as GPT.</span></span></p>
<p><span class="koboSpan" id="kobo.1005.1">In the next chapter, we will discuss how to apply pre-trained generative models from prototype </span><span class="No-Break"><span class="koboSpan" id="kobo.1006.1">to production.</span></span></p>
<h1 id="_idParaDest-74"><a id="_idTextAnchor121"/><span class="koboSpan" id="kobo.1007.1">References</span></h1>
<p><span class="koboSpan" id="kobo.1008.1">This reference section serves as a repository of sources referenced within this book; you can explore these resources to further enhance your understanding and knowledge of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1009.1">subject matter:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.1010.1">Bahdanau, D., Cho, K., and Bengio, Y. </span><span class="koboSpan" id="kobo.1010.2">(2014). </span><em class="italic"><span class="koboSpan" id="kobo.1011.1">Neural machine translation by jointly learning to align and translate</span></em><span class="koboSpan" id="kobo.1012.1">. </span><span class="koboSpan" id="kobo.1012.2">arXiv </span><span class="No-Break"><span class="koboSpan" id="kobo.1013.1">preprint arXiv:1409.0473.</span></span></li>
<li><span class="koboSpan" id="kobo.1014.1">Bengio, Y., Ducharme, R., and Vincent, P. </span><span class="koboSpan" id="kobo.1014.2">(2003). </span><em class="italic"><span class="koboSpan" id="kobo.1015.1">A neural probabilistic language model. </span><span class="koboSpan" id="kobo.1015.2">The Journal of Machine Learning Research</span></em><span class="koboSpan" id="kobo.1016.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1017.1">3, 1137-1155.</span></span></li>
<li><span class="koboSpan" id="kobo.1018.1">Dadgar, S. </span><span class="koboSpan" id="kobo.1018.2">M. </span><span class="koboSpan" id="kobo.1018.3">H., Araghi, M. </span><span class="koboSpan" id="kobo.1018.4">S., and Farahani, M. </span><span class="koboSpan" id="kobo.1018.5">M. </span><span class="koboSpan" id="kobo.1018.6">(2016). </span><em class="italic"><span class="koboSpan" id="kobo.1019.1">Improving text classification performance based on TFIDF and LSI index</span></em><span class="koboSpan" id="kobo.1020.1">. </span><span class="koboSpan" id="kobo.1020.2">2016 IEEE International Conference on Engineering &amp; </span><span class="No-Break"><span class="koboSpan" id="kobo.1021.1">Technology (ICETECH).</span></span></li>
<li><span class="koboSpan" id="kobo.1022.1">Elman, J. </span><span class="koboSpan" id="kobo.1022.2">L. </span><span class="koboSpan" id="kobo.1022.3">(1990). </span><em class="italic"><span class="koboSpan" id="kobo.1023.1">Finding structure in time. </span><span class="koboSpan" id="kobo.1023.2">Cognitive science</span></em><span class="koboSpan" id="kobo.1024.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1025.1">14(2), 179-211.</span></span></li>
<li><span class="koboSpan" id="kobo.1026.1">Hochreiter, S., and Schmidhuber, J. </span><span class="koboSpan" id="kobo.1026.2">(1997). </span><em class="italic"><span class="koboSpan" id="kobo.1027.1">Long short-term memory. </span><span class="koboSpan" id="kobo.1027.2">Neural computation</span></em><span class="koboSpan" id="kobo.1028.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1029.1">9(8), 1735-1780.</span></span></li>
<li><span class="koboSpan" id="kobo.1030.1">Kim, Y. </span><span class="koboSpan" id="kobo.1030.2">(2014). </span><em class="italic"><span class="koboSpan" id="kobo.1031.1">Convolutional neural networks for sentence classification</span></em><span class="koboSpan" id="kobo.1032.1">. </span><span class="koboSpan" id="kobo.1032.2">arXiv </span><span class="No-Break"><span class="koboSpan" id="kobo.1033.1">preprint arXiv:1408.5882.</span></span></li>
<li><span class="koboSpan" id="kobo.1034.1">Mikolov, T., Sutskever, I., Chen, K., Corrado, G. </span><span class="koboSpan" id="kobo.1034.2">S., and Dean, J. </span><span class="koboSpan" id="kobo.1034.3">(2013). </span><em class="italic"><span class="koboSpan" id="kobo.1035.1">Distributed representations of words and phrases and their compositionality. </span><span class="koboSpan" id="kobo.1035.2">Advances in neural information processing </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1036.1">systems</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1037.1">, 26.</span></span></li>
<li><span class="koboSpan" id="kobo.1038.1">Pennington, J., Socher, R., and Manning, C. </span><span class="koboSpan" id="kobo.1038.2">(2014). </span><em class="italic"><span class="koboSpan" id="kobo.1039.1">GloVe: Global vectors for word representation. </span><span class="koboSpan" id="kobo.1039.2">Proceedings of the 2014 conference on empirical methods in natural language processing (</span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1040.1">EMNLP)</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1041.1">, 1532-1543.</span></span></li>
<li><span class="koboSpan" id="kobo.1042.1">Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. </span><span class="koboSpan" id="kobo.1042.2">N., ... </span><span class="koboSpan" id="kobo.1042.3">and Polosukhin, I. </span><span class="koboSpan" id="kobo.1042.4">(2017). </span><em class="italic"><span class="koboSpan" id="kobo.1043.1">Attention is all you need. </span><span class="koboSpan" id="kobo.1043.2">Advances in neural information processing </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1044.1">systems</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1045.1">, 30.</span></span></li>
</ul>
</div>
</body></html>