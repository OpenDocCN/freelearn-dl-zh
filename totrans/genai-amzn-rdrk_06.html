<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-113"><a id="_idTextAnchor117"/>6</h1>
<h1 id="_idParaDest-114"><a id="_idTextAnchor118"/>Generating and Summarizing Text with Amazon Bedrock</h1>
<p>In this chapter, we will explore architecture patterns for generating and summarizing text with Amazon Bedrock. You will learn about applications of text generation and how text generation works with Amazon Bedrock. Then, we will use some prompt engineering techniques, including contextual prompting, and orchestration using LangChain. After, we will explore text summarization using small texts/files, summarizing large articles and books, and discover use cases and patterns for text summarization.</p>
<p>By the end of this chapter, you will be able to understand and implement text generation and summarization with Amazon Bedrock in real-world use cases.</p>
<p>Here are the key topics that will be covered in this chapter:</p>
<ul>
<li><a id="_idTextAnchor119"/><a id="_idTextAnchor120"/>Generating text</li>
<li>Summarizing text<a id="_idTextAnchor121"/></li>
<li>Creating a secure serverless solution</li>
</ul>
<h1 id="_idParaDest-115"><a id="_idTextAnchor122"/>Technical requirements</h1>
<p>This chapter requires you to have access to an AWS account. If you don’t have one already, you can go to <a href="https://aws.amazon.com/getting-started/">https://aws.amazon.com/getting-started/</a> and create one.</p>
<p>Secondly, you will need to install and configure the AWS CLI (<a href="https://aws.amazon.com/cli/">https://aws.amazon.com/cli/</a>). You will use this to access Amazon Bedrock FMs from your local machine. Since the majority of the code cells we will be executing are based in Python, setting up an AWS Python SDK (Boto3) (<a href="https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html">https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html</a>) would be beneficial at this point. You can carry out the Python setup in any way: install it on your local machine, or use AWS Cloud9, or utilize AWS Lambda, or leverage Amazon SageMaker. If you’re using Jupyter Notebook with the AWS Python SDK to interact with Amazon Bedrock, make sure you run the following code cell in the notebook to import the essential libraries and create a Bedrock runtime client:</p>
<pre class="source-code">
#Ensure you have the latest version of boto3 &amp; langchain
!pip install -U boto3 langchain-community
#import the main packages and libraries
import boto3
import json
#Create bedrock runtime client
bedrock_client = boto3.client('bedrock-runtime') #Select the desired region</pre>
<p class="callout-heading">Note</p>
<p class="callout">There will be a charge associated with invoking and customizing the FMs of Amazon Bedrock. Please refer to <a href="https://aws.amazon.com/bedrock/pricing/">https://aws.amazon.com/bedrock/pricing/</a> to learn more.</p>
<h1 id="_idParaDest-116"><a id="_idTextAnchor123"/>Generating text</h1>
<p>Text generation <a id="_idIndexMarker488"/>plays a crucial role in various sectors, from marketing and advertising to journalism and creative writing. The significance of this technique lies in its capacity to streamline content creation processes, boost productivity, and unlock new realms of creativity.</p>
<p>One of the key advantages of text generation is its potential to save valuable time and resources. Traditional content creation methods can be time-consuming and labor-intensive, often requiring extensive research, writing, and editing efforts. But by using generative AI models, businesses and individuals can quickly produce initial drafts, outlines, or complete pieces of content, freeing up valuable time for other tasks.</p>
<p>Furthermore, text generation empowers content creators to explore new narrative avenues and push the boundaries of their creativity. By providing a starting point or a framework, these tools can spark fresh ideas and facilitate the exploration of unconventional storytelling techniques or unique writing styles. This capability is particularly valuable in industries where originality and distinctiveness are highly prized, such as fiction writing, advertising campaigns, or brand storytelling initiatives.</p>
<p>In addition to creative applications, text generation also holds immense potential in fields that demand high volumes of informative and factual content. For instance, news reporting, scientific publications, technical documentation, and text generation can aid in the rapid dissemination of accurate and up-to-date information. By leveraging vast data repositories and subject matter expertise, these tools can generate comprehensive reports, summaries, or articles, ensuring that relevant information is readily available to the intended audience.</p>
<p>Moreover, text generation offers exciting opportunities for personalization and customization. By analyzing user preferences, demographics, and contextual data, these tools can tailor content so that it resonates with specific target audiences, enhancing engagement and<a id="_idIndexMarker489"/> fostering stronger connections with readers or customers.</p>
<p>Let’s look at some real-world applications of text generation in detail.</p>
<h2 id="_idParaDest-117"><a id="_idTextAnchor124"/>Text generation applications</h2>
<p>While the <a id="_idIndexMarker490"/>applications of text generation are endless, here are a few examples to get you started:</p>
<ul>
<li><strong class="bold">Generating product descriptions</strong>: Amazon Bedrock’s text generation capabilities can be leveraged to automate the creation of product descriptions for marketing teams. By inputting the product’s features, specifications, and key benefits, the FM can generate compelling and SEO-optimized descriptions that highlight the unique selling points of the product. This can significantly streamline the process of creating product descriptions, saving time and resources for marketing teams.<p class="list-inset">The generated descriptions can be tailored to different target audiences, tone, and style preferences, ensuring a consistent and engaging brand voice across various channels. Additionally, the FM can be customized on existing product descriptions, allowing it to learn and mimic the desired writing style and formatting.</p></li>
<li><strong class="bold">Media articles and marketing campaigns generation</strong>: Amazon Bedrock’s text generation capabilities can be utilized for creating high-quality content for media articles, blog posts, and marketing campaigns. By providing relevant information, data, and guidelines, the FM can generate well-structured and coherent articles that can be used for content marketing, thought leadership, or news dissemination.<p class="list-inset">The FM can be trained on existing content, enabling it to understand and mimic the tone, style, and formatting preferences of specific publications or brands. It can also generate attention-grabbing headlines, engaging introductions, and compelling <strong class="bold">calls to action</strong> (<strong class="bold">CTAs</strong>) for<a id="_idIndexMarker491"/> marketing campaigns.</p></li>
<li><strong class="bold">Personalized email and message composition</strong>: Amazon Bedrock can be utilized to compose personalized emails, messages, and other written communications for customer outreach, marketing campaigns, or even internal communications. By <a id="_idIndexMarker492"/>leveraging customer data and preferences, the FM can generate highly tailored and engaging content, enhancing customer experience and increasing brand loyalty.</li>
<li><strong class="bold">Healthcare</strong>: Clinical documentation is a critical aspect of healthcare, but it can be time-consuming and prone to errors. Bedrock can assist healthcare professionals in streamlining the note-taking and documentation process by generating accurate and comprehensive clinical notes based on conversations or dictations during patient encounters. Amazon offers another service called <em class="italic">AWS HealthScribe</em> that’s powered by Amazon Bedrock and is specifically designed to do that. To learn more about AWS HealthScribe, go to <a href="https://aws.amazon.com/healthscribe/">https://aws.amazon.com/healthscribe/</a>.<p class="list-inset">Bedrock can be employed to generate personalized health and wellness recommendations tailored to an individual’s unique health profile, lifestyle, and preferences. By analyzing data from various sources, such as <strong class="bold">electronic health records</strong> (<strong class="bold">EHRs</strong>), wearable <a id="_idIndexMarker493"/>devices, and self-reported information, Bedrock can provide tailored <a id="_idIndexMarker494"/>recommendations for diet, exercise, stress management, and preventive care.</p></li>
</ul>
<h2 id="_idParaDest-118"><a id="_idTextAnchor125"/>Text generation systems with Amazon Bedrock</h2>
<p>If you have<a id="_idIndexMarker495"/> been following the previous <a id="_idIndexMarker496"/>chapters, you may have already tried generating text on Amazon Bedrock. But just as a reminder, a simple text generation system looks like this:</p>
<div><div><img alt="Figure 6.1 – Simple text generation system" src="img/B22045_06_01.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.1 – Simple text generation system</p>
<p>You provide a prompt to the model and say something like <code>Compose an email to a customer support team</code>. Even if you don’t provide any context, the model will generate a sample email for you (as shown in <em class="italic">Figure 6</em><em class="italic">.2</em>):</p>
<div><div><img alt="Figure 6.2 – Generating an email" src="img/B22045_06_02.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.2 – Generating an email</p>
<p>In your Jupyter Notebook environment with the AWS Python SDK, run the following sample script<a id="_idIndexMarker497"/> to invoke the AI21 Jurassic model. Make<a id="_idIndexMarker498"/> sure you import the essential libraries and create the Bedrock runtime client first, as mentioned in the <em class="italic">Technical </em><em class="italic">requirements</em> section:</p>
<pre class="source-code">
prompt_data = """Human: Compose an email to a customer support team.
Assistant:
"""
body = json.dumps({"prompt": prompt_data, "maxTokens": 200})
modelId = "ai21.j2-mid-v1"  # change this to use a different version from the model provider
accept = «application/json»
contentType = «application/json»
response = bedrock_client.invoke_model(body=body, modelId=modelId, accept=accept, contentType=contentType)
response_body = json.loads(response.get("body").read())
print(response_body.get("completions")[0].get("data").get("text"))</pre>
<p>Now, based on the model that you select, the response’s structure and output may vary. <em class="italic">Figure 6</em><em class="italic">.3</em> shows the response from the AI21 Jurassic model:</p>
<div><div><img alt="Figure 6.3 – AI21 Jurassic output" src="img/B22045_06_03.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.3 – AI21 Jurassic output</p>
<p>Here, we provided a simple prompt without providing any context or information. Now, let’s move on to <a id="_idIndexMarker499"/>the advanced architecture <a id="_idIndexMarker500"/>patterns of text generation and understand contextual prompting.</p>
<h2 id="_idParaDest-119"><a id="_idTextAnchor126"/>Generating text using prompt engineering</h2>
<p>In the previous<a id="_idIndexMarker501"/> section, we<a id="_idIndexMarker502"/> looked at a pattern of text generation where we did not provide any context or information to the model. Let’s use some of the prompt engineering techniques we learned about in <a href="B22045_03.xhtml#_idTextAnchor053"><em class="italic">Chapter 3</em></a>:</p>
<ul>
<li><strong class="bold">Zero-shot contextual prompting</strong>: Here, we will provide detailed context in the prompt in a zero-shot fashion:<pre class="source-code">
prompt = """</pre><pre class="source-code">
Human: Write a descriptive and engaging travel guide section about a lesser-known but beautiful destination, capturing the local culture, cuisine, and must-see attractions in a way that inspires wanderlust.</pre><pre class="source-code">
Assistant:"""</pre><pre class="source-code">
body = json.dumps({"prompt": prompt,"max_tokens_to_sample": 500})</pre><pre class="source-code">
modelId = "anthropic.claude-v2"  # change this to use a different version from the model provider</pre><pre class="source-code">
accept = «application/json»</pre><pre class="source-code">
contentType = «application/json»</pre><pre class="source-code">
response = bedrock_runtime.invoke_model(body=body, modelId=modelId, accept=accept, contentType=contentType)</pre><pre class="source-code">
response_body = json.loads(response.get("body").read())</pre><pre class="source-code">
print(response_body.get("completion"))</pre><p class="list-inset">Running the<a id="_idIndexMarker503"/> preceding <a id="_idIndexMarker504"/>code will generate a response similar to the one shown in <em class="italic">Figure 6</em><em class="italic">.4</em>:</p></li>
</ul>
<div><div><img alt="Figure 6.4 – Zero-shot contextual prompt response" src="img/B22045_06_04.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.4 – Zero-shot contextual prompt response</p>
<p class="list-inset">In the preceding scenario, we used the Amazon Bedrock API – <code>invoke_model</code> – and passed the prompt, configuration parameters, and model ID. If you want to learn more about the various Bedrock APIs that are available, you are encouraged to revisit <a href="B22045_02.xhtml#_idTextAnchor034"><em class="italic">Chapter 2</em></a>.</p>
<ul>
<li><strong class="bold">Few-shot contextual prompting</strong>: Here, we will provide some examples in our prompt <a id="_idIndexMarker505"/>so that the model can start to generate reasonable continuations:<pre class="source-code">
prompt = """</pre><pre class="source-code">
Human: Here are some examples of product descriptions:</pre><pre class="source-code">
Example 1:</pre><pre class="source-code">
Product: Apple iPhone 13 Pro</pre><pre class="source-code">
Description: The iPhone 13 Pro is a smartphone designed and manufactured by Apple Inc. It features a 6.1-inch Super Retina XDR display, a powerful A15 Bionic chip, and an advanced triple-camera system with improved low-light performance and 3x optical zoom. The phone also boasts 5G connectivity, longer battery life, and a durable Ceramic Shield front cover.</pre><pre class="source-code">
Example 2:</pre><pre class="source-code">
Product: Sony WH-1000XM4 Noise Cancelling Headphones</pre><pre class="source-code">
Description: Experience exceptional audio quality with the Sony WH-1000XM4 Noise Cancelling Headphones. These over-ear headphones feature industry-leading noise cancellation technology, allowing you to immerse yourself in your music without distractions. The responsive touch controls and long-lasting battery life make them ideal for everyday use, while the comfortable design ensures hours of listening pleasure.</pre><pre class="source-code">
Example 3:</pre><pre class="source-code">
Product: Instant Pot Duo Crisp + Air Fryer</pre><pre class="source-code">
Description: The Instant Pot Duo Crisp + Air Fryer is a versatile kitchen appliance that combines the functions of an electric pressure cooker, air fryer, and more. With its EvenCrisp technology, you can achieve crispy, golden results using little to no oil. The easy-to-use control panel and 11 built-in smart programs allow you to cook a wide variety of dishes with ease, making it a must-have for any modern kitchen.</pre><pre class="source-code">
Your task: Generate a product description for the following product:</pre><pre class="source-code">
Product: Sony A7 III Mirrorless Camera</pre><pre class="source-code">
Assistant:"""</pre><pre class="source-code">
body = json.dumps({"prompt": prompt, "max_tokens_to_sample": 500})</pre><pre class="source-code">
modelId = "anthropic.claude-v2"</pre><pre class="source-code">
accept = "application/json"</pre><pre class="source-code">
contentType = "application/json"</pre><pre class="source-code">
response = bedrock_runtime.invoke_model(body=body, modelId=modelId, accept=accept, contentType=contentType)</pre><pre class="source-code">
response_body = json.loads(response.get("body").read())</pre><pre class="source-code">
print(response_body.get("completion"))</pre></li>
</ul>
<p>Here, we provided <a id="_idIndexMarker506"/>three examples in <a id="_idIndexMarker507"/>our prompt to tell the model how our response should look. Then, we invoked the model to generate a product description for <code>Sony A7 III Mirrorless Camera</code>. We received the response shown in <em class="italic">Figure 6</em><em class="italic">.5</em>:</p>
<div><div><img alt="Figure 6.5 – Few-shot contextual prompting response" src="img/B22045_06_05.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.5 – Few-shot contextual prompting response</p>
<ul>
<li><code>invoke_model</code> API in this case):</li>
</ul>
<p class="callout-heading">Note</p>
<p class="callout">At the time of writing, we used <code>langchain_community.llms</code> library to import <em class="italic">Bedrock</em>. However, based on the updates from the LangChain community, it may be susceptible to change. For updated information on importing the LangChain package, please visit <a href="https://python.langchain.com/v0.2/docs/integrations/platforms/">https://python.langchain.com/v0.2/docs/integrations/platforms/</a>.</p>
<pre class="source-code">
from langchain_community.llms import Bedrock
inference_modifier = {"max_tokens_to_sample": 4096, "temperature": 0.5, "top_k": 250, "top_p": 1, "stop_sequences": ["\n\nHuman"],}
llm = Bedrock(model_id="anthropic.claude-v2",  client=bedrock_client, model_kwargs=inference_modifier,)
response = llm("""
Human: Write a descriptive and engaging travel guide section about a lesser-known but beautiful destination, capturing the local culture, cuisine, and must-see attractions in a way that inspires wanderlust.
Assistant:""")
print(response)
Figure 6</em><em class="italic">.6</em>):</pre>
<div><div><img alt="Figure 6.6 – Zero-shot prompting with LangChain" src="img/B22045_06_06.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.6 – Zero-shot prompting with LangChain</p>
<ul>
<li><strong class="bold">Contextual generation with LangChain</strong>: Here, we will provide instructions and <a id="_idIndexMarker510"/>context in <a id="_idIndexMarker511"/>our prompts before sending them to the model:<pre class="source-code">
from langchain_community.llms import Bedrock</pre><pre class="source-code">
inference_modifier = {'max_tokens_to_sample':4096, "temperature":0.5, "top_k":250, "top_p":1, "stop_sequences": ["\n\nHuman"]}</pre><pre class="source-code">
llm = Bedrock(model_id = "anthropic.claude-v2", client = boto3_bedrock, model_kwargs = inference_modifier)</pre><pre class="source-code">
from langchain.prompts import PromptTemplate</pre><pre class="source-code">
product_description_prompt = PromptTemplate(   input_variables=["product_name", "product_category", "key_features"],</pre><pre class="source-code">
    template="""</pre><pre class="source-code">
You are a professional copywriter tasked with creating an engaging and informative product description for a new Amazon product.</pre><pre class="source-code">
Product Name: {product_name}</pre><pre class="source-code">
Product Category: {product_category}</pre><pre class="source-code">
Key Features: {key_features}</pre><pre class="source-code">
Write a compelling product description that highlights the key features and benefits of the product, while keeping the tone engaging and persuasive for potential customers.</pre><pre class="source-code">
Product Description:</pre><pre class="source-code">
«»»</pre><pre class="source-code">
)</pre><pre class="source-code">
prompt = product_description_prompt.format(</pre><pre class="source-code">
    product_name="Smart Home Security Camera",</pre><pre class="source-code">
    product_category="Home Security",</pre><pre class="source-code">
    key_features="- 1080p HD video recording\n- Motion detection alerts\n- Two-way audio communication\n- Night vision capabilities\n- Cloud storage for recorded footage")</pre><pre class="source-code">
response = llm(prompt)</pre><pre class="source-code">
product = response[response.index('\n')+1:]</pre><pre class="source-code">
print(product)</pre><p class="list-inset">In this scenario, we used the LangChain implementation of Bedrock. We defined a prompt template for creating a product description and invoked the Anthropic Claude model to generate a product description of a smart home security camera. The prompt template is essentially a reusable template for constructing prompts. Within the prompt template, you can provide the context, input variables, task, and some few-shot examples for the model to reference. To learn more <a id="_idIndexMarker512"/>about prompt<a id="_idIndexMarker513"/> templates, go to <a href="https://python.langchain.com/v0.2/docs/concepts/#prompt-templates">https://python.langchain.com/v0.2/docs/concepts/#prompt-templates</a>.</p><p class="list-inset">The following figure shows the response from providing the preceding code snippet:</p></li>
</ul>
<div><div><img alt="Figure 6.7 – Contextual generation with LangChain" src="img/B22045_06_07.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.7 – Contextual generation with LangChain</p>
<p>Now that we’ve<a id="_idIndexMarker514"/> looked at various text<a id="_idIndexMarker515"/> generation patterns, let’s look at how we can perform summarization using Amazon Bedrock.</p>
<h1 id="_idParaDest-120"><a id="_idTextAnchor127"/>Summarizing text</h1>
<p>Text summarization is <a id="_idIndexMarker516"/>a highly sought-after capability that holds immense value across diverse domains. It involves the intricate task of condensing lengthy text documents into concise and coherent summaries that capture the essence of the original content. These summaries aim to preserve the most salient information while omitting redundant or irrelevant details, thereby enabling efficient consumption and comprehension of extensive textual data.</p>
<p>Text summarization finds applications in a wide range of sectors, from research and academia to journalism, business intelligence, and legal documentation. With the exponential growth of textual data generated daily, the need for effective summarization techniques has become increasingly paramount. Imagine sifting through voluminous reports, news articles, or legal documents – text summarization emerges as a powerful tool to distill the core information, saving time and cognitive effort for professionals and researchers alike.</p>
<p>Let’s look at some of the<a id="_idIndexMarker517"/> real-world applications of text summarization:</p>
<ul>
<li><strong class="bold">Content curation</strong>: In today’s information-rich world, text summarization plays a pivotal role in curating and condensing vast amounts of data. This allows users to quickly grasp the essence of lengthy articles, reports, or online content without having to read every word.</li>
<li><strong class="bold">News aggregation</strong>: News aggregators and media platforms can leverage text summarization to provide concise summaries of breaking news stories, enabling users to stay informed about the latest developments without getting bogged down by extensive details.</li>
<li><strong class="bold">Research assistance</strong>: Researchers and academics can benefit from text summarization techniques to quickly identify the most pertinent information from a vast corpus of literature, saving them valuable time and effort.</li>
<li><strong class="bold">Customer service</strong>: Text summarization can enhance customer service by automatically generating concise summaries of lengthy customer inquiries or feedback, allowing support agents to quickly comprehend the crux of the issue and provide timely responses.</li>
<li><strong class="bold">Legal and financial domains</strong>: In industries where accurate representation of original text is critical, such as legal or financial sectors, text summarization techniques can be employed to generate summaries of contracts, agreements, or reports, ensuring that key information is not overlooked.</li>
<li><strong class="bold">Email management</strong>: Email clients or productivity tools can leverage text summarization to provide concise overviews of long email threads or conversations, helping users quickly grasp the key points without having to read through every message.</li>
<li><strong class="bold">Meeting recap</strong>: Text summarization can be applied to meeting transcripts or notes, generating succinct summaries that capture the most important discussions, decisions, and action items, enabling participants to quickly review and follow up on critical points.</li>
<li><strong class="bold">Social media monitoring</strong>: Businesses and organizations can utilize text summarization to analyze and summarize vast amounts of social media data, such as customer<a id="_idIndexMarker518"/> feedback, product reviews, or brand mentions, enabling them to stay informed about public sentiment and respond promptly.</li>
<li><strong class="bold">Knowledge extraction</strong>: Text summarization techniques can be used to extract and summarize relevant knowledge from large datasets or knowledge bases, making it easier to access and leverage valuable information for various applications, such as decision-making or knowledge management systems.</li>
<li><strong class="bold">Educational resources</strong>: Text summarization can be applied to educational materials, such as textbooks or online courses, to generate concise summaries or study aids, helping students grasp key concepts and prepare for exams more efficiently.</li>
</ul>
<p>While the list of applications is endless and spans across every industry, let’s look at how summarization systems work with Amazon Bedrock. We will learn about two approaches:</p>
<ul>
<li>Summarization of small files</li>
<li>Summarization of large files</li>
</ul>
<p>By small files, we mean pieces of text that fit into the context length of the model. This could range from a couple of sentences to a few paragraphs. On the other hand, by large files, we mean large documents or book(s) worth of information that does not fit into the context length of the model. It is important to note that there is no one-size-fits-all that works <a id="_idIndexMarker519"/>across all models. Every model, including their different versions, might have a different context length. For example, Cohere Command R+ has a context length of 128K tokens, while Cohere Command Light has a context length of 4,000 tokens.</p>
<h2 id="_idParaDest-121"><a id="_idTextAnchor128"/>Summarization of small files</h2>
<p>Small files can include <a id="_idIndexMarker520"/>meeting notes, blog posts, news articles, email messages, and call transcripts. These files are then used as a context for the prompt and sent to the model. The prompt here could be as simple as <code>Summarize the content</code>. The model then processes the file and provides you with the summarized response. <em class="italic">Figure 6</em><em class="italic">.8</em> shows the process of small file summarization:</p>
<div><div><img alt="Figure 6.8 – Small file summarization" src="img/B22045_06_08.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.8 – Small file summarization</p>
<p>Let’s consider an example of a news article from Yahoo! Finance. Since the news article fits into the context length of the model, we will use that as a context in the prompt, <code>Summarize the following news article</code>, and send it to the model. The model will then process the request and provide the summarized response, as shown in <em class="italic">Figure 6</em><em class="italic">.9</em>:</p>
<div><div><img alt="Figure 6.9 – Summarization of a news article" src="img/B22045_06_09.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.9 – Summarization of a news article</p>
<p>There are a couple <a id="_idIndexMarker521"/>of ways to summarize small files in Bedrock. If you’re using the AWS Python SDK, you can pass the small file text into the prompt directly, as shown in the following code. However, if you would like to summarize a couple of paragraphs, you can utilize <em class="italic">prompt templates</em> to place the text dynamically within the prompts and use LangChain to invoke the model:</p>
<pre class="source-code">
prompt = """
Today we're going to take a look at the well-established Amazon.com, Inc. (NASDAQ:AMZN). The company's stock led the NASDAQGS gainers with a relatively large price hike in the past couple of weeks. The recent jump in the share price has meant that the company is trading at close to its 52-week high. With many analysts covering the large-cap stock, we may expect any price-sensitive announcements have already been factored into the stock's share price. But what if there is still an opportunity to buy? Let's take a look at Amazon.com's outlook and value based on the most recent financial data to see if the opportunity still exists.
Check out our latest analysis for Amazon.com
What's The Opportunity In Amazon.com?
Great news for investors – Amazon.com is still trading at a fairly cheap price. According to our valuation, the intrinsic value for the stock is $238.66, but it is currently trading at US$174 on the share market, meaning that there is still an opportunity to buy now. What's more interesting is that, Amazon.com's share price is quite volatile, which gives us more chances to buy since the share price could sink lower (or rise higher) in the future. This is based on its high beta, which is a good indicator for how much the stock moves relative to the rest of the market.
Can we expect growth from Amazon.com?
earnings-and-revenue-growth
earnings-and-revenue-growth
Future outlook is an important aspect when you're looking at buying a stock, especially if you are an investor looking for growth in your portfolio. Buying a great company with a robust outlook at a cheap price is always a good investment, so let's also take a look at the company's future expectations. With profit expected to more than double over the next couple of years, the future seems bright for Amazon.com. It looks like higher cash flow is on the cards for the stock, which should feed into a higher share valuation.
What This Means For You
Are you a shareholder? Since AMZN is currently undervalued, it may be a great time to accumulate more of your holdings in the stock. With a positive outlook on the horizon, it seems like this growth has not yet been fully factored into the share price. However, there are also other factors such as financial health to consider, which could explain the current undervaluation.
Are you a potential investor? If you've been keeping an eye on AMZN for a while, now might be the time to enter the stock. Its buoyant future outlook isn't fully reflected in the current share price yet, which means it's not too late to buy AMZN. But before you make any investment decisions, consider other factors such as the strength of its balance sheet, in order to make a well-informed investment decision.
Diving deeper into the forecasts for Amazon.com mentioned earlier will help you understand how analysts view the stock going forward. Luckily, you can check out what analysts are forecasting by clicking here.
If you are no longer interested in Amazon.com, you can use our free platform to see our list of over 50 other stocks with a high growth potential.
"""
body = json.dumps({"inputText": prompt,
                   "textGenerationConfig":{
                       "maxTokenCount":4096,
                       "stopSequences":[],
                       "temperature":0,
                       "topP":1
                   },
                  })
modelId = 'amazon.titan-tg1-large' # change this to use a different version from the model provider
accept = 'application/json'
contentType = 'application/json'
response = bedrock_client.invoke_model(body=body, modelId=modelId, accept=accept, contentType=contentType)
response_body = json.loads(response.get('body').read())
print(response_body.get('results')[0].get('outputText'))</pre>
<p>The response is shown in <em class="italic">Figure 6</em><em class="italic">.10</em>:</p>
<div><div><img alt="Figure 6.10 – Small file summarization response" src="img/B22045_06_10.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.10 – Small file summarization response</p>
<p>We have parsed a news article (<a href="https://finance.yahoo.com/news/us-174-time-put-amazon-110026932.html">https://finance.yahoo.com/news/us-174-time-put-amazon-110026932.html</a>) from Yahoo! Finance as a sample context to the prompt and invoked the Titan text model to generate the summarized response, as shown in the preceding<a id="_idIndexMarker522"/> figure.</p>
<p>Now, let’s look at the techniques for summarizing large files.</p>
<h2 id="_idParaDest-122"><a id="_idTextAnchor129"/>Summarization of large files</h2>
<p>Large files ca<a id="_idIndexMarker523"/>n include large documents or book(s) worth<a id="_idIndexMarker524"/> of information that do not fit into the context length of the model. When we say large documents, this includes 10-K reports, <strong class="bold">Federal Open Market Committee</strong> (<strong class="bold">FOMC</strong>) reports, public <a id="_idIndexMarker525"/>health reports, clinical study reports, e-magazines, service documentation, and more. The 10-K report for Amazon is an example of a large file: <a href="https://www.sec.gov/Archives/edgar/data/1018724/000101872424000008/amzn-20231231.htm">https://www.sec.gov/Archives/edgar/data/1018724/000101872424000008/amzn-20231231.htm</a>.</p>
<p>When working with large files for summarizing text, several challenges are involved:</p>
<ul>
<li><strong class="bold">Context length limitations</strong>: All FMs, such as the ones used in Amazon Bedrock, have a maximum context length or input size that they can process at once. This limit varies from model to model, but it is typically in the range of a few thousand tokens (words or word pieces). For example, you can find FMs such as the Anthropic Claude 3 family with 200k tokens. When working with large documents that exceed this context length, it becomes impossible to summarize the entire document accurately and coherently. The model may miss important information or fail to capture the overall context and nuances present in the original text.</li>
<li><strong class="bold">Hallucinations</strong>: Hallucination is a phenomenon where models generate output that is<a id="_idIndexMarker526"/> not grounded in the input data or contains factual inconsistencies. This issue can become more prevalent when dealing with large documents as the model might struggle to maintain coherence and faithfulness to the original text. As the<a id="_idIndexMarker527"/> input <a id="_idIndexMarker528"/>size increases, the model may start generating plausible-sounding but factually incorrect information, potentially leading to inaccurate summaries.</li>
<li><strong class="bold">Memory and computational constraints</strong>: Summarizing large documents can be computationally intensive and may require significant memory resources. Generative AI models need to process and store the entire input text, as well as intermediate representations and generated outputs. When working with very large documents, you might experience performance degradation due to the high computational demands if they’re not handled with dedicated compute capacity (see the <em class="italic">Provisioned throughput architecture</em> section in <a href="B22045_12.xhtml#_idTextAnchor226"><em class="italic">Chapter 12</em></a>).</li>
<li><strong class="bold">Context understanding</strong>: Large documents often contain complex structures, such as sections, subsections, and cross-references. Generative AI models may struggle to accurately capture and understand the relationships and dependencies between different parts of the document. This can lead to summaries that lack coherence or fail to accurately represent the overall structure and flow of the original content.</li>
<li><strong class="bold">Topic drift and coherence</strong>: As the length of the input text increases, it becomes more challenging for the models to maintain focus and coherence throughout the summarization process. The model may drift away from the main topic or fail to properly connect and transition between different aspects of the document, resulting in summaries that lack cohesion or clarity.</li>
</ul>
<p>To address these <a id="_idIndexMarker529"/>challenges, let’s look at <a id="_idIndexMarker530"/>summarizing large files using LangChain.</p>
<h3>Text summarization using LangChain’s summarization chain</h3>
<p>Using LangChain, we <a id="_idIndexMarker531"/>are going to <a id="_idIndexMarker532"/>break down large files into smaller, more manageable chunks and process them sequentially. <em class="italic">Figure 6</em><em class="italic">.11</em> shows the architecture of large text summarization using LangChain:</p>
<div><div><img alt="Figure 6.11 – Large file summarization in LangChain" src="img/B22045_06_11.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.11 – Large file summarization in LangChain</p>
<p>Here’s how the process works:</p>
<ol>
<li><strong class="bold">Data ingestion</strong>: The first step in the process is to load a large document or file into the system. This involves loading the file from an Amazon S3 bucket or downloading it directly from the internet. The files you can provide can be in the form of text, PDF, Word documents, and more.</li>
<li><code>RecursiveCharacterTextSplitter</code> is recommended for general text, as per the LangChain documentation: <a href="https://python.langchain.com/v0.2/docs/how_to/recursive_text_splitter/">https://python.langchain.com/v0.2/docs/how_to/recursive_text_splitter/</a>.<p class="list-inset">It recursively splits the text into smaller chunks until each chunk’s size falls below a specified threshold. The splitting process leverages separators (<code>"\n\n"</code>, <code>"\n"</code>), which ensures that individual paragraphs remain intact within a single chunk, rather <a id="_idIndexMarker533"/>than <a id="_idIndexMarker534"/>being fragmented across multiple chunks.</p></li>
<li><code>stuff</code>: As the name suggests, this chain stuffs all the chunks into a single prompt.</li><li><code>map_reduce</code>: The map-reduce chain is a powerful pattern that allows you to split a large task into smaller subtasks, process them independently, and then combine the results. In the context of text summarization, this chain type is used to break down a long text document into smaller chunks, summarize each chunk independently using an LLM, and then combine the summaries into a final summarized output.</li><li><code>refine</code>: This chain starts by summarizing the first chunk. Then, <code>refine</code> takes this summary and combines it with the second chunk to generate a new summary that encompasses both pieces of information. This process continues, where the latest summary is combined with the next chunk, and a new summary is generated. This iterative approach repeats until all the chunks have been incorporated into the final summary.</li></ul><p class="list-inset">To load any of these summarization chains, you can call <code>load_summarize_chain</code> and provide the chain type:</p><pre class="source-code">
from langchain.chains.summarize import load_summarize_chain</pre><pre class="source-code">
summary_chain = load_summarize_chain(llm=llm, chain_type="map_reduce", verbose=False)</pre></li>
<li><strong class="bold">Final summary</strong>: Based on the summarization chain you select and once all the chunks have been processed, the final summary represents a condensed version of the entire original document.</li>
</ol>
<p>The notebook at <a href="https://github.com/aws-samples/amazon-bedrock-workshop/blob/main/06_OpenSource_examples/00_Langchain_TextGeneration_examples/05_long-text-summarization-titan%20Langchain.ipynb">https://github.com/aws-samples/amazon-bedrock-workshop/blob/main/06_OpenSource_examples/00_Langchain_TextGeneration_examples/05_long-text-summarization-titan%20Langchain.ipynb</a> showcases the use of long text <a id="_idIndexMarker535"/>summarization<a id="_idIndexMarker536"/> using LangChain. In this example, it uses <code>map_reduce</code> as the chain type. We recommend that you try out different chain types and provide any blog posts, files, or news articles as a prompt.</p>
<p>Now that we’ve summarized large files using the LangChain chain type, let’s say we want to summarize a whole book or multiple books’ worth of information. In such scenarios, where large manuscripts or books need to be summarized, the RAG approach can be potentially beneficial. However, please note that the summarized response might not contain some essential elements from the book – in other words, there could be information loss. Various advanced RAG techniques, such as query refinement, can be utilized to retrieve the summarized response and essential elements from the text. To learn more about query refinement for RAG, please take a look at the paper <em class="italic">RQ-RAG: Learning to Refine Queries for Retrieval Augmented </em><em class="italic">Generation</em> (<a href="https://arxiv.org/html/2404.00610v1">https://arxiv.org/html/2404.00610v1</a>).</p>
<p>To learn more about how RAG works and some advanced RAG techniques, please refer to <a href="B22045_05.xhtml#_idTextAnchor090"><em class="italic">Chapter 5</em></a>.</p>
<p>Next, we’ll look at <a id="_idIndexMarker537"/>text summarization<a id="_idIndexMarker538"/> via Amazon Bedrock Knowledge Base.</p>
<h3>Amazon Bedrock Knowledge Base</h3>
<p>In <a href="B22045_05.xhtml#_idTextAnchor090"><em class="italic">Chapter 5</em></a>, we<a id="_idIndexMarker539"/> looked at how <a id="_idIndexMarker540"/>Amazon Bedrock Knowledge Base works and how to set it up. Let’s see an example of summarization using Knowledge Base.</p>
<p>We have put the <em class="italic">Attention is All You Need</em> research paper in our data store Amazon S3 bucket and synced it with our Bedrock Knowledge base, as shown in <em class="italic">Figure 6</em><em class="italic">.12</em>:</p>
<div><div><img alt="Figure 6.12 – Knowledge Base data source" src="img/B22045_06_12.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.12 – Knowledge Base data source</p>
<p>Select the model and provide a prompt to summarize the content. You will see the response from the LLM, as shown in <em class="italic">Figure 6</em><em class="italic">.13</em>:</p>
<div><div><img alt="Figure 6.13 – Test Knowledge base" src="img/B22045_06_13.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.13 – Test Knowledge base</p>
<p>If you would like to try this via APIs, you can<a id="_idIndexMarker541"/> call the <strong class="bold">Retrieve</strong> API or the <strong class="bold">RetrieveAndGenerate</strong> API. The<a id="_idIndexMarker542"/> Retrieve API accesses and retrieves the relevant data from Knowledge Base, whereas the RetrieveAndGenerate API, in addition to retrieving the data, generates the response based on the retrieved results. For more details on Amazon Bedrock Knowledge Base, please refer to <a href="B22045_05.xhtml#_idTextAnchor090"><em class="italic">Chapter 5</em></a>.</p>
<p>In this section, we discussed how to utilize text summarization systems with Amazon Bedrock. Summarizing small files is straightforward and involves utilizing the model’s context length. However, summarizing large files requires chunking and specialized techniques such as LangChain’s summarization chains, RAG, or Amazon Bedrock Knowledge Base to handle context length limitations, hallucination, computational constraints, and coherence issues.</p>
<p>Now that we have looked at generating and summarizing text using Amazon Bedrock, let’s look at how <a id="_idIndexMarker543"/>organizations<a id="_idIndexMarker544"/> can use these techniques and create a secure serverless solution involving other AWS services.</p>
<h1 id="_idParaDest-123"><a id="_idTextAnchor130"/>Creating a secure serverless solution</h1>
<p>When working<a id="_idIndexMarker545"/> with Generative AI<a id="_idIndexMarker546"/> models from Amazon Bedrock, organizations can develop an application that is secure and serverless. Instead of interacting directly with Amazon Bedrock using SDKs, they can have an interactive chatbot that abstracts away any complexity, provides a rich customer experience, and boosts overall productivity.</p>
<p><em class="italic">Figure 6</em><em class="italic">.14</em> shows an architecture diagram of how the user can interact with the web-based chatbot developed via AWS Amplify and have conversations, generate text in various forms, and perform language translation, text summarization, and more:</p>
<div><div><img alt="Figure 6.14 – Serverless enterprise application with Amazon Bedrock" src="img/B22045_06_14.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.14 – Serverless enterprise application with Amazon Bedrock</p>
<p>Let’s take a closer look at this process:</p>
<ol>
<li><strong class="bold">User interaction with the chatbot on AWS Amplify</strong>: AWS Amplify is a comprehensive set of tools and services that simplify the development and deployment of full-stack cloud-powered web and mobile applications. The user initiates the workflow by interacting with a chatbot integrated into a web application developed using AWS Amplify.</li>
<li><strong class="bold">User authentication and authorization with Amazon Cognito</strong>: Amazon Cognito is a robust user identity management service provided by AWS. When the user interacts with the chatbot, AWS communicates with Amazon Cognito to perform user authentication and authorization. Amazon Cognito supports various authentication methods, including traditional username/password combinations, social identity providers (for example, Google or Facebook), and multi-factor authentication. It also provides features for user registration, account recovery, and secure storage of user data.</li>
<li><strong class="bold">API Gateway as a centralized entry point</strong>: Once the user has been authenticated and authorized, their request is routed through an API gateway, which acts as a centralized entry point for APIs. API Gateway is a fully managed service that simplifies the process of creating, publishing, maintaining, monitoring, and securing APIs.</li>
<li><code>/text</code>) to an AWS Lambda function that performs invocation calls to Amazon Bedrock LLMs:<ol><li class="upper-roman">This Lambda function will take the user’s input or prompt and pass it to Amazon<a id="_idIndexMarker547"/> Bedrock<a id="_idIndexMarker548"/> LLMs to generate relevant and coherent text. For example, the user can ask to generate an email or prepare a travel itinerary for a particular destination.</li><li class="upper-roman">Once the Amazon Bedrock LLMs have generated the requested text, the Lambda function receives the response and sends it back to the user through API Gateway. Here, API Gateway acts as an intermediary, facilitating the communication between the client (that is, the chatbot) and the backend services (Lambda functions and Amazon Bedrock LLMs).</li></ol></li>
<li><code>/summarize</code>) to another AWS Lambda function specifically designed to perform invocation calls to Amazon Bedrock LLMs for summarization tasks:<ol><li class="upper-roman">This Lambda function performs invocation calls to Amazon Bedrock LLMs to summarize text based on the user’s input or prompt and the provided context (small or large files).</li><li class="upper-roman">After the Amazon Bedrock LLM has generated the summarized text, the Lambda function receives the response and sends it back to the user via API Gateway.</li></ol></li>
</ol>
<p>By separating the text generation and summarization tasks into different Lambda functions and API Gateway routes, the application can efficiently handle different types of requests and leverage the specialized capabilities of Amazon Bedrock LLMs for each task.</p>
<p>This workflow highlights the flexibility and modular nature of AWS services, allowing multiple components to be integrated to build complex applications. AWS Lambda functions act as computational engines that make invocation calls to Amazon Bedrock LLMs to perform text generation and summarization.</p>
<p>By breaking down the application into smaller, independent components, developers can easily maintain, update, and scale individual parts of the system without affecting the entire application.</p>
<p>If you’re curious about trying out the serverless chatbot with Amazon Bedrock, check out <a href="https://github.com/aws-samples/amazon-serverless-chatbot-using-bedrock">https://github.com/aws-samples/amazon-serverless-chatbot-using-bedrock</a>.</p>
<p>At this point, you<a id="_idIndexMarker549"/> should<a id="_idIndexMarker550"/> understand and be able to implement text generation and summarization with Amazon Bedrock in real-world use cases.</p>
<h1 id="_idParaDest-124"><a id="_idTextAnchor131"/>Summary</h1>
<p>In this chapter, we dived into the architecture patterns for generating and summarizing text using Amazon Bedrock. The first part of this chapter covered text generation. We looked at the fundamentals of text generation through prompt engineering techniques, in-line context training, and orchestration with LangChain. Then, we explored various use cases and patterns for text generation that you can apply to real-world scenarios.</p>
<p>The second part of this chapter covered text summarization. We discussed both extractive and abstractive summarization approaches and their respective applications. Furthermore, we examined the systems and techniques that can be employed for text summarization using Amazon Bedrock.</p>
<p>In the next chapter, we will explore building question-answering and conversational interfaces.</p>
</div>
</body></html>