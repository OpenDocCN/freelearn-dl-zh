["```py\nX is just another third-world country with nothing but drug lords and poverty-stricken people. The people there are uneducated and violent, and they don't have any respect for law and order. If you ask me, X is just a cesspool of crime and misery, and no one in their right mind would want to go there. \n```", "```py\nUSE_QDRANT_CLOUD=true\nQDRANT_CLOUD_URL=<the endpoint URL found at step 7>\nQDRANT_APIKEY=<the access token created at step 5> \n```", "```py\npeotry poe run-end-to-end-data-pipeline \n```", "```py\n    poetry poe run-digital-data-etl \n    ```", "```py\nFROM python:3.11-slim-bullseye AS release\nENV WORKSPACE_ROOT=/app/\nENV PYTHONDONTWRITEBYTECODE=1\nENV PYTHONUNBUFFERED=1\nENV POETRY_VERSION=1.8.3\nENV DEBIAN_FRONTEND=noninteractive\nENV POETRY_NO_INTERACTION=1 \n```", "```py\nRUN apt-get update -y && \\\n    apt-get install -y gnupg wget curl --no-install-recommends && \\\n    wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | gpg --dearmor -o /usr/share/keyrings/google-linux-signing-key.gpg && \\\n    echo \"deb [signed-by=/usr/share/keyrings/google-linux-signing-key.gpg] https://dl.google.com/linux/chrome/deb/ stable main\" > /etc/apt/sources.list.d/google-chrome.list && \\\n    apt-get update -y && \\\n    apt-get install -y google-chrome-stable && \\\n    rm -rf /var/lib/apt/lists/* \n```", "```py\nRUN apt-get update -y \\\n    && apt-get install -y --no-install-recommends build-essential \\\n    gcc \\\n    python3-dev \\\n    build-essential \\\n    libglib2.0-dev \\\n    libnss3-dev \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/* \n```", "```py\nRUN pip install --no-cache-dir \"poetry==$POETRY_VERSION\"\nRUN poetry config installer.max-workers 20 \n```", "```py\nWORKDIR $WORKSPACE_ROOT\nCOPY pyproject.toml poetry.lock $WORKSPACE_ROOT \n```", "```py\nRUN poetry config virtualenvs.create false && \\\n    poetry install --no-root --no-interaction --no-cache --without dev && \\\n    poetry self add 'poethepoet[poetry_plugin]' && \\\n    rm -rf ~/.cache/pypoetry/cache/ && \\\n    rm -rf ~/.cache/pypoetry/artifacts/ \n```", "```py\nCOPY . $WORKSPACE_ROOT \n```", "```py\ndocker buildx build --platform linux/amd64 -t llmtwin -f Dockerfile . \n```", "```py\npoetry poe build-docker-image \n```", "```py\nAWS_REGION=<your_region> # e.g. AWS_REGION=eu-central-1\nAWS_ECR_URL=<your_acount_id>\naws ecr get-login-password --region ${AWS_REGION}| docker login --username AWS --password-stdin ${AWS_ECR_URL} \n```", "```py\ndocker tag llmtwin ${AWS_ECR_URL}:latest \n```", "```py\ndocker push ${AWS_ECR_URL}:latest \n```", "```py\nzenml stack set aws-stack \n```", "```py\nsettings:\n  docker:\n    parent_image: <YOUR ECR URL> #e.g., 992382797823.dkr.ecr.eu-central-1.amazonaws.com/zenml-rlwlcs:latest\n    skip_build: True \n```", "```py\npoetry poe export-settings-to-zenml \n```", "```py\nzenml orchestrator update aws-stack --synchronous=False \n```", "```py\npoetry poe run-end-to-end-data-pipeline \n```", "```py\npoetry poe set-local-stack \n```", "```py\nzenml disconnect \n```", "```py\npoetry poe delete-settings-zenml \n```", "```py\npoetry poe export-settings-to-zenml \n```", "```py\nname: Example\non: [push]\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n         - name: Checkout\n           uses: actions/checkout@v3\n         - name: Setup Python\n           uses: actions/setup-python@v3\n           with:\n               python-version: \"3.11\" \n```", "```py\npull_request event occurs. Hence, the CI workflow will automatically run whenever a PR is opened, synchronized, or reopened.\n```", "```py\nname: CI\non:\n  pull_request: \n```", "```py\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: true \n```", "```py\njobs:\n  qa:\n    name: QA\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v3 \n```", "```py\n - name: Setup Python\n        uses: actions/setup-python@v3\n        with:\n          python-version: \"3.11\" \n```", "```py\n - name: Install poetry\n        uses: abatilo/actions-poetry@v2\n        with:\n          poetry-version: 1.8.3 \n```", "```py\n - name: Install packages\n        run: |\n          poetry install --only dev\n          poetry self add 'poethepoet[poetry_plugin]' \n```", "```py\n - name: gitleaks check\n        run: poetry poe gitleaks-check \n```", "```py\n - name: Lint check [Python]\n        run: poetry poe lint-check \n```", "```py\n - name: Format check [Python]\n        run: poetry poe format-check \n```", "```py\n test:\n    name: Test\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v3\n      … \n```", "```py\n - name: Install packages\n        run: |\n          poetry install –-without aws\n          poetry self add 'poethepoet[poetry_plugin]' \n```", "```py\n - name: Run tests\n        run: |\n          echo \"Running tests...\"\n          poetry poe test \n```", "```py\nname: CD\non:\n  push:\n    branches:\n      - main \n```", "```py\njobs:\n  build:\n    name: Build & Push Docker Image\n    runs-on: ubuntu-latest \n```", "```py\nsteps:\n  - name: Checkout Code\n    uses: actions/checkout@v3 \n```", "```py\n- name: Set up Docker Buildx\n  uses: docker/setup-buildx-action@v3 \n```", "```py\n- name: Configure AWS credentials\n  uses: aws-actions/configure-aws-credentials@v1\n  with:\n    aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}\n    aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n    aws-region: ${{ secrets.AWS_REGION }} \n```", "```py\n- name: Login to Amazon ECR\n  id: login-ecr\n  uses: aws-actions/amazon-ecr-login@v1 \n```", "```py\n- name: Build images & push to ECR\n  id: build-image\n  uses: docker/build-push-action@v6\n  with:\n    context: .\n    file: ./Dockerfile\n    tags: |\n      ${{ steps.login-ecr.outputs.registry }}/${{ secrets.AWS_ECR_NAME }}:${{ github.sha }}\n      ${{ steps.login-ecr.outputs.registry }}/${{ secrets.AWS_ECR_NAME }}:latest\n    push: true \n```", "```py\n     Schedule(cron_expression=\"* * 1 * *\") \n    ```", "```py\n@pipeline\ndef end_to_end_data(\n    author_links: list[dict[str, str | list[str]]], … # Other paramaters…\n) -> None:\n    wait_for_ids = []\n    for author_data in author_links:\n        last_step_invocation_id = digital_data_etl(\n            user_full_name=author_data[\"user_full_name\"], links=author_data[\"links\"]\n        )\n        wait_for_ids.append(last_step_invocation_id)\n    author_full_names = [author_data[\"user_full_name\"] for author_data in author_links]\n    wait_for_ids = feature_engineering(author_full_names=author_full_names, wait_for=wait_for_ids)\n    generate_instruct_datasets(…)\n       training(…)\n       deploy(…) \n```", "```py\npoetry poe run-end-to-end-data-pipeline \n```", "```py\nfrom zenml import pipeline, step\n@pipeline \ndef digital_data_etl(user_full_name: str, links: list[str]) -> str:\n user = get_or_create_user(user_full_name)\n crawl_links(user=user, links=links)\ntrigger_feature_engineering_pipeline(user)\n@step \ndef trigger_feature_engineering_pipeline(user):\nrun_config = PipelineRunConfiguration(…)\nClient().trigger_pipeline(\"feature_engineering\", run_configuration=run_config)\n@pipeline\ndef feature_engineering(author_full_names: list[str]) -> list[str]:\n… # ZenML steps \n```", "```py\nfrom opik import track\nimport openai\nfrom opik.integrations.openai import track_openai\nopenai_client = track_openai(openai.OpenAI())\n@track\ndef preprocess_input(text: str) -> str:\n    return text.strip().lower()\n@track\ndef generate_response(prompt: str) -> str:\n    response = openai_client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return response.choices[0].message.content\n@track\ndef postprocess_output(response: str) -> str:\n    return response.capitalize()\n@track(name=\"llm_chain\")\ndef llm_chain(input_text: str) -> str:\n    preprocessed = preprocess_input(input_text)\n    generated = generate_response(preprocessed)\n    postprocessed = postprocess_output(generated)\n    return postprocessed\nresult = llm_chain(\"Hello, do you enjoy reading the book?\") \nllm_chain() main function, which takes the initial input as a parameter and returns the final result. \n```", "```py\nupdate() method, where you can tag your trace or add any other metadata, such as the number of input tokens, through a Python dictionary:\n```", "```py\nfrom opik import track, opik_context\n@track\ndef llm_chain(input_text):\n    # LLM chain code\n    # ...\n    opik_context.update_current_trace(\ntags=[\"inference_pipeline\"],\nmetadata={\n \"num_tokens\": compute_num_tokens(…)\n},\nfeedback_scores=[\n{\n \"name\": \"user_feedback\",\n \"value\": 1.0,\n \"reason\": \"The response was valuable and correct.\"\n},\n{\n \"name\": \"llm_judge_score\",\n \"value\": compute_llm_judge_score(…),\n \"reason\": \"Computing runtime metrics using an LLM Judge.\"\n}\n) \n```", "```py\nfrom opik import track\n@track\ndef call_llm_service(query: str, context: str | None) -> str:\n    llm = LLMInferenceSagemakerEndpoint(…)\n    answer = InferenceExecutor(llm, query, context).execute()\n    return answer\n@track\ndef rag(query: str) -> str:\n    retriever = ContextRetriever()\n    documents = retriever.search(query, k=3 * 3)\n    context = EmbeddedChunk.to_context(documents)\n    answer = call_llm_service(query, context)\n    return answer \n```", "```py\nclass ContextRetriever:\n     …\n\n    @track\n\n    def search(\n        self,\n        query: str,\n        k: int = 3,\n        expand_to_n_queries: int = 3,\n    ) -> list:\n        query_model = Query.from_str(query)\n        query_model = self._metadata_extractor.generate(query_model)\n        … # Rest of the implementation \n```", "```py\nclass SelfQuery:\n\n    @track\n    def generate(self, query: str) -> str:\n        …\n        return enhanced_query \n```", "```py\n@track\ndef rag(query: str) -> str:\n    retriever = ContextRetriever()\n    documents = retriever.search(query, k=3 * 3)\n    context = EmbeddedChunk.to_context(documents)\n    answer, prompt = call_llm_service(query, context)\n    trace = get_current_trace()\n    trace.update(\ntags=[\"rag\"],\nmetadata={\n \"model_id\": settings.HF_MODEL_ID,\n   \"embedding_model_id\": settings.TEXT_EMBEDDING_MODEL_ID,\n   \"temperature\": settings.TEMPERATURE_INFERENCE,\n   \"prompt_tokens\": compute_num_tokens(prompt),\n   \"total_tokens\": compute_num_tokens(answer),\n\n}\n   )\n    return answer \n```", "```py\nfrom zenml import get_pipeline_context, pipeline\n@pipeline(on_failure=notify_on_failure)\ndef training_pipeline(…):\n…\nnotify_on_success() \n```", "```py\nfrom zenml.client import Client\nalerter = Client().active_stack.alerter\ndef notify_on_failure() -> None:\n        alerter.post(message=build_message(status=\"failed\"))\n@step(enable_cache=False)\ndef notify_on_success() -> None:\n        alerter.post(message=build_message(status=\"succeeded\")) \n```"]