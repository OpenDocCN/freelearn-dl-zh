- en: From DRL to AGI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our journey through this book has been an exploration of the evolution of reinforcement
    and **deep reinforcement learning** (**DRL**). We have looked at many methods
    that you can use to solve a variety of problems in a variety of environments,
    but in general, we have stuck to a single environment; however, the true goal
    of DRL is to be able to build an agent that can learn across many different environments,
    an agent that can generalize its knowledge across tasks, much like we animals
    do. That type of agent, the type that can generalize across multiple tasks without
    human intervention, is known as an artificial general intelligence, or AGI. This
    field is currently exploding in growth for a variety of reasons and will be our
    focus in this final chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will look at how DRL builds the AGI agent. We will first
    look at the concept of meta learning, or learning to learn. Then we will learn
    how meta learning can be applied to reinforcement learning, looking at an example
    of model-agnostic meta learning as applied to DRL. Moving past the meta, we move
    on to hindsight experience replay, a method of using trajectory hindsight in order
    to improve learning across tasks. Next, we will move on to **generative adversarial
    imitation learning** (**GAIL**) and see how this is implemented. We will finish
    the chapter with a new concept that is being applied to DRL known as imagination
    and reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a brief summary of the topics we will cover in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Learning meta learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing meta reinforcement learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using hindsight experience replay
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Imagination and reasoning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding imagination-augmented agents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this last chapter, we will cover a wide variety of complex examples quickly.
    Each section of this chapter could warrant an entire chapter or book on its own.
    If any of this material piques your interest, be sure to do further research online;
    some areas may or may not have developed since this material was written. In the
    next section, we will look at ML and MRL.
  prefs: []
  type: TYPE_NORMAL
- en: Learning meta learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The word "meta" is defined as "referring to itself or its type or genre". When
    talking about meta learning, we are talking about understanding the learning process
    of learning—that is, instead of thinking about how an agent learns a task, we
    want to think about how an agent could learn to learn across tasks. It is both
    an interesting and yet abstract problem, so we first want to explore what meta
    learning is. In the next section, we will explore how machine learning can learn
    to learn.
  prefs: []
  type: TYPE_NORMAL
- en: Learning 2 learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Any good learning model should be trained across a variety of tasks and then
    generalized to fit the best distribution of those tasks. While we have covered
    very little with respect to general machine learning, consider the simple image
    classification problem with a deep learning model. We would typically train such
    a model with one goal or task, perhaps to identify whether a dataset contains
    a cat or dog, but not both, and nothing else. With meta learning, the cat/dog
    dataset would be one training entry in a set of image classification tasks that
    could cover a broad range of tasks, from recognizing flowers to cars. The following
    example images demonstrate this concept further:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/41ab7d2e-ccde-4a1b-902e-263ba4a1f743.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of meta learning applied to image classification (Image source Google)
  prefs: []
  type: TYPE_NORMAL
- en: 'The concept, then, involves training the model to classify the tasks of identifying
    a mouse or moose and cat or wolverine, and then, with meta learning, apply that
    to the task of identifying a tiger or bear. Essentially, we train the model in
    the same way that we test the model recursively—iteratively using collections
    of few-shot samples to expose to the network in learning sets or mini batches.
    You will have heard the term few-shot learning used in the context of meta learning
    to describe the small number of samples used for each task that are exposed to
    the model as a way of generalizing learning. The way that the model is updated
    through this process has been classified into the following three current schools
    of thought:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Metric based**: Solutions are so named because they depend on training a
    metric to gauge and monitor performance. This often requires the learner to learn
    the kernel that defines the distribution the network is trying to model rather
    than tune the network explicitly. What we find is that adversarial learning, using
    two somewhat opposing networks, can balance and refine this learning to learn
    the metric in a form of encoding or embedding. Some great examples of this type
    of approach are convolutional siamese networks for few-shot learning, matching
    networks, full-context embeddings, relational networks, and prototypical networks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model based**: Solutions define a group of methods that rely on some form
    of memory augmentation or context. Memory-augmented neural networks, or MANNs,
    are the primary implementation you will find using this solution. This concept
    is further based on a neural Turing machine (NTM), which describes a controller
    network that learns to read and write from memory-soft attention. An example of
    how this looks is taken from the following NTM architecture diagram:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/1cb35d13-5976-48a3-add9-d2cbe0caa944.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of a neural Turing machine
  prefs: []
  type: TYPE_NORMAL
- en: The NTM architecture is used to power the MANN model for meta learning. Training
    a MANN requires some attention to the details of how the model memorizes and encodes
    the tasks. This model is often trained in such a way as to lengthen the amount
    of time that it takes for disparate training data to be reintroduced and remembered.
    Essentially, the agent is trained longer and longer for individual tasks, and
    then forced to recall memory of prelearned tasks later in further training. Interestingly
    enough, this is a method we humans will often use to focus on learning specific
    complex tasks. Then, we retest this knowledge later in order to reinforce this
    knowledge in memory. This same concept very much applies to MANN, and many believe
    that NTM or memory is a key ingredient to any meta learning pattern.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimization based**: Solutions are as much a combination of the two previous
    solutions as it is an antipattern. In optimization-based problems, we consider
    the root of the problem, and therefore the problem of optimizing our function,
    using not only gradient descent but also introducing gradient descent through
    context or time. Gradient descent through context or time is also known as **backpropagation
    through time** (**BPTT**), and is something we briefly touched on when we looked
    at recurrent networks. By introducing recurrent networks or **long short-term
    memory** (**LSTM**) layers into a network, we encourage the network to remember
    gradient context. Another way to think of this is that the network learns the
    history of the gradients it applied during training. The meta learner therefore
    gets trained using the process shown in the following diagram:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/5ced2008-0997-412b-ab93-d79c698f80bb.png)'
  prefs: []
  type: TYPE_IMG
- en: Training the meta learned with LSTM
  prefs: []
  type: TYPE_NORMAL
- en: The diagram was sourced from the paper *Optimization as a Model for Few-Shot
    Learning*, by Sachin Ravi and Hugo Larochelle, and it is a much-simplified version
    of the original. In the diagram, we can see how the meta learner is trained outside
    the regular model, often in an outside loop, where the inside loop is defined
    as the training process of the individual classification, regression, or other
    forms of learning-based task.
  prefs: []
  type: TYPE_NORMAL
- en: While there are three different forms of meta learning, we will pay particular
    attention to the optimization form, and in particular, a method that works by
    assuming an agnostic model, which we will explore in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Model-agnostic meta learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Model-agnostic meta learning** (**MAML**) is described as a general optimization
    method that will work with any machine learning method that uses gradient descent
    for optimization or learning. The intuition here is that we want to find a loss
    approximation that best matches the task we are currently undertaking. MAML does
    this by adding context through our model training tasks. That context is used
    to refine the model training parameters and thereby allow our model to better
    apply gradient loss for a specific task.'
  prefs: []
  type: TYPE_NORMAL
- en: This example uses the MNIST dataset, a set of 60,000 handwritten digits that
    is commonly used for base image classification tasks. While the dataset has been
    solved with high accuracy using a number of methods, it is often the base comparison
    for image classification tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'This will likely still sound abstract, so in the next exercise, we pull down
    a PyTorch ML framework called `learn2learn` and show how MAML can be used:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will first create a new virtual environment and then download a package
    called `learn2learn`, a meta learning framework that provides a great implementation
    of MAML in PyTorch. Make sure that you create a new environment and install PyTorch
    and the Gym environment, as we previously did. You can install `learn2learn` with
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to see how `learn2learn` is used in a basic task, we are going to
    review the basic MNIST training sample found in the repository, but we won''t
    look at every section of the code example that has been provided in the source
    code `Chapter_14_learn.py`. Open the sample up and review the top section of the
    code, as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This top section of code shows the `learn2learn` `import` statement and the
    definition of the `Net` class. This is the network model we will be training.
    Note how the model is composed of two convolutional/pooling layers followed by
    a fully connected linear layer to an output layer. Note the use of `ways` as an
    input variable that defines the number of outputs from the last output layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we will scroll down to the `main` function. This is where all the main
    setup and initialization occurs. This sample being more robust than most, it provides
    for input parameters that you can use instead of altering the hyperparameters
    in the code. The top of the `main` function is shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Although we haven't gone through an image classification example before, hopefully
    the code will be relatively understandable and familiar to you. The main point
    to note is the construction of the `meta_model` using the `l2l.algorithms.MAML`
    model on the highlighted line of code. Note how the `meta_model` wraps the `model` network
    by using it as an input.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'From here, we will scroll down to the familiar training loop we have seen so
    many times before. This time, however, there are some interesting differences.
    Look specifically at the code just inside the first iteration loop, as shown in
    the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Note how we are constructing a `learner` clone of the `meta_model` learner.
    The `learner` clone becomes our target learning network. The last two lines show
    the construction of a sampler for the training and validation tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s see how `learner` is used to compute the loss again in an iterative
    manner using another loop, as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: At this point, run the sample and observe the output to get a sense of how training
    is done.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we understand some of the basic code setup, we are going to move on
    to explore how the sample trains and computes loss in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Training a meta learner
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `learn2learn` framework provides the MAML framework for building the learner
    model we can use to learn to learn; however, it is not automatic and does require
    a bit of setup and thought regarding how loss is computed for your particular
    set of tasks. We have already seen where we compute loss—now we will look closer
    at how loss is computed across tasks. Reopen `Chapter_14_learn.py` and go through
    the following exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Scroll back down to the innermost training loop within the `main` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The inner loop here is called a **fast adaptive training loop**, since we are
    showing our network a few or mini batches or shots of data for training. Computing
    the loss of the network is done using the `compute_loss` function, as shown in
    the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Note how the computation of loss is done iteratively over task training batches
    by iterating through the `dataloader` list. We then compute the average loss for
    all tasks by taking the total loss, `loss`, and dividing it by the number of dataloaders.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This average `loss` and accuracy, `acc`, are returned from the `compute_loss`
    function. From that learning instance, the learner is then adapted or updated
    using the following line of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'After the fast adaptive looping and updating the learner through each loop,
    we can then validate the learner with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The `valid_error` validation error and `valid_acc` accuracy are then accumulated
    on the total `iteration_error` error and `iteration_acc` accuracy values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We finish by calculating the average iteration and accuracy errors, `iteration_error`
    or `iteration_acc` values, and then propagating that error back through the networks
    with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The training for this example is quite quick, so run the example again and observe
    how quickly the algorithm can train across meta learning tasks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each meta learning step involves pushing the loss back through the network using
    BPTT, since the meta network is composed of recurrent layers. That detail is abstracted
    for us here, but hopefully you can appreciate how seamlessly we were able to introduce
    meta learning into training this regular image classification task. In the next
    section, we will look at how we can apply meta learning to reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing meta reinforcement learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, that we understand the concept of meta learning, we can move on to meta
    reinforcement learning. Meta-RL—or RL^2 (RL Squared), as it has been called—is
    quickly evolving, but the additional complexity still makes this method currently
    inaccessible. While the concept is very similar to vanilla meta, it still introduces
    a number of subtle nuances for RL. Some of these can be difficult to understand,
    so hopefully the following diagram can help. It was taken from a paper titled
    *Reinforcement Learning, Fast and Slow* by *Botvinick, et al. 2019* ([https://www.cell.com/action/showPdf?pii=S1364-6613%2819%2930061-0](https://www.cell.com/action/showPdf?pii=S1364-6613%2819%2930061-0)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0f6a295d-b2dc-4130-a376-e93868d180ba.png)'
  prefs: []
  type: TYPE_IMG
- en: Meta reinforcement learning
  prefs: []
  type: TYPE_NORMAL
- en: 'In the diagram, you can see that familiar inner and outer loops that are characteristic
    of meta learning. This means that we also go from evaluating a policy ![](img/7715985a-bfda-45c7-9fad-ac9d84f9253d.png)
    for any observed state to also now including the last action, last reward, and
    observed state in meta-RL. These differences are summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning = ![](img/c6e1f0bd-6030-4027-a98a-9007f4c48774.png) distribution
    over ![](img/3088d7ca-eb64-4bfb-8ef9-a79bedb4d2d2.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Meta reinforcement learning = ![](img/591c6379-1708-4e19-af52-7ec21db9225e.png) distribution
    over ![](img/1d1e849d-5700-4005-bba1-841cb7b6a3e7.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As we have seen with regular meta learning, there are a number of variations
    that are used and experimented on within meta-RL, but they all share the following
    three common elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model memory**: We add memory to our model in the form of recurrent network
    layers or LSTM. Typically, the outer loop is composed of the memory component
    engaged by LSTM layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distribution of MDPs**: The agent/algorithm needs to be trained across multiple
    different MDPs, which is typically done by exposing it to different or randomized
    environments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Meta-learning algorithm**: The agent needs a meta learning algorithm for
    itself to learn to learn.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Unity Obstacle Tower Challenge was most likely developed to encourage developers
    to build a meta-RL agent, but as we have seen, the winning entry used a variation
    of hierarchical reinforcement learning. While HRL is designed to accomplish the
    same function as meta-RL, it lacks the automatic generation of memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to get a sense of the diversity of meta-RL algorithms, we will look
    at a list of what appear to be the most current methods used:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Optimizing weights**: This is essentially MAML or another variation called
    Reptile. MAML is currently one of the more popular variations used, and one we
    will explore later in detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Meta-learning hyperparameters**: There are a few hyperparameters that we
    use internally to balance learning within RL. These are the gamma and alpha values
    that we have tuned before, but imagine if they could be autotuned with meta-learning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Meta-learning loss**: This considers that the loss function itself may need
    to be tuned, and uses a pattern to evolve it over iterations. This method uses
    evolutionary strategies that are outside the scope of this book.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Meta-learning exploration**: This uses meta learning to build more effective
    exploration strategies. This, in turn, reduces the amount of time exploring and
    increases effective training performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Episodic control**: This provides the agent with a method to keep important
    episodes in memory and forget others. This sounds a lot like prioritized experience
    replay, but the method of control here is within the calculation of loss and not
    from replay.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evolutionary algorithms**: These are gradient-free, optimization-based solutions
    that use a form of genetic search to find solution methods. The collision of evolutionary
    algorithms and deep learning is an ongoing endeavor that many have tried and failed
    with. Both methods are very powerful and capable on their own, so it is perhaps
    only a matter of time before they get combined into a working model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see, there is plenty of variation in meta-RL methods, and we will
    look at how one method is implemented in detail in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: MAML-RL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `learn2learn` repository holds another great example of how to use their
    library for a few variations of this method. A good method for us to look at will
    be an implementation of Meta-SGD, which further extends MAML by adopting per-parameter
    learning rates using vanilla policy gradients, and is often referred to as MetaSGD-VPG.
    This concept was originally presented in the paper *Meta Reinforcement Learning
    with Task Embedding and Shared Policy*, which was itself presented at IJCAI-19.
  prefs: []
  type: TYPE_NORMAL
- en: Make sure that you have completed all the installation steps from the last exercise
    before proceeding. If you have troubles running the sample, repeat the installation
    in a new virtual environment. Some issues may be related to the version of PyTorch
    you are using, so check that your version is compatible.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open up `Chapter_14_MetaSGD-VPG.py` and go through the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You will need to install the cherry RL package first by entering the following
    command in your virtual environment window:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We won''t review the entire code listing, just the critical sections. First,
    let''s look at the `main` function, which starts the initialization and hosts
    the training. The start of this function is shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: In the definition of the `main` function, we can see all the relevant hyperparameters
    as well as their selected defaults. Note that the two new groups of hyperparameters
    for the adaption and meta learning steps are prefixed with `adapt` and `meta`,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will look at the initialization of the environment, policy, meta learner,
    and optimizer using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we can see the three training loops. First, the outer iteration loop
    controls the number of meta learning repetitions. Inside that loop, we have the
    task setup and configuration loop; remember that we want each learning session
    to require a different but related task. The third, innermost loop is where the
    adaption occurs, and we push the loss back through the model. The code for all
    three loops is shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'After the fast adaptive looping takes place, we then jump back to the second
    loop and calculate the validation loss with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The validation loss is computed over the second loop for each different task.
    This loss is then accumulated into the iteration loss, `iteration_loss`, value.
    Leaving the second loop, we then print out some stats and calculate the adaption
    loss, `adaption_loss`, and push that as a gradient back through the network for
    training with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Remember that the divisors in both loss equations (iteration and adaption)
    both use a similar value of `20`, `meta_bsz` `= 20`, and `adapt_bsz = 20`. The
    base loss function is defined by the `maml_a2c_loss` and `compute_advantages`
    functions, as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Note how the cherry RL library saves us the implementation of some tricky code.
    Fortunately, we should already know what the cherry functions `ch.td.discount`
    and `ch.pg.generalized_advantage` are, as we encountered them in previous chapters,
    and so we won't need to review them here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the example as you normally would and observe the output. An example of
    the generated output is shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/55603a29-3aae-4937-ae15-b26db7666157.png)'
  prefs: []
  type: TYPE_IMG
- en: Example output from Chapter_14_MetaSQG-VPG.py
  prefs: []
  type: TYPE_NORMAL
- en: Note the amount of training time that the sample expects to take when running
    on a CPU when it first starts. While the prediction comes down significantly from
    five days to just over two in less than an hour, it still demonstrates the computational
    requirements for this type of training. Therefore, if you plan to do any serious
    work on meta-RL, you will likely want to use a very fast GPU for training. When
    testing on a very fast GPU, the preceding sample took 1,000 times less time to
    process. Yes, you read that right, 1,000 times less time. While you likely may
    not experience such a vast difference, any upgrade from a CPU to GPU will be significant.
  prefs: []
  type: TYPE_NORMAL
- en: There is a strong belief that is held by many of those in the RL community that
    meta-RL is the next big leap that we need to solve in order to get closer to AGI.
    Most of the development of this field is still guided by what is currently the
    state of the art, and how and when changes will dictate the future of RL. With
    this in mind, we are going to look at some other potential next-level steps, starting
    in the next section with HER.
  prefs: []
  type: TYPE_NORMAL
- en: Using hindsight experience replay
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hindsight experience replay was introduced by OpenAI as a method to deal with
    sparse rewards, but the algorithm has also been shown to successfully generalize
    across tasks due in part to the novel mechanism by which HER works. The analogy
    used to explain HER is a game of shuffleboard, the object of which is to slide
    a disc down a long table to reach a goal target. When first learning the game,
    we will often repeatedly fail, with the disc falling off the table or playing
    area. Except, it is presumed that we learn by expecting to fail and give ourselves
    a reward when we do so. Then, internally, we can work backward by reducing our
    failure reward and thereby increasing other non-failure rewards. In some ways,
    this method resembles Pierarchy (a form of HRL that we looked at earlier), but
    without the extensive pretraining parts.
  prefs: []
  type: TYPE_NORMAL
- en: The next collection of samples in the following sections has again been sourced
    from [https://github.com/higgsfield](https://github.com/higgsfield), and are the
    result of a young man named Dulat Yerzat from Almaty, Kazakhstan.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the samples for `Chapter_14_wo_HER.py` and `Chapter_14_HER.py`. These
    two samples are comparisons of simple DQN networks that are applied with and without
    HER. Go through the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Both examples are almost the same, aside from the implementation of HER, so
    the comparison will help us understand how the code works. Next, the environment
    has been simplified and custom built to perform that simple bit shifting of a
    random set of bits. The code to create the environment is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We never really went over how to build a custom environment, but as you can
    see, it can be quite simple. Next, we will look at the simple DQN model that we
    will use to train, as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'That is about as simple a DQN model as you can get. Next, let''s compare the
    two examples by viewing the code side by side, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/1c6c16c6-4389-42fd-a144-2d488bc5b30f.png)'
  prefs: []
  type: TYPE_IMG
- en: Comparison of code examples in VS
  prefs: []
  type: TYPE_NORMAL
- en: 'The new section of code is also shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: What we see here is the addition of another loop not unlike meta-RL, but this
    time, instead of wrapping the inner loop, it sits as a sibling. The second loop
    is activated after an episode is completed from the first inner loop. It then
    loops through every event in the previous episode and adjusts the goals or targets
    based on the returned reward based on the new goal. This is essentially the hindsight
    part.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The remaining parts of this example resemble many of our previous examples,
    and should be quite familiar by now. One interesting part, though, is the `get_action`
    function, as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Note here that we are using an `epsilon` value that is defaulted to `.1` to
    denote the tendency for exploration. In fact, you might notice that this example
    uses no variable exploration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing with the differences, the next key difference is the `compute_td_loss`
    function, as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the example without HER first and observe the results, then run the example
    with HER. The output for the example with HER is shown in the following excerpt:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/79196e9f-67de-45fe-aec1-383d28af335a.png)'
  prefs: []
  type: TYPE_IMG
- en: Example output from Chapter_14_HER.py
  prefs: []
  type: TYPE_NORMAL
- en: Compared to the example without HER, the preceding output is significantly better.
    You will have to run both examples yourself to see the exact difference. Note
    how the calculation of loss remains consistently variable and doesn't converge,
    while the mean reward increases. In the next section, we move to what is expected
    to be the next wave in RL—imagination and reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: Imagination and reasoning in RL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Something that we can observe from our own experience of learning is how imagination
    can benefit the learning process. Pure imagination is the stuff of deep abstract
    thoughts and dreams, often closer to a hallucination than any way to solve a real
    problem. Except, this same imagination can be used to span gaps in our understanding
    of knowledge and allow us to reason out possible solutions. Say that we are trying
    to solve the problem of putting a puzzle together, and all we have are three remaining,
    mostly black pieces, as shown in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/88801726-c736-427f-9580-328e2c8886a0.png)'
  prefs: []
  type: TYPE_IMG
- en: Imagining what the three missing puzzle pieces may look like
  prefs: []
  type: TYPE_NORMAL
- en: Given the simplicity of the preceding diagram, it is quite easy for us to imagine
    what those puzzle pieces may look like. We are able to fill in those gaps quite
    easily using our imagination from previous observations and reasoning. This use
    of imagination to fill in gaps is something we use all the time, and it is often
    said that the more imaginative you are, the more intelligent you are as well.
    Now it remains to be seen if this path to AI will indeed prove that theory, but
    it certainly looks like a possibility.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagination is not pulled from a vacuum, and likewise, in order to give our
    agents imagination, we have to essentially bootstrap their memory or previous
    learnings. We will do this in the next exercise in order for us to later generate
    the imagination from these learnings. Open sample `Chapter_14_Imagine_A2C.py`
    and go through the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The base agent we will use to generate the training bootstrap for our imagination
    will be a simple A2C Vanilla PG method. Let''s first scroll down in the file and
    look at the `ActorCritic` class that defines our agent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'What we can see is a simple PG agent that will be powered by an A2C synchronous
    actor-critic. Next, we come to another new class called `RolloutStorage`. Rollout
    storage is similar in concept to experience replay, but it also enables us to
    have an ongoing calculation of returns, as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'If we scroll down to the `main` function, we can see that there are 16 synchronous
    environments that are being run with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We will talk more about the `RolloutStorage` class later. For now, move down
    to the training section of code. It is the typical double-loop code, the outside
    loop controlling episodes and the inside loop controlling steps, as shown in the
    following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The rest of the training code should be familiar, but it should be worth reviewing
    in detail on your own.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next major difference we want to observe is at the end of the outer training
    loop. This last block of code is where the loss is calculated and pushed back
    through the network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Note the highlighted line in the preceding code block. This is unique in that
    we are clipping the gradient to a maximum value that is likely to avoid exploding
    gradients. The last section of code at the end renders out the playing area and
    shows the agent playing the game.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Exploding gradients are when a gradient value becomes so large that it causes
    the network to forget knowledge. The network weights start to be trained in wild
    fluctuations and any previous knowledge will often be lost.
  prefs: []
  type: TYPE_NORMAL
- en: Run the code as you normally would and observe the output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Running the preceding code will also create a saved-state dictionary of memories
    that we will use to populate the imagination later. You must run this last exercise
    to completion if you want to continue working with later exercises. In the next
    section, we will explore how these latent traces can be used to generate an agent's
    imagination.
  prefs: []
  type: TYPE_NORMAL
- en: Generating imagination
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the current version of this algorithm, we first need to bootstrap the memories
    that we populate in the agent with a previous run by an agent, or perhaps a human.
    This is really no different than imitation learning or behavioral cloning, except
    we are using an on-policy agent that we will later use as an off-policy base for
    our imagination. Before we combine imagination into our agent, we can see how
    the predicted next state will look compared to what the agent''s actual state
    will be. Let''s see how this works by opening up the next example `Chapter_14_Imagination.py`
    and go through the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: This example works by loading the previous saved-state dictionary we generated
    in the last exercise. Make sure that this data is generated and saved with a prefix
    of `actor_critic_` files in the same folder before continuing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The purpose of this code is to extract the saved-state observation dictionary
    we recorded earlier. We then want to extract the observation and use it to imagine
    what the next state will look like. Then we can compare how well the imagined
    and next state resemble each other. This amount of resemblance will in turn be
    used to train an imagination loss later. We can see how the previous model is
    loaded by looking at the following line of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding line of code reloads our previously trained model. Now we want
    to use the imagination to (for instance) reasonably fill in the areas where the
    agent may not have explored. Scrolling down, we can see the training loop that
    will learn the imagination part of the agent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This loop loops through the previously played games and encodes the actions
    using one-hot encoding. Scrolling down, we can see how the `imagined_state` state
    and the `imagined_reward` reward are learned:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: This is the section of code that learns to correctly imagine the target state
    and reward from playing through the previously observed observations. Of course,
    the more observations, the better the imagination, but at some point, too many
    observations will eliminate all of the imagination entirely. Balancing this new
    trade-off will require a bit of trial and error on your own.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scrolling down to the bottom of the file, you can see where an example of the
    imagination and target states are outputted with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The following example screenshot shows the best the original author was able
    to get by training the agent for a considerable amount of time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/9d845819-71d5-4b23-b772-b452c54d1bec.png)'
  prefs: []
  type: TYPE_IMG
- en: Example comparison of imagined versus actual
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the example and your output: depending on your previous amount of training,
    it may not look as good. Again, the quality of the imagination will be based on
    the previous experiences and amount of training to refine the imagination itself.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'One last thing to note is how the imagined image is getting extracted. This
    is done using an inverted CNN in the `BasicBlock` class that converts the encoding
    back to an image of the correct resolution. The code for the `BasicBlock` class
    is shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, training the imagination process itself is not that difficult.
    The real difficulty is putting this all together in a running agent, and we will
    see how this is done in the next section when we learn about I2A.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding imagination-augmented agents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The concept of **imagination-augmented agents** (**I2A**) was released in a
    paper titled *Imagination-Augmented Agents for Deep Reinforcement Learning* in
    February 2018 by T. Weber, et al. We have already talked about why imagination
    is important for learning and learning to learn. Imagination allows us to fill
    in the gaps in our learning and make leaps in our knowledge, if you will.
  prefs: []
  type: TYPE_NORMAL
- en: Giving agents an imagination allows us to combine model-based and model-free
    learning. Most of the agent algorithms we have used in this book have been model-free,
    meaning that we have no representative model of the environment. Early on, we
    did cover model-based RL with MC and DP, but most of our efforts have been fixed
    on model-free agents. The benefit of having a model of the environment is that
    the agent can then plan. Without a model, our agent just becomes reactionary through
    trial and error attempts. Adding imagination allows us to combine some aspects
    of using a model of the environment while being model free. Essentially, we hope
    to achieve the best of both worlds using imagination.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have already explored the core role of imagination in the I2A architecture.
    This was the part we looked at in the last section that generated the imagined
    features and reward, essentially the model part. The following diagram illustrates
    the I2A architecture, the imagination core part, and the rollout encoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e5fd38c1-0c19-416d-b92b-70aa02ef2919.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary of the I2A architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'The I2A architecture demonstrates the complexity of the systems that we can
    start to build on top of DRL in the hopes of adding additional learning advantages,
    such as imagination. In order to really understand this architecture, we should
    look at a code example. Open up `Chapter_14_I2A.py` and go through the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have already covered the first part of the architecture, so at this stage,
    we can start with the policy itself. Look at the I2A policy class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: For the most part, this is a fairly simple PG policy, except with the addition
    of imagination elements. Note how, in the `forward` function, the forward pass
    refers to the imagination needed to extract the `imagined_state` and `imagined_reward`
    values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we scroll down a little bit more and come to the `ImaginationCore` class.
    The class encapsulates the functionality we have seen before, but all wrapped
    in a single class, as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have seen how these big pieces work, it is time to get to the `main`
    function. We will start by looking at the first dozen or so lines of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Note the flow of code. The code goes from instantiating an environment model
    `env_model` and the `distil_policy` from an `ActorCritic` class. Then the code
    sets up the optimizer and instantiates the `imagination` object of the `ImaginationCore` type with
    inputs of `env_model` and `distil_policy`. The last line creates the `actor_critic
    I2A` policy using the `imagination` object as input.
  prefs: []
  type: TYPE_NORMAL
- en: 'Jump down to the training loop. Note that it looks fairly standard:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'After the inner episode loop is complete, we then jump down to the loss calculation
    and update the code, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: One thing to pay attention to here is that we are using two loss gradients to
    push back the loss to the `distil` model, which adjusts the `distil` model parameters
    and the `actor_critic` model or policy and its parameters. Without getting too
    bogged down in details, the main concept here is that we train the `distil` model
    to learn the imagination and the other loss for general policy training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the example again. Wait until it starts and then you may want to shut it
    down after a few rounds, because, this sample can take upwards of an hour per
    iteration on a slower CPU, possibly longer. The following is an example screenshot
    of the start of training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/a625ef87-3a79-47f0-a2fb-c13ca20068f5.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of Chapter_14_I2A.py training
  prefs: []
  type: TYPE_NORMAL
- en: Now, if you want to run this exercise to completion, you should use a GPU at
    the very least. Ten thousand hours of CPU training would take a year, and is not
    something you will likely want to spend time on. If you do use a GPU, you will
    have to modify the sample to support a GPU, and this will require uncommenting
    sections and setting up PyTorch so that it can run with CUDA.
  prefs: []
  type: TYPE_NORMAL
- en: This completes this section and the content for this chapter, as well as the
    book. In the next section, we will look at the last set of exercises.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following is a mix of simple and very difficult exercises. Choose those
    exercises that you feel appropriate to your interests, abilities, and resources.
    Some of the exercises in the following list could require considerable resources,
    so pick those that are within your time/resource budget:'
  prefs: []
  type: TYPE_NORMAL
- en: Tune the hyperparameters for sample `Chapter_14_learn.py`.This sample is a standard
    deep learning model, but the parameters should be familiar enough to figure out
    on your own.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tune the hyperparameters for sample `Chapter_14_MetaSGD-VPG.py`, as you normally
    would.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tune the hyperparameters for sample **`Chapter_14_Imagination.py`**. There are
    a few new hyperparameters in this sample that you should familiarize yourself
    with.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tune the hyperparameters for the `Chapter_14_wo_HER.py` and `Chapter_14_HER.py`
    examples. It can be very beneficial for your understanding to train the sample
    with and without HER using the same techniques.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tune the hyperparameters for the `Chapter_14_Imagine_A2C.py` example. What effect
    does this have on running the `Chapter_14_Imagination.py` example later?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Upgrade the HER example (`Chapter_14_HER.py`) to use a different PG or value/DQN
    method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Upgrade the `Chapter_14_MetaSGD-VPG.py` example to use a more advanced PG or
    DQN method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Adapt the `Chapter_14_MetaSGD-VPG.py` example to train on different environments
    that use continuous or possibly even discrete actions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the `Chapter_14_I2A.py `sample to completion. You will need to configure
    the example to run with CUDA, as well as install PyTorch with CUDA.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tune the hyperparameters for the `Chapter_14_I2A.py `sample. You may decide
    to do only partial training runs using just the CPU, which is acceptable. Therefore,
    you could train a couple of iterations at a time and still optimize those new
    hyperparameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do the exercises of most interest to you and remember to have fun.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked beyond DRL and into the realm of AGI, or at least
    where we hope we are going with AGI. More importantly, though, we looked at what
    the next phase of DRL is, how we can tackle its current shortcomings, and where
    it could go next. We looked at meta learning and what it means to learn to learn.
    Then we covered the excellent `learn2learn` library and saw how it could be used
    on a simple deep learning problem and then a more advanced meta-RL problem with
    MAML. From there, we looked at another new approach to learning using hindsight
    with HER. From hindsight, we moved to imagination and reasoning and how this could
    be incorporated into an agent. Then we finished the chapter by looking at I2A—imagination-augmented
    agents—and how imagination can help fill in the gaps in our knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: I just want to thank you for taking the time to work through this book with
    us. It has been an amazing journey covering almost the entire RL and DRL alphabet
    of concepts, terms, and acronyms. This book started with the basics of RL and
    went deep, very deep, into DRL. Provided you have the mathematical background,
    you can likely venture out on your own now, and build your own latest and greatest
    agent. RL, and in particular DRL, suffers from the myth that you require extensive
    computational resources to make valuable contributions. While for certain research
    this is certainly the case, there are a lot of other more rudimentary elements
    that still need a better understanding that can be improved upon. The field of
    DRL is still relatively new, and it is quite likely that we have missed things
    along the way. Therefore, whatever your resources, you likely still could make
    a valuable contribution to DRL in the coming years. If you do plan to pursue this
    dream, I wish you success and hope that this book contributes to your journey.
  prefs: []
  type: TYPE_NORMAL
