- en: From DRL to AGI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从 DRL 到 AGI
- en: Our journey through this book has been an exploration of the evolution of reinforcement
    and **deep reinforcement learning** (**DRL**). We have looked at many methods
    that you can use to solve a variety of problems in a variety of environments,
    but in general, we have stuck to a single environment; however, the true goal
    of DRL is to be able to build an agent that can learn across many different environments,
    an agent that can generalize its knowledge across tasks, much like we animals
    do. That type of agent, the type that can generalize across multiple tasks without
    human intervention, is known as an artificial general intelligence, or AGI. This
    field is currently exploding in growth for a variety of reasons and will be our
    focus in this final chapter.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这本书中的旅程是对强化和**深度强化学习**（**DRL**）演变的探索。我们查看了许多你可以用来解决各种环境中的各种问题的方法，但总的来说，我们一直坚持在一个单一的环境中；然而，DRL
    的真正目标是能够构建一个能够在许多不同环境中学习的代理，一个能够在多个任务中泛化其知识的代理，就像我们动物一样。这种类型的代理，那种可以在没有人类干预的情况下跨多个任务泛化的代理，被称为人工通用智能，或
    AGI。这个领域目前正在因各种原因迅速增长，并将是我们本章的重点。
- en: In this chapter, we will look at how DRL builds the AGI agent. We will first
    look at the concept of meta learning, or learning to learn. Then we will learn
    how meta learning can be applied to reinforcement learning, looking at an example
    of model-agnostic meta learning as applied to DRL. Moving past the meta, we move
    on to hindsight experience replay, a method of using trajectory hindsight in order
    to improve learning across tasks. Next, we will move on to **generative adversarial
    imitation learning** (**GAIL**) and see how this is implemented. We will finish
    the chapter with a new concept that is being applied to DRL known as imagination
    and reasoning.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨如何构建 DRL 的 AGI 代理。我们首先将探讨元学习或学习如何学习的概念。然后，我们将学习元学习如何应用于强化学习，并查看一个应用于
    DRL 的模型无关元学习的例子。超越元学习，我们将转向后见之明经验回放，这是一种使用轨迹后见之明来提高跨任务学习的方法。接下来，我们将转向**生成对抗模仿学习**（**GAIL**），并了解其实现方式。我们将以一个新概念结束本章，这个概念正在应用于
    DRL，称为想象和推理。
- en: 'Here is a brief summary of the topics we will cover in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是本章我们将涵盖的主题简要概述：
- en: Learning meta learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习元学习
- en: Introducing meta reinforcement learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍元强化学习
- en: Using hindsight experience replay
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用后见之明经验回放
- en: Imagination and reasoning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 想象和推理
- en: Understanding imagination-augmented agents
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解增强想象力的代理
- en: In this last chapter, we will cover a wide variety of complex examples quickly.
    Each section of this chapter could warrant an entire chapter or book on its own.
    If any of this material piques your interest, be sure to do further research online;
    some areas may or may not have developed since this material was written. In the
    next section, we will look at ML and MRL.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一章中，我们将快速涵盖各种复杂示例。本章的每个部分都足以成为一个完整的章节或一本书。如果你对其中任何内容感兴趣，请务必在网上进行进一步研究；一些领域可能或可能没有在撰写此材料后发展。在下一节中，我们将探讨机器学习（ML）和元强化学习（MRL）。
- en: Learning meta learning
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习元学习
- en: The word "meta" is defined as "referring to itself or its type or genre". When
    talking about meta learning, we are talking about understanding the learning process
    of learning—that is, instead of thinking about how an agent learns a task, we
    want to think about how an agent could learn to learn across tasks. It is both
    an interesting and yet abstract problem, so we first want to explore what meta
    learning is. In the next section, we will explore how machine learning can learn
    to learn.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: “元”一词定义为“指代自身或其类型或体裁”。当谈论元学习时，我们是在谈论理解学习过程的学习——也就是说，我们不是在思考一个代理如何学习一个任务，而是想思考一个代理如何能够在多个任务中学习如何学习。这是一个既有趣又抽象的问题，因此我们首先想探索元学习是什么。在下一节中，我们将探讨机器学习如何学习如何学习。
- en: Learning 2 learn
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习如何学习
- en: 'Any good learning model should be trained across a variety of tasks and then
    generalized to fit the best distribution of those tasks. While we have covered
    very little with respect to general machine learning, consider the simple image
    classification problem with a deep learning model. We would typically train such
    a model with one goal or task, perhaps to identify whether a dataset contains
    a cat or dog, but not both, and nothing else. With meta learning, the cat/dog
    dataset would be one training entry in a set of image classification tasks that
    could cover a broad range of tasks, from recognizing flowers to cars. The following
    example images demonstrate this concept further:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 任何好的学习模型都应该在各种任务上进行训练，然后推广以适应这些任务的最佳分布。虽然我们在广义机器学习方面涉及很少，但考虑一下简单的深度学习图像分类问题。我们通常会以一个目标或任务来训练这样的模型，比如识别数据集中是否包含猫或狗，但不是两者，也没有其他。通过元学习，猫/狗数据集将是一组图像分类任务中的一个训练条目，这些任务可以覆盖广泛的任务，从识别花朵到汽车。以下示例图像进一步说明了这一概念：
- en: '![](img/41ab7d2e-ccde-4a1b-902e-263ba4a1f743.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/41ab7d2e-ccde-4a1b-902e-263ba4a1f743.png)'
- en: Example of meta learning applied to image classification (Image source Google)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 将元学习应用于图像分类的例子（图片来源：Google）
- en: 'The concept, then, involves training the model to classify the tasks of identifying
    a mouse or moose and cat or wolverine, and then, with meta learning, apply that
    to the task of identifying a tiger or bear. Essentially, we train the model in
    the same way that we test the model recursively—iteratively using collections
    of few-shot samples to expose to the network in learning sets or mini batches.
    You will have heard the term few-shot learning used in the context of meta learning
    to describe the small number of samples used for each task that are exposed to
    the model as a way of generalizing learning. The way that the model is updated
    through this process has been classified into the following three current schools
    of thought:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这个概念涉及训练模型来分类识别老鼠或驼鹿以及猫或猞猁的任务，然后通过元学习，将这一方法应用于识别老虎或熊的任务。本质上，我们以递归测试模型的方式训练模型——迭代地使用少量样本集合暴露给网络，作为泛化学习的一种方式。您可能已经听说过在元学习背景下使用的“少样本学习”这个术语，它描述了每个任务中暴露给模型的少量样本。通过这个过程更新模型的方式已被归类为以下三种当前思想流派：
- en: '**Metric based**: Solutions are so named because they depend on training a
    metric to gauge and monitor performance. This often requires the learner to learn
    the kernel that defines the distribution the network is trying to model rather
    than tune the network explicitly. What we find is that adversarial learning, using
    two somewhat opposing networks, can balance and refine this learning to learn
    the metric in a form of encoding or embedding. Some great examples of this type
    of approach are convolutional siamese networks for few-shot learning, matching
    networks, full-context embeddings, relational networks, and prototypical networks.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于度量**：这些解决方案之所以被称为如此，是因为它们依赖于训练一个度量来评估和监控性能。这通常要求学习者在定义网络试图模拟的分布的核而不是显式调整网络。我们发现，使用两个相对对立的网络进行对抗性学习可以平衡和细化这种学习，以编码或嵌入的形式学习度量。这种类型方法的一些优秀例子包括用于少样本学习的卷积孪生网络、匹配网络、全上下文嵌入、关系网络和原型网络。'
- en: '**Model based**: Solutions define a group of methods that rely on some form
    of memory augmentation or context. Memory-augmented neural networks, or MANNs,
    are the primary implementation you will find using this solution. This concept
    is further based on a neural Turing machine (NTM), which describes a controller
    network that learns to read and write from memory-soft attention. An example of
    how this looks is taken from the following NTM architecture diagram:'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于模型**：解决方案定义了一组依赖于某种形式记忆增强或上下文的方法。记忆增强神经网络（MANN）是使用此解决方案时将找到的主要实现。这一概念进一步基于神经图灵机（NTM），它描述了一个学习从记忆-软注意力中读取和写入的控制器网络。以下NTM架构图展示了这一概念的一个例子：'
- en: '![](img/1cb35d13-5976-48a3-add9-d2cbe0caa944.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1cb35d13-5976-48a3-add9-d2cbe0caa944.png)'
- en: Example of a neural Turing machine
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 神经图灵机的例子
- en: The NTM architecture is used to power the MANN model for meta learning. Training
    a MANN requires some attention to the details of how the model memorizes and encodes
    the tasks. This model is often trained in such a way as to lengthen the amount
    of time that it takes for disparate training data to be reintroduced and remembered.
    Essentially, the agent is trained longer and longer for individual tasks, and
    then forced to recall memory of prelearned tasks later in further training. Interestingly
    enough, this is a method we humans will often use to focus on learning specific
    complex tasks. Then, we retest this knowledge later in order to reinforce this
    knowledge in memory. This same concept very much applies to MANN, and many believe
    that NTM or memory is a key ingredient to any meta learning pattern.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NTM架构被用来为元学习提供动力。训练MANN需要对模型如何记忆和编码任务细节给予关注。这个模型通常以延长不同训练数据重新引入和记忆所需的时间的方式来训练。本质上，代理（agent）在单个任务上的训练时间会越来越长，然后在后续的训练中被迫回忆预先学习任务的记忆。有趣的是，这正是我们人类经常用来专注于学习特定复杂任务的方法。然后，我们会在之后重新测试这些知识，以加强记忆中的这些知识。这个概念在MANN中同样适用，许多人认为NTM或记忆是任何元学习模式的关键成分。
- en: '**Optimization based**: Solutions are as much a combination of the two previous
    solutions as it is an antipattern. In optimization-based problems, we consider
    the root of the problem, and therefore the problem of optimizing our function,
    using not only gradient descent but also introducing gradient descent through
    context or time. Gradient descent through context or time is also known as **backpropagation
    through time** (**BPTT**), and is something we briefly touched on when we looked
    at recurrent networks. By introducing recurrent networks or **long short-term
    memory** (**LSTM**) layers into a network, we encourage the network to remember
    gradient context. Another way to think of this is that the network learns the
    history of the gradients it applied during training. The meta learner therefore
    gets trained using the process shown in the following diagram:'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于优化的**：解决方案既是前两种解决方案的结合，也是一种反模式。在基于优化的问题中，我们考虑问题的根源，因此优化我们的函数问题，不仅使用梯度下降，还通过上下文或时间引入梯度下降。通过上下文或时间进行的梯度下降也被称为**时间反向传播**（**BPTT**），这是我们查看循环网络时简要提到过的。通过将循环网络或**长短期记忆**（**LSTM**）层引入网络，我们鼓励网络记住梯度上下文。另一种思考方式是，网络学习它在训练期间应用的梯度历史。因此，元学习器通过以下图示所示的过程进行训练：'
- en: '![](img/5ced2008-0997-412b-ab93-d79c698f80bb.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5ced2008-0997-412b-ab93-d79c698f80bb.png)'
- en: Training the meta learned with LSTM
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LSTM训练元学习
- en: The diagram was sourced from the paper *Optimization as a Model for Few-Shot
    Learning*, by Sachin Ravi and Hugo Larochelle, and it is a much-simplified version
    of the original. In the diagram, we can see how the meta learner is trained outside
    the regular model, often in an outside loop, where the inside loop is defined
    as the training process of the individual classification, regression, or other
    forms of learning-based task.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 该图来源于Sachin Ravi和Hugo Larochelle发表的论文《Optimization as a Model for Few-Shot Learning》，并且是原始版本的一个大幅简化版。在图中，我们可以看到元学习器是如何在常规模型之外进行训练的，通常在一个外部循环中，而内部循环则定义为单个分类、回归或其他基于学习任务的训练过程。
- en: While there are three different forms of meta learning, we will pay particular
    attention to the optimization form, and in particular, a method that works by
    assuming an agnostic model, which we will explore in the next section.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然存在三种不同的元学习形式，但我们将特别关注优化形式，特别是通过假设无模型（agnostic model）的方法，我们将在下一节中探讨。
- en: Model-agnostic meta learning
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型无关的元学习
- en: '**Model-agnostic meta learning** (**MAML**) is described as a general optimization
    method that will work with any machine learning method that uses gradient descent
    for optimization or learning. The intuition here is that we want to find a loss
    approximation that best matches the task we are currently undertaking. MAML does
    this by adding context through our model training tasks. That context is used
    to refine the model training parameters and thereby allow our model to better
    apply gradient loss for a specific task.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型无关的元学习**（**MAML**）被描述为一种通用的优化方法，它将适用于任何使用梯度下降进行优化或学习的机器学习方法。这里的直觉是我们希望找到一个损失近似，它能最好地匹配我们当前正在执行的任务。MAML通过在模型训练任务中添加上下文来实现这一点。这个上下文被用来细化模型训练参数，从而使我们的模型能够更好地为特定任务应用梯度损失。'
- en: This example uses the MNIST dataset, a set of 60,000 handwritten digits that
    is commonly used for base image classification tasks. While the dataset has been
    solved with high accuracy using a number of methods, it is often the base comparison
    for image classification tasks.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子使用了MNIST数据集，这是一个包含60,000个手写数字的集合，通常用于基本的图像分类任务。虽然该数据集已经通过多种方法以高精度解决，但它通常是图像分类任务的基准比较。
- en: 'This will likely still sound abstract, so in the next exercise, we pull down
    a PyTorch ML framework called `learn2learn` and show how MAML can be used:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能仍然听起来很抽象，所以在下一次练习中，我们将下载一个名为`learn2learn`的PyTorch机器学习框架，并展示如何使用MAML：
- en: 'We will first create a new virtual environment and then download a package
    called `learn2learn`, a meta learning framework that provides a great implementation
    of MAML in PyTorch. Make sure that you create a new environment and install PyTorch
    and the Gym environment, as we previously did. You can install `learn2learn` with
    the following command:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先创建一个新的虚拟环境，然后下载一个名为`learn2learn`的包，这是一个元学习框架，它提供了PyTorch中MAML的出色实现。确保你创建一个新的环境并安装PyTorch和Gym环境，就像我们之前做的那样。你可以使用以下命令安装`learn2learn`：
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In order to see how `learn2learn` is used in a basic task, we are going to
    review the basic MNIST training sample found in the repository, but we won''t
    look at every section of the code example that has been provided in the source
    code `Chapter_14_learn.py`. Open the sample up and review the top section of the
    code, as shown in the following code:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了了解如何在基本任务中使用`learn2learn`，我们将回顾存储库中找到的基本MNIST训练样本，但不会查看源代码`Chapter_14_learn.py`中提供的每个代码示例的每个部分。打开样本并查看代码的顶部部分，如下所示：
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This top section of code shows the `learn2learn` `import` statement and the
    definition of the `Net` class. This is the network model we will be training.
    Note how the model is composed of two convolutional/pooling layers followed by
    a fully connected linear layer to an output layer. Note the use of `ways` as an
    input variable that defines the number of outputs from the last output layer.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这段代码的顶部部分显示了`learn2learn`的`import`语句和`Net`类的定义。这是我们将要训练的网络模型。注意模型由两个卷积/池化层组成，随后是一个连接到输出层的全连接线性层。注意使用`ways`作为输入变量，它定义了最后一个输出层的输出数量。
- en: 'Next, we will scroll down to the `main` function. This is where all the main
    setup and initialization occurs. This sample being more robust than most, it provides
    for input parameters that you can use instead of altering the hyperparameters
    in the code. The top of the `main` function is shown in the following code:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将向下滚动到`main`函数。这里发生所有的主设置和初始化。这个样本比大多数样本更健壮，它提供了输入参数，你可以使用这些参数而不是在代码中修改超参数。以下代码展示了`main`函数的顶部：
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Although we haven't gone through an image classification example before, hopefully
    the code will be relatively understandable and familiar to you. The main point
    to note is the construction of the `meta_model` using the `l2l.algorithms.MAML`
    model on the highlighted line of code. Note how the `meta_model` wraps the `model` network
    by using it as an input.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 虽然我们之前没有通过图像分类示例，但希望代码对你来说相对容易理解且熟悉。需要注意的是，在代码高亮行使用`l2l.algorithms.MAML`模型构建`meta_model`。注意`meta_model`是如何通过将其作为输入来包装`model`网络的。
- en: 'From here, we will scroll down to the familiar training loop we have seen so
    many times before. This time, however, there are some interesting differences.
    Look specifically at the code just inside the first iteration loop, as shown in
    the following code:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从这里，我们将向下滚动到我们之前多次见过的熟悉的训练循环。然而，这次有一些有趣的不同之处。具体查看第一个迭代循环内部的代码，如下所示：
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note how we are constructing a `learner` clone of the `meta_model` learner.
    The `learner` clone becomes our target learning network. The last two lines show
    the construction of a sampler for the training and validation tasks.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们是如何构建一个`meta_model`学习器的`learner`克隆。这个`learner`克隆变成了我们的目标学习网络。最后两行展示了为训练和验证任务构建采样器的过程。
- en: 'Next, let''s see how `learner` is used to compute the loss again in an iterative
    manner using another loop, as shown in the following code:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们看看如何使用`learner`在另一个循环中以迭代方式再次计算损失，如下所示：
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: At this point, run the sample and observe the output to get a sense of how training
    is done.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个阶段，运行样本并观察输出，以了解训练是如何进行的。
- en: Now that we understand some of the basic code setup, we are going to move on
    to explore how the sample trains and computes loss in the next section.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了一些基本的代码设置，我们将继续探索如何在下一节中探索样本的训练和损失计算。
- en: Training a meta learner
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练元学习器
- en: 'The `learn2learn` framework provides the MAML framework for building the learner
    model we can use to learn to learn; however, it is not automatic and does require
    a bit of setup and thought regarding how loss is computed for your particular
    set of tasks. We have already seen where we compute loss—now we will look closer
    at how loss is computed across tasks. Reopen `Chapter_14_learn.py` and go through
    the following exercise:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`learn2learn`框架提供了MAML框架，用于构建我们可以用来学习如何学习的学习者模型；然而，它不是自动的，并且确实需要一些设置和思考，关于如何计算特定任务集的损失。我们已经看到了我们计算损失的地方——现在我们将更详细地看看如何在任务之间计算损失。重新打开`Chapter_14_learn.py`并完成以下练习：'
- en: Scroll back down to the innermost training loop within the `main` function.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 滚动回`main`函数中最内层的训练循环。
- en: 'The inner loop here is called a **fast adaptive training loop**, since we are
    showing our network a few or mini batches or shots of data for training. Computing
    the loss of the network is done using the `compute_loss` function, as shown in
    the following code:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这里的内部循环被称为**快速自适应训练循环**，因为我们向网络展示了一些或小批量或数据样本进行训练。计算网络的损失是通过`compute_loss`函数完成的，如下面的代码所示：
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note how the computation of loss is done iteratively over task training batches
    by iterating through the `dataloader` list. We then compute the average loss for
    all tasks by taking the total loss, `loss`, and dividing it by the number of dataloaders.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意损失是如何通过迭代任务训练批次来计算的，通过迭代`dataloader`列表。然后我们通过将总损失`loss`除以数据加载器的数量来计算所有任务的平均损失。
- en: 'This average `loss` and accuracy, `acc`, are returned from the `compute_loss`
    function. From that learning instance, the learner is then adapted or updated
    using the following line of code:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个平均`loss`和准确度`acc`由`compute_loss`函数返回。从这个学习实例中，学习者随后使用以下代码行进行适应或更新：
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'After the fast adaptive looping and updating the learner through each loop,
    we can then validate the learner with the following code:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在快速自适应循环和通过每个循环更新学习者之后，我们可以使用以下代码验证学习者：
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The `valid_error` validation error and `valid_acc` accuracy are then accumulated
    on the total `iteration_error` error and `iteration_acc` accuracy values.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`valid_error`验证错误和`valid_acc`准确度随后累计在总的`iteration_error`错误和`iteration_acc`准确度值上。'
- en: 'We finish by calculating the average iteration and accuracy errors, `iteration_error`
    or `iteration_acc` values, and then propagating that error back through the networks
    with the following code:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过以下代码计算平均迭代和准确度误差，`iteration_error`或`iteration_acc`值，然后将该误差反向传播到网络中：
- en: '[PRE8]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The training for this example is quite quick, so run the example again and observe
    how quickly the algorithm can train across meta learning tasks.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个示例的训练相当快，所以再次运行示例并观察算法在元学习任务上的训练速度有多快。
- en: Each meta learning step involves pushing the loss back through the network using
    BPTT, since the meta network is composed of recurrent layers. That detail is abstracted
    for us here, but hopefully you can appreciate how seamlessly we were able to introduce
    meta learning into training this regular image classification task. In the next
    section, we will look at how we can apply meta learning to reinforcement learning.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 每个元学习步骤都涉及使用BPTT将损失反向推回网络，因为元网络由循环层组成。这个细节在这里被抽象化，但希望你能欣赏我们如何无缝地将元学习引入到训练这个常规图像分类任务中。在下一节中，我们将看看如何将元学习应用于强化学习。
- en: Introducing meta reinforcement learning
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍元强化学习
- en: 'Now, that we understand the concept of meta learning, we can move on to meta
    reinforcement learning. Meta-RL—or RL^2 (RL Squared), as it has been called—is
    quickly evolving, but the additional complexity still makes this method currently
    inaccessible. While the concept is very similar to vanilla meta, it still introduces
    a number of subtle nuances for RL. Some of these can be difficult to understand,
    so hopefully the following diagram can help. It was taken from a paper titled
    *Reinforcement Learning, Fast and Slow* by *Botvinick, et al. 2019* ([https://www.cell.com/action/showPdf?pii=S1364-6613%2819%2930061-0](https://www.cell.com/action/showPdf?pii=S1364-6613%2819%2930061-0)):'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们理解了元学习的概念，我们可以继续探讨元强化学习。元-RL——或者称为RL^2（RL平方），正如它被称呼的那样——正在迅速发展，但额外的复杂性仍然使得这种方法目前难以接触。虽然概念与原始元学习非常相似，但它仍然为RL引入了许多细微差别。其中一些可能难以理解，因此希望以下图表能有所帮助。它来自一篇题为《强化学习，快与慢》的论文，作者为Botvinick等人，2019年([https://www.cell.com/action/showPdf?pii=S1364-6613%2819%2930061-0](https://www.cell.com/action/showPdf?pii=S1364-6613%2819%2930061-0))：
- en: '![](img/0f6a295d-b2dc-4130-a376-e93868d180ba.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![模型记忆](img/0f6a295d-b2dc-4130-a376-e93868d180ba.png)'
- en: Meta reinforcement learning
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 元强化学习
- en: 'In the diagram, you can see that familiar inner and outer loops that are characteristic
    of meta learning. This means that we also go from evaluating a policy ![](img/7715985a-bfda-45c7-9fad-ac9d84f9253d.png)
    for any observed state to also now including the last action, last reward, and
    observed state in meta-RL. These differences are summarized as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在图中，你可以看到典型的元学习特征，即熟悉的内外循环。这意味着我们也在元-RL中从评估任何观察到的状态的政策，现在也包括最后一步、最后奖励和观察到的状态。这些差异总结如下：
- en: Reinforcement learning = ![](img/c6e1f0bd-6030-4027-a98a-9007f4c48774.png) distribution
    over ![](img/3088d7ca-eb64-4bfb-8ef9-a79bedb4d2d2.png)
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习 = ![强化学习](img/c6e1f0bd-6030-4027-a98a-9007f4c48774.png) 分布在 ![状态空间](img/3088d7ca-eb64-4bfb-8ef9-a79bedb4d2d2.png)
    上
- en: Meta reinforcement learning = ![](img/591c6379-1708-4e19-af52-7ec21db9225e.png) distribution
    over ![](img/1d1e849d-5700-4005-bba1-841cb7b6a3e7.png)
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 元强化学习 = ![元强化学习](img/591c6379-1708-4e19-af52-7ec21db9225e.png) 分布在 ![状态空间](img/1d1e849d-5700-4005-bba1-841cb7b6a3e7.png)
    上
- en: 'As we have seen with regular meta learning, there are a number of variations
    that are used and experimented on within meta-RL, but they all share the following
    three common elements:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的常规元学习，元-RL中有许多变体被使用和实验，但它们都共享以下三个共同元素：
- en: '**Model memory**: We add memory to our model in the form of recurrent network
    layers or LSTM. Typically, the outer loop is composed of the memory component
    engaged by LSTM layers.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型记忆**：我们通过循环网络层或LSTM的形式向我们的模型添加记忆。通常，外循环由LSTM层激活的记忆组件组成。'
- en: '**Distribution of MDPs**: The agent/algorithm needs to be trained across multiple
    different MDPs, which is typically done by exposing it to different or randomized
    environments.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MDPs的分布**：代理/算法需要在多个不同的MDPs上进行训练，这通常是通过将其暴露于不同的或随机化的环境中来完成的。'
- en: '**Meta-learning algorithm**: The agent needs a meta learning algorithm for
    itself to learn to learn.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**元学习算法**：代理需要一种元学习算法来学习如何学习。'
- en: The Unity Obstacle Tower Challenge was most likely developed to encourage developers
    to build a meta-RL agent, but as we have seen, the winning entry used a variation
    of hierarchical reinforcement learning. While HRL is designed to accomplish the
    same function as meta-RL, it lacks the automatic generation of memory.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Unity Obstacle Tower Challenge很可能是为了鼓励开发者构建元-RL代理而开发的，但正如我们所看到的，获胜的条目使用了分层强化学习的变体。虽然HRL旨在完成与元-RL相同的功能，但它缺乏自动生成记忆的能力。
- en: 'In order to get a sense of the diversity of meta-RL algorithms, we will look
    at a list of what appear to be the most current methods used:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解元-RL算法的多样性，我们将查看一个似乎是最新的方法列表：
- en: '**Optimizing weights**: This is essentially MAML or another variation called
    Reptile. MAML is currently one of the more popular variations used, and one we
    will explore later in detail.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化权重**：这本质上上是MAML或另一个称为Reptile的变体。MAML是目前使用较为流行的变体之一，我们将在稍后详细探讨。'
- en: '**Meta-learning hyperparameters**: There are a few hyperparameters that we
    use internally to balance learning within RL. These are the gamma and alpha values
    that we have tuned before, but imagine if they could be autotuned with meta-learning.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**元学习超参数**：我们有几个内部使用的超参数来平衡RL中的学习。这些是我们之前调整过的gamma和alpha值，但想象一下，如果它们可以通过元学习来自动调整。'
- en: '**Meta-learning loss**: This considers that the loss function itself may need
    to be tuned, and uses a pattern to evolve it over iterations. This method uses
    evolutionary strategies that are outside the scope of this book.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**元学习损失**：这考虑了损失函数本身可能需要调整，并使用一种模式在迭代中进化它。这种方法使用的是本书范围之外的进化策略。'
- en: '**Meta-learning exploration**: This uses meta learning to build more effective
    exploration strategies. This, in turn, reduces the amount of time exploring and
    increases effective training performance.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**元学习探索**：这使用元学习来构建更有效的探索策略。这反过来又减少了探索所需的时间，并提高了有效的训练性能。'
- en: '**Episodic control**: This provides the agent with a method to keep important
    episodes in memory and forget others. This sounds a lot like prioritized experience
    replay, but the method of control here is within the calculation of loss and not
    from replay.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**周期性控制**：这为智能体提供了一种方法来保持重要的周期在记忆中，并忘记其他周期。这听起来很像优先经验回放，但这里的控制方法是在损失计算中，而不是从回放中。'
- en: '**Evolutionary algorithms**: These are gradient-free, optimization-based solutions
    that use a form of genetic search to find solution methods. The collision of evolutionary
    algorithms and deep learning is an ongoing endeavor that many have tried and failed
    with. Both methods are very powerful and capable on their own, so it is perhaps
    only a matter of time before they get combined into a working model.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**进化算法**：这些是梯度无关的、基于优化的解决方案，它们使用一种遗传搜索形式来找到解决方案方法。进化算法与深度学习的碰撞是一个持续的努力，许多人尝试过并失败了。这两种方法本身都非常强大和有能力，因此它们可能很快就会结合成一个有效的模型。'
- en: As you can see, there is plenty of variation in meta-RL methods, and we will
    look at how one method is implemented in detail in the next section.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，元RL方法有很多变化，我们将在下一节中详细查看一种方法的实现。
- en: MAML-RL
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MAML-RL
- en: The `learn2learn` repository holds another great example of how to use their
    library for a few variations of this method. A good method for us to look at will
    be an implementation of Meta-SGD, which further extends MAML by adopting per-parameter
    learning rates using vanilla policy gradients, and is often referred to as MetaSGD-VPG.
    This concept was originally presented in the paper *Meta Reinforcement Learning
    with Task Embedding and Shared Policy*, which was itself presented at IJCAI-19.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`learn2learn`存储库包含如何使用他们的库来实现这种方法的一些变体的另一个很好的例子。我们将要查看的一个好方法是Meta-SGD的实现，它通过采用vanilla策略梯度来使用每个参数的学习率进一步扩展了MAML，通常被称为MetaSGD-VPG。这个概念最初在论文《带有任务嵌入和共享策略的元强化学习》中提出，该论文本身是在IJCAI-19上提出的。'
- en: Make sure that you have completed all the installation steps from the last exercise
    before proceeding. If you have troubles running the sample, repeat the installation
    in a new virtual environment. Some issues may be related to the version of PyTorch
    you are using, so check that your version is compatible.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，请确保您已经完成了上一个练习中的所有安装步骤。如果您在运行示例时遇到问题，请在新虚拟环境中重新安装。一些问题可能与您使用的PyTorch版本有关，因此请检查您的版本是否兼容。
- en: 'Open up `Chapter_14_MetaSGD-VPG.py` and go through the following steps:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 打开`Chapter_14_MetaSGD-VPG.py`并按照以下步骤进行：
- en: 'You will need to install the cherry RL package first by entering the following
    command in your virtual environment window:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您需要首先通过在您的虚拟环境窗口中输入以下命令来安装cherry RL包：
- en: '[PRE9]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We won''t review the entire code listing, just the critical sections. First,
    let''s look at the `main` function, which starts the initialization and hosts
    the training. The start of this function is shown in the following code:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们不会审查整个代码列表，只审查关键部分。首先，让我们看看`main`函数，它启动初始化并托管训练。此函数的开始部分如下所示：
- en: '[PRE10]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In the definition of the `main` function, we can see all the relevant hyperparameters
    as well as their selected defaults. Note that the two new groups of hyperparameters
    for the adaption and meta learning steps are prefixed with `adapt` and `meta`,
    respectively.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在`main`函数的定义中，我们可以看到所有相关的超参数以及它们选择的默认值。请注意，用于适应和元学习步骤的两个新超参数组分别以前缀`adapt`和`meta`开头。
- en: 'Next, we will look at the initialization of the environment, policy, meta learner,
    and optimizer using the following code:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将使用以下代码查看环境的初始化、策略、元学习者和优化器的初始化：
- en: '[PRE11]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Here, we can see the three training loops. First, the outer iteration loop
    controls the number of meta learning repetitions. Inside that loop, we have the
    task setup and configuration loop; remember that we want each learning session
    to require a different but related task. The third, innermost loop is where the
    adaption occurs, and we push the loss back through the model. The code for all
    three loops is shown here:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到三个训练循环。首先，外层迭代循环控制元学习的重复次数。在这个循环内部，我们有任务设置和配置循环；记住，我们希望每个学习会话都需要一个不同但相关的任务。第三个、最内层的循环是自适应发生的地方，我们将损失反向传递通过模型。所有三个循环的代码如下所示：
- en: '[PRE12]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'After the fast adaptive looping takes place, we then jump back to the second
    loop and calculate the validation loss with the following code:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在快速自适应循环完成后，我们然后回到第二个循环并使用以下代码计算验证损失：
- en: '[PRE13]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The validation loss is computed over the second loop for each different task.
    This loss is then accumulated into the iteration loss, `iteration_loss`, value.
    Leaving the second loop, we then print out some stats and calculate the adaption
    loss, `adaption_loss`, and push that as a gradient back through the network for
    training with the following code:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证损失是在第二个循环中为每个不同的任务计算的。然后，这个损失被累积到迭代损失 `iteration_loss` 中。离开第二个循环后，我们打印出一些统计数据并计算自适应损失
    `adaption_loss`，并使用以下代码将这个损失作为梯度反向传递通过网络进行训练：
- en: '[PRE14]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Remember that the divisors in both loss equations (iteration and adaption)
    both use a similar value of `20`, `meta_bsz` `= 20`, and `adapt_bsz = 20`. The
    base loss function is defined by the `maml_a2c_loss` and `compute_advantages`
    functions, as shown in the following code:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记住，在损失方程（迭代和自适应）中的除数都使用了一个相似的值 `20`，`meta_bsz` `= 20`，和 `adapt_bsz = 20`。基本损失函数由
    `maml_a2c_loss` 和 `compute_advantages` 函数定义，如下面的代码所示：
- en: '[PRE15]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Note how the cherry RL library saves us the implementation of some tricky code.
    Fortunately, we should already know what the cherry functions `ch.td.discount`
    and `ch.pg.generalized_advantage` are, as we encountered them in previous chapters,
    and so we won't need to review them here.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 注意樱桃RL库如何帮助我们避免了某些复杂代码的实现。幸运的是，我们应该已经知道樱桃函数 `ch.td.discount` 和 `ch.pg.generalized_advantage`
    是什么，因为我们已经在之前的章节中遇到过它们，所以我们在这里不需要回顾它们。
- en: 'Run the example as you normally would and observe the output. An example of
    the generated output is shown in the following code:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照正常方式运行示例并观察输出。以下代码展示了生成的输出示例：
- en: '![](img/55603a29-3aae-4937-ae15-b26db7666157.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/55603a29-3aae-4937-ae15-b26db7666157.png)'
- en: Example output from Chapter_14_MetaSQG-VPG.py
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 来自第14章 Chapter_14_MetaSQG-VPG.py 的示例输出
- en: Note the amount of training time that the sample expects to take when running
    on a CPU when it first starts. While the prediction comes down significantly from
    five days to just over two in less than an hour, it still demonstrates the computational
    requirements for this type of training. Therefore, if you plan to do any serious
    work on meta-RL, you will likely want to use a very fast GPU for training. When
    testing on a very fast GPU, the preceding sample took 1,000 times less time to
    process. Yes, you read that right, 1,000 times less time. While you likely may
    not experience such a vast difference, any upgrade from a CPU to GPU will be significant.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 注意样本在首次运行于CPU时预期的训练时间量。当预测在不到一个小时的时间内从五天减少到略超过两天时，它仍然展示了此类训练的计算需求。因此，如果你计划进行任何严肃的元强化学习（meta-RL）工作，你可能会想使用一个非常快的GPU进行训练。在非常快的GPU上进行测试时，前面的样本处理时间减少了1,000倍。是的，你读对了，1,000倍。虽然你可能不会体验到如此巨大的差异，但从CPU升级到GPU的任何提升都将非常显著。
- en: There is a strong belief that is held by many of those in the RL community that
    meta-RL is the next big leap that we need to solve in order to get closer to AGI.
    Most of the development of this field is still guided by what is currently the
    state of the art, and how and when changes will dictate the future of RL. With
    this in mind, we are going to look at some other potential next-level steps, starting
    in the next section with HER.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习（RL）社区中，许多人坚信元强化学习（meta-RL）是我们需要解决以接近通用人工智能（AGI）的下一个重大飞跃。这个领域的大部分发展仍然由当前最先进的技术所指导，以及如何和何时变化将决定RL的未来。考虑到这一点，我们将探讨一些其他潜在的下一步，从下一节中的HER开始。
- en: Using hindsight experience replay
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用事后经验重放
- en: Hindsight experience replay was introduced by OpenAI as a method to deal with
    sparse rewards, but the algorithm has also been shown to successfully generalize
    across tasks due in part to the novel mechanism by which HER works. The analogy
    used to explain HER is a game of shuffleboard, the object of which is to slide
    a disc down a long table to reach a goal target. When first learning the game,
    we will often repeatedly fail, with the disc falling off the table or playing
    area. Except, it is presumed that we learn by expecting to fail and give ourselves
    a reward when we do so. Then, internally, we can work backward by reducing our
    failure reward and thereby increasing other non-failure rewards. In some ways,
    this method resembles Pierarchy (a form of HRL that we looked at earlier), but
    without the extensive pretraining parts.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI引入了回溯经验重放作为处理稀疏奖励的方法，但该算法也已被证明能够成功地在任务间泛化，部分原因是HER工作的新颖机制。用来解释HER的类比是一个保龄球游戏，其目的是将一个圆盘滑过一张长桌以达到目标。当我们刚开始学习这个游戏时，我们经常会反复失败，圆盘掉落桌子或游戏区域。但是，我们假设我们通过预期失败并在失败时给自己奖励来学习。然后，在内部，我们可以通过减少失败奖励来反向工作，从而增加其他非失败奖励。在某种程度上，这种方法类似于层次强化学习（我们之前看过的HRL的一种形式），但没有广泛的预训练部分。
- en: The next collection of samples in the following sections has again been sourced
    from [https://github.com/higgsfield](https://github.com/higgsfield), and are the
    result of a young man named Dulat Yerzat from Almaty, Kazakhstan.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的部分样本再次来源于[https://github.com/higgsfield](https://github.com/higgsfield)，是来自哈萨克斯坦阿拉木图的一名年轻人Dulat
    Yerzat的结果。
- en: 'Open the samples for `Chapter_14_wo_HER.py` and `Chapter_14_HER.py`. These
    two samples are comparisons of simple DQN networks that are applied with and without
    HER. Go through the following steps:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 打开`Chapter_14_wo_HER.py`和`Chapter_14_HER.py`的样本。这两个样本是应用了和未应用HER的简单DQN网络的比较。按照以下步骤进行：
- en: 'Both examples are almost the same, aside from the implementation of HER, so
    the comparison will help us understand how the code works. Next, the environment
    has been simplified and custom built to perform that simple bit shifting of a
    random set of bits. The code to create the environment is as follows:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这两个示例几乎相同，除了HER的实现，所以比较将帮助我们理解代码的工作方式。接下来，环境已经被简化并定制构建，以执行简单的随机位移动操作。创建环境的代码如下：
- en: '[PRE16]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We never really went over how to build a custom environment, but as you can
    see, it can be quite simple. Next, we will look at the simple DQN model that we
    will use to train, as shown in the following code:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从未真正讲解过如何构建自定义环境，但正如你所见，它可以相当简单。接下来，我们将查看我们将用于训练的简单DQN模型，如下面的代码所示：
- en: '[PRE17]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'That is about as simple a DQN model as you can get. Next, let''s compare the
    two examples by viewing the code side by side, as shown in the following screenshot:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这就是你可以得到的简单DQN模型。接下来，让我们通过并排查看代码来比较这两个示例，如下面的截图所示：
- en: '![](img/1c6c16c6-4389-42fd-a144-2d488bc5b30f.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1c6c16c6-4389-42fd-a144-2d488bc5b30f.png)'
- en: Comparison of code examples in VS
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: VS中的代码示例比较
- en: 'The new section of code is also shown here:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 新的代码部分也在这里显示：
- en: '[PRE18]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: What we see here is the addition of another loop not unlike meta-RL, but this
    time, instead of wrapping the inner loop, it sits as a sibling. The second loop
    is activated after an episode is completed from the first inner loop. It then
    loops through every event in the previous episode and adjusts the goals or targets
    based on the returned reward based on the new goal. This is essentially the hindsight
    part.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在这里看到的是添加了另一个循环，这与元强化学习类似，但这次它作为一个兄弟存在。第二个循环在第一个内部循环完成一个回合后激活。然后，它遍历前一个回合中的每个事件，并根据新的目标调整目标或目标，基于返回的奖励。这本质上就是回溯部分。
- en: 'The remaining parts of this example resemble many of our previous examples,
    and should be quite familiar by now. One interesting part, though, is the `get_action`
    function, as shown in the following code:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个例子剩余的部分与我们之前的许多例子相似，现在应该已经很熟悉了。但有趣的部分是`get_action`函数，如下面的代码所示：
- en: '[PRE19]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Note here that we are using an `epsilon` value that is defaulted to `.1` to
    denote the tendency for exploration. In fact, you might notice that this example
    uses no variable exploration.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这里我们使用了一个默认为`.1`的`epsilon`值，表示探索的倾向。实际上，你可能注意到这个例子没有使用变量探索。
- en: 'Continuing with the differences, the next key difference is the `compute_td_loss`
    function, as shown in the following code:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续探讨差异，下一个关键差异是`compute_td_loss`函数，如下面的代码所示：
- en: '[PRE20]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Run the example without HER first and observe the results, then run the example
    with HER. The output for the example with HER is shown in the following excerpt:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先运行不带HER的示例并观察结果，然后运行带有HER的示例。带有HER的示例的输出如下所示：
- en: '![](img/79196e9f-67de-45fe-aec1-383d28af335a.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/79196e9f-67de-45fe-aec1-383d28af335a.png)'
- en: Example output from Chapter_14_HER.py
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 从`Chapter_14_HER.py`章节的示例输出
- en: Compared to the example without HER, the preceding output is significantly better.
    You will have to run both examples yourself to see the exact difference. Note
    how the calculation of loss remains consistently variable and doesn't converge,
    while the mean reward increases. In the next section, we move to what is expected
    to be the next wave in RL—imagination and reasoning.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 与没有HER的示例相比，前面的输出明显更好。您需要亲自运行这两个示例以查看确切差异。注意损失的计算始终保持一致且没有收敛，而平均奖励却在增加。在下一节中，我们将转向RL领域预期的下一波浪潮——想象和推理。
- en: Imagination and reasoning in RL
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RL中的想象和推理
- en: 'Something that we can observe from our own experience of learning is how imagination
    can benefit the learning process. Pure imagination is the stuff of deep abstract
    thoughts and dreams, often closer to a hallucination than any way to solve a real
    problem. Except, this same imagination can be used to span gaps in our understanding
    of knowledge and allow us to reason out possible solutions. Say that we are trying
    to solve the problem of putting a puzzle together, and all we have are three remaining,
    mostly black pieces, as shown in the following image:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们自己的学习经验中，我们可以观察到想象如何有助于学习过程。纯想象是深层次抽象思维和梦想的素材，通常比解决实际问题的任何方法更接近于幻觉。然而，这种相同的想象力可以用来填补我们对知识的理解中的空白，并允许我们推理出可能的解决方案。比如说，我们正在尝试解决拼图的问题，而我们只有三个剩余的、大部分是黑色的碎片，如下面的图像所示：
- en: '![](img/88801726-c736-427f-9580-328e2c8886a0.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/88801726-c736-427f-9580-328e2c8886a0.png)'
- en: Imagining what the three missing puzzle pieces may look like
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 想象三个缺失的拼图碎片可能的样子
- en: Given the simplicity of the preceding diagram, it is quite easy for us to imagine
    what those puzzle pieces may look like. We are able to fill in those gaps quite
    easily using our imagination from previous observations and reasoning. This use
    of imagination to fill in gaps is something we use all the time, and it is often
    said that the more imaginative you are, the more intelligent you are as well.
    Now it remains to be seen if this path to AI will indeed prove that theory, but
    it certainly looks like a possibility.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 由于前面的图示非常简单，我们很容易想象那些拼图碎片可能的样子。我们能够利用之前的观察和推理的想象力轻松填补这些空白。这种使用想象力填补空白的方法是我们经常使用的，人们常说，想象力越丰富，智力也越高。现在，这条通往AI的道路是否真的能证明这一理论，还有待观察，但它确实看起来是一个可能性。
- en: 'Imagination is not pulled from a vacuum, and likewise, in order to give our
    agents imagination, we have to essentially bootstrap their memory or previous
    learnings. We will do this in the next exercise in order for us to later generate
    the imagination from these learnings. Open sample `Chapter_14_Imagine_A2C.py`
    and go through the following steps:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 想象并非凭空而来，同样地，为了给我们的代理赋予想象，我们必须基本上引导他们的记忆或之前的经验。我们将在下一项练习中这样做，以便我们能够从这些经验中生成想象。打开样本`Chapter_14_Imagine_A2C.py`并按照以下步骤进行：
- en: 'The base agent we will use to generate the training bootstrap for our imagination
    will be a simple A2C Vanilla PG method. Let''s first scroll down in the file and
    look at the `ActorCritic` class that defines our agent:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用一个简单的A2C Vanilla PG方法作为生成我们想象训练引导的基础代理。让我们首先在文件中向下滚动，看看定义我们的代理的`ActorCritic`类：
- en: '[PRE21]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'What we can see is a simple PG agent that will be powered by an A2C synchronous
    actor-critic. Next, we come to another new class called `RolloutStorage`. Rollout
    storage is similar in concept to experience replay, but it also enables us to
    have an ongoing calculation of returns, as shown in the following code:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以看到一个简单的PG代理，它将由A2C同步的actor-critic提供动力。接下来，我们来到另一个新类，称为`RolloutStorage`。Rollout
    storage在概念上与经验回放相似，但它还使我们能够进行持续的回报计算，如下面的代码所示：
- en: '[PRE22]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'If we scroll down to the `main` function, we can see that there are 16 synchronous
    environments that are being run with the following code:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们滚动到`main`函数，我们可以看到有16个同步环境正在以下代码的运行下进行：
- en: '[PRE23]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We will talk more about the `RolloutStorage` class later. For now, move down
    to the training section of code. It is the typical double-loop code, the outside
    loop controlling episodes and the inside loop controlling steps, as shown in the
    following code:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将在稍后更多地讨论`RolloutStorage`类。现在，向下移动到代码的训练部分。这是典型的双层循环代码，外部循环控制剧集，内部循环控制步骤，如下面的代码所示：
- en: '[PRE24]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The rest of the training code should be familiar, but it should be worth reviewing
    in detail on your own.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 其余的训练代码应该很熟悉，但你应该自己详细复习。
- en: 'The next major difference we want to observe is at the end of the outer training
    loop. This last block of code is where the loss is calculated and pushed back
    through the network:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们想要观察的下一个主要区别是在外部训练循环的末尾。这段最后的代码块是计算损失并将其推回网络的地方：
- en: '[PRE25]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Note the highlighted line in the preceding code block. This is unique in that
    we are clipping the gradient to a maximum value that is likely to avoid exploding
    gradients. The last section of code at the end renders out the playing area and
    shows the agent playing the game.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意前面代码块中突出显示的行。这是独特的，因为我们正在将梯度裁剪到可能避免爆炸梯度的最大值。代码的最后部分渲染出游戏区域，并显示代理玩游戏。
- en: Exploding gradients are when a gradient value becomes so large that it causes
    the network to forget knowledge. The network weights start to be trained in wild
    fluctuations and any previous knowledge will often be lost.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 爆炸梯度是指梯度值变得如此之大，以至于它使网络忘记了知识。网络权重开始进行剧烈波动，任何以前的知识通常会丢失。
- en: Run the code as you normally would and observe the output.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照正常方式运行代码并观察输出。
- en: Running the preceding code will also create a saved-state dictionary of memories
    that we will use to populate the imagination later. You must run this last exercise
    to completion if you want to continue working with later exercises. In the next
    section, we will explore how these latent traces can be used to generate an agent's
    imagination.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面的代码还会创建一个保存状态字典的记忆，我们将用它来填充想象。如果你想继续进行后续练习，你必须完成这个最后的练习。在下一节中，我们将探讨如何使用这些潜在痕迹来生成代理的想象。
- en: Generating imagination
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成想象
- en: 'In the current version of this algorithm, we first need to bootstrap the memories
    that we populate in the agent with a previous run by an agent, or perhaps a human.
    This is really no different than imitation learning or behavioral cloning, except
    we are using an on-policy agent that we will later use as an off-policy base for
    our imagination. Before we combine imagination into our agent, we can see how
    the predicted next state will look compared to what the agent''s actual state
    will be. Let''s see how this works by opening up the next example `Chapter_14_Imagination.py`
    and go through the following steps:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个算法的当前版本中，我们首先需要通过一个代理或可能是一个人的先前运行来引导我们在代理中填充的记忆。这实际上与模仿学习或行为克隆没有太大区别，只是我们使用的是一个我们将后来用作想象离策略基础的在线策略代理。在我们将想象结合到代理之前，我们可以看到预测的下一个状态与代理的实际状态相比将是什么样子。让我们通过打开下一个示例`Chapter_14_Imagination.py`并执行以下步骤来了解这是如何工作的：
- en: This example works by loading the previous saved-state dictionary we generated
    in the last exercise. Make sure that this data is generated and saved with a prefix
    of `actor_critic_` files in the same folder before continuing.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个例子通过加载我们在上一个练习中生成的上一个保存状态字典来工作。在继续之前，请确保这些数据是在同一文件夹中带有前缀为`actor_critic_`的文件中生成并保存的。
- en: 'The purpose of this code is to extract the saved-state observation dictionary
    we recorded earlier. We then want to extract the observation and use it to imagine
    what the next state will look like. Then we can compare how well the imagined
    and next state resemble each other. This amount of resemblance will in turn be
    used to train an imagination loss later. We can see how the previous model is
    loaded by looking at the following line of code:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这段代码的目的是提取我们之前记录的保存状态观察字典。然后我们想要提取观察结果，并使用它来想象下一个状态将是什么样子。然后我们可以比较想象的状态和下一个状态之间的相似程度。这种相似程度将反过来用于训练想象损失。我们可以通过查看以下代码行来了解如何加载先前的模型：
- en: '[PRE26]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The preceding line of code reloads our previously trained model. Now we want
    to use the imagination to (for instance) reasonably fill in the areas where the
    agent may not have explored. Scrolling down, we can see the training loop that
    will learn the imagination part of the agent:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上一行代码重新加载了我们之前训练的模型。现在我们想使用想象力（例如）合理地填补代理可能没有探索的区域。向下滚动，我们可以看到将学习代理的想象力部分的训练循环：
- en: '[PRE27]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This loop loops through the previously played games and encodes the actions
    using one-hot encoding. Scrolling down, we can see how the `imagined_state` state
    and the `imagined_reward` reward are learned:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此循环遍历之前玩过的游戏，并使用独热编码对动作进行编码。向下滚动，我们可以看到如何学习`imagined_state`状态和`imagined_reward`奖励：
- en: '[PRE28]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This is the section of code that learns to correctly imagine the target state
    and reward from playing through the previously observed observations. Of course,
    the more observations, the better the imagination, but at some point, too many
    observations will eliminate all of the imagination entirely. Balancing this new
    trade-off will require a bit of trial and error on your own.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这是学习从之前观察到的观察中正确想象目标状态和奖励的代码部分。当然，观察越多，想象力越好，但到了某个点，过多的观察将完全消除所有想象力。平衡这种新的权衡将需要一些自己的尝试和错误。
- en: 'Scrolling down to the bottom of the file, you can see where an example of the
    imagination and target states are outputted with the following code:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向文件底部滚动，您可以看到以下代码输出的想象力和目标状态的示例：
- en: '[PRE29]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The following example screenshot shows the best the original author was able
    to get by training the agent for a considerable amount of time:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下示例截图展示了原始作者通过长时间训练代理所能达到的最佳效果：
- en: '![](img/9d845819-71d5-4b23-b772-b452c54d1bec.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9d845819-71d5-4b23-b772-b452c54d1bec.png)'
- en: Example comparison of imagined versus actual
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 想象与实际比较示例
- en: 'Run the example and your output: depending on your previous amount of training,
    it may not look as good. Again, the quality of the imagination will be based on
    the previous experiences and amount of training to refine the imagination itself.'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行示例并查看您的输出：根据您之前的训练量，它可能看起来并不那么好。再次强调，想象力的质量将基于先前的经验和训练量来完善想象力本身。
- en: 'One last thing to note is how the imagined image is getting extracted. This
    is done using an inverted CNN in the `BasicBlock` class that converts the encoding
    back to an image of the correct resolution. The code for the `BasicBlock` class
    is shown here:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后要注意的一点是如何提取想象图像。这是通过在`BasicBlock`类中使用反转CNN来完成的，它将编码转换回正确分辨率的图像。`BasicBlock`类的代码在此处显示：
- en: '[PRE30]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: As we can see, training the imagination process itself is not that difficult.
    The real difficulty is putting this all together in a running agent, and we will
    see how this is done in the next section when we learn about I2A.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，训练想象力过程本身并不困难。真正的困难是将这一切整合到一个运行代理中，我们将在下一节中了解如何做到这一点，当我们学习I2A时。
- en: Understanding imagination-augmented agents
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解增强想象力代理
- en: The concept of **imagination-augmented agents** (**I2A**) was released in a
    paper titled *Imagination-Augmented Agents for Deep Reinforcement Learning* in
    February 2018 by T. Weber, et al. We have already talked about why imagination
    is important for learning and learning to learn. Imagination allows us to fill
    in the gaps in our learning and make leaps in our knowledge, if you will.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '**增强想象力代理**（**I2A**）的概念于2018年2月由T. Weber等人发表在题为《Imagination-Augmented Agents
    for Deep Reinforcement Learning》的论文中提出。我们之前已经讨论了为什么想象力对学习和学习学习很重要。想象力使我们能够填补学习中的空白，并在我们的知识上实现飞跃。'
- en: Giving agents an imagination allows us to combine model-based and model-free
    learning. Most of the agent algorithms we have used in this book have been model-free,
    meaning that we have no representative model of the environment. Early on, we
    did cover model-based RL with MC and DP, but most of our efforts have been fixed
    on model-free agents. The benefit of having a model of the environment is that
    the agent can then plan. Without a model, our agent just becomes reactionary through
    trial and error attempts. Adding imagination allows us to combine some aspects
    of using a model of the environment while being model free. Essentially, we hope
    to achieve the best of both worlds using imagination.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 给予智能体想象能力，使我们能够结合基于模型和无模型的学习。我们在这本书中使用的多数智能体算法都是无模型的，这意味着我们没有环境的代表模型。早期，我们确实覆盖了基于模型的RL，包括MC和DP，但我们的大部分努力都集中在无模型智能体上。拥有环境模型的好处是智能体可以规划。没有模型，我们的智能体就只是通过试错尝试变得反应性。增加想象能力使我们能够在无模型的同时结合使用环境模型的一些方面。本质上，我们希望通过想象实现两者的最佳结合。
- en: 'We have already explored the core role of imagination in the I2A architecture.
    This was the part we looked at in the last section that generated the imagined
    features and reward, essentially the model part. The following diagram illustrates
    the I2A architecture, the imagination core part, and the rollout encoder:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经探讨了想象在I2A架构中的核心作用。这是我们上一节中查看的部分，它生成了想象特征和奖励，本质上属于模型部分。以下图展示了I2A架构、想象核心部分和
    rollout 编码器：
- en: '![](img/e5fd38c1-0c19-416d-b92b-70aa02ef2919.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e5fd38c1-0c19-416d-b92b-70aa02ef2919.png)'
- en: Summary of the I2A architecture
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: I2A架构总结
- en: 'The I2A architecture demonstrates the complexity of the systems that we can
    start to build on top of DRL in the hopes of adding additional learning advantages,
    such as imagination. In order to really understand this architecture, we should
    look at a code example. Open up `Chapter_14_I2A.py` and go through the following
    steps:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: I2A架构展示了我们可以构建在DRL之上的系统的复杂性，以期望添加额外的学习优势，例如想象。为了真正理解这个架构，我们应该查看一个代码示例。打开`Chapter_14_I2A.py`并按照以下步骤进行：
- en: 'We have already covered the first part of the architecture, so at this stage,
    we can start with the policy itself. Look at the I2A policy class:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们已经涵盖了架构的第一部分，所以在这个阶段，我们可以从策略本身开始。看看I2A策略类：
- en: '[PRE31]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: For the most part, this is a fairly simple PG policy, except with the addition
    of imagination elements. Note how, in the `forward` function, the forward pass
    refers to the imagination needed to extract the `imagined_state` and `imagined_reward`
    values.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在大多数情况下，这是一个相当简单的PG策略，除了增加了想象元素。注意在`forward`函数中，前向传递指的是提取`imagined_state`和`imagined_reward`值所需的想象。
- en: 'Next, we scroll down a little bit more and come to the `ImaginationCore` class.
    The class encapsulates the functionality we have seen before, but all wrapped
    in a single class, as shown in the following code:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们再向下滚动一点，来到`ImaginationCore`类。这个类封装了我们之前看到的功能，但全部封装在一个单独的类中，如下面的代码所示：
- en: '[PRE32]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now that we have seen how these big pieces work, it is time to get to the `main`
    function. We will start by looking at the first dozen or so lines of code:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经看到了这些大型组件是如何工作的，是时候进入`main`函数了。我们将从查看代码的前十几行开始：
- en: '[PRE33]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Note the flow of code. The code goes from instantiating an environment model
    `env_model` and the `distil_policy` from an `ActorCritic` class. Then the code
    sets up the optimizer and instantiates the `imagination` object of the `ImaginationCore` type with
    inputs of `env_model` and `distil_policy`. The last line creates the `actor_critic
    I2A` policy using the `imagination` object as input.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 注意代码的流程。代码从实例化环境模型`env_model`和`distil_policy`，来自`ActorCritic`类开始。然后代码设置优化器，并实例化`ImaginationCore`类型的`imagination`对象，输入为`env_model`和`distil_policy`。最后一行使用`imagination`对象作为输入创建`actor_critic
    I2A`策略。
- en: 'Jump down to the training loop. Note that it looks fairly standard:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 跳转到训练循环。注意它看起来相当标准：
- en: '[PRE34]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'After the inner episode loop is complete, we then jump down to the loss calculation
    and update the code, as shown here:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 内部剧集循环完成后，我们接着跳转到损失计算和更新代码，如下所示：
- en: '[PRE35]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: One thing to pay attention to here is that we are using two loss gradients to
    push back the loss to the `distil` model, which adjusts the `distil` model parameters
    and the `actor_critic` model or policy and its parameters. Without getting too
    bogged down in details, the main concept here is that we train the `distil` model
    to learn the imagination and the other loss for general policy training.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这里需要注意的一点是，我们正在使用两个损失梯度将损失推回到`distil`模型，该模型调整`distil`模型的参数和`actor_critic`模型或策略及其参数。不深入细节，这里的主要概念是我们训练`distil`模型来学习想象力和用于一般策略训练的其他损失。
- en: 'Run the example again. Wait until it starts and then you may want to shut it
    down after a few rounds, because, this sample can take upwards of an hour per
    iteration on a slower CPU, possibly longer. The following is an example screenshot
    of the start of training:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次运行示例。等待它开始，然后你可能想在几轮之后关闭它，因为这个样本在较慢的CPU上每迭代可能需要超过一个小时，可能更长。以下是一个训练开始的示例截图：
- en: '![](img/a625ef87-3a79-47f0-a2fb-c13ca20068f5.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a625ef87-3a79-47f0-a2fb-c13ca20068f5.png)'
- en: Example of Chapter_14_I2A.py training
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '`Chapter_14_I2A.py`训练示例'
- en: Now, if you want to run this exercise to completion, you should use a GPU at
    the very least. Ten thousand hours of CPU training would take a year, and is not
    something you will likely want to spend time on. If you do use a GPU, you will
    have to modify the sample to support a GPU, and this will require uncommenting
    sections and setting up PyTorch so that it can run with CUDA.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你想完成这个练习，至少应该使用一个GPU。在CPU上训练一万个小时需要一年时间，这并不是你愿意花时间的事情。如果你使用GPU，你将不得不修改样本以支持GPU，这将需要取消注释部分并设置PyTorch以便它可以与CUDA一起运行。
- en: This completes this section and the content for this chapter, as well as the
    book. In the next section, we will look at the last set of exercises.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了本节、本章的内容以及整本书的内容。在下一节中，我们将查看最后一组练习。
- en: Exercises
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: 'The following is a mix of simple and very difficult exercises. Choose those
    exercises that you feel appropriate to your interests, abilities, and resources.
    Some of the exercises in the following list could require considerable resources,
    so pick those that are within your time/resource budget:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些简单和非常困难的练习的组合。选择那些你觉得适合你兴趣、能力和资源的练习。以下列表中的某些练习可能需要相当多的资源，因此请选择那些在你时间/资源预算范围内的练习：
- en: Tune the hyperparameters for sample `Chapter_14_learn.py`.This sample is a standard
    deep learning model, but the parameters should be familiar enough to figure out
    on your own.
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整`Chapter_14_learn.py`样本的超参数。这个样本是一个标准的深度学习模型，但参数应该足够熟悉，可以自己找出。
- en: Tune the hyperparameters for sample `Chapter_14_MetaSGD-VPG.py`, as you normally
    would.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照常规方式调整`Chapter_14_MetaSGD-VPG.py`样本的超参数。
- en: Tune the hyperparameters for sample **`Chapter_14_Imagination.py`**. There are
    a few new hyperparameters in this sample that you should familiarize yourself
    with.
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整`Chapter_14_Imagination.py`样本的超参数。在这个样本中有几个新的超参数，你应该熟悉一下。
- en: Tune the hyperparameters for the `Chapter_14_wo_HER.py` and `Chapter_14_HER.py`
    examples. It can be very beneficial for your understanding to train the sample
    with and without HER using the same techniques.
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整`Chapter_14_wo_HER.py`和`Chapter_14_HER.py`示例的超参数。使用相同的技术在有和没有HER的情况下训练样本对你的理解非常有帮助。
- en: Tune the hyperparameters for the `Chapter_14_Imagine_A2C.py` example. What effect
    does this have on running the `Chapter_14_Imagination.py` example later?
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整`Chapter_14_Imagine_A2C.py`示例的超参数。这对后续运行`Chapter_14_Imagination.py`示例有什么影响？
- en: Upgrade the HER example (`Chapter_14_HER.py`) to use a different PG or value/DQN
    method.
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将HER示例（`Chapter_14_HER.py`）升级为使用不同的PG或价值/DQN方法。
- en: Upgrade the `Chapter_14_MetaSGD-VPG.py` example to use a more advanced PG or
    DQN method.
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`Chapter_14_MetaSGD-VPG.py`示例升级为使用更先进的PG或DQN方法。
- en: Adapt the `Chapter_14_MetaSGD-VPG.py` example to train on different environments
    that use continuous or possibly even discrete actions.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`Chapter_14_MetaSGD-VPG.py`示例调整以在不同的环境中进行训练，这些环境使用连续或甚至可能是离散的动作。
- en: Train the `Chapter_14_I2A.py `sample to completion. You will need to configure
    the example to run with CUDA, as well as install PyTorch with CUDA.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`Chapter_14_I2A.py`样本训练完成。你需要配置示例以使用CUDA运行，以及安装带有CUDA的PyTorch。
- en: Tune the hyperparameters for the `Chapter_14_I2A.py `sample. You may decide
    to do only partial training runs using just the CPU, which is acceptable. Therefore,
    you could train a couple of iterations at a time and still optimize those new
    hyperparameters.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整`Chapter_14_I2A.py`样本的超参数。你可以选择只使用CPU进行部分训练运行，这是可以接受的。因此，你可以一次训练几个迭代，并仍然优化那些新的超参数。
- en: Do the exercises of most interest to you and remember to have fun.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 做你最感兴趣的练习，并记得要享受乐趣。
- en: Summary
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we looked beyond DRL and into the realm of AGI, or at least
    where we hope we are going with AGI. More importantly, though, we looked at what
    the next phase of DRL is, how we can tackle its current shortcomings, and where
    it could go next. We looked at meta learning and what it means to learn to learn.
    Then we covered the excellent `learn2learn` library and saw how it could be used
    on a simple deep learning problem and then a more advanced meta-RL problem with
    MAML. From there, we looked at another new approach to learning using hindsight
    with HER. From hindsight, we moved to imagination and reasoning and how this could
    be incorporated into an agent. Then we finished the chapter by looking at I2A—imagination-augmented
    agents—and how imagination can help fill in the gaps in our knowledge.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们超越了DRL，进入了通用人工智能（AGI）的领域，或者至少是我们希望AGI会走向的方向。更重要的是，我们探讨了DRL的下一阶段，我们如何解决其当前的不足，以及它可能走向何方。我们探讨了元学习以及学习如何学习意味着什么。然后，我们介绍了优秀的`learn2learn`库，并看到了它如何被用于一个简单的深度学习问题，以及一个更高级的元强化学习问题（MAML）。从那里，我们探讨了使用后见之明（HER）的另一种新的学习方法。从后见之明转向想象和推理，以及这些如何被纳入智能体中。然后，我们通过探讨I2A——想象增强智能体——以及想象如何帮助我们填补知识空白来结束本章。
- en: I just want to thank you for taking the time to work through this book with
    us. It has been an amazing journey covering almost the entire RL and DRL alphabet
    of concepts, terms, and acronyms. This book started with the basics of RL and
    went deep, very deep, into DRL. Provided you have the mathematical background,
    you can likely venture out on your own now, and build your own latest and greatest
    agent. RL, and in particular DRL, suffers from the myth that you require extensive
    computational resources to make valuable contributions. While for certain research
    this is certainly the case, there are a lot of other more rudimentary elements
    that still need a better understanding that can be improved upon. The field of
    DRL is still relatively new, and it is quite likely that we have missed things
    along the way. Therefore, whatever your resources, you likely still could make
    a valuable contribution to DRL in the coming years. If you do plan to pursue this
    dream, I wish you success and hope that this book contributes to your journey.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我只想感谢你抽出时间与我们共同完成这本书。这是一段令人惊叹的旅程，涵盖了几乎整个强化学习（RL）和深度强化学习（DRL）的概念、术语和缩写。这本书从RL的基础开始，深入到DRL。只要你具备数学背景，你现在很可能可以独立探索，并构建你自己的最新和最优秀的智能体。RL，尤其是DRL，有一个神话，即你需要大量的计算资源才能做出有价值的贡献。虽然对于某些研究来说这确实如此，但还有很多更基础的元素需要更好的理解，并且可以进一步改进。DRL领域仍然相对较新，我们很可能在旅途中遗漏了一些东西。因此，无论你的资源如何，你很可能在未来几年内对DRL做出有价值的贡献。如果你确实计划追求这个梦想，我祝愿你成功，并希望这本书能对你的旅程有所帮助。
