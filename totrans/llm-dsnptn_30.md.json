["```py\nfrom typing import List, Dict, Any\nimport random\nclass LLMAgent:\n    def __init__(self, llm, action_space: List[str]):\n        self.llm = llm\n        self.action_space = action_space\n        self.memory = []\n        self.current_goal = None\n```", "```py\n    def set_goal(self, goal: str):\n        self.current_goal = goal\n    def perceive(self, observation: str):\n        self.memory.append(observation)\n```", "```py\n    def think(self) -> str:\n        context = f\"Goal: {self.current_goal}\\n\"\n        context += \"Recent observations:\\n\"\n        context += \"\\n\".join(self.memory[-5:])  # Last 5 observations\n        context += \"\\nThink about the current situation and the goal. What should be done next?\"\n        return self.llm.generate(context)\n```", "```py\n    def decide(self, thought: str) -> str:\n        context = f\"Thought: {thought}\\n\"\n        context += \"Based on this thought, choose the most appropriate action from the following:\\n\"\n        context += \", \".join(self.action_space)\n        context += \"\\nChosen action:\"\n        return self.llm.generate(context)\n```", "```py\n    def act(self, action: str) -> str:\n        outcomes = [\n            f\"Action '{action}' was successful.\",\n            f\"Action '{action}' failed.\",\n            f\"Action '{action}' had an unexpected outcome.\"\n        ]\n        return random.choice(outcomes)\n```", "```py\n    def run_step(self):\n        thought = self.think()\n        action = self.decide(thought)\n        outcome = self.act(action)\n        self.perceive(outcome)\n        return thought, action, outcome\n```", "```py\n# Example usage\nllm = SomeLLMModel()  # Replace with your actual LLM\naction_space = [\"move\", \"grab\", \"drop\", \"use\", \"talk\"]\nagent = LLMAgent(llm, action_space)\nagent.set_goal(\"Find the key and unlock the door\")\nagent.perceive(\"You are in a room with a table and a chair. There's a drawer in the table.\")\n```", "```py\nfor _ in range(5):  # Run for 5 steps\n    thought, action, outcome = agent.run_step()\n    print(f\"Thought: {thought}\")\n    print(f\"Action: {action}\")\n    print(f\"Outcome: {outcome}\")\n    print()\n```", "```py\nclass HierarchicalGoal:\n    def __init__(\n        self, description: str,\n        subgoals: List['HierarchicalGoal'] = None\n    ):\n        self.description = description\n        self.subgoals = subgoals or []\n        self.completed = False\n    def add_subgoal(self, subgoal: 'HierarchicalGoal'):\n        self.subgoals.append(subgoal)\n    def mark_completed(self):\n        self.completed = True\n```", "```py\nclass PlanningAgent(LLMAgent):\n    def __init__(self, llm, action_space: List[str]):\n        super().__init__(llm, action_space)\n        self.goal_stack = []\n        self.current_plan = []\n    def set_hierarchical_goal(self, goal: HierarchicalGoal):\n        self.goal_stack = [goal]\n```", "```py\n    def think(self) -> str:\n        if not self.current_plan:\n            self.create_plan()\n        context = f\"Current goal: {self.goal_stack[-1].description}\\n\"\n        context += \"Current plan:\\n\"\n        context += \"\\n\".join(self.current_plan)\n        context += \"\\nRecent observations:\\n\"\n        context += \"\\n\".join(self.memory[-5:])\n        context += \"\\nThink about the current situation, goal, and plan. What should be done next?\"\n        return self.llm.generate(context)\n```", "```py\n    def create_plan(self):\n        context = f\"Goal: {self.goal_stack[-1].description}\\n\"\n        context += \"Create a step-by-step plan to achieve this goal. Each step should be an action from the following list:\\n\"\n        context += \", \".join(self.action_space)\n        context += \"\\nPlan:\"\n        plan_text = self.llm.generate(context)\n        self.current_plan = [\n            step.strip() for step in plan_text.split(\"\\n\")\n            if step.strip()\n        ]\n```", "```py\n    def update_goals(self):\n        current_goal = self.goal_stack[-1]\n        if current_goal.completed:\n            self.goal_stack.pop()\n            if self.goal_stack:\n                self.current_plan = []  # Reset plan for the next goal\n        elif current_goal.subgoals:\n            next_subgoal = next(\n                (\n                    sg for sg in current_goal.subgoals\n                    if not sg.completed\n                ),\n                None\n            )\n            if next_subgoal:\n                self.goal_stack.append(next_subgoal)\n                self.current_plan = []  # Reset plan for the new subgoal\n```", "```py\n    def run_step(self):\n        thought, action, outcome = super().run_step()\n        self.update_goals()\n        return thought, action, outcome\n```", "```py\nplanning_agent = PlanningAgent(llm, action_space)\nmain_goal = HierarchicalGoal(\"Escape the room\")\nmain_goal.add_subgoal(HierarchicalGoal(\"Find the key\"))\nmain_goal.add_subgoal(HierarchicalGoal(\"Unlock the door\"))\nplanning_agent.set_hierarchical_goal(main_goal)\nplanning_agent.perceive(\"You are in a room with a table and\na chair. There's a drawer in the table.\")\nfor _ in range(10):  # Run for 10 steps\n    thought, action, outcome = planning_agent.run_step()\n    print(f\"Thought: {thought}\")\n    print(f\"Action: {action}\")\n    print(f\"Outcome: {outcome}\")\n    print(f\"Current goal: {planning_agent.goal_stack[-1].description}\")\n    print()\n```", "```py\nfrom collections import deque\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nclass MemoryEntry:\n    def __init__(self, text: str, embedding: np.ndarray):\n        self.text = text\n        self.embedding = embedding\n```", "```py\nclass EpisodicMemory:\n    def __init__(self, capacity: int, embedding_model):\n        self.capacity = capacity\n        self.embedding_model = embedding_model\n        self.memory = deque(maxlen=capacity)\n```", "```py\n    def add(self, text: str):\n        embedding = self.embedding_model.encode(text)\n        self.memory.append(MemoryEntry(text, embedding))\n    def retrieve_relevant(self, query: str, k: int = 5) -> List[str]:\n        query_embedding = self.embedding_model.encode(query)\n        similarities = [\n            cosine_similarity(\n                [query_embedding],\n                [entry.embedding]\n            )[0][0] for entry in self.memory\n        ]\n        top_indices = np.argsort(similarities)[-k:][::-1]\n        return [self.memory[i].text for i in top_indices]\n```", "```py\nclass MemoryAwareAgent(PlanningAgent):\n    def __init__(\n        self, llm, action_space: List[str], embedding_model\n    ):\n        super().__init__(llm, action_space)\n        self.episodic_memory = EpisodicMemory(\n            capacity=1000, embedding_model=embedding_model\n        )\n    def perceive(self, observation: str):\n        super().perceive(observation)\n        self.episodic_memory.add(observation)\n```", "```py\n    def think(self) -> str:\n        relevant_memories = self.episodic_memory.retrieve_relevant(\n            self.current_goal, k=3\n        )\n        context = f\"Current goal: {self.goal_stack[-1].description}\\n\"\n        context += \"Current plan:\\n\"\n        context += \"\\n\".join(self.current_plan)\n        context += \"\\nRecent observations:\\n\"\n        context += \"\\n\".join(self.memory[-5:])\n        context += \"\\nRelevant past experiences:\\n\"\n        context += \"\\n\".join(relevant_memories)\n        context += \"\\nThink about the current situation, goal, plan, and past experiences. What should be done next?\"\n        return self.llm.generate(context)\n```", "```py\nembedding_model = SomeEmbeddingModel()  # Replace with your actual embedding model\nmemory_agent = MemoryAwareAgent(llm, action_space, embedding_model)\nmain_goal = HierarchicalGoal(\"Solve the puzzle\")\nmemory_agent.set_hierarchical_goal(main_goal)\nmemory_agent.perceive(\"You are in a room with a complex puzzle on the wall.\")\nThe agent continues to interact with its environment over 10 steps, utilizing its memory system to make better decisions based on both current observations and past experiences:\nfor _ in range(10):  # Run for 10 steps\n    thought, action, outcome = memory_agent.run_step()\n    print(f\"Thought: {thought}\")\n    print(f\"Action: {action}\")\n    print(f\"Outcome: {outcome}\")\n    print()\n```", "```py\nimport numpy as np\nclass ActionEvaluator:\n    def __init__(self, llm):\n        self.llm = llm\n    def evaluate_action(\n        self, action: str, context: str\n    ) -> Dict[str, float]:\n        prompt = f\"\"\"\n        Context: {context}\n        Action: {action}\n```", "```py\n        Provide your evaluation as three numbers separated by commas:\n        \"\"\"\n        response = self.llm.generate(prompt)\n        relevance, success_prob, impact = map(\n            float, response.split(',')\n        )\n        return {\n            'relevance': relevance,\n            'success_probability': success_prob,\n            'impact': impact\n        }\n```", "```py\nclass StrategicDecisionAgent(MemoryAwareAgent):\n    def __init__(\n        self, llm, action_space: List[str], embedding_model\n    ):\n        super().__init__(llm, action_space, embedding_model)\n        self.action_evaluator = ActionEvaluator(llm)\n    def decide(self, thought: str) -> str:\n        context = f\"Thought: {thought}\\n\"\n        context += f\"Current goal: {self.goal_stack[-1].description}\\n\"\n        context += \"Recent observations:\\n\"\n        context += \"\\n\".join(self.memory[-5:])\n        action_scores = {}\n        for action in self.action_space:\n            evaluation = self.action_evaluator.evaluate_action(\n                action, context\n            )\n            score = np.mean(list(evaluation.values()))\n            action_scores[action] = score\n        best_action = max(action_scores, key=action_scores.get)\n        return best_action\n```", "```py\nstrategic_agent = StrategicDecisionAgent(\n    llm, action_space, embedding_model\n)\nmain_goal = HierarchicalGoal(\"Navigate the maze and find the treasure\")\nstrategic_agent.set_hierarchical_goal(main_goal)\nstrategic_agent.perceive(\"You are at the entrance of a complex maze. There are multiple paths ahead.\")\n```", "```py\nfor _ in range(10):  # Run for 10 steps\n    thought, action, outcome = strategic_agent.run_step()\n    print(f\"Thought: {thought}\")\n    print(f\"Chosen action: {action}\")\n    print(f\"Outcome: {outcome}\")\n    print()\n```", "```py\nimport random\nfrom collections import defaultdict\nclass AdaptiveLearningAgent(StrategicDecisionAgent):\n    def __init__(self, llm, action_space: List[str], embedding_model):\n        super().__init__(llm, action_space, embedding_model)\n        self.q_values = defaultdict(lambda: defaultdict(float))\n        self.learning_rate = 0.1\n        self.discount_factor = 0.9\n        self.epsilon = 0.1  # For exploration-exploitation tradeoff\n```", "```py\n    def decide(self, thought: str) -> str:\n        if random.random() < self.epsilon:\n            return random.choice(self.action_space)  # Exploration: randomly pick an action\n        state = self.get_state_representation()\n        q_values = {action: self.q_values[state][action]\n        for action in self.action_space}\n        return max(q_values, key=q_values.get)  # Exploitation: pick action with highest Q-value\n```", "```py\n    def get_state_representation(self) -> str:\n        return f\"Goal: {self.goal_stack[-1].description},\n            Last observation: {self.memory[-1]}\"\n```", "```py\n    def update_q_values(\n        self, state: str, action: str, reward: float,\n        next_state: str\n    ):\n        current_q = self.q_values[state][action]\n        next_max_q = max(\n            self.q_values[next_state].values()\n        ) if self.q_values[next_state] else 0\n        new_q = current_q + self.learning_rate * (\n            reward + self.discount_factor * next_max_q - current_q\n        )\n        self.q_values[state][action] = new_q\n```", "```py\n    def run_step(self):\n        state = self.get_state_representation()\n        thought, action, outcome = super().run_step()\n        next_state = self.get_state_representation()\n        reward = self.compute_reward(outcome)\n        self.update_q_values(state, action, reward, next_state)\n        return thought, action, outcome\n    def compute_reward(self, outcome: str) -> float:\n        if \"successful\" in outcome.lower():\n            return 1.0\n        elif \"failed\" in outcome.lower():\n            return -0.5\n        else:\n            return 0.0\n```", "```py\nadaptive_agent = AdaptiveLearningAgent(llm, action_space,\n    embedding_model)\nmain_goal = HierarchicalGoal(\"Explore and map the unknown planet\")\nadaptive_agent.set_hierarchical_goal(main_goal)\nadaptive_agent.perceive(\"You have landed on an alien planet. The environment is strange and unfamiliar.\")\n```", "```py\nfor _ in range(20):  # Run for 20 steps\n    thought, action, outcome = adaptive_agent.run_step()\n    print(f\"Thought: {thought}\")\n    print(f\"Chosen action: {action}\")\n    print(f\"Outcome: {outcome}\")\n    print(\n        f\"Current Q-values: {dict(\n            adaptive_agent.q_values[\n                adaptive_agent.get_state_representation()\n            ]\n        )}\"\n)\n    print()\n```", "```py\nclass EthicalConstraint:\n    def __init__(self, description: str, check_function):\n        self.description = description\n        self.check_function = check_function\n```", "```py\nclass EthicalAgent(AdaptiveLearningAgent):\n    def __init__(\n        self, llm, action_space: List[str],\n        embedding_model,\n        ethical_constraints: List[EthicalConstraint]\n    ):\n        super().__init__(llm, action_space, embedding_model)\n        self.ethical_constraints = ethical_constraints\n    def decide(self, thought: str) -> str:\n        action = super().decide(thought)\n        if not self.is_action_ethical(action, thought):\n            print(f\"Warning: Action '{action}' violated ethical constraints. Choosing a different action.\")\n            alternative_actions = [\n                a for a in self.action_space if a != action]\n            return (\n                random.choice(alternative_actions)\n                if alternative_actions\n                else \"do_nothing\"\n            )\n        return action\n    def is_action_ethical(self, action: str, context: str) -> bool:\n        for constraint in self.ethical_constraints:\n            if not constraint.check_function(action, context):\n                print(f\"Ethical constraint violated: {constraint.description}\")\n                return False\n        return True\n```", "```py\ndef no_harm(action: str, context: str) -> bool:\n    harmful_actions = [\"attack\", \"destroy\", \"damage\"]\n    return not any(ha in action.lower() for ha in harmful_actions)\ndef respect_privacy(action: str, context: str) -> bool:\n    privacy_violating_actions = [\"spy\", \"eavesdrop\", \"hack\"]\n    return not any(\n        pva in action.lower()\n        for pva in privacy_violating_actions\n    )\n```", "```py\nethical_constraints = [\n    EthicalConstraint(\"Do no harm\", no_harm),\n    EthicalConstraint(\"Respect privacy\", respect_privacy)\n]\nethical_agent = EthicalAgent(\n    llm, action_space + [\"attack\", \"spy\"],\n    embedding_model, ethical_constraints\n)\nmain_goal = HierarchicalGoal(\"Gather information about the alien civilization\")\nethical_agent.set_hierarchical_goal(main_goal)\nethical_agent.perceive(\"You've encountered an alien settlement. The inhabitants seem peaceful but wary.\")\n```", "```py\nfor _ in range(15):  # Run for 15 steps\n    thought, action, outcome = ethical_agent.run_step()\n    print(f\"Thought: {thought}\")\n    print(f\"Chosen action: {action}\")\n    print(f\"Outcome: {outcome}\")\n    print()\n```"]