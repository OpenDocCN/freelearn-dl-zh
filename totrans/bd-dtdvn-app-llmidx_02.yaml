- en: <title>Kickstarting Your Journey with LlamaIndex</title>
  prefs: []
  type: TYPE_NORMAL
- en: Kickstarting Your Journey with LlamaIndex
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As this ebook edition doesn't have fixed pagination, the page numbers below
    are hyperlinked for reference only, based on the printed edition of this book.
  prefs: []
  type: TYPE_NORMAL
- en: It’s time to dive deeper and gain a more technical understanding of how LlamaIndex
    works its magic under the hood. In this chapter, we’ll explore some of the key
    concepts and components that make up LlamaIndex’s architecture. We’ll learn about
    the core building blocks used by the framework to ingest, structure, and query
    our data. Understanding these fundamentals will provide a solid foundation before
    we start applying them hands-on. We’ll go through the theoretical aspects of each
    concept and then connect the dots between the theory and practical application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the main topics covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Uncovering the essential building blocks of LlamaIndex – **Documents** , **Nodes**
    , and **indexes**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building our first interactive, augmented **large language model** ( **LLM**
    ) application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Starting our **personalized intelligent tutoring system** ( **PITS** ) project
    – a hands-on exercise
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <title>Technical requirements</title>
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You will need to install the following Python libraries in your environment
    to be able to run the examples included in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '*PYYAML* ( https://pyyaml.org/wiki/PyYAMLDocumentation )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Wikipedia* ( https://wikipedia.readthedocs.io/en/latest/ )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Two LlamaIndex integration packages will also be required:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Wikipedia* *reader* ( https://pypi.org/project/llama-index-readers-wikipedia/
    )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*OpenAI* *LLMs* ( https://pypi.org/project/llama-index-llms-openai/ )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All code samples from this chapter can be found in the `ch3` subfolder of the
    book’s GitHub repository: https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex
    .'
  prefs: []
  type: TYPE_NORMAL
- en: <title>Uncovering the essential building blocks of LlamaIndex – documents, nodes,
    and indexes</title>
  prefs: []
  type: TYPE_NORMAL
- en: Uncovering the essential building blocks of LlamaIndex – documents, nodes, and
    indexes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'from llama_index.core import Document text = "The quick brown fox jumps over
    the lazy dog." doc = Document(     text=text,     metadata={''author'': ''John
    Doe'',''category'': ''others''},     id_=''1'' ) print(doc) pip install wikipedia
    pip install llama-index-readers-wikipedia from llama_index.readers.wikipedia import
    WikipediaReader loader = WikipediaReader() documents = loader.load_data(     pages=[''Pythagorean
    theorem'',''General relativity''] ) print(f"loaded {len(documents)} documents")
    from llama_index.core import Document from llama_index.core.schema import TextNode
    doc = Document(text="This is a sample document text") n1 = TextNode(text=doc.text[0:16],
    doc_id=doc.id_) n2 = TextNode(text=doc.text[17:30], doc_id=doc.id_) print(n1)
    print(n2) Node ID: 102b570f-5b22-48b5-b9b6-6378597e920d Text: This is a sample
    Node ID: 0ad81b09-bf12-4063-bfe4-6c5fd3c36cd4 Text: document text from llama_index.core
    import Document from llama_index.core.node_parser import TokenTextSplitter doc
    = Document(     text=(     "This is sentence 1\. This is sentence 2\. "     "Sentence
    3 here."     ),     metadata={"author": "John Smith"} ) splitter = TokenTextSplitter(
        chunk_size=12,     chunk_overlap=0,     separator=" " ) nodes = splitter.get_nodes_from_documents([doc])
    for node in nodes:     print(node.text)     print(node.metadata) Metadata length
    (6) is close to chunk size (12). Resulting chunks are less than 50 tokens. Consider
    increasing the chunk size or decreasing the size of your metadata to avoid this.
    This is sentence 1. {''author'': ''John Smith''} This is sentence 2. {''author'':
    ''John Smith''} Sentence 3 here. {''author'': ''John Smith''} from llama_index.core
    import Document from llama_index.core.schema import (     TextNode,     NodeRelationship,
        RelatedNodeInfo ) doc = Document(text="First sentence. Second Sentence") n1
    = TextNode(text="First sentence", node_id=doc.doc_id) n2 = TextNode(text="Second
    sentence", node_id=doc.doc_id) n1.relationships[NodeRelationship.NEXT] = n2.node_id
    n2.relationships[NodeRelationship.PREVIOUS] = n1.node_id print(n1.relationships)
    print(n2.relationships) from llama_index.core import SummaryIndex, Document from
    llama_index.core.schema import TextNode nodes = [     TextNode(         text="Lionel
    Messi is a football player from Argentina."     ),     TextNode(         text="He
    has won the Ballon d''Or trophy 7 times."     ),     TextNode(text="Lionel Messi''s
    hometown is Rosario."),     TextNode(text="He was born on June 24, 1987.") ] index
    = SummaryIndex(nodes) query_engine = index.as_query_engine() response = query_engine.query("What
    is Messi''s hometown?") print(response) Messi''s hometown is Rosario.'
  prefs: []
  type: TYPE_NORMAL
- en: As we’re getting started with LlamaIndex, it’s time to understand some of the
    key concepts and components that make up its architecture. You may consider this
    chapter as a quick introduction to the typical **retrieval-augmented generation**
    ( **RAG** ) architecture with LlamaIndex and an overview of the most important
    tools provided by this framework. It should give you a basic understanding of
    how to build a simple RAG application. In the next chapters, we’ll take it step
    by step and explore in detail each one of the components presented here.
  prefs: []
  type: TYPE_NORMAL
- en: At a high level, LlamaIndex helps connect external data sources to LLMs. To
    do this effectively, it needs to ingest, structure, and organize your data in
    a way that allows for efficient retrieval and querying. In this first part of
    our chapter, we’ll explore the core elements that enable LlamaIndex to augment
    LLMs – Documents, Nodes, and indexes.
  prefs: []
  type: TYPE_NORMAL
- en: Documents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It all begins with the data.
  prefs: []
  type: TYPE_NORMAL
- en: Now, trying to handle raw data directly can be as tricky as holding water in
    your hands. It’s often all over the place without any set structure. This is where
    we need to step in and give it some shape. That’s exactly what we do in LlamaIndex
    with something called Documents. A Document is how we capture and contain any
    kind of data, whether you enter it manually or load it over from an external source.
    It’s like putting the data in a nice bottle so it’s easier to handle.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine you’ve got a bunch of your company’s procedures saved as PDFs and you
    want to make sense of them using a powerful language model such as GPT-4\. In
    LlamaIndex, each of these procedures would be turned into its `Document` object
    – and it’s not just about files. Say you have data sitting in a database or coming
    through an API – those can be Documents, too. Check out *Figure 3* *.1* for a
    visual overview:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_03_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 – Documents can come from multiple sources
  prefs: []
  type: TYPE_NORMAL
- en: Think of the `Document` class as a container. It holds not just the raw text
    or data from wherever it originated but also any extra bits of information you
    decide to tag along. This extra info, called **metadata** , is a game changer
    when you start searching through your Documents because it lets you get really
    specific with your queries.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a basic example of how a Document can be created manually:'
  prefs: []
  type: TYPE_NORMAL
- en: In this example, after importing the `Document` class, we create a `Document`
    object called `doc` . The object contains the actual text, a document ID, and
    some additional metadata of our choice that is provided as a dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some of the most important attributes of a `Document` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '`text` : This attribute stores the text content of the document'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`metadata` : This attribute is a dictionary that can be used to include additional
    information about the document, such as the file name or categories. The keys
    in the metadata dictionary must be strings and the values can be strings, floats,
    or integers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`id_` : This is a unique ID for each Document. You can set this manually if
    you want, but if you don’t specify an ID, LlamaIndex will automatically generate
    one for you'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are also other attributes that you can find by consulting the GitHub repository
    of LlamaIndex. However, to keep things simple, at this moment we will only focus
    on these three. These attributes provide various ways to customize and enhance
    the functionality of the `Document` class in LlamaIndex.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 3* *.2* presents the basic structure of a LlamaIndex Document.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_03_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 – The basic structure of a document
  prefs: []
  type: TYPE_NORMAL
- en: The LlamaIndex Documents contain data in its unprocessed, or **raw** , form.
    Although the given example illustrates how we can manually create one, typically,
    in practical applications, these Documents are generated in bulk by sourcing them
    from various data sources. This bulk ingestion of data uses predefined **data
    loaders** – sometimes called **connectors** or simply **readers** – from an extensive
    library known as **LlamaHub** ( https://llamahub.ai/ ).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Developed primarily by the LlamaIndex community, these plug-and-play packages
    extend the functionality of the core components of the framework. They provide
    different LLMs, agent tools, embedding models, vector stores, and data loaders.
    These data ingestion tools offer compatibility with a wide range of data file
    formats, databases, and API endpoints. There are more than 130 different data
    readers in LlamaHub already and the list keeps growing. We’ll cover the topic
    of LlamaHub in much more detail in the next chapter. For now, we’ll focus on the
    data loaders.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a basic example of automated data ingestion using one of the predefined
    LlamaHub data loaders. Before you can run the example, make sure you install the
    libraries mentioned in the *technical requirements* section and complete all the
    necessary environment preparations mentioned in *Chapter 2* if you haven’t already:'
  prefs: []
  type: TYPE_NORMAL
- en: The first library allows for easy access and parsing of data from Wikipedia
    while the second one is the LlamaIndex integration for the Wikipedia data loader.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have installed the two libraries, you’ll be able to run the following
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: The `WikpediaReader` loader extracts the text from Wikipedia articles using
    the Wikipedia Python package. Apart from `WikipediaReader` , there are many more
    specialized data connectors available in the LlamaHub.
  prefs: []
  type: TYPE_NORMAL
- en: So, creating Documents is a very straightforward process. But how do the raw
    `Document` objects get converted into a format that LLMs can efficiently process
    and reason over? This is where Nodes come in.
  prefs: []
  type: TYPE_NORMAL
- en: Nodes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While Documents represent the raw data and can be used as such, Nodes are smaller
    chunks of content extracted from the Documents. The goal is to break down Documents
    into smaller, more manageable pieces of text. This serves a few purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Allows our proprietary knowledge to fit within the model’s prompt limits**
    : Imagine that if we had an internal procedure that is 50 pages long, we would
    definitely run into size limit problems when trying to feed that in the context
    of our prompt. However, most likely, in practice, we wouldn’t need to feed the
    entire procedure in one prompt. Therefore, selecting just the relevant Nodes can
    solve this problem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Creates semantic units of data centered around specific information** : This
    can make it easier to work with and analyze the data, as it is organized into
    smaller, more focused units.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Allows the creation of relationships between Nodes** : This means that Nodes
    can be linked together based on their relationships, creating a network of interconnected
    data. This can be useful for understanding the connections and dependencies between
    different pieces of information within the Documents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Take a look at *Figure 3* *.3* for a visual representation of this concept:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_03_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 – Relationships between Nodes extracted from a Document
  prefs: []
  type: TYPE_NORMAL
- en: In LlamaIndex, Nodes can also store images but we won’t focus on that functionality
    in this book. Our main protagonist from now on will be the `TextNode` class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a list of some important attributes of the `TextNode` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '`text` : The chunk of text derived from an original Document.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`start_char_idx` and `end_char_idx` are optional integer values that can store
    the starting and ending character positions of the text within the Document. This
    could be helpful when the text is part of a larger Document, and you need to pinpoint
    the exact location.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_template` and `metadata_template` are template fields that define how
    the text and metadata are formatted. They help produce a more structured and readable
    representation of `TextNode` .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`metadata_seperator` : This is a string field that defines the separator between
    metadata fields. When multiple metadata items are included, this separator is
    used to maintain readability and structure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any useful `metadata` such as the parent Document ID, relationships to other
    Nodes, and optional tags. This metadata can be used for storing additional context
    when necessary. We’ll talk about it in more detail in *Chapter 4* , *Ingesting
    Data into Our* *RAG Workflow* .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Just like in the case of Documents, if you want to see a full list of the `TextNode`
    attributes, you can find them described on the LlamaIndex GitHub repository: https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/schema.py
    .'
  prefs: []
  type: TYPE_NORMAL
- en: You should know that the Nodes will automatically inherit any metadata already
    present at the Document level but their metadata can also be individually customized.
  prefs: []
  type: TYPE_NORMAL
- en: There are several ways in which Nodes can be created in LlamaIndex, which we
    will discuss in upcoming subsections. Let’s start with the manual creation of
    Nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Manually creating the Node objects
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here is a simple example of how we can manually create `Node` objects:'
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we’re using the text-slicing capabilities of Python to manually
    extract the text for the two Nodes. This manual approach can be very handy when
    you really want to have full control of both the text of the Nodes and the accompanying
    metadata.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand what’s happening backstage, let’s have a look at the output of
    this code:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the two Nodes contain a randomly generated ID and the segments
    of text that we have sliced from the original Document. The `TextNode` constructor
    automatically generated an ID for each node using the Python UUID module. But
    we can customize that identifier after creating the Nodes if we want to employ
    a different identification scheme.
  prefs: []
  type: TYPE_NORMAL
- en: Automatically extracting Nodes from Documents using splitters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Because **Document chunking** is very important in an RAG workflow, LlamaIndex
    comes with built-in tools for this purpose. One such tool is `TokenTextSplitter`
    .
  prefs: []
  type: TYPE_NORMAL
- en: As an example of how we can automatically generate Nodes, `TokenTextSplitter`
    attempts to split the Document text into chunks that contain whole sentences.
    Each chunk will include one or more sentences and there’s also a default overlap
    between the chunks to maintain more context.
  prefs: []
  type: TYPE_NORMAL
- en: 'Under the hood, there are a number of parameters that we can customize on `SimpleNodeParser`
    such as `chunk_size` and `chunk_overlap` but we will talk about them more and
    how this text splitter works in the next chapter. For now, let’s have a look at
    a simple example of how to use `TokenTextSplitter` with its default settings on
    a `Document` object:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code output this time:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Given that chunk size is how much content can be processed at a time, if the
    metadata is too large, it will take up most of the space in each chunk, leaving
    less room for the actual content text. This can lead to chunks that are mostly
    metadata with very little actual content. In our example, the warning is triggered
    because the effective chunk size (the chunk size minus the space taken up by the
    metadata) results in chunks that would be less than 50 tokens. This is considered
    too small for efficient processing.
  prefs: []
  type: TYPE_NORMAL
- en: This was just a basic example meant to illustrate an automatic method for chunking
    the data in separated Nodes. If you look at the metadata of each node, you’ll
    also notice that it was automatically inherited from the originating Document.
  prefs: []
  type: TYPE_NORMAL
- en: Are there any other ways to create Nodes?
  prefs: []
  type: TYPE_NORMAL
- en: Yes, there are a few other methods. In the next chapter, we’ll go more in-depth
    with the text-splitting and node-parsing techniques available in LlamaIndex. You
    will also have the opportunity to understand how they work under the hood and
    what kind of customization options they provide.
  prefs: []
  type: TYPE_NORMAL
- en: But wait, there’s more to understand about Nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Nodes don’t like to be alone – they crave relationships
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we’ve covered some basic examples of how to create simple Nodes, how
    about adding some relationships between them?
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example that manually creates a simple relationship between two Nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we’ve manually created two Nodes and defined a **previous**
    or **next** relationship between them. The relationship tracks the order of Nodes
    within the original Document. This code tells LlamaIndex that the two Nodes belong
    to the initial Document and they also come in a particular order.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 3* *.4* shows exactly what LlamaIndex understands now after we ran
    the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_03_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 – Previous or next relationship between two Nodes
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You should know that LlamaIndex contains the necessary tools to *automatically*
    create relationships between the Nodes. For example, when using the automated
    node parsers discussed previously, in their default configuration, LlamaIndex
    will automatically create previous or next relationships between the Nodes it
    generates.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are other types of relationships that we could define. In addition to
    simple relationships such as previous or next, Nodes can be connected using the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`SOURCE` : The **source relationship** represents the original source Document
    that a node was extracted or parsed from. When you parse a Document into multiple
    Nodes, you can track which Document each node originated from using the source
    relationship.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PARENT` : The **parent relationship** indicates a hierarchical structure where
    the node with this relationship is one level higher than the associated node.
    In a tree structure, a parent node would have one or more children. This relationship
    is used to navigate or manage nested data structures where you might have a main
    node and subordinate Nodes representing sections, paragraphs, or other subdivisions
    of the main node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CHILD` : This is the opposite of `PARENT` . A node with the **child relationship**
    is a subordinate of another node – the parent. Child Nodes can be seen as the
    leaves or branches in a tree structure stemming from their parent node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But why are relationships important? Let’s discuss why they are useful.
  prefs: []
  type: TYPE_NORMAL
- en: Why are relationships important?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Creating relationships between Nodes in LlamaIndex can be useful for several
    reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Enables more contextual querying** : By linking Nodes together, you can leverage
    their relationships during querying to retrieve additional relevant context. For
    example, when querying a node, you could also return the previous or next Nodes
    to provide more context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Allows tracking provenance** : Relationships encode provenance – where source
    Nodes originated and how they are connected. This is useful when you need to identify
    the original source of a node for example.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Enables navigation through nodes** : Traversing Nodes by their relationships
    enables new types of queries. For example, finding the next node that contains
    some keyword. Navigation along relationships provides another dimension for searching.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Supports the construction of knowledge graphs** : Nodes and relationships
    are the building blocks of knowledge graphs. Linking Nodes into a graph structure
    allows for constructing knowledge graphs from text using LlamaIndex. We’ll talk
    more about knowledge graphs during *Chapter 5* , *Indexing* *with LlamaIndex.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Improves the index structure** : Some LlamaIndex indexes, such as trees and
    graphs, utilize node relationships to build their internal structure. Relationships
    allow the construction of more complex and expressive index topologies. We will
    discuss this more in *Chapter 5* , *Indexing* *with LlamaIndex.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In summary, relationships augment the Nodes with additional contextual connections.
    This supports more expressive querying, source-tracking knowledge graph construction,
    and complex index structures.
  prefs: []
  type: TYPE_NORMAL
- en: With raw data ingested as Documents and structured into Nodes that can be queried,
    the last step is to organize Nodes into efficient indexes.
  prefs: []
  type: TYPE_NORMAL
- en: Indexes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our third important concept – the index – refers to a specific data structure
    used to organize a collection of Nodes for optimized storage and retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: A simplified analogy
  prefs: []
  type: TYPE_NORMAL
- en: Getting your data into shape for RAG is kind of like getting your clothes ready
    for a big trip – you have to make sure everything is organized and accessible!
    Let’s say you’re packing for an important business trip. You could just throw
    everything into your suitcase, but your shirts, socks, pants, and other stuff
    would get mixed up! The problem is that when you want to grab what you need quickly,
    you may pull out the wrong item and end up inventing an entirely new dress code.
  prefs: []
  type: TYPE_NORMAL
- en: That’s exactly why indexing your data is so crucial when prepping for LLM augmentation.
    Without indexing, your data is a messy pile of disorganized facts and files, and
    it’s like digging through a bursting suitcase for a matching pair of socks.
  prefs: []
  type: TYPE_NORMAL
- en: Proper indexing neatly sorts information into categories that make sense. For
    example, our sales records are in one index, and support tickets in another. It’s
    just like packing related items together. This transforms messy data into neatly
    organized knowledge that AI can make use of. You go from randomly hunting through
    a suitcase to grabbing exactly what you need from custom pockets.
  prefs: []
  type: TYPE_NORMAL
- en: So, remember – to avoid frustration and wasted time down the road, put in the
    work early to index and structure your data. It will make your job much easier
    down the line.
  prefs: []
  type: TYPE_NORMAL
- en: 'LlamaIndex supports different types of indexes, each with its strengths and
    trade-offs. Here is a list of some of the available index types:'
  prefs: []
  type: TYPE_NORMAL
- en: '`SummaryIndex` : This is very similar to a box for recipes – it keeps your
    Nodes in order, so you can access them one by one. It takes in a set of documents,
    chunks them up into Nodes, and then concatenates them into a list. It’s great
    for reading through a big Document.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DocumentSummaryIndex` : This constructs a concise summary for each document,
    mapping these summaries back to their respective nodes. It facilitates efficient
    information retrieval by using these summaries to quickly identify relevant documents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`VectorStoreIndex` : This is one of the more sophisticated types of indexes
    and probably the workhorse in most RAG applications. It converts text into vector
    embeddings and uses math to group similar Nodes, helping locate Nodes that are
    alike.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TreeIndex` : The perfect solution for those who love order. This index behaves
    similarly to putting smaller boxes inside bigger ones, organizing Nodes by levels
    in a tree-like structure. Inside, each parent node stores summaries of the children
    nodes. These are generated by the LLM, using a general summarization prompt. This
    particular index can be very useful for summarization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`KeywordTableIndex` : Imagine you need to find a dish by the ingredients you
    have. The keyword index connects important words to the Nodes they’re in. It makes
    finding any node easy by looking up keywords.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`KnowledgeGraphIndex` : This is useful when you need to link facts in a big
    network of data stored as a knowledge graph. This one is good for answering tricky
    questions about lots of connected information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ComposableGraph` : This allows you to create complex index structures in which
    Document-level indexes are indexed in higher-level collections. That’s right:
    you can even build an index of indexes if you want to access the data from multiple
    Documents in a larger collection of Documents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ll talk more about the inner workings of these indexes and other variations
    in *Chapter 5* , *Indexing with LlamaIndex* . This is just an overview of the
    topic.
  prefs: []
  type: TYPE_NORMAL
- en: 'All the index types in LlamaIndex share some common core features:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Building the index** : Each index type can be constructed by passing in a
    set of Nodes during initialization. This builds the underlying index structure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inserting new Nodes** : After an index is built, new Nodes can be manually
    inserted. This adds to the existing index structure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Querying the index** : Once built, indexes provide a query interface to retrieve
    relevant Nodes based on a specific query. The retrieval logic varies by index
    type.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The specifics of index structure and querying differ across index types. But
    this building, inserting, and querying pattern is consistent. Understanding the
    particular features of each index type is really important if you want to exploit
    their full potential. During *Chapter 5* , *Indexing with LlamaIndex* , we will
    cover this topic in much more detail and I will give you specific examples for
    each type of index.
  prefs: []
  type: TYPE_NORMAL
- en: 'For now, let’s consider a simple example to illustrate the creation of `SummaryIndex`
    :'
  prefs: []
  type: TYPE_NORMAL
- en: This is very simple to follow. We first defined a set of Nodes containing the
    data and then created `SummaryIndex` based on these Nodes. This index is a simple
    list-based data structure.
  prefs: []
  type: TYPE_NORMAL
- en: Think of `SummaryIndex` as a little notepad where you jot down points from lots
    of stories. When it’s getting set up, it takes a big bunch of stories, breaks
    them into smaller bits, and lines them up in a list. The best part? LlamaIndex
    doesn’t even need to use the LLM when it builds this type of index.
  prefs: []
  type: TYPE_NORMAL
- en: Are we there yet?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Almost. Indexes are great for organizing data, but how do we get answers from
    them? That’s where **retrievers** and **response synthesizers** come in!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use the Lionel Messi index we just created as an example. Say you ask,
    “What is Messi’s hometown?” See the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: The summary index organizes all Nodes sequentially in a list.
  prefs: []
  type: TYPE_NORMAL
- en: When queried, it retrieves all Nodes, allowing the synthesis of a response with
    full context.
  prefs: []
  type: TYPE_NORMAL
- en: How does this actually work under the hood?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`QueryEngine` contains a retriever, which is responsible for retrieving relevant
    Nodes from the index for the query. The retriever does a lookup to fetch and rank
    relevant Nodes from the index for that query. It grabs Nodes from the index that
    are likely to contain information about Messi’s hometown.'
  prefs: []
  type: TYPE_NORMAL
- en: But just getting back a list of Nodes isn’t very useful. Another part of `QueryEngine`
    called the **node postprocessor** comes into play at this point. This part enables
    the transformation, re-ranking, or filtering of Nodes after they’ve been retrieved
    and before the final response is crafted. There are many types of postprocessors
    available, and each can be configured and customized depending on the use case.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `QueryEngine` object also contains a response synthesizer, which takes
    the retrieved Nodes and crafts the final response using the LLM by performing
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: The response synthesizer takes the Nodes selected by the retriever and processed
    by the node postprocessor and formats them into an LLM prompt.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The prompt contains the query along with context from the Nodes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This prompt is given to the LLM to generate a response.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Any necessary postprocessing is done on the raw response using the LLM to return
    the final natural language answer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'So, `index.as_query_engine()` is creating a full query engine for us, containing
    a default version of the three elements: retriever, node postprocessor, and response
    synthesizer.'
  prefs: []
  type: TYPE_NORMAL
- en: We’ll get into a lot more detail on these three elements in *Chapters 6* and
    *7* .
  prefs: []
  type: TYPE_NORMAL
- en: The final result of running this engine will be a natural language answer such
    as `Messi's hometown` `is Rosario` .
  prefs: []
  type: TYPE_NORMAL
- en: Remember
  prefs: []
  type: TYPE_NORMAL
- en: 'This is just a basic example using a particular type of index called `SummaryIndex`
    . Each index type behaves differently as we will discuss in *Chapter 5* . For
    example: a `TreeIndex` arranges Nodes in a hierarchy, allowing for summarization
    and a `KeywordIndex` maps keywords for fast lookup. The index structure impacts
    performance and determines its best use cases. By itself, the index structure
    defines the data management logic. As we have seen, the index needs to be combined
    with a retriever, postprocessor, and response synthesizer to form a complete query
    pipeline, allowing applications to leverage the indexed data.'
  prefs: []
  type: TYPE_NORMAL
- en: More details will be added in the upcoming chapters. But, at this point, you
    should have a high-level idea of Indexes and their role.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s have a look at *Figure 3* *.5* for an overview of the complete flow.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_03_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 – The complete RAG workflow with LlamaIndex
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in *Figure 3* *.5* , the process involves the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Loading data as Documents
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Parsing Documents into coherent Nodes
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Building an optimized index from Nodes
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Running queries over the index to retrieve relevant Nodes
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Synthesizing the final response
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Too much to remember? Let’s recap the building blocks of LlamaIndex.
  prefs: []
  type: TYPE_NORMAL
- en: A quick recap of the key concepts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here is a quick rundown of what we have covered so far:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Documents** : The raw data ingested'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Nodes** : Logical chunks extracted from Documents'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Indexes** : Data structures organizing Nodes based on use case'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**QueryEngine** : This contains a retriever, node postprocessor, and response
    synthesizer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding these building blocks is crucial for working with LlamaIndex.
    They allow you to effectively structure and connect external data to LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Now, you have a conceptual foundation. Next, let’s solidify this knowledge by
    looking at a simplified workflow model and building an actual application.
  prefs: []
  type: TYPE_NORMAL
- en: <title>Building our first interactive, augmented LLM application</title>
  prefs: []
  type: TYPE_NORMAL
- en: Building our first interactive, augmented LLM application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'from llama_index.core import Document, SummaryIndex from llama_index.core.node_parser
    import SimpleNodeParser from llama_index.readers.wikipedia import WikipediaReader
    loader = WikipediaReader() documents = loader.load_data(pages=["Messi Lionel"])
    parser = SimpleNodeParser.from_defaults() nodes = parser.get_nodes_from_documents(documents)
    index = SummaryIndex(nodes) query_engine = index.as_query_engine() print("Ask
    me anything about Lionel Messi!") while True:     question = input("Your question:
    ")     if question.lower() == "exit":         break     response = query_engine.query(question)
        print(response) import logging logging.basicConfig(level=logging.DEBUG) from
    llama_index.llms.openai import OpenAI from llama_index.core.settings import Settings
    Settings.llm = OpenAI(temperature=0.8, model="gpt-4") from llama_index.core.schema
    import TextNode from llama_index.core import SummaryIndex nodes = [     TextNode(text="Lionel
    Messi''s hometown is Rosario."),     TextNode(text="He was born on June 24, 1987.")
    ] index = SummaryIndex(nodes) query_engine = index.as_query_engine() response
    = query_engine.query(     "What is Messi''s hometown?" ) print(response) from
    llama_index.llms.openai import OpenAI llm = OpenAI(     model="gpt-3.5-turbo-1106",
        temperature=0.2,     max_tokens=50,     additional_kwargs={         "seed":
    12345678,         "top_p": 0.5     } ) response = llm.complete(     "Explain the
    concept of gravity in one sentence" ) print(response)'
  prefs: []
  type: TYPE_NORMAL
- en: It’s time to connect the dots and do something practical with all this knowledge.
    If we put all the previous code together, we can now build our first LlamaIndex
    application.
  prefs: []
  type: TYPE_NORMAL
- en: For this next step, make sure you’ve already taken care of the technical requirements
    mentioned at the beginning of the chapter. For the following code example, we’ll
    need the Wikipedia package to be able to parse a certain Wikipedia article and
    extract our sample data from there.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the Wikipedia package has been successfully installed, the sample app
    should run without issues. Here is the code:'
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted that this does not function as a genuine chat system because
    it does not retain the context of the conversation. It could be more accurately
    described as a simple Q&A system.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a quick walk-through for the code:'
  prefs: []
  type: TYPE_NORMAL
- en: We start by loading a Wikipedia page on Lionel Messi as a Document using the
    `WikipediaReader` data loader. This ingests the raw text data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we parse the Document into smaller Node chunks using `SimpleNodeParser`
    . This splits the text into logical segments
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then build `SummaryIndex` from the Nodes. This organizes the Nodes sequentially
    for full context retrieval
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We define `QueryEngine` , forming a complete query pipeline
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we create a loop that queries the index, passing our question to `QueryEngine`
    . This handles retrieving relevant Nodes, prompting the LLM, and returning the
    final response
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Again, you can have a look at *Figure 3* *.5* to visualize the overall workflow
    – ingesting data, parsing it into Nodes, building an index, and querying it to
    retrieve and synthesize the final answer.
  prefs: []
  type: TYPE_NORMAL
- en: But what if we want to know exactly what happens behind the scenes?
  prefs: []
  type: TYPE_NORMAL
- en: Using the logging features of LlamaIndex to understand the logic and debug our
    applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you run code like in our previous example, you might feel like there’s
    some *magic* happening behind the scenes. You pass in some text, call a simple
    indexing method, and boom – you can start querying an AI assistant powered by
    your own data.
  prefs: []
  type: TYPE_NORMAL
- en: But as your applications get more complex, you’ll want to understand exactly
    how LlamaIndex is doing its thing under the hood. This is where **logging** becomes
    important. LlamaIndex provides tons of helpful log statements that show you step-by-step
    what’s going on during indexing and querying. It’s like having a little debug
    narrator describing each action.
  prefs: []
  type: TYPE_NORMAL
- en: 'Enabling basic logging is as simple as adding this code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'With debug logging enabled, you’ll see how LlamaIndex does things, such as
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Parses your Documents into Nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decide which indexing structure to use
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Formats prompts for the LLM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieves relevant Nodes based on your queries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Synthesizes a response from the Nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As we’ll see in the next chapters, logging also reveals useful data such as
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The number of tokens used for API calls
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Latency information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any warnings or errors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: When things aren’t working as expected, don’t panic! Just check the logs. They
    provide crucial clues for identifying issues. For now, using the basic logging
    feature should do fine. With this feature enabled, most of the backstage activities
    will now be displayed during run time so you’ll be able to monitor the flow of
    your app step by step. We’ll talk more about advanced debugging during *Chapter
    9* , *Customizing and Deploying Our* *LlamaIndex Project* .
  prefs: []
  type: TYPE_NORMAL
- en: Now, how about some tweaking?
  prefs: []
  type: TYPE_NORMAL
- en: Customizing the LLM used by LlamaIndex
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s say we would like to configure the framework to use another LLM. By default,
    LlamaIndex uses the OpenAI API with the **GPT-3.5-Turbo** model. Here’s an overview
    of the key features of GPT-3.5-Turbo:'
  prefs: []
  type: TYPE_NORMAL
- en: It’s faster and cheaper to run compared to **GPT-4**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While not as advanced as other models, such as GPT-4, it’s still a very capable
    generative and conversational model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can perform very well on a variety of **natural language processing** ( **NLP**
    ) tasks such as classification, summarization, or translation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can see why the creators of LlamaIndex have chosen this model. All things
    considered, it provides a good balance of performance and cost for most use cases.
    For most applications, it’s probably sufficient. As you have seen already if you’ve
    tested the application, it handles the questions about Lionel Messi pretty well.
  prefs: []
  type: TYPE_NORMAL
- en: But what if we need to customize that for a more specific case? Let’s say we
    need the best possible performance of GPT-4, the larger context provided by **Claude-2**
    , or maybe we want to use an open-source AI for our purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Easy as 1-2-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We only need to add three lines of code at the beginning of our app:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Make sure you add the `Settings.llm` line immediately after your imports so
    that it applies to all the other operations. Here’s the explanation for each step:'
  prefs: []
  type: TYPE_NORMAL
- en: The first line imports the OpenAI class from `llama_index.llms.openai` so that
    we can use it to initialize an OpenAI LLM
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The second import is responsible for the `Settings` class. We’ll use it to customize
    the LLM
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we configure `Settings` with an OpenAI LLM instance using the GPT-4 model
    and set the `temperature` to `0.8` , overriding the default LLM
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We just configured LlamaIndex to use GPT-4 for all operations instead of the
    default GPT-3.5-Turbo model. The next part of the code will build an index and
    run a simple query using the newly configured LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we need to talk about the `temperature` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: The temperature parameter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'On OpenAI models such as GPT-3.5 and GPT-4, this parameter controls the randomness
    and creativity of the AI’s responses. Check out *Figure 3* *.6* for an overview:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_03_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.6 – Effect of temperature on output variability
  prefs: []
  type: TYPE_NORMAL
- en: The `temperature` values for the OpenAI models range from `0` to `2` . Higher
    values produce more random, creative output. Lower values produce more focused,
    deterministic output.
  prefs: []
  type: TYPE_NORMAL
- en: A `temperature` value of `0` will produce almost the same output every time
    for the same input prompt. You noticed that I’ve used the word “almost.” That
    is because even with the `0` setting, most models will probably still produce
    slight answer variations given the same prompt. This is caused by inherent randomness
    in the model’s initialization or subtle variations in the model’s internal state
    that can occur due to factors such as floating-point precision limitations or
    the stochastic nature of certain operations within the neural network. Even with
    a `temperature` value of `0` , which aims to minimize randomness, these small
    variations can lead to slightly different outputs for identical inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Setting the right `temperature` depends on your use case – whether you want
    responses strongly based on factual data or more imaginative ones. For code generation
    or data analysis tasks, a `temperature` value of `0.2` would be appropriate, while
    more creativity-focused tasks such as writing or chatbot responses would benefit
    from a setting of `0.5` and higher.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you have a use case that really requires consistent responses for multiple
    iterations using the same prompt, here’s some practical advice. In my experimental
    research, I have achieved the most consistent results using the GPT-3.5-Turbo-1106
    model with a `temperature` value of `0` .
  prefs: []
  type: TYPE_NORMAL
- en: 'Apart from `temperature` , there are several other parameters you can tune
    by passing them as a dictionary to the `additional_kwargs` argument. If you plan
    on using OpenAI models in your RAG workflow, I advise you to familiarize yourself
    with these LLM settings, as they can be very important in an RAG scenario. Apart
    from `temperature` , the `top_p` and `seed` parameters are particularly useful
    as they can be leveraged to control the randomness of the outputs. For a detailed
    list, you can consult the official OpenAI documentation here: https://platform.openai.com/docs/models
    .'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a simple playground that you could use for experimenting with different
    LLM settings:'
  prefs: []
  type: TYPE_NORMAL
- en: Using the previous code, you can experiment with different settings, examining
    the output and finding the best configuration for your particular use case.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are wondering what different LLMs available right now can do for your
    RAG purposes, here is a side-by-side comparison extracted from the LlamaIndex
    documentation. This list was built by the LlamaIndex community by testing various
    LLMs: https://docs.llamaindex.ai/en/stable/module_guides/models/llms.html .'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how Settings can be used for customization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You’ve probably noticed that I have used something called `Settings` to customize
    the AI model in the previous section. A brief explanation is in order.
  prefs: []
  type: TYPE_NORMAL
- en: '`Settings` is a key component in LlamaIndex that allows you to customize and
    configure the *elements* used during indexing and querying. It contains common
    objects needed across LlamaIndex such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`LLM` : This allows for the overriding of the default LLM with a custom one
    as we’ve seen in the previous example'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Embedding model` : This is used for generating vectors for text to enable
    semantic search. These vectors are called **embeddings** and we’ll talk about
    them in much more detail during *Chapter 5* , *Indexing* *with LlamaIndex*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NodeParser` : This is used for setting the default node parser'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CallbackManager` : This handles callbacks for events within LlamaIndex. As
    we will see later, this is used for debugging and tracing our apps'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are also other parameters that can be tweaked in `Settings` . We’ll dive
    much deeper into different customization options during *Chapter 9* , *Customizing
    and Deploying Our LlamaIndex Project* . Regardless of what you want to change,
    the customization will be done like in the previous example. Once you’ve defined
    your custom `Settings` , all subsequent operations will use this configuration.
  prefs: []
  type: TYPE_NORMAL
- en: OK. We’ve covered enough concepts for one chapter. How about some coding?
  prefs: []
  type: TYPE_NORMAL
- en: <title>Starting our PITS project – hands-on exercise</title>
  prefs: []
  type: TYPE_NORMAL
- en: Starting our PITS project – hands-on exercise
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'pip install pyyaml LOG_FILE = "session_data/user_actions.log" SESSION_FILE
    = "session_data/user_session_state.yaml" CACHE_FILE = "cache/pipeline_cache.json"
    CONVERSATION_FILE = "cache/chat_history.json" QUIZ_FILE = "cache/quiz.csv" SLIDES_FILE
    = "cache/slides.json" STORAGE_PATH = "ingestion_storage/" INDEX_STORAGE = "index_storage"
    QUIZ_SIZE = 5 ITEMS_ON_SLIDE = 4 from global_settings import SESSION_FILE import
    yaml import os def save_session(state):     state_to_save = {key: value for key,
    value in state.items()}     with open(SESSION_FILE, ''w'') as file:         yaml.dump(state_to_save,
    file) def load_session(state):     if os.path.exists(SESSION_FILE):         with
    open(SESSION_FILE, ''r'') as file:             try:                 loaded_state
    = yaml.safe_load(file) or {}                 for key, value in loaded_state.items():
                        state[key] = value                 return True             except
    yaml.YAMLError:                 return False     return False def delete_session(state):
        if os.path.exists(SESSION_FILE):         os.remove(SESSION_FILE)     for key
    in list(state.keys()):         del state[key] from datetime import datetime from
    global_settings import LOG_FILE import os def log_action(action, action_type):
        timestamp = datetime.now().strftime(''%Y-%m-%d %H:%M:%S'')     log_entry =
    f"{timestamp}: {action_type} : {action}\n"     with open(LOG_FILE, ''a'') as file:
            file.write(log_entry) def reset_log():     with open(LOG_FILE, ''w'')
    as file:         file.truncate(0)'
  prefs: []
  type: TYPE_NORMAL
- en: Are you ready for a bit of hands-on practice? It’s time to start building our
    PITS project. We have enough theoretical groundwork laid out and, in this chapter,
    we’ll begin the preparation for the more advanced elements to come.
  prefs: []
  type: TYPE_NORMAL
- en: I’ve tried to build the project in a modular structure. I believe it helps a
    lot with code clarity and will enable us to go through some of the important concepts
    from LlamaIndex one by one. As I mentioned in the previous chapter, you can either
    write the code alongside reading the book or download and study it in full using
    the GitHub repository that I’ve made available to you.
  prefs: []
  type: TYPE_NORMAL
- en: Disclaimer
  prefs: []
  type: TYPE_NORMAL
- en: There are many aspects that can be improved in the existing code base, and quite
    a few features are missing from it for PITS to be considered a production-ready
    application. For example, in my implementation, there is no authentication and
    the application is a single user. Also, to keep the code short, I’ve not dealt
    much with error handling. But, of course, these are not bugs but features. This
    way, you can continue the story of PITS, adding the missing elements and transforming
    it into a commercial-grade application. Why not?
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we start, I’d like to briefly explain the code structure that will underpin
    our application. Here’s a list of Python source code files used by our PITS along
    with brief descriptions for each:'
  prefs: []
  type: TYPE_NORMAL
- en: '`app.py` : The main entry point for the Streamlit app. This handles the initialization
    of the application and manages the navigation between different screens based
    on the application logic'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`document_uploader.py` : This interfaces with LlamaIndex to ingest and index
    uploaded Documents'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`training_material_builder.py` : This constructs the learning materials (slides
    and narration) based on the user’s current knowledge. It utilizes uploaded and
    indexed materials to generate the learning content'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`training_interface.py` : This is where the actual teaching will take place.
    It displays the slides and the tutor narration together with the conversational
    side panel for user interactions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`quiz_builder.py` : This generates quizzes based on the ingested materials
    and the user’s current knowledge'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`quiz_interface.py` : This administers quizzes and evaluates the user’s knowledge
    level depending on the results – what everyone hated in high school'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`conversation_engine.py` : This manages the conversational side panel, responding
    to user queries and providing explanations. It also keeps track of the context
    of conversations with the tutor to avoid repetition and ensure relevant assistance.
    It also retrieves summaries of previous discussions and ensures the tutor picks
    up where it left off'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`storage_manager.py` : This handles all file operations, such as saving and
    loading session states and user uploads. It manages local file storage and can
    be later adapted for cloud storage solutions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`session_functions.py` : This handle storing and retrieving session information
    locally – and eventually in the cloud'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logging_functions.py` : This handles the logging of all user interactions
    with the app. Writes descriptive log statements with timestamps to track the user’s
    actions throughout the app. Stores and retrieves application logs locally – and
    eventually in the cloud'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`global_settings.py` : This contains application settings, configurations,
    and eventually Streamlit’s secrets for deployment. It centralizes parameters for
    easy management and updates'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`user_onboarding.py` : This module takes care of the user onboarding steps'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`index_builder.py` : This module builds the indexes used throughout the application'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keep in mind that, currently, the application is designed to run locally. During
    *Chapter 9* , *Customizing and Deploying Our LlamaIndex Project* , we will discuss
    the deployment options available with Streamlit apps in more detail. Before continuing,
    make sure you have installed the second package mentioned at the beginning of
    the chapter – the YAML package for Python.
  prefs: []
  type: TYPE_NORMAL
- en: This one will be required by PITS for its `session_functions` module. I will
    explain it in a few moments.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install it, use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For now, we will focus on three of the PITS modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '`global_settings.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`session_functions.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logging_functions.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s have a look at the source code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will first start with the global settings in `global_settings.py` :'
  prefs: []
  type: TYPE_NORMAL
- en: This is where we will store our global configurations. We’ll use the different
    parameters here to customize the experience of PITS and adjust some of its internal
    settings.
  prefs: []
  type: TYPE_NORMAL
- en: For now, the only two parameters I would like to emphasize are `LOG_FILE` and
    `SESSION_FILE` . They are used to define the storage location for “our log file”
    and session-related data. The `log` file will be used to remember all user interactions
    and maintain conversational context. The `session` file will allow resuming existing
    sessions while maintaining the session state.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s move on to `session_functions.py` .
  prefs: []
  type: TYPE_NORMAL
- en: 'The `session_functions.py` module contains functions that handle the saving,
    loading, and deleting of a user’s session state:'
  prefs: []
  type: TYPE_NORMAL
- en: The `save_session` function takes the current state as an argument, which includes
    all the necessary information about the user’s session and writes it to a file
    named `SESSION_FILE` . The state is converted into YAML format before saving,
    which ensures that it can be easily reloaded later.
  prefs: []
  type: TYPE_NORMAL
- en: This function attempts to read `SESSION_FILE` , if it exists, and loads the
    stored session data into the provided state object. If the file is read successfully
    and the YAML content is correctly parsed, it returns `True` , indicating that
    the session state has been restored. Otherwise, it returns `False` .
  prefs: []
  type: TYPE_NORMAL
- en: When a session needs to be cleared, this function deletes `SESSION_FILE` and
    removes all the keys from the passed state object, effectively resetting the session.
  prefs: []
  type: TYPE_NORMAL
- en: Why YAML?
  prefs: []
  type: TYPE_NORMAL
- en: I’ve used YAML as the format for serialization instead of Streamlit own persistence
    format because it’s human readable and platform independent. YAML works well with
    hierarchical data structures, making it easy to read and edit outside of the application
    if necessary. It allows the session state to be stored in a structured, standard
    format that can easily be transferred or modified as needed. YAML is often used
    for configuration files, but it’s also suitable for storing simple data structures
    such as, in our case, the session state.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also need to create `logging_functions.py` . Here is the code:'
  prefs: []
  type: TYPE_NORMAL
- en: The `logging_functions.py` module is responsible for recording events, user
    actions, and other significant occurrences during the execution of the application
    into a log file. I’ve designed it to keep track of user actions and system events
    mainly to provide context for the PITS agent during its interactions with the
    user but also for monitoring and debugging purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s what the functions in the module do:'
  prefs: []
  type: TYPE_NORMAL
- en: '`log_action(action, action_type)` : This function records an action or event.
    It accepts two arguments: `action` , which is a string describing what occurred,
    and `action_type` , which categorizes the action. The function gets the current
    `timestamp` , formats it with the action and type, and appends this entry to `LOG_FILE`
    . This helps maintain a chronological record of actions and events'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reset_log()` : In the current implementation, when the users return to an
    existing session, they have the option to start a new one. When that happens,
    we clear the log file to avoid collecting too much data. This function opens `LOG_FILE`
    and truncates its content, effectively deleting all the logged entries. This is
    usually not a common thing to do in production environments, as logs are valuable
    for historical data analysis, but in our case, it simplifies the flow'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I know I’ve promised we’ll have fun writing the code for PITS, and I am perfectly
    aware that logging seems less *ha-ha* and more *ho-hum* , but trust me, there’s
    no fun if you can’t debug your app. We needed to lay the foundations here and
    we’ll continue with the rest of the modules in the next chapters.
  prefs: []
  type: TYPE_NORMAL
- en: <title>Summary</title>
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covered foundational concepts such as Documents, Nodes, and indexes
    – the core building blocks of LlamaIndex. I’ve demonstrated a simple workflow
    to load data as Documents, parse it into coherent Nodes using parsers, build an
    optimized index from the Nodes, and then query the index to retrieve relevant
    Nodes and synthesize a response.
  prefs: []
  type: TYPE_NORMAL
- en: The logging features of LlamaIndex were introduced as an important tool for
    understanding the underlying logic and debugging applications. Logs reveal how
    LlamaIndex parses, indexes, prompts the LLM, retrieves Nodes, and synthesizes
    responses. Customizing the LLM and other services used by LlamaIndex was shown
    using the `Settings` class.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve also started to build our PITS tutoring application, laying the groundwork
    with session management and logging functions. This modular structure will enable
    the exploration of LlamaIndex’s capabilities incrementally as the app is built
    up.
  prefs: []
  type: TYPE_NORMAL
- en: With the foundational knowledge established, it’s time to move on to more advanced
    LlamaIndex features. The journey continues!
  prefs: []
  type: TYPE_NORMAL
