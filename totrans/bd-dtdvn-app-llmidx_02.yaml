- en: '<html:html><html:head><html:title>Kickstarting Your Journey with LlamaIndex</html:title></html:head>
    <html:body><html:div class="epub-source"><html:h1 class="H1---Chapter" id="_idParaDest-47">Kickstarting
    Your Journey with LlamaIndex</html:h1> <html:div id="_idContainer034"><html:p
    style="font-style:italic;">As this ebook edition doesn''t have fixed pagination,
    the page numbers below are hyperlinked for reference only, based on the printed
    edition of this book.</html:p> <html:p>It’s time to dive deeper and gain a more
    technical understanding of how LlamaIndex works its magic under the hood. In this
    chapter, we’ll explore some of the key concepts and components that make up LlamaIndex’s
    architecture. We’ll learn about the core building blocks used by the framework
    to ingest, structure, and query our data. Understanding these fundamentals will
    provide a solid foundation before we start applying them hands-on. We’ll go through
    the theoretical aspects of each concept and then connect the dots between the
    theory and <html:span class="No-Break">practical application.</html:span></html:p>
    <html:p>Here are the main topics covered in <html:span class="No-Break">this chapter:</html:span></html:p>
    <html:ul><html:li>Uncovering the essential building blocks of LlamaIndex – <html:strong
    class="bold">Documents</html:strong> , <html:strong class="bold">Nodes</html:strong>
    , <html:span class="No-Break">and</html:span> <html:span class="No-Break"><html:strong
    class="bold">indexes</html:strong></html:span></html:li> <html:li>Building our
    first interactive, augmented <html:strong class="bold">large language model</html:strong>
    ( <html:span class="No-Break"><html:strong class="bold">LLM</html:strong></html:span>
    <html:span class="No-Break">) application</html:span></html:li> <html:li>Starting
    our <html:strong class="bold">personalized intelligent tutoring system</html:strong>
    ( <html:strong class="bold">PITS</html:strong> ) project – a <html:span class="No-Break">hands-on
    exercise</html:span></html:li></html:ul> <html:a id="_idTextAnchor047"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>Technical
    requirements</html:title></html:head> <html:body><html:div class="epub-source"><html:h1
    id="_idParaDest-48">Technical requirements</html:h1> <html:div id="_idContainer034"><html:p>You
    will need to install the following Python libraries in your environment to be
    able to run the examples included in <html:span class="No-Break">this chapter:</html:span></html:p>
    <html:ul><html:li><html:span class="No-Break"><html:em class="italic">PYYAML</html:em></html:span>
    <html:span class="No-Break">(</html:span> <html:a><html:span class="No-Break">https://pyyaml.org/wiki/PyYAMLDocumentation</html:span></html:a>
    <html:span class="No-Break">)</html:span></html:li> <html:li><html:span class="No-Break"><html:em
    class="italic">Wikipedia</html:em></html:span> <html:span class="No-Break">(</html:span>
    <html:a><html:span class="No-Break">https://wikipedia.readthedocs.io/en/latest/</html:span></html:a>
    <html:span class="No-Break">)</html:span></html:li></html:ul> <html:p>Two LlamaIndex
    integration packages will also <html:span class="No-Break">be required:</html:span></html:p>
    <html:ul><html:li><html:em class="italic">Wikipedia</html:em> <html:span class="No-Break"><html:em
    class="italic">reader</html:em></html:span> <html:span class="No-Break">(</html:span>
    <html:a><html:span class="No-Break">https://pypi.org/project/llama-index-readers-wikipedia/</html:span></html:a>
    <html:span class="No-Break">)</html:span></html:li> <html:li><html:em class="italic">OpenAI</html:em>
    <html:span class="No-Break"><html:em class="italic">LLMs</html:em></html:span>
    <html:span class="No-Break">(</html:span> <html:a><html:span class="No-Break">https://pypi.org/project/llama-index-llms-openai/</html:span></html:a>
    <html:span class="No-Break">)</html:span></html:li></html:ul> <html:p>All code
    samples from this chapter can be found in the <html:code class="literal">ch3</html:code>
    subfolder of the book’s GitHub <html:span class="No-Break">repository:</html:span>
    <html:a><html:span class="No-Break">https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex</html:span></html:a>
    <html:span class="No-Break">.</html:span></html:p> <html:a id="_idTextAnchor048"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>Uncovering
    the essential building blocks of LlamaIndex – documents, nodes, and indexes</html:title></html:head>
    <html:body><html:div class="epub-source"><html:h1 id="_idParaDest-49">Uncovering
    the essential building blocks of LlamaIndex – documents, nodes, and indexes</html:h1>
    <html:div id="_idContainer034">from llama_index.core import Document text = "The
    quick brown fox jumps over the lazy dog." doc = Document(     text=text,     metadata={''author'':
    ''John Doe'',''category'': ''others''},     id_=''1'' ) print(doc) pip install
    wikipedia pip install llama-index-readers-wikipedia from llama_index.readers.wikipedia
    import WikipediaReader loader = WikipediaReader() documents = loader.load_data(
        pages=[''Pythagorean theorem'',''General relativity''] ) print(f"loaded {len(documents)}
    documents") from llama_index.core import Document from llama_index.core.schema
    import TextNode doc = Document(text="This is a sample document text") n1 = TextNode(text=doc.text[0:16],
    doc_id=doc.id_) n2 = TextNode(text=doc.text[17:30], doc_id=doc.id_) print(n1)
    print(n2) Node ID: 102b570f-5b22-48b5-b9b6-6378597e920d Text: This is a sample
    Node ID: 0ad81b09-bf12-4063-bfe4-6c5fd3c36cd4 Text: document text from llama_index.core
    import Document from llama_index.core.node_parser import TokenTextSplitter doc
    = Document(     text=(     "This is sentence 1\. This is sentence 2\. "     "Sentence
    3 here."     ),     metadata={"author": "John Smith"} ) splitter = TokenTextSplitter(
        chunk_size=12,     chunk_overlap=0,     separator=" " ) nodes = splitter.get_nodes_from_documents([doc])
    for node in nodes:     print(node.text)     print(node.metadata) Metadata length
    (6) is close to chunk size (12). Resulting chunks are less than 50 tokens. Consider
    increasing the chunk size or decreasing the size of your metadata to avoid this.
    This is sentence 1. {''author'': ''John Smith''} This is sentence 2. {''author'':
    ''John Smith''} Sentence 3 here. {''author'': ''John Smith''} from llama_index.core
    import Document from llama_index.core.schema import (     TextNode,     NodeRelationship,
        RelatedNodeInfo ) doc = Document(text="First sentence. Second Sentence") n1
    = TextNode(text="First sentence", node_id=doc.doc_id) n2 = TextNode(text="Second
    sentence", node_id=doc.doc_id) n1.relationships[NodeRelationship.NEXT] = n2.node_id
    n2.relationships[NodeRelationship.PREVIOUS] = n1.node_id print(n1.relationships)
    print(n2.relationships) from llama_index.core import SummaryIndex, Document from
    llama_index.core.schema import TextNode nodes = [     TextNode(         text="Lionel
    Messi is a football player from Argentina."     ),     TextNode(         text="He
    has won the Ballon d''Or trophy 7 times."     ),     TextNode(text="Lionel Messi''s
    hometown is Rosario."),     TextNode(text="He was born on June 24, 1987.") ] index
    = SummaryIndex(nodes) query_engine = index.as_query_engine() response = query_engine.query("What
    is Messi''s hometown?") print(response) Messi''s hometown is Rosario. <html:p>As
    we’re getting started with <html:a id="_idIndexMarker089"></html:a>LlamaIndex,
    it’s time to understand some of the key concepts and components that make up its
    architecture. You may consider this chapter as a quick introduction to the <html:a
    id="_idIndexMarker090"></html:a>typical <html:strong class="bold">retrieval-augmented
    generation</html:strong> ( <html:strong class="bold">RAG</html:strong> ) architecture
    with LlamaIndex and an overview of the most important tools provided by this framework.
    It should give you a basic understanding of how to build a simple RAG application.
    In the next chapters, we’ll take it step by step and explore in detail each one
    of the components <html:span class="No-Break">presented here.</html:span></html:p>
    <html:p>At a high level, LlamaIndex helps connect external data sources to LLMs.
    To do this effectively, it needs to ingest, structure, and organize your data
    in a way that allows for efficient retrieval and querying. In this first part
    of our chapter, we’ll explore the core elements that enable LlamaIndex to augment
    LLMs – Documents, Nodes, <html:span class="No-Break">and indexes.</html:span></html:p>
    <html:a id="_idTextAnchor049"></html:a><html:h2 id="_idParaDest-50">Documents</html:h2>
    <html:p>It all begins with <html:span class="No-Break">the data.</html:span></html:p>
    <html:p>Now, trying to handle <html:a id="_idIndexMarker091"></html:a>raw data
    directly can be as tricky as holding water in your hands. It’s often all over
    the place without any set structure. This is where we need to step in and give
    it some shape. That’s exactly what we do in LlamaIndex with something called Documents.
    A Document <html:a id="_idIndexMarker092"></html:a>is how we capture and contain
    any kind of data, whether you enter it manually or load it over from an external
    source. It’s like putting the data in a nice bottle so it’s easier <html:span
    class="No-Break">to handle.</html:span></html:p> <html:p>Imagine <html:a id="_idIndexMarker093"></html:a>you’ve
    got a bunch of your company’s procedures saved as PDFs and you want to make sense
    of them using a powerful language model such as GPT-4\. In LlamaIndex, each of
    these procedures would be turned into its <html:code class="literal">Document</html:code>
    object – and it’s <html:a id="_idIndexMarker094"></html:a>not just about files.
    Say you have data sitting in a database or coming through an API – those can be
    Documents, too. Check out <html:span class="No-Break"><html:em class="italic">Figure
    3</html:em></html:span> <html:em class="italic">.1</html:em> for a <html:span
    class="No-Break">visual overview:</html:span></html:p> <html:p class="IMG---Caption"
    lang="en-US">Figure 3.1 – Documents can come from multiple sources</html:p> <html:p>Think
    of the <html:code class="literal">Document</html:code> class as a container. It
    holds not just the raw text or data from wherever it originated but also any extra
    bits of information you decide to tag along. This <html:a id="_idIndexMarker095"></html:a>extra
    info, called <html:strong class="bold">metadata</html:strong> , is a game changer
    when you start searching through your Documents because it lets you get really
    specific with <html:span class="No-Break">your queries.</html:span></html:p> <html:p>Here
    is a basic example of how a Document can be <html:span class="No-Break">created
    manually:</html:span></html:p> <html:p>In this <html:a id="_idIndexMarker096"></html:a>example,
    after importing the <html:code class="literal">Document</html:code> class, we
    create a <html:code class="literal">Document</html:code> object called <html:code
    class="literal">doc</html:code> . The object contains the actual text, a document
    ID, and <html:a id="_idIndexMarker097"></html:a>some additional metadata of our
    choice that is provided as <html:span class="No-Break">a dictionary.</html:span></html:p>
    <html:p>Here are some of the most important attributes of a <html:span class="No-Break"><html:code
    class="literal">Document</html:code></html:span> <html:span class="No-Break">object:</html:span></html:p>
    <html:ul><html:li><html:code class="literal">text</html:code> : This attribute
    stores the text content of <html:span class="No-Break">the document</html:span></html:li>
    <html:li><html:code class="literal">metadata</html:code> : This attribute is a
    dictionary that can be used to include additional information about the document,
    such as the file name or categories. The keys in the metadata dictionary must
    be strings and the values can be strings, floats, <html:span class="No-Break">or
    integers</html:span></html:li> <html:li><html:code class="literal">id_</html:code>
    : This is a unique ID for each Document. You can set this manually if you want,
    but if you don’t specify an ID, LlamaIndex will automatically generate one <html:span
    class="No-Break">for you</html:span></html:li></html:ul> <html:p>There are also
    other attributes that you can find by consulting the GitHub repository of LlamaIndex.
    However, to keep things simple, at this moment we will only focus on these three.
    These attributes provide various ways to customize and enhance the functionality
    of the <html:code class="literal">Document</html:code> class <html:span class="No-Break">in
    LlamaIndex.</html:span></html:p> <html:p><html:span class="No-Break"><html:em
    class="italic">Figure 3</html:em></html:span> <html:em class="italic">.2</html:em>
    presents the basic structure of a <html:span class="No-Break">LlamaIndex Document.</html:span></html:p>
    <html:p class="IMG---Caption" lang="en-US">Figure 3.2 – The basic structure of
    a document</html:p> <html:p>The LlamaIndex Documents contain data in its unprocessed,
    or <html:strong class="bold">raw</html:strong> , form. Although the given example
    illustrates how we can manually create one, typically, in practical applications,
    these Documents are generated in bulk by sourcing them from various data sources.
    This bulk ingestion of data uses <html:a id="_idIndexMarker098"></html:a>predefined
    <html:strong class="bold">data loaders</html:strong> – sometimes <html:a id="_idIndexMarker099"></html:a>called
    <html:strong class="bold">connectors</html:strong> or <html:a id="_idIndexMarker100"></html:a>simply
    <html:strong class="bold">readers</html:strong> – from an extensive library <html:a
    id="_idIndexMarker101"></html:a>known as <html:span class="No-Break"><html:strong
    class="bold">LlamaHub</html:strong></html:span> <html:span class="No-Break">(</html:span>
    <html:a><html:span class="No-Break">https://llamahub.ai/</html:span></html:a>
    <html:span class="No-Break">).</html:span></html:p> <html:p class="callout-heading">Note</html:p>
    <html:p class="callout">Developed primarily by the LlamaIndex community, these
    plug-and-play packages extend the functionality of the core components of the
    framework. They provide different LLMs, agent tools, embedding models, vector
    stores, and data loaders. These data ingestion tools offer compatibility with
    a wide range of data file formats, databases, and API endpoints. There are more
    than 130 different data readers in LlamaHub already and the list keeps growing.
    We’ll cover the topic of LlamaHub in much more detail in the next chapter. For
    now, we’ll focus on the <html:span class="No-Break">data loaders.</html:span></html:p>
    <html:p>Here is a basic example of automated data ingestion using one of the predefined
    LlamaHub data loaders. Before you can run the example, make sure you install the
    libraries mentioned in the <html:em class="italic">technical requirements</html:em>
    section and complete all the necessary environment preparations mentioned in <html:a><html:span
    class="No-Break"><html:em class="italic">Chapter 2</html:em></html:span></html:a>
    if you <html:span class="No-Break">haven’t already:</html:span></html:p> <html:p>The
    first <html:a id="_idIndexMarker102"></html:a>library <html:a id="_idIndexMarker103"></html:a>allows
    for easy access and parsing of data from Wikipedia while the second one is the
    LlamaIndex integration for the Wikipedia <html:span class="No-Break">data loader.</html:span></html:p>
    <html:p>Once you have installed the two libraries, you’ll be able to run the <html:span
    class="No-Break">following example:</html:span></html:p> <html:p>The <html:code
    class="literal">WikpediaReader</html:code> loader <html:a id="_idIndexMarker104"></html:a>extracts
    the text from Wikipedia articles using the Wikipedia Python package. Apart from
    <html:code class="literal">WikipediaReader</html:code> , there are many more specialized
    data connectors available in <html:span class="No-Break">the LlamaHub.</html:span></html:p>
    <html:p>So, creating <html:a id="_idIndexMarker105"></html:a>Documents is a very
    straightforward process. But how do <html:a id="_idIndexMarker106"></html:a>the
    raw <html:code class="literal">Document</html:code> objects get converted into
    a format that LLMs can efficiently process and reason over? This is where Nodes
    <html:span class="No-Break">come in.</html:span></html:p> <html:a id="_idTextAnchor050"></html:a><html:h2
    id="_idParaDest-51">Nodes</html:h2> <html:p>While <html:a id="_idIndexMarker107"></html:a>Documents
    represent the raw data and can be used as such, Nodes <html:a id="_idIndexMarker108"></html:a>are
    smaller chunks of content extracted from the Documents. The goal is to break down
    Documents into smaller, more manageable pieces of text. This serves a <html:span
    class="No-Break">few purposes:</html:span></html:p> <html:ul><html:li><html:strong
    class="bold">Allows our proprietary knowledge to fit within the model’s prompt
    limits</html:strong> : Imagine that if we had an internal procedure that is 50
    pages long, we would definitely run into size limit problems when trying to feed
    that in the context of our prompt. However, most likely, in practice, we wouldn’t
    need to feed the entire procedure in one prompt. Therefore, selecting just the
    relevant Nodes can solve <html:span class="No-Break">this problem.</html:span></html:li>
    <html:li><html:strong class="bold">Creates semantic units of data centered around
    specific information</html:strong> : This can make it easier to work with and
    analyze the data, as it is organized into smaller, more <html:span class="No-Break">focused
    units.</html:span></html:li> <html:li><html:strong class="bold">Allows the creation
    of relationships between Nodes</html:strong> : This means that Nodes can be linked
    together based on their relationships, creating a network of interconnected data.
    This can be useful for understanding the connections and dependencies between
    different pieces of information within <html:span class="No-Break">the Documents.</html:span></html:li></html:ul>
    <html:p>Take a look at <html:span class="No-Break"><html:em class="italic">Figure
    3</html:em></html:span> <html:em class="italic">.3</html:em> for a visual representation
    of <html:span class="No-Break">this concept:</html:span></html:p> <html:p class="IMG---Caption"
    lang="en-US">Figure 3.3 – Relationships between Nodes extracted from a Document</html:p>
    <html:p>In LlamaIndex, Nodes <html:a id="_idIndexMarker109"></html:a>can also
    store images but we won’t focus on that functionality <html:a id="_idIndexMarker110"></html:a>in
    this book. Our main protagonist from now on will be the <html:span class="No-Break"><html:code
    class="literal">TextNode</html:code></html:span> <html:span class="No-Break">class.</html:span></html:p>
    <html:p>Here’s a list <html:a id="_idIndexMarker111"></html:a>of some important
    attributes of the <html:span class="No-Break"><html:code class="literal">TextNode</html:code></html:span>
    <html:span class="No-Break">class:</html:span></html:p> <html:ul><html:li><html:code
    class="literal">text</html:code> : The chunk of text derived from an <html:span
    class="No-Break">original Document.</html:span></html:li> <html:li><html:code
    class="literal">start_char_idx</html:code> and <html:code class="literal">end_char_idx</html:code>
    are optional integer values that can store the starting and ending character positions
    of the text within the Document. This could be helpful when the text is part of
    a larger Document, and you need to pinpoint the <html:span class="No-Break">exact
    location.</html:span></html:li> <html:li><html:code class="literal">text_template</html:code>
    and <html:code class="literal">metadata_template</html:code> are template fields
    that define how the text and metadata are formatted. They help produce a more
    structured and readable representation <html:span class="No-Break">of</html:span>
    <html:span class="No-Break"><html:code class="literal">TextNode</html:code></html:span>
    <html:span class="No-Break">.</html:span></html:li> <html:li><html:code class="literal">metadata_seperator</html:code>
    : This is a string field that defines the separator between metadata fields. When
    multiple metadata items are included, this separator is used to maintain readability
    <html:span class="No-Break">and structure.</html:span></html:li> <html:li>Any
    useful <html:code class="literal">metadata</html:code> such as the parent Document
    ID, relationships to other Nodes, and optional tags. This metadata can be used
    for storing additional context when necessary. We’ll talk about it in more detail
    in <html:a><html:span class="No-Break"><html:em class="italic">Chapter 4</html:em></html:span></html:a>
    , <html:em class="italic">Ingesting Data into Our</html:em> <html:span class="No-Break"><html:em
    class="italic">RAG Workflow</html:em></html:span> <html:span class="No-Break">.</html:span></html:li></html:ul>
    <html:p>Just like in the case of Documents, if you want to see a full list of
    the <html:code class="literal">TextNode</html:code> attributes, you can find them
    described on the LlamaIndex GitHub <html:span class="No-Break">repository:</html:span>
    <html:a><html:span class="No-Break">https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/schema.py</html:span></html:a>
    <html:span class="No-Break">.</html:span></html:p> <html:p>You should know <html:a
    id="_idIndexMarker112"></html:a>that the Nodes will automatically inherit any
    <html:a id="_idIndexMarker113"></html:a>metadata already present at the Document
    level but their metadata can also be <html:span class="No-Break">individually
    customized.</html:span></html:p> <html:p>There are several ways in which Nodes
    can be created in LlamaIndex, which we will discuss in upcoming subsections. Let’s
    start with the manual creation <html:span class="No-Break">of Nodes.</html:span></html:p>
    <html:a id="_idTextAnchor051"></html:a><html:h2 id="_idParaDest-52">Manually creating
    the Node objects</html:h2> <html:p>Here is a simple example of how <html:a id="_idIndexMarker114"></html:a>we
    can manually create <html:span class="No-Break"><html:code class="literal">Node</html:code></html:span>
    <html:span class="No-Break">objects:</html:span></html:p> <html:p>In this example,
    we’re using the text-slicing capabilities of Python to manually extract the text
    for the two Nodes. This manual approach can be very handy when you really want
    to have full control of both the text of the Nodes and the <html:span class="No-Break">accompanying
    metadata.</html:span></html:p> <html:p>To understand what’s happening backstage,
    let’s have a look at the output of <html:span class="No-Break">this code:</html:span></html:p>
    <html:p class="callout-heading">Note</html:p> <html:p class="callout">As you can
    see, the two Nodes contain a randomly generated ID and the segments of text that
    we have sliced from the original Document. The <html:code class="literal">TextNode</html:code>
    constructor automatically generated an ID for each node using the Python UUID
    module. But we can customize that identifier after creating the Nodes if we want
    to employ a different <html:span class="No-Break">identification scheme.</html:span></html:p>
    <html:a id="_idTextAnchor052"></html:a><html:h2 id="_idParaDest-53">Automatically
    extracting Nodes from Documents using splitters</html:h2> <html:p>Because <html:strong
    class="bold">Document chunking</html:strong> is <html:a id="_idIndexMarker115"></html:a>very
    important in an RAG workflow, LlamaIndex comes <html:a id="_idIndexMarker116"></html:a>with
    built-in tools for this <html:a id="_idIndexMarker117"></html:a>purpose. One such
    <html:a id="_idIndexMarker118"></html:a>tool <html:span class="No-Break">is</html:span>
    <html:span class="No-Break"><html:code class="literal">TokenTextSplitter</html:code></html:span>
    <html:span class="No-Break">.</html:span></html:p> <html:p>As an example of how
    we can automatically generate Nodes, <html:code class="literal">TokenTextSplitter</html:code>
    attempts to split the <html:a id="_idIndexMarker119"></html:a>Document text into
    chunks that contain whole sentences. Each chunk will include one or more sentences
    and there’s also a default overlap between the chunks to maintain <html:span class="No-Break">more
    context.</html:span></html:p> <html:p>Under the hood, there are a number of parameters
    that we can customize on <html:code class="literal">SimpleNodeParser</html:code>
    such as <html:code class="literal">chunk_size</html:code> and <html:code class="literal">chunk_overlap</html:code>
    but we will talk about them more and how this text splitter works in the next
    chapter. For now, let’s have a look at a simple example of how to use <html:code
    class="literal">TokenTextSplitter</html:code> with its default settings on a <html:span
    class="No-Break"><html:code class="literal">Document</html:code></html:span> <html:span
    class="No-Break">object:</html:span></html:p> <html:p>Here <html:a id="_idIndexMarker120"></html:a>is
    the <html:a id="_idIndexMarker121"></html:a>code <html:a id="_idIndexMarker122"></html:a>output
    <html:span class="No-Break">this time:</html:span></html:p> <html:p class="callout-heading">Note</html:p>
    <html:p class="callout">Given that chunk size is how much content can be processed
    at a time, if the metadata is too large, it will take up most of the space in
    each chunk, leaving less room for the actual content text. This can lead to chunks
    that are mostly metadata with very little actual content. In our example, the
    warning is triggered because the effective chunk size (the chunk size minus the
    space taken up by the metadata) results in chunks that would be less than 50 tokens.
    This is considered too small for <html:span class="No-Break">efficient processing.</html:span></html:p>
    <html:p>This <html:a id="_idIndexMarker123"></html:a>was just a basic example
    meant to <html:a id="_idIndexMarker124"></html:a>illustrate an automatic method
    for <html:a id="_idIndexMarker125"></html:a>chunking the data in separated Nodes.
    If you look at the metadata of each node, you’ll also notice that it was automatically
    inherited from the <html:span class="No-Break">originating Document.</html:span></html:p>
    <html:p class="callout-heading">Are there any other ways to create Nodes?</html:p>
    <html:p class="callout">Yes, there are a few other methods. In the next chapter,
    we’ll go more in-depth with the text-splitting and node-parsing techniques available
    in LlamaIndex. You will also have the opportunity to understand how they work
    under the hood and what kind of customization options <html:span class="No-Break">they
    provide.</html:span></html:p> <html:p>But wait, there’s more to understand <html:span
    class="No-Break">about Nodes.</html:span></html:p> <html:a id="_idTextAnchor053"></html:a><html:h2
    id="_idParaDest-54">Nodes don’t like to be alone – they crave relationships</html:h2>
    <html:p>Now that <html:a id="_idIndexMarker126"></html:a>we’ve covered some basic
    examples of how to create <html:a id="_idIndexMarker127"></html:a>simple Nodes,
    how about adding some relationships <html:span class="No-Break">between them?</html:span></html:p>
    <html:p>Here’s an example that manually creates a simple relationship between
    <html:span class="No-Break">two Nodes:</html:span></html:p> <html:p>In this example,
    we’ve <html:a id="_idIndexMarker128"></html:a>manually created two Nodes and defined
    a <html:strong class="bold">previous</html:strong> or <html:strong class="bold">next</html:strong>
    relationship <html:a id="_idIndexMarker129"></html:a>between them. The relationship
    tracks the order of Nodes within the original Document. This code tells LlamaIndex
    that the two Nodes belong to the initial Document and they also come in a <html:span
    class="No-Break">particular order.</html:span></html:p> <html:p><html:span class="No-Break"><html:em
    class="italic">Figure 3</html:em></html:span> <html:em class="italic">.4</html:em>
    shows exactly what LlamaIndex understands now after we ran <html:span class="No-Break">the
    code:</html:span></html:p> <html:p class="IMG---Caption" lang="en-US">Figure 3.4
    – Previous or next relationship between two Nodes</html:p> <html:p class="callout-heading">Note</html:p>
    <html:p class="callout">You should know that LlamaIndex contains the necessary
    tools to <html:em class="italic">automatically</html:em> create relationships
    between the Nodes. For example, when using the automated node parsers discussed
    previously, in their default configuration, LlamaIndex will automatically create
    previous or next relationships between the Nodes <html:span class="No-Break">it
    generates.</html:span></html:p> <html:p>There are other types of relationships
    that we could define. In addition to simple relationships such as previous or
    next, Nodes can be connected using <html:span class="No-Break">the following:</html:span></html:p>
    <html:ul><html:li><html:code class="literal">SOURCE</html:code> : The <html:strong
    class="bold">source relationship</html:strong> represents <html:a id="_idIndexMarker130"></html:a>the
    original source Document that a node was extracted or parsed from. When you parse
    a Document into multiple Nodes, you can track which Document each node originated
    from using the <html:span class="No-Break">source relationship.</html:span></html:li>
    <html:li><html:code class="literal">PARENT</html:code> : The <html:strong class="bold">parent
    relationship</html:strong> indicates a hierarchical structure where the node with
    this relationship is one level <html:a id="_idIndexMarker131"></html:a>higher
    than the associated node. In a tree structure, a parent node would have one or
    more children. This relationship is used to navigate or manage nested data structures
    where you might have a main node and subordinate Nodes representing sections,
    paragraphs, or other subdivisions of the <html:span class="No-Break">main node.</html:span></html:li>
    <html:li><html:code class="literal">CHILD</html:code> : This is the opposite of
    <html:code class="literal">PARENT</html:code> . A node with the <html:strong class="bold">child
    relationship</html:strong> is a subordinate of another node – the <html:a id="_idIndexMarker132"></html:a>parent.
    Child Nodes can be <html:a id="_idIndexMarker133"></html:a>seen as the leaves
    or branches in <html:a id="_idIndexMarker134"></html:a>a tree structure stemming
    from their <html:span class="No-Break">parent node.</html:span></html:li></html:ul>
    <html:p>But why are relationships important? Let’s discuss why they <html:span
    class="No-Break">are useful.</html:span></html:p> <html:a id="_idTextAnchor054"></html:a><html:h2
    id="_idParaDest-55">Why are relationships important?</html:h2> <html:p>Creating
    <html:a id="_idIndexMarker135"></html:a>relationships between Nodes in LlamaIndex
    can be useful for <html:span class="No-Break">several reasons:</html:span></html:p>
    <html:ul><html:li><html:strong class="bold">Enables more contextual querying</html:strong>
    : By linking Nodes together, you can leverage their relationships during querying
    to retrieve additional relevant context. For example, when querying a node, you
    could also return the previous or next Nodes to provide <html:span class="No-Break">more
    context.</html:span></html:li> <html:li><html:strong class="bold">Allows tracking
    provenance</html:strong> : Relationships encode provenance – where source Nodes
    originated and how they are connected. This is useful when you need to identify
    the original source of a node <html:span class="No-Break">for example.</html:span></html:li>
    <html:li><html:strong class="bold">Enables navigation through nodes</html:strong>
    : Traversing Nodes by their relationships enables new types of queries. For example,
    finding the next node that contains some keyword. Navigation along relationships
    provides another dimension <html:span class="No-Break">for searching.</html:span></html:li>
    <html:li><html:strong class="bold">Supports the construction of knowledge graphs</html:strong>
    : Nodes and relationships are the building blocks of knowledge graphs. Linking
    Nodes into a graph structure allows for constructing knowledge graphs from text
    using LlamaIndex. We’ll talk more about knowledge graphs during <html:a><html:span
    class="No-Break"><html:em class="italic">Chapter 5</html:em></html:span></html:a>
    , <html:em class="italic">Indexing</html:em> <html:span class="No-Break"><html:em
    class="italic">with LlamaIndex.</html:em></html:span></html:li> <html:li><html:strong
    class="bold">Improves the index structure</html:strong> : Some LlamaIndex indexes,
    such as trees and graphs, utilize node relationships to build their internal structure.
    Relationships allow the construction of more complex and expressive index topologies.
    We will discuss this more in <html:a><html:span class="No-Break"><html:em class="italic">Chapter
    5</html:em></html:span></html:a> , <html:em class="italic">Indexing</html:em>
    <html:span class="No-Break"><html:em class="italic">with LlamaIndex.</html:em></html:span></html:li></html:ul>
    <html:p>In summary, relationships <html:a id="_idIndexMarker136"></html:a>augment
    the Nodes with additional contextual connections. This supports more expressive
    querying, source-tracking knowledge graph construction, and complex <html:span
    class="No-Break">index structures.</html:span></html:p> <html:p>With raw data
    ingested as Documents and structured into Nodes that can be queried, the last
    step is to organize Nodes into <html:span class="No-Break">efficient indexes.</html:span></html:p>
    <html:a id="_idTextAnchor055"></html:a><html:h2 id="_idParaDest-56">Indexes</html:h2>
    <html:p>Our third important concept – the <html:a id="_idIndexMarker137"></html:a>index
    – refers to a specific data structure used to organize a <html:a id="_idIndexMarker138"></html:a>collection
    of Nodes for optimized storage <html:span class="No-Break">and retrieval.</html:span></html:p>
    <html:p class="callout-heading">A simplified analogy</html:p> <html:p class="callout">Getting
    your data into shape for RAG is kind of like getting your clothes ready for a
    big trip – you have to make sure everything is organized and accessible! Let’s
    say you’re packing for an important business trip. You could just throw everything
    into your suitcase, but your shirts, socks, pants, and other stuff would get mixed
    up! The problem is that when you want to grab what you need quickly, you may pull
    out the wrong item and end up inventing an entirely new <html:span class="No-Break">dress
    code.</html:span></html:p> <html:p>That’s exactly why indexing your data is so
    crucial when prepping for LLM augmentation. Without indexing, your data is a messy
    pile of disorganized facts and files, and it’s like digging through a bursting
    suitcase for a matching pair <html:span class="No-Break">of socks.</html:span></html:p>
    <html:p>Proper indexing neatly sorts information into categories that make sense.
    For example, our sales records are in one index, and support tickets in another.
    It’s just like packing related items together. This transforms messy data into
    neatly organized knowledge that AI can make use of. You go from randomly hunting
    through a suitcase to grabbing exactly what you need from <html:span class="No-Break">custom
    pockets.</html:span></html:p> <html:p>So, remember – to avoid frustration and
    wasted time down the road, put in the work early to index and structure your data.
    It will make your job much easier down <html:span class="No-Break">the line.</html:span></html:p>
    <html:p>LlamaIndex supports different types of <html:a id="_idIndexMarker139"></html:a>indexes,
    each with its strengths and trade-offs. Here is a list of some of the <html:a
    id="_idIndexMarker140"></html:a>available <html:span class="No-Break">index types:</html:span></html:p>
    <html:ul><html:li><html:code class="literal">SummaryIndex</html:code> : This is
    <html:a id="_idIndexMarker141"></html:a>very similar to a box for recipes – it
    <html:a id="_idIndexMarker142"></html:a>keeps your Nodes in order, so you can
    access them one by one. It takes in a set of documents, chunks them up into Nodes,
    and then concatenates them into a list. It’s great for reading through a <html:span
    class="No-Break">big Document.</html:span></html:li> <html:li><html:code class="literal">DocumentSummaryIndex</html:code>
    : This constructs a concise summary for each document, mapping these <html:a id="_idIndexMarker143"></html:a>summaries
    back to <html:a id="_idIndexMarker144"></html:a>their respective nodes. It facilitates
    efficient information retrieval by using these summaries to quickly identify <html:span
    class="No-Break">relevant documents.</html:span></html:li> <html:li><html:code
    class="literal">VectorStoreIndex</html:code> : This is one of the more sophisticated
    types of indexes and probably the workhorse <html:a id="_idIndexMarker145"></html:a>in
    most RAG applications. It converts <html:a id="_idIndexMarker146"></html:a>text
    into vector embeddings and uses math to group similar Nodes, helping locate Nodes
    that <html:span class="No-Break">are alike.</html:span></html:li> <html:li><html:code
    class="literal">TreeIndex</html:code> : The <html:a id="_idIndexMarker147"></html:a>perfect
    solution for those who love order. This index <html:a id="_idIndexMarker148"></html:a>behaves
    similarly to putting smaller boxes inside bigger ones, organizing Nodes by levels
    in a tree-like structure. Inside, each parent node stores summaries of the children
    nodes. These are generated by the LLM, using a general summarization prompt. This
    particular index can be very useful <html:span class="No-Break">for summarization.</html:span></html:li>
    <html:li><html:code class="literal">KeywordTableIndex</html:code> : Imagine you
    need to find a dish by the ingredients you have. The <html:a id="_idIndexMarker149"></html:a>keyword
    index <html:a id="_idIndexMarker150"></html:a>connects important words to the
    Nodes they’re in. It makes finding any node easy by looking <html:span class="No-Break">up
    keywords.</html:span></html:li> <html:li><html:code class="literal">KnowledgeGraphIndex</html:code>
    : This is useful when you need to link facts in a big network of data stored as
    a <html:a id="_idIndexMarker151"></html:a>knowledge graph. This <html:a id="_idIndexMarker152"></html:a>one
    is good for answering tricky questions about lots of <html:span class="No-Break">connected
    information.</html:span></html:li> <html:li><html:code class="literal">ComposableGraph</html:code>
    : This allows you to <html:a id="_idIndexMarker153"></html:a>create complex index
    <html:a id="_idIndexMarker154"></html:a>structures in which Document-level indexes
    are indexed in higher-level collections. That’s right: you can even build an index
    of indexes if you want to access the data from multiple Documents in a larger
    collection <html:span class="No-Break">of Documents.</html:span></html:li></html:ul>
    <html:p>We’ll talk more about the inner workings of these <html:a id="_idIndexMarker155"></html:a>indexes
    and other variations in <html:a><html:span class="No-Break"><html:em class="italic">Chapter
    5</html:em></html:span></html:a> , <html:em class="italic">Indexing with LlamaIndex</html:em>
    . This is just an overview of <html:span class="No-Break">the topic.</html:span></html:p>
    <html:p>All the index types <html:a id="_idIndexMarker156"></html:a>in LlamaIndex
    share some <html:a id="_idIndexMarker157"></html:a>common <html:span class="No-Break">core
    features:</html:span></html:p> <html:ul><html:li><html:strong class="bold">Building
    the index</html:strong> : Each index type can be constructed by passing in a set
    of Nodes during initialization. This builds the underlying <html:span class="No-Break">index
    structure.</html:span></html:li> <html:li><html:strong class="bold">Inserting
    new Nodes</html:strong> : After an index is built, new Nodes can be manually inserted.
    This adds to the existing <html:span class="No-Break">index structure.</html:span></html:li>
    <html:li><html:strong class="bold">Querying the index</html:strong> : Once built,
    indexes provide a query interface to retrieve relevant Nodes based on a specific
    query. The retrieval logic varies by <html:span class="No-Break">index type.</html:span></html:li></html:ul>
    <html:p>The specifics of index structure and querying differ across index types.
    But this building, inserting, and querying pattern is consistent. Understanding
    the particular features of each index type is really important if you want to
    exploit their full potential. During <html:a><html:span class="No-Break"><html:em
    class="italic">Chapter 5</html:em></html:span></html:a> , <html:em class="italic">Indexing
    with LlamaIndex</html:em> , we will cover this topic in much more detail and I
    will give you specific examples for each type <html:span class="No-Break">of index.</html:span></html:p>
    <html:p>For now, let’s consider a simple example to illustrate the creation <html:span
    class="No-Break">of</html:span> <html:span class="No-Break"><html:code class="literal">SummaryIndex</html:code></html:span>
    <html:span class="No-Break">:</html:span></html:p> <html:p>This is very <html:a
    id="_idIndexMarker158"></html:a>simple to follow. We first defined a set of Nodes
    containing the data and then created <html:code class="literal">SummaryIndex</html:code>
    based on these Nodes. This index is a simple list-based <html:span class="No-Break">data
    structure.</html:span></html:p> <html:p>Think of <html:code class="literal">SummaryIndex</html:code>
    as a <html:a id="_idIndexMarker159"></html:a>little notepad where you jot down
    points from lots of stories. When it’s getting set up, it takes a big bunch of
    stories, breaks them into smaller bits, and lines them up in a list. The best
    part? LlamaIndex doesn’t even need to use the LLM when it builds this type <html:span
    class="No-Break">of index.</html:span></html:p> <html:a id="_idTextAnchor056"></html:a><html:h2
    id="_idParaDest-57">Are we there yet?</html:h2> <html:p>Almost. Indexes are <html:a
    id="_idIndexMarker160"></html:a>great for <html:a id="_idIndexMarker161"></html:a>organizing
    <html:a id="_idIndexMarker162"></html:a>data, but how do we get answers from them?
    That’s where <html:strong class="bold">retrievers</html:strong> and <html:strong
    class="bold">response synthesizers</html:strong> <html:span class="No-Break">come
    in!</html:span></html:p> <html:p>Let’s use the Lionel Messi index we just created
    as an example. Say you ask, “What is Messi’s hometown?” See <html:span class="No-Break">the
    following:</html:span></html:p> <html:p>This is <html:span class="No-Break">the
    output:</html:span></html:p> <html:p>The summary <html:a id="_idIndexMarker163"></html:a>index
    organizes all Nodes sequentially in <html:span class="No-Break">a list.</html:span></html:p>
    <html:p>When queried, it retrieves all Nodes, allowing the synthesis of a response
    with <html:span class="No-Break">full context.</html:span></html:p> <html:a id="_idTextAnchor057"></html:a><html:h2
    id="_idParaDest-58">How does this actually work under the hood?</html:h2> <html:p><html:code
    class="literal">QueryEngine</html:code> contains <html:a id="_idIndexMarker164"></html:a>a
    retriever, which is responsible for retrieving relevant Nodes from the index for
    the query. The retriever does a lookup to fetch and rank relevant Nodes from the
    index for that query. It grabs Nodes from the index that are likely to contain
    information about <html:span class="No-Break">Messi’s hometown.</html:span></html:p>
    <html:p>But just getting back a list of Nodes isn’t very useful. Another part
    of <html:code class="literal">QueryEngine</html:code> called <html:a id="_idIndexMarker165"></html:a>the
    <html:strong class="bold">node postprocessor</html:strong> comes into play at
    this point. This part enables the transformation, re-ranking, or filtering of
    Nodes after they’ve been retrieved and before the final response is crafted. There
    are many types of postprocessors available, and each can be configured and customized
    depending on the <html:span class="No-Break">use case.</html:span></html:p> <html:p>The
    <html:code class="literal">QueryEngine</html:code> object also contains a response
    synthesizer, which takes the retrieved Nodes and crafts the final response using
    the LLM by performing the <html:span class="No-Break">following steps:</html:span></html:p>
    <html:ol><html:li>The response synthesizer takes the Nodes selected by the retriever
    and processed by the node postprocessor and formats them into an <html:span class="No-Break">LLM
    prompt.</html:span></html:li> <html:li>The prompt contains the query along with
    context from <html:span class="No-Break">the Nodes.</html:span></html:li> <html:li>This
    prompt is given to the LLM to generate <html:span class="No-Break">a response.</html:span></html:li>
    <html:li>Any necessary postprocessing is done on the raw response using the LLM
    to return the final natural <html:span class="No-Break">language answer.</html:span></html:li></html:ol>
    <html:p>So, <html:code class="literal">index.as_query_engine()</html:code> is
    creating <html:a id="_idIndexMarker166"></html:a>a full query engine for us, containing
    a default version of the three elements: retriever, node postprocessor, and <html:span
    class="No-Break">response synthesizer.</html:span></html:p> <html:p>We’ll get
    into a lot more detail on these three elements in <html:em class="italic">Chapters
    6</html:em> <html:span class="No-Break">and</html:span> <html:span class="No-Break"><html:em
    class="italic">7</html:em></html:span> <html:span class="No-Break">.</html:span></html:p>
    <html:p>The final result of running this engine will be a natural language answer
    such as <html:code class="literal">Messi''s hometown</html:code> <html:span class="No-Break"><html:code
    class="literal">is Rosario</html:code></html:span> <html:span class="No-Break">.</html:span></html:p>
    <html:p class="callout-heading">Remember</html:p> <html:p class="callout">This
    is just a basic example using a particular type of index called <html:code class="literal">SummaryIndex</html:code>
    . Each index type behaves differently as we will discuss in <html:a><html:span
    class="No-Break"><html:em class="italic">Chapter 5</html:em></html:span></html:a>
    . For example: a <html:code class="literal">TreeIndex</html:code> arranges Nodes
    in a hierarchy, allowing for summarization and a <html:code class="literal">KeywordIndex</html:code>
    maps keywords for fast lookup. The index structure impacts performance and determines
    its best use cases. By itself, the index structure defines the data management
    logic. As we have seen, the index needs to be combined with a retriever, postprocessor,
    and response synthesizer to form a complete query pipeline, allowing applications
    to leverage the <html:span class="No-Break">indexed data.</html:span></html:p>
    <html:p>More details will be added in the upcoming chapters. But, at this point,
    you should have a high-level idea of Indexes and <html:span class="No-Break">their
    role.</html:span></html:p> <html:p>Let’s have a look at <html:span class="No-Break"><html:em
    class="italic">Figure 3</html:em></html:span> <html:em class="italic">.5</html:em>
    for an overview of the <html:span class="No-Break">complete flow.</html:span></html:p>
    <html:p class="IMG---Caption" lang="en-US">Figure 3.5 – The complete RAG workflow
    with LlamaIndex</html:p> <html:p>As shown in <html:span class="No-Break"><html:em
    class="italic">Figure 3</html:em></html:span> <html:em class="italic">.5</html:em>
    , the process <html:a id="_idIndexMarker167"></html:a>involves the <html:span
    class="No-Break">following steps:</html:span></html:p> <html:ol><html:li>Loading
    data <html:span class="No-Break">as Documents</html:span></html:li> <html:li>Parsing
    Documents into <html:span class="No-Break">coherent Nodes</html:span></html:li>
    <html:li>Building an optimized index <html:span class="No-Break">from Nodes</html:span></html:li>
    <html:li>Running queries over the index to retrieve <html:span class="No-Break">relevant
    Nodes</html:span></html:li> <html:li>Synthesizing the <html:span class="No-Break">final
    response</html:span></html:li></html:ol> <html:p>Too much to remember? Let’s recap
    the building blocks <html:span class="No-Break">of LlamaIndex.</html:span></html:p>
    <html:a id="_idTextAnchor058"></html:a><html:h2 id="_idParaDest-59">A quick recap
    of the key concepts</html:h2> <html:p>Here is a quick rundown of what we have
    covered <html:span class="No-Break">so far:</html:span></html:p> <html:ul><html:li><html:strong
    class="bold">Documents</html:strong> : The <html:a id="_idIndexMarker168"></html:a>raw
    <html:span class="No-Break">data ingested</html:span></html:li> <html:li><html:strong
    class="bold">Nodes</html:strong> : Logical <html:a id="_idIndexMarker169"></html:a>chunks
    extracted <html:span class="No-Break">from Documents</html:span></html:li> <html:li><html:strong
    class="bold">Indexes</html:strong> : Data <html:a id="_idIndexMarker170"></html:a>structures
    organizing Nodes based on <html:span class="No-Break">use case</html:span></html:li>
    <html:li><html:strong class="bold">QueryEngine</html:strong> : This <html:a id="_idIndexMarker171"></html:a>contains
    a retriever, node postprocessor, and <html:span class="No-Break">response synthesizer</html:span></html:li></html:ul>
    <html:p>Understanding these building blocks is crucial for working with LlamaIndex.
    They allow you to effectively structure and connect external data <html:span class="No-Break">to
    LLMs.</html:span></html:p> <html:p>Now, you have a conceptual foundation. Next,
    let’s solidify this knowledge by looking at a simplified workflow model and building
    an <html:span class="No-Break">actual application.</html:span></html:p> <html:a
    id="_idTextAnchor059"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>Building
    our first interactive, augmented LLM application</html:title></html:head> <html:body><html:div
    class="epub-source"><html:h1 id="_idParaDest-60">Building our first interactive,
    augmented LLM application</html:h1> <html:div id="_idContainer034">from llama_index.core
    import Document, SummaryIndex from llama_index.core.node_parser import SimpleNodeParser
    from llama_index.readers.wikipedia import WikipediaReader loader = WikipediaReader()
    documents = loader.load_data(pages=["Messi Lionel"]) parser = SimpleNodeParser.from_defaults()
    nodes = parser.get_nodes_from_documents(documents) index = SummaryIndex(nodes)
    query_engine = index.as_query_engine() print("Ask me anything about Lionel Messi!")
    while True:     question = input("Your question: ")     if question.lower() ==
    "exit":         break     response = query_engine.query(question)     print(response)
    import logging logging.basicConfig(level=logging.DEBUG) from llama_index.llms.openai
    import OpenAI from llama_index.core.settings import Settings Settings.llm = OpenAI(temperature=0.8,
    model="gpt-4") from llama_index.core.schema import TextNode from llama_index.core
    import SummaryIndex nodes = [     TextNode(text="Lionel Messi''s hometown is Rosario."),
        TextNode(text="He was born on June 24, 1987.") ] index = SummaryIndex(nodes)
    query_engine = index.as_query_engine() response = query_engine.query(     "What
    is Messi''s hometown?" ) print(response) from llama_index.llms.openai import OpenAI
    llm = OpenAI(     model="gpt-3.5-turbo-1106",     temperature=0.2,     max_tokens=50,
        additional_kwargs={         "seed": 12345678,         "top_p": 0.5     } )
    response = llm.complete(     "Explain the concept of gravity in one sentence"
    ) print(response) <html:p>It’s time to connect the dots <html:a id="_idIndexMarker172"></html:a>and
    do something practical with all this knowledge. If we put all the previous code
    together, we can now build our first <html:span class="No-Break">LlamaIndex application.</html:span></html:p>
    <html:p>For this next step, make sure you’ve already taken care of the technical
    requirements mentioned at the beginning of the chapter. For the following code
    example, we’ll need the Wikipedia package to be able to parse a certain Wikipedia
    article and extract our sample data <html:span class="No-Break">from there.</html:span></html:p>
    <html:p>Once the Wikipedia package has been successfully installed, the sample
    app should run without issues. Here is <html:span class="No-Break">the code:</html:span></html:p>
    <html:p>It should be noted that this <html:a id="_idIndexMarker173"></html:a>does
    not function as a genuine chat system because it does not retain the context of
    the conversation. It could be more accurately described as a simple <html:span
    class="No-Break">Q&A system.</html:span></html:p> <html:p>Here’s a quick walk-through
    for <html:span class="No-Break">the code:</html:span></html:p> <html:ol><html:li>We
    start by loading a Wikipedia page on Lionel Messi as a Document using the <html:code
    class="literal">WikipediaReader</html:code> data loader. This ingests the raw
    <html:span class="No-Break">text data</html:span></html:li> <html:li>Next, we
    parse the Document into smaller Node chunks using <html:code class="literal">SimpleNodeParser</html:code>
    . This splits the text into <html:span class="No-Break">logical segments</html:span></html:li>
    <html:li>We then build <html:code class="literal">SummaryIndex</html:code> from
    the Nodes. This organizes the Nodes sequentially for full <html:span class="No-Break">context
    retrieval</html:span></html:li> <html:li>We define <html:code class="literal">QueryEngine</html:code>
    , forming a complete <html:span class="No-Break">query pipeline</html:span></html:li>
    <html:li>Finally, we create a loop that queries the index, passing our question
    to <html:code class="literal">QueryEngine</html:code> . This handles retrieving
    relevant Nodes, prompting the LLM, and returning the <html:span class="No-Break">final
    response</html:span></html:li></html:ol> <html:p>Again, you can have a look at
    <html:span class="No-Break"><html:em class="italic">Figure 3</html:em></html:span>
    <html:em class="italic">.5</html:em> to visualize the overall workflow – ingesting
    data, parsing it into Nodes, building an index, and querying it to retrieve and
    synthesize the <html:span class="No-Break">final answer.</html:span></html:p>
    <html:p>But what if we want to know exactly what happens behind <html:span class="No-Break">the
    scenes?</html:span></html:p> <html:a id="_idTextAnchor060"></html:a><html:h2 id="_idParaDest-61">Using
    the logging features of LlamaIndex to understand the logic and debug our applications</html:h2>
    <html:p>When you run code <html:a id="_idIndexMarker174"></html:a>like in our
    previous example, you might feel like there’s some <html:em class="italic">magic</html:em>
    happening behind the scenes. You pass in some text, call a simple indexing method,
    and boom – you can start querying an AI assistant powered by your <html:span class="No-Break">own
    data.</html:span></html:p> <html:p>But as your applications get more complex,
    you’ll want to understand exactly how LlamaIndex is doing its thing under the
    hood. This is where <html:strong class="bold">logging</html:strong> becomes important.
    LlamaIndex provides <html:a id="_idIndexMarker175"></html:a>tons of helpful log
    statements that show you step-by-step what’s going on during indexing and querying.
    It’s like having a little debug narrator describing <html:span class="No-Break">each
    action.</html:span></html:p> <html:p>Enabling basic logging is as simple as adding
    <html:span class="No-Break">this code:</html:span></html:p> <html:p>With debug
    logging enabled, you’ll see how LlamaIndex does things, such as <html:span class="No-Break">the
    following:</html:span></html:p> <html:ul><html:li>Parses your Documents <html:span
    class="No-Break">into Nodes</html:span></html:li> <html:li>Decide which indexing
    structure <html:span class="No-Break">to use</html:span></html:li> <html:li>Formats
    prompts for <html:span class="No-Break">the LLM</html:span></html:li> <html:li>Retrieves
    relevant Nodes based on <html:span class="No-Break">your queries</html:span></html:li>
    <html:li>Synthesizes a response from <html:span class="No-Break">the Nodes</html:span></html:li></html:ul>
    <html:p>As we’ll see in the next chapters, logging also reveals useful data such
    as <html:span class="No-Break">the following:</html:span></html:p> <html:ul><html:li>The
    number of tokens used for <html:span class="No-Break">API calls</html:span></html:li>
    <html:li><html:span class="No-Break">Latency information</html:span></html:li>
    <html:li>Any warnings <html:span class="No-Break">or errors</html:span></html:li></html:ul>
    <html:p class="callout-heading">Note</html:p> <html:p class="callout">When things
    aren’t working as expected, don’t panic! Just check the logs. They provide crucial
    clues for identifying issues. For now, using the basic logging feature should
    do fine. With this feature enabled, most of the backstage activities will now
    be displayed during run time so you’ll be able to monitor the flow of your app
    step by step. We’ll talk more about advanced debugging during <html:a><html:span
    class="No-Break"><html:em class="italic">Chapter 9</html:em></html:span></html:a>
    , <html:em class="italic">Customizing and Deploying Our</html:em> <html:span class="No-Break"><html:em
    class="italic">LlamaIndex Project</html:em></html:span> <html:span class="No-Break">.</html:span></html:p>
    <html:p>Now, how about <html:span class="No-Break">some tweaking?</html:span></html:p>
    <html:a id="_idTextAnchor061"></html:a><html:h2 id="_idParaDest-62">Customizing
    the LLM used by LlamaIndex</html:h2> <html:p>Let’s say we <html:a id="_idIndexMarker176"></html:a>would
    like to configure the framework to use another LLM. By default, LlamaIndex uses
    the OpenAI API with <html:a id="_idIndexMarker177"></html:a>the <html:strong class="bold">GPT-3.5-Turbo</html:strong>
    model. Here’s an overview <html:a id="_idIndexMarker178"></html:a>of the key features
    <html:span class="No-Break">of GPT-3.5-Turbo:</html:span></html:p> <html:ul><html:li>It’s
    faster and cheaper to run compared <html:a id="_idIndexMarker179"></html:a><html:span
    class="No-Break">to</html:span> <html:span class="No-Break"><html:strong class="bold">GPT-4</html:strong></html:span></html:li>
    <html:li>While not as advanced as other models, such as GPT-4, it’s still a very
    capable generative and <html:span class="No-Break">conversational model</html:span></html:li>
    <html:li>It can perform very well on a variety <html:a id="_idIndexMarker180"></html:a>of
    <html:strong class="bold">natural language processing</html:strong> ( <html:strong
    class="bold">NLP</html:strong> ) tasks such as classification, summarization,
    <html:span class="No-Break">or translation</html:span></html:li></html:ul> <html:p>You
    can see why the creators of LlamaIndex have chosen this model. All things considered,
    it provides a good balance of performance and cost for most use cases. For most
    applications, it’s probably sufficient. As you have seen already if you’ve tested
    the application, it handles the questions about Lionel Messi <html:span class="No-Break">pretty
    well.</html:span></html:p> <html:p>But what if we need to customize that for a
    more specific case? Let’s say we need the best possible performance of GPT-4,
    the larger context provided <html:a id="_idIndexMarker181"></html:a>by <html:strong
    class="bold">Claude-2</html:strong> , or maybe we want to use an open-source AI
    for <html:span class="No-Break">our purposes.</html:span></html:p> <html:a id="_idTextAnchor062"></html:a><html:h2
    id="_idParaDest-63">Easy as 1-2-3</html:h2> <html:p>We <html:a id="_idIndexMarker182"></html:a>only
    need to add three lines of code at the beginning of <html:span class="No-Break">our
    app:</html:span></html:p> <html:p>Make sure you add the <html:code class="literal">Settings.llm</html:code>
    line immediately after your imports so that it applies to all the other operations.
    Here’s the explanation for <html:span class="No-Break">each step:</html:span></html:p>
    <html:ol><html:li>The first line imports the OpenAI class from <html:code class="literal">llama_index.llms.openai</html:code>
    so that we can use it to initialize an <html:span class="No-Break">OpenAI LLM</html:span></html:li>
    <html:li>The second import is responsible for the <html:code class="literal">Settings</html:code>
    class. We’ll use it to customize <html:span class="No-Break">the LLM</html:span></html:li>
    <html:li>Next, we configure <html:code class="literal">Settings</html:code> with
    an OpenAI LLM instance using the GPT-4 model and set the <html:code class="literal">temperature</html:code>
    to <html:code class="literal">0.8</html:code> , overriding the <html:span class="No-Break">default
    LLM</html:span></html:li></html:ol> <html:p>We just configured LlamaIndex to use
    GPT-4 for all operations instead of the default GPT-3.5-Turbo model. The next
    part of the code will build an index and run a simple query <html:a id="_idIndexMarker183"></html:a>using
    the newly <html:span class="No-Break">configured LLM:</html:span></html:p> <html:p>Next,
    we need to talk about the <html:span class="No-Break"><html:code class="literal">temperature</html:code></html:span>
    <html:span class="No-Break">parameter.</html:span></html:p> <html:a id="_idTextAnchor063"></html:a><html:h2
    id="_idParaDest-64">The temperature parameter</html:h2> <html:p>On OpenAI models
    such as <html:a id="_idIndexMarker184"></html:a>GPT-3.5 and GPT-4, this parameter
    controls the randomness and creativity of the AI’s responses. Check out <html:span
    class="No-Break"><html:em class="italic">Figure 3</html:em></html:span> <html:em
    class="italic">.6</html:em> for <html:span class="No-Break">an overview:</html:span></html:p>
    <html:p class="IMG---Caption" lang="en-US">Figure 3.6 – Effect of temperature
    on output variability</html:p> <html:p>The <html:code class="literal">temperature</html:code>
    values for the OpenAI models range from <html:code class="literal">0</html:code>
    to <html:code class="literal">2</html:code> . Higher values produce more random,
    creative output. Lower values produce more focused, <html:span class="No-Break">deterministic
    output.</html:span></html:p> <html:p>A <html:code class="literal">temperature</html:code>
    value of <html:code class="literal">0</html:code> will produce almost the same
    output every time for the same input prompt. You noticed that I’ve used the word
    “almost.” That is because even with the <html:code class="literal">0</html:code>
    setting, most models will probably still produce slight answer variations given
    the same prompt. This is caused by inherent randomness in the model’s initialization
    or subtle variations in the model’s internal state that can occur due to factors
    such as floating-point precision limitations or the stochastic nature of certain
    operations within the neural network. Even with a <html:code class="literal">temperature</html:code>
    value of <html:code class="literal">0</html:code> , which aims to minimize randomness,
    these small variations can lead to slightly different outputs for <html:span class="No-Break">identical
    inputs.</html:span></html:p> <html:p>Setting the right <html:code class="literal">temperature</html:code>
    depends on your use case – whether you want responses strongly based on factual
    data or more imaginative ones. For code generation or data analysis tasks, a <html:code
    class="literal">temperature</html:code> value of <html:code class="literal">0.2</html:code>
    would be appropriate, while more creativity-focused tasks such as writing or chatbot
    responses would benefit from a setting of <html:code class="literal">0.5</html:code>
    <html:span class="No-Break">and higher.</html:span></html:p> <html:p class="callout-heading">Note</html:p>
    <html:p class="callout">If you have a use case that really requires consistent
    responses for multiple iterations using the same prompt, here’s some practical
    advice. In my experimental research, I have achieved the most consistent results
    using the GPT-3.5-Turbo-1106 model with a <html:code class="literal">temperature</html:code>
    value <html:span class="No-Break">of</html:span> <html:span class="No-Break"><html:code
    class="literal">0</html:code></html:span> <html:span class="No-Break">.</html:span></html:p>
    <html:p>Apart from <html:code class="literal">temperature</html:code> , there
    are several other parameters you can tune by passing them as a dictionary to the
    <html:code class="literal">additional_kwargs</html:code> argument. If you plan
    on using OpenAI models in your RAG workflow, I advise you to familiarize yourself
    with these LLM settings, as they can be very important in an RAG scenario. Apart
    from <html:code class="literal">temperature</html:code> , the <html:code class="literal">top_p</html:code>
    and <html:code class="literal">seed</html:code> parameters are particularly useful
    as they can be leveraged to control the randomness of the outputs. For a detailed
    list, you can consult the official OpenAI documentation <html:span class="No-Break">here:</html:span>
    <html:a><html:span class="No-Break">https://platform.openai.com/docs/models</html:span></html:a>
    <html:span class="No-Break">.</html:span></html:p> <html:p>Here’s a simple <html:a
    id="_idIndexMarker185"></html:a>playground that you could use for experimenting
    with different <html:span class="No-Break">LLM settings:</html:span></html:p>
    <html:p>Using the previous code, you can experiment with different settings, examining
    the output and finding the best configuration for your particular <html:span class="No-Break">use
    case.</html:span></html:p> <html:p>If you are wondering what different LLMs available
    right now can do for your RAG purposes, here is a side-by-side comparison extracted
    from the LlamaIndex documentation. This list <html:a id="_idIndexMarker186"></html:a>was
    built by the LlamaIndex community by testing various <html:span class="No-Break">LLMs:</html:span>
    <html:a><html:span class="No-Break">https://docs.llamaindex.ai/en/stable/module_guides/models/llms.html</html:span></html:a>
    <html:span class="No-Break">.</html:span></html:p> <html:a id="_idTextAnchor064"></html:a><html:h2
    id="_idParaDest-65">Understanding how Settings can be used for customization</html:h2>
    <html:p>You’ve <html:a id="_idIndexMarker187"></html:a>probably noticed that I
    have used something called <html:code class="literal">Settings</html:code> to
    customize the AI model in the previous section. A brief explanation is <html:span
    class="No-Break">in order.</html:span></html:p> <html:p><html:code class="literal">Settings</html:code>
    is a key component in LlamaIndex that allows you to customize and configure the
    <html:em class="italic">elements</html:em> used during indexing and querying.
    It contains common objects needed across LlamaIndex such as <html:span class="No-Break">the
    following:</html:span></html:p> <html:ul><html:li><html:code class="literal">LLM</html:code>
    : This allows for the overriding of the default LLM with a custom one as we’ve
    seen in the <html:span class="No-Break">previous example</html:span></html:li>
    <html:li><html:code class="literal">Embedding model</html:code> : This is used
    for generating vectors for text to enable semantic search. These vectors are <html:a
    id="_idIndexMarker188"></html:a>called <html:strong class="bold">embeddings</html:strong>
    and we’ll talk about them in much more detail during <html:a><html:span class="No-Break"><html:em
    class="italic">Chapter 5</html:em></html:span></html:a> , <html:em class="italic">Indexing</html:em>
    <html:span class="No-Break"><html:em class="italic">with LlamaIndex</html:em></html:span></html:li>
    <html:li><html:code class="literal">NodeParser</html:code> : This is used for
    setting the default <html:span class="No-Break">node parser</html:span></html:li>
    <html:li><html:code class="literal">CallbackManager</html:code> : This handles
    callbacks for events within LlamaIndex. As we will see later, this is used for
    debugging and tracing <html:span class="No-Break">our apps</html:span></html:li></html:ul>
    <html:p>There are also other parameters that can be tweaked in <html:code class="literal">Settings</html:code>
    . We’ll dive much deeper into different customization options during <html:a><html:span
    class="No-Break"><html:em class="italic">Chapter 9</html:em></html:span></html:a>
    , <html:em class="italic">Customizing and Deploying Our LlamaIndex Project</html:em>
    . Regardless of what you want to change, the customization <html:a id="_idIndexMarker189"></html:a>will
    be done like in the previous example. Once you’ve defined your custom <html:code
    class="literal">Settings</html:code> , all subsequent operations will use <html:span
    class="No-Break">this configuration.</html:span></html:p> <html:p>OK. We’ve covered
    enough concepts for one chapter. How about <html:span class="No-Break">some coding?</html:span></html:p>
    <html:a id="_idTextAnchor065"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>Starting
    our PITS project – hands-on exercise</html:title></html:head> <html:body><html:div
    class="epub-source"><html:h1 id="_idParaDest-66">Starting our PITS project – hands-on
    exercise</html:h1> <html:div id="_idContainer034">pip install pyyaml LOG_FILE
    = "session_data/user_actions.log" SESSION_FILE = "session_data/user_session_state.yaml"
    CACHE_FILE = "cache/pipeline_cache.json" CONVERSATION_FILE = "cache/chat_history.json"
    QUIZ_FILE = "cache/quiz.csv" SLIDES_FILE = "cache/slides.json" STORAGE_PATH =
    "ingestion_storage/" INDEX_STORAGE = "index_storage" QUIZ_SIZE = 5 ITEMS_ON_SLIDE
    = 4 from global_settings import SESSION_FILE import yaml import os def save_session(state):
        state_to_save = {key: value for key, value in state.items()}     with open(SESSION_FILE,
    ''w'') as file:         yaml.dump(state_to_save, file) def load_session(state):
        if os.path.exists(SESSION_FILE):         with open(SESSION_FILE, ''r'') as
    file:             try:                 loaded_state = yaml.safe_load(file) or
    {}                 for key, value in loaded_state.items():                     state[key]
    = value                 return True             except yaml.YAMLError:                 return
    False     return False def delete_session(state):     if os.path.exists(SESSION_FILE):
            os.remove(SESSION_FILE)     for key in list(state.keys()):         del
    state[key] from datetime import datetime from global_settings import LOG_FILE
    import os def log_action(action, action_type):     timestamp = datetime.now().strftime(''%Y-%m-%d
    %H:%M:%S'')     log_entry = f"{timestamp}: {action_type} : {action}\n"     with
    open(LOG_FILE, ''a'') as file:         file.write(log_entry) def reset_log():
        with open(LOG_FILE, ''w'') as file:         file.truncate(0) <html:p>Are <html:a
    id="_idIndexMarker190"></html:a>you ready for a bit of hands-on practice? It’s
    time to start building our PITS project. We have enough theoretical groundwork
    laid out and, in this chapter, we’ll begin the preparation for the more advanced
    elements <html:span class="No-Break">to come.</html:span></html:p> <html:p>I’ve
    tried to build the project in a modular structure. I believe it helps a lot with
    code clarity and will enable us to go through some of the important concepts from
    LlamaIndex one by one. As I mentioned in the previous chapter, you can either
    write the code alongside reading the book or download and study it in full using
    the GitHub repository that I’ve made available <html:span class="No-Break">to
    you.</html:span></html:p> <html:p class="callout-heading">Disclaimer</html:p>
    <html:p class="callout">There are many aspects that can be improved in the existing
    code base, and quite a few features are missing from it for PITS to be considered
    a production-ready application. For example, in my implementation, there is no
    authentication and the application is a single user. Also, to keep the code short,
    I’ve not dealt much with error handling. But, of course, these are not bugs but
    features. This way, you can continue the story of PITS, adding the missing elements
    and transforming it into a commercial-grade application. <html:span class="No-Break">Why
    not?</html:span></html:p> <html:p>Before we start, I’d like to briefly explain
    the code structure that will underpin our application. Here’s a list of Python
    source code files used by our PITS along with brief descriptions <html:span class="No-Break">for
    each:</html:span></html:p> <html:ul><html:li><html:code class="literal">app.py</html:code>
    : The main entry point for the Streamlit app. This handles the initialization
    of the application and manages the navigation between different screens based
    on the <html:span class="No-Break">application logic</html:span></html:li> <html:li><html:code
    class="literal">document_uploader.py</html:code> : This interfaces with LlamaIndex
    to ingest and index <html:span class="No-Break">uploaded Documents</html:span></html:li>
    <html:li><html:code class="literal">training_material_builder.py</html:code> :
    This constructs the learning materials (slides and narration) based on the user’s
    current knowledge. It utilizes uploaded and <html:a id="_idIndexMarker191"></html:a>indexed
    materials to generate the <html:span class="No-Break">learning content</html:span></html:li>
    <html:li><html:code class="literal">training_interface.py</html:code> : This is
    where the actual teaching will take place. It displays the slides and the tutor
    narration together with the conversational side panel for <html:span class="No-Break">user
    interactions</html:span></html:li> <html:li><html:code class="literal">quiz_builder.py</html:code>
    : This generates quizzes based on the ingested materials and the user’s <html:span
    class="No-Break">current knowledge</html:span></html:li> <html:li><html:code class="literal">quiz_interface.py</html:code>
    : This administers quizzes and evaluates the user’s knowledge level depending
    on the results – what everyone hated in <html:span class="No-Break">high school</html:span></html:li>
    <html:li><html:code class="literal">conversation_engine.py</html:code> : This
    manages the conversational side panel, responding to user queries and providing
    explanations. It also keeps track of the context of conversations with the tutor
    to avoid repetition and ensure relevant assistance. It also retrieves summaries
    of previous discussions and ensures the tutor picks up where it <html:span class="No-Break">left
    off</html:span></html:li> <html:li><html:code class="literal">storage_manager.py</html:code>
    : This handles all file operations, such as saving and loading session states
    and user uploads. It manages local file storage and can be later adapted for cloud
    <html:span class="No-Break">storage solutions</html:span></html:li> <html:li><html:code
    class="literal">session_functions.py</html:code> : This handle storing and retrieving
    session information locally – and eventually in <html:span class="No-Break">the
    cloud</html:span></html:li> <html:li><html:code class="literal">logging_functions.py</html:code>
    : This handles the logging of all user interactions with the app. Writes descriptive
    log statements with timestamps to track the user’s actions throughout the app.
    Stores and retrieves application logs locally – and eventually in <html:span class="No-Break">the
    cloud</html:span></html:li> <html:li><html:code class="literal">global_settings.py</html:code>
    : This contains application settings, configurations, and eventually <html:a id="_idIndexMarker192"></html:a>Streamlit’s
    secrets for deployment. It centralizes parameters for easy management <html:span
    class="No-Break">and updates</html:span></html:li> <html:li><html:code class="literal">user_onboarding.py</html:code>
    : This module takes care of the user <html:span class="No-Break">onboarding steps</html:span></html:li>
    <html:li><html:code class="literal">index_builder.py</html:code> : This module
    builds the indexes used throughout <html:span class="No-Break">the application</html:span></html:li></html:ul>
    <html:p>Keep in mind that, currently, the application is designed to run locally.
    During <html:a><html:span class="No-Break"><html:em class="italic">Chapter 9</html:em></html:span></html:a>
    , <html:em class="italic">Customizing and Deploying Our LlamaIndex Project</html:em>
    , we will discuss the deployment options available with Streamlit apps in more
    detail. Before continuing, make sure you have installed the second package mentioned
    at the beginning of the chapter – the YAML package <html:span class="No-Break">for
    Python.</html:span></html:p> <html:p>This one will be required by PITS for its
    <html:code class="literal">session_functions</html:code> module. I will explain
    it in a <html:span class="No-Break">few moments.</html:span></html:p> <html:p>To
    install it, use the <html:span class="No-Break">following code:</html:span></html:p>
    <html:p>For now, we <html:a id="_idIndexMarker193"></html:a>will focus on three
    of the <html:span class="No-Break">PITS modules:</html:span></html:p> <html:ul><html:li><html:span
    class="No-Break"><html:code class="literal">global_settings.py</html:code></html:span></html:li>
    <html:li><html:span class="No-Break"><html:code class="literal">session_functions.py</html:code></html:span></html:li>
    <html:li><html:span class="No-Break"><html:code class="literal">logging_functions.py</html:code></html:span></html:li></html:ul>
    <html:a id="_idTextAnchor066"></html:a><html:h2 id="_idParaDest-67">Let’s have
    a look at the source code</html:h2> <html:p>We will first start with <html:a id="_idIndexMarker194"></html:a>the
    global settings <html:span class="No-Break">in</html:span> <html:span class="No-Break"><html:code
    class="literal">global_settings.py</html:code></html:span> <html:span class="No-Break">:</html:span></html:p>
    <html:p>This is where we will store our global configurations. We’ll use the different
    parameters here to customize the experience of PITS and adjust some of its <html:span
    class="No-Break">internal settings.</html:span></html:p> <html:p>For now, the
    only two <html:a id="_idIndexMarker195"></html:a>parameters I would like to emphasize
    are <html:code class="literal">LOG_FILE</html:code> and <html:code class="literal">SESSION_FILE</html:code>
    . They are used to define the storage location for “our log file” and session-related
    data. The <html:code class="literal">log</html:code> file will be used to remember
    all user interactions and maintain conversational context. The <html:code class="literal">session</html:code>
    file will allow resuming existing sessions while maintaining the <html:span class="No-Break">session
    state.</html:span></html:p> <html:p>Now, let’s move on <html:span class="No-Break">to</html:span>
    <html:span class="No-Break"><html:code class="literal">session_functions.py</html:code></html:span>
    <html:span class="No-Break">.</html:span></html:p> <html:p>The <html:code class="literal">session_functions.py</html:code>
    module contains functions that handle the saving, loading, and deleting of a user’s
    <html:span class="No-Break">session state:</html:span></html:p> <html:p>The <html:code
    class="literal">save_session</html:code> function takes the current state as an
    argument, which includes all the necessary information about the user’s session
    and writes it to a file named <html:code class="literal">SESSION_FILE</html:code>
    . The state is converted into YAML format before saving, which ensures that it
    can be easily <html:span class="No-Break">reloaded later.</html:span></html:p>
    <html:p>This function attempts <html:a id="_idIndexMarker196"></html:a>to read
    <html:code class="literal">SESSION_FILE</html:code> , if it exists, and loads
    the stored session data into the provided state object. If the file is read successfully
    and the YAML content is correctly parsed, it returns <html:code class="literal">True</html:code>
    , indicating that the session state has been restored. Otherwise, it <html:span
    class="No-Break">returns</html:span> <html:span class="No-Break"><html:code class="literal">False</html:code></html:span>
    <html:span class="No-Break">.</html:span></html:p> <html:p>When a session needs
    to be cleared, this function deletes <html:code class="literal">SESSION_FILE</html:code>
    and removes all the keys from the passed state object, effectively resetting <html:span
    class="No-Break">the session.</html:span></html:p> <html:p class="callout-heading">Why
    YAML?</html:p> <html:p class="callout">I’ve used YAML as the format for serialization
    instead of Streamlit own persistence format because it’s human readable and platform
    independent. YAML works well with hierarchical data structures, making it easy
    to read and edit outside of the application if necessary. It allows the session
    state to be stored in a structured, standard format that can easily be transferred
    or modified as needed. YAML is often used for configuration files, but it’s also
    suitable for storing simple data structures such as, in our case, the <html:span
    class="No-Break">session state.</html:span></html:p> <html:p>We also need to create
    <html:code class="literal">logging_functions.py</html:code> . Here is <html:span
    class="No-Break">the code:</html:span></html:p> <html:p>The <html:code class="literal">logging_functions.py</html:code>
    module is responsible for recording events, user actions, and <html:a id="_idIndexMarker197"></html:a>other
    significant occurrences during the execution of the application into a log file.
    I’ve designed it to keep track of user actions and system events mainly to provide
    context for the PITS agent during its interactions with the user but also for
    monitoring and <html:span class="No-Break">debugging purposes.</html:span></html:p>
    <html:p>Here’s what the <html:a id="_idIndexMarker198"></html:a>functions in the
    <html:span class="No-Break">module do:</html:span></html:p> <html:ul><html:li><html:code
    class="literal">log_action(action, action_type)</html:code> : This function records
    an action or event. It accepts two arguments: <html:code class="literal">action</html:code>
    , which is a string describing what occurred, and <html:code class="literal">action_type</html:code>
    , which categorizes the action. The function gets the current <html:code class="literal">timestamp</html:code>
    , formats it with the action and type, and appends this entry to <html:code class="literal">LOG_FILE</html:code>
    . This helps maintain a chronological record of actions <html:span class="No-Break">and
    events</html:span></html:li> <html:li><html:code class="literal">reset_log()</html:code>
    : In the current implementation, when the users return to an existing session,
    they have the option to start a new one. When that happens, we clear the log file
    to avoid collecting too much data. This function opens <html:code class="literal">LOG_FILE</html:code>
    and truncates its content, effectively deleting all the logged entries. This is
    usually not a common thing to do in production environments, as logs are valuable
    for historical data analysis, but in our case, it simplifies <html:span class="No-Break">the
    flow</html:span></html:li></html:ul> <html:p>I know I’ve <html:a id="_idIndexMarker199"></html:a>promised
    we’ll have fun writing the code for PITS, and I am perfectly aware that logging
    seems less <html:em class="italic">ha-ha</html:em> and more <html:em class="italic">ho-hum</html:em>
    , but trust me, there’s no fun if you can’t debug your app. We needed to lay the
    foundations here and we’ll continue with the rest of the modules in the <html:span
    class="No-Break">next chapters.</html:span></html:p> <html:a id="_idTextAnchor067"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>Summary</html:title></html:head>
    <html:body><html:div class="epub-source"><html:h1 id="_idParaDest-68">Summary</html:h1>
    <html:div id="_idContainer034"><html:p>This chapter covered foundational concepts
    such as Documents, Nodes, and indexes – the core building blocks of LlamaIndex.
    I’ve demonstrated a simple workflow to load data as Documents, parse it into coherent
    Nodes using parsers, build an optimized index from the Nodes, and then query the
    index to retrieve relevant Nodes and synthesize <html:span class="No-Break">a
    response.</html:span></html:p> <html:p>The logging features of LlamaIndex were
    introduced as an important tool for understanding the underlying logic and debugging
    applications. Logs reveal how LlamaIndex parses, indexes, prompts the LLM, retrieves
    Nodes, and synthesizes responses. Customizing the LLM and other services used
    by LlamaIndex was shown using the <html:span class="No-Break"><html:code class="literal">Settings</html:code></html:span>
    <html:span class="No-Break">class.</html:span></html:p> <html:p>We’ve also started
    to build our PITS tutoring application, laying the groundwork with session management
    and logging functions. This modular structure will enable the exploration of LlamaIndex’s
    capabilities incrementally as the app is <html:span class="No-Break">built up.</html:span></html:p>
    <html:p>With the foundational knowledge established, it’s time to move on to more
    advanced LlamaIndex features. The <html:span class="No-Break">journey continues!</html:span></html:p></html:div></html:div></html:body></html:html>'
  prefs: []
  type: TYPE_NORMAL
