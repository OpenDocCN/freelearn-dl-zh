- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Training Pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ll explore the key components of an LLM training pipeline,
    from data ingestion and preprocessing to model architecture and optimization strategies.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll gain insights into implementing effective monitoring and logging systems,
    ensuring you can track your model’s progress and make data-driven decisions throughout
    the training process.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll be covering the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Components of a training pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data input and preprocessing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLM architecture design considerations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loss functions and optimization strategies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pipeline modularity and reusability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling your training pipeline for larger models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Components of a training pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An LLM training pipeline consists of several interconnected steps, each playing
    a role in the model’s development. We’ll present a basic pipeline here and explore
    many of these components in further depth as we progress through the chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dataset creation**: Builds preprocessed data into a format suitable for training,
    often involving shuffling and batching.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model architecture**: Defines the structure of the LLM, including the number
    of layers, attention mechanisms, and other architectural choices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training loop**: The core of the pipeline where the model learns from the
    data through forward and backward passes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimization**: Handles parameter updates based on calculated gradients and
    chosen optimization strategies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evaluation**: Regularly assesses model performance on validation data to
    track progress and prevent overfitting. We will cover this topic in more detail
    in [*Chapter 14*](B31249_14.xhtml#_idTextAnchor230).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Checkpointing**: Periodically saves model states to resume training or use
    for inference. We will cover this topic in detail in [*Chapter 10*](B31249_10.xhtml#_idTextAnchor162).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Logging and monitoring**: Continuously tracks training metrics and resource
    utilization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We’ll implement a basic LLM training pipeline using PyTorch and the Transformers
    library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: PyTorch is a popular deep learning framework that enables building neural networks
    through a dynamic computational graph, while the Transformers library implements
    the popular transformer architecture we discussed in [*Chapter 1*](B31249_01.xhtml#_idTextAnchor014).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code block demonstrates the loading of a Wikipedia dataset and
    the tokenization of its text content using a pre-trained GPT-2 tokenizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code block, we’re setting up the data ingestion and preprocessing
    components of our pipeline. We use the Hugging Face Datasets library to load a
    Wikipedia dataset, which provides a large corpus of text suitable for training
    an LLM. We then initialize a tokenizer based on the **GPT-2 model**, which will
    be used to preprocess our text data.
  prefs: []
  type: TYPE_NORMAL
- en: The `preprocess_function` defined above takes raw text examples and tokenizes
    them, truncating to a maximum length of 512 tokens and padding shorter sequences
    to this length. This ensures all our input sequences have the same length, which
    is necessary for efficient batch processing. We choose a `max_length` value of
    `512` as a balance between context length and memory efficiency. Longer sequences
    provide more context but require more memory and computation. Some recent LLM
    models, such as **Gemini 1.5 Pro**, can get as many as 2 million tokens in content
    length ([https://cloud.google.com/vertex-ai/generative-ai/docs/long-context](https://cloud.google.com/vertex-ai/generative-ai/docs/long-context)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we create our training DataLoader, which will handle batching and shuffling
    of our dataset during training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We set the batch size to `8`, which is chosen as a balance between memory usage
    and training efficiency. Larger batch sizes can lead to faster training but require
    more GPU memory. For LLMs, which often have a large number of parameters, smaller
    batch sizes are often necessary to fit the model and data in GPU memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then initialize our model architecture using the pre-trained GPT-2 model.
    This gives us a strong starting point for our LLM, leveraging the knowledge already
    captured in the pre-trained weights. Using a pre-trained model as a starting point
    is a common practice in transfer learning, allowing us to benefit from the general
    language understanding learned by the model on a large corpus of text. See the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the preceding code, for optimization, we use the `lr`) to `5e-5`,
    which is a common choice for fine-tuning pre-trained models. The learning rate
    is a hyperparameter that determines the size of the adjustments made to the model’s
    weights during training, influencing how quickly and effectively the model learns.
  prefs: []
  type: TYPE_NORMAL
- en: This learning rate offers a good balance between learning speed and stability.
    It’s small enough to allow for fine-grained updates to the pre-trained weights,
    but large enough to allow meaningful learning to occur.
  prefs: []
  type: TYPE_NORMAL
- en: The subsequent code blocks outline the essential stages of training a language
    model, including setting up the training process, initializing a logging tool,
    executing the main training loop with forward and backward passes, performing
    evaluation to assess model performance, and saving checkpoints of the model’s
    parameters during training.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by setting up the training loop:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we initialize the Weights & Biases (`wandb`) library for experiment tracking
    and logging of training metrics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We next implement an evaluation phase to assess the model’s performance on
    the training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we save a checkpoint of the model’s state dictionary at the end of
    each epoch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'These snippets implement the training loop, evaluation, checkpointing, and
    logging components of our pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: We set the number of training epochs to `3`, which means the model will iterate
    through the entire dataset three times during training. This hyperparameter can
    be adjusted based on your specific needs – increasing it may lead to better model
    performance if the model is underfitting, and decreasing it can help prevent overfitting
    and reduce training time. Monitor validation loss during training to determine
    the optimal number of epochs for your particular dataset and model architecture.
  prefs: []
  type: TYPE_NORMAL
- en: The learning rate scheduler implements a linear decay with warmup, which helps
    stabilize training in the early stages and then gradually reduces the learning
    rate to fine-tune the model more precisely. The learning rate controls how much
    a model adjusts its internal parameters during training – a higher rate means
    bigger adjustments but potential overshooting, while a lower rate means more precise
    but slower learning.
  prefs: []
  type: TYPE_NORMAL
- en: We use `wandb`) for logging, which allows us to track our training progress
    in real time and compare different runs ([https://wandb.ai/site](https://wandb.ai/site)).
    This is crucial for monitoring the training process and making informed decisions
    about hyperparameter tuning and model architecture changes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The training loop iterates over our data for the specified number of epochs.
    In each iteration, we do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Move the batch to the appropriate device (GPU if available)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform a forward pass through the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the loss
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform backpropagation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the model parameters
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the learning rate scheduler
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Log the training loss
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After each epoch, we perform a simple evaluation of the training data (in a
    real scenario, you’d use a separate validation set), log the evaluation loss,
    and save a checkpoint of the model. Checkpointing is needed for long-running training
    processes, allowing us to resume training from a saved state if needed.
  prefs: []
  type: TYPE_NORMAL
- en: As we’ve seen, the training pipeline involves several essential steps. Before
    the model architecture and training loop can function effectively, however, we
    must address data input and preprocessing, which we will discuss next.
  prefs: []
  type: TYPE_NORMAL
- en: Data input and preprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Efficient data handling is crucial for LLM training, as we discussed in *Part
    1* of this book. Here, let’s explore advanced techniques for data input and preprocessing:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required Python packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load and combine multiple datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize the tokenizer and perform `preprocess`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the DataLoader:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this enhanced preprocessing pipeline, we’re loading multiple datasets to
    increase the diversity of our training data. This is needed for LLMs, as a diverse
    dataset helps the model learn a broader range of language patterns and knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: We use a longer `max_length` value of `1024` tokens to provide more context
    to the model. This increased context length allows the model to capture longer-range
    dependencies in the text, which can be beneficial for many language-understanding
    tasks. However, it also increases memory usage and computational requirements,
    so there’s a trade-off to consider.
  prefs: []
  type: TYPE_NORMAL
- en: The `preprocess_function` now creates labels for causal language modeling by
    shifting the input sequences. This is a common approach for training language
    models, where the model’s task is to predict the next token given the previous
    tokens. During preprocessing, handling edge cases such as emojis, URLs, and non-standard
    characters can enhance model performance. Emojis can convey nuanced emotions and
    context, requiring appropriate encoding or tokenization to preserve their meaning
    without introducing noise. URLs often contain valuable information but can vary
    widely in structure, so they might be replaced with placeholder tokens to maintain
    consistency while preventing the model from overfitting to specific links. Non-standard
    characters, including symbols from different languages or special punctuation,
    need careful normalization or removal to reduce complexity and avoid confusion
    during training. By addressing these edge cases through strategies such as normalization,
    token replacement, and selective filtering, preprocessing pipelines can better
    prepare diverse and complex data, enhancing the robustness and accuracy of the
    resulting language models.
  prefs: []
  type: TYPE_NORMAL
- en: We use multiprocessing (`num_proc=4`) to speed up the preprocessing. The number
    of processes should be adjusted based on your CPU cores and available memory.
    Multiprocessing can significantly reduce preprocessing time, especially for large
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: The batch size is increased to `16`, which is more suitable for larger GPU memory.
    The custom `collate_fn` in the DataLoader ensures proper batching of our preprocessed
    data. This function stacks the arrays for each key in the batch, creating tensor-like
    structures that can be efficiently processed by PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: With the data appropriately prepared, we now turn our attention to the LLM architecture
    design considerations, which dictate the model’s capacity to effectively learn
    from and understand data input.
  prefs: []
  type: TYPE_NORMAL
- en: LLM architecture design considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When designing the architecture for an LLM, several factors come into play.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the key factors influencing LLM architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Vocabulary size**: Determines the size of the input and output embedding
    layers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Maximum sequence length (context size)**: Defines the amount of preceding
    text the model can consider'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Embedding dimension**: Specifies the size of each token’s vector representation,
    influencing the model’s ability to capture information'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of transformer layers**: Represents the depth of the network, impacting
    the complexity of patterns the model can learn'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of attention heads**: Allows the model to attend to different parts
    of the input simultaneously'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model size (number of parameters)**: Overall capacity of the model, influenced
    by embedding dimension, number of layers, and attention heads'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dataset size**: The amount and diversity of training data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of training steps**: The duration of the optimization process'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Computational resources**: Hardware constraints that affect model size, training
    speed, and overall feasibility.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Risk of overfitting**: Higher with larger models and smaller datasets'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data quality**: The cleanliness and relevance of the training data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Efficiency of model architecture**: Design choices that can improve performance
    without drastically increasing model size'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training algorithms**: Optimization techniques and strategies'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data curation practices**: Methods for selecting and preparing training data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Test time compute**: Computational resources available during inference'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following code block, we provide examples of configuring some of these
    factors using a GPT-2 style language model, specifying key architectural parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This configuration creates a GPT-2 style model with `12` layers and `12` attention
    heads. Let’s break down the key parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`vocab_size`: Set to `50257`, which is the vocabulary size of the original
    GPT-2 model. This determines the size of the embedding layer and the output layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_positions` and `n_ctx`: Both are set to `1024`, matching our preprocessing
    step. This defines the maximum sequence length the model can handle.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_embd`: The embedding dimension, set to `768`. This determines the size of
    the hidden states throughout the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_layer`: The number of transformer layers, set to `12`. More layers can capture
    more complex patterns but increase computational requirements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_head`: The number of attention heads, set to `12`. Multiple attention heads
    allow the model to focus on different aspects of the input simultaneously.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The embedding dimension of `768` and the `12` layers provide a balanced trade-off
    between model capacity and computational efficiency. This configuration results
    in a model with about 124 million parameters, which is substantial but still trainable
    on common GPU hardware.
  prefs: []
  type: TYPE_NORMAL
- en: For larger models, you might increase `n_layer`, `n_embd`, and `n_head`. However,
    this would also increase the computational requirements and the risk of overfitting,
    especially on smaller datasets. When scaling up, consider techniques such as gradient
    accumulation, mixed precision training, and distributed training to manage the
    increased computational load.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a broader scope, **scaling laws** can be considered. The scaling laws for
    LLMs describe how performance improves predictably as three key factors increase:
    model size (number of parameters), dataset size (amount of training data), and
    the number of training steps (optimization iterations). Specifically, larger models
    tend to capture more complex patterns and exhibit better generalization, larger
    datasets provide more diverse information for learning, and more training steps
    allow the model to refine its understanding and reduce errors. For optimal performance,
    these factors should scale proportionally – for instance, increasing the model
    size should be matched by a corresponding increase in dataset size and training
    steps. This balanced scaling ensures that each component supports the others,
    preventing bottlenecks such as overfitting smaller models on vast datasets or
    undertraining large models with insufficient data.'
  prefs: []
  type: TYPE_NORMAL
- en: However, recent advancements and practical challenges have shown that simply
    scaling these factors is not always sufficient for continual performance improvements.
    Issues such as diminishing returns, where each additional parameter or data point
    contributes less to overall performance, have become more apparent. Additionally,
    the immense computational and energy resources required for training increasingly
    large models raise sustainability and accessibility concerns. Data quality also
    becomes a critical factor, as larger datasets may introduce more noise and biases,
    potentially degrading model performance. For more details about this, please see
    the article at [https://www.pcgamer.com/software/ai/open-ai-co-founder-reckons-ai-training-has-hit-a-wall-forcing-ai-labs-to-train-their-models-smarter-not-just-bigger/](https://www.pcgamer.com/software/ai/open-ai-co-founder-reckons-ai-training-has-hit-a-wall-forcing-ai-labs-to-train-their-models-smarter-not-just-bigger/).
  prefs: []
  type: TYPE_NORMAL
- en: 'To address these challenges, researchers are exploring more efficient model
    architectures, improved training algorithms, better data curation practices, and
    test time compute. See my Medium article for more details on test time compute:
    [https://kenhuangus.medium.com/test-time-compute-3633a4c55716](https://kenhuangus.medium.com/test-time-compute-3633a4c55716)'
  prefs: []
  type: TYPE_NORMAL
- en: At the beginning of 2025, DeepSeek (an AI startup in China) announced some model
    training innovations by introducing a suite of techniques aimed at significantly
    increasing efficiency and reducing costs, while simultaneously enhancing the model’s
    reasoning capabilities ([https://arxiv.org/abs/2501.12948](https://arxiv.org/abs/2501.12948)).
    Unlike traditional approaches that rely heavily on vast computational resources
    and human-supervised fine-tuning, DeepSeek leverages large-scale reinforcement
    learning focused on reasoning tasks, using automated reward systems rather than
    human feedback. Key innovations include multi-token prediction, which allows the
    model to learn from multiple future tokens at once, increasing sample efficiency,
    and speeding up training. DeepSeek also employs a mixture-of-experts architecture
    to activate only relevant sub-networks for each task, thus reducing computational
    load. By optimizing both algorithms and hardware, DeepSeek has managed to train
    highly capable models at a fraction of the cost and time required by competitors,
    setting new standards for open, efficient, and powerful AI development.
  prefs: []
  type: TYPE_NORMAL
- en: Having explored the architectural design considerations and model training innovations
    for LLMs — along with a code example demonstrating how to configure model training
    parameters — we are now ready to examine how these architectural choices are actually
    learned during training. In this next section, we will discuss the loss function
    and optimization strategies, which serve as the engine that drives the model to
    adjust its internal parameters based on both the training data and the architecture
    we have defined.
  prefs: []
  type: TYPE_NORMAL
- en: Loss functions and optimization strategies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LLMs typically use **cross-entropy loss** for training. This approach measures
    the difference between the model’s predicted probability distribution of words
    and the actual distribution in the training data. By minimizing this loss, LLMs
    learn to generate more accurate and contextually appropriate text. Cross-entropy
    loss is particularly well-suited for language tasks due to its ability to handle
    the high dimensionality and discrete nature of textual data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s implement this along with some advanced optimization techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import the required PyTorch libraries and specific modules from the
    Transformers library for optimization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we configure the AdamW optimizer with a specified learning rate and weight
    decay:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we define a linear learning rate scheduler with a warm-up period:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Subsequently, we set up the training device and initiate the main training
    loop:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we implement gradient clipping to prevent exploding gradients during
    training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this optimization setup, we use the `AdamW` optimizer with a learning rate
    of `5e-5` and weight decay of `0.01`. The algorithm adapts the learning rate for
    each parameter based on the first and second moments of the gradients, allowing
    it to handle sparse gradients effectively. This makes `AdamW` particularly useful
    for training large neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: The weight decay of `0.01` adds a small regularization term to the loss function,
    which can help prevent overfitting by penalizing large weight values.
  prefs: []
  type: TYPE_NORMAL
- en: We implement a `warmup`. The `warmup` phase helps stabilize training in the
    early stages by gradually increasing the learning rate from a very small value.
    After the `warmup` phase, the learning rate decreases linearly. This schedule
    can help the model converge to a better optimum.
  prefs: []
  type: TYPE_NORMAL
- en: In the training loop, we implement `max_norm` value of `1.0`. Gradient clipping
    prevents exploding gradients by scaling down gradient values that exceed a certain
    threshold. This is particularly important for LLMs, which can be prone to unstable
    gradients due to their depth and the long-range dependencies they capture.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we learned about AdamW optimization, learning rate scheduling
    with warmup, and gradient clipping for stable LLM training. Next, we talk about
    logging the training process, which is crucial for monitoring progress and using
    tools like **TensorBoard** to gain insights for improvement.
  prefs: []
  type: TYPE_NORMAL
- en: Logging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Effective logging can be useful for tracking the progress of LLM training.
  prefs: []
  type: TYPE_NORMAL
- en: The following code blocks demonstrate how to integrate TensorBoard for effective
    logging during the training of an LLM using PyTorch. Let’s break down each part.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first initialize the TensorBoard `SummaryWriter` for logging training progress:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we set the model to training mode, initialize variables for tracking
    loss, define the logging interval, and record the start time to monitor training
    performance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we move on to the training loop. We process each batch by moving data
    to the appropriate device, performing forward and backward passes, applying gradient
    clipping, and updating the model’s parameters using the optimizer and scheduler:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We log the training metrics to TensorBoard at specified intervals, calculate
    the average loss, measure the elapsed time, print the progress to the console,
    and reset the tracking variables for the next interval:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This enhanced training loop uses TensorBoard for logging training loss and
    learning rate. TensorBoard is a powerful tool for visualizing training progress
    and comparing different runs. We log the following metrics:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`log_interval` batches. A decreasing trend in this metric indicates that the
    model is learning.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learning rate**: We log the current learning rate to visualize how it changes
    over time due to our learning rate scheduler.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We set `log_interval` to `100`, meaning we log and print out progress information
    every 100 batches. This interval strikes a balance between getting frequent updates
    and not slowing down training too much with logging operations. You may need to
    adjust this based on your dataset size and training speed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output or log information includes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Current epoch and batch number
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Current learning rate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time per batch (in milliseconds)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Current loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This detailed logging allows you to monitor the training process closely, helping
    you identify issues such as unstable loss, learning rate problems, or unexpectedly
    slow training.
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline modularity and reusability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Modularity** and **reusability** are fundamental principles for building
    efficient pipelines because they make code more maintainable, adaptable, and reliable.
    By breaking down a pipeline into independent, reusable modules (such as data preprocessing,
    model training, and evaluation components), developers can easily modify individual
    parts without affecting others, test each component separately, and reuse proven
    code across different projects.'
  prefs: []
  type: TYPE_NORMAL
- en: This approach not only saves development time but also ensures consistency in
    operations, reduces the chance of errors, and makes it easier for teams to collaborate
    by working on separate modules while maintaining clear interfaces between components.
    In the case of training pipelines, encapsulating processes in reusable classes
    allows for flexible configuration, seamless integration with different datasets,
    and straightforward sharing of standardized implementations across multiple projects.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make our pipeline more modular and reusable, let’s encapsulate our training
    process in a class:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start with class definition:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we define the train epoch function. The function sets the model to training
    mode and iterates over the training data, processing each batch by computing the
    loss, performing backpropagation with gradient clipping, and updating the model
    parameters using the optimizer and scheduler:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we periodically log the training progress to both TensorBoard and the
    console by checking if the current batch index is a multiple of the `log_interval`;
    if it is, we calculate the average loss and elapsed time since the last log, record
    the training loss and learning rate to TensorBoard using the `SummaryWriter`,
    print a formatted progress update including batch number, learning rate, milliseconds
    per batch, and current loss to the console, and then reset the accumulated `total_loss`
    and `start_time` for the next logging interval:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, the `train` function orchestrates the training process by looping through
    the specified number of epochs, printing a message at the start of each epoch,
    invoking the `train_epoch` method to perform training for that epoch, and, finally,
    closing the writer once all epochs are completed. It serves as the main entry
    point for training, providing a structure where additional features such as validation
    and checkpointing can be integrated as needed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Lastly, we instantiate the `LLMTrainer` class with the specified model, training
    data loader, optimizer, scheduler, and device. Then, the training process is started
    by calling the `train` method to execute three full training epochs, thereby initiating
    and managing the model’s learning cycle:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This modular design offers several advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: '`LLMTrainer` class, making it easier to manage and understand.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reusability**: You can easily use this trainer for different models or datasets
    by creating a new instance with different parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Extensibility**: The class structure makes it easy to add new functionality.
    For example, you could add methods for validation, checkpointing, or early stopping.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Separation of concerns**: The training logic is separated from the model
    definition and data preparation, following good software engineering principles.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following log demonstrates the training process over 3 epochs, with periodic
    logging every 100 batches. Each log entry includes the current batch number, total
    batches, learning rate, milliseconds per batch, and the average loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is an explanation of the above simulated log:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Starting epoch 1`, indicating the commencement of a new training cycle'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`100/1000 batches`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lr 0.01`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ms/batch 45.67`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`loss 2.35`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learning rate schedule**: Notice how the learning rate decreases over epochs,
    reflecting the scheduler’s adjustments to facilitate better convergence*   `Training
    completed. Writer closed.`) indicates the end of the training process and the
    closure of the logging writer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The log provides a clear overview of the training dynamics, allowing developers
    and researchers to monitor the model’s learning progress, adjust hyperparameters
    if necessary, and ensure that the training is proceeding as expected.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling your training pipeline for larger models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To train larger models, we need to employ techniques such as gradient accumulation
    and mixed precision training.
  prefs: []
  type: TYPE_NORMAL
- en: 'To train very large language models that might not fit on a single GPU, the
    following code introduces a special `LargeScaleLLMTrainer`. It uses two main tricks
    to handle this:'
  prefs: []
  type: TYPE_NORMAL
- en: First, gradient accumulation allows us to simulate having access to a larger
    GPU. Instead of updating the model's parameters after every small batch of data,
    we process several small batches, accumulating their gradients along the way.
    Only after a predefined number of batches do we perform an actual update to the
    model's parameters. This technique enables the model to learn as if it had seen
    a much larger batch of data, without requiring the memory capacity of an extremely
    large GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Second, it employs mixed precision training, a technique where the computer
    performs many calculations using smaller, lower-precision numbers (which require
    less memory and are faster to compute), while reserving higher-precision numbers
    for situations where accuracy is critical. This approach accelerates training
    and reduces overall memory usage. To mitigate potential issues that can arise
    from using lower-precision values, GradScaler is used to maintain numerical stability
    during backpropagation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code defines how this special trainer works, including how it
    processes data, calculates the loss, and updates the model’s learning using these
    tricks. It also still includes important steps like making sure the gradients
    (how the model should change) don’t get too big and logging progress so we can
    see how the training is going. Finally, it shows a simple example of how to use
    this special trainer. Now, let us break it into several parts:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us start by importing the relevant Python package and defining the class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can then define the training epoch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we implement the following code block, which updates the model’s parameters,
    learning rate, and gradient scaler only after processing a defined number of batches
    (`accumulation_steps)`, effectively simulating a larger batch size while managing
    memory constraints:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then periodically calculate and log the average training loss and learning
    rate to TensorBoard, while also printing a summary of the current training progress
    to the console at intervals defined by `log_interval`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We demonstrate the initialization and execution of a large-scale language model
    training process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This enhanced trainer uses two key techniques for scaling to larger models:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`accumulation_steps`). This allows us to effectively increase the batch size
    without increasing memory usage, which is effective for training large models
    on limited GPU memory. We divide the loss by `accumulation_steps` to maintain
    the same effective learning rate.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`float16`, where possible, while maintaining `float32` master weights. This
    can significantly speed up training and reduce memory usage, especially on modern
    GPUs with tensor cores.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`GradScaler` is used to prevent underflow in `float16` calculations. It scales
    the loss to prevent small gradient values, then unscales before the optimizer
    step.'
  prefs: []
  type: TYPE_NORMAL
- en: We still apply gradient clipping, but now it’s done after unscaling the gradients
    to ensure we’re clipping the true gradient values.
  prefs: []
  type: TYPE_NORMAL
- en: For even larger models, you might consider techniques such as **model parallelism**
    (splitting the model across multiple GPUs), **pipeline parallelism** (splitting
    the model into stages), or using specialized libraries such as **DeepSpeed** or
    **Megatron-LM**. These advanced techniques allow the training of models with billions
    of parameters across multiple GPUs or even multiple machines. Memory offloading
    can be a good alternative when GPU memory is insufficient to handle vast amounts
    of data and model parameters. Memory offloading involves transferring parts of
    the model’s data or computations to alternative memory storage, such as **Non-Volatile
    Memory Express** (**NVMe**) SSDs. By leveraging NVMe memory, which offers high-speed
    data access compared to traditional storage, systems can effectively manage and
    store intermediate activations, gradients, and model states that exceed GPU memory
    capacity. This approach allows for training larger models or using higher batch
    sizes without requiring immediate GPU memory expansion. However, it introduces
    additional latency due to data transfer between the GPU and NVMe storage, which
    can impact training speed. Optimizing data access patterns and utilizing efficient
    offloading strategies can minimize performance overhead and maintain effective
    training workflows when employing memory offloading techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about a practical pattern of pipeline design for
    training LLMs. You learned how to create efficient data preprocessing workflows,
    implement model architectures, and apply advanced optimization strategies. You
    now understand how to set up effective logging systems to track your model’s progress.
    You also explored techniques for building modular and reusable pipelines and discovered
    methods for scaling your training process to accommodate larger models. With these
    skills, you’re well equipped to train state-of-the-art language models efficiently
    and effectively.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll explore the hyperparameter tuning pattern.
  prefs: []
  type: TYPE_NORMAL
