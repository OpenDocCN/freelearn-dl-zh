<html><head></head><body>
<div id="_idContainer119">
<h1 class="chapter-number" id="_idParaDest-125"><a id="_idTextAnchor132"/><span class="koboSpan" id="kobo.1.1">7</span></h1>
<h1 id="_idParaDest-126"><a id="_idTextAnchor133"/><span class="koboSpan" id="kobo.2.1">Building Question Answering Systems and Conversational Interfaces</span></h1>
<p><span class="koboSpan" id="kobo.3.1">In this chapter, we will delve into</span><a id="_idIndexMarker551"/><span class="koboSpan" id="kobo.4.1"> the realm of </span><strong class="bold"><span class="koboSpan" id="kobo.5.1">question answering</span></strong><span class="koboSpan" id="kobo.6.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.7.1">QA</span></strong><span class="koboSpan" id="kobo.8.1">) and conversational interfaces, harnessing the power of Amazon Bedrock. </span><span class="koboSpan" id="kobo.8.2">The chapter begins with unveiling real-world use cases of QA with Amazon Bedrock, demonstrating the practical applications and benefits of this technology. </span><span class="koboSpan" id="kobo.8.3">Moving forward, the chapter will cover architectural patterns for QA on both small and large documents, providing a solid foundation to understand the underlying mechanics. </span><span class="koboSpan" id="kobo.8.4">Additionally, the concept of conversation memory will be explained, allowing for the storage and utilization of chat history, thereby enabling more contextually aware and </span><span class="No-Break"><span class="koboSpan" id="kobo.9.1">coherent conversations.</span></span></p>
<p><span class="koboSpan" id="kobo.10.1">The chapter will also dive into the concept of embeddings and their significance within the architectural flow of QA systems. </span><span class="koboSpan" id="kobo.10.2">Furthermore, we will learn about prompt engineering techniques for chatbots, equipping you with the skills to craft effective prompts and enhance the performance of their conversational interfaces. </span><span class="koboSpan" id="kobo.10.3">Contextual awareness will also be addressed, explaining how to develop chatbots that can seamlessly integrate and leverage external files and </span><span class="No-Break"><span class="koboSpan" id="kobo.11.1">data sources.</span></span></p>
<p><span class="koboSpan" id="kobo.12.1">Finally, we will conclude by exploring real-world use cases of conversational interfaces, showcasing the diverse applications and potential impact of this technology across </span><span class="No-Break"><span class="koboSpan" id="kobo.13.1">various domains.</span></span></p>
<p><span class="koboSpan" id="kobo.14.1">The key topics that will be covered in this chapter include </span><span class="No-Break"><span class="koboSpan" id="kobo.15.1">the following:</span></span></p>
<ul>
<li><span class="No-Break"><span class="koboSpan" id="kobo.16.1">QA overview</span></span></li>
<li><span class="koboSpan" id="kobo.17.1">Document ingestion with </span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">Amazon Bedrock</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.19.1">Conversational interfaces</span></span><a id="_idTextAnchor134"/><a id="_idTextAnchor135"/><a id="_idTextAnchor136"/></li>
</ul>
<h1 id="_idParaDest-127"><a id="_idTextAnchor137"/><span class="koboSpan" id="kobo.20.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.21.1">This chapter requires you to have access to an AWS account. </span><span class="koboSpan" id="kobo.21.2">If you don’t have it already, you can go to </span><a href="https://aws.amazon.com/getting-started/"><span class="koboSpan" id="kobo.22.1">https://aws.amazon.com/getting-started/</span></a><span class="koboSpan" id="kobo.23.1"> and create an </span><span class="No-Break"><span class="koboSpan" id="kobo.24.1">AWS account.</span></span></p>
<p><span class="koboSpan" id="kobo.25.1">Secondly, you will need to install and configure the AWS CLI (</span><a href="https://aws.amazon.com/cli/"><span class="koboSpan" id="kobo.26.1">https://aws.amazon.com/cli/</span></a><span class="koboSpan" id="kobo.27.1">) after you create an account, which will be needed to access Amazon Bedrock FMs from your local machine. </span><span class="koboSpan" id="kobo.27.2">Since a major chunk of the code cells we will execute is based in Python, setting up the AWS SDK for Python (Boto3) (</span><a href="https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html"><span class="koboSpan" id="kobo.28.1">https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html</span></a><span class="koboSpan" id="kobo.29.1">) will be beneficial at this point. </span><span class="koboSpan" id="kobo.29.2">You can carry out the Python setup in the following ways – install it on your local machine, use AWS Cloud9, utilize AWS Lambda, or leverage </span><span class="No-Break"><span class="koboSpan" id="kobo.30.1">Amazon SageMaker.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.31.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.32.1">There is a charge associated with the invocation and customization of the FMs of Amazon Bedrock. </span><span class="koboSpan" id="kobo.32.2">Refer to </span><a href="https://aws.amazon.com/bedrock/pricing/"><span class="koboSpan" id="kobo.33.1">https://aws.amazon.com/bedrock/pricing/</span></a><span class="koboSpan" id="kobo.34.1"> to </span><span class="No-Break"><span class="koboSpan" id="kobo.35.1">learn more.</span></span></p>
<h1 id="_idParaDest-128"><a id="_idTextAnchor138"/><span class="koboSpan" id="kobo.36.1">QA overview</span></h1>
<p><span class="koboSpan" id="kobo.37.1">QA systems are</span><a id="_idIndexMarker552"/><span class="koboSpan" id="kobo.38.1"> designed to understand natural language queries and provide relevant answers based on a given context or knowledge source. </span><span class="koboSpan" id="kobo.38.2">These systems leverage advanced NLP techniques and machine learning models to comprehend the intent behind a user’s question, extracting the most appropriate response from the </span><span class="No-Break"><span class="koboSpan" id="kobo.39.1">available information.</span></span></p>
<p><span class="koboSpan" id="kobo.40.1">Let’s consider an example scenario of a typical QA system: suppose you are a content writer for a technology company and you need to explain the concept of </span><strong class="bold"><span class="koboSpan" id="kobo.41.1">optical character recognition</span></strong><span class="koboSpan" id="kobo.42.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.43.1">OCR</span></strong><span class="koboSpan" id="kobo.44.1">) to</span><a id="_idIndexMarker553"/><span class="koboSpan" id="kobo.45.1"> your audience. </span><span class="koboSpan" id="kobo.45.2">A QA system could assist you in this task by providing relevant information from its knowledge base, or by analyzing a given text corpus related </span><span class="No-Break"><span class="koboSpan" id="kobo.46.1">to OCR.</span></span></p>
<p><span class="koboSpan" id="kobo.47.1">Here’s how a QA system might handle a query such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.48.1">What is optical character recognition </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.49.1">used for?</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.50.1">:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.51.1">Query understanding</span></strong><span class="koboSpan" id="kobo.52.1">: The</span><a id="_idIndexMarker554"/><span class="koboSpan" id="kobo.53.1"> system first analyzes the query to understand the user’s intent and extract key entities and concepts. </span><span class="koboSpan" id="kobo.53.2">In this case, it recognizes that the query asks about the use cases or applications of optical </span><span class="No-Break"><span class="koboSpan" id="kobo.54.1">character recognition.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.55.1">Context retrieval</span></strong><span class="koboSpan" id="kobo.56.1">: The system then searches its knowledge base or the provided text corpus to find relevant information related to OCR and its applications. </span><span class="koboSpan" id="kobo.56.2">It may identify passages or paragraphs that discuss OCR’s purpose and </span><span class="No-Break"><span class="koboSpan" id="kobo.57.1">practical uses.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.58.1">Answer extraction</span></strong><span class="koboSpan" id="kobo.59.1">: After retrieving the relevant context, the system employs advanced NLP techniques, such as named entity recognition, relation extraction, and semantic analysis, to identify the most relevant information that directly answers the query. </span><span class="koboSpan" id="kobo.59.2">It may extract specific use cases or applications of OCR from </span><span class="No-Break"><span class="koboSpan" id="kobo.60.1">the text.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.61.1">Answer generation</span></strong><span class="koboSpan" id="kobo.62.1">: Finally, the system composes a concise and natural-sounding answer based on the extracted information. </span><span class="koboSpan" id="kobo.62.2">For example, it might respond with </span><a id="_idIndexMarker555"/><span class="koboSpan" id="kobo.63.1">something like </span><span class="No-Break"><span class="koboSpan" id="kobo.64.1">the following:</span></span></li>
</ul>
<pre class="console"><span class="koboSpan" id="kobo.65.1">
"Optical character recognition (OCR) is used to convert text in images, scanned documents, or handwritten text into machine-readable and editable text format. </span><span class="koboSpan" id="kobo.65.2">It has several applications, including:
Digitizing printed documents, books, and archives for easier storage and search.
</span><span class="koboSpan" id="kobo.65.3">Extracting text from images or photographs for indexing or further processing.
</span><span class="koboSpan" id="kobo.65.4">Automating data entry tasks by extracting information from forms, invoices, or receipts.
</span><span class="koboSpan" id="kobo.65.5">Enabling text-to-speech capabilities for visually impaired users.
</span><span class="koboSpan" id="kobo.65.6">Facilitating translation of text between languages by first converting it to editable format."</span></pre>
<p><span class="koboSpan" id="kobo.66.1">The system may also</span><a id="_idIndexMarker556"/><span class="koboSpan" id="kobo.67.1"> provide additional context, examples, or relevant information to enhance the user’s understanding of </span><span class="No-Break"><span class="koboSpan" id="kobo.68.1">the topic.</span></span></p>
<p><span class="koboSpan" id="kobo.69.1">Theoretically speaking, all of this looks perfect and straightforward. </span><span class="koboSpan" id="kobo.69.2">However, let’s ponder over some challenges in </span><span class="No-Break"><span class="koboSpan" id="kobo.70.1">this situation.</span></span></p>
<p><span class="koboSpan" id="kobo.71.1">Since these QA systems are designed to automatically generate responses to inquiries by analyzing and extracting relevant information from a provided set of data or text sources, they may or may not explicitly contain the complete answer to the given query. </span><span class="koboSpan" id="kobo.71.2">In order words, a system’s ability to infer and combine disparate pieces of information from various contexts is crucial, as the complete answer may not be readily available in a single, self-contained statement within the </span><span class="No-Break"><span class="koboSpan" id="kobo.72.1">provided data.</span></span></p>
<p><span class="koboSpan" id="kobo.73.1">QA poses a significant challenge, as it necessitates models to develop a deep comprehension of the semantic meaning and intent behind a query, rather than merely relying on superficial keyword matching or pattern recognition. </span><span class="koboSpan" id="kobo.73.2">This elevated level of language understanding is crucial for accurately identifying the relevant information required to formulate a suitable response, even when the exact phrasing or terminology differs between the query and the </span><span class="No-Break"><span class="koboSpan" id="kobo.74.1">available context.</span></span></p>
<p><span class="koboSpan" id="kobo.75.1">Overcoming these hurdles is essential for developing intelligent systems that can engage in fluid dialogue, provide accurate information, and enhance user experiences across a wide range of domains </span><span class="No-Break"><span class="koboSpan" id="kobo.76.1">and applications.</span></span></p>
<p><span class="koboSpan" id="kobo.77.1">At the time of writing, tons of generative AI use cases have spawned in a short period. </span><span class="koboSpan" id="kobo.77.2">Enterprises are scaling their conversational interfaces – chatbots and QA systems – with the goal of reducing manual labor and replacing existing frameworks with automated generative </span><span class="No-Break"><span class="koboSpan" id="kobo.78.1">AI systems.</span></span></p>
<p><span class="koboSpan" id="kobo.79.1">One of the most promising applications of LLMs and generative AI technology is, in fact, QA. </span><span class="koboSpan" id="kobo.79.2">Being able </span><a id="_idIndexMarker557"/><span class="koboSpan" id="kobo.80.1">to ask natural language questions and receive accurate, relevant answers could transform how we interact with information </span><span class="No-Break"><span class="koboSpan" id="kobo.81.1">and computers.</span></span></p>
<h2 id="_idParaDest-129"><a id="_idTextAnchor139"/><span class="koboSpan" id="kobo.82.1">Potential QA applications</span></h2>
<p><span class="koboSpan" id="kobo.83.1">The applications for a</span><a id="_idIndexMarker558"/><span class="koboSpan" id="kobo.84.1"> robust QA system are far-reaching across many industries </span><span class="No-Break"><span class="koboSpan" id="kobo.85.1">and domains:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.86.1">Customer service</span></strong><span class="koboSpan" id="kobo.87.1">: Allow customers to ask questions and receive tailored help and troubleshooting in a natural language rather than </span><span class="No-Break"><span class="koboSpan" id="kobo.88.1">search documentation</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.89.1">Research and analytics</span></strong><span class="koboSpan" id="kobo.90.1">: Allow analysts and researchers to ask open-ended exploratory questions to discover insights across </span><span class="No-Break"><span class="koboSpan" id="kobo.91.1">large datasets</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.92.1">Education</span></strong><span class="koboSpan" id="kobo.93.1">: Create intelligent tutoring systems where students can ask follow-up questions and receive explanations at </span><span class="No-Break"><span class="koboSpan" id="kobo.94.1">their level</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.95.1">Knowledge management</span></strong><span class="koboSpan" id="kobo.96.1">: Make an organization’s data, documentation, and processes more accessible by allowing natural </span><span class="No-Break"><span class="koboSpan" id="kobo.97.1">language queries</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.98.1">Of course, as with any generative AI system, there are concerns around factual accuracy, safety, and potential misuse that must be carefully addressed as QA systems are developed </span><span class="No-Break"><span class="koboSpan" id="kobo.99.1">and deployed.</span></span></p>
<p><span class="koboSpan" id="kobo.100.1">Nonetheless, the ability to break down barriers between humans and information through natural </span><a id="_idIndexMarker559"/><span class="koboSpan" id="kobo.101.1">language queries represents a key frontier in AI’s advancement. </span><span class="koboSpan" id="kobo.101.2">With FMs available on Amazon Bedrock, such QA systems powered by LLMs provide an exciting glimpse at </span><span class="No-Break"><span class="koboSpan" id="kobo.102.1">that future.</span></span></p>
<h2 id="_idParaDest-130"><a id="_idTextAnchor140"/><span class="koboSpan" id="kobo.103.1">QA systems with Amazon Bedrock</span></h2>
<p><span class="koboSpan" id="kobo.104.1">Enterprise-grade QA</span><a id="_idIndexMarker560"/><span class="koboSpan" id="kobo.105.1"> systems are usually built on the foundation</span><a id="_idIndexMarker561"/><span class="koboSpan" id="kobo.106.1"> of cutting-edge NLP techniques, including transformer architectures and transfer learning. </span><span class="koboSpan" id="kobo.106.2">They should be designed to understand the nuances of human language, enabling it to comprehend complex queries and extract relevant information from various </span><span class="No-Break"><span class="koboSpan" id="kobo.107.1">data sources.</span></span></p>
<p><span class="koboSpan" id="kobo.108.1">One of the key advantages of Amazon Bedrock is its ability to handle open-ended questions that require reasoning and inference. </span><span class="koboSpan" id="kobo.108.2">Unlike traditional QA systems that rely on predefined rules or patterns, Bedrock can understand the underlying context and provide thoughtful responses based on the information it </span><span class="No-Break"><span class="koboSpan" id="kobo.109.1">has learned.</span></span></p>
<p><span class="koboSpan" id="kobo.110.1">With a plethora of FMs available on Amazon Bedrock, developers, data scientists or generative AI enthusiasts can build applications or services that can potentially excel at dealing with ambiguity and uncertainty. </span><span class="koboSpan" id="kobo.110.2">If the available information is incomplete or contradictory, these engaging applications can provide responses that reflect their level of certainty, or they can request additional information, making the interaction more natural </span><span class="No-Break"><span class="koboSpan" id="kobo.111.1">and human-like.</span></span></p>
<p><span class="koboSpan" id="kobo.112.1">Moreover, Amazon Bedrock is highly scalable and can be easily integrated into various applications and platforms, such as chatbots, virtual assistants, and knowledge management systems. </span><span class="koboSpan" id="kobo.112.2">Its cloud-based architecture and high availability nature ensure that it can handle</span><a id="_idIndexMarker562"/><span class="koboSpan" id="kobo.113.1"> high </span><a id="_idIndexMarker563"/><span class="koboSpan" id="kobo.114.1">volumes of queries and adapt to changing data and </span><span class="No-Break"><span class="koboSpan" id="kobo.115.1">user requirements.</span></span></p>
<h3><span class="koboSpan" id="kobo.116.1">QA without context</span></h3>
<p><span class="koboSpan" id="kobo.117.1">In scenarios where </span><a id="_idIndexMarker564"/><span class="koboSpan" id="kobo.118.1">no additional context or </span><a id="_idIndexMarker565"/><span class="koboSpan" id="kobo.119.1">supporting documents are provided, QA systems must rely solely on their pre-trained knowledge to generate responses. </span><span class="koboSpan" id="kobo.119.2">This type of open-domain </span><strong class="bold"><span class="koboSpan" id="kobo.120.1">QA without context</span></strong><span class="koboSpan" id="kobo.121.1"> presents several key challenges compared to scenarios where context is given. </span><span class="koboSpan" id="kobo.121.2">Some of these challenges are </span><span class="No-Break"><span class="koboSpan" id="kobo.122.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.123.1">Knowledge scope and completeness</span></strong><span class="koboSpan" id="kobo.124.1">: When no context is provided, the QA system’s knowledge comes entirely from what was present in its training data. </span><span class="koboSpan" id="kobo.124.2">This makes the scope and completeness of the training data extremely important. </span><span class="koboSpan" id="kobo.124.3">Ideally, the training data should cover a wide range of topics with factual accuracy. </span><span class="koboSpan" id="kobo.124.4">However, training datasets can have gaps, biases, or errors, which then get encoded into the </span><span class="No-Break"><span class="koboSpan" id="kobo.125.1">model’s knowledge.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.126.1">Querying the right knowledge</span></strong><span class="koboSpan" id="kobo.127.1">: Without context to ground the question, the QA system must accurately map the question to the relevant areas of knowledge in its parameters. </span><span class="koboSpan" id="kobo.127.2">This requires strong natural language understanding capabilities to correctly interpret the query, identify key entities/relations, and retrieve the appropriate factual knowledge to formulate </span><span class="No-Break"><span class="koboSpan" id="kobo.128.1">a response.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.129.1">Hallucination</span></strong><span class="koboSpan" id="kobo.130.1">: A critical challenge is hallucination – when the model generates incorrect information that contradicts its training data. </span><span class="koboSpan" id="kobo.130.2">Without grounding context, there are fewer constraints on what a model may generate. </span><span class="koboSpan" id="kobo.130.3">Hallucinations can range from subtle mistakes to completely fabricated outputs presented with </span><span class="No-Break"><span class="koboSpan" id="kobo.131.1">high confidence.</span></span></li>
</ul>
<h3><span class="koboSpan" id="kobo.132.1">Prompt examples and templates for QA without context</span></h3>
<p><span class="koboSpan" id="kobo.133.1">When an LLM is </span><a id="_idIndexMarker566"/><span class="koboSpan" id="kobo.134.1">asked a</span><a id="_idIndexMarker567"/><span class="koboSpan" id="kobo.135.1"> question without any additional context, it can be difficult for the LLM to understand the question and generate an accurate answer. </span><span class="koboSpan" id="kobo.135.2">It can be like providing them with a puzzle with missing pieces. </span><span class="koboSpan" id="kobo.135.3">Prompt engineering helps us provide the missing pieces, making it easier for LLMs to understand our questions and deliver </span><span class="No-Break"><span class="koboSpan" id="kobo.136.1">accurate answers.</span></span></p>
<p><span class="koboSpan" id="kobo.137.1">Thus, careful prompt engineering is required to steer generation in the right direction and encourage well-calibrated, truthful responses. </span><span class="koboSpan" id="kobo.137.2">There are three main techniques for prompt engineering in QA </span><span class="No-Break"><span class="koboSpan" id="kobo.138.1">without context:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.139.1">Query reformulation</span></strong><span class="koboSpan" id="kobo.140.1">: This involves rephrasing the initial question to better match the model’s knowledge. </span><span class="koboSpan" id="kobo.140.2">For example, instead of asking </span><strong class="source-inline"><span class="koboSpan" id="kobo.141.1">What is the capital of France?</span></strong><span class="koboSpan" id="kobo.142.1">, you could ask </span><strong class="source-inline"><span class="koboSpan" id="kobo.143.1">What city is the capital of France?</span></strong><span class="koboSpan" id="kobo.144.1">. </span><span class="koboSpan" id="kobo.144.2">Let’s take another example. </span><span class="koboSpan" id="kobo.144.3">Instead of asking </span><strong class="source-inline"><span class="koboSpan" id="kobo.145.1">What caused the extinction of dinosaurs?</span></strong><span class="koboSpan" id="kobo.146.1"> (a broad question), the reformulated prompt would look like </span><strong class="source-inline"><span class="koboSpan" id="kobo.147.1">What is the most widely accepted theory for the extinction of dinosaurs?</span></strong><span class="koboSpan" id="kobo.148.1"> (which focuses on a </span><span class="No-Break"><span class="koboSpan" id="kobo.149.1">specific aspect).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.150.1">Model priming</span></strong><span class="koboSpan" id="kobo.151.1">: This involves providing instructions in the prompt to better communicate the desired output format. </span><span class="koboSpan" id="kobo.151.2">For example, you could prompt the LLM to provide a short answer, a list of possible answers, or a paragraph of text. </span><span class="koboSpan" id="kobo.151.3">For example, instead of asking </span><strong class="source-inline"><span class="koboSpan" id="kobo.152.1">What are the Great Lakes?</span></strong><span class="koboSpan" id="kobo.153.1">, you could ask </span><strong class="source-inline"><span class="koboSpan" id="kobo.154.1">Provide a list of the five Great Lakes of North America</span></strong><span class="koboSpan" id="kobo.155.1"> (which specifies a desired </span><span class="No-Break"><span class="koboSpan" id="kobo.156.1">answer format).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.157.1">Attribution calibration</span></strong><span class="koboSpan" id="kobo.158.1">: This involves encouraging models to estimate and communicate their confidence level in their answers. </span><span class="koboSpan" id="kobo.158.2">This can help users to better understand the reliability of the information that the LLM provides. </span><span class="koboSpan" id="kobo.158.3">Say that you simply ask </span><strong class="source-inline"><span class="koboSpan" id="kobo.159.1">Who wrote Hamlet?</span></strong><span class="koboSpan" id="kobo.160.1"> This seems like a straightforward question, but the LLM might be unsure whether it’s referring to the authorship of the original play or a modern adaptation. </span><span class="koboSpan" id="kobo.160.2">Instead, you could ask the model with attribution calibration in a certain manner, such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.161.1">Can you tell me definitively who wrote the original play Hamlet? </span><span class="koboSpan" id="kobo.161.2">Based on my understanding of literature, I am very likely (or less certainly) correct in my answer</span></strong><span class="koboSpan" id="kobo.162.1">. </span><span class="koboSpan" id="kobo.162.2">This version of the prompt offers a range of confidence levels (</span><strong class="source-inline"><span class="koboSpan" id="kobo.163.1">very likely</span></strong><span class="koboSpan" id="kobo.164.1"> or </span><strong class="source-inline"><span class="koboSpan" id="kobo.165.1">less certain</span></strong><span class="koboSpan" id="kobo.166.1">) instead of just </span><strong class="source-inline"><span class="koboSpan" id="kobo.167.1">confident</span></strong><span class="koboSpan" id="kobo.168.1"> or </span><strong class="source-inline"><span class="koboSpan" id="kobo.169.1">unsure</span></strong><span class="koboSpan" id="kobo.170.1">. </span><span class="koboSpan" id="kobo.170.2">This allows the LLM to express a more nuanced level of certainty based on the information it </span><span class="No-Break"><span class="koboSpan" id="kobo.171.1">has processed.</span></span></li>
<li><span class="koboSpan" id="kobo.172.1">In addition to the preceding techniques, you should leverage system prompts in order to shape the interpretation and response of LLMs when queried by the end users. </span><span class="koboSpan" id="kobo.172.2">Think of system prompts as carefully crafted instructions that are meant to guide the model’s behavior, directing it toward the </span><span class="No-Break"><span class="koboSpan" id="kobo.173.1">desired outcome.</span></span></li>
<li><span class="koboSpan" id="kobo.174.1">For instance, when crafting prompts for role-playing scenarios, system prompts can define the personality traits, communication style, and domain knowledge the AI should exhibit. </span><span class="koboSpan" id="kobo.174.2">Imagine you’re creating a virtual assistant. </span><span class="koboSpan" id="kobo.174.3">Through system prompts, you can specify a helpful, informative persona, ensuring that the FM uses language and knowledge appropriate for </span><span class="No-Break"><span class="koboSpan" id="kobo.175.1">the role.</span></span></li>
<li><span class="koboSpan" id="kobo.176.1">Additionally, system prompts can help maintain consistency in the model’s responses, especially during prolonged interactions. </span><span class="koboSpan" id="kobo.176.2">By outlining the persona and desired tone throughout the prompts, you ensure that the model stays true to its </span><a id="_idIndexMarker568"/><span class="koboSpan" id="kobo.177.1">character, fostering </span><a id="_idIndexMarker569"/><span class="koboSpan" id="kobo.178.1">trust and a more natural </span><span class="No-Break"><span class="koboSpan" id="kobo.179.1">user experience.</span></span></li>
<li><span class="koboSpan" id="kobo.180.1">For an example of system prompts with the Anthropic Claude model, we encourage you to peruse through </span><a href="https://promptengineering.org/claudes-system-prompt-a-prompt-engineering-case-study/"><span class="koboSpan" id="kobo.181.1">https://promptengineering.org/claudes-system-prompt-a-prompt-engineering-case-study/</span></a><span class="koboSpan" id="kobo.182.1">. </span><span class="koboSpan" id="kobo.182.2">You should always keep in mind that the best prompts will depend on the specific question and the capabilities of the LLM you use. </span><span class="koboSpan" id="kobo.182.3">Experiment with different phrasing and templates to find what works best for </span><span class="No-Break"><span class="koboSpan" id="kobo.183.1">your needs.</span></span></li>
<li><span class="koboSpan" id="kobo.184.1">By using prompt engineering, it is always possible to improve the accuracy and reliability of LLMs on QA tasks </span><span class="No-Break"><span class="koboSpan" id="kobo.185.1">without context.</span></span></li>
</ul>
<h4><span class="koboSpan" id="kobo.186.1">Simple question prompts</span></h4>
<p><span class="koboSpan" id="kobo.187.1">One of the most straightforward ways to prompt a generative model is to pose a direct question, formatted within triple quotes in the case of a multiline comprehensive prompt within the code. </span><span class="koboSpan" id="kobo.187.2">Let’s experiment with an example in the Amazon Bedrock </span><span class="No-Break"><span class="koboSpan" id="kobo.188.1">chat playground.</span></span></p>
<p><span class="koboSpan" id="kobo.189.1">In order to execute run simple QA prompts in Amazon Bedrock playground, let’s head back to the AWS console and navigate to the Amazon Bedrock landing page. </span><span class="koboSpan" id="kobo.189.2">Once you reach the landing page, scroll through the left pane and click on the </span><strong class="bold"><span class="koboSpan" id="kobo.190.1">Chat</span></strong><span class="koboSpan" id="kobo.191.1"> option </span><span class="No-Break"><span class="koboSpan" id="kobo.192.1">under </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.193.1">Playgrounds</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.194.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.195.1">Select a particular model in the chat playground by navigating to </span><strong class="bold"><span class="koboSpan" id="kobo.196.1">Select Model</span></strong><span class="koboSpan" id="kobo.197.1">. </span><span class="koboSpan" id="kobo.197.2">In our example, let’s select</span><a id="_idIndexMarker570"/><span class="koboSpan" id="kobo.198.1"> the </span><strong class="bold"><span class="koboSpan" id="kobo.199.1">Jurassic-2 Ultra</span></strong><span class="koboSpan" id="kobo.200.1"> FM and initiate the conversation with the following example in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.201.1">Figure 7</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.202.1">.1</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.203.1">.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer108">
<span class="koboSpan" id="kobo.204.1"><img alt="Figure 7.1 – A simple prompt with Amazon Bedrock models in the chat playground" src="image/B22045_07_01.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.205.1">Figure 7.1 – A simple prompt with Amazon Bedrock models in the chat playground</span></p>
<p><span class="koboSpan" id="kobo.206.1">As depicted in the preceding example, a simple prompt such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.207.1">What is Rabindranath Tagore's famous poem "Geetanjali" about?</span></strong><span class="koboSpan" id="kobo.208.1"> was used without any context provided to the model. </span><span class="koboSpan" id="kobo.208.2">In order to further the chat with the model, a follow-up question was also asked, </span><strong class="source-inline"><span class="koboSpan" id="kobo.209.1">What else are some of his famous poems?</span></strong><span class="koboSpan" id="kobo.210.1">, to which the model provided a decent response. </span><span class="koboSpan" id="kobo.210.2">(You can run this sample prompt in your Bedrock playground with other models and continue the conversation chain to witness any differences in </span><span class="No-Break"><span class="koboSpan" id="kobo.211.1">the responses.)</span></span></p>
<p><span class="koboSpan" id="kobo.212.1">You can also leverage </span><strong class="bold"><span class="koboSpan" id="kobo.213.1">Compare mode</span></strong><span class="koboSpan" id="kobo.214.1"> in </span><strong class="bold"><span class="koboSpan" id="kobo.215.1">Chat Playground</span></strong><span class="koboSpan" id="kobo.216.1"> by toggling the slider at the right side of the </span><strong class="bold"><span class="koboSpan" id="kobo.217.1">Chat Playground</span></strong><span class="koboSpan" id="kobo.218.1"> window, as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.219.1">Figure 7</span></em></span><em class="italic"><span class="koboSpan" id="kobo.220.1">.2</span></em><span class="koboSpan" id="kobo.221.1">, and execute a similar prompt against multiple FMs available on Amazon Bedrock. </span><span class="koboSpan" id="kobo.221.2">As visible in the following figure, three models are compared on a particular question. </span><span class="koboSpan" id="kobo.221.3">Note the third model was added by </span><a id="_idIndexMarker571"/><span class="koboSpan" id="kobo.222.1">clicking on the </span><strong class="bold"><span class="koboSpan" id="kobo.223.1">+</span></strong><span class="koboSpan" id="kobo.224.1"> option on the </span><span class="No-Break"><span class="koboSpan" id="kobo.225.1">right side.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer109">
<span class="koboSpan" id="kobo.226.1"><img alt="Figure 7.2 – Simple QA prompting with Compare Mode in Amazon Bedrock" src="image/B22045_07_02.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.227.1">Figure 7.2 – Simple QA prompting with Compare Mode in Amazon Bedrock</span></p>
<p><span class="koboSpan" id="kobo.228.1">Similarly, by using Amazon Bedrock APIs, the models can be prompted in a </span><span class="No-Break"><span class="koboSpan" id="kobo.229.1">QA context:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.230.1">
prompt = """You are an expert AI assistant. </span><span class="koboSpan" id="kobo.230.2">You will answer questions in a succinct manner. </span><span class="koboSpan" id="kobo.230.3">If you are unsure about the
answer, say 'I am not sure about this answer'
Question: How can I connect my old Samsung TV with my Mac laptop?
</span><span class="koboSpan" id="kobo.230.4">Answer:"""
parameters = {
    "maxTokenCount":1024,
    "temperature":0.1,
    "topP":0.8,
    "stopSequences":[]
    }</span></pre>
<p><span class="koboSpan" id="kobo.231.1">Using the preceding prompt, the FM available in Amazon Bedrock can be invoked; the model can then provide a particular response. </span><span class="koboSpan" id="kobo.231.2">You are encouraged to run this prompt with the Amazon </span><a id="_idIndexMarker572"/><span class="koboSpan" id="kobo.232.1">Titan model and capture the response as </span><span class="No-Break"><span class="koboSpan" id="kobo.233.1">an exercise.</span></span></p>
<h4><span class="koboSpan" id="kobo.234.1">Model encouragement and constraints</span></h4>
<p><span class="koboSpan" id="kobo.235.1">Optionally, you can encourage the model by framing the prompt in a motivational way. </span><span class="koboSpan" id="kobo.235.2">By combining model encouragement and constraints, you can create more effective prompts that guide the LLMs to generate </span><span class="No-Break"><span class="koboSpan" id="kobo.236.1">high-quality responses.</span></span></p>
<p><span class="koboSpan" id="kobo.237.1">Here are </span><span class="No-Break"><span class="koboSpan" id="kobo.238.1">some examples:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.239.1">Providing context and specific keywords can encourage the model to generate more </span><span class="No-Break"><span class="koboSpan" id="kobo.240.1">accurate responses</span></span></li>
<li><span class="koboSpan" id="kobo.241.1">Setting length and format constraints can help the model generate responses that are concise </span><span class="No-Break"><span class="koboSpan" id="kobo.242.1">and structured</span></span></li>
<li><span class="koboSpan" id="kobo.243.1">Restricting the model to a specific domain or topic can help it generate responses that are more accurate </span><span class="No-Break"><span class="koboSpan" id="kobo.244.1">and relevant</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.245.1">A prompt example can be formatted in the </span><span class="No-Break"><span class="koboSpan" id="kobo.246.1">following order:</span></span></p>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.247.1">You are an expert in explaining complex scientific concepts in a clear and engaging manner. </span><span class="koboSpan" id="kobo.247.2">Your ability to break down intricate topics into understandable terms makes you an invaluable resource for </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.248.1">educational purposes.</span></strong></span></p>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.249.1">Constraints: Assume your audience consists of college students or professionals with a basic understanding of computer science and physics. </span><span class="koboSpan" id="kobo.249.2">Your explanation should be accessible yet informative, covering both theoretical and practical aspects of </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.250.1">quantum computing.</span></strong></span></p>
<p><span class="koboSpan" id="kobo.251.1">This is followed by </span><span class="No-Break"><span class="koboSpan" id="kobo.252.1">the question:</span></span></p>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.253.1">Could you please provide a comprehensive overview of quantum computing, including its principles, potential applications, and the challenges </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.254.1">it faces?</span></strong></span></p>
<p><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.255.1">Figure 7</span></em></span><em class="italic"><span class="koboSpan" id="kobo.256.1">.3</span></em><span class="koboSpan" id="kobo.257.1"> illustrates the sample usage of model encouragement, along with constraints to invoke </span><a id="_idIndexMarker573"/><span class="koboSpan" id="kobo.258.1">the Meta Llama model on Amazon Bedrock’s </span><span class="No-Break"><span class="koboSpan" id="kobo.259.1">chat playground.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer110">
<span class="koboSpan" id="kobo.260.1"><img alt="Figure 7.3 – A simple prompt example of using model encouragement and constraints on the Meta Llama 3 model in Amazon Bedrock’s chat playground" src="image/B22045_07_03.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.261.1">Figure 7.3 – A simple prompt example of using model encouragement and constraints on the Meta Llama 3 model in Amazon Bedrock’s chat playground</span></p>
<p><span class="koboSpan" id="kobo.262.1">You are encouraged to execute the prompt at your end and note the difference in responses with/without the constraints and model encouragement. </span><span class="koboSpan" id="kobo.262.2">You will notice that this type of prompt can help prime the model to provide a thoughtful, </span><span class="No-Break"><span class="koboSpan" id="kobo.263.1">thorough response.</span></span></p>
<p><span class="koboSpan" id="kobo.264.1">Here is another example for you to execute, either in Amazon Bedrock’s chat playground or by using Amazon Bedrock APIs to invoke </span><span class="No-Break"><span class="koboSpan" id="kobo.265.1">the model:</span></span></p>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.266.1">You have an excellent grasp of complex machine learning concepts and can explain them in a clear and </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.267.1">understandable way.</span></strong></span></p>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.268.1">Could you please explain the concept of gradient descent in </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.269.1">machine learning?</span></strong></span></p>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.270.1">Please keep your explanation concise and suitable for readers with a basic understanding of </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.271.1">machine learning.</span></strong></span></p>
<p><span class="koboSpan" id="kobo.272.1">Let’s say you invoke </span><a id="_idIndexMarker574"/><span class="koboSpan" id="kobo.273.1">an FM with a hypothetical question without any relevant context. </span><span class="koboSpan" id="kobo.273.2">In some cases, it may end up hallucinating. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.274.1">Figure 7</span></em></span><em class="italic"><span class="koboSpan" id="kobo.275.1">.4</span></em><span class="koboSpan" id="kobo.276.1"> illustrates a fascinating scenario where the model ends up hallucinating when queried about an imaginary BMX Subaru bike, which doesn’t really exist in </span><span class="No-Break"><span class="koboSpan" id="kobo.277.1">real life!</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer111">
<span class="koboSpan" id="kobo.278.1"><img alt="Figure 7.4 – A QA prompt sample without context in Amazon Bedrock’s chat playground" src="image/B22045_07_04.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.279.1">Figure 7.4 – A QA prompt sample without context in Amazon Bedrock’s chat playground</span></p>
<p><span class="koboSpan" id="kobo.280.1">If proper prompt instructions are provided with context, the model will strive to find the relevant content within the context and then provide a reasonable </span><span class="No-Break"><span class="koboSpan" id="kobo.281.1">desirable response.</span></span></p>
<p><span class="koboSpan" id="kobo.282.1">Keep in mind that while QA without context is extremely challenging, strategies such as constitutional AI and iterative refinement techniques that leverage and re-combine the model’s internal</span><a id="_idIndexMarker575"/><span class="koboSpan" id="kobo.283.1"> knowledge in novel ways can help improve performance on </span><span class="No-Break"><span class="koboSpan" id="kobo.284.1">open-domain QA.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.285.1">Note</span></p>
<p class="callout"><strong class="bold"><span class="koboSpan" id="kobo.286.1">Constitutional AI</span></strong><span class="koboSpan" id="kobo.287.1"> is an</span><a id="_idIndexMarker576"/><span class="koboSpan" id="kobo.288.1"> area of AI research concerned with developing AI systems that adhere to ethical principles and legal frameworks. </span><span class="koboSpan" id="kobo.288.2">It can involve designing AI systems that are fair, transparent, and accountable and respect human rights </span><span class="No-Break"><span class="koboSpan" id="kobo.289.1">and privacy.</span></span></p>
<h3><span class="koboSpan" id="kobo.290.1">QA with context</span></h3>
<p><strong class="bold"><span class="koboSpan" id="kobo.291.1">QA with context</span></strong><span class="koboSpan" id="kobo.292.1"> involves </span><a id="_idIndexMarker577"/><span class="koboSpan" id="kobo.293.1">providing an input text and </span><a id="_idIndexMarker578"/><span class="koboSpan" id="kobo.294.1">a question, and the language model must generate an answer based solely on the information contained within the given text. </span><span class="koboSpan" id="kobo.294.2">This task requires the model to comprehend the context, identify relevant details, and synthesize a coherent response that directly addresses the query while avoiding introducing </span><span class="No-Break"><span class="koboSpan" id="kobo.295.1">external information.</span></span></p>
<p><span class="koboSpan" id="kobo.296.1">For this use case, it is beneficial to structure the prompt by presenting the input text first, followed by the question. </span><span class="koboSpan" id="kobo.296.2">This ordering allows the model to fully process the context before attempting to formulate an answer, potentially improving response quality and accuracy. </span><span class="koboSpan" id="kobo.296.3">As indicated in the previous section, incorporating techniques such as model encouragement can further enhance performance on </span><span class="No-Break"><span class="koboSpan" id="kobo.297.1">QA tasks.</span></span></p>
<p><span class="koboSpan" id="kobo.298.1">The ideal prompt will have the </span><span class="No-Break"><span class="koboSpan" id="kobo.299.1">following structure:</span></span></p>
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.300.1">input_text: {{text}}</span></strong></span></p>
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.301.1">question: {{question}}</span></strong></span></p>
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.302.1">answer:</span></strong></span></p>
<p><span class="koboSpan" id="kobo.303.1">Now, let’s see what the content of the prompt would </span><span class="No-Break"><span class="koboSpan" id="kobo.304.1">be like:</span></span></p>
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.305.1">Input_text</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.306.1">:</span></span></p>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.307.1">"""The Arctic fox (Vulpes lagopus) is a small fox native to the Arctic regions of the Northern Hemisphere and common throughout the Arctic tundra biome. </span><span class="koboSpan" id="kobo.307.2">It is well adapted to living in cold environments, with dense fur insulation and a compact body shape that limits exposure to the cold. </span><span class="koboSpan" id="kobo.307.3">Adults weigh 3–5 kg (6.6–11 lb) and have a body length of 46–68 cm (18–27 in). </span><span class="koboSpan" id="kobo.307.4">Their thick fur is an insulating blanket that keeps them warm even in the depths of an Arctic winter. </span><span class="koboSpan" id="kobo.307.5">The Arctic fox has a deep, thick underfur that insulates it from the cold and a dense, insulating guard hair coat </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.308.1">on top."""</span></strong></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.309.1">Question</span></strong><span class="koboSpan" id="kobo.310.1">: </span><strong class="source-inline"><span class="koboSpan" id="kobo.311.1">What are some key adaptations that allow the Arctic fox to survive in cold </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.312.1">Arctic environments?</span></strong></span></p>
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.313.1">Example output</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.314.1">:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.315.1">
"The Arctic fox has several key adaptations that allow it to survive in the cold Arctic environments:
1. </span><span class="koboSpan" id="kobo.315.2">Dense fur insulation to keep it warm.
</span><span class="koboSpan" id="kobo.315.3">2. </span><span class="koboSpan" id="kobo.315.4">A compact body shape that limits exposure to the cold.
</span><span class="koboSpan" id="kobo.315.5">3. </span><span class="koboSpan" id="kobo.315.6">A thick underfur that insulates it from the cold.
</span><span class="koboSpan" id="kobo.315.7">4. </span><span class="koboSpan" id="kobo.315.8">A dense, insulating guard hair coat on top.
</span><span class="koboSpan" id="kobo.315.9">These physical adaptations, such as thick fur coats and compact body shape, enable the Arctic fox to withstand the extreme cold temperatures of the Arctic tundra region."</span></pre>
<p><span class="koboSpan" id="kobo.316.1">The preceding example prompt showcases a scenario where an answer about the Arctic fox’s physical adaptations to cold environments is provided, and the question prompts the model to identify and summarize the relevant details from </span><span class="No-Break"><span class="koboSpan" id="kobo.317.1">the text.</span></span></p>
<p><span class="koboSpan" id="kobo.318.1">Next, let’s walk </span><a id="_idIndexMarker579"/><span class="koboSpan" id="kobo.319.1">through an example</span><a id="_idIndexMarker580"/><span class="koboSpan" id="kobo.320.1"> prompt of QA with context using Amazon </span><span class="No-Break"><span class="koboSpan" id="kobo.321.1">Bedrock APIs:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.322.1">
# Import the respective libraries
import boto3
import botocore
import os
import json
import sys
#create bedrock runtime client
bedrock_runtime = boto3.client('bedrock-runtime')
#Provide the model paramters
model_parameters = {
     "maxTokenCount":1024,
     "temperature":0,
     "stopSequences":[],
     "topP":0.9
     }
#Provide relevant context to the model
context= """Using your Apple Watch to locate a misplaced iPhone is a handy feature that can save you a lot of time and frustration. </span><span class="koboSpan" id="kobo.322.2">The process typically begins by opening the Control Center on your watch by swiping up from the</span><strong class="source-inline"> </strong><span class="koboSpan" id="kobo.323.1">bottom of the watch face. </span><span class="koboSpan" id="kobo.323.2">From there, you'll see an icon that looks like a ringing iPhone - tapping this will remotely activate a loud pinging sound on your iPhone, even if it's on silent mode. </span><span class="koboSpan" id="kobo.323.3">If you're within earshot, simply follow the sound to track down your missing device. </span><span class="koboSpan" id="kobo.323.4">Alternatively, you can use the FindMy app on your Apple Watch, which provides a map showing the last known location of your iPhone. </span><span class="koboSpan" id="kobo.323.5">Tap the "Devices" tab, select your iPhone, and it will display its location, as well as give you the option to force it to emit a sound to aid in your search. </span><span class="koboSpan" id="kobo.323.6">For an even quicker option, you can simply raise your wrist and ask Siri "Hey Siri, find my iPhone," and the virtual assistant will attempt to pinpoint the location of your iPhone and provide directions. </span><span class="koboSpan" id="kobo.323.7">However, for any of these methods to work, your iPhone must be powered on, connected to a cellular or WiFi network, and have the Find My feature enabled in Settings under your Apple ID. </span><span class="koboSpan" id="kobo.323.8">As long as those criteria are met, your Apple Watch can be a powerful tool for tracking down a wandering iPhone."""
</span><span class="koboSpan" id="kobo.323.9">#Take the entire context/excerpt provided above and augment to the model along with the input question
question = "How can I find my iPhone from my Apple watch in case I lose my phone?"
</span><span class="koboSpan" id="kobo.323.10">prompt_data = f""" Answer the user's question solely only on the information provided between &lt;&gt;&lt;/&gt; XML tags. </span><span class="koboSpan" id="kobo.323.11">Think step by step and provide detailed instructions.
</span><span class="koboSpan" id="kobo.323.12">&lt;context&gt;
{context}
&lt;/context&gt;
Question: {question}
Answer:"""
#Now, you can Invoke the foundation model using boto3 to generate the output response.
</span><span class="koboSpan" id="kobo.323.13">body = json.dumps({"inputText": prompt_data, "textGenerationConfig": model_parameters})
accept = "application/json"
contentType = "application/json"
# You can change this modelID to use an alternate version from the model provider
modelId = "amazon.titan-tg1-large"
response = bedrock_runtime.invoke_model(
    body=body, modelId=modelId, accept=accept, contentType=contentType)
generated_response_body = json.loads(response.get("body").read())
print(generated_response_body.get("results")[0].get("outputText").strip())</span></pre>
<p><span class="koboSpan" id="kobo.324.1">Run the preceding code, and try to invoke the Amazon Bedrock FM on your own to test the results. </span><span class="koboSpan" id="kobo.324.2">The generated output may look akin to </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.325.1">Figure 7</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.326.1">.5</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.327.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer112">
<span class="koboSpan" id="kobo.328.1"><img alt="Figure 7.5 – Example output generated from an Amazon Bedrock FM" src="image/B22045_07_05.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.329.1">Figure 7.5 – Example output generated from an Amazon Bedrock FM</span></p>
<p><span class="koboSpan" id="kobo.330.1">After executing the code to invoke the model, you will observe that the model can generate an appropriate response in most cases by leveraging the information provided </span><span class="No-Break"><span class="koboSpan" id="kobo.331.1">as context.</span></span></p>
<p><span class="koboSpan" id="kobo.332.1">Now that we have</span><a id="_idIndexMarker581"/><span class="koboSpan" id="kobo.333.1"> covered prompt engineering</span><a id="_idIndexMarker582"/><span class="koboSpan" id="kobo.334.1"> with QA use cases on Bedrock, let’s walk through document ingestion frameworks with </span><span class="No-Break"><span class="koboSpan" id="kobo.335.1">Amazon Bedrock.</span></span></p>
<h1 id="_idParaDest-131"><a id="_idTextAnchor141"/><span class="koboSpan" id="kobo.336.1">Document ingestion with Amazon Bedrock</span></h1>
<p><span class="koboSpan" id="kobo.337.1">The architectural pattern for QA systems with context can be broadly divided into two categories – </span><em class="italic"><span class="koboSpan" id="kobo.338.1">QA on small documents</span></em><span class="koboSpan" id="kobo.339.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.340.1">QA on large documents on knowledge bases</span></em><span class="koboSpan" id="kobo.341.1">. </span><span class="koboSpan" id="kobo.341.2">While the core components remain similar, the approach and techniques employed may vary, depending on the size and complexity of the </span><span class="No-Break"><span class="koboSpan" id="kobo.342.1">input data.</span></span></p>
<h2 id="_idParaDest-132"><a id="_idTextAnchor142"/><span class="koboSpan" id="kobo.343.1">QA on small documents</span></h2>
<p><span class="koboSpan" id="kobo.344.1">For QA systems designed to</span><a id="_idIndexMarker583"/><span class="koboSpan" id="kobo.345.1"> handle small documents, such as paragraphs or short articles, the architectural pattern typically follows a pipeline approach consisting of the </span><span class="No-Break"><span class="koboSpan" id="kobo.346.1">following stages:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.347.1">Query processing</span></strong><span class="koboSpan" id="kobo.348.1">: The natural language query is preprocessed by converting it to a </span><span class="No-Break"><span class="koboSpan" id="kobo.349.1">vector representation.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.350.1">Document retrieval</span></strong><span class="koboSpan" id="kobo.351.1">: Relevant documents or passages are retrieved from the corpus based on the query keywords or semantic similarity measures. </span><span class="koboSpan" id="kobo.351.2">For smaller documents, retrieval can be straightforward; you can directly embed and index the entire document or passage within your vector store. </span><span class="koboSpan" id="kobo.351.3">In another scenario, since the input documents are smaller in nature, there might not be a need to split them into smaller chunks as long as they can fit within the token size limit of the model. </span><span class="koboSpan" id="kobo.351.4">Once inspected, the document can be directly parsed in context within the model </span><span class="No-Break"><span class="koboSpan" id="kobo.352.1">prompt template.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.353.1">Passage ranking</span></strong><span class="koboSpan" id="kobo.354.1">: Once retrieved, the passages are ranked by their relevance to the query. </span><span class="koboSpan" id="kobo.354.2">This ranking can be done using techniques such as </span><strong class="bold"><span class="koboSpan" id="kobo.355.1">term frequency-inverse document frequency</span></strong><span class="koboSpan" id="kobo.356.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.357.1">TF-IDF</span></strong><span class="koboSpan" id="kobo.358.1">) semantic similarity, or specialized </span><a id="_idIndexMarker584"/><span class="koboSpan" id="kobo.359.1">neural ranking models. </span><span class="koboSpan" id="kobo.359.2">Automation of passage ranking can be made possible using an orchestrator or type or vector database. </span><span class="koboSpan" id="kobo.359.3">For instance, Amazon Kendra has a SOTA semantic searching mechanism built in to perform </span><span class="No-Break"><span class="koboSpan" id="kobo.360.1">relevance ranking.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.361.1">Answer extraction</span></strong><span class="koboSpan" id="kobo.362.1">: The top-ranked passages are analyzed to identify the most relevant spans or phrases that potentially answer the query. </span><span class="koboSpan" id="kobo.362.2">This stage often involves techniques such as named entity recognition, coreference resolution, and QA models. </span><span class="koboSpan" id="kobo.362.3">Hence, in the case of generative AI frameworks, relevant context extraction can be performed by these LLMs without the need to explicitly invoke </span><span class="No-Break"><span class="koboSpan" id="kobo.363.1">complicated techniques.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.364.1">Answer scoring and ranking</span></strong><span class="koboSpan" id="kobo.365.1">: The extracted answer candidates are scored and ranked based on their confidence or relevance to the query, using techniques such as answer verification models or scoring functions. </span><span class="koboSpan" id="kobo.365.2">There are some re-ranking models, such as Cohere Rerank, that can also be leveraged to improve </span><span class="No-Break"><span class="koboSpan" id="kobo.366.1">recall performance.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.367.1">Answer generation</span></strong><span class="koboSpan" id="kobo.368.1">: The top-ranked answer is generated, potentially involving post-processing steps such as formatting, rephrasing, or generating natural </span><span class="No-Break"><span class="koboSpan" id="kobo.369.1">language responses.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.370.1">This pipeline approach is well-suited for QA on small documents, as it allows for efficient retrieval and ranking of relevant passages, followed by targeted answer extraction and scoring, without the need to split the document into chunks or process it in a </span><span class="No-Break"><span class="koboSpan" id="kobo.371.1">different way.</span></span></p>
<p><span class="koboSpan" id="kobo.372.1">Let’s walk through an example of small document ingestion with </span><span class="No-Break"><span class="koboSpan" id="kobo.373.1">Amazon Bedrock.</span></span></p>
<p><span class="koboSpan" id="kobo.374.1">For small document ingestion with Amazon Bedrock and LangChain, you can use the </span><strong class="bold"><span class="koboSpan" id="kobo.375.1">TextLoader</span></strong><span class="koboSpan" id="kobo.376.1"> or </span><strong class="bold"><span class="koboSpan" id="kobo.377.1">PDFLoader</span></strong><span class="koboSpan" id="kobo.378.1"> to </span><a id="_idIndexMarker585"/><span class="koboSpan" id="kobo.379.1">load the </span><a id="_idIndexMarker586"/><span class="koboSpan" id="kobo.380.1">documents directly into memory. </span><span class="koboSpan" id="kobo.380.2">In LangChain, </span><strong class="source-inline"><span class="koboSpan" id="kobo.381.1">TextLoader</span></strong><span class="koboSpan" id="kobo.382.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.383.1">PDFLoader</span></strong><span class="koboSpan" id="kobo.384.1"> are actually </span><em class="italic"><span class="koboSpan" id="kobo.385.1">Python</span></em><span class="koboSpan" id="kobo.386.1"> classes, not software components. </span><span class="koboSpan" id="kobo.386.2">Here’s a </span><span class="No-Break"><span class="koboSpan" id="kobo.387.1">brief explanation:</span></span></p>
<ul>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.388.1">TextLoader</span></strong><span class="koboSpan" id="kobo.389.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.390.1">PDFLoader</span></strong><span class="koboSpan" id="kobo.391.1"> are used to load and parse text and PDF </span><span class="No-Break"><span class="koboSpan" id="kobo.392.1">documents, respectively.</span></span></li>
<li><span class="koboSpan" id="kobo.393.1">These classes are part of LangChain’s document loader functionality, which helps in preparing documents for further processing in </span><span class="No-Break"><span class="koboSpan" id="kobo.394.1">AI applications.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.395.1">Here’s an example</span><a id="_idIndexMarker587"/> <span class="No-Break"><span class="koboSpan" id="kobo.396.1">with TextLoader.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.397.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.398.1">As shown in the previous chapters, ensure that the necessary libraries for LangChain are installed along with Chroma DB. </span><span class="koboSpan" id="kobo.398.2">We are using Chroma DB only for example purposes. </span><span class="koboSpan" id="kobo.398.3">You can leverage other vector databases, such as Chroma, Weaviate, Pinecone, and Faiss, based on their use case. </span><span class="koboSpan" id="kobo.398.4">If Chroma DB is not installed, execute </span><strong class="source-inline"><span class="koboSpan" id="kobo.399.1">!pip install chromadb</span></strong><span class="koboSpan" id="kobo.400.1"> before running the </span><span class="No-Break"><span class="koboSpan" id="kobo.401.1">following code.</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.402.1">
from langchain.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma
# Load the document
loader = TextLoader('path/to/document.txt')
documents = loader.load()
# Split the documents into chunks
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
texts = text_splitter.split_documents(documents)
# Create embeddings and store in Chroma vector store
from langchain_community.embeddings import BedrockEmbeddings
embeddings = BedrockEmbeddings()
db = Chroma.from_documents(texts, embeddings)</span></pre>
<h2 id="_idParaDest-133"><a id="_idTextAnchor143"/><span class="koboSpan" id="kobo.403.1">QA for large documents on knowledge bases</span></h2>
<p><span class="koboSpan" id="kobo.404.1">When dealing </span><a id="_idIndexMarker588"/><span class="koboSpan" id="kobo.405.1">with large documents on knowledge bases, the architectural pattern may need to be adapted to handle the scale and complexity of the data. </span><span class="koboSpan" id="kobo.405.2">A common approach is to incorporate techniques from information retrieval and open-domain QA systems. </span><span class="koboSpan" id="kobo.405.3">The following steps highlight the process of ingesting large documents, creating a vector index, and creating an end-to-end </span><span class="No-Break"><span class="koboSpan" id="kobo.406.1">QA pipeline:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.407.1">Knowledge base construction</span></strong><span class="koboSpan" id="kobo.408.1">: The large corpus or knowledge base is preprocessed, indexed, and structured in a way that facilitates efficient retrieval </span><span class="No-Break"><span class="koboSpan" id="kobo.409.1">and querying.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.410.1">Query processing</span></strong><span class="koboSpan" id="kobo.411.1">: Similar to the small document case, the natural language query is preprocessed by converting it to a </span><span class="No-Break"><span class="koboSpan" id="kobo.412.1">vector representation.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.413.1">Document or </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.414.1">passage retrieval</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.415.1">:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.416.1">Chunking</span></strong><span class="koboSpan" id="kobo.417.1">: For larger documents, directly embedding the entire document might not be ideal. </span><span class="koboSpan" id="kobo.417.2">You should consider chunking the document into smaller, more manageable segments such as paragraphs </span><span class="No-Break"><span class="koboSpan" id="kobo.418.1">or sentences.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.419.1">Small-to-big retrieval</span></strong><span class="koboSpan" id="kobo.420.1">: In this case, the following process </span><span class="No-Break"><span class="koboSpan" id="kobo.421.1">is followed:</span></span><ol><li class="lower-roman"><span class="koboSpan" id="kobo.422.1">Embed and search using smaller chunks </span><span class="No-Break"><span class="koboSpan" id="kobo.423.1">during retrieval.</span></span></li><li class="lower-roman"><span class="koboSpan" id="kobo.424.1">Identify relevant chunks based on their </span><span class="No-Break"><span class="koboSpan" id="kobo.425.1">retrieved scores.</span></span></li><li class="lower-roman"><span class="koboSpan" id="kobo.426.1">Use the retrieved chunk IDs to access and provide the corresponding larger document segment to the LLM for answer generation. </span><span class="koboSpan" id="kobo.426.2">This way, the LLM has access to the broader context, while retrieval leverages smaller, more </span><span class="No-Break"><span class="koboSpan" id="kobo.427.1">focused units.</span></span></li></ol></li><li><strong class="bold"><span class="koboSpan" id="kobo.428.1">Efficiency</span></strong><span class="koboSpan" id="kobo.429.1">: Chunking and </span><em class="italic"><span class="koboSpan" id="kobo.430.1">small-to-big</span></em><span class="koboSpan" id="kobo.431.1"> retrieval can help improve efficiency by</span><a id="_idIndexMarker589"/><span class="koboSpan" id="kobo.432.1"> reducing the computational load of embedding and searching </span><span class="No-Break"><span class="koboSpan" id="kobo.433.1">massive documents.</span></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.434.1">Passage re-ranking</span></strong><span class="koboSpan" id="kobo.435.1">: The retrieved passages or knowledge base entries may undergo further reranking or filtering based on their relevance to the query, using techniques such as neural re-rankers or semantic </span><span class="No-Break"><span class="koboSpan" id="kobo.436.1">similarity measures.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.437.1">Answer extraction and generation</span></strong><span class="koboSpan" id="kobo.438.1">: Depending on the nature of the query and the knowledge base, answer extraction and generation may involve techniques such as multi-hop reasoning, knowledge graph traversal, or generating natural language responses from </span><span class="No-Break"><span class="koboSpan" id="kobo.439.1">structured data.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.440.1">Answer scoring and ranking</span></strong><span class="koboSpan" id="kobo.441.1">: Similar to the small document case, the extracted answer candidates are scored and ranked based on their confidence factor or relevance to </span><span class="No-Break"><span class="koboSpan" id="kobo.442.1">the query.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.443.1">Answer presentation</span></strong><span class="koboSpan" id="kobo.444.1">: The final answer or set of answers is presented to the user, potentially involving formatting, summarization, or generating natural </span><span class="No-Break"><span class="koboSpan" id="kobo.445.1">language explanations.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.446.1">Additional points </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.447.1">worth considering</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.448.1">:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.449.1">Adaptive retrieval limits</span></strong><span class="koboSpan" id="kobo.450.1">: Depending on the complexity of the query and document collection, setting an adaptive limit on the number of retrieved documents can </span><span class="No-Break"><span class="koboSpan" id="kobo.451.1">optimize performance.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.452.1">Compression</span></strong><span class="koboSpan" id="kobo.453.1">: Techniques such as summarization or information extraction can pre-process large documents to condense information without losing context, further aiding the LLM during </span><span class="No-Break"><span class="koboSpan" id="kobo.454.1">answer generation.</span></span></li></ul></li>
</ol>
<p><span class="koboSpan" id="kobo.455.1">This approach is particularly useful for QA systems operating on large, diverse, and potentially unstructured knowledge bases, as it leverages information retrieval techniques to efficiently retrieve and rank relevant information before answer extraction </span><a id="_idIndexMarker590"/><span class="No-Break"><span class="koboSpan" id="kobo.456.1">and generation.</span></span></p>
<p><span class="koboSpan" id="kobo.457.1">For large document ingestion, it is recommended to use Amazon Bedrock’s knowledge bases to handle the ingestion workflow and store the embeddings in a vector database, as detailed in </span><a href="B22045_05.xhtml#_idTextAnchor090"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.458.1">Chapter 5</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.459.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.460.1">Regardless of the document size, modern QA systems tend to incorporate advanced techniques, such as transformer-based language models, graph neural networks, and multi-task learning. </span><span class="koboSpan" id="kobo.460.2">Additionally, techniques such as transfer learning, few-shot learning, and domain adaptation are commonly employed to adapt QA models to different domains or </span><span class="No-Break"><span class="koboSpan" id="kobo.461.1">knowledge bases.</span></span></p>
<p><span class="koboSpan" id="kobo.462.1">It’s important to note that the specific implementation details and techniques employed may vary depending on the requirements, constraints, and resources available for a particular QA system. </span><span class="koboSpan" id="kobo.462.2">The </span><a id="_idIndexMarker591"/><span class="koboSpan" id="kobo.463.1">architectural pattern serves as a general framework, providing a solid foundation to understand the underlying mechanics and guide the design and development of QA systems tailored to different use cases </span><span class="No-Break"><span class="koboSpan" id="kobo.464.1">and domains.</span></span></p>
<h2 id="_idParaDest-134"><a id="_idTextAnchor144"/><span class="koboSpan" id="kobo.465.1">QA implementation patterns with Amazon Bedrock</span></h2>
<p><span class="koboSpan" id="kobo.466.1">In this section, we</span><a id="_idIndexMarker592"/><span class="koboSpan" id="kobo.467.1"> will explore different </span><a id="_idIndexMarker593"/><span class="koboSpan" id="kobo.468.1">patterns pertaining to QA. </span><span class="koboSpan" id="kobo.468.2">First, we will look at how to ask queries to a model directly. </span><span class="koboSpan" id="kobo.468.3">Thereafter, another approach using RAG will be covered wherein we will add contextual information. </span><span class="koboSpan" id="kobo.468.4">Let </span><span class="No-Break"><span class="koboSpan" id="kobo.469.1">us begin!</span></span></p>
<h3><span class="koboSpan" id="kobo.470.1">The baseline approach: unbound exploration in the realm of knowledge</span></h3>
<p><span class="koboSpan" id="kobo.471.1">In this initial pattern, we embark on a journey where questions are posed directly to the model, unencumbered by external constraints. </span><span class="koboSpan" id="kobo.471.2">The responses we receive are rooted in the model’s foundational knowledge. </span><span class="koboSpan" id="kobo.471.3">However, as you clearly understand by now, this approach presents a formidable challenge – the outputs are broad and generic, devoid of the nuances and specifics that define a customer’s unique business landscape. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.472.1">Figure 7</span></em></span><em class="italic"><span class="koboSpan" id="kobo.473.1">.6</span></em><span class="koboSpan" id="kobo.474.1"> depicts the journey of said user when interacting with Amazon Bedrock and using direct prompts, with small documents within the prompt to invoke </span><span class="No-Break"><span class="koboSpan" id="kobo.475.1">the model.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer113">
<span class="koboSpan" id="kobo.476.1"><img alt="Figure 7.6 – Prompting the Bedrock LLM for QA generation with direct input prompts" src="image/B22045_07_06.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.477.1">Figure 7.6 – Prompting the Bedrock LLM for QA generation with direct input prompts</span></p>
<p><span class="koboSpan" id="kobo.478.1">Note that we covered this approach in detail when we illustrated how to leverage the Amazon Bedrock Titan model to provide informative responses to user queries, as showcased in the </span><em class="italic"><span class="koboSpan" id="kobo.479.1">QA with </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.480.1">context</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.481.1"> section</span></span></p>
<p><span class="koboSpan" id="kobo.482.1">As shown previously,  the example demonstrated how the Bedrock Titan model can generate responses without any contextual information provided. </span><span class="koboSpan" id="kobo.482.2">Subsequently, we manually incorporated context into the model’s input to enhance the quality of its responses. </span><span class="koboSpan" id="kobo.482.3">It’s important to note that this approach does not involve any RAG to incorporate external knowledge into the </span><span class="No-Break"><span class="koboSpan" id="kobo.483.1">model’s output.</span></span></p>
<p><span class="koboSpan" id="kobo.484.1">While this straightforward approach can work well for short documents or singleton applications, it may not scale effectively for enterprise-level QA scenarios. </span><span class="koboSpan" id="kobo.484.2">In such cases, where large volumes of enterprise documents need to be considered, the entire context may not fit within the prompt sent to the model, necessitating more </span><span class="No-Break"><span class="koboSpan" id="kobo.485.1">advanced techniques.</span></span></p>
<h3><span class="koboSpan" id="kobo.486.1">The RAG approach: contextual illumination</span></h3>
<p><span class="koboSpan" id="kobo.487.1">In this second pattern, we will embark on a more refined journey, one that harnesses the power of RAG. </span><span class="koboSpan" id="kobo.487.2">Here, we artfully weave our questions with relevant contextual information, creating a tapestry that is more likely to contain the answers or insights that we seek. </span><span class="koboSpan" id="kobo.487.3">This approach is analogous to entering the library with a well-curated reading list, guiding </span><a id="_idIndexMarker594"/><span class="koboSpan" id="kobo.488.1">us</span><a id="_idIndexMarker595"/><span class="koboSpan" id="kobo.489.1"> toward the shelves that hold the knowledge </span><span class="No-Break"><span class="koboSpan" id="kobo.490.1">we desire.</span></span></p>
<p><span class="koboSpan" id="kobo.491.1">However, even in this enhanced approach, a limitation persists – the amount of contextual information we can incorporate is bound by the context window imposed by the model. </span><span class="koboSpan" id="kobo.491.2">It’s akin to carrying a finite number of books in our metaphorical backpack, forcing us to carefully curate the contextual information we bring along, lest we exceed the weight limit and leave behind potentially </span><span class="No-Break"><span class="koboSpan" id="kobo.492.1">crucial insights.</span></span></p>
<p><span class="koboSpan" id="kobo.493.1">As you learned in </span><a href="B22045_05.xhtml#_idTextAnchor090"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.494.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.495.1">, RAG combines the use of embeddings to index the corpus of documents, building a knowledge base, and the use of an LLM to perform the embeddings, with the goal of eventually extracting relevant information from a subset of documents within this knowledge base. </span><span class="koboSpan" id="kobo.495.2">In preparation for RAG, the documents comprising the knowledge base are split into chunks of a fixed or variable size. </span><span class="koboSpan" id="kobo.495.3">These chunks are then passed through the model to obtain their respective embedding vectors. </span><span class="koboSpan" id="kobo.495.4">Each embedding vector, along with its corresponding document chunk and additional metadata, is stored in a vector database, optimized for efficient similarity searches </span><span class="No-Break"><span class="koboSpan" id="kobo.496.1">between vectors.</span></span></p>
<p><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.497.1">Figure 7</span></em></span><em class="italic"><span class="koboSpan" id="kobo.498.1">.7</span></em><span class="koboSpan" id="kobo.499.1"> illustrates a RAG-based workflow using Amazon Bedrock in the context of a QA </span><span class="No-Break"><span class="koboSpan" id="kobo.500.1">generation framework.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer114">
<span class="koboSpan" id="kobo.501.1"><img alt="Figure 7.7 – QA with Amazon Bedrock using the RAG approach" src="image/B22045_07_07.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.502.1">Figure 7.7 – QA with Amazon Bedrock using the RAG approach</span></p>
<p><span class="koboSpan" id="kobo.503.1">By leveraging this RAG approach, we can tap into a vast repository of contextual information, allowing our generative AI models to produce more informed and accurate outputs. </span><span class="koboSpan" id="kobo.503.2">However, we must remain mindful of the token limitations and carefully curate the contextual information we incorporate. </span><span class="koboSpan" id="kobo.503.3">Doing so would ensure that we strike a balance between the breadth and depth of the domain knowledge (being parsed to the model to provide </span><a id="_idIndexMarker596"/><span class="koboSpan" id="kobo.504.1">a</span><a id="_idIndexMarker597"/><span class="koboSpan" id="kobo.505.1"> response instead of the model hallucinating), while staying within the </span><span class="No-Break"><span class="koboSpan" id="kobo.506.1">model’s constraints.</span></span></p>
<p><span class="koboSpan" id="kobo.507.1">In this approach, we will build upon the code discussed in the previous section on small document ingestion. </span><span class="koboSpan" id="kobo.507.2">However, you will find differentiating snippets in the code – specifically around identifying a similarity with the query from the source data and leveraging the pertinent information, augmented for the prompt in order to invoke </span><span class="No-Break"><span class="koboSpan" id="kobo.508.1">the LLM:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.509.1">
#importing the respective libraries
from langchain.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma
!pip install chromadb
Import boto3
Import botocore
#Create client side Amazon Bedrock connection with Boto3 library
region = os.environ.get("AWS_REGION")
bedrock_runtime = boto3.client(
    service_name='bedrock-runtime',
    region_name=region,
)
# Load the document
loader = TextLoader('path/to/document.txt')
documents = loader.load()
# Split the documents into chunks
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
texts = text_splitter.split_documents(documents)
# Create embeddings and store in Chroma vector store
from langchain_community.embeddings import BedrockEmbeddings
embeddings = BedrockEmbeddings(client=boto3_bedrock, model_id="amazon.titan-embed-text-v1")
db = Chroma.from_documents(texts, embeddings)
# Enter a user query
query = "Enter your query here"
#Perform Similarity search by finding relevant information from the embedded data
retriever = db.similarity_search(query, k=3)
full_context = '\n'.join([f'Document {indexing+1}: ' + i.page_content for indexing, i in enumerate(retriever)])
print(full_context)
#Since we have the relevant documents identified within "full_context", we can use the LLM to generate an optimal answer based on the retreived documents. </span><span class="koboSpan" id="kobo.509.2">Prior to that, let us format our prompt template before feeding to the LLM.
</span><span class="koboSpan" id="kobo.509.3">prompt_template = f"""Answer the user's question solely only on the information provided between &lt;&gt;&lt;/&gt; XML tags. </span><span class="koboSpan" id="kobo.509.4">Think step by step and provide detailed instructions.
</span><span class="koboSpan" id="kobo.509.5">&lt;context&gt;
{full_context}
&lt;/context&gt;
Question: {query}
Answer:"""
PROMPT = PromptTemplate.from_template(prompt_template)
#Prompt data input creation to feed to the LLM
prompt_data_input = PROMPT.format(human_input=query, context=context_string)
#Now, you can Invoke the foundation model using boto3 to generate the output response.
</span><span class="koboSpan" id="kobo.509.6">body = json.dumps({"inputText": prompt_data_input, "textGenerationConfig": model_parameters})
accept = "application/json"
contentType = "application/json"
# You can change this modelID to use an alternate version from the model provider
modelId = "amazon.titan-tg1-large"
response = bedrock_runtime.invoke_model(
    body=body, modelId=modelId, accept=accept, contentType=contentType)
generated_response_body = json.loads(response.get("body").read())
print(generated_response_body.get("results")[0].get("outputText").strip())</span></pre>
<p><span class="koboSpan" id="kobo.510.1">Executing this code will</span><a id="_idIndexMarker598"/><span class="koboSpan" id="kobo.511.1"> give</span><a id="_idIndexMarker599"/><span class="koboSpan" id="kobo.512.1"> you an understanding of aptly structuring the prompt template and invoking the model to generate a </span><span class="No-Break"><span class="koboSpan" id="kobo.513.1">desirable response.</span></span></p>
<p><span class="koboSpan" id="kobo.514.1">You are further encouraged to execute the code on different documents and experiment with different vector DBs and FMs to gain a deeper understanding of </span><span class="No-Break"><span class="koboSpan" id="kobo.515.1">this approach.</span></span></p>
<p><span class="koboSpan" id="kobo.516.1">Users should target finding relevant documents to provide accurate answers to their queries. </span><span class="koboSpan" id="kobo.516.2">Two of the key challenges that users experience when working on their generative AI use cases may include </span><span class="No-Break"><span class="koboSpan" id="kobo.517.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.518.1">Managing large documents that exceed the </span><span class="No-Break"><span class="koboSpan" id="kobo.519.1">token limit</span></span></li>
<li><span class="koboSpan" id="kobo.520.1">Identifying the most relevant documents for a </span><span class="No-Break"><span class="koboSpan" id="kobo.521.1">given question</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.522.1">To tackle these challenges, the RAG approach proposes the </span><span class="No-Break"><span class="koboSpan" id="kobo.523.1">following strategy:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.524.1">Document preparation and embeddings</span></strong><span class="koboSpan" id="kobo.525.1">: Before answering questions, the documents must be processed and stored in a document store index, as shown in the </span><em class="italic"><span class="koboSpan" id="kobo.526.1">Document ingestion with Amazon Bedrock</span></em><span class="koboSpan" id="kobo.527.1"> section. </span><span class="koboSpan" id="kobo.527.2">The steps involved include </span><span class="No-Break"><span class="koboSpan" id="kobo.528.1">the following:</span></span><ol><li class="upper-roman"><span class="koboSpan" id="kobo.529.1">Load </span><span class="No-Break"><span class="koboSpan" id="kobo.530.1">the documents.</span></span></li><li class="upper-roman"><span class="koboSpan" id="kobo.531.1">Process and split them into smaller, </span><span class="No-Break"><span class="koboSpan" id="kobo.532.1">manageable chunks.</span></span></li><li class="upper-roman"><span class="koboSpan" id="kobo.533.1">Create </span><a id="_idIndexMarker600"/><span class="koboSpan" id="kobo.534.1">numerical </span><a id="_idIndexMarker601"/><span class="koboSpan" id="kobo.535.1">vector representations (embeddings) of each chunk using the Amazon Bedrock Titan Embeddings model or alternate </span><span class="No-Break"><span class="koboSpan" id="kobo.536.1">embeddings models.</span></span></li><li class="upper-roman"><span class="koboSpan" id="kobo.537.1">Create an index using the chunks and their </span><span class="No-Break"><span class="koboSpan" id="kobo.538.1">corresponding embeddings.</span></span></li></ol></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.539.1">Question handling</span></strong><span class="koboSpan" id="kobo.540.1">: Once the document index is prepared, users can ask questions, and relevant document chunks will be fetched based on the query. </span><span class="koboSpan" id="kobo.540.2">The following steps will </span><span class="No-Break"><span class="koboSpan" id="kobo.541.1">be executed:</span></span><ol><li class="upper-roman" value="1"><span class="koboSpan" id="kobo.542.1">Create an embedding of the </span><span class="No-Break"><span class="koboSpan" id="kobo.543.1">input question.</span></span></li><li class="upper-roman"><span class="koboSpan" id="kobo.544.1">Compare the question embedding with the embeddings in </span><span class="No-Break"><span class="koboSpan" id="kobo.545.1">the index.</span></span></li><li class="upper-roman"><span class="koboSpan" id="kobo.546.1">Fetch the </span><em class="italic"><span class="koboSpan" id="kobo.547.1">Top K</span></em><span class="koboSpan" id="kobo.548.1"> relevant </span><span class="No-Break"><span class="koboSpan" id="kobo.549.1">document chunks.</span></span></li><li class="upper-roman"><span class="koboSpan" id="kobo.550.1">Add those chunks as part of the context in </span><span class="No-Break"><span class="koboSpan" id="kobo.551.1">the prompt.</span></span></li><li class="upper-roman"><span class="koboSpan" id="kobo.552.1">Send the prompt to the Amazon </span><span class="No-Break"><span class="koboSpan" id="kobo.553.1">Bedrock FM.</span></span></li><li class="upper-roman"><span class="koboSpan" id="kobo.554.1">Receive a contextual answer based on the </span><span class="No-Break"><span class="koboSpan" id="kobo.555.1">retrieved documents.</span></span></li></ol></li>
</ul>
<p><span class="koboSpan" id="kobo.556.1">By following this approach within the code, we can leverage the power of generative AI, embeddings, and vector datastores to provide accurate and context-aware responses to user queries, even</span><a id="_idIndexMarker602"/><span class="koboSpan" id="kobo.557.1"> when dealing with large</span><a id="_idIndexMarker603"/> <span class="No-Break"><span class="koboSpan" id="kobo.558.1">document sets.</span></span></p>
<p><span class="koboSpan" id="kobo.559.1">Now that we have uncovered QA answering systems in detail, it’s time to uncover the realm of its derivative – aka </span><span class="No-Break"><span class="koboSpan" id="kobo.560.1">conversational interfaces.</span></span></p>
<h1 id="_idParaDest-135"><a id="_idTextAnchor145"/><span class="koboSpan" id="kobo.561.1">Conversational interfaces</span></h1>
<p><strong class="bold"><span class="koboSpan" id="kobo.562.1">Conversational interfaces</span></strong><span class="koboSpan" id="kobo.563.1">, such </span><a id="_idIndexMarker604"/><span class="koboSpan" id="kobo.564.1">as virtual assistants or chatbots, have found widespread application across various domains, including customer service, sales, and e-commerce, offering swift and efficient responses to users. </span><span class="koboSpan" id="kobo.564.2">They can be accessed through diverse channels, such as websites, messaging applications, and social media platforms, thereby ensuring a seamless </span><span class="No-Break"><span class="koboSpan" id="kobo.565.1">user experience.</span></span></p>
<h2 id="_idParaDest-136"><a id="_idTextAnchor146"/><span class="koboSpan" id="kobo.566.1">Chatbot using Amazon Bedrock</span></h2>
<p><span class="koboSpan" id="kobo.567.1">In the realm of </span><a id="_idIndexMarker605"/><span class="koboSpan" id="kobo.568.1">generative AI, Amazon</span><a id="_idIndexMarker606"/><span class="koboSpan" id="kobo.569.1"> Bedrock provides a robust platform </span><a id="_idIndexMarker607"/><span class="koboSpan" id="kobo.570.1">for developing and deploying chatbots. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.571.1">Figure 7</span></em></span><em class="italic"><span class="koboSpan" id="kobo.572.1">.8</span></em><span class="koboSpan" id="kobo.573.1"> highlights the overall conversational flow inculcated with Amazon Bedrock with chat history integration. </span><span class="koboSpan" id="kobo.573.2">The flow involves the </span><span class="No-Break"><span class="koboSpan" id="kobo.574.1">following steps:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.575.1">A given user asks a particular question via the interface to the appropriate </span><span class="No-Break"><span class="koboSpan" id="kobo.576.1">Bedrock LLM.</span></span></li>
<li><span class="koboSpan" id="kobo.577.1">The model stores the conversational history to a particular database, say DynamoDB. </span><span class="koboSpan" id="kobo.577.2">The chat history and the question are appended to form an augmented prompt. </span><span class="koboSpan" id="kobo.577.3">The conversational history is stored in a database, such as DynamoDB. </span><span class="koboSpan" id="kobo.577.4">This history, along with the current user query, is used to create an augmented prompt. </span><span class="koboSpan" id="kobo.577.5">This augmented prompt is then used to inform the generative AI model, which improves the chatbot’s responses in future interactions. </span><span class="koboSpan" id="kobo.577.6">By incorporating the conversational history, the chatbot can avoid prompting the user with questions they have already been asked. </span><span class="koboSpan" id="kobo.577.7">This fosters a more natural and </span><span class="No-Break"><span class="koboSpan" id="kobo.578.1">engaging conversation.</span></span></li>
<li><span class="koboSpan" id="kobo.579.1">The augmented prompt is retrieved to get the relevant response from </span><span class="No-Break"><span class="koboSpan" id="kobo.580.1">the LLM.</span></span></li>
<li><span class="koboSpan" id="kobo.581.1">The conversation continues in the form of feedback, wherein the output generated is then fed back in the form of a conversation chain to continue the ongoing</span><a id="_idIndexMarker608"/><span class="koboSpan" id="kobo.582.1"> interaction </span><a id="_idIndexMarker609"/><span class="koboSpan" id="kobo.583.1">with </span><a id="_idIndexMarker610"/><span class="No-Break"><span class="koboSpan" id="kobo.584.1">the user.</span></span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer115">
<span class="koboSpan" id="kobo.585.1"><img alt="Figure 7.8 – A conversational flow with Amazon Bedrock" src="image/B22045_07_08.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.586.1">Figure 7.8 – A conversational flow with Amazon Bedrock</span></p>
<p><span class="koboSpan" id="kobo.587.1">The use cases for chatbots built on Amazon Bedrock are diverse and versatile, catering to a wide range </span><span class="No-Break"><span class="koboSpan" id="kobo.588.1">of scenarios:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.589.1">Basic chatbot – zero shot</span></strong><span class="koboSpan" id="kobo.590.1">: This use case involves the development of a basic chatbot that leverages a pre-trained FM to engage in conversational interactions without any additional context or prompting. </span><span class="koboSpan" id="kobo.590.2">For instance, the following prompt can </span><span class="No-Break"><span class="koboSpan" id="kobo.591.1">be provided:</span></span></li>
</ul>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.592.1">You are a friendly and helpful conversational AI assistant. </span><span class="koboSpan" id="kobo.592.2">You should engage in natural language conversations on a wide range of topics, answering questions to the best of your knowledge and abilities. </span><span class="koboSpan" id="kobo.592.3">If you are unsure about something, you can respond politely that you don't have enough information about that particular topic. </span><span class="koboSpan" id="kobo.592.4">Your main goal is to provide useful information to users in a conversational manner. </span><span class="koboSpan" id="kobo.592.5">You do not need any additional context or examples to </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.593.1">start conversing.</span></strong></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.594.1">Prompt-based chatbot (LangChain)</span></strong><span class="koboSpan" id="kobo.595.1">: In this scenario, the chatbot is designed to operate within a specific context defined by a prompt template. </span><span class="koboSpan" id="kobo.595.2">Leveraging the LangChain library, developers can create chatbots that can engage in contextualized conversations, providing relevant and tailored responses. </span><span class="koboSpan" id="kobo.595.3">For instance, the following code snippet showcases how Langchain can be used </span><a id="_idIndexMarker611"/><span class="koboSpan" id="kobo.596.1">with </span><a id="_idIndexMarker612"/><span class="koboSpan" id="kobo.597.1">a prompt</span><a id="_idIndexMarker613"/><span class="koboSpan" id="kobo.598.1"> template and engage with the user in a </span><span class="No-Break"><span class="koboSpan" id="kobo.599.1">conversational chain:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.600.1">
from langchain import PromptTemplate, LLMChain</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.601.1">
from langchain_community.llms import Bedrock</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.602.1">
# Define the prompt template</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.603.1">
template = """You are a helpful travel assistant. </span><span class="koboSpan" id="kobo.603.2">You will be provided with information about a user's travel plans, and your task is to provide relevant suggestions and recommendations based on their preferences and requirements.</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.604.1">
Travel Details: {travel_details}</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.605.1">
Using the information provided, suggest some activities, attractions, restaurants, or any other recommendations that would enhance the user's travel experience. </span><span class="koboSpan" id="kobo.605.2">Provide your response in a conversational and friendly tone."""</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.606.1">
# Create the prompt template object</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.607.1">
prompt = PromptTemplate(template=template, input_variables=["travel_details"])</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.608.1">Now, users can create the LLM chain and provide a sample prompt, such as the following one, and invoke the </span><span class="No-Break"><span class="koboSpan" id="kobo.609.1">model accordingly:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.610.1">
# Sample user input</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.611.1">
user_travel_details = """I'm planning a 5-day trip to Paris with my family (two adults and two children, ages 8 and 12). </span><span class="koboSpan" id="kobo.611.2">We're interested in exploring the city's history, architecture, and cultural attractions. </span><span class="koboSpan" id="kobo.611.3">We also enjoy trying local cuisine and engaging in family-friendly activities."""</span></pre></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.612.1">A persona-driven chatbot</span></strong><span class="koboSpan" id="kobo.613.1">: This use case involves the creation of chatbots with well-defined personas or roles. </span><span class="koboSpan" id="kobo.613.2">For instance, a career coach chatbot can be developed to engage in dialogues with users, offering guidance and advice on career-related matters, while maintaining a consistent persona throughout the interaction. </span><span class="koboSpan" id="kobo.613.3">For example, a chatbot can be used as a teaching assistant, providing students </span><a id="_idIndexMarker614"/><span class="koboSpan" id="kobo.614.1">with</span><a id="_idIndexMarker615"/><span class="koboSpan" id="kobo.615.1"> information</span><a id="_idIndexMarker616"/><span class="koboSpan" id="kobo.616.1"> and answering their questions. </span><span class="koboSpan" id="kobo.616.2">The chatbot can be designed to match the personality of a teacher, or it can take on a more playful persona to make learning more engaging. </span><span class="koboSpan" id="kobo.616.3">Yet another scenario can involve a persona-driven chatbot in a customer service or healthcare sector. </span><span class="koboSpan" id="kobo.616.4">Specifically, a chatbot in healthcare can be used to provide patients with information about their health conditions or to answer questions about medications. </span><span class="koboSpan" id="kobo.616.5">The chatbot can be designed to be empathetic and understanding, and it can use language that is easy for patients </span><span class="No-Break"><span class="koboSpan" id="kobo.617.1">to understand.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.618.1">Context-aware chatbot</span></strong><span class="koboSpan" id="kobo.619.1">: In this advanced use case, the chatbot is designed to understand and respond based on contextual information provided through external files. </span><span class="koboSpan" id="kobo.619.2">By generating embeddings from these files, the chatbot can comprehend and incorporate the provided context into its responses, delivering highly relevant and context-specific interactions. </span><span class="koboSpan" id="kobo.619.3">For instance, the examples provided in </span><a href="B22045_05.xhtml#_idTextAnchor090"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.620.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.621.1"> on RAG highlight a context-aware Chatbot use case, where a prompt is provided with the context extracted from ingested documents/external files to augment the prompt with the </span><span class="No-Break"><span class="koboSpan" id="kobo.622.1">matched context.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.623.1">These use cases</span><a id="_idIndexMarker617"/><span class="koboSpan" id="kobo.624.1"> demonstrate</span><a id="_idIndexMarker618"/><span class="koboSpan" id="kobo.625.1"> the </span><a id="_idIndexMarker619"/><span class="koboSpan" id="kobo.626.1">versatility and power of chatbots built on Amazon Bedrock, enabling developers to create conversational interfaces tailored to diverse user needs </span><span class="No-Break"><span class="koboSpan" id="kobo.627.1">and scenarios.</span></span></p>
<h2 id="_idParaDest-137"><a id="_idTextAnchor147"/><span class="koboSpan" id="kobo.628.1">Empowering chatbot development with Amazon Bedrock and the LangChain framework</span></h2>
<p><span class="koboSpan" id="kobo.629.1">In the realm of </span><a id="_idIndexMarker620"/><span class="koboSpan" id="kobo.630.1">conversationa</span><a id="_idIndexMarker621"/><span class="koboSpan" id="kobo.631.1">l interfaces, such </span><a id="_idIndexMarker622"/><span class="koboSpan" id="kobo.632.1">as </span><a id="_idIndexMarker623"/><span class="koboSpan" id="kobo.633.1">chatbots, maintaining context and retaining memory across interactions is paramount. </span><span class="koboSpan" id="kobo.633.2">This is true not only for short-term exchanges but also for long-term conversations, where the ability to recall and build upon previous interactions </span><span class="No-Break"><span class="koboSpan" id="kobo.634.1">is crucial.</span></span></p>
<p><span class="koboSpan" id="kobo.635.1">As discussed in the previous section on QA systems in greater detail (in addition to </span><a href="B22045_05.xhtml#_idTextAnchor090"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.636.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.637.1">), LangChain provides memory components in two distinct forms to address this need. </span><span class="koboSpan" id="kobo.637.2">First, it offers a suite of helper utilities designed to manage and manipulate previous chat messages. </span><span class="koboSpan" id="kobo.637.3">These utilities are modular and highly versatile, enabling their integration into various workflows </span><span class="No-Break"><span class="koboSpan" id="kobo.638.1">and applications.</span></span></p>
<p><span class="koboSpan" id="kobo.639.1">Secondly, LangChain streamlines the process of incorporating these memory utilities into chains, which are fundamental building blocks to create complex conversational systems. </span><span class="koboSpan" id="kobo.639.2">By leveraging LangChain’s abstractions and easy-to-use interfaces, developers can effortlessly define and interact with different types of memory components, enabling the creation of sophisticated and </span><span class="No-Break"><span class="koboSpan" id="kobo.640.1">context-aware chatbots.</span></span></p>
<p><span class="koboSpan" id="kobo.641.1">Whether you’re building a simple QA bot or a complex, multi-turn conversational agent, LangChain’s memory management capabilities, combined with its integration with Amazon Bedrock, empower you to craft intelligent and engaging </span><span class="No-Break"><span class="koboSpan" id="kobo.642.1">chatbot experiences.</span></span></p>
<h2 id="_idParaDest-138"><a id="_idTextAnchor148"/><span class="koboSpan" id="kobo.643.1">Crafting context-aware conversational interfaces – the fundamental pillars</span></h2>
<p><span class="koboSpan" id="kobo.644.1">As detailed in </span><a id="_idIndexMarker624"/><span class="koboSpan" id="kobo.645.1">the </span><em class="italic"><span class="koboSpan" id="kobo.646.1">QA systems with Amazon Bedrock</span></em><span class="koboSpan" id="kobo.647.1"> section, the cornerstone of developing a </span><a id="_idIndexMarker625"/><span class="koboSpan" id="kobo.648.1">context-aware chatbot lies in the generation of contextual embeddings. </span><span class="koboSpan" id="kobo.648.2">As you are aware by now, this initial phase entails an ingestion process that feeds your data through an embedding model, wherein these embeddings are then meticulously stored in a specialized data structure, often referred to as a vector store, facilitating efficient retrieval </span><span class="No-Break"><span class="koboSpan" id="kobo.649.1">and manipulation.</span></span></p>
<p><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.650.1">Figure 7</span></em></span><em class="italic"><span class="koboSpan" id="kobo.651.1">.9</span></em><span class="koboSpan" id="kobo.652.1"> depicts a process where documents or files are taken as input, processed, or transformed, and then converted into embeddings that are stored in a </span><span class="No-Break"><span class="koboSpan" id="kobo.653.1">vector store.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer116">
<span class="koboSpan" id="kobo.654.1"><img alt="Figure 7.9 – Chunking large documents and storing embeddings in a vector store" src="image/B22045_07_09.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.655.1">Figure 7.9 – Chunking large documents and storing embeddings in a vector store</span></p>
<p><span class="koboSpan" id="kobo.656.1">Identical to QA </span><a id="_idIndexMarker626"/><span class="koboSpan" id="kobo.657.1">system implementation patterns with Bedrock, the second critical component in the orchestration of user interactions can be defined as the </span><strong class="bold"><span class="koboSpan" id="kobo.658.1">request handling mechanism</span></strong><span class="koboSpan" id="kobo.659.1">. </span><span class="koboSpan" id="kobo.659.2">This </span><a id="_idIndexMarker627"/><span class="koboSpan" id="kobo.660.1">intricate process involves receiving user input, interpreting the intent and context, invoking the appropriate models or services, and synthesizing the relevant responses. </span><span class="koboSpan" id="kobo.660.2">It acts as the central hub, choreographing the various components to deliver a seamless and contextually relevant conversational experience. </span><span class="koboSpan" id="kobo.660.3">In our scenario, this form or orchestrator or request handling hub can be executed with Langchain or Amazon Bedrock agents. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.661.1">Figure 7</span></em></span><em class="italic"><span class="koboSpan" id="kobo.662.1">.10</span></em><span class="koboSpan" id="kobo.663.1"> illustrates the QA conversational interface workflow to retrieve a relevant response from the chunked documents, by extracting the desired information from the </span><span class="No-Break"><span class="koboSpan" id="kobo.664.1">vector store.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer117">
<span class="koboSpan" id="kobo.665.1"><img alt="Figure 7.10 – A QA conversational workflow with a similarity search and chunking the relevant information" src="image/B22045_07_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.666.1">Figure 7.10 – A QA conversational workflow with a similarity search and chunking the relevant information</span></p>
<p><span class="koboSpan" id="kobo.667.1">Within this request handling phase, the system leverages the previously generated embeddings, employing sophisticated algorithms to identify the most pertinent information from the vector store. </span><span class="koboSpan" id="kobo.667.2">This contextual retrieval enables the chatbot to provide responses</span><a id="_idIndexMarker628"/><span class="koboSpan" id="kobo.668.1"> that are </span><a id="_idIndexMarker629"/><span class="koboSpan" id="kobo.669.1">tailored to the specific conversational thread, accounting for the user’s intents, previous utterances, and the overarching </span><span class="No-Break"><span class="koboSpan" id="kobo.670.1">conversational context.</span></span></p>
<p><span class="koboSpan" id="kobo.671.1">Now, let’s dive into a context-aware architectural workflow in the case of </span><span class="No-Break"><span class="koboSpan" id="kobo.672.1">conversational interfaces.</span></span></p>
<h2 id="_idParaDest-139"><a id="_idTextAnchor149"/><span class="koboSpan" id="kobo.673.1">A context-aware chatbot architectural flow</span></h2>
<p><span class="koboSpan" id="kobo.674.1">The process flow for </span><a id="_idIndexMarker630"/><span class="koboSpan" id="kobo.675.1">this </span><a id="_idIndexMarker631"/><span class="koboSpan" id="kobo.676.1">architecture (also depicted in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.677.1">Figure 7</span></em></span><em class="italic"><span class="koboSpan" id="kobo.678.1">.11</span></em><span class="koboSpan" id="kobo.679.1">) is </span><span class="No-Break"><span class="koboSpan" id="kobo.680.1">as follows:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.681.1">Initially, the contextual documents are transformed into numerical embeddings using a powerful embeddings model, such as Amazon’s Titan Embeddings model. </span><span class="koboSpan" id="kobo.681.2">These embeddings are stored in a specialized vector database for </span><span class="No-Break"><span class="koboSpan" id="kobo.682.1">efficient retrieval.</span></span></li>
<li><span class="koboSpan" id="kobo.683.1">The user’s query is encoded into a numerical representation using an embeddings model, enabling the system to understand its </span><span class="No-Break"><span class="koboSpan" id="kobo.684.1">semantic meaning.</span></span></li>
<li><span class="koboSpan" id="kobo.685.1">The user’s query embeddings and the chat history are fed into an FM, specifically the text embeddings model, which searches the vector database for the most relevant </span><span class="No-Break"><span class="koboSpan" id="kobo.686.1">contextual information.</span></span></li>
<li><span class="koboSpan" id="kobo.687.1">The vector database returns the contextual embeddings that best match the query, allowing</span><a id="_idIndexMarker632"/><span class="koboSpan" id="kobo.688.1"> the LLM </span><a id="_idIndexMarker633"/><span class="koboSpan" id="kobo.689.1">to generate a response that incorporates the </span><span class="No-Break"><span class="koboSpan" id="kobo.690.1">relevant context.</span></span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer118">
<span class="koboSpan" id="kobo.691.1"><img alt="Figure 7.11 – A conversational architectural flow for context-aware chatbots" src="image/B22045_07_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.692.1">Figure 7.11 – A conversational architectural flow for context-aware chatbots</span></p>
<p><span class="koboSpan" id="kobo.693.1">The code for this architectural flow using Amazon Titan is available </span><span class="No-Break"><span class="koboSpan" id="kobo.694.1">at </span></span><a href="https://github.com/aws-samples/amazon-bedrock-workshop/blob/main/06_OpenSource_examples/02_Langchain_Chatbot_examples/00_Chatbot_Titan.ipynb"><span class="No-Break"><span class="koboSpan" id="kobo.695.1">https://github.com/aws-samples/amazon-bedrock-workshop/blob/main/06_OpenSource_examples/02_Langchain_Chatbot_examples/00_Chatbot_Titan.ipynb</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.696.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.697.1">Additional examples with different FMs from Anthropic, AI21 Labs, and Meta are also available on the Amazon Bedrock samples page at the same link. </span><span class="koboSpan" id="kobo.697.2">More examples will continue to be added to Amazon Bedrock GitHub Samples over time for users to experiment and leverage for their enterprise </span><span class="No-Break"><span class="koboSpan" id="kobo.698.1">use cases.</span></span></p>
<p><span class="koboSpan" id="kobo.699.1">Furthermore, information on building a contextual chatbot application using Knowledge Bases with Amazon bedrock can be found </span><span class="No-Break"><span class="koboSpan" id="kobo.700.1">here: </span></span><a href="https://aws.amazon.com/blogs/machine-learning/build-a-contextual-chatbot-application-using-knowledge-bases-for-amazon-bedrock/"><span class="No-Break"><span class="koboSpan" id="kobo.701.1">https://aws.amazon.com/blogs/machine-learning/build-a-contextual-chatbot-application-using-knowledge-bases-for-amazon-bedrock/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.702.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.703.1">We also encourage you to read about a well-defined QA bot built on an AWS solution, expanding your horizon of possibilities to build an enterprise-level conversational </span><span class="No-Break"><span class="koboSpan" id="kobo.704.1">chatbot: </span></span><a href="https://aws.amazon.com/blogs/machine-learning/deploy-generative-ai-self-service-question-answering-using-the-qnabot-on-aws-solution-powered-by-amazon-lex-with-amazon-kendra-and-amazon-bedrock/"><span class="No-Break"><span class="koboSpan" id="kobo.705.1">https://aws.amazon.com/blogs/machine-learning/deploy-generative-ai-self-service-question-answering-using-the-qnabot-on-aws-solution-powered-by-amazon-lex-with-amazon-kendra-and-amazon-bedrock/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.706.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.707.1">In the realm of generative AI, QA patterns and conversational interfaces represent the ever-evolving journey of knowledge acquisition and dissemination. </span><span class="koboSpan" id="kobo.707.2">As we navigate these paths, we continually seek innovative ways to refine our queries, augment our context, and push the boundaries of what is possible, all in the pursuit of unlocking the treasure trove of knowledge that lies within these </span><span class="No-Break"><span class="koboSpan" id="kobo.708.1">remarkable models.</span></span></p>
<p><span class="koboSpan" id="kobo.709.1">As enterprises continue to embrace generative AI and seek more intelligent and automated solutions, Amazon Bedrock stands out as a powerful tool to build advanced QA systems</span><a id="_idIndexMarker634"/><span class="koboSpan" id="kobo.710.1"> that can</span><a id="_idIndexMarker635"/><span class="koboSpan" id="kobo.711.1"> enhance customer experiences, streamline operations, and unlock new possibilities in </span><span class="No-Break"><span class="koboSpan" id="kobo.712.1">human-computer interactions.</span></span></p>
<h1 id="_idParaDest-140"><a id="_idTextAnchor150"/><span class="koboSpan" id="kobo.713.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.714.1">In this chapter, we explored architectural intricacies and key components that power modern QA interfaces and chatbots. </span><span class="koboSpan" id="kobo.714.2">We gained insights into prompt engineering techniques that facilitate natural and engaging conversations. </span><span class="koboSpan" id="kobo.714.3">We further illustrated how QA systems and conversational systems can be designed seamlessly with Amazon Bedrock, highlighting the architectural workflow for </span><span class="No-Break"><span class="koboSpan" id="kobo.715.1">these patterns.</span></span></p>
<p><span class="koboSpan" id="kobo.716.1">In the next chapter, we will uncover more potential use cases and applications of generative AI with Amazon Bedrock. </span><span class="koboSpan" id="kobo.716.2">We will gain a deeper understanding of entity extraction and code generation using Amazon Bedrock and its potential real-world </span><span class="No-Break"><span class="koboSpan" id="kobo.717.1">use cases.</span></span></p>
</div>
</body></html>