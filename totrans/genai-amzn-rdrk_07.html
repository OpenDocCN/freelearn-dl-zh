<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-125"><a id="_idTextAnchor132"/>7</h1>
<h1 id="_idParaDest-126"><a id="_idTextAnchor133"/>Building Question Answering Systems and Conversational Interfaces</h1>
<p>In this chapter, we will delve into<a id="_idIndexMarker551"/> the realm of <strong class="bold">question answering</strong> (<strong class="bold">QA</strong>) and conversational interfaces, harnessing the power of Amazon Bedrock. The chapter begins with unveiling real-world use cases of QA with Amazon Bedrock, demonstrating the practical applications and benefits of this technology. Moving forward, the chapter will cover architectural patterns for QA on both small and large documents, providing a solid foundation to understand the underlying mechanics. Additionally, the concept of conversation memory will be explained, allowing for the storage and utilization of chat history, thereby enabling more contextually aware and coherent conversations.</p>
<p>The chapter will also dive into the concept of embeddings and their significance within the architectural flow of QA systems. Furthermore, we will learn about prompt engineering techniques for chatbots, equipping you with the skills to craft effective prompts and enhance the performance of their conversational interfaces. Contextual awareness will also be addressed, explaining how to develop chatbots that can seamlessly integrate and leverage external files and data sources.</p>
<p>Finally, we will conclude by exploring real-world use cases of conversational interfaces, showcasing the diverse applications and potential impact of this technology across various domains.</p>
<p>The key topics that will be covered in this chapter include the following:</p>
<ul>
<li>QA overview</li>
<li>Document ingestion with Amazon Bedrock</li>
<li>Conversational interfaces<a id="_idTextAnchor134"/><a id="_idTextAnchor135"/><a id="_idTextAnchor136"/></li>
</ul>
<h1 id="_idParaDest-127"><a id="_idTextAnchor137"/>Technical requirements</h1>
<p>This chapter requires you to have access to an AWS account. If you don’t have it already, you can go to <a href="https://aws.amazon.com/getting-started/">https://aws.amazon.com/getting-started/</a> and create an AWS account.</p>
<p>Secondly, you will need to install and configure the AWS CLI (<a href="https://aws.amazon.com/cli/">https://aws.amazon.com/cli/</a>) after you create an account, which will be needed to access Amazon Bedrock FMs from your local machine. Since a major chunk of the code cells we will execute is based in Python, setting up the AWS SDK for Python (Boto3) (<a href="https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html">https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html</a>) will be beneficial at this point. You can carry out the Python setup in the following ways – install it on your local machine, use AWS Cloud9, utilize AWS Lambda, or leverage Amazon SageMaker.</p>
<p class="callout-heading">Note</p>
<p class="callout">There is a charge associated with the invocation and customization of the FMs of Amazon Bedrock. Refer to <a href="https://aws.amazon.com/bedrock/pricing/">https://aws.amazon.com/bedrock/pricing/</a> to learn more.</p>
<h1 id="_idParaDest-128"><a id="_idTextAnchor138"/>QA overview</h1>
<p>QA systems are<a id="_idIndexMarker552"/> designed to understand natural language queries and provide relevant answers based on a given context or knowledge source. These systems leverage advanced NLP techniques and machine learning models to comprehend the intent behind a user’s question, extracting the most appropriate response from the available information.</p>
<p>Let’s consider an example scenario of a typical QA system: suppose you are a content writer for a technology company and you need to explain the concept of <strong class="bold">optical character recognition</strong> (<strong class="bold">OCR</strong>) to<a id="_idIndexMarker553"/> your audience. A QA system could assist you in this task by providing relevant information from its knowledge base, or by analyzing a given text corpus related to OCR.</p>
<p>Here’s how a QA system might handle a query such as <code>What is optical character recognition </code><code>used for?</code>:</p>
<ul>
<li><strong class="bold">Query understanding</strong>: The<a id="_idIndexMarker554"/> system first analyzes the query to understand the user’s intent and extract key entities and concepts. In this case, it recognizes that the query asks about the use cases or applications of optical character recognition.</li>
<li><strong class="bold">Context retrieval</strong>: The system then searches its knowledge base or the provided text corpus to find relevant information related to OCR and its applications. It may identify passages or paragraphs that discuss OCR’s purpose and practical uses.</li>
<li><strong class="bold">Answer extraction</strong>: After retrieving the relevant context, the system employs advanced NLP techniques, such as named entity recognition, relation extraction, and semantic analysis, to identify the most relevant information that directly answers the query. It may extract specific use cases or applications of OCR from the text.</li>
<li><strong class="bold">Answer generation</strong>: Finally, the system composes a concise and natural-sounding answer based on the extracted information. For example, it might respond with <a id="_idIndexMarker555"/>something like the following:</li>
</ul>
<pre class="console">
"Optical character recognition (OCR) is used to convert text in images, scanned documents, or handwritten text into machine-readable and editable text format. It has several applications, including:
Digitizing printed documents, books, and archives for easier storage and search.
Extracting text from images or photographs for indexing or further processing.
Automating data entry tasks by extracting information from forms, invoices, or receipts.
Enabling text-to-speech capabilities for visually impaired users.
Facilitating translation of text between languages by first converting it to editable format."</pre>
<p>The system may also<a id="_idIndexMarker556"/> provide additional context, examples, or relevant information to enhance the user’s understanding of the topic.</p>
<p>Theoretically speaking, all of this looks perfect and straightforward. However, let’s ponder over some challenges in this situation.</p>
<p>Since these QA systems are designed to automatically generate responses to inquiries by analyzing and extracting relevant information from a provided set of data or text sources, they may or may not explicitly contain the complete answer to the given query. In order words, a system’s ability to infer and combine disparate pieces of information from various contexts is crucial, as the complete answer may not be readily available in a single, self-contained statement within the provided data.</p>
<p>QA poses a significant challenge, as it necessitates models to develop a deep comprehension of the semantic meaning and intent behind a query, rather than merely relying on superficial keyword matching or pattern recognition. This elevated level of language understanding is crucial for accurately identifying the relevant information required to formulate a suitable response, even when the exact phrasing or terminology differs between the query and the available context.</p>
<p>Overcoming these hurdles is essential for developing intelligent systems that can engage in fluid dialogue, provide accurate information, and enhance user experiences across a wide range of domains and applications.</p>
<p>At the time of writing, tons of generative AI use cases have spawned in a short period. Enterprises are scaling their conversational interfaces – chatbots and QA systems – with the goal of reducing manual labor and replacing existing frameworks with automated generative AI systems.</p>
<p>One of the most promising applications of LLMs and generative AI technology is, in fact, QA. Being able <a id="_idIndexMarker557"/>to ask natural language questions and receive accurate, relevant answers could transform how we interact with information and computers.</p>
<h2 id="_idParaDest-129"><a id="_idTextAnchor139"/>Potential QA applications</h2>
<p>The applications for a<a id="_idIndexMarker558"/> robust QA system are far-reaching across many industries and domains:</p>
<ul>
<li><strong class="bold">Customer service</strong>: Allow customers to ask questions and receive tailored help and troubleshooting in a natural language rather than search documentation</li>
<li><strong class="bold">Research and analytics</strong>: Allow analysts and researchers to ask open-ended exploratory questions to discover insights across large datasets</li>
<li><strong class="bold">Education</strong>: Create intelligent tutoring systems where students can ask follow-up questions and receive explanations at their level</li>
<li><strong class="bold">Knowledge management</strong>: Make an organization’s data, documentation, and processes more accessible by allowing natural language queries</li>
</ul>
<p>Of course, as with any generative AI system, there are concerns around factual accuracy, safety, and potential misuse that must be carefully addressed as QA systems are developed and deployed.</p>
<p>Nonetheless, the ability to break down barriers between humans and information through natural <a id="_idIndexMarker559"/>language queries represents a key frontier in AI’s advancement. With FMs available on Amazon Bedrock, such QA systems powered by LLMs provide an exciting glimpse at that future.</p>
<h2 id="_idParaDest-130"><a id="_idTextAnchor140"/>QA systems with Amazon Bedrock</h2>
<p>Enterprise-grade QA<a id="_idIndexMarker560"/> systems are usually built on the foundation<a id="_idIndexMarker561"/> of cutting-edge NLP techniques, including transformer architectures and transfer learning. They should be designed to understand the nuances of human language, enabling it to comprehend complex queries and extract relevant information from various data sources.</p>
<p>One of the key advantages of Amazon Bedrock is its ability to handle open-ended questions that require reasoning and inference. Unlike traditional QA systems that rely on predefined rules or patterns, Bedrock can understand the underlying context and provide thoughtful responses based on the information it has learned.</p>
<p>With a plethora of FMs available on Amazon Bedrock, developers, data scientists or generative AI enthusiasts can build applications or services that can potentially excel at dealing with ambiguity and uncertainty. If the available information is incomplete or contradictory, these engaging applications can provide responses that reflect their level of certainty, or they can request additional information, making the interaction more natural and human-like.</p>
<p>Moreover, Amazon Bedrock is highly scalable and can be easily integrated into various applications and platforms, such as chatbots, virtual assistants, and knowledge management systems. Its cloud-based architecture and high availability nature ensure that it can handle<a id="_idIndexMarker562"/> high <a id="_idIndexMarker563"/>volumes of queries and adapt to changing data and user requirements.</p>
<h3>QA without context</h3>
<p>In scenarios where <a id="_idIndexMarker564"/>no additional context or <a id="_idIndexMarker565"/>supporting documents are provided, QA systems must rely solely on their pre-trained knowledge to generate responses. This type of open-domain <strong class="bold">QA without context</strong> presents several key challenges compared to scenarios where context is given. Some of these challenges are as follows:</p>
<ul>
<li><strong class="bold">Knowledge scope and completeness</strong>: When no context is provided, the QA system’s knowledge comes entirely from what was present in its training data. This makes the scope and completeness of the training data extremely important. Ideally, the training data should cover a wide range of topics with factual accuracy. However, training datasets can have gaps, biases, or errors, which then get encoded into the model’s knowledge.</li>
<li><strong class="bold">Querying the right knowledge</strong>: Without context to ground the question, the QA system must accurately map the question to the relevant areas of knowledge in its parameters. This requires strong natural language understanding capabilities to correctly interpret the query, identify key entities/relations, and retrieve the appropriate factual knowledge to formulate a response.</li>
<li><strong class="bold">Hallucination</strong>: A critical challenge is hallucination – when the model generates incorrect information that contradicts its training data. Without grounding context, there are fewer constraints on what a model may generate. Hallucinations can range from subtle mistakes to completely fabricated outputs presented with high confidence.</li>
</ul>
<h3>Prompt examples and templates for QA without context</h3>
<p>When an LLM is <a id="_idIndexMarker566"/>asked a<a id="_idIndexMarker567"/> question without any additional context, it can be difficult for the LLM to understand the question and generate an accurate answer. It can be like providing them with a puzzle with missing pieces. Prompt engineering helps us provide the missing pieces, making it easier for LLMs to understand our questions and deliver accurate answers.</p>
<p>Thus, careful prompt engineering is required to steer generation in the right direction and encourage well-calibrated, truthful responses. There are three main techniques for prompt engineering in QA without context:</p>
<ul>
<li><code>What is the capital of France?</code>, you could ask <code>What city is the capital of France?</code>. Let’s take another example. Instead of asking <code>What caused the extinction of dinosaurs?</code> (a broad question), the reformulated prompt would look like <code>What is the most widely accepted theory for the extinction of dinosaurs?</code> (which focuses on a specific aspect).</li>
<li><code>What are the Great Lakes?</code>, you could ask <code>Provide a list of the five Great Lakes of North America</code> (which specifies a desired answer format).</li>
<li><code>Who wrote Hamlet?</code> This seems like a straightforward question, but the LLM might be unsure whether it’s referring to the authorship of the original play or a modern adaptation. Instead, you could ask the model with attribution calibration in a certain manner, such as <code>Can you tell me definitively who wrote the original play Hamlet? Based on my understanding of literature, I am very likely (or less certainly) correct in my answer</code>. This version of the prompt offers a range of confidence levels (<code>very likely</code> or <code>less certain</code>) instead of just <code>confident</code> or <code>unsure</code>. This allows the LLM to express a more nuanced level of certainty based on the information it has processed.</li>
<li>In addition to the preceding techniques, you should leverage system prompts in order to shape the interpretation and response of LLMs when queried by the end users. Think of system prompts as carefully crafted instructions that are meant to guide the model’s behavior, directing it toward the desired outcome.</li>
<li>For instance, when crafting prompts for role-playing scenarios, system prompts can define the personality traits, communication style, and domain knowledge the AI should exhibit. Imagine you’re creating a virtual assistant. Through system prompts, you can specify a helpful, informative persona, ensuring that the FM uses language and knowledge appropriate for the role.</li>
<li>Additionally, system prompts can help maintain consistency in the model’s responses, especially during prolonged interactions. By outlining the persona and desired tone throughout the prompts, you ensure that the model stays true to its <a id="_idIndexMarker568"/>character, fostering <a id="_idIndexMarker569"/>trust and a more natural user experience.</li>
<li>For an example of system prompts with the Anthropic Claude model, we encourage you to peruse through <a href="https://promptengineering.org/claudes-system-prompt-a-prompt-engineering-case-study/">https://promptengineering.org/claudes-system-prompt-a-prompt-engineering-case-study/</a>. You should always keep in mind that the best prompts will depend on the specific question and the capabilities of the LLM you use. Experiment with different phrasing and templates to find what works best for your needs.</li>
<li>By using prompt engineering, it is always possible to improve the accuracy and reliability of LLMs on QA tasks without context.</li>
</ul>
<h4>Simple question prompts</h4>
<p>One of the most straightforward ways to prompt a generative model is to pose a direct question, formatted within triple quotes in the case of a multiline comprehensive prompt within the code. Let’s experiment with an example in the Amazon Bedrock chat playground.</p>
<p>In order to execute run simple QA prompts in Amazon Bedrock playground, let’s head back to the AWS console and navigate to the Amazon Bedrock landing page. Once you reach the landing page, scroll through the left pane and click on the <strong class="bold">Chat</strong> option under <strong class="bold">Playgrounds</strong>.</p>
<p>Select a particular model in the chat playground by navigating to <strong class="bold">Select Model</strong>. In our example, let’s select<a id="_idIndexMarker570"/> the <strong class="bold">Jurassic-2 Ultra</strong> FM and initiate the conversation with the following example in <em class="italic">Figure 7</em><em class="italic">.1</em>.</p>
<div><div><img alt="Figure 7.1 – A simple prompt with Amazon Bedrock models in the chat playground" src="img/B22045_07_01.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.1 – A simple prompt with Amazon Bedrock models in the chat playground</p>
<p>As depicted in the preceding example, a simple prompt such as <code>What is Rabindranath Tagore's famous poem "Geetanjali" about?</code> was used without any context provided to the model. In order to further the chat with the model, a follow-up question was also asked, <code>What else are some of his famous poems?</code>, to which the model provided a decent response. (You can run this sample prompt in your Bedrock playground with other models and continue the conversation chain to witness any differences in the responses.)</p>
<p>You can also leverage <strong class="bold">Compare mode</strong> in <strong class="bold">Chat Playground</strong> by toggling the slider at the right side of the <strong class="bold">Chat Playground</strong> window, as shown in <em class="italic">Figure 7</em><em class="italic">.2</em>, and execute a similar prompt against multiple FMs available on Amazon Bedrock. As visible in the following figure, three models are compared on a particular question. Note the third model was added by <a id="_idIndexMarker571"/>clicking on the <strong class="bold">+</strong> option on the right side.</p>
<div><div><img alt="Figure 7.2 – Simple QA prompting with Compare Mode in Amazon Bedrock" src="img/B22045_07_02.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.2 – Simple QA prompting with Compare Mode in Amazon Bedrock</p>
<p>Similarly, by using Amazon Bedrock APIs, the models can be prompted in a QA context:</p>
<pre class="source-code">
prompt = """You are an expert AI assistant. You will answer questions in a succinct manner. If you are unsure about the
answer, say 'I am not sure about this answer'
Question: How can I connect my old Samsung TV with my Mac laptop?
Answer:"""
parameters = {
    "maxTokenCount":1024,
    "temperature":0.1,
    "topP":0.8,
    "stopSequences":[]
    }</pre>
<p>Using the preceding prompt, the FM available in Amazon Bedrock can be invoked; the model can then provide a particular response. You are encouraged to run this prompt with the Amazon <a id="_idIndexMarker572"/>Titan model and capture the response as an exercise.</p>
<h4>Model encouragement and constraints</h4>
<p>Optionally, you can encourage the model by framing the prompt in a motivational way. By combining model encouragement and constraints, you can create more effective prompts that guide the LLMs to generate high-quality responses.</p>
<p>Here are some examples:</p>
<ul>
<li>Providing context and specific keywords can encourage the model to generate more accurate responses</li>
<li>Setting length and format constraints can help the model generate responses that are concise and structured</li>
<li>Restricting the model to a specific domain or topic can help it generate responses that are more accurate and relevant</li>
</ul>
<p>A prompt example can be formatted in the following order:</p>
<p><code>You are an expert in explaining complex scientific concepts in a clear and engaging manner. Your ability to break down intricate topics into understandable terms makes you an invaluable resource for </code><code>educational purposes.</code></p>
<p><code>Constraints: Assume your audience consists of college students or professionals with a basic understanding of computer science and physics. Your explanation should be accessible yet informative, covering both theoretical and practical aspects of </code><code>quantum computing.</code></p>
<p>This is followed by the question:</p>
<p><code>Could you please provide a comprehensive overview of quantum computing, including its principles, potential applications, and the challenges </code><code>it faces?</code></p>
<p><em class="italic">Figure 7</em><em class="italic">.3</em> illustrates the sample usage of model encouragement, along with constraints to invoke <a id="_idIndexMarker573"/>the Meta Llama model on Amazon Bedrock’s chat playground.</p>
<div><div><img alt="Figure 7.3 – A simple prompt example of using model encouragement and constraints on the Meta Llama 3 model in Amazon Bedrock’s chat playground" src="img/B22045_07_03.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.3 – A simple prompt example of using model encouragement and constraints on the Meta Llama 3 model in Amazon Bedrock’s chat playground</p>
<p>You are encouraged to execute the prompt at your end and note the difference in responses with/without the constraints and model encouragement. You will notice that this type of prompt can help prime the model to provide a thoughtful, thorough response.</p>
<p>Here is another example for you to execute, either in Amazon Bedrock’s chat playground or by using Amazon Bedrock APIs to invoke the model:</p>
<p><code>You have an excellent grasp of complex machine learning concepts and can explain them in a clear and </code><code>understandable way.</code></p>
<p><code>Could you please explain the concept of gradient descent in </code><code>machine learning?</code></p>
<p><code>Please keep your explanation concise and suitable for readers with a basic understanding of </code><code>machine learning.</code></p>
<p>Let’s say you invoke <a id="_idIndexMarker574"/>an FM with a hypothetical question without any relevant context. In some cases, it may end up hallucinating. <em class="italic">Figure 7</em><em class="italic">.4</em> illustrates a fascinating scenario where the model ends up hallucinating when queried about an imaginary BMX Subaru bike, which doesn’t really exist in real life!</p>
<div><div><img alt="Figure 7.4 – A QA prompt sample without context in Amazon Bedrock’s chat playground" src="img/B22045_07_04.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.4 – A QA prompt sample without context in Amazon Bedrock’s chat playground</p>
<p>If proper prompt instructions are provided with context, the model will strive to find the relevant content within the context and then provide a reasonable desirable response.</p>
<p>Keep in mind that while QA without context is extremely challenging, strategies such as constitutional AI and iterative refinement techniques that leverage and re-combine the model’s internal<a id="_idIndexMarker575"/> knowledge in novel ways can help improve performance on open-domain QA.</p>
<p class="callout-heading">Note</p>
<p class="callout"><strong class="bold">Constitutional AI</strong> is an<a id="_idIndexMarker576"/> area of AI research concerned with developing AI systems that adhere to ethical principles and legal frameworks. It can involve designing AI systems that are fair, transparent, and accountable and respect human rights and privacy.</p>
<h3>QA with context</h3>
<p><strong class="bold">QA with context</strong> involves <a id="_idIndexMarker577"/>providing an input text and <a id="_idIndexMarker578"/>a question, and the language model must generate an answer based solely on the information contained within the given text. This task requires the model to comprehend the context, identify relevant details, and synthesize a coherent response that directly addresses the query while avoiding introducing external information.</p>
<p>For this use case, it is beneficial to structure the prompt by presenting the input text first, followed by the question. This ordering allows the model to fully process the context before attempting to formulate an answer, potentially improving response quality and accuracy. As indicated in the previous section, incorporating techniques such as model encouragement can further enhance performance on QA tasks.</p>
<p>The ideal prompt will have the following structure:</p>
<p><code>input_text: {{text}}</code></p>
<p><code>question: {{question}}</code></p>
<p><code>answer:</code></p>
<p>Now, let’s see what the content of the prompt would be like:</p>
<p><strong class="bold">Input_text</strong>:</p>
<p><code>"""The Arctic fox (Vulpes lagopus) is a small fox native to the Arctic regions of the Northern Hemisphere and common throughout the Arctic tundra biome. It is well adapted to living in cold environments, with dense fur insulation and a compact body shape that limits exposure to the cold. Adults weigh 3–5 kg (6.6–11 lb) and have a body length of 46–68 cm (18–27 in). Their thick fur is an insulating blanket that keeps them warm even in the depths of an Arctic winter. The Arctic fox has a deep, thick underfur that insulates it from the cold and a dense, insulating guard hair coat </code><code>on top."""</code></p>
<p><code>What are some key adaptations that allow the Arctic fox to survive in cold </code><code>Arctic environments?</code></p>
<p><strong class="bold">Example output</strong>:</p>
<pre class="console">
"The Arctic fox has several key adaptations that allow it to survive in the cold Arctic environments:
1. Dense fur insulation to keep it warm.
2. A compact body shape that limits exposure to the cold.
3. A thick underfur that insulates it from the cold.
4. A dense, insulating guard hair coat on top.
These physical adaptations, such as thick fur coats and compact body shape, enable the Arctic fox to withstand the extreme cold temperatures of the Arctic tundra region."</pre>
<p>The preceding example prompt showcases a scenario where an answer about the Arctic fox’s physical adaptations to cold environments is provided, and the question prompts the model to identify and summarize the relevant details from the text.</p>
<p>Next, let’s walk <a id="_idIndexMarker579"/>through an example<a id="_idIndexMarker580"/> prompt of QA with context using Amazon Bedrock APIs:</p>
<pre class="source-code">
# Import the respective libraries
import boto3
import botocore
import os
import json
import sys
#create bedrock runtime client
bedrock_runtime = boto3.client('bedrock-runtime')
#Provide the model paramters
model_parameters = {
     "maxTokenCount":1024,
     "temperature":0,
     "stopSequences":[],
     "topP":0.9
     }
#Provide relevant context to the model
context= """Using your Apple Watch to locate a misplaced iPhone is a handy feature that can save you a lot of time and frustration. The process typically begins by opening the Control Center on your watch by swiping up from the<code> </code>bottom of the watch face. From there, you'll see an icon that looks like a ringing iPhone - tapping this will remotely activate a loud pinging sound on your iPhone, even if it's on silent mode. If you're within earshot, simply follow the sound to track down your missing device. Alternatively, you can use the FindMy app on your Apple Watch, which provides a map showing the last known location of your iPhone. Tap the "Devices" tab, select your iPhone, and it will display its location, as well as give you the option to force it to emit a sound to aid in your search. For an even quicker option, you can simply raise your wrist and ask Siri "Hey Siri, find my iPhone," and the virtual assistant will attempt to pinpoint the location of your iPhone and provide directions. However, for any of these methods to work, your iPhone must be powered on, connected to a cellular or WiFi network, and have the Find My feature enabled in Settings under your Apple ID. As long as those criteria are met, your Apple Watch can be a powerful tool for tracking down a wandering iPhone."""
#Take the entire context/excerpt provided above and augment to the model along with the input question
question = "How can I find my iPhone from my Apple watch in case I lose my phone?"
prompt_data = f""" Answer the user's question solely only on the information provided between &lt;&gt;&lt;/&gt; XML tags. Think step by step and provide detailed instructions.
&lt;context&gt;
{context}
&lt;/context&gt;
Question: {question}
Answer:"""
#Now, you can Invoke the foundation model using boto3 to generate the output response.
body = json.dumps({"inputText": prompt_data, "textGenerationConfig": model_parameters})
accept = "application/json"
contentType = "application/json"
# You can change this modelID to use an alternate version from the model provider
modelId = "amazon.titan-tg1-large"
response = bedrock_runtime.invoke_model(
    body=body, modelId=modelId, accept=accept, contentType=contentType)
generated_response_body = json.loads(response.get("body").read())
print(generated_response_body.get("results")[0].get("outputText").strip())</pre>
<p>Run the preceding code, and try to invoke the Amazon Bedrock FM on your own to test the results. The generated output may look akin to <em class="italic">Figure 7</em><em class="italic">.5</em>:</p>
<div><div><img alt="Figure 7.5 – Example output generated from an Amazon Bedrock FM" src="img/B22045_07_05.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.5 – Example output generated from an Amazon Bedrock FM</p>
<p>After executing the code to invoke the model, you will observe that the model can generate an appropriate response in most cases by leveraging the information provided as context.</p>
<p>Now that we have<a id="_idIndexMarker581"/> covered prompt engineering<a id="_idIndexMarker582"/> with QA use cases on Bedrock, let’s walk through document ingestion frameworks with Amazon Bedrock.</p>
<h1 id="_idParaDest-131"><a id="_idTextAnchor141"/>Document ingestion with Amazon Bedrock</h1>
<p>The architectural pattern for QA systems with context can be broadly divided into two categories – <em class="italic">QA on small documents</em> and <em class="italic">QA on large documents on knowledge bases</em>. While the core components remain similar, the approach and techniques employed may vary, depending on the size and complexity of the input data.</p>
<h2 id="_idParaDest-132"><a id="_idTextAnchor142"/>QA on small documents</h2>
<p>For QA systems designed to<a id="_idIndexMarker583"/> handle small documents, such as paragraphs or short articles, the architectural pattern typically follows a pipeline approach consisting of the following stages:</p>
<ol>
<li><strong class="bold">Query processing</strong>: The natural language query is preprocessed by converting it to a vector representation.</li>
<li><strong class="bold">Document retrieval</strong>: Relevant documents or passages are retrieved from the corpus based on the query keywords or semantic similarity measures. For smaller documents, retrieval can be straightforward; you can directly embed and index the entire document or passage within your vector store. In another scenario, since the input documents are smaller in nature, there might not be a need to split them into smaller chunks as long as they can fit within the token size limit of the model. Once inspected, the document can be directly parsed in context within the model prompt template.</li>
<li><strong class="bold">Passage ranking</strong>: Once retrieved, the passages are ranked by their relevance to the query. This ranking can be done using techniques such as <strong class="bold">term frequency-inverse document frequency</strong> (<strong class="bold">TF-IDF</strong>) semantic similarity, or specialized <a id="_idIndexMarker584"/>neural ranking models. Automation of passage ranking can be made possible using an orchestrator or type or vector database. For instance, Amazon Kendra has a SOTA semantic searching mechanism built in to perform relevance ranking.</li>
<li><strong class="bold">Answer extraction</strong>: The top-ranked passages are analyzed to identify the most relevant spans or phrases that potentially answer the query. This stage often involves techniques such as named entity recognition, coreference resolution, and QA models. Hence, in the case of generative AI frameworks, relevant context extraction can be performed by these LLMs without the need to explicitly invoke complicated techniques.</li>
<li><strong class="bold">Answer scoring and ranking</strong>: The extracted answer candidates are scored and ranked based on their confidence or relevance to the query, using techniques such as answer verification models or scoring functions. There are some re-ranking models, such as Cohere Rerank, that can also be leveraged to improve recall performance.</li>
<li><strong class="bold">Answer generation</strong>: The top-ranked answer is generated, potentially involving post-processing steps such as formatting, rephrasing, or generating natural language responses.</li>
</ol>
<p>This pipeline approach is well-suited for QA on small documents, as it allows for efficient retrieval and ranking of relevant passages, followed by targeted answer extraction and scoring, without the need to split the document into chunks or process it in a different way.</p>
<p>Let’s walk through an example of small document ingestion with Amazon Bedrock.</p>
<p>For small document ingestion with Amazon Bedrock and LangChain, you can use the <code>TextLoader</code> and <code>PDFLoader</code> are actually <em class="italic">Python</em> classes, not software components. Here’s a brief explanation:</p>
<ul>
<li><code>TextLoader</code> and <code>PDFLoader</code> are used to load and parse text and PDF documents, respectively.</li>
<li>These classes are part of LangChain’s document loader functionality, which helps in preparing documents for further processing in AI applications.</li>
</ul>
<p>Here’s an example<a id="_idIndexMarker587"/> with TextLoader.</p>
<p class="callout-heading">Note</p>
<p class="callout">As shown in the previous chapters, ensure that the necessary libraries for LangChain are installed along with Chroma DB. We are using Chroma DB only for example purposes. You can leverage other vector databases, such as Chroma, Weaviate, Pinecone, and Faiss, based on their use case. If Chroma DB is not installed, execute <code>!pip install chromadb</code> before running the following code.</p>
<pre class="source-code">
from langchain.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma
# Load the document
loader = TextLoader('path/to/document.txt')
documents = loader.load()
# Split the documents into chunks
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
texts = text_splitter.split_documents(documents)
# Create embeddings and store in Chroma vector store
from langchain_community.embeddings import BedrockEmbeddings
embeddings = BedrockEmbeddings()
db = Chroma.from_documents(texts, embeddings)</pre>
<h2 id="_idParaDest-133"><a id="_idTextAnchor143"/>QA for large documents on knowledge bases</h2>
<p>When dealing <a id="_idIndexMarker588"/>with large documents on knowledge bases, the architectural pattern may need to be adapted to handle the scale and complexity of the data. A common approach is to incorporate techniques from information retrieval and open-domain QA systems. The following steps highlight the process of ingesting large documents, creating a vector index, and creating an end-to-end QA pipeline:</p>
<ol>
<li><strong class="bold">Knowledge base construction</strong>: The large corpus or knowledge base is preprocessed, indexed, and structured in a way that facilitates efficient retrieval and querying.</li>
<li><strong class="bold">Query processing</strong>: Similar to the small document case, the natural language query is preprocessed by converting it to a vector representation.</li>
<li><strong class="bold">Document or </strong><strong class="bold">passage retrieval</strong>:<ul><li><strong class="bold">Chunking</strong>: For larger documents, directly embedding the entire document might not be ideal. You should consider chunking the document into smaller, more manageable segments such as paragraphs or sentences.</li><li><strong class="bold">Small-to-big retrieval</strong>: In this case, the following process is followed:<ol><li class="lower-roman">Embed and search using smaller chunks during retrieval.</li><li class="lower-roman">Identify relevant chunks based on their retrieved scores.</li><li class="lower-roman">Use the retrieved chunk IDs to access and provide the corresponding larger document segment to the LLM for answer generation. This way, the LLM has access to the broader context, while retrieval leverages smaller, more focused units.</li></ol></li><li><strong class="bold">Efficiency</strong>: Chunking and <em class="italic">small-to-big</em> retrieval can help improve efficiency by<a id="_idIndexMarker589"/> reducing the computational load of embedding and searching massive documents.</li></ul></li>
<li><strong class="bold">Passage re-ranking</strong>: The retrieved passages or knowledge base entries may undergo further reranking or filtering based on their relevance to the query, using techniques such as neural re-rankers or semantic similarity measures.</li>
<li><strong class="bold">Answer extraction and generation</strong>: Depending on the nature of the query and the knowledge base, answer extraction and generation may involve techniques such as multi-hop reasoning, knowledge graph traversal, or generating natural language responses from structured data.</li>
<li><strong class="bold">Answer scoring and ranking</strong>: Similar to the small document case, the extracted answer candidates are scored and ranked based on their confidence factor or relevance to the query.</li>
<li><strong class="bold">Answer presentation</strong>: The final answer or set of answers is presented to the user, potentially involving formatting, summarization, or generating natural language explanations.</li>
<li><strong class="bold">Additional points </strong><strong class="bold">worth considering</strong>:<ul><li><strong class="bold">Adaptive retrieval limits</strong>: Depending on the complexity of the query and document collection, setting an adaptive limit on the number of retrieved documents can optimize performance.</li><li><strong class="bold">Compression</strong>: Techniques such as summarization or information extraction can pre-process large documents to condense information without losing context, further aiding the LLM during answer generation.</li></ul></li>
</ol>
<p>This approach is particularly useful for QA systems operating on large, diverse, and potentially unstructured knowledge bases, as it leverages information retrieval techniques to efficiently retrieve and rank relevant information before answer extraction <a id="_idIndexMarker590"/>and generation.</p>
<p>For large document ingestion, it is recommended to use Amazon Bedrock’s knowledge bases to handle the ingestion workflow and store the embeddings in a vector database, as detailed in <a href="B22045_05.xhtml#_idTextAnchor090"><em class="italic">Chapter 5</em></a>.</p>
<p>Regardless of the document size, modern QA systems tend to incorporate advanced techniques, such as transformer-based language models, graph neural networks, and multi-task learning. Additionally, techniques such as transfer learning, few-shot learning, and domain adaptation are commonly employed to adapt QA models to different domains or knowledge bases.</p>
<p>It’s important to note that the specific implementation details and techniques employed may vary depending on the requirements, constraints, and resources available for a particular QA system. The <a id="_idIndexMarker591"/>architectural pattern serves as a general framework, providing a solid foundation to understand the underlying mechanics and guide the design and development of QA systems tailored to different use cases and domains.</p>
<h2 id="_idParaDest-134"><a id="_idTextAnchor144"/>QA implementation patterns with Amazon Bedrock</h2>
<p>In this section, we<a id="_idIndexMarker592"/> will explore different <a id="_idIndexMarker593"/>patterns pertaining to QA. First, we will look at how to ask queries to a model directly. Thereafter, another approach using RAG will be covered wherein we will add contextual information. Let us begin!</p>
<h3>The baseline approach: unbound exploration in the realm of knowledge</h3>
<p>In this initial pattern, we embark on a journey where questions are posed directly to the model, unencumbered by external constraints. The responses we receive are rooted in the model’s foundational knowledge. However, as you clearly understand by now, this approach presents a formidable challenge – the outputs are broad and generic, devoid of the nuances and specifics that define a customer’s unique business landscape. <em class="italic">Figure 7</em><em class="italic">.6</em> depicts the journey of said user when interacting with Amazon Bedrock and using direct prompts, with small documents within the prompt to invoke the model.</p>
<div><div><img alt="Figure 7.6 – Prompting the Bedrock LLM for QA generation with direct input prompts" src="img/B22045_07_06.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.6 – Prompting the Bedrock LLM for QA generation with direct input prompts</p>
<p>Note that we covered this approach in detail when we illustrated how to leverage the Amazon Bedrock Titan model to provide informative responses to user queries, as showcased in the <em class="italic">QA with </em><em class="italic">context</em> section</p>
<p>As shown previously,  the example demonstrated how the Bedrock Titan model can generate responses without any contextual information provided. Subsequently, we manually incorporated context into the model’s input to enhance the quality of its responses. It’s important to note that this approach does not involve any RAG to incorporate external knowledge into the model’s output.</p>
<p>While this straightforward approach can work well for short documents or singleton applications, it may not scale effectively for enterprise-level QA scenarios. In such cases, where large volumes of enterprise documents need to be considered, the entire context may not fit within the prompt sent to the model, necessitating more advanced techniques.</p>
<h3>The RAG approach: contextual illumination</h3>
<p>In this second pattern, we will embark on a more refined journey, one that harnesses the power of RAG. Here, we artfully weave our questions with relevant contextual information, creating a tapestry that is more likely to contain the answers or insights that we seek. This approach is analogous to entering the library with a well-curated reading list, guiding <a id="_idIndexMarker594"/>us<a id="_idIndexMarker595"/> toward the shelves that hold the knowledge we desire.</p>
<p>However, even in this enhanced approach, a limitation persists – the amount of contextual information we can incorporate is bound by the context window imposed by the model. It’s akin to carrying a finite number of books in our metaphorical backpack, forcing us to carefully curate the contextual information we bring along, lest we exceed the weight limit and leave behind potentially crucial insights.</p>
<p>As you learned in <a href="B22045_05.xhtml#_idTextAnchor090"><em class="italic">Chapter 5</em></a>, RAG combines the use of embeddings to index the corpus of documents, building a knowledge base, and the use of an LLM to perform the embeddings, with the goal of eventually extracting relevant information from a subset of documents within this knowledge base. In preparation for RAG, the documents comprising the knowledge base are split into chunks of a fixed or variable size. These chunks are then passed through the model to obtain their respective embedding vectors. Each embedding vector, along with its corresponding document chunk and additional metadata, is stored in a vector database, optimized for efficient similarity searches between vectors.</p>
<p><em class="italic">Figure 7</em><em class="italic">.7</em> illustrates a RAG-based workflow using Amazon Bedrock in the context of a QA generation framework.</p>
<div><div><img alt="Figure 7.7 – QA with Amazon Bedrock using the RAG approach" src="img/B22045_07_07.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.7 – QA with Amazon Bedrock using the RAG approach</p>
<p>By leveraging this RAG approach, we can tap into a vast repository of contextual information, allowing our generative AI models to produce more informed and accurate outputs. However, we must remain mindful of the token limitations and carefully curate the contextual information we incorporate. Doing so would ensure that we strike a balance between the breadth and depth of the domain knowledge (being parsed to the model to provide <a id="_idIndexMarker596"/>a<a id="_idIndexMarker597"/> response instead of the model hallucinating), while staying within the model’s constraints.</p>
<p>In this approach, we will build upon the code discussed in the previous section on small document ingestion. However, you will find differentiating snippets in the code – specifically around identifying a similarity with the query from the source data and leveraging the pertinent information, augmented for the prompt in order to invoke the LLM:</p>
<pre class="source-code">
#importing the respective libraries
from langchain.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma
!pip install chromadb
Import boto3
Import botocore
#Create client side Amazon Bedrock connection with Boto3 library
region = os.environ.get("AWS_REGION")
bedrock_runtime = boto3.client(
    service_name='bedrock-runtime',
    region_name=region,
)
# Load the document
loader = TextLoader('path/to/document.txt')
documents = loader.load()
# Split the documents into chunks
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
texts = text_splitter.split_documents(documents)
# Create embeddings and store in Chroma vector store
from langchain_community.embeddings import BedrockEmbeddings
embeddings = BedrockEmbeddings(client=boto3_bedrock, model_id="amazon.titan-embed-text-v1")
db = Chroma.from_documents(texts, embeddings)
# Enter a user query
query = "Enter your query here"
#Perform Similarity search by finding relevant information from the embedded data
retriever = db.similarity_search(query, k=3)
full_context = '\n'.join([f'Document {indexing+1}: ' + i.page_content for indexing, i in enumerate(retriever)])
print(full_context)
#Since we have the relevant documents identified within "full_context", we can use the LLM to generate an optimal answer based on the retreived documents. Prior to that, let us format our prompt template before feeding to the LLM.
prompt_template = f"""Answer the user's question solely only on the information provided between &lt;&gt;&lt;/&gt; XML tags. Think step by step and provide detailed instructions.
&lt;context&gt;
{full_context}
&lt;/context&gt;
Question: {query}
Answer:"""
PROMPT = PromptTemplate.from_template(prompt_template)
#Prompt data input creation to feed to the LLM
prompt_data_input = PROMPT.format(human_input=query, context=context_string)
#Now, you can Invoke the foundation model using boto3 to generate the output response.
body = json.dumps({"inputText": prompt_data_input, "textGenerationConfig": model_parameters})
accept = "application/json"
contentType = "application/json"
# You can change this modelID to use an alternate version from the model provider
modelId = "amazon.titan-tg1-large"
response = bedrock_runtime.invoke_model(
    body=body, modelId=modelId, accept=accept, contentType=contentType)
generated_response_body = json.loads(response.get("body").read())
print(generated_response_body.get("results")[0].get("outputText").strip())</pre>
<p>Executing this code will<a id="_idIndexMarker598"/> give<a id="_idIndexMarker599"/> you an understanding of aptly structuring the prompt template and invoking the model to generate a desirable response.</p>
<p>You are further encouraged to execute the code on different documents and experiment with different vector DBs and FMs to gain a deeper understanding of this approach.</p>
<p>Users should target finding relevant documents to provide accurate answers to their queries. Two of the key challenges that users experience when working on their generative AI use cases may include the following:</p>
<ul>
<li>Managing large documents that exceed the token limit</li>
<li>Identifying the most relevant documents for a given question</li>
</ul>
<p>To tackle these challenges, the RAG approach proposes the following strategy:</p>
<ul>
<li><strong class="bold">Document preparation and embeddings</strong>: Before answering questions, the documents must be processed and stored in a document store index, as shown in the <em class="italic">Document ingestion with Amazon Bedrock</em> section. The steps involved include the following:<ol><li class="upper-roman">Load the documents.</li><li class="upper-roman">Process and split them into smaller, manageable chunks.</li><li class="upper-roman">Create <a id="_idIndexMarker600"/>numerical <a id="_idIndexMarker601"/>vector representations (embeddings) of each chunk using the Amazon Bedrock Titan Embeddings model or alternate embeddings models.</li><li class="upper-roman">Create an index using the chunks and their corresponding embeddings.</li></ol></li>
<li><strong class="bold">Question handling</strong>: Once the document index is prepared, users can ask questions, and relevant document chunks will be fetched based on the query. The following steps will be executed:<ol><li class="upper-roman" value="1">Create an embedding of the input question.</li><li class="upper-roman">Compare the question embedding with the embeddings in the index.</li><li class="upper-roman">Fetch the <em class="italic">Top K</em> relevant document chunks.</li><li class="upper-roman">Add those chunks as part of the context in the prompt.</li><li class="upper-roman">Send the prompt to the Amazon Bedrock FM.</li><li class="upper-roman">Receive a contextual answer based on the retrieved documents.</li></ol></li>
</ul>
<p>By following this approach within the code, we can leverage the power of generative AI, embeddings, and vector datastores to provide accurate and context-aware responses to user queries, even<a id="_idIndexMarker602"/> when dealing with large<a id="_idIndexMarker603"/> document sets.</p>
<p>Now that we have uncovered QA answering systems in detail, it’s time to uncover the realm of its derivative – aka conversational interfaces.</p>
<h1 id="_idParaDest-135"><a id="_idTextAnchor145"/>Conversational interfaces</h1>
<p><strong class="bold">Conversational interfaces</strong>, such <a id="_idIndexMarker604"/>as virtual assistants or chatbots, have found widespread application across various domains, including customer service, sales, and e-commerce, offering swift and efficient responses to users. They can be accessed through diverse channels, such as websites, messaging applications, and social media platforms, thereby ensuring a seamless user experience.</p>
<h2 id="_idParaDest-136"><a id="_idTextAnchor146"/>Chatbot using Amazon Bedrock</h2>
<p>In the realm of <a id="_idIndexMarker605"/>generative AI, Amazon<a id="_idIndexMarker606"/> Bedrock provides a robust platform <a id="_idIndexMarker607"/>for developing and deploying chatbots. <em class="italic">Figure 7</em><em class="italic">.8</em> highlights the overall conversational flow inculcated with Amazon Bedrock with chat history integration. The flow involves the following steps:</p>
<ol>
<li>A given user asks a particular question via the interface to the appropriate Bedrock LLM.</li>
<li>The model stores the conversational history to a particular database, say DynamoDB. The chat history and the question are appended to form an augmented prompt. The conversational history is stored in a database, such as DynamoDB. This history, along with the current user query, is used to create an augmented prompt. This augmented prompt is then used to inform the generative AI model, which improves the chatbot’s responses in future interactions. By incorporating the conversational history, the chatbot can avoid prompting the user with questions they have already been asked. This fosters a more natural and engaging conversation.</li>
<li>The augmented prompt is retrieved to get the relevant response from the LLM.</li>
<li>The conversation continues in the form of feedback, wherein the output generated is then fed back in the form of a conversation chain to continue the ongoing<a id="_idIndexMarker608"/> interaction <a id="_idIndexMarker609"/>with <a id="_idIndexMarker610"/>the user.</li>
</ol>
<div><div><img alt="Figure 7.8 – A conversational flow with Amazon Bedrock" src="img/B22045_07_08.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.8 – A conversational flow with Amazon Bedrock</p>
<p>The use cases for chatbots built on Amazon Bedrock are diverse and versatile, catering to a wide range of scenarios:</p>
<ul>
<li><strong class="bold">Basic chatbot – zero shot</strong>: This use case involves the development of a basic chatbot that leverages a pre-trained FM to engage in conversational interactions without any additional context or prompting. For instance, the following prompt can be provided:</li>
</ul>
<p><code>You are a friendly and helpful conversational AI assistant. You should engage in natural language conversations on a wide range of topics, answering questions to the best of your knowledge and abilities. If you are unsure about something, you can respond politely that you don't have enough information about that particular topic. Your main goal is to provide useful information to users in a conversational manner. You do not need any additional context or examples to </code><code>start conversing.</code></p>
<ul>
<li><strong class="bold">Prompt-based chatbot (LangChain)</strong>: In this scenario, the chatbot is designed to operate within a specific context defined by a prompt template. Leveraging the LangChain library, developers can create chatbots that can engage in contextualized conversations, providing relevant and tailored responses. For instance, the following code snippet showcases how Langchain can be used <a id="_idIndexMarker611"/>with <a id="_idIndexMarker612"/>a prompt<a id="_idIndexMarker613"/> template and engage with the user in a conversational chain:<pre class="source-code">
from langchain import PromptTemplate, LLMChain</pre><pre class="source-code">
from langchain_community.llms import Bedrock</pre><pre class="source-code">
# Define the prompt template</pre><pre class="source-code">
template = """You are a helpful travel assistant. You will be provided with information about a user's travel plans, and your task is to provide relevant suggestions and recommendations based on their preferences and requirements.</pre><pre class="source-code">
Travel Details: {travel_details}</pre><pre class="source-code">
Using the information provided, suggest some activities, attractions, restaurants, or any other recommendations that would enhance the user's travel experience. Provide your response in a conversational and friendly tone."""</pre><pre class="source-code">
# Create the prompt template object</pre><pre class="source-code">
prompt = PromptTemplate(template=template, input_variables=["travel_details"])</pre><p class="list-inset">Now, users can create the LLM chain and provide a sample prompt, such as the following one, and invoke the model accordingly:</p><pre class="source-code">
# Sample user input</pre><pre class="source-code">
user_travel_details = """I'm planning a 5-day trip to Paris with my family (two adults and two children, ages 8 and 12). We're interested in exploring the city's history, architecture, and cultural attractions. We also enjoy trying local cuisine and engaging in family-friendly activities."""</pre></li>
<li><strong class="bold">A persona-driven chatbot</strong>: This use case involves the creation of chatbots with well-defined personas or roles. For instance, a career coach chatbot can be developed to engage in dialogues with users, offering guidance and advice on career-related matters, while maintaining a consistent persona throughout the interaction. For example, a chatbot can be used as a teaching assistant, providing students <a id="_idIndexMarker614"/>with<a id="_idIndexMarker615"/> information<a id="_idIndexMarker616"/> and answering their questions. The chatbot can be designed to match the personality of a teacher, or it can take on a more playful persona to make learning more engaging. Yet another scenario can involve a persona-driven chatbot in a customer service or healthcare sector. Specifically, a chatbot in healthcare can be used to provide patients with information about their health conditions or to answer questions about medications. The chatbot can be designed to be empathetic and understanding, and it can use language that is easy for patients to understand.</li>
<li><strong class="bold">Context-aware chatbot</strong>: In this advanced use case, the chatbot is designed to understand and respond based on contextual information provided through external files. By generating embeddings from these files, the chatbot can comprehend and incorporate the provided context into its responses, delivering highly relevant and context-specific interactions. For instance, the examples provided in <a href="B22045_05.xhtml#_idTextAnchor090"><em class="italic">Chapter 5</em></a> on RAG highlight a context-aware Chatbot use case, where a prompt is provided with the context extracted from ingested documents/external files to augment the prompt with the matched context.</li>
</ul>
<p>These use cases<a id="_idIndexMarker617"/> demonstrate<a id="_idIndexMarker618"/> the <a id="_idIndexMarker619"/>versatility and power of chatbots built on Amazon Bedrock, enabling developers to create conversational interfaces tailored to diverse user needs and scenarios.</p>
<h2 id="_idParaDest-137"><a id="_idTextAnchor147"/>Empowering chatbot development with Amazon Bedrock and the LangChain framework</h2>
<p>In the realm of <a id="_idIndexMarker620"/>conversationa<a id="_idIndexMarker621"/>l interfaces, such <a id="_idIndexMarker622"/>as <a id="_idIndexMarker623"/>chatbots, maintaining context and retaining memory across interactions is paramount. This is true not only for short-term exchanges but also for long-term conversations, where the ability to recall and build upon previous interactions is crucial.</p>
<p>As discussed in the previous section on QA systems in greater detail (in addition to <a href="B22045_05.xhtml#_idTextAnchor090"><em class="italic">Chapter 5</em></a>), LangChain provides memory components in two distinct forms to address this need. First, it offers a suite of helper utilities designed to manage and manipulate previous chat messages. These utilities are modular and highly versatile, enabling their integration into various workflows and applications.</p>
<p>Secondly, LangChain streamlines the process of incorporating these memory utilities into chains, which are fundamental building blocks to create complex conversational systems. By leveraging LangChain’s abstractions and easy-to-use interfaces, developers can effortlessly define and interact with different types of memory components, enabling the creation of sophisticated and context-aware chatbots.</p>
<p>Whether you’re building a simple QA bot or a complex, multi-turn conversational agent, LangChain’s memory management capabilities, combined with its integration with Amazon Bedrock, empower you to craft intelligent and engaging chatbot experiences.</p>
<h2 id="_idParaDest-138"><a id="_idTextAnchor148"/>Crafting context-aware conversational interfaces – the fundamental pillars</h2>
<p>As detailed in <a id="_idIndexMarker624"/>the <em class="italic">QA systems with Amazon Bedrock</em> section, the cornerstone of developing a <a id="_idIndexMarker625"/>context-aware chatbot lies in the generation of contextual embeddings. As you are aware by now, this initial phase entails an ingestion process that feeds your data through an embedding model, wherein these embeddings are then meticulously stored in a specialized data structure, often referred to as a vector store, facilitating efficient retrieval and manipulation.</p>
<p><em class="italic">Figure 7</em><em class="italic">.9</em> depicts a process where documents or files are taken as input, processed, or transformed, and then converted into embeddings that are stored in a vector store.</p>
<div><div><img alt="Figure 7.9 – Chunking large documents and storing embeddings in a vector store" src="img/B22045_07_09.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.9 – Chunking large documents and storing embeddings in a vector store</p>
<p>Identical to QA <a id="_idIndexMarker626"/>system implementation patterns with Bedrock, the second critical component in the orchestration of user interactions can be defined as the <strong class="bold">request handling mechanism</strong>. This <a id="_idIndexMarker627"/>intricate process involves receiving user input, interpreting the intent and context, invoking the appropriate models or services, and synthesizing the relevant responses. It acts as the central hub, choreographing the various components to deliver a seamless and contextually relevant conversational experience. In our scenario, this form or orchestrator or request handling hub can be executed with Langchain or Amazon Bedrock agents. <em class="italic">Figure 7</em><em class="italic">.10</em> illustrates the QA conversational interface workflow to retrieve a relevant response from the chunked documents, by extracting the desired information from the vector store.</p>
<div><div><img alt="Figure 7.10 – A QA conversational workflow with a similarity search and chunking the relevant information" src="img/B22045_07_10.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.10 – A QA conversational workflow with a similarity search and chunking the relevant information</p>
<p>Within this request handling phase, the system leverages the previously generated embeddings, employing sophisticated algorithms to identify the most pertinent information from the vector store. This contextual retrieval enables the chatbot to provide responses<a id="_idIndexMarker628"/> that are <a id="_idIndexMarker629"/>tailored to the specific conversational thread, accounting for the user’s intents, previous utterances, and the overarching conversational context.</p>
<p>Now, let’s dive into a context-aware architectural workflow in the case of conversational interfaces.</p>
<h2 id="_idParaDest-139"><a id="_idTextAnchor149"/>A context-aware chatbot architectural flow</h2>
<p>The process flow for <a id="_idIndexMarker630"/>this <a id="_idIndexMarker631"/>architecture (also depicted in <em class="italic">Figure 7</em><em class="italic">.11</em>) is as follows:</p>
<ol>
<li>Initially, the contextual documents are transformed into numerical embeddings using a powerful embeddings model, such as Amazon’s Titan Embeddings model. These embeddings are stored in a specialized vector database for efficient retrieval.</li>
<li>The user’s query is encoded into a numerical representation using an embeddings model, enabling the system to understand its semantic meaning.</li>
<li>The user’s query embeddings and the chat history are fed into an FM, specifically the text embeddings model, which searches the vector database for the most relevant contextual information.</li>
<li>The vector database returns the contextual embeddings that best match the query, allowing<a id="_idIndexMarker632"/> the LLM <a id="_idIndexMarker633"/>to generate a response that incorporates the relevant context.</li>
</ol>
<div><div><img alt="Figure 7.11 – A conversational architectural flow for context-aware chatbots" src="img/B22045_07_11.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.11 – A conversational architectural flow for context-aware chatbots</p>
<p>The code for this architectural flow using Amazon Titan is available at <a href="https://github.com/aws-samples/amazon-bedrock-workshop/blob/main/06_OpenSource_examples/02_Langchain_Chatbot_examples/00_Chatbot_Titan.ipynb">https://github.com/aws-samples/amazon-bedrock-workshop/blob/main/06_OpenSource_examples/02_Langchain_Chatbot_examples/00_Chatbot_Titan.ipynb</a>.</p>
<p>Additional examples with different FMs from Anthropic, AI21 Labs, and Meta are also available on the Amazon Bedrock samples page at the same link. More examples will continue to be added to Amazon Bedrock GitHub Samples over time for users to experiment and leverage for their enterprise use cases.</p>
<p>Furthermore, information on building a contextual chatbot application using Knowledge Bases with Amazon bedrock can be found here: <a href="https://aws.amazon.com/blogs/machine-learning/build-a-contextual-chatbot-application-using-knowledge-bases-for-amazon-bedrock/">https://aws.amazon.com/blogs/machine-learning/build-a-contextual-chatbot-application-using-knowledge-bases-for-amazon-bedrock/</a>.</p>
<p>We also encourage you to read about a well-defined QA bot built on an AWS solution, expanding your horizon of possibilities to build an enterprise-level conversational chatbot: <a href="https://aws.amazon.com/blogs/machine-learning/deploy-generative-ai-self-service-question-answering-using-the-qnabot-on-aws-solution-powered-by-amazon-lex-with-amazon-kendra-and-amazon-bedrock/">https://aws.amazon.com/blogs/machine-learning/deploy-generative-ai-self-service-question-answering-using-the-qnabot-on-aws-solution-powered-by-amazon-lex-with-amazon-kendra-and-amazon-bedrock/</a>.</p>
<p>In the realm of generative AI, QA patterns and conversational interfaces represent the ever-evolving journey of knowledge acquisition and dissemination. As we navigate these paths, we continually seek innovative ways to refine our queries, augment our context, and push the boundaries of what is possible, all in the pursuit of unlocking the treasure trove of knowledge that lies within these remarkable models.</p>
<p>As enterprises continue to embrace generative AI and seek more intelligent and automated solutions, Amazon Bedrock stands out as a powerful tool to build advanced QA systems<a id="_idIndexMarker634"/> that can<a id="_idIndexMarker635"/> enhance customer experiences, streamline operations, and unlock new possibilities in human-computer interactions.</p>
<h1 id="_idParaDest-140"><a id="_idTextAnchor150"/>Summary</h1>
<p>In this chapter, we explored architectural intricacies and key components that power modern QA interfaces and chatbots. We gained insights into prompt engineering techniques that facilitate natural and engaging conversations. We further illustrated how QA systems and conversational systems can be designed seamlessly with Amazon Bedrock, highlighting the architectural workflow for these patterns.</p>
<p>In the next chapter, we will uncover more potential use cases and applications of generative AI with Amazon Bedrock. We will gain a deeper understanding of entity extraction and code generation using Amazon Bedrock and its potential real-world use cases.</p>
</div>
</body></html>