- en: <title>Building Chatbots and Agents with LlamaIndex</title>
  prefs: []
  type: TYPE_NORMAL
- en: Building Chatbots and Agents with LlamaIndex
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As this ebook edition doesn't have fixed pagination, the page numbers below
    are hyperlinked for reference only, based on the printed edition of this book.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter provides an in-depth look at implementing chatbots and intelligent
    agents using the capabilities of LlamaIndex. We will explore the various chat
    engine modes available, from simple chatbots to more advanced context-aware and
    question- **condensing engines** . Then, we’ll dive into **agent architectures**
    , analyzing tools, **reasoning loops** , and parallel execution methods. You will
    gain practical knowledge so that you can build conversational interfaces powered
    by LLMs that can understand user needs and orchestrate responses or actions by
    utilizing tools and data sources.
  prefs: []
  type: TYPE_NORMAL
- en: 'Throughout this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding chatbots and agents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing agentic strategies in our apps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hands-on – implementing conversation tracking for PITS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <title>Technical requirements</title>
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following LlamaIndex integration packages will be required for the sample
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Database* *Tool* : https://pypi.org/project/llama-index-tools-database/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*OpenAI* *Agent* : https://pypi.org/project/llama-index-agent-openai/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Wikipedia* *Reader* : https://pypi.org/search/?q=llama-index-readers-wikipedia'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*LLM Compiler* *Agent* : https://pypi.org/project/llama-index-packs-agents-llm-compiler/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All the code samples in this chapter can be found in the `ch8` subfolder of
    this book’s GitHub repository: https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex
    .'
  prefs: []
  type: TYPE_NORMAL
- en: <title>Understanding chatbots and agents</title>
  prefs: []
  type: TYPE_NORMAL
- en: Understanding chatbots and agents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'chat_engine = index.as_chat_engine() response = chat_engine.chat("Hi, how are
    you?") chat_engine.chat_repl() chat_engine.reset() from llama_index.core.storage.chat_store
    import SimpleChatStore from llama_index.core.chat_engine import SimpleChatEngine
    from llama_index.core.memory import ChatMemoryBuffer try:     chat_store = SimpleChatStore.from_persist_path(
            persist_path="chat_memory.json"     ) except FileNotFoundError:     chat_store
    = SimpleChatStore() memory = ChatMemoryBuffer.from_defaults(     token_limit=2000,
        chat_store=chat_store,     chat_store_key="user_X"     ) chat_engine = SimpleChatEngine.from_defaults(memory=memory)
    while True:     user_message = input("You: ")     if user_message.lower() == ''exit'':
            print("Exiting chat")         break     response = chat_engine.chat(user_message)
        print(f"Chatbot: {response}") chat_store.persist(persist_path="chat_memory.json")
    from llama_index.core.chat_engine import SimpleChatEngine chat_engine = SimpleChatEngine.from_defaults()
    chat_engine.chat_repl() from llama_index.llms.openai import OpenAI llm = OpenAI(temperature=0.8,
    model="gpt-4") chat_engine = SimpleChatEngine.from_defaults(llm=llm) from llama_index.core
    import VectorStoreIndex, SimpleDirectoryReader docs = SimpleDirectoryReader(input_dir="files").load_data()
    index = VectorStoreIndex.from_documents(docs) chat_engine = index.as_chat_engine(
        chat_mode="context",     system_prompt=(         "You''re a chatbot, able
    to talk about "         "general topics, as well as answering specific "         "questions
    about ancient Rome."     ), ) chat_engine.chat_repl() retriever = index.as_retriever(retriever_mode=''default'')
    chat_engine = ContextChatEngine.from_defaults(     retriever=retriever     ) from
    llama_index.core import VectorStoreIndex, SimpleDirectoryReader from llama_index.core.chat_engine
    import CondenseQuestionChatEngine from llama_index.core.llms import ChatMessage
    documents = SimpleDirectoryReader("files").load_data() index = VectorStoreIndex.from_documents(documents)
    query_engine=index.as_query_engine() chat_history = [     ChatMessage(         role="user",
            content="Arch of Constantine is a famous"         "building in Rome"     ),
        ChatMessage(         role="user",         content="The Pantheon should not
    be "         "regarded as a famous building"     ), ] chat_engine = CondenseQuestionChatEngine.from_defaults(
        query_engine=query_engine,     chat_history=chat_history ) response = chat_engine.chat(
        "What are two of the most famous structures in ancient Rome?" ) print(response)
    The Colosseum and the Pantheon. The Colosseum and the Arch of Constantine are
    two famous buildings in ancient Rome. index.as_chat_engine(chat_mode="condense_question")
    index.as_chat_engine(chat_mode="condense_plus_context")'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the modern business ecosystem, the role of **chatbot systems** is increasingly
    important. First appearing in the 1960s ( https://en.wikipedia.org/wiki/ELIZA
    ), chatbots have always fascinated both developers and technology users alike.
    *Figure 8* *.1* shows the user interface of one of these early systems:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_08_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – The ELIZA chatbot interface
  prefs: []
  type: TYPE_NORMAL
- en: While these systems were rudimentary initially and seen as more of an experiment,
    with the advancement of NLP technologies, the experience they offer has become
    increasingly interesting and valuable to users.
  prefs: []
  type: TYPE_NORMAL
- en: '**Chatbot-based support systems** offer today’s consumers a self-service experience.
    For users, self-service support services have two major advantages over human
    support:'
  prefs: []
  type: TYPE_NORMAL
- en: They are available 24/7, even outside normal working hours
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The user does not have to *hold the line* to access them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even if there is some reluctance to use these systems at first, once they discover
    these advantages, users soon get used to interacting with them.
  prefs: []
  type: TYPE_NORMAL
- en: Don’t necessarily think of chatbots as a technology designed to replace human
    support and interaction entirely. Although they have made enormous progress in
    recent years, these technologies, while getting more and more advanced, still
    have their shortcomings.
  prefs: []
  type: TYPE_NORMAL
- en: Lacking real empathy and the human touch, even under ideal operating conditions,
    chatbot-based services are unlikely to replace human support completely. But that
    doesn’t mean they aren’t extremely valuable, both for organizations and their
    users.
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps the greatest value they bring is when they work in a blended experience,
    where users can receive both human support and access to self-service platforms
    that are interfaced with chatbot technologies. Implemented strategically, these
    systems can vastly improve not only the support offered to end consumers but also
    the internal interactions between an organization’s employees.
  prefs: []
  type: TYPE_NORMAL
- en: '**ChatOps** , for example, is a model increasingly used by modern organizations
    ( https://www.ibm.com/blog/benefits-of-chatops/ ).'
  prefs: []
  type: TYPE_NORMAL
- en: Definition
  prefs: []
  type: TYPE_NORMAL
- en: ChatOps refers to the ability to integrate chat platforms with operational workflows,
    facilitating transparent collaboration among team members, processes, tools, and
    automated bots to enhance service dependability, accelerate recovery, and boost
    collaborative productivity.
  prefs: []
  type: TYPE_NORMAL
- en: Based on the idea of **conversation-driven collaboration** , the ChatOps model
    combines **DevOps** principles ( https://en.wikipedia.org/wiki/DevOps ) by simplifying
    and accelerating interactions between team members using chatbots.
  prefs: []
  type: TYPE_NORMAL
- en: Whether we use them for internal communication or in interactions with our users,
    chatbots can only be useful to the extent that they can solve real problems. This
    depends on how well they can understand the context of the interaction and how
    relevant the answers they provide are.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 8* *.2* provides a visual representation of the ChatOps model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_08_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 – The ChatOps paradigm
  prefs: []
  type: TYPE_NORMAL
- en: If, in the beginning, the main limitation of chatbots came from the *clumsy*
    way of interacting with the user, with the evolution of NLP technologies, the
    main shortcoming has become, more recently, the lack of integration with the organization’s
    knowledge base.
  prefs: []
  type: TYPE_NORMAL
- en: After all, what good is a natural communication experience if the answers given
    by the system aren’t useful in solving the user’s requests?
  prefs: []
  type: TYPE_NORMAL
- en: This brings us to RAG.
  prefs: []
  type: TYPE_NORMAL
- en: By now, I think it has become obvious that without being connected to an organization’s
    knowledge base, a chatbot can, at best, be considered a technology experiment.
    Even conversational engines based on powerful LLMs such as GPT-4 can, at best,
    provide generic answers that don’t always address the specific problems of each
    organization. Perhaps worse, not being anchored in validated documentation, they
    can *hallucinate* very convincingly, creating unpleasant or even potentially dangerous
    experiences.
  prefs: []
  type: TYPE_NORMAL
- en: As you’ve probably guessed by now, LlamaIndex also offers RAG tools for implementing
    chatbot technologies. In this chapter, we will explore the options available to
    us and understand how we can implement very simple systems to advanced chatbot
    mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: But first, let’s see how this functionality is built into LlamaIndex.
  prefs: []
  type: TYPE_NORMAL
- en: Discovering ChatEngine
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous chapters, we saw how we can build a query engine to run queries
    based on our data. This mechanism allows us to integrate multiple types of indexes,
    retrievers, node postprocessors, and response synthesizers at the same time, thus
    being able to access our proprietary data in multiple ways. Unfortunately, the
    `QueryEngine` class does not provide any mechanism to keep the history of a conversation.
    That means each query is a separate interaction and there is no contextual memory
    to allow a true *conversation* .
  prefs: []
  type: TYPE_NORMAL
- en: For that purpose, however, we have **ChatEngine** . Unlike query engines, `ChatEngine`
    allows us to have an actual conversation, giving us both the context of our proprietary
    data and the history of the chat. To simplify this concept even further, imagine
    a `QueryEngine` class with memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'In its simplest form, a chat engine can be initialized just as easily, based
    on an index:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once initialized, a chat engine can be queried using various methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '`chat()` : This method initiates a synchronous chat session, processing the
    user’s message and returning the response immediately.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`achat()` : This method is similar to `chat()` but executes the query asynchronously,
    allowing multiple requests to be processed simultaneously. This can be useful,
    for example, in a web or mobile application where we want to avoid blocking the
    main thread during server queries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stream_chat()` : This method opens a streaming chat session, where responses
    can be returned as they are generated, for more dynamic interaction. This is particularly
    useful for long or complex responses that require significant processing time,
    allowing the user to start seeing parts of the response before all processing
    is complete.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`astream_chat()` : This method is an asynchronous version of `stream_chat()`
    that allows us to handle streaming interactions in an asynchronous context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Another option is to initiate a **Read-Eval-Print** ( **REPL** ) loop with
    `ChatEngine` :'
  prefs: []
  type: TYPE_NORMAL
- en: A REPL chat is akin to a ChatGPT interface, where a user sends a message or
    question, the LLM processes the input, generates a response, and then immediately
    displays it to the user. This loop continues for as long as the user keeps providing
    input, creating an interactive conversation.
  prefs: []
  type: TYPE_NORMAL
- en: 'To reset a chat conversation, you can use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: This is useful when you want to clear the history and begin a new conversation
    thread.
  prefs: []
  type: TYPE_NORMAL
- en: So, the basics are very straightforward. Next, let’s talk about the different
    **built-in chat modes** available in LlamaIndex.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the different chat modes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When initializing a chat engine, we can use the `chat_mode` argument to invoke
    various chat engine types predefined in LlamaIndex. I will show you how each of
    these engines works. We will discuss them one by one and get a good understanding
    of the advantages and use cases best suited for each of them.
  prefs: []
  type: TYPE_NORMAL
- en: But first, let’s have a short introduction to how chat memory is managed within
    LlamaIndex.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how chat memory works
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `ChatMemoryBuffer` class is a specialized memory buffer that’s designed
    to store chat history efficiently while also managing the token limit imposed
    by different LLMs. This structure is important because we can pass it as an argument
    when initializing chat engines using the `memory` parameter. By saving and restoring
    this buffer from one session to another, we can implement persistence for our
    conversations.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two different storage options for the chat store:'
  prefs: []
  type: TYPE_NORMAL
- en: The default `SimpleChatStore` , which stores the conversation in memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The more advanced `RedisChatStore` , which stores the chat history in a Redis
    database, eliminating the need to manually persist and load the chat history
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `chat_store` attribute, which is an instance of the `BaseChatStore` class,
    is used for the actual storage and retrieval of chat messages. This modular approach
    allows different storage implementations, such as a simple in-memory store or
    more complex database-backed stores.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also have the `chat_store_key` parameter, which is used to uniquely identify
    the chat session or conversation within the chat store. This is useful for retrieving
    the correct conversation history when there are multiple conversations stored
    in the same chat store. Here’s a basic example of **conversation history persistence**
    using `SimpleChatStore` :'
  prefs: []
  type: TYPE_NORMAL
- en: 'After importing the necessary libraries, we can try to load the previous conversation.
    If there is no previous conversation save file, we simply initialize an empty
    `chat_store` :'
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s now time to initialize our memory buffer by using `chat_store` as an argument.
    Although not needed here, for a more detailed illustration, we will also customize
    `token_limit` and `chat_store_key` :'
  prefs: []
  type: TYPE_NORMAL
- en: 'OK; we have all the necessary pieces. Let’s put them together into a `SimpleChatEngine`
    class and create a chat loop:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the user types `exit` and we break the loop, we use the `persist()` method
    to store the current conversation for future sessions:'
  prefs: []
  type: TYPE_NORMAL
- en: In case you’re wondering why we haven’t used the `chat_repl()` method shown
    previously and created a chat loop instead, the answer is in the following note.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: While the `chat()` , `achat()` , `stream_chat()` , and `astream_chat()` methods
    can benefit from loading and resuming previous conversations, by design, the `chat_repl()`
    method will reset the conversation history during initialization.
  prefs: []
  type: TYPE_NORMAL
- en: '`ChatMemoryBuffer` also plays an important role in ensuring that the conversation’s
    context remains within the token limits of the model being used. Among other parameters
    available for `ChatMemoryBuffer` , the `token_limit` attribute specifies the maximum
    number of tokens that can be stored in the memory buffer. This limit is essential
    to ensure we stay within the maximum context window size of the current LLM we
    are using.'
  prefs: []
  type: TYPE_NORMAL
- en: When the conversation exceeds the context limit, a sliding window method is
    applied. Older parts of the conversation are truncated to ensure that the most
    recent and relevant parts are retained and processed by the LLM within its token
    constraints.
  prefs: []
  type: TYPE_NORMAL
- en: An analogy to better understand the sliding window method
  prefs: []
  type: TYPE_NORMAL
- en: Imagine a conversation with an LLM as a train journey, where each piece of dialogue
    adds a carriage. However, the train can only be so long due to the tracks’ length
    limit, representing the model’s context window limit. To keep the journey going
    and add new carriages – in our case, messages – older ones need to be detached
    and left behind. This ensures the train can continue its journey, carrying the
    most recent and relevant parts of the conversation, while staying within the limits
    of the track. Just like in a train journey, where we might prioritize which carriages
    to keep based on their importance, the sliding window method prioritizes newer
    conversation parts, keeping the dialogue flowing smoothly.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand how memory works, let’s talk about the different available
    chat modes.
  prefs: []
  type: TYPE_NORMAL
- en: Simple mode
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This is the most **basic chat engine** available. It allows for a simple, direct
    conversation with the LLM, without any connection to our proprietary data. *Figure
    8* *.3* explains this chat mode visually:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_08_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 – SimpleChatEngine
  prefs: []
  type: TYPE_NORMAL
- en: The user’s experience in this mode is defined by the inherent capabilities and
    limitations of the LLM, such as its context window size and overall performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'To initialize this mode, we can use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want, we can customize the LLM using the `llm` argument:'
  prefs: []
  type: TYPE_NORMAL
- en: As you probably won’t be using this mode too much in your RAG designs, let’s
    talk about the more advanced options that are available.
  prefs: []
  type: TYPE_NORMAL
- en: Context mode
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`ContextChatEngine` is designed to enhance chat interactions by leveraging
    our proprietary knowledge. It works by retrieving relevant text from an index
    based on the user’s input, integrating this retrieved information into the system
    prompt to provide context, and then generating a response with the help of the
    LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Have a look at *Figure 8* *.4* for a visual representation of this chat mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_08_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 – ContextChatEngine
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several parameters that we can customize for this chat engine:'
  prefs: []
  type: TYPE_NORMAL
- en: '`retriever` : The actual retriever that’s used to retrieve relevant text from
    the index based on the user’s message. When the chat engine is initialized directly
    from the index, it will use the default retriever for that particular index type'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`llm` : An instance of an LLM, which will be used for generating responses'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`memory` : A `ChatMemoryBuffer` object, which is used to store and manage the
    chat history'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chat_history` : This is an optional list of `ChatMessage` instances representing
    the history of the conversation. It can be used to maintain continuity in a conversation.
    This history includes all messages that have been exchanged in the chat session,
    including both user and chatbot messages. For instance, it can be used to continue
    a conversation from a certain point. A `ChatMessage` object contains three attributes:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`role` : This defaults to *user*'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`content` : The actual message'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Any optional arguments provided via `additional_kwargs`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prefix_messages` : A list of `ChatMessage` instances that may be used as predefined
    messages or prompts before the actual user message. This can be useful for setting
    a particular tone or context for the chat'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`node_postprocessors` : An optional list of `BaseNodePostprocessor` instances
    for further processing the nodes retrieved by the retriever. This can be used
    to implement guardrails, scrub sensitive information from the context, or make
    any other adjustments to the retrieved nodes if required'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`context_template` : A string template that can be used to format the prompt
    that feeds the context to the LLM'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`callback_manager` : An optional `CallbackManager` instance for managing callbacks
    during the chat process. This is useful for tracing and debugging purposes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`system_prompt` : An optional string that’s used as a system prompt, providing
    initial context or instructions for the chatbot'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`service_context` : An optional `ServiceContext` instance, which can be used
    to make additional customizations to the chat engine'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To implement `ContextChatEngine` , we must load our data and build an index,
    then optionally configure the chat engine with different parameters as needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a quick example based on our sample data files, which can be found in
    the `ch8/files` subfolder in this book’s GitHub repository:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we initialized `chat_engine` from the index. Alternatively,
    we could have defined it standalone, providing a retriever as an argument, like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: Overall, this chat mode is particularly effective for queries that relate to
    the knowledge contained within our data, supporting both general conversations
    and more specific discussions based on the indexed content.
  prefs: []
  type: TYPE_NORMAL
- en: Because the engine first retrieves context from the index and uses it to generate
    responses, this approach makes the chat experience a lot more useful and natural
    for users seeking specific information from the indexed data.
  prefs: []
  type: TYPE_NORMAL
- en: Condense question mode
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`CondenseQuestionChatEngine` streamlines the user interaction by first **condensing
    the conversation** and the latest user message into a standalone question with
    the help of the LLM. This standalone question, which tries to capture the essential
    elements of the conversation, is then sent to a query engine built on our proprietary
    data to generate a response.'
  prefs: []
  type: TYPE_NORMAL
- en: The main benefit of using this approach is that it maintains the conversation
    focused on the topic, preserving the essential points of the entire dialogue throughout
    every interaction. And it always responds in the context of our proprietary data.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 8* *.5* describes the operation of this particular chat mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_08_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.5 – CondenseQuestionChatEngine
  prefs: []
  type: TYPE_NORMAL
- en: The fact that the final response comes from our retrieved proprietary data and
    not directly from the LLM can also be a disadvantage sometimes. This chat mode
    may struggle with more general questions, such as inquiries about previous interactions,
    due to its reliance on querying the knowledge base for every response.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at some of the key parameters of `CondenseQuestionChatEngine` :'
  prefs: []
  type: TYPE_NORMAL
- en: '`query_engine` : This is a `BaseQueryEngine` instance that’s used to query
    the condensed question. Any type of query engine may be used here, including complex
    constructs with routing functionality'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`condense_question_prompt` : This is a `BasePromptTemplate` instance that’s
    used for condensing the conversation and user message into a single, standalone
    question'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Memory` : A `ChatMemoryBuffer` instance that’s used to manage and store the
    chat history'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`llm` : A language model instance for generating the condensed question'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`verbose` : A Boolean flag for printing verbose logs during operation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`callback_manager` : An optional `CallbackManager` instance for managing callbacks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To implement this chat engine, we typically initialize it with a query engine
    and, optionally, configure it with custom parameters. The conversation is condensed
    into a question using a predefined template that can be customized using the `condense_question_prompt`
    parameter. The resulting question is then sent to the query engine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a brief implementation example:'
  prefs: []
  type: TYPE_NORMAL
- en: In the first part of the code, we ingested our sample files, created an index,
    and then created a simple query engine. Next, we introduced a previous conversation
    context by creating a chat history consisting of two `ChatMessage` objects. Specifically,
    we instructed the chat engine not to consider the Pantheon as a famous building.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s create our chat engine and query it:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see what happened in the background:'
  prefs: []
  type: TYPE_NORMAL
- en: '`CondenseQuestionChatEngine` took the user’s message, along with the provided
    chat history, and condensed them into a standalone question. This process involved
    using the LLM and `condense_question_prompt` to generate a question that encapsulates
    the essence of the conversation context and the user’s latest query.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, the engine forwarded this condensed question to the query engine, which
    searched the indexed data for relevant information.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The query engine, having access to the information from `VectorStoreIndex` ,
    processed the question and returned an answer. This answer reflects the collective
    context of the previous conversation and the specific query about famous structures
    in ancient Rome.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Without the added chat history, the output of the sample would have been similar
    to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: This is because the two buildings are explicitly mentioned in our sample data.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, once we add the new conversational context, the output looks like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another way of initializing this chat engine would be directly from the index,
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: This chat mode is particularly useful for complex conversations where the context
    and nuances of previous exchanges play a crucial role in understanding and accurately
    responding to the latest query. It ensures that the chatbot remains aware of the
    conversation’s history, thus making the interaction more coherent and contextually
    relevant.
  prefs: []
  type: TYPE_NORMAL
- en: The next chat mode we’ll talk about uses a mix of two other approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Condense and context mode
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`CondensePlusContextChatEngine` offers an even more comprehensive chat interaction
    by combining the benefits of condensed questions and context retrieval.'
  prefs: []
  type: TYPE_NORMAL
- en: 'While the previous chat engine we discussed is more straightforward and focuses
    on simplifying the conversation into a question for response generation, `CondensePlusContextChatEngine`
    takes an extra step to enrich the conversation with additional context from the
    indexed data, leading to more detailed and context-aware responses. The trade-off
    here is an increase in response generation time due to the additional step performed.
    Let’s explore how it works under the hood by looking at *Figure 8* *.6* :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_08_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.6 – CondensePlusContextChatEngine
  prefs: []
  type: TYPE_NORMAL
- en: First, this engine condenses a conversation and the latest user message into
    a standalone question. Then, it retrieves relevant context from the index using
    this condensed question. Finally, it uses both the retrieved context and the condensed
    question to generate a response with the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some of the key parameters of `CondensePlusContextChatEngine` :'
  prefs: []
  type: TYPE_NORMAL
- en: '`retriever` : Used to fetch context based on the condensed question'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`llm` : The LLM that’s used to generate the condensed question and the final
    response'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`memory` : A `ChatMemoryBuffer` instance for storing and managing chat history'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`context_prompt` : A prompt template for formatting the context in the system
    prompt'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`condense_prompt` : A prompt for condensing the conversation into a standalone
    question'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`system_prompt` : A prompt with instructions for the chatbot'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`skip_condense` : A Boolean flag to bypass the condensation step if desired'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`node_postprocessors` : An optional list of `BaseNodePostprocessors` for additional
    processing of retrieved nodes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`callback_manager` : As usual, this can be used for managing callbacks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`verbose` : A Boolean flag for enabling verbose logging during operation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To build this particular chat engine from an index, we can use the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: This chat mode is ideal in scenarios where both the context of the conversation
    and specific information from the indexed data are crucial for generating accurate
    and relevant responses. It enhances the chat experience by ensuring the responses
    are both contextually relevant and enriched with specific details from the indexed
    content.
  prefs: []
  type: TYPE_NORMAL
- en: OK. It’s time to discover the more advanced chat modes.
  prefs: []
  type: TYPE_NORMAL
- en: <title>Implementing agentic strategies in our apps</title>
  prefs: []
  type: TYPE_NORMAL
- en: Implementing agentic strategies in our apps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'from llama_index.core.tools import FunctionTool def calculate_average(*values):
        """     Calculates the average of the provided values.     """     return
    sum(values) / len(values) average_tool = FunctionTool.from_defaults(     fn=calculate_average)
    pip install llama-index-tools-database from llama_index.tools.database import
    DatabaseToolSpec db_tools = DatabaseToolSpec(<db_specific_configuration>) tool_list
    = db_tools.to_tool_list() pip install llama-index-agent-openai from llama_index.tools.database
    import DatabaseToolSpec from llama_index.core.tools import FunctionTool from llama_index.agent.openai
    import OpenAIAgent from llama_index.llms.openai import OpenAI def write_text_to_file(text,
    filename):     """     Writes the text to a file with the specified filename.
        Args:         text (str): The text to be written to the file.         filename
    (str): File name to write the text into.     Returns: None     """     with open(filename,
    ''w'') as file:         file.write(text) save_tool = FunctionTool.from_defaults(fn=write_text_to_file)
    db_tools = DatabaseToolSpec(uri="sqlite:///files//database//employees.db") tools
    = [save_tool]+db_tools.to_tool_list() llm = OpenAI(model="gpt-4") agent = OpenAIAgent.from_tools(
        tools=tools,     llm=llm,     verbose=True,     max_function_calls=20 ) response
    = agent.chat(     "For each IT department employee with a salary lower "     "than
    the average organization salary, write an email,"     "announcing a 10% raise
    and then save all emails into "     "a file called ''emails.txt''") print(response)
    from llama_index.agent.react import ReActAgent agent = ReActAgent.from_tools(tools)
    from llama_index.core.tools.tool_spec.load_and_search.base import (     LoadAndSearchToolSpec)
    from llama_index.tools.database import DatabaseToolSpec from llama_index.agent.openai
    import OpenAIAgent from llama_index.llms.openai import OpenAI db_tools = DatabaseToolSpec(
        uri="sqlite:///files//database//employees.db") tool_list = db_tools.to_tool_list()
    tools=LoadAndSearchToolSpec.from_defaults( tool_list[0] ).to_tool_list() llm =
    OpenAI(model="gpt-4") agent = OpenAIAgent.from_tools(     tools=tools,     llm=llm,
        verbose=True ) response = agent.chat(     "Who has the highest salary in the
    Employees table?''") print(response) pip install llama-index-readers-wikipedia
    from llama_index.agent.openai import OpenAIAgent from llama_index.core.tools.ondemand_loader_tool
    import(     OnDemandLoaderTool) from llama_index.readers.wikipedia import WikipediaReader
    tool = OnDemandLoaderTool.from_defaults(     WikipediaReader(),     name="WikipediaReader",
        description="args: {''pages'': [<list of pages>],         ''query_str'': <query>}"
    ) agent = OpenAIAgent.from_tools(     tools=[tool],     verbose=True ) response
    = agent.chat(     "What were some famous buildings in ancient Rome?") print(response)
    pip install llama-index-packs-agents-llm-compiler from llama_index.tools.database
    import DatabaseToolSpec from llama_index.packs.agents_llm_compiler import LLMCompilerAgentPack
    db_tools = DatabaseToolSpec(     uri="sqlite:///files//database//employees.db")
    agent = LLMCompilerAgentPack(db_tools.to_tool_list()) response = agent.run(     "Using
    only the available tools, "     "List the HR department employee "     "with the
    highest salary " ) from llama_index.core.agent import AgentRunner from llama_index.agent.openai
    import OpenAIAgentWorker from llama_index.tools.database import DatabaseToolSpec
    db_tools = DatabaseToolSpec(     uri="sqlite:///files//database//employees.db"
    ) tools = db_tools.to_tool_list() step_engine = OpenAIAgentWorker.from_tools(
        tools,     verbose=True ) agent = AgentRunner(step_engine) input =  (     "Find
    the highest paid HR employee and write "     "them an email announcing a bonus"
    ) response = agent.chat(input) print(response) task = agent.create_task(input)
    step_output = agent.run_step(task.task_id) while not step_output.is_last:     step_output
    = agent.run_step(task.task_id) response = agent.finalize_response(task.task_id)
    print(response)'
  prefs: []
  type: TYPE_NORMAL
- en: '*The name is Bot.* *Chat Bot* .'
  prefs: []
  type: TYPE_NORMAL
- en: At the beginning of this chapter, we talked about the growing popularity of
    the ChatOps model. This model is based on the interaction between groups of human
    operators and AI agents, who can understand the context of discussions to provide
    answers to questions but also to perform certain functions, thus playing the role
    of virtual assistants for the group they serve.
  prefs: []
  type: TYPE_NORMAL
- en: You probably realize, however, that the chat engine models we have discussed
    so far can only answer questions and cannot execute functions or interact in ways
    other than read-only with backend data.
  prefs: []
  type: TYPE_NORMAL
- en: For these use cases, we need **agents** .
  prefs: []
  type: TYPE_NORMAL
- en: The major difference between an agent and a simple chat engine is that an agent
    operates based on a **reasoning loop** and has several tools at its disposal.
    After all, who would be Bond without the gadgets that Q always provides?
  prefs: []
  type: TYPE_NORMAL
- en: Unlike a simple chatbot, which can – at best – answer questions, either directly
    with the help of an LLM or by extracting proprietary data from a knowledge base,
    agents are much more powerful and can handle far more complex scenarios. This
    gives them a lot more utility in a business context, where human interactions
    augmented by AI are becoming increasingly prevalent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s understand the core components of an agent: the tools and the reasoning
    loop.'
  prefs: []
  type: TYPE_NORMAL
- en: Building tools and ToolSpec classes for our agents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We briefly discussed tools in *Chapter 6* , *Querying Our Data, Part 1 – Context
    Retrieval.* However, because the main topic of *Chapter 6* was data querying,
    I only showed you how different query engines or retrievers can be wrapped in
    tools and then become components of a router. In many ways, you can think of a
    router as a very simple type of agent. It uses LLM reasoning to decide which query
    engine or retriever should be used, depending on their specified purpose and the
    actual user query.
  prefs: []
  type: TYPE_NORMAL
- en: But tools can be a lot more useful.
  prefs: []
  type: TYPE_NORMAL
- en: 'A tool can also be a wrapper for any kind of user-defined function, capable
    of reading or writing data, calling functions from external APIs, or executing
    any kind of code. This means that tools come in two different flavors:'
  prefs: []
  type: TYPE_NORMAL
- en: '`QueryEngineTool` : This can encapsulate any existing query engine. This is
    the kind we covered during *Chapter 6* and it can only provide read-only access
    to our data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FunctionTool` : This enables any user-defined function to be transformed into
    a tool. This is a universal type of tool as it allows any type of operation to
    be executed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because we have already seen examples of how `QueryEngineTool` works, let’s
    focus on `FunctionTool` instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example of how we can define one:'
  prefs: []
  type: TYPE_NORMAL
- en: To enable agents to assimilate our functions as tools, they must contain descriptive
    docstrings, just like in the previous example. LlamaIndex relies on these **docstrings**
    to provide agents with an *understanding* of the purpose and proper usage of a
    particular tool wrapping a user-defined function.
  prefs: []
  type: TYPE_NORMAL
- en: Definition
  prefs: []
  type: TYPE_NORMAL
- en: In Python, a docstring is a string literal that occurs as the first statement
    in a module, function, class, or method definition. It is used to document the
    purpose and usage of the code block it describes. Docstrings can be accessed from
    the code at runtime using the `__doc__` attribute on the object they describe,
    and they are also the primary way that documentation is generated in Python.
  prefs: []
  type: TYPE_NORMAL
- en: This description will be used by the reasoning loop of an agent to determine
    which particular tool is fit for solving a specific task, allowing the agent to
    decide the execution path.
  prefs: []
  type: TYPE_NORMAL
- en: However, competent agents are usually able to handle more than just one tool.
  prefs: []
  type: TYPE_NORMAL
- en: For this purpose, LlamaIndex also provides the `ToolSpec` class. Akin to a collection
    of individual tools, `ToolSpec` specifies a full set of tools for a particular
    service. It’s like equipping our agent with a complete API for a particular type
    of technology.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can build custom `ToolSpec` classes but there is also a growing number of
    them already available on LlamaHub: https://llamahub.ai/?tab=tools . They cover
    different types of service integrations, such as Gmail, Slack, SalesForce, Shopify,
    and many others.'
  prefs: []
  type: TYPE_NORMAL
- en: The LlamaHub agent tool repository
  prefs: []
  type: TYPE_NORMAL
- en: The LlamaHub agent tool repository is a key addition to LlamaHub, providing
    a curated collection of tool specs that enable agents to interact with and extend
    the functionality of a range of services. This repository simplifies the agent
    design process for various APIs and includes numerous practical examples in its
    notebooks for easy integration and use.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take the `DatabaseToolSpec` class available on LlamaHub as an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'This `ToolSpec` class can be found here: https://llamahub.ai/l/tools-database?from=tools
    . First, let’s have a look at *Figure 8* *.7* to understand its structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_08_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.7 – DatabaseToolSpec
  prefs: []
  type: TYPE_NORMAL
- en: 'Built on top of the SQLAlchemy library ( https://www.sqlalchemy.org/ ) this
    tool collection can access many types of databases while providing three simple
    tools:'
  prefs: []
  type: TYPE_NORMAL
- en: '`list_tables` : A tool that lists the tables in the database schema'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`describe_tables` : A tool that describes the schema of a table'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`load_data` : A tool that accepts a SQL query as input and returns the resulting
    data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quick note
  prefs: []
  type: TYPE_NORMAL
- en: SQLAlchemy is a powerful and versatile toolkit for Python that allows developers
    to work with various databases, such as Microsoft SQL Server, OracleDB, MySQL,
    and others, in a more Pythonic way, abstracting away many of the complexities
    of database interaction and query construction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because this is not a LlamaIndex core component but comes as an integration
    package instead, it must be installed in our environment first:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, to initialize this `ToolSpec` , all we have to do is import it:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we must configure our database access, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Once the `ToolSpec` class has been built, if we want to initialize an agent
    with it, we have to convert it into a list of tools using the `to_tool_list()`
    method. This is because agents expect a list of tools as an argument.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how we can easily convert the `ToolSpec` class into a list of tool objects:'
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we can pass `tool_list` as an argument when initializing any
    type of agent. Our agent will now be capable of *understanding* the schema of
    the database and extracting any required information from its tables. You can
    find a full example of how to use this `ToolSpec` class later in this chapter
    in the *OpenAIAgent* section. Next, let’s see how reasoning loops work.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding reasoning loops
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Having so many specialized tools already available for our agents is a great
    advantage. But unfortunately, a box full of some of the best-quality instruments
    is not always enough. Our agents also need to know *when* to use each of these
    tools.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, the RAG applications we build need to decide – as autonomously
    as possible – which tool to use, depending on the specific user query and the
    dataset they are operating on. Any hard-coded solution will only deliver good
    results in a limited number of scenarios. This is where reasoning loops come in.
  prefs: []
  type: TYPE_NORMAL
- en: The reasoning loop is a fundamental aspect of agents, enabling them to intelligently
    decide which tools to use in different scenarios. This aspect is important because,
    in complex, real-world applications, the requirements can vary significantly and
    a static approach would limit the agent’s effectiveness.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 8* *.8* presents a visual representation of the reasoning loop concept:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_08_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.8 – The reasoning loop in an agent
  prefs: []
  type: TYPE_NORMAL
- en: The reasoning loop is responsible for the decision-making process. It evaluates
    the context, understands the requirements of the task at hand, and then selects
    the appropriate tools from its arsenal to accomplish the task. This dynamic approach
    allows agents to adapt to various scenarios, making them versatile and efficient.
  prefs: []
  type: TYPE_NORMAL
- en: In LlamaIndex, the implementation of the reasoning loop is tailored to the type
    of agent. For instance, `OpenAIAgent` uses the Function API to make decisions,
    while `ReActAgent` relies on chat or text completion endpoints for its reasoning
    process.
  prefs: []
  type: TYPE_NORMAL
- en: This loop is not just about selecting the right tool, though; it’s also about
    determining the sequence in which the tools should be used and the specific parameters
    that should be applied. It’s the brain of the agent, orchestrating the tools to
    work together seamlessly, much like a skilled craftsman uses a combination of
    tools to create something greater than the sum of its parts.
  prefs: []
  type: TYPE_NORMAL
- en: This ability to intelligently interact with various tools and data sources,
    and read and modify data dynamically, sets agents apart from simpler chat engines
    and makes them invaluable in a business context where adaptability and intelligence
    are key.
  prefs: []
  type: TYPE_NORMAL
- en: The remaining types of chat modes that I’m going to describe over the next few
    pages are not simple chat engines but agents at their core. They all operate using
    a list of tools but implement the reasoning loop in different ways.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAIAgent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This specialized agent leverages the capabilities of OpenAI models, particularly
    those supporting the function calling API. It works with OpenAI models that have
    been designed to support the function calling API. They can interpret and execute
    function calls as part of their capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Quick note
  prefs: []
  type: TYPE_NORMAL
- en: 'These models are designed to interpret prompts and context to determine when
    a function call is appropriate. They respond with outputs that adhere to the defined
    structure of the function, based on the patterns they’ve learned during training.
    For more information on this topic and a list of supported models, you may consult
    the official OpenAI documentation: https://platform.openai.com/docs/guides/function-calling
    .'
  prefs: []
  type: TYPE_NORMAL
- en: The key advantage of this agent type is that the tool selection logic is implemented
    directly on the model itself. When a task is provided by the user to **OpenAIAgent**
    , along with any previous chat history, the function API will analyze the context
    and decide whether another tool needs to be invoked or if a final response can
    be returned. If it determines that another tool is required, the function API
    will output the name of that tool. `OpenAIAgent` will then execute the tool, passing
    the tool’s response back into the chat history. This cycle continues until the
    API returns a final message, indicating the reasoning loop is complete.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 8* *.9* explains this process visually:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_08_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.9 – The simplified workflow of OpenAIAgent
  prefs: []
  type: TYPE_NORMAL
- en: With the model handling the complex logic of tool selection and chaining, `OpenAIAgent`
    is a great solution for tool orchestration. One tradeoff is less flexibility compared
    to other architectures as the tool selection logic is hard-coded into the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: However, for many use cases, the pre-trained capabilities of the function API
    model are sufficient to enable effective tool orchestration and task completion.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before proceeding to the next example, make sure you install the required integration
    package:'
  prefs: []
  type: TYPE_NORMAL
- en: To implement OpenAIAgent, we must define the available tools and then initialize
    the agent with these components, adding any other custom parameters we desire.
    The best way to explain how they work is through an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the following example, we are using an SQLite database containing a single
    table called *Employees* . This table contains some randomly chosen salary data
    for 10 employees from different departments. *Table 8.1* displays the contents
    of the *Employees* table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **ID** | **Name** | **Department** | **Salary** | **Email** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | Alice | IT | 36420.77 | Alice_IT@org.com |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Karen | Finance | 57705.06 | Alice_Finance@org.com |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | Helen | IT | 52612.51 | Helen_IT@org.com |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | Jackie | Finance | 61374.58 | Jack_Finance@org.com |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | David | Finance | 32242.72 | David_Finance@org.com |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | Cora | HR | 62040.53 | Alice_HR@org.com |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | Ingrid | IT | 70821.96 | Alice_IT@org.com |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | Jack | IT | 57268.89 | Jack_IT@org.com |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | Bob | Finance | 76868.23 | Bob_Finance@org.com |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | Bill | HR | 74161.45 | Bob_HR@org.com |'
  prefs: []
  type: TYPE_TB
- en: Table 8.1 – The sample Employees table from the Employees.db file
  prefs: []
  type: TYPE_NORMAL
- en: 'The database file itself can be found in the `ch8/files/database` subfolder
    of this book’s GitHub repository. Let’s have a look at the code:'
  prefs: []
  type: TYPE_NORMAL
- en: The first part is responsible for the imports.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, it’s time to define a simple function that’s going to become a custom
    tool for our agent. This simple tool will allow us to save files in the local
    folder. Notice the detailed docstring that we are providing to the agent:'
  prefs: []
  type: TYPE_NORMAL
- en: Once the function has been defined, we must wrap it into a new tool called `save_tool`
    .
  prefs: []
  type: TYPE_NORMAL
- en: 'We also initialize an entire `ToolSpec` class from the imported `DatabaseToolSpec`
    . We need these tools because the agent will have to read data from our SQLite
    database to solve the task:'
  prefs: []
  type: TYPE_NORMAL
- en: Once we’ve created `db_tools` , we must join it with `save_tool` and put them
    into a single list called `tools` . We’ll use this list as an argument for initializing
    the agent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s build our agent. Notice that we’re not using the default LLM in
    this case; instead, we’re configuring our agent to use GPT-4 for more accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding code, we initialized our agent using the list of tools we prepared.
    The `verbose` argument will make the agent display every execution step for better
    visibility of the reasoning process. We also set `max_function_calls` to a larger
    value because, for complex tasks, the default value may not be enough to allow
    the agent to complete the task.
  prefs: []
  type: TYPE_NORMAL
- en: A quick note on the max_function_calls parameter
  prefs: []
  type: TYPE_NORMAL
- en: It may be tempting to simply set this to a very large value to avoid exhausting
    the function calls and increase the chances for the agent to solve the task. Keep
    in mind, however, that every function call incurs costs, and sometimes, agents
    have the bad habit of entering infinite loops. I call them *rogue agents* when
    they do that. Chances are that if your agent implementation requires a lot of
    LLM calls to solve even simple tasks, you’re probably doing something wrong when
    defining or describing the underlying tools.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s continue with our code. It’s time to dispatch the task to our agent:'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the task we provided is relatively complex. Multiple steps will
    be required to solve it. As we are not providing too many details in the query,
    our agent will have to figure out the structure of the database and then craft
    a SQL query to extract the average salary in the organization and the list of
    employees from the IT department who are paid below the average.
  prefs: []
  type: TYPE_NORMAL
- en: Since the `verbose` argument is set to `True` , running this sample will show
    you the entire reasoning logic and steps performed by the agent.
  prefs: []
  type: TYPE_NORMAL
- en: Notice how, in each step, the agent incorporates outputs from the tools into
    its ongoing reasoning process. Once it has the list of employees, it will compose
    an email for each one. The final step of the task is to use our custom-created
    tool and save the results in a local file.
  prefs: []
  type: TYPE_NORMAL
- en: This is just a simple example. In a more complex implementation, instead of
    saving the text locally, for example, we could import `GmailToolSpec` from LlamaHub
    and create email drafts that can be manually reviewed later and sent by the user.
    Unfortunately, that would have made the example much longer as `GmailToolSpec`
    requires stored credentials for the Google API, but I leave it to you to experiment
    with that `ToolSpec` class ( https://llamahub.ai/l/tools-gmail?from=tools ) and
    all the other tools available on LlamaHub.
  prefs: []
  type: TYPE_NORMAL
- en: 'The customizable parameters of `OpenAIAgent` are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`tools` : A list of `BaseTool` instances that the agent can utilize during
    the chat session. These tools can range from specialized query engines to custom
    processing modules or collections of tools extracted from `ToolSpec` classes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`llm` : Any OpenAI model that supports the function calling API. The default
    model that’s used is `gpt-3.5-turbo-0613`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`memory` : Just like with any chat engine, this is a `ChatMemoryBuffer` instance
    that can be used for storing and managing the chat history'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prefix_messages` : A list of `ChatMessage` instances that serve as pre-configured
    messages or prompts at the start of the chat session'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_function_calls` : The maximum number of function calls that can be made
    to the OpenAI model during a single chat interaction. The default is 5'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`default_tool_choice` : A string indicating the default choice of tool to be
    used when multiple tools are available. This is useful for coercing the agent
    into using a specific tool'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`callback_manager` : An optional `CallbackManager` instance for managing callbacks
    during the chat process, aiding in tracing, and debugging'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`system_prompt` : An optional initial system prompt that provides context or
    instructions for the agent'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`verbose` : A Boolean flag to enable detailed logging during operation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall, `OpenAIAgent` stands out from other chat engines due to its ability
    to execute complex function calls, on top of contextually rich conversations.
    This makes it particularly suitable for scenarios where advanced functionalities,
    such as integrating external tools or processing user queries in more sophisticated
    ways, are required. `OpenAIAgent` provides a versatile and powerful platform for
    creating engaging and intelligent chat experiences.
  prefs: []
  type: TYPE_NORMAL
- en: But wait – there are other types of agents too.
  prefs: []
  type: TYPE_NORMAL
- en: ReActAgent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In contrast to `OpenAIAgent` , **ReActAgent** uses more generic text completion
    endpoints that can be driven by any LLM. It operates based on a **ReAct** loop
    within a chat mode built on top of a set of tools.
  prefs: []
  type: TYPE_NORMAL
- en: This loop involves deciding whether to use any of the available tools, potentially
    using it and observing its output, and then deciding whether to repeat the process
    or provide a final response. This flexibility allows it to choose between using
    tools or relying solely on the LLM. However, this also means that its performance
    is heavily dependent on the quality of the LLM, often requiring more nuanced prompting
    to ensure accurate knowledge base queries, rather than relying on potentially
    inaccurate model-generated responses.
  prefs: []
  type: TYPE_NORMAL
- en: 'The input prompt for `ReActAgent` is carefully designed to guide the model
    in tool selection, using a format inspired by the ReAct paper by Yao, S., et al.
    (2022), *ReAct: Synergizing Reasoning and Acting in Language* *Models* ( https://arxiv.org/abs/2210.03629
    ).'
  prefs: []
  type: TYPE_NORMAL
- en: It presents a list of available tools and asks the model to select one and provide
    the required parameters in JSON format. This explicit prompt is critical to the
    agent’s decision-making process. After selecting a tool, the agent executes it
    and integrates the response into the chat history. This cycle of prompting, execution,
    and response integration continues until a satisfactory response is achieved.
    For an overall visual representation of the workflow, you may review the diagram
    that was presented for `OpenAIAgent` in *Figure 8* *.9* .
  prefs: []
  type: TYPE_NORMAL
- en: Unlike `OpenAIAgent` , which uses a function calling API with a model capable
    of selecting and chaining together multiple tools, the `ReActAgent` class’s logic
    must be fully encoded through its prompts.
  prefs: []
  type: TYPE_NORMAL
- en: '`ReActAgent` uses a predefined loop with a maximum number of iterations, along
    with strategic prompting, to mimic a reasoning loop. Nevertheless, with strategic
    prompt engineering, `ReActAgent` can achieve effective tool orchestration and
    chained execution, similar to the output of the OpenAI Function API.'
  prefs: []
  type: TYPE_NORMAL
- en: The key difference is that whereas the logic of the OpenAI Function API is embedded
    in the model, `ReActAgent` relies on the structure of its prompts to induce the
    desired tool selection behavior. This approach offers considerable flexibility
    as it can adapt to different language model backends, allowing for different implementations
    and applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, we have the usual customizable parameters that we discussed for
    `OpenAIAgent` : `tools` , `llm` , `memory` , `callback_manager` , and `verbose`
    .'
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, `ReActAgent` comes with a few specific parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`max_iterations` : Similar to `max_function_calls` , this parameter sets the
    maximum number of iterations the ReAct loop can execute. This limit ensures that
    the agent does not enter an endless loop of processing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`react_chat_formatter` : This formats the chat history into a structured list
    of `ChatMessages` , alternating between user and assistant roles, based on the
    provided tools, chat history, and reasoning steps. This helps maintain clarity
    and consistency in the reasoning loop'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_parser` : An optional instance of the `ReActOutputParser` class. This
    parser processes the outputs generated by the agent, helping in interpreting,
    and formatting them appropriately'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tool_retriever` : An optional instance of `ObjectRetriever` for `BaseTool`
    . This retriever can be used to dynamically fetch tools based on certain criteria.
    Similar to how we index nodes, there is also an option to create an `ObjectIndex`
    index to index a set of tools. This can be especially useful when we have to work
    with a large number of tools. You can find more information about this feature
    in the official documentation: https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/usage_pattern.html#function-retrieval-agents'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`context` : An optional string providing initial instructions for the agent'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Initializing and using `ReActAgent` is done the same as with the OpenAI one,
    except this time, you won’t need to install any integration packages first – this
    type of agent is part of the core LlamaIndex components:'
  prefs: []
  type: TYPE_NORMAL
- en: Overall, `ReActAgent` stands out for its flexibility as it can use any LLM to
    drive its unique ReAct loop, enabling it to smartly choose and use various tools.
    It’s like having a virtual assistant that not only answers questions but also
    intelligently decides when to consult external sources, making the conversation
    more contextually relevant and improving the user experience.
  prefs: []
  type: TYPE_NORMAL
- en: How do we interact with agents?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are two main methods that we can use to interact with an agent: `chat()`
    and `query()` . The first method utilizes stored conversation history to provide
    context-informed responses, making it suitable for ongoing dialogues.'
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the former method operates in a stateless mode, treating
    each call independently without reference to past interactions. This is better
    suited for standalone requests.
  prefs: []
  type: TYPE_NORMAL
- en: Enhancing our agents with the help of utility tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To improve the capabilities of the existing tools, LlamaIndex also provides
    two very useful so-called *utility tools* – `OnDemandLoaderTool` and `LoadAndSearchToolSpec`
    . They are universal and can be used with any type of agent to augment the standard
    tool functionality in certain scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: One common issue when interacting with an API is that we might receive a very
    long response in return. Our agents may not always be able to handle such large
    outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Problems may arise because they may overflow the context window of the LLM or
    sometimes, key context may be diluted by a large amount of data, decreasing the
    accuracy of the agent’s reasoning logic.
  prefs: []
  type: TYPE_NORMAL
- en: 'A good way to understand this issue is by looking at our previous example for
    `OpenAIAgent` . In that case, we used a collection of tools called `DatabaseToolSpec`
    to retrieve data from our sample *Employees* table. If you’ve run that particular
    agent with the `Verbose` parameter set to `True` , then you’ve probably noticed
    that the outputs produced by the `load_data` tool are in the form of LlamaIndex
    document objects, as we can see in *Figure 8* *.10* :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_08_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.10 – Sample output for the OpenAIAgent code example
  prefs: []
  type: TYPE_NORMAL
- en: This means that whenever the agent calls the `load_data` tool, using a SQL query
    to interrogate the database, instead of simply receiving the output of the query,
    it gets a whole document in return – along with a bunch of additional data, such
    as the ID of the document, metadata fields, hashes, and so on. The agent has to
    extract the actual query results from that data using the LLM, hence the aforementioned
    potential issues.
  prefs: []
  type: TYPE_NORMAL
- en: So, what if we want to extract *only* the result of the query, without all the
    additional data on top of it? That is the job of `LoadAndSearchToolSpec` .
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the LoadAndSearchToolSpec utility
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This utility tool is designed to help the agent handle large volumes of data
    from API endpoints, as demonstrated in *Figure 8* *.11* :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_08_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.11 – Visualization of a direct API call versus interaction via LoadAndSearchToolSpec
  prefs: []
  type: TYPE_NORMAL
- en: 'It takes an existing tool and generates two separate tools: one for loading
    and indexing data – by default, using a vector index – and another for conducting
    searches on this indexed data. The agent will now use the *Load* tool to ingest
    the data, and, similar to a caching mechanism, it will store it in an index. In
    the next step, the agent will use the *Search* tool to extract only the needed
    information using a built-in query engine.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how that translates into code. We will adapt the previous `OpenAIAgent`
    example so that it uses `LoadAndSearchToolSpec` :'
  prefs: []
  type: TYPE_NORMAL
- en: Once we finished with the imports, we initialized our `DatabaseToolSpec` utility,
    which points to the same sample SQLite database as in the previous example. However,
    this time, we didn’t add any additional tools since we’ll only run a simple query.
    For that reason, we only pass the first tool from `ToolSpec` – that is, `tool_list[0]`
    – as an argument to `LoadAndSearchToolSpec` . That’s the `load_data` function,
    by the way. We don’t need the other two functions available in the database’s
    `ToolSpec` this time.
  prefs: []
  type: TYPE_NORMAL
- en: 'From this point on, the code is very much straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you look at the output – presented in *Figure 8* *.12* – you’ll notice the
    reduced amount of data the agent has to deal with this time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_08_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.12 – Sample agent output when LoadAndSearchToolSpec is used
  prefs: []
  type: TYPE_NORMAL
- en: Instead of receiving an entire document as a response, the first call returns
    just a confirmation message that the data has been loaded and indexed, while the
    second extracts the final response using a query. We’ll talk about another utility
    tool next.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding OnDemandLoaderTool
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another important utility is `OnDemandLoaderTool` . This utility is designed
    to make the process of loading, indexing, and querying data seamless and efficient
    within an agent’s workflow, particularly when dealing with large volumes of data
    from various sources.
  prefs: []
  type: TYPE_NORMAL
- en: It simplifies the process of using data loaders for agents by allowing them
    to trigger the loading, indexing, and querying of data through a single tool call.
  prefs: []
  type: TYPE_NORMAL
- en: The normal approach in a RAG workflow would be to ingest all data at the start
    of our application, then chunk it, index it, and build a query engine on it. But
    that may not always be the most efficient method.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say we have a large number of data sources. Ingesting and indexing all
    of them during startup would take a very long time, negatively affecting the user
    experience. And what if the user asks a question that cannot be answered by the
    agent based on the ingested data sources alone? That’s where a feature like this
    becomes useful.
  prefs: []
  type: TYPE_NORMAL
- en: '`OnDemandLoaderTool` is especially useful in scenarios where data requirements
    are dynamic and unpredictable. Instead of pre-loading a vast amount of data at
    startup, which may not all be relevant to the user’s current needs, this tool
    enables an agent to fetch, index, and query data on demand. This approach significantly
    enhances efficiency as it allows the agent to focus only on the relevant data
    at any given time, rather than handling large datasets that may not be immediately
    necessary.'
  prefs: []
  type: TYPE_NORMAL
- en: 'How does it work? It takes any existing data loader and wraps it into a tool
    that can be used by the agent as required. Before running the code, make sure
    you install the Wikipedia integration package:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the sample code. We’ll start with the imports:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s define an on-demand tool for our agent, based on `WikipediaReader`
    :'
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice how I provided usage instructions in the description argument. These
    should help the agent better *understand* how to properly use the tool, although
    it might still take a few tries to get it right. Now, it’s time to initialize
    the agent:'
  prefs: []
  type: TYPE_NORMAL
- en: Important side note
  prefs: []
  type: TYPE_NORMAL
- en: One big advantage of using this approach is that once data has been loaded into
    the index, it’s also cached. Therefore, subsequent queries on the same topic will
    run faster.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, `OnDemandLoaderTool` can be chained together with other, regular
    tools, allowing the agent to handle more complex scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: With that, we’ve covered the basics. Now, let’s have a look at more advanced
    types of agents.
  prefs: []
  type: TYPE_NORMAL
- en: Using the LLMCompiler agent for more advanced scenarios
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I saved the best for last.
  prefs: []
  type: TYPE_NORMAL
- en: While they tend to perform well in many scenarios, both OpenAI and ReAct agents
    have some drawbacks. Because current LLMs are not very good at long-term planning,
    they can sometimes get stuck in an infinite loop without finding the desired solution.
    At other times, their attention can be distracted by certain outputs they receive
    during execution, and this can cause them to stop before solving the given task.
  prefs: []
  type: TYPE_NORMAL
- en: But probably the biggest drawback of these types of agents is their serialized
    way of working. In other words, the execution of the steps is done in sequence.
    These agents wait for the output generated by one step to trigger the next step.
    This is a very inefficient approach in many practical scenarios. Often, a series
    of steps can be executed in parallel, significantly improving application performance
    and user experience. Based on these premises, I will now present a more advanced
    form of agent.
  prefs: []
  type: TYPE_NORMAL
- en: Inspired by the paper by Kim, S., et al. (2023), *An LLM Compiler for Parallel
    Function Calling* ( https://arxiv.org/abs/2312.04511 ), this agent implementation
    offers outstanding performance and scalability. The concept is based on the ability
    of LLMs to execute multiple functions in parallel and draws inspiration from classical
    compilers to efficiently orchestrate multi-function execution.
  prefs: []
  type: TYPE_NORMAL
- en: The **LLMCompiler agent** orchestrates these parallel function calls using a
    three-part system that plans, dispatches, and executes tasks, resulting in faster
    and more accurate multi-function calls compared to sequential methods. Just as
    compilers transform and optimize code to run efficiently, LLMCompiler transforms
    natural language queries into optimized sequences of function calls that can be
    executed in parallel when dependencies allow. This makes calling multiple tools
    with LLMs faster, cheaper, and potentially more accurate. An additional advantage
    is that it works with any kind of LLM, including both open source and closed source
    models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Under the hood, an LLMCompileraAgent has three main components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**LLM planner** : Formulates execution strategies and dependencies from user
    input and examples'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Task-fetching unit** : Sends and updates function-calling tasks based on
    the dependencies'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Executor** : Executes tasks in parallel using associated tools'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 8* *.13* explains the structure of the LLMCompiler agent visually:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_08_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.13 – An overview of the LLMCompiler agent’s architecture
  prefs: []
  type: TYPE_NORMAL
- en: The *LLM planner* determines the order of function calls and their interdependencies
    according to user input. Next, the *task-fetching unit* initiates parallel execution
    of these functions, replacing variables with the outputs from prior tasks. The
    *executor* then carries out these function calls with the relevant tools. Combined,
    these elements enhance the efficiency of parallel function calling in LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: The **directed acyclic graph** ( **DAG** ) of tasks is a key data structure
    created by the *LLM planner* from user inputs and examples. This planning graph
    captures task dependencies and enables optimized parallel execution ( https://en.wikipedia.org/wiki/Directed_acyclic_graph
    ).
  prefs: []
  type: TYPE_NORMAL
- en: The DAG facilitates the simultaneous execution of tasks that do not depend on
    each other. Should one task rely on the completion of another, the prerequisite
    task must finish before the dependent task can commence. Independent tasks, on
    the other hand, are capable of being executed concurrently without any dependency
    constraints.
  prefs: []
  type: TYPE_NORMAL
- en: Quick note
  prefs: []
  type: TYPE_NORMAL
- en: While OpenAI has already introduced parallel function calling into their API,
    the LLMCompiler is still superior in its approach because it manifests fault tolerance
    in case of wrong LLM decisions and can replan, depending on the outputs generated.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand how we can implement an agent using the LLMCompiler, let’s have
    a look at a simple example. But first, to run the example, you’ll need to install
    the necessary integration package:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'After importing `LLMCompilerAgentPack` and `DatabaseToolSpec` , we initialized
    the database tools and used the tool list to initialize the agent. It’s now time
    to interact with the agent, this time using the `run()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 8* *.14* shows the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_08_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.14 – Sample output of the LLMCompiler agent
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the output, we can see both the execution plan generated by the agent
    and the actual steps performed. Quite neat, isn’t it?
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, LLMCompiler-based agents represent a leap forward in addressing
    the limitations of serial execution found in traditional agents, pushing the boundaries
    of what’s possible in terms of chatbot implementations and user interaction.
  prefs: []
  type: TYPE_NORMAL
- en: Using the low-level Agent Protocol API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Taking inspiration from the **Agent Protocol** ( https://agentprotocol.ai/ )
    and several research papers, the LlamaIndex community also created a more granular
    way to control the agents. This provides enhanced control and flexibility for
    executing user queries. It enables users to manage the agent’s actions with finer
    detail, facilitating the development of more sophisticated agentic systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'The entire concept is based on two main components, `AgentRunner` and `AgentWorker`
    , and works as described in *Figure 8* *.15* :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_08_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.15 – The AgentRunner and AgentWorker orchestration model
  prefs: []
  type: TYPE_NORMAL
- en: We use **agent runners** to orchestrate tasks and store conversational memory.
    **Agent workers** control the execution of each task step by step without storing
    the state themselves. The agent runner manages the overall process and integrates
    the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of benefits, there are multiple reasons to use agents like this. Firstly,
    it allows for a clear separation of concerns: agent runners manage the task’s
    overall orchestration and memory, while agent workers focus only on executing
    specific steps of a task. This division enhances the maintainability and scalability
    of the system.'
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, the architecture promotes enhanced visibility and control over the
    agent’s decision-making process. We can observe and intervene at each step, with
    very good insight into the agent’s operation. This is particularly useful for
    debugging and refining our agent’s behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Another key benefit is the flexibility it provides. We can tailor the behavior
    of agents according to the specific needs of the application. We can modify or
    extend the functionality of agent workers, or integrate custom logic within the
    agent runner, making the system highly adaptable. This setup also supports modular
    development. We can build or update individual components without affecting the
    entire system, facilitating easier updates and iterations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a sample implementation that takes one of our previous examples and
    applies this more granular approach. We’ll implement `OpenAIAgent` in a low-level
    fashion by using `AgentRunner` and `OpenAIAgentWorker` :'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we’ve imported the necessary components and prepared the tool list for
    the agent. We’re using the same `employees.db` database as before. Next, we’ll
    define the agent worker:'
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s time to initialize our agent runner and prepare the input that will contain
    the task:'
  prefs: []
  type: TYPE_NORMAL
- en: There are two distinct methods to engage with our agent now. Let’s take a look.
  prefs: []
  type: TYPE_NORMAL
- en: Option A – the end-to-end interaction, using the chat() method
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `chat()` method offers a seamless, end-to-end interaction, executing the
    task without requiring intervention at each reasoning step:'
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s very straightforward: just two lines of code, at which point we wait for
    the agent to solve the task and provide a final response when all the steps are
    completed.'
  prefs: []
  type: TYPE_NORMAL
- en: Option B – the step-by-step interaction, using the create_task() method
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For more granular control, we could leverage the agent runner and use a step-by-step
    method that allows us to create a task, run each step individually, and then finalize
    the response:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first part, we created a new task for the agent runner and executed
    the first step of the task. Because this method provides manual control of the
    execution of each step, we have to manually implement a loop in our code. We will
    repeatedly call `run_step()` until the output indicates all steps are complete:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The previous loop will run until the last step is completed. Then, it’s time
    to synthesize and display the final answer:'
  prefs: []
  type: TYPE_NORMAL
- en: This allows us to execute and observe each reasoning step individually. The
    `create_task()` method initializes a new task, `run_step()` executes each step,
    returning an output, and `finalize_response()` generates the final response once
    all steps are complete.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, this option is particularly useful when you need to monitor the agent’s
    decisions closely or when you want to step in at certain points to guide the process
    or to handle exceptions.
  prefs: []
  type: TYPE_NORMAL
- en: Now, it’s time to apply this fresh knowledge and add some chat features to our
    PITS project.
  prefs: []
  type: TYPE_NORMAL
- en: <title>Hands-on – implementing conversation tracking for PITS</title>
  prefs: []
  type: TYPE_NORMAL
- en: Hands-on – implementing conversation tracking for PITS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'import os import json import streamlit as st from openai import OpenAI from
    llama_index.core import load_index_from_storage from llama_index.core import StorageContext
    from llama_index.core.memory import ChatMemoryBuffer from llama_index.core.tools
    import QueryEngineTool, ToolMetadata from llama_index.agent.openai import OpenAIAgent
    from llama_index.core.storage.chat_store import SimpleChatStore from global_settings
    import INDEX_STORAGE, CONVERSATION_FILE def load_chat_store():     try:         chat_store
    = SimpleChatStore.from_persist_path(             CONVERSATION_FILE         )     except
    FileNotFoundError:         chat_store = SimpleChatStore()     return chat_store
    def display_messages(chat_store, container):     with container:         for message
    in chat_store.get_messages(key="0"):             with st.chat_message(message.role):
                    st.markdown(message.content) def initialize_chatbot(user_name,
    study_subject,                        chat_store, container, context):     memory
    = ChatMemoryBuffer.from_defaults(         token_limit=3000,         chat_store=chat_store,
            chat_store_key="0"     )     storage_context = StorageContext.from_defaults(
            persist_dir=INDEX_STORAGE     )     index = load_index_from_storage(         storage_context,
    index_id="vector"     )     study_materials_engine = index.as_query_engine(         similarity_top_k=3
        )     study_materials_tool = QueryEngineTool(         query_engine=study_materials_engine,
            metadata=ToolMetadata(             name="study_materials",             description=(
                    f"Provides official information about "                 f"{study_subject}.
    Use a detailed plain "                 f"text question as input to the tool."
                ),         )     )     agent = OpenAIAgent.from_tools(         tools=[study_materials_tool],
            memory=memory,         system_prompt=(             f"Your name is PITS,
    a personal tutor. Your "             f"purpose is to help {user_name} study and
    "             f"better understand the topic of: "             f"{study_subject}.
    We are now discussing the "             f"slide with the following content: {context}"
            )     )     display_messages(chat_store, container)     return agent def
    chat_interface(agent, chat_store, container):     prompt = st.chat_input("Type
    your question here:")     if prompt:         with container:             with
    st.chat_message("user"):                 st.markdown(prompt)             response
    = str(agent.chat(prompt))             with st.chat_message("assistant"):                 st.markdown(response)
            chat_store.persist(CONVERSATION_FILE)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this practical section, we’ll use some of our newfound knowledge to further
    improve our personal tutoring project. Like any professional tutor, eager to teach
    students and answer their questions, PITS should have a proper conversational
    engine at its core. It should be able to understand the topic, be aware of the
    current context, and keep track of the entire interaction with the student. Because
    the learning process will probably take place through multiple sessions, PITS
    must be able to persist the entire conversation and resume the interaction when
    a new session is initiated. We’ll implement all these features in `coversation_engine.py`
    . This module is not meant to be used directly in our app architecture. Instead,
    it will provide three callable functions that we will later import and use in
    the `training_UI.py` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '`load_chat_store` : This function is responsible for retrieving the chatbot
    conversation from previous sessions. We’re using a generic `chat_store_key="0"`
    key. In a multi-user scenario, this key could be used to store chat conversations
    for different users in the same chat store.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initialize_chatbot` : This function is responsible for loading the training
    material vector index from storage, defining a query engine tool on the index,
    and then initializing `OpenAIAgent` using this tool as an argument. It also provides
    the agent with a system prompt that contains context information describing the
    purpose of the agent, the username and study topic, as well as the current slide
    content. The function returns the initialized agent, which will then be used by
    `chat_interface` to implement the actual conversation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chat_interface` : This function implements the ongoing conversation by taking
    the user input and generating an answer from the agent. It also persists the conversation
    after each interaction. If the user ends the current session, on resume, the conversation
    will be continued from that point.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once implemented in the main training interface, this chat should look similar
    to what’s shown in *Figure 8* *.16* :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_08_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.16 – Screenshot from the PITS training UI
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s have a look at the code. The first part contains all the necessary imports:'
  prefs: []
  type: TYPE_NORMAL
- en: You’ll notice that in the first part of the code, we imported a lot of components.
    The `os` and `json` modules will be used for the chat persistence feature. The
    specific LlamaIndex elements will be used to implement the agent with all its
    required components.
  prefs: []
  type: TYPE_NORMAL
- en: We also imported the `INDEX_STORAGE` and `CONVERSATION_FILE` locations from
    the `global_settings.py` module. Because the chat conversation will be implemented
    using Streamlit, we also have to import the `streamlit` library.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s have a look at the `load_chat_store` function, which is responsible
    for resuming the previous conversation by loading the chat history from the local
    storage file specified by `CONVERSATION_FILE` :'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the `load_chat_store` function tries to retrieve the conversation
    history from the local storage file. If the storage file does not exist, a new
    empty `chat_store` is created. The function returns `chat_store` .
  prefs: []
  type: TYPE_NORMAL
- en: 'The next function is responsible for displaying the entire conversation history
    in the Streamlit interface:'
  prefs: []
  type: TYPE_NORMAL
- en: The `display_messages` function takes a chat store and a Streamlit container
    as arguments. It extracts all messages from the chat store using `get_messages()`
    . The function iterates over and displays each message from the chat store, assigning
    appropriate roles – *user* or *assistant* – to each.
  prefs: []
  type: TYPE_NORMAL
- en: The messages are displayed in the Streamlit container using Streamlit’s `chat_message()`
    method, which has the advantage of automatically adding a corresponding icon for
    each role.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next function is responsible for initializing the agent. This function
    takes five arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '`user_name` : The name of the user – to enable a more personal experience.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`study_subject` : The topic covered by the study materials.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chat_store` : Used to initialize the conversation history.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`container` : This is the Streamlit container where the chat conversation will
    be displayed. It’s not used by this function itself and instead passed further
    to the `display_messages` function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`context` : This is the content of the current slide being displayed in the
    training interface. This context will be fed into the agent’s system prompt to
    ground any answer on the current context of the user.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s see the first part of the function:'
  prefs: []
  type: TYPE_NORMAL
- en: Here, we have defined a `ChatMemoryBuffer` object for the agent, specifying
    the `chat_store` attribute containing the conversation history. We used the same
    `chat_store_key` as before. This is important to allow the agent to correctly
    retrieve the chat history.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll prepare the tools for the agent:'
  prefs: []
  type: TYPE_NORMAL
- en: Here, we first retrieved our vector index by using a `StorageContext` object
    and the `load_index_from_storage()` method. We had to specify the *ID* of the
    index – *vector* – because in our case, the storage contains more than one index.
  prefs: []
  type: TYPE_NORMAL
- en: After loading the index, we created a simple query engine configured with `similarity_top_k=3`
    and then created a `QueryEngineTool` utility, providing a proper description in
    its metadata so that the agent can *understand* its purpose and usage. The top-k
    similarity parameter is set to `3` to retrieve the three most relevant pieces
    of information from the index.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next part will initialize `OpenAIAgent` :'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding code, we initialized `OpenAIAgent` while providing `QueryEngineTool`
    , `memory` , and `system_prompt` as arguments. This prompt is used to provide
    the LLM with background information to contextualize its responses, ensuring they
    are relevant to the current discussion topic and the user’s study needs.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, I’ve tried to keep the code as simple as possible. Many things
    could be improved in this implementation. After initializing the agent, we call
    `display_messages` to display the existing conversation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our last function is responsible for handling the actual conversation. It takes
    three arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '`agent` : The agent engine that will be used to run the chat'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chat_store` : The `chat_store` argument that will be used to persist the conversation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`container` : The Streamlit container where the messages will be displayed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s have a look at the code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'This `chat_interface` function displays a chat input widget using Streamlit’s
    `chat_input()` method. Upon receiving input, it does the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Adds the user’s question to the chat interface in the specified container
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calls the chat method of `OpenAIAgent` to process the question and generate
    a response
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adds the chatbot’s response to the chat interface in the specified container
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Persists the new conversation to `CONVERSATION_FILE` using the chat store’s
    persist method to ensure continuity across sessions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That’s it for now. We’ll talk about more of the features of PITS in the next
    few chapters.
  prefs: []
  type: TYPE_NORMAL
- en: <title>Summary</title>
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter provided an in-depth exploration of building chatbots and agents
    with LlamaIndex. We covered `ChatEngine` for conversation tracking and different
    built-in chat modes, such as simple, context, condense question, and condense
    plus context.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we explored different agent architectures and strategies using `OpenAIAgent`
    , `ReActAgent` , and the more advanced LLMCompiler agent. Key concepts such as
    tools, tool orchestration, reasoning loops, and parallel execution were explained.
  prefs: []
  type: TYPE_NORMAL
- en: We concluded this chapter with a hands-on implementation of conversation tracking
    for the PITS tutoring application.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, you should now have a comprehensive understanding of leveraging LlamaIndex
    capabilities to create useful and engaging conversational interfaces.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the next chapter, we’ll discover how to customize our RAG pipeline
    and provide a straightforward guide to deploying it with Streamlit. We’ll also
    explore advanced tracing methods for seamless debugging and unravel strategies
    for evaluating our applications.
  prefs: []
  type: TYPE_NORMAL
- en: '<title>Part 4: Customization, Prompt Engineering, and Final Words</title>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 4: Customization, Prompt Engineering, and Final Words'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the final part of this book, we explore customizing RAG components for robust,
    production-ready applications, covering tracing and evaluation methods as well
    as deployment with platforms such as Streamlit. We also discover techniques for
    effective prompt engineering and understand how prompts can enhance a RAG workflow.
    We conclude with reflections on the transformative potential of RAG and AI, emphasizing
    continuous learning, community engagement, and ethical considerations, alongside
    a forward-looking perspective on the role of technology and responsible development
    in shaping the future.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 9* , *Customizing and Deploying Our LlamaIndex Project*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Chapter 10* , *Prompt Engineering Guidelines and Best Practices*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Chapter 11* , *Conclusion and Additional Resources*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
