<html><head></head><body>
        

                            
                    <h1 class="header-title">Policy Gradient Methods</h1>
                
            
            
                
<p class="mce-root">Previously, our <strong>reinforcement learning</strong> (<strong>RL</strong>) methods have focused on finding the maximum or best value for choosing a particular action in any given state. While this has worked well for us in previous chapters, it certainly is not without its own problems, one of which is always determining when to actually take the max or best action, hence our exploration/exploitation trade-off. As we have seen, the best action is not always the best and it can be better to take the average of the best. However, mathematically averaging is dangerous and tells us nothing about what the agent actually sampled in the environment. Ideally, we want a method that can learn the distribution of actions for each state in the environment. This introduces a new class of methods in RL known as <strong>Policy Gradient</strong> (<strong>PG</strong>) methods and this will be our focus in this chapter.</p>
<p>In this chapter, we will take a look at PG methods and how they improve on our previous attempts in many different ways. We first look at understanding the intuition behind PG methods and then look to the first method, REINFORCE. After that, we will explore the class of advantage functions and introduce ourselves to actor-critic methods. From there, we will move on to looking at <strong>Deep Deterministic Policy Gradient</strong> methods and how they can be used to solve Lunar Lander. Then, we will progress to an advanced method known as <strong>Trust Region Policy Optimization</strong> and how it estimates returns based on regions of trust.  </p>
<p class="mce-root">Following is a summary of the main topics we will focus on in this chapter:</p>
<ul>
<li style="font-weight: 400">Understanding policy gradient methods</li>
<li style="font-weight: 400">Introducing REINFORCE</li>
<li>Using advantage actor-critic</li>
<li>Building a deep deterministic policy gradient</li>
<li style="font-weight: 400">Exploring trust region policy optimization</li>
</ul>
<p class="mce-root"/>
<p>PG methods are mathematically far more complex than our previous attempts and go deeper into statistical and probabilistic methods. While we will focus on understanding the intuition and not the mathematics behind these methods, it may still be confusing to some readers. If you find this, you may find a refresher on statistics and probability will help. In the next section, we look to begin our understanding of the intuition behind PG methods.</p>
<p>All of the code for this entire chapter was originally sourced from this GitHub repository: <a href="https://github.com/seungeunrho/minimalRL">https://github.com/seungeunrho/minimalRL</a>. The original author did an excellent job of sourcing the original collection. As per usual, the code has been significantly modified to fit the style of this book and the other code to be consistent.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Understanding policy gradient methods</h1>
                
            
            
                
<p>One thing we need to understand about PG methods is why we need them and what the intuition is behind them. Then, we can cover some of the mathematics very briefly before diving into the code. So, let's cover the motivation behind using PG methods and what they hope to achieve beyond the other previous methods we have looked at. I have summarized the main points of why/what PG methods do and try to solve:</p>
<ul>
<li><strong>Deterministic versus stochastic functions</strong>: We often learn early in science and mathematics that many problems require a single or deterministic answer. In the real world, however, we often equate some amount of error to deterministic calculations to quantify their accuracy. This quantification of how accurate a value is can be taken a step further with stochastic or probabilistic methods.</li>
</ul>
<p style="padding-left: 60px">Stochastic methods are often used to quantify expectation of risk or uncertainty and they do this by finding the distribution that describes a range of values. Whereas previously we used a value function to find the optimum state-value that described an action, now we want to understand the distribution that generated that value. The following diagram shows an example of a deterministic versus stochastic function output, a distribution next to the mean, median, mode, and max values:</p>
<div><img class="aligncenter size-full wp-image-495 image-border" src="img/5cfb41d8-97a3-488f-851d-8fea1ee8cb20.png" style="width:27.33em;height:20.50em;"/></div>
<p>A skewed normal distribution </p>
<p style="padding-left: 90px">Previously, we assumed that our agent was always sampling from a perfectly normal distribution. This assumption allowed us to use the max or even mean (average) values. However, a normal distribution is never just normal and in most cases, an environment may not even be distributed close to normal.  </p>
<ul>
<li><strong>Deterministic versus stochastic environments</strong>: The other problem we have with our assumption of everything being normally distributed is that it often isn't and, in the real world, we often need to interpret an environment as random or stochastic. Our previous environments have been for the most part static, meaning they change very little between episodes. Real-world environments are never entirely static and, in games, that is certainly the case. So, we need an algorithm that can respond to random changes in the environment.</li>
<li><strong>Discrete versus continuous action spaces</strong>: We have already spent some time considering discrete versus continuous observation spaces and learned how to handle these environments with discretization and deep learning, except real-world environments and/or games are not always discrete. That is, instead of discrete actions such as up, down, left, and right, we now need to consider continuous actions such as left 10-80%, right 10-90%, up 10-90%, and so on. Fortunately, PG methods provide a mechanism that makes continuous actions easier to implement. Reciprocally, discrete action spaces are doable but don't train as well as continuous.  </li>
</ul>
<p>PG methods work much better in continuous action spaces due to the nature of the algorithm itself. They can be used to solve discrete action space environments but they generally will not perform as well as the other methods we will cover later.  </p>
<p>Now that we understand why we need PG methods, we need to move on to the how in the next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Policy gradient ascent</h1>
                
            
            
                
<p>The basic intuition behind PG methods is we move from finding a value function that describes a deterministic policy to a stochastic policy with parameters used to define a policy distribution. Thinking this way, we can now assume that our policy function needs to be defined so that our policy, π, can be set by adjusting parameters θ so that we understand the probability of taking a given action in a state. Mathematically, we can simply define this like so:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/00f9e2d1-8818-4ab3-a261-6c999c443811.png" style="width:4.08em;height:1.58em;"/></p>
<p>You should consider the mathematics we cover in this chapter the minimum you need to understand the code. If you are indeed serious about developing your own extensions to PG methods, then you likely want to spend some time exploring the mathematics further using <em>An Introduction to Reinforcement Learning</em> (Barto/Sutton, 2nd edition, 2017). </p>
<p> π denotes the policy determined by parameters θ, where we plan to find those parameters easily enough with a deep learning network. Now, we have seen previously how we used deep learning to minimize the loss of a network with gradient descent and we will now turn the problem upside down. We now want to find the parameters that give us the best probability of taking an action for a given state that should maximize an action to 1.0 or 100%. That means instead of reducing a number, we now need to maximize it using gradient ascent. This also transforms our update from a value to a parameter that describes the policy and we rewrite our update equations like so:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/4f8529fd-d471-4a6f-b120-93ceebfb8fbd.png" style="width:15.25em;height:1.75em;"/></p>
<p>In the equation, we have the following:</p>
<ul>
<li><img class="fm-editor-equation" src="img/13e8748f-339a-4b34-a29a-839da749f190.png" style="width:2.17em;height:1.17em;"/> The parameter's value at the previous time step</li>
<li><img style="font-size: 1em;color: #333333;width:2.17em;height:0.75em;" class="fm-editor-equation" src="img/74b55c70-2a06-4368-8361-495ce087eda0.png"/> The learning rate</li>
<li><img style="font-size: 1em;color: #333333;width:7.17em;height:1.50em;" class="fm-editor-equation" src="img/809ad4df-d51c-4332-8e7d-db6729c249d0.png"/> The calculated update gradient for action <em>a*</em>, or the optimal action</li>
</ul>
<p>The intuition here is that we are pushing toward the action that will yield the best policy. However, what we find is that making a further assumption of assuming all pushes are equal is just as egregious. After all, we should be able to introduce those deterministic predictions of value back into the preceding equation as a further guide to the real value. We can do this by updating the last equation like so:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/b4d273e3-613f-495c-926a-3a729c361454.png" style="width:19.17em;height:2.08em;"/></p>
<p>Here, we now introduce the following:</p>
<p><img class="fm-editor-equation" src="img/88a4e9d4-32e1-4fa5-8033-fddde0e88f8e.png" style="width:4.67em;height:2.25em;"/>: This becomes our guess at a Q value for the given state and action pair.</p>
<p> </p>
<p>Hence, state-action pairs with higher estimated Q values will benefit more than those that do not, except, we now have to take a step back and reconsider our old friend the exploration/exploitation dilemma and consider how our algorithm/agent needs to select actions. We no longer want our agent to take just the best or random action but, instead, use the learnings of the policy itself. That means a couple of things. Our agent now needs to continually sample and learn off of the same policy meaning PG is on policy but it also means we need to update our update equation to account for this like so:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/361bcbbc-97b1-4c07-8f62-21f759b2eaee.png" style="width:19.58em;height:4.00em;"/></p>
<p>Here, we now introduce the following:</p>
<p><img class="fm-editor-equation" src="img/0ab082c0-c579-4786-a43a-f0aac23e503b.png" style="width:4.75em;height:1.83em;"/>: This is the probability of a given action in a given state—essentially, what the policy itself predicts.</p>
<p>Dividing by the policy's probability of taking an action in a given state accounts for how frequently that action may be taken. Hence, if an action is twice as popular as another, it will be updated only half as much, but likely for twice the number of times. Again, this tries to eliminate skewing of actions that get sampled more often and allow for the algorithm to weight those rare but beneficial actions more accordingly.  </p>
<p class="mce-root"/>
<p>Now that you understand the basic intuition of our new update and process, we can see how this works in practice. Implementing PG methods in practice is more difficult mathematically but fortunately, deep learning alleviates that for us by providing gradient ascent as we will see when we tackle our first practical algorithm in the next section. </p>


            

            
        
    

        

                            
                    <h1 class="header-title">Introducing REINFORCE</h1>
                
            
            
                
<p>The first algorithm we will look at is known as <strong>REINFORCE</strong>. It introduces the concept of PG in a very elegant manner, especially in PyTorch, which masks many of the mathematical complexities of this implementation. REINFORCE also works by solving the optimization problem in reverse. That is, instead of using gradient ascent, it reverses the mathematics so we can express the problem as a loss function and hence use gradient descent. The update equation now transforms to the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/8d747008-3ce8-4cb7-9ede-4bb66720b5fa.png" style="width:19.67em;height:2.00em;"/></p>
<p>Here, we now assume the following:</p>
<ul>
<li><img class="fm-editor-equation" src="img/4217bf51-fd01-413c-9ae3-048cbf1b2a4d.png" style="width:4.17em;height:1.42em;"/> This is the advantage over the baseline expressed by <img class="fm-editor-equation" src="img/68346397-7b2a-47a9-89fd-3e0e0f38d3aa.png" style="width:3.67em;height:1.67em;"/>; we will get to the advantage function in more detail shortly.</li>
<li><img style="font-size: 1em;color: #333333;width:7.50em;height:1.33em;" class="fm-editor-equation" src="img/93e9f09b-c099-4b70-8751-c51453a078ed.png"/> This is the gradient now expressed as a loss and is equivalent to <img style="font-size: 1em;color: #333333;width:4.00em;height:2.58em;" class="fm-editor-equation" src="img/0a3b256d-6a7d-44c2-a32c-60c9c7cc198b.png"/>, assuming with the chain rule and the derivation of <em>1/x = log x</em>. </li>
</ul>
<p>Essentially, we flip the equation using the chain rule and the property <em>1/x = log x</em>. Again, breaking down the mathematics in detail is outside the scope of this book, but the critical intuition here is the use of the log function as a derivation trick to invert our equation into a loss function combined with the advantage function.</p>
<p class="mce-root"><strong>REINFORCE</strong> stands for <strong>REward Increment = Non-negative Factor <em>x</em> Offset Reinforcement <em>x </em>Characteristic E</strong><strong>ligibility</strong>. The acronym attempts to describe the mathematical intuition of the algorithm itself, where the non-negative factor represents the advantage function, <img class="fm-editor-equation" src="img/5940afc2-e6d5-4774-bcd9-c2749e24b14e.png" style="width:1.00em;height:1.50em;"/>. Offset reinforcement is the gradient itself denoted by <img class="fm-editor-equation" src="img/2763d48c-1832-4d50-b259-747bfdc4cb5b.png" style="width:1.08em;height:1.08em;"/>. Then, we introduce characteristic eligibility, which reverts back to our learning of TD and eligibility traces using <img class="fm-editor-equation" src="img/274a93f2-167b-4ccd-9d92-986c6638dab3.png" style="width:1.58em;height:1.17em;"/>. Scaling this whole factor by <img class="fm-editor-equation" src="img/93a38c62-f037-42e7-a4f0-6f8c3f1aa219.png" style="width:1.00em;height:0.92em;"/> or the learning rate allows us to adjust how quickly the algorithm/agent learns.</p>
<p class="mce-root"/>
<p>Being able to intuitively tune the hyperparameters, the learning rate (alpha) and discount factor (gamma), should be a skill you have already started to master. However, PG methods bring a different intuition into how an agent wants/needs to learn. As such, be sure to spend an equal amount of time understanding how tuning these values has changed.</p>
<p>Of course, as game programmers, the best way for us to understand this is to work with the code and that is exactly what we will do in the next exercise. Open example <kbd>Chapter_8_REINFORCE.py</kbd> and follow the exercise here:</p>
<ol>
<li>REINFORCE in PyTorch becomes a nice compact algorithm and the entire code listing is shown here:</li>
</ol>
<pre style="padding-left: 60px">import gym<br/>import torch<br/>import torch.nn as nn<br/>import torch.nn.functional as F<br/>import torch.optim as optim<br/>from torch.distributions import Categorical<br/><br/>#Hyperparameters<br/>learning_rate = 0.0002<br/>gamma = 0.98<br/><br/>class REINFORCE(nn.Module):<br/>  def __init__(self, input_shape, num_actions):<br/>    super(REINFORCE, self).__init__()<br/>    self.data = []<br/> <br/>    self.fc1 = nn.Linear(input_shape, 128)<br/>    self.fc2 = nn.Linear(128, num_actions)<br/>    self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)<br/> <br/> def act(self, x):<br/>   x = F.relu(self.fc1(x))<br/>   x = F.softmax(self.fc2(x), dim=0)<br/>   return x<br/> <br/> def put_data(self, item):<br/>   self.data.append(item)<br/> <br/> def train_net(self):<br/>   R = 0<br/>   for r, log_prob in self.data[::-1]:<br/>     R = r + gamma * R<br/>     loss = -log_prob * R<br/>     self.optimizer.zero_grad()<br/>     loss.backward()<br/>     self.optimizer.step()<br/>   self.data = []<br/><br/>env = gym.make('LunarLander-v2')<br/>pi = REINFORCE(env.observation_space.shape[0], env.action_space.n)<br/>score = 0.0<br/>print_interval = 100<br/>iterations = 10000<br/>min_play_reward = 20<br/><br/>def play_game():<br/>  done = False<br/>  state = env.reset()<br/>  its = 500<br/>  while(not done and its &gt; 0):<br/>    its -= 1<br/>    prob = pi.act(torch.from_numpy(state).float())<br/>    m = Categorical(prob)<br/>    action = m.sample()<br/>    next_state, reward, done, _ = env.step(action.item())<br/>    env.render()<br/>    state = next_state<br/><br/>for iteration in range(iterations):<br/>  s = env.reset()<br/>  for t in range(501):<br/>    prob = pi.act(torch.from_numpy(s).float())<br/>    m = Categorical(prob)<br/>    action = m.sample()<br/>    s_prime, r, done, info = env.step(action.item())<br/>    pi.put_data((r,torch.log(prob[action])))<br/> <br/>    s = s_prime<br/>    score += r<br/>    if done:<br/>      if score/print_interval &gt; min_play_reward:<br/>        play_game()<br/>      break<br/>  pi.train_net()<br/>  if iteration%print_interval==0 and iteration!=0:<br/>    print("# of episode :{}, avg score : {}".format(iteration, score/print_interval))<br/>    score = 0.0 <br/><br/>env.close()</pre>
<p class="mce-root"/>
<ol start="2">
<li>As usual, we start with our usual imports with one new addition from <kbd>torch.distributions</kbd> called <kbd>Categorical</kbd>. Now, <kbd>Categorical</kbd> is used to sample our action space from a continuous probability back to discrete action value. After that, we initialize our base hyperparameters, <kbd>learning_rate</kbd> and <kbd>gamma</kbd>.</li>
<li>Next, we come to a new class called <kbd>REINFORCE</kbd>, which encapsulates the functionality of our agent algorithm. We have seen most of this code before in DQN and DDQN configurations. However, we want to focus on the training function, <kbd>train_net</kbd>, shown here:</li>
</ol>
<pre style="padding-left: 60px">def train_net(self):<br/>   R = 0<br/>   for r, log_prob in self.data[::-1]:<br/>     R = r + gamma * R<br/>     loss = -log_prob * R<br/>     self.optimizer.zero_grad()<br/>     loss.backward()<br/>     self.optimizer.step()<br/>   self.data = []</pre>
<ol start="4">
<li><kbd>train_net</kbd> is where we use the loss calculation to push back (backpropagate) errors in the policy network. Notice, in this class, we don't use a replay buffer but instead, just use a list called <kbd>data</kbd>. It should also be clear that we push all of the values in the list back through the network.</li>
<li>After the class definition, we jump to creating the environment and setting up some additional variables, shown here:</li>
</ol>
<pre style="padding-left: 60px">env = gym.make('LunarLander-v2')<br/>pi = REINFORCE(env.observation_space.shape[0], env.action_space.n)<br/>score = 0.0<br/>print_interval = 100<br/>iterations = 10000<br/>min_play_reward = 20</pre>
<ol start="6">
<li>You can see we are back to playing the Lunar Lander environment. The other variables are similar to ones we used before to control the amount of training and how often we output results. If you change this to a different environment, you will most likely need to adjust these values.</li>
</ol>
<ol start="7">
<li>Again, the training iteration code is quite similar to our previous examples with one keen difference and that is how we sample and execute actions in the environment. Here is the code that accomplishes this part:</li>
</ol>
<pre style="padding-left: 60px">prob = pi.act(torch.from_numpy(s).float())<br/>m = Categorical(prob)<br/>action = m.sample()<br/>s_prime, r, done, info = env.step(action.item())<br/>pi.put_data((r,torch.log(prob[action])))</pre>
<ol start="8">
<li>The main thing to notice here is we are extracting the probability of an action from the policy generated by REINFORCE using <kbd>pi.act</kbd>. After that, we convert this probability into a categorical or discrete bin of values with <kbd>Categorical</kbd>. We then extract the discrete action value using <kbd>m.sample()</kbd>. This conversion is necessary for a discrete action space such as the Lunar Lander v2 environment.</li>
</ol>
<p style="padding-left: 60px">Later, we will see how we can use this in a continuous space environment without the conversion. If you scroll up to the <kbd>play_game</kbd> function, you will note that the same code block is used to extract the action from the policy when playing the game. Pay special attention to the last line where <kbd>pi.put_data</kbd> is used to store the results and notice how we are using <kbd>torch.log</kbd> on the <kbd>prop[action]</kbd> value. Remember, by using the log function here, we convert or reverse the need to use gradient ascent to maximize an action value. Instead, we can use gradient descent and <kbd>backprop</kbd> on our policy network.</p>
<ol start="9">
<li>Run the code as you normally do and observe the results. This algorithm will generally train quickly.</li>
</ol>
<p>The elegance of this algorithm especially in PyTorch obfuscates the complex mathematics here beautifully. Unfortunately, that may not be a good thing unless you understand the intuition. In the next section, we explore the advantage function we mentioned earlier in the last exercise and look at how this relates to actor-critic methods.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Using advantage actor-critic</h1>
                
            
            
                
<p>We have already discussed the concept of advantage a few times throughout a few previous chapters including the last exercise. Advantage is often thought of as understanding the difference between applying different agents/policies to the same problem. The algorithm learns the advantage and, in turn, the benefits it provides to enhancing reward. This is a bit abstract so let's see how this applies to one of our previous algorithms like DDQN. With DDQN, advantage was defined by understanding how to narrow the gap in moving to a known target or goal. Refer back to <a href="42d53358-6f57-4f67-96ce-d8587cbe7cc5.xhtml">Chapter 7</a>, <em>Going Deeper with DDQN</em>, if you need a refresher.</p>
<p>The concept of advantage can be extended to what we refer to as actor-critic methods. With actor-critic, we define advantage by training two networks, one as an actor; that is, it makes decisions on the policy, and another network critiques those decisions based on expected returns. The goal now will not only be to optimize the actor and critic but to do so in a manner to reduce the number of surprises. You can think of a surprise as a time when the agent may expect some reward but instead doesn't or possibly receives more reward. With AC methods, the goal is to minimize surprises and it does this by using a value-based approach (DQN) as the critic and a PG (REINFORCE) method as the actor. See the following diagram to see how this comes together:</p>
<div><img class="aligncenter size-full wp-image-496 image-border" src="img/5796624f-66f6-4219-b37d-a50ad7a3ea96.png" style="width:23.92em;height:15.83em;"/></div>
<p>Explaining actor-critic methods</p>
<p>In the next section, we jump in and see how AC can be applied to our previous PG example.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Actor-critic</h1>
                
            
            
                
<p>AC methods use a combination of networks to predict the output of the value and policy functions, where our value function network resembles DQN and our policy function is defined using a PG method such as REINFORCE. Now, for the most part, this is as simple as it sounds; however, there are several details in the way in which we code these implementations that require some attention. We will, therefore, cover the details of this implementation as we review the code. Open <kbd>Chapter_8_ActorCritic.py</kbd> and follow the next exercise:</p>
<ol>
<li>As this code follows the same pattern as previous examples, we will only need to cover a few sections in detail. The first section of importance is the new <kbd>ActorCritic</kbd> class at the top of the file and shown here:</li>
</ol>
<pre style="padding-left: 60px">class ActorCritic(nn.Module):<br/> def __init__(self, input_shape, num_actions):<br/>   super(ActorCritic, self).__init__()<br/>   self.data = []<br/>   self.fc1 = nn.Linear(input_shape,256)<br/>   self.fc_pi = nn.Linear(256,num_actions)<br/>   self.fc_v = nn.Linear(256,1)<br/>   self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)<br/><br/> def pi(self, x, softmax_dim = 0):<br/>   x = F.relu(self.fc1(x))<br/>   x = self.fc_pi(x)<br/>   prob = F.softmax(x, dim=softmax_dim)<br/>   return prob<br/><br/> def v(self, x):<br/>   x = F.relu(self.fc1(x))<br/>   v = self.fc_v(x)<br/>   return v<br/><br/> def put_data(self, transition):<br/>   self.data.append(transition)<br/> <br/> def make_batch(self):<br/>   s_lst, a_lst, r_lst, s_prime_lst, done_lst = [], [], [], [], []<br/>   for transition in self.data:<br/>     s,a,r,s_prime,done = transition<br/>     s_lst.append(s)<br/>     a_lst.append([a])<br/>     r_lst.append([r/100.0])<br/>     s_prime_lst.append(s_prime)<br/>     done_mask = 0.0 if done else 1.0  <br/>     done_lst.append([done_mask])<br/>    <br/>     s_batch, a_batch, r_batch, s_prime_batch, done_batch<br/>       = torch.tensor(s_lst, dtype=torch.float),<br/>       torch.tensor(a_lst), \<br/>       torch.tensor(r_lst, dtype=torch.float),<br/>       torch.tensor(s_prime_lst,   dtype=torch.float), \ <br/>       torch.tensor(done_lst, dtype=torch.float)<br/><br/>     self.data = []<br/>     return s_batch, a_batch, r_batch, s_prime_batch, done_batch<br/><br/> def train_net(self):<br/>   s, a, r, s_prime, done = self.make_batch()<br/>   td_target = r + gamma * self.v(s_prime) * done<br/>   delta = td_target - self.v(s)<br/>   pi = self.pi(s, softmax_dim=1)<br/>   pi_a = pi.gather(1,a)<br/>   loss = -torch.log(pi_a) * delta.detach() <br/>     + F.smooth_l1_loss(self.v(s), td_target.detach())<br/>   self.optimizer.zero_grad()<br/>   loss.mean().backward()<br/>   self.optimizer.step()</pre>
<ol start="2">
<li>Starting at the <kbd>init</kbd> function, we can see that we construct three <kbd>Linear</kbd> network layers: <kbd>fc1</kbd> and <kbd>fc_pi</kbd> for <kbd>policy</kbd> and <kbd>fc_v</kbd> for <kbd>value</kbd>. Then, right after <kbd>init</kbd>, we see the <kbd>pi</kbd> and <kbd>v</kbd> functions. These functions do the forward pass for each network (<kbd>pi</kbd> and <kbd>v</kbd>). Notice how both networks share <kbd>fc1</kbd> as an input layer. That means that the first layer in our network will be used to encode network state in a form both the actor and critic networks will share. Sharing layers like this is common in more advanced network configurations.</li>
<li>Next, we see the <kbd>put_data</kbd> function, which just puts memories into a replay or experience buffer.</li>
<li>After that, we have an imposing function called <kbd>make_batch</kbd>, which just builds the batches of data we use in experience replay.   </li>
<li>We will skip over the <kbd>ActorCritic</kbd> training function, <kbd>train_net</kbd>, and jump down to the iteration training code shown here:</li>
</ol>
<pre style="padding-left: 60px">for iteration in range(iterations):<br/> done = False<br/> s = env.reset()<br/> while not done:<br/>   for t in range(n_rollout):<br/>     prob = model.pi(torch.from_numpy(s).float())<br/>     m = Categorical(prob)<br/>     a = m.sample().item()<br/>     s_prime, r, done, info = env.step(a) <br/>     model.put_data((s,a,r,s_prime,done))<br/><br/>     s = s_prime<br/>     score += r<br/>     if done:<br/>       if score/print_interval &gt; min_play_reward:<br/>         play_game()<br/>       break <br/><br/>     model.train_net()<br/><br/>     if iteration%print_interval==0 and iteration!=0:<br/>       print("# of episode :{},<br/>         avg score : {:.1f}".format(iteration, score/print_interval))<br/>       score = 0.0<br/><br/>env.close()</pre>
<ol start="6">
<li>You may not have noticed, but our last exercise used episodic training or what we refer to as Monte Carlo or off-policy training. This time, our training takes place on-policy, and that means our agent acts as soon as it receives new updates. Otherwise, the code is quite similar to many other examples we have and will run.</li>
<li>Run the example as you normally would. Training may take a while, so start it running and jump back to this book.</li>
</ol>
<p>Now that we understand the basic layout of the example code, it is time to get into the details of training in the next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Training advantage AC</h1>
                
            
            
                
<p>Using advantage and training multiple networks to work together as you may imagine is not trivial. Therefore, we want to focus a whole exercise on understanding how training works in AC. Open up <kbd>Chapter_8_ActorCritic.py</kbd> again and follow the exercise:</p>
<ol>
<li>Our main focus will be the <kbd>train_net</kbd> function in the <kbd>ActorCritic</kbd> class we saw before. Starting with the first two lines, we can see this is where the training batch is first made and we calculate <kbd>td_target</kbd>. Recall we covered the form of TD error calculation check when we implemented DDQN:</li>
</ol>
<pre style="padding-left: 60px">s, a, r, s_prime, done = self.make_batch()<br/>td_target = r + gamma * self.v(s_prime) * done</pre>
<p class="mce-root"/>
<ol start="2">
<li>Next, we calculate the change or delta between our target and the value function. Again, this was covered in DDQN and the code to do this is shown here:</li>
</ol>
<pre style="padding-left: 60px">delta = td_target - self.v(s) </pre>
<ol start="3">
<li>After that, we do a forward pass over the π network with <kbd>self.pi</kbd> and then gather the results. The gather function essentially aligns or gathers data. Interested readers should consult the PyTorch site for further documentation on <kbd>gather</kbd>. The code for this step is here:</li>
</ol>
<pre style="padding-left: 60px">pi = self.pi(s, softmax_dim=1)<br/>pi_a = pi.gather(1,a)</pre>
<ol start="4">
<li>Then, we calculate the loss with the following code:</li>
</ol>
<pre style="padding-left: 60px">loss = -torch.log(pi_a) * delta.detach() <br/>     + F.smooth_l1_loss(self.v(s), td_target.detach())</pre>
<ol start="5">
<li>Loss is calculated using the updated policy method where we use log to inverse optimize for our actions. Recall, in our previous discussion, the introduction of the <img class="fm-editor-equation" src="img/aafe4f10-5c43-4378-8061-8031f956c590.png" style="width:1.17em;height:1.75em;"/>function. This function denotes the advantage function where we take the negative log of the policy and add it to the output of the L1 squared errors from the value function, <kbd>v</kbd>, and <kbd>td_target</kbd>. The <kbd>detach</kbd> function on the tensors just allows for the network to not update those values when training.</li>
<li>Finally, we push the loss back through the network with the following code:</li>
</ol>
<pre style="padding-left: 60px">self.optimizer.zero_grad()<br/>loss.mean().backward()<br/>self.optimizer.step()</pre>
<ol start="7">
<li>There's nothing new here. The code first zeroes out the gradients, then calculates the mean loss of the batch and pushes that backward with a call to <kbd>backward</kbd>, finishing with stepping the optimizer using <kbd>step</kbd>.</li>
</ol>
<p>You will need to tune the hyperparameters in this example to train an agent to complete the environment. Of course, you are more than up to the challenge by now. In the next section, we will move up and look at another class of PG methods.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">Building a deep deterministic policy gradient</h1>
                
            
            
                
<p>One of the problems we face with PG methods is that of variability or too much randomness. Of course, we might expect that from sampling from a stochastic or random policy. The <strong>Deep Deterministic Policy Gradient</strong> (<strong>DDPG</strong>) method was introduced in a paper titled <em>Continuous control with deep reinforcement learning</em>, in 2015 by Tim Lillicrap. It was meant to address the problem of controlling actions through continuous action spaces, something we have avoided until now. Remember that a continuous action space differs from a discrete space in that the actions may indicate a direction but also an amount or value that expresses the effort in that direction whereas, with discrete actions, any action choice is assumed to always be at 100% effort. </p>
<p>So, why does this matter? Well, in our previous chapter exercises, we explored PG methods over discrete action spaces. By using these methods in discrete spaces, we essentially buffered or masked the problem of variability by converting action probabilities into discrete values. However, in environments with continuous control or continuous action spaces, this does not work so well. Enter DDPG. The answer is in the name: deep deterministic policy gradient, which, in essence, means we are introducing determinism back into a PG method to rectify the problem of variability.</p>
<p>The last two PG methods we will cover in this chapter, DDPG and TRPO, are generally considered need-specific and in some cases too complex to implement effectively. Therefore, in the past few years, these methods have not seen much use in more state-of-the-art development. The code for these methods has been provided for completeness but the explanations may be rushed.</p>
<p>Let's see how this looks in code by opening <kbd>Chapter_8_DDPG.py</kbd> and following the next exercise:</p>
<ol>
<li>The full source code for this example is too large to list in full. Instead, we will go through the relevant sections in the exercise, starting with the hyperparameters, here:</li>
</ol>
<pre style="padding-left: 60px">lr_mu = 0.0005<br/>lr_q = 0.001<br/>gamma = 0.99<br/>batch_size = 32<br/>buffer_limit = 50000<br/>tau = 0.005</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="2">
<li>It looks like we have introduced a few new hyperparameters but we really only introduce one new one called <kbd>tau</kbd>. The other variables, <kbd>lr_mu</kbd> and <kbd>lr_q</kbd>, are learning rates for two different networks.</li>
<li>Next, we jump past the <kbd>ReplayBuffer</kbd> class, which is something we have seen before for storing experiences, then past the other code until we come to the environment setup and more variable definitions, as shown here:</li>
</ol>
<pre style="padding-left: 60px">env = gym.make('Pendulum-v0')<br/>memory = ReplayBuffer()<br/><br/>q, q_target = QNet(), QNet()<br/>q_target.load_state_dict(q.state_dict())<br/>mu, mu_target = MuNet(), MuNet()<br/>mu_target.load_state_dict(mu.state_dict())<br/><br/>score = 0.0<br/>print_interval = 20<br/>min_play_reward = 0<br/>iterations = 10000<br/><br/>mu_optimizer = optim.Adam(mu.parameters(), lr=lr_mu)<br/>q_optimizer = optim.Adam(q.parameters(), lr=lr_q)<br/>ou_noise = OrnsteinUhlenbeckNoise(mu=np.zeros(1))</pre>
<ol start="4">
<li>First, we see the setup of a new environment, <kbd>Pendulum</kbd>. Now, <kbd>Pendulum</kbd> is a continuous control environment that requires learning continuous space actions. After that, <kbd>memory</kbd> and <kbd>ReplayBuffer</kbd> are created, followed by the creation of a couple of classes called <kbd>QNet</kbd> and <kbd>MuNet</kbd>. Next, more control/monitoring parameters are initialized. Just before the last line, we see the creation of two optimizers, <kbd>mu_optimizer</kbd> and <kbd>q_optimizer</kbd>, for the <kbd>MuNet</kbd> and <kbd>QNet</kbd> networks respectively. Finally, on the last line, we see the creation of a new tensor called <kbd>ou_noise</kbd>. There is a lot of new stuff going on here but we will see how this all comes together shortly:</li>
</ol>
<pre style="padding-left: 60px">for iteration in range(iterations):<br/>  s = env.reset() <br/>  for t in range(300):</pre>
<p class="mce-root"/>
<ol start="5">
<li>Next, move down to the top of the train loop shown in the preceding lines. We make sure that the algorithm can loop entirely through an episode. Hence, we set the range in the inner loop to a value higher than the iterations the agent is given in the environment:</li>
</ol>
<pre style="padding-left: 60px">a = mu(torch.from_numpy(s).float()) <br/>a = a.item() + ou_noise()[0]<br/>s_prime, r, done, info = env.step([a]) memory.put((s,a,r/100.0,s_prime,done))<br/>score +=r<br/>s = s_prime</pre>
<ol start="6">
<li>Next comes the trial and error training code. Notice that the <kbd>a</kbd> action is taken from the network called <kbd>mu</kbd>. Then, in the next line, we add the <kbd>ou_noise</kbd> value to it. After that, we let the agent take a step, put the results in memory, and update the score and state. The noise value we use here is based on the Ornstein-Uhlenbeck process and is generated by the class of the same name. This process generates a moving stochastic value that tends to converge to the value <img class="fm-editor-equation" src="img/29efbecd-091a-4a5a-823e-d5b784d76923.png" style="width:0.92em;height:1.25em;"/> or <kbd>mu</kbd>.</li>
</ol>
<p style="padding-left: 60px">Recall that we initialized this value to zeroes in the earlier instantiation of the <kbd>OrnsteinUhlenbeckNoise</kbd> class. The intuition here is that we want the noise to converge to 0 over the experiment. This has the effect of controlling the amount of exploration the agent performs. More noise yields more uncertainty in the actions it selects and hence the agent selects more randomly. You can think of noise in an action now as the amount of uncertainty an agent has in that action and how much more it needs to explore to reduce that uncertainty.</p>
<p>The Ornstein-Uhlenbeck process is used for noise here because it converges in a random but predictable manner, in that it always converges. You could, of course, use any value you like here for the noise, even something more deterministic.</p>
<ol start="7">
<li>Staying inside the training loop, we jump to the section that performs the actual training using the following code:</li>
</ol>
<pre style="padding-left: 60px">if memory.size()&gt;2000:<br/>  for i in range(10):<br/>    train(mu, mu_target, q, q_target, memory, q_optimizer, mu_optimizer)<br/>    soft_update(mu, mu_target)<br/>    soft_update(q, q_target)</pre>
<ol start="8">
<li>We can see here that once the memory <kbd>ReplayBuffer</kbd> is above <kbd>2000</kbd>, the agent begins training in loops of 10. First, we see a call to the <kbd>train</kbd> function with the various networks/models constructed, <kbd>mu</kbd>, <kbd>mu_target</kbd>, <kbd>q</kbd>, and <kbd>q_target</kbd>; <kbd>memory</kbd>; and the <kbd>q_optimizer</kbd> and <kbd>mu_optimizer</kbd> optimizers. Then, there are two calls to the <kbd>soft_update</kbd> function with the various models. <kbd>soft_update</kbd>, shown here, just converges the input model to the target in an iterative fashion using <kbd>tau</kbd> to scale the amount of change per iteration:</li>
</ol>
<pre style="padding-left: 60px">def soft_update(net, net_target):<br/>  for param_target, param in zip(net_target.parameters(), net.parameters()):<br/>   param_target.data.copy_(param_target.data * (1.0 - tau) + param.data * tau)</pre>
<ol start="9">
<li>This convergence from some acting model to a target is not new but, with the introduction of AC, it does complicate things. Before we get to that though, let's run the sample and see it in action. Run the code as you normally would and wait: this one can take a while. If your agent reaches a high enough score, you will be rewarded with the following:</li>
</ol>
<div><img class="aligncenter size-full wp-image-497 image-border" src="img/cf2ad031-a0b2-411c-94fc-929479f40949.png" style="width:22.42em;height:18.25em;"/></div>
<p>An example of the Pendulum environment</p>
<p>As the sample runs pay particular attention to how the score is being updated on the screen. Try and get a sense of how this may look graphically. In the next section, we will explore the finer details of this last example.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Training DDPG</h1>
                
            
            
                
<p>Now, as you may have noticed in the last example, <kbd>Chapter_8_DDPG.py</kbd> is using four networks/models to train, using two networks as actors and two as critics, but also using two networks as targets and two as current. This gives us the following diagram:</p>
<div><img class="aligncenter size-full wp-image-498 image-border" src="img/a09bd1f4-973d-4c20-b4a6-d11ea0d3237e.png" style="width:94.33em;height:37.33em;"/></div>
<p>Diagram of actor-critic target-current networks</p>
<p>Each oval in the preceding diagram represents a complete deep learning network. Notice how the critic, the value or Q network implementation, is taking both environment outputs reward and state. The critic then pushes a value back to the actor or policy target network.</p>
<p>Open example <kbd>Chapter_8_DDPG.py</kbd> back up and follow the next exercise to see how this comes together in code:</p>
<ol>
<li>We will first look at our definition of the critic or the <kbd>QNet</kbd> network class shown here:</li>
</ol>
<pre style="padding-left: 60px">class QNet(nn.Module):<br/>  def __init__(self):<br/>    super(QNet, self).__init__()<br/><br/>    self.fc_s = nn.Linear(3, 64)<br/>    self.fc_a = nn.Linear(1,64)<br/>    self.fc_q = nn.Linear(128, 32)<br/>    self.fc_3 = nn.Linear(32,1)<br/><br/>  def forward(self, x, a):<br/>    h1 = F.relu(self.fc_s(x))<br/>    h2 = F.relu(self.fc_a(a))<br/>    cat = torch.cat([h1,h2], dim=1)<br/>    q = F.relu(self.fc_q(cat))<br/>    q = self.fc_3(q)<br/>    return q</pre>
<ol start="2">
<li>The construction of this network is also a little different and what happens here is the <kbd>fc_s</kbd> layer encodes the state, then <kbd>fc_a</kbd> encodes the actions. These two layers are joined in the forward pass to create a single Q layer, <kbd>fc_q</kbd>, which is then output through the last layer, <kbd>fc_3</kbd>.</li>
</ol>
<p>If you need help picturing these types of networks, it can be and is often helpful to draw them out. The key here is to look at the code in the train function, which describes how the layers are joined.</p>
<ol start="3">
<li>Moving from the critic, we move to the actor network as defined by the <kbd>MuNet</kbd> class and shown here:</li>
</ol>
<pre style="padding-left: 60px">class MuNet(nn.Module):<br/>  def __init__(self):<br/>    super(MuNet, self).__init__()<br/>   <br/>    self.fc1 = nn.Linear(3, 128)<br/>    self.fc2 = nn.Linear(128, 64)<br/>    self.fc_mu = nn.Linear(64, 1)<br/><br/>  def forward(self, x):<br/>    x = F.relu(self.fc1(x))<br/>    x = F.relu(self.fc2(x))<br/>    mu = torch.tanh(self.fc_mu(x))*2 <br/>    return mu</pre>
<ol start="4">
<li><kbd>MuNet</kbd> is a fairly simple implementation of a network that encodes the state from 3 values to 128 input neurons, <kbd>fc1</kbd>, followed by 64 hidden layer neurons, <kbd>fc2</kbd>, and then finally output to a single value on the output layer, <kbd>fc_mu</kbd>. The only note of interest is the way we translate <kbd>fc_mu</kbd>, the output layer in the <kbd>forward</kbd> function, into the <kbd>mu</kbd> output value. This is done to account for the control range in the <kbd>Pendulum</kbd> environment, which takes action values in the range -2 to 2. If you convert this sample into another environment, be sure to account for any change in action space values.</li>
<li>Next, we will move down to the start of the <kbd>train</kbd> function, as shown here:</li>
</ol>
<pre style="padding-left: 60px">def train(mu, mu_target, q, q_target, memory, q_optimizer, mu_optimizer):<br/>  s,a,r,s_prime,done_mask = memory.sample(batch_size)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<ol start="6">
<li>The <kbd>train</kbd> function takes all of the networks, memory, and optimizer as inputs. In the first line, it extracts the <kbd>s</kbd> state, <kbd>a</kbd> action, <kbd>r</kbd> reward, <kbd>s_prime</kbd> next state, and <kbd>done_mask</kbd> from the <kbd>replayBuffer</kbd> memory: </li>
</ol>
<pre style="padding-left: 60px">target = r + gamma * q_target(s_prime, mu_target(s_prime))<br/>q_loss = F.smooth_l1_loss(q(s,a), target.detach())<br/>q_optimizer.zero_grad()<br/>q_loss.backward()<br/>q_optimizer.step()</pre>
<ol start="7">
<li>The first block of code inside the function calculates the target value based on the output of the <kbd>q_target</kbd> network, which takes as input the last state, <kbd>s_prime</kbd>, and <kbd>mu_target</kbd> output from the last state. Then, we calculate the <kbd>q_loss</kbd> loss based on the output of the <kbd>q</kbd> network with an input state and action using the <kbd>target</kbd> values as the target. This somewhat abstract conversion is to convert the value from stochastic into deterministic. On the last three lines, we see the typical optimizer code for zeroing the gradient and doing a backprop pass:</li>
</ol>
<pre style="padding-left: 60px">mu_loss = -q(s,mu(s)).mean() <br/>mu_optimizer.zero_grad()<br/>mu_loss.backward()<br/>mu_optimizer.step()</pre>
<ol start="8">
<li>Calculating the <kbd>mu_loss</kbd><strong> </strong>policy loss is much simpler and all we do is take the output of the <kbd>q</kbd> network using the state and output action from the <kbd>mu</kbd> network. A couple of things to note is that we make the loss negative and take the mean or average. Then, we finish the function with a typical optimizer backprop on <kbd>mu_loss</kbd>.</li>
<li>If the agent is still running from the previous exercise, examine the results with this newfound knowledge. Consider how or what hyperparameters you could tune to improve the results of this example.</li>
</ol>
<p>For some, the pure code explanation of DDPG may be a bit abstract but hopefully not. Hopefully, by this point, you can read the code and assume the mathematics or intuition you need to understand the concepts. In the next section, we will look to what is considered one of the more complicated methods in DRL, the <strong>Trust Region Policy Optimization</strong> (<strong>TRPO</strong>) method.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Exploring trust region policy optimization</h1>
                
            
            
                
<p>PG methods suffer from several technical issues, some of which you may have already noticed. These issues manifest themselves in training and you may have already observed this in lack of training convergence or wobble. This is caused by several factors we can summarize here:</p>
<ul>
<li><strong>Gradient ascent versus gradient descent</strong>: In PG, we use gradient ascent to assume the maximum action value is at the top of a hill. However, our chosen optimization methods (SGD or ADAM) are tuned for gradient descent or looking for values at the bottom of hills or flat areas, meaning they work well finding the bottom of a trough but do poorly finding the top of a ridge, especially if the ridge or hill is steep. A comparison of this is shown here:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-499 image-border" src="img/b74693d2-6ad3-45b6-b363-9016b555cd1d.png" style="width:100.42em;height:59.42em;"/></p>
<p>A comparison of gradient descent and ascent</p>
<p style="padding-left: 90px">Finding the peak, therefore, becomes the problem, especially in environments that require fine control or narrow discrete actions. This often appears as training wobble, where the agent keeps increasing in score but then falls back several steps every so often.</p>
<p class="mce-root"/>
<ul>
<li><strong>Policy versus parameter space mapping</strong>: By its very nature, we will often be required to map a policy to some known action space either through continuous or discretization transformation. This step, unsurprisingly, is not without issue. Discretizing action space can be especially problematic and further compounds the hill-climbing problem from earlier.</li>
<li><strong>Static versus dynamic learning rate</strong>: In part due to the optimizer problem we mentioned earlier, we also tend to find that using a static learning rate is problematic. That is, we often find that we need to decrease the learning rate as the agent continues to find the peak of those maximum action hills.</li>
<li><strong>Policy sampling efficiency</strong>: PG methods restrict us from updating the policy more than once per trajectory. If we try to update more frequently, after <em>x</em> number of steps, for instance, we see training diverge. Therefore, we are restricted to one update per trajectory or episode. This can provide for very poor sample efficiency especially in training environments with multiple steps.</li>
</ul>
<p>TRPO and another PG method called <strong>proximal policy optimization</strong>, which we will look at in <a href="2f6812c0-fd1f-4eda-9df2-6c67c8077aec.xhtml">Chapter 9</a>, <em>Asynchronous Action and the Policy</em>, attempt to resolve all of the previous issues using several common strategies.</p>
<p>The code for this TRPO implementation was sourced directly from <a href="https://github.com/ikostrikov/pytorch-trpo">https://github.com/ikostrikov/pytorch-trpo</a> and, at the time of writing, the code was only modified slightly to allow for easier running. This is a great example and worthy of further exploration and enhancements.  </p>
<p>Before we get to reviewing each of these strategies, let's open the code for and follow the next exercise:</p>
<ol>
<li>TRPO is a hefty algorithm not easily run in a single file. We will start by looking at the code structure by opening the TRPO folder in the source folder for this chapter. This example covers code in several files and we will only review small snippets here. It is recommended you quickly review the source fully before continuing.</li>
<li>Refer to the <kbd>main.py</kbd><strong> </strong>file; this is the startup file. <kbd>main</kbd> takes several parameters defined within this file as inputs when running.  </li>
<li>Scroll to about the middle and you will see where the environment is constructed on the main policy and value networks, as shown here:</li>
</ol>
<pre style="padding-left: 60px">env = gym.make(args.env_name)<br/><br/>num_inputs = env.observation_space.shape[0]<br/>num_actions = env.action_space.shape[0]<br/><br/>env.seed(args.seed)<br/>torch.manual_seed(args.seed)<br/><br/>policy_net = Policy(num_inputs, num_actions)<br/>value_net = Value(num_inputs)</pre>
<ol start="4">
<li>Next, scroll down further until you come to that familiar training loop. For the most part, this should look similar to other examples, except for the introduction of another <kbd>while</kbd> loop, shown here:</li>
</ol>
<pre style="padding-left: 60px">while num_steps &lt; args.batch_size:</pre>
<ol start="5">
<li>This code assures that one agent episode consists of a given number of steps determined by <kbd>batch_size</kbd>. However, we still don't break the inner training loop until the environment says the episode is done. However, now, an episode or trajectory update is not done until the provided <kbd>batch_size</kbd> is reached. This attempts to solve the PG method sampling problem we talked about earlier.</li>
<li>Do a quick review of each of the source files; the list here summarizes the purpose of each:
<ul>
<li><kbd>main.py</kbd>: This is the startup source file and main point of agent training.</li>
<li><kbd>conjugate_gradients.py</kbd>: This is a helper method to conjugate or join gradients.</li>
<li><kbd>models.py</kbd>: This file defines the network class Policy (actor) and value (the critic). The construction of these networks is a little unique so be sure to see how.</li>
<li><kbd>replay_memory.py</kbd>: This is a helper class to contain replay memory.</li>
<li><kbd>running_state.py</kbd>: This is a helper class to compute the running variance of the state, essentially, running estimates of mean and standard deviation. This can be beneficial for an arbitrary sampling of a normal distribution.</li>
<li><kbd>trpo.py</kbd>: This is the code specific to TRPO and is meant to address those PG problems we mentioned earlier.</li>
<li><kbd>utils.py</kbd>: This provides some helper methods.</li>
</ul>
</li>
</ol>
<ol start="7">
<li>Use the default start parameters and run <kbd>main.py</kbd> as you normally would a Python file and watch the output, as shown here:</li>
</ol>
<div><img class="aligncenter size-full wp-image-500 image-border" src="img/fdc9d012-3339-4c9d-ac24-976eb98af9f9.png" style="width:64.83em;height:42.67em;"/></div>
<p>The output of the TRPO sample</p>
<p>The output from this particular implementation is more verbose and displays factors that monitor performance over the issues we mentioned PG methods suffer from. In the next few sections, we will walk through exercises that show how TRPO tries to address these problems.</p>
<p>Jonathan Hui (<a href="https://medium.com/@jonathan_hui">https://medium.com/@jonathan_hui</a>) has several excellent posts on Medium.com that discuss various implementations of DRL algorithms. He does an especially good job of explaining the mathematics behind TRPO and other more complex methods.   </p>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">Conjugate gradients</h1>
                
            
            
                
<p>The fundamental problem we need to address with policy methods is the conversion to a natural gradient form of gradient ascent. Previously, we handled conjugating this gradient by simply applying the log function. However, this does not yield a natural gradient. Natural gradients are not susceptible to model parameterization and provide an invariant method to compute stable gradients. Let's look at how this is done in code by opening up our IDE to the TRPO example again and following the next exercise:</p>
<ol>
<li>Open the <kbd>trpo.py</kbd> file in the <kbd>TRPO</kbd> folder. The three functions in this file are meant to address the various problems we encounter with PG. The first problem we encounter is to reverse the gradient and the code to do that is shown here:</li>
</ol>
<pre style="padding-left: 60px">def conjugate_gradients(Avp, b, nsteps, residual_tol=1e-10):<br/>  x = torch.zeros(b.size())<br/>  r = b.clone()<br/>  p = b.clone()<br/>  rdotr = torch.dot(r, r)<br/>  for i in range(nsteps):<br/>    _Avp = Avp(p)<br/>    alpha = rdotr / torch.dot(p, _Avp)<br/>    x += alpha * p<br/>    r -= alpha * _Avp<br/>    new_rdotr = torch.dot(r, r)<br/>    betta = new_rdotr / rdotr<br/>    p = r + betta * p<br/>    rdotr = new_rdotr<br/>    if rdotr &lt; residual_tol:<br/>      break<br/>  return x</pre>
<ol start="2">
<li>The <kbd>conjugate_gradients</kbd> function is used iteratively to produce a natural more stable gradient we can use for the ascent.</li>
<li>Scroll down to the <kbd>trpo_step</kbd> function and you will see how this method is used as shown in the code here:</li>
</ol>
<pre style="padding-left: 60px">stepdir = conjugate_gradients(Fvp, -loss_grad, 10)</pre>
<ol start="4">
<li>This outputs a <kbd>stepdir</kbd> tensor denoting the gradient used to step the network. We can see by the input parameters the output conjugate gradient will be solved over 10 iterations using an approximation function, <kbd>Fvp</kbd>, and the inverse of the loss gradient, <kbd>loss_grad</kbd>. This is entangled with some other optimizations so we will pause here for now.</li>
</ol>
<p>Conjugate gradients are one method we can use to better manage the gradient descent versus gradient ascent problem we encounter with PG methods. Next, we will look at further optimization again to address problems with gradient ascent.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Trust region methods</h1>
                
            
            
                
<p>A further optimization we can apply to gradient ascent is using trust regions or controlled regions of updates. These methods are of course fundamental to TRPO given the name but the concept is further extended to other policy-based methods. In TRPO, we extend regions of trust over the approximation functions using the <strong>Minorize-Maximization</strong> or <strong>MM</strong> algorithm. The intuition of MM is that there is a lower bound function that we can always expect the returns/reward to be higher than. Hence, if we maximize this lower bound function, we also attain our best policy. Gradient descent by default is a line search algorithm but this again introduces the problem of overshooting. Instead, we can first approximate the step size and then establish a region of trust within that step. This trust region then becomes the space we optimize for. </p>
<p>The analogy we often use to explain this involves asking you to think of yourself climbing a narrow ridge. You run the risk of falling off either side of the ridge so using normal gradient descent or line search becomes dangerous. Instead, you decide that to avoid falling you want to step on the center of the ridge or the center region you trust. The following screenshot taken from a blog post by Jonathan Hui shows this concept further:</p>
<div><img class="aligncenter size-full wp-image-501 image-border" src="img/9dcd8c89-aaae-4fdd-b0a7-86f70eb29def.png" style="width:36.17em;height:20.17em;"/></div>
<p>A comparison of line search versus Trust region</p>
<p>We can see how this looks in code by opening up the TRPO folder and following the next exercise:</p>
<ol>
<li>Open up <kbd>trpo.py</kbd> again and scroll down to the following block of code:</li>
</ol>
<pre style="padding-left: 60px">def linesearch(model, f, x, fullstep, expected_improve_rate,   <br/>  max_backtracks=10, accept_ratio=.1):<br/>  fval = f(True).data   <br/>  print("fval before", fval.item())<br/>  for (_n_backtracks, stepfrac) in enumerate(.5**np.arange(max_backtracks)):<br/>    xnew = x + stepfrac * fullstep<br/>    set_flat_params_to(model, xnew)<br/>    newfval = f(True).data<br/>    actual_improve = fval - newfval<br/>    expected_improve = expected_improve_rate * stepfrac<br/>    ratio = actual_improve / expected_improve<br/>    print("a/e/r", actual_improve.item(), expected_improve.item(), <br/>     ratio.item())<br/><br/>  if ratio.item() &gt; accept_ratio and actual_improve.item() &gt; 0:<br/>    print("fval after", newfval.item())<br/>    return True, xnew<br/><br/>  return False, x</pre>
<ol start="2">
<li>The <kbd>linesearch</kbd> function is used to find how far up the ridge we want to locate the next region of trust. This function is used to indicate the distance to the next region of trust and is executed with the following code:</li>
</ol>
<pre style="padding-left: 60px">success, new_params = linesearch(model, get_loss, prev_params, fullstep,<br/>   neggdotstepdir / lm[0])</pre>
<ol start="3">
<li>Notice the use of <kbd>neggdotstepdir</kbd>. This value is calculated from the step direction, <kbd>stepdir</kbd>, we calculated in the last exercise with the following code:</li>
</ol>
<pre style="padding-left: 60px">neggdotstepdir = (-loss_grad * stepdir).sum(0, keepdim=True)</pre>
<ol start="4">
<li>Now that we have a direction with <kbd>neggdotstepdir</kbd> and an amount with <kbd>linesearch</kbd>, we can determine the trust region with the following code:</li>
</ol>
<pre style="padding-left: 60px"> set_flat_params_to(model, new_params)</pre>
<p class="mce-root"/>
<ol start="5">
<li>The <kbd>set_flat_params_to</kbd> function is in <kbd>utils.py</kbd> and the code is shown here:</li>
</ol>
<pre style="padding-left: 60px">def set_flat_params_to(model, flat_params):<br/>  prev_ind = 0<br/>  for param in model.parameters():<br/>    flat_size = int(np.prod(list(param.size())))<br/>    param.data.copy_(<br/>      flat_params[prev_ind:prev_ind + flat_size].view(param.size()))<br/>    prev_ind += flat_size</pre>
<ol start="6">
<li>This code essentially flattens the parameters into the trust region. This is the trust region we test to determine whether the next step is within, using the <kbd>linesearch</kbd> function.</li>
</ol>
<p>Now we understand the concept of trust regions and the need to properly control step size, direction, and amount when using PG methods. In the next section, we will look at the step itself.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The TRPO step</h1>
                
            
            
                
<p>As you can see now, taking a step or update with TRPO is not trivial and things are still going to get more complicated. The step itself requires the agent to learn several factors from updating the policy and value function to also attain an advantage, also known as actor-critic. Understanding the actual details of the step function is beyond the scope of this book and you are again referred to those external references. However, it may be helpful to review what constitutes a step in TRPO and how this may compare complexity wise to other methods we look at in the future. Open up the sample TRPO folder again and follow the next exercise:</p>
<ol>
<li>Open up the <kbd>main.py</kbd> file and find the following line of code, around line 130:</li>
</ol>
<pre style="padding-left: 60px">trpo_step(policy_net, get_loss, get_kl, args.max_kl, args.damping)</pre>
<ol start="2">
<li>This last line of code is within the <kbd>update_params</kbd> function, which is where the bulk of the training takes place.  </li>
<li>You can further see at almost the bottom of the <kbd>main.py</kbd> file a call to the <kbd>update_params</kbd> function with <kbd>batch</kbd>, <kbd>batch</kbd> being a sample from <kbd>memory</kbd>, as shown in the following code:</li>
</ol>
<pre style="padding-left: 60px">batch = memory.sample()<br/>update_params(batch)</pre>
<p class="mce-root"/>
<ol start="4">
<li>Scroll back up to the <kbd>update_params</kbd> function and notice the first loop that builds <kbd>returns</kbd>, <strong><kbd>deltas</kbd></strong>, and <kbd>advantages</kbd> with the following code:</li>
</ol>
<pre style="padding-left: 60px">for i in reversed(range(rewards.size(0))):<br/>  returns[i] = rewards[i] + args.gamma * prev_return * masks[i]<br/>  deltas[i] = rewards[i] + args.gamma * prev_value *<br/>    masks[i] - values.data[i]<br/>  advantages[i] = deltas[i] + args.gamma * args.tau * <br/>    prev_advantage * masks[i]</pre>
<ol start="5">
<li>Notice how we reverse the rewards and then loop through them to build our various lists <kbd>returns</kbd>, <strong><kbd>deltas</kbd></strong>, and <kbd>advantages</kbd>.  </li>
<li>From there, we flatten the parameters and set the value network, the critic. Then, we calculate the advantages and actions mean and standard deviation. We do this as we are working with distributions and not deterministic values.</li>
<li>After that, we use the <kbd>trpo_step</kbd> function to take a training step or update in the policy.</li>
</ol>
<p>You may have noticed the use of <kbd>kl</kbd> in the source code. This stands for KL divergence and is something we will explore in a later chapter.</p>
<p>Keep the example running for around 5,000 training iterations. This may take some time so be patient. It is worth running to completion, if not just once. The TRPO example in this section is meant to be experimented with and used with various control environments. In the next section, be sure to review the experiments you can play with to explore more about this method.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Exercises</h1>
                
            
            
                
<p>Use the exercises for your enjoyment and learning and to gain additional experience. Deep learning and deep reinforcement learning are very much areas where your knowledge will only improve by working with the examples. Don't expect to be a natural with training agents; it takes a lot of trial and error. Fortunately, the amount of experience we need is not as much as our poor agents require but still expect to put some time in.</p>
<ol>
<li>Open example <kbd>Chapter_8_REINFORCE.py</kbd> back up and alter the hyperparameters to see what effect this has on training.</li>
<li>Open example <kbd>Chapter_8_ActorCritic.py</kbd> back up and alter the hyperparameters to see what effect this has on training.</li>
</ol>
<ol start="3">
<li>Open example <kbd>Chapter_8_DDPG.py</kbd> back up and alter the hyperparameters to see what effect this has on training.</li>
<li>How can you convert the <strong>REINFORCE</strong> or <kbd>ActorCritic</kbd> examples to use continuous action spaces? Attempt to do this for new environments such as <kbd>LunarLanderContinous-v2</kbd>.</li>
<li>Set up example <kbd>Chapter_8_DDPG.py</kbd> to use the <kbd>LunarLanderContinuous-v2</kbd> or another continuous environment. You will need to modify the action state from <kbd>3</kbd> to the environment you choose.</li>
<li>Tune the hyperparameters for the <kbd>Chapter_8_DDPG.py</kbd> example. This will require you to learn and understand that new parameter, <kbd>tau</kbd>.</li>
<li>Tune the hyperparameters for the <strong>TRPO</strong> example. This will require you to learn how to set the hyperparameters from the command line and then tweak those parameters. You should not be modifying any code to complete this exercise.</li>
<li>Enable the MuJoCo environments and run one of these environments with the TRPO example.</li>
<li>Add plotting output to the various examples.</li>
<li>Convert one of the single-file examples into use a main method that takes arguments and allows a user to train hyperparameters dynamically, instead of modifying source code.</li>
</ol>
<p>Be sure to complete from 1-3 of the preceding exercises before moving on to the next section and end of this chapter.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p class="mce-root">In this chapter, we introduced policy gradient methods, where we learned how to use a stochastic policy to drive our agent with the REINFORCE algorithm. After that, we learned that part of the problem of sampling from a stochastic policy is the randomness of sampling from a stochastic policy. We found that this could be corrected using dual agent networks, with one that represents the acting network and another as a critic. In this case, the actor is the policy network that refers back to the critic network, which uses a deterministic value function. Then, we saw how PG could be improved upon by seeing how DDPG works. Finally, we looked at what is considered one of the more complex methods in DRL, TRPO, and saw how it tries to manage the several shortcomings of PG methods.</p>
<p>Continuing with our look at PG methods, we will move on to explore next-generation methods such as PPO, AC2, AC2, and ACER in the next chapter.</p>


            

            
        
    </body></html>