<html><head></head><body>
  <div><h1 class="chapterNumber">5</h1>
    <h1 id="_idParaDest-125" class="chapterTitle">Boosting RAG Performance with Expert Human Feedback</h1>
    <p class="normal"><strong class="keyWord">Human feedback</strong> (<strong class="keyWord">HF</strong>) is not just useful for generative AI—it’s essential, especially when it comes to models using RAG. A generative AI model uses information from datasets with various documents during training. The data that trained the AI model is set in stone in the model’s parameters; we can’t change it unless we train it again. However, in the world of retrieval-based text and multimodal datasets, there is information we can see and tweak. That is where HF comes in. By providing feedback on what the AI model pulls from its datasets, HF can directly influence the quality of its future responses. Engaging with this process makes humans an active player in the RAG’s development. It adds a new dimension to AI projects: adaptive RAG.</p>
    <p class="normal">We have explored and implemented naïve, advanced, and modular RAG so far. Now, we will add adaptive RAG to our generative AI toolbox. We know that even the best generative AI system with the best metrics cannot convince a dissatisfied user that it is helpful if it isn’t. We will introduce adaptive RAG with an HF loop. The system thus becomes adaptive because the documents used for retrieval are updated. Integrating HF in RAG leads to a pragmatic hybrid approach because it involves humans in an otherwise automated generative process. We will thus leverage HF, which we will use to build a hybrid adaptive RAG program in Python from scratch, going through the key steps of building a RAG-driven generative AI system from the ground up. By the end of this chapter, you will have a theoretical understanding of the adaptive RAG framework and practical experience in building an AI model based on HF.</p>
    <p class="normal">This chapter covers the following topics:</p>
    <ul>
      <li class="bulletList">Defining the adaptive RAG ecosystem</li>
      <li class="bulletList">Applying adaptive RAG to augmented retrieval queries</li>
      <li class="bulletList">Automating augmented generative AI inputs with HF</li>
      <li class="bulletList">Automating end-user feedback rankings to trigger expert HF</li>
      <li class="bulletList">Creating an automated feedback system for a human expert</li>
      <li class="bulletList">Integrating HF with adaptive RAG for GPT-4o</li>
    </ul>
    <p class="normal">Let’s begin by defining adaptive RAG.</p>
    <h1 id="_idParaDest-126" class="heading-1">Adaptive RAG</h1>
    <p class="normal">No, RAG cannot solve all our problems and challenges. RAG, just like any generative model, can also <a id="_idIndexMarker307"/>produce irrelevant and incorrect output! RAG might be a useful option, however, because we feed pertinent documents to the generative AI model that inform its responses. Nonetheless, the quality of RAG outputs depends on the accuracy and relevance of the underlying data, which calls for verification! That’s where adaptive RAG comes in. Adaptive RAG introduces human, real-life, pragmatic feedback that will improve a RAG-driven generative AI ecosystem.</p>
    <p class="normal">The core information in a generative AI model is parametric (stored as weights). But in the context of RAG, this data can be visualized and controlled, as we saw in <em class="chapterRef">Chapter 2</em>, <em class="italic">RAG Embedding Vector Stores with Deep Lake and OpenAI</em>. Despite this, challenges remain; for example, the end-user might write fuzzy queries, or the RAG data retrieval might be faulty. An HF process is, therefore, highly recommended to ensure the system’s reliability.</p>
    <p class="normal"><em class="italic">Figure 1.3</em> from <em class="chapterRef">Chapter 1</em>, <em class="italic">Why Retrieval Augmented Generation?</em>, represents the complete RAG framework and ecosystem. Let’s zoom in on the adaptive RAG ecosystem and focus on the key processes that come into play, as shown in the following figure:</p>
    <figure class="mediaobject"><img src="img/B31169_05_01.png" alt="A diagram of a process  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 5.1: A variant of an adaptive RAG ecosystem</p>
    <p class="normal">The variant <a id="_idIndexMarker308"/>of an adaptive RAG ecosystem in this chapter includes the following components, as shown in <em class="italic">Figure 5.1</em>, for the retriever:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">D1</strong>: <strong class="keyWord">Collect and process</strong> Wikipedia articles on LLMs by fetching and cleaning the data</li>
      <li class="bulletList"><strong class="keyWord">D4</strong>: <strong class="keyWord">Retrieval query</strong> to query the retrieval dataset</li>
    </ul>
    <p class="normal">The generator’s components are:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">G1</strong>: <strong class="keyWord">Input </strong>entered by an end-user</li>
      <li class="bulletList"><strong class="keyWord">G2</strong>: <strong class="keyWord">Augmented input with HF </strong>that will augment the user’s initial input and <strong class="keyWord">prompt engineering </strong>to configure the GPT-4o model’s prompt</li>
      <li class="bulletList"><strong class="keyWord">G4</strong>: <strong class="keyWord">Generation and output </strong>to run the generative AI model and obtain a response</li>
    </ul>
    <p class="normal">The evaluator’s components are:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">E1</strong>: <strong class="keyWord">Metrics</strong> to apply a cosine similarity measurement</li>
      <li class="bulletList"><strong class="keyWord">E2</strong>: <strong class="keyWord">Human feedback</strong> to obtain and process the ultimate measurement of a system through end-user and expert feedback</li>
    </ul>
    <p class="normal">In this chapter, we will illustrate adaptive RAG by building a hybrid adaptive RAG program in Python on Google Colab. We will build this program from scratch to acquire a clear understanding of an adaptive process, which may vary depending on a project’s goals, but the underlying principles remain the same. Through this hands-on experience, you will learn <a id="_idIndexMarker309"/>how to develop and customize a RAG system when a ready-to-use one fails to meet the users’ expectations. This is important because human users can be dissatisfied with a response no matter what the performance metrics show. We will also explore the incorporation of human user rankings to gather expert feedback on our RAG-driven generative AI system. Finally, we will implement an automated ranking system that will decide how to augment the user input for the generative model, offering practical insights into how a RAG-driven system can be successfully implemented in a company.</p>
    <p class="normal">We will develop a proof of concept for a hypothetical company called <em class="italic">Company C</em>. This company would like to deploy a conversational agent that explains what AI is. The goal is for the employees of this company to understand the basic terms, concepts, and applications of AI. The ML engineer in charge of this RAG-driven generative AI example would like future users to acquire a better knowledge of AI while implementing other AI projects across the sales, production, and delivery domains.</p>
    <p class="normal">Company C currently faces serious issues with customer support. With a growing number of products and services, their product line of smartphones of the C-phone series has been experiencing technical problems with too many customer requests. The IT department would like to set up a conversational agent for these customers. However, the teams are not convinced. The IT department has thus decided to first set up a conversational agent to explain what an LLM is and how it can be helpful in the C-phone series customer support service.</p>
    <p class="normal">The program will be hybrid and adaptive to fulfill the needs of Company C:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Hybrid</strong>: Real-life scenarios go beyond theoretical frameworks and configurations. The system is hybrid because we are integrating HF within the retrieval process that can be processed in real time. However, we will not parse the content of the documents with a keyword alone. We will label the documents (which are Wikipedia URLs in this case), which can be done automatically, controlled, and improved <em class="italic">by a human</em>, if necessary. As we show in this chapter, some documents will be replaced by human-expert feedback and relabeled. The program will automatically retrieve human-expert feedback documents and raw retrieved documents to form a hybrid (human-machine) <em class="italic">dynamic</em> RAG system.</li>
      <li class="bulletList"><strong class="keyWord">Adaptive</strong>: We will introduce human user ranking, expert feedback, and automated document re-ranking. This HF loop takes us deep into modular RAG and adaptive RAG. Adaptive RAG leverages the flexibility of a RAG system to adapt its responses to the queries. In this case, we want HF to be triggered to improve the quality of the output.</li>
    </ul>
    <p class="normal">Real-life projects will inevitably require an ML engineer to go beyond the boundaries of pre-determined categories. Pragmatism and necessity encourage creative and innovative solutions. For example, for the hybrid, dynamic, and adaptive aspects of the system, ML engineers <a id="_idIndexMarker310"/>could imagine any process that works with any type of algorithm: classical software functions, ML clustering algorithms, or any function that works. In real-life AI, what works, works!</p>
    <p class="normal">It’s time to build a proof of concept to show Company C’s management how hybrid adaptive RAG-driven generative AI can successfully help their teams by:</p>
    <ul>
      <li class="bulletList">Proving that AI can work with a proof of concept before scaling and investing in a project</li>
      <li class="bulletList">Showing that an AI system can be customized for a specific project</li>
      <li class="bulletList">Developing solid ground-up skills to face any AI challenge</li>
      <li class="bulletList">Building the company’s data governance and control of AI systems</li>
      <li class="bulletList">Laying solid grounds to scale the system by solving the problems that will come up during the proof of concept</li>
    </ul>
    <p class="normal">Let’s go to our keyboards!</p>
    <h1 id="_idParaDest-127" class="heading-1">Building hybrid adaptive RAG in Python</h1>
    <p class="normal">Let’s now <a id="_idIndexMarker311"/>start building the proof of concept <a id="_idIndexMarker312"/>of a hybrid adaptive RAG-driven generative AI configuration. Open <code class="inlineCode">Adaptive_RAG.ipynb</code> on GitHub. We will focus on HF and, as such, will not use an existing framework. We will build our own pipeline and introduce HF.</p>
    <p class="normal">As established earlier, the program is divided into three separate parts: the <strong class="keyWord">retriever</strong>, <strong class="keyWord">generator</strong>, and <strong class="keyWord">evaluator</strong> functions, which can be separate agents in a real-life project’s pipeline. Try to separate these functions from the start because, in a project, several teams might be working in parallel on separate aspects of the RAG framework.</p>
    <div><p class="normal">The titles of each of the following sections correspond exactly to the names of each section in the program on GitHub. The retriever functionality comes first.</p>
    </div>
    <h2 id="_idParaDest-128" class="heading-2">1. Retriever</h2>
    <p class="normal">We will first outline the initial steps required to set up the environment for a RAG-driven generative <a id="_idIndexMarker313"/>AI model. This process begins with the installation of <a id="_idIndexMarker314"/>essential software components and libraries that facilitate the retrieval and processing of data. We specifically cover the downloading of crucial files and the installation of packages needed for effective data retrieval and web scraping.</p>
    <h3 id="_idParaDest-129" class="heading-3">1.1. Installing the retriever’s environment</h3>
    <p class="normal">Let’s begin by downloading <code class="inlineCode">grequests.py</code> from the <code class="inlineCode">commons</code> directory of the GitHub repository. This <a id="_idIndexMarker315"/>repository contains resources that can be common to several programs in the repository, thus avoiding redundancy.</p>
    <p class="normal">The download is standard and built around the request:</p>
    <pre class="programlisting code"><code class="hljs-code">url = "https://raw.githubusercontent.com/Denis2054/RAG-Driven-Generative-AI/main/commons/grequests.py"
output_file = "grequests.py"
</code></pre>
    <p class="normal">We will only need two packages for the retriever since we are building a RAG-driven generative AI model from scratch. We will install:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">requests</code>, the HTTP library to retrieve Wikipedia documents:
        <pre class="programlisting code-one"><code class="hljs-code">!pip install requests==2.32.3
</code></pre>
      </li>
      <li class="bulletList"><code class="inlineCode">beautifulsoup4</code>, to scrape information from web pages:
        <pre class="programlisting code-one"><code class="hljs-code">!pip install beautifulsoup4==4.12.3
</code></pre>
      </li>
    </ul>
    <p class="normal">We now need a dataset.</p>
    <h3 id="_idParaDest-130" class="heading-3">1.2.1. Preparing the dataset</h3>
    <p class="normal">For this proof of concept, we will retrieve Wikipedia documents by scraping them through <a id="_idIndexMarker316"/>their URLs. The dataset will contain automated or human-crafted labels for each document, which is the first step toward indexing the documents of a dataset:</p>
    <pre class="programlisting code"><code class="hljs-code">import requests
from bs4 import BeautifulSoup
import re
# URLs of the Wikipedia articles mapped to keywords
urls = {
    "prompt engineering": "https://en.wikipedia.org/wiki/Prompt_engineering",
    "artificial intelligence":"https://en.wikipedia.org/wiki/Artificial_intelligence",
    "llm": "https://en.wikipedia.org/wiki/Large_language_model",
    "llms": "https://en.wikipedia.org/wiki/Large_language_model"
}
</code></pre>
    <p class="normal">One or more labels precede each URL. This approach might be sufficient for a relatively small dataset.</p>
    <p class="normal">For specific projects, including a proof of concept, this approach can provide a solid first step to go from naïve RAG (content search with keywords) to searching a dataset with indexes (the labels in this case). We now have to process the data.</p>
    <h3 id="_idParaDest-131" class="heading-3">1.2.2. Processing the data</h3>
    <p class="normal">We first <a id="_idIndexMarker317"/>apply a standard scraping and text-cleaning function to the document that will be retrieved:</p>
    <pre class="programlisting code"><code class="hljs-code">def fetch_and_clean(url):
    # Fetch the content of the URL
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')
    # Find the main content of the article, ignoring side boxes and headers
    content = soup.find('div', {'class': 'mw-parser-output'})
    # Remove less relevant sections such as "See also", "References", etc.
    for section_title in ['References', 'Bibliography', 'External links', 'See also']:
        section = content.find('span', {'id': section_title})
        if section:
            for sib in section.parent.find_next_siblings():
                sib.decompose()
            section.parent.decompose()
    # Focus on extracting and cleaning text from paragraph tags only
    paragraphs = content.find_all('p')
    cleaned_text = ' '.join(paragraph.get_text(separator=' ', strip=True) for paragraph in paragraphs)
    cleaned_text = re.sub(r'\[\d+\]', '', cleaned_text)  # Remove citation markers like [1], [2], etc.
    return cleaned_text
</code></pre>
    <p class="normal">The code fetches the document’s content based on its URL, which is, in turn, based on its label. This straightforward approach may satisfy a project’s needs depending on its goals. An ML <a id="_idIndexMarker318"/>engineer or developer must always be careful not to overload a system with costly and unprofitable functions. Moreover, labeling website URLs can guide a retriever pipeline to the correct locations to process data, regardless of the techniques (load balancing, API call optimization, etc.) applied. In the end, each project or sub-project will require one or several techniques, depending on its specific needs.</p>
    <p class="normal">Once the fetching and cleaning function is ready, we can implement the retrieval process for the user’s input.</p>
    <h3 id="_idParaDest-132" class="heading-3">1.3. Retrieval process for user input</h3>
    <p class="normal">The first <a id="_idIndexMarker319"/>step here involves identifying a keyword within the user’s input. The function <code class="inlineCode">process_query</code> takes two parameters: <code class="inlineCode">user_input</code> and <code class="inlineCode">num_words</code>. The number of words to retrieve is restricted by factors like the input limitations of the model, cost considerations, and overall system performance:</p>
    <pre class="programlisting code"><code class="hljs-code">import textwrap
def process_query(user_input, num_words):
    user_input = user_input.lower()
    # Check for any of the specified keywords in the input
    matched_keyword = next((keyword for keyword in urls if keyword in user_input), None)
</code></pre>
    <p class="normal">Upon finding <a id="_idIndexMarker320"/>a match between a keyword in the user input and the keywords associated with URLs, the following functions for fetching and cleaning the data are triggered:</p>
    <pre class="programlisting code"><code class="hljs-code">if matched_keyword:
    print(f"Fetching data from: {urls[matched_keyword]}")
    cleaned_text = fetch_and_clean(urls[matched_keyword])
   
    # Limit the display to the specified number of words from the cleaned text
    words = cleaned_text.split()  # Split the text into words
    first_n_words = ' '.join(words[:num_words])  # Join the first n words into a single string
</code></pre>
    <p class="normal">The <code class="inlineCode">num_words</code> parameter helps in chunking the text. While this basic approach may work for use cases with a manageable volume of data, it’s recommended to embed the data into vectors for more complex scenarios.</p>
    <p class="normal">The cleaned and truncated text is then formatted for display:</p>
    <pre class="programlisting code"><code class="hljs-code">    # Wrap the first n words to 80 characters wide for display
    wrapped_text = textwrap.fill(first_n_words, width=80)
    print("\nFirst {} words of the cleaned text:".format(num_words))
    print(wrapped_text)  # Print the first n words as a well-formatted paragraph
    # Use the exact same first_n_words for the GPT-4 prompt to ensure consistency
    prompt = f"Summarize the following information about {matched_keyword}:\n{first_n_words}"
    wrapped_prompt = textwrap.fill(prompt, width=80)  # Wrap prompt text
    print("\nPrompt for Generator:", wrapped_prompt)
    # Return the specified number of words
    return first_n_words
else:
    print("No relevant keywords found. Please enter a query related to 'LLM', 'LLMs', or 'Prompt Engineering'.")
    return None
</code></pre>
    <p class="normal">Note that the <a id="_idIndexMarker321"/>function ultimately returns the first <code class="inlineCode">n</code> words, providing a concise and relevant snippet of information based on the user’s query. This design allows the system to manage data retrieval efficiently while also maintaining user engagement.</p>
    <h2 id="_idParaDest-133" class="heading-2">2. Generator</h2>
    <p class="normal">The generator <a id="_idIndexMarker322"/>ecosystem contains several components, several <a id="_idIndexMarker323"/>of which overlap with the retriever functions and user interfaces in the RAG-driven generative AI frameworks:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">2.1. Adaptive RAG selection based on human rankings</strong>: This will be based on the ratings of a user panel over time. In a real-life pipeline, this functionality could be a separate program.</li>
      <li class="bulletList"><strong class="keyWord">2.2. Input</strong>: In a real-life <a id="_idIndexMarker324"/>project, a <strong class="keyWord">user interface</strong> (<strong class="keyWord">UI</strong>) will manage the input. This interface and the associated process should be carefully designed in collaboration with the users, ideally in a workshop setting where their needs and preferences can be fully understood.</li>
      <li class="bulletList"><strong class="keyWord">2.3. Mean ranking simulation scenario</strong>: Calculating the mean value of the user evaluation scores and functionality.</li>
      <li class="bulletList"><strong class="keyWord">2.4. Checking the input before running the generator</strong>: Displaying the input.</li>
      <li class="bulletList"><strong class="keyWord">2.5. Installing the generative AI environment</strong>: The installation of the generative AI model’s environment, in this case, OpenAI, can be part of another environment in the pipeline in which other team members may be working, implementing, and deploying in production independently of the retriever functionality.</li>
      <li class="bulletList"><strong class="keyWord">2.6. Content generation</strong>: In this section of the program, an OpenAI model will process the input and provide a response that will be evaluated by the evaluator.</li>
    </ul>
    <p class="normal">Let’s begin by describing the adaptive RAG system.</p>
    <h3 id="_idParaDest-134" class="heading-3">2.1. Integrating HF-RAG for augmented document inputs</h3>
    <p class="normal">The <a id="_idIndexMarker325"/>dynamic nature of information retrieval and the necessity for contextually relevant data augmentation in generative AI models require a flexible system capable of adapting to varying levels <a id="_idIndexMarker326"/>of input quality. We introduce an <strong class="keyWord">adaptive RAG selection system</strong>, which employs HF scores to determine the optimal retrieval strategy for document implementation within the RAG ecosystem. Adaptive functionality takes us beyond naïve RAG and constitutes a hybrid RAG system.</p>
    <p class="normal">Human <a id="_idIndexMarker327"/>evaluators assign mean scores ranging from 1 to 5 to assess the relevance and quality of documents. These scores trigger distinct operational modes, as shown in the following figure:</p>
    <figure class="mediaobject"><img src="img/B31169_05_02.png" alt="A diagram of a system  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 5.2: Automated RAG triggers</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Scores of 1 to 2</strong> indicate a lack of compensatory capability by the RAG system, suggesting the need for maintenance or possibly model fine-tuning. RAG will be temporarily deactivated until the system is improved. The user input will be processed but there will be no retrieval.</li>
      <li class="bulletList"><strong class="keyWord">Scores of 3 to 4</strong> initiate an augmentation with human-expert feedback only, utilizing flashcards or snippets to refine the output. Document-based RAG will be deactivated, but the human-expert feedback data will augment the input.</li>
      <li class="bulletList"><strong class="keyWord">Scores of 5</strong> initiate keyword-search RAG enhanced by previously gathered HF when necessary, utilizing flashcards or targeted information snippets to refine the output. The user is not required to provide new feedback in this case.</li>
    </ul>
    <div><p class="normal">This program implements one of many scenarios. The scoring system, score levels, and triggers will vary from one project to another, depending on the specification goals to attain. It is recommended to organize workshops with a panel of users to decide how to implement this adaptive RAG system.</p>
    </div>
    <p class="normal">This <a id="_idIndexMarker328"/>adaptive approach aims to optimize the balance between automated retrieval and human insight, ensuring the generative model’s outputs are of the highest possible relevance and accuracy. Let’s now enter the input.</p>
    <h3 id="_idParaDest-135" class="heading-3">2.2. Input</h3>
    <p class="normal">A user <a id="_idIndexMarker329"/>of Company C is prompted to enter a question:</p>
    <pre class="programlisting code"><code class="hljs-code"># Request user input for keyword parsing
user_input = input("Enter your query: ").lower()
</code></pre>
    <p class="normal">In this example and program, we will focus on one question and topic: <code class="inlineCode">What is an LLM?</code>. The question appears and is memorized by the model:</p>
    <pre class="programlisting code"><code class="hljs-code">Enter your query: What is an LLM?
</code></pre>
    <div><p class="normal">This program is a proof of concept with a strategy and example for the panel of users in Company C who wish to understand an LLM. Other topics can be added, and the program can be expanded to meet further needs. It is recommended to organize workshops with a panel of users to decide the next steps.</p>
    </div>
    <p class="normal">We have prepared the environment and will now activate a RAG scenario.</p>
    <h3 id="_idParaDest-136" class="heading-3">2.3. Mean ranking simulation scenario</h3>
    <p class="normal">For the sake of this program, let’s assume that the human user feedback panel has been evaluating <a id="_idIndexMarker330"/>the hybrid adaptive RAG system for some time with the functions provided in sections <em class="italic">3.2. Human user rating</em> and <em class="italic">3.3. Human-expert evaluation</em>. The user feedback panel ranks the responses a number of times, which automatically updates by calculating the mean of the ratings and storing it in a ranking variable named <code class="inlineCode">ranking</code>. The <code class="inlineCode">ranking</code> score will help the management team decide whether to downgrade the rank of a document, upgrade it, or suppress documents through manual or automated functions. You can even simulate one of the scenarios described in the section <em class="italic">2.1. Integrating HF-RAG for augmented document inputs</em>.</p>
    <p class="normal">We will begin with a 1 to 5 ranking, which will deactivate RAG so that we can see the native response of the generative model:</p>
    <pre class="programlisting code"><code class="hljs-code">#Select a score between 1 and 5 to run the simulation
ranking=1
</code></pre>
    <p class="normal">Then, we will modify this value to activate RAG without additional human-expert feedback with <code class="inlineCode">ranking=5</code>. Finally, we will modify this value to activate human feedback RAG without retrieving documents with <code class="inlineCode">ranking=3</code>.</p>
    <p class="normal">In a real-life environment, these rankings will be triggered automatically with the functionality described in sections <em class="italic">3.2</em> and <em class="italic">3.3</em> after user feedback panel workshops are organized to define the system’s expected behavior. If you wish to run the three scenarios described in section <em class="italic">2.1</em>, make sure to initialize the <code class="inlineCode">text_input</code> variable that the generative model processes to respond:</p>
    <pre class="programlisting code"><code class="hljs-code"># initializing the text for the generative AI model simulations
text_input=[]
</code></pre>
    <p class="normal">Each time you switch scenarios, make sure to come back and reinitialize <code class="inlineCode">text_input</code>.</p>
    <div><p class="normal">Due to its probabilistic nature, the generative AI model’s output may vary from one run to another.</p>
    </div>
    <p class="normal">Let’s go through the three rating categories described in section <em class="italic">2.1</em>.</p>
    <h4 class="heading-4">Ranking 1–2: No RAG</h4>
    <p class="normal">The ranking of the generative AI’s output is very low. All RAG functionality is deactivated until <a id="_idIndexMarker331"/>the management team can analyze and improve the system. In this case, <code class="inlineCode">text_input</code> is equal to <code class="inlineCode">user_input</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">if ranking&gt;=1 and ranking&lt;3:
  text_input=user_input
</code></pre>
    <p class="normal">The generative AI model, in this case, GPT-4o, will generate the following output in section <em class="italic">2.6. Content generation</em>:</p>
    <pre class="programlisting con"><code class="hljs-con">GPT-4 Response:
---------------
It seems like you're asking about "LLM" which stands for "Language Model for Dialogue Applications" or more commonly referred to as a "Large Language Model."
An LLM is a type of artificial intelligence model designed to understand, generate, and interact with human language. These models are trained on vast amounts of text data and use this training to generate text, answer questions, summarize information, translate languages, and perform other language-related tasks. They are a subset of machine learning models known as transformers, which have been revolutionary in the field of natural language processing (NLP).
Examples of LLMs include OpenAI's GPT (Generative Pre-trained Transformer) series and Google's BERT (Bidirectional Encoder Representations from
Transformers).
---------------
</code></pre>
    <p class="normal">This output cannot satisfy the user panel of Company C in this particular use case. They cannot relate this explanation to their customer service issues. Furthermore, many users will not bother going further since they have described their needs to the management team and expect pertinent responses. Let’s see what human-expert feedback RAG can provide.</p>
    <h4 class="heading-4">Ranking 3–4: Human-expert feedback RAG</h4>
    <p class="normal">In this scenario, human-expert feedback (see <em class="italic">section 3.4. Human-expert evaluation</em>) was triggered <a id="_idIndexMarker332"/>by poor user feedback ratings with automated RAG documents <code class="inlineCode">(ranking=5)</code> and without RAG <code class="inlineCode">(ranking 1-2)</code>. The human-expert panel has filled in a flashcard, which has now been stored as an expert-level RAG document.</p>
    <p class="normal">The program first checks the ranking and activates HF retrieval:</p>
    <pre class="programlisting code"><code class="hljs-code">hf=False
if ranking&gt;3 and ranking&lt;5:
  hf=True
</code></pre>
    <p class="normal">The program will then fetch the proper document from an expert panel (selected experts within a corporation) dataset based on keywords, embeddings, or other search methods that fit the goals of a project. In this case, we assume we have found the right flashcard and download it:</p>
    <pre class="programlisting code"><code class="hljs-code">if hf==True:
  from grequests import download
  directory = "Chapter05"
  filename = "human_feedback.txt"
  download(directory, filename, private_token)
</code></pre>
    <p class="normal">We verify if the file exists and load its content, clean it, store it in <code class="inlineCode">content</code>, and assign it to <code class="inlineCode">text_input</code> for the GPT-4 model:</p>
    <pre class="programlisting code"><code class="hljs-code">if hf==True:
  # Check if 'human_feedback.txt' exists
    efile = os.path.exists('human_feedback.txt')
    if efile:
        # Read and clean the file content
        with open('human_feedback.txt', 'r') as file:
            content = file.read().replace('\n', ' ').replace('#', '')  # Removing new line and markdown characters
            #print(content)  # Uncomment for debugging or maintenance display
        text_input=content
        print(text_input)
    else:
      print("File not found")
      hf=False
</code></pre>
    <p class="normal">The content of the file explains both what an LLM is and how it can help Company C improve customer support:</p>
    <pre class="programlisting con"><code class="hljs-con">A Large Language Model (LLM) is an advanced AI system trained on vast amounts of text data to generate human-like text responses. It understands and generates language based on the patterns and information it has learned during training. LLMs are highly effective in various language-based tasks, including answering questions, making recommendations, and facilitating conversations. They can be continually updated with new information and trained to understand specific domains or industries.For the C-phone series customer support, incorporating an LLM could significantly enhance service quality and efficiency. The conversational agent powered by an LLM can provide instant responses to customer inquiries, reducing wait times and freeing up human agents for more complex issues. It can be programmed to handle common technical questions about the C-phone series, troubleshoot problems, guide users through setup processes, and offer tips for optimizing device performance. Additionally, it can be used to gather customer feedback, providing valuable insights into user experiences and product performance. This feedback can then be used to improve products and services. Furthermore, the LLM can be designed to escalate issues to human agents when necessary, ensuring that customers receive the best possible support at all levels. The agent can also provide personalized recommendations for customers based on their usage patterns and preferences, enhancing user satisfaction and loyalty.
</code></pre>
    <p class="normal">If you <a id="_idIndexMarker333"/>now run sections <em class="italic">2.4</em> and <em class="italic">2.5</em> once and section <em class="italic">2.6</em> to generate the content based on this <code class="inlineCode">text_input</code>, the response will be satisfactory:</p>
    <pre class="programlisting con"><code class="hljs-con">GPT-4 Response:
---------------
A Large Language Model (LLM) is a sophisticated AI system trained on extensive
text data to generate human-like text responses. It understands and generates
language based on patterns and information learned during training. LLMs are
highly effective in various language-based tasks such as answering questions,
making recommendations, and facilitating conversations. They can be continuously updated with new information and trained to understand specific domains or industries.  For the C-phone series customer support, incorporating an LLM could significantly enhance service quality and efficiency. The conversational agent powered by an LLM can provide instant responses to customer inquiries, reducing wait times and freeing up human agents for more complex issues. 
It can be programmed to handle common technical questions about the C-phone series,
troubleshoot problems, guide users through setup processes, and offer tips for
optimizing device performance. Additionally, it can be used to gather customer
feedback, providing valuable insights into user experiences and product
performance. This feedback can then be used to improve products and services.
Furthermore, the LLM can be designed to escalate issues to human agents when
necessary, ensuring that customers receive the best possible support at all
levels. The agent can also provide personalized recommendations for customers
based on their usage patterns and preferences, enhancing user satisfaction and
loyalty.
---------------
</code></pre>
    <p class="normal">The <a id="_idIndexMarker334"/>preceding response is now much better since it defines LLMs and also shows how to improve customer service for Company C’s C-phone series.</p>
    <p class="normal">We will take this further in <em class="chapterRef">Chapter 9</em>, <em class="italic">Empowering AI Models: Fine-Tuning RAG Data and Human Feedback</em>, in which we will fine-tune a generative model daily (or as frequently as possible) to improve its responses, thus alleviating the volume of RAG data. But for now, let’s see what the system can achieve without HF but with RAG documents.</p>
    <h4 class="heading-4">Ranking 5: RAG with no human-expert feedback documents</h4>
    <p class="normal">Some <a id="_idIndexMarker335"/>users do not require RAG documents that include human-expert RAG flashcards, snippets, or documents. This might be the case, particularly, if software engineers are the users.</p>
    <p class="normal">In this case, the maximum number of words is limited to 100 to optimize API costs, but can be modified as you wish using the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">if ranking&gt;=5:
  max_words=100 #Limit: the size of the data we can add to the input
  rdata=process_query(user_input,max_words)
  print(rdata) # for maintenance if necessary
  if rdata:
        rdata_clean = rdata.replace('\n', ' ').replace('#', '')
        rdata_sentences = rdata_clean.split('. ')
        print(rdata)
  text_input=rdata
  print(text_input)
</code></pre>
    <p class="normal">When we run the generative AI model, a reasonable output is produced that software engineers can relate to their business:</p>
    <pre class="programlisting con"><code class="hljs-con">GPT-4 Response:
---------------
A large language model (LLM) is a type of language model known for its
capability to perform general-purpose language generation and other natural language processing tasks such as classification. LLMs develop these abilities by learning statistical relationships from text documents through a computationally intensive training process that includes both self-supervised
and semi-supervised learning. These models can generate text, a form of
generative AI, by taking an input text and repeatedly predicting the next token or word. LLMs are based on artificial neural networks. As of March 2024, the most advanced and capable LLMs are constructed using a decoder-only transformer architecture.
---------------
</code></pre>
    <p class="normal">We can see that the output refers to March 2024 data, although GPT-4-turbo’s training cutoff date was in December 2023, as explained in OpenAI’s documentation: <a href="https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4">https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4</a>.</p>
    <div><p class="normal">In production, at the end-user level, the error in the output can come from the data retrieved or the generative AI model. This shows the importance of HF. In this case, this error will hopefully be corrected in the retrieval documents or by the generative AI model. But we left the error in to illustrate that HF is not an option but a necessity.</p>
    </div>
    <p class="normal">These temporal RAG augmentations clearly justify the need for RAG-driven generative AI. However, it remains up to the users to decide if these types of outputs are sufficient or require <a id="_idIndexMarker336"/>more corporate customization in closed environments, such as within or for a company.</p>
    <p class="normal">For the remainder of this program, let’s assume <code class="inlineCode">ranking&gt;=5</code> for the next steps to show how the evaluator is implemented in the <em class="italic">Evaluator</em> section. Let’s install the generative AI environment to generate content based on the user input and the document retrieved.</p>
    <h3 id="_idParaDest-137" class="heading-3">2.4.–2.5. Installing the generative AI environment</h3>
    <div><p class="normal"><em class="italic">2.4. Checking the input before running the generator</em> displays the user input and retrieved document before augmenting the input with this information. Then we continue to <em class="italic">2.5. Installing the generative AI environment</em>.</p>
    </div>
    <p class="normal">Only <a id="_idIndexMarker337"/>run this section once. If you modified the <a id="_idIndexMarker338"/>scenario in section <em class="italic">2.3</em>, you can skip this section to run the generative AI model again. This installation is not at the top of this notebook because a project team may choose to run this part of the program in another environment or even another server in production.</p>
    <p class="normal">It is recommended to separate the retriever and generator functions as much as possible since they might be activated by different programs and possibly at different times. One development team might only work on the retriever functions while another team works on the generator functions.</p>
    <p class="normal">We first install OpenAI:</p>
    <pre class="programlisting code"><code class="hljs-code">!pip install openai==1.40.3
</code></pre>
    <p class="normal">Then, we retrieve the API key. Store your OpenAI key in a safe location. In this case, it is stored on Google Drive:</p>
    <pre class="programlisting code"><code class="hljs-code">#API Key
#Store your key in a file and read it(you can type it directly in the notebook but it will be visible for somebody next to you)
from google.colab import drive
drive.mount('/content/drive')
f = open("drive/MyDrive/files/api_key.txt", "r")
API_KEY=f.readline().strip()
f.close()
#The OpenAI Key
import os
import openai
os.environ['OPENAI_API_KEY'] =API_KEY
openai.api_key = os.getenv("OPENAI_API_KEY")
</code></pre>
    <p class="normal">We <a id="_idIndexMarker339"/>are now <a id="_idIndexMarker340"/>all set for content generation.</p>
    <h3 id="_idParaDest-138" class="heading-3">2.6. Content generation</h3>
    <p class="normal">To generate <a id="_idIndexMarker341"/>content, we first import and set up what we need. We’ve <a id="_idIndexMarker342"/>introduced <code class="inlineCode">time</code> to measure the speed of the response and have chosen <code class="inlineCode">gpt-4o</code> as our conversational model:</p>
    <pre class="programlisting code"><code class="hljs-code">import openai
from openai import OpenAI
import time
client = OpenAI()
gptmodel="gpt-4o"
start_time = time.time()  # Start timing before the request
</code></pre>
    <p class="normal">We then define a standard Gpt-4o prompt, giving it enough information to respond and leaving the rest up to the model and RAG data:</p>
    <pre class="programlisting code"><code class="hljs-code">def call_gpt4_with_full_text(itext):
    # Join all lines to form a single string
    text_input = '\n'.join(itext)
    prompt = f"Please summarize or elaborate on the following content:\n{text_input}"
    try:
      response = client.chat.completions.create(
         model=gptmodel,
         messages=[
            {"role": "system", "content": "You are an expert Natural Language Processing exercise expert."},
            {"role": "assistant", "content": "1.You can explain read the input and answer in detail"},
            {"role": "user", "content": prompt}
         ],
         temperature=0.1  # Add the temperature parameter here and other parameters you need
        )
      return response.choices[0].message.content.strip()
    except Exception as e:
        return str(e)
</code></pre>
    <p class="normal">The <a id="_idIndexMarker343"/>code then <a id="_idIndexMarker344"/>formats the output:</p>
    <pre class="programlisting code"><code class="hljs-code">import textwrap
def print_formatted_response(response):
    # Define the width for wrapping the text
    wrapper = textwrap.TextWrapper(width=80)  # Set to 80 columns wide, but adjust as needed
    wrapped_text = wrapper.fill(text=response)
    # Print the formatted response with a header and footer
    print("GPT-4 Response:")
    print("---------------")
    print(wrapped_text)
    print("---------------\n")
# Assuming 'gpt4_response' contains the response from the previous GPT-4 call
print_formatted_response(gpt4_response)
</code></pre>
    <p class="normal">The response is satisfactory in this case, as we saw in section <em class="italic">2.3</em>. In the <code class="inlineCode">ranking=5</code> scenario, which is the one we are now evaluating, we get the following output:</p>
    <pre class="programlisting con"><code class="hljs-con">GPT-4 Response:
---------------
GPT-4 Response:
---------------
### Summary: A large language model (LLM) is a computational model known for its ability to perform general-purpose language generation and other natural language processing tasks, such as classification. LLMs acquire these abilities by learning statistical relationships from vast amounts of text during a computationally intensive self-supervised and semi-supervised training process.They can be used for text generation, a form of generative AI, by taking input text and repeatedly predicting the next token or word. LLMs are artificial neural networks that use the transformer architecture…
</code></pre>
    <p class="normal">The response <a id="_idIndexMarker345"/>looks fine, but is it really accurate? Let’s run <a id="_idIndexMarker346"/>the evaluator to find out.</p>
    <h2 id="_idParaDest-139" class="heading-2">3. Evaluator</h2>
    <p class="normal">Depending on each project’s specifications and needs, we can implement as many mathematical <a id="_idIndexMarker347"/>and human evaluation functions as necessary. In this section, we will implement two automatic metrics: response time and cosine similarity score. We will then implement two interactive evaluation functions: human user rating and human-expert evaluation.</p>
    <h3 id="_idParaDest-140" class="heading-3">3.1. Response time</h3>
    <p class="normal">The response time <a id="_idIndexMarker348"/>was calculated and displayed in the API call with:</p>
    <pre class="programlisting code"><code class="hljs-code">import time
…
start_time = time.time()  # Start timing before the request
…
response_time = time.time() - start_time  # Measure response time
print(f"Response Time: {response_time:.2f} seconds")  # Print response time
</code></pre>
    <p class="normal">In this case, we can display the response time without further development:</p>
    <pre class="programlisting code"><code class="hljs-code">print(f"Response Time: {response_time:.2f} seconds")  # Print response time
</code></pre>
    <p class="normal">The output will vary depending on internet connectivity and the capacity of OpenAI’s servers. In this case, the output is:</p>
    <pre class="programlisting con"><code class="hljs-con">Response Time: 7.88 seconds
</code></pre>
    <p class="normal">It seems long, but online conversational agents take some time to answer as well. Deciding if this performance is sufficient remains a management decision. Let’s run the cosine similarity score next.</p>
    <h3 id="_idParaDest-141" class="heading-3">3.2. Cosine similarity score</h3>
    <p class="normal">Cosine similarity measures the cosine of the angle between two non-zero vectors. In the context <a id="_idIndexMarker349"/>of text analysis, these vectors are typically <strong class="keyWord">TF-IDF</strong> (<strong class="keyWord">Term Frequency-Inverse Document Frequency</strong>) representations of the text, which <a id="_idIndexMarker350"/>weigh terms based on their importance relative to the document and a corpus.</p>
    <p class="normal">GPT-4o’s input, which is <code class="inlineCode">text_input</code>, and the model’s response, which is <code class="inlineCode">gpt4_response</code>, are treated by TF-IDF as two separate “documents.” The <code class="inlineCode">vectorizer</code> transforms the documents into vectors. Then, vectorization considers how terms are shared and emphasized between the input and the response with the <code class="inlineCode">vectorizer.fit_transform([text1, text2])</code>.</p>
    <p class="normal">The goal is to quantify the thematic and lexical overlap through the following function:</p>
    <pre class="programlisting code"><code class="hljs-code">from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
def calculate_cosine_similarity(text1, text2):
    vectorizer = TfidfVectorizer()
    tfidf = vectorizer.fit_transform([text1, text2])
    similarity = cosine_similarity(tfidf[0:1], tfidf[1:2])
    return similarity[0][0]
# Example usage with your existing functions
similarity_score = calculate_cosine_similarity(text_input, gpt4_response)
print(f"Cosine Similarity Score: {similarity_score:.3f}")
</code></pre>
    <p class="normal">Cosine similarity relies on <code class="inlineCode">TfidfVectorizer</code> to transform the two documents into TF-IDF vectors. The <code class="inlineCode">cosine_similarity</code> function then calculates the similarity between these vectors. A result of <code class="inlineCode">1</code> indicates identical texts, while <code class="inlineCode">0</code> shows no similarity. The output of the function is:</p>
    <pre class="programlisting con"><code class="hljs-con">Cosine Similarity Score: 0.697
</code></pre>
    <p class="normal">The score shows a strong similarity between the input and the output of the model. But how will a human user rate this response? Let’s find out.</p>
    <h3 id="_idParaDest-142" class="heading-3">3.3. Human user rating</h3>
    <p class="normal">The human user rating interface provides human user feedback. As reiterated throughout <a id="_idIndexMarker351"/>this chapter, I recommend designing this interface and process after fully understanding user needs through a workshop with them. In this section, we will assume that the human user panel is a group of software developers testing the system.</p>
    <p class="normal">The code begins with the interface’s parameters:</p>
    <pre class="programlisting code"><code class="hljs-code"># Score parameters
counter=20                     # number of feedback queries
score_history=30               # human feedback
threshold=4                    # minimum rankings to trigger human expert feedback
</code></pre>
    <p class="normal">In this simulation, the parameters show that the system has computed human feedback:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">counter=20</code> shows the number of ratings already entered by the users</li>
      <li class="bulletList"><code class="inlineCode">score_history=60</code> shows the total score of the 20 ratings</li>
      <li class="bulletList"><code class="inlineCode">threshold=4</code> states the minimum mean rating, <code class="inlineCode">score_history/counter</code>, to obtain without triggering a human-expert feedback request</li>
    </ul>
    <p class="normal">We will now run the interface to add an instance to these parameters. The provided Python code defines the <code class="inlineCode">evaluate_response</code> function, designed to assess the relevance and coherence of responses generated by a language model such as GPT-4. Users rate the generated text on a scale from <code class="inlineCode">1</code> (poor) to <code class="inlineCode">5</code> (excellent), with the function ensuring valid input through recursive checks. The code calculates statistical metrics like mean scores to gauge the model’s performance over multiple evaluations.</p>
    <p class="normal">The evaluation function is a straightforward feedback request to obtain values between <code class="inlineCode">1</code> and <code class="inlineCode">5</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">import numpy as np
def evaluate_response(response):
    print("\nGenerated Response:")
    print(response)
    print("\nPlease evaluate the response based on the following criteria:")
    print("1 - Poor, 2 - Fair, 3 - Good, 4 - Very Good, 5 - Excellent")
    score = input("Enter the relevance and coherence score (1-5): ")
    try:
        score = int(score)
        if 1 &lt;= score &lt;= 5:
            return score
        else:
            print("Invalid score. Please enter a number between 1 and 5.")
            return evaluate_response(response)  # Recursive call if the input is invalid
    except ValueError:
        print("Invalid input. Please enter a numerical value.")
        return evaluate_response(response)  # Recursive call if the input is invalid
</code></pre>
    <p class="normal">We then <a id="_idIndexMarker352"/>call the function:</p>
    <pre class="programlisting code"><code class="hljs-code">score = evaluate_response(gpt4_response)
print("Evaluator Score:", score)
</code></pre>
    <p class="normal">The function first displays the response, as shown in the following excerpt:</p>
    <pre class="programlisting con"><code class="hljs-con">Generated Response:
### Summary:
A large language model (LLM) is a computational model…
</code></pre>
    <p class="normal">Then, the user enters an evaluation score between 1 and 5, which is <code class="inlineCode">1</code> in this case:</p>
    <pre class="programlisting con"><code class="hljs-con">Please evaluate the response based on the following criteria:
1 - Poor, 2 - Fair, 3 - Good, 4 - Very Good, 5 - Excellent
Enter the relevance and coherence score (1-5): 3
</code></pre>
    <p class="normal">The code then computes the statistics:</p>
    <pre class="programlisting code"><code class="hljs-code">counter+=1
score_history+=score
mean_score=round(np.mean(score_history/counter), 2)
if counter&gt;0:
  print("Rankings      :", counter)
  print("Score history : ", mean_score)
</code></pre>
    <p class="normal">The output shows a relatively very low rating:</p>
    <pre class="programlisting con"><code class="hljs-con">Evaluator Score: 3
Rankings      : 21
Score history :  3.0
</code></pre>
    <p class="normal">The evaluator score is <code class="inlineCode">3</code>, the overall ranking is <code class="inlineCode">3</code>, and the score history is <code class="inlineCode">3</code> also! Yet, the cosine similarity <a id="_idIndexMarker353"/>was positive. The human-expert evaluation request will be triggered because we set the threshold to <code class="inlineCode">4</code>:</p>
    <pre class="programlisting con"><code class="hljs-con">threshold=4                   
</code></pre>
    <p class="normal">What’s going on? Let’s ask an expert and find out!</p>
    <h3 id="_idParaDest-143" class="heading-3">3.4. Human-expert evaluation</h3>
    <p class="normal">Metrics such as cosine similarity indeed measure similarity but not in-depth accuracy. Time <a id="_idIndexMarker354"/>performance will not determine the accuracy of a response either. But if the rating is too low, why is that? Because the user is not satisfied with the response!</p>
    <p class="normal">The code first downloads thumbs-up and thumbs-down images for the human-expert user:</p>
    <pre class="programlisting code"><code class="hljs-code">from grequests import download
# Define your variables
directory = "commons"
filename = "thumbs_up.png"
download(directory, filename, private_token)
# Define your variables
directory = "commons"
filename = "thumbs_down.png"
download(directory, filename, private_token)
</code></pre>
    <p class="normal">The parameters to trigger an expert’s feedback are <code class="inlineCode">counter_threshold</code> and <code class="inlineCode">score_threshold</code>. The number of user ratings must exceed the expert’s threshold counter, which is <code class="inlineCode">counter_threshold=10</code>. The threshold of the mean score of the ratings is <code class="inlineCode">4</code> in this scenario: <code class="inlineCode">score_threshold=4</code>. We can now simulate the triggering of an expert feedback request:</p>
    <pre class="programlisting code"><code class="hljs-code">if counter&gt;counter_threshold and score_history&lt;=score_threshold:
  print("Human expert evaluation is required for the feedback loop.")
</code></pre>
    <p class="normal">In this case, the output will confirm the expert feedback loop because of the poor mean ratings and the number of times the users rated the response:</p>
    <pre class="programlisting con"><code class="hljs-con">Human expert evaluation is required for the feedback loop.
</code>
expert_feedback.txt</code>, as shown in the following excerpt of the code:</pre>
    <pre class="programlisting code"><code class="hljs-code">import base64
from google.colab import output
from IPython.display import display, HTML
def image_to_data_uri(file_path):
    """
    Convert an image to a data URI.
    """
    with open(file_path, 'rb') as image_file:
        encoded_string = base64.b64encode(image_file.read()).decode()
        return f'data:image/png;base64,{encoded_string}'
thumbs_up_data_uri = image_to_data_uri('/content/thumbs_up.png')
thumbs_down_data_uri = image_to_data_uri('/content/thumbs_down.png')
def display_icons():
    # Define the HTML content with the two clickable images
…/…
def save_feedback(feedback):
    with open('/content/expert_feedback.txt', 'w') as f:
        f.write(feedback)
    print("Feedback saved successfully.")
# Register the callback
output.register_callback('notebook.save_feedback', save_feedback)
print("Human Expert Adaptive RAG activated")
# Display the icons with click handlers
display_icons()
</code></pre>
    <p class="normal">The code will display the icons shown in the following figure. If the expert user presses the thumbs-down icon, they will be prompted to enter feedback.</p>
    <figure class="mediaobject"><img src="img/B31169_05_03.png" alt="A thumbs down and thumbs down  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 5.3: Feedback icons</p>
    <p class="normal">You can add a function for thumbs-down meaning that the response was incorrect and that the <a id="_idIndexMarker356"/>management team has to communicate with the user panel or add a prompt to the user feedback interface. This is a management decision, of course. In our scenario, the human expert pressed the thumbs-down icon and was prompted to enter a response:</p>
    <figure class="mediaobject"><img src="img/B31169_05_04.png" alt="A screenshot of a computer  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 5.4: “Enter feedback” prompt</p>
    <p class="normal">The human expert provided the response, which was saved in <code class="inlineCode">'/content/expert_feedback.txt'</code>. Through this, we have finally discovered the inaccuracy, which is in the content of the file displayed in the following cell:</p>
    <pre class="programlisting con"><code class="hljs-con">There is an inaccurate statement in the text:
"As of March 2024, the largest and most capable LLMs are built with a decoder-only transformer-based architecture."
This statement is not accurate because the largest and most capable Large Language Models, such as Meta's Llama models, have a transformer-based architecture, but they are not "decoder-only." These models use the architecture of the transformer, which includes both encoder and decoder components.
</code></pre>
    <p class="normal">The preceding <a id="_idIndexMarker357"/>expert’s feedback can then be used to improve the RAG dataset. With this, we have explored the depths of HF-RAG interactions. Let’s summarize our journey and move on to the next steps.</p>
    <h1 id="_idParaDest-144" class="heading-1">Summary</h1>
    <p class="normal">As we wrap up our hands-on approach to pragmatic AI implementations, it’s worth reflecting on the transformative journey we’ve embarked on together, exploring the dynamic world of adaptive RAG. We first examined how HF not only complements but also critically enhances generative AI, making it a more powerful tool customized to real-world needs. We described the adaptive RAG ecosystem and then went hands-on, building from the ground up. Starting with data collection, processing, and querying, we integrated these elements into a RAG-driven generative AI system. Our approach wasn’t just about coding; it was about adding adaptability to AI through continuous HF loops.</p>
    <p class="normal">By augmenting GPT-4’s capabilities with expert insights from previous sessions and end-user evaluations, we demonstrated the practical application and significant impact of HF. We implemented a system where the output is not only generated but also ranked by end-users. Low rankings triggered an expert feedback loop, emphasizing the importance of human intervention in refining AI responses. Building an adaptive RAG program from scratch ensured a deep understanding of how integrating HF can shift a standard AI system to one that evolves and improves over time.</p>
    <p class="normal">This chapter wasn’t just about learning; it was about doing, reflecting, and transforming theoretical knowledge into practical skills. We are now ready to scale RAG-driven AI to production-level volumes and complexity in the next chapter.</p>
    <h1 id="_idParaDest-145" class="heading-1">Questions</h1>
    <p class="normal">Answer the following questions with <em class="italic">Yes</em> or <em class="italic">No</em>:</p>
    <ol>
      <li class="numberedList" value="1">Is human feedback essential in improving RAG-driven generative AI systems? </li>
      <li class="numberedList">Can the core data in a generative AI model be changed without retraining the model? </li>
      <li class="numberedList">Does Adaptive RAG involve real-time human feedback loops to improve retrieval? </li>
      <li class="numberedList">Is the primary focus of Adaptive RAG to replace all human input with automated responses? </li>
      <li class="numberedList">Can human feedback in Adaptive RAG trigger changes in the retrieved documents? </li>
      <li class="numberedList">Does Company C use Adaptive RAG solely for customer support issues? </li>
      <li class="numberedList">Is human feedback used only when the AI responses have high user ratings? </li>
      <li class="numberedList">Does the program in this chapter provide only text-based retrieval outputs? </li>
      <li class="numberedList">Is the Hybrid Adaptive RAG system static, meaning it cannot adjust based on feedback? </li>
      <li class="numberedList">Are user rankings completely ignored in determining the relevance of AI responses?</li>
    </ol>
    <h1 id="_idParaDest-146" class="heading-1">References</h1>
    <ul>
      <li class="bulletList"><em class="italic">Studying Large Language Model Behaviors Under Realistic Knowledge Conflicts</em> by Evgenii Kortukov, Alexander Rubinstein, Elisa Nguyen, Seong Joon Oh: <a href="https://arxiv.org/abs/2404.16032">https://arxiv.org/abs/2404.16032</a></li>
      <li class="bulletList">OpenAI models: <a href="https://platform.openai.com/docs/models">https://platform.openai.com/docs/models</a></li>
    </ul>
    <h1 id="_idParaDest-147" class="heading-1">Further reading</h1>
    <p class="normal">For more information on the vectorizer and cosine similarity functionality implemented in this chapter, use the following links:</p>
    <ul>
      <li class="bulletList">Feature extraction – <code class="inlineCode">TfidfVectorizer</code>: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html">https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html</a></li>
      <li class="bulletList"><code class="inlineCode">sklearn.metrics</code> – <code class="inlineCode">cosine_similarity</code>: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html</a></li>
    </ul>
    <h1 id="_idParaDest-148" class="heading-1">Join our community on Discord</h1>
    <p class="normal">Join our community’s Discord space for discussions with the author and other readers:</p>
    <p class="normal"><a href="https://www.packt.link/rag">https://www.packt.link/rag</a></p>
    <p class="normal"><img src="img/QR_Code50409000288080484.png" alt=""/></p>
  </div>
</body></html>