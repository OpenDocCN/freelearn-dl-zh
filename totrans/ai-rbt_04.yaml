- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Recognizing Objects Using Neural Networks and Supervised Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用神经网络和监督学习识别对象
- en: This is the chapter where we’ll start to combine **robotics** and **artificial
    intelligence** (**AI**) to accomplish some of the tasks we laid out so carefully
    in previous chapters. The subject of this chapter is **object recognition** –
    we will be teaching the robot to recognize what a toy is so that it can then decide
    what to pick up and what to leave alone. We will be using **convolutional neural
    networks** (**CNNs**) as machine learning tools for separating objects in images,
    recognizing them, and locating them in the camera frame so that the robot can
    then locate them. More specifically, we’ll be using images to recognize objects.
    We’ll be taking a picture and then looking to see whether the computer recognizes
    specific types of objects in those pictures. We won’t be recognizing objects themselves,
    but rather images or pictures of objects. We’ll also be putting bounding boxes
    around objects, separating them from other objects and background pixels.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们将开始将**机器人技术**和**人工智能**（**AI**）结合起来以完成我们在前几章中仔细规划的一些任务的章节。本章的主题是**对象识别**
    – 我们将教会机器人识别玩具，以便它可以决定要捡起什么，留下什么。我们将使用**卷积神经网络**（**CNNs**）作为机器学习工具，在图像中分离对象、识别它们并在相机帧中定位它们，以便机器人可以找到它们。更具体地说，我们将使用图像来识别对象。我们将拍照，然后查看计算机是否在那些照片中识别特定类型的对象。我们不会识别对象本身，而是识别对象的图像或图片。我们还将围绕对象放置边界框，将它们与其他对象和背景像素分开。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: A brief overview of image processing
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像处理简要概述
- en: Understanding our object recognition task
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解我们的对象识别任务
- en: Image manipulation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像处理
- en: Using YOLOv8 – an object recognition model
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用YOLOv8 – 一个对象识别模型
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You will be able to accomplish all of this chapter’s tasks without a robot if
    yours cannot walk yet. We will, however, get better results if the camera is in
    the proper position on the robot. If you don’t have a robot, you can still do
    all of these tasks with a laptop and a USB camera.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的机器人还不能行走，您将能够完成本章的所有任务而无需机器人。然而，如果摄像头在机器人上的位置正确，我们将获得更好的结果。如果您没有机器人，您仍然可以使用笔记本电脑和USB摄像头完成所有这些任务。
- en: 'Overall, here’s the hardware and software that you will need to complete the
    tasks in this chapter:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，以下是您完成本章任务所需的硬件和软件：
- en: 'Hardware:'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 硬件：
- en: A laptop computer
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 笔记本电脑
- en: Nvidia Jetson Nano
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nvidia Jetson Nano
- en: USB camera
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: USB摄像头
- en: 'Software:'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 软件：
- en: Python 3
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 3
- en: OpenCV2
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenCV2
- en: TensorFlow
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow
- en: YOLOv8, which is available at [https://github.com/ultralytics/ultralytics](https://github.com/ultralytics/ultralytics)
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可在[https://github.com/ultralytics/ultralytics](https://github.com/ultralytics/ultralytics)找到的YOLOv8
- en: The source code for this chapter can be found at [https://github.com/PacktPublishing/Artificial-Intelligence-for-Robotics-2e](https://github.com/PacktPublishing/Artificial-Intelligence-for-Robotics-2e).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的源代码可在[https://github.com/PacktPublishing/Artificial-Intelligence-for-Robotics-2e](https://github.com/PacktPublishing/Artificial-Intelligence-for-Robotics-2e)找到。
- en: In the next section, we will discuss what image processing is.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论什么是图像处理。
- en: A brief overview of image processing
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像处理简要概述
- en: Most of you will be very familiar with computer images, formats, pixel depths,
    and maybe even convolutions. We will be discussing these concepts in the following
    sections; if you already know this, skip ahead. If this is new territory, read
    carefully, because everything we’ll do after is based on this information.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数人对计算机图像、格式、像素深度甚至卷积都非常熟悉。我们将在以下章节中讨论这些概念；如果您已经了解这些，可以跳过。如果这是新领域，请仔细阅读，因为我们将要做的一切都基于这些信息。
- en: Images are stored in a computer as a two-dimensional array of **pixels** or
    picture elements. Each pixel is a tiny dot. Thousands or millions of tiny dots
    make up each image. Each pixel is a number or series of numbers that describe
    its color. If the image is only a grayscale or black-and-white image, then each
    pixel is represented by a single number that corresponds to how dark or light
    the tiny dot is. This is straightforward so far.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图像在计算机中以**像素**或图像元素组成的二维数组形式存储。每个像素是一个小点。数千或数百万个小点构成了每一幅图像。每个像素是一个或一系列数字，描述了其颜色。如果图像仅是灰度或黑白图像，那么每个像素由一个数字表示，该数字对应于小点的明暗程度。到目前为止，这很简单。
- en: If the image is a color picture, then each dot has three numbers that are combined
    to make its color. Usually, these numbers are the intensity of **Red, Green, and
    Blue** (**RGB**) colors. The combination (0,0,0) represents black (or the absence
    of all colors), while (255,255,255) is white (the sum of all colors). This process
    is called the additive color model. If you work with watercolors instead of computer
    pixels, you’ll know that adding all the colors in your watercolor box makes black
    – that is a subtractive color model. Red, green, and blue are primary colors that
    can be used to make all of the other colors. Since an RGB pixel is represented
    by three colors, the actual image is a three-dimensional array rather than a two-dimensional
    one since each pixel has three numbers, making an array of (height, width, 3).
    So, a picture that is 800 x 600 pixels would be represented by an array of dimensions
    given by (800,600,3), or 1,440,000\. That is a lot of numbers. We will be working
    very hard to minimize the number of pixels we are processing at any given time.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果图像是彩色图片，那么每个点有三个数字组合起来形成其颜色。通常，这些数字是**红、绿、蓝**（**RGB**）颜色的强度。组合（0,0,0）代表黑色（或所有颜色的缺失），而（255,255,255）是白色（所有颜色的总和）。这个过程称为加色模型。如果你用水彩而不是计算机像素工作，你会知道将你水彩盒中的所有颜色混合在一起会得到黑色——这是一个减色模型。红、绿、蓝是原色，可以用来制作所有其他颜色。由于RGB像素由三种颜色表示，所以实际图像是一个三维数组，而不是二维数组，因为每个像素有三个数字，形成一个（高度，宽度，3）的数组。因此，800
    x 600像素的图片将表示为一个（800,600,3）维度的数组，或1,440,000个数字。这有很多数字。我们将非常努力地减少在任何给定时间处理的像素数量。
- en: 'While RGB is one set of three numbers that can describe a pixel, there are
    other ways of describing the **color formula** that have various usages. We don’t
    have to use RGB – for instance, we can also use **Cyan, Yellow, and Magenta**
    (**CYM**), which are the complementary colors to RGB, as shown in *Figure 4**.2*.
    We can also break down colors using the **Hue, Saturation, and Value** (**HSV**)
    model, which classifies color by hue (shade of color), saturation (intensity of
    color), and value (brightness of color). HSV is a very useful color space for
    certain calculations, such as converting a color image into grayscale (black and
    white). To turn RGB into a grayscale pixel, you have to do a bit of math – you
    can’t just pull out one channel and keep it. The formula for RGB to grayscale,
    as defined by the **National Television System Committee** (**NTSC**), is as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然**RGB**是一组可以描述像素的三个数字，但还有其他描述**颜色公式**的方法，这些方法有各种用途。我们不必使用RGB——例如，我们还可以使用**青色、黄色和品红色**（**CMY**），它们是RGB的互补色，如图*4**.2*所示。我们还可以使用**色调、饱和度和值**（**HSV**）模型来分解颜色，该模型通过色调（颜色的阴影）、饱和度（颜色的强度）和值（颜色的亮度）来分类颜色。HSV是某些计算非常有用的颜色空间，例如将彩色图像转换为灰度（黑白）。要将RGB转换为灰度像素，你必须做一些数学运算——你不能只是拉出一个通道并保留它。RGB到灰度的公式，如**国家电视系统委员会**（**NTSC**）定义的，如下所示：
- en: '*0.299*Red + 0.587*Green +* *0.114*Blue*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*0.299*红 + 0.587*绿 + *0.114*蓝'
- en: This is because the different wavelengths of light behave differently in our
    eyes, which are more sensitive to green. If you have color in the HSV color model,
    then creating a grayscale image involves considering *V* (value) and throwing
    the *H* and *S* values away. As you can imagine, this is a lot simpler. This is
    important to understand as we will be doing quite a bit of image manipulation
    throughout this chapter. But first, in the following section, we’ll discuss the
    image recognition task we will be performing in this chapter.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为不同波长的光在我们眼睛中的表现不同，我们眼睛对绿色更为敏感。如果你在HSV颜色模型中有颜色，那么创建灰度图像需要考虑*V*（值）并丢弃*H*和*S*值。正如你可以想象的那样，这要简单得多。这一点很重要，因为在本章中我们将进行大量的图像处理。但在开始之前，在接下来的部分，我们将讨论本章将要执行图像识别任务。
- en: Understanding our object recognition task
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解我们的目标识别任务
- en: Having a computer or robot recognize an image of a toy is not as simple as taking
    two pictures and then saying `if picture A = picture B, then toy`. We are going
    to have to do quite a bit of work to be able to recognize a variety of objects
    that are randomly rotated, strewn about, and at various distances. We could write
    a program to recognize simple shapes – hexagons, for instance, or simple color
    blobs – but nothing as complex as a toy stuffed dog. Writing a program that did
    some sort of analysis of an image and computed the pixels, colors, distributions,
    and ranges of every possible permutation would be extremely difficult, and the
    result would be very fragile – it would fail at the slightest change in lighting
    or color.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让计算机或机器人识别玩具的图像并不像拍两张照片然后说“如果图片 A 等于图片 B，那么是玩具”那么简单。我们需要做大量的工作才能识别出各种随机旋转、散布在不同位置且距离不同的物体。我们可以编写一个程序来识别简单的形状——例如六边形或简单的颜色块——但无法像填充狗玩具那样复杂。编写一个对图像进行某种分析并计算每个可能排列的像素、颜色、分布和范围的程序将极其困难，而且结果将非常脆弱——它会在光线或颜色发生最轻微的变化时失败。
- en: Speaking from experience, I had a recent misadventure with a large robot that
    used a traditional computer vision system to find its battery charger station.
    That robot mistook an old, faded soft drink machine for its charger – let’s just
    say that I had to go buy more fuses.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 从经验出发，我最近遇到了一个大型机器人，它使用传统的计算机视觉系统来寻找其电池充电站。那个机器人将一个旧、褪色的饮料机误认为是其充电器——让我们说，我不得不去买更多的保险丝。
- en: What we will do instead is teach the robot to recognize a set of images corresponding
    to toys that we will take from various angles. We will do this by using a special
    type of **artificial neural network** (**ANN**) that performs convolution operations
    on images. It is classified as an AI technique because instead of programming
    our software to recognize objects by writing code, we will be training a neural
    network to correctly *segment* (separate from the rest of the image) and *label*
    (classify) groups of pixels in an image by how closely they resemble groups of
    labeled pixels that the network was trained on. Rather than the code determining
    the robot’s behavior, it is the data we train the network on that does the work.
    Since we (the humans) will be training the neural network by providing segmented
    and labeled images, this is called **supervised learning**. This involves telling
    the network what we want it to learn and reinforcing (rewarding) the network based
    on how well it performs. We’ll discuss **unsupervised learning** in [*Chapter
    8*](B19846_08.xhtml#_idTextAnchor235). This process entails us not telling the
    software exactly what to learn, which means it must determine that for itself.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将采取的做法是教机器人识别一组与我们从不同角度拍摄的玩具对应的图像。我们将通过使用一种特殊的**人工神经网络**（**ANN**）来实现这一点，该网络对图像执行卷积操作。它被归类为人工智能技术，因为我们不是通过编写代码来编程软件识别物体，而是训练一个神经网络，使其能够通过如何接近网络训练时学习到的标记像素组来正确地**分割**（从图像的其余部分分离）和**标记**（分类）图像中的像素组。而不是代码决定机器人的行为，而是我们训练网络时所使用的数据来完成这项工作。由于我们将通过提供分割和标记的图像来训练神经网络，因此这被称为**监督学习**。这涉及到告诉网络我们希望它学习的内容，并根据其表现的好坏来强化（奖励）网络。我们将在[*第
    8 章*](B19846_08.xhtml#_idTextAnchor235)中讨论**无监督学习**。这个过程涉及到我们不对软件确切地说明要学习的内容，这意味着它必须自己确定这一点。
- en: To clarify, in this section, we will tell the ANN what we want it to learn,
    which in this case is to recognize a class of objects we will call *toys*, and
    to draw a bounding box around those objects. This bounding box will tell other
    parts of the robot that a toy is visible and where it is in the image.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了澄清，在本节中，我们将告诉人工神经网络（ANN）我们希望它学习的内容，在本例中是识别我们称之为“玩具”的一类物体，并在这些物体周围绘制边界框。这个边界框将告诉机器人的其他部分玩具是可见的，以及它在图像中的位置。
- en: I’ll be emphasizing the unique components we will use to accomplish our task
    of recognizing toys in an image. Do you remember what the storyboard from [*Chapter
    3*](B19846_03.xhtml#_idTextAnchor043) told us to do?
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我将强调我们将使用的独特组件来完成识别图像中玩具的任务。你还记得[*第 3 章*](B19846_03.xhtml#_idTextAnchor043)中的故事板告诉我们做什么吗？
- en: '![Figure 4.1 – Use case for identifying objects as toys](img/B19846_04_1.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.1 – 识别玩具用例](img/B19846_04_1.jpg)'
- en: Figure 4.1 – Use case for identifying objects as toys
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1 – 识别玩具用例
- en: Our image recognizer must figure out what objects are toys and then locate them
    in the image. This is illustrated in the preceding sketch; objects marked as toys
    have circles around them. The image recognizer must recognize not just *what*
    they are, but also *where* they are.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的图像识别器必须确定哪些是玩具，然后在图像中定位它们。这在前面的草图中有说明；标记为玩具的物体周围有圆圈。图像识别器不仅要识别它们是什么，还要识别它们在哪里。
- en: Image manipulation
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像处理
- en: 'So, now that we have an image, what can we do with it? You have probably played
    with Adobe Photoshop or some other image manipulation program such as GIMP, and
    you know that there are hundreds of operations, filters, changes, and tricks you
    can perform on images. For instance, can make an image brighter or darker by adjusting
    the brightness. We can increase the contrast between the white parts of the image
    and the dark parts. We can make an image blurry, usually by applying a Gaussian
    blur filter. We can also make an image sharper (somewhat) by using a filter such
    as an unsharp mask. You can also use an edge detector filter, such as the Canny
    filter, to isolate the edges of an image, where color or value changes. We will
    be using all of these techniques to help the computer identify images:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，现在我们有了图像，我们能用它做什么呢？你可能玩过Adobe Photoshop或其他图像处理程序，如GIMP，你知道可以在图像上执行数百种操作、过滤器、更改和技巧。例如，可以通过调整亮度使图像变得更亮或更暗。我们可以增加图像白色部分和黑色部分之间的对比度。我们可以通过应用高斯模糊过滤器使图像变得模糊。我们还可以通过使用如非锐化掩模之类的过滤器使图像（在一定程度上）变得更清晰。你还可以使用边缘检测过滤器，如Canny过滤器，来隔离图像的边缘，其中颜色或值发生变化。我们将使用所有这些技术来帮助计算机识别图像：
- en: '![Figure 4.2 – Various convolutions applied to an image](img/B19846_04_2.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图4.2 – 应用到图像上的各种卷积](img/B19846_04_2.jpg)'
- en: Figure 4.2 – Various convolutions applied to an image
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2 – 应用到图像上的各种卷积
- en: By performing these manipulations, we want the computer to not have the computer
    software be sensitive to the size of the image, which is called **scale invariance**,
    the angle at which the photograph was taken, or **angle invariance**, and the
    lighting available, which is known as **illumination invariance**. This is all
    very desirable in a computer vision system – we would not want an AI system that
    only recognizes our toys from the same angle and distance as the original image.
    Remember, we are going to train our vision system to recognize toys based on a
    labeled set of training images we take in advance, and the robot will have to
    recognize objects based on what it learned from the training set. Here, we are
    going to use features from the image that mostly don’t change based on size, angle,
    distance, or lighting. What sorts of features might these be?
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 通过执行这些操作，我们希望计算机软件对图像的大小、拍摄照片的角度或**角度不变性**以及可用的照明，即**照明不变性**不敏感。在计算机视觉系统中，这些都是非常理想的——我们不希望一个AI系统只能从与原始图像相同的角度和距离识别我们的玩具。记住，我们将训练我们的视觉系统根据我们事先拍摄的标记训练图像来识别玩具，机器人将必须根据从训练集中学习到的内容来识别物体。在这里，我们将使用那些主要不基于大小、角度、距离或照明的图像特征。这些特征可能是什么？
- en: If we look at a common household object, such as a chair, and inspect it from
    several angles, what about the chair does not change? The easy answer is the edges
    and corners. The chair has the same number of corners all the time, and we can
    see a consistent number of them from most angles. It also has a consistent number
    of edges.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们从一个常见的家庭用品，比如一把椅子，从几个角度检查它，那么椅子的哪些部分不会改变？简单的答案是边缘和角落。椅子始终有相同数量的角落，并且我们可以从大多数角度看到一致数量的它们。它也有一致的边缘数量。
- en: Admittedly, that is a bit of an oversimplification of the approach. We will
    be training our ANN on a whole host of image features that may or may not be unique
    to this object and let it decide which work and which do not. We will accomplish
    this by using a generic approach to image manipulation called **convolution**.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 承认，这确实是对方法的一种简化的描述。我们将训练我们的神经网络（ANN）在一系列可能或可能不独特于该对象的所有图像特征上，并让它决定哪些有用，哪些无用。我们将通过使用一种通用的图像处理方法，称为**卷积**来实现这一点。
- en: Convolution
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积
- en: Every once in a while, you’ll come across some mathematical construction that
    turns a complex task into just a bunch of adding, subtracting, multiplying, and
    dividing. Vectors in geometry work like that, and, in image processing, we have
    the **convolution kernel**. It transpires that most of the common image processing
    techniques – edge detection, corner detection, blurring, sharpening, enhancing,
    and so on – can be accomplished with a simple array construct.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，你会遇到一些数学构造，将复杂任务转化为只是一些加法、减法、乘法和除法。几何中的向量就是这样工作的，在图像处理中，我们有**卷积核**。结果是，大多数常见的图像处理技术——边缘检测、角点检测、模糊、锐化、增强等等——都可以通过一个简单的数组结构实现。
- en: It is pretty easy to understand that in an image, the neighbors of a pixel are
    just as important to what a pixel is as the pixel itself. If you were going to
    try and find all the edge pixels of a box, you would look for a pixel that has
    one type of color on one side, and another type on the other. We need a function
    to find edges by comparing pixels on one side of a pixel to the other.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易理解，在图像中，像素的邻居对像素本身的重要性与像素本身一样重要。如果你要去尝试找到盒子的所有边缘像素，你会寻找一种颜色在一侧，另一种颜色在另一侧的像素。我们需要一个函数，通过比较像素的一侧与另一侧的像素来找到边缘。
- en: 'The convolution kernel is a matrix function that applies weights to the pixel
    neighbors – or pixels around the one pixel we are analyzing. The function is usually
    written like this, as a 3x3 matrix:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积核是一个矩阵函数，它将权重应用于像素邻居——或者我们正在分析的像素周围的像素。该函数通常写成这样，作为一个3x3的矩阵：
- en: '| -1 | 0 | 1 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| -1 | 0 | 1 |'
- en: '| -2 | 0 | 2 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| -2 | 0 | 2 |'
- en: '| -1 | 0 | 1 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| -1 | 0 | 1 |'
- en: Table 4.1 – A sample convolution kernel
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 表4.1 – 一个示例卷积核
- en: '**Sobel edge detection** is represented in the *Y* direction. This detects
    edges going up and down. Each block represents a pixel. The pixel being processed
    is in the center. The neighbors of the pixels on each side are the other blocks
    – top, bottom, left, and right. To compute the convolution, the corresponding
    weight is applied to the value of each pixel by multiplying the value (intensity)
    of that pixel, and then adding all of the results. If this image is in color –
    RGB – then we compute the convolution for each color separately and then combine
    the results. Here is an example of a convolution being applied to an image:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**Sobel边缘检测**在*Y*方向上表示。这检测上下方向的边缘。每个块代表一个像素。正在处理的像素位于中心。像素两侧的邻居是其他块——顶部、底部、左侧和右侧。为了计算卷积，将相应的权重应用于每个像素的值，通过乘以该像素的值（强度），然后将所有结果相加。如果这幅图像是彩色的——RGB——那么我们将分别对每种颜色进行卷积计算，然后将结果合并。以下是将卷积应用于图像的示例：'
- en: "![Figure 4.3 – Result of a Sobel edge detection convolutio\uFEFFn](img/B19846_04_3.jpg)"
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图4.3 – Sobel边缘检测卷积的结果](img/B19846_04_3.jpg)'
- en: Figure 4.3 – Result of a Sobel edge detection convolution
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3 – Sobel边缘检测卷积的结果
- en: The resulting image is the same size as the original. Note that we only get
    the edge as the result – if the colors are the same on either side of the center
    pixel, they cancel each other out and we get zero, or black. If they are different,
    we get 255, or white, as the answer. If we need a more complex result, we may
    also use a 5x5 convolution, which takes into account the two nearest pixels on
    each side, instead of just one.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图像与原始图像大小相同。请注意，我们只得到边缘作为结果——如果中心像素两侧的颜色相同，它们会相互抵消，我们得到零，或黑色。如果它们不同，我们得到255，或白色，作为答案。如果我们需要一个更复杂的结果，我们也可以使用5x5卷积，它考虑了每侧的两个最近像素，而不仅仅是其中一个。
- en: The good news is that you don’t have to choose which convolution operation to
    apply to the input images – we will build a software frontend that will set up
    all of the convolutions. This *frontend* is just the part of the program that
    sets up the networks before we start training them. The neural network package
    we’ll be using will determine which convolutions provide the most data and support
    the training output we want.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，你不必选择要应用于输入图像的卷积操作——我们将构建一个软件前端，它会设置所有的卷积。这个*前端*只是程序的一部分，在开始训练之前设置网络。我们将使用的神经网络包将确定哪些卷积提供了最多的数据并支持我们想要的训练输出。
- en: '"But wait," I hear you say. "What if the pixel is on the edge of the image
    and we don’t have neighbor pixels on one side?" In that case, we have to add padding
    to the image – which is a border of extra pixels that permits us to consider the
    edge pixels as well.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: “但是等等，”我听到你说，“如果像素位于图像的边缘，而我们没有一边的相邻像素怎么办？”在这种情况下，我们必须向图像添加填充——这是一个额外的像素边界，允许我们考虑边缘像素。
- en: In the next section, we’ll get into the guts of a neural network.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将深入了解神经网络的内部结构。
- en: Artificial neurons
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人工神经元
- en: 'What is a **neuron**? And how do we make a network out of them? If you can
    remember what you learned in biology, a biological or natural neuron has inputs,
    or dendrites, that connect it to other neurons or sensor inputs. All the inputs
    come to a central body and then leave via the axion, or connection, to other neurons
    via other dendrites. The connection between neurons is called a **synapse**, which
    is a tiny gap that the signal from the nerve must jump. A neuron takes inputs,
    processes them, and activates or sends an output after some threshold level is
    reached. An **artificial neuron** is a software construction that approximates
    the workings of the neurons in your brain and is a very simplified version of
    the natural neuron. It has several inputs, a set of weights, a bias, an activation
    function, and then some outputs to other neurons as a result of the network, as
    shown in the following figure:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是**神经元**？我们如何将它们组合成网络？如果你能记住你在生物学中学到的知识，一个生物或自然神经元有输入，或树突，将它们连接到其他神经元或传感器输入。所有输入都汇聚到一个中央体，然后通过轴突，或连接，通过其他树突离开，到达其他神经元。神经元之间的连接称为**突触**，这是一个信号必须跳过的微小间隙。神经元接收输入，处理它们，并在达到某个阈值水平后激活或发送输出。**人工神经元**是一种软件构造，它近似于你大脑中神经元的运作方式，是自然神经元的非常简化的版本。它有几个输入，一组权重，一个偏差，一个激活函数，然后作为网络的结果输出到其他神经元，如图所示：
- en: '![Figure 4.4 – Diagram of an artificial neuron](img/B19846_04_4.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图4.4 – 人工神经元的示意图](img/B19846_04_4.jpg)'
- en: Figure 4.4 – Diagram of an artificial neuron
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4 – 人工神经元的示意图
- en: 'Let’s describe each component in detail:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细描述每个组件：
- en: '**Input**: This is a number or value that’s received from other neurons or
    as an input to the network. In our image processing example, these are pixels.
    This number can be a float or an integer – but it must be just one number.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入**：这是一个从其他神经元或作为网络输入接收的数字或值。在我们的图像处理示例中，这些是像素。这个数字可以是浮点数或整数——但它必须只是一个数字。'
- en: '**Weight**: This is the adjustable value we change to *train* the neuron. Increasing
    the weight means that the input is more important to our answer, and likewise
    decreasing the weight means the input is used less. To determine the value of
    a neuron, we must combine the values of all the inputs. As the neural network
    is trained, the weights are adjusted on each input, which favors some inputs over
    others. We multiply the input by the weight and then sum all of the results together.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**权重**：这是我们为了**训练**神经元而改变的可调整值。增加权重意味着输入对我们的答案更重要，同样地，减少权重意味着输入的使用较少。为了确定神经元的值，我们必须组合所有输入的值。随着神经网络的训练，每个输入的权重都会进行调整，这有利于某些输入而牺牲其他输入。我们将输入乘以权重，然后将所有结果相加。'
- en: '**Bias**: This is a number that’s added to the sum of the weights. Bias prevents
    the neuron from getting stuck at zero and improves training. This is usually a
    small number. Imagine a scenario where all of the inputs to a neuron are zero;
    in this case, the weights would have no effect. Adding a small bias allows the
    neuron to still have an output, and the network can use that to affect learning.
    Without the bias, a neuron with zeros on its inputs can’t be trained (changing
    the weights has no effect) and is *stuck*.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**偏差**：这是一个加到权重总和上的数字。偏差防止神经元陷入零，并改善训练。这通常是一个很小的数字。想象一下这样一个场景，一个神经元的所有输入都是零；在这种情况下，权重将没有任何效果。添加一个小偏差允许神经元仍然有输出，网络可以使用这一点来影响学习。没有偏差，输入为零的神经元无法进行训练（改变权重没有效果）并且是**卡住**的。'
- en: '**Activation function**: This determines the output of the neuron based on
    the weighted sum of its inputs. The most common types are the **Rectified Linear
    Unit** (**ReLU**) – if the value of the neuron is less than zero, the output is
    zero; otherwise, the output is the input value – and the **sigmoid** (S-shaped)
    function, which is a log function. The activation function propagates information
    across the network and introduces non-linearity to the output of the neuron, which
    allows the neural network to approximate non-linear functions:'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**激活函数**：这决定了神经元输出的值，基于其输入的加权和。最常见类型的是**ReLU（修正线性单元**） – 如果神经元的值小于零，输出为零；否则，输出是输入值
    – 以及**S型函数**，这是一个对数函数。激活函数在网络中传播信息，并为神经元的输出引入非线性，这使得神经网络能够逼近非线性函数：'
- en: "![Figure 4.5 – Common activation \uFEFFfunctions](img/B19846_04_5.jpg)"
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图4.5 – 常见激活函数](img/B19846_04_5.jpg)'
- en: Figure 4.5 – Common activation functions
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5 – 常见激活函数
- en: '**Outputs**: Each layer in the sequential neural network is connected to the
    next layer. Some layers are fully connected – with each neuron in the first layer
    connected to each neuron in the second layer. Others are sparsely connected. There
    is a common process in neural network training called **dropout**, where we randomly
    remove connections. This forces the network to have multiple paths for each bit
    of information it learns, which strengthens the network and makes it able to handle
    more diverse inputs.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出**：序列神经网络中的每一层都连接到下一层。有些层是完全连接的 – 第一层的每个神经元都与第二层的每个神经元连接。其他层是稀疏连接的。在神经网络训练中有一个常见的流程称为**dropout**，其中我们随机移除连接。这迫使网络为它学习的每一点信息有多条路径，这加强了网络，并使其能够处理更多样化的输入。'
- en: '**Max pooling of outputs**: We use a special type of network layer (compared
    to a fully connected or sparse layer) called max pooling, where groups of neurons
    corresponding to regions in our image – say a 2x2 block of pixels – go to one
    neuron in the next level. The max pool neuron only takes the largest value from
    each of the four input neurons. This has the effect of downsampling the image
    (making it smaller). This allows the network to associate small features (such
    as the wheels in a Hot Wheels car) with larger features, such as the hood or windshield,
    to identify a toy car:'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出最大池化**：我们使用一种特殊的网络层（与全连接或稀疏层相比），称为最大池化，其中对应于图像中区域的神经元组 – 比如一个2x2像素块 – 被映射到下一层的单个神经元。最大池化神经元只从四个输入神经元中取最大的值。这具有下采样图像（使其变小）的效果。这允许网络将小特征（如Hot
    Wheels汽车的轮子）与较大特征（如引擎盖或挡风玻璃）关联起来，以识别玩具车：'
- en: '![Figure 4.6 – Max pooling operation](img/B19846_04_6.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图4.6 – 最大池化操作](img/B19846_04_6.jpg)'
- en: Figure 4.6 – Max pooling operation
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6 – 最大池化操作
- en: Now that you understand what a neural network is composed of, let’s explore
    how to train and test one.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了神经网络由什么组成，让我们来探讨如何训练和测试一个神经网络。
- en: Training a CNN
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练CNN
- en: 'I want to provide you with an end-to-end look at what we will be doing in the
    code for the rest of this chapter. Remember that we are building a CNN that examines
    pixels in a video frame and outputs if one or more pixel areas that resemble toys
    are in the image, and where they are. The following diagram shows the process
    that we will go through to train the neural network, step by step:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我想要向您展示本章剩余部分代码中我们将要执行的操作的全过程。请记住，我们正在构建一个卷积神经网络（CNN），它检查视频帧中的像素，并输出图像中是否有一个或多个类似玩具的像素区域，以及它们的位置。以下图表显示了我们将逐步进行神经网络训练的过程：
- en: '![Figure 4.7 – CNN process](img/B19846_04_7.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图4.7 – CNN过程](img/B19846_04_7.jpg)'
- en: Figure 4.7 – CNN process
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.7 – CNN过程
- en: For this task, I decided to use an already existing neural network rather than
    build one from scratch. There are a lot of good CNN object detectors available,
    and honestly, it’s hard to improve on an existing model structure. The one I’ve
    picked for this book is called **YOLOv8**, where *YOLO* stands for *You Only Look
    Once*. Let’s understand how we can use this model for our task.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个任务，我决定使用一个现成的神经网络而不是从头开始构建。有很多好的CNN目标检测器可用，而且说实话，很难改进现有的模型结构。我为这本书选择的是称为**YOLOv8**的模型，其中*YOLO*代表*You
    Only Look Once*。让我们了解我们如何使用这个模型来完成我们的任务。
- en: Using YOLOv8 – an object recognition model
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用YOLOv8 – 一个目标识别模型
- en: Before we dive into the details of the YOLOv8 model, let’s talk about why I
    selected it. First of all, the learning process is pretty much the same for any
    CNN we might use. YOLO is a strong open source object detection model with a lot
    of development behind it. It’s considered state of the art, and it already does
    what we need – it detects objects and shows us where they are in images by drawing
    bounding boxes around them. So, it tells us what objects are, and where they are
    located. As you will see, it is very easy to use and can be extended to detect
    other classes of objects other than what it was originally trained for. There
    are a lot of YOLO users out there who can provide a lot of support and a good
    basis for learning about AI object recognition for robotics.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入YOLOv8模型细节之前，让我们谈谈为什么我选择了它。首先，对于任何我们可能使用的CNN，学习过程基本上是相同的。YOLO是一个强大的开源物体检测模型，背后有许多开发。它被认为是行业最佳，它已经能够完成我们所需要的任务——通过在图像周围绘制边界框来检测物体并显示它们的位置。因此，它告诉我们物体是什么，以及它们在哪里。正如您将看到的，它非常容易使用，并且可以扩展以检测除了它最初训练的类别之外的其他类别的物体。有许多YOLO用户可以提供大量支持，并为我们学习机器人AI物体识别提供了一个很好的基础。
- en: As I mentioned at the beginning of this chapter, we have two tasks we need to
    accomplish to reach our goal of picking up toys with a robot. First, we must determine
    whether the robot can detect a toy with its camera (determine whether there is
    a toy in the camera image) and then figure out where it is in that image so that
    we can drive over to it and pick it up. In this chapter, we’ll learn how to detect
    toys, while in [*Chapter 7*](B19846_07.xhtml#_idTextAnchor221), we’ll discuss
    how we determine distance and navigate to the toy.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如我在本章开头提到的，我们需要完成两个任务才能达到用机器人捡起玩具的目标。首先，我们必须确定机器人是否可以用它的摄像头检测到玩具（确定摄像头图像中是否有玩具），然后确定它在图像中的位置，这样我们就可以开过去并捡起它。在本章中，我们将学习如何检测玩具，而在[*第7章*](B19846_07.xhtml#_idTextAnchor221)中，我们将讨论我们如何确定距离并导航到玩具。
- en: YOLOv8 does both tasks in one pass, hence its name. Other kinds of object recognition
    models, such as the one I created in the first edition of this book, identified
    and located objects in images in two steps. First, it found that an object was
    present, and then it figured out where in the image it was located in a separate
    pass. This separate pass would use a sliding window approach, taking a segment
    of the image and using the detection part of the neural network to say `yes` or
    `no` if that segment contained an object it recognized. Then, it would slide the
    window it was considering across the image and test again. This was repeated until
    we had a bunch of image segments that contained the detected object. Then, a process
    called *minmax* would select the smallest box (min) that contained all the visible
    parts of the object (max).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: YOLOv8一次完成两项任务，因此得名。其他类型的物体识别模型，例如我在本书第一版中创建的模型，在图像中识别和定位物体需要两个步骤。首先，它发现图像中存在物体，然后在单独的步骤中确定物体在图像中的位置。这个单独的步骤会使用滑动窗口方法，取图像的一部分，并使用神经网络中的检测部分来表示“是”或“否”，如果该部分包含它所识别的物体。然后，它会将考虑的窗口在图像上滑动并再次测试。这个过程会重复，直到我们有一系列包含检测到的物体的图像部分。然后，一个称为*minmax*的过程会选择包含物体所有可见部分的最小框（min）。
- en: YOLOv8 takes a different approach by combining two neural networks – one that
    detects objects it has been taught to recognize and another that is trained to
    draw bounding boxes based on the center of the object. The direct output of YOLOv8
    includes both the detection and the bounding box for the object. YOLOv8 can also
    *segment* images by pixels, identifying not just a box with the object, but all
    the pixels that belong to that object. We’ll be using a bounding box to help us
    drive the robot to our toys.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: YOLOv8通过结合两个神经网络采取不同的方法——一个检测它被训练来识别的物体，另一个训练根据物体的中心绘制边界框。YOLOv8的直接输出包括物体的检测和边界框。YOLOv8还可以通过像素*分割*图像，不仅识别包含物体的框，还包括属于该物体的所有像素。我们将使用边界框来帮助我们驾驶机器人到达玩具。
- en: 'YOLOv8 comes pretrained on a whole series of object classes (about 80), but
    we can check whether it already works with the toys we want to detect. Let’s test
    YOLOv8’s ability to detect our toys. We can install YOLOv8 using this simple command
    on our PC:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: YOLOv8在一系列物体类别（大约80个）上进行了预训练，但我们仍然可以检查它是否已经能够检测我们想要检测的玩具。让我们测试YOLOv8检测我们玩具的能力。我们可以在PC上使用以下简单命令安装YOLOv8：
- en: '[PRE0]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now, to test our detection with a picture of toys in the playroom, we will
    use the smallest (in terms of model size) of the YOLOv8 detection models – the
    `yolov8n.pt`). This is the pretrained neural network that Ultralytics provides
    with YOLOv8:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了测试我们在游戏室玩具照片上的检测，我们将使用YOLOv8检测模型中最小（就模型大小而言）的模型——`yolov8n.pt`）。这是Ultralytics与YOLOv8一起提供的预训练神经网络：
- en: '[PRE1]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'As shown in the following figure, the only thing detected by the off-the-shelf
    YOLOv8 object model is an upside-down matchbook car, which it incorrectly labels
    as a skateboard:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如以下图所示，现成的YOLOv8目标模型仅检测到一个颠倒的火柴盒车，并将其错误地标记为滑板：
- en: '![Figure 4.8 – YOLOv8 output without specific training on our toys](img/B19846_04_8.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图4.8 – YOLOv8输出，未针对我们的玩具进行特定训练](img/B19846_04_8.jpg)'
- en: Figure 4.8 – YOLOv8 output without specific training on our toys
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.8 – YOLOv8输出，未针对我们的玩具进行特定训练
- en: You have to admit, the little toy car does resemble a skateboard from this angle,
    but this is not the result we want. We need all the toys in the image to be detected,
    not just one. What can we do about this?
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 您必须承认，从这个角度看，这个小玩具车确实有点像滑板，但这不是我们想要的结果。我们需要检测图像中的所有玩具，而不仅仅是其中一个。我们该怎么办？
- en: The answer is that we can add new training to the network, get all the advantages
    of YOLOv8, and have our custom objects detected as well. For this, we can use
    a process called **transfer learning**.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是我们可以向网络添加新的训练，获得YOLOv8的所有优势，并且我们的自定义对象也能被检测到。为此，我们可以使用一个称为**迁移学习**的过程。
- en: 'Here is an overview of how we will train our toy detector, after which we will
    discuss these steps in greater detail:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我们将如何训练我们的玩具检测器的概述，之后我们将更详细地讨论这些步骤：
- en: First, we will prepare a training set of images of the room with toys. This
    means we must take a lot of pictures of the toys from the viewpoint of the robot,
    using the same camera the robot will use. We want to take pictures from all the
    different angles and sides of the toys. I went around the room clockwise, then
    anti-clockwise, snapping pictures every few inches. I took 48 pictures in this
    step.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将准备一个包含玩具房间图像的训练集。这意味着我们必须从机器人视角拍摄大量玩具的照片，使用机器人将使用的相同相机。我们希望从玩具的所有不同角度和侧面拍照。我按顺时针方向绕着房间走，然后逆时针走，每隔几英寸拍一张照片。在这个步骤中，我拍了48张照片。
- en: 'Next, we must use a data labeling program such as RoboFlow ([https://roboflow.com](https://roboflow.com))
    to annotate the images (you can refer to the relevant documentation for detailed
    instructions). The program lets us draw boxes around the objects we want to recognize
    (toys) and label them with a tag – we will use the name `toy`. We are separating
    the parts of the image that contain toys and telling the neural network what to
    call this type of object:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们必须使用像RoboFlow（[https://roboflow.com](https://roboflow.com)）这样的数据标注程序来标注图像（您可以参考相关文档以获取详细说明）。该程序允许我们在想要识别的对象（玩具）周围绘制方框，并用标签进行标记——我们将使用名称`toy`。我们正在将包含玩具的图像部分分离出来，并告诉神经网络这种类型对象的名称：
- en: '![Figure 4.9 – Annotating using RoboFlow, a free data labeling tool](img/B19846_04_09.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图4.9 – 使用RoboFlow进行标注，一个免费的数据标注工具](img/B19846_04_09.jpg)'
- en: Figure 4.9 – Annotating using RoboFlow, a free data labeling tool
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.9 – 使用RoboFlow进行标注，一个免费的数据标注工具
- en: 'Then, we must split the training set into three parts: a set we use to train
    the network, a set we use to validate the training, and a set we use to test the
    network. We will create a set of 87% of the images for training, 8% for validation,
    and 5% for testing. We’ll put the training data and test data in separate folders.
    RoboFlow has a procedure for this under the **Generate** tab, where there is a
    section labeled **Train/Test Split**.'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们必须将训练集分成三部分：一部分用于训练网络，一部分用于验证训练，另一部分用于测试网络。我们将创建包含87%图像的训练集，8%的验证集和5%的测试集。我们将训练数据和测试数据放在不同的文件夹中。RoboFlow在**生成**标签页下有相应的流程，其中有一个标记为**训练/测试分割**的部分。
- en: 'Now, we must take each image and multiply its training value by combining parts
    of different images in a mosaic. We’ll take parts of four random different pictures
    and combine them. This will increase our training set three-fold, a process called
    **data augmentation**. This is built into RoboFlow. I started with 36 pictures;
    after augmentation, I had 99:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们必须将每张图像的训练值通过将不同图像的部分组合成马赛克来乘以。我们将从四张随机不同的图片中取部分并组合它们。这将使我们的训练集增加三倍，这个过程称为**数据增强**。这是RoboFlow内置的功能。我开始时有36张图片；经过增强后，我有99张：
- en: '![Figure 4.10 – Mosaic data augmentation creates more training data from our
    limited number of pictures](img/B19846_04_10.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.10 – 瓦片数据增强从我们有限的图片数量中创建更多训练数据](img/B19846_04_10.jpg)'
- en: Figure 4.10 – Mosaic data augmentation creates more training data from our limited
    number of pictures
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.10 – 瓦片数据增强从我们有限的图片数量中创建更多训练数据
- en: Why are we using this mosaic approach? We still want to have valid bounding
    boxes. The mosaic process resizes any partial bounding boxes that intersect the
    edges.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们使用这种瓦片方法？我们仍然希望有有效的边界框。瓦片过程会调整任何与边缘相交的局部边界框的大小。
- en: 'Next, we will be building two programs: the training program, which runs on
    our desktop computer and trains the network, and the working program, which uses
    the trained network to find toys. The training process may not run on our small
    computer onboard the robot or may take a long time to run, so we’ll use the desktop
    computer for this.'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将构建两个程序：训练程序，它在我们的台式计算机上运行并训练网络；以及工作程序，它使用训练好的网络来寻找玩具。训练过程可能不会在我们的机器人机载小计算机上运行，或者可能需要很长时间才能运行，所以我们将使用台式计算机来完成这项工作。
- en: 'Now, we need to train the network. To achieve this, we must do the following:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要训练网络。为了实现这一点，我们必须做以下事情：
- en: First, we must scale all our images down to reduce the processing time to a
    reasonable level.
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们必须将所有图像缩小以减少处理时间到合理的水平。
- en: Then, we must initialize the network with uniform random weights.
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们必须使用均匀随机权重初始化网络。
- en: Next, we must encode a labeled image and input that to the network. The neural
    network only uses the image data to predict what class of object is in the picture,
    and what its bounding box should be. Since we pre-labeled the image with the correct
    answer and used the correct bounding box, we can judge whether the answer is right
    or wrong. If it is right, we can reinforce the weights on the inputs that contributed
    to this answer by incrementing them (the training value). If the answer is wrong,
    we can reduce the weights instead. In neural networks, the error between the desired
    result and the actual result is called **loss**. Repeat this process for each
    image.
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们必须对标记的图像进行编码，并将其输入到网络中。神经网络只使用图像数据来预测图片中包含的对象类别及其边界框。由于我们预先用正确的答案标记了图像并使用了正确的边界框，我们可以判断答案是否正确。如果答案是正确的，我们可以通过增加（训练值）来加强导致这个答案的输入权重。如果答案是错误的，我们可以减少权重。在神经网络中，期望结果和实际结果之间的误差称为**损失**。对每张图像重复这个过程。
- en: Now, we must test the network by running the testing set of images – which are
    pictures of the same toys, but that were not in the training set. We must analyze
    what sort of output we get over this set (how many are wrong and how many are
    right). If this answer is above 90%, we stop. Otherwise, we go back and run all
    the training images again.
  id: totrans-110
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们必须通过运行测试图像集来测试网络——这些图像是训练集中没有的相同玩具的图片。我们必须分析在这个集合上得到的输出类型（有多少是错误的，有多少是正确的）。如果这个答案超过90%，我们就停止。否则，我们返回并再次运行所有训练图像。
- en: Once we are happy with the results – and we should need between 50 and 100 iterations
    to get there – we must stop and store the weights that we ended up with in the
    training network. This is our **trained CNN**.
  id: totrans-111
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们对结果满意——我们通常需要50到100次迭代才能达到这个目标——我们就必须停止并存储训练网络中最终得到的权重。这是我们**训练好的CNN**。
- en: Our next task is to find the toys. To do this, we must *deploy* the trained
    network by loading it and using our video images from the live robot to look for
    toys. We’ll get a probability of an image containing a toy from 0% to 100%. We’ll
    scan the input video image in sections and find which sections contain toys. If
    we are not happy with this network, we can reload this network into the training
    program and train it some more.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们接下来的任务是找到玩具。为此，我们必须通过加载它并使用从实时机器人视频图像中获取的图像来**部署**训练好的网络。我们将从0%到100%得到包含玩具的图像的概率。我们将以部分扫描输入视频图像，并找出哪些部分包含玩具。如果我们对这个网络不满意，我们可以将其重新加载到训练程序中，并对其进行更多训练。
- en: Now, let’s cover this in detail, step by step. We have a bit more theory to
    cover before we start writing the code.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们一步一步详细地介绍这个过程。在我们开始编写代码之前，我们还有一些理论需要讲解。
- en: Understanding how to train our toy detector
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解如何训练我们的玩具检测器
- en: 'Our first task is to prepare a training set. We’ll put the camera on the robot
    and drive the robot around using the teleoperation interface (or just by pushing
    it around by hand), snapping still photos every foot or so. We just need pictures
    with toys in the image since we will be annotating the toys. We need about 200
    pictures – the more, the better. We also need to have a set of pictures in the
    daytime with natural light, and at night, if your room changes lighting between
    day and night. This affords us several advantages: we are using the same room
    and the same camera to find the toys, all under the same lighting conditions.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的首要任务是准备一个训练集。我们将把相机放在机器人上，使用遥操作界面（或者只是用手推动它）来驾驶机器人，每隔大约一英尺就拍一张静态照片。我们只需要包含玩具的图片，因为我们将会标注玩具。我们需要大约200张图片——越多越好。我们还需要一套白天有自然光和夜晚（如果你的房间在白天和夜晚之间改变照明）的图片。这给我们带来了几个优势：我们使用相同的房间和相同的相机在相同的照明条件下寻找玩具。
- en: Now, we need to label the images. We’ll load the images into RoboFlow to create
    a dataset called `toydetector`. Use the **Upload** tab and drag and drop the images
    or select the folder that contains the images.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要对图像进行标注。我们将图像加载到RoboFlow中，创建一个名为`toydetector`的数据集。使用**上传**标签，拖放图像或选择包含图像的文件夹。
- en: The process for us is fairly straightforward. We look at each picture in turn
    and draw a box around any toy objects. We hit *Enter* or the `toy`. This is going
    to take a while.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们来说，这个过程相当直接。我们依次查看每张图片，并在任何玩具对象周围画一个框。我们按*Enter*键或输入`toy`。这需要一些时间。
- en: Once we’ve labeled around 160 toys in our images, we can use the **Generate**
    button in RoboFlow to create our dataset. We must set up the preprocessing task
    to resize our images to 640x640 pixels. This makes the best use of our limited
    computer capacity on the robot. Then, we must augment the dataset to create additional
    images of our limited set, as mentioned previously. We’ll use the mosaic method
    to augment our dataset while preserving the bounding boxes. To do this, we must
    use the **Generate** tab in RoboFlow, then click **Add Augmentation Step** to
    select the type of operation that will affect our images. Then, we must add the
    mosiac augmentation to create more images out of our training set. Now, we can
    hit the **Generate** button to create our dataset.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在图像中标注了大约160个玩具后，我们可以使用RoboFlow中的**生成**按钮来创建我们的数据集。我们必须设置预处理任务，将我们的图像调整到640x640像素。这使我们在机器人上的有限计算机容量得到最佳利用。然后，我们必须增强数据集以创建我们有限集合的额外图像，如前所述。我们将使用马赛克方法来增强数据集，同时保留边界框。为此，我们必须使用RoboFlow中的**生成**标签，然后点击**添加增强步骤**来选择将影响我们图像的操作类型。然后，我们必须添加马赛克增强来从我们的训练集中创建更多图像。现在，我们可以点击**生成**按钮来创建我们的数据集。
- en: We started with 48 images that I took (back in step 1); after augmentation,
    we have 114\. We’ll set our test/train split so that it contains 99 images in
    the training set, nine images in the validation set, and six images in the test
    set (87% training, 8% validation, and 5% testing). This makes the best use of
    our limited dataset.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从48张我拍摄的图片（在步骤1中）开始；经过增强后，我们有114张。我们将设置测试/训练分割，使其包含99张训练图像，9张验证图像和6张测试图像（87%训练，8%验证和5%测试）。这使我们在有限的数据集上得到最佳利用。
- en: 'To download our datasets from RoboFlow, we must install RoboFlow’s interface
    on our computer. It’s a Python package:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 要从RoboFlow下载我们的数据集，我们必须在计算机上安装RoboFlow的界面。它是一个Python包：
- en: '[PRE2]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then, we must create a short Python program called `downloadDataset.py`. When
    you build your dataset, RoboFlow will provide a unique `api_key` value; this will
    be the password for your account that authorizes access. This goes into the program
    as follows, where I put the asterisks:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们必须创建一个名为`downloadDataset.py`的简短Python程序。当你构建你的数据集时，RoboFlow将提供一个唯一的`api_key`值；这将是授权访问你账户的密码。它如下所示，我在这里放置了星号：
- en: '[PRE3]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In the next section, we will retrain the network with this command:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将使用以下命令重新训练网络：
- en: '[PRE4]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Once we’ve done this, the program will produce a lot of output, as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们完成这些，程序将产生大量的输出，如下所示：
- en: '[PRE5]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: One of the critical parts of training our model is the **training optimizer**.
    We will use **stochastic gradient descent** (**SGD**) for this. SGD is another
    of those simple concepts with a fancy name. Stochastic just means *random*. What
    we want to do is tweak the weights of our neurons to give a better answer than
    we got the first time – this is what we are training, by adjusting the weights.
    We want to change the weights a small amount – but in which direction? We want
    to change the weights in the direction that improves the answer – it makes the
    prediction closer to what we want it to be.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 训练我们模型的一个关键部分是**训练优化器**。我们将使用**随机梯度下降（SGD**）来完成这项工作。SGD是那些有华丽名字的简单概念之一。随机只是意味着*随机*。我们想要做的是调整神经元的权重，以给出比第一次更好的答案——这就是我们通过调整权重来训练的内容。我们想要改变权重的一小部分——但朝哪个方向？我们想要改变权重的方向，以改善答案——它使预测更接近我们想要的样子。
- en: To understand this better, let’s do a little thought experiment. We have a neuron
    that we know is producing the wrong answer and needs adjusting. We’ll add a small
    amount to the weight and see how the answer changes. It gets slightly worse –
    the number is further away from the correct answer. So, we must subtract a small
    amount instead – and, as you might think, the answer gets better. We have reduced
    the amount of error slightly. If we make a graph of the error produced by the
    neuron, we’ll see that we are moving toward an error of zero, or the graph is
    descending toward some minimum value. Another way of saying this is that the slope
    of the line is negative – going toward zero. The amount of the slope can be called
    a **gradient** – just as you would refer to the slope or steepness of a hill as
    the gradient. We can calculate the partial derivative (in other words, the slope
    of the error curve near this point), which tells us the slope of the line.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这一点，让我们做一个简单的思想实验。我们有一个神经元，我们知道它正在产生错误的答案并且需要调整。我们将增加一点权重并看看答案如何变化。它变得稍微糟糕了——数字离正确答案更远了。所以，我们必须减去一小部分——正如你可能想到的，答案变得更好。我们稍微减少了错误量。如果我们绘制神经元产生的错误图，我们会看到我们正在朝着零误差移动，或者图正在下降到某个最小值。另一种说法是，线的斜率是负的——趋向于零。斜率的大小可以称为**梯度**——就像你将山丘的斜率或陡峭程度称为梯度一样。我们可以计算偏导数（换句话说，就是误差曲线在此点的斜率），这告诉我们线的斜率。
- en: 'The way we go about adjusting the weights on the network as a whole to minimize
    the loss between the ground truth and the predicted value is called `Y1`, `Y2`,
    and `Y3`. We have three weights – `W1`, `W2`, and `W3`. We’ll have the bias, `B`,
    and our activation function, `D`, which is the ReLU rectifier. The values of our
    inputs are 0.2, 0.7, and 0.02\. The weights are 0.3, 0.2, and 0.5\. Our bias is
    0.3, and the desired output is 1.0\. We calculate the sum of the inputs and weights
    and we get a value of 0.21\. After adding our bias, we get 0.51\. The ReLU function
    passes any value greater than zero, so the activated output of this neuron is
    0.51\. Our desired value is 1.0, which comes from the truth (label) data. So,
    our error is 0.49\. If we add the training rate value to each weight, what happens?
    Take a look at the following diagram:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们调整整个网络上的权重以最小化真实值和预测值之间损失的方法被称为`Y1`、`Y2`和`Y3`。我们有三个权重——`W1`、`W2`和`W3`。我们将有偏差`B`和我们的激活函数`D`，它是ReLU整流器。我们的输入值是0.2、0.7和0.02。权重是0.3、0.2和0.5。我们的偏差是0.3，期望的输出是1.0。我们计算输入和权重的总和，得到0.21的值。加上偏差后，我们得到0.51。ReLU函数通过任何大于零的值，所以这个神经元的激活输出是0.51。我们的期望值是1.0，这来自真实（标签）数据。所以，我们的错误是0.49。如果我们将训练率值加到每个权重上，会发生什么？看看下面的图：
- en: '![Figure 4.11 – How backpropagation works to adjust weights](img/B19846_04_11.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![图4.11 – 反向传播如何调整权重](img/B19846_04_11.jpg)'
- en: Figure 4.11 – How backpropagation works to adjust weights
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.11 – 反向传播如何调整权重
- en: The output value now goes up to 0.5192\. Our error goes down to 0.4808\. We
    are on the right track! The gradient of our error slope is *(.4808-.49) / 1 =
    -0.97*. The *1* is because we just have one training sample so far. So, where
    does the stochastic part come from? Our recognition network may have 50 million
    neurons. We can’t be doing all of this math for each one. So, we must take a random
    sampling of inputs rather than all of them to determine whether our training is
    positive or negative.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 输出值现在上升到 0.5192。我们的错误下降到 0.4808。我们正在正确的道路上！我们错误斜率的梯度是 *(0.4808-0.49) / 1 = -0.97*。这里的
    *1* 是因为我们到目前为止只有一个训练样本。那么，随机部分从何而来？我们的识别网络可能有 5000 万个神经元。我们不可能对每个神经元都进行所有这些数学运算。因此，我们必须对输入进行随机采样，而不是全部采样，以确定我们的训练是正面的还是负面的。
- en: 'In math terms, the slope of an equation is provided by the derivative of that
    equation. So, in practice, backpropagation takes the partial derivative of the
    error between training epochs to determine the slope of the error, and thus determine
    whether we are training our network correctly. As the slope gets smaller, we reduce
    our training rate to a smaller number to get closer and closer to the correct
    answer:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 用数学术语来说，方程的斜率由该方程的导数提供。因此，在实践中，反向传播计算训练周期之间错误的偏导数，以确定错误的斜率，并据此确定我们是否正确地训练了网络。随着斜率的减小，我们降低训练速率到一个更小的数字，以便越来越接近正确答案：
- en: "![Figure 4.1\uFEFF2 – The gradient descent process](img/B19846_04_12.jpg)"
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: "![图 4.1\uFEFF2 – 梯度下降过程](img/B19846_04_12.jpg)"
- en: Figure 4.12 – The gradient descent process
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.12 – 梯度下降过程
- en: 'Now, we can to our next problem: how do we propagate our weight adjustments
    up the layers of the neural network? We can determine the error at the output
    neuron – just the label value minus the output of the network. How do we apply
    this information to the previous layer? Each neuron’s contribution to the error
    is proportional to its weight. We must divide the error by the weight of each
    input, and that value is now the applied error of the next neuron up the chain.
    Then, we can recompute their weights and so on. This is why neural networks take
    so much compute power:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们来解决下一个问题：我们如何将权重调整传播到神经网络层？我们可以在输出神经元处确定错误——即标签值减去网络的输出。我们如何将这个信息应用到前一层？每个神经元对错误的贡献与其权重成正比。我们必须将错误除以每个输入的权重，这个值现在就是链中下一个神经元的应用错误。然后，我们可以重新计算它们的权重，依此类推。这就是为什么神经网络需要如此多的计算能力：
- en: '![Figure 4.13 – Backpropagation error](img/B19846_04_13.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.13 – 反向传播错误](img/B19846_04_13.jpg)'
- en: Figure 4.13 – Backpropagation error
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.13 – 反向传播错误
- en: We backpropagate the error back up the network from the end back to the beginning.
    Then, we start all over again with the next cycle.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将错误反向传播回网络，从末端开始，一直传播到开始处。然后，我们从头开始进行下一轮。
- en: At this point, we can test our toy detector. Let’s see how we can do this.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以测试我们的玩具检测器。让我们看看我们如何做到这一点。
- en: Building the toy detector
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建玩具检测器
- en: 'We can use the following command to test how we did:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下命令来测试我们的结果：
- en: '[PRE6]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The program produces the following output. We can find our image with the labeled
    detections in the `./runs/detect/predict` directory with a number appended depending
    on how many times we run the detection:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 程序产生了以下输出。我们可以在 `./runs/detect/predict` 目录中找到带有标记检测的图像，目录中附加的数字取决于我们运行检测的次数：
- en: '[PRE7]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output of our prediction is shown in the following figure:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们预测的结果显示在下面的图中：
- en: '![Figure 4.14 – The toy detector in action](img/B19846_04_14.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.14 – 玩具检测器在工作](img/B19846_04_14.jpg)'
- en: Figure 4.14 – The toy detector in action
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.14 – 玩具检测器在工作
- en: 'With this, we have successfully created a toy detector using a neural network.
    The output of the detector, which we will use in [*Chapter 5*](B19846_05.xhtml#_idTextAnchor159)
    to direct the robot and the robot arm to drive to the toy and then pick it up,
    looks like this:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们已成功使用神经网络创建了一个玩具检测器。检测器的输出，我们将在 [*第 5 章*](B19846_05.xhtml#_idTextAnchor159)
    中使用它来指导机器人和机械臂驶向玩具并抓取它，看起来是这样的：
- en: '[PRE8]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: For each detection, the neural network will provide several bits of information.
    We get the `x` and `y` locations of the center of the bounding box, and then the
    height and width of that box. Then, we get a confidence number of how certain
    the network is of the decision that this is a detection. Finally, we get the class
    of the object (what kind of object), which is, of course, a toy.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个检测，神经网络将提供一些信息。我们得到边界框中心的 `x` 和 `y` 位置，然后是那个框的高度和宽度。然后，我们得到一个置信度数字，表示网络对这个决策是检测的确定性。最后，我们得到物体的类别（是什么类型的物体），当然是一个玩具。
- en: When we ran the training process for the neural network, if you look in the
    `training` folder found in `runs/detect/train`, there are a whole series of graphs.
    What do these graphs tell us?
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行神经网络的训练过程时，如果你查看 `runs/detect/train` 中的 `training` 文件夹，你会看到一系列图表。这些图表告诉我们什么？
- en: 'The first one we need to look at is `F1_curve`. This is the product of precision
    and recall. **Precision** is the ratio of true positives (correctly classified
    objects) from all positives. **Recall** is the proportion of positive detections
    that were identified correctly. So, precision is defined as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要查看的是 `F1_curve`。这是精确度和召回率的乘积。**精确度**是所有正例中正确分类的对象的比例。**召回率**是正确识别的正检测的比例。因此，精确度定义为以下：
- en: Precision =  TP _ TP + FP
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 精确度 = TP / (TP + FP)
- en: Precision is the true positives divided by true positives and false positives
    (items that were identified as detections but were not).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 精确度是真实正例数除以真实正例数和假正例数（被识别为检测但实际不是的项）。
- en: 'Recall is defined slightly differently:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率的定义略有不同：
- en: Recall =  TP _ TP + FN
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率 = TP / (TP + FN)
- en: Here, recall is the true positives divided by true positives plus false negatives.
    A false negative is a missed detection or an object that was not detected when
    it was, in fact, present.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，召回率是真实正例数除以真实正例数加上假负例数。一个假负例是一个漏检或实际上存在但未被检测到的物体。
- en: 'To create the F1 curve, we must multiply precision and recall together and
    plot it against *confidence*. The graph shows the level of confidence in our detections
    that produces the best result of trading off precision and recall:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建 F1 曲线，我们必须将精确度和召回率相乘，并将其与 *置信度* 对应。图表显示了产生最佳结果（在精确度和召回率之间权衡）的检测置信度水平：
- en: '![Figure 4.15 – The F1 confidence curve](img/B19846_04_15.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.15 – F1 置信曲线](img/B19846_04_15.jpg)'
- en: Figure 4.15 – The F1 confidence curve
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.15 – F1 置信曲线
- en: 'In this case, a confidence level of 0.21 gives a detection rate of 0.87\. This
    means that we get the best ratio of true detections to false detections. However,
    this best ratio – 87% – occurs at 0.21 confidence – a rather low number. Detections
    at this low confidence level are hard to distinguish and can be caused by noise
    in measurements. It might be more desirable to have our peak at a higher confidence
    level. I tried several approaches to address this. I ran 200 epochs rather than
    100 and moved the peak F1 confidence level to 51%, but the detection level dropped
    a bit to 85%. Then, I changed the gradient descent technique from SDM to **Adam**,
    an adaptive gradient descent technique that reduces the learning rate as you get
    closer to our goal. This can be done using the following code:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，置信度为 0.21 时，检测率为 0.87。这意味着我们得到了最佳的真实检测与误检测的比率。然而，这个最佳比率 – 87% – 发生在 0.21
    的置信度 – 这是一个相当低的数字。在这个低置信度水平上的检测很难区分，可能是由于测量中的噪声引起的。可能更希望我们的峰值出现在更高的置信度水平。我尝试了几个方法来解决这个问题。我运行了
    200 个 epoch 而不是 100，并将峰值 F1 置信度水平移动到 51%，但检测水平略有下降到 85%。然后，我将梯度下降技术从 SDM 更改为 **Adam**，这是一种自适应梯度下降技术，当接近我们的目标时，它会降低学习率。这可以通过以下代码实现：
- en: '[PRE9]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This produced a more satisfactory result of 88% true detections at 49% confidence,
    which I think will do a better job for our toy detector. In reviewing my detections,
    there were a few false positives (furniture and other objects being detected as
    toys), so I think that this version will be our toy detector neural network. Although
    I used a fairly small dataset, it would not hurt to have more pictures to work
    with from different angles. Before wrapping this chapter up, let’s briefly summarize
    what we’ve learned so far.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了 88% 的真实检测率在 49% 置信度下的更令人满意的结果，我认为这将更好地为我们的小玩具检测器工作。在回顾我的检测时，有几个误报（家具和其他被检测为玩具的物体），所以我认为这个版本将是我们的玩具检测器神经网络。尽管我使用了一个相当小的数据集，但拥有更多不同角度的图片来工作也不会有害。在结束这一章之前，让我们简要总结一下到目前为止我们已经学到的内容。
- en: Summary
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we dove head-first into the world of ANNs. An ANN can be thought
    of as a stepwise non-linear approximation function that slowly adjusts itself
    to fit a curve that matches the desired input to the desired output. The learning
    process consists of several steps, including preparing data, labeling data, creating
    the network, initializing the weights, creating the forward pass that provides
    the output, and calculating the loss (also called the error). We created a special
    type of ANN, a CNN, to examine images. The network was trained using images with
    toys, to which we added bounding boxes to tell the network what part of the image
    was a toy. We trained the network to get an accuracy better than 87% in classifying
    images with toys in them. Finally, we tested the network to verify its output
    and tuned our results using the Adam adaptive descent algorithm.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们一头扎进了人工神经网络的世界。人工神经网络可以被视为一种逐步的非线性逼近函数，它逐渐调整自己以适应曲线，使所需的输入与所需的输出相匹配。学习过程包括几个步骤，包括准备数据、标记数据、创建网络、初始化权重、创建正向传递以提供输出，以及计算损失（也称为误差）。我们创建了一种特殊类型的人工神经网络，即卷积神经网络（CNN），来检查图像。网络使用带有玩具的图像进行训练，我们在图像上添加了边界框来告诉网络图像的哪一部分是玩具。我们训练网络，使其在包含玩具的图像分类中达到超过87%的准确率。最后，我们测试了网络以验证其输出，并使用Adam自适应下降算法调整我们的结果。
- en: In the next chapter, we will look at machine learning for the robot arm in terms
    of reinforcement learning and genetic algorithms.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将从强化学习和遗传算法的角度探讨机器人臂的机器学习。
- en: Questions
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: We went through a lot in this chapter. You can use the framework provided to
    investigate the properties of neural networks. Try several activation functions,
    or different settings for convolutions, to see what changes in the training process.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在本章中，我们经历了很多。你可以使用提供的框架来研究神经网络的特性。尝试几种激活函数，或者不同的卷积设置，看看训练过程中有什么变化。
- en: Draw a diagram of an artificial neuron and label the parts. Look up a natural,
    human biological neuron and compare them.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制一个人工神经元的图并标注各部分。查找一个自然的人类生物神经元，并将它们进行比较。
- en: Which features of a real neuron and an artificial neuron are the same? Which
    ones are different?
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 真实神经元和人工神经元有哪些相同的特征？有哪些不同的？
- en: What effect does the learning rate have on gradient descent? What if the learning
    rate is too large? Too small?
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 学习率对梯度下降有什么影响？如果学习率太大？太小？
- en: What relationship does the first layer of a neural network have with the input?
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 神经网络的第一层与输入有什么关系？
- en: What relationship does the last layer of a neural network have with the output?
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 神经网络的最外层与输出有什么关系？
- en: Look up three kinds of loss functions and describe how they work. Include mean
    square loss and the two kinds of cross-entropy loss.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查找三种损失函数并描述它们的工作原理。包括均方损失和两种交叉熵损失。
- en: What would you change if your network was trained and reached 40% accuracy of
    the classification and got stuck, or was unable to learn anything further?
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你的网络在训练后达到了40%的分类准确率并陷入停滞，或者无法进一步学习，你会做些什么改变？
- en: Further reading
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'For more information on the topics that were covered in this chapter, please
    refer to the following resources:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 关于本章涵盖的主题的更多信息，请参考以下资源：
- en: '*Python Deep Learning Cookbook*, by Indra den Bakker, Packt Publishing, 2017'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《Python深度学习食谱》*，作者：Indra den Bakker，Packt出版社，2017年'
- en: '*Artificial Intelligence with Python*, by Prateek Joshi, Packt Publishing,
    2017'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《用Python实现人工智能》*，作者：Prateek Joshi，Packt出版社，2017年'
- en: '*Python Deep Learning*, by Valentino Zocca, Gianmario Spacagna, Daniel Slater,
    and Peter Roelants, Packt Publishing, 2017'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《Python深度学习》*，作者：瓦伦蒂诺·佐卡，吉安马里奥·斯帕卡尼亚，丹尼尔·斯莱特，以及彼得·罗兰茨，Packt出版社，2017年'
- en: '*PyImageSearch Blog*, by Adrian Rosebrock, available at [pyimagesearch.com](http://pyimagesearch.com),
    2018'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《PyImageSearch博客》*，作者：阿德里安·罗斯布鲁克，可在[pyimagesearch.com](http://pyimagesearch.com)找到，2018年'
