- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recognizing Objects Using Neural Networks and Supervised Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is the chapter where we’ll start to combine **robotics** and **artificial
    intelligence** (**AI**) to accomplish some of the tasks we laid out so carefully
    in previous chapters. The subject of this chapter is **object recognition** –
    we will be teaching the robot to recognize what a toy is so that it can then decide
    what to pick up and what to leave alone. We will be using **convolutional neural
    networks** (**CNNs**) as machine learning tools for separating objects in images,
    recognizing them, and locating them in the camera frame so that the robot can
    then locate them. More specifically, we’ll be using images to recognize objects.
    We’ll be taking a picture and then looking to see whether the computer recognizes
    specific types of objects in those pictures. We won’t be recognizing objects themselves,
    but rather images or pictures of objects. We’ll also be putting bounding boxes
    around objects, separating them from other objects and background pixels.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: A brief overview of image processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding our object recognition task
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image manipulation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using YOLOv8 – an object recognition model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will be able to accomplish all of this chapter’s tasks without a robot if
    yours cannot walk yet. We will, however, get better results if the camera is in
    the proper position on the robot. If you don’t have a robot, you can still do
    all of these tasks with a laptop and a USB camera.
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, here’s the hardware and software that you will need to complete the
    tasks in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hardware:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A laptop computer
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Nvidia Jetson Nano
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: USB camera
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Software:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python 3
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenCV2
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: YOLOv8, which is available at [https://github.com/ultralytics/ultralytics](https://github.com/ultralytics/ultralytics)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The source code for this chapter can be found at [https://github.com/PacktPublishing/Artificial-Intelligence-for-Robotics-2e](https://github.com/PacktPublishing/Artificial-Intelligence-for-Robotics-2e).
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will discuss what image processing is.
  prefs: []
  type: TYPE_NORMAL
- en: A brief overview of image processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most of you will be very familiar with computer images, formats, pixel depths,
    and maybe even convolutions. We will be discussing these concepts in the following
    sections; if you already know this, skip ahead. If this is new territory, read
    carefully, because everything we’ll do after is based on this information.
  prefs: []
  type: TYPE_NORMAL
- en: Images are stored in a computer as a two-dimensional array of **pixels** or
    picture elements. Each pixel is a tiny dot. Thousands or millions of tiny dots
    make up each image. Each pixel is a number or series of numbers that describe
    its color. If the image is only a grayscale or black-and-white image, then each
    pixel is represented by a single number that corresponds to how dark or light
    the tiny dot is. This is straightforward so far.
  prefs: []
  type: TYPE_NORMAL
- en: If the image is a color picture, then each dot has three numbers that are combined
    to make its color. Usually, these numbers are the intensity of **Red, Green, and
    Blue** (**RGB**) colors. The combination (0,0,0) represents black (or the absence
    of all colors), while (255,255,255) is white (the sum of all colors). This process
    is called the additive color model. If you work with watercolors instead of computer
    pixels, you’ll know that adding all the colors in your watercolor box makes black
    – that is a subtractive color model. Red, green, and blue are primary colors that
    can be used to make all of the other colors. Since an RGB pixel is represented
    by three colors, the actual image is a three-dimensional array rather than a two-dimensional
    one since each pixel has three numbers, making an array of (height, width, 3).
    So, a picture that is 800 x 600 pixels would be represented by an array of dimensions
    given by (800,600,3), or 1,440,000\. That is a lot of numbers. We will be working
    very hard to minimize the number of pixels we are processing at any given time.
  prefs: []
  type: TYPE_NORMAL
- en: 'While RGB is one set of three numbers that can describe a pixel, there are
    other ways of describing the **color formula** that have various usages. We don’t
    have to use RGB – for instance, we can also use **Cyan, Yellow, and Magenta**
    (**CYM**), which are the complementary colors to RGB, as shown in *Figure 4**.2*.
    We can also break down colors using the **Hue, Saturation, and Value** (**HSV**)
    model, which classifies color by hue (shade of color), saturation (intensity of
    color), and value (brightness of color). HSV is a very useful color space for
    certain calculations, such as converting a color image into grayscale (black and
    white). To turn RGB into a grayscale pixel, you have to do a bit of math – you
    can’t just pull out one channel and keep it. The formula for RGB to grayscale,
    as defined by the **National Television System Committee** (**NTSC**), is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*0.299*Red + 0.587*Green +* *0.114*Blue*'
  prefs: []
  type: TYPE_NORMAL
- en: This is because the different wavelengths of light behave differently in our
    eyes, which are more sensitive to green. If you have color in the HSV color model,
    then creating a grayscale image involves considering *V* (value) and throwing
    the *H* and *S* values away. As you can imagine, this is a lot simpler. This is
    important to understand as we will be doing quite a bit of image manipulation
    throughout this chapter. But first, in the following section, we’ll discuss the
    image recognition task we will be performing in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding our object recognition task
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having a computer or robot recognize an image of a toy is not as simple as taking
    two pictures and then saying `if picture A = picture B, then toy`. We are going
    to have to do quite a bit of work to be able to recognize a variety of objects
    that are randomly rotated, strewn about, and at various distances. We could write
    a program to recognize simple shapes – hexagons, for instance, or simple color
    blobs – but nothing as complex as a toy stuffed dog. Writing a program that did
    some sort of analysis of an image and computed the pixels, colors, distributions,
    and ranges of every possible permutation would be extremely difficult, and the
    result would be very fragile – it would fail at the slightest change in lighting
    or color.
  prefs: []
  type: TYPE_NORMAL
- en: Speaking from experience, I had a recent misadventure with a large robot that
    used a traditional computer vision system to find its battery charger station.
    That robot mistook an old, faded soft drink machine for its charger – let’s just
    say that I had to go buy more fuses.
  prefs: []
  type: TYPE_NORMAL
- en: What we will do instead is teach the robot to recognize a set of images corresponding
    to toys that we will take from various angles. We will do this by using a special
    type of **artificial neural network** (**ANN**) that performs convolution operations
    on images. It is classified as an AI technique because instead of programming
    our software to recognize objects by writing code, we will be training a neural
    network to correctly *segment* (separate from the rest of the image) and *label*
    (classify) groups of pixels in an image by how closely they resemble groups of
    labeled pixels that the network was trained on. Rather than the code determining
    the robot’s behavior, it is the data we train the network on that does the work.
    Since we (the humans) will be training the neural network by providing segmented
    and labeled images, this is called **supervised learning**. This involves telling
    the network what we want it to learn and reinforcing (rewarding) the network based
    on how well it performs. We’ll discuss **unsupervised learning** in [*Chapter
    8*](B19846_08.xhtml#_idTextAnchor235). This process entails us not telling the
    software exactly what to learn, which means it must determine that for itself.
  prefs: []
  type: TYPE_NORMAL
- en: To clarify, in this section, we will tell the ANN what we want it to learn,
    which in this case is to recognize a class of objects we will call *toys*, and
    to draw a bounding box around those objects. This bounding box will tell other
    parts of the robot that a toy is visible and where it is in the image.
  prefs: []
  type: TYPE_NORMAL
- en: I’ll be emphasizing the unique components we will use to accomplish our task
    of recognizing toys in an image. Do you remember what the storyboard from [*Chapter
    3*](B19846_03.xhtml#_idTextAnchor043) told us to do?
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – Use case for identifying objects as toys](img/B19846_04_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – Use case for identifying objects as toys
  prefs: []
  type: TYPE_NORMAL
- en: Our image recognizer must figure out what objects are toys and then locate them
    in the image. This is illustrated in the preceding sketch; objects marked as toys
    have circles around them. The image recognizer must recognize not just *what*
    they are, but also *where* they are.
  prefs: []
  type: TYPE_NORMAL
- en: Image manipulation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So, now that we have an image, what can we do with it? You have probably played
    with Adobe Photoshop or some other image manipulation program such as GIMP, and
    you know that there are hundreds of operations, filters, changes, and tricks you
    can perform on images. For instance, can make an image brighter or darker by adjusting
    the brightness. We can increase the contrast between the white parts of the image
    and the dark parts. We can make an image blurry, usually by applying a Gaussian
    blur filter. We can also make an image sharper (somewhat) by using a filter such
    as an unsharp mask. You can also use an edge detector filter, such as the Canny
    filter, to isolate the edges of an image, where color or value changes. We will
    be using all of these techniques to help the computer identify images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – Various convolutions applied to an image](img/B19846_04_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 – Various convolutions applied to an image
  prefs: []
  type: TYPE_NORMAL
- en: By performing these manipulations, we want the computer to not have the computer
    software be sensitive to the size of the image, which is called **scale invariance**,
    the angle at which the photograph was taken, or **angle invariance**, and the
    lighting available, which is known as **illumination invariance**. This is all
    very desirable in a computer vision system – we would not want an AI system that
    only recognizes our toys from the same angle and distance as the original image.
    Remember, we are going to train our vision system to recognize toys based on a
    labeled set of training images we take in advance, and the robot will have to
    recognize objects based on what it learned from the training set. Here, we are
    going to use features from the image that mostly don’t change based on size, angle,
    distance, or lighting. What sorts of features might these be?
  prefs: []
  type: TYPE_NORMAL
- en: If we look at a common household object, such as a chair, and inspect it from
    several angles, what about the chair does not change? The easy answer is the edges
    and corners. The chair has the same number of corners all the time, and we can
    see a consistent number of them from most angles. It also has a consistent number
    of edges.
  prefs: []
  type: TYPE_NORMAL
- en: Admittedly, that is a bit of an oversimplification of the approach. We will
    be training our ANN on a whole host of image features that may or may not be unique
    to this object and let it decide which work and which do not. We will accomplish
    this by using a generic approach to image manipulation called **convolution**.
  prefs: []
  type: TYPE_NORMAL
- en: Convolution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Every once in a while, you’ll come across some mathematical construction that
    turns a complex task into just a bunch of adding, subtracting, multiplying, and
    dividing. Vectors in geometry work like that, and, in image processing, we have
    the **convolution kernel**. It transpires that most of the common image processing
    techniques – edge detection, corner detection, blurring, sharpening, enhancing,
    and so on – can be accomplished with a simple array construct.
  prefs: []
  type: TYPE_NORMAL
- en: It is pretty easy to understand that in an image, the neighbors of a pixel are
    just as important to what a pixel is as the pixel itself. If you were going to
    try and find all the edge pixels of a box, you would look for a pixel that has
    one type of color on one side, and another type on the other. We need a function
    to find edges by comparing pixels on one side of a pixel to the other.
  prefs: []
  type: TYPE_NORMAL
- en: 'The convolution kernel is a matrix function that applies weights to the pixel
    neighbors – or pixels around the one pixel we are analyzing. The function is usually
    written like this, as a 3x3 matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '| -1 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| -2 | 0 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| -1 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: Table 4.1 – A sample convolution kernel
  prefs: []
  type: TYPE_NORMAL
- en: '**Sobel edge detection** is represented in the *Y* direction. This detects
    edges going up and down. Each block represents a pixel. The pixel being processed
    is in the center. The neighbors of the pixels on each side are the other blocks
    – top, bottom, left, and right. To compute the convolution, the corresponding
    weight is applied to the value of each pixel by multiplying the value (intensity)
    of that pixel, and then adding all of the results. If this image is in color –
    RGB – then we compute the convolution for each color separately and then combine
    the results. Here is an example of a convolution being applied to an image:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 4.3 – Result of a Sobel edge detection convolutio\uFEFFn](img/B19846_04_3.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – Result of a Sobel edge detection convolution
  prefs: []
  type: TYPE_NORMAL
- en: The resulting image is the same size as the original. Note that we only get
    the edge as the result – if the colors are the same on either side of the center
    pixel, they cancel each other out and we get zero, or black. If they are different,
    we get 255, or white, as the answer. If we need a more complex result, we may
    also use a 5x5 convolution, which takes into account the two nearest pixels on
    each side, instead of just one.
  prefs: []
  type: TYPE_NORMAL
- en: The good news is that you don’t have to choose which convolution operation to
    apply to the input images – we will build a software frontend that will set up
    all of the convolutions. This *frontend* is just the part of the program that
    sets up the networks before we start training them. The neural network package
    we’ll be using will determine which convolutions provide the most data and support
    the training output we want.
  prefs: []
  type: TYPE_NORMAL
- en: '"But wait," I hear you say. "What if the pixel is on the edge of the image
    and we don’t have neighbor pixels on one side?" In that case, we have to add padding
    to the image – which is a border of extra pixels that permits us to consider the
    edge pixels as well.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll get into the guts of a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Artificial neurons
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'What is a **neuron**? And how do we make a network out of them? If you can
    remember what you learned in biology, a biological or natural neuron has inputs,
    or dendrites, that connect it to other neurons or sensor inputs. All the inputs
    come to a central body and then leave via the axion, or connection, to other neurons
    via other dendrites. The connection between neurons is called a **synapse**, which
    is a tiny gap that the signal from the nerve must jump. A neuron takes inputs,
    processes them, and activates or sends an output after some threshold level is
    reached. An **artificial neuron** is a software construction that approximates
    the workings of the neurons in your brain and is a very simplified version of
    the natural neuron. It has several inputs, a set of weights, a bias, an activation
    function, and then some outputs to other neurons as a result of the network, as
    shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – Diagram of an artificial neuron](img/B19846_04_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – Diagram of an artificial neuron
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s describe each component in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input**: This is a number or value that’s received from other neurons or
    as an input to the network. In our image processing example, these are pixels.
    This number can be a float or an integer – but it must be just one number.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weight**: This is the adjustable value we change to *train* the neuron. Increasing
    the weight means that the input is more important to our answer, and likewise
    decreasing the weight means the input is used less. To determine the value of
    a neuron, we must combine the values of all the inputs. As the neural network
    is trained, the weights are adjusted on each input, which favors some inputs over
    others. We multiply the input by the weight and then sum all of the results together.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bias**: This is a number that’s added to the sum of the weights. Bias prevents
    the neuron from getting stuck at zero and improves training. This is usually a
    small number. Imagine a scenario where all of the inputs to a neuron are zero;
    in this case, the weights would have no effect. Adding a small bias allows the
    neuron to still have an output, and the network can use that to affect learning.
    Without the bias, a neuron with zeros on its inputs can’t be trained (changing
    the weights has no effect) and is *stuck*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Activation function**: This determines the output of the neuron based on
    the weighted sum of its inputs. The most common types are the **Rectified Linear
    Unit** (**ReLU**) – if the value of the neuron is less than zero, the output is
    zero; otherwise, the output is the input value – and the **sigmoid** (S-shaped)
    function, which is a log function. The activation function propagates information
    across the network and introduces non-linearity to the output of the neuron, which
    allows the neural network to approximate non-linear functions:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: "![Figure 4.5 – Common activation \uFEFFfunctions](img/B19846_04_5.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 – Common activation functions
  prefs: []
  type: TYPE_NORMAL
- en: '**Outputs**: Each layer in the sequential neural network is connected to the
    next layer. Some layers are fully connected – with each neuron in the first layer
    connected to each neuron in the second layer. Others are sparsely connected. There
    is a common process in neural network training called **dropout**, where we randomly
    remove connections. This forces the network to have multiple paths for each bit
    of information it learns, which strengthens the network and makes it able to handle
    more diverse inputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Max pooling of outputs**: We use a special type of network layer (compared
    to a fully connected or sparse layer) called max pooling, where groups of neurons
    corresponding to regions in our image – say a 2x2 block of pixels – go to one
    neuron in the next level. The max pool neuron only takes the largest value from
    each of the four input neurons. This has the effect of downsampling the image
    (making it smaller). This allows the network to associate small features (such
    as the wheels in a Hot Wheels car) with larger features, such as the hood or windshield,
    to identify a toy car:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 4.6 – Max pooling operation](img/B19846_04_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 – Max pooling operation
  prefs: []
  type: TYPE_NORMAL
- en: Now that you understand what a neural network is composed of, let’s explore
    how to train and test one.
  prefs: []
  type: TYPE_NORMAL
- en: Training a CNN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'I want to provide you with an end-to-end look at what we will be doing in the
    code for the rest of this chapter. Remember that we are building a CNN that examines
    pixels in a video frame and outputs if one or more pixel areas that resemble toys
    are in the image, and where they are. The following diagram shows the process
    that we will go through to train the neural network, step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7 – CNN process](img/B19846_04_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 – CNN process
  prefs: []
  type: TYPE_NORMAL
- en: For this task, I decided to use an already existing neural network rather than
    build one from scratch. There are a lot of good CNN object detectors available,
    and honestly, it’s hard to improve on an existing model structure. The one I’ve
    picked for this book is called **YOLOv8**, where *YOLO* stands for *You Only Look
    Once*. Let’s understand how we can use this model for our task.
  prefs: []
  type: TYPE_NORMAL
- en: Using YOLOv8 – an object recognition model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we dive into the details of the YOLOv8 model, let’s talk about why I
    selected it. First of all, the learning process is pretty much the same for any
    CNN we might use. YOLO is a strong open source object detection model with a lot
    of development behind it. It’s considered state of the art, and it already does
    what we need – it detects objects and shows us where they are in images by drawing
    bounding boxes around them. So, it tells us what objects are, and where they are
    located. As you will see, it is very easy to use and can be extended to detect
    other classes of objects other than what it was originally trained for. There
    are a lot of YOLO users out there who can provide a lot of support and a good
    basis for learning about AI object recognition for robotics.
  prefs: []
  type: TYPE_NORMAL
- en: As I mentioned at the beginning of this chapter, we have two tasks we need to
    accomplish to reach our goal of picking up toys with a robot. First, we must determine
    whether the robot can detect a toy with its camera (determine whether there is
    a toy in the camera image) and then figure out where it is in that image so that
    we can drive over to it and pick it up. In this chapter, we’ll learn how to detect
    toys, while in [*Chapter 7*](B19846_07.xhtml#_idTextAnchor221), we’ll discuss
    how we determine distance and navigate to the toy.
  prefs: []
  type: TYPE_NORMAL
- en: YOLOv8 does both tasks in one pass, hence its name. Other kinds of object recognition
    models, such as the one I created in the first edition of this book, identified
    and located objects in images in two steps. First, it found that an object was
    present, and then it figured out where in the image it was located in a separate
    pass. This separate pass would use a sliding window approach, taking a segment
    of the image and using the detection part of the neural network to say `yes` or
    `no` if that segment contained an object it recognized. Then, it would slide the
    window it was considering across the image and test again. This was repeated until
    we had a bunch of image segments that contained the detected object. Then, a process
    called *minmax* would select the smallest box (min) that contained all the visible
    parts of the object (max).
  prefs: []
  type: TYPE_NORMAL
- en: YOLOv8 takes a different approach by combining two neural networks – one that
    detects objects it has been taught to recognize and another that is trained to
    draw bounding boxes based on the center of the object. The direct output of YOLOv8
    includes both the detection and the bounding box for the object. YOLOv8 can also
    *segment* images by pixels, identifying not just a box with the object, but all
    the pixels that belong to that object. We’ll be using a bounding box to help us
    drive the robot to our toys.
  prefs: []
  type: TYPE_NORMAL
- en: 'YOLOv8 comes pretrained on a whole series of object classes (about 80), but
    we can check whether it already works with the toys we want to detect. Let’s test
    YOLOv8’s ability to detect our toys. We can install YOLOv8 using this simple command
    on our PC:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, to test our detection with a picture of toys in the playroom, we will
    use the smallest (in terms of model size) of the YOLOv8 detection models – the
    `yolov8n.pt`). This is the pretrained neural network that Ultralytics provides
    with YOLOv8:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'As shown in the following figure, the only thing detected by the off-the-shelf
    YOLOv8 object model is an upside-down matchbook car, which it incorrectly labels
    as a skateboard:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8 – YOLOv8 output without specific training on our toys](img/B19846_04_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 – YOLOv8 output without specific training on our toys
  prefs: []
  type: TYPE_NORMAL
- en: You have to admit, the little toy car does resemble a skateboard from this angle,
    but this is not the result we want. We need all the toys in the image to be detected,
    not just one. What can we do about this?
  prefs: []
  type: TYPE_NORMAL
- en: The answer is that we can add new training to the network, get all the advantages
    of YOLOv8, and have our custom objects detected as well. For this, we can use
    a process called **transfer learning**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an overview of how we will train our toy detector, after which we will
    discuss these steps in greater detail:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we will prepare a training set of images of the room with toys. This
    means we must take a lot of pictures of the toys from the viewpoint of the robot,
    using the same camera the robot will use. We want to take pictures from all the
    different angles and sides of the toys. I went around the room clockwise, then
    anti-clockwise, snapping pictures every few inches. I took 48 pictures in this
    step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we must use a data labeling program such as RoboFlow ([https://roboflow.com](https://roboflow.com))
    to annotate the images (you can refer to the relevant documentation for detailed
    instructions). The program lets us draw boxes around the objects we want to recognize
    (toys) and label them with a tag – we will use the name `toy`. We are separating
    the parts of the image that contain toys and telling the neural network what to
    call this type of object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.9 – Annotating using RoboFlow, a free data labeling tool](img/B19846_04_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 – Annotating using RoboFlow, a free data labeling tool
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we must split the training set into three parts: a set we use to train
    the network, a set we use to validate the training, and a set we use to test the
    network. We will create a set of 87% of the images for training, 8% for validation,
    and 5% for testing. We’ll put the training data and test data in separate folders.
    RoboFlow has a procedure for this under the **Generate** tab, where there is a
    section labeled **Train/Test Split**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, we must take each image and multiply its training value by combining parts
    of different images in a mosaic. We’ll take parts of four random different pictures
    and combine them. This will increase our training set three-fold, a process called
    **data augmentation**. This is built into RoboFlow. I started with 36 pictures;
    after augmentation, I had 99:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.10 – Mosaic data augmentation creates more training data from our
    limited number of pictures](img/B19846_04_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 – Mosaic data augmentation creates more training data from our limited
    number of pictures
  prefs: []
  type: TYPE_NORMAL
- en: Why are we using this mosaic approach? We still want to have valid bounding
    boxes. The mosaic process resizes any partial bounding boxes that intersect the
    edges.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will be building two programs: the training program, which runs on
    our desktop computer and trains the network, and the working program, which uses
    the trained network to find toys. The training process may not run on our small
    computer onboard the robot or may take a long time to run, so we’ll use the desktop
    computer for this.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, we need to train the network. To achieve this, we must do the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: First, we must scale all our images down to reduce the processing time to a
    reasonable level.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we must initialize the network with uniform random weights.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we must encode a labeled image and input that to the network. The neural
    network only uses the image data to predict what class of object is in the picture,
    and what its bounding box should be. Since we pre-labeled the image with the correct
    answer and used the correct bounding box, we can judge whether the answer is right
    or wrong. If it is right, we can reinforce the weights on the inputs that contributed
    to this answer by incrementing them (the training value). If the answer is wrong,
    we can reduce the weights instead. In neural networks, the error between the desired
    result and the actual result is called **loss**. Repeat this process for each
    image.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, we must test the network by running the testing set of images – which are
    pictures of the same toys, but that were not in the training set. We must analyze
    what sort of output we get over this set (how many are wrong and how many are
    right). If this answer is above 90%, we stop. Otherwise, we go back and run all
    the training images again.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Once we are happy with the results – and we should need between 50 and 100 iterations
    to get there – we must stop and store the weights that we ended up with in the
    training network. This is our **trained CNN**.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Our next task is to find the toys. To do this, we must *deploy* the trained
    network by loading it and using our video images from the live robot to look for
    toys. We’ll get a probability of an image containing a toy from 0% to 100%. We’ll
    scan the input video image in sections and find which sections contain toys. If
    we are not happy with this network, we can reload this network into the training
    program and train it some more.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, let’s cover this in detail, step by step. We have a bit more theory to
    cover before we start writing the code.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how to train our toy detector
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our first task is to prepare a training set. We’ll put the camera on the robot
    and drive the robot around using the teleoperation interface (or just by pushing
    it around by hand), snapping still photos every foot or so. We just need pictures
    with toys in the image since we will be annotating the toys. We need about 200
    pictures – the more, the better. We also need to have a set of pictures in the
    daytime with natural light, and at night, if your room changes lighting between
    day and night. This affords us several advantages: we are using the same room
    and the same camera to find the toys, all under the same lighting conditions.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, we need to label the images. We’ll load the images into RoboFlow to create
    a dataset called `toydetector`. Use the **Upload** tab and drag and drop the images
    or select the folder that contains the images.
  prefs: []
  type: TYPE_NORMAL
- en: The process for us is fairly straightforward. We look at each picture in turn
    and draw a box around any toy objects. We hit *Enter* or the `toy`. This is going
    to take a while.
  prefs: []
  type: TYPE_NORMAL
- en: Once we’ve labeled around 160 toys in our images, we can use the **Generate**
    button in RoboFlow to create our dataset. We must set up the preprocessing task
    to resize our images to 640x640 pixels. This makes the best use of our limited
    computer capacity on the robot. Then, we must augment the dataset to create additional
    images of our limited set, as mentioned previously. We’ll use the mosaic method
    to augment our dataset while preserving the bounding boxes. To do this, we must
    use the **Generate** tab in RoboFlow, then click **Add Augmentation Step** to
    select the type of operation that will affect our images. Then, we must add the
    mosiac augmentation to create more images out of our training set. Now, we can
    hit the **Generate** button to create our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: We started with 48 images that I took (back in step 1); after augmentation,
    we have 114\. We’ll set our test/train split so that it contains 99 images in
    the training set, nine images in the validation set, and six images in the test
    set (87% training, 8% validation, and 5% testing). This makes the best use of
    our limited dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'To download our datasets from RoboFlow, we must install RoboFlow’s interface
    on our computer. It’s a Python package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we must create a short Python program called `downloadDataset.py`. When
    you build your dataset, RoboFlow will provide a unique `api_key` value; this will
    be the password for your account that authorizes access. This goes into the program
    as follows, where I put the asterisks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In the next section, we will retrain the network with this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we’ve done this, the program will produce a lot of output, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: One of the critical parts of training our model is the **training optimizer**.
    We will use **stochastic gradient descent** (**SGD**) for this. SGD is another
    of those simple concepts with a fancy name. Stochastic just means *random*. What
    we want to do is tweak the weights of our neurons to give a better answer than
    we got the first time – this is what we are training, by adjusting the weights.
    We want to change the weights a small amount – but in which direction? We want
    to change the weights in the direction that improves the answer – it makes the
    prediction closer to what we want it to be.
  prefs: []
  type: TYPE_NORMAL
- en: To understand this better, let’s do a little thought experiment. We have a neuron
    that we know is producing the wrong answer and needs adjusting. We’ll add a small
    amount to the weight and see how the answer changes. It gets slightly worse –
    the number is further away from the correct answer. So, we must subtract a small
    amount instead – and, as you might think, the answer gets better. We have reduced
    the amount of error slightly. If we make a graph of the error produced by the
    neuron, we’ll see that we are moving toward an error of zero, or the graph is
    descending toward some minimum value. Another way of saying this is that the slope
    of the line is negative – going toward zero. The amount of the slope can be called
    a **gradient** – just as you would refer to the slope or steepness of a hill as
    the gradient. We can calculate the partial derivative (in other words, the slope
    of the error curve near this point), which tells us the slope of the line.
  prefs: []
  type: TYPE_NORMAL
- en: 'The way we go about adjusting the weights on the network as a whole to minimize
    the loss between the ground truth and the predicted value is called `Y1`, `Y2`,
    and `Y3`. We have three weights – `W1`, `W2`, and `W3`. We’ll have the bias, `B`,
    and our activation function, `D`, which is the ReLU rectifier. The values of our
    inputs are 0.2, 0.7, and 0.02\. The weights are 0.3, 0.2, and 0.5\. Our bias is
    0.3, and the desired output is 1.0\. We calculate the sum of the inputs and weights
    and we get a value of 0.21\. After adding our bias, we get 0.51\. The ReLU function
    passes any value greater than zero, so the activated output of this neuron is
    0.51\. Our desired value is 1.0, which comes from the truth (label) data. So,
    our error is 0.49\. If we add the training rate value to each weight, what happens?
    Take a look at the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.11 – How backpropagation works to adjust weights](img/B19846_04_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.11 – How backpropagation works to adjust weights
  prefs: []
  type: TYPE_NORMAL
- en: The output value now goes up to 0.5192\. Our error goes down to 0.4808\. We
    are on the right track! The gradient of our error slope is *(.4808-.49) / 1 =
    -0.97*. The *1* is because we just have one training sample so far. So, where
    does the stochastic part come from? Our recognition network may have 50 million
    neurons. We can’t be doing all of this math for each one. So, we must take a random
    sampling of inputs rather than all of them to determine whether our training is
    positive or negative.
  prefs: []
  type: TYPE_NORMAL
- en: 'In math terms, the slope of an equation is provided by the derivative of that
    equation. So, in practice, backpropagation takes the partial derivative of the
    error between training epochs to determine the slope of the error, and thus determine
    whether we are training our network correctly. As the slope gets smaller, we reduce
    our training rate to a smaller number to get closer and closer to the correct
    answer:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 4.1\uFEFF2 – The gradient descent process](img/B19846_04_12.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 4.12 – The gradient descent process
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can to our next problem: how do we propagate our weight adjustments
    up the layers of the neural network? We can determine the error at the output
    neuron – just the label value minus the output of the network. How do we apply
    this information to the previous layer? Each neuron’s contribution to the error
    is proportional to its weight. We must divide the error by the weight of each
    input, and that value is now the applied error of the next neuron up the chain.
    Then, we can recompute their weights and so on. This is why neural networks take
    so much compute power:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.13 – Backpropagation error](img/B19846_04_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.13 – Backpropagation error
  prefs: []
  type: TYPE_NORMAL
- en: We backpropagate the error back up the network from the end back to the beginning.
    Then, we start all over again with the next cycle.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we can test our toy detector. Let’s see how we can do this.
  prefs: []
  type: TYPE_NORMAL
- en: Building the toy detector
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can use the following command to test how we did:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The program produces the following output. We can find our image with the labeled
    detections in the `./runs/detect/predict` directory with a number appended depending
    on how many times we run the detection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of our prediction is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.14 – The toy detector in action](img/B19846_04_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.14 – The toy detector in action
  prefs: []
  type: TYPE_NORMAL
- en: 'With this, we have successfully created a toy detector using a neural network.
    The output of the detector, which we will use in [*Chapter 5*](B19846_05.xhtml#_idTextAnchor159)
    to direct the robot and the robot arm to drive to the toy and then pick it up,
    looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: For each detection, the neural network will provide several bits of information.
    We get the `x` and `y` locations of the center of the bounding box, and then the
    height and width of that box. Then, we get a confidence number of how certain
    the network is of the decision that this is a detection. Finally, we get the class
    of the object (what kind of object), which is, of course, a toy.
  prefs: []
  type: TYPE_NORMAL
- en: When we ran the training process for the neural network, if you look in the
    `training` folder found in `runs/detect/train`, there are a whole series of graphs.
    What do these graphs tell us?
  prefs: []
  type: TYPE_NORMAL
- en: 'The first one we need to look at is `F1_curve`. This is the product of precision
    and recall. **Precision** is the ratio of true positives (correctly classified
    objects) from all positives. **Recall** is the proportion of positive detections
    that were identified correctly. So, precision is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Precision =  TP _ TP + FP
  prefs: []
  type: TYPE_NORMAL
- en: Precision is the true positives divided by true positives and false positives
    (items that were identified as detections but were not).
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall is defined slightly differently:'
  prefs: []
  type: TYPE_NORMAL
- en: Recall =  TP _ TP + FN
  prefs: []
  type: TYPE_NORMAL
- en: Here, recall is the true positives divided by true positives plus false negatives.
    A false negative is a missed detection or an object that was not detected when
    it was, in fact, present.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create the F1 curve, we must multiply precision and recall together and
    plot it against *confidence*. The graph shows the level of confidence in our detections
    that produces the best result of trading off precision and recall:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.15 – The F1 confidence curve](img/B19846_04_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.15 – The F1 confidence curve
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, a confidence level of 0.21 gives a detection rate of 0.87\. This
    means that we get the best ratio of true detections to false detections. However,
    this best ratio – 87% – occurs at 0.21 confidence – a rather low number. Detections
    at this low confidence level are hard to distinguish and can be caused by noise
    in measurements. It might be more desirable to have our peak at a higher confidence
    level. I tried several approaches to address this. I ran 200 epochs rather than
    100 and moved the peak F1 confidence level to 51%, but the detection level dropped
    a bit to 85%. Then, I changed the gradient descent technique from SDM to **Adam**,
    an adaptive gradient descent technique that reduces the learning rate as you get
    closer to our goal. This can be done using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This produced a more satisfactory result of 88% true detections at 49% confidence,
    which I think will do a better job for our toy detector. In reviewing my detections,
    there were a few false positives (furniture and other objects being detected as
    toys), so I think that this version will be our toy detector neural network. Although
    I used a fairly small dataset, it would not hurt to have more pictures to work
    with from different angles. Before wrapping this chapter up, let’s briefly summarize
    what we’ve learned so far.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we dove head-first into the world of ANNs. An ANN can be thought
    of as a stepwise non-linear approximation function that slowly adjusts itself
    to fit a curve that matches the desired input to the desired output. The learning
    process consists of several steps, including preparing data, labeling data, creating
    the network, initializing the weights, creating the forward pass that provides
    the output, and calculating the loss (also called the error). We created a special
    type of ANN, a CNN, to examine images. The network was trained using images with
    toys, to which we added bounding boxes to tell the network what part of the image
    was a toy. We trained the network to get an accuracy better than 87% in classifying
    images with toys in them. Finally, we tested the network to verify its output
    and tuned our results using the Adam adaptive descent algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at machine learning for the robot arm in terms
    of reinforcement learning and genetic algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We went through a lot in this chapter. You can use the framework provided to
    investigate the properties of neural networks. Try several activation functions,
    or different settings for convolutions, to see what changes in the training process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Draw a diagram of an artificial neuron and label the parts. Look up a natural,
    human biological neuron and compare them.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which features of a real neuron and an artificial neuron are the same? Which
    ones are different?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What effect does the learning rate have on gradient descent? What if the learning
    rate is too large? Too small?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What relationship does the first layer of a neural network have with the input?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What relationship does the last layer of a neural network have with the output?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Look up three kinds of loss functions and describe how they work. Include mean
    square loss and the two kinds of cross-entropy loss.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What would you change if your network was trained and reached 40% accuracy of
    the classification and got stuck, or was unable to learn anything further?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information on the topics that were covered in this chapter, please
    refer to the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python Deep Learning Cookbook*, by Indra den Bakker, Packt Publishing, 2017'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Artificial Intelligence with Python*, by Prateek Joshi, Packt Publishing,
    2017'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Python Deep Learning*, by Valentino Zocca, Gianmario Spacagna, Daniel Slater,
    and Peter Roelants, Packt Publishing, 2017'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*PyImageSearch Blog*, by Adrian Rosebrock, available at [pyimagesearch.com](http://pyimagesearch.com),
    2018'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
