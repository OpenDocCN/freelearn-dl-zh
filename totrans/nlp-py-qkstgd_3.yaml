- en: Leveraging Linguistics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we are going to pick up a simple use case and see how we can
    solve it. Then, we repeat this task again, but on a slightly different text corpus.
  prefs: []
  type: TYPE_NORMAL
- en: This helps us learn about build intuition when using linguistics in NLP. I will
    be using spaCy here, but you are free to use NLTK or an equivalent. There are
    programmatic differences in their APIs and styles, but the underlying theme remains
    the same.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, we had our first taste of handling free text. Specifically,
    we learned how to tokenize text into words and sentences, pattern match with regex,
    and make fast substitutions.
  prefs: []
  type: TYPE_NORMAL
- en: By doing all of this, we operated with text on a *string* as the main representation.
    In this chapter, we will use *language* and *grammar* as the main representations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn about the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: spaCy, the natural language library for industrial use
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The NLP pipeline, and a bit of English grammar
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-life examples regarding what we can do with linguistics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linguistics and NLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**T**his section is dedicated to introducing you to the ideas and tools that
    have been around during several decades of linguistics. The most traditional way
    to introduce this is to take an idea, talk about it at length, and then put all
    of this together.'
  prefs: []
  type: TYPE_NORMAL
- en: Here, I am going to do this the other way around. We will solve two problems
    and, in the process, look at the tools we will be using. Instead of talking to
    you about a number 8 spanner, I am giving you a car engine and the tools, and
    I will introduce the tools as I use them.
  prefs: []
  type: TYPE_NORMAL
- en: Most NLP tasks are solved in a sequential pipeline, with the results from one
    component feeding into the next.
  prefs: []
  type: TYPE_NORMAL
- en: There is a wide variety of data structures that are used to store pipeline results
    and intermediate steps. Here, for simplicity, I am going to use only the data
    structures that are already in spaCy and the native Python ones like lists and
    dictionaries.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we will tackle the following real-life inspired challenges:'
  prefs: []
  type: TYPE_NORMAL
- en: Redacting names from any document, for example, for GDPR compliance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making quizzes from any text, for example, from a Wikipedia article
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can install spaCy via `conda` or `pip`. Since I am in a `conda` environment,
    I will use the `conda` installation, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Let's download the English language model provided by spaCy. We are going to
    use `en_core_web_lg` (the `lg` at the end stands for *large*). This means that
    this is the most comprehensive and best performing model that spaCy has released
    for general-purpose use.
  prefs: []
  type: TYPE_NORMAL
- en: 'You only need to do this once:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: If you run into any errors when you download this, you can use the smaller model
    instead.
  prefs: []
  type: TYPE_NORMAL
- en: For Windows Shell, you can use `python -m spacy download en` as the administrator.
    From a Linux Terminal, you can use `sudo python -m spacy download en`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s get the imports out of the way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The version I am using here is `version 2.0.11` from conda, but you can use
    any version above 2.0.x.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing textacy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Textacy is a very underappreciated set of tools that revolves around spaCy.
    Its tagline tells you exactly what it does: *NLP, before and after spaCy*. It
    implements tools that use spaCy under the hood, ranging from data-streaming utilities
    for production use to higher level text-clustering functions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can install textacy via `pip` or `conda`. On `conda`, it''s available on
    the `conda-forge` channel instead of the main `conda` channel. We''ve done this
    by adding a `-c` flag and the channel name after that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have the set up and have installation out of our way, let's get
    ready to tackle our challenge in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Redacting names with named entity recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The challenge for this section is to replace all human names with [REDACTED]
    in free text.
  prefs: []
  type: TYPE_NORMAL
- en: Let's imagine that you are a new engineer at the European Bank Co. In preparation
    for the **General Data Processing Regulation** (**GDPR**), the bank is scrubbing
    off names of their customers from all of their old records and special internal
    communications like email and memos. They ask you to do this.
  prefs: []
  type: TYPE_NORMAL
- en: The first way you can do this is to look up the names of your customers and
    match each of them against all of your emails. This can be painfully slow and
    error-prone. For example, let's say the bank has a customer named John D'Souza
    – you might simply refer to him as DSouza in an email, so an exact match for D'Souza
    will never be scrubbed from the system.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we will use an automatic NLP technique to assist us. We will parse all
    of our emails from spaCy and simply replace everyone's names with the token [REDACTED].
    This will be at least 5-10 times faster than matching millions of substrings against
    millions of substrings.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use a small excerpt from the *Harry Potter and Chamber of Secrets*,
    talking about flu as an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s parse the text with spaCy. This runs the entire NLP pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '`doc` now contains a parsed version of the text. We can use it to do anything
    we want! For example, the following command will print out all the named entities
    that were detected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The spaCy object `doc` has an attribute called `ents` which stores all detected
    entities. To find this, spaCy has done a few things behind the scenes for us,
    for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sentence segmentation**, to break the long text into smaller sentences'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tokenization**, to break each sentence into individual words or tokens'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Removed stop words**, to remove words like *a, an, the,* and *of*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NER** for statistical techniques in order to find out which *entities* are
    there in the text and label them with the entity''s type'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s take a quick look at the `doc` object, too:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The `doc` object has a specific object called `ents`, which is short for entities.
    We can use these to look up all of the entities in our text. Additionally, each
    entity has a label:'
  prefs: []
  type: TYPE_NORMAL
- en: In spaCy, all information is stored by numeric hashing. Therefore, `entity.label`
    will be a numeric entry like 378, while `entity.label_` will be human-readable,
    for example, `PERSON`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In spaCy, all human-readable labels can also be explained using the simple
    `spacy.explain(label)` syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Using spaCy''s NER, let''s write a simple function to replace each PERSON name
    with [REDACTED]:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The function takes in text as a string and parses it in the `doc` object using
    the `nlp` object, which we loaded earlier. Then, it traverses each token in the
    document (remember tokenization?). Each token is added to a list. If the token
    has the entity type of a person, it is replaced with [REDACTED] instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the end, we reconstruct the original sentence by converting this list back
    into a string:'
  prefs: []
  type: TYPE_NORMAL
- en: As an exercise, try completing this challenge in-place by editing the original
    string itself instead of creating a new string.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The preceding output is still a leaky faucet if you are trying to make GDPR-compliant
    edits. By using two [REDACTED] blocks instead of one, we are disclosing the number
    of words in a name. This can be seriously harmful if we were to use this in some
    other context, for example, redacting locations or organization names.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s fix this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We do this by merging entities separately from the pipeline. Notice the two
    extra lines of code which call `ent.merge()` on all entities found. The `ent.merge()`
    function combines all of the tokens in each *entity* into one single token. This
    is why it needs to be called on each entity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This output, in practice, can still be incomplete. You might want to remove
    the gender here, for example, *Madam*. Since we are already disclosing the designation,
    which is *nurse*, giving away the gender makes it easier to infer for people (or
    even machines) who are reading this document.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise: Remove any gender pronouns in reference to names.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hint: Look up the co-reference resolution to help you with this.'
  prefs: []
  type: TYPE_NORMAL
- en: Entity types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'spaCy supports the following entity types in the large language model that
    we loaded in the `nlp` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| PERSON | People, including fictional people |'
  prefs: []
  type: TYPE_TB
- en: '| NORP | Nationalities or religious or political groups |'
  prefs: []
  type: TYPE_TB
- en: '| FAC | Buildings, airports, highways, bridges, and so on |'
  prefs: []
  type: TYPE_TB
- en: '| ORG | Companies, agencies, institutions, and so on |'
  prefs: []
  type: TYPE_TB
- en: '| GPE | Countries, cities, states |'
  prefs: []
  type: TYPE_TB
- en: '| LOC | Non-GPE locations, mountain ranges, bodies of water |'
  prefs: []
  type: TYPE_TB
- en: '| PRODUCT | Objects, vehicles, foods, and so on (not services) |'
  prefs: []
  type: TYPE_TB
- en: '| EVENT | Named hurricanes, battles, wars, sports events, and so on |'
  prefs: []
  type: TYPE_TB
- en: '| WORK_OF_ART | Titles of books, songs, and so on |'
  prefs: []
  type: TYPE_TB
- en: '| LAW | Named documents made into laws |'
  prefs: []
  type: TYPE_TB
- en: '| LANGUAGE | Any named language |'
  prefs: []
  type: TYPE_TB
- en: '| DATE | Absolute or relative dates or periods |'
  prefs: []
  type: TYPE_TB
- en: '| TIME | Times smaller than a day |'
  prefs: []
  type: TYPE_TB
- en: '| PERCENT | Percentage, including *%* |'
  prefs: []
  type: TYPE_TB
- en: '| MONEY | Monetary values, including unit |'
  prefs: []
  type: TYPE_TB
- en: '| QUANTITY | Measurements, such as weight or distance |'
  prefs: []
  type: TYPE_TB
- en: '| ORDINAL | *First*, *second*, and so on |'
  prefs: []
  type: TYPE_TB
- en: '| CARDINAL | Numerals that do not fall under another type |'
  prefs: []
  type: TYPE_TB
- en: Let's look at some examples of the preceding entity types in real-world sentences.
    We will also use `spacy.explain()` on all of the entities to build a quick mental
    model of how these things work.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given how lazy I am, I will write a function that I can reuse again and again
    so that I can simply focus on learning and not debugging code for different examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s give it a spin with a few simple examples to begin with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at a slightly longer sentence and Eastern example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Interesting – the model got `Taj Mahal` wrong. Taj Mahal is obviously a world-famous
    monument. However, the model has made a believable mistake, because `Taj Mahal`
    was also the stage name of a blues musician.
  prefs: []
  type: TYPE_NORMAL
- en: In most production use cases, we *fine-tune* the built-in spaCy models for specific
    languages using our own annotations. This will teach the model that Taj Mahal,
    for us, is almost always a monument and not a blues musician.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see if the model repeats these mistakes in other examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s try a different sentence with a different meaning of Ashoka:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Here, spaCy is able to leverage the word `University` to infer that Ashoka is
    a name of an organization and not King Ashoka from Indian history.
  prefs: []
  type: TYPE_NORMAL
- en: It has also figured out that `Young India Fellowship` is one logical entity
    and has not tagged `India` as a location.
  prefs: []
  type: TYPE_NORMAL
- en: It helps to see a few examples such as these ones to form a mental model regarding
    what the limits of what we can and cannot do are.
  prefs: []
  type: TYPE_NORMAL
- en: Automatic question generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Can you automatically convert a sentence into a question?
  prefs: []
  type: TYPE_NORMAL
- en: For instance, *Martin Luther King Jr. was a civil rights activist and skilled
    orator.* to *Who was Martin Luther King Jr.?*
  prefs: []
  type: TYPE_NORMAL
- en: Notice that when we convert a sentence into a question, the answer might not
    be in the original sentence anymore. To me, the answer to that question might
    be something different, and that's fine. We are not aiming for answers here.
  prefs: []
  type: TYPE_NORMAL
- en: Part-of-speech tagging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes, we want to pull out keywords or keyphrases from a larger body of
    text quickly. This helps us mentally paint a picture of what this text is about.
    This is particularly helpful in the analysis of texts, like long emails or essays.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a quick hack, we can pull out all relevant *nouns*. This is because most
    keywords are in fact nouns of some form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We need noun chunks. Noun chunks are noun phrases – not single words, but a
    short phrase which describes the noun. For example, *the blue skies* or *the world's
    largest conglomerate*.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get the noun chunks from a document, simply iterate over `doc.noun_chunks`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Our example text has two sentences, and we can pull out noun phrase chunks from
    each sentence. We will pull out noun phrases instead of single words. This means
    that we are able to pull out *an Indian classical instrument* as one noun. This
    is quite useful, and we will see why in a moment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s take a quick look at all of the parts-of-speech tags in our example
    text. We will use verbs and adjectives to write some simple question-generating
    logic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Notice that here, *instrument* is tagged as a NOUN, while *Indian* and *classical*
    are tagged as adjectives. This makes sense. Additionally, *Bansoori* and *Guitar*
    are tagged as PROPN, or proper nouns.
  prefs: []
  type: TYPE_NORMAL
- en: '**Common nouns versus** **proper nouns:** Nouns name people, places, and things.
    Common nouns name general items such as waiter, jeans, and country. Proper nouns
    name specific things such as Roger, Levi''s, and India.'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a ruleset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Quite often when using linguistics, you will be writing custom rules. Here
    is one data structure suggestion to help you store these rules: a list of dictionaries.
    Each dictionary in turn can have elements ranging from simple string lists to
    lists of strings. Avoid nesting a list of dictionaries inside a dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Here, I have written two rules. Each rule is simply a collection of part-of-speech
    tags that has been stored under the `req_tags` key. Each rule is comprised of
    all of the tags that I will look for in a particular sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on `id`, I will use a hardcoded question template to generate my questions.
    In practice, you can and should move the question template to your ruleset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, I need a function to pull out all of the tokens that match a particular
    tag. We do this by simply iterating over the entire list of and matching each
    token against the target tag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'On runtime complexity:'
  prefs: []
  type: TYPE_NORMAL
- en: This is slow O(n). As an exercise, can you think of a way to reduce this to
    O(1)?
  prefs: []
  type: TYPE_NORMAL
- en: 'Hint: You can precompute some results and store them, but at the cost of more
    memory consumption.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, I am going to write a function to use the preceding ruleset, and also
    use a question template.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the broad outline that I will follow for each sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: For each rule ID, check if all the required tags (`req_tags`) meet the conditions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find the first rule ID that matches
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find the words that match the required part of the speech tags
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fill in the corresponding question template and return the question string
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Within each rule ID match, I do something more: I drop all but the first match
    for each part-of-speech tag that I receive. For instance, when I query for `NNP`,
    I later pick the first element with `NNP[0]`, convert it into a string, and drop
    all other matches.'
  prefs: []
  type: TYPE_NORMAL
- en: 'While this is a perfectly good approach for simple sentences, this breaks down
    when you have conditional statements or complex reasoning. Let''s run the preceding
    function for each sentence in the example text and see what questions we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This is quite good. In practice, you will need a much larger set, maybe 10-15
    rulesets and corresponding templates just to have a decent coverage of *What?*
    questions.
  prefs: []
  type: TYPE_NORMAL
- en: Another few rulesets might be needed to cover *When*, *Who*, and *Where* type
    questions. For instance, *Who plays Bansoori?* is also a valid question from the
    second sentence that we have in the preceding code.
  prefs: []
  type: TYPE_NORMAL
- en: Question and answer generation using dependency parsing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This means PoS tagging and a rule-driven engine can have large coverage and
    reasonable precision with respect to the questions – but it will still be a little
    tedious to maintain, debug, and generalize this system.
  prefs: []
  type: TYPE_NORMAL
- en: We need a set of better tools that is less reliant on the *state* of tokens
    and more on the relationship between them. This will allow you to change the relationship
    to form a question instead. This is where dependency parsing comes in.
  prefs: []
  type: TYPE_NORMAL
- en: What is a dependency parser?
  prefs: []
  type: TYPE_NORMAL
- en: '"A dependency parser analyzes the grammatical structure of a sentence, establishing
    relationships between "head" words and words which modify those heads."'
  prefs: []
  type: TYPE_NORMAL
- en: '- from [Stanford NNDEP Project](https://nlp.stanford.edu/software/nndep.html)'
  prefs: []
  type: TYPE_NORMAL
- en: A dependency parser helps us understand the various ways in which parts of the
    sentence interact or depend on each other. For instance, how is a noun modified
    by adjectives?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Some of these terms are simple enough to guess, for example, `ROOT` is where
    the dependency tree might begin, `nsubj` is the noun or nominal subject, and `cc`
    is a conjunction. However, this is still incomplete. Luckily for us, spaCy includes
    the nifty `explain()` function to help us interpret these:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the following explainer text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: This gives us a good starting point to Google away and pick up some linguistics-specific
    terms. For example, a *conjunct* is often used to connect two clauses, while an
    *attribute* is simply a way to highlight something which is a property of the
    nominal subject.
  prefs: []
  type: TYPE_NORMAL
- en: Nominal subjects are usually nouns or pronouns, which, in turn, are actors (via
    verbs) or have properties (via attributes).
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the relationship
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'spaCy has a built-in tool called **displacy** for displaying simple, but clean
    and powerful visualizations. It offers two primary modes: named entity recognition
    and dependency parsing. Here, we will use the `dep`, or dependency mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take the first sentence for a quick study: we can see that **instrument**
    is **amod**, or adjectively modified by **Indian classicial**. We pulled this
    phrase earlier as a noun chunk:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/550acbbf-98b4-49f1-bce5-933a2c814a19.png)'
  prefs: []
  type: TYPE_IMG
- en: This means that when we pulled noun phrase chunks out of this sentence, spaCy
    must have finished dependency parsing already under the hood.
  prefs: []
  type: TYPE_NORMAL
- en: Also, notice the direction of arrows while the NOUN (instrument) is being modified
    by ADJ. It is the `attr` of the ROOT VERB (is).
  prefs: []
  type: TYPE_NORMAL
- en: 'I leave the dependency visualization of the second sentence up to you to complete:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c77e59c7-1051-490d-ac47-b5899ee247c3.png)'
  prefs: []
  type: TYPE_IMG
- en: This logical tree structure of simple sentences is what we will exploit to simplify
    our question generation. To do this, we need two important pieces of information
  prefs: []
  type: TYPE_NORMAL
- en: The main verb, also known as the ROOT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The subjects on which this ROOT verb is acting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's write some functions to extract these dependency entities in the spaCy
    token format, without converting them into strings.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing textacy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Alternatively, we can import them from textacy itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Inside a Jupyter Notebook, you can see the docstring AND function implementation
    by using the `??` syntax inside the Jupyter Notebook itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Usually, when you ask somebody a question, they are often about a piece of information,
    for example, *What is the capital of India?* Sometimes, they are also about a
    certain action, for example, W*hat did you do on Sunday?*
  prefs: []
  type: TYPE_NORMAL
- en: 'Answering *what* means that we need to find out what the verbs are acting on.
    This means that we need to find the subjects of the verb. Let''s take a more concrete
    but simple example to explore this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: What are the entities in this sentence?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/b7a4ae6b-1235-4487-b4ba-57abeacd1086.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding example might return ORG for the smaller en model. This is why
    using `en_core_web_lg` is important. It gives much better performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try the first few lines of Berlin''s Wikipedia entry:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/2004af84-7959-461e-ac29-4adffdaf01ac.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s find out the main verb in this sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: And what are the nominal subjects of this verb?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'You will notice that this has a reasonable overlap with the noun phrases that
    we pulled from our part-of-speech tagging. However, some of them are different,
    too:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: As an exercise, extend this approach to at least add *Who*, *Where*, and *When*
    questions as a best practice.
  prefs: []
  type: TYPE_NORMAL
- en: Leveling up – question and answer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have been trying to generate questions. But if you were trying to
    make an automated quiz for students, you would also need to mine the right answer.
  prefs: []
  type: TYPE_NORMAL
- en: The answer, in this case, will be simply the objects of a verb. What is an object
    of a verb?
  prefs: []
  type: TYPE_NORMAL
- en: In the sentence, "Give the book to me", "book" is the direct object of the verb
    "give", and "me" is the indirect object.
  prefs: []
  type: TYPE_NORMAL
- en: – from the Cambridge English Dictionary
  prefs: []
  type: TYPE_NORMAL
- en: 'Loosely, the object is the piece on which our verb acts. This is almost always
    the answer to our *what* question. Let''s write a question to find the objects
    of any verb – or, we can pull it from `textacy.spacier.utils.`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s do this for all of the verbs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at the output of our functions from the example text. The first
    is the sentence itself, then the root verb, then the lemma form of that verb,
    followed by the subjects of the verb, and finally the objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s arrange the preceding pieces of information into a neat function that
    we can then reuse:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s run it on our example text and see where it goes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'This seems right to me. Let''s run this on a larger sample of sentences. This
    sample has varying degrees of complexities and sentence structures:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s run it on the whole large example text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Putting it together and the end
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Linguistics is incredibly powerful. I have given you only a taste of its immense
    utility here. We looked at two motivating use cases and a lot of powerful ideas.
    For each use case, I have listed the related idea here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Redacting names:'
  prefs:
  - PREF_OL
  - PREF_UL
  type: TYPE_NORMAL
- en: Named entity recognition
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Question and answer generation:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Part-of-speech tagging
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Lemmatization
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Dependency parsing
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We now have a way to generate questions and answers. What were you going to
    ask the user? Can you match our answers against the user's answers?
  prefs: []
  type: TYPE_NORMAL
- en: Sure, an exact match is perfect. But we should also be looking for *meaning*
    matches, for example, *cake* with *pastry*, or *honesty* with *truthfulness*.
  prefs: []
  type: TYPE_NORMAL
- en: We could use a synonym dictionary – but how do we extend this into sentences
    and documents?
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will answer all of these questions using text representations.
  prefs: []
  type: TYPE_NORMAL
