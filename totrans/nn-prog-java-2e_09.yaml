- en: Chapter 9. Optimizing and Adapting Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, the reader will be presented with techniques that help to
    optimize neural networks, in order to get the best performance. Tasks such as
    input selection, dataset separation and filtering, choosing the number of hidden
    neurons, and cross-validation strategies are examples of what can be adjusted
    to improve a neural network''s performance. Furthermore, this chapter focuses
    on methods for adapting neural networks to real-time data. Two implementations
    of these techniques are presented here. Application problems will be selected
    for exercises. This chapter deals with the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Input selection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dimensionality reduction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data filtering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Structure selection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pruning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validation strategies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cross-validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Online retraining
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stochastic online learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adaptive neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adaptive resonance theory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common issues in neural network implementations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When developing a neural network application, it is quite common to face problems
    regarding how accurate the results are. The source of these problems can be various:'
  prefs: []
  type: TYPE_NORMAL
- en: Bad input selection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Noisy data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Too big a dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsuitable structure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inadequate number of hidden neurons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inadequate learning rate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Insufficient stop condition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bad dataset segmentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bad validation strategy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The design of a neural network application sometimes requires a lot of patience
    and the use of trial and error methods. There is no methodology stating specifically
    which number of hidden units and/or architecture should be used, but there are
    recommendations on how to choose these parameters properly. Another issue programmers
    may face is a long training time, which often causes the neural network to not
    learn the data. No matter how long the training runs, the neural network won't
    converge.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Designing a neural network requires the programmer or designer to test and redesign
    the neural structure as many times as needed, until an acceptable result is obtained.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the neural network solution designer may wish to improve
    the results. Because a neural network can learn until the learning algorithm reaches
    the stop condition, the number of epochs or the mean squared error, the results
    are not accurate enough or not generalized. This will require a redesign of the
    neural structure, or a new dataset selection.
  prefs: []
  type: TYPE_NORMAL
- en: Input selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the key tasks in designing a neural network application is to select
    appropriate inputs. For the unsupervised case, one wishes to use only relevant
    variables on which the neural network will find the patterns. And for the supervised
    case, there is a need to map the outputs to the inputs, so one needs to choose
    only the input variables which somewhat have influence on the output.
  prefs: []
  type: TYPE_NORMAL
- en: Data correlation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One strategy that helps in selecting good inputs in the supervised case is the
    correlation between data series, which is implemented in [Chapter 5](ch05.xhtml
    "Chapter 5. Forecasting Weather"), *Forecasting Weather*. A correlation between
    data series is a measure of how one data sequence reacts or influences the other.
    Suppose we have one dataset containing a number of data series, from which we
    choose one to be an output. Now we need to select the inputs from the remaining
    variables.
  prefs: []
  type: TYPE_NORMAL
- en: The correlation takes values from *-1* to 1, where values near to *+1* indicate
    a positive correlation, values near -1 indicate a negative correlation, and values
    near *0* indicate no correlation at all.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, let''s see three charts of two variables *X* and *Y*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Data correlation](img/B5964_09_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the first chart, to the left, visually one can see that as one variable decreases,
    the other increases its value (corr. -0.8). The middle chart shows the case when
    the two variables vary in the same direction, therefore positive correlation (corr.
    +0.7). The third chart, to the right, shows a case where there is no correlation
    between the variables (corr. *-0.1*).
  prefs: []
  type: TYPE_NORMAL
- en: There is no threshold rule as to which correlation should be taken into account
    as a limit; it depends on the application. While absolute correlation values greater
    than 0.5 may be suitable for one application, in others, values near 0.2 may add
    a significant contribution.
  prefs: []
  type: TYPE_NORMAL
- en: Transforming data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Linear correlation is very good in detecting behaviors between data series when
    they are presumably linear. However, if two data series form a parable when plotted
    together, linear correlation won't be able to identify any relation. That's why
    sometimes we need to transform data into a view that exhibits a linear correlation.
  prefs: []
  type: TYPE_NORMAL
- en: Data transformation depends on the problem that is being faced. It consists
    of inserting an additional data series with processed data from one or more data
    series. One example is an equation (possibly nonlinear) that includes one or more
    parameters. Some behaviors are more detectable under a transformed view of the
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data transformation also involves a bit of knowledge about the problem. However,
    by seeing the scatter plot of two data series, it becomes easier to choose which
    transformation to apply.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another interesting point is regarding removing redundant data. Sometimes this
    is desired when there is a lot of available data in both unsupervised and supervised
    learning. As an example, let''s see a chart of two variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dimensionality reduction](img/B5964_09_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: It can be seen that both X and *Y* variables share the same shape, so this can
    be interpreted as a redundancy, as both variables are carrying almost the same
    information due the high positive correlation. Thus, one can consider a technique
    called **Principal Component Analysis** (**PCA**) which gives a good approach
    for dealing with these cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'The result of PCA will be a new variable summarizing the previous two (or more).
    Basically, the original data series are subtracted by the mean and then multiplied
    by the transposed eigenvectors of the covariance matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dimensionality reduction](img/B05964_09_02_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *SXY* is the covariance between the variables *X* and *Y*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The derived new data will be then:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dimensionality reduction](img/B05964_09_02_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s see now what a new variable would look like in a chart, compared to
    the original ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dimensionality reduction](img/B5964_09_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In our framework, we are going to add the class `PCA` that will perform this
    transformation and preprocessing before applying the data into a neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Data filtering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Noisy data and bad data are also sources of problems in neural network applications;
    that's why we need to filter data. One of the common data filtering techniques
    can be performed by excluding the records that exceed the usual range. For example,
    temperature values are between -40 and 40, so a value such as 50 would be considered
    an outlier and could be taken out.
  prefs: []
  type: TYPE_NORMAL
- en: 'The 3-sigma rule is a good and effective measure for filtering. It consists
    in filtering the values that are beyond three times the standard deviation from
    the mean:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Data filtering](img/B05964_09_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s add a class to deal with data filtering:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'These classes can be called in `DataSet` by the following methods, which are
    then called elsewhere for filtering and reducing dimensionality:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Cross-validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Among a number of strategies for validating a neural network, one very important
    one is cross-validation. This strategy ensures that all data has been presented
    to the neural network as training and test data. The dataset is partitioned into
    *K* groups, of which one is separated for testing while the others are for training:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cross-validation](img/B5964_09_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In our code, let''s create a class called `CrossValidation` to manage cross-validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Structure selection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To choose an adequate structure for a neural network is also a very important
    step. However, this is often done empirically, since there is no rule on how many
    hidden units a neural network should have. The only measure of how many units
    are adequate is the neural network performance. One assumes that if the general
    error is low enough, then the structure is suitable. Nevertheless, there might
    be a smaller structure that could yield the same result.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this context, there are basically two methodologies: constructive and pruning.
    The constructive consists in starting with only the input and output layers, then
    adding new neurons at a hidden layer, until a good result can be obtained. The
    destructive approach, also known as pruning, works on a bigger structure on which
    the neurons having few contributions to the output are taken out.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The constructive approach is depicted in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Structure selection](img/B5964_09_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Pruning is the way back: when given a high number of neurons, one wishes to
    *prune* those whose sensitivity is very low, that is, whose contribution to the
    error is minimal:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Structure selection](img/B5964_09_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'To implement pruning, we`ve added the following properties in the class `NeuralNet`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'A method called `removeNeuron` in the class `NeuralLayer`, which actually sets
    all the connections of the neuron to zero, disables weight updating and fires
    only zero at the neuron`s output. This method is called if the property pruning
    of the `NeuralNet` object is set to true. The sensitivity calculation is according
    to the chain rule, as shown in [Chapter 3](ch03.xhtml "Chapter 3. Perceptrons
    and Supervised Learning"), *Perceptrons and Supervised Learning* and implemented
    in the `calcNewWeigth` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Online retraining
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: During the learning process, it is important to design how the training should
    be performed. Two basic approaches are batch and incremental learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'In batch learning, all the records are fed to the network, so it can evaluate
    the error and then update the weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Online retraining](img/B5964_09_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In incremental learning, the update is performed after each record has been
    sent to the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Online retraining](img/B5964_09_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Both approaches work well and have advantages and disadvantages. While batch
    learning can used for a less frequent, though more directed, weight update, incremental
    learning provides a method for fine-tuned weight adjustment. In that context,
    it is possible to design a mode of learning that enables the network to learn
    continually.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As a suggested exercise, the reader may pick one of the datasets available in
    the code and design a training using part of the records, and then train using
    another part in both modes, online and batch. See the `IncrementalLearning.java`
    file for details.
  prefs: []
  type: TYPE_NORMAL
- en: Stochastic online learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Offline learning means that the neural network learns while not in *operation*.
    Every neural network application is supposed to work in an environment, and in
    order to be at production, it should be properly trained. Offline training is
    suitable for putting the network into operation, since its outputs may be varied
    over large ranges of values, which would certainly compromise the system, if it
    is in operation. But when it comes to online learning, there are restrictions.
    While in offline learning, it's possible to use cross-validation and bootstrapping
    to predict errors, in online learning, this can't be done since there's no "training
    dataset" anymore. However, one would need online training when some improvement
    in the neural network's performance is desired.
  prefs: []
  type: TYPE_NORMAL
- en: 'A stochastic method is used when online learning is performed. This algorithm
    to improve neural network training is composed of two main features: random choice
    of samples for training and variation of learning rate in runtime (online). This
    training method has been used when noise is found in the objective function. It
    helps to escape the local minimum (one of the best solutions) and to reach the
    global minimum (the best solution):'
  prefs: []
  type: TYPE_NORMAL
- en: 'The pseudo-algorithm is displayed below (source: [ftp://ftp.sas.com/pub/neural/FAQ2.html#A_styles](ftp://ftp.sas.com/pub/neural/FAQ2.html#A_styles)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Java project has created the class `BackpropagtionOnline` inside the `learn`
    package. The differences between this algorithm and classic Backpropagation was
    programmed by changing the `train()` method, by adding two new methods: `generateIndexRandomList()`
    and `reduceLearningRate()`. The first one generates a random list of indexes to
    be used in the training step and the second one executes the learning rate online
    variation according to the following heuristic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This method will be called at the end of the `train()` method.
  prefs: []
  type: TYPE_NORMAL
- en: Application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It has used data from previous chapters to test this new way to train neural
    nets. The same neural net topology defined in each chapter ([Chapter 5](ch05.xhtml
    "Chapter 5. Forecasting Weather"), *Forecasting Weather* and [Chapter 8](ch08.xhtml
    "Chapter 8. Text Recognition"), *Text Recognition*) has been used to train the
    nets of this chapter. The first one is the weather forecasting problem and the
    second one is the OCR. The following table shows the comparison of results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Application](img/B5964_09_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In addition, charts of the MSE evolution have been plotted and are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Application](img/B5964_09_09.jpg)![Application](img/B5964_09_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The curve showed in the first chart (Weather Forecast) has a saw shape, because
    of the variation of learning rate. Besides, it's very similar to the curve, as
    shown in [Chapter 5](ch05.xhtml "Chapter 5. Forecasting Weather"), *Forecasting
    Weather* On the other hand, the second chart (OCR) shows that the training process
    was faster and stops near the 900th epoch because it reached a very small MSE
    error.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other experiments were made: training neural nets with a backpropagation algorithm,
    and considering the learning rate found by the online approach. The MSE values
    reduced in both problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Application](img/B5964_09_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Another important observation consists in the fact that training process demonstrated
    by the training terminated almost in the 3,000th epoch. Therefore, it's faster
    and better than the training process seen in [Chapter 8](ch08.xhtml "Chapter 8. Text
    Recognition"), *Text Recognition* using the same algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Adaptive neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Analogous to human learning, neural networks may also work in order not to forget
    previous knowledge. Using the traditional approaches for neural learning, this
    is nearly impossible, due to the fact that every training implies replacing all
    the connections already made by new ones, thereby *forgetting* the previous knowledge.
    Thus a need arises to make the neural networks adapt to new knowledge by incrementing
    instead of replacing their current knowledge. To address that issue, we are going
    to explore one method called **adaptive resonance theory** (**ART**).
  prefs: []
  type: TYPE_NORMAL
- en: Adaptive resonance theory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The question that drove the development of this theory was: *How can an adaptive
    system remain plastic to a significant input and yet keep stability for irrelevant
    inputs?* In other words: *How can it retain previously learned information while
    learning new information?*'
  prefs: []
  type: TYPE_NORMAL
- en: We've seen that competitive learning in unsupervised learning deals with pattern
    recognition, whereby similar inputs yield similar outputs or fire the same neurons.
    In an ART topology, the resonance comes in when the information is being retrieved
    from the network, by providing a feedback from the competitive layer and the input
    layer. So, while the network receives data to learn, there is an oscillation resulting
    from the feedback between the competitive and input layers. This oscillation stabilizes
    when the pattern is fully developed inside the neural network. This resonance
    then reinforces the stored pattern.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A new class called `ART` has created into the some package, inheriting from
    `CompetitiveLearning`. Besides other small contributions, its great change is
    the vigilance test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The training method is shown below. It''s possible to notice that, firstly,
    global variables and the neural net are initialized; after that, the number of
    training sets and the training patterns are stored; then the training process
    begins. The first step of this process is to calculate the index of the winner
    neuron; the second is make attribution of the neural net output. The next step
    consists of verifying whether the neural net has learned or not, whether it has
    learned that weights are fixed; if not, another training sample is presented to
    the net:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we''ve seen a few topics that make a neural network work better,
    either by improving its accuracy or by extending its knowledge. These techniques
    help a lot in designing solutions with artificial neural networks. The reader
    is welcome to apply this framework in any desired task that neural networks can
    be used on, in order to explore the enhanced power that these structures can have.
    Even simple details such as selecting input data may influence the entire learning
    process, as well as filtering bad data or eliminating redundant variables. We
    demonstrated two implementations, two strategies that help to improve the performance
    of a neural network: stochastic online learning and adaptive resonance theory.
    These methodologies enable the network to extend its knowledge and therefore adapt
    to new, changing environments.'
  prefs: []
  type: TYPE_NORMAL
