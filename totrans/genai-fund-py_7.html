<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-130"><a id="_idTextAnchor225"/>7</h1>
<h1 id="_idParaDest-131"><a id="_idTextAnchor226"/>Mastering the Fundamentals of Prompt Engineering</h1>
<p>In <a href="B21773_05.xhtml#_idTextAnchor180"><em class="italic">Chapter 5</em></a>, we briefly evaluated a fine-tuned <strong class="bold">Large Language Model</strong> (<strong class="bold">LLM</strong>) against a general-purpose model using in-context learning or the few-shot prompting approach. In this chapter, we will revisit and explore prompting techniques to examine how well we can adapt a general-purpose LLM without fine-tuning. We explore various prompting strategies that leverage the model’s inherent capabilities to produce targeted and contextually relevant outputs. We will start by examining the shift toward prompt-based language models. Then, we will revisit zero- and few-shot methods, explain prompt-chaining, and <a id="_idIndexMarker517"/>discuss various strategies, including more advanced techniques such as <strong class="bold">Retrieval Augmented Generation</strong> (<strong class="bold">RAG</strong>). At the end of the chapter, we will apply what we have learned and design a prompting strategy with the aim of consistently eliciting factual, accurate, and consistent responses that accomplish a specific business task.</p>
<p>Before diving into specific prompt engineering techniques, we will review a few breakthroughs <a id="_idIndexMarker518"/>that pioneered <strong class="bold">State-of-the-Art</strong> (<strong class="bold">SOTA</strong>) prompt-based models. Research from early 2018 demonstrated how pretraining LLMs could enable few-shot generalization – accurate performance on new tasks given only a prompt statement and a few demonstrations. Follow-up work further tailored model architectures and training specifically for excelling at prompt-based inference across many text-specific tasks. More recent methods optimized model efficiency and stability, enabling accurate and reliable and efficient prompt completion. These innovations laid the groundwork for prompt engineering, demonstrating the remarkable versatility of prompt-based models with minimal input data. Now, prompt design is becoming its own subfield of research – unlocking SOTA performance for an ever-expanding range of tasks. Let’s get started<a id="_idTextAnchor227"/>.</p>
<h1 id="_idParaDest-132"><a id="_idTextAnchor228"/>The shift to prompt-based approaches</h1>
<p>As discussed in prior chapters, the development of the original GPT marked a significant advance in <a id="_idIndexMarker519"/>natural language generation, introducing the use of prompts to instruct the model. This method allowed models such as GPT to perform tasks such as translations – converting text such as “<em class="italic">Hello, how are you?</em>” to “<em class="italic">Bonjour, comment ça va?</em>” – without task-specific training, leveraging deeply contextualized semantic patterns learned during pretraining. This concept of interacting with language models via natural language prompts was significantly expanded with OpenAI’s GPT-3 in 2020. Unlike its predecessors, GPT-3 showcased remarkable capabilities in understanding and responding to prompts in zero- and few-shot learning scenarios, a stark contrast to earlier models that weren’t as adept at such direct interactions. The methodologies, including the specific training strategies and datasets used for achieving GPT-3’s advanced performance, remain largely undisclosed. Nonetheless, it is inferred from OpenAI’s public research that the model learned to follow instructions based on its vast training corpus, and not explicit instruction-tuning. GPT-3’s success in performing tasks based on simple and direct prompting highlighted the potential for language models to understand and execute a wide range of tasks without requiring explicit task-specific training data for each new task. This led to a new paradigm in NLP research and applications, focusing on how effectively a model could be prompted with instructions to perform tasks such as summarization, translation, content generation, and more.</p>
<p>After the release of GPT-3, OpenAI was among the first to introduce specialized fine-tuning to respond more accurately to instructions in their release of InstructGPT (Ouyang et al., 2022). The researchers aimed to teach the model to closely follow instructions using <a id="_idIndexMarker520"/>two novel approaches. The first was <strong class="bold">Supervised Fine-Tuning</strong> (<strong class="bold">SFT</strong>), which involved fine-tuning using datasets carefully crafted from prompts and response pairs. These <em class="italic">demonstration</em> datasets were then used to perform SFT on top of the GPT-3 pretrained model, refining it to provide responses more closely aligned with human responses. <em class="italic">Figure 7</em><em class="italic">.1</em> provides an example of a prompt and response pair.</p>
<div><div><img alt="Figure 7.1: InstructGPT SFT instruction and output pairs" src="img/B21773_07_01.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.1: InstructGPT SFT instruction and output pairs</p>
<p>The second <a id="_idIndexMarker521"/>approach involved additional refinement using <strong class="bold">Reinforcement Learning from Human Feedback</strong> (<strong class="bold">RLHF</strong>). <strong class="bold">Reinforcement Learning</strong> (<strong class="bold">RL</strong>), established decades ago, aims to enhance <a id="_idIndexMarker522"/>autonomous agents’ decision-making capabilities. It does this by teaching them to optimize their actions based on the trade-off between risk and reward. The policy captures the guidelines for the agent’s behavior, dynamically updating <a id="_idIndexMarker523"/>as new insights and feedback are learned to refine decisions further. RL is the exact technology used in many robotic applications and is most famously applied to autonomous driving.</p>
<p>RLHF is a variation of traditional RL, incorporating human feedback alongside the usual risk/reward signals to direct LLM behavior toward better alignment with human judgment. In practice, human labelers would provide preference ratings on model outputs from various prompts, and these ratings would be used to update the model policy, steering the LLM to generate responses that better conform to expected user intent across a range of tasks. In effect, this technique helped to reduce the model’s tendency to generate inappropriate, biased, harmful, or otherwise undesirable content. Although RLHF is not a perfect solution in this regard, it represents a significant step toward models that better understand and align with human values.</p>
<p>Later that <a id="_idIndexMarker524"/>year, following OpenAI’s introduction of InstructGPT, Google unveiled <strong class="bold">Fine-tuned Language Net</strong> or <strong class="bold">FLAN</strong> (Wei et al., 2021). FLAN represented another leap toward prompt-based LLMs, employing explicit instruction tuning. Google’s approach relied on formatting existing datasets into instructions, enabling the model to understand various tasks. Specifically, the authors of FLAN merged multiple NLP datasets across different categories, such as translation and question answering, creating distinct instruction templates for each dataset to frame them as instruction-following tasks. For example, the FLAN team leveraged ANLI challenges (Nie et al., 2020) to construct question-answer pairs explicitly designed to test the model’s understanding of complex textual relationships and reasoning. By framing these challenges as <a id="_idIndexMarker525"/>question-answer pairs, the FLAN team could directly measure a model’s proficiency in deducing these relationships under a unified instruction-following framework. Through this innovative approach, FLAN effectively broadened the scope of tasks a model can learn from, enhancing its overall performance and adaptability across a diverse set of NLU benchmarks. <em class="italic">Figure 7</em><em class="italic">.2</em> presents a theoretical example of question-answer pairs based on ANLI.</p>
<div><div><img alt="Figure 7.2: Training templates based on the ANLI dataset" src="img/B21773_07_02.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.2: Training templates based on the ANLI dataset</p>
<p>Again, the central idea behind FLAN was that each benchmark dataset (e.g., ANLI) could be translated into an intuitive instruction format, yielding a broad mixture of instructional data and natural language tasks.</p>
<p>These advancements, among others, represent a significant evolution in the capabilities of LLMs, transitioning from models that required specific training for each task to those that can intuitively follow instructions and adapt to a multitude of tasks with a simple prompt. This shift has not only broadened the scope of tasks these models can perform but also demonstrated the potential for AI to process and generate human language in complex ways with unprecedented precision.</p>
<p>With this <a id="_idIndexMarker526"/>insight, we can shift our focus to prompt engineering. This discipline combines technical skill, creativity, and human psychology to maximize how models comprehend and respond, appropriately and accurately, to instructions. We will learn prompting techniques that increasingly influence the model’s behavior toward pre<a id="_idTextAnchor229"/>cision.</p>
<h1 id="_idParaDest-133"><a id="_idTextAnchor230"/>Basic prompting – guiding principles, types, and structures</h1>
<p>In <a href="B21773_05.xhtml#_idTextAnchor180"><em class="italic">Chapter 5</em></a>, we introduced the concept of zero- and few-shot learning, providing the model either <a id="_idIndexMarker527"/>a direct instruction, or a direct instruction paired with examples specific to the task. In this section, we will focus on zero-shot learning, where prompting becomes a critical tool for guiding the model to perform specific tasks without prior explicit training on those tasks. This section explores elements of a prompt and how to structure it effectively for zero-shot learning. However, we will first establish some critical guiding principles to help us understand expected model b<a id="_idTextAnchor231"/>ehavior.</p>
<h2 id="_idParaDest-134"><a id="_idTextAnchor232"/>Guiding principles for model interaction</h2>
<p>It is absolutely critical to understand that LLMs, despite their unprecedented SOTA performance <a id="_idIndexMarker528"/>on natural <a id="_idIndexMarker529"/>language tasks, have significant inherent limitations, weaknesses, and susceptibilities. As described in <a href="B21773_01.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a>, LLMs cannot establish rationale or perform logical operations natively. Our interactions with LLMs are typically supplemented by a highly sophisticated application layer that enables the raw model to carry on an extended exchange, integrate with systems that perform computations, and retrieve additional information and knowledge not intrinsic to the model itself. Independent of supplemental integrations, many LLMs are prone to erratic behavior. The most common <a id="_idIndexMarker530"/>of these is often referred to as <strong class="bold">hallucination</strong>, where the model generates a plausible output that is not entirely factual. As such, we should approach the general use of LLMs with the following guidelines in mind:</p>
<ul>
<li><strong class="bold">Apply domain knowledge and subject-matter expertise</strong>: As SOTA LLMs are prone to generating inaccuracies that sound plausible, in use cases where factuality and precision are essential (e.g., code generation, technical writing, or academic research), users must have a firm grasp of the subject matter to detect potential inaccuracies. For example, suppose a user without medical expertise were to prompt a model for healthcare advice. In that case, the model may confuse, conflate, or simply invent information that could result in misleading or potentially dangerous advice. A mitigant for this behavior could be to provide the model with information from a reputable health journal and instruct <a id="_idIndexMarker531"/>it to generate its answers explicitly from the passages provided. This technique is often <a id="_idIndexMarker532"/>called grounding, and we will cover it in depth later. However, even when supplementing the model’s knowledge with verified information, the model <a id="_idIndexMarker533"/>can still misrepresent facts. Without expertise in the specific domain in question, we may never detect misinformation. Consequently, we should generally avoid using LLMs when we cannot verify the model output. Moreover, we should avoid using LLMs in high-stake scenarios where erroneous output could have profound implications.</li>
<li><strong class="bold">Acknowledge bias, underrepresentation, and toxicity</strong>: We have described how LLMs are trained at an enormous scale and often on uncurated datasets. Inevitably, LLMs will learn, exhibit, and amplify societal biases. The model will propagate stereotypes, reflect biased assumptions, and generate toxic and harmful content. Moreover, LLMs can overrepresent certain populations and grossly underrepresent others, leading to a skewed or warped sociological perspective. These notions of bias can manifest in many ways. We will explore this topic, and other ethical implications of LLM use, in detail in <a href="B21773_08.xhtml#_idTextAnchor251"><em class="italic">Chapter 8</em></a>.</li>
<li><strong class="bold">Avoid ambiguity and lack of clarity</strong>: Since LLMs were trained to synthesize information resembling human responses, they can often exhibit notions of creativity. In practice, if prompting is ambiguous or lacks clarity, the model will likely use its vast contextualized knowledge to “assume” or “infer” the meaning or objective of a given prompt or instruction. It may apply some context from its training instead of responding with a clarifying question. As we will describe in the next section, it is crucial to provide clarity by contextualizing input in most cases.</li>
</ul>
<p>Now that <a id="_idIndexMarker534"/>we have established <a id="_idIndexMarker535"/>a few overarching principles to help navigate interactions and keep us within the boundaries of appropriate use, we can deconstruct the various eleme<a id="_idTextAnchor233"/>nts of a prompt.</p>
<h2 id="_idParaDest-135"><a id="_idTextAnchor234"/>Prompt elements and structure</h2>
<p>Generally, a prompt acts as a guide, directing the model’s response toward the desired outcome. It <a id="_idIndexMarker536"/>typically comprises key elements that frame the task at hand, providing clarity and direction <a id="_idIndexMarker537"/>for the model’s generative capabilities. The following table presents the essential elements of a zero-shot prompt.</p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-4">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">Instruction</strong></p>
</td>
<td class="No-Table-Style">
<p>A clear, concise statement describing what you want the model to do. This could be a direct command, a question, or a statement that implies a task.</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">Context</strong></p>
</td>
<td class="No-Table-Style">
<p>Relevant information or background is needed to understand the instruction or the task. This could include definitions or clarifications.</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">Input</strong></p>
</td>
<td class="No-Table-Style">
<p>Following the instructions, the model should work with specific data or content. This could be a piece of text, a question, or any information relevant to the task.</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">Output cue</strong></p>
</td>
<td class="No-Table-Style">
<p>An indication of how the model’s response is to be structured. This can be part of the instruction or implied through the prompt’s formatting.</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 7.1: Basic elements of a zero-shot prompt</p>
<p>We can then structure these elements to maximize the zero-shot approach, whereby the model relies entirely on the prompt to understand and execute a task. In this context, we use the term <em class="italic">task</em> to describe a specific natural language task, such as summarization or translation. However, we will also encounter the term <em class="italic">task</em> applied more <a id="_idIndexMarker538"/>broadly to refer to the output the model should provide. Let’s explore a few concrete examples <a id="_idIndexMarker539"/>of various tasks. In this case, we will be referring to specific NLP tasks and applying a standard structure combining the key elements we’ve described:</p>
<ul>
<li><code>Renewable energy sources like solar and wind power offer sustainable alternatives to fossil fuels, reducing greenhouse gas emissions and promoting </code><code>environmental conservation...</code></p><p class="list-inset"><code>"Renewable energy sources, such as solar and wind, play a crucial role in reducing emissions and conserving </code><code>the environment."</code></p></li>
<li><code>"Hello, how </code><code>are you?"</code></p><p class="list-inset"><code>This translates to "Hola, ¿</code><code>cómo estás?"</code></p><p class="list-inset">The structured templates help us to efficiently and reliably prompt the model for a wide range of inputs, while maintaining a structure that the model has learned to <a id="_idIndexMarker542"/>recognize and respond to. In fact, we can take this a step further by asking the model to provide a specific format in its output. Using the output cue, we can instruct the model to provide a specified format such as Markdown.</p></li>
<li><code>"Please write a Python function to calculate the square of </code><code>a number."</code></p><p class="list-inset"><strong class="bold">Output Cue</strong>: By using the Markdown format in the output cue, the model knows to provide this format and returns the following:</p><pre class="source-code">
def square(number):</pre><pre class="source-code">
    return number ** 2</pre><p class="list-inset">Using LangChain to produce JSON-formatted output, we can leverage the same approach. Specifically, LangChain’s <code>PromptTemplate</code> provides a flexible way to dynamically define a structure for our prompts and insert elements:</p><pre class="source-code">
from langchain.prompts import PromptTemplate</pre><pre class="source-code">
from langchain.llms import OpenAI</pre><pre class="source-code">
# Define a prompt template requesting JSON formatted output</pre><pre class="source-code">
prompt_structure = PromptTemplate(</pre><pre class="source-code">
    template="""</pre><pre class="source-code">
        Context: {context}</pre><pre class="source-code">
        Instruction: {instruction}</pre><pre class="source-code">
        Text: {text_to_process}</pre><pre class="source-code">
        Output Cue: Format the response in JSON with one element called summary.</pre><pre class="source-code">
    """,</pre><pre class="source-code">
    input_variables=["context," "instruction",</pre><pre class="source-code">
        "text_to_process"]</pre><pre class="source-code">
)</pre><pre class="source-code">
# Dynamic elements for the prompt</pre><pre class="source-code">
context = "Summarizing long text passages."</pre><pre class="source-code">
instruction = "Summarize the key points from the following text in JSON format."</pre><pre class="source-code">
text_to_process = """</pre><pre class="source-code">
Mars is the fourth planet from the Sun. The surface of Mars is orange-red because…</pre><pre class="source-code">
"""</pre><pre class="source-code">
formatted_prompt = prompt_structure.format_prompt(</pre><pre class="source-code">
    context=context,</pre><pre class="source-code">
    instruction=instruction,</pre><pre class="source-code">
    text_to_process=text_to_process</pre><pre class="source-code">
)</pre><pre class="source-code">
llm = OpenAI(model_name='gpt-3.5-turbo-instruct',</pre><pre class="source-code">
    temperature=0.9, max_tokens = 256)</pre><pre class="source-code">
response = llm.invoke(formatted_prompt)</pre><pre class="source-code">
print(response)</pre><p class="list-inset">This produces the following:</p><pre class="source-code">
{</pre><pre class="source-code">
    "summary": "Mars is the fourth planet from the Sun, known for its orange-red surface and high-contrast features that make it a popular object for telescope viewing."</pre><pre class="source-code">
}</pre></li>
</ul>
<p>Crafting <a id="_idIndexMarker544"/>effective prompts for zero-shot learning with LLMs requires a clear understanding of the task, thoughtful <a id="_idIndexMarker545"/>structuring of the prompt, and consideration of how the model interprets and responds to different elements within the prompt. By applying these principles, we can guide models to perform various tasks accurately and effectively. Subsequently, we will explore methods to guide models’ behavior through positive affirmations, emotional engagement, and other cognitive-behavioral tech<a id="_idTextAnchor235"/>niques.</p>
<h1 id="_idParaDest-136"><a id="_idTextAnchor236"/>Elevating prompts – iteration and influencing model behaviors</h1>
<p>In this section, we will <a id="_idIndexMarker546"/>introduce techniques for enhancing AI model interactions inspired by cognitive-behavioral research. Behavioral prompting can guide models toward more accurate and nuanced responses. For example, LLM performance can be improved by providing the model with positive emotional stimuli, asking the model to assume a persona or character, or using situational prompting (i.e., role-play). However, it is crucial to recognize that these techniques can also be misused or used to inadvertently introduce stereotypes, as they rely on assumptions and generalizations that may not accurately reflect individual experiences or diverse perspectives. Without careful consideration and monitoring, there is a risk of reinforcing existing biases or <a id="_idIndexMarker547"/>creating new ones, potentially leading to skewed or harmful output. Given these challenges, we will explore a responsible approach to employing cognitive-behavioral techniques in AI interactions, aiming to harness their benefits while minimizing risks and ensuring inclusivity and <a id="_idTextAnchor237"/>fairness.</p>
<h2 id="_idParaDest-137"><a id="_idTextAnchor238"/>LLMs respond to emotional cues</h2>
<p>Research conducted by Microsoft in collaboration with various institutions, including the Beijing Normal University psychology department, suggests that LLMs can mimic and display some <a id="_idIndexMarker548"/>aspects of human emotional intelligence. This can lead to improved task performance when prompts are infused with emotional stimuli. In particular, the researchers hypothesize that emphasizing positive words can trigger more constructive and effective responses. The phenomenon is not well understood, but the effect is that positive emotional cues seem to improve model performance on various tasks consistently (Li et al., 2023). For example, the researchers input phrases encouraging confidence and positive outcomes, such as <code>This is very important to my career</code> or <code>Believe in your abilities and strive for excellence. Your hard work will yield remarkable results</code>. These types of psychological elements not only positively influenced the output quality but also made interactions more effective an<a id="_idTextAnchor239"/>d nuanced.</p>
<h2 id="_idParaDest-138"><a id="_idTextAnchor240"/>Effect of personas</h2>
<p>Research has shown that the effect of <strong class="bold">personas</strong> enables LLMs to better simulate human-like <a id="_idIndexMarker549"/>interactions by adopting varied characteristics, perspectives, and response styles tailored to specific user profiles. This approach enhances the model’s capability for engaging in more personalized, contextually relevant conversations. Incorporating personas into LLM prompts effectively shifts the distribution of possible outputs, directing the model’s generation process toward responses that resonate with specific character traits or perspectives. Thanks to the model’s extensive training on diverse datasets, it can replicate patterns aligning with the described characteristics of personas, focusing its attention and selection process on a more defined set of outputs. This enhances relevance and specificity.</p>
<p>For clarity, let’s reconsider our hypothetical business StyleSprint, a fashion brand. If we prompt an LLM with “<code>As a creative and bold fashion brand, how would StyleSprint describe its upcoming summer collection?</code>”, the model, adopting the brand’s persona, might generate a response such as the following:</p>
<p>“<code>This summer, StyleSprint dares you to stand out! Our collection is a vibrant explosion of colors and patterns, designed for the fearless fashionista. Think bold prints, daring cuts, and a statement that screams confidence. From sun-drenched beaches to chic city streets, our pieces promise to make every moment unforgettable. Embrace your boldness with StyleSprint </code><code>this summer.</code>”.</p>
<p>In this example, we can see how personas guide the model’s outputs to reflect specific brand tones, such as creativity and boldness in this case, making the marketing material more engaging <a id="_idIndexMarker550"/>and aligned with the brand’s identity. Furthermore, using a few-shot approach, we could imbue model responses with a very specific branded tone. We will discuss few-shot in the sections that follow.</p>
<p>However, as discussed, personas should be used with caution. Personas can perpetuate stereotypes and biases, particularly against marginalized groups. A study conducted by researchers at Stanford University found that generating personas based on intersectional demographic groups often yields higher rates of racial stereotypes and patterns of <strong class="bold">othering</strong>, or portraying someone or a group as fundamentally different or alien, compared to human-written texts. In some cases, model outputs could amplify narratives and tropes (Cheng, Durmus, &amp; Jura<a id="_idTextAnchor241"/>fsky, 2023).</p>
<h2 id="_idParaDest-139"><a id="_idTextAnchor242"/>Situational prompting or role-play</h2>
<p>Role-play <a id="_idIndexMarker551"/>in LLMs, in the same way as personas, involves <a id="_idIndexMarker552"/>adopting specific identities or characteristics. However, the two serve different purposes and are applied in distinct contexts. Personas are predefined sets of traits or characteristics that an LLM mimics to tailor its responses, focusing on consistency with those traits. As we have demonstrated with our StyleSprint example, this is useful for creating content with a specific tone or perspective.</p>
<p>Conversely, role-play extends beyond adopting a set of traits to engage in a scenario or narrative dynamically. It involves the LLM taking on a character within a simulated environment or story, responding to inputs in a manner that aligns with both a persona and the evolving <a id="_idIndexMarker553"/>context of the role-play scenario. This can be especially useful in complex simulations where the LLM must navigate and <a id="_idIndexMarker554"/>contribute to ongoing narratives or dialogues that require understanding and adapting to new information or changing circumstances in real time.</p>
<div><div><img alt="Figure 7.3﻿: Persona versus role-play" src="img/B21773_07_03.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.3: Persona versus role-play</p>
<p>Revisiting our real-world scenario, role-play could be particularly useful for creating interactive and engaging customer service experiences. For example, StyleSprint could design a role-play scenario where the LLM acts as a virtual personal stylist. In this role, the model would engage customers with prompts such as <code>I'm your personal stylist for today! What's the occasion you're dressing for?</code>. Based on the customer’s response, the LLM could ask follow-up questions to narrow down preferences, such as <code>Do you prefer bold colors or pastel shades?</code>. Finally, it could recommend outfits from StyleSprint’s collection that match the customer’s needs, saying something such as <code>For a summer wedding, I recommend our Floral Maxi Dress paired with the Vintage Sun Hat. It's elegant, yet perfect for an </code><code>outdoor setting!</code>.</p>
<p>In this case, we leverage the LLM’s ability to dynamically adapt its dialogue based on customer inputs to create an advanced recommender system that facilitates a highly personalized shopping experience. It not only helps in providing tailored fashion advice but also engages customers in a novel way.</p>
<p>Having examined how behavior-inspired techniques, such as personas and role-play, influence model behavior through zero-shot learning, let’s now turn our attention to few-shot learning. This is also known as in-context learning, which we described in <a href="B21773_05.xhtml#_idTextAnchor180"><em class="italic">Chapter 5</em></a>. Recall that the few-shot approach can enhance the consistency, stability, and reliability <a id="_idIndexMarker555"/>of model responses. By providing the model with a few examples of the desired output within the prompt itself, few-shot learning effectively teaches the model the specific task at hand, leading to more predictable and a<a id="_idTextAnchor243"/>ccurate outputs.</p>
<h1 id="_idParaDest-140"><a id="_idTextAnchor244"/>Advanced prompting in action – few-shot learning and prompt chaining</h1>
<p>In few-shot settings, the LLM is presented with a small number of examples of a task within the input <a id="_idIndexMarker556"/>prompt, guiding the model to generate responses that align with these examples. As discussed in the prior chapter, this method significantly <a id="_idIndexMarker557"/>reduces the need for fine-tuning on large, task-specific datasets. Instead, it leverages the model’s pre-existing knowledge and ability to infer context from the examples provided. In <a href="B21773_05.xhtml#_idTextAnchor180"><em class="italic">Chapter 5</em></a>, we saw how this approach was particularly useful for StyleSprint by enabling the model to answer specific questions after being provided with just a few examples, enhancing consistency and creativity in brand messaging.</p>
<p>This method typically involves using between 10 and 100 examples, depending on the model’s context window. Recall that the context window is the limit of tokens a language model can process in one turn. The primary benefit of the few-shot approach is that it minimizes the risk of the model learning a too-narrow distribution from a specific dataset through fine-tuning. Although the performance of few-shot may not always match its fine-tuned counterpart, few-shot learning often outperforms both one-shot and zero-shot learning, showing significant improvements in task adaptation and accuracy. This is especially true as more examples are added to the context window (Brown et al., 2020).</p>
<p>Applications such as LangChain provide a simple and convenient pattern for few-shot implementation. Consider a scenario in which StyleSprint would like to generate taglines for its seasonal collections. In this case, we can provide the model with examples written by the content team to guide the model toward consistency with the brand tone:</p>
<pre class="source-code">
examples = [
    {
        "prompt": "Describe the new summer collection in a bold and adventurous tone.",
        "response": "Dive into summer with StyleSprint's latest collection! Featuring daring designs and vibrant colors, it's all about making bold statements. Perfect for the fearless fashionista ready to conquer the heat."
    },
    {
        "prompt": "How would you introduce our eco-friendly line to environmentally conscious customers?",
        "response": "Embrace sustainable style with StyleSprint's eco-friendly line. Crafted from recycled materials, each piece combines fashion with responsibility, designed for the eco-conscious and trendy."
    }
]</pre>
<p>The LangChain API <a id="_idIndexMarker558"/>offers <code>FewShotPromptTemplate</code> to format the examples <a id="_idIndexMarker559"/>consistently:</p>
<pre class="source-code">
from langchain.prompts.few_shot import FewShotPromptTemplate
from langchain.prompts.prompt import PromptTemplate
# Create a formatter
prompt_format = PromptTemplate(
    input_variables=["prompt", "response"],
    template="Prompt: {prompt}\nResponse: {response}")
# Create the FewShotPromptTemplate
few_shot_prompt = FewShotPromptTemplate(
    examples=examples, example_prompt=prompt_format,
    suffix="Prompt: {input}", input_variables=["input"])</pre>
<p>We can now <a id="_idIndexMarker560"/>apply the template to an LLM to generate a response that we <a id="_idIndexMarker561"/>can expect will closely align with the tone and style of our examples:</p>
<pre class="source-code">
from langchain import LLMChain, OpenAI
# Setup the LLM and LLMChain
llm = OpenAI(temperature=0)
llm_chain = LLMChain(llm=llm, prompt=few_shot_prompt)
# Define the input prompt
input_prompt = "Create a catchy tagline for our winter collection."
# Invoke the chain to generate output
response = llm_chain.run(input_prompt)
# Extract and print the generated slogan
generated_slogan = response
print(generated_slogan) 
    # =&gt; Response: "Stay warm,
    stay stylish,
    stay ahead with StyleSprint's winter collection!"</pre>
<p>Now that we have a consistent and programmatic method for providing the model with examples, we can iterate over the model responses using prompt chaining. A prompt chain generally <a id="_idIndexMarker562"/>refers to chaining together multiple prompts and LLM interactions to have a conversation with the model and iteratively build on the results. Remember, the model itself cannot store information and effectively has no memory or <a id="_idIndexMarker563"/>prior inputs and outputs. Instead, the application layer stores prior inputs and outputs, which are then provided to the model with each exchange. For example, you might start with an initial prompt such as the following:</p>
<pre class="source-code">
"Write a slogan for a winter clothing line"</pre>
<p>The LLM might generate the following:</p>
<pre class="source-code">
"Be warm, be cozy, be you"</pre>
<p>You could then construct a follow-up prompt using the following:</p>
<pre class="source-code">
"Modify the slogan to be more specific about the quality of the clothing"</pre>
<p>You could then keep iterating to improve the output.</p>
<p>Chaining facilitates guiding and interactively refining the text generated rather than relying purely on the given examples. Notice that our prior few-shot code had already established a chain, which we can now use to iterate as follows:</p>
<pre class="source-code">
response = llm_chain.run("Rewrite the last tag to something about embracing the winter")
Response # 
=&gt; Response: Embrace the winter wonderland with StyleSprint's latest collection. From cozy knits to chic outerwear, our pieces will keep you stylish and warm all season long.</pre>
<p>The model is now working from both the examples we provided and any additional instructions we want to include as part of the chain. Prompt chaining, combined with few-shot learning, provides a powerful framework for iteratively guiding language model outputs. By leveraging application state to maintain conversation context, we can steer the model toward desired responses in line with our provided examples. This approach balances harnessing the model’s inferential capabilities and retaining control to align its creative outputs.</p>
<p>Next, we will dive into our practice project, which implements RAG. RAG augments model responses by retrieving and incorporating external data sources. This technique mitigates <a id="_idIndexMarker564"/>hallucination risks by grounding AI-generated text in real data. For <a id="_idIndexMarker565"/>example, StyleSprint may leverage past customer survey results or catalog data to enhance product descriptions. By combining retrieval with prompt chaining, RAG provides a scalable method for balancing<a id="_idTextAnchor245"/> creativity with accuracy.</p>
<h1 id="_idParaDest-141"><a id="_idTextAnchor246"/>Practice project: Implementing RAG with LlamaIndex using Python</h1>
<p>For our practice project, we will shift from LangChain to exploring another library that facilitates the <a id="_idIndexMarker566"/>RAG approach. LlamaIndex is <a id="_idIndexMarker567"/>an open source library that is specifically designed for RAG-based applications. LlamaIndex simplifies ingestion and indexing across various data <a id="_idIndexMarker568"/>sources. However, before we dive into implementation, we will explain the underlying methods and approach behind RAG.</p>
<p>As discussed, the key premise of RAG is to enhance LLM outputs by supplying relevant context from external data sources. These sources should provide specific and verified information to ground model outputs. Moreover, RAG can optionally leverage the few-shot approach by retrieving few-shot examples at inference time to guide generation. This approach alleviates the need to store examples in the prompt chain and only retrieves relevant examples when needed. In essence, the RAG approach is a culmination of many of the prompt engineering techniques we have already discussed. It provides structure, chaining, few-shot learning, and grounding.</p>
<p>At a high level, the RAG pipeline can be described as follows:</p>
<ol>
<li>The RAG component ingests and indexes domain-specific data sources using vector embeddings to encode semantics. As we learned in <a href="B21773_03.xhtml#_idTextAnchor081"><em class="italic">Chapter 3</em></a>, these embeddings are imbued with deeply contextualized, rich semantic information that the component uses later to perform a semantic search.</li>
<li>The component then uses the initial prompt as a search query. The query is input to retrieval systems, which find the most relevant snippets from the indexed dat<a id="_idTextAnchor247"/>a based on vector similarity. Similar to how we applied semantic similarity in prior chapters, RAG leverages a similarity metric to rank results by semantic relevance.</li>
<li>Lastly, the original prompt is augmented with information from the retrieved contexts, and the augmented prompt is passed to the LLM to generate a response grounded in the external data.</li>
</ol>
<p>RAG <a id="_idIndexMarker569"/>introduces two major benefits. First, like the chaining approach, the indexed external data acts as a form of memory, overcoming the LLM’s statelessness. Second, this memory can rapidly scale beyond model <a id="_idIndexMarker570"/>context window limitations, since examples are curated and only provided at the time of the request as needed. Ultimately, RAG unlocks otherwise unattainable capabilities in reliable and factual text generation.</p>
<p>In our practice project, we revisited the StyleSprint product descriptions. This time, we want to leverage RAG to retrieve detailed information about the product to produce very specific descriptions. For the purpose of keeping this project accessible, we will implement an in-memory vector store (Faiss) instead of an external database. We begin with installing the necessary libraries. We will leverage LlamaIndex’s integrated support for Faiss:</p>
<pre class="console">
pip install llama-index faiss-cpu llama-index-vector-stores-faiss</pre>
<p>We will then import the necessary libraries, load the data, and create the index. This vector store will rely on OpenAI’s embeddings, so we must also define <code>OPENAI_API_KEY</code> using a valid key:</p>
<pre class="source-code">
assert os.getenv("OPENAI_API_KEY") is not None, 
    "Please set OPENAI_API_KEY"
# load document vectors
documents = SimpleDirectoryReader("products/").load_data()
# load faiss index
d = 1536 # dimension of the vectors
faiss_index = faiss.IndexFlatL2(d)
# create vector store
vector_store = FaissVectorStore(faiss_index=faiss_index)
# initialize storage context
storage_context = StorageContext.from_defaults(
    vector_store=vector_store)
# create index
index = VectorStoreIndex.from_documents(
    documents,storage_context=storage_context)</pre>
<p>We <a id="_idIndexMarker571"/>now have a vector store that the model <a id="_idIndexMarker572"/>can rely on to retrieve our very specific product data. This means we can query for very specific responses augmented by our data:</p>
<pre class="source-code">
# query the index
query_engine = index.as_query_engine()
response = query_engine.query("describe summer dress with price")
print(response) 
=&gt; A lightweight summer dress with a vibrant floral print is priced at 59.99.</pre>
<p>The result is a response that not only provides an accurate description of the summer dress but also includes specific details, such as the price. This level of detail enriches the customer’s shopping experience, providing relevant and real-time information for customers to consider when making a purchase.</p>
<p>The <a id="_idIndexMarker573"/>next step is to evaluate our RAG implementation to ensure that the answer is relevant, faithful to the source text, reflective <a id="_idIndexMarker574"/>of contextual accuracy, and not in any way harmful or inappropriate. We can apply an open source evaluation framework (RAGAS), which provides implementation of the following metrics:</p>
<ul>
<li><strong class="bold">Faithfulness</strong> assesses the degree to which the generated response is faithful or true to the original context</li>
<li><strong class="bold">Answer relevance</strong> evaluates how relevant the generated answer is to the given question</li>
<li><strong class="bold">Context precision</strong> measures the precision of the context used to generate the answer</li>
<li><strong class="bold">Context recall</strong> measures the recall of the context used to generate the answer</li>
<li><strong class="bold">Context relevancy</strong> assesses the relevancy of the context used to generate the answer</li>
<li><strong class="bold">Harmfulness</strong> evaluates whether a submission (or answer) contains anything that could potentially cause harm to individuals, groups, or society at large</li>
</ul>
<p>This suite of metrics provides an objective measure of RAG application performance based on a comparison to ground truth. In our case, we can use responses generated from our product data, along with context and ground truth derived from the original dataset, to construct an evaluation dataset and perform a comprehensive evaluation using the metrics described.</p>
<p>The following is a simplified code snippet implementing the RAGAS evaluation for our generated product descriptions. A complete working implementation is available in the <code>Chapter 7</code> folder of the GitHub companion to this book (<a href="https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python">https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python</a>).</p>
<pre class="source-code">
# Define the evaluation data
eval_data: Dict[str, Any] = {
   "question": questions, # list of sampled questions
   "answer": engine_responses, # responses from RAG application
   "contexts": contexts, # product metadata
"ground_truth": ground_truth, # corresponding descriptions written by a human
}
# Create a dataset from the evaluation data
dataset: Dataset = Dataset.from_dict(eval_data)
# Define the evaluation metrics
metrics: List[Callable] = [
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
    context_relevancy,
    harmfulness,
]
# Evaluate the model using the defined metrics
result: Dict[str, float] = evaluate(dataset, metrics=metrics)
print(result)</pre>
<p>Our evaluation program should produce the following:</p>
<pre class="source-code">
{'faithfulness': 0.9167, 'answer_relevancy': 0.9961, 'context_precision': 0.5000, 'context_recall': 0.7500, 'harmfulness': 0.0000}</pre>
<p>We can <a id="_idIndexMarker575"/>observe that the system performs <a id="_idIndexMarker576"/>well in generating accurate and relevant answers, as evidenced by high faithfulness and answer relevancy scores. While context precision shows room for improvement, half of the relevant information is correctly identified. Context recall is effective, retrieving most of the relevant context. The absence of harmful content ensures safe interactions. Overall, the system displays robust performance in answering accurately and contextually, but could benefit from refinements in pinpointing the most pertinent context snippets.</p>
<p>As discussed in <em class="italic">Chapters 5</em> and <em class="italic">6</em>, the evaluation of LLMs often requires the additional operational burden of collecting ground-truth data. However, doing so makes it possible to perform a robust evaluation of model and application performance.</p>
<h1 id="_idParaDest-142"><a id="_idTextAnchor248"/>Summary</h1>
<p>In this chapter, we explored the intricacies of prompt engineering. We also explored advanced strategies to elicit precise and consistent responses from LLMs, offering a versatile alternative to fine-tuning. We traced the evolution of instruction-based models, highlighting how they’ve shifted the paradigm toward an intuitive understanding and adaptation to tasks through simple prompts. We expanded on the adaptability of LLMs with techniques such as few-shot learning and retrieval augmentation, which allow for dynamic model guidance across diverse tasks with minimal explicit training. The chapter further explored the structuring of effective prompts, and the use of personas and situational prompting to tailor model responses more closely to specific interaction contexts, enhancing the model’s applicability and interaction quality. We also addressed the nuanced aspects of prompt engineering, including the influence of emotional cues on model performance and the implementation of RLHF to refine model outputs. These discussions underscored the potential of LLMs to exhibit some level of emotional intelligence, leading to more effective and nuanced interactions. However, alongside these technological strides, we stressed the paramount importance of ethical considerations. We highlighted the need for responsible adoption and vigilance to mitigate potential harm and biases associated with these techniques, ensuring fairness, integrity, and the prevention of misuse.</p>
<p>Lastly, we learned how to implement and evaluate the RAG approach to ground the LLM in contextual information from trusted sources and produce answers that are relevant and faithful to the source text. In the next chapter, we will look more closely at the role of individuals in advancing generative AI while emphasizing the dual responsibility of developers and researchers to navigate this rapidly evolving field with a conscientious approach, balancing innovation with ethi<a id="_idTextAnchor249"/>cal imperatives and societal impacts.</p>
<h1 id="_idParaDest-143"><a id="_idTextAnchor250"/>References</h1>
<p>This reference section serves as a repository of sources referenced within this book; you can explore these resources to further enhance your understanding and knowledge of the subject matter:</p>
<ul>
<li>Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., &amp; Lowe, R. (2022). <em class="italic">Training language models to follow instructions with human feedback</em>. In arXiv [cs.CL]. <a href="http://arxiv.org/abs/2203.02155">http://arxiv.org/abs/2203.02155</a></li>
<li>Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., &amp; Le, Q. V. (2021). <em class="italic">Finetuned language models are zero-shot learners</em>. In arXiv [cs.CL]. <a href="http://arxiv.org/abs/2109.01652">http://arxiv.org/abs/2109.01652</a></li>
<li>Nie, Y., Williams, A., Dinan, E., Bansal, M., Weston, J., &amp; Kiela, D. (2020). <em class="italic">Adversarial NLI: A new benchmark for natural language </em><em class="italic">understanding</em>. Arxiv.org.</li>
<li>Li, C., Wang, J., Zhang, Y., Zhu, K., Hou, W., Lian, J., Luo, F., Yang, Q., &amp; Xie, X. (2023). <em class="italic">Large Language Models understand and can be enhanced by emotional stimuli</em>. In arXiv [cs.CL]. <a href="http://arxiv.org/abs/2307.11760">http://arxiv.org/abs/2307.11760</a></li>
<li>Cheng, M., Durmus, E., &amp; Jurafsky, D. (2023). <em class="italic">Marked personas: Using natural language prompts to measure stereotypes in language models. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</em> (Volume 1: Long Papers).</li>
<li>Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., … Amodei, D. (2020). <em class="italic">Language Models are Few-Shot Learners</em>. In arXiv [cs.CL]. <a href="http://arxiv.org/abs/2005.14165">http://arxiv.org/abs/2005.14165</a></li>
</ul>
</div>
</body></html>