<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">All about Rainbow DQN</h1>
                </header>
            
            <article>
                
<p class="mce-root">Throughout this book, we have learned how the various threads in <strong>Reinforcement Learning</strong> (<strong>RL</strong>) combined to form modern RL and then advanced to <strong>Deep Reinforcement Learning</strong> (<strong>DRL</strong>) with the inclusion of <strong>Deep Learning</strong> (<strong>DL</strong>). Like most other specialized fields from this convergence, we now see a divergence back to specialized methods for specific classes of environments. We started to see this in the chapters where we covered <strong>Policy Gradient</strong> (<strong>PG</strong>) methods and the environments it specialized on are continuous control. The flip side of this is the more typical episodic game environment, which is episodic with some form of discrete control mechanism. These environments typically perform better with DQN but the problem then becomes about DQN. Well, in this chapter, we will look at how smart people solved that by introducing Rainbow DQN.</p>
<p>In this chapter, we will introduce Rainbow DQN and understand the problems it works to address and the solutions it provides. Since we have already covered a majority of those solutions in other chapters, we will cover the few that make Rainbow special, by first looking at noisy or fuzzy networks for a better understanding of sampling and exploration. Then, we will look at distributed RL and how it can be used to improve value estimates by predicting distributions, not unlike our policy networks from PG, combining these improvements and others into Rainbow and seeing how well that performs. Finally, we will look at hierarchical DQN for understanding tasks and sub-tasks for possible training environments, with the plan to use this advanced DQN on more sophisticated environments later.</p>
<p>Here are the main topics we will cover in this chapter:</p>
<ul>
<li class="mce-root"><span>Rainbow – combining improvements in deep reinforcement learning</span></li>
<li>Using TensorBoard</li>
<li>Introducing distributional RL</li>
<li>Understanding noisy networks</li>
<li>Unveiling Rainbow DQN</li>
</ul>
<p class="mce-root"/>
<p class="mce-root">Be sure to brush up on your understanding of probability, statistics, stochastic processes, and/or Bayesian inference and variational inference methods. This is something you should have already been doing in previous chapters but that knowledge will now be essential as we move to some of the more advanced content in DRL—and DL, for that matter.</p>
<p>In the next section, we will look to understand what is Rainbow DQN and why it was needed.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Rainbow – combining improvements in deep reinforcement learning</h1>
                </header>
            
            <article>
                
<p>The paper that introduced Rainbow DQN, <em>Rainbow: Combining Improvements in Deep Reinforcement Learning</em>, by DeepMind in October 2017 was developed to address several failings in DQN. DQN was introduced by the same group at DeepMind, led by David Silver to beat Atari games better than humans. However, as we learned over several chapters, while the algorithm was groundbreaking, it did suffer from some shortcomings. Some of these we have already addressed with advances such as DDQN and experience replay. To understand what encompasses all of Rainbow, let's look at the main elements it contributes to RL/DRL:</p>
<ul>
<li class="task-list-item"><strong>DQN</strong><span>: This is, of course, the core algorithm, something we should have a good understanding of by now. We covered DQN in <a href="a9e9aefb-40af-4886-9b4f-94e725dd2f92.xhtml">Chapter 6</a>, <em>Going Deep with DQN</em>.</span></li>
<li class="task-list-item"><strong>Double DQN</strong><span>: This is not to be confused with DDQN or dueling DQN. Again, we already covered this in <a href="42d53358-6f57-4f67-96ce-d8587cbe7cc5.xhtml">Chapter 7</a>, <em>Going Deeper with DDQN</em>.</span></li>
<li class="task-list-item"><strong>Prioritized Experience Replay:</strong> This is another improvement we already covered in <a href="a9e9aefb-40af-4886-9b4f-94e725dd2f92.xhtml">Chapter 6</a>, <em>Going Deep with DQN</em>.</li>
<li class="task-list-item"><strong>Dueling Network Architecture</strong> (<strong>DDQN</strong>): This is an element we have covered already and is mentioned previously.</li>
<li class="task-list-item"><strong>Multi-step returns</strong>: This is our calculation of TD lambda and estimations of expectation or advantage.</li>
<li class="task-list-item"><strong>Distributional RL</strong>: This tries to understand the value distribution, not unlike our policy model in actor-critic except, in this case, we use values. This enhancement will be covered in its own section in this chapter.</li>
<li class="task-list-item"><strong>Noisy Nets</strong>: Noisy or fuzzy networks are DL networks that learn to balance a distribution of weight parameters rather than actual discriminant values of network weights. These advanced DL networks have been used to better understand data distributions and hence data by modeling the weights it uses as distributions. We will cover these advanced DL networks in a further section in this chapter.</li>
</ul>
<p>Many of these improvements we have already covered in previous chapters, apart from distributional RL and noisy nets. We will cover both of these improvements in this chapter starting with distributional RL in a future section. Before we do that though, let's take a step back and improve our logging output capabilities with TensorBoard in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using TensorBoard</h1>
                </header>
            
            <article>
                
<p>At this point in this book, we need to move beyond building toy examples and look to building modules or frameworks you can use to train your own agents in the future. In fact, we will use the code in this chapter for training agents to solve other challenge environments we present in later chapters. That means we need a more general way to capture our progress, preferably to log files that we can view later. Since building such frameworks is such a common task to machine learning as a whole, Google developed a very useful logging framework called TensorBoard. TensorBoard was originally developed as a subset of the other DL framework we mentioned earlier, TensorFlow. Fortunately, for us, PyTorch includes an extension that supports logging to TensorBoard. So, in this section, we are going to set up and install TensorBoard for use as a logging and graphing platform.</p>
<p>In the next exercise, we will install TensorBoard for use with PyTorch. If you have only ever used PyTorch, you likely need to follow these instructions. For those of you that have already installed TensorFlow previously, you will already be good to go and can skip this exercise:</p>
<ol>
<li>Open an Anaconda or Python shell and switch to your virtual environment. You likely already have one open. You may want to create an entirely separate clean virtual environment for <span>TensorBoard</span>. This minimizes the amount of code that can or will break in that environment. <span>TensorBoard</span> is a server application and is best treated as such, meaning the environment it runs on should be pristine.</li>
<li>Then, we need to install TensorBoard with the following command run from your Anaconda or Python shell:</li>
</ol>
<pre style="padding-left: 60px"><strong><span>pip install tensorboard --upgrade</span></strong></pre>
<p style="padding-left: 90px">This installs TensorBoard into your virtual environment.</p>
<ol start="3">
<li>Next, to avoid dependency issues that may arise, we need to run the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong><span>pip install future</span></strong></pre>
<ol start="4">
<li>After that is all installed, we can run TensorBoard with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>tensorboard --logdir runs</strong></pre>
<ol start="5">
<li>This will start a server application on port <kbd>6006</kbd>, by default, and pull logs generated from a folder called <kbd>runs</kbd>, the default used by PyTorch. If you need to customize the port or input log folder, you can use the following command options:</li>
</ol>
<pre style="padding-left: 60px"><strong><span>tensorboard --logdir=/path_to_log_dir/ --port 6006</span></strong></pre>
<ol start="6">
<li>TensorBoard is a server application with a web interface. This is quite common for applications that we always want on and pulling from some output log or other data processing folder. The following diagram shows TensorBoard running in the shell:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-637 image-border" src="assets/73b2b78f-0a04-403d-a8b0-af0d601d02fa.png" style="width:77.58em;height:6.67em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">TensorBoard starting up</div>
<ol start="7">
<li>When TB first runs, it will output the address that you can use in your browser to see the interface. Copy the URL as shown in the diagram and paste it to your favorite web browser. Note that if you have trouble accessing the page, it may be a binding issue, meaning your computer may be preventing this access. A couple of things to try are using a <kbd>localhost:6006</kbd> or <kbd>127.0.0.1:6006</kbd> address and/or use the bind all option for TB.</li>
<li>When your browser opens and assuming you have not run TensorBoard before or put output in the data folder, you will see something like the following:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-638 image-border" src="assets/75b8dfd3-8c5d-4cf0-ba31-7cd6e6ebf197.png" style="width:34.58em;height:24.25em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Empty TensorBoard running</div>
<p>The one important point to note when running TB for the first time is that it is using the right data folder. You should see the <kbd>Data location: runs</kbd> label designating the folder that will contain the logging output.</p>
<p>With TB set up, we can now move on to exploring the innovations that made Rainbow DQN so much better than vanilla DQN. While we have already used and explored a few of those innovations already, we can now move on to understanding the remaining two innovations that were included in Rainbow. We start with distributional RL in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing distributional RL</h1>
                </header>
            
            <article>
                
<p>The name distributional RL can be a bit misleading and may conjure up images of multilayer distributed networks of DQN all working together. Well, that indeed may be a description of distributed RL, but distribution RL is where we try and find the value distribution that DQN is predicting, that is, not just find the maximum or mean value but understanding the data distribution that generated it. This is quite similar to both intuition and purpose for PG methods. We do this by projecting our known or previously predicted distribution into a future or future predicted distribution.</p>
<p>This definitely requires us to review a code example, so open <kbd>Chapter_10_QRDQN.py</kbd> and follow the next exercise:</p>
<ol>
<li>The entire code listing is too big to drop here, so we will look at sections of importance. We will start with the <strong>QRDQN</strong> or <strong>Quantile Regressive DQN</strong>. Quantile regression is a technique to predict distributions from observations. The QRDQN listing follows:</li>
</ol>
<pre style="padding-left: 60px">class QRDQN(nn.Module):<br/>    def __init__(self, num_inputs, num_actions, num_quants):<br/>        super(QRDQN, self).__init__()<br/>        <br/>        self.num_inputs = num_inputs<br/>        self.num_actions = num_actions<br/>        self.num_quants = num_quants<br/>        <br/>        self.features = nn.Sequential(<br/>            nn.Linear(num_inputs, 32),<br/>            nn.ReLU(),<br/>            nn.Linear(32, 64),<br/>            nn.ReLU(),<br/>            nn.Linear(64, 128),<br/>            nn.ReLU(),<br/>            nn.Linear(128, self.num_actions * self.num_quants)<br/>        )        <br/>        self.num_quants, use_cuda=USE_CUDA)<br/>        <br/>    def forward(self, x):<br/>        batch_size = x.size(0)<br/>        x = self.features(x)    <br/>        x = x.view(batch_size, self.num_actions, self.num_quants)        <br/>        return x<br/>    <br/>    def q_values(self, x):<br/>        x = self.forward(x)<br/>        return x.mean(2)<br/>    <br/>    def act(self, state, epsilon):<br/>        if random.random() &gt; epsilon:<br/>            state = autograd.Variable(torch.FloatTensor(np.array(state, dtype=np.float32)).unsqueeze(0), volatile=True)<br/>            qvalues = self.forward(state).mean(2)<br/>            action = qvalues.max(1)[1]<br/>            action = action.data.cpu().numpy()[0]<br/>        else:<br/>            action = random.randrange(self.num_actions)<br/>        return action</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="2">
<li>Most of this code looks the same as before, but one thing to note and not get confused by is <kbd>qvalues</kbd> denotes a Q value (state-action) and not a Q policy value as we saw with PG methods.</li>
<li>Next, we will scroll down to the <kbd>projection_distribution</kbd> function, as shown here:</li>
</ol>
<pre style="padding-left: 60px">def projection_distribution(dist, next_state, reward, done):<br/>    next_dist = target_model(next_state)<br/>    next_action = next_dist.mean(2).max(1)[1]<br/>    next_action = next_action.unsqueeze(1).unsqueeze(1).expand(batch_size, 1, num_quant)<br/>    next_dist = next_dist.gather(1, next_action).squeeze(1).cpu().data<br/><br/>    expected_quant = reward.unsqueeze(1) + 0.99 * next_dist * (1 - done.unsqueeze(1))<br/>    expected_quant = autograd.Variable(expected_quant)<br/><br/>    quant_idx = torch.sort(dist, 1, descending=False)[1]<br/><br/>    tau_hat = torch.linspace(0.0, 1.0 - 1./num_quant, num_quant) + 0.5 / num_quant<br/>    tau_hat = tau_hat.unsqueeze(0).repeat(batch_size, 1)<br/>    quant_idx = quant_idx.cpu().data<br/>    batch_idx = np.arange(batch_size)<br/>    tau = tau_hat[:, quant_idx][batch_idx, batch_idx]<br/>        <br/>    return tau, expected_quant</pre>
<ol start="4">
<li>This code is quite mathematical and outside the scope of this book. It essentially just extracts what it believes to be the distribution of Q values.</li>
<li>After that, we can see the construction of our two models, denoting that we are building a DDQN model here using the following code:</li>
</ol>
<pre style="padding-left: 60px">current_model = QRDQN(env.observation_space.shape[0], env.action_space.n, num_quant)<br/>target_model = QRDQN(env.observation_space.shape[0], env.action_space.n, num_quant)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="6">
<li>After that, we get the computation of the TD loss with the <kbd>computer_td_loss</kbd> function, shown here:</li>
</ol>
<pre style="padding-left: 60px">def compute_td_loss(batch_size):<br/>    state, action, reward, next_state, done = replay_buffer.sample(batch_size) <br/><br/>    state = autograd.Variable(torch.FloatTensor(np.float32(state)))<br/>    next_state = autograd.Variable(torch.FloatTensor(np.float32(next_state)), volatile=True)<br/>    action = autograd.Variable(torch.LongTensor(action))<br/>    reward = torch.FloatTensor(reward)<br/>    done = torch.FloatTensor(np.float32(done))<br/><br/>    dist = current_model(state)<br/>    action = action.unsqueeze(1).unsqueeze(1).expand(batch_size, 1, num_quant)<br/>    dist = dist.gather(1, action).squeeze(1)<br/>    <br/>    tau, expected_quant = projection_distribution(dist, next_state, reward, done)<br/>    k = 1<br/>    <br/>    huber_loss = 0.5 * tau.abs().clamp(min=0.0, max=k).pow(2)<br/>    huber_loss += k * (tau.abs() - tau.abs().clamp(min=0.0, max=k))<br/>    quantile_loss = (tau - (tau &lt; 0).float()).abs() * huber_loss<br/>    loss = torch.tensor(quantile_loss.sum() / num_quant, requires_grad=True)<br/>        <br/>    optimizer.zero_grad()<br/>    loss.backward()<br/>    nn.utils.clip_grad_norm(current_model.parameters(), 0.5)<br/>    optimizer.step()<br/>    <br/>    return loss</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="7">
<li>This loss calculation function is similar to other DQN implementations we have seen before although this one does expose a few twists and turns. Most of the twists are introduced by using <strong>Quantile Regression</strong> (<strong>QR</strong>). QR is essentially about predicting the distribution using quants or quantiles, that is, slices of the probability to iteratively determine the predicted distribution. This predicted distribution is then used to determine the network loss and train it back through the DL network. If you scroll back up, you can note the introduction of the three new hyperparameters that allow us to tune that search. These new values, shown here, allow us to define the number iterations, <kbd>num_quants</kbd>, and search range, <kbd>Vmin</kbd> and <kbd>Vmax</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>num_quant = 51<br/>Vmin = -10<br/>Vmax = 10</strong></pre>
<ol start="8">
<li>Finally, we can see how the training code is run by scrolling to the bottom of the code and reviewing it here:</li>
</ol>
<pre style="padding-left: 60px">state = env.reset()<br/>for iteration in range(1, iterations + 1):<br/>    action = current_model.act(state, epsilon_by_frame(iteration))<br/>    <br/>    next_state, reward, done, _ = env.step(action)<br/>    replay_buffer.push(state, action, reward, next_state, done)<br/>    <br/>    state = next_state<br/>    episode_reward += reward<br/>    <br/>    if done:<br/>        state = env.reset()<br/>        all_rewards.append(episode_reward)<br/>        episode_reward = 0<br/>        <br/>    if len(replay_buffer) &gt; batch_size:<br/>        loss = compute_td_loss(batch_size)<br/>        losses.append(loss.item())<br/>        <br/>    if iteration % 200 == 0:<br/>        plot(iteration, all_rewards, losses, episode_reward)<br/>        <br/>    if iteration % 1000 == 0:<br/>        update_target(current_model, target_model)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="9">
<li>We have seen very similar code in Chapters 6 and 7 when we previously looked at DQN so we won't review it here. Instead, familiarize yourself with the DQN model again if you need to. Note the differences between it and PG methods. When you are ready, run the code as you normally would. The output of running the sample is shown here:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-868 image-border" src="assets/973d2e7f-a4f7-427c-a407-f0b192ec7e6a.png" style="width:30.00em;height:32.75em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Example output from Chapter_10_QRDQN.py</div>
<p>The output generated from this sample is just a reminder that we have more information being output to a log folder. To see that log folder, we need to run TensorBoard again and we will do that in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Back to TensorBoard</h1>
                </header>
            
            <article>
                
<p>With the sample from the last exercise still running, we want to return to TensorBoard and now see the output from our sample running. To do that, open a new Python/Anaconda command shell and follow the next exercise:</p>
<ol>
<li>Open the shell to the same folder you are running your previous exercise code example in. Switch to your virtual environment or a special one just for TB and then run the following command to start the process:</li>
</ol>
<pre style="padding-left: 60px"><strong>tensorboard --logdir=runs</strong></pre>
<ol start="2">
<li>This will start TB in the current folder using that <kbd>runs</kbd> folder as the data dump directory. After the sample is running for a while, you may see something like the following when you visit the TB web interface now:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-869 image-border" src="assets/14068085-91c6-41a9-bd47-290e50f4f0c6.png" style="width:111.92em;height:73.33em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">TensorBoard output from Chapter_10_QRDQN.py</div>
<ol start="3">
<li>Turn up the <strong>Smoothing</strong> control as shown in the screenshot to see the visual trend of the data. Seeing a general trend in the data allows you to extrapolate if or when your agent may be fully trained.</li>
<li>Now, we need to go back to the <kbd>Chapter_10_QRDQN.py</kbd> example code and see how we generated this output data. First, notice the new <kbd>import</kbd> and declaration of a new variable, <kbd>writer</kbd>, of the <kbd>SummaryWriter</kbd> class imported from <kbd>torch.utils.tensorboard</kbd> and shown here:</li>
</ol>
<pre style="padding-left: 60px">from common.replay_buffer import ReplayBuffer<br/><strong>from torch.utils.tensorboard import SummaryWriter</strong><br/><br/>env_id = "LunarLander-v2"<br/>env = gym.make(env_id)<br/><strong>writer = SummaryWriter()</strong></pre>
<ol start="5">
<li>The <kbd>writer</kbd> object is used to output to the log files that get constructed in the <kbd>run</kbd> folder. Every time we run this example piece of code now, this writer will output to the <kbd>run</kbd> folder. You can alter this behavior by inputting a directory into the <kbd>SummaryWriter</kbd> constructor.</li>
<li>Next, scroll down to the revised <kbd>plot</kbd> function. This function, shown here, now generates the log output we can visualize with TB:</li>
</ol>
<pre style="padding-left: 60px">def plot(iteration, rewards, losses, ep_reward): <br/>    print("Outputing Iteration " + str(iteration))<br/>    writer.add_scalar('Train/Rewards', rewards[-1], iteration)<br/>    writer.add_scalar('Train/Losses', losses[-1], iteration) <br/>    writer.add_scalar('Train/Exploration', epsilon_by_frame(iteration), iteration)<br/>    writer.add_scalar('Train/Episode', ep_reward, iteration)<br/>    writer.flush()</pre>
<ol start="7">
<li>This updated block of code now outputs results using TB <kbd>writer</kbd> and not <kbd>matplotlib plot</kbd>, as we did before. Each <kbd>writer.add_scalar</kbd> call adds a value to the data plot we visualized earlier. There are plenty of other functions you can call on to add many different types of output. Considering the ease with which we can generate impressive output, you likely may ever find a need to use <kbd>matplotlib</kbd> again.</li>
<li>Go back to your TB web interface and observe the continued training output.</li>
</ol>
<p>This code sample may need some tuning to be able to tune the agent to a successful policy. However, you now have at your disposal even more powerful tools TensorBoard to assist you in doing that. In the next section, we will look at the last improvement introduced by Rainbow, noisy networks.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding noisy networks</h1>
                </header>
            
            <article>
                
<p>Noisy networks are not those networks that need to know everything—those would be nosey networks. Instead, noisy networks introduce the concept of noise into the weights used to predict the output through the network. So, instead of having a single scalar value to denote the weight in a perceptron, we now think of weights as being pulled from some form of distribution. Obviously, we have a common theme going on here and that is going from working with numbers as single scalar values to what is better described as a distribution of data. If you have studied the subject of Bayesian or variational inference, you will likely understand this concept concretely.</p>
<p>For those without that background, let's look at what a distribution could be in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img class="aligncenter size-full wp-image-642 image-border" src="assets/08f99ae7-4012-4212-8ed5-be3f45e89823.png" style="width:37.58em;height:26.42em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Example of different data distributions</div>
<p>The source for the preceding diagram comes from a blog post by Akshay Sharma (<a href="https://medium.com/mytake/understanding-different-types-of-distributions-you-will-encounter-as-a-data-scientist-27ea4c375eec">https://medium.com/mytake/understanding-different-types-of-distributions-you-will-encounter-as-a-data-scientist-27ea4c375eec</a>). What is shown in the diagram is a sampling pattern of various well-known data distributions. Basic statistics assumes that all data is always evenly or normally distributed. In statistics, you will learn of other distributions you use to define various tests of variance or fit such as the Chi or Student's t. You are also likely quite familiar with a uniform distribution if you have ever sampled a random number in a computer program. Most computer programs always assume a uniform distribution, which in some ways is the problem we have in machine learning and hence the move to better understanding how real data or actions/events are distributed.</p>
<div class="packt_infobox">Variational inference or quantitative risk analysis is a technique whereby we use typical equations of engineering, economics, or other disciplines and assume their inputs are distributions rather that discriminant values. By using distributions of data as input we, therefore, assume our output is also a distribution in some form. That distribution can then be used to evaluate terms of risk or reward.</div>
<p>It's time for another exercise, so open <kbd>Chapter_10_NDQN.py</kbd> and follow the next exercise:</p>
<ol>
<li>This is another big example so we will only focus on what is important. Let's start by scrolling down and looking at the <kbd>NoisyDQN</kbd> class here:</li>
</ol>
<pre style="padding-left: 60px">class NoisyDQN(nn.Module):<br/>    def __init__(self, num_inputs, num_actions):<br/>        super(NoisyDQN, self).__init__()<br/>        <br/>        self.linear = nn.Linear(env.observation_space.shape[0], 128)<br/>        <strong>self.noisy1 = NoisyLinear(128, 128)</strong><br/><strong>        self.noisy2 = NoisyLinear(128, env.action_space.n)</strong><br/>        <br/>    def forward(self, x):<br/>        x = F.relu(self.linear(x))<br/>        x = F.relu(self.noisy1(x))<br/>        x = self.noisy2(x)<br/>        return x<br/>    <br/>    def act(self, state):<br/>        state = autograd.Variable(torch.FloatTensor(state).unsqueeze(0), volatile=True)<br/>        q_value = self.forward(state)<br/>        action = q_value.max(1)[1].item()<br/>        return action<br/>    <br/>    def reset_noise(self):<br/>        self.noisy1.reset_noise()<br/>        self.noisy2.reset_noise()</pre>
<ol start="2">
<li>This is quite similar to our previous DQN samples but with a key difference: the addition of a new specialized DL network layer type called <kbd>NoisyLinear</kbd><strong>.</strong></li>
<li>Scrolling down further, we can see the <kbd>td_compute_loss</kbd> function updated to handle the noisy or fuzzy layers:</li>
</ol>
<pre style="padding-left: 60px">def compute_td_loss(batch_size, beta):<br/>    state, action, reward, next_state, done, weights, indices = replay_buffer.sample(batch_size, beta) <br/><br/>    state = autograd.Variable(torch.FloatTensor(np.float32(state)))<br/>    next_state = autograd.Variable(torch.FloatTensor(np.float32(next_state)))<br/>    action = autograd.Variable(torch.LongTensor(action))<br/>    reward = autograd.Variable(torch.FloatTensor(reward))<br/>    done = autograd.Variable(torch.FloatTensor(np.float32(done)))<br/>    weights = autograd.Variable(torch.FloatTensor(weights))<br/><br/>    q_values = current_model(state)<br/>    next_q_values = target_model(next_state)<br/><br/>    q_value = q_values.gather(1, action.unsqueeze(1)).squeeze(1)<br/>    next_q_value = next_q_values.max(1)[0]<br/>    expected_q_value = reward + gamma * next_q_value * (1 - done)<br/>    <br/>    loss = (q_value - expected_q_value.detach()).pow(2) * weights<br/>    prios = loss + 1e-5<br/>    loss = loss.mean()<br/>        <br/>    optimizer.zero_grad()<br/>    loss.backward()<br/>    optimizer.step()<br/>    <br/>    replay_buffer.update_priorities(indices, prios.data.cpu().numpy())<br/>    current_model.reset_noise()<br/>    target_model.reset_noise()<br/>    <br/>    return loss</pre>
<ol start="4">
<li>This function is quite close to our previous vanilla DQN examples and that is because all of the work/difference is going on in the new noisy layers, which we will get to shortly.</li>
<li>Scroll back up the definition of the <kbd>NoisyLinear</kbd> class, as seen here:</li>
</ol>
<pre style="padding-left: 60px">class NoisyLinear(nn.Module):<br/>    def __init__(self, in_features, out_features, std_init=0.4):<br/>        super(NoisyLinear, self).__init__()<br/>        <br/>        self.in_features = in_features<br/>        self.out_features = out_features<br/>        self.std_init = std_init<br/>        <br/>        self.weight_mu = nn.Parameter(torch.FloatTensor(out_features, in_features))<br/>        self.weight_sigma = nn.Parameter(torch.FloatTensor(out_features, in_features))<br/>        self.register_buffer('weight_epsilon', torch.FloatTensor(out_features, in_features))<br/>        <br/>        self.bias_mu = nn.Parameter(torch.FloatTensor(out_features))<br/>        self.bias_sigma = nn.Parameter(torch.FloatTensor(out_features))<br/>        self.register_buffer('bias_epsilon', torch.FloatTensor(out_features))<br/>        <br/>        self.reset_parameters()<br/>        self.reset_noise()<br/>    <br/>    def forward(self, x):<br/>        if self.training: <br/>            weight = self.weight_mu + self.weight_sigma.mul(autograd.Variable(self.weight_epsilon))<br/>            bias = self.bias_mu + self.bias_sigma.mul(autograd.Variable(self.bias_epsilon))<br/>        else:<br/>            weight = self.weight_mu<br/>            bias = self.bias_mu<br/>        <br/>        return F.linear(x, weight, bias)<br/>    <br/>    def reset_parameters(self):<br/>        mu_range = 1 / math.sqrt(self.weight_mu.size(1))<br/>        <br/>        self.weight_mu.data.normal_(-mu_range, mu_range)<br/>        self.weight_sigma.data.fill_(self.std_init / math.sqrt(self.weight_sigma.size(1)))<br/>        <br/>        self.bias_mu.data.normal_(-mu_range, mu_range)<br/>        self.bias_sigma.data.fill_(self.std_init / math.sqrt(self.bias_sigma.size(0)))<br/>    <br/>    def reset_noise(self):<br/>        epsilon_in = self._scale_noise(self.in_features)<br/>        epsilon_out = self._scale_noise(self.out_features)<br/>        <br/>        self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))<br/>        self.bias_epsilon.copy_(self._scale_noise(self.out_features))<br/>    <br/>    def _scale_noise(self, size):<br/>        x = torch.randn(size)<br/>        x = x.sign().mul(x.abs().sqrt())<br/>        return x</pre>
<ol start="6">
<li>The <kbd>NoisyLinear</kbd> class is a layer that uses a normal distribution to define each of the weights in the layers. This distribution is assumed to be normal, which means it is defined by a mean, mu, and standard variation, sigma. So, if we assumed 100 weights in a layer previously, we would now have two values (mu and sigma) that now define how the weight is sampled. In turn, the values for mu and sigma also become the values we train the network on.</li>
</ol>
<div class="packt_infobox">With other frameworks, building in the ability to apply variational weights to layers is quite difficult and often requires more code. Fortunately, this is one of the strengths with PyTorch and it boosts a built-in probabilistic framework designed to predict and handle distributional data.</div>
<ol start="7">
<li>Different distributions may use different descriptive values to define them. The normal or Gaussian distribution is defined by mu and sigma, while the uniform distribution is often just defined by a min/max values and the triangle would be min/max and peak value for instance. We almost always prefer to use a normal distribution for most natural events.</li>
<li>Scroll down to the training code and you will see virtually the same code as the last exercise with one key difference. Instead of epsilon for exploration, we introduce a term called beta. Beta becomes our de-facto exploration term and replaces epsilon.</li>
<li>Run the application as you normally would and observe the training in TensorBoard, as shown in the screenshot here:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-643 image-border" src="assets/79eca78e-3c2f-490c-aa79-4dede0817696.png" style="width:37.50em;height:28.67em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">TensorBoard output from sample Chapter_10_NDQN.py</div>
<p>The arrows and the direction the arrow is pointing denotes the direction in which we want our agent/algorithm to move. We get these trend plots by increasing the <strong>Smoothing</strong> parameter to the max, .99. By doing this, it is easier to see the general or median trend.</p>
<p>One thing we need to revisit before moving on to Rainbow is how exploration is managed when using noisy networks. This will also help to explain the use of that the new beta parameter for <em>z</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Noisy networks for exploration and importance sampling</h1>
                </header>
            
            <article>
                
<p class="mce-root">Using noisy networks also introduces fuzziness in our action prediction. That is, since the weights of the network are now being pulled from a distribution that also means that they equally becoming distributional. We can also say they are stochastic and that stochasticity is defined by a distribution, basically meaning that the same input could yield two completely different results, which means we can no longer take just the max or best action because that is now just fuzzy. Instead, we need a way to decrease the size of the sampling distributions we use for weights and therefore the uncertainty we have in the actions the agent selects.</p>
<div class="mce-root packt_infobox">Decreasing the size of a distribution is more or less the same as reducing the uncertainty in that data. This is a cornerstone of data science and machine learning.</div>
<p>We reduce this uncertainty by introducing a factor called beta that is increased over time. This increase is not unlike epsilon but just in reverse. Let's see how this looks in code by opening <kbd>Chapter_10_NDQN.py</kbd> <span>back up</span> <span>and follow the exercise here:</span></p>
<ol>
<li>We can see how beta is defined by looking at the main code here:</li>
</ol>
<pre style="padding-left: 60px">beta_start = 0.4<br/>beta_iterations = 50000 <br/>beta_by_iteration = lambda iteration: min(1.0, beta_start + iteration * (1.0 - beta_start) / beta_iterations)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="2">
<li>This setup and equation are again not unlike how we defined epsilon previously. The difference here is that beta increases gradually.</li>
<li>Beta is used to correct the weights being trained and hence introduces the concept of importance sampling. Importance sampling is about how much importance we have on the weights before correcting/sampling them. Beta then becomes the importance sampling factor where a value of 1.0 means 100% important and 0 means no importance.</li>
<li>Open up the <kbd>replay_buffer.py</kbd> file found in the <kbd>common</kbd> folder in the same project. Scroll down to the <kbd>sample</kbd> function and notice the code, as shown here:</li>
</ol>
<pre style="padding-left: 60px">assert beta &gt; 0<br/><br/>idxes = self._sample_proportional(batch_size)<br/><br/>weights = []<br/>p_min = self._it_min.min() / self._it_sum.sum()<br/>max_weight = (p_min * len(self._storage)) ** (-beta)<br/><br/>for idx in idxes:<br/>    p_sample = self._it_sum[idx] / self._it_sum.sum()<br/>    weight = (p_sample * len(self._storage)) ** (-beta)<br/>    weights.append(weight / max_weight)<br/>weights = np.array(weights)<br/>encoded_sample = self._encode_sample(idxes)<br/>return tuple(list(encoded_sample) + [weights, idxes])</pre>
<ol start="5">
<li>The <kbd>sample</kbd> function is part of the <kbd>PrioritizedExperienceReplay</kbd> class we are using to hold experiences. There's no need for us to review this whole class other than to realize it orders experiences in terms of priority. Sampling weights for the network based on the importance factor, <kbd>beta</kbd>.</li>
<li>Finally, jump back to the sample code and review the plot function. The line that generates our plot of beta in TensorBoard now looks like this:</li>
</ol>
<pre style="padding-left: 60px">writer.add_scalar('Train/Beta', beta_by_iteration(iteration), iteration)</pre>
<ol start="7">
<li>At this point, you can review more of the code or try and tune the new hyperparameters before continuing.</li>
</ol>
<p>That completes our look at noisy and not nosey networks for exploration. We saw how we could introduce distributions to be used as the weights for our DL network. Then, we saw how, to compensate for that, we needed to introduce a new training parameter, beta. In the next section, we see how all these pieces come together in Rainbow DQN.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Unveiling Rainbow DQN</h1>
                </header>
            
            <article>
                
<p>The author of <em>Rainbow: Combining Improvements in Deep Reinforcement Learning</em>, Matteo Hessel (<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hessel%2C+M">https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Hessel%2C+M</a>), did several comparisons against other state-of-the-art models in DRL, many of which we have already looked at. They performed these comparisons against the standard 2D classic Atari games with impressive results. Rainbow DQN outperformed all of the current <span>state-of-the-art</span> algorithms. In the paper, they used the familiar classic Atari environment. This is fine since DeepMind has a lot of data for that environment that is specific to applicable models to compare with. However, many have observed that the paper lacks a comparison between PG methods, such as PPO. Of course, PPO is an OpenAI advancement and it may have been perceived by Google DeepMind to be an infringement or just wanting to avoid acknowledgment by comparing it at all. Unfortunately, this also suggests that even a highly intellectual pursuit such as DRL cannot be removed from politics.</p>
<div class="packt_tip">Methods such as PPO have been used to beat or best some of the biggest challenges in DRL currently. PPO was in fact responsible for taking the 100 thousand dollar grand prize in the Unity Obstacle Tower Challenge. For that reason, you should not discount PG methods anytime soon.</div>
<p>Given that previous plot, we should be expecting some big things from Rainbow. So, let's open up <kbd>Chapter_10_Rainbow.py</kbd> and follow the next exercise:</p>
<ol>
<li>This example will be very familiar by now and we will limit ourselves to looking at just the differences, starting with the main implementation of the <kbd>RainbowDQN</kbd> class itself here:</li>
</ol>
<pre style="padding-left: 60px">class RainbowDQN(nn.Module):<br/>    def __init__(self, num_inputs, num_actions, num_atoms, Vmin, Vmax):<br/>        super(RainbowDQN, self).__init__()<br/>        <br/>        self.num_inputs = num_inputs<br/>        self.num_actions = num_actions<br/>        self.num_atoms = num_atoms<br/>        self.Vmin = Vmin<br/>        self.Vmax = Vmax<br/>        <br/>        self.linear1 = nn.Linear(num_inputs, 32)<br/>        self.linear2 = nn.Linear(32, 64)<br/>        <br/>        self.noisy_value1 = NoisyLinear(64, 64, use_cuda=False)<br/>        self.noisy_value2 = NoisyLinear(64, self.num_atoms, use_cuda=False)<br/>        <br/>        self.noisy_advantage1 = NoisyLinear(64, 64, use_cuda=False)<br/>        self.noisy_advantage2 = NoisyLinear(64, self.num_atoms * self.num_actions, use_cuda=False)<br/>        <br/>    def forward(self, x):<br/>        batch_size = x.size(0)<br/>        <br/>        x = F.relu(self.linear1(x))<br/>        x = F.relu(self.linear2(x))<br/>        <br/>        value = F.relu(self.noisy_value1(x))<br/>        value = self.noisy_value2(value)<br/>        <br/>        advantage = F.relu(self.noisy_advantage1(x))<br/>        advantage = self.noisy_advantage2(advantage)<br/>        <br/>        value = value.view(batch_size, 1, self.num_atoms)<br/>        advantage = advantage.view(batch_size, self.num_actions, self.num_atoms)        <br/>        x = value + advantage - advantage.mean(1, keepdim=True)<br/>        x = F.softmax(x.view(-1, self.num_atoms)).view(-1, self.num_actions, self.num_atoms)        <br/>        return x<br/>        <br/>    def reset_noise(self):<br/>        self.noisy_value1.reset_noise()<br/>        self.noisy_value2.reset_noise()<br/>        self.noisy_advantage1.reset_noise()<br/>        self.noisy_advantage2.reset_noise()<br/>    <br/>    def act(self, state):<br/>        state = autograd.Variable(torch.FloatTensor(state).unsqueeze(0), volatile=True)<br/>        dist = self.forward(state).data.cpu()<br/>        dist = dist * torch.linspace(self.Vmin, self.Vmax, self.num_atoms)<br/>        action = dist.sum(2).max(1)[1].numpy()[0]<br/>        return action</pre>
<p class="mce-root"/>
<ol start="2">
<li>The preceding code defines the network structure for the RainbowDQN. This network is a bit complicated so we have put the major elements in the diagram here:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-870 image-border" src="assets/5542d332-f67b-4fe3-9c44-113580d656d9.png" style="width:46.50em;height:27.42em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Rainbow Network Architecture</span></div>
<ol start="3">
<li>If you go over the <kbd>init</kbd> and <kbd>forward</kbd> functions, you should be able to see how this diagram was built.</li>
</ol>
<ol start="4">
<li>We can't leave the preceding code just yet and we need to review the act function again and shown here:</li>
</ol>
<pre style="padding-left: 60px">def act(self, state):<br/>        state = autograd.Variable(torch.FloatTensor(state).unsqueeze(0), volatile=True)<br/>        dist = self.forward(state).data.cpu()<br/>        dist = dist * torch.linspace(self.Vmin, self.Vmax, self.num_atoms)<br/>        action = dist.sum(2).max(1)[1].numpy()[0]<br/>        return action</pre>
<ol start="5">
<li>The <kbd>act</kbd> function shows how the agent selects an action. We have refined the action selection strategy here and now use the values for <kbd>Vmin</kbd>, <kbd>Vmax</kbd>, and <kbd>num_atoms</kbd> . We use these values as inputs into <kbd>torch.linspace</kbd> as a way to create a discrete distribution ranging in value from <kbd>Vmin</kbd> to <kbd>Vmax</kbd> and in steps defined by <kbd>num_atoms</kbd>. This outputs scaling values within the min/max ranges that are then multiplied by the original distribution, <kbd>dist</kbd>, output from the <kbd>forward</kbd> function. This multiplying a distribution returned by the <kbd>forward</kbd> function and the one generated from <kbd>torch.linspace</kbd> applies a type of scaling.</li>
</ol>
<div class="packt_tip">You may have noticed that the hyperparameters, <kbd>num_atoms</kbd>, <kbd>Vmin</kbd>, and <kbd>Vmax</kbd>, now perform dual purposes in tuning parameters in the model. This is generally a bad thing. That is, you always want the hyperparameters you define to be single-purpose.</div>
<ol start="6">
<li>Next, we will scroll down and look at the differences in the <kbd>projection_distribution</kbd> function. Remember this function is what performs the distributional part of finding the distribution rather than a discrete value:</li>
</ol>
<pre style="padding-left: 60px">def projection_distribution(next_state, rewards, dones):<br/>    batch_size = next_state.size(0)<br/>    <br/>    delta_z = float(Vmax - Vmin) / (num_atoms - 1)<br/>    support = torch.linspace(Vmin, Vmax, num_atoms)<br/>    <br/>    next_dist = target_model(next_state).data.cpu() * support<br/>    next_action = next_dist.sum(2).max(1)[1]<br/>    next_action = next_action.unsqueeze(1).unsqueeze(1).expand(next_dist.size(0), 1, next_dist.size(2))<br/>    next_dist = next_dist.gather(1, next_action).squeeze(1)<br/>        <br/>    rewards = rewards.unsqueeze(1).expand_as(next_dist)<br/>    dones = dones.unsqueeze(1).expand_as(next_dist)<br/>    support = support.unsqueeze(0).expand_as(next_dist)<br/>    <br/>    Tz = rewards + (1 - dones) * 0.99 * support<br/>    Tz = Tz.clamp(min=Vmin, max=Vmax)<br/>    b = (Tz - Vmin) / delta_z<br/>    l = b.floor().long()<br/>    u = b.ceil().long()<br/>        <br/>    offset = torch.linspace(0, (batch_size - 1) * num_atoms, batch_size).long()\<br/>                    .unsqueeze(1).expand(batch_size, num_atoms)<br/><br/>    proj_dist = torch.zeros(next_dist.size()) <br/>    proj_dist.view(-1).index_add_(0, (l + offset).view(-1), (next_dist * (u.float() - b)).view(-1))<br/>    proj_dist.view(-1).index_add_(0, (u + offset).view(-1), (next_dist * (b - l.float())).view(-1))<br/>        <br/>    return proj_dist</pre>
<ol start="7">
<li>This code is quite different than the quantile regression code we looked at previously. The primary difference here is the use of the PyTorch libraries here whereas before, the code was more low-level. Using the libraries is a bit more verbose but hopefully, you can appreciate how more explanatory the code is now compared to the previous example.</li>
<li>One thing to note here is that we continue to use <kbd>epsilon</kbd> for exploration, as the following code shows:</li>
</ol>
<pre style="padding-left: 60px">epsilon_start = 1.0<br/>epsilon_final = 0.01<br/>epsilon_decay = 50000<br/><br/>epsilon_by_frame = lambda iteration: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * iteration / epsilon_decay)</pre>
<ol start="9">
<li>Run the example as you normally would and observe the output.</li>
</ol>
<p>Keep in mind that since this example lacks a prioritized replay buffer, it fails to be a complete RainbowDQN implementation. However, it does cover the 80/20 rule and implementing a prioritized replay buffer is left as an exercise to the reader. Let the sample keep running while we jump to the next section on observing training.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">When does training fail?</h1>
                </header>
            
            <article>
                
<p>One thing that trips any newcomer to DL and certainly deep reinforcement learning is when to know whether your model is failing, is just being a bit stubborn, or is not ever going to work. It is a question that causes frustration and angst in the AI <span><span>field</span></span> and often leaves you to wonder: <em>what if I let that agent train a day longer</em>? Unfortunately, if you speak to experts, they will often say just be patient and keep training, but this perhaps builds on those frustrations. After all, what if what you built has no hope of ever doing anything—are you wasting time and energy to keep it going?</p>
<p>Another issue that many face is that the more complex an algorithm/model gets, the more time it takes to train, except you never know how long that is unless you trained it before or read a really well-written paper that uses the exact same model. Even with the same exact model, the environment may also differ perhaps being more complex as well. With all of these factors at play, as well as the pain of tuning hyperparameters, it is a wonder why anyone of sane mind would want to work in DRL at all.</p>
<div class="packt_tip">The author hosts a Deep Learning Meetup support group for RL and DRL. One of the frequent discussions in this group is how DL researchers can keep their sanity and/or reduce their stress levels. If you work in AI, you understand the constant need to live up to the hype overcoming the world. This hype is a good thing but can also be a bad thing when it involves investors or impatient bosses.</div>
<p>Fortunately, with advanced tools such as TensorBoard, we can gain insights into how are agent trains or hopes to train.</p>
<p>Open up TensorBoard and follow the next exercise to see how to effectively diagnose training problems:</p>
<ol>
<li>The output from TB is shown in the following screenshot:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-645 image-border" src="assets/eea7ff49-3611-4ef4-a3a5-4503e588c03a.png" style="width:42.92em;height:33.08em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>TensorBoard output from Rainbow DQN</span></div>
<ol start="2">
<li>From the preceding screenshot, where the <strong>Smoothing</strong> has been upped to .99, we can see training is failing. Remember, the graphs in the screenshot are annotated to show the preferred direction. For all of those plots, that is not the case. However, don't assume that if the plot is going in the opposite direction that it is necessarily bad—it isn't. Instead, any movement is a better indication of some training activity. This is also the reason we smooth these plots so highly.</li>
</ol>
<ol start="3">
<li>The one key plot that often dictates future training performance is the <strong>Losses</strong> plot. An agent will be learning when losses are decreasing and will be forgetting/confused if the losses are increasing. If losses remain constant, then the agent is stagnant and could be confused or stuck. A helpful summary of this is shown in the screenshot here:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-646 image-border" src="assets/bc442dbe-4558-4574-a52b-73049901ac6a.png" style="width:36.42em;height:31.67em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>The Losses plot summarized</span></div>
<ol start="4">
<li>The preceding screenshot shows the ideal training over 500 thousand episodes and for this environment, you can expect to train double or triple that amount. As a general rule, it is best to consider no movement, positive or negative, over 10% or the training time to be a failure. For example, if you are training an agent for 1 million iterations, then your 10% window would be about 100 thousand iterations. If your agent is training constantly or flat-lining in any plot, aside from <span class="packt_screen">Advantage</span>, over a period equal to or larger than the 10% window size, it may be best to tune hyperparameters and start again.</li>
</ol>
<ol start="5">
<li>Again, pay special attention to the <span class="packt_screen">Losses</span> plot as this provides the strongest indicator for training problems.</li>
<li>You can run view the results of multiple training efforts side by side by just running the sample repeatedly for the same number of iterations, as the screenshot here shows:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-647 image-border" src="assets/0cbc852e-2d96-4c24-ae03-0a8cd81e8af4.png" style="width:39.75em;height:32.67em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Examples of multiple training outputs on the same graph</span></div>
<ol start="7">
<li>Stop the current sample change some hyperparameters and run it again to see the preceding example.</li>
</ol>
<p>These simple rules will hopefully help you to avoid frustrations on building/training your own models on new or different environments. Fortunately, we have several more chapters to work through and that includes plenty of more exercises like those featured in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exercises</h1>
                </header>
            
            <article>
                
<p>When it comes to working in the real world, the experience you build from doing these exercises may mean the difference between not getting that job and certainly keeping it. As a programmer, you don't have the luxury of just understanding how something works; you're a mechanic/engineer that needs to get their hands dirty and actually do the work:</p>
<ol>
<li>Tune the hyperparameters for <kbd>Chapter_10_QRDQN.py</kbd> and see what effect this has on training.</li>
<li>Tune the hyperparameters for <kbd>Chapter_10_NDQN.py</kbd> and see what effect this has on training.</li>
<li>Tune the hyperparameters for <kbd>Chapter_10_Rainbow.py</kbd> and see what effect this has on training.</li>
<li>Run and tune the hyperparameters for any of this chapter's samples on another environment such as CartPole or FrozenLake or something more complex such as Atari. Reducing the complexity of an environment is also helpful if your computer is older and needs to work harder training agents.</li>
<li>This chapter also includes sample code for Hierarchical DQNs and Categorical DQNs in the <kbd>Chapter_10_HDQN.py</kbd> and <kbd>Chapter_10_C51.py</kbd> samples. Run these examples, review the code, and do some investigation on your own on what improvements these samples bring to DRL.</li>
<li>Add the ability to save/load the trained model from any of the examples. Can you now use the trained model to show the agent playing the game?</li>
<li>Add the ability to output other training values to TensorBoard that you may think are important to training an agent.</li>
<li>Add <kbd>NoisyLinear</kbd> layers to the <kbd>Chapter_10_QRDQN.py</kbd> example. There may already be code that is just commented out in the example.</li>
<li>Add a prioritized replay buffer to the <kbd>Chapter_10_Rainbow.py</kbd> example. You can use the same method found in the <kbd>Chapter_10_NDQN.py</kbd> example.</li>
<li>TensorBoard allows you to output and visualize a trained model. Use TensorBoard to output the trained model from one of the examples.</li>
</ol>
<p>Obviously, the number of exercises has increased to reflect your increasing skill level and/or interest in DRL. You certainly don't need to complete all of these exercises but 2-3 will go a long way. In the next section, we will summarize the chapter.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we looked specifically at one of the more <span>state-of-the-art</span> advances in DRL from DeepMind called Rainbow DQN. Rainbow combines several improvements layered on top of DQN that allow dramatic increases in training performance. As we have already covered many of these improvements, we only needed to review a couple of new advances. Before doing that though, we installed TensorBoard as a tool to investigate training performance. Then, we looked at the first advancement in distributional RL and how to model the action by understanding the sampling distribution. Continuing with distributions, we then looked at noisy network layers—network layers that don't have individual weights but rather individual distributions to describe each weight. Building on this example, we moved onto Rainbow DQN with our last example, finishing off with a quick discussion on when to determine whether an agent is not trainable or flat-lining.</p>
<p>For the next chapter, we will move from building DRL algorithms/agents to building environments with Unity and constructing agents in those environments with the ML-Agents toolkit.</p>


            </article>

            
        </section>
    </body></html>