["```py\n    import spacy\n    nlp = spacy.load(\"en_core_web_md\")\n    ```", "```py\n    doc = nlp(\"I went there\")\n    ```", "```py\nUSA\nN.Y.\nCity\n33\n3rd\n!\n…\n?\n's\n```", "```py\n    import spacy\n    nlp = spacy.load(\"en_core_web_md\")\n    ```", "```py\n    doc = nlp(\"I own a ginger cat.\")\n    print([token.text for token in doc])\n    >>> ['I', 'own', 'a', 'ginger', 'cat', '.']\n    ```", "```py\nimport spacy\nnlp = spacy.load(\"en_core_web_md\")\ndoc = nlp(\"It's been a crazy week!!!\")\nprint([token.text for token in doc])\n>>> ['It', \"'s\", 'been', 'a', 'crazy', 'week', '!', '!', '!']\n```", "```py\n    import spacy\n    from spacy.symbols import ORTH\n    ```", "```py\n    nlp = spacy.load(\"en_core_web_sm\")\n    doc = nlp(\"lemme that\")\n    print([w.text for w in doc])\n    >>> ['lemme', 'that']\n    ```", "```py\n    special_case = [{ORTH: \"lem\"}, {ORTH: \"me\"}]\n    nlp.tokenizer.add_special_case(\"lemme\", special_case)\n    print([w.text for w in nlp(\"lemme that\")])\n    >>> ['lem', 'me', 'that']\n    ```", "```py\nprint([w.text for w in nlp(\"lemme!\")])\n>>> ['lem', 'me', '!']\n```", "```py\n    import spacy\n    nlp = spacy.load(\"en_core_web_sm\")\n    text = \"Let's go!\"\n    doc = nlp(text)\n    ```", "```py\n    tok_exp = nlp.tokenizer.explain(text)\n    for t in tok_exp:\n        print(t[1], \"\\t\", t[0])\n    >>> Let  SPECIAL-1\n    's  SPECIAL-2\n    go  TOKEN\n    ! SUFFIX\n    ```", "```py\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\ntext = \"I flied to N.Y yesterday. It was around 5 pm.\"\ndoc = nlp(text)\nfor sent in doc.sents:\n    print(sent.text)\n>>> I flied to N.Y yesterday.\nIt was around 5 pm.\n```", "```py\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"I went there for working and worked for 3 years.\")\nfor token in doc:\n    print(token.text, token.lemma_)\n>>> I I\nwent go\nthere there\nfor for\nworking work\nand and\nworked work\nfor for\n3 3\nyears year\n. .\n```", "```py\nList me all flights to Atlanta.\nI need a flight to NY.\nI flew to Atlanta yesterday evening and forgot my baggage.\n```", "```py\n    import spacy\n    nlp = spacy.load(\"en_core_web_sm\")\n    nlp.get_pipe(\"attribute_ruler\").add([[{\n        \"TEXT\": \"Angeltown\"}]], {\"LEMMA\": \"Los Angeles\"})\n    ```", "```py\n    doc = nlp(\"I am flying to Angeltown\")\n    for token in doc:\n        print(token.text, token.lemma_)\n    >>> I I\n    am be\n    flying fly\n    to to\n    Angeltown Los Angeles\n    ```", "```py\ndoc = nlp(\"I like cats.\")\nprint(doc.text)\n>>> I like cats.\n```", "```py\nfor token in doc:\n    print(token.text)\n>>> I\nlike\ncats\n.\n```", "```py\nprint(doc[1])\n>>> like\n```", "```py\nprint(len(doc))\n>>> 4\n```", "```py\ndoc = nlp(\"This is a sentence. This is the second sentence\")\nsentences = list(doc.sents)\nprint(sentences)\n>>> [This is a sentence., This is the second sentence]\n```", "```py\ndoc = nlp(\"I flied to New York with Ashley.\")\nprint(doc.ents)\n>>> (New York, Ashley)\n```", "```py\ndoc = nlp(\"Sweet brown fox jumped over the fence.\")\nprint(list(doc.noun_chunks))\n>>> [Sweet brown fox, the fence]\n```", "```py\nprint(doc.lang_)\n>>> en\n```", "```py\nfrom pprint import pprint\ndoc = nlp(\"Hi\")\njson_doc = doc.to_json()\npprint(json_doc)\n>>> {'ents': [],\n'sents': [{'end': 2, 'start': 0}],\n'text': 'Hi',\n'tokens': [{'dep': 'ROOT',\n            'end': 2,\n            'head': 0,\n            'id': 0,\n            'lemma': 'hi',\n            'morph': '',\n            'pos': 'INTJ',\n            'start': 0,\n            'tag': 'UH'}]}\n```", "```py\ndoc = nlp(\"Hello Madam!\")\nprint(doc[0])\n>>> Hello\n```", "```py\nprint(doc[0].text_with_ws)\nprint(doc[2].text_with_ws)\n>>> 'Hello '\n'!'\n```", "```py\nprint(len(doc[0]))\n>>> 5\n```", "```py\ntoken = doc[2]\nprint(token.i)\n>>> 2\n```", "```py\nprint(doc[0].idx)\nprint(doc[1].idx)\n>>> 0\n6\n```", "```py\ntoken = doc[0]\nprint(token.doc)\n>>> Hello Madam!\n```", "```py\ntoken = doc[1]\nprint(token.sent)\n>>> Hello Madam!\n```", "```py\ndoc = nlp(\"He entered the room. Then he nodded.\")\nprint(doc[0].is_sent_start)\n>>> True\nprint(doc[5].is_sent_start)\n>>> True\nprint(doc[6].is_sent_start)\n>>> False\n```", "```py\ndoc = nlp(\"I went there.\")\nprint(doc[1].lemma_)\n>>> go\n```", "```py\ndoc = nlp(\"The Brazilian president visited Beijing\")\nprint(doc.ents)\n>>> (Brazilian, Beijing)\nprint(doc[1].ent_type_, spacy.explain(doc[1].ent_type_))\n>>> NORP Nationalities or religious or political groups\n```", "```py\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"All my moves are coldly calculated.\")\nprint(doc[4:6])\n>>> coldly calculated\n```", "```py\ndoc = nlp(\"All my moves are coldly calculated.\")\nprint(doc[5:]) # end index empty means rest of the string calculated.\nprint(doc[4:-1]) # minus indexes are supported\n>>> calculated.\ncoldly calculated\n```", "```py\ndoc = nlp(\"Recife has a thousand charms, it's a little piece of Brazil.\")\nprint(doc.char_span(37, 59))\n>>> little piece of Brazil\n```", "```py\ndoc = nlp(\"You went there after you saw me\")\nspan = doc[2:4]\nfor token in span:\n    print(token)\n>>> there\nafter\n```", "```py\ndoc = nlp(\"Hi Lorena!!\")\nspan = doc[:2]\nlen(span)\n>>> 2\n```", "```py\ndoc = nlp(\"You went there after you saw me\")\nspan = doc[2:6]\nprint(span)\n>>> there after you saw\nprint(span[1:3])\n>>> after you\n```", "```py\ndoc = nlp(\"You went there after you saw me\")\nspan = doc[2:6]\nprint(span.doc)\n>>> You went there after you saw me\nprint(span.sent)\n>>> You went there after you saw me\n```", "```py\ndoc = nlp(\"You went there after you saw me\")\nspan = doc[2:6]\nprint(span.start) # index of the first token of the Span\n>>> 2\nprint(span.end)\n>>> 6\nprint(span.start_char) # start offset of the Span at character level\n>>> 9\nprint(span.end_char)\n>>> 28\n```", "```py\ndoc = nlp(\"You went there after you saw me\")\nspan = doc[2:6]\nsmall_doc = span.as_doc()\nprint(type(small_doc))\n>>> <class 'spacy.tokens.doc.Doc'>\n```", "```py\ndoc = nlp(\"Hello, hi!\")\nprint(doc[0].lower_)\n>>> hello\n```", "```py\ndoc = nlp(\"Cat and Cat123\")\nprint(doc[0].is_alpha)\n>>> True\nprint(doc[2].is_alpha)\n>>>False\n```", "```py\ndoc = nlp(\"UR7 and Várzea\")\nprint(doc[0].is_ascii)\n>>>True\nprint(doc[2].is_ascii)\n>>>False\n```", "```py\ndoc = nlp(\"Cat Cat123 123\")\nprint(doc[0].is_digit)\n>>>False\nprint(doc[1].is_digit)\n>>>False\nprint(doc[2].is_digit)\n>>> True\n```", "```py\ndoc = nlp(\"You, him and Sally\")\nprint(doc[1])\n>>>,\nprint(doc[1].is_punct)\n>>>True\n```", "```py\ndoc = nlp(\"( [ He said yes. ] )\")\nprint(doc[0])\n>>> (\nprint(doc[0].is_left_punct)\n>>>True\nprint(doc[-2])\n>>>]\nprint(doc[-2].is_right_punct)\n>>>True\n```", "```py\ndoc = nlp(\"I emailed you at least 100 times\")\nprint(doc[-2])\n>>>100\nprint(doc[-2].like_num)\n>>>True\ndoc = nlp(\"I emailed you at least hundred times\")\nprint(doc[-2])\n>>>hundred\nprint(doc[-2].like_num)\n>>>True\ndoc = nlp(\"His email is hello@hello.com and his website is https://nicewebsite.com\")\nprint(doc[3])\n>>>hello@hellp.com\nprint(doc[3].like_email)\n>>>True\nprint(doc[8])\n>>>https://nicewebsite.com\nprint(doc[8].like_url)\n>>>True\n```", "```py\ndoc = nlp(\"Girl called Kathy has a nickname Cat123.\")\nfor token in doc:\n    print(token.text, token.shape_)\n>>>Girl Xxxx\ncalled xxxx\nKathy Xxxxx\nhas xxx\na x\nnickname xxxx\nCat123 Xxxddd\n. .\n```", "```py\ndoc = nlp(\"One step forward, and you're no longer in the same place.\")\nfor token in doc:\n    print(token, token.is_stop)\n>>>One True\nstep False\nforward False\n, False\nand True\nyou True\n're True\nno True\nlonger False\nin True\nthe True\nsame True\nplace False\n. False\n```"]