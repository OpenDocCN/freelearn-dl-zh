<html><head></head><body>
		<div><h1 id="_idParaDest-91" class="chapter-number"><a id="_idTextAnchor090"/>5</h1>
			<h1 id="_idParaDest-92"><a id="_idTextAnchor091"/>Extracting Faces</h1>
			<p>In this chapter, we will start our hands-on activities with the code. To begin with, we’ll cover the process of extracting faces.</p>
			<p>Extracting faces is a series of steps in a process that involves many different stages, but it’s the first discrete stage in creating a deepfake. This chapter will first talk about how to run the face extraction script, then we will go hands-on with the code, explaining what each part does.</p>
			<p>In this chapter, we will cover the following key sections:</p>
			<ul>
				<li>Getting image files from a video</li>
				<li>Running extract on frame images</li>
				<li>Getting hands-on with the code</li>
			</ul>
			<h1 id="_idParaDest-93"><a id="_idTextAnchor092"/>Technical requirements</h1>
			<p>To proceed, we recommend that you download the code from the GitHub repo at <a href="https://github.com/PacktPublishing/Exploring-Deepfakes">https://github.com/PacktPublishing/Exploring-Deepfakes</a> and follow the instructions in the <code>readme.md</code> file to install Anaconda and to create a virtual environment with all required libraries to run the scripts. The repo will also contain any errata or updates that have happened since the book was published, so please check there for any updates.</p>
			<h1 id="_idParaDest-94"><a id="_idTextAnchor093"/>Getting image files from a video</h1>
			<p>Videos are not <a id="_idIndexMarker226"/>designed for frame-by-frame access and can cause <a id="_idIndexMarker227"/>problems when processed out of order. Accessing a video file is a complicated process and not good for a beginner-level chapter like this. For this reason, the first task is to convert any videos that you want to extract from into <a id="_idIndexMarker228"/>individual frames. The best way to do this is to use <strong class="bold">FFmpeg</strong>. If you followed the installation instructions in the <em class="italic">Technical requirements</em> section, you will have FFmpeg installed and ready to use.</p>
			<p>Let’s begin the process.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">When you see code or command examples such as those present here with text inside of curly brackets, you should replace that text with the information that is explained in the brackets. For example, if it says <code>cd {Folder with video}</code> and the video is in the <code>c:\Videos\</code> folder, then you should enter <code>cd c:\Videos</code>.</p>
			<p>Place the video into a folder, then open an Anaconda prompt and enter the following commands:</p>
			<pre class="console">
cd {Folder with video}
mkdir frames
ffmpeg -i {Video Filename} frames\video_frame_%05d.png</pre>
			<p>This will fill the <code>frames</code> folder with numbered images containing the exported frames.</p>
			<p>There are a lot of <a id="_idIndexMarker229"/>options you can control with FFmpeg, from the <a id="_idIndexMarker230"/>resolution of the output image to the number of frames to be skipped. These features are beyond the scope of this book but have been covered extensively elsewhere. We advise searching for a guide to ffmpeg’s command line options if you’re going to do much more than the basics.</p>
			<h1 id="_idParaDest-95"><a id="_idTextAnchor094"/>Running extract on frame images</h1>
			<p>To run the <a id="_idIndexMarker231"/>extract process on a video, you can run the extract program from the cloned <code>git</code> repository folder. To do this, simply run the following in an Anaconda prompt:</p>
			<pre class="console">
cd {Folder of the downloaded git repo}\
python C5-face_detection.py {Folder of frame images}</pre>
			<p>This will run the face extraction process on each of the images in the folder and put the extracted images into the <code>face_images</code> folder, which (by default) will be inside the folder of frame images. This folder will contain three types of files for each face detected, along <a id="_idIndexMarker232"/>with a file containing all alignments.</p>
			<h2 id="_idParaDest-96"><a id="_idTextAnchor095"/>face_alignments.json</h2>
			<p>There will just be one of these files. It is a JSON-formatted file containing the landmark positions and <a id="_idIndexMarker233"/>warp matrix for every face found in the images. This file is human-readable like any JSON file and can be read or edited (though it’s probably not something that you’d do manually).</p>
			<h3>face_landmarks_{filename}_{face number}.png</h3>
			<p>This is a copy of <a id="_idIndexMarker234"/>the original image with a bounding box drawn around the face and five <strong class="bold">landmark</strong> points written out. Landmarks are common points on the face. We’ll cover the usage of these landmark points later, but the ones we’re most interested in are the eyes, nose, and corners of the mouth.</p>
			<div><div><img src="img/B17535_05_001.jpg" alt="Figure 5.1 – Example of face_landmarks_bryan_0.png"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.1 – Example of face_landmarks_bryan_0.png</p>
			<p class="callout-heading">Author’s note</p>
			<p class="callout">All of the images of people used for practical demonstration in this section are of the authors.</p>
			<h2 id="_idParaDest-97"><a id="_idTextAnchor096"/>face_bbox_{filename}_{face number}.png</h2>
			<p>This image represents the original <strong class="bold">bounding box</strong> (the smallest box that surrounds the detected face) found <a id="_idIndexMarker235"/>in the original image. It will be in the full original size and angle of the face found in the image.</p>
			<div><div><img src="img/B17535_05_002.jpg" alt="Figure 5.2 – Example of face_bbox_bryan_0.png"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.2 – Example of face_bbox_bryan_0.png</p>
			<h2 id="_idParaDest-98"><a id="_idTextAnchor097"/>face_aligned_{filename}_{face number}.png</h2>
			<p>This image will be a smaller size (the default is 256x256) image of the face. This is an <strong class="bold">aligned</strong> face image, where the face has been lined up according to the landmarks. This image should <a id="_idIndexMarker236"/>generally have the face centered in the image and lined up vertically. If the face in the bounding box image was crooked or angled in the box, it should be straightened in the aligned image. There may also be a black cutout where the edge of the original frame cuts off the data.</p>
			<p>This image is the most important image and is the one that will be used for training the model. It is critical for quality training that the aligned face is a good-quality image. This is the data that you’ll want to clean to get a successful deepfake.</p>
			<div><div><img src="img/B17535_05_003.jpg" alt="Figure 5.3 – Example of face_aligned_bryan_0.png"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.3 – Example of face_aligned_bryan_0.png</p>
			<h2 id="_idParaDest-99"><a id="_idTextAnchor098"/>face_mask_{filename}_{face number}.png</h2>
			<p>This image <a id="_idIndexMarker237"/>matches the size of the aligned image. In fact, it matches the aligned image in the crop, rotation, and size. The mask is an AI-predicted outline of the face that will be used later to ensure that the face is trained properly and help swap the final face back onto the image.</p>
			<div><div><img src="img/B17535_05_004.jpg" alt="Figure 5.4 – Example of face_mask_bryan_0.png"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.4 – Example of face_mask_bryan_0.png</p>
			<p>The mask <a id="_idIndexMarker238"/>should line up with the aligned face perfectly, showing where the face and edges are. You can see how the mask overlays onto the face in <em class="italic">Figure 5</em><em class="italic">.5</em>.</p>
			<div><div><img src="img/B17535_05_005.jpg" alt="Figure 5.5 – Example of mask overlayed on aligned face"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.5 – Example of mask overlayed on aligned face</p>
			<p>So, now that you <a id="_idIndexMarker239"/>know how to run the face detector and all of its output, let’s get into the hands-on section and determine exactly what it’<a id="_idTextAnchor099"/>s doing.</p>
			<h1 id="_idParaDest-100"><a id="_idTextAnchor100"/>Getting hands-on with the code</h1>
			<p>Now it’s time to get into the code. We’ll go over exactly what <code>C5-face_detection.py</code> does and why each option was chosen. There are five main parts to the code: initialization, image preparation, face detection, face landmarking/aligning, and masking.</p>
			<h2 id="_idParaDest-101"><a id="_idTextAnchor101"/>Initialization</h2>
			<p>Let’s begin.</p>
			<p class="callout-heading">Author’s note</p>
			<p class="callout">Formatting for easy reading in a book requires modifying the spacing in the samples. Python, however, is whitespace sensitive and uses spacing as a part of the language syntax. This means that copying code from this book will almost definitely contain the wrong spacing. For this reason, we highly recommend pulling the code from the Git repository for the book at <a href="https://github.com/PacktPublishing/Exploring-Deepfakes">https://github.com/PacktPublishing/Exploring-Deepfakes</a> if you plan on running it.</p>
			<ol>
				<li>First, we <a id="_idIndexMarker240"/>import all required libraries:<pre class="source-code">
import os
import torch
import cv2
import json_tricks
import numpy as np
from tqdm import tqdm
from argparse import ArgumentParser
from face_alignment.detection.sfd import FaceDetector
from face_alignment import FaceAlignment, LandmarksType
from skimage.transform._geometric import _umeyama as umeyama
from lib.bisenet import BiSeNet</pre></li>
			</ol>
			<p>In this section, we’re importing all the important libraries and functions that we will use. These are all used in the code and will be explained when they’re used, but it’s a good idea to get familiar with the imports for any project since it lets you understand the functions being used and where they came from.</p>
			<p>Many of these imports are from the Python standard library, and if you’re interested in <a id="_idIndexMarker241"/>reading more about them, you can find their documentation online. We’ll be explaining those that aren’t part of the standard library as we come to them, including where you can find documentation on them.</p>
			<ol>
				<li value="2">Next, let’s skip to the end of the code for a moment, where we’ll look at argument parsing:<pre class="source-code">
if __name__ == "__main__":
""" Process images in a directory into aligned face images
Example CLI:
------------
python face_detection.py "C:/media_files/"
"""
parser = ArgumentParser()
parser.add_argument("path",
  help="folder of images to run detection on")
parser.add_argument("--cpu", action="store_true",
  help="Force CPU usage")
parser.add_argument("--size", default=256,
  help="height and width to save the face images")
parser.add_argument("--max_detection_size", default=1024,
  help="Maximum size of an image to run detection on.
    (If you get memory errors, reduce this size)")
parser.add_argument("--jpg", action="store_true",
  help="use JPG instead of PNG for image saving
    (not recommended due to artifacts in JPG images)")
parser.add_argument("--min_size", default=.1,
  help="Minimum size of the face relative to image")
parser.add_argument("--min_confidence", default=.9,
  help="Minimum confidence for the face detection")
parser.add_argument("--export-path",
  default="$path/face_images",
  help="output folder (replaces $path with the input)"
opt = parser.parse_args()
opt.export_path = opt.export_path.replace('$path',opt.path)
main(opt)</pre></li>
			</ol>
			<p>In this section, we <a id="_idIndexMarker242"/>define the options available in the script. It lets you change many options without needing to modify the code at all. Most of these will be fine with the defaults, but the argument parser included in the Python standard libraries gives us an easy-to-use method to make those changes at runtime. The documents and guides for the argument parser are part of the standard library, so we’ll skip over the basics, except to state that we use <code>parser.add_argument</code> for each argument that we want, and all the arguments get put into the <code>opt</code> variable.</p>
			<p>One special <a id="_idIndexMarker243"/>thing we’re doing here is to change the <code>export_path</code> variable after it is defined, replacing the <code>$path</code> string in the variable to put the output folder into the input folder. If the user overrides the default, there will be nothing to replace (or the user could use <code>$path</code> to specify a different subfolder in the input folder if desired).</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Python norms require that this section is at the end of the file to work. However, it’s very important to at least look at this section so that you know what it is doing before we go through the rest of the code.</p>
			<ol>
				<li value="3">Next, we return to the following code:<pre class="source-code">
def main(opt):</pre></li>
			</ol>
			<p>This section begins with the main function we’re using to do all the actual work.</p>
			<ol>
				<li value="4">The first thing we do is make sure the output folder exists:<pre class="source-code">
if not os.path.exists(opt.export_path):
  os.mkdir(opt.export_path)</pre></li>
			</ol>
			<p>This creates the output folder if it doesn’t already exist.</p>
			<ol>
				<li value="5">Then <a id="_idIndexMarker244"/>we enable <strong class="bold">graphics processing unit</strong> (<strong class="bold">GPU</strong>) acceleration if it’s available:<pre class="source-code">
device = "cuda" if torch.cuda.is_available() and not opt.cpu else "cpu"</pre></li>
			</ol>
			<p>In this part, we begin with the <code>PyTorch</code> for CUDA availability and save the device variable for later use.</p>
			<ol>
				<li value="6">Next, we’ll initialize the face detector and aligner:<pre class="source-code">
face_detector = FaceDetector(device=device, verbose=False)
face_aligner = FaceAlignment(LandmarksType._2D, device=device, verbose=False)</pre></li>
			</ol>
			<p>This code defines the face detector and aligner that we’ll be using; it specifies what devices we’re going to use and tells the aligner to use a model trained for <code>2D</code> face landmarks. These classes come from the <code>face_alignment</code> library available at <a href="https://github.com/1adrianb/face-alignment">https://github.com/1adrianb/face-alignment</a>. Both the detector and <a id="_idIndexMarker248"/>the aligner are AI models that have been trained for their specific uses. The detector finds any faces in a given image, returning their position through the use of a bounding box. The aligner takes those detected faces and finds landmarks that we can use to align the face to a known position.</p>
			<ol>
				<li value="7">Next, we define our masker and prepare it. We load the pre-trained weights and set it to use CUDA if NVIDIA support is enabled:<pre class="source-code">
masker = BiSeNet(n_classes=19)
if device == "cuda":
  masker.cuda()
model_path = os.path.join(".", "binaries",
  "BiSeNet.pth")
masker.load_state_dict(torch.load(model_path))
masker.eval()
desired_segments = [1, 2, 3, 4, 5, 6, 10, 12, 13]</pre></li>
			</ol>
			<p>We use the <code>desired_segments</code> variable to define which of the masker segments we want to use. In our current masker, this gives us the face itself and discards background, hair, and clothing so that our model will only need to learn the information that we want to swap.</p>
			<ol>
				<li value="8">Lastly, we get a list of files in the input folder:<pre class="source-code">
alignment_data = {}
list_of_images_in_dir = [ file for file in os.listdir(opt.path) if os.path.isfile(os.path.join(opt.path,file)) ]</pre></li>
			</ol>
			<p>This code first <a id="_idIndexMarker249"/>prepares an empty dictionary to store the alignment data. This dictionary will store all the data that we want to store and save with the face data we’ll use to convert with later.</p>
			<p>Next, it will get a list of all files in the folder. This assumes that each file is an image that we want to import any faces from. If there are any non-image files, it will fail, so it’s important to keep extra files out of the folder.</p>
			<p>Next, files in the directory are listed and checked to ensure they’re files before being stored in a list.</p>
			<h2 id="_idParaDest-102"><a id="_idTextAnchor102"/>Image preparation</h2>
			<p>The image preparation <a id="_idIndexMarker250"/>is the next step.</p>
			<ol>
				<li>This loads the images and gets them ready to be processed by the rest of the tools:<pre class="source-code">
for file in tqdm(list_of_images_in_dir):
  filename, extension = os.path.splitext(file)</pre></li>
			</ol>
			<p>This is the start of a loop that will go over every file in the directory to process them. The <code>tqdm</code> library creates a nice readable status bar, including predictions for how long the process will take. This one line is enough to get the basics, but there are a lot more features that it can provide. You can see the full documentation at <a href="https://github.com/tqdm/tqdm#documentation">https://github.com/tqdm/tqdm#documentation</a>.</p>
			<ol>
				<li value="2">Next, we load the image and convert it to RGB color order:<pre class="source-code">
image_bgr = cv2.imread(os.path.join(opt.path,file))
image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)
height, width, channels = image_rgb.shape</pre></li>
			</ol>
			<p>OpenCV is a library of tools for operating on images. You can find documentation for it at <a href="https://docs.opencv.org/">https://docs.opencv.org/</a>. In this code, we use it to open images and handle various image tasks, which we’ll explain as they come up.</p>
			<p>OpenCV loads the images in a <strong class="bold">blue, green, red</strong> (<strong class="bold">BGR</strong>) color order, which is unusual, so we must convert it to <strong class="bold">red, green, blue</strong> (<strong class="bold">RGB</strong>) color order for later processing since the other libraries expect the files in that color order. If we don’t convert it, all the colors will be off, and many tools will provide the wrong results.</p>
			<p>Then we get <a id="_idIndexMarker251"/>the image’s shape; this will give us the height and width of the image, as well as the number of color channels (this will be three since we loaded them as color images).</p>
			<ol>
				<li value="3">Next, we need to check whether we need to resize the image:<pre class="source-code">
adjustment = opt.max_detection_size / max(height, width)
if adjustment &lt; 1.0:
  resized_image = cv2.resize(image_rgb, None, fx=adjustment, fy=adjustment)
else:
  resized_image = image_rgb
  adjustment = 1.0</pre></li>
			</ol>
			<p>When you’re running images through AI models, the image’s dimensions can drastically change the amount of memory used. This can cause an error if it exceeds the memory available for AI to use. In this code, we’re getting a maximum size from the options, then finding what adjustment we need to make to resize the image down to the maximum size and keep <a id="_idIndexMarker252"/>track of that change so that the results from the face detector can be restored to work on the original image. This process allows us to use a smaller image for the face detection AI while still using the face from the full-sized image, giving us the best resolution and details.</p>
			<h2 id="_idParaDest-103"><a id="_idTextAnchor103"/>Face detection</h2>
			<p>Now it’s time to detect the <a id="_idIndexMarker253"/>faces within the image. This process goes through each image to detect any faces.</p>
			<ol>
				<li>First, we check the resized image for any faces:<pre class="source-code">
faces = face_detector.detect_from_image(resized_image)</pre></li>
			</ol>
			<p>This runs the face detector to find any faces in the image. The library makes this a very easy process. We send the resized image through and get back a list of all the faces found as well as their bounding boxes. These boxes are all relative to the smaller, resized image to ensure we have enough memory to handle it.</p>
			<ol>
				<li value="2">Next, we’ll iterate over each face:<pre class="source-code">
for idx, face in enumerate(faces):
  top,left,bottom,right = (face[0:4] /
    adjustment).astype(int)
  confidence = face[4]
  if confidence &lt; opt.min_confidence:
    Continue</pre></li>
			</ol>
			<p>As input into the loop we use the built-in Python function <code>enumerate</code>, which gives us a count of each of the faces as it finds them. We use this number later to identify the face by number.Then, we break out the bounding box size and divide it by the adjustment size. This restores the face bounding box that was detected to match the original image instead of the smaller resized image. We store the adjusted face detection in variables that we round to integers so we can keep it lined up with the individual pixels.</p>
			<p>Next, the confidence level of the face detection AI is used to skip any faces that fall below the confidence level, which is set up in the options.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">The confidence level from the face detection AI varies from <code>0</code> (no face) to <code>1</code> (absolute certainty of a face). Most AI uses the range of <code>0</code> to <code>1</code> since it’s easier for AI to work with a known range. If you are doing AI work and get results you don’t expect, you might want to check whether it’s been restricted down to the range of <code>0</code> to <code>1</code>.</p>
			<ol>
				<li value="3">Next, we make <a id="_idIndexMarker254"/>sure the face is big enough to use:<pre class="source-code">
face_height = bottom - top
face_width = right – left
face_size = face_height * face_width
if face_size/(height*width) &lt; opt.min_size:
  continue</pre></li>
			</ol>
			<p>This code finds the face height and width, then uses that to find the overall size of the face, and compares it against a minimum face size, skipping any faces that are too small. It uses the original size of the frame to filter out faces that aren’t the main focus more easily. This is set low but can be useful if you have lots of faces in a single image, such as in a crowd scene.</p>
			<ol>
				<li value="4">Next, we write out the bounding box as an image:<pre class="source-code">
detected_face = image_bgr[y1:y2,x1:x2]
cv2.imwrite(os.path.join( opt.export_path,
              f"face_bbox_{filename}_{fn}.png"),
            detected_face)</pre></li>
			</ol>
			<p>This code creates an image containing just the parts of the image that fit inside the face bounding box of the face and saves it as a <code>.png</code> file into the output folder. This lets <a id="_idIndexMarker255"/>you see the face that is detected; however, it’s not actually needed for any future step, so it can be removed if you don’t want to save this data.</p>
			<h2 id="_idParaDest-104"><a id="_idTextAnchor104"/>Face landmarking/aligning</h2>
			<p>The next step is to <a id="_idIndexMarker256"/>detect landmarks of the face and align them to a known position.</p>
			<ol>
				<li>First, we will get alignments:<pre class="source-code">
landmarks = face_aligner.get_landmarks_from_image(
  image_rgb, detected_faces = [face[0:4]/adjustment])</pre></li>
			</ol>
			<p>Here, we use the same library we used for face detection, passing the full image and the adjusted bounding boxes in order to get landmarks for the face. The library returns 68 landmark positions, which are based on specific points on the face.</p>
			<ol>
				<li value="2">Next, we draw a box:<pre class="source-code">
landmark_image = image_bgr.copy()
landmark_image = cv2.rectangle(landmark_image,
  (top, left), (bottom, right), thickness=10,
  color=(0, 0, 0))</pre></li>
			</ol>
			<p>Here, we generate a new copy of the original image in BGR color format so we can draw onto the image without damaging our original copy. We then use OpenCV to draw a rectangle for the detected face. Its <code>thickness</code> is set to <code>10</code> pixels and drawn in black.</p>
			<ol>
				<li value="3">Next, we create the landmarks we’re going to use for alignment:<pre class="source-code">
right_eye = np.mean(landmarks[0][36:42],axis=0)
left_eye = np.mean(landmarks[0][42:48],axis=0)
nose_tip = landmarks[0][30]
right_mouth = landmarks[0][48]
left_mouth = landmarks[0][54]
limited_landmarks = np.stack(
  (right_eye,
   left_eye,
   nose_tip,
   right_mouth,
   left_mouth))</pre></li>
			</ol>
			<p>In this code, we split the 68 landmarks down to just 5. We use the average of the landmarks <a id="_idIndexMarker257"/>around each eye to find the average eye position, and then get the tip of the nose and the corners of the mouth and save them into a new array. This reduced set of landmarks helps to keep alignment consistent since the 68 landmarks contain a lot of noisy edge points.</p>
			<ol>
				<li value="4">Next, we will draw the landmarks onto the image:<pre class="source-code">
landmark_image = cv2.rectangle(landmark_image,
  (x1,y1), (x2,y2), thickness=10, color=(0,0,0))
colors = [[255,0,0], # Blue
          [0,255,0], # Green
          [0,0,255], # Red
          [255,255,0], # Cyan
          [0,255,255]] # Yellow
for count, landmark in enumerate(limited_landmarks):
  landmark_adjusted = landmark.astype(int)
  landmark_image = cv2.circle(landmark_image,  tuple(landmark_adjusted), radius=10,
  thickness=-1, color=colors[count])
cv2.imwrite(os.path.join(opt.export_path,
              f"face_landmarks_{filename}_{idx}.png",
            landmark_image)</pre></li>
			</ol>
			<p>Here, we define a set of colors, then use those to draw individual dots for each landmark position. We save them as a <code>.png</code> image file in the output folder. These images are for demonstration and debugging purposes and aren’t ever used later, so you can remove them (and this save call) if you don’t need those debug images.</p>
			<ol>
				<li value="5">Next, we define the mean face:<pre class="source-code">
MEAN_FACE = np.array([[0.25, 0.22],
                      [0.75, 0.22],
                      [0.50, 0.51],
                      [0.26, 0.78],
                      [0.74, 0.78]])</pre></li>
			</ol>
			<p>Here, we <a id="_idIndexMarker258"/>define another array; this is based on the average face locations of the different landmarks. We use this in the next part to align the image. These numbers are based on where we want the face to be but could be any numbers that can be reliably detected on the face.</p>
			<ol>
				<li value="6">Next, we generate the transformation matrix to align the face:<pre class="source-code">
warp_matrix = umeyama(limited_landmarks,
  MEAN_FACE * (opt.size*.6)+(opt.size*.2), True)</pre></li>
			</ol>
			<p>This is a bit complicated to understand, so we’ll go into depth here. We use an algorithm to align two point sets created by Shinji Umeyama as implemented in the <code>SciKit-Image</code> library. This takes two sets of points, one a known set (in this case, <code>MEAN_FACE</code>, which we defined earlier) and the other an unknown set (in this case, the five landmark points from the detected face saved in <code>limited_landmarks</code>), and aligns them. Next, we multiply the landmarks by the size that <a id="_idIndexMarker259"/>we want the image to end up being and add a border around the face so that it is centered with some extra space around the edges.</p>
			<p>The <code>umeyama</code> algorithm creates a matrix that we save as <code>warp_matrix</code>, which encodes the translations necessary to create an aligned face.</p>
			<ol>
				<li value="7">Next, we add the landmarks and <code>warp_matrix</code> to the list of alignment data:<pre class="source-code">
alignment_data[file] = {"landmark": landmarks,
                        "warp_matrix": warp_matrix}</pre></li>
				<li>Finally, we create and write the aligned face image:<pre class="source-code">
aligned_face = image_bgr.copy()
aligned_face = cv2.warpAffine(aligned_face,
  warp_matrix[:2], (opt.size,opt.size))
cv2.imwrite(os.path.join(opt.export_path,
              f"face_aligned_{filename}_{idx}.png"),
            aligned_face)</pre></li>
			</ol>
			<p>Here, the code creates a new copy of the original image and then uses the <code>warpAffine</code> function from the OpenCV library to apply the <code>warp_matrix</code> that was generated by the <code>umeyama</code> algorithm. The matrix includes all the information – translation (moving the image side to side or up and down), scaling (resizing to fit), and rotation – to align the face with the pre-defined landmarks. Finally, it saves that newly aligned image as a file.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">While OpenCV does all image processes in BGR color order, it’s fine to do any tasks that don’t depend on the color order, such as this <code>cv2.warpAffine()</code> step here. If you do ignore the color order, you must be careful since it can get easy to forget which color order you are using, leading to complicated bugs where the colors are all wrong. In this case, since the next step will be to write the image out as a file using <code>cv2.imwrite()</code>, we are fine using the BGR color order image.</p>
			<ol>
				<li value="9">Next, we save the data we’ll need to later reconstruct the image:<pre class="source-code">
if file not in alignment_data.keys():
  alignment_data[file] = {"faces": list()}
  alignment_data[file]['faces'].append({
    "landmark": landmarks,"warp_matrix": warp_matrix})</pre></li>
			</ol>
			<p>We save <a id="_idIndexMarker260"/>the landmarks and the warp matrix into a dictionary, which we’ll later save as a JSON file. This information is important to save for later processing steps, so we must make sure to save it.</p>
			<ol>
				<li value="10">Next, we’re going to create a mask image:<pre class="source-code">
mask_face = cv2.resize(aligned_face, (512, 512))
mask_face = torch.tensor(mask_face,
  device=device).unsqueeze(0)
mask_face = mask_face.permute(0, 3, 1, 2) / 255
if device == "cuda":
  mask_face.cuda()
segments = masker(mask_face)[0]</pre></li>
			</ol>
			<p>The masker is another AI that has specific requirements for the image that it is given. To meet these requirements, we must first process the face image in certain ways. The first is that the masker AI expects images that are 512x512 pixels. Since our aligned faces can be different sizes, we need to make a copy of the image that is in the expected 512x512 size.</p>
			<p>We then convert it to a <code>PyTorch</code> Tensor instead of a <code>Numpy</code> array and then <code>unsqueeze</code> the tensor. This adds an additional dimension since the masker works on an array containing one or more images; even though we’re only feeding it one, we still need to give it that extra dimension containing our single image to match the shape expected.</p>
			<p>Next, the masker expects the channels to be in a different order than we have them. To do this, we <code>permute</code> the array into the correct order. Also, traditionally images are stored in the range of <code>0</code>-<code>255</code>, which allows for <code>256</code> variations of each separate color, but the masker expects the image colors to be a float in the range of 0-1. We divide the range by <code>255</code> to get into the expected range.</p>
			<p>Next, if NVIDIA support is enabled, we convert the image into a CUDA variable, which converts it into <a id="_idIndexMarker261"/>a format that can be used by the GPU as well as handle being moved to the GPU.</p>
			<p>Finally, we run the masker AI on the image that we’ve prepared, saving the mask output to a new variable. The masker outputs multiple arrays of information, but only the first array is useful to us now, so we save only that one and discard all the others.</p>
			<ol>
				<li value="11">Next, we process the masker output and save it to an image file:<pre class="source-code">
segments = torch.softmax(segments, dim=1)
segments = torch.nn.functional.interpolate(segments,
  size=(256, 256),
  mode="bicubic",
  align_corners=False)
mask = torch.where( torch.sum(
    segments[:,desired_segments,:,:], dim=1) &gt; .7,
  255, 0)[0]
mask = mask.cpu().numpy()
cv2.imwrite(os.path.join(opt.export_path,
              f"face_mask_{filename}_{idx}.png"),
            mask)</pre></li>
			</ol>
			<p>Now that the result has been returned from the masker, we still need to process it to get something useful. First, we use <code>softmax</code> to convert the result from absolute to relative values. This lets us look at the mask as an array of likelihoods that each pixel belongs to a particular class instead of the raw values from the model.</p>
			<p>Next, we use <code>interpolate</code>, which is a <code>Pytorch</code> method, to resize the data back to the <a id="_idIndexMarker262"/>original face image size. We have to do this because, like the input, the output of the masker model is 512x512. We use <code>bicubic</code> because it gives the smoothest results, but other options could be chosen instead.</p>
			<p>Next, we use <code>sum</code> and <code>where</code> to <code>255</code> or <code>0</code>. We also use <code>desired_segments</code> to remove the segments that aren’t useful to us. We’re using <code>.7</code> as a threshold here, so if we’re 70% sure that a given pixel should be in the mask, we keep it, but if it’s below that 70% cutoff, we throw that pixel out.</p>
			<p>Next, we move the data back to the CPU (if it was already on the CPU, then nothing changes) and convert it to a <code>Numpy</code> array.</p>
			<p>Finally, we save the mask image as a <code>.png</code> file so we can use it later.</p>
			<ol>
				<li value="12">The last step of the entire extract process is to write the alignment data as a file:<pre class="source-code">
with open(os.path.join(opt.export_path,
  f"face_alignments.json", "w") as alignment_file:
  alignment_file.write(
    json_tricks.dumps(alignment_data, indent=4))</pre></li>
			</ol>
			<p>Once each image is processed, the last step of the file is to save the file containing all the landmark data for later use. Here we use the <code>json_tricks</code> library, which has <a id="_idIndexMarker263"/>some useful functionality for writing out <code>Numpy</code> arrays as a JSON file. The library handles everything for writing and reading back the JSON file as <code>Numpy</code> arrays, so we can simply pass the full <code>dictionary</code> of arrays without manually converting them to lists or other default Python types for the Python standard library JSON to handle. For full documentation on this, please visit their documentation page at <a href="https://json-tricks.readthedocs.io/en/latest/">https://json-tricks.readthedocs.io/en/latest/</a>.</p>
			<p>At this point, we’ve extracted all the faces from a folder full of images. We’ve run them through multiple AI to get the data we need and formatted all that data for later use. This data is now ready for training, which will be covered in the next chapter.</p>
			<h1 id="_idParaDest-105"><a id="_idTextAnchor105"/>Summary</h1>
			<p>Extraction is the first step of the training process. In this chapter, we examined the data that we will need in later steps as well as the process of extracting the required training data from the source images. We went hands-on with the process, using multiple AIs to detect and landmark faces and generate a mask, as well as the necessary steps to process and save that data.</p>
			<p>The <code>C5-face_detection.py</code> file can process a directory of images. So, we covered how to convert a video into a directory of images and how to process that directory through the script. The script creates all the files you need for training and some interesting debug images that let you visualize each of the processes the detector uses to process the images. We then looked at the entire process, line by line, so that we knew exactly what was going on inside that script, learning not just what was being output, but exactly how that output was created.</p>
			<p>After finishing the detection process, you can go through data cleaning, as talked about in <a href="B17535_03.xhtml#_idTextAnchor054"><em class="italic">Chapter 3</em></a>, <em class="italic">Mastering Data</em>, to make sure your data is ready for the subject of the next chapter: training.</p>
			<h1 id="_idParaDest-106"><a id="_idTextAnchor106"/>Exercises</h1>
			<ol>
				<li>We used pre-existing libraries for face detection, landmarking, and aligning landmarks. There are other libraries that offer similar functionality. Not all libraries work the same way, and implementing the differences is an extremely useful exercise. Try replacing the <code>face_alignment</code> library with another library for detecting faces, such as <a href="https://github.com/timesler/facenet-pytorch">https://github.com/timesler/facenet-pytorch</a> or <a href="https://github.com/serengil/deepface">https://github.com/serengil/deepface</a>. Open source has lots of useful libraries but learning the differences and when to use one over another can be difficult, and converting between them can be a useful practice.</li>
				<li>We used <code>2D</code> landmarks for alignment in this chapter, but there may be a need for <code>3D</code> landmarks instead. Try replacing the following:<pre class="source-code">
face_aligner = FaceAlignment(LandmarksType._2D,
  device=device, verbose=False)</pre></li>
			</ol>
			<p>with:</p>
			<pre class="source-code">
face_aligner = FaceAlignment(LandmarksType._3D,
  device=device, verbose=False)</pre>
			<p>and adjust the rest of the process accordingly. You will also need to modify <code>MEAN_FACE</code> to account for the third dimension.</p>
			<p>What other problems do <code>3D</code> landmarks include?  What do you gain by using them?</p>
			<ol>
				<li value="3">In deepfakes, we’re most interested in faces, so this process uses techniques specific to faces. Imagine what you’d need to do to extract images of different objects. Watches, hats, or sunglasses, for example. The repo at <a href="https://github.com/ultralytics/yolov5">https://github.com/ultralytics/yolov5</a> has a pre-trained model that can detect hundreds of different objects. Try extracting a different object instead of faces. Think in particular about how to do an alignment: can you utilize edge detection or color patterns to find points to which you can align?</li>
				<li>Umeyama’s method treats every point that it aligns with equal importance, but what happens if you try to align with all 68 landmarks instead of just the <code>5</code>? What about 2 points? Can you find a better method? A faster method? A more accurate one? Try modifying the script to output all 68 landmark points in the <code>face_landmarks</code> <code>.png</code> file so you can visualize the process.</li>
			</ol>
		</div>
		<div><div></div>
		</div>
	<div><p>EBSCOhost - printed on 11/27/2023 6:20 AM via . All use subject to <a target="_blank" href="https://www.ebsco.com/terms-of-use">https://www.ebsco.com/terms-of-use</a></p></div>
</body></html>