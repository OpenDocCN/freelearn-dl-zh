<html><head></head><body>
		<div id="_idContainer073">
			<h1 id="_idParaDest-91" class="chapter-number"><a id="_idTextAnchor090"/>5</h1>
			<h1 id="_idParaDest-92"><a id="_idTextAnchor091"/>Extracting Faces</h1>
			<p>In this chapter, we will start our hands-on activities with the code. To begin with, we’ll cover the process of <span class="No-Break">extracting faces.</span></p>
			<p>Extracting faces is a series of steps in a process that involves many different stages, but it’s the first discrete stage in creating a deepfake. This chapter will first talk about how to run the face extraction script, then we will go hands-on with the code, explaining what each <span class="No-Break">part does.</span></p>
			<p>In this chapter, we will cover the following <span class="No-Break">key sections:</span></p>
			<ul>
				<li>Getting image files from <span class="No-Break">a video</span></li>
				<li>Running extract on <span class="No-Break">frame images</span></li>
				<li>Getting hands-on with <span class="No-Break">the code</span></li>
			</ul>
			<h1 id="_idParaDest-93"><a id="_idTextAnchor092"/>Technical requirements</h1>
			<p>To proceed, we recommend that you download the code from the GitHub repo at <a href="https://github.com/PacktPublishing/Exploring-Deepfakes">https://github.com/PacktPublishing/Exploring-Deepfakes</a> and follow the instructions in the <strong class="source-inline">readme.md</strong> file to install Anaconda and to create a virtual environment with all required libraries to run the scripts. The repo will also contain any errata or updates that have happened since the book was published, so please check there for <span class="No-Break">any updates.</span></p>
			<h1 id="_idParaDest-94"><a id="_idTextAnchor093"/>Getting image files from a video</h1>
			<p>Videos are not <a id="_idIndexMarker226"/>designed for frame-by-frame access and can cause <a id="_idIndexMarker227"/>problems when processed out of order. Accessing a video file is a complicated process and not good for a beginner-level chapter like this. For this reason, the first task is to convert any videos that you want to extract from into <a id="_idIndexMarker228"/>individual frames. The best way to do this is to use <strong class="bold">FFmpeg</strong>. If you followed the installation instructions in the <em class="italic">Technical requirements</em> section, you will have FFmpeg installed and ready <span class="No-Break">to use.</span></p>
			<p>Let’s begin <span class="No-Break">the process.</span></p>
			<p class="callout-heading">Tip</p>
			<p class="callout">When you see code or command examples such as those present here with text inside of curly brackets, you should replace that text with the information that is explained in the brackets. For example, if it says <strong class="source-inline">cd {Folder with video}</strong> and the video is in the <strong class="source-inline">c:\Videos\</strong> folder, then you should enter <span class="No-Break"><strong class="source-inline">cd c:\Videos</strong></span><span class="No-Break">.</span></p>
			<p>Place the video into a folder, then open an Anaconda prompt and enter the <span class="No-Break">following commands:</span></p>
			<pre class="console">
cd {Folder with video}
mkdir frames
ffmpeg -i {Video Filename} frames\video_frame_%05d.png</pre>
			<p>This will fill the <strong class="source-inline">frames</strong> folder with numbered images containing the <span class="No-Break">exported frames.</span></p>
			<p>There are a lot of <a id="_idIndexMarker229"/>options you can control with FFmpeg, from the <a id="_idIndexMarker230"/>resolution of the output image to the number of frames to be skipped. These features are beyond the scope of this book but have been covered extensively elsewhere. We advise searching for a guide to ffmpeg’s command line options if you’re going to do much more than <span class="No-Break">the basics.</span></p>
			<h1 id="_idParaDest-95"><a id="_idTextAnchor094"/>Running extract on frame images</h1>
			<p>To run the <a id="_idIndexMarker231"/>extract process on a video, you can run the extract program from the cloned <strong class="source-inline">git</strong> repository folder. To do this, simply run the following in an <span class="No-Break">Anaconda prompt:</span></p>
			<pre class="console">
cd {Folder of the downloaded git repo}\
python C5-face_detection.py {Folder of frame images}</pre>
			<p>This will run the face extraction process on each of the images in the folder and put the extracted images into the <strong class="source-inline">face_images</strong> folder, which (by default) will be inside the folder of frame images. This folder will contain three types of files for each face detected, along <a id="_idIndexMarker232"/>with a file containing <span class="No-Break">all alignments.</span></p>
			<h2 id="_idParaDest-96"><a id="_idTextAnchor095"/>face_alignments.json</h2>
			<p>There will just be one of these files. It is a JSON-formatted file containing the landmark positions and <a id="_idIndexMarker233"/>warp matrix for every face found in the images. This file is human-readable like any JSON file and can be read or edited (though it’s probably not something that you’d <span class="No-Break">do manually).</span></p>
			<h3>face_landmarks_{filename}_{face number}.png</h3>
			<p>This is a copy of <a id="_idIndexMarker234"/>the original image with a bounding box drawn around the face and five <strong class="bold">landmark</strong> points written out. Landmarks are common points on the face. We’ll cover the usage of these landmark points later, but the ones we’re most interested in are the eyes, nose, and corners of <span class="No-Break">the mouth.</span></p>
			<div>
				<div id="_idContainer068" class="IMG---Figure">
					<img src="image/B17535_05_001.jpg" alt="Figure 5.1 – Example of face_landmarks_bryan_0.png"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.1 – Example of face_landmarks_bryan_0.png</p>
			<p class="callout-heading">Author’s note</p>
			<p class="callout">All of the images of people used for practical demonstration in this section are of <span class="No-Break">the authors.</span></p>
			<h2 id="_idParaDest-97"><a id="_idTextAnchor096"/>face_bbox_{filename}_{face number}.png</h2>
			<p>This image represents the original <strong class="bold">bounding box</strong> (the smallest box that surrounds the detected face) found <a id="_idIndexMarker235"/>in the original image. It will be in the full original size and angle of the face found in <span class="No-Break">the image.</span></p>
			<div>
				<div id="_idContainer069" class="IMG---Figure">
					<img src="image/B17535_05_002.jpg" alt="Figure 5.2 – Example of face_bbox_bryan_0.png"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.2 – Example of face_bbox_bryan_0.png</p>
			<h2 id="_idParaDest-98"><a id="_idTextAnchor097"/>face_aligned_{filename}_{face number}.png</h2>
			<p>This image will be a smaller size (the default is 256x256) image of the face. This is an <strong class="bold">aligned</strong> face image, where the face has been lined up according to the landmarks. This image should <a id="_idIndexMarker236"/>generally have the face centered in the image and lined up vertically. If the face in the bounding box image was crooked or angled in the box, it should be straightened in the aligned image. There may also be a black cutout where the edge of the original frame cuts off <span class="No-Break">the data.</span></p>
			<p>This image is the most important image and is the one that will be used for training the model. It is critical for quality training that the aligned face is a good-quality image. This is the data that you’ll want to clean to get a <span class="No-Break">successful deepfake.</span></p>
			<div>
				<div id="_idContainer070" class="IMG---Figure">
					<img src="image/B17535_05_003.jpg" alt="Figure 5.3 – Example of face_aligned_bryan_0.png"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.3 – Example of face_aligned_bryan_0.png</p>
			<h2 id="_idParaDest-99"><a id="_idTextAnchor098"/>face_mask_{filename}_{face number}.png</h2>
			<p>This image <a id="_idIndexMarker237"/>matches the size of the aligned image. In fact, it matches the aligned image in the crop, rotation, and size. The mask is an AI-predicted outline of the face that will be used later to ensure that the face is trained properly and help swap the final face back onto <span class="No-Break">the image.</span></p>
			<div>
				<div id="_idContainer071" class="IMG---Figure">
					<img src="image/B17535_05_004.jpg" alt="Figure 5.4 – Example of face_mask_bryan_0.png"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.4 – Example of face_mask_bryan_0.png</p>
			<p>The mask <a id="_idIndexMarker238"/>should line up with the aligned face perfectly, showing where the face and edges are. You can see how the mask overlays onto the face in <span class="No-Break"><em class="italic">Figure 5</em></span><span class="No-Break"><em class="italic">.5</em></span><span class="No-Break">.</span></p>
			<div>
				<div id="_idContainer072" class="IMG---Figure">
					<img src="image/B17535_05_005.jpg" alt="Figure 5.5 – Example of mask overlayed on aligned face"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.5 – Example of mask overlayed on aligned face</p>
			<p>So, now that you <a id="_idIndexMarker239"/>know how to run the face detector and all of its output, let’s get into the hands-on section and determine exactly what <span class="No-Break">it’<a id="_idTextAnchor099"/>s doing.</span></p>
			<h1 id="_idParaDest-100"><a id="_idTextAnchor100"/>Getting hands-on with the code</h1>
			<p>Now it’s time to get into the code. We’ll go over exactly what <strong class="source-inline">C5-face_detection.py</strong> does and why each option was chosen. There are five main parts to the code: initialization, image preparation, face detection, face landmarking/aligning, <span class="No-Break">and masking.</span></p>
			<h2 id="_idParaDest-101"><a id="_idTextAnchor101"/>Initialization</h2>
			<p><span class="No-Break">Let’s begin.</span></p>
			<p class="callout-heading">Author’s note</p>
			<p class="callout">Formatting for easy reading in a book requires modifying the spacing in the samples. Python, however, is whitespace sensitive and uses spacing as a part of the language syntax. This means that copying code from this book will almost definitely contain the wrong spacing. For this reason, we highly recommend pulling the code from the Git repository for the book at <a href="https://github.com/PacktPublishing/Exploring-Deepfakes">https://github.com/PacktPublishing/Exploring-Deepfakes</a> if you plan on <span class="No-Break">running it.</span></p>
			<ol>
				<li>First, we <a id="_idIndexMarker240"/>import all <span class="No-Break">required libraries:</span><pre class="source-code">
import os
import torch
import cv2
import json_tricks
import numpy as np
from tqdm import tqdm
from argparse import ArgumentParser
from face_alignment.detection.sfd import FaceDetector
from face_alignment import FaceAlignment, LandmarksType
from skimage.transform._geometric import _umeyama as umeyama
from lib.bisenet import BiSeNet</pre></li>
			</ol>
			<p>In this section, we’re importing all the important libraries and functions that we will use. These are all used in the code and will be explained when they’re used, but it’s a good idea to get familiar with the imports for any project since it lets you understand the functions being used and where they <span class="No-Break">came from.</span></p>
			<p>Many of these imports are from the Python standard library, and if you’re interested in <a id="_idIndexMarker241"/>reading more about them, you can find their documentation online. We’ll be explaining those that aren’t part of the standard library as we come to them, including where you can find documentation <span class="No-Break">on them.</span></p>
			<ol>
				<li value="2">Next, let’s skip to the end of the code for a moment, where we’ll look at <span class="No-Break">argument parsing:</span><pre class="source-code">
if __name__ == "__main__":
""" Process images in a directory into aligned face images
Example CLI:
------------
python face_detection.py "C:/media_files/"
"""
parser = ArgumentParser()
parser.add_argument("path",
  help="folder of images to run detection on")
parser.add_argument("--cpu", action="store_true",
  help="Force CPU usage")
parser.add_argument("--size", default=256,
  help="height and width to save the face images")
parser.add_argument("--max_detection_size", default=1024,
  help="Maximum size of an image to run detection on.
    (If you get memory errors, reduce this size)")
parser.add_argument("--jpg", action="store_true",
  help="use JPG instead of PNG for image saving
    (not recommended due to artifacts in JPG images)")
parser.add_argument("--min_size", default=.1,
  help="Minimum size of the face relative to image")
parser.add_argument("--min_confidence", default=.9,
  help="Minimum confidence for the face detection")
parser.add_argument("--export-path",
  default="$path/face_images",
  help="output folder (replaces $path with the input)"
opt = parser.parse_args()
opt.export_path = opt.export_path.replace('$path',opt.path)
main(opt)</pre></li>
			</ol>
			<p>In this section, we <a id="_idIndexMarker242"/>define the options available in the script. It lets you change many options without needing to modify the code at all. Most of these will be fine with the defaults, but the argument parser included in the Python standard libraries gives us an easy-to-use method to make those changes at runtime. The documents and guides for the argument parser are part of the standard library, so we’ll skip over the basics, except to state that we use <strong class="source-inline">parser.add_argument</strong> for each argument that we want, and all the arguments get put into the <span class="No-Break"><strong class="source-inline">opt</strong></span><span class="No-Break"> variable.</span></p>
			<p>One special <a id="_idIndexMarker243"/>thing we’re doing here is to change the <strong class="source-inline">export_path</strong> variable after it is defined, replacing the <strong class="source-inline">$path</strong> string in the variable to put the output folder into the input folder. If the user overrides the default, there will be nothing to replace (or the user could use <strong class="source-inline">$path</strong> to specify a different subfolder in the input folder <span class="No-Break">if desired).</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">Python norms require that this section is at the end of the file to work. However, it’s very important to at least look at this section so that you know what it is doing before we go through the rest of <span class="No-Break">the code.</span></p>
			<ol>
				<li value="3">Next, we return to the <span class="No-Break">following code:</span><pre class="source-code">
def main(opt):</pre></li>
			</ol>
			<p>This section begins with the main function we’re using to do all the <span class="No-Break">actual work.</span></p>
			<ol>
				<li value="4">The first thing we do is make sure the output <span class="No-Break">folder exists:</span><pre class="source-code">
if not os.path.exists(opt.export_path):
  os.mkdir(opt.export_path)</pre></li>
			</ol>
			<p>This creates the output folder if it doesn’t <span class="No-Break">already exist.</span></p>
			<ol>
				<li value="5">Then <a id="_idIndexMarker244"/>we enable <strong class="bold">graphics processing unit</strong> (<strong class="bold">GPU</strong>) acceleration if <span class="No-Break">it’s available:</span><pre class="source-code">
device = "cuda" if torch.cuda.is_available() and not opt.cpu else "cpu"</pre></li>
			</ol>
			<p>In this part, we begin with the <strong class="bold">artificial intelligence</strong> (<strong class="bold">AI</strong>) work. In this case, we determine <a id="_idIndexMarker245"/>whether there is an NVIDIA GPU and if there is we use it for hardware acceleration <a id="_idIndexMarker246"/>unless the user has disabled it with a command line switch. The hardware acceleration layer <a id="_idIndexMarker247"/>we use is provided by NVIDIA and is called <strong class="bold">CUDA</strong>. We check with <strong class="source-inline">PyTorch</strong> for CUDA availability and save the device variable for <span class="No-Break">later use.</span></p>
			<ol>
				<li value="6">Next, we’ll initialize the face detector <span class="No-Break">and aligner:</span><pre class="source-code">
face_detector = FaceDetector(device=device, verbose=False)
face_aligner = FaceAlignment(LandmarksType._2D, device=device, verbose=False)</pre></li>
			</ol>
			<p>This code defines the face detector and aligner that we’ll be using; it specifies what devices we’re going to use and tells the aligner to use a model trained for <strong class="source-inline">2D</strong> face landmarks. These classes come from the <strong class="source-inline">face_alignment</strong> library available at <a href="https://github.com/1adrianb/face-alignment">https://github.com/1adrianb/face-alignment</a>. Both the detector and <a id="_idIndexMarker248"/>the aligner are AI models that have been trained for their specific uses. The detector finds any faces in a given image, returning their position through the use of a bounding box. The aligner takes those detected faces and finds landmarks that we can use to align the face to a <span class="No-Break">known position.</span></p>
			<ol>
				<li value="7">Next, we define our masker and prepare it. We load the pre-trained weights and set it to use CUDA if NVIDIA support <span class="No-Break">is enabled:</span><pre class="source-code">
masker = BiSeNet(n_classes=19)
if device == "cuda":
  masker.cuda()
model_path = os.path.join(".", "binaries",
  "BiSeNet.pth")
masker.load_state_dict(torch.load(model_path))
masker.eval()
desired_segments = [1, 2, 3, 4, 5, 6, 10, 12, 13]</pre></li>
			</ol>
			<p>We use the <strong class="source-inline">desired_segments</strong> variable to define which of the masker segments we want to use. In our current masker, this gives us the face itself and discards background, hair, and clothing so that our model will only need to learn the information that we want <span class="No-Break">to swap.</span></p>
			<ol>
				<li value="8">Lastly, we get a list of files in the <span class="No-Break">input folder:</span><pre class="source-code">
alignment_data = {}
list_of_images_in_dir = [ file for file in os.listdir(opt.path) if os.path.isfile(os.path.join(opt.path,file)) ]</pre></li>
			</ol>
			<p>This code first <a id="_idIndexMarker249"/>prepares an empty dictionary to store the alignment data. This dictionary will store all the data that we want to store and save with the face data we’ll use to convert <span class="No-Break">with later.</span></p>
			<p>Next, it will get a list of all files in the folder. This assumes that each file is an image that we want to import any faces from. If there are any non-image files, it will fail, so it’s important to keep extra files out of <span class="No-Break">the folder.</span></p>
			<p>Next, files in the directory are listed and checked to ensure they’re files before being stored in <span class="No-Break">a list.</span></p>
			<h2 id="_idParaDest-102"><a id="_idTextAnchor102"/>Image preparation</h2>
			<p>The image preparation <a id="_idIndexMarker250"/>is the <span class="No-Break">next step.</span></p>
			<ol>
				<li>This loads the images and gets them ready to be processed by the rest of <span class="No-Break">the tools:</span><pre class="source-code">
for file in tqdm(list_of_images_in_dir):
  filename, extension = os.path.splitext(file)</pre></li>
			</ol>
			<p>This is the start of a loop that will go over every file in the directory to process them. The <strong class="source-inline">tqdm</strong> library creates a nice readable status bar, including predictions for how long the process will take. This one line is enough to get the basics, but there are a lot more features that it can provide. You can see the full documentation <span class="No-Break">at </span><a href="https://github.com/tqdm/tqdm#documentation"><span class="No-Break">https://github.com/tqdm/tqdm#documentation</span></a><span class="No-Break">.</span></p>
			<ol>
				<li value="2">Next, we load the image and convert it to RGB <span class="No-Break">color order:</span><pre class="source-code">
image_bgr = cv2.imread(os.path.join(opt.path,file))
image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)
height, width, channels = image_rgb.shape</pre></li>
			</ol>
			<p>OpenCV is a library of tools for operating on images. You can find documentation for it at <a href="https://docs.opencv.org/">https://docs.opencv.org/</a>. In this code, we use it to open images and handle various image tasks, which we’ll explain as they <span class="No-Break">come up.</span></p>
			<p>OpenCV loads the images in a <strong class="bold">blue, green, red</strong> (<strong class="bold">BGR</strong>) color order, which is unusual, so we must convert it to <strong class="bold">red, green, blue</strong> (<strong class="bold">RGB</strong>) color order for later processing since the other libraries expect the files in that color order. If we don’t convert it, all the colors will be off, and many tools will provide the <span class="No-Break">wrong results.</span></p>
			<p>Then we get <a id="_idIndexMarker251"/>the image’s shape; this will give us the height and width of the image, as well as the number of color channels (this will be three since we loaded them as <span class="No-Break">color images).</span></p>
			<ol>
				<li value="3">Next, we need to check whether we need to resize <span class="No-Break">the image:</span><pre class="source-code">
adjustment = opt.max_detection_size / max(height, width)
if adjustment &lt; 1.0:
  resized_image = cv2.resize(image_rgb, None, fx=adjustment, fy=adjustment)
else:
  resized_image = image_rgb
  adjustment = 1.0</pre></li>
			</ol>
			<p>When you’re running images through AI models, the image’s dimensions can drastically change the amount of memory used. This can cause an error if it exceeds the memory available for AI to use. In this code, we’re getting a maximum size from the options, then finding what adjustment we need to make to resize the image down to the maximum size and keep <a id="_idIndexMarker252"/>track of that change so that the results from the face detector can be restored to work on the original image. This process allows us to use a smaller image for the face detection AI while still using the face from the full-sized image, giving us the best resolution <span class="No-Break">and details.</span></p>
			<h2 id="_idParaDest-103"><a id="_idTextAnchor103"/>Face detection</h2>
			<p>Now it’s time to detect the <a id="_idIndexMarker253"/>faces within the image. This process goes through each image to detect <span class="No-Break">any faces.</span></p>
			<ol>
				<li>First, we check the resized image for <span class="No-Break">any faces:</span><pre class="source-code">
faces = face_detector.detect_from_image(resized_image)</pre></li>
			</ol>
			<p>This runs the face detector to find any faces in the image. The library makes this a very easy process. We send the resized image through and get back a list of all the faces found as well as their bounding boxes. These boxes are all relative to the smaller, resized image to ensure we have enough memory to <span class="No-Break">handle it.</span></p>
			<ol>
				<li value="2">Next, we’ll iterate over <span class="No-Break">each face:</span><pre class="source-code">
for idx, face in enumerate(faces):
  top,left,bottom,right = (face[0:4] /
    adjustment).astype(int)
  confidence = face[4]
  if confidence &lt; opt.min_confidence:
    Continue</pre></li>
			</ol>
			<p>As input into the loop we use the built-in Python function <strong class="source-inline">enumerate</strong>, which gives us a count of each of the faces as it finds them. We use this number later to identify the face by number.Then, we break out the bounding box size and divide it by the adjustment size. This restores the face bounding box that was detected to match the original image instead of the smaller resized image. We store the adjusted face detection in variables that we round to integers so we can keep it lined up with the <span class="No-Break">individual pixels.</span></p>
			<p>Next, the confidence level of the face detection AI is used to skip any faces that fall below the confidence level, which is set up in <span class="No-Break">the options.</span></p>
			<p class="callout-heading">Tip</p>
			<p class="callout">The confidence level from the face detection AI varies from <strong class="source-inline">0</strong> (no face) to <strong class="source-inline">1</strong> (absolute certainty of a face). Most AI uses the range of <strong class="source-inline">0</strong> to <strong class="source-inline">1</strong> since it’s easier for AI to work with a known range. If you are doing AI work and get results you don’t expect, you might want to check whether it’s been restricted down to the range of <strong class="source-inline">0</strong> <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">1</strong></span><span class="No-Break">.</span></p>
			<ol>
				<li value="3">Next, we make <a id="_idIndexMarker254"/>sure the face is big enough <span class="No-Break">to use:</span><pre class="source-code">
face_height = bottom - top
face_width = right – left
face_size = face_height * face_width
if face_size/(height*width) &lt; opt.min_size:
  continue</pre></li>
			</ol>
			<p>This code finds the face height and width, then uses that to find the overall size of the face, and compares it against a minimum face size, skipping any faces that are too small. It uses the original size of the frame to filter out faces that aren’t the main focus more easily. This is set low but can be useful if you have lots of faces in a single image, such as in a <span class="No-Break">crowd scene.</span></p>
			<ol>
				<li value="4">Next, we write out the bounding box as <span class="No-Break">an image:</span><pre class="source-code">
detected_face = image_bgr[y1:y2,x1:x2]
cv2.imwrite(os.path.join( opt.export_path,
              f"face_bbox_{filename}_{fn}.png"),
            detected_face)</pre></li>
			</ol>
			<p>This code creates an image containing just the parts of the image that fit inside the face bounding box of the face and saves it as a <strong class="source-inline">.png</strong> file into the output folder. This lets <a id="_idIndexMarker255"/>you see the face that is detected; however, it’s not actually needed for any future step, so it can be removed if you don’t want to save <span class="No-Break">this data.</span></p>
			<h2 id="_idParaDest-104"><a id="_idTextAnchor104"/>Face landmarking/aligning</h2>
			<p>The next step is to <a id="_idIndexMarker256"/>detect landmarks of the face and align them to a <span class="No-Break">known position.</span></p>
			<ol>
				<li>First, we will <span class="No-Break">get alignments:</span><pre class="source-code">
landmarks = face_aligner.get_landmarks_from_image(
  image_rgb, detected_faces = [face[0:4]/adjustment])</pre></li>
			</ol>
			<p>Here, we use the same library we used for face detection, passing the full image and the adjusted bounding boxes in order to get landmarks for the face. The library returns 68 landmark positions, which are based on specific points on <span class="No-Break">the face.</span></p>
			<ol>
				<li value="2">Next, we draw <span class="No-Break">a box:</span><pre class="source-code">
landmark_image = image_bgr.copy()
landmark_image = cv2.rectangle(landmark_image,
  (top, left), (bottom, right), thickness=10,
  color=(0, 0, 0))</pre></li>
			</ol>
			<p>Here, we generate a new copy of the original image in BGR color format so we can draw onto the image without damaging our original copy. We then use OpenCV to draw a rectangle for the detected face. Its <strong class="source-inline">thickness</strong> is set to <strong class="source-inline">10</strong> pixels and drawn <span class="No-Break">in black.</span></p>
			<ol>
				<li value="3">Next, we create the landmarks we’re going to use <span class="No-Break">for alignment:</span><pre class="source-code">
right_eye = np.mean(landmarks[0][36:42],axis=0)
left_eye = np.mean(landmarks[0][42:48],axis=0)
nose_tip = landmarks[0][30]
right_mouth = landmarks[0][48]
left_mouth = landmarks[0][54]
limited_landmarks = np.stack(
  (right_eye,
   left_eye,
   nose_tip,
   right_mouth,
   left_mouth))</pre></li>
			</ol>
			<p>In this code, we split the 68 landmarks down to just 5. We use the average of the landmarks <a id="_idIndexMarker257"/>around each eye to find the average eye position, and then get the tip of the nose and the corners of the mouth and save them into a new array. This reduced set of landmarks helps to keep alignment consistent since the 68 landmarks contain a lot of noisy <span class="No-Break">edge points.</span></p>
			<ol>
				<li value="4">Next, we will draw the landmarks onto <span class="No-Break">the image:</span><pre class="source-code">
landmark_image = cv2.rectangle(landmark_image,
  (x1,y1), (x2,y2), thickness=10, color=(0,0,0))
colors = [[255,0,0], # Blue
          [0,255,0], # Green
          [0,0,255], # Red
          [255,255,0], # Cyan
          [0,255,255]] # Yellow
for count, landmark in enumerate(limited_landmarks):
  landmark_adjusted = landmark.astype(int)
  landmark_image = cv2.circle(landmark_image,  tuple(landmark_adjusted), radius=10,
  thickness=-1, color=colors[count])
cv2.imwrite(os.path.join(opt.export_path,
              f"face_landmarks_{filename}_{idx}.png",
            landmark_image)</pre></li>
			</ol>
			<p>Here, we define a set of colors, then use those to draw individual dots for each landmark position. We save them as a <strong class="source-inline">.png</strong> image file in the output folder. These images are for demonstration and debugging purposes and aren’t ever used later, so you can remove them (and this save call) if you don’t need those <span class="No-Break">debug images.</span></p>
			<ol>
				<li value="5">Next, we define the <span class="No-Break">mean face:</span><pre class="source-code">
MEAN_FACE = np.array([[0.25, 0.22],
                      [0.75, 0.22],
                      [0.50, 0.51],
                      [0.26, 0.78],
                      [0.74, 0.78]])</pre></li>
			</ol>
			<p>Here, we <a id="_idIndexMarker258"/>define another array; this is based on the average face locations of the different landmarks. We use this in the next part to align the image. These numbers are based on where we want the face to be but could be any numbers that can be reliably detected on <span class="No-Break">the face.</span></p>
			<ol>
				<li value="6">Next, we generate the transformation matrix to align <span class="No-Break">the face:</span><pre class="source-code">
warp_matrix = umeyama(limited_landmarks,
  MEAN_FACE * (opt.size*.6)+(opt.size*.2), True)</pre></li>
			</ol>
			<p>This is a bit complicated to understand, so we’ll go into depth here. We use an algorithm to align two point sets created by Shinji Umeyama as implemented in the <strong class="source-inline">SciKit-Image</strong> library. This takes two sets of points, one a known set (in this case, <strong class="source-inline">MEAN_FACE</strong>, which we defined earlier) and the other an unknown set (in this case, the five landmark points from the detected face saved in <strong class="source-inline">limited_landmarks</strong>), and aligns them. Next, we multiply the landmarks by the size that <a id="_idIndexMarker259"/>we want the image to end up being and add a border around the face so that it is centered with some extra space around <span class="No-Break">the edges.</span></p>
			<p>The <strong class="source-inline">umeyama</strong> algorithm creates a matrix that we save as <strong class="source-inline">warp_matrix</strong>, which encodes the translations necessary to create an <span class="No-Break">aligned face.</span></p>
			<ol>
				<li value="7">Next, we add the landmarks and <strong class="source-inline">warp_matrix</strong> to the list of <span class="No-Break">alignment data:</span><pre class="source-code">
alignment_data[file] = {"landmark": landmarks,
                        "warp_matrix": warp_matrix}</pre></li>
				<li>Finally, we create and write the aligned <span class="No-Break">face image:</span><pre class="source-code">
aligned_face = image_bgr.copy()
aligned_face = cv2.warpAffine(aligned_face,
  warp_matrix[:2], (opt.size,opt.size))
cv2.imwrite(os.path.join(opt.export_path,
              f"face_aligned_{filename}_{idx}.png"),
            aligned_face)</pre></li>
			</ol>
			<p>Here, the code creates a new copy of the original image and then uses the <strong class="source-inline">warpAffine</strong> function from the OpenCV library to apply the <strong class="source-inline">warp_matrix</strong> that was generated by the <strong class="source-inline">umeyama</strong> algorithm. The matrix includes all the information – translation (moving the image side to side or up and down), scaling (resizing to fit), and rotation – to align the face with the pre-defined landmarks. Finally, it saves that newly aligned image as <span class="No-Break">a file.</span></p>
			<p class="callout-heading">Tip</p>
			<p class="callout">While OpenCV does all image processes in BGR color order, it’s fine to do any tasks that don’t depend on the color order, such as this <strong class="source-inline">cv2.warpAffine()</strong> step here. If you do ignore the color order, you must be careful since it can get easy to forget which color order you are using, leading to complicated bugs where the colors are all wrong. In this case, since the next step will be to write the image out as a file using <strong class="source-inline">cv2.imwrite()</strong>, we are fine using the BGR color <span class="No-Break">order image.</span></p>
			<ol>
				<li value="9">Next, we save the data we’ll need to later reconstruct <span class="No-Break">the image:</span><pre class="source-code">
if file not in alignment_data.keys():
  alignment_data[file] = {"faces": list()}
  alignment_data[file]['faces'].append({
    "landmark": landmarks,"warp_matrix": warp_matrix})</pre></li>
			</ol>
			<p>We save <a id="_idIndexMarker260"/>the landmarks and the warp matrix into a dictionary, which we’ll later save as a JSON file. This information is important to save for later processing steps, so we must make sure to <span class="No-Break">save it.</span></p>
			<ol>
				<li value="10">Next, we’re going to create a <span class="No-Break">mask image:</span><pre class="source-code">
mask_face = cv2.resize(aligned_face, (512, 512))
mask_face = torch.tensor(mask_face,
  device=device).unsqueeze(0)
mask_face = mask_face.permute(0, 3, 1, 2) / 255
if device == "cuda":
  mask_face.cuda()
segments = masker(mask_face)[0]</pre></li>
			</ol>
			<p>The masker is another AI that has specific requirements for the image that it is given. To meet these requirements, we must first process the face image in certain ways. The first is that the masker AI expects images that are 512x512 pixels. Since our aligned faces can be different sizes, we need to make a copy of the image that is in the expected <span class="No-Break">512x512 size.</span></p>
			<p>We then convert it to a <strong class="source-inline">PyTorch</strong> Tensor instead of a <strong class="source-inline">Numpy</strong> array and then <strong class="source-inline">unsqueeze</strong> the tensor. This adds an additional dimension since the masker works on an array containing one or more images; even though we’re only feeding it one, we still need to give it that extra dimension containing our single image to match the <span class="No-Break">shape expected.</span></p>
			<p>Next, the masker expects the channels to be in a different order than we have them. To do this, we <strong class="source-inline">permute</strong> the array into the correct order. Also, traditionally images are stored in the range of <strong class="source-inline">0</strong>-<strong class="source-inline">255</strong>, which allows for <strong class="source-inline">256</strong> variations of each separate color, but the masker expects the image colors to be a float in the range of 0-1. We divide the range by <strong class="source-inline">255</strong> to get into the <span class="No-Break">expected range.</span></p>
			<p>Next, if NVIDIA support is enabled, we convert the image into a CUDA variable, which converts it into <a id="_idIndexMarker261"/>a format that can be used by the GPU as well as handle being moved to <span class="No-Break">the GPU.</span></p>
			<p>Finally, we run the masker AI on the image that we’ve prepared, saving the mask output to a new variable. The masker outputs multiple arrays of information, but only the first array is useful to us now, so we save only that one and discard all <span class="No-Break">the others.</span></p>
			<ol>
				<li value="11">Next, we process the masker output and save it to an <span class="No-Break">image file:</span><pre class="source-code">
segments = torch.softmax(segments, dim=1)
segments = torch.nn.functional.interpolate(segments,
  size=(256, 256),
  mode="bicubic",
  align_corners=False)
mask = torch.where( torch.sum(
    segments[:,desired_segments,:,:], dim=1) &gt; .7,
  255, 0)[0]
mask = mask.cpu().numpy()
cv2.imwrite(os.path.join(opt.export_path,
              f"face_mask_{filename}_{idx}.png"),
            mask)</pre></li>
			</ol>
			<p>Now that the result has been returned from the masker, we still need to process it to get something useful. First, we use <strong class="source-inline">softmax</strong> to convert the result from absolute to relative values. This lets us look at the mask as an array of likelihoods that each pixel belongs to a particular class instead of the raw values from <span class="No-Break">the model.</span></p>
			<p>Next, we use <strong class="source-inline">interpolate</strong>, which is a <strong class="source-inline">Pytorch</strong> method, to resize the data back to the <a id="_idIndexMarker262"/>original face image size. We have to do this because, like the input, the output of the masker model is 512x512. We use <strong class="source-inline">bicubic</strong> because it gives the smoothest results, but other options could be <span class="No-Break">chosen instead.</span></p>
			<p>Next, we use <strong class="source-inline">sum</strong> and <strong class="source-inline">where</strong> to <strong class="bold">binarize</strong> the mask. Basically, we take the mask, which is a bunch of probabilities, and turn it into a mask where the only options are <strong class="source-inline">255</strong> or <strong class="source-inline">0</strong>. We also use <strong class="source-inline">desired_segments</strong> to remove the segments that aren’t useful to us. We’re using <strong class="source-inline">.7</strong> as a threshold here, so if we’re 70% sure that a given pixel should be in the mask, we keep it, but if it’s below that 70% cutoff, we throw that <span class="No-Break">pixel out.</span></p>
			<p>Next, we move the data back to the CPU (if it was already on the CPU, then nothing changes) and convert it to a <span class="No-Break"><strong class="source-inline">Numpy</strong></span><span class="No-Break"> array.</span></p>
			<p>Finally, we save the mask image as a <strong class="source-inline">.png</strong> file so we can use <span class="No-Break">it later.</span></p>
			<ol>
				<li value="12">The last step of the entire extract process is to write the alignment data as <span class="No-Break">a file:</span><pre class="source-code">
with open(os.path.join(opt.export_path,
  f"face_alignments.json", "w") as alignment_file:
  alignment_file.write(
    json_tricks.dumps(alignment_data, indent=4))</pre></li>
			</ol>
			<p>Once each image is processed, the last step of the file is to save the file containing all the landmark data for later use. Here we use the <strong class="source-inline">json_tricks</strong> library, which has <a id="_idIndexMarker263"/>some useful functionality for writing out <strong class="source-inline">Numpy</strong> arrays as a JSON file. The library handles everything for writing and reading back the JSON file as <strong class="source-inline">Numpy</strong> arrays, so we can simply pass the full <strong class="source-inline">dictionary</strong> of arrays without manually converting them to lists or other default Python types for the Python standard library JSON to handle. For full documentation on this, please visit their documentation page <span class="No-Break">at </span><a href="https://json-tricks.readthedocs.io/en/latest/"><span class="No-Break">https://json-tricks.readthedocs.io/en/latest/</span></a><span class="No-Break">.</span></p>
			<p>At this point, we’ve extracted all the faces from a folder full of images. We’ve run them through multiple AI to get the data we need and formatted all that data for later use. This data is now ready for training, which will be covered in the <span class="No-Break">next chapter.</span></p>
			<h1 id="_idParaDest-105"><a id="_idTextAnchor105"/>Summary</h1>
			<p>Extraction is the first step of the training process. In this chapter, we examined the data that we will need in later steps as well as the process of extracting the required training data from the source images. We went hands-on with the process, using multiple AIs to detect and landmark faces and generate a mask, as well as the necessary steps to process and save <span class="No-Break">that data.</span></p>
			<p>The <strong class="source-inline">C5-face_detection.py</strong> file can process a directory of images. So, we covered how to convert a video into a directory of images and how to process that directory through the script. The script creates all the files you need for training and some interesting debug images that let you visualize each of the processes the detector uses to process the images. We then looked at the entire process, line by line, so that we knew exactly what was going on inside that script, learning not just what was being output, but exactly how that output <span class="No-Break">was created.</span></p>
			<p>After finishing the detection process, you can go through data cleaning, as talked about in <a href="B17535_03.xhtml#_idTextAnchor054"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, <em class="italic">Mastering Data</em>, to make sure your data is ready for the subject of the next <span class="No-Break">chapter: training.</span></p>
			<h1 id="_idParaDest-106"><a id="_idTextAnchor106"/>Exercises</h1>
			<ol>
				<li>We used pre-existing libraries for face detection, landmarking, and aligning landmarks. There are other libraries that offer similar functionality. Not all libraries work the same way, and implementing the differences is an extremely useful exercise. Try replacing the <strong class="source-inline">face_alignment</strong> library with another library for detecting faces, such as <a href="https://github.com/timesler/facenet-pytorch">https://github.com/timesler/facenet-pytorch</a> or <a href="https://github.com/serengil/deepface">https://github.com/serengil/deepface</a>. Open source has lots of useful libraries but learning the differences and when to use one over another can be difficult, and converting between them can be a <span class="No-Break">useful practice.</span></li>
				<li>We used <strong class="source-inline">2D</strong> landmarks for alignment in this chapter, but there may be a need for <strong class="source-inline">3D</strong> landmarks instead. Try replacing <span class="No-Break">the following:</span><pre class="source-code">
face_aligner = FaceAlignment(LandmarksType._2D,
  device=device, verbose=False)</pre></li>
			</ol>
			<p><span class="No-Break">with:</span></p>
			<pre class="source-code">
face_aligner = FaceAlignment(LandmarksType._3D,
  device=device, verbose=False)</pre>
			<p>and adjust the rest of the process accordingly. You will also need to modify <strong class="source-inline">MEAN_FACE</strong> to account for the <span class="No-Break">third dimension.</span></p>
			<p>What other problems do <strong class="source-inline">3D</strong> landmarks include?  What do you gain by <span class="No-Break">using them?</span></p>
			<ol>
				<li value="3">In deepfakes, we’re most interested in faces, so this process uses techniques specific to faces. Imagine what you’d need to do to extract images of different objects. Watches, hats, or sunglasses, for example. The repo at <a href="https://github.com/ultralytics/yolov5">https://github.com/ultralytics/yolov5</a> has a pre-trained model that can detect hundreds of different objects. Try extracting a different object instead of faces. Think in particular about how to do an alignment: can you utilize edge detection or color patterns to find points to which you <span class="No-Break">can align?</span></li>
				<li>Umeyama’s method treats every point that it aligns with equal importance, but what happens if you try to align with all 68 landmarks instead of just the <strong class="source-inline">5</strong>? What about 2 points? Can you find a better method? A faster method? A more accurate one? Try modifying the script to output all 68 landmark points in the <strong class="source-inline">face_landmarks</strong> <strong class="source-inline">.png</strong> file so you can visualize <span class="No-Break">the process.</span></li>
			</ol>
		</div>
		<div>
			<div id="_idContainer074" class="IMG---Figure">
			</div>
		</div>
	<div style="width:100%; margin-top:20px; "><div style="text-align:left; padding:10px" aria-hidden="true"><span style="font-size: 0.75em">EBSCOhost - printed on 11/27/2023 6:20 AM via . All use subject to <a target="_blank" href="https://www.ebsco.com/terms-of-use">https://www.ebsco.com/terms-of-use</a></span></div></div>
</body></html>