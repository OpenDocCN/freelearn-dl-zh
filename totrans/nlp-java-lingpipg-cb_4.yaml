- en: Chapter 4. Tagging Words and Tokens
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Interesting phrase detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Foreground- or background-driven interesting phrase detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden Markov Models (HMM) – part-of-speech
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: N-best word tagging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Confidence-based tagging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training word tagging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Word-tagging evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conditional random fields (CRF) for word/token tagging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modifying CRFs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Words and tokens are the focus of this chapter. The more common extraction technologies,
    such as named entity recognition, are actually encoded into the concepts presented
    here, but this will have to wait until [Chapter 5](part0061_split_000.html#page
    "Chapter 5. Finding Spans in Text – Chunking"), *Finding Spans in Text – Chunking*.
    We will start easy with finding interesting sets of tokens. Then, we will move
    on to HMM and finish with one of the most complex components of LingPipe—CRF.
    As usual, we show you how to evaluate tagging and train your own taggers.
  prefs: []
  type: TYPE_NORMAL
- en: Interesting phrase detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Imagine that a program can take a bunch of text data and automatically find
    the interesting parts, where "interesting" means that the word or phrase occurs
    more often than expected. It has a very nice property—no training data is needed,
    and it works for any language that we have tokens for. You have seen this most
    often in tag clouds such as the one in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Interesting phrase detection](img/00010.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The preceding figure shows a tag cloud generated for the [lingpipe.com](http://lingpipe.com)
    home page. However, be aware that tag clouds are considered to be the "mullets
    of the Internet" as noted by Jeffery Zeldman in [http://www.zeldman.com/daily/0405d.shtml](http://www.zeldman.com/daily/0405d.shtml),
    so you will be on shaky ground if you deploy such a feature on a website.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To get the interesting phrases from a small dataset with tweets about Disney,
    perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fire up the command line and type:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The program should respond with something like:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You can also supply a `.csv` file in our standard format as an argument to see
    different data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The output tends to be tantalizingly useless. Tantalizingly useless means that
    some useful phrases show up, but with a bunch of less interesting phrases that
    you will never want in your summary of what is interesting in the data. On the
    interesting side, we can see `Crayola Color`, `Lindsey Lohan`, `Episode VII`,
    and so on. On the junk side, we can see `ncipes azules`, `pictures releases`,
    and so on. There are lots of ways to address the junk output—the obvious first
    step will be to use a language ID classifier to throw out non-English.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here, we will go through the source in its entirety, broken by explanatory
    text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we see the path, imports, and the `main()` method. Our ternary operator
    that supplies a default file name or reads from the command line is the last line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'After collecting the input data, the first interesting code constructs a tokenized
    language model which differs in significant ways from the character language models
    used in [Chapter 1](part0014_split_000.html#page "Chapter 1. Simple Classifiers"),
    *Simple Classifiers*. A tokenized language model operates over tokens created
    by `TokenizerFactory`, and the `ngram` parameter dictates the number of tokens
    used instead of the number of characters. A subtlety of `TokenizedLM` is that
    it can also use character language models to make predictions for tokens it has
    not seen before. See the *Foreground- or background-driven interesting phrase
    detection* recipe to understand how this works in practice; don''t use the preceding
    constructor unless there are no unknown tokens when estimating. Also, the relevant
    Javadoc provides more details on this. In the following code snippet, the language
    model is trained:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The next relevant step is the creation of collocations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The parameterization controls the phrase length in tokens; it also sets a minimum
    count of how often the phrase can be seen and how many phrases to return. We can
    look at phrases of length 3 as we have a language model that stores 3 grams. Next,
    we will visit the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `SortedSet<ScoredObject<String[]>>` collocation is sorted from a high score
    to a low score. The intuition behind the score is that a higher score is given
    when the tokens are seen together more than one would expect, given their singleton
    frequency in the training data. In other words, phrases are scored depending on
    how much they vary from the independence assumption based on the tokens. See the
    Javadoc at [http://alias-i.com/lingpipe/docs/api/com/aliasi/lm/TokenizedLM.html](http://alias-i.com/lingpipe/docs/api/com/aliasi/lm/TokenizedLM.html)
    for the exact definitions—an interesting exercise will be to create your own score
    and compare it with what is done in LingPipe.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Given that this code is close to being usable on a website, it is worth discussing
    tuning. Tuning is the process of looking at system output and making changes based
    on the mistakes the system makes. Some changes that we would immediately consider
    include:'
  prefs: []
  type: TYPE_NORMAL
- en: A language ID classifier that will be handy to filter out non-English texts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some thought around how to better tokenize the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Varying token lengths to include 3 grams and unigrams in the summary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using named entity recognition to highlight proper nouns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Foreground- or background-driven interesting phrase detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Like the previous recipe, this recipe finds interesting phrases, but it uses
    another language model to determine what is interesting. Amazon''s statistically
    improbable phrases (**SIP**) work this way. You can get a clear view from their
    website at [http://www.amazon.com/gp/search-inside/sipshelp.html](http://www.amazon.com/gp/search-inside/sipshelp.html):'
  prefs: []
  type: TYPE_NORMAL
- en: '*"Amazon.com''s Statistically Improbable Phrases, or "SIPs", are the most distinctive
    phrases in the text of books in the Search Inside!™ program. To identify SIPs,
    our computers scan the text of all books in the Search Inside! program. If they
    find a phrase that occurs a large number of times in a particular book relative
    to all Search Inside! books, that phrase is a SIP in that book.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: SIPs are not necessarily improbable within a particular book, but they are improbable
    relative to all books in Search Inside!."
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The foreground model will be the book being processed, and the background model
    will be all the other books in Amazon's Search Inside!™ program. While Amazon
    has probably introduced tweaks that differ, it is the same basic idea.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are a few sources of data worth looking at to get interesting phrases
    with two separate language models. The key is you want the background model to
    function as the source of expected word/phrase distributions that will help highlight
    interesting phrases in the foreground model. Some examples include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Time-separated Twitter data**: The examples of time-separated Twitter data
    are as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Background model**: This refers to a year worth of tweets about Disney World
    up to yesterday.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Foreground model**: Tweets for today.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interesting phrases**: What''s new in Disney World today on Twitter.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Topic-separated Twitter data**: The examples of topic-separated Twitter data
    are as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Background model**: Tweets about Disneyland'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Foreground model**: Tweets about Disney World'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interesting phrases**: What is said about Disney World that is not said about
    Disneyland'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Books on very similar topics**: The examples of books on similar topics are
    as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Background model**: A pile of early sci-fi novels'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Foreground model**: Jules Verne''s *War of the Worlds*'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interesting phrases**: The unique phrases and concepts of "War of the Worlds"'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are the steps to run a foreground or background model on tweets about
    Disneyland versus tweets about Disney World:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the command line, type:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will look something like:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The foreground model consists of tweets for the search term, `disneyland`, and
    the background model consists of tweets for the search term, `disneyworld`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The top tied results are for unique features of California-based Disneyland,
    namely, the name of the castle, Sleeping Beauty's Castle, and a theme park built
    in the parking lot of Disneyland, California Adventure.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The next bigram is for *Winter Dreams*, which refers to a premier for a film.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Overall, not a bad output to distinguish between the tweets of the two resorts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The code is in `src/com/lingpipe/cookbook/chapter4/InterestingPhrasesForegroundBackground.java`.
    The exposition starts after we loaded the raw `.csv` data for the foreground and
    background models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'What is being built here is the model that is used to judge the novelty of
    phrases in the foreground model. Then, we will create and train the foreground
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will access the `newTermSet()` method from the foreground model. The
    parameters and `phraseSize` determine how long the token sequences are; `minCount`
    specifies a minimum number of instances of the phrase to be considered, and `maxReturned`
    controls how many results to return:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The preceding `for` loop prints out the phrases in order of the most surprising
    phrase to the least surprising one.
  prefs: []
  type: TYPE_NORMAL
- en: The details of what is going on here are beyond the scope of the recipe, but
    the Javadoc again starts us down the road to enlightenment.
  prefs: []
  type: TYPE_NORMAL
- en: The exact scoring used is the z-score, as defined in `BinomialDistribution.z(double,int,int)`,
    with the success probability defined by the n-grams probability estimate in the
    background model, the number of successes being the count of the n-gram in this
    model, and the number of trials being the total count in this model.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This recipe is the first place where we have faced unknown tokens, which can
    have very bad properties if not handled correctly. It is easy to see why this
    is a problem with a maximum likelihood of a token-based language model, which
    is a fancy name for a language model that provides an estimate of some unseen
    tokens by multiplying the likelihoods of each token. Each likelihood is the number
    of times the token was seen in training divided by the number of tokens seen in
    data. For example, consider training on the following data from *A Connecticut
    Yankee in King Arthur''s Court*:'
  prefs: []
  type: TYPE_NORMAL
- en: '*"The ungentle laws and customs touched upon in this tale are historical, and
    the episodes which are used to illustrate them are also historical."*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This is very little training data, but it is sufficient for the point being
    made. Consider how we will get an estimate for the phrase, "The ungentle inlaws"
    using our language model. There are 24 words with "The" occurring once; we will
    assign a probability of 1/24 to this. We will assign a probability of 1/24 to
    "ungentle" as well. If we stop here, we can say that the likelihood of "The ungentle"
    is 1/24 * 1/24\. However, the next word is "inlaws", which does not exist in the
    training data. If this token is assigned a value of 0/24, it will make the likelihood
    of the entire string 0 (1/24 * 1/24 * 0/20). This means that whenever there is
    an unseen token for which the estimate is likely going to be zero, this is generally
    an unhelpful property.
  prefs: []
  type: TYPE_NORMAL
- en: 'The standard response to this issue is to substitute and approximate the value
    to stand in for data that has not been seen in training. There are a few approaches
    to solving this problem:'
  prefs: []
  type: TYPE_NORMAL
- en: Provide a low but non-zero estimate for unknown tokens. This is a very common
    approach.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use character language models with the unknown token. There are provisions for
    this in the class—refer to the Javadoc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are lots of other approaches and substantial research literature. Good
    search terms are "back off" and "smoothing".
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden Markov Models (HMM) – part-of-speech
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe brings in the first hard-core linguistic capability of LingPipe;
    it refers to the grammatical category for words or **part-of-speech** (**POS**).
    What are the verbs, nouns, adjectives, and so on in text?
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s jump right in and drag ourselves back to those awkward middle-school
    years in English class or our equivalent:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As always, head over to your friendly command prompt and type the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The system will respond with a prompt to which we will add a Jorge Luis Borges
    quote:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The system will respond delightfully to this quote with:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Appended to each token is `_` with a part-of-speech tag; `nn` is noun, `rb`
    is adverb, and so on. The complete tag set and description of the corpus of the
    tagger can be found at [http://en.wikipedia.org/wiki/Brown_Corpus](http://en.wikipedia.org/wiki/Brown_Corpus).
    Play around with it a bit. POS tagger was one of the first breakthrough machine-learning
    applications in NLP back in the '90s. You can expect this one to perform at better
    than 90-percent accuracy, although it might suffer a bit on Twitter data given
    that that the underlying corpus was collected in 1961.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As appropriate for a recipe book, we are not revealing the fundamentals of
    how part-of-speech taggers are built. There is Javadoc, the Web, and the research
    literature to help you understand the underlying technology—in the recipe for
    training an HMM, there is a brief discussion of the underlying HMM. This is about
    how to use the API as presented:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The code starts by setting up `TokenizerFactory`, which makes sense because
    we need to know what the words that are going to get the parts of speech are.
    The next line reads in a previously trained part-of-speech tagger as `HiddenMarkovModel`.
    We will not go into too much detail; you just need to know that an HMM assigns
    a part-of-speech tag for token *n* as a function of the tag assignments that preceded
    it.
  prefs: []
  type: TYPE_NORMAL
- en: The fact that these tags are not directly observed in data makes the Markov
    model hidden. Usually, one or two tokens back are looked at. There is a lot going
    on with HMMs that is worth understanding.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next line with HmmDecoder decoder wraps the HMM in code to tag provided
    tokens. Our standard interactive `while` loop is up next with all the interesting
    bits happening in the firstBest(tokenList,decoder) method at the end. The method
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Note the `decoder.tag(tokenList)` call that produces a `Tagging<String>` tagging.
    Tagging does not have an iterator or useful encapsulation of the tag/token pair,
    so the information is accessed by incrementing an index i.
  prefs: []
  type: TYPE_NORMAL
- en: N-best word tagging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The certainty-driven nature of Computer Science is not reflected in the vagaries
    of linguistics where reasonable PhDs can agree or disagree at least until Chomsky's
    henchmen show up. This recipe uses the same HMM trained in the preceding recipe
    but provides a ranked list of possible tags for each word.
  prefs: []
  type: TYPE_NORMAL
- en: Where might this be helpful? Imaging a search engine that searched for words
    and a tag—not necessarily part-of-speech. The search engine can index the word
    and the top *n*-best tags that will allow a match into a non-first best tag. This
    can help increase recall.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`N`-best analyses push the sophistication boundaries of NLP developers. What
    used to be a singleton is now a ranked list, but it is where the next level of
    performance occurs. Let''s get started by performing the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Put away your copy of *Syntactic Structures* face down and type out the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, enter the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It yields the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output lists from most likely to least likely the estimate of the entire
    sequence of tokens given the estimates from the HMM. Remember that the joint probabilities
    are log 2 based. To compare joint probabilities subtract -93.9 from -91.1 for
    a difference of 2.8\. So, the tagger thinks that option 1 is 2 ^ 2.8 = 7 times
    less likely to occur than option 0\. The source of this difference is in assigning
    green to noun rather than adjective.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The code to load the model and command I/O is the same as that of the previous
    recipe. The difference is in the method used to get and display the tagging:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: There is nothing much to it other than working out the formatting issues as
    the taggings are being iterated over.
  prefs: []
  type: TYPE_NORMAL
- en: Confidence-based tagging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is another view into the tagging probabilities; this reflects the probability
    assignments at the level of word. The code reflects the underlying `TagLattice`
    and offers insights into whether the tagger is confident or not.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This recipe will focus the probability estimates on the individual token. Perform
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Type in the following on the command line or IDE equivalent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, enter the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It yields the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This view of the data distributes the joint probabilities of the tag and word.
    We can see that there is `.208` chance that `green` should be tagged as `nn` or
    a singular noun, but the correct analysis is still `.788` with adjective `jj`.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We are still using the same old HMM from the *Hidden Markov Models (HMM) –
    part-of-speech* recipe but using different parts of it. The code to read in the
    model is exactly the same, with a major difference in how we report results. `src/com/lingpipe/cookbook/chapter4/ConfidenceBasedTagger.java`
    the method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The method demonstrates the underlying lattice of tokens to the probabilities
    explicitly, which is at the heart of the HMM. Change the termination condition
    on the `for` loop to see more or fewer tags.
  prefs: []
  type: TYPE_NORMAL
- en: Training word tagging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Word tagging gets much more interesting when you can create your own models.
    The realm of annotating part-of-speech tagging corpora is a bit too much for a
    mere recipe book—annotation of the part-of-speech data is very difficult because
    it requires considerable linguistic knowledge to do well. This recipe will directly
    address the machine-learning component of the HMM-based sentence detector.
  prefs: []
  type: TYPE_NORMAL
- en: As this is a recipe book, we will minimally explain what an HMM is. The token
    language models that we have been working with do their previous context calculations
    on some number of words/tokens that precede the current word being estimated.
    HMMs take into account some length of the previous tags while calculating estimates
    for the current token's tag. This allows for seemingly disparate neighbors such
    as `of` and `in` to be similar, because they are both prepositions.
  prefs: []
  type: TYPE_NORMAL
- en: In the *Sentence detection* recipe, from [Chapter 5](part0061_split_000.html#page
    "Chapter 5. Finding Spans in Text – Chunking"), *Finding Spans in Text – Chunking*,
    a useful but not very flexible sentence detector is based on the `HeuristicSentenceModel`
    in LingPipe. Rather than mucking about with modifying/extending the `HeuristicSentenceModel`,
    we will build a machine-learning-based sentence system with the data that we annotate.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The steps here describe how to run the program in `src/com/lingpipe/cookbook/chapter4/HMMTrainer.java`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Either create a new corpus of the sentence-annotated data or use the following
    default data, which is in `data/connecticut_yankee_EOS.txt`. If you are rolling
    your own data, simply edit some text with `['' and '']` to mark the sentence boundaries.
    Our example looks like the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Go to the command prompt and run the program with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It will give the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is a tokenized text with one of the three tags: `BOS` for the beginning
    of a sentence, `EOS` for the end of a sentence, and `WORD` for all other tokens.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Like many span-based markups, the `span` annotation is translated into a token-level
    annotation, as shown earlier in the recipe''s output. So, the first order of business
    is to collect the annotated text, set up `TokenizerFactory`, and then call a parsing
    subroutine to add to `List<Tagging<String>>`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The subroutine to parse the preceding format works by first tokenizing the
    text with `IndoEuropeanTokenizer`, which has the desirable property of treating
    the `['' and '']` sentence delimiters as separate tokens. It does not check whether
    the sentence delimiters are well formed—a more robust solution will be needed
    to do this. The tricky bit is that we want to ignore this markup in the resulting
    token stream, but we want to use it to have the token following `['' be a BOS
    and the token preceding '']` be `EOS`. Other tokens are just `WORD`. The subroutine
    builds a parallel `Lists<String>` instance for tags and tokens, which is then
    used to create `Tagging<String>` and is added to `taggingList`. The tokenization
    recipes in [Chapter 2](part0027_split_000.html#page "Chapter 2. Finding and Working
    with Words"), *Finding and Working with Words*, cover what is going on with the
    tokenizer. Have a look at the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: There is a subtlety with the preceding code. The training data is treated as
    a single tagging—this will emulate what the input will look like when we use the
    sentence detector on novel data. If more than one document/chapter/paragraph is
    being used for training, then we will call this subroutine for each block of text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Returning to the `main()` method, we will set up `ListCorpus` and add the tagging
    to the training side of the corpus, one tagging at a time. There is an `addTest()`
    method as well, but this recipe is not concerned with evaluation; if it was, we
    would most likely use `XValidatingCorpus` anyway:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will create `HmmCharLmEstimator`, which is our HMM. Note that there
    are constructors that allow for customized parameters that affect performance—see
    the Javadoc. Next, the estimator is trained against the corpus, and `HmmDecoder`
    is created, which will actually tag tokens, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: That's it! To truly wrap this as a proper sentence detector, we will need to
    map back to the character offsets in the original text, but this is covered in
    [Chapter 5](part0061_split_000.html#page "Chapter 5. Finding Spans in Text – Chunking"),
    *Finding Spans in Text – Chunking*. This is sufficient to show how to work with
    HMMs. A more full-featured approach will make sure that each BOS has a matching
    EOS and the other way around. The HMM has no such requirement.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have a small and easy-to-use corpus of the part-of-speech tags; this allows
    us to show how training the HMM for a very different problem works out to be the
    same thing. It is like our *How to classify a sentiment – simple version* recipe,
    in [Chapter 1](part0014_split_000.html#page "Chapter 1. Simple Classifiers"),
    *Simple Classifiers*; the only difference between the language ID and sentiment
    is the training data. We will start with a hard-coded corpus for simplicity—it
    is in `src/com/lingpipe/cookbook/chapter4/TinyPosCorus.java`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The corpus manually creates tokens as well as the tags for the tokens in the
    static `WORDS_TAGS` and creates `Tagging<String>` for each sentence; `Tagging<String>`
    consists of two aligned `List<String>` instances in this case. The taggings are
    then sent to the `handle()` method for the `Corpus` superclass. Swapping in this
    corpus looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We just commented out the code that loads the corpus with sentence detection
    and features in `TinyPosCorpus` in its place. It doesn''t need data to be added
    so we will just train the HMM with it. To avoid confusion we have created a separate
    class `HmmTrainerPos.java`. Running it results in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The only mistake is that `in` is a transitive verb `TV`. The training data is
    very small so mistakes are to be expected. Like the difference in language ID
    and sentiment classification in [Chapter 1](part0014_split_000.html#page "Chapter 1. Simple
    Classifiers"), *Simple Classifiers*, the HMM is used to learn a very different
    phenomenon just by changing what the training data is.
  prefs: []
  type: TYPE_NORMAL
- en: Word-tagging evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Word tagging evaluation drives developments in downstream technologies such
    as named entity detection, which, in turn, drives high-end applications such as
    coreference. You will notice that much of the evaluation resembles the evaluation
    from our classifiers except that each tag is evaluated like its own classifier
    category.
  prefs: []
  type: TYPE_NORMAL
- en: This recipe should serve to get you started on evaluation, but be aware that
    there is a very good tutorial on tagging evaluation on our website at [http://alias-i.com/lingpipe/demos/tutorial/posTags/read-me.html](http://alias-i.com/lingpipe/demos/tutorial/posTags/read-me.html);
    this recipe goes into greater detail on how to best understand tagger performance.
  prefs: []
  type: TYPE_NORMAL
- en: This recipe is short and easy to use, so you have no excuses to not evaluate
    your tagger.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following is the class source for our evaluator located at `src/com/lingpipe/cookbook/chapter4/TagEvaluator.java`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will call out the interesting bits of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First off, we will set up `TaggerEvaluator` with a null `HmmDecoder` and `boolean`
    that controls whether the tokens are stored or not. The `HmmDecoder` object will
    be set in the cross-validation code later in the code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will load `TinyPosCorpus` from the previous recipe and use it to populate
    `XValididatingObjectCorpus`—a pretty neat trick that allows for easy conversion
    between corpus types. Note that we pick 10 folds—the corpus only has 11 training
    examples, so we want to maximize the amount of training data per fold. See the
    *How to train and evaluate with cross validation* recipe in [Chapter 1](part0014_split_000.html#page
    "Chapter 1. Simple Classifiers"), *Simple Classifiers*, if you are new to this
    concept. Have a look at the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following code snippet is a `for()` loop that iterates over the number
    of folds. The first half of the loop handles training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The rest of the loop first creates a decoder for the HMM, sets the evaluator
    to use this decoder, and then applies the appropriately configured evaluator to
    the test portion of the corpus:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The last lines apply after all folds of the corpus have been used for training
    and testing. Notice that the evaluator is `BaseClassifierEvaluator`! It reports
    on each tag as a category:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Brace yourself for the torrent of evaluation. The following is a small bit
    of it, namely, the confusion matrix that you should be familiar with from [Chapter
    1](part0014_split_000.html#page "Chapter 1. Simple Classifiers"), *Simple Classifiers*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: That's it. You have an evaluation setup that is strongly related to the classifier
    evaluation from [Chapter 1](part0014_split_000.html#page "Chapter 1. Simple Classifiers"),
    *Simple Classifiers*.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are evaluation classes for the n-best word tagging, that is, `NBestTaggerEvaluator`
    and `MarginalTaggerEvaluator`, for the confidence ranked. Again, look at the more
    detailed tutorial on part-of-speech tagging for a quite thorough presentation
    on evaluation metrics and some example software to help tune the HMM.
  prefs: []
  type: TYPE_NORMAL
- en: Conditional random fields (CRF) for word/token tagging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Conditional random fields** (**CRF**) are an extension of the *Logistic regression*
    recipe in [Chapter 3](part0036_split_000.html#page "Chapter 3. Advanced Classifiers"),
    *Advanced Classifiers*, but are applied to word tagging. At the end of [Chapter
    1](part0014_split_000.html#page "Chapter 1. Simple Classifiers"), *Simple Classifiers*,
    we discussed various ways to encode a problem into a classification problem. CRFs
    treat the sequence tagging problem as finding the best category where each category
    (C) is one of the C*T tag (T) assignments to tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if we have the tokens `The` and `rain` and tag `d` for determiner
    and `n` for noun, then the set of categories for the CRF classifier are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Category 1**: `d d`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Category 2**: `n d`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Category 3**: `n n`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Category 4**: `d d`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Various optimizations are applied to keep this combinatoric nightmare computable,
    but this is the general idea. Crazy, but it works.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, CRFs allow random features to be used in training in the exact
    same way that logistic regression does for classification. Additionally, it has
    data structures optimized for HMM style observations against context. Its use
    for part-of-speech tagging is not very exciting, because our current HMMs are
    pretty close to state of the art. Where CRFs really make a difference is in use
    cases like named entity detection which are covered in [Chapter 5](part0061_split_000.html#page
    "Chapter 5. Finding Spans in Text – Chunking"), *Finding Spans in Text – Chunking*,
    but we wanted to address the pure CRF implementation before complicating the presentation
    with a chunking interface.
  prefs: []
  type: TYPE_NORMAL
- en: There is an excellent detailed tutorial on CRFs at [http://alias-i.com/lingpipe/demos/tutorial/crf/read-me.html](http://alias-i.com/lingpipe/demos/tutorial/crf/read-me.html);
    this recipe follows this tutorial fairly closely. You will find more information
    and proper references there.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'All of the technologies we have been presenting up to now were invented in
    the previous millennium; this is a technology from the new millennium. Perform
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the command line, type:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The console continues with the convergence results that should be familiar
    from the *Logistic regression* recipe of [Chapter 3](part0036_split_000.html#page
    "Chapter 3. Advanced Classifiers"), *Advanced Classifiers*, and we will get the
    standard command prompt:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In response to this, we will get some fairly confused output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This is an awful output, but the CRF has been trained on 11 sentences. So, let's
    not be too harsh—particularly since this technology mostly reigns supreme for
    word tagging and span tagging when given sufficient training data to do its work.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Like logistic regression, there are many configuration-related tasks that we
    need to perform to get this class up and running. This recipe will address the
    CRF-specific aspects of the code and refer to *the Logistic regression* recipe
    of [Chapter 3](part0036_split_000.html#page "Chapter 3. Advanced Classifiers"),
    *Advanced Classifiers* for the logistic-regression aspects of the configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Starting at the top of the `main()` method, we will get our corpus, which was
    discussed in the earlier three recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Next up is the feature extractor, which is the actual input to the CRF trainer.
    The only reason it is final is that an anonymous inner class will access it to
    demonstrate how feature extraction works in the next recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: We will address how this class works later in the recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next block of configuration is for the underlying logistic-regression algorithm.
    Refer to the *logistic regression* recipe in [Chapter 3](part0036_split_000.html#page
    "Chapter 3. Advanced Classifiers"), *Advanced Classifiers*, for more information
    on this. Have a look at the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Next up, the CRF is trained with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The rest of the code just uses the standard I/O loop. Refer to [Chapter 2](part0027_split_000.html#page
    "Chapter 2. Finding and Working with Words"), *Finding and Working with Words*,
    for how the `tokenizerFactory` works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: SimpleCrfFeatureExtractor
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now, we will get to the feature extractor. The provided implementation closely
    mimics the features of a standard HMM. The class at `com/lingpipe/cookbook/chapter4/SimpleCrfFeatureExtractor.java`
    starts with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The `ChainCrfFeatureExtractor` interface requires an `extract()` method with
    the tokens and associated tags that get converted into `ChainCrfFeatures<String>`
    in this case. This is handled by an inner class below `SimpleChainCrfFeatures`;
    this inner class extends `ChainCrfFeatures` and provides implementations of the
    abstract methods, `nodeFeatures()` and `edgeFeatures()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'The following constructor access passes the tokens and tags to the super class,
    which will do the bookkeeping to support looking up `tags` and `tokens`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'The node features are computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: The tokens are indexed by their position in the sentence. The node feature for
    the word/token in position `n` is the `String` value returned by the base class
    method `token(n)` from `ChainCrfFeatures` with the prefix `TOK_`. The value here
    is `1.0`. Feature values can be usefully adjusted to values other than 1.0, which
    is handy for more sophisticated approaches to CRFs, such as using the confidence
    estimates of other classifiers. Take a look at the following recipe for an example
    of this.
  prefs: []
  type: TYPE_NORMAL
- en: 'Like HMMs, there are features that are dependent on other positions in the
    input—these are called **edge features**. The edge features take two arguments:
    one for the position that features are being generated for `n` and `k`, which
    will apply to all other positions in the sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: The next recipe will address how to modify feature extraction.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is an extensive research literature referenced in the Javadoc and a much
    more exhaustive tutorial on the LingPipe website.
  prefs: []
  type: TYPE_NORMAL
- en: Modifying CRFs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The power and appeal of CRFs comes from rich feature extraction—proceed with
    an evaluation harness that provides feedback on your explorations. This recipe
    will detail how to create more complex features.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will not train and run a CRF; instead, we will print out the features. Substitute
    this feature extractor for the one in the previous recipe to see them at work.
    Perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to a command line and type:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The feature extractor class outputs for each token in the training data the
    truth tagging that is being used to learn:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This reflects the training tagging for the token `John` as determined by `src/com/lingpipe/cookbook/chapter4/TinyPosCorpus.java`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The node features follow the top-three POS tags from our Brown corpus HMM tagger
    and the `TOK_John` feature:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, the edge features are displayed for the other tokens in the sentence,
    "John ran":'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The rest of the output are the features for the remaining tokens in the sentence
    and then the remaining sentences in `TinyPosCorpus`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our feature extraction code occurs in `src/com/lingpipe/cookbook/chapter4/ModifiedCrfFeatureExtractor.java`.
    We will start with the `main()` method that loads a corpus, runs the contents
    past the feature extractor, and prints it out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: We will tee up `TinyPosCorpus` from the previous recipe as our corpus, and then,
    we will create a feature extractor from the containing class. The use of `final`
    is required by referencing the variable in the anonymous inner class that follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apologies for the anonymous inner class, but it is just the easiest way to
    access what is stored in the corpus for various reasons such as copying and printing.
    In this case, we are just generating and printing the features found for the training
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: The corpus contains `Tagging` objects, and they, in turn, contain a `List<String>`
    of tokens and tags. Then, this information is used to create a `ChainCrfFeatures<String>`
    object by applying the `featureExtractor.extract()` method to the tokens and tags.
    This will involve substantial computation, as will be shown.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will do reporting of the training data with tokens and the expected
    tagging:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will follow with the features that will be used to inform the CRF
    model in attempting to produce the preceding tagging for nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, the edge features are produced by the following iteration of relative
    positions to the source node `i`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'This is it to print out the features. Now, we will address how the feature
    extractor is constructed. We assume that you are familiar with the previous recipe.
    First, the constructor that brings in the Brown corpus POS tagger:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'The constructor brings in some external resources for feature generation, namely,
    a POS tagger trained on the Brown corpus. Why involve another POS tagger for a
    POS tagger? We will call the role of the Brown POS tagger a "feature tagger" to
    distinguish it from the tagger we are trying to build. A few reasons to use the
    feature tagger are:'
  prefs: []
  type: TYPE_NORMAL
- en: We are using a stupidly small corpus for training, and a more robust generalized
    POS feature tagger will help things out. `TinyPosCorpus` is too small for even
    this benefit, but with a bit more data, the fact that there is a feature `at`
    that unifies `the`, `a`, and `some` will help the CRF recognize that `some dog`
    is `'DET'` `'N'`, even though it has never seen `some` in training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have had to work with tag sets that are not aligned with POS feature taggers.
    The CRF can use these observations in the foreign tag set to better reason about
    the desired tagging. The simplest case is that `at`, from the Brown corpus tag
    set, maps cleanly onto `DET` in this tag set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There can be performance improvements to run multiple taggers that are either
    trained on different data or use different technologies to tag. The CRF can then,
    hopefully, recognize contexts where one tagger outperforms others and use this
    information to guide the analysis. Back in the day, our MUC-6 system featured
    3 POS taggers that voted for the best output. Letting the CRF sort it out will
    be a superior approach.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The guts of feature extraction are accessed with the `extract` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: '`ModChainCrfFeatures` is created as an inner class just to keep the proliferation
    of classes to a minimum, and the enclosing class is very lightweight:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: The preceding constructor hands off the tokens and tags to the super class,
    which handles bookkeeping of this data. Then, the "feature tagger" is applied
    to the tokens, and the resulting output is assigned to the member variable, `mBrownTaggingLattice`.
    The code will access the tagging, one token at a time, so it must be computed
    now.
  prefs: []
  type: TYPE_NORMAL
- en: 'The feature creation step happens with two methods: `nodeFeatures` and `edgeFeatures`.
    We will start with a simple enhancement of `edgeFeatures` from the previous recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: The code adds a token-shaped feature that generalizes `12` and `34` into `2-DIG`
    and many other generalizations. To CRF, the similarity of `12` and `34` as two-digit
    numbers is non-existent unless feature extraction says otherwise. Refer to the
    Javadoc for the complete categorizer output.
  prefs: []
  type: TYPE_NORMAL
- en: Candidate-edge features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'CRFs allow random features to be applied, so the question is what features
    make sense to use. Edge features are used in conjunction with node features, so
    another issue is whether a feature should be applied to edges or nodes. Edge features
    will be used to reason about relationships between the current word/token to those
    around it. Some possible edge features are:'
  prefs: []
  type: TYPE_NORMAL
- en: The token shape (all caps, starts with a number, and so on) of the previous
    token as done earlier.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recognition of iambic pentameter that requires a correct ordering of stressed
    and unstressed syllables. This will require a syllable-stress tokenizer as well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It often happens that text contains one or more languages—this is called code
    switching. It is a common occurrence in tweets. A reasonable edge feature will
    be the language of surrounding tokens; this language will better model that the
    next word is likely to be of the same language as the previous word.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Node features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The node features tend to be where the action is in CRFs, and they can get
    very rich. The *Named entity recognition using CRFs with better features* recipe
    in [Chapter 5](part0061_split_000.html#page "Chapter 5. Finding Spans in Text
    – Chunking"), *Finding Spans in Text – Chunking*, is an example. We will add part-of-speech
    tags in this recipe to the token feature of the previous recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Then as in the previous recipe the token feature is added with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: This results in the token string being prepended with `TOK_` and a count of
    `1`. Note that while `tag(n)` is available in training, it doesn't make sense
    to use this information, as that is what the CRF is trying to predict.
  prefs: []
  type: TYPE_NORMAL
- en: Next, the top-three tags are extracted from the POS feature tagger and added
    with the associated conditional probability. CRFs will be able to work with the
    varying weights productively.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When generating new features, some thought about the sparseness of data is
    worth considering. If dates were likely to be an important feature for the CRF,
    it would probably not be a good idea to do the standard Computer Science thing
    and convert the date to milliseconds since Jan 1, 1970 GMT. The reason is that
    the `MILLI_1000000000` feature will be treated as completely different from `MILLI_1000000001`.
    There are a few reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: The underlying classifier does not know that the two values are nearly the same
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The classifier does not know that the `MILLI_` prefix is the same—the common
    prefix is only there for human convenience
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The feature is unlikely to occur in training more than once and will likely
    be pruned by a minimum feature count
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instead of normalizing dates to milliseconds, consider an abstraction over the
    dates that will likely have many instances in training data, such as the `has_date`
    feature that ignores the actual date but notes the existence of the date. If the
    date is important, then compute all the important information about the date.
    If it is a day of the week, then map to days of the week. If temporal order matters,
    then map to coarser measurement that is likely to have many measurements. Generally
    speaking, CRFs and the underlying logistic regression classifier are robust against
    ineffective features, so feel free to be creative—you are unlikely to make accuracy
    worse by adding features.
  prefs: []
  type: TYPE_NORMAL
