- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Surveying GenAI Types and Modes: An Overview of GANs, Diffusers, and Transformers'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we established the key distinction between generative
    and discriminative models. Discriminative models focus on predicting outputs by
    learning `p(output``∣``input)`, or the conditional probability of some expected
    output given an input or set of inputs. In contrast, generative models, such as
    `p(next token``∣``previous tokens)`, based on the probabilities of possible continuations
    given the current context. Tokens are represented as vectors containing embeddings
    that capture latent features and rich semantic dependencies learned through extensive
    training.
  prefs: []
  type: TYPE_NORMAL
- en: We briefly surveyed leading generative approaches, including **Generative Adversarial
    Networks** (**GANs**), **Variational Autoencoders** (**VAEs**), diffusion models,
    and autoregressive transformers. Each methodology possesses unique strengths suitable
    for different data types and tasks. For example, GANs are adept at generating
    high-fidelity photographic images through an adversarial process. Diffusion models
    take a probabilistic approach, iteratively adding and removing noise from data
    to learn robust generative representations. Autoregressive transformers leverage
    self-attention and massive scale to achieve remarkable controlled text generation.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will explore the theoretical foundations and real-world
    applications of these techniques in greater depth. We will make direct comparisons,
    elucidating architectural innovations and enhancements that improve training stability
    and output quality over time. Through practical examples, we will see how researchers
    have adapted these models to produce art, music, videos, stories, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: To enable an unbiased comparison, we will concentrate primarily on image synthesis
    tasks. GANs and diffusion models are specifically architected for image data,
    harnessing advances in convolutional processing and computer vision. Transformers,
    powered by self-attention, excel at language modeling but can also generate images.
    This will allow us to benchmark performance on a common task.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, we will have implemented state-of-the-art image
    generation models and explored how these core methods enhance and complement each
    other.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding General Artificial Intelligence (GAI) Types – distinguishing features
    of GANs, diffusers, and transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The often-stunning human-like quality we experience from GAI can be attributed
    to deep-generative machine learning advances. In particular, three fundamental
    methods have inspired many derivative innovations – GANs, diffusion models, and
    transformers. Each has its distinct strengths and is particularly well-suited
    for specific applications.
  prefs: []
  type: TYPE_NORMAL
- en: We briefly described GANs, a groundbreaking approach that exploits the adversarial
    interplay between two competing neural networks – a generator and a discriminator
    – to generate hyper-realistic synthetic data. Over time, GANs have seen substantial
    advancements, achieving greater control in data generation, higher image fidelity,
    and enhanced training stability. For instance, NVIDIA’s StyleGAN has created highly
    detailed and realistic human faces. The adversarial training process of GANs,
    where one network generates data and the other evaluates it, allows you to create
    highly refined and detailed synthetic images, enhancing realism with each training
    iteration. The synthetic images generated can be utilized in a plethora of domains.
    In the entertainment industry, they can be used to create realistic characters
    for video games or films. In research, they provide a means to augment datasets,
    especially in scenarios where real data is scarce or sensitive. Moreover, in computer
    vision, these synthetic images aid in training and fine-tuning other machine-learning
    models, advancing applications like facial recognition.
  prefs: []
  type: TYPE_NORMAL
- en: Diffusion models, an innovative generative modeling alternative, explicitly
    address some GAN limitations. As discussed briefly in [*Chapter 1*](B21773_01.xhtml#_idTextAnchor015),
    diffusion models adopt a unique approach to introducing and systematically removing
    noise, enabling high-quality image synthesis with less training complexity. In
    medical imaging, diffusion models can significantly enhance image clarity by generating
    high-resolution synthetic examples to train other machine-learning models. Introducing
    and then iteratively removing noise can help reconstruct high-fidelity images
    from lower-quality inputs, which is invaluable in scenarios where obtaining high-resolution
    medical images is challenging.
  prefs: []
  type: TYPE_NORMAL
- en: Simultaneously, generative transformers, initially designed for language modeling,
    have been adopted for multimodal synthesis. Today, transformers are not confined
    to language and have permeated into audio, images, and video applications. For
    instance, OpenAI’s GPT-4 excels in processing and generating text, while DALL-E
    creates images from textual descriptions, a perfect example of the interplay between
    methods. When integrated, GPT-4 and DALL-E form a robust multimodal system. GPT-4
    processes and understands textual instructions, while DALL-E takes the interpreted
    instructions to generate corresponding visual representations. A practical application
    of this combination could be automated digital advertisement creation. For example,
    given textual descriptions of a product and the desired aesthetic, GPT-4 could
    interpret these instructions, and DALL-E could generate visually compelling advertisements
    accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Deconstructing GAI methods – exploring GANs, diffusers, and transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s deconstruct these core approaches to understand their distinct characteristics
    and illustrate their transformative role in advancing generative machine learning.
    As GAI continues to move forward, it’s crucial to understand how these approaches
    drive innovation.
  prefs: []
  type: TYPE_NORMAL
- en: A closer look at GANs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GANs, introduced by Goodfellow et al. in 2014, primarily consist of two neural
    networks – the **Generator** (**G**) and the **Discriminator** (**D**). G aims
    to create synthetic data resembling real data, while D strives to distinguish
    real from synthetic data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this setup, the following occurs:'
  prefs: []
  type: TYPE_NORMAL
- en: G receives input from a “latent space,” a high-dimensional space representing
    structured randomness. This structured randomness serves as a seed to generate
    synthetic data, transforming it into meaningful information.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: D evaluates the generated data, attempting to differentiate between real (or
    reference) and synthetic data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In short, the process begins with G deriving random noise from the latent space
    to create data. This synthetic data, along with real data, is supplied to D, which
    then tries to discern between the two. Feedback from D informs the parameters
    of G to refine its data generation process. The adversarial interaction continues
    until an equilibrium is reached.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Equilibrium** in GANs occurs when D can no longer differentiate between real
    and synthetic data, assigning an equal probability of 0.5 to both. Arriving at
    this state signals that the synthetic data produced by G is indistinguishable
    from real data, which is the core objective of the synthesis process.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ultimately, the success of GANs has had meaningful implications for various
    sectors. In the automotive industry, GANs have been used to simulate real-world
    scenarios for autonomous vehicle testing. In the entertainment sector, GANs are
    deployed to generate digital characters and realistic environments for filmmaking
    and game design. In the art world, GANs can literally craft new words. Moreover,
    the development of GANs has continued to move forward over the years with significant
    improvements in quality, control, and overall performance.
  prefs: []
  type: TYPE_NORMAL
- en: Advancement of GANs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Since its inception, GAN technology has evolved significantly with several
    notable advancements:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Conditional GANs (cGANs)**: Introduced by Mirza and Osindero in 2014, conditional
    GANs incorporated specific conditions during data generation, enabling more controlled
    outputs. cGANs have been used in tasks such as image-to-image translation (e.g.,
    converting photos into paintings).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deep Convolutional GANs (DCGANs)**: In 2015, Radford et al. enhanced GANs
    by integrating convolutional layers, which help to analyze image data in small,
    overlapping regions to capture fine granularity, substantially improving the visual
    quality of the synthetic output. DCGANs can generate realistic images for applications
    such as fashion design, where the model evolves new designs from existing trends.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Wasserstein GANs (WGANs)**: Introduced by Arjovsky et al. in 2017, Wasserstein
    GANs applied the Wasserstein distance metric to GANs’ objective function, facilitating
    a more accurate measurement of differences between real and synthetic data. Specifically,
    the metric helps you find the most efficient way to make the generated data distribution
    resemble the real data distribution. This small adjustment leads to a more stable
    learning process, minimizing volatility during training. WGANs have helped generate
    realistic medical imagery to aid in training diagnostic AI algorithms, improving
    a model’s ability to generalize from synthetic to actual data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Following the advent of Wasserstein GANs, the landscape experienced a surge
    of inventive expansions, each tailor-made to address specific challenges or open
    new avenues in synthetic data generation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Progressively growing GANs** incrementally increase the resolution during
    training, starting with lower-resolution images and gradually moving to higher
    resolution. This approach allows the model to learn coarse-to-fine details effectively,
    making training more manageable and generating high-quality images (Karras et
    al. 2017). These high-resolution images can enhance the realism and immersion
    of virtual reality environments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CycleGANs** facilitates image-to-image translations, bridging domain adaptation
    tasks (Zhu et al., 2017). For example, a CycleGAN could transform a summer scene
    into a winter scene without requiring example pairs (e.g., summer-winter) during
    training. CycleGANs have been used to simulate weather conditions in autonomous
    vehicle testing, evaluating system performance under varying environmental conditions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**BigGANs** push the boundaries in high-resolution image generation, showcasing
    the versatility of GANs in complex generation tasks. They achieve this by scaling
    up the size of the model (more layers and units per layer) and the batch size
    during training, alongside other architectural and training innovations (Brock
    et al., 2018). BigGANs have been used to generate realistic textures for video
    games, enhancing gaming environments’ realism.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These developments significantly broadened what GANs could achieve, ranging
    from high-resolution image synthesis to domain adaptation and cross-modal generation
    tasks. However, despite these incredible advancements, GANs have suffered from
    some continual limitations, which inspired alternative approaches such as diffusion.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations and challenges of GANs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The training process of GANs requires a careful balance between the G and D
    networks. It requires substantial computational resources, often demanding powerful
    GPUs and enormous datasets to achieve desirable outcomes. Moreover, there are
    complexities in training GANs that arise from challenges such as vanishing gradients
    and mode collapse. While the vanishing gradient problem is a problem broadly affecting
    deep neural networks, mode collapse is a challenge that is particularly unique
    to the training of GANs. Let’s explore these a bit further:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Vanishing gradients**: This issue arises during the neural network training
    phase when the gradient of the loss function diminishes to a point where the learning
    either drastically slows or halts. The crux of GANs lies in the delicate balance
    of learning between the G and D models. Disproportionate learning can hinder the
    overall training process. In practical terms, the issue of vanishing gradients
    can lead to longer training times and increased computational costs, which might
    render GANs impractical for time-sensitive or resource-constrained applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mode collapse**: Inherent to GANs, mode collapse occurs when the G starts
    producing a narrow variety of samples, thereby stifling output diversity and undermining
    a network’s effectiveness. Techniques such as a gradient penalty and spectral
    normalization have alleviated these issues. This phenomenon can significantly
    degrade the quality of generated data, limiting the use of GANs in applications
    that require diverse outputs, such as data augmentation for machine learning or
    generating diverse design alternatives in creative industries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of course, GANs carry the same ethical considerations as any state-of-the-art
    generative synthesis. For instance, they can be used to create deepfakes or generate
    biased outputs that reinforce societal prejudices. For example, when GANs, often
    used to generate synthetic data (e.g., faces), underrepresent certain groups,
    downstream applications may exhibit gender or racial bias (Kenfack et al., 2021).
  prefs: []
  type: TYPE_NORMAL
- en: Even with the advent of other generative models such as diffusion models and
    Transformer-based image generators, GANs have played a seminal role in shaping
    the trajectory of generative image synthesis, showcasing both the potential and
    some of the challenges inherent in this domain.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we better understand GANs in the context of deep generative models,
    let’s shift our focus to a successor in image generation, the diffusion model.
  prefs: []
  type: TYPE_NORMAL
- en: A closer look at diffusion models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Having explored the dynamics of GANs, let’s transition our attention to a subsequent
    innovation in image generation – the diffusion model. Initially proposed by Sohl-Dickstein
    et al. in 2015, diffusion models present a novel approach, where a neural network
    iteratively introduces and subsequently removes noise from data to generate highly
    refined images. Unlike GANs, which leverage an adversarial mechanism involving
    two contrasting models, diffusion models apply a more gradual, iterative process
    of noise manipulation within the data.
  prefs: []
  type: TYPE_NORMAL
- en: In practical terms, GANs have shown substantial merit in art and design, creating
    realistic faces or generating sharp, high-fidelity images from descriptions. They
    are also used in data augmentation, expanding datasets by generating realistic
    synthetic data to augment the training of machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Conversely, diffusion models excel in tasks requiring a structured approach
    to image generation, such as in medical imaging. Their iterative process can enhance
    the quality of medical images, such as MRI or CT scans, where noise reduction
    and clarity are paramount. This makes diffusion models invaluable in clinical
    settings, aiding in better diagnostics and analysis. Moreover, their controlled
    and gradual process offers a more predictable or stable training process compared
    to the adversarial and dynamic training of GANs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The foundation of diffusion models is anchored in two primary processes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`x``₀`) and iteratively introduces Gaussian noise, akin to progressively applying
    a fog-like filter, transforming the data into indistinguishable noise (`x``ₜ`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`p``θ`) attempts to eliminate (or de-fog) the noise from the noisy data (`x``ₜ`),
    aiming to revert to the original clean state (`x``ₜ₋₁`). Specifically, this reversion
    is orchestrated by estimating the probability of transitioning from the noisy
    state back to the clear state, using a conditional distribution denoted as `p``θ``(x``ₜ₋₁``|x``ₜ``)`.
    A **conditional distribution** tells us the likelihood of one event happening
    when we know another related event has occurred. In this case, the reversion estimates
    the likelihood of reverting to the original state, given some amount of noise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the pivotal work *Score-Based Generative Modeling through Stochastic Differential
    Equations*, the authors propose a novel framework that unifies score-based generative
    models and diffusion probabilistic modeling by employing `p``θ`.
  prefs: []
  type: TYPE_NORMAL
- en: The reverse model (`p``θ`) was implemented using convolutional networks to predict
    variations in the Gaussian noise distribution – a critical component of the noise-introduction
    process within the forward diffusion. Initially, the efficacy of this approach
    was validated on more straightforward datasets. However, the methodology’s applicability
    was later significantly improved to handle more complex images (Ho et al., 2020).
    This expansion demonstrated the practical potential of diffusion models in generating
    highly refined images across a broader spectrum of complexities.
  prefs: []
  type: TYPE_NORMAL
- en: Advancement of diffusion models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Since its inception, diffusion model technology has witnessed key advancements,
    propelling its capabilities in image generation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Simplified training objectives**: Ho et al. proposed simplified training
    objectives that predict Gaussian noise directly, eliminating the need for conditional
    means and facilitating the application to more complex datasets (Ho et al., 2020).
    This advancement facilitated handling more complex datasets, potentially aiding
    in tasks such as anomaly detection or complex data synthesis, which could be resource-intensive
    with traditional models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**UNet modules with self-attention**: Ho et al. also incorporated UNet modules
    with self-attention into the diffusion model architecture, inspired by PixelCNN++
    by Salimans et al. (2017), enhancing a model’s performance on complex datasets
    (Ho et al., 2020). Again, enhancing performance on complex datasets facilitates
    better image restoration, which is particularly beneficial in fields such as medical
    imaging or satellite imagery analysis, where high-fidelity image reconstruction
    is crucial.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Synchronization with SDEs**: Song et al. defined diffusion models as solutions
    to SDEs, linking score learning with denoising score-matching losses and expanding
    model usage for image generation, editing, in-painting, and colorization (Song
    et al., 2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Following these foundational advancements, diffusion models witnessed a wave
    of innovative enhancements as researchers introduced novel methodologies to address
    existing challenges and broaden a model’s applicability in generative modeling
    tasks. These advancements include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Noise conditioning and annealing strategies**: Song et al. improved score-based
    models by including noise conditioning and annealing strategies, achieving performance
    comparable to GANs on benchmark datasets like the Flickr-Faces-HQ dataset (Song
    et al., 2021), which is a high-quality image dataset of human faces designed to
    measure GAN performance. Achieving performance comparable to GANs could make diffusion
    models a viable alternative for high-fidelity image generation tasks in areas
    where GANs are traditionally used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Latent Diffusion Models** (**LDMs**): Rombach et al. addressed computational
    inefficiency by proposing LDMs, which operate in a compressed latent space learned
    by autoencoders, employing perceptual losses to create a visually equivalent,
    reduced latent space (Rombach et al., 2021). By addressing computational inefficiency,
    LDMs could expedite the image generation process, making them suitable for real-time
    applications or scenarios where computational resources are limited.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Classifier-free guidance**: Ho & Salimans introduced classifier-free guidance
    for controlled generation without relying on pre-trained networks, marking a step
    toward more flexible generation techniques (Ho & Salimans, 2022). This advancement
    led to more flexible generation techniques, enabling more controlled and customized
    image generation in applications such as design, advertising, or content creation
    without relying on pre-trained networks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Subsequent explorations in the diffusion model domain extended its applications,
    showcasing versatility:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Video generation**: Ho et al. adapted diffusion models for video generation,
    demonstrating their utility beyond static image generation (Ho et al., 2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**3D data processing**: Luo & Hu extended the application to 3D data processing,
    showcasing the flexibility of diffusion models (Luo & Hu, 2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The evolution of diffusion models has led to enhanced image generation and expanded
    applications in video, 3D data processing, and rapid learning methodologies. However,
    the methodology does have its challenges and limitations, outlined in some detail
    in the section that follows..
  prefs: []
  type: TYPE_NORMAL
- en: Limitations and challenges of diffusion models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Despite their evident benefits and notable progress, diffusion models have
    some unique limitations, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sampling speed**: A notable limitation of diffusion models is the slow sampling
    process, particularly when compared to GANs. Sampling, in this context, refers
    to the process of generating new data points from the learned distribution of
    a model. The speed at which new samples can be generated is crucial for many real-time
    or near-real-time applications, and the slower sampling speed of diffusion models
    can be a significant drawback.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stability during large-scale training**: The stability of diffusion models
    during large-scale training is another area requiring further exploration. Large-scale
    training refers to training a model on a substantial amount of data, sometimes
    leading to instability in the model’s learning process. Ensuring stability during
    this phase is crucial to achieve reliable and consistent performance from the
    model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A close examination of the societal impact of the media generated by these models
    is crucial, especially given the level of fine control now possible over the generated
    content. However, diffusion models’ inherent simplicity, versatility, and positive
    inductive biases signify a bright future. These attributes suggest a trajectory
    of rapid development within generative modeling, potentially integrating diffusion
    models as pivotal components in various disciplines, such as computer vision and
    graphics.
  prefs: []
  type: TYPE_NORMAL
- en: A closer look at generative transformers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The revolutionary advent of transformer models has significantly impacted the
    task of generating high-fidelity images from text descriptions. Notable models
    such as **CLIP** (**Contrastive Language-Image Pretraining**) and DALL-E utilized
    transformers in unique ways to create images based on natural language captions.
    This section will discuss the transformer-based approach for text-to-image generation,
    its foundations, the key techniques, the resulting benefits, and some challenges.
  prefs: []
  type: TYPE_NORMAL
- en: A brief overview of transformer architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The original transformer architecture, introduced by Vaswani et al. in 2017,
    is a cornerstone of many modern language-processing systems. In fact, the transformer
    may be considered the most important architecture in the area of GAI, as it is
    foundational to the GPT series of models and many other state-of-the-art generative
    methods. As such, we’ll cover the architecture briefly in our survey of generative
    approaches but will have a dedicated chapter, where we will have the opportunity
    to deconstruct and implement the transformer from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: At the core of the transformer architecture lies the **self-attention mechanism**,
    a unique approach that captures complex relationships among different elements
    within an ordered data sequence. These elements, known as **tokens**, represent
    words in a sentence or characters in a word based on the level of granularity
    chosen for **tokenization**.
  prefs: []
  type: TYPE_NORMAL
- en: The principle of **attention** in this architecture enables a model to focus
    on certain pivotal aspects of the input data while potentially disregarding less
    significant parts. This mechanism augments the model’s understanding of the context
    and the relative importance of words in a sentence.
  prefs: []
  type: TYPE_NORMAL
- en: The transformer bifurcates into two main segments, the **encoder** and the **decoder**,
    each comprising multiple layers of self-attention mechanisms. While the encoder
    discerns relationships between different positions in the input sequence, the
    decoder focuses on the outputs from the encoder, employing a variant of self-attention
    termed **masked self-attention** to prevent consideration of future outputs it
    hasn’t generated yet.
  prefs: []
  type: TYPE_NORMAL
- en: The calculation of **attention weights** through the scaled dot-product of query
    and key vectors plays a crucial role in determining the level of focus on different
    parts of the input. Additionally, **multi-head attention** allows the model to
    channel attention toward multiple data points simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, to retain the sequence order of data, the model adopts a strategy known
    as **positional encoding**. This mechanism is vital for tasks requiring an understanding
    of sequence or temporal dynamics, ensuring the model preserves the initial order
    of data throughout its processing.
  prefs: []
  type: TYPE_NORMAL
- en: Again, we will revisit the transformer architecture in [*Chapter 3*](B21773_03.xhtml#_idTextAnchor081)
    to further reinforce our understanding, as it is foundational to the continued
    research and evolution of generative AI. Nevertheless, with at least a fundamental
    grasp of the Transformer architecture, we are better positioned to dissect transformer-driven
    generative modeling paradigms across a spectrum of applications.
  prefs: []
  type: TYPE_NORMAL
- en: Generative modeling paradigms with transformers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In tackling various tasks, transformers adopt distinct training paradigms aligning
    with the task at hand. For example, discriminative tasks such as classification
    might use a masking paradigm:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Masked Language Modeling** (**MLM**): MLM is a discriminative pretraining
    technique used by models such as **BERT** (**Bidirectional Encoder Representations
    from Transformers**). During training, some percentage of input tokens are randomly
    masked out. The model must then predict the original masked words based on the
    context of the surrounding unmasked words. This teaches the model to build robust
    context-based representations, facilitating many downstream **natural language
    processing** (**NLP**) tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MLM, as utilized in BERT, has been instrumental in enhancing the performance
    of NLP systems across various domains. For instance, it can power medical coding
    systems in healthcare by accurately identifying and categorizing medical terms
    within clinical notes. This automatic coding can save significant time and reduce
    errors in medical documentation, thereby improving the efficiency and accuracy
    of healthcare data management.
  prefs: []
  type: TYPE_NORMAL
- en: 'For generative tasks, the focus shifts to creating new data sequences, requiring
    different training paradigms:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sequence-to-sequence modeling**: Sequence-to-sequence models employ both
    an encoder and a decoder. The encoder maps the input sequence to a latent representation.
    The decoder then generates the target sequence token by token from that representation.
    This paradigm is useful for tasks such as translation, summarization, and question-answering.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Autoregressive modeling**: Autoregressive modeling generates sequences by
    predicting the next token conditioned only on previous tokens. The model produces
    outputs one step at a time, with each new token depending on those preceding it.
    Autoregressive transformers such as GPT leverage this technique for controlled
    text generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformers combine self-attention for long-range dependencies, pre-trained
    representations, and autoregressive decoding to adapt to discriminative and generative
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced generative synthesis can be achieved with different architectures that
    make trade-offs between complexity, scalability, and specialization. For example,
    instead of using both the encoder and decoder, many state-of-the-art generative
    models employ a decoder-only or encoder-only approach. The encoder-decoder framework
    is often the most computationally intensive learning to specialize in, as it increases
    model size. Decoder-only architectures leverage powerful pre-trained language
    models such as GPT as the decoder, reducing parameters through weight sharing.
    Encoder-only methods forego decoding, instead, they encode inputs and perform
    regression or search on the resulting embeddings. Each approach has advantages
    that suit certain use cases, datasets, and computational budgets. In the following
    sections, we explore examples of models that employ these derivative transformer
    architectures for creative applications, such as image generation and captioning.
  prefs: []
  type: TYPE_NORMAL
- en: Encoder-only approach
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In certain models, only the encoder network maps the input to an embedding space.
    The output is then generated directly from this embedding, eliminating the need
    for a decoder. While this straightforward architecture has typically found its
    place in classification or regression tasks, recent advancements have broadened
    its application to more complex tasks. In particular, models developed for tasks
    such as image synthesis leverage the encoder-only setup to process both text and
    visual inputs, creating a multimodal relationship that facilitates the generation
    of high-fidelity images from natural language instruction.
  prefs: []
  type: TYPE_NORMAL
- en: Decoder-only approach
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Similarly, some models operate using a decoder-only strategy, where a singular
    decoder network is tasked with both encoding the input and generating output.
    This mechanism starts by joining the input and output sequences, which the decoder
    processes. Despite its simplicity and the characteristic sharing of parameters
    between input and output stages, the effectiveness of this architecture relies
    heavily on the pretraining of robust decoders. Recently, even more complex tasks
    such as text-to-image synthesis have seen the successful deployment of the decoder-only
    architecture, illustrating its versatility and adaptability to diverse applications.
  prefs: []
  type: TYPE_NORMAL
- en: Advancement of transformers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Transformer mechanisms with other novel techniques to tackle generative tasks.
    This evolution led to distinct approaches to handling text and image generation.
    In this section, we will explore some of these innovative models and their unique
    methodologies in advancing GAI.
  prefs: []
  type: TYPE_NORMAL
- en: Encoder-decoder image generation with DALL-E
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Introduced by Ramesh et al. in 2021, DALL-E employs an encoder-decoder framework
    to facilitate text-to-image generation. This model comprises two primary components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Text encoder**: Applies the transformer’s encoder, processing plain text
    to derive a semantic embedding that serves as the context for the image decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image decoder**: Applies the transformer’s decoder to generate the image
    autoregressively, predicting each pixel based on the text embedding and previously
    predicted pixels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By training on image-caption datasets, DALL-E refines the transition from text
    to detailed image renderings. This setup underscores the capability of dedicated
    encoder and decoder modules for conditional image generation.
  prefs: []
  type: TYPE_NORMAL
- en: Encoder-only image captioning with CLIP
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: CLIP, conceptualized by Radford et al. in 2021, adopts an encoder-only approach
    for image-text tasks. Key components include a visual encoder and a text encoder.
  prefs: []
  type: TYPE_NORMAL
- en: Visual Encoder and Text Encoder process the image and candidate captions, respectively,
    determining the matching caption based on encoded representations.
  prefs: []
  type: TYPE_NORMAL
- en: Pretraining on extensive image-text datasets enables CLIP to establish a shared
    embedding space, facilitating efficient inference for retrieval-based captioning.
  prefs: []
  type: TYPE_NORMAL
- en: Improving image fidelity with scaled transformers (DALL-E 2)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Ramesh et al. in 2022 extended DALL-E to DALL-E 2, showcasing techniques to
    enhance visual quality:'
  prefs: []
  type: TYPE_NORMAL
- en: '**A scaled-up decoder**: By expanding the decoder to 3.5 billion parameters
    and applying classifier-free guidance during sampling, visual quality in complex
    image distributions such as human faces is significantly improved.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hierarchical decoding for high-resolution images (GLIDE)**: Proposed by Nichol
    et al. in 2021, GLIDE employs a hierarchical generation strategy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A coarse-to-fine approach**: This entails an initial low-resolution image
    prediction followed by progressive detailing through up-sampling and refining,
    capturing global structure and high-frequency textures.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multimodal image generation with GPT-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'GPT-4 developed by OpenAI, is a powerful multimodal model based on the Transformer
    architecture. GPT-4 demonstrates a capability for conditional image generation
    without requiring continued training or fine-tuning:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pretraining and fine-tuning**: The massive scale of GPT-4 and its pretraining
    on diverse datasets enable a robust understanding of relationships between textual
    and visual data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multimodal generation:** GPT-4 can generate images based on text descriptions.
    The model uses a deep neural network to encode the semantic meaning of the text
    into a visual representation. Given a text prompt, GPT-4 generates an image by
    predicting the visual content consistent with the provided text. This involves
    taking high-dimensional text embeddings and processing them through successive
    neural network layers to generate a corresponding visual representation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a pretrained multimodal model eliminates the need for a separate encoder
    module for image inputs, facilitating rapid adaptation for image generation tasks.
    This approach underscores the versatility and power of Transformer architectures
    in generative tasks, providing a streamlined methodology to translate text into
    high-quality images.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer architectures offer many benefits for controlled image generation
    when compared to GANs. Their autoregressive nature ensures precise control over
    image construction while allowing you to adapt to varying computational needs
    and diverse downstream applications. However, transformers also introduce new
    challenges in this domain.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations and challenges of transformer-based approaches
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Some early transformers-based approaches demonstrated slower sampling speed
    and restricted fidelity compared to GANs. Generating or manipulating images while
    maintaining precise control over specific attributes or characteristics of the
    objects within those images remains challenging. Additionally, training large-scale
    transformers that can overcome these challenges demands extensive computing resources.
    Notwithstanding, current multimodal results demonstrate a rapidly evolving and
    promising landscape.
  prefs: []
  type: TYPE_NORMAL
- en: We must also remember that alongside technical challenges there are broader
    sociotechnical implications and considerations.
  prefs: []
  type: TYPE_NORMAL
- en: Bias and ethics in generative models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Significant advancements in generative models such as GANs, diffusers, and transformers
    necessitate serious contemplation of potential bias and ethical implications.
  prefs: []
  type: TYPE_NORMAL
- en: We need to remain alert to the risk of reinforcing prejudices and stereotypes
    that reflect skewed training data. For instance, diffusion models trained on data
    that over-represents specific demographics might propagate these biases in their
    output. Analogously, language models exposed to toxic or violent content during
    training might generate similar content.
  prefs: []
  type: TYPE_NORMAL
- en: The directive nature of prompt-based generation also, unfortunately, opens doors
    to misuse if deployed carelessly. Transformers risk facilitating impersonation,
    misinformation, and the creation of deceptive content. Image synthesis models
    such as GANs could potentially be exploited to generate non-consensual deepfakes
    or artificial media.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the potential for ultra-realistic output prompts ethical dilemmas
    regarding consent, privacy, identity, and copyright. The ability to create convincingly
    real yet fictional faces or voices complicates the distinction between real and
    synthetic, necessitating careful examination of training data sources and generative
    capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Further, as these technologies become ubiquitous, their societal impact must
    be considered. Defining clear policies will be crucial as the distinction between
    authentic and AI-generated content becomes increasingly ambiguous. Upholding principles
    of integrity, attribution, and consent remains vital.
  prefs: []
  type: TYPE_NORMAL
- en: Despite these risks, the potential benefits of generative models are substantial.
    Addressing bias proactively, advocating transparency, auditing data and models,
    and implementing safeguards become increasingly critical as technologies evolve.
    Ultimately, the responsibility to ensure fairness, accountability, and ethical
    practice falls on all developers and practitioners.
  prefs: []
  type: TYPE_NORMAL
- en: Applying GAI models – image generation using GANs, diffusers, and transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this hands-on section, we’ll reinforce the concepts discussed throughout
    the chapter by putting them into practice. You’ll get a first-hand experience
    and deep dive into the actual implementation of generative models, specifically
    GANs, diffusion models, and transformers.
  prefs: []
  type: TYPE_NORMAL
- en: The Python code provided will guide you through this process. Manipulating and
    observing the code in action will build your understanding of the intricate workings
    and potential applications of these models. This exercise will provide insight
    into model capabilities for tasks like generating art from prompts and synthesizing
    hyper-realistic images.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll be utilizing the highly versatile `PyTorch` library, a popular choice
    among machine learning practitioners, to facilitate our operations. `PyTorch`
    provides a powerful and dynamic toolset to define and compute gradients, which
    is central to training these models.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we’ll also use the `diffusers` library. It’s a specialized library
    that provides functionality to implement diffusion models. This library enables
    us to reproduce state-of-the-art diffusion models directly from our workspace.
    It underpins the creation, training, and usage of denoising diffusion probabilistic
    models at an unprecedented level of simplicity, without compromising the models’
    complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Through this practical session, we’ll explore how to operate and integrate these
    libraries and implement and manipulate GANs, diffusers, and transformers using
    the Python programming language. This hands-on experience will complement the
    theoretical knowledge we have gained in the chapter, enabling us to see these
    models in action in the real world.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this section, you will not only have a conceptual understanding
    of these generative models but also understand how they are implemented, trained,
    and used for several innovative applications in data science and machine learning.
    You’ll have a much deeper understanding of how these models work and the experience
    of implementing them yourself.
  prefs: []
  type: TYPE_NORMAL
- en: Working with Jupyter Notebook and Google Colab
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Jupyter notebooks enable live code execution, visualization, and explanatory
    text, suitable for prototyping and data analysis. Google Colab, conversely, is
    a cloud-based version of Jupyter Notebook, designed for machine learning prototyping.
    It provides free GPU resources and integrates with Google Drive for file storage
    and sharing. We’ll leverage Colab as our prototyping environment going forward.
  prefs: []
  type: TYPE_NORMAL
- en: Stable diffusion transformer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We begin with a pre-trained stable diffusion model, a text-to-image latent diffusion
    model created by researchers and engineers from CompVis, Stability AI, and LAION
    (Patil et al., 2022). The diffusion process is used to draw samples from complex,
    high-dimensional distributions, and when it interacts with the text embeddings,
    it creates a powerful conditional image synthesis model.
  prefs: []
  type: TYPE_NORMAL
- en: The term “stable” in this context refers to the fact that during training, a
    model maintains certain properties that stabilize the learning process. Stable
    diffusion models offer rich potential to create entirely new samples from a given
    data distribution, based on text prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Again, for our practical example, we will Google Colab to alleviate a lot of
    initial setups. Colab also provides all of the computational resources needed
    to begin experimenting right away. We start by installing some libraries, and
    with three simple functions, we will build out a minimal `StableDiffusionPipeline`
    using a well-established open-source implementation of the stable diffusion method.
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s navigate to our pre-configured Python environment, Google Colab,
    and install the `diffusers` open-source library, which will provide most of the
    key underlying components we need for our experiment.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first cell, we install all dependencies using the following `bash` command.
    Note the exclamation point at the beginning of the line, which tells our environment
    to reach down to its underlying process and install the packages we need:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we import the libraries we’ve just installed to make them available to
    our Python program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we’re ready for our three functions, which will execute the three tasks
    – loading the pre-trained model, generating the images based on prompting, and
    rendering the images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In summary, `load_model` loads a machine learning model identified by `model_id`
    onto a GPU for faster processing. The `generate_images` function takes this model
    and a list of prompts to create our images. Within this function, you will notice
    `torch.autocast("cuda")`, which is a special command that allows PyTorch (our
    underlying machine learning library) to perform operations faster while maintaining
    accuracy. Lastly, the `render_images` function displays these images in a simple
    grid format, making use of the `matplotlib` visualization library to render our
    output.
  prefs: []
  type: TYPE_NORMAL
- en: 'With our functions defined, we select our model version, define our pipeline,
    and execute our image generation process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The output in *Figure 2**.1* is a vivid example of the imaginativeness and creativity
    we typically expect from human art, generated entirely by the diffusion process.
    Except, how do we measure whether the model was faithful to the text provided?
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1: Output for the prompts “A hyper-realistic photo of a friendly
    lion” (left) and “A stylized oil painting of a NYC Brownstone” (right)](img/B21773_02_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.1: Output for the prompts “A hyper-realistic photo of a friendly lion”
    (left) and “A stylized oil painting of a NYC Brownstone” (right)'
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to evaluate the quality and relevance of our generated images
    in relation to the prompts. This is where CLIP comes into play. CLIP is designed
    to measure the alignment between text and images by analyzing their semantic similarities,
    giving us a true quantitative measure of the fidelity of our synthetic images
    to the prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Scoring with the CLIP model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: CLIP is trained to understand the relationship between text and images by learning
    to place similar images and text near each other in a shared space. When evaluating
    a generated image, CLIP checks how closely the image aligns with the textual description
    provided. A higher score indicates a better match, meaning the image accurately
    represents the text. Conversely, a lower score suggests a deviation from the text,
    indicating a lesser quality or fidelity to the prompt, providing a quantitative
    measure of how well the generated image adheres to the intended description.
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, we will import the necessary libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We begin by loading the CLIP model, processor, and necessary parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define a processing function to adjust the textual prompts and images,
    ensuring that they are in the correct format for CLIP inference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'In this step, we initiate the evaluation process by inputting the images and
    textual prompts into the CLIP model. This is done in parallel across multiple
    devices to optimize performance. The model then computes similarity scores, known
    as logits, for each image-text pair. These scores indicate how well each image
    corresponds to the text prompts. To interpret these scores more intuitively, we
    convert them into probabilities, which indicate the likelihood that an image aligns
    with any of the given prompts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, we display the images along with their scores, visually representing
    how well each image adheres to the provided prompts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'With everything detailed, let’s execute the pipeline as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We now have scores for each of our synthetic images that quantify the fidelity
    of the synthetic image to the text provided, based on the CLIP model, which interprets
    both image and text data as one combined mathematical representation (or geometric
    space) and can measure their similarity.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2: CLIP scores](img/B21773_02_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.2: CLIP scores'
  prefs: []
  type: TYPE_NORMAL
- en: For our “friendly lion,” we computed scores of 83% and 17% for each prompt,
    which we can interpret as an 83% likelihood that the image aligns with the first
    prompt.
  prefs: []
  type: TYPE_NORMAL
- en: 'In practical scenarios, this metric can be applied across various domains:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Content moderation**: Automatically moderating or flagging inappropriate
    content by comparing images to a set of predefined descriptive prompts'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image retrieval**: Facilitating refined image searches by matching textual
    queries to a vast database of images, hence narrowing down the search to the most
    relevant visuals'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image captioning**: Assisting in generating accurate captions for images
    by identifying the most relevant descriptive prompts'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Advertising**: Tailoring advertisements based on the content of images on
    a web page to enhance user engagement'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Accessibility**: Enhancing accessibility features by providing accurate descriptions
    of images for individuals with visual impairments'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This evaluation method not only speeds up processes that would otherwise require
    manual inspection but also lends itself to many applications that could benefit
    from a deeper understanding and contextual analysis of visual data. We will revisit
    the CLIP evaluation in [*Chapter 4*](B21773_04.xhtml#_idTextAnchor123), where
    we simulate a real-world scenario to determine the quality and appropriateness
    of automatically generated captions for a set of product images.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter explored the theoretical underpinnings and real-world applications
    of leading GAI techniques, including GANs, diffusion models, and transformers.
    We examined their unique strengths, including GANs’ ability to synthesize highly
    realistic images, diffusion models’ elegant image generation process, and transformers’
    exceptional language generation capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Using a cloud-based Python environment, we implemented these models to generate
    compelling images and evaluated their output quality using CLIP. We analyzed how
    techniques such as progressive growing and classifier guidance enhanced output
    fidelity over time. We also considered societal impacts, urging developers to
    address potential harm through transparency and ethical practices.
  prefs: []
  type: TYPE_NORMAL
- en: Generative methods have unlocked remarkable creative potential, but thoughtful
    oversight is critical as capabilities advance. We can guide these technologies
    toward broadly beneficial outcomes by grounding ourselves in core methodologies,
    scrutinizing their limitations, and considering downstream uses. The path ahead
    will require continued research and ethical reflection to unlock AI’s creative
    promise while mitigating risks.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This reference section serves as a repository of sources referenced within
    this book; you can explore these resources to further enhance your understanding
    and knowledge of the subject matter:'
  prefs: []
  type: TYPE_NORMAL
- en: Kenfack, P. J., Arapov, D. D., Hussain, R., Ahsan Kazmi, S. M., & Khan, A. (2021).
    *On the fairness of generative adversarial networks (**GANs)*. [Arxiv.org](https://Arxiv.org).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
    S., Courville, A., & Bengio, Y. (2014). *Generative adversarial nets. Advances
    in neural information processing* *systems*, 27.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., Sutskever,
    I., & Chen, M. (2021). *GLIDE: Towards photorealistic image generation and editing
    with text-guided diffusion models*. arXiv preprint arXiv:2112.10741.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
    G., Askell, A., Mishkin, P., Clark, J., Krueger, G., & Sutskever, I. (2021). *Learning
    Transferable Visual Models From Natural Language Supervision.* ArXiv. /abs/2103.00020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M.,
    & Sutskever, I. (2022). *Hierarchical text-conditional image generation with clip
    latents*. arXiv preprint arXiv:2204.06125.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M.,
    & Sutskever, I. (2021). *Zero-shot text-to-image generation. In International
    Conference on Machine Learning* (pp. 8821–8831). PMLR.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N.,
    Kaiser, L., & Polosukhin, I. (2017). *Attention is all you need. Advances in neural
    information processing* *systems*, 30.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Arjovsky, M., Chintala, S. & Bottou, L. (2017). *Wasserstein GAN. In Proceedings
    of the 31st International Conference on Neural Information Processing* *System
    (NIPS).*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Brock, A., Donahue, J., & Simonyan, K. (2018). *BigGANs: Large Scale GAN Training
    for High Fidelity Natural Image* *Synthesis.* [https://arxiv.org/abs/1809.11096](https://arxiv.org/abs/1809.11096).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Karras, T., Aila, T., Laine, S., & Lehtinen, J. (2017). *Progressive Growing
    of GANs for Improved Quality, Stability, and* *Variation.* [https://arxiv.org/abs/1710.10196](https://arxiv.org/abs/1710.10196).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mirza, M., & Osindero, S. (2014). *Conditional Generative Adversarial* *Nets*.
    [https://arxiv.org/abs/1411.1784](https://arxiv.org/abs/1411.1784).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford, A., Metz, L., & Chintala, S. (2015). *Unsupervised representation learning
    with deep convolutional generative adversarial networks. 3rd International Conference
    for* *Learning Representations.*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu, J.-Y., Park, T., Isola, P., & Efros, A. A. (2017). *Unpaired Image-to-Image
    Translation Using Cycle-Consistent Adversarial Networks. In Proceedings of the
    IEEE International Conference on Computer* *Vision (ICCV).*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ho, J., & Salimans, T. (2022). *Classifier-Free Diffusion Guidance. Advances
    in Neural Information Processing* *Systems*, 34.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ho, J., Salimans, T., Gritsenko, A. A., Chan, W., Norouzi, M., & Fleet, D. J.
    (2022). *Video diffusion models*. arXiv preprint arXiv:2205.10477.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ho, J., Jain, A., & Abbeel, P. (2020). *Denoising diffusion probabilistic models.
    Advances in Neural Information Processing Systems*, 33, 6840–6851.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luo, S., & Hu, W. (2021). *Diffusion probabilistic models for 3d point cloud
    generation. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern*
    *Recognition*, 2837–2845.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2021). *High-resolution
    image synthesis with latent diffusion models. Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern* *Recognition*, 10684–10695.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Salimans, T., Karpathy, A., Chen, X., & Kingma, D. P. (2017). *PixelCNN++:
    Improving the pixelcnn with discretized logistic mixture likelihood and other
    modifications*. arXiv preprint arXiv:1701.05517.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song, Y., Meng, C., & Ermon, S. (2021). *Denoising diffusion implicit models*.
    arXiv preprint arXiv:2010.02502.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song, Y., & Ermon, S. (2021). *Improved techniques for training score-based
    generative models. Advances in Neural Information Processing Systems*, 33, 12438–12448.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sohl-Dickstein, J., Weiss, E. A., Maheswaranathan, N., & Ganguli, S. (2015).
    *Deep unsupervised learning using nonequilibrium thermodynamics*. arXiv preprint
    arXiv:1503.03585.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ho, J., Jain, A., & Abbeel, P. (2020). *Denoising diffusion probabilistic models.
    Advances in Neural Information Processing Systems*, 33, 6840–6851.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., ... & Sutskever,
    I. (2022). *Zero-shot text-to-image generation. International Conference on Machine*
    *Learning*, 8821-8831.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ...
    & Amodei, D. (2020). *Language models are few-shot learners. Advances in neural
    information processing systems*, 33, 1877–1901.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Patil, S., Cuenca, P., Lambert, N., & von Platen, P. (2022). *Stable diffusion
    with diffusers*. Hugging Face Blog. [https://huggingface.co/blog/stable_diffusion](https://huggingface.co/blog/stable_diffusion).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boris Dayma, Suraj Patil, Pedro Cuenca, Khalid Saifullah, Tanishq Abraham, Phúc
    Lê, Luke, Ritobrata Ghosh. (2022, June 4). *DALL-E Mini Explained*. W&B; Weights
    & Biases, Inc. [https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-Mini-Explained-with-Demo--Vmlldzo4NjIxODA](https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-Mini-Explained-with-Demo--Vmlldzo4NjIxODA).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
