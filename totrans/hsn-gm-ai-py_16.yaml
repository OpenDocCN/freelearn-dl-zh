- en: 3D Worlds
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are almost nearing the end of our journey into what **artificial general
    intelligence** (**AGI**) is and how **deep reinforcement learning** (**DRL**)
    can be used to help us get there. While it is still questionable whether DRL is
    indeed the right path to AGI, it is what appears to be our current best option.
    However, the reason we are questioning DRL is because of its ability or inability
    to master diverse 3D spaces or worlds, the same 3D spaces we humans and all animals
    have mastered but something we find very difficult to train RL agents on. In fact,
    it is the belief of many an AGI researcher that solving the 3D state-space problem
    could go a long way to solving true general artificial intelligence. We will look
    at why that is the case in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: For this chapter, we are going to look at why 3D worlds pose such a unique problem
    to DRL agents and the ways we can train them to interpret state. We will look
    at how typical 3D agents use vision to interpret state and we will look to the
    type of deep learning networks derived from that. Then we look to a practical
    example of using 3D vision in an environment and what options we have for processing
    state. Next, sticking with Unity, we look at the Obstacle Tower Challenge, an
    AI challenge with a $100,000 prize, and what implementation was used to win the
    prize. Moving to the end of the chapter, we will look at another 3D environment
    called Habitat and how it can be used for developing agents.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a summary of the main points we will discuss this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Reasoning on 3D worlds
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a visual agent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generalizing 3D vision
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Challenging the Unity Obstacle Tower Challenge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring habitat—embodied agents by FAIR
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The examples in this chapter can take an especially long time to train, so please
    either be patient or perhaps just choose to do one. This not only saves you time
    but reduces energy consumption. In the next chapter, we explore why 3D worlds
    are so special.
  prefs: []
  type: TYPE_NORMAL
- en: Reasoning on 3D worlds
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, why are 3D worlds so important, or are at least believed to be so? Well,
    it all has to come down to state interpretation, or what we in DRL like to call
    state representation. A lot of work is being done on better representation of
    state for RL and other problems. The theory is that being able to represent just
    key or converged points of state allow us to simplify the problem dramatically.
    We have looked at doing just that using various techniques over several chapters.
    Recall how we discretized the state representation of a continuous observation
    space into a discrete space using a grid mesh. This technique is how we solved
    more difficult continuous space problems with the tools we had at the time. Over
    the course of several chapters since then, we saw how we could input that continuous
    space directly into our deep learning network. That included the ability to directly
    input an image as the game state, a screenshot, using convolutional neural networks.
    However, 3D worlds, ones that represent the real world, pose a unique challenge
    to representing state.
  prefs: []
  type: TYPE_NORMAL
- en: So what is the difficulty in representing the state space in a 3D environment?
    Could we not just give the agent sensors, as we did in other environments? Well,
    yes and no. The problem is that giving the agent sensors is putting our bias on
    what the agent needs to use in order to interpret the problem. For example, we
    could give the agent a sensor that told it the distance of an object directly
    in front of it, as well as to its left and right. While that would likely be enough
    information for any driving agent, would it work for an agent that needed to climb
    stairs? Not likely. Instead, we would likely need to give the height of the stairs
    as another sensor input, which means our preferred method of introducing state
    to an agent for a 3D world is using vision or an image of the environment. The
    reason for this, of course, is to remove any bias on our part (us humans) and
    we can best do that by just feeding the environment state as an image directly
    to the agent.
  prefs: []
  type: TYPE_NORMAL
- en: We have already seen how we could input game state using an image of the playing
    area when we looked at playing Atari games. However, those game environments were
    all 2D, meaning the state space was essentially flattened or converged. The word
    **converged** works here because this becomes the problem when tackling 3D environments
    and the real world. In 3D space, one vantage point could potentially yield to
    multiple states, and likewise, multiple state spaces can be observed by one single
    vantage point.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see how this works in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3615c006-7587-4155-9acf-ad804d03949f.png)'
  prefs: []
  type: TYPE_IMG
- en: Examples of agent state in a 3D world
  prefs: []
  type: TYPE_NORMAL
- en: In the diagram, we can see the agent, the blue dot in the center of Visual Hallway
    environment in Unity with the ML-Agents toolkit. We will review an example of
    this environment shortly, so don't worry about reviewing it just yet. You can
    see from the diagram how the agent is observing different observations of state
    from the same physical position using an agent camera. An agent camera is the
    vision we give to the agent to observe the world.
  prefs: []
  type: TYPE_NORMAL
- en: From this camera, the agent ingests the state as a visual observation that is
    fed as an image into a deep learning network. This image is broken up with 2D
    convolutional neural network layers into features, which the agent learns. The
    problem is that we are using 2D filters to try and digest 3D information. In [Chapter
    7](42d53358-6f57-4f67-96ce-d8587cbe7cc5.xhtml), *Going Deeper with DDQN*, we explored
    using CNNs to ingest the image state from Atari games and, as we have seen, this
    works very well.
  prefs: []
  type: TYPE_NORMAL
- en: You will need the ML-Agents toolkit installed and should have opened the **UnitySDK**
    test project. If you need assistance with this, return to [Chapter 11](ab9a7f4f-60d8-4643-8627-199cf95bcf55.xhtml),
    *Exploiting ML-Agents, *and follow some of the exercises there first.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unity does the same thing for its agent camera setup, and in the next exercise
    we will see how the following looks:'
  prefs: []
  type: TYPE_NORMAL
- en: Locate the folder at `ml-agents/ml-agents/mlagents/trainers` located in the
    ML-Agents repository. If you need help pulling the repository, follow the previous
    information tip given.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From this folder, locate and open the `models.py` file in a text or Python IDE.
    ML-Agents is written in TensorFlow, which may be intimidating at first, but the
    code follows many of the same principles as PyTorch.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Around line 250 a `create_visual_observation_encoder` function from the `LearningModel` base
    class is created. This is the base class model that ML-Agents, the PPO, and SAC
    implementations use.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ML-Agents was originally developed in Keras and then matured to TensorFlow in
    order to improve performance. Since that time, PyTorch has seen a huge surge in
    popularity for academic researchers, as well as builders. At the time of writing,
    PyTorch is the fastest growing DL framework. It remains to be seen if Unity will
    also follow suit and convert the code to PyTorch, or just upgrade it to TensorFlow
    2.0.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `create_visual_observation_encoder` function is the base function for encoding
    state, and the full definition of the function (minus comments) is shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'While the code is in TensorFlow, there are a few obvious indicators of common
    terms, such as layers and conv2d. With that information, you can see that this
    encoder uses two CNN layers: one with a kernel size of 8 x 8, a stride of 4 x
    4, and 16 filters; followed by a second layer that uses a kernel size of 4 x 4,
    a stride of 2 x 2, and 32 filters.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Notice again the use of no pooling layers. This is because spatial information
    is lost when we use pooling between CNN layers. However, depending on the depth
    of the network, a single pooling layer near the top can be beneficial.
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice the return from the function is a hidden flat layer denoted by `hidden_flat`.
    Recall that our CNN layers are being used to learn state that is then fed into
    our learning network as the following diagram shows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/f158292d-be01-4052-a899-7c7353056078.png)'
  prefs: []
  type: TYPE_IMG
- en: Example network diagram
  prefs: []
  type: TYPE_NORMAL
- en: The preceding diagram is a simplified network diagram showing that the CNN layers
    flatten as they feed into the hidden middle layer. Flattening is converting that
    convolutional 2D data into a one-dimensional vector and then feeding that into
    the rest of the network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can see how the image source is defined by opening up the Unity editor to
    the **ML-Agents UnitySDK** project to the **VisualHallway** scene located in the
    `Assets/ML-Agents/Examples/Hallway/Scenes` folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Expand the first **VisualSymbolFinderArea** and select the **Agent** object
    in the **Hierarchy** window. Then, in the **Inspector** window, locate and double-click
    on the **Brain** to bring it up in the following window:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/e8ab9385-dd56-41f1-9f1c-f8e16e985c52.png)'
  prefs: []
  type: TYPE_IMG
- en: Inspecting the VisualHallwayLearning Brain
  prefs: []
  type: TYPE_NORMAL
- en: The important thing to note here is that the agent is set up to accept an image
    of size 84 x 84 pixels. That means the agent camera is sampled down to an image
    size matching the same pixel area. A relatively small pixel area for this environment
    works because of the lack of detail in the scene. If the detail increased, we
    would likely also need to increase the resolution of the input image.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we look at training the agent visually using the ML-Agents
    toolkit.
  prefs: []
  type: TYPE_NORMAL
- en: Training a visual agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unity develops a 2D and 3D gaming engine/platform that has become the most popular
    platform for building games. Most of these games are the 3D variety, hence the
    specialized interest by Unity in mastering the task of agents that can tackle
    more 3D natural worlds. It naturally follows then that Unity has invested substantially
    into this problem and has/is working with DeepMind to develop this further. How
    this collaboration turns out remains to be seen, but one thing is for certain
    is that Unity will be our go-to platform for exploring 3D agent training.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next exercise, we are going to jump back into Unity and look at how
    we can train an agent in a visual 3D environment. Unity is arguably the best place
    to set up and build these type of environments as we have seen in the earlier
    chapters. Open the Unity editor and follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Open the **VisualHallway** scene located in the `Assets/ML-Agents/Examples/Hallway/Scenes`
    folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Locate the **Academy** object in the scene hierarchy window and set the **Control**
    option to enabled on the **Hallway Academy** component **Brains** section and
    as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/8d787f4a-007b-4d0f-9960-e166f925ad35.png)'
  prefs: []
  type: TYPE_IMG
- en: Setting the academy to control the learning brain
  prefs: []
  type: TYPE_NORMAL
- en: This sets the Academy to control the Brain for the agent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, select all the **VisualSymbolFinderArea** objects from **(1)** to **(7)**
    and then make sure to enable them all by clicking the object''s **Active** option
    in the Inspector window, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/e657e8c6-2c23-473a-94c7-2d98d97914e8.png)'
  prefs: []
  type: TYPE_IMG
- en: Enabling all the sub-environments in the scene
  prefs: []
  type: TYPE_NORMAL
- en: This enables all the sub environment areas and allows us to run an additional
    seven agents when training. As we have seen when using actor-critic methods, being
    able to sample more efficiently from the environment has many advantages. Almost
    all the example ML-Agents environments provide for multiple sub-training environments.
    These multiple environments are considered separate environments but allow for
    the brain to be trained synchronously with multiple agents.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Save the scene and the project file from the **File** menu.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open a new Python or Anaconda shell and set the virtual environment to use the
    one you set up earlier for ML-Agents. If you need help, refer to [Chapter 11](ab9a7f4f-60d8-4643-8627-199cf95bcf55.xhtml),
    *Exploiting ML-Agents*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Navigate to the Unity `ml-agents` folder and execute the following command
    to start training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This will start the Python trainer, and after a few seconds, will prompt you
    to click Play in the editor. After you do that, the agents in all the environments
    will begin training and you will be able to visualize this in the editor. An example
    of how this looks in the command shell is shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/48f2e1f1-d14a-4bc4-a4b5-7ec0a648f834.png)'
  prefs: []
  type: TYPE_IMG
- en: Running the ML-Agents trainer
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have reviewed how to train an agent in Unity with ML-Agents, we
    can move on to explore some other undocumented training options in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: If you encounter problems training the Hallway environment, you can always try
    one of the other various environments. It is not uncommon for a few of the environments
    to become broken because of releases or version conflicts.
  prefs: []
  type: TYPE_NORMAL
- en: Generalizing 3D vision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As previously mentioned in [Chapter 11](ab9a7f4f-60d8-4643-8627-199cf95bcf55.xhtml),
    *Exploiting ML-Agents*, we saw how the team at Unity is one of the leaders in
    training agents for 3D worlds. After all, they do have a strong vested interest
    in providing an AI platform that developers can just plug into and build intelligent
    agents. Except, the very agents that fit this broad type of application are now
    considered the first step to AGI because if Unity can successfully build a universal
    agent to play any game, it will have effectively built a first-level AGI.
  prefs: []
  type: TYPE_NORMAL
- en: The problem with defining AGI is trying to understand how broad or general an
    intelligence has to be as well as how we quantify the agent's understanding of
    that environment and possible ability to transfer knowledge to other tasks. We
    really won't know how best to define what that is until someone has the confidence
    to stand up and claim to have developed an AGI. A big part of that claim will
    depend on how well an agent can generalize environmental state and a big part
    of that will be generalizing 3D vision itself.
  prefs: []
  type: TYPE_NORMAL
- en: Unity has an undocumented way to alter the type of visual encoder you can use
    in training on an environment (at least at time of writing).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next exercise, we look at how the hyperparameter can be added to the
    configuration and set for different visual encoders by following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Locate the `trainer_config.yaml` configuration file located in the `mlagents/ml-agents/config`
    folder and open it in an IDE or text editor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**YAML** is an acronym that stands for for **YAML ain''t markup language**.
    The format of the ML-Agents configuration markup files is quite similar to Windows
    INI configuration files of old.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This file defines the configuration for the various learning brains. Locate
    the section for the `VisualHallwayLearning` brain as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'These hyperparameters are additional to a set of base values set in a default
    brain configuration at the top of the config file and shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The hyperparameter of interest for us is the `vis_encode_type` value set to
    simple highlighted in the preceding code example. ML-Agents supports two additional
    types of visual encoding by changing that option like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`vis_enc_type`: Hyperparameter to set type of visual encoding:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`simple`: This is the default and the version we already looked at.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nature_cnn`: This defines a CNN architecture proposed by a paper in Nature,
    we will look at this closer shortly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resnet`: ResNet is a published CNN architecture that has been shown to be
    very effective at image classification.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will change the default value in our `VisualHallwayLearning` brain by adding
    a new line to the end of the brain''s configuration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we know how to set these, let''s see what they look like by opening
    the `models.py` code like we did earlier from the `ml-agents/trainers` folder.
    Scroll down past the `create_visual_observation_encoder` function to the `create_nature_cnn_observation_encoder`
    function shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The main difference with this implementation is the use of a third layer called
    `conv3`. We can see this third layer has a kernel size of 3 x 3, a stride of 1
    x 1 and 64 filters. With a smaller kernel and stride size, we can see this new
    layer is being used to extract finer features. How useful that is depends on the
    environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we want to look at the third visual encoding implementation listed just
    after the last function. The next function is `create_resent_visual_observation_encoder`
    and is shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: You can now go back and update the `vis_enc_type` hyperparameter in the config
    file and retrain the visual agent. Note which encoder is more successful if you
    have time to run both versions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We have seen what variations of visual encoders that ML-Agents supports and
    the team at Unity has also included a relatively new variant called ResNet. ResNet
    is an important achievement, and thus far, has shown to be useful for training
    agents in some visual environments. Therefore, in the next section, we will spend
    some extra time looking at ResNet.
  prefs: []
  type: TYPE_NORMAL
- en: ResNet for visual observation encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Convolutional layers have been used in various configurations for performing
    image classification and recognition tasks successfully for some time now. The
    problem we encounter with using straight 2D CNNs is we are essentially flattening
    state representations, but generally not in a good way. This means that we are
    taking a visual observation of a 3D space and flattening it to a 2D image that
    we then try and extract important features from. This results in an agent thinking
    it is in the same state if it recognizes the same visual features from potentially
    different locations in the same 3D environment. This creates confusion in the
    agent and you can visualize this by watching an agent just wander in circles.
  prefs: []
  type: TYPE_NORMAL
- en: The same type of agent confusion can often be seen happening due to vanishing
    or exploding gradients. We haven't encountered this problem very frequently because
    our networks have been quite shallow. However, in order to improve network performance,
    we often deepen the network by adding additional layers. In fact, in some vision
    classification networks, there could be 100 layers or more of convolution trying
    to extract all manner of features. By adding this many additional layers, we introduce
    the opportunity for vanishing gradients. A vanishing gradient is a term we use
    for a gradient that becomes so small as to appear to vanish, or really have no
    effect on training/learning. Remember that our gradient calculation requires a
    total loss that is then transferred back through the network. The more layers
    the loss needs to push back through the network, the smaller it becomes. This
    is a major issue in the deep CNN networks that we use for image classification
    and interpretation.
  prefs: []
  type: TYPE_NORMAL
- en: 'ResNet or residual CNN networks were introduced as a way of allowing for deeper
    encoding structures without suffering vanishing gradients. Residual networks are
    so named because they carry forth a residual identity called an **identity shortcut
    connection**. The following diagram, sourced from the *Deep Residual Learning
    for Image Recognition* paper, shows the basic components in a residual block:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/88c43406-c183-43a5-9841-a062dce3403f.png)'
  prefs: []
  type: TYPE_IMG
- en: A residual block
  prefs: []
  type: TYPE_NORMAL
- en: The intuition from the authors in the paper is that stacked layers shouldn't
    degrade network performance just because they are stacked. Instead, by pushing
    the output of the last layer to the layer ahead, we are effectively able to isolate
    training to individual layers. We refer to this as an **identity** because the
    size of the output from the last layer will likely not match the input of the
    next layer, since we are bypassing the middle layer. Instead, we multiply the
    output of the last layer with an identity input tensor in order to match the output
    to the input.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s jump back to the ResNet encoder implementation back in ML-Agents and
    see how this is done in the next exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Open the `models.py` file located in the `mlagents/ml-agents/trainers` folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Scroll down to the `create_resnet_visual_observation_encoder` function again.
    Look at the first two lines that define some variables for building up the residual
    network as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, scroll down a little more to where we enumerate the number of channels
    listed to build up each of the input layers. The code is shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The `n_channels` variable represents the number of channels or filters used
    in each of the input convolution layers. Thus, we are creating three groups of
    residual layers with an input layer and blocks in between. The blocks are used
    to isolate the training to each of the layers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Keep scrolling down, and we can see where the blocks are constructed between
    the layers with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This code creates a network structure similar to what is shown in the following
    diagram:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/078574c1-5e9c-4d9d-8858-1ef548849664.png)'
  prefs: []
  type: TYPE_IMG
- en: Diagram of the ResNet architecture in ML-Agents
  prefs: []
  type: TYPE_NORMAL
- en: In essence, we still only have three distinct convolutional layers extracting
    features, but each of those layers can now be trained independently. Furthermore,
    we can likely increase the depth of this network several times and expect an increase
    in visual encoding performance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go back and, if you have not already done so, train a visual agent with residual
    networks for visual observation encoding.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What you will likely find if you went back and trained another visual agent
    with residual networks is the agent performs marginally better, but they still
    can get confused. Again, this is more of a problem with the visual encoding system
    than the DRL itself. However, it is believed that once we can tackle visual encoding
    of visual environments, real AGI will certainly be a lot closer.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we look at a special environment that the team at Unity
    put together (with the help of Google DeepMind) in order to challenge DRL researchers,
    which is the very problem of visual encoding 3D worlds.
  prefs: []
  type: TYPE_NORMAL
- en: Challenging the Unity Obstacle Tower Challenge
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In late 2018, Unity, with the help of DeepMind, began development of a challenge
    designed to task researchers in the most challenging areas of DRL. The challenge
    was developed with Unity as a Gym interface environment and featured a game using
    a 3D first-person perspective. The 3D perspective is a type of game interface
    made famous with the likes of games such as Tomb Raider and Resident Evil, to
    name just a couple of examples. An example of the game interface is shown in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9134c7f0-ca58-413f-bc3d-fa8cd1de16be.png)'
  prefs: []
  type: TYPE_IMG
- en: Example the obstacle tower challenge
  prefs: []
  type: TYPE_NORMAL
- en: The Obstacle Tower Challenge is not only in 3D, but the patterns and materials
    in the rooms and on the walls change over the levels. This makes vision generalization
    even more difficult. Furthermore, the challenge poses multiple concurrent steps
    to complete tasks. That is, each level requires the character to find a door and
    open it. On more advancing levels, the doors require a special key to be activated
    or acquired, which makes this almost a multi-task RL problem—not a problem we
    have considered solving previously. Fortunately, as we demonstrated using ML-Agents
    Curiosity Learning, multi-step RL can be accomplished, provided the tasks are
    linearly connected. This means there is no branching or tasks that require decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-task reinforcement learning is quickly advancing in research but it is
    still a very complicated topic. The current preferred method to solve MTRL is
    called **meta reinforcement learning**. We will cover Meta Reinforcement Learning
    in [Chapter 14](a171ddfa-e639-4b4e-9652-4279b5ac872a.xhtml), *From DRL to AGI*,
    where we will talk about the next evolutions of DRL in the coming months and/or
    years.
  prefs: []
  type: TYPE_NORMAL
- en: For the next exercise, we are going to closely review the work of the winner
    of the Unity Obstacle Tower Challenge, Alex Nichol. Alex won the $100,000 challenge
    by entering a modified PPO agent that was pre-trained on classified images and
    human recorded demonstrations (behavioural cloning). He essentially won by better
    generalizing the agent's observations of state using a number of engineered solutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open up your Anaconda prompt and follow the next example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is recommended that you create a new virtual environment before installing
    any new code and environments. This can easily be done with Anaconda using the
    following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'First, you will need to download and install the Unity Obstacle Tower Challenge
    from this repository ([https://github.com/Unity-Technologies/obstacle-tower-env](https://github.com/Unity-Technologies/obstacle-tower-env))
    or just use the following commands from a new virtual environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the OTC environment is quite simple and can be done with this simple
    block of code that just performs random actions in the environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The code to run the OTC environment should be quite familiar by now, but does
    have one item to note. The agent cycles through episodes or lives, but the agent
    only has a certain number of lives. This environment simulates a real game, and
    hence, the agent only has a limited number of tries and time to complete the challenge.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, pull down the repository from Alex Nichol (`unixpickle`) here: [https://github.com/unixpickle/obs-tower2.git](https://github.com/unixpickle/obs-tower2.git),
    or check the `Chapter_13/obs-tower2` source folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Navigate to the folder and run the following command to install the required
    dependencies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, you need to configure some environment variables to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'How you set these environment variables will depend on your OS and at what
    level you want them set. For Windows users, you can set the environment variable
    using the **System Environment Variables** setup panel as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/75adffed-d9da-4814-ab62-c582c01ed82c.png)'
  prefs: []
  type: TYPE_IMG
- en: Setting the environment variables (Windows)
  prefs: []
  type: TYPE_NORMAL
- en: Now with everything set up, it is time to move on to pre-training the agent.
    We will cover that training in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Pre-training the agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have already covered a number of ways to manage training performance often
    caused by low rewards or rewards sparsity. This covered using a technique called
    behavioural cloning, whereby a human demonstrates a set of actions leading to
    a reward and those actions are then fed back into the agent as a pre-trained policy.
    The winning implementation here used a combination of behavioural cloning with
    pre-trained image classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will continue from where we left off in the last exercise and learn what
    steps we need to perform in order to pre-train a classifier first:'
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, we need to capture images of the environment in order to pre-train
    a classifier. This requires you to run the `record.py` script located at the `obs_tower2/recorder/record.py`
    folder. Make sure when running this script that your environment variables are
    configured correctly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The documentation or `README.md` on the repository is good but is only really
    intended for advanced users who are very interested in replicating results. If
    you do encounter issues in this walkthrough, refer back to that documentation.
  prefs: []
  type: TYPE_NORMAL
- en: Running the script will launch the Unity OTC and allow you as a player to interact
    with the game. As you play the game, the `record.py` script will record your moves
    as images after every episode. You will need to play several games in order to
    have enough training data. Alternatively, Alex has provided a number of recordings
    online at this location: [http://obstower.aqnichol.com/](http://obstower.aqnichol.com/).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Note:**'
  prefs: []
  type: TYPE_NORMAL
- en: The recordings and labels are both in tar files with the recordings weighing
    in at 25 GB.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we need to label the recorded images in order to assist the agent in classification.
    Locate and run the `main.py` script located in the `obs_tower2/labeler/` folder.
    This will launch a web application. As long as you have your paths set correctly,
    you can now open a browser and go to `http://127.0.0.1:5000` (localhost, port
    `5000`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You will now be prompted to label images by using the web interface. For each
    image, classify the state as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/4863d210-ccdd-417c-a78f-d796b60ecab9.png)'
  prefs: []
  type: TYPE_IMG
- en: Labeling image data for classification
  prefs: []
  type: TYPE_NORMAL
- en: Alex notes in his original documentation that he could label 20-40 images per
    second after some practice. Again, if you want to avoid this step, just download
    the tar files containing his example recordings and labels.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, you will need to run that classifier with the training input images and
    labels you either just generated or downloaded. Run the classified by executing
    the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: After the classification is done, the results will be output to a `save_classifier.pk1` file
    periodically. The whole process may take several hours to train completely.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'With the pre-classifier built, we can move to behavioral cloning using the
    human sample playing. This means you will used the saved and pre-labelled sessions
    as inputs for later agent training. You can start the process by running the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this script generates periodic output to a `save_clone.pkl` file and
    the whole script can take a day or more to run. When the script is complete, copy
    the output to a `save_prior.pkl` file like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This creates a prior set of recordings or memories we will use to train the
    agent in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Prierarchy – implicit hierarchies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Alex used the notion of hierarchical reinforcement learning in order to tackle
    the problem of multi-task agent learning that OTC requires you to solve. HRL is
    another method outside Meta-RL that has been used to successfully solve multi-task
    problems. Prierarchy-RL refines this by building a prior hierarchy that allows
    an action or action-state to be defined by entropy or uncertainty. High entropy
    or highly uncertain actions become high level or top-based actions. This is someone
    abstract in concept, so let''s look at a code example to see how this comes together:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The base agent used to win the challenge was PPO; following is a full source
    listing of that agent and a refresher to PPO:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Familiarize yourself with the differences between this implementation and what
    we covered for PPO. Our example was simplified for explanation purposes but follows
    the same patterns.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Pay particular attention to the code in `inner_loop` and understand how this
    works:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Open the `prierarchy.py` file located in the root `obs_tower2` folder and as
    shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: What we see here is the `Pierarchy` class, an extension to `PPO`, which works
    by extending the `inner_loop` function. Simply, this code refines the KL-Divergence
    calculation that allowed us to secure that spot on the hill without falling off.
    Recall this was our discussion of the clipped objective function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Notice the use of the `prior` policy or the policy that was generated based
    on the pre-training and behavioral cloning done earlier. This prior policy defines
    if actions are high or low in uncertainty. That way, an agent can actually use
    the prior hierarchy or prierarchy to select a series of high and then lower entropy/uncertain
    actions.  The following diagram illustrates how this effectively works:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/f367f2af-2a1b-45e6-973f-b8dd7cf7a3bc.png)'
  prefs: []
  type: TYPE_IMG
- en: Agent selecting action based on entropy hierarchy
  prefs: []
  type: TYPE_NORMAL
- en: Thus, instead of deciding when and if to explore, the agent decides random actions
    based on their hierarchy or uncertainty. This means that higher-level actions
    can be reduced in uncertainty quickly because each successive action has less
    uncertainty.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A helpful example when trying to understand Priercarchy is the movie *Groundhog
    Day*, starring Bill Murray. In the movie, the character continually cycles through
    the same day, attempting by trial and error to find the optimum path to break
    out of the path. In the movie, we can see the character try thousands, perhaps
    millions, of different combinations, but we see this done in hierarchical steps.
    We first see the character wildly going about his day never accomplishing anything,
    until he learns through past hierarchical actions what are the best possible rewards.
    He learns that by improving on himself, his time in eternity becomes more pleasant.
    In the end, we see the character try to live their best life, only to discover
    they solved the game and can move on to the next day.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can train the agent by running the following command on the first 10 levels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, to train the agent on floors greater than 10, you can use the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Every 10 levels in the OTC, the game theme changes. This means the wall color
    and textures will change as well as the tasks that need to get completed. As we
    mentioned earlier, this visual change, combined with 3D, will make the Unity OTC
    one of the most difficult and benchmark challenges to beat when we first get smart/bold
    and/or brave enough to tackle AGI. AGI and the road to more general intelligence
    with DRL will be our focus for [Chapter 14](a171ddfa-e639-4b4e-9652-4279b5ac872a.xhtml),
    *From DRL to AGI*.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we look at 3D world Habitat by Facebook, which is more
    difficult but equally fun.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring Habitat – embodied agents by FAIR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Habitat is a relatively new entry by Facebook AI Research for a new form of
    embodied agents. This platform represents the ability to represent full 3D worlds
    displayed from real-world complex scenes. The environment is intended for AI research
    of robots and robotic-like applications that DRL will likely power in the coming
    years. To be fair though, pun intended, this environment is implemented to train
    all forms of AI on this type of environment. The current Habitat repository only
    features some simple examples and implementation of PPO.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Habitat platform comes in two pieces: the Habitat Sim and Habitat API.
    The simulation environment is a full 3D powered world that can render at thousands
    of frames per second, which is powered by photogrammetry RGBD data. RGBD is essentially
    RGB color data plus depth. Therefore, any image taken will have a color value
    and depth. This allows the data to be mapped in 3D as a hyper-realistic representation
    of the real environment. You can explore what one of these environments look like
    by using Habitat itself in your browser by following the next quick exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate your browser to [https://aihabitat.org/demo/.](https://aihabitat.org/demo/)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Habitat will currently only run in Chrome or on your desktop.
  prefs: []
  type: TYPE_NORMAL
- en: 'It may take some time to load the app so be patient. When the app is loaded,
    you will see something like the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/ebbec784-c61d-4fa2-aa67-1cca633cf1d8.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of Habitat running in the browser
  prefs: []
  type: TYPE_NORMAL
- en: Use the WASD keys to move around in the environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Habitat supports importing from the following three vendors: [MatterPort3D](https://niessner.github.io/Matterport/),
    [Gibson](http://gibsonenv.stanford.edu/database/), and [Replica](https://github.com/facebookresearch/Replica-Dataset),
    who produce tools and utilities to capture RGBD data and have libraries of this
    data. Now that we understand what Habitat is, we will set it up in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Habitat
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At the time of writing, Habitat was still a new product, but the documentation
    worked well to painlessly install and run an agent for training. In our next exercise,
    we walk through parts of that documentation to install and run a training agent
    in Habitat:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open an Anaconda command prompt and navigate to a clean folder. Use the following
    commands to download and install the Habitat:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, create a new virtual environment and install the required dependencies
    with the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to build the Habitat Sim with the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Download the test scenes from the following link: [http://dl.fbaipublicfiles.com/habitat/habitat-test-scenes.zip](http://dl.fbaipublicfiles.com/habitat/habitat-test-scenes.zip).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unzip the scene files into a familiar path, one that you can link to later.
    These files are sets of RGBD data that represent the scenes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: RGBD image capture is not new, and traditionally, it has been expensive since
    it requires moving a camera equipped with a special sensor around a room. Thankfully,
    most modern cell phones also feature this depth sensor. This depth sensor is often
    used to build augmented reality applications now. Perhaps in a few years, agents
    themselves will be trained to capture these types of images using just a simple
    cell phone.
  prefs: []
  type: TYPE_NORMAL
- en: 'After everything is installed, we can test the Habitat installation by running
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: That will launch the Sim in non-interactive fashion and play some random moves.
    If you want to see or interact with the environment, you will need to download
    and install the interactive plugin found in the repository documentation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After the Sim is installed, we can move on to installing the API and training
    an agent in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Training in Habitat
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At the time of writing, Habitat was quite new but showed amazing potential,
    especially for training agents. This means the environment currently only has
    a simple and PPO agent implementation in which you can quickly train agents. Of
    course, since Habitat uses PyTorch, you could probably implement one of the other
    algorithms we have covered. In the next exercise, we finish off by looking at
    the PPO implementation in Habitat and how to run it:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Download and install the Habitat API with the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, you can use the API in a number of ways. We will first look
    at a basic code example you could write to run the Sim:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the Sim allows us to program an agent using the same familiar
    Gym style interface we are used to.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we need to install the Habitat Baselines package. This package is the
    RL portion and currently provides an example of PPO. The package is named Baselines
    after the OpenAI testing package of the same name.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Install the Habitat Baselines package using the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'After the installation, you can run the `run.py` script in order to train an
    agent with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, you can test this agent with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Habitat is a fairly recent development and opens the door to training agents/robots
    in real-world environments. While Unity and ML-Agents are great platforms for
    training agents in 3D game environments, they still do not compare to the complexity
    of the real world. In the real world, objects are rarely perfect and are often
    very complex, which makes these environments especially difficult to generalize,
    and therefore, train on. In the next section, we finish the chapter with our typical
    exercises.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we progressed through this book, the exercises have morphed from learning
    exercises to almost research efforts, and that is the case in this chapter. Therefore,
    the exercises in this chapter are meant for the hardcore RL enthusiast and may
    not be for everyone:'
  prefs: []
  type: TYPE_NORMAL
- en: Tune the hyperparameters for one of the sample visual environments in the ML-Agents
    toolkit.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Modify the visual observation standard encoder found in the ML-Agents toolkit
    to include additional layers or different kernel filter settings.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train an agent with `nature_cnn` or `resnet` visual encoder networks and compare
    their performance with earlier examples using the base visual encoder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Modify the `resnet` visual encoder to accommodate many more layers or other
    variations of filter/kernel size.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download, install, and play the Unity Obstacle Tower Challenge and see how far
    you can get in the game. As you play, think of yourself as an agent and reflect
    on what actions you are taking and how they reflect your current task trajectory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build your own implementation of an algorithm to test against the Unity OTC.
    Completing this challenge will be especially rewarding if you beat the results
    of the previous winner. This challenge is still somewhat open and anyone claiming
    to do higher than level 20 will probably make a big impact on DRL in the future.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Replace the PPO base example in the Habitat Baselines module with an implementation
    of Rainbow DQN. How does the performance compare?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implement a different visual encoder for the Habitat Baselines framework. Perhaps
    use the previous examples of `nature_cnn` or `resnet`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compete in the Habitat Challenge. This is a challenge that requires an agent
    to complete a navigation task through a series of waypoints. It's certainly not
    as difficult as the OTC, but the visual environment is far more complex.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Habitat is intended more for sensor development instead of visual development.
    See if you are able to combine visual observation encoding with other sensor input
    as a type of combined visual and sensor observation input.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The exercises in this chapter are intended to be entirely optional; please choose
    to do these only if you have a reason to do so. They likely will require additional
    time as this is a very complex area to develop in.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored the concept of 3D worlds for not only games but
    the real world. The real world, and to a greater extent 3D worlds, are the next
    great frontier in DRL research. We looked at why 3D creates nuances for DRL that
    we haven't quite figured out how best to solve. Then, we looked at using 2D visual
    observation encoders but tuned for 3D spaces, with variations in the Nature CNN
    and ResNet or residual networks. After that, we looked at the Unity Obstacle Tower
    Challenge, which challenged developers to build an agent capable of solving the
    3D multi-task environment.
  prefs: []
  type: TYPE_NORMAL
- en: From there, we looked at the winning entries use of Prierarchy; a form of HRL
    in order to manage multiple task spaces. We also looked at the code in detail
    to see how this reflected in the winners modified PPO implementation. Lastly,
    we finished the chapter by looking at Habitat; an advanced AI environment that
    uses RGBD and depth based color data, to render real-world environments in 3D.
  prefs: []
  type: TYPE_NORMAL
- en: We are almost done with our journey, and in the next and final chapter, we will
    look at how DRL is moving toward artificial general intelligence, or what we refer
    to as AGI.
  prefs: []
  type: TYPE_NORMAL
