<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-76"><a id="_idTextAnchor113" class="calibre5 pcalibre1 pcalibre"/>4</h1>
<h1 id="_idParaDest-77" class="calibre4"><a id="_idTextAnchor114" class="calibre5 pcalibre1 pcalibre"/>Streamlining Text Preprocessing Techniques for Optimal NLP Performance</h1>
<p class="calibre6">Text preprocessing stands<a id="_idIndexMarker328" class="calibre5 pcalibre1 pcalibre"/> as a vital initial step in the realm of <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>). It encompasses converting raw, unrefined text<a id="_idIndexMarker329" class="calibre5 pcalibre1 pcalibre"/> data into a format that machine learning algorithms can readily comprehend. To extract meaningful insights from textual data, it is essential to clean, normalize, and transform the data into a more structured form. This chapter provides an overview of the most commonly used text preprocessing<a id="_idIndexMarker330" class="calibre5 pcalibre1 pcalibre"/> techniques, including tokenization, stemming, lemmatization, stop word removal, and <strong class="bold">part-of-speech</strong> (<strong class="bold">POS</strong>) tagging, along with their advantages and limitations.</p>
<p class="calibre6">Effective text preprocessing is essential for various NLP tasks, including sentiment analysis, language translation, and information retrieval. By applying these techniques, raw text data can be transformed into a structured and normalized format that can be easily analyzed using statistical and machine learning methods. However, selecting the appropriate preprocessing techniques can be challenging since the optimal methods depend on the specific task and dataset at hand. Therefore, it is important to carefully evaluate and compare different text preprocessing techniques to determine the most effective approach for a given application.</p>
<p class="calibre6">The following topics will be covered in this chapter:</p>
<ul class="calibre14">
<li class="calibre15">Lowercasing in N<a id="_idTextAnchor115" class="calibre5 pcalibre1 pcalibre"/>LP</li>
<li class="calibre15">Removing special characters and punctuations</li>
<li class="calibre15">Removing stop words</li>
<li class="calibre15">Named entity recognition (NER)</li>
<li class="calibre15">POS tagging</li>
<li class="calibre15">Explaining the preprocessing pipeline</li>
</ul>
<h1 id="_idParaDest-78" class="calibre4"><a id="_idTextAnchor116" class="calibre5 pcalibre1 pcalibre"/>Technical requirements</h1>
<p class="calibre6">To follow along with the examples and exercises in this chapter on text preprocessing, you will need a working knowledge of a programming language such as Python, as well as some familiarity with NLP concepts. You will also need to have certain libraries installed, such as <strong class="bold">Natural Language Toolkit</strong> (<strong class="bold">NLTK</strong>), <strong class="bold">spaCy</strong>, and <strong class="bold">scikit-learn</strong>. These libraries provide powerful tools for text preprocessing and feature extraction. It is recommended that you have access to a <strong class="bold">Jupyter Notebook</strong> environment or another interactive coding environment to facilitate experimentation and exploration. Additionally, having a sample dataset to work with can help you understand the various techniques and their effects on text data.</p>
<p class="calibre6">Text normalization is the process of transforming text into a standard form to ensure consistency and reduce variations. Different techniques are used for normalizing text, including lowercasing, removing special characters, spell checking, and stemming or lemmatization. We will explain these steps in detail, and how to use them, with code examples.</p>
<h1 id="_idParaDest-79" class="calibre4"><a id="_idTextAnchor117" class="calibre5 pcalibre1 pcalibre"/>Lowercasing in NLP</h1>
<p class="calibre6">Lowercasing is a common text preprocessing technique<a id="_idIndexMarker331" class="calibre5 pcalibre1 pcalibre"/> that’s used in NLP to standardize text and reduce the complexity of vocabulary. In this technique, all the text is converted into lowercase characters.</p>
<p class="calibre6">The main purpose of lowercasing is to make the text uniform and avoid any discrepancies that may arise from capitalization. By converting all the text into lowercase, the machine learning algorithms can treat the same words that are capitalized and non-capitalized as the same, reducing the overall vocabulary size and making the text easier to process.</p>
<p class="calibre6">Lowercasing is particularly useful for tasks such as text classification, sentiment analysis, and language modeling, where the meaning of the text is not affected by the capitalization of the words. However, it may not be suitable for certain tasks, such as NER, where capitalization can be an important feature.</p>
<h1 id="_idParaDest-80" class="calibre4"><a id="_idTextAnchor118" class="calibre5 pcalibre1 pcalibre"/>Removing special characters and punctuation</h1>
<p class="calibre6">Removing special characters<a id="_idIndexMarker332" class="calibre5 pcalibre1 pcalibre"/> and punctuation<a id="_idIndexMarker333" class="calibre5 pcalibre1 pcalibre"/> is an important step in text preprocessing. Special characters and punctuation marks do not add much meaning to the text and can cause issues for machine learning models if they are not removed. One way to perform this task is by using regular expressions, such as the following:</p>
<pre class="source-code">
re.sub(r"[^a-zA-Z0-9]+", "", string)</pre> <p class="calibre6">This will remove non-characters and numbers from our input string. Sometimes, there may be special characters that we would want to replace with a whitespace. Take a look at the following examples:</p>
<ul class="calibre14">
<li class="calibre15">president-elect</li>
<li class="calibre15">body-type</li>
</ul>
<p class="calibre6">In these two examples, we would want to replace the “-” with whitespace, as follows:</p>
<ul class="calibre14">
<li class="calibre15">President elect</li>
<li class="calibre15">Body type</li>
</ul>
<p class="calibre6">Next, we’ll cover stop word removal.</p>
<h2 id="_idParaDest-81" class="calibre7"><a id="_idTextAnchor119" class="calibre5 pcalibre1 pcalibre"/>Stop word removal</h2>
<p class="calibre6">Stop words are words<a id="_idIndexMarker334" class="calibre5 pcalibre1 pcalibre"/> that do not contribute much to the meaning of a sentence or piece of text, and therefore can be safely removed without us losing much information. Examples of stop words include “a,” “an,” “the,” “and,” “in,” “at,” “on,” “to,” “for,” “is,” “are,” and so on.</p>
<p class="calibre6">Stop word removal is a common text preprocessing <a id="_idIndexMarker335" class="calibre5 pcalibre1 pcalibre"/>step that is performed<a id="_idIndexMarker336" class="calibre5 pcalibre1 pcalibre"/> before any text analysis<a id="_idIndexMarker337" class="calibre5 pcalibre1 pcalibre"/> tasks, such as <strong class="bold">sentiment analysis</strong>, <strong class="bold">topic modeling</strong>, or <strong class="bold">information retrieval</strong>. The goal is to reduce<a id="_idIndexMarker338" class="calibre5 pcalibre1 pcalibre"/> the size of the vocabulary and the dimensionality of the feature space, which can improve the efficiency and effectiveness of subsequent analysis steps.</p>
<p class="calibre6">The process of stop word removal involves identifying a list of stop words (usually predefined or learned from a corpus), tokenizing the input text into words or tokens, and then removing any words that match the stop word list. The resulting text consists of only the important words that carry the meaning of the text.</p>
<p class="calibre6">Stop word removal can be performed using various programming languages, tools, and libraries. For example, NLTK, which is a popular Python library for NLP, provides a list of stop words for various languages, as well as a method for removing stop words from text.</p>
<p class="calibre6">Here’s an example of stop word removal:</p>
<p class="calibre6"><em class="italic">This is a sample sentence demonstrating stop </em><em class="italic">word filtration.</em></p>
<p class="calibre6">After performing stop word removal, we get the following output:</p>
<p class="calibre6"><em class="italic">Sample sentence demonstrating stop </em><em class="italic">word filtration</em></p>
<p class="calibre6">This chapter contains Python code dedicated to this. You can refer to it for each of the actions that are described in this chapter.</p>
<p class="calibre6">As we can see, the stop<a id="_idIndexMarker339" class="calibre5 pcalibre1 pcalibre"/> words “This,” “is,” and “a,” have been removed from the original sentence, leaving only the important words.</p>
<h3 class="calibre8">Spell checking and correction</h3>
<p class="calibre6">Spell checking and correction involves<a id="_idIndexMarker340" class="calibre5 pcalibre1 pcalibre"/> correcting misspelled words in the text. This is important because misspelled words can cause inconsistencies in the data and affect the accuracy of algorithms. For example, take a look at the following sentence:</p>
<p class="calibre6"><em class="italic">I am going to </em><em class="italic">the bakkery</em></p>
<p class="calibre6">This would be transformed into the following:</p>
<p class="calibre6"><em class="italic">I am going to </em><em class="italic">the bakery</em></p>
<p class="calibre6">Let’s move on to lemmatization.</p>
<h3 class="calibre8">Lemmatization</h3>
<p class="calibre6"><strong class="bold">Lemmatization</strong> is a text normalization approach <a id="_idIndexMarker341" class="calibre5 pcalibre1 pcalibre"/>that aims to simplify a word to its base<a id="_idIndexMarker342" class="calibre5 pcalibre1 pcalibre"/> or dictionary form, referred to as a lemma. The primary objective of lemmatization is to aggregate various forms of the same word, facilitating their analysis as a unified term.</p>
<p class="calibre6">For example, consider the following sentence:</p>
<p class="calibre6"><em class="italic">Three cats were chasing the mice in the fields, while one cat watched </em><em class="italic">one mouse.</em></p>
<p class="calibre6">In the context of this sentence, “cat” and “cats” are two different forms of the same word, and “mouse” and “mice” are also two different forms of the same word. Lemmatization would reduce these words to their base forms:</p>
<p class="calibre6"><em class="italic">the cat be chasing the mouse in the field, while one cat watched </em><em class="italic">one mouse.</em></p>
<p class="calibre6">In this case, “cat” and “cats” have both been reduced to their base form of “cat,” and “mouse” and “mice” have both been reduced to their base form of “mouse.” This allows for better analysis of the text since the occurrences of “cat” and “mouse” are now treated as the same term, regardless of their inflectional variations.</p>
<p class="calibre6">Lemmatization is different from stemming, which involves reducing a word to a common stem that may not necessarily be a word in its own right. For example, the stem of “cats” and “cat” would both be “cat.” The lemma of “cats” and “cat” would be “cat” as well.</p>
<p class="calibre6">Lemmatization can be performed<a id="_idIndexMarker343" class="calibre5 pcalibre1 pcalibre"/> using various NLP libraries and tools, such as NLTK, spaCy, and Stanford CoreNLP.</p>
<h3 class="calibre8">Stemming</h3>
<p class="calibre6">Stemming involves reducing words<a id="_idIndexMarker344" class="calibre5 pcalibre1 pcalibre"/> to their fundamental or root form, referred to as the “stem.” This process<a id="_idIndexMarker345" class="calibre5 pcalibre1 pcalibre"/> is commonly used in NLP to prepare text for analysis, retrieval, or storage. Stemming algorithms work by cutting off the ends or suffixes of words, leaving only the stem.</p>
<p class="calibre6">The goal of stemming is to convert all inflected or derived forms of a word into a common base form. For example, the stem of the word “running” is “run,” and the stem of the word “runs” is also “run.”</p>
<p class="calibre6">One commonly used stemming algorithm is the Porter stemming algorithm. This algorithm is based on a series of rules that identify suffixes and remove them from words to obtain the stem. For example, the Porter algorithm would convert the word “leaping” into “leap” by removing the “ing” suffix.</p>
<p class="calibre6">Let’s look at an example sentence to see stemming in action:</p>
<p class="calibre6"><em class="italic">They are running and leaping across </em><em class="italic">the walls</em></p>
<p class="calibre6">Here’s the stemmed text (using the Porter algorithm):</p>
<p class="calibre6"><em class="italic">They are run and leap across </em><em class="italic">the wall</em></p>
<p class="calibre6">As you can see, the words “running” and “leaping” have been converted into their base forms of “run” and “leap,” respectively, and the suffix “s” has been removed from “walls.”</p>
<p class="calibre6">Stemming can be useful for text analysis tasks such as information retrieval or sentiment analysis as it reduces the number of unique words in a document or corpus and can help to group similar words. However, stemming can also introduce errors as it can sometimes produce stems that are not actual words or produce stems that are not the intended base form of the word. For example, the stemmer might produce “walk” as the stem for both “walked” and “walking,” even though “walk” and “walked” have different<a id="_idIndexMarker346" class="calibre5 pcalibre1 pcalibre"/> meanings. Therefore, it’s important to evaluate the results of stemming to ensure that it is producing accurate and useful results.</p>
<h1 id="_idParaDest-82" class="calibre4"><a id="_idTextAnchor120" class="calibre5 pcalibre1 pcalibre"/>NER</h1>
<p class="calibre6">NER is an NLP technique that’s designed<a id="_idIndexMarker347" class="calibre5 pcalibre1 pcalibre"/> to detect and categorize named entities within text, including but not limited to person’s names, organization’s names, locations, and more. NER’s primary objective is to autonomously identify and extract information about these named entities from unstructured text data.</p>
<p class="calibre6">NER typically involves<a id="_idIndexMarker348" class="calibre5 pcalibre1 pcalibre"/> using machine learning models, such as <strong class="bold">conditional random fields</strong> (<strong class="bold">CRFs</strong>) or <strong class="bold">recurrent neural networks</strong> (<strong class="bold">RNNs</strong>), to tag words in a given sentence<a id="_idIndexMarker349" class="calibre5 pcalibre1 pcalibre"/> with their corresponding entity types. The models are trained on large annotated datasets that contain text with labeled entities. These models then use context-based rules to identify named entities in new text.</p>
<p class="calibre6">There are several categories<a id="_idIndexMarker350" class="calibre5 pcalibre1 pcalibre"/> of named entities that can be identified by NER, including the following:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Person</strong>: A named individual, such as “Barack Obama”</li>
<li class="calibre15"><strong class="bold">Organization</strong>: A named company, institution, or organization, such as “Google”</li>
<li class="calibre15"><strong class="bold">Location</strong>: A named place, such as “New York City”</li>
<li class="calibre15"><strong class="bold">Date</strong>: A named date or time, such as “January 1, 2023”</li>
<li class="calibre15"><strong class="bold">Product</strong>: A named product or brand, such <a id="_idIndexMarker351" class="calibre5 pcalibre1 pcalibre"/>as “iPhone”</li>
</ul>
<p class="calibre6">Here’s an example of how NER<a id="_idIndexMarker352" class="calibre5 pcalibre1 pcalibre"/> works. Take a look at the following sentence:</p>
<p class="calibre6"><em class="italic">Apple Inc. is a technology company headquartered in </em><em class="italic">Cupertino, California.</em></p>
<p class="calibre6">Here, NER would identify “Apple Inc.” as an organization and “Cupertino, California” as a location. The output of an NER system could be a structured representation of the sentence, as shown here:</p>
<pre class="source-code">
{"organization": "Apple Inc.",
"location": "Cupertino, California"}</pre> <p class="calibre6">NER has many applications<a id="_idIndexMarker353" class="calibre5 pcalibre1 pcalibre"/> in various fields, including <strong class="bold">information retrieval</strong>, <strong class="bold">question-answering</strong>, <strong class="bold">sentiment analysis</strong>, and more. It can be used to automatically extract structured information from unstructured text data, which can be further analyzed or used for downstream tasks.</p>
<p class="calibre6">There are different approaches and tools to perform NER, but the general steps when performing NER are as follows:</p>
<ol class="calibre16">
<li class="calibre15"><strong class="bold">Data collection</strong>: The first step is to collect<a id="_idIndexMarker354" class="calibre5 pcalibre1 pcalibre"/> the data that will be used for NER. This data can be in the form of unstructured text, such as articles, social media posts, or web pages.</li>
<li class="calibre15"><strong class="bold">Preprocessing</strong>: The next step is to preprocess<a id="_idIndexMarker355" class="calibre5 pcalibre1 pcalibre"/> the data, which involves various steps such as tokenization, stop word removal, stemming or lemmatization, and normalization.</li>
<li class="calibre15"><strong class="bold">Labeling</strong>: After preprocessing, the next step<a id="_idIndexMarker356" class="calibre5 pcalibre1 pcalibre"/> is to label the data with named entity tags. There are different tagging schemes, but one<a id="_idIndexMarker357" class="calibre5 pcalibre1 pcalibre"/> of the most commonly used is the <strong class="bold">Inside-Outside-Beginning</strong> (<strong class="bold">IOB</strong>) tagging scheme. In this scheme, each word in the text is labeled as either <strong class="bold">B</strong> (<strong class="bold">beginning of a named entity</strong>), <strong class="bold">I</strong> (<strong class="bold">inside of a named entity</strong>), or <strong class="bold">O</strong> (<strong class="bold">outside of a </strong><strong class="bold">named entity</strong>).</li>
<li class="calibre15"><strong class="bold">Training</strong>: Once the data has been<a id="_idIndexMarker358" class="calibre5 pcalibre1 pcalibre"/> labeled, the next step is to train a machine learning model to recognize named entities in new, unseen text. Different types of models can be used for NER, such as rule-based systems, statistical models, and deep learning models.</li>
<li class="calibre15"><strong class="bold">Evaluation</strong>: After training the model, it is important<a id="_idIndexMarker359" class="calibre5 pcalibre1 pcalibre"/> to evaluate its performance on a test dataset. This can help identify any issues with the model, such as overfitting, underfitting, or bias.</li>
<li class="calibre15"><strong class="bold">Deployment</strong>: Finally, the trained model can be deployed<a id="_idIndexMarker360" class="calibre5 pcalibre1 pcalibre"/> to perform NER on new, unseen text. This can be done in real time or in batch mode, depending on the application’s requirements.</li>
</ol>
<p class="calibre6">Here’s an example of how NER can be performed:</p>
<p class="calibre6">Original text:</p>
<p class="calibre6"><em class="italic">Apple is negotiating to buy a Chinese start-up </em><em class="italic">this year.</em></p>
<p class="calibre6">Preprocessed text:</p>
<p class="calibre6"><em class="italic">apple negotiating buy Chinese </em><em class="italic">start-up year</em></p>
<p class="calibre6">Tagged text:</p>
<p class="calibre6"><em class="italic">B-ORG   O   O   B-LOC   O   O</em></p>
<p class="calibre6">In this example, the named <a id="_idIndexMarker361" class="calibre5 pcalibre1 pcalibre"/>entities “Apple” and “Chinese” are identified as an organization (B-ORG) and a location (B-LOC), respectively. “this year” is not recognized as a named entity in this example, but it would be if a more complex tagging scheme is used or if the model is trained on data that would promote that.</p>
<p class="calibre6">Several libraries can be used for NER, depending on the programming language and specific needs of the project. Let’s take a look at some commonly used libraries:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">spaCy</strong>: <strong class="bold">spaCy</strong> is a widely used open source library<a id="_idIndexMarker362" class="calibre5 pcalibre1 pcalibre"/> designed for various NLP<a id="_idIndexMarker363" class="calibre5 pcalibre1 pcalibre"/> tasks, including NER. Offering pre-trained models across multiple languages, the library additionally empowers users to undertake model training for distinct domains tailored to their specific needs.</li>
<li class="calibre15"><strong class="bold">NLTK</strong>: This is another widely used library<a id="_idIndexMarker364" class="calibre5 pcalibre1 pcalibre"/> for NLP tasks, including<a id="_idIndexMarker365" class="calibre5 pcalibre1 pcalibre"/> NER. It provides several pre-trained models and also allows users to train their models.</li>
<li class="calibre15"><strong class="bold">Stanford Named Entity Recognizer</strong> (<strong class="bold">NER</strong>): This is a Java-based NER tool <a id="_idIndexMarker366" class="calibre5 pcalibre1 pcalibre"/>that provides pre-trained<a id="_idIndexMarker367" class="calibre5 pcalibre1 pcalibre"/> models for several languages, including English, German, and Chinese.</li>
<li class="calibre15"><strong class="bold">AllenNLP</strong>: AllenNLP is a popular open source<a id="_idIndexMarker368" class="calibre5 pcalibre1 pcalibre"/> library for building<a id="_idIndexMarker369" class="calibre5 pcalibre1 pcalibre"/> and evaluating NLP models, including NER. It provides pre-trained models for several tasks, including NER, and also allows users to train their own models.</li>
<li class="calibre15"><strong class="bold">Flair</strong>: Flair is a Python library<a id="_idIndexMarker370" class="calibre5 pcalibre1 pcalibre"/> for state-of-the-art<a id="_idIndexMarker371" class="calibre5 pcalibre1 pcalibre"/> NLP, including NER. It provides pre-trained models for several languages and also allows users to train their own models.</li>
<li class="calibre15"><strong class="bold">General Architecture for Text Engineering</strong> (<strong class="bold">GATE</strong>): This is a suite of tools for NLP, including<a id="_idIndexMarker372" class="calibre5 pcalibre1 pcalibre"/> NER. It provides<a id="_idIndexMarker373" class="calibre5 pcalibre1 pcalibre"/> a graphical interface for creating and evaluating NLP models and also allows users to develop custom plugins for specific tasks.</li>
</ul>
<p class="calibre6">There are many other libraries available for NER, and the choice of library will depend on factors such as the programming language, available models, and specific requirements of the project. In the next section, we will explain POS tagging and different methods to perform this task.</p>
<h1 id="_idParaDest-83" class="calibre4"><a id="_idTextAnchor121" class="calibre5 pcalibre1 pcalibre"/>POS tagging</h1>
<p class="calibre6">POS tagging is the practice of attributing<a id="_idIndexMarker374" class="calibre5 pcalibre1 pcalibre"/> grammatical labels, such as nouns, verbs, adjectives, and others, to individual words within a sentence. This tagging process holds significance as a foundational step in various NLP tasks, including text classification, sentiment analysis, and machine translation.</p>
<p class="calibre6">POS tagging can be performed using different approaches such as rule-based methods, statistical methods, and deep learning-based methods. In this section, we’ll provide a brief overview of each approach.</p>
<h2 id="_idParaDest-84" class="calibre7"><a id="_idTextAnchor122" class="calibre5 pcalibre1 pcalibre"/>Rule-based methods</h2>
<p class="calibre6">Rule-based methods for<a id="_idIndexMarker375" class="calibre5 pcalibre1 pcalibre"/> POS tagging<a id="_idIndexMarker376" class="calibre5 pcalibre1 pcalibre"/> involve defining a set of rules or patterns that can be used to automatically tag words in a text with their corresponding parts of speech, such as nouns, verbs, adjectives, and so on.</p>
<p class="calibre6">The process involves defining a set of rules or patterns for identifying the different parts of speech in a sentence. For example, a rule may state that any word ending in “-ing” is a gerund (a verb acting as a noun), while another rule may state that any word preceded by an article such as “a” or “an” is likely a noun.</p>
<p class="calibre6">These rules are typically based on linguistic knowledge, such as knowledge of grammar and syntax, and are often specific to a particular language. They can also be supplemented with lexicons or dictionaries that provide additional information about the meanings and usage of words.</p>
<p class="calibre6">The process of rule-based tagging involves applying these rules to a given text and identifying the parts of speech for each word. This can be done manually but is typically automated using software tools and programming languages that support regular expressions and pattern matching.</p>
<p class="calibre6">One advantage of rule-based methods is that they can be highly accurate when the rules are well-designed and cover a wide range of linguistic phenomena. They can also be customized to specific domains or genres of text, such as scientific literature or legal documents.</p>
<p class="calibre6">However, one limitation of rule-based methods is that they may not be able to capture the full complexity and variability of natural language, and may require significant effort to develop and maintain the rules as language evolves and changes over time. They may also struggle with ambiguity, such as in cases where a word can have multiple possible parts of speech depending on the context.</p>
<p class="calibre6">Despite these limitations, rule-based methods<a id="_idIndexMarker377" class="calibre5 pcalibre1 pcalibre"/> for POS tagging remain<a id="_idIndexMarker378" class="calibre5 pcalibre1 pcalibre"/> an important approach in NLP, especially for applications that require high accuracy and precision.</p>
<h2 id="_idParaDest-85" class="calibre7"><a id="_idTextAnchor123" class="calibre5 pcalibre1 pcalibre"/>Statistical methods</h2>
<p class="calibre6">Statistical methods for POS tagging<a id="_idIndexMarker379" class="calibre5 pcalibre1 pcalibre"/> are based on using probabilistic models<a id="_idIndexMarker380" class="calibre5 pcalibre1 pcalibre"/> to automatically assign the most likely POS tag to each word in a sentence. These methods rely on a training corpus of tagged text, where the POS tags have already been assigned to the words, to learn the probabilities of a particular word being associated with each tag.</p>
<p class="calibre6">Two main types of statistical<a id="_idIndexMarker381" class="calibre5 pcalibre1 pcalibre"/> methods are used for POS tagging: <strong class="bold">Hidden Markov Models</strong> (<strong class="bold">HMMs</strong>) and CRFs.</p>
<p class="calibre6">HMMs serve as a category of probabilistic models that are extensively applied in handling sequential data, including text. In the context of POS tagging, HMMs represent the probability distribution of a sequence of POS tags concerning a sequence of words. HMMs assume that the likelihood of a POS tag at a specific position within a sentence is contingent solely upon the preceding tag in the sequence. Furthermore, they presume that the likelihood of a particular word, given its tag, remains independent of other words within the sentence. To identify the most probable sequence of POS tags for a given sentence, HMMs employ the Viterbi algorithm.</p>
<p class="calibre6">CRFs are another type of probabilistic model that is commonly used for sequence labeling tasks, including POS tagging. CRFs differ from HMMs in that they model the conditional probability of the output sequence (that is, the POS tags) given the input sequence (that is, the words), rather than the joint probability of the output and input sequences. This allows CRFs to capture more complex dependencies between the input and output sequences than HMMs. CRFs use an iterative algorithm, such as gradient descent or L-BFGS, to learn the optimal set of weights for the model.</p>
<p class="calibre6">Let’s look at the advantages<a id="_idIndexMarker382" class="calibre5 pcalibre1 pcalibre"/> of statistical methods:</p>
<ul class="calibre14">
<li class="calibre15">Statistical methods can capture the context of a word and the relationships between words in a sentence, leading to more accurate tagging results</li>
<li class="calibre15">These methods can handle unseen words and sentences that are not present in the training data</li>
<li class="calibre15">Statistical methods can be trained on large datasets, allowing them to capture more variations<a id="_idIndexMarker383" class="calibre5 pcalibre1 pcalibre"/> and patterns in the language</li>
</ul>
<p class="calibre6">Now, let’s look<a id="_idIndexMarker384" class="calibre5 pcalibre1 pcalibre"/> at the disadvantages:</p>
<ul class="calibre14">
<li class="calibre15">These methods require a large amount of annotated data for training, which can be time-consuming and expensive to create</li>
<li class="calibre15">Statistical methods can be sensitive to the quality of the training data and may perform poorly if the data is noisy or biased</li>
<li class="calibre15">Statistical models are typically black boxes, making it difficult to interpret the decisions made by the model</li>
</ul>
<h2 id="_idParaDest-86" class="calibre7"><a id="_idTextAnchor124" class="calibre5 pcalibre1 pcalibre"/>Deep learning-based methods</h2>
<p class="calibre6">Deep learning-based methods<a id="_idIndexMarker385" class="calibre5 pcalibre1 pcalibre"/> for POS tagging<a id="_idIndexMarker386" class="calibre5 pcalibre1 pcalibre"/> involve training a neural network model to predict the POS tags for each word in a given sentence. These methods can learn complex patterns and relationships in the text data to accurately tag words with their appropriate parts of speech.</p>
<p class="calibre6">One of the most popular deep learning-based methods for POS tagging is using an RNN with LSTM cells. LSTM-based models can process sequences of words and capture dependencies between them. The input to the model is a sequence of word embeddings, which are vector representations of words in a high-dimensional space. These embeddings are learned during the training process.</p>
<p class="calibre6">The LSTM-based model is comprised of three main layers: an input layer, an LSTM layer, and an output layer. The structure involves taking word embeddings as input into the input layer. Subsequently, the LSTM layer processes the sequence of these embeddings, aiming to grasp the interdependencies inherent within them. Ultimately, the output layer is responsible for predicting the POS tag for each word within the input sequence. Another popular deep learning-based method for POS tagging is using<a id="_idIndexMarker387" class="calibre5 pcalibre1 pcalibre"/> a transformer-based model, such as <strong class="bold">Bidirectional Encoder Representations from Transformers</strong> (<strong class="bold">BERT</strong>). BERT is a language model that comes pre-trained and employs a transformer-based architecture to acquire a profound understanding of contextual relationships among words within a sentence. It undergoes training with vast quantities of text data and can be fine-tuned to excel in diverse NLP tasks, one of which is POS tagging.</p>
<p class="calibre6">To use BERT for POS tagging, the input sentence must be tokenized, and each token must be assigned an initial POS tag. The token embeddings are then fed into the pre-trained BERT model, which outputs contextualized embeddings for each token. These embeddings are passed through a feedforward neural network to predict the final POS tag for each token.</p>
<p class="calibre6">Deep learning approaches for POS tagging have demonstrated leading-edge performance across numerous benchmark datasets. Nonetheless, their effectiveness demands substantial training data and computational resources, and the training process can be time-consuming. Moreover, they may suffer from a lack of interpretability, which makes it difficult to understand how the model is making its predictions.</p>
<p class="calibre6">Several libraries are available<a id="_idIndexMarker388" class="calibre5 pcalibre1 pcalibre"/> for performing POS tagging in various programming<a id="_idIndexMarker389" class="calibre5 pcalibre1 pcalibre"/> languages, including Python, Java, and C++. Some popular NLP libraries that provide POS tagging functionality include NLTK, spaCy, Stanford CoreNLP, and Apache OpenNLP.</p>
<p class="calibre6">Here is an example of POS tagging using the NLTK library in Python:</p>
<pre class="source-code">
import nltk
input_sentence = "The young white cat jumps over the lazy dog"
processed_tokens = nltk.word_tokenize(input_sentence)
tags = nltk.pos_tag(processed_tokens)
print(tags)</pre> <p class="calibre6">The output is as follows:</p>
<pre class="source-code">
[('The', 'DT'), (young, 'JJ'), (white, 'NN'), ('cat', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]</pre> <p class="calibre6">In this example, the <code>nltk.pos_tag()</code> function is used to tag the words in the sentence. The function returns a list of tuples <a id="_idIndexMarker390" class="calibre5 pcalibre1 pcalibre"/>where each tuple contains<a id="_idIndexMarker391" class="calibre5 pcalibre1 pcalibre"/> a word and its POS tag. The POS<a id="_idIndexMarker392" class="calibre5 pcalibre1 pcalibre"/> tags that have been used here are based on the <strong class="bold">Penn </strong><strong class="bold">Treebank tagset</strong>.</p>
<h2 id="_idParaDest-87" class="calibre7"><a id="_idTextAnchor125" class="calibre5 pcalibre1 pcalibre"/>Regular expressions</h2>
<p class="calibre6">A regular expression is a type<a id="_idIndexMarker393" class="calibre5 pcalibre1 pcalibre"/> of text pattern<a id="_idIndexMarker394" class="calibre5 pcalibre1 pcalibre"/> that has various applications in modern programming languages and software. They are useful for validating whether an input conforms to a particular text pattern, locating text within a larger text body that matches the pattern, replacing text that matches the pattern with alternative text or rearranging parts of the matched text, and dividing a block of text into a list of subtexts, but can cause unintended consequences if used incorrectly.</p>
<p class="calibre6">In computer science and mathematics, the term <strong class="bold">regular expression</strong> is derived from the concept of “regularity” in mathematical expressions.</p>
<p class="calibre6">A regular expression, often referred to as regex or regexp, is a series<a id="_idIndexMarker395" class="calibre5 pcalibre1 pcalibre"/> of characters that constitutes a search pattern. Regular expressions are used to match and manipulate text, typically in the context of text processing, search algorithms, and NLP.</p>
<p class="calibre6">A regular expression comprises a mix of characters and metacharacters, which collectively establish a pattern to search for within a text string. The simplest form of a regular expression is a mere sequence of characters that must be matched precisely. For example, the regular expression “hello” would match any string that contains the characters “hello” in sequence.</p>
<p class="calibre6">Metacharacters are unique characters within regular expressions that possess pre-defined meanings. For instance, the “.” (dot) metacharacter is employed to match any individual character, whereas the “*” (asterisk) metacharacter is used to match zero or more instances of the preceding characters or group. Regular expressions can be used for a wide range of text-processing tasks. Let’s take a closer look.</p>
<h3 class="calibre8">Validating input</h3>
<p class="calibre6">Regular expressions can be used<a id="_idIndexMarker396" class="calibre5 pcalibre1 pcalibre"/> to validate input<a id="_idIndexMarker397" class="calibre5 pcalibre1 pcalibre"/> by matching it against a pattern. For example, you can use a regular expression to validate an email address or a phone number.</p>
<h3 class="calibre8">Text manipulation</h3>
<p class="calibre6">Text manipulation using regular expressions<a id="_idIndexMarker398" class="calibre5 pcalibre1 pcalibre"/> involves using<a id="_idIndexMarker399" class="calibre5 pcalibre1 pcalibre"/> pattern-matching techniques to find and manipulate text strings in a document or dataset. Regular expressions are powerful tools for working with text data, allowing for complex search and replace operations, text extraction, and formatting.</p>
<p class="calibre6">Some common text manipulation tasks that can be accomplished with regular expressions are as follows:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Search and replace</strong>: Using regular expressions to search for specific patterns or character sequences in a document and replace them with other text or formatting</li>
<li class="calibre15"><strong class="bold">Data extraction</strong>: Regular expressions can be used for data extraction from text by defining patterns that match specific data formats</li>
</ul>
<p class="calibre6">Here are the general steps for using<a id="_idIndexMarker400" class="calibre5 pcalibre1 pcalibre"/> regular expressions for data extraction:</p>
<ol class="calibre16">
<li class="calibre15"><strong class="bold">Define a regular expression pattern</strong>: The first step is to define a regular expression pattern that matches the data you want to extract. For example, if you want to extract all phone numbers from a text document, you can define a pattern that matches the format of a phone number.</li>
<li class="calibre15"><strong class="bold">Compile the regular expression pattern</strong>: After establishing the regular expression pattern, the next step involves compiling it into a regular expression object, which can then be utilized for matching purposes.</li>
<li class="calibre15"><strong class="bold">Search for the pattern in the text</strong>: Once you have compiled the regular expression object, you can use it to search for the pattern in the text. You can search for the pattern in a single string or a larger block of text.</li>
<li class="calibre15"><strong class="bold">Extract the matched data</strong>: After you have searched for the pattern in the text, you can extract the data that matches that pattern. You can extract all occurrences of the matched data or only the first occurrence.</li>
</ol>
<p class="calibre6">Here’s an example of how to extract all email addresses from a string using regular expressions in Python:</p>
<pre class="source-code">
import re
text = "John's email is john@example.com and Jane's email is jane@example.com"
# Pattern for email addresses:
pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
regex = re.compile(pattern)
# Search for all occurrences of the pattern in the text:
matches = regex.findall(text)
print(matches)</pre> <p class="calibre6">Here’s the<a id="_idIndexMarker401" class="calibre5 pcalibre1 pcalibre"/> output:</p>
<pre class="source-code">
['john@example.com', 'jane@example.com']</pre> <p class="calibre6">Next, we’ll cover text cleaning.</p>
<h3 class="calibre8">Text cleaning</h3>
<p class="calibre6">Text cleaning means<a id="_idIndexMarker402" class="calibre5 pcalibre1 pcalibre"/> using regular expressions<a id="_idIndexMarker403" class="calibre5 pcalibre1 pcalibre"/> to clean and standardize text data, thereby removing unwanted characters, whitespace, or other formatting.</p>
<p class="calibre6">Here are some common text-cleaning<a id="_idIndexMarker404" class="calibre5 pcalibre1 pcalibre"/> techniques that use regular expressions:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Removing special characters</strong>: Regular expressions can be used to match and remove specific characters such as punctuation marks, brackets, and other special symbols. For example, the <strong class="source-inline1">[^a-zA-Z0-9]</strong> regular expression will match any non-alphanumeric character.</li>
<li class="calibre15"><strong class="bold">Removing stop words</strong>: Stop words are common words such as “the,” “and,” and “but” that are often removed from text to focus on the most meaningful words. Regular expressions can be used to match and remove these words from text.</li>
<li class="calibre15"><strong class="bold">Removing HTML tags</strong>: If you’re working with text that has been scraped from a website, you may need to remove HTML tags before analyzing the text. Regular expressions can be used to match and remove HTML tags.</li>
<li class="calibre15"><strong class="bold">Converting text into lowercase</strong>: Regular expressions can be used to convert all text into lowercase or uppercase, which can make it easier to compare and analyze.</li>
<li class="calibre15"><strong class="bold">Normalizing text</strong>: Normalization involves transforming text into a standard format. Regular expressions can be<a id="_idIndexMarker405" class="calibre5 pcalibre1 pcalibre"/> used to perform tasks such as stemming and lemmatization, which involves reducing words to their root form.</li>
</ul>
<p class="calibre6">By using regular expressions<a id="_idIndexMarker406" class="calibre5 pcalibre1 pcalibre"/> for text cleaning, you can remove noise and irrelevant information from text, making it easier to analyze and extract meaningful insights.</p>
<h3 class="calibre8">Parsing</h3>
<p class="calibre6"><strong class="bold">Parsing</strong> involves analyzing a text string<a id="_idIndexMarker407" class="calibre5 pcalibre1 pcalibre"/> to discern its grammatical <a id="_idIndexMarker408" class="calibre5 pcalibre1 pcalibre"/>structure according to a specified grammar. Regular expressions serve as potent instruments for text parsing, especially when dealing with uncomplicated and regular grammatical patterns.</p>
<p class="calibre6">To parse text using regular expressions, you need to define a grammar for the language you want to parse. The grammar should specify the possible components of a sentence, such as nouns, verbs, adjectives, and so on, as well as the rules that dictate how these components can be combined to form valid sentences.</p>
<p class="calibre6">Once you have defined the grammar, you can use regular expressions to identify the individual components of a sentence and the relationships between them. For example, you can use regular expressions to match all the nouns in a sentence or to identify the subject and object of a verb.</p>
<p class="calibre6">One common approach to parsing with regular expressions is to define a set of patterns that correspond to the different parts of speech and sentence structures in your grammar. For example, you might define a pattern for matching nouns, a pattern for matching verbs, and a pattern for matching sentences that consist of a subject followed by a verb and an object.</p>
<p class="calibre6">To use these patterns for parsing, you would apply them to a text string using a regular expression engine, which would match the patterns to the appropriate parts of the string. The output of the parsing process would be a parse tree or other data structure that represents the grammatical structure of the sentence.</p>
<p class="calibre6">One limitation of regular expression parsing is that it is generally not suitable for handling more complex or ambiguous grammar. For example, it can be difficult to handle cases where a word could be either a noun or a verb depending on the context, or where the structure of a sentence is ambiguous.</p>
<p class="calibre6">We can also use regular expressions to break a larger text document into smaller chunks or tokens based on specific patterns or delimiters.</p>
<p class="calibre6">To use regular expressions for text manipulation, you typically need to define a pattern that matches the text you want to find or manipulate. This pattern can include special characters and syntax to define the specific sequence of characters, numbers, or other elements that make up the text string.</p>
<p class="calibre6">For example, the regular expression pattern <em class="italic">\d{3}-\d{2}-\d{4}</em> might be used to search for and extract Social Security numbers in a larger text document. This pattern matches a sequence of three digits, followed by a dash, then two more digits, another dash, and four final digits followed by a non-digit, which together represent the standard format for a Social Security number in the USA.</p>
<p class="calibre6">Once you have defined your regular expression pattern, you can use it with various text manipulation tools and programming languages, such as grep, sed, awk, Perl, Python, and many others, to perform complex text manipulation tasks.</p>
<p class="calibre6">Some programming languages, such as Perl and Python, have built-in support for regular expressions. Other programming languages, such as Java and C++, require you to use a library or API to work with regular expressions.</p>
<p class="calibre6">While regular expressions are powerful tools<a id="_idIndexMarker409" class="calibre5 pcalibre1 pcalibre"/> for text processing, they can also be complex<a id="_idIndexMarker410" class="calibre5 pcalibre1 pcalibre"/> and difficult to understand. It’s important to be familiar with the syntax and behavior of regular expressions to use them effectively in your code.</p>
<h2 id="_idParaDest-88" class="calibre7"><a id="_idTextAnchor126" class="calibre5 pcalibre1 pcalibre"/>Tokenization</h2>
<p class="calibre6">Tokenization is a process in NLP<a id="_idIndexMarker411" class="calibre5 pcalibre1 pcalibre"/> that involves breaking down a piece of text or a sentence into individual words or terms, known as tokens. The tokenization process can be applied to various forms of data, such as textual documents, social media posts, web pages, and more.</p>
<p class="calibre6">The tokenization process is an important initial step in many NLP tasks as it transforms unstructured text data into a structured format that can be analyzed using machine learning algorithms or other techniques. These tokens can be used to perform various operations in the text, such as counting word frequencies, identifying the most common phrases, and so on.</p>
<p class="calibre6">There are different methods of tokenization:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Word tokenization</strong>: This method splits a piece<a id="_idIndexMarker412" class="calibre5 pcalibre1 pcalibre"/> of text into individual <a id="_idIndexMarker413" class="calibre5 pcalibre1 pcalibre"/>words or tokens using whitespace, punctuation, and other characters as delimiters. For example, take a look at the following sentence:<p class="calibre6"><em class="italic">The nimble white cat jumps over the </em><em class="italic">sleepy dog</em></p><p class="calibre6">This can be tokenized into the following list of words:</p><p class="calibre6"><em class="italic">[“The”, “nimble”, “white”, “cat”, “jumps”, “over”, “the”, “</em><em class="italic">sleepy”, “dog”]</em></p></li>
<li class="calibre15"><strong class="bold">Sentence tokenization</strong>: This method splits a piece of text<a id="_idIndexMarker414" class="calibre5 pcalibre1 pcalibre"/> into individual sentences by using punctuation<a id="_idIndexMarker415" class="calibre5 pcalibre1 pcalibre"/> marks such as periods, exclamation marks, and question marks as delimiters. For example, take a look at the following paragraph:<p class="calibre6"><em class="italic">This is the </em><em class="italic">first sentence.</em></p><p class="calibre6"><em class="italic">This is the </em><em class="italic">second sentence.</em></p><p class="calibre6"><em class="italic">This is the </em><em class="italic">third sentence</em>.</p><p class="calibre6">This can be tokenized into the following list of sentences:</p><p class="calibre6"><em class="italic">[“This is the </em><em class="italic">first sentence.”,</em></p><p class="calibre6"><em class="italic">“This is the </em><em class="italic">second sentence.”,</em></p><p class="calibre6"><em class="italic">“This is the </em><em class="italic">third sentence.”]</em></p></li>
<li class="calibre15"><strong class="bold">Regular expression tokenization</strong>: This method uses regular<a id="_idIndexMarker416" class="calibre5 pcalibre1 pcalibre"/> expressions to define the tokenization<a id="_idIndexMarker417" class="calibre5 pcalibre1 pcalibre"/> rules. Regular expressions can be used to match patterns in the text, such as email addresses, URLs, or phone numbers, and extract them as individual tokens.</li>
</ul>
<p class="calibre6">Tokenization is an important step in NLP and is used in many applications, such as sentiment analysis, document classification, machine translation, and more.</p>
<p class="calibre6">Tokenization is also an important step in language models. For example, in BERT, which is a well-known language model, a tokenizer is a sub-word tokenizer, meaning it breaks down words into<a id="_idIndexMarker418" class="calibre5 pcalibre1 pcalibre"/> smaller sub-word units called tokens. It uses <strong class="bold">WordPiece</strong> tokenization, which is a data-driven<a id="_idIndexMarker419" class="calibre5 pcalibre1 pcalibre"/> approach that builds a large vocabulary of sub-words based on the corpus of text being trained on.</p>
<p class="calibre6">Using a tokenizer is an important step in language models as well. For example, BERT utilizes a WordPiece tokenizer, which employs the technique of dividing words into either their full forms<a id="_idIndexMarker420" class="calibre5 pcalibre1 pcalibre"/> or smaller components known as word pieces. This means that a single word can be represented by several tokens. It employs a data-driven approach that builds a large vocabulary of sub-words based on the corpus of text being trained on. These sub-word units are represented as embeddings that are used as input to the BERT model.</p>
<p class="calibre6">One of the key features of the BERT<a id="_idIndexMarker421" class="calibre5 pcalibre1 pcalibre"/> tokenizer is that it can handle <strong class="bold">out-of-vocabulary</strong> (<strong class="bold">OOV</strong>) words. If the tokenizer encounters a word that is not in its vocabulary, it will break the word down into sub-words and represent the word as a combination of its sub-word embeddings. We will explain BERT and its tokenizer in more detail later in this book. The benefit of using a tokenizer in language models is that we can limit the number of inputs to the size of our dictionary rather than all possible inputs. For example, BERT has a 30,000-word vocabulary size, which helps us limit the size of the deep learning language<a id="_idIndexMarker422" class="calibre5 pcalibre1 pcalibre"/> model. Using a bigger tokenizer will increase the size of the model. In the next section, we will explain how to use the methods that were covered in this chapter in a complete preprocessing pipeline.</p>
<h1 id="_idParaDest-89" class="calibre4"><a id="_idTextAnchor127" class="calibre5 pcalibre1 pcalibre"/>Explaining the preprocessing pipeline</h1>
<p class="calibre6">We will explain a complete preprocessing <a id="_idIndexMarker423" class="calibre5 pcalibre1 pcalibre"/>pipeline that has been provided by the authors to you, the reader.</p>
<p class="calibre6">As shown in the following code, the input is a formatted text with encoded tags, similar to what we can extract from HTML web pages:</p>
<pre class="source-code">
"&lt;SUBJECT LINE&gt; Employees details&lt;END&gt;&lt;BODY TEXT&gt;Attached are 2 files,\n1st one is pairoll, 2nd is healtcare!&lt;END&gt;"</pre> <p class="calibre6">Let’s take a look at the effect of applying each step to the text:</p>
<ol class="calibre16">
<li class="calibre15">Decode/remove encoding:<p class="calibre6"><em class="italic">Employees details. Attached are 2 files, 1st one is pairoll, 2nd </em><em class="italic">is healtcare!</em></p></li>
<li class="calibre15">Lowercasing:<p class="calibre6"><em class="italic">employees details. attached are 2 files, 1st one is pairoll, 2nd </em><em class="italic">is healtcare!</em></p></li>
<li class="calibre15">Digits to words:<p class="calibre6"><em class="italic">employees details. attached are two files, first one is pairoll, second </em><em class="italic">is healtcare!</em></p></li>
<li class="calibre15">Remove punctuation and other special characters:<p class="calibre6"><em class="italic">employees details attached are two files first one is pairoll second </em><em class="italic">is healtcare</em></p></li>
<li class="calibre15">Spelling corrections:<p class="calibre6"><em class="italic">employees details attached are two files first one is payroll second </em><em class="italic">is healthcare</em></p></li>
<li class="calibre15">Remove stop words:<p class="calibre6"><em class="italic">employees details attached two files first one payroll </em><em class="italic">second healthcare</em></p></li>
<li class="calibre15">Stemming:<p class="calibre6"><em class="italic">employe detail attach two file first one payrol </em><em class="italic">second healthcar</em></p></li>
<li class="calibre15">Lemmatizing:<p class="calibre6"><em class="italic">employe detail attach two file first one payrol </em><em class="italic">second healthcar</em></p></li>
</ol>
<p class="calibre6">With that, we’ve learned <a id="_idIndexMarker424" class="calibre5 pcalibre1 pcalibre"/>about different preprocessing methods. Next, we’ll review a piece of code for performing NER and POS.</p>
<h2 id="_idParaDest-90" class="calibre7"><a id="_idTextAnchor128" class="calibre5 pcalibre1 pcalibre"/>Code for NER and POS</h2>
<p class="calibre6">For this example, we used <a id="_idIndexMarker425" class="calibre5 pcalibre1 pcalibre"/>the spaCy library for Python<a id="_idIndexMarker426" class="calibre5 pcalibre1 pcalibre"/> to perform these tasks. Here our input is:</p>
<pre class="source-code">
The companies that would be releasing their quarterly reports tomorrow are Microsoft, 4pm, Google, 4pm, and AT&amp;T, 6pm.</pre> <p class="calibre6">Here’s the output for NER:</p>
<p class="calibre6"><em class="italic">The companies that would be releasing their quarterly DATE reports tomorrow DATE are Microsoft ORG , 4pm TIME , Google ORG , 4pm TIME , and AT&amp;T ORG , 6pm </em><em class="italic">TIME .</em></p>
<p class="calibre6">As you can see, using NER, we were able to detect parts of the sentence that are related to company names (ORG) or dates.</p>
<p class="calibre6"><em class="italic">Figure 4</em><em class="italic">.1</em> shows an example of performing POS tagging:</p>
<div><div><img alt="Figure 4.1 – POS tagging using spaCy" src="img/B18949_04_1.jpg" class="calibre3"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 4.1 – POS tagging using spaCy</p>
<p class="calibre6">Here’s the output:</p>
<pre class="source-code">
[['companies', 'NOUN'],
 ['releasing', 'VERB'],
 ['quarterly', 'ADJ'],
 ['reports', 'NOUN'],
 ['tomorrow', 'NOUN'],
 ['Microsoft', 'PROPN'],
 ['pm', 'NOUN'],
 ['Google', 'PROPN'],
 ['pm', 'NOUN'],
 ['AT&amp;T', 'PROPN'],
 ['pm', 'NOUN']]</pre> <p class="calibre6">The preceding code examples<a id="_idIndexMarker427" class="calibre5 pcalibre1 pcalibre"/> exemplify the various<a id="_idIndexMarker428" class="calibre5 pcalibre1 pcalibre"/> aspects of preprocessing, which processes raw text and transforms it into a form that suits the downstream model so that it suits the purpose of the overall design.</p>
<h1 id="_idParaDest-91" class="calibre4"><a id="_idTextAnchor129" class="calibre5 pcalibre1 pcalibre"/>Summary</h1>
<p class="calibre6">In this chapter, we covered a range of techniques and methods for text preprocessing, including normalization, tokenization, stop word removal, POS tagging, and more. We explored different approaches to these techniques, such as rule-based methods, statistical methods, and deep learning-based methods. We also discussed the advantages and disadvantages of each method and provided examples and code snippets to illustrate their use.</p>
<p class="calibre6">At this point, you should have a solid understanding of the importance of text preprocessing and the various techniques and methods available for cleaning and preparing text data for analysis. You should be able to implement these techniques using popular libraries and frameworks in Python and understand the trade-offs between different approaches. Furthermore, you should have a better understanding of how to process text data to achieve better results in NLP tasks such as sentiment analysis, topic modeling, and text classification.</p>
<p class="calibre6">In the next chapter, we will explain text classification, and different methods for performing this task.</p>
</div>
</body></html>