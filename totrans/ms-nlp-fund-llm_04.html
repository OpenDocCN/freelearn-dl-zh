<html><head></head><body>
<div id="_idContainer285" class="calibre2">
<h1 class="chapter-number" id="_idParaDest-76"><a id="_idTextAnchor113" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1.1">4</span></h1>
<h1 id="_idParaDest-77" class="calibre4"><a id="_idTextAnchor114" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.2.1">Streamlining Text Preprocessing Techniques for Optimal NLP Performance</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.3.1">Text preprocessing stands</span><a id="_idIndexMarker328" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.4.1"> as a vital initial step in the realm of </span><strong class="bold"><span class="kobospan" id="kobo.5.1">natural language processing</span></strong><span class="kobospan" id="kobo.6.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.7.1">NLP</span></strong><span class="kobospan" id="kobo.8.1">). </span><span class="kobospan" id="kobo.8.2">It encompasses converting raw, unrefined text</span><a id="_idIndexMarker329" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.9.1"> data into a format that machine learning algorithms can readily comprehend. </span><span class="kobospan" id="kobo.9.2">To extract meaningful insights from textual data, it is essential to clean, normalize, and transform the data into a more structured form. </span><span class="kobospan" id="kobo.9.3">This chapter provides an overview of the most commonly used text preprocessing</span><a id="_idIndexMarker330" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.10.1"> techniques, including tokenization, stemming, lemmatization, stop word removal, and </span><strong class="bold"><span class="kobospan" id="kobo.11.1">part-of-speech</span></strong><span class="kobospan" id="kobo.12.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.13.1">POS</span></strong><span class="kobospan" id="kobo.14.1">) tagging, along with their advantages </span><span><span class="kobospan" id="kobo.15.1">and limitations.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.16.1">Effective text preprocessing is essential for various NLP tasks, including sentiment analysis, language translation, and information retrieval. </span><span class="kobospan" id="kobo.16.2">By applying these techniques, raw text data can be transformed into a structured and normalized format that can be easily analyzed using statistical and machine learning methods. </span><span class="kobospan" id="kobo.16.3">However, selecting the appropriate preprocessing techniques can be challenging since the optimal methods depend on the specific task and dataset at hand. </span><span class="kobospan" id="kobo.16.4">Therefore, it is important to carefully evaluate and compare different text preprocessing techniques to determine the most effective approach for a </span><span><span class="kobospan" id="kobo.17.1">given application.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.18.1">The following topics will be covered in </span><span><span class="kobospan" id="kobo.19.1">this chapter:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><span class="kobospan" id="kobo.20.1">Lowercasing </span><span><span class="kobospan" id="kobo.21.1">in N</span><a id="_idTextAnchor115" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.22.1">LP</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.23.1">Removing special characters </span><span><span class="kobospan" id="kobo.24.1">and punctuations</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.25.1">Removing </span><span><span class="kobospan" id="kobo.26.1">stop words</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.27.1">Named entity </span><span><span class="kobospan" id="kobo.28.1">recognition (NER)</span></span></li>
<li class="calibre15"><span><span class="kobospan" id="kobo.29.1">POS tagging</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.30.1">Explaining the </span><span><span class="kobospan" id="kobo.31.1">preprocessing pipeline</span></span></li>
</ul>
<h1 id="_idParaDest-78" class="calibre4"><a id="_idTextAnchor116" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.32.1">Technical requirements</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.33.1">To follow along with the examples and exercises in this chapter on text preprocessing, you will need a working knowledge of a programming language such as Python, as well as some familiarity with NLP concepts. </span><span class="kobospan" id="kobo.33.2">You will also need to have certain libraries installed, such as </span><strong class="bold"><span class="kobospan" id="kobo.34.1">Natural Language Toolkit</span></strong><span class="kobospan" id="kobo.35.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.36.1">NLTK</span></strong><span class="kobospan" id="kobo.37.1">), </span><strong class="bold"><span class="kobospan" id="kobo.38.1">spaCy</span></strong><span class="kobospan" id="kobo.39.1">, and </span><strong class="bold"><span class="kobospan" id="kobo.40.1">scikit-learn</span></strong><span class="kobospan" id="kobo.41.1">. </span><span class="kobospan" id="kobo.41.2">These libraries provide powerful tools for text preprocessing and feature extraction. </span><span class="kobospan" id="kobo.41.3">It is recommended that you have access to a </span><strong class="bold"><span class="kobospan" id="kobo.42.1">Jupyter Notebook</span></strong><span class="kobospan" id="kobo.43.1"> environment or another interactive coding environment to facilitate experimentation and exploration. </span><span class="kobospan" id="kobo.43.2">Additionally, having a sample dataset to work with can help you understand the various techniques and their effects on </span><span><span class="kobospan" id="kobo.44.1">text data.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.45.1">Text normalization is the process of transforming text into a standard form to ensure consistency and reduce variations. </span><span class="kobospan" id="kobo.45.2">Different techniques are used for normalizing text, including lowercasing, removing special characters, spell checking, and stemming or lemmatization. </span><span class="kobospan" id="kobo.45.3">We will explain these steps in detail, and how to use them, with </span><span><span class="kobospan" id="kobo.46.1">code examples.</span></span></p>
<h1 id="_idParaDest-79" class="calibre4"><a id="_idTextAnchor117" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.47.1">Lowercasing in NLP</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.48.1">Lowercasing is a common text preprocessing technique</span><a id="_idIndexMarker331" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.49.1"> that’s used in NLP to standardize text and reduce the complexity of vocabulary. </span><span class="kobospan" id="kobo.49.2">In this technique, all the text is converted into </span><span><span class="kobospan" id="kobo.50.1">lowercase characters.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.51.1">The main purpose of lowercasing is to make the text uniform and avoid any discrepancies that may arise from capitalization. </span><span class="kobospan" id="kobo.51.2">By converting all the text into lowercase, the machine learning algorithms can treat the same words that are capitalized and non-capitalized as the same, reducing the overall vocabulary size and making the text easier </span><span><span class="kobospan" id="kobo.52.1">to process.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.53.1">Lowercasing is particularly useful for tasks such as text classification, sentiment analysis, and language modeling, where the meaning of the text is not affected by the capitalization of the words. </span><span class="kobospan" id="kobo.53.2">However, it may not be suitable for certain tasks, such as NER, where capitalization can be an </span><span><span class="kobospan" id="kobo.54.1">important feature.</span></span></p>
<h1 id="_idParaDest-80" class="calibre4"><a id="_idTextAnchor118" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.55.1">Removing special characters and punctuation</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.56.1">Removing special characters</span><a id="_idIndexMarker332" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.57.1"> and punctuation</span><a id="_idIndexMarker333" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.58.1"> is an important step in text preprocessing. </span><span class="kobospan" id="kobo.58.2">Special characters and punctuation marks do not add much meaning to the text and can cause issues for machine learning models if they are not removed. </span><span class="kobospan" id="kobo.58.3">One way to perform this task is by using regular expressions, such as </span><span><span class="kobospan" id="kobo.59.1">the following:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.60.1">
re.sub(r"[^a-zA-Z0-9]+", "", string)</span></pre> <p class="calibre6"><span class="kobospan" id="kobo.61.1">This will remove non-characters and numbers from our input string. </span><span class="kobospan" id="kobo.61.2">Sometimes, there may be special characters that we would want to replace with a whitespace. </span><span class="kobospan" id="kobo.61.3">Take a look at the </span><span><span class="kobospan" id="kobo.62.1">following examples:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><span><span class="kobospan" id="kobo.63.1">president-elect</span></span></li>
<li class="calibre15"><span><span class="kobospan" id="kobo.64.1">body-type</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.65.1">In these two examples, we would want to replace the “-” with whitespace, </span><span><span class="kobospan" id="kobo.66.1">as follows:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><span><span class="kobospan" id="kobo.67.1">President elect</span></span></li>
<li class="calibre15"><span><span class="kobospan" id="kobo.68.1">Body type</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.69.1">Next, we’ll cover stop </span><span><span class="kobospan" id="kobo.70.1">word removal.</span></span></p>
<h2 id="_idParaDest-81" class="calibre7"><a id="_idTextAnchor119" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.71.1">Stop word removal</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.72.1">Stop words are words</span><a id="_idIndexMarker334" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.73.1"> that do not contribute much to the meaning of a sentence or piece of text, and therefore can be safely removed without us losing much information. </span><span class="kobospan" id="kobo.73.2">Examples of stop words include “a,” “an,” “the,” “and,” “in,” “at,” “on,” “to,” “for,” “is,” “are,” and </span><span><span class="kobospan" id="kobo.74.1">so on.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.75.1">Stop word removal is a common text preprocessing </span><a id="_idIndexMarker335" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.76.1">step that is performed</span><a id="_idIndexMarker336" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.77.1"> before any text analysis</span><a id="_idIndexMarker337" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.78.1"> tasks, such as </span><strong class="bold"><span class="kobospan" id="kobo.79.1">sentiment analysis</span></strong><span class="kobospan" id="kobo.80.1">, </span><strong class="bold"><span class="kobospan" id="kobo.81.1">topic modeling</span></strong><span class="kobospan" id="kobo.82.1">, or </span><strong class="bold"><span class="kobospan" id="kobo.83.1">information retrieval</span></strong><span class="kobospan" id="kobo.84.1">. </span><span class="kobospan" id="kobo.84.2">The goal is to reduce</span><a id="_idIndexMarker338" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.85.1"> the size of the vocabulary and the dimensionality of the feature space, which can improve the efficiency and effectiveness of subsequent </span><span><span class="kobospan" id="kobo.86.1">analysis steps.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.87.1">The process of stop word removal involves identifying a list of stop words (usually predefined or learned from a corpus), tokenizing the input text into words or tokens, and then removing any words that match the stop word list. </span><span class="kobospan" id="kobo.87.2">The resulting text consists of only the important words that carry the meaning of </span><span><span class="kobospan" id="kobo.88.1">the text.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.89.1">Stop word removal can be performed using various programming languages, tools, and libraries. </span><span class="kobospan" id="kobo.89.2">For example, NLTK, which is a popular Python library for NLP, provides a list of stop words for various languages, as well as a method for removing stop words </span><span><span class="kobospan" id="kobo.90.1">from text.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.91.1">Here’s an example of stop </span><span><span class="kobospan" id="kobo.92.1">word removal:</span></span></p>
<p class="calibre6"><em class="italic"><span class="kobospan" id="kobo.93.1">This is a sample sentence demonstrating stop </span></em><span><em class="italic"><span class="kobospan" id="kobo.94.1">word filtration.</span></em></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.95.1">After performing stop word removal, we get the </span><span><span class="kobospan" id="kobo.96.1">following output:</span></span></p>
<p class="calibre6"><em class="italic"><span class="kobospan" id="kobo.97.1">Sample sentence demonstrating stop </span></em><span><em class="italic"><span class="kobospan" id="kobo.98.1">word filtration</span></em></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.99.1">This chapter contains Python code dedicated to this. </span><span class="kobospan" id="kobo.99.2">You can refer to it for each of the actions that are described in </span><span><span class="kobospan" id="kobo.100.1">this chapter.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.101.1">As we can see, the stop</span><a id="_idIndexMarker339" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.102.1"> words “This,” “is,” and “a,” have been removed from the original sentence, leaving only the </span><span><span class="kobospan" id="kobo.103.1">important words.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.104.1">Spell checking and correction</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.105.1">Spell checking and correction involves</span><a id="_idIndexMarker340" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.106.1"> correcting misspelled words in the text. </span><span class="kobospan" id="kobo.106.2">This is important because misspelled words can cause inconsistencies in the data and affect the accuracy of algorithms. </span><span class="kobospan" id="kobo.106.3">For example, take a look at the </span><span><span class="kobospan" id="kobo.107.1">following sentence:</span></span></p>
<p class="calibre6"><em class="italic"><span class="kobospan" id="kobo.108.1">I am going to </span></em><span><em class="italic"><span class="kobospan" id="kobo.109.1">the bakkery</span></em></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.110.1">This would be transformed into </span><span><span class="kobospan" id="kobo.111.1">the following:</span></span></p>
<p class="calibre6"><em class="italic"><span class="kobospan" id="kobo.112.1">I am going to </span></em><span><em class="italic"><span class="kobospan" id="kobo.113.1">the bakery</span></em></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.114.1">Let’s move on </span><span><span class="kobospan" id="kobo.115.1">to lemmatization.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.116.1">Lemmatization</span></h3>
<p class="calibre6"><strong class="bold"><span class="kobospan" id="kobo.117.1">Lemmatization</span></strong><span class="kobospan" id="kobo.118.1"> is a text normalization approach </span><a id="_idIndexMarker341" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.119.1">that aims to simplify a word to its base</span><a id="_idIndexMarker342" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.120.1"> or dictionary form, referred to as a lemma. </span><span class="kobospan" id="kobo.120.2">The primary objective of lemmatization is to aggregate various forms of the same word, facilitating their analysis as a </span><span><span class="kobospan" id="kobo.121.1">unified term.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.122.1">For example, consider the </span><span><span class="kobospan" id="kobo.123.1">following sentence:</span></span></p>
<p class="calibre6"><em class="italic"><span class="kobospan" id="kobo.124.1">Three cats were chasing the mice in the fields, while one cat watched </span></em><span><em class="italic"><span class="kobospan" id="kobo.125.1">one mouse.</span></em></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.126.1">In the context of this sentence, “cat” and “cats” are two different forms of the same word, and “mouse” and “mice” are also two different forms of the same word. </span><span class="kobospan" id="kobo.126.2">Lemmatization would reduce these words to their </span><span><span class="kobospan" id="kobo.127.1">base forms:</span></span></p>
<p class="calibre6"><em class="italic"><span class="kobospan" id="kobo.128.1">the cat be chasing the mouse in the field, while one cat watched </span></em><span><em class="italic"><span class="kobospan" id="kobo.129.1">one mouse.</span></em></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.130.1">In this case, “cat” and “cats” have both been reduced to their base form of “cat,” and “mouse” and “mice” have both been reduced to their base form of “mouse.” </span><span class="kobospan" id="kobo.130.2">This allows for better analysis of the text since the occurrences of “cat” and “mouse” are now treated as the same term, regardless of their </span><span><span class="kobospan" id="kobo.131.1">inflectional variations.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.132.1">Lemmatization is different from stemming, which involves reducing a word to a common stem that may not necessarily be a word in its own right. </span><span class="kobospan" id="kobo.132.2">For example, the stem of “cats” and “cat” would both be “cat.” </span><span class="kobospan" id="kobo.132.3">The lemma of “cats” and “cat” would be “cat” </span><span><span class="kobospan" id="kobo.133.1">as well.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.134.1">Lemmatization can be performed</span><a id="_idIndexMarker343" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.135.1"> using various NLP libraries and tools, such as NLTK, spaCy, and </span><span><span class="kobospan" id="kobo.136.1">Stanford CoreNLP.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.137.1">Stemming</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.138.1">Stemming involves reducing words</span><a id="_idIndexMarker344" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.139.1"> to their fundamental or root form, referred to as the “stem.” </span><span class="kobospan" id="kobo.139.2">This process</span><a id="_idIndexMarker345" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.140.1"> is commonly used in NLP to prepare text for analysis, retrieval, or storage. </span><span class="kobospan" id="kobo.140.2">Stemming algorithms work by cutting off the ends or suffixes of words, leaving only </span><span><span class="kobospan" id="kobo.141.1">the stem.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.142.1">The goal of stemming is to convert all inflected or derived forms of a word into a common base form. </span><span class="kobospan" id="kobo.142.2">For example, the stem of the word “running” is “run,” and the stem of the word “runs” is </span><span><span class="kobospan" id="kobo.143.1">also “run.”</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.144.1">One commonly used stemming algorithm is the Porter stemming algorithm. </span><span class="kobospan" id="kobo.144.2">This algorithm is based on a series of rules that identify suffixes and remove them from words to obtain the stem. </span><span class="kobospan" id="kobo.144.3">For example, the Porter algorithm would convert the word “leaping” into “leap” by removing the “</span><span><span class="kobospan" id="kobo.145.1">ing” suffix.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.146.1">Let’s look at an example sentence to see stemming </span><span><span class="kobospan" id="kobo.147.1">in action:</span></span></p>
<p class="calibre6"><em class="italic"><span class="kobospan" id="kobo.148.1">They are running and leaping across </span></em><span><em class="italic"><span class="kobospan" id="kobo.149.1">the walls</span></em></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.150.1">Here’s the stemmed text (using the </span><span><span class="kobospan" id="kobo.151.1">Porter algorithm):</span></span></p>
<p class="calibre6"><em class="italic"><span class="kobospan" id="kobo.152.1">They are run and leap across </span></em><span><em class="italic"><span class="kobospan" id="kobo.153.1">the wall</span></em></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.154.1">As you can see, the words “running” and “leaping” have been converted into their base forms of “run” and “leap,” respectively, and the suffix “s” has been removed </span><span><span class="kobospan" id="kobo.155.1">from “walls.”</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.156.1">Stemming can be useful for text analysis tasks such as information retrieval or sentiment analysis as it reduces the number of unique words in a document or corpus and can help to group similar words. </span><span class="kobospan" id="kobo.156.2">However, stemming can also introduce errors as it can sometimes produce stems that are not actual words or produce stems that are not the intended base form of the word. </span><span class="kobospan" id="kobo.156.3">For example, the stemmer might produce “walk” as the stem for both “walked” and “walking,” even though “walk” and “walked” have different</span><a id="_idIndexMarker346" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.157.1"> meanings. </span><span class="kobospan" id="kobo.157.2">Therefore, it’s important to evaluate the results of stemming to ensure that it is producing accurate and </span><span><span class="kobospan" id="kobo.158.1">useful results.</span></span></p>
<h1 id="_idParaDest-82" class="calibre4"><a id="_idTextAnchor120" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.159.1">NER</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.160.1">NER is an NLP technique that’s designed</span><a id="_idIndexMarker347" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.161.1"> to detect and categorize named entities within text, including but not limited to person’s names, organization’s names, locations, and more. </span><span class="kobospan" id="kobo.161.2">NER’s primary objective is to autonomously identify and extract information about these named entities from unstructured </span><span><span class="kobospan" id="kobo.162.1">text data.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.163.1">NER typically involves</span><a id="_idIndexMarker348" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.164.1"> using machine learning models, such as </span><strong class="bold"><span class="kobospan" id="kobo.165.1">conditional random fields</span></strong><span class="kobospan" id="kobo.166.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.167.1">CRFs</span></strong><span class="kobospan" id="kobo.168.1">) or </span><strong class="bold"><span class="kobospan" id="kobo.169.1">recurrent neural networks</span></strong><span class="kobospan" id="kobo.170.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.171.1">RNNs</span></strong><span class="kobospan" id="kobo.172.1">), to tag words in a given sentence</span><a id="_idIndexMarker349" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.173.1"> with their corresponding entity types. </span><span class="kobospan" id="kobo.173.2">The models are trained on large annotated datasets that contain text with labeled entities. </span><span class="kobospan" id="kobo.173.3">These models then use context-based rules to identify named entities in </span><span><span class="kobospan" id="kobo.174.1">new text.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.175.1">There are several categories</span><a id="_idIndexMarker350" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.176.1"> of named entities that can be identified by NER, including </span><span><span class="kobospan" id="kobo.177.1">the following:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.178.1">Person</span></strong><span class="kobospan" id="kobo.179.1">: A named individual, such as “</span><span><span class="kobospan" id="kobo.180.1">Barack Obama”</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.181.1">Organization</span></strong><span class="kobospan" id="kobo.182.1">: A named company, institution, or organization, such </span><span><span class="kobospan" id="kobo.183.1">as “Google”</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.184.1">Location</span></strong><span class="kobospan" id="kobo.185.1">: A named place, such as “New </span><span><span class="kobospan" id="kobo.186.1">York City”</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.187.1">Date</span></strong><span class="kobospan" id="kobo.188.1">: A named date or time, such as “January </span><span><span class="kobospan" id="kobo.189.1">1, 2023”</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.190.1">Product</span></strong><span class="kobospan" id="kobo.191.1">: A named product or brand, such </span><a id="_idIndexMarker351" class="calibre5 pcalibre1 pcalibre"/><span><span class="kobospan" id="kobo.192.1">as “iPhone”</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.193.1">Here’s an example of how NER</span><a id="_idIndexMarker352" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.194.1"> works. </span><span class="kobospan" id="kobo.194.2">Take a look at the </span><span><span class="kobospan" id="kobo.195.1">following sentence:</span></span></p>
<p class="calibre6"><em class="italic"><span class="kobospan" id="kobo.196.1">Apple Inc. </span><span class="kobospan" id="kobo.196.2">is a technology company headquartered in </span></em><span><em class="italic"><span class="kobospan" id="kobo.197.1">Cupertino, California.</span></em></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.198.1">Here, NER would identify “Apple Inc.” </span><span class="kobospan" id="kobo.198.2">as an organization and “Cupertino, California” as a location. </span><span class="kobospan" id="kobo.198.3">The output of an NER system could be a structured representation of the sentence, as </span><span><span class="kobospan" id="kobo.199.1">shown here:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.200.1">
{"organization": "Apple Inc.",
"location": "Cupertino, California"}</span></pre> <p class="calibre6"><span class="kobospan" id="kobo.201.1">NER has many applications</span><a id="_idIndexMarker353" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.202.1"> in various fields, including </span><strong class="bold"><span class="kobospan" id="kobo.203.1">information retrieval</span></strong><span class="kobospan" id="kobo.204.1">, </span><strong class="bold"><span class="kobospan" id="kobo.205.1">question-answering</span></strong><span class="kobospan" id="kobo.206.1">, </span><strong class="bold"><span class="kobospan" id="kobo.207.1">sentiment analysis</span></strong><span class="kobospan" id="kobo.208.1">, and more. </span><span class="kobospan" id="kobo.208.2">It can be used to automatically extract structured information from unstructured text data, which can be further analyzed or used for </span><span><span class="kobospan" id="kobo.209.1">downstream tasks.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.210.1">There are different approaches and tools to perform NER, but the general steps when performing NER are </span><span><span class="kobospan" id="kobo.211.1">as follows:</span></span></p>
<ol class="calibre16">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.212.1">Data collection</span></strong><span class="kobospan" id="kobo.213.1">: The first step is to collect</span><a id="_idIndexMarker354" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.214.1"> the data that will be used for NER. </span><span class="kobospan" id="kobo.214.2">This data can be in the form of unstructured text, such as articles, social media posts, or </span><span><span class="kobospan" id="kobo.215.1">web pages.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.216.1">Preprocessing</span></strong><span class="kobospan" id="kobo.217.1">: The next step is to preprocess</span><a id="_idIndexMarker355" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.218.1"> the data, which involves various steps such as tokenization, stop word removal, stemming or lemmatization, </span><span><span class="kobospan" id="kobo.219.1">and normalization.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.220.1">Labeling</span></strong><span class="kobospan" id="kobo.221.1">: After preprocessing, the next step</span><a id="_idIndexMarker356" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.222.1"> is to label the data with named entity tags. </span><span class="kobospan" id="kobo.222.2">There are different tagging schemes, but one</span><a id="_idIndexMarker357" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.223.1"> of the most commonly used is the </span><strong class="bold"><span class="kobospan" id="kobo.224.1">Inside-Outside-Beginning</span></strong><span class="kobospan" id="kobo.225.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.226.1">IOB</span></strong><span class="kobospan" id="kobo.227.1">) tagging scheme. </span><span class="kobospan" id="kobo.227.2">In this scheme, each word in the text is labeled as either </span><strong class="bold"><span class="kobospan" id="kobo.228.1">B</span></strong><span class="kobospan" id="kobo.229.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.230.1">beginning of a named entity</span></strong><span class="kobospan" id="kobo.231.1">), </span><strong class="bold"><span class="kobospan" id="kobo.232.1">I</span></strong><span class="kobospan" id="kobo.233.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.234.1">inside of a named entity</span></strong><span class="kobospan" id="kobo.235.1">), or </span><strong class="bold"><span class="kobospan" id="kobo.236.1">O</span></strong><span class="kobospan" id="kobo.237.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.238.1">outside of a </span></strong><span><strong class="bold"><span class="kobospan" id="kobo.239.1">named entity</span></strong></span><span><span class="kobospan" id="kobo.240.1">).</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.241.1">Training</span></strong><span class="kobospan" id="kobo.242.1">: Once the data has been</span><a id="_idIndexMarker358" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.243.1"> labeled, the next step is to train a machine learning model to recognize named entities in new, unseen text. </span><span class="kobospan" id="kobo.243.2">Different types of models can be used for NER, such as rule-based systems, statistical models, and deep </span><span><span class="kobospan" id="kobo.244.1">learning models.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.245.1">Evaluation</span></strong><span class="kobospan" id="kobo.246.1">: After training the model, it is important</span><a id="_idIndexMarker359" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.247.1"> to evaluate its performance on a test dataset. </span><span class="kobospan" id="kobo.247.2">This can help identify any issues with the model, such as overfitting, underfitting, </span><span><span class="kobospan" id="kobo.248.1">or bias.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.249.1">Deployment</span></strong><span class="kobospan" id="kobo.250.1">: Finally, the trained model can be deployed</span><a id="_idIndexMarker360" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.251.1"> to perform NER on new, unseen text. </span><span class="kobospan" id="kobo.251.2">This can be done in real time or in batch mode, depending on the </span><span><span class="kobospan" id="kobo.252.1">application’s requirements.</span></span></li>
</ol>
<p class="calibre6"><span class="kobospan" id="kobo.253.1">Here’s an example of how NER can </span><span><span class="kobospan" id="kobo.254.1">be performed:</span></span></p>
<p class="calibre6"><span><span class="kobospan" id="kobo.255.1">Original text:</span></span></p>
<p class="calibre6"><em class="italic"><span class="kobospan" id="kobo.256.1">Apple is negotiating to buy a Chinese start-up </span></em><span><em class="italic"><span class="kobospan" id="kobo.257.1">this year.</span></em></span></p>
<p class="calibre6"><span><span class="kobospan" id="kobo.258.1">Preprocessed text:</span></span></p>
<p class="calibre6"><em class="italic"><span class="kobospan" id="kobo.259.1">apple negotiating buy Chinese </span></em><span><em class="italic"><span class="kobospan" id="kobo.260.1">start-up year</span></em></span></p>
<p class="calibre6"><span><span class="kobospan" id="kobo.261.1">Tagged text:</span></span></p>
<p class="calibre6"><em class="italic"><span class="kobospan" id="kobo.262.1">B-ORG   O   O   B-LOC   O   O</span></em></p>
<p class="calibre6"><span class="kobospan" id="kobo.263.1">In this example, the named </span><a id="_idIndexMarker361" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.264.1">entities “Apple” and “Chinese” are identified as an organization (B-ORG) and a location (B-LOC), respectively. </span><span class="kobospan" id="kobo.264.2">“this year” is not recognized as a named entity in this example, but it would be if a more complex tagging scheme is used or if the model is trained on data that would </span><span><span class="kobospan" id="kobo.265.1">promote that.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.266.1">Several libraries can be used for NER, depending on the programming language and specific needs of the project. </span><span class="kobospan" id="kobo.266.2">Let’s take a look at some commonly </span><span><span class="kobospan" id="kobo.267.1">used libraries:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.268.1">spaCy</span></strong><span class="kobospan" id="kobo.269.1">: </span><strong class="bold"><span class="kobospan" id="kobo.270.1">spaCy</span></strong><span class="kobospan" id="kobo.271.1"> is a widely used open source library</span><a id="_idIndexMarker362" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.272.1"> designed for various NLP</span><a id="_idIndexMarker363" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.273.1"> tasks, including NER. </span><span class="kobospan" id="kobo.273.2">Offering pre-trained models across multiple languages, the library additionally empowers users to undertake model training for distinct domains tailored to their </span><span><span class="kobospan" id="kobo.274.1">specific needs.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.275.1">NLTK</span></strong><span class="kobospan" id="kobo.276.1">: This is another widely used library</span><a id="_idIndexMarker364" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.277.1"> for NLP tasks, including</span><a id="_idIndexMarker365" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.278.1"> NER. </span><span class="kobospan" id="kobo.278.2">It provides several pre-trained models and also allows users to train </span><span><span class="kobospan" id="kobo.279.1">their models.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.280.1">Stanford Named Entity Recognizer</span></strong><span class="kobospan" id="kobo.281.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.282.1">NER</span></strong><span class="kobospan" id="kobo.283.1">): This is a Java-based NER tool </span><a id="_idIndexMarker366" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.284.1">that provides pre-trained</span><a id="_idIndexMarker367" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.285.1"> models for several languages, including English, German, </span><span><span class="kobospan" id="kobo.286.1">and Chinese.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.287.1">AllenNLP</span></strong><span class="kobospan" id="kobo.288.1">: AllenNLP is a popular open source</span><a id="_idIndexMarker368" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.289.1"> library for building</span><a id="_idIndexMarker369" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.290.1"> and evaluating NLP models, including NER. </span><span class="kobospan" id="kobo.290.2">It provides pre-trained models for several tasks, including NER, and also allows users to train their </span><span><span class="kobospan" id="kobo.291.1">own models.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.292.1">Flair</span></strong><span class="kobospan" id="kobo.293.1">: Flair is a Python library</span><a id="_idIndexMarker370" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.294.1"> for state-of-the-art</span><a id="_idIndexMarker371" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.295.1"> NLP, including NER. </span><span class="kobospan" id="kobo.295.2">It provides pre-trained models for several languages and also allows users to train their </span><span><span class="kobospan" id="kobo.296.1">own models.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.297.1">General Architecture for Text Engineering</span></strong><span class="kobospan" id="kobo.298.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.299.1">GATE</span></strong><span class="kobospan" id="kobo.300.1">): This is a suite of tools for NLP, including</span><a id="_idIndexMarker372" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.301.1"> NER. </span><span class="kobospan" id="kobo.301.2">It provides</span><a id="_idIndexMarker373" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.302.1"> a graphical interface for creating and evaluating NLP models and also allows users to develop custom plugins for </span><span><span class="kobospan" id="kobo.303.1">specific tasks.</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.304.1">There are many other libraries available for NER, and the choice of library will depend on factors such as the programming language, available models, and specific requirements of the project. </span><span class="kobospan" id="kobo.304.2">In the next section, we will explain POS tagging and different methods to perform </span><span><span class="kobospan" id="kobo.305.1">this task.</span></span></p>
<h1 id="_idParaDest-83" class="calibre4"><a id="_idTextAnchor121" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.306.1">POS tagging</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.307.1">POS tagging is the practice of attributing</span><a id="_idIndexMarker374" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.308.1"> grammatical labels, such as nouns, verbs, adjectives, and others, to individual words within a sentence. </span><span class="kobospan" id="kobo.308.2">This tagging process holds significance as a foundational step in various NLP tasks, including text classification, sentiment analysis, and </span><span><span class="kobospan" id="kobo.309.1">machine translation.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.310.1">POS tagging can be performed using different approaches such as rule-based methods, statistical methods, and deep learning-based methods. </span><span class="kobospan" id="kobo.310.2">In this section, we’ll provide a brief overview of </span><span><span class="kobospan" id="kobo.311.1">each approach.</span></span></p>
<h2 id="_idParaDest-84" class="calibre7"><a id="_idTextAnchor122" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.312.1">Rule-based methods</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.313.1">Rule-based methods for</span><a id="_idIndexMarker375" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.314.1"> POS tagging</span><a id="_idIndexMarker376" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.315.1"> involve defining a set of rules or patterns that can be used to automatically tag words in a text with their corresponding parts of speech, such as nouns, verbs, adjectives, and </span><span><span class="kobospan" id="kobo.316.1">so on.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.317.1">The process involves defining a set of rules or patterns for identifying the different parts of speech in a sentence. </span><span class="kobospan" id="kobo.317.2">For example, a rule may state that any word ending in “-ing” is a gerund (a verb acting as a noun), while another rule may state that any word preceded by an article such as “a” or “an” is likely </span><span><span class="kobospan" id="kobo.318.1">a noun.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.319.1">These rules are typically based on linguistic knowledge, such as knowledge of grammar and syntax, and are often specific to a particular language. </span><span class="kobospan" id="kobo.319.2">They can also be supplemented with lexicons or dictionaries that provide additional information about the meanings and usage </span><span><span class="kobospan" id="kobo.320.1">of words.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.321.1">The process of rule-based tagging involves applying these rules to a given text and identifying the parts of speech for each word. </span><span class="kobospan" id="kobo.321.2">This can be done manually but is typically automated using software tools and programming languages that support regular expressions and </span><span><span class="kobospan" id="kobo.322.1">pattern matching.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.323.1">One advantage of rule-based methods is that they can be highly accurate when the rules are well-designed and cover a wide range of linguistic phenomena. </span><span class="kobospan" id="kobo.323.2">They can also be customized to specific domains or genres of text, such as scientific literature or </span><span><span class="kobospan" id="kobo.324.1">legal documents.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.325.1">However, one limitation of rule-based methods is that they may not be able to capture the full complexity and variability of natural language, and may require significant effort to develop and maintain the rules as language evolves and changes over time. </span><span class="kobospan" id="kobo.325.2">They may also struggle with ambiguity, such as in cases where a word can have multiple possible parts of speech depending on </span><span><span class="kobospan" id="kobo.326.1">the context.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.327.1">Despite these limitations, rule-based methods</span><a id="_idIndexMarker377" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.328.1"> for POS tagging remain</span><a id="_idIndexMarker378" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.329.1"> an important approach in NLP, especially for applications that require high accuracy </span><span><span class="kobospan" id="kobo.330.1">and precision.</span></span></p>
<h2 id="_idParaDest-85" class="calibre7"><a id="_idTextAnchor123" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.331.1">Statistical methods</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.332.1">Statistical methods for POS tagging</span><a id="_idIndexMarker379" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.333.1"> are based on using probabilistic models</span><a id="_idIndexMarker380" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.334.1"> to automatically assign the most likely POS tag to each word in a sentence. </span><span class="kobospan" id="kobo.334.2">These methods rely on a training corpus of tagged text, where the POS tags have already been assigned to the words, to learn the probabilities of a particular word being associated with </span><span><span class="kobospan" id="kobo.335.1">each tag.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.336.1">Two main types of statistical</span><a id="_idIndexMarker381" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.337.1"> methods are used for POS tagging: </span><strong class="bold"><span class="kobospan" id="kobo.338.1">Hidden Markov Models</span></strong><span class="kobospan" id="kobo.339.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.340.1">HMMs</span></strong><span class="kobospan" id="kobo.341.1">) </span><span><span class="kobospan" id="kobo.342.1">and CRFs.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.343.1">HMMs serve as a category of probabilistic models that are extensively applied in handling sequential data, including text. </span><span class="kobospan" id="kobo.343.2">In the context of POS tagging, HMMs represent the probability distribution of a sequence of POS tags concerning a sequence of words. </span><span class="kobospan" id="kobo.343.3">HMMs assume that the likelihood of a POS tag at a specific position within a sentence is contingent solely upon the preceding tag in the sequence. </span><span class="kobospan" id="kobo.343.4">Furthermore, they presume that the likelihood of a particular word, given its tag, remains independent of other words within the sentence. </span><span class="kobospan" id="kobo.343.5">To identify the most probable sequence of POS tags for a given sentence, HMMs employ the </span><span><span class="kobospan" id="kobo.344.1">Viterbi algorithm.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.345.1">CRFs are another type of probabilistic model that is commonly used for sequence labeling tasks, including POS tagging. </span><span class="kobospan" id="kobo.345.2">CRFs differ from HMMs in that they model the conditional probability of the output sequence (that is, the POS tags) given the input sequence (that is, the words), rather than the joint probability of the output and input sequences. </span><span class="kobospan" id="kobo.345.3">This allows CRFs to capture more complex dependencies between the input and output sequences than HMMs. </span><span class="kobospan" id="kobo.345.4">CRFs use an iterative algorithm, such as gradient descent or L-BFGS, to learn the optimal set of weights for </span><span><span class="kobospan" id="kobo.346.1">the model.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.347.1">Let’s look at the advantages</span><a id="_idIndexMarker382" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.348.1"> of </span><span><span class="kobospan" id="kobo.349.1">statistical methods:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><span class="kobospan" id="kobo.350.1">Statistical methods can capture the context of a word and the relationships between words in a sentence, leading to more accurate </span><span><span class="kobospan" id="kobo.351.1">tagging results</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.352.1">These methods can handle unseen words and sentences that are not present in the </span><span><span class="kobospan" id="kobo.353.1">training data</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.354.1">Statistical methods can be trained on large datasets, allowing them to capture more variations</span><a id="_idIndexMarker383" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.355.1"> and patterns in </span><span><span class="kobospan" id="kobo.356.1">the language</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.357.1">Now, let’s look</span><a id="_idIndexMarker384" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.358.1"> at </span><span><span class="kobospan" id="kobo.359.1">the disadvantages:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><span class="kobospan" id="kobo.360.1">These methods require a large amount of annotated data for training, which can be time-consuming and expensive </span><span><span class="kobospan" id="kobo.361.1">to create</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.362.1">Statistical methods can be sensitive to the quality of the training data and may perform poorly if the data is noisy </span><span><span class="kobospan" id="kobo.363.1">or biased</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.364.1">Statistical models are typically black boxes, making it difficult to interpret the decisions made by </span><span><span class="kobospan" id="kobo.365.1">the model</span></span></li>
</ul>
<h2 id="_idParaDest-86" class="calibre7"><a id="_idTextAnchor124" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.366.1">Deep learning-based methods</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.367.1">Deep learning-based methods</span><a id="_idIndexMarker385" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.368.1"> for POS tagging</span><a id="_idIndexMarker386" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.369.1"> involve training a neural network model to predict the POS tags for each word in a given sentence. </span><span class="kobospan" id="kobo.369.2">These methods can learn complex patterns and relationships in the text data to accurately tag words with their appropriate parts </span><span><span class="kobospan" id="kobo.370.1">of speech.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.371.1">One of the most popular deep learning-based methods for POS tagging is using an RNN with LSTM cells. </span><span class="kobospan" id="kobo.371.2">LSTM-based models can process sequences of words and capture dependencies between them. </span><span class="kobospan" id="kobo.371.3">The input to the model is a sequence of word embeddings, which are vector representations of words in a high-dimensional space. </span><span class="kobospan" id="kobo.371.4">These embeddings are learned during the </span><span><span class="kobospan" id="kobo.372.1">training process.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.373.1">The LSTM-based model is comprised of three main layers: an input layer, an LSTM layer, and an output layer. </span><span class="kobospan" id="kobo.373.2">The structure involves taking word embeddings as input into the input layer. </span><span class="kobospan" id="kobo.373.3">Subsequently, the LSTM layer processes the sequence of these embeddings, aiming to grasp the interdependencies inherent within them. </span><span class="kobospan" id="kobo.373.4">Ultimately, the output layer is responsible for predicting the POS tag for each word within the input sequence. </span><span class="kobospan" id="kobo.373.5">Another popular deep learning-based method for POS tagging is using</span><a id="_idIndexMarker387" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.374.1"> a transformer-based model, such as </span><strong class="bold"><span class="kobospan" id="kobo.375.1">Bidirectional Encoder Representations from Transformers</span></strong><span class="kobospan" id="kobo.376.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.377.1">BERT</span></strong><span class="kobospan" id="kobo.378.1">). </span><span class="kobospan" id="kobo.378.2">BERT is a language model that comes pre-trained and employs a transformer-based architecture to acquire a profound understanding of contextual relationships among words within a sentence. </span><span class="kobospan" id="kobo.378.3">It undergoes training with vast quantities of text data and can be fine-tuned to excel in diverse NLP tasks, one of which is </span><span><span class="kobospan" id="kobo.379.1">POS tagging.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.380.1">To use BERT for POS tagging, the input sentence must be tokenized, and each token must be assigned an initial POS tag. </span><span class="kobospan" id="kobo.380.2">The token embeddings are then fed into the pre-trained BERT model, which outputs contextualized embeddings for each token. </span><span class="kobospan" id="kobo.380.3">These embeddings are passed through a feedforward neural network to predict the final POS tag for </span><span><span class="kobospan" id="kobo.381.1">each token.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.382.1">Deep learning approaches for POS tagging have demonstrated leading-edge performance across numerous benchmark datasets. </span><span class="kobospan" id="kobo.382.2">Nonetheless, their effectiveness demands substantial training data and computational resources, and the training process can be time-consuming. </span><span class="kobospan" id="kobo.382.3">Moreover, they may suffer from a lack of interpretability, which makes it difficult to understand how the model is making </span><span><span class="kobospan" id="kobo.383.1">its predictions.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.384.1">Several libraries are available</span><a id="_idIndexMarker388" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.385.1"> for performing POS tagging in various programming</span><a id="_idIndexMarker389" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.386.1"> languages, including Python, Java, and C++. </span><span class="kobospan" id="kobo.386.2">Some popular NLP libraries that provide POS tagging functionality include NLTK, spaCy, Stanford CoreNLP, and </span><span><span class="kobospan" id="kobo.387.1">Apache OpenNLP.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.388.1">Here is an example of POS tagging using the NLTK library </span><span><span class="kobospan" id="kobo.389.1">in Python:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.390.1">
import nltk
input_sentence = "The young white cat jumps over the lazy dog"
processed_tokens = nltk.word_tokenize(input_sentence)
tags = nltk.pos_tag(processed_tokens)
print(tags)</span></pre> <p class="calibre6"><span class="kobospan" id="kobo.391.1">The output is </span><span><span class="kobospan" id="kobo.392.1">as follows:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.393.1">
[('The', 'DT'), (young, 'JJ'), (white, 'NN'), ('cat', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]</span></pre> <p class="calibre6"><span class="kobospan" id="kobo.394.1">In this example, the </span><strong class="source-inline"><span class="kobospan" id="kobo.395.1">nltk.pos_tag()</span></strong><span class="kobospan" id="kobo.396.1"> function is used to tag the words in the sentence. </span><span class="kobospan" id="kobo.396.2">The function returns a list of tuples </span><a id="_idIndexMarker390" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.397.1">where each tuple contains</span><a id="_idIndexMarker391" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.398.1"> a word and its POS tag. </span><span class="kobospan" id="kobo.398.2">The POS</span><a id="_idIndexMarker392" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.399.1"> tags that have been used here are based on the </span><strong class="bold"><span class="kobospan" id="kobo.400.1">Penn </span></strong><span><strong class="bold"><span class="kobospan" id="kobo.401.1">Treebank tagset</span></strong></span><span><span class="kobospan" id="kobo.402.1">.</span></span></p>
<h2 id="_idParaDest-87" class="calibre7"><a id="_idTextAnchor125" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.403.1">Regular expressions</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.404.1">A regular expression is a type</span><a id="_idIndexMarker393" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.405.1"> of text pattern</span><a id="_idIndexMarker394" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.406.1"> that has various applications in modern programming languages and software. </span><span class="kobospan" id="kobo.406.2">They are useful for validating whether an input conforms to a particular text pattern, locating text within a larger text body that matches the pattern, replacing text that matches the pattern with alternative text or rearranging parts of the matched text, and dividing a block of text into a list of subtexts, but can cause unintended consequences if </span><span><span class="kobospan" id="kobo.407.1">used incorrectly.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.408.1">In computer science and mathematics, the term </span><strong class="bold"><span class="kobospan" id="kobo.409.1">regular expression</span></strong><span class="kobospan" id="kobo.410.1"> is derived from the concept of “regularity” in </span><span><span class="kobospan" id="kobo.411.1">mathematical expressions.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.412.1">A regular expression, often referred to as regex or regexp, is a series</span><a id="_idIndexMarker395" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.413.1"> of characters that constitutes a search pattern. </span><span class="kobospan" id="kobo.413.2">Regular expressions are used to match and manipulate text, typically in the context of text processing, search algorithms, </span><span><span class="kobospan" id="kobo.414.1">and NLP.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.415.1">A regular expression comprises a mix of characters and metacharacters, which collectively establish a pattern to search for within a text string. </span><span class="kobospan" id="kobo.415.2">The simplest form of a regular expression is a mere sequence of characters that must be matched precisely. </span><span class="kobospan" id="kobo.415.3">For example, the regular expression “hello” would match any string that contains the characters “hello” </span><span><span class="kobospan" id="kobo.416.1">in sequence.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.417.1">Metacharacters are unique characters within regular expressions that possess pre-defined meanings. </span><span class="kobospan" id="kobo.417.2">For instance, the “.” </span><span class="kobospan" id="kobo.417.3">(dot) metacharacter is employed to match any individual character, whereas the “*” (asterisk) metacharacter is used to match zero or more instances of the preceding characters or group. </span><span class="kobospan" id="kobo.417.4">Regular expressions can be used for a wide range of text-processing tasks. </span><span class="kobospan" id="kobo.417.5">Let’s take a </span><span><span class="kobospan" id="kobo.418.1">closer look.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.419.1">Validating input</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.420.1">Regular expressions can be used</span><a id="_idIndexMarker396" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.421.1"> to validate input</span><a id="_idIndexMarker397" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.422.1"> by matching it against a pattern. </span><span class="kobospan" id="kobo.422.2">For example, you can use a regular expression to validate an email address or a </span><span><span class="kobospan" id="kobo.423.1">phone number.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.424.1">Text manipulation</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.425.1">Text manipulation using regular expressions</span><a id="_idIndexMarker398" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.426.1"> involves using</span><a id="_idIndexMarker399" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.427.1"> pattern-matching techniques to find and manipulate text strings in a document or dataset. </span><span class="kobospan" id="kobo.427.2">Regular expressions are powerful tools for working with text data, allowing for complex search and replace operations, text extraction, </span><span><span class="kobospan" id="kobo.428.1">and formatting.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.429.1">Some common text manipulation tasks that can be accomplished with regular expressions are </span><span><span class="kobospan" id="kobo.430.1">as follows:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.431.1">Search and replace</span></strong><span class="kobospan" id="kobo.432.1">: Using regular expressions to search for specific patterns or character sequences in a document and replace them with other text </span><span><span class="kobospan" id="kobo.433.1">or formatting</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.434.1">Data extraction</span></strong><span class="kobospan" id="kobo.435.1">: Regular expressions can be used for data extraction from text by defining patterns that match specific </span><span><span class="kobospan" id="kobo.436.1">data formats</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.437.1">Here are the general steps for using</span><a id="_idIndexMarker400" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.438.1"> regular expressions for </span><span><span class="kobospan" id="kobo.439.1">data extraction:</span></span></p>
<ol class="calibre16">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.440.1">Define a regular expression pattern</span></strong><span class="kobospan" id="kobo.441.1">: The first step is to define a regular expression pattern that matches the data you want to extract. </span><span class="kobospan" id="kobo.441.2">For example, if you want to extract all phone numbers from a text document, you can define a pattern that matches the format of a </span><span><span class="kobospan" id="kobo.442.1">phone number.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.443.1">Compile the regular expression pattern</span></strong><span class="kobospan" id="kobo.444.1">: After establishing the regular expression pattern, the next step involves compiling it into a regular expression object, which can then be utilized for </span><span><span class="kobospan" id="kobo.445.1">matching purposes.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.446.1">Search for the pattern in the text</span></strong><span class="kobospan" id="kobo.447.1">: Once you have compiled the regular expression object, you can use it to search for the pattern in the text. </span><span class="kobospan" id="kobo.447.2">You can search for the pattern in a single string or a larger block </span><span><span class="kobospan" id="kobo.448.1">of text.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.449.1">Extract the matched data</span></strong><span class="kobospan" id="kobo.450.1">: After you have searched for the pattern in the text, you can extract the data that matches that pattern. </span><span class="kobospan" id="kobo.450.2">You can extract all occurrences of the matched data or only the </span><span><span class="kobospan" id="kobo.451.1">first occurrence.</span></span></li>
</ol>
<p class="calibre6"><span class="kobospan" id="kobo.452.1">Here’s an example of how to extract all email addresses from a string using regular expressions </span><span><span class="kobospan" id="kobo.453.1">in Python:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.454.1">
import re
text = "John's email is john@example.com and Jane's email is jane@example.com"
# Pattern for email addresses:
pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
regex = re.compile(pattern)
# Search for all occurrences of the pattern in the text:
matches = regex.findall(text)
print(matches)</span></pre> <p class="calibre6"><span class="kobospan" id="kobo.455.1">Here’s </span><span><span class="kobospan" id="kobo.456.1">the</span></span><span><a id="_idIndexMarker401" class="calibre5 pcalibre1 pcalibre"/></span><span><span class="kobospan" id="kobo.457.1"> output:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.458.1">
['john@example.com', 'jane@example.com']</span></pre> <p class="calibre6"><span class="kobospan" id="kobo.459.1">Next, we’ll cover </span><span><span class="kobospan" id="kobo.460.1">text cleaning.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.461.1">Text cleaning</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.462.1">Text cleaning means</span><a id="_idIndexMarker402" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.463.1"> using regular expressions</span><a id="_idIndexMarker403" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.464.1"> to clean and standardize text data, thereby removing unwanted characters, whitespace, or </span><span><span class="kobospan" id="kobo.465.1">other formatting.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.466.1">Here are some common text-cleaning</span><a id="_idIndexMarker404" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.467.1"> techniques that use </span><span><span class="kobospan" id="kobo.468.1">regular expressions:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.469.1">Removing special characters</span></strong><span class="kobospan" id="kobo.470.1">: Regular expressions can be used to match and remove specific characters such as punctuation marks, brackets, and other special symbols. </span><span class="kobospan" id="kobo.470.2">For example, the </span><strong class="source-inline1"><span class="kobospan" id="kobo.471.1">[^a-zA-Z0-9]</span></strong><span class="kobospan" id="kobo.472.1"> regular expression will match any </span><span><span class="kobospan" id="kobo.473.1">non-alphanumeric character.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.474.1">Removing stop words</span></strong><span class="kobospan" id="kobo.475.1">: Stop words are common words such as “the,” “and,” and “but” that are often removed from text to focus on the most meaningful words. </span><span class="kobospan" id="kobo.475.2">Regular expressions can be used to match and remove these words </span><span><span class="kobospan" id="kobo.476.1">from text.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.477.1">Removing HTML tags</span></strong><span class="kobospan" id="kobo.478.1">: If you’re working with text that has been scraped from a website, you may need to remove HTML tags before analyzing the text. </span><span class="kobospan" id="kobo.478.2">Regular expressions can be used to match and remove </span><span><span class="kobospan" id="kobo.479.1">HTML tags.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.480.1">Converting text into lowercase</span></strong><span class="kobospan" id="kobo.481.1">: Regular expressions can be used to convert all text into lowercase or uppercase, which can make it easier to compare </span><span><span class="kobospan" id="kobo.482.1">and analyze.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.483.1">Normalizing text</span></strong><span class="kobospan" id="kobo.484.1">: Normalization involves transforming text into a standard format. </span><span class="kobospan" id="kobo.484.2">Regular expressions can be</span><a id="_idIndexMarker405" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.485.1"> used to perform tasks such as stemming and lemmatization, which involves reducing words to their </span><span><span class="kobospan" id="kobo.486.1">root form.</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.487.1">By using regular expressions</span><a id="_idIndexMarker406" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.488.1"> for text cleaning, you can remove noise and irrelevant information from text, making it easier to analyze and extract </span><span><span class="kobospan" id="kobo.489.1">meaningful insights.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.490.1">Parsing</span></h3>
<p class="calibre6"><strong class="bold"><span class="kobospan" id="kobo.491.1">Parsing</span></strong><span class="kobospan" id="kobo.492.1"> involves analyzing a text string</span><a id="_idIndexMarker407" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.493.1"> to discern its grammatical </span><a id="_idIndexMarker408" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.494.1">structure according to a specified grammar. </span><span class="kobospan" id="kobo.494.2">Regular expressions serve as potent instruments for text parsing, especially when dealing with uncomplicated and regular </span><span><span class="kobospan" id="kobo.495.1">grammatical patterns.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.496.1">To parse text using regular expressions, you need to define a grammar for the language you want to parse. </span><span class="kobospan" id="kobo.496.2">The grammar should specify the possible components of a sentence, such as nouns, verbs, adjectives, and so on, as well as the rules that dictate how these components can be combined to form </span><span><span class="kobospan" id="kobo.497.1">valid sentences.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.498.1">Once you have defined the grammar, you can use regular expressions to identify the individual components of a sentence and the relationships between them. </span><span class="kobospan" id="kobo.498.2">For example, you can use regular expressions to match all the nouns in a sentence or to identify the subject and object of </span><span><span class="kobospan" id="kobo.499.1">a verb.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.500.1">One common approach to parsing with regular expressions is to define a set of patterns that correspond to the different parts of speech and sentence structures in your grammar. </span><span class="kobospan" id="kobo.500.2">For example, you might define a pattern for matching nouns, a pattern for matching verbs, and a pattern for matching sentences that consist of a subject followed by a verb and </span><span><span class="kobospan" id="kobo.501.1">an object.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.502.1">To use these patterns for parsing, you would apply them to a text string using a regular expression engine, which would match the patterns to the appropriate parts of the string. </span><span class="kobospan" id="kobo.502.2">The output of the parsing process would be a parse tree or other data structure that represents the grammatical structure of </span><span><span class="kobospan" id="kobo.503.1">the sentence.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.504.1">One limitation of regular expression parsing is that it is generally not suitable for handling more complex or ambiguous grammar. </span><span class="kobospan" id="kobo.504.2">For example, it can be difficult to handle cases where a word could be either a noun or a verb depending on the context, or where the structure of a sentence </span><span><span class="kobospan" id="kobo.505.1">is ambiguous.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.506.1">We can also use regular expressions to break a larger text document into smaller chunks or tokens based on specific patterns </span><span><span class="kobospan" id="kobo.507.1">or delimiters.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.508.1">To use regular expressions for text manipulation, you typically need to define a pattern that matches the text you want to find or manipulate. </span><span class="kobospan" id="kobo.508.2">This pattern can include special characters and syntax to define the specific sequence of characters, numbers, or other elements that make up the </span><span><span class="kobospan" id="kobo.509.1">text string.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.510.1">For example, the regular expression pattern </span><em class="italic"><span class="kobospan" id="kobo.511.1">\d{3}-\d{2}-\d{4}</span></em><span class="kobospan" id="kobo.512.1"> might be used to search for and extract Social Security numbers in a larger text document. </span><span class="kobospan" id="kobo.512.2">This pattern matches a sequence of three digits, followed by a dash, then two more digits, another dash, and four final digits followed by a non-digit, which together represent the standard format for a Social Security number in </span><span><span class="kobospan" id="kobo.513.1">the USA.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.514.1">Once you have defined your regular expression pattern, you can use it with various text manipulation tools and programming languages, such as grep, sed, awk, Perl, Python, and many others, to perform complex text </span><span><span class="kobospan" id="kobo.515.1">manipulation tasks.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.516.1">Some programming languages, such as Perl and Python, have built-in support for regular expressions. </span><span class="kobospan" id="kobo.516.2">Other programming languages, such as Java and C++, require you to use a library or API to work with </span><span><span class="kobospan" id="kobo.517.1">regular expressions.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.518.1">While regular expressions are powerful tools</span><a id="_idIndexMarker409" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.519.1"> for text processing, they can also be complex</span><a id="_idIndexMarker410" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.520.1"> and difficult to understand. </span><span class="kobospan" id="kobo.520.2">It’s important to be familiar with the syntax and behavior of regular expressions to use them effectively in </span><span><span class="kobospan" id="kobo.521.1">your code.</span></span></p>
<h2 id="_idParaDest-88" class="calibre7"><a id="_idTextAnchor126" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.522.1">Tokenization</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.523.1">Tokenization is a process in NLP</span><a id="_idIndexMarker411" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.524.1"> that involves breaking down a piece of text or a sentence into individual words or terms, known as tokens. </span><span class="kobospan" id="kobo.524.2">The tokenization process can be applied to various forms of data, such as textual documents, social media posts, web pages, </span><span><span class="kobospan" id="kobo.525.1">and more.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.526.1">The tokenization process is an important initial step in many NLP tasks as it transforms unstructured text data into a structured format that can be analyzed using machine learning algorithms or other techniques. </span><span class="kobospan" id="kobo.526.2">These tokens can be used to perform various operations in the text, such as counting word frequencies, identifying the most common phrases, and </span><span><span class="kobospan" id="kobo.527.1">so on.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.528.1">There are different methods </span><span><span class="kobospan" id="kobo.529.1">of tokenization:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.530.1">Word tokenization</span></strong><span class="kobospan" id="kobo.531.1">: This method splits a piece</span><a id="_idIndexMarker412" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.532.1"> of text into individual </span><a id="_idIndexMarker413" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.533.1">words or tokens using whitespace, punctuation, and other characters as delimiters. </span><span class="kobospan" id="kobo.533.2">For example, take a look at the </span><span><span class="kobospan" id="kobo.534.1">following sentence:</span></span><p class="calibre6"><em class="italic"><span class="kobospan" id="kobo.535.1">The nimble white cat jumps over the </span></em><span><em class="italic"><span class="kobospan" id="kobo.536.1">sleepy dog</span></em></span></p><p class="calibre6"><span class="kobospan" id="kobo.537.1">This can be tokenized into the following list </span><span><span class="kobospan" id="kobo.538.1">of words:</span></span></p><p class="calibre6"><em class="italic"><span class="kobospan" id="kobo.539.1">[“The”, “nimble”, “white”, “cat”, “jumps”, “over”, “the”, “</span></em><span><em class="italic"><span class="kobospan" id="kobo.540.1">sleepy”, “dog”]</span></em></span></p></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.541.1">Sentence tokenization</span></strong><span class="kobospan" id="kobo.542.1">: This method splits a piece of text</span><a id="_idIndexMarker414" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.543.1"> into individual sentences by using punctuation</span><a id="_idIndexMarker415" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.544.1"> marks such as periods, exclamation marks, and question marks as delimiters. </span><span class="kobospan" id="kobo.544.2">For example, take a look at the </span><span><span class="kobospan" id="kobo.545.1">following paragraph:</span></span><p class="calibre6"><em class="italic"><span class="kobospan" id="kobo.546.1">This is the </span></em><span><em class="italic"><span class="kobospan" id="kobo.547.1">first sentence.</span></em></span></p><p class="calibre6"><em class="italic"><span class="kobospan" id="kobo.548.1">This is the </span></em><span><em class="italic"><span class="kobospan" id="kobo.549.1">second sentence.</span></em></span></p><p class="calibre6"><em class="italic"><span class="kobospan" id="kobo.550.1">This is the </span></em><span><em class="italic"><span class="kobospan" id="kobo.551.1">third sentence</span></em></span><span><span class="kobospan" id="kobo.552.1">.</span></span></p><p class="calibre6"><span class="kobospan" id="kobo.553.1">This can be tokenized into the following list </span><span><span class="kobospan" id="kobo.554.1">of sentences:</span></span></p><p class="calibre6"><em class="italic"><span class="kobospan" id="kobo.555.1">[“This is the </span></em><span><em class="italic"><span class="kobospan" id="kobo.556.1">first sentence.”,</span></em></span></p><p class="calibre6"><em class="italic"><span class="kobospan" id="kobo.557.1">“This is the </span></em><span><em class="italic"><span class="kobospan" id="kobo.558.1">second sentence.”,</span></em></span></p><p class="calibre6"><em class="italic"><span class="kobospan" id="kobo.559.1">“This is the </span></em><span><em class="italic"><span class="kobospan" id="kobo.560.1">third sentence.”]</span></em></span></p></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.561.1">Regular expression tokenization</span></strong><span class="kobospan" id="kobo.562.1">: This method uses regular</span><a id="_idIndexMarker416" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.563.1"> expressions to define the tokenization</span><a id="_idIndexMarker417" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.564.1"> rules. </span><span class="kobospan" id="kobo.564.2">Regular expressions can be used to match patterns in the text, such as email addresses, URLs, or phone numbers, and extract them as </span><span><span class="kobospan" id="kobo.565.1">individual tokens.</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.566.1">Tokenization is an important step in NLP and is used in many applications, such as sentiment analysis, document classification, machine translation, </span><span><span class="kobospan" id="kobo.567.1">and more.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.568.1">Tokenization is also an important step in language models. </span><span class="kobospan" id="kobo.568.2">For example, in BERT, which is a well-known language model, a tokenizer is a sub-word tokenizer, meaning it breaks down words into</span><a id="_idIndexMarker418" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.569.1"> smaller sub-word units called tokens. </span><span class="kobospan" id="kobo.569.2">It uses </span><strong class="bold"><span class="kobospan" id="kobo.570.1">WordPiece</span></strong><span class="kobospan" id="kobo.571.1"> tokenization, which is a data-driven</span><a id="_idIndexMarker419" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.572.1"> approach that builds a large vocabulary of sub-words based on the corpus of text being </span><span><span class="kobospan" id="kobo.573.1">trained on.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.574.1">Using a tokenizer is an important step in language models as well. </span><span class="kobospan" id="kobo.574.2">For example, BERT utilizes a WordPiece tokenizer, which employs the technique of dividing words into either their full forms</span><a id="_idIndexMarker420" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.575.1"> or smaller components known as word pieces. </span><span class="kobospan" id="kobo.575.2">This means that a single word can be represented by several tokens. </span><span class="kobospan" id="kobo.575.3">It employs a data-driven approach that builds a large vocabulary of sub-words based on the corpus of text being trained on. </span><span class="kobospan" id="kobo.575.4">These sub-word units are represented as embeddings that are used as input to the </span><span><span class="kobospan" id="kobo.576.1">BERT model.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.577.1">One of the key features of the BERT</span><a id="_idIndexMarker421" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.578.1"> tokenizer is that it can handle </span><strong class="bold"><span class="kobospan" id="kobo.579.1">out-of-vocabulary</span></strong><span class="kobospan" id="kobo.580.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.581.1">OOV</span></strong><span class="kobospan" id="kobo.582.1">) words. </span><span class="kobospan" id="kobo.582.2">If the tokenizer encounters a word that is not in its vocabulary, it will break the word down into sub-words and represent the word as a combination of its sub-word embeddings. </span><span class="kobospan" id="kobo.582.3">We will explain BERT and its tokenizer in more detail later in this book. </span><span class="kobospan" id="kobo.582.4">The benefit of using a tokenizer in language models is that we can limit the number of inputs to the size of our dictionary rather than all possible inputs. </span><span class="kobospan" id="kobo.582.5">For example, BERT has a 30,000-word vocabulary size, which helps us limit the size of the deep learning language</span><a id="_idIndexMarker422" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.583.1"> model. </span><span class="kobospan" id="kobo.583.2">Using a bigger tokenizer will increase the size of the model. </span><span class="kobospan" id="kobo.583.3">In the next section, we will explain how to use the methods that were covered in this chapter in a complete </span><span><span class="kobospan" id="kobo.584.1">preprocessing pipeline.</span></span></p>
<h1 id="_idParaDest-89" class="calibre4"><a id="_idTextAnchor127" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.585.1">Explaining the preprocessing pipeline</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.586.1">We will explain a complete preprocessing </span><a id="_idIndexMarker423" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.587.1">pipeline that has been provided by the authors to you, </span><span><span class="kobospan" id="kobo.588.1">the reader.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.589.1">As shown in the following code, the input is a formatted text with encoded tags, similar to what we can extract from HTML </span><span><span class="kobospan" id="kobo.590.1">web pages:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.591.1">
"&lt;SUBJECT LINE&gt; Employees details&lt;END&gt;&lt;BODY TEXT&gt;Attached are 2 files,\n1st one is pairoll, 2nd is healtcare!&lt;END&gt;"</span></pre> <p class="calibre6"><span class="kobospan" id="kobo.592.1">Let’s take a look at the effect of applying each step to </span><span><span class="kobospan" id="kobo.593.1">the text:</span></span></p>
<ol class="calibre16">
<li class="calibre15"><span><span class="kobospan" id="kobo.594.1">Decode/remove encoding:</span></span><p class="calibre6"><em class="italic"><span class="kobospan" id="kobo.595.1">Employees details. </span><span class="kobospan" id="kobo.595.2">Attached are 2 files, 1st one is pairoll, 2nd </span></em><span><em class="italic"><span class="kobospan" id="kobo.596.1">is healtcare!</span></em></span></p></li>
<li class="calibre15"><span><span class="kobospan" id="kobo.597.1">Lowercasing:</span></span><p class="calibre6"><em class="italic"><span class="kobospan" id="kobo.598.1">employees details. </span><span class="kobospan" id="kobo.598.2">attached are 2 files, 1st one is pairoll, 2nd </span></em><span><em class="italic"><span class="kobospan" id="kobo.599.1">is healtcare!</span></em></span></p></li>
<li class="calibre15"><span class="kobospan" id="kobo.600.1">Digits </span><span><span class="kobospan" id="kobo.601.1">to words:</span></span><p class="calibre6"><em class="italic"><span class="kobospan" id="kobo.602.1">employees details. </span><span class="kobospan" id="kobo.602.2">attached are two files, first one is pairoll, second </span></em><span><em class="italic"><span class="kobospan" id="kobo.603.1">is healtcare!</span></em></span></p></li>
<li class="calibre15"><span class="kobospan" id="kobo.604.1">Remove punctuation and other </span><span><span class="kobospan" id="kobo.605.1">special characters:</span></span><p class="calibre6"><em class="italic"><span class="kobospan" id="kobo.606.1">employees details attached are two files first one is pairoll second </span></em><span><em class="italic"><span class="kobospan" id="kobo.607.1">is healtcare</span></em></span></p></li>
<li class="calibre15"><span><span class="kobospan" id="kobo.608.1">Spelling corrections:</span></span><p class="calibre6"><em class="italic"><span class="kobospan" id="kobo.609.1">employees details attached are two files first one is payroll second </span></em><span><em class="italic"><span class="kobospan" id="kobo.610.1">is healthcare</span></em></span></p></li>
<li class="calibre15"><span class="kobospan" id="kobo.611.1">Remove </span><span><span class="kobospan" id="kobo.612.1">stop words:</span></span><p class="calibre6"><em class="italic"><span class="kobospan" id="kobo.613.1">employees details attached two files first one payroll </span></em><span><em class="italic"><span class="kobospan" id="kobo.614.1">second healthcare</span></em></span></p></li>
<li class="calibre15"><span><span class="kobospan" id="kobo.615.1">Stemming:</span></span><p class="calibre6"><em class="italic"><span class="kobospan" id="kobo.616.1">employe detail attach two file first one payrol </span></em><span><em class="italic"><span class="kobospan" id="kobo.617.1">second healthcar</span></em></span></p></li>
<li class="calibre15"><span><span class="kobospan" id="kobo.618.1">Lemmatizing:</span></span><p class="calibre6"><em class="italic"><span class="kobospan" id="kobo.619.1">employe detail attach two file first one payrol </span></em><span><em class="italic"><span class="kobospan" id="kobo.620.1">second healthcar</span></em></span></p></li>
</ol>
<p class="calibre6"><span class="kobospan" id="kobo.621.1">With that, we’ve learned </span><a id="_idIndexMarker424" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.622.1">about different preprocessing methods. </span><span class="kobospan" id="kobo.622.2">Next, we’ll review a piece of code for performing NER </span><span><span class="kobospan" id="kobo.623.1">and POS.</span></span></p>
<h2 id="_idParaDest-90" class="calibre7"><a id="_idTextAnchor128" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.624.1">Code for NER and POS</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.625.1">For this example, we used </span><a id="_idIndexMarker425" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.626.1">the spaCy library for Python</span><a id="_idIndexMarker426" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.627.1"> to perform these tasks. </span><span class="kobospan" id="kobo.627.2">Here our </span><span><span class="kobospan" id="kobo.628.1">input is:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.629.1">
The companies that would be releasing their quarterly reports tomorrow are Microsoft, 4pm, Google, 4pm, and AT&amp;T, 6pm.</span></pre> <p class="calibre6"><span class="kobospan" id="kobo.630.1">Here’s the output </span><span><span class="kobospan" id="kobo.631.1">for NER:</span></span></p>
<p class="calibre6"><em class="italic"><span class="kobospan" id="kobo.632.1">The companies that would be releasing their quarterly DATE reports tomorrow DATE are Microsoft ORG , 4pm TIME , Google ORG , 4pm TIME , and AT&amp;T ORG , 6pm </span></em><span><em class="italic"><span class="kobospan" id="kobo.633.1">TIME .</span></em></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.634.1">As you can see, using NER, we were able to detect parts of the sentence that are related to company names (ORG) </span><span><span class="kobospan" id="kobo.635.1">or dates.</span></span></p>
<p class="calibre6"><span><em class="italic"><span class="kobospan" id="kobo.636.1">Figure 4</span></em></span><em class="italic"><span class="kobospan" id="kobo.637.1">.1</span></em><span class="kobospan" id="kobo.638.1"> shows an example of performing </span><span><span class="kobospan" id="kobo.639.1">POS tagging:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer284">
<span class="kobospan" id="kobo.640.1"><img alt="Figure 4.1 – POS tagging using spaCy" src="image/B18949_04_1.jpg" class="calibre3"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.641.1">Figure 4.1 – POS tagging using spaCy</span></p>
<p class="calibre6"><span class="kobospan" id="kobo.642.1">Here’s </span><span><span class="kobospan" id="kobo.643.1">the output:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.644.1">
[['companies', 'NOUN'],
 ['releasing', 'VERB'],
 ['quarterly', 'ADJ'],
 ['reports', 'NOUN'],
 ['tomorrow', 'NOUN'],
 ['Microsoft', 'PROPN'],
 ['pm', 'NOUN'],
 ['Google', 'PROPN'],
 ['pm', 'NOUN'],
 ['AT&amp;T', 'PROPN'],
 ['pm', 'NOUN']]</span></pre> <p class="calibre6"><span class="kobospan" id="kobo.645.1">The preceding code examples</span><a id="_idIndexMarker427" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.646.1"> exemplify the various</span><a id="_idIndexMarker428" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.647.1"> aspects of preprocessing, which processes raw text and transforms it into a form that suits the downstream model so that it suits the purpose of the </span><span><span class="kobospan" id="kobo.648.1">overall design.</span></span></p>
<h1 id="_idParaDest-91" class="calibre4"><a id="_idTextAnchor129" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.649.1">Summary</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.650.1">In this chapter, we covered a range of techniques and methods for text preprocessing, including normalization, tokenization, stop word removal, POS tagging, and more. </span><span class="kobospan" id="kobo.650.2">We explored different approaches to these techniques, such as rule-based methods, statistical methods, and deep learning-based methods. </span><span class="kobospan" id="kobo.650.3">We also discussed the advantages and disadvantages of each method and provided examples and code snippets to illustrate </span><span><span class="kobospan" id="kobo.651.1">their use.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.652.1">At this point, you should have a solid understanding of the importance of text preprocessing and the various techniques and methods available for cleaning and preparing text data for analysis. </span><span class="kobospan" id="kobo.652.2">You should be able to implement these techniques using popular libraries and frameworks in Python and understand the trade-offs between different approaches. </span><span class="kobospan" id="kobo.652.3">Furthermore, you should have a better understanding of how to process text data to achieve better results in NLP tasks such as sentiment analysis, topic modeling, and </span><span><span class="kobospan" id="kobo.653.1">text classification.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.654.1">In the next chapter, we will explain text classification, and different methods for performing </span><span><span class="kobospan" id="kobo.655.1">this task.</span></span></p>
</div>
</body></html>