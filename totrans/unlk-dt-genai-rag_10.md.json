["```py\n chroma_client = chromadb.Client()\ncollection_name = \"google_environmental_report\"\nvectorstore = Chroma.from_documents(\n               documents=dense_documents,\n               embedding=embedding_function,\n               collection_name=collection_name,\n               client=chroma_client\n)\n```", "```py\n %pip install faiss-cpu\n```", "```py\n from langchain_community.vectorstores import FAISS\nvectorstore = FAISS.from_documents(\n               documents=dense_documents,\n               embedding=embedding_function\n)\n```", "```py\n %pip install faiss-gpu\n```", "```py\n %pip install weaviate-client\n%pip install langchain-weaviate\n```", "```py\n import weaviate\nfrom langchain_weaviate.vectorstores import WeaviateVectorStore\nfrom weaviate.embedded import EmbeddedOptions\nfrom langchain.vectorstores import Weaviate\nfrom tqdm import tqdm\n```", "```py\n weaviate_client = weaviate.Client(\n    embedded_options=EmbeddedOptions())\n```", "```py\n try:\n     weaviate_client.schema.delete_class(collection_name)\nexcept:\n        pass\n```", "```py\n weaviate_client.schema.create_class({\n               \"class\": collection_name,\n               \"description\": \"Google Environmental\n                               Report\",\n               \"properties\": [\n                             {\n                                  \"name\": \"text\",\n                                  \"dataType\": [\"text\"],\n                                  \"description\": \"Text\n                                   content of the document\"\n                             },\n                             {\n                                  \"name\": \"doc_id\",\n                                  \"dataType\": [\"string\"],\n                                  \"description\": \"Document\n                                       ID\"\n                             },\n                             {\n                                  \"name\": \"source\",\n                                  \"dataType\": [\"string\"],\n                                  \"description\": \"Document\n                                       source\"\n                             }\n               ]\n})\n```", "```py\n dense_documents = [Document(page_content=text,\nmetadata={\"doc_id\": str(i), \"source\": \"dense\"}) for i,\n          text in enumerate(splits)]\nsparse_documents = [Document(page_content=text, metadata={\"doc_id\": str(i), \"source\": \"sparse\"}) for i,\n          text in enumerate(splits)]\n```", "```py\n vectorstore = Weaviate(\n               client=weaviate_client,\n               embedding=embedding_function,\n               index_name=collection_name,\n               text_key=\"text\",\n               attributes=[\"doc_id\", \"source\"],\n               by_text=False\n)\n```", "```py\n weaviate_client.batch.configure(batch_size=100)\nwith weaviate_client.batch as batch:\n    for doc in tqdm(dense_documents, desc=\"Processing\n        documents\"):\n                    properties = {\n                                  \"text\": doc.page_content,\n                                  \"doc_id\":doc.metadata[\n                                               \"doc_id\"],\n                                  \"source\": doc.metadata[\n                                                \"source\"]\n                             }\n                    vector=embedding_function.embed_query(\n                               doc.page_content)\n                           batch.add_data_object(\n                               data_object=properties,\n                               class_name=collection_name,\n                               vector=vector\n                           )\n```", "```py\n dense_retriever = vectorstore.as_retriever(\n                      search_kwargs={\"k\": 10})\n```", "```py\n dense_retriever = vectorstore.as_retriever(\n               search_type=\"similarity_score_threshold\",\n               search_kwargs={\"score_threshold\": 0.5}\n)\n```", "```py\n dense_retriever = vectorstore.as_retriever(\n                      search_type=\"mmr\"\n)\n```", "```py\n sparse_retriever = BM25Retriever.from_documents(\n    sparse_documents, k=10)\n```", "```py\n ensemble_retriever = EnsembleRetriever(\n    retrievers=[dense_retriever, sparse_retriever],\n    weights=[0.5, 0.5], c=0, k=10)\n```", "```py\n %pip install langchain_core\n%pip install --upgrade --quiet        wikipedia\n```", "```py\n from langchain_community.retrievers import WikipediaRetriever\nretriever = WikipediaRetriever(load_max_docs=10)\ndocs = retriever.get_relevant_documents(query=\n    \"What defines the golden age of piracy in the\n     Caribbean?\")\nmetadata_title = docs[0].metadata['title']\nmetadata_summary = docs[0].metadata['summary']\nmetadata_source = docs[0].metadata['source']\npage_content = docs[0].page_content\nprint(f\"First document returned:\\n\")\nprint(f\"Title: {metadata_title}\\n\")\nprint(f\"Summary: {metadata_summary}\\n\")\nprint(f\"Source: {metadata_source}\\n\")\nprint(f\"Page content:\\n\\n{page_content}\\n\")\n```", "```py\n First document returned:\nTitle: Golden Age of Piracy\nSummary: The Golden Age of Piracy is a common designation for the period between the 1650s and the 1730s, when maritime piracy was a significant factor in the histories of the North Atlantic and Indian Oceans. Histories of piracy often subdivide the Golden Age of Piracy into three periods:\nThe buccaneering period (approximately 1650 to 1680)…\n```", "```py\n from langchain_community.retrievers import KNNRetriever\ndense_retriever = KNNRetriever.from_texts(splits,\n    OpenAIEmbeddings(), k=10)\nensemble_retriever = EnsembleRetriever(\n    retrievers=[dense_retriever, sparse_retriever],\n    weights=[0.5, 0.5], c=0, k=10)\n```", "```py\n    <st c=\"38258\">langchain-openai</st> library provides integration between OpenAI’s language models and LangChain.\n    ```", "```py\n    <st c=\"38663\">import openai</st>\n    ```", "```py\n    <st c=\"38735\">ChatOpenAI</st> is used to interact with OpenAI’s chat models, and <st c=\"38798\">OpenAIEmbeddings</st> is used for generating embeddings from text.\n    ```", "```py\n    <st c=\"39026\">env.txt</st> file to store sensitive information (an API key) in a way that we can hide it from our versioning system, practicing better and more secure secret management.\n    ```", "```py\n    <st c=\"39256\">os.environ['OPENAI_API_KEY'] = os.getenv(</st>\n    ```", "```py\n     <st c=\"39298\">'OPENAI_API_KEY')</st>\n    ```", "```py\n    <st c=\"39425\">OPENAI_API_KEY</st>. Then, we set the OpenAI API key for the <st c=\"39481\">openai</st> library using the retrieved value from the environment variable. At this point, we can use the OpenAI integration with LangChain to call the LLM that is hosted at OpenAI with the proper access.\n    ```", "```py\n    <st c=\"39734\">llm = ChatOpenAI(model_name=\"gpt-4o-mini\",</st>\n    ```", "```py\n     <st c=\"39777\">temperature=0)</st>\n    ```", "```py\n    <st c=\"42529\">%pip install --upgrade langchain-together</st>\n    ```", "```py\n    <st c=\"42651\">from langchain_together import ChatTogether</st>\n    ```", "```py\n    <st c=\"42785\">ChatTogether</st> integration and loads the API key (don’t forget to add it to the <st c=\"42863\">env.txt</st> file before running this line of code!).\n    ```", "```py\n    <st c=\"43042\">os.environ['TOGETHER_API_KEY'] = os.getenv(</st>\n    ```", "```py\n     <st c=\"43086\">'TOGETHER_API_KEY')</st>\n    ```", "```py\n    <st c=\"43370\">llama3llm = ChatTogether(</st>\n    ```", "```py\n     <st c=\"43396\">together_api_key=os.environ['TOGETHER_API_KEY'],</st>\n    ```", "```py\n    **<st c=\"43445\">model=\"meta-llama/Llama-3-70b-chat-hf\",</st>**\n    ```", "```py\n    **<st c=\"43485\">)</st>**\n    ```", "```py\n    **<st c=\"43487\">mistralexpertsllm = ChatTogether(</st>**\n    ```", "```py\n     **<st c=\"43520\">together_api_key=os.environ['TOGETHER_API_KEY'],</st>**\n    ```", "```py\n     **<st c=\"43569\">model=\"mistralai/Mixtral-8x22B-Instruct-v0.1\",</st>**\n    ```", "```py\n    **<st c=\"43616\">)</st>**\n    the results.\n    ```", "```py\n    <st c=\"43824\">llama3_rag_chain_from_docs = (</st>\n    ```", "```py\n     <st c=\"43855\">RunnablePassthrough.assign(context=(lambda x:</st>\n    ```", "```py\n     <st c=\"43901\">format_docs(x[\"context\"])))</st>\n    ```", "```py\n     <st c=\"43929\">| RunnableParallel(</st>\n    ```", "```py\n     <st c=\"43949\">{\"relevance_score\": (</st>\n    ```", "```py\n     <st c=\"43971\">RunnablePassthrough()</st>\n    ```", "```py\n     <st c=\"43993\">| (lambda x: relevance_prompt_template.</st>\n    ```", "```py\n     <st c=\"44033\">format(</st>\n    ```", "```py\n     <st c=\"44041\">question=x['question'],</st>\n    ```", "```py\n     <st c=\"44065\">retrieved_context=x['context']))</st>\n    ```", "```py\n     <st c=\"44098\">| llama3llm</st>\n    ```", "```py\n     <st c=\"44110\">| StrOutputParser()</st>\n    ```", "```py\n     <st c=\"44130\">), \"answer\": (</st>\n    ```", "```py\n     <st c=\"44145\">RunnablePassthrough()</st>\n    ```", "```py\n     <st c=\"44167\">| prompt</st>\n    ```", "```py\n     <st c=\"44176\">| llama3llm</st>\n    ```", "```py\n     <st c=\"44188\">| StrOutputParser()</st>\n    ```", "```py\n     <st c=\"44208\">)}</st>\n    ```", "```py\n     <st c=\"44211\">)</st>\n    ```", "```py\n     <st c=\"44213\">| RunnablePassthrough().assign(</st>\n    ```", "```py\n     <st c=\"44245\">final_answer=conditional_answer)</st>\n    ```", "```py\n     <st c=\"44278\">)</st>\n    ```", "```py\n    <st c=\"44389\">llama3_rag_chain_with_source = RunnableParallel(</st>\n    ```", "```py\n     <st c=\"44438\">{\"context\": ensemble_retriever,</st>\n    ```", "```py\n     <st c=\"44470\">\"question\": RunnablePassthrough()}</st>\n    ```", "```py\n    <st c=\"44505\">).assign(answer=llama3_rag_chain_from_docs)</st>\n    ```", "```py\n    <st c=\"44801\">llama3_result = llama3_rag_chain_with_source.invoke(</st>\n    ```", "```py\n    **<st c=\"44854\">user_query)</st>**\n    ```", "```py\n    **<st c=\"44866\">llama3_retrieved_docs = llama3_result['context']</st>**\n    ```", "```py\n    **<st c=\"44915\">print(f\"Original Question: {user_query}\\n\")</st>**\n    ```", "```py\n    **<st c=\"44959\">print(f\"Relevance Score:</st>**\n    ```", "```py\n     **<st c=\"44984\">{llama3_result['answer']['relevance_score']}\\n\")</st>**\n    ```", "```py\n    **<st c=\"45033\">print(f\"Final Answer:</st>**\n    ```", "```py\n     **<st c=\"45055\">\\n{llama3_result['answer']['final_answer']}\\n\\n\")</st>**\n    ```", "```py\n    **<st c=\"45105\">print(\"Retrieved Documents:\")</st>**\n    ```", "```py\n    **<st c=\"45135\">for i, doc in enumerate(llama3_retrieved_docs,</st>**\n    ```", "```py\n     **<st c=\"45182\">start=1):</st>**\n    ```", "```py\n    **<st c=\"45192\">print(f\"Document {i}: Document ID:</st>**\n    ```", "```py\n     **<st c=\"45227\">{doc.metadata['id']} source:</st>**\n    ```", "```py\n     **<st c=\"45256\">{doc.metadata['source']}\")</st>**\n    ```", "```py\n    `<st c=\"45364\">What are Google's environmental initiatives?</st>` <st c=\"45408\">is</st> <st c=\"45412\">as follows:</st>\n\n    ```", "```py\n\n    ```", "```py\n\n    ```", "```py\n\n    ```", "```py \n    ```", "```py\n    <st c=\"46048\">mistralexperts_rag_chain_from_docs = (</st>\n    ```", "```py\n    **<st c=\"46087\">RunnablePassthrough.assign(context=(lambda x:</st>**\n    ```", "```py\n     **<st c=\"46133\">format_docs(x[\"context\"])))</st>**\n    ```", "```py\n     **<st c=\"46161\">| RunnableParallel(</st>**\n    ```", "```py\n     **<st c=\"46181\">{\"relevance_score\": (RunnablePassthrough()</st>**\n    ```", "```py\n     **<st c=\"46224\">| (lambda x: relevance_prompt_template.format(</st>**\n    ```", "```py\n     **<st c=\"46271\">question=x['question'],</st>**\n    ```", "```py\n     **<st c=\"46295\">retrieved_context=x['context']))</st>**\n    ```", "```py\n     **<st c=\"46328\">| mistralexpertsllm</st>**\n    ```", "```py\n     **<st c=\"46348\">| StrOutputParser()</st>**\n    ```", "```py\n     **<st c=\"46368\">), \"answer\": (</st>**\n    ```", "```py\n    ****<st c=\"46383\">RunnablePassthrough()</st>****\n    ```", "```py\n     ****<st c=\"46405\">| prompt</st>****\n    ```", "```py\n     ****<st c=\"46414\">| mistralexpertsllm</st>****\n    ```", "```py\n     ****<st c=\"46434\">| StrOutputParser()</st>****\n    ```", "```py\n     ****<st c=\"46454\">)}</st>****\n    ```", "```py\n     ****<st c=\"46457\">)</st>****\n    ```", "```py\n     ****<st c=\"46459\">| RunnablePassthrough().assign(</st>****\n    ```", "```py\n     ****<st c=\"46491\">final_answer=conditional_answer)</st>****\n    ```", "```py\n    ****<st c=\"46524\">)</st>****\n    ```", "```py\n    ****<st c=\"46663\">mistralexperts_rag_chain_with_source = RunnableParallel(</st>****\n    ```", "```py\n     ****<st c=\"46720\">{\"context\": ensemble_retriever, \"question\": RunnablePassthrough()}</st>****\n    ```", "```py\n    ****<st c=\"46787\">).assign(answer=mistralexperts_rag_chain_from_docs)</st>****\n    ```", "```py\n    ****<st c=\"47052\">mistralexperts_result = mistralexperts_rag_chain_with_source.invoke(user_query)</st>****\n    ```", "```py\n    ****<st c=\"47132\">mistralexperts_retrieved_docs = mistralexperts_result[</st>****\n    ```", "```py\n     ****<st c=\"47187\">'context']</st>****\n    ```", "```py\n    ****<st c=\"47198\">print(f\"Original Question: {user_query}\\n\")</st>****\n    ```", "```py\n    ****<st c=\"47242\">print(f\"Relevance Score: {mistralexperts_result['answer']['relevance_score']}\\n\")</st>****\n    ```", "```py\n    ****<st c=\"47324\">print(f\"Final Answer:\\n{mistralexperts_result['answer']['final_answer']}\\n\\n\")</st>****\n    ```", "```py\n    ****<st c=\"47403\">print(\"Retrieved Documents:\")</st>****\n    ```", "```py\n    ****<st c=\"47433\">for i, doc in enumerate(mistralexperts_retrieved_docs, start=1):</st>****\n    ```", "```py\n     ****<st c=\"47498\">print(f\"Document {i}: Document ID:</st>****\n    ```", "```py\n    ****<st c=\"47533\">{doc.metadata['id']} source: {doc.metadata['source']}\")</st>****\n    ```", "```py\n    **`<st c=\"47657\">What are Google's environmental initiatives?</st>` <st c=\"47701\">Is</st> <st c=\"47705\">the following:</st>\n\n    ```", "```py\n\n    ```", "```py\n\n    ```", "```py\n\n    ```", "```py\n\n    <st c=\"48733\">Compare this to the original response we saw in</st> <st c=\"48782\">previous chapters:</st>\n\n    ```", "```py** \n    ```"]