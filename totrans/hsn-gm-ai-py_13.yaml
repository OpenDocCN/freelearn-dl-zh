- en: Exploiting ML-Agents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At some point, we need to move beyond building and training agent algorithms
    and explore building our own environments. Building your own environments will
    also give you more experience in making good reward functions. We have virtually
    omitted this important question in **Reinforcement Learning** (**RL**) and **Deep
    Reinforcement Learning** (**DRL**) and that is what makes a good reward function.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will look to answer the question of what makes a good reward
    function or what a reward function is. We will talk about reward functions by
    building new environments with the Unity game engine. We will start by installing
    and setting up Unity ML-Agents, an advanced DRL kit for building agents and environments.
    From there, we will look at how to build one of the standard Unity demo environments
    for our use with our PyTorch models. Conveniently, this leads us to working with
    the ML-Agents toolkit for using a Unity environment from Python and PyTorch with
    our previously explored Rainbow DQN model. After that, we will look at creating
    a new environment, and then finish this chapter by looking at advances Unity has
    developed for furthering RL.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the main topics we will cover in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Installing ML-Agents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a Unity environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a Unity environment with Rainbow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a new environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advancing RL with ML-Agents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unity is the largest and most frequently used game engine for game development.
    You likely already know this if you are a game developer. The game engine itself
    is developed with C++ but it provides a scripting interface in C# that 99% of
    its game developers use. As such, we will need to expose you to some C# code in
    this chapter, but just a tiny amount.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll install Unity and the ML-Agents toolkit.
  prefs: []
  type: TYPE_NORMAL
- en: Installing ML-Agents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Installing Unity, the game engine itself, is not very difficult, but when working
    with ML-Agents, you need to be careful when you pick your version. As such, the
    next exercise is intended to be more configurable, meaning you may need to ask/answer
    questions while performing the exercise. We did this to make this exercise longer
    lasting since this toolkit has been known to change frequently with many breaking
    changes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unity will run on any major desktop computer (Windows, Mac, or Linux), so open
    your development computer and follow along with the next exercise to install Unity
    and the ML-Agents toolkit:'
  prefs: []
  type: TYPE_NORMAL
- en: Before installing Unity, check the ML-Agents GitHub installation page ([https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md)) and
    confirm which version of Unity is currently supported. At the time of writing,
    this is 2017.4, and we will prefer to use only that version even though the documentation suggests
    later versions are supported.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can download and install Unity directly or through the Unity Hub. Since
    managing multiple versions of Unity is so common, Unity built a management app,
    the Unity Hub, for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: Download and install the required minimum version of Unity. If you have never
    installed Unity, you will need to create a user account and verify their license
    agreement. After you create a user account, you will be able to run the Unity
    editor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open a Python/Anaconda command shell and make sure to activate your virtual
    environment with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Install the Unity Gym wrapper with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Change to a root working folder, preferably `C:` or `/`, and create a directory
    for cloning the ML-Agents toolkit into with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, assuming you have `git` installed, use `git` to pull down the ML-Agents
    toolkit with this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The reason we prefer a root folder is that the ML-Agents directory structure
    can get quite deep and this may cause too long filename errors on some operating
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the entire installation is best done by consulting the current Unity
    docs and using their most recent guide. A good place to start is the first example
    environment, the 3D Balance Ball. You can find this document at [https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Getting-Started-with-Balance-Ball.md](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Getting-Started-with-Balance-Ball.md).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take some time and explore the ML-Agents toolkit on your own. It is meant to
    be quite accessible and if your only experience in DRL is this book, you should
    have plenty of background by now to understand the general gist of running Unity
    environments. We will review some of these procedures but there are plenty of
    other helpful guides out there that can help you run ML-Agents in Unity. Our priority
    here will be using Unity to build environments and possibly new environments we
    can use to test our models on. While we won't use the ML-Agents toolkit to train
    agents, we will use the Gym wrappers, which do require knowledge of what a brain
    or academy is.
  prefs: []
  type: TYPE_NORMAL
- en: Adam Kelly has an excellent blog, Immersive limit ([http://www.immersivelimit.com/tutorials/tag/Unity+ML+Agents](http://www.immersivelimit.com/tutorials/tag/Unity+ML+Agents)),
    devoted to machine learning and DRL with a specialization of creating very cool
    ML-Agents environments and projects.
  prefs: []
  type: TYPE_NORMAL
- en: ML-Agents currently uses PPO and Soft Actor-Critic methods to train agents.
    It also provides several helpful modules for state encoding using convolutional
    and recurrent networks, hence allowing for visual observation encoding and memory
    or context. Additionally, it provides methods for defining discrete or continuous
    action spaces, as well as enabling mixing action or observation types. The toolkit
    is extremely well done but, with the rapidly changing landscape of ML, it has
    become quickly outdated and/or perhaps just out-hyped. In the end, it also appears
    that most researchers or serious practitioners of DRL just want to build their
    own frameworkll for now.
  prefs: []
  type: TYPE_NORMAL
- en: While DRL is quite complicated, the amount of code to make something powerful
    is still quite small. Therefore, we will likely see a plethora of RL frameworks
    trying to gain a foothold on the space. Whether you decide to use one of these
    frameworks or build your own is up to you. Just remember that frameworks come
    and go, but the more underlying knowledge you have on a topic, the better your
    ability to guide future decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Regardless of whether you decide to use the ML-Agents framework for training
    DRL agents, use another framework, or build your own, Unity provides you with
    an excellent opportunity to build new and more exciting environments. We learn how
    to build a Unity environment we can train with DRL in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Building a Unity environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The ML-Agents toolkit provides not only a DRL training framework but also a
    mechanism to quickly and easily set up AI agents within a Unity game. Those agents
    can then be externally controlled through a Gym interface—yes, that same interface
    we used to train most of our previous agent/algorithms. One of the truly great
    things about this platform is that Unity provides several new demo environments
    that we can explore. Later, we will look at how to build our own environments
    for training agents.
  prefs: []
  type: TYPE_NORMAL
- en: The exercises in this section are meant to summarize the setup steps required
    to build an executable environment to train with Python. They are intended for
    newcomers to Unity who don't want to learn all about Unity to just build a training
    environment. If you encounter issues using these exercises, it is likely the SDK
    may have changed. If that is the case, then just revert back and consult the full
    online documentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Building a Unity environment for agent training requires a few specialized
    steps we will cover in this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: First, open the Unity editor, either through the Unity Hub or just Unity itself.
    Remember to use a version that supports the ML-Agents toolkit.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Using the Unity Hub, we can add the project using the **Add** button, as shown
    in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/d317976a-e439-4a1f-b61f-a33e2ea27a23.png)'
  prefs: []
  type: TYPE_IMG
- en: Adding a project in the Unity Hub
  prefs: []
  type: TYPE_NORMAL
- en: After you click **Add**, you will be prompted to locate the project folder.
    Use the dialog to find and select the `UnitySDK` project folder we pulled down
    with `git` in the previous exercise. This folder should be located in your `/mlagents/ml-agents/UnitySDK`
    folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When the project has been added, it will also be added to the top of the list
    of projects. You will likely see a warning icon indicating you need to select
    the version number. Select a version of Unity that coincides with the ML-Agents
    toolkit and then select the project to launch it in the editor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You may be prompted to **Upgrade** the project. If you are prompted, then select
    **Yes** to do so. If the upgrade fails or the project won't run right, then you
    can just delete all of the old files and try again with a different version of
    Unity. Loading this project may take some time, so be patient, grab a drink, and
    wait.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After the project finishes loading and the editor opens, open the scene for
    the `3DBall` environment located in the `Assets/ML-Agents/Examples/Hallway/Scenes`
    folder by double-clicking on the `3DBall` scene file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We need to set the Academy to control the brain, that is, allow the brain to
    be trained. To do that, select the **Academy**, then locate the **Hallway Academy**
    component in the **Inspector** window, and select the **Control** option, as shown
    in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/d5f6d096-ee5f-4a4a-ae6e-bd4c2e820467.png)'
  prefs: []
  type: TYPE_IMG
- en: Setting the academy to control the brain
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to modify the run parameters for the environment. The idea here
    is that we will build the Unity environment as an executable game that we can
    then use the wrappers on to train an agent to play. However, to do that, we need
    to make some assumptions about the game:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The game is windowless and runs in the background.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Any player actions need to be controlled by the agent. A dialog prompts for
    warnings, errors, or anything else that must be avoided.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Make sure that the training scene is loaded first.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Before finishing that though, turn the **Control** option back off or on for
    the academy and run the scene by pressing the **Play** button at the top of the
    interface. You will be able to observe an already trained agent play through the
    scene. Make sure to turn the **Control** option back on when you are done viewing
    the agent play.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, the ML-Agents toolkit will allow you to train directly from here by just
    running a separate Python command shell and script controlling the editor. As
    of yet, this is not possible and our only way to run an environment is with wrappers.
    In the next section, we will finish setting up the environment by setting some
    final parameters and building them.
  prefs: []
  type: TYPE_NORMAL
- en: Building for Gym wrappers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Configuring the setup of an environment just requires setting a few additional
    parameters. We will learn how to do this in the following exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: From the editor menu, select **Edit | Project Settings... **to open the **Project
    Settings** window. You can anchor this window or close it after you've finished
    editing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the **Player** option. Player, in this case, denotes the player or game
    runner—not to be confused with an actual human player. Change the text in the
    **Company Name** and **Product Name** fields to `GameAI`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open the **Resolution and Presentation** section and then be sure that **Run
    In Background*** is checked and **Display Resolution Dialog** is set to **Disabled**,
    as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/61c81f5c-cebf-46de-82d9-f7e36ab8e030.png)'
  prefs: []
  type: TYPE_IMG
- en: Setting the Player settings
  prefs: []
  type: TYPE_NORMAL
- en: Close the dialog or anchor it and then, from the menu, select **File | Build
    Settings**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click the **Add Open Scene** button and be sure to select your default platform
    for training. This should be a desktop environment you can easily run with Python.
    The following screenshot shows the Windows option by default:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/8ebf8c6e-436c-42ef-8df8-901d5cdd67c5.png)'
  prefs: []
  type: TYPE_IMG
- en: Building the scene into a game environment
  prefs: []
  type: TYPE_NORMAL
- en: Click the Build button at the bottom of the dialog to build the executable environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You will be prompted to save the output to a folder. Be sure to note the location
    of this folder and/or save it someplace accessible. A good suggested location
    is the `mlagents` root folder in `/mlagents`. Create a new folder called `desktop`and
    save the output there.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The environment should be built and runnable now as a Gym environment. We will
    set up this environment and start training it as an environment in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Training a Unity environment with Rainbow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Training an agent to learn a Unity environment is not unlike much of the training
    we have already done. There are a few slight changes to the way we interact and
    set up the environment but overall it is much the same, which makes it a further
    plus for us because now we can go back and train several different agents/algorithms
    on completely new environments that we can even design. Furthermore, we now can
    use other DRL frameworks to train agents with Python— outside the ML-Agents agents,
    that is. We will cover more on using other frameworks in [Chapter 12](6d061d35-176a-421a-9b62-aed35f48a6b7.xhtml),
    *DRL Frameworks*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next exercise, we see how to convert one of our latest and most state-of-the-art
    samples, `Chapter_10_Rainbow.py`, and turn it into `Chapter_11_Unity_Rainbow.py`.
    Open `Chapter_11_Unity_Rainbow.py` and follow the next exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: We first need to copy the output folder from the last build, the desktop folder,
    and place it in the same folder as this chapter's source code. This will allow
    us to launch that build as the environment our agent will train on.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Since you will likely want to convert a few of our previous samples to run
    Unity environments, we will go through the required changes step by step, starting
    first with the new import, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This imports the `UnityEnviroment` class, which is a Gym adapter to Unity.
    We next use this class to instantiate the `env` environment, like so; note that
    we have placed commented lines for other operating systems:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we get `brain` and `brain_name` from the environment. Unity uses the
    concept of a brain to control agents. We will explore agent brains in a later
    section. For now, realize that we just take the first available brain with the
    following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we extract the action (`action_size`) and state size (`state_size`) from
    the brain and use these as inputs to construct our `RainbowDQN` models, like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The last part we need to worry about is down in the training code and has to
    do with how the environment is reset. Unity allows for multiple agents/brains
    to run in concurrent environments concurrently, either as a way mechanism for
    A2C/A3C or other mechanisms. As such, it requires a bit more care as to which
    specific brain and mode we want to reset. The following code shows how we reset
    the environment when training Unity:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As mentioned, the purpose of the slightly confusing indexing has to do with
    which brain/agent you want to pull the state from. Unity may have multiple brains
    training multiple agents in multiple sub-environments, all either working together
    or against each other. We will cover more about training multiple agent environments
    in [Chapter 14](a171ddfa-e639-4b4e-9652-4279b5ac872a.xhtml), *From DRL to AGI*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We also have to change any other occurrences of when the environment may reset
    itself like in the following example when the algorithm checks whether the episode
    is done, with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Run the code and watch the agent train. You won't see any visuals other than
    TensorBoard output, assuming you go through the steps and run TB in another shell,
    which you can likely do on your own by now.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This example may be problematic to run due to API compatibility issues. If you
    encounter problems when running the sample, then try and set up a whole new virtual
    environment and install everything again. If the issue continues, then check online
    for help in places such as Stack Overflow or GitHub. Be sure to also refer to
    the latest Unity documentation on ML-Agents.
  prefs: []
  type: TYPE_NORMAL
- en: The real benefit of plugging in and using Unity is the ability to construct
    your own environments and then use those new environments with your own or another
    RL framework. In the next section, we will look at the basics of building your
    own RL environment with Unity ML-Agents.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a new environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The great thing about the ML-Agents toolkit is the ability it provides for creating
    new agent environments quickly and simply. You can even transform existing games
    or game projects into training environments for a range of purposes, from building
    full robotic simulations to simple game agents or even game agents that play as
    non-player characters. There is even potential to use DRL agents for game quality
    assurance testing. Imagine building an army of game testers that learn to play
    your game with just trial and error. The possibilities are endless and Unity is
    even building a full cloud-based simulation environment for running or training
    these agents in the future.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will walk through using a game project as a new training
    environment. Any environment you create in Unity would be best tested with the
    ML-Agents toolkit before you set up your own Python code. DRL agents are masters
    at finding bugs and/or cheats. As such, you will almost always want to test the
    environment first with ML-Agents before training it with your own code. I already
    recommended that you go through the process of setting up and running the ML-Agents
    Python code to train agents. Remember that once you export an environment for
    Gym training, it becomes windowless and you will not have any knowledge of how
    well the agent trains or performs in the environment. If there are any cheats
    or bugs to be found, the agent will most assuredly find them. After all, your
    agent will attempt millions of different trial and error combinations trying to
    find how to play the game.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to look at the Basic ML-Agents environment as a way of understanding
    how to build our own extended or new environment. The ML-Agents documentation
    is an excellent source to fall back on if the information here is lacking. This
    exercise is intended to get you up to speed building your own environments quickly:'
  prefs: []
  type: TYPE_NORMAL
- en: Open up the **Unity Editor** to the **UnitySDK** ML-Agents project we previously
    had open. Locate and open (double-click) the **Basic** scene at `Assets/ML-Agents/Examples/Basic/Scenes`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'At the center of any environment is the Academy. Locate and select the **Academy**
    object in the **Hierarchy** window and then view the properties in the **Inspector**
    window, as shown in the screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/8bfa360d-b17b-416c-9a01-b67c806967ba.png)'
  prefs: []
  type: TYPE_IMG
- en: Inspecting the Academy
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on and select the **BasicLearning (LearningBrain)** brain in the **Basic
    Academy | Broadcast Hub | Brains** entry. This will highlight the entry in the
    **Project** window. Select the **BasicLearning** brain in the **Project** window
    and view the brain setup in the **Inspector** window, as shown in the screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/db8d17df-1a65-491d-adf8-4c14fb9f2e8b.png)'
  prefs: []
  type: TYPE_IMG
- en: Inspecting the Learning Brain
  prefs: []
  type: TYPE_NORMAL
- en: We can see a few things about the brain here. A brain controls an agent so the
    brain's observation and action space effectively become the same as the agent's.
    In the **Inspector** window, you can see there are 20 vector observations and
    an action space of three discrete actions. For this environment, the actions are
    left or right and null. The 0 action becomes a null or pause action.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we want to inspect the agent itself. Click on and expand the **Basic**
    object in the **Hierarchy** window. Select the **BasicAgent** object and then
    review the **Inspector** window, as the screenshot shows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/6eefe3a4-eb8a-4652-bf53-e35f3c2bb995.png)'
  prefs: []
  type: TYPE_IMG
- en: Inspecting the Basic Agent
  prefs: []
  type: TYPE_NORMAL
- en: Inspect the Basic Agent component and you can see the **Brain** is set to the **BasicLearning**
    brain and there are other properties displayed here. Note how the Reset On Done
    and On Demand Decisions are both checked. **Reset On Done** enables the environment
    to reset itself when an episode is complete—what you would expect is the default
    behavior but is not. **On Demand Decisions** equate to using on- versus off-policy
    models and is more relevant when training with ML-Agents toolkit.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pressing Play will show you the agent playing the game. Watch how the agent
    plays and while the agent moves around, be sure to select and inspect objects
    in the editor. Unity is great for seeing how your game mechanics work and this
    comes in especially handy when building your own agent environments.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The academy, brain, and agent are the main elements you will need to consider
    when building any new environment. As long as you follow this basic example, you
    should be able to construct a simple working environment quickly. The other tricky
    part of building your own environment is the special coding you may have to do
    and we will cover that in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Coding an agent/environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unity provides an excellent interface for prototyping and building commercial
    games. You can actually get quite far with very little coding. Unfortunately,
    that is currently not the case when building new ML-Agents environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'As such, we will explore the important coding parts in the next exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Next, locate and open the **Scripts** folder under `Assets/ML-Agents/Examples/Basic`
    and inside that double-click to open `BasicAgent.cs`. This is a C# (CSharp) file
    and it will open in the default editor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'At the top of the file, you will note that this `BasicAgent` class is extended
    from `Agent` and not `MonoBehaviour`, which is the Unity default. `Agent` is a
    special class in Unity, which as you likely guessed, defines an agent that is
    capable of exploring the environment. However, in this case, agent refers more
    to a worker as in a worker in asynchronous or synchronous actor-critic. This means
    a single brain may control multiple agents, which is often the case:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Skipping down past the fields, we will jump to the method definitions starting
    with `CollectObservations`, shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Inside this method, we can see how the agent/brain collects observations from
    the environment. In this case, the observation is added using `AddVectorObs`,
    which adds the observation as a one-hot encoded vector of the required size. In
    this case, the vector size is 20, the same as the brain's state size.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One-hot encoding is a method by which we encode can encode class information
    in terms of binary values inside a vector. Hence, if a one-hot encoded vector
    denoting class or position 1 was active, it would be written as [0,1,0,0].
  prefs: []
  type: TYPE_NORMAL
- en: 'The main action method is the `AgentAction` method. This where the agent performs
    actions in the environment, be these actions moving or something else:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The first part of this code just determines how the agent moves based on the
    action it has taken. You can see how the code adjusts the agent''s position based
    on its move. Then, we see the following line of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This line adds a step reward, meaning it always adds this reward every step.
    It does this as a way of limiting the agent's moves. Hence, the longer the agent
    takes to make the wrong decisions, the less the reward. We sometimes use a step
    reward but it can also have negative effects and it often makes sense to eliminate
    a step reward entirely.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'At the bottom of the `AgentAction` method, we can see what happens when the
    agent reaches the small or large goal. If the agent reaches the large goal it
    gets a reward of 1 and .1 if it makes the small goal. With that, we can also see
    that, when it reaches a goal, the episode terminates using a call to `Done()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Reverse the numbers for the rewards, save the code, and return to the editor.
    Set the **Academy** to Control the brain and then train the agent with the ML-Agents
    or the code we developed earlier. You should very clearly see the agent having
    a preference for the smaller goal.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extending these concepts and building your own environment now will be up to
    you. The sky really is the limit and Unity provides several excellent examples
    to work with and learn from. In the next section, we will take the opportunity
    to look at the advances ML-Agents provides as mechanisms to enhance your agents
    or even explore new ways to train.
  prefs: []
  type: TYPE_NORMAL
- en: Advancing RL with ML-Agents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The ML-Agents toolkit, the part that allows you to train DRL agents, is considered
    one of the more serious and top-end frameworks for training agents. Since the
    framework was developed on top of Unity, it tends to perform better on Unity-like
    environments. However, not unlike many others who spend time training agents,
    the Unity developers realized early on that some environments present such difficult
    challenges as to require us to assist our agents.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, this assistance is not so much direct but rather indirect and often directly
    relates to how easy or difficult it is for an agent to find rewards. This, in
    turn, directly relates to how well the environment designer can build a reward
    function that an agent can use to learn an environment. There are also the times
    when an environment''s state space is so large and not obvious that creating a
    typical reward function is just not possible. With all that in mind, Unity has
    gone out of its way to enhance the RL inside ML-Agents with the following new
    forms of learning:'
  prefs: []
  type: TYPE_NORMAL
- en: Curriculum learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Behavioral cloning (imitation learning)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Curiosity learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training generalized RL agents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will cover each form of learning in a quick example using the Unity environment.
  prefs: []
  type: TYPE_NORMAL
- en: Curriculum learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Curriculum learning allows you to train an agent by increasing the complexity
    of the task as the agent learns. This is fairly intuitive and likely very similar
    to the way we learn various tasks from math to programming.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow the exercise to quickly see how you can set up for curriculum learning:'
  prefs: []
  type: TYPE_NORMAL
- en: Open the `WallJump` scene located in the `Assets/ML-Agents/Examples/WallJump/Scenes`
    folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Select the **Academy** in the scene and review the settings of the **Wall Jump
    Academy** component in the **Inspector** window and as shown in the screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/3c4c96dc-e061-470a-ac33-738fb0274f84.png)'
  prefs: []
  type: TYPE_IMG
- en: Inspecting the WallJump Academy
  prefs: []
  type: TYPE_NORMAL
- en: Inside the academy is an expanded section called **Reset Parameters**. These
    parameters represent training level parameters for various training states we
    want to put the agent through.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'These parameters now need to be configured in a configuration file the ML-Agents
    toolkit will use to train the agent with curriculum. The contents of this file
    can be found or created at `config/curricula/wall-jump/` and consist of the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Understanding these parameters can be best done by referring back to the ML-Agents
    docs. Basically, the idea here is that these parameters control the wall height
    which is increased over time. Hence, the agent needs to learn to move the block
    over to jump over the wall as it gets harder and harder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set the **Control** flag on the **Academy** brains and then run an ML-Agents
    session in a Python shell with the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Assuming the configuration files are in the correct place, you will be prompted
    to run the editor and watch the agent train in the environment. The results of
    this example are shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/308194a0-9602-4a54-aae2-4c80548cb9ee.png)'
  prefs: []
  type: TYPE_IMG
- en: The output of curriculum training example
  prefs: []
  type: TYPE_NORMAL
- en: Curriculum learning solves the problem of an environment not having an obvious
    answer in a novel way. In this case, the agent's goal is to make the target square.
    However, if the wall started out very high and the agent needed to move the block
    there to jump over it, it likely won't even understand it needs to get to a block.
    Therefore, we help it to train by first allowing it to get to the goal but then
    make it gradually harder to do so. As the difficulty increases, the agent learns
    how to use the block to jump over the wall.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we look to another method that helps agents to solve tasks
    with difficult to find or what we call sparse rewards.
  prefs: []
  type: TYPE_NORMAL
- en: Behavioral cloning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Behavioral cloning is sometimes also referred to as imitation learning. While
    not exactly both the same, we will use the terms interchangeably here. In RL,
    we use the term sparse rewards or rewards sparsity for any environment where it
    is difficult for an agent to just finish a task by trial and error and perhaps
    luck. The larger an environment is, the more sparse the rewards and in many cases,
    the observation space can be so large that any hope of training an agent at all
    is extremely difficult. Fortunately, a method called behavioral cloning or imitation
    learning can solve the problem of sparse rewards by using the observations of
    humans as previous sampled observations. Unity provides three methods to generate
    previous observations and they are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Generative Adversarial Imitation Learning** (**GAIL**): You can use something
    called the GAIL reward signal to enhance learning rewards from a few observations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pretraining**: This allows you to use prerecorded demonstrations likely from
    a human and use those to bootstrap the learning of the agent. If you use pretraining,
    you also need to provide a configuration section in your ML-Agents config file
    like so:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '**Behavioral Cloning** (**BC**): In this training, setup happens directly in
    the Unity editor. This is great for environments where small demonstrations can
    help to increase an agent''s learning. BC does not work so well on larger environments
    with a large observation state space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These three methods can be combined in a variety of configurations and used
    together in the case of pretraining and GAIL with other methods such as curiosity
    learning, which we will see later.
  prefs: []
  type: TYPE_NORMAL
- en: 'It can be especially entertaining to train an agent in real time with BC, as
    we''ll see in the next exercise. Follow the next exercise to explore using the
    BC method of demonstrating to an agent:'
  prefs: []
  type: TYPE_NORMAL
- en: Open the **TennisIL** scene located in the `Assets/ML-Agents/Examples/Tennis/Scenes`
    folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This environment is an example of a sparse rewards environment, whereby the
    agent needs to find and hit the ball back to its opponent. This environment makes
    for an excellent example to test BC with on.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Select the **Academy** object in the **Hierarchy** window and then check the
    **Control** option of **TennisLearning (LearningBrain)** in the **Inspector**
    window, as shown in the screenshot here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/f4b71a0c-c0d9-4e85-bad5-6437fccc68c2.png)'
  prefs: []
  type: TYPE_IMG
- en: Turning on the learning brain to control
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, there are two brains in this scene: one student brain— the
    learning brain, and one teacher brain—the player brain. The teacher brain, controlled
    by the human player, won''t control an actual agent but rather just take direct
    inputs from the player. The student brain observes the teacher''s actions and
    uses those as samples in its policy. In a basic sense, this becomes the teacher
    working from the human policy that the target policy, the agent, needs to learn.
    This is really no different than us having current and target networks.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The next thing we have to do is customize the ML-Agents hyperparameters config
    file. We customize the file by adding the following entry for `StudentBrain`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The highlighted elements in the preceding configuration show the `trainer:`
    set to `imitation` and `brain_to_imitate:` as `TeacherBrain`. Plenty of more information
    about setting up the configuration for ML-Agents can be found with the online
    docs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, you need to open a Python/Anaconda shell and change to the `mlagents`
    folder. After that, run the following command to start training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This will start the trainer and in short while you will be prompted to start
    the Unity editor in **Play** mode.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Press **Play** to put the editor in play mode and use the *WASD* controls to
    maneuver the paddle to play tennis against the agent. Assuming you do well, the
    agent will also improve. A screenshot of this training is shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/e6f26fd0-b5df-429d-a9cc-5962d065294e.png)'
  prefs: []
  type: TYPE_IMG
- en: Training the tennis agent with BC
  prefs: []
  type: TYPE_NORMAL
- en: Imitation learning was a key ingredient in training the agent, AlphaStar. AlphaStar
    was shown to beat human players at a very complex real-time strategy game called
    *StarCraft 2*. It has many keen benefits in getting agents past the sparse rewards
    problem. However, there are many in the RL community that want to avoid IL or
    BC because it can introduce human bias. Human bias has been shown to decrease
    agent performance when compared to agents trained entirely without BC. In fact,
    AlphaStar was trained to a sufficient enough level of playability before it was
    trained on itself. It was this self-training that is believed to be responsible
    for the innovation that allowed it to beat human players.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we look at another exciting way Unity has tried to capture
    a method to counter sparse reward problems.
  prefs: []
  type: TYPE_NORMAL
- en: Curiosity learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up until now, we have only ever considered external rewards given to the agent
    from the environment. Yet, we and other animals receive a wide variety of external
    and internal rewards. Internal rewards are often characterized by emotion or feeling.
    An agent could have an internal reward that gives it +1 every time it looks to
    some face, perhaps denoting some internal love or infatuation reward. These types
    of rewards are called intrinsic rewards and they represent rewards that are internal
    or self-derived by the agent. This has some powerful capabilities for everything
    from creating interesting motivated agents to enhancing an agent's learning ability.
  prefs: []
  type: TYPE_NORMAL
- en: It is the second way in which Unity introduced curiosity learning or the internal
    curiosity reward system as a way of letting agents explore more when they get
    surprised. That is, whenever an agent is surprised by an action, its curiosity
    increases and hence it needs to explore the state actions in the space that surprised
    it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unity has produced a very powerful example of curiosity learning in an environment
    called Pyramids. It is the goal of the agent in this environment to find a pile
    of yellow blocks with a gold block on top. Knock over the pile of blocks and then
    get the gold block. The problem is that there are piles of boxes in many rooms
    at the start but none start yellow. To turn the blocks yellow, that agent needs
    to find and press a button. Finding this sequence of tasks using straight RL could
    be problematic and/or time-consuming. Fortunately, with CL, we can improve this
    performance dramatically. We will look at how to use CL in the next section to
    train the Pyramids environment:'
  prefs: []
  type: TYPE_NORMAL
- en: Open the **Pyramids** scene located in the `Assets/ML-Agents/Examples/Pyramids/Scenes`
    folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Press Play to run the default agent; this will be one trained with Unity. When
    you run the agent, watch it play through the environment and you will see the
    agent first find the button, press it, then locate the pile of blocks it needs
    to knock over. It will knock over the boxes as shown in the sequence of screenshots
    here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/bd8c5703-0567-418a-aa63-667979789af6.png)'
  prefs: []
  type: TYPE_IMG
- en: Pyramids agent playing the environment
  prefs: []
  type: TYPE_NORMAL
- en: Training an agent with curiosity just requires setting the Academy to control
    the brain and running the ML-Agents trainer with the proper configuration. This
    documentation to drive CL has changed several times over the course of ML-Agents
    development. Therefore, it is recommended you consult the ML-Agents docs for the
    most recent documentation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: CL can be very powerful and the whole concept of intrinsic rewards has some
    fun and interesting application towards games. Imagine being able to power internal
    reward systems for enemy agents that may play to greed, power, or some other evil
    trait. In the next section, we finish out this chapter with a look at training
    generalized reinforcement learning agents.
  prefs: []
  type: TYPE_NORMAL
- en: Training generalized reinforcement learning agents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We often have to remind ourselves that RL is just a derivation of data science
    best practices and we often have to consider how could you fix a training issue
    with data science. In the case of RL, we see the same issues we see in data science
    and machine learning only at different scales and exposed in another manner. One
    example of this is when an agent is overfitted to an environment that we then
    try to apply to other general variations of that environment. For instance, imagine
    the Frozen Lake environment that could be various sizes or even provide random
    starting points or other variations. By introducing these types of variations,
    we allow our agent to better generalize to a wider variety of similar environments.
    It is this generalization that we want to introduce into our environment.
  prefs: []
  type: TYPE_NORMAL
- en: '**AGI** or **Artificial General Intelligence** is the concept of generalized
    training agents to the *n*^(th) degree. It is expected that a truly AGI agent
    would be able to be placed in any environment and learn to solve the task. This
    could take an amount of training but ideally, no other hyperparameters or other
    human intervention should be required.'
  prefs: []
  type: TYPE_NORMAL
- en: 'By making the environment stochastic, we are essentially increasing the likelihood
    of our methods that use distributional RL and noisy networks will also become
    more powerful. Unfortunately, enabling these types of parameters with other training
    code, or our PyTorch code, is not currently available. In the next exercise, we''ll
    learn how to set up a generalized training environment:'
  prefs: []
  type: TYPE_NORMAL
- en: Open the **WallJump** scene located in the `Assets/ML-Agents/Examples/WallJump/Scenes`
    folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**WallJump** is already set up and configured with several reset parameters
    we looked at earlier when we reviewed curriculum learning. This time, instead
    of progressively changing those parameters, we are going to have the environment
    sample them randomly.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The parameters we want to resample are based on this sample. We can create a
    new generalized YAML file called `walljump_generalize.yaml` in the config folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open and put the following text in this file and then save it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This sets up the sampling distributions for how we will sample the values.
    The values for the environment can then be sampled with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also define new sampler types or ways of sampling data values using
    a custom sampler that our classes place in the `sample_class.py` file in the ML-Agents
    code. The following is an example of a custom sampler:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, you can configure the config file to run this sampler like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Remember that you still need to sample the values and modify the environment's
    configuration when the agent resets. This will require modifying the code to sample
    the inputs using the appropriate samplers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can then run the Unity ML-Agents trainer code with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Being able to train agents in this manner allows your agents to be more robust
    and able to tackle various incarnations of your environments. If you are building
    a game that needs a practical agent, you will most likely need to train your agents
    in a generalized manner. Generalized agents will generally be able to adapt to
    unforeseen changes in the environment far better than an agent trained otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: That about does it for this chapter and, in the next section, we'll look at
    gaining further experience with the sample exercises for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The exercises in this section are intended to introduce you to Unity ML-Agents
    in more detail. If your preference is not to use ML-Agents as a training framework,
    then move on to the next section and the end of this chapter. For those of you
    still here, ML-Agents on its own is a powerful toolkit for quickly exploring DRL
    agents. The toolkit hides most of the details of DRL but that should not be a
    problem for you to figure out by now:'
  prefs: []
  type: TYPE_NORMAL
- en: Set up and run one of the Unity ML-Agents sample environments in the editor
    to train an agent. This will require that you consult the Unity ML-Agents documentation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tune the hyperparameters of a sample Unity environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start TensorBoard and run it so that it collects logs from the Unity runs folder.
    This will allow you to watch the training performance of the agents being trained
    with ML-Agents.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build a Unity environment and train it with the Rainbow DQN example.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Customize one of the existing Unity environments by changing the setup, parameters,
    reset parameters, and/or reward function. That is, change the reward feedback
    the agent receives when completing actions or tasks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set up and train an agent with pretrained data. This will require you to set
    up a player brain to record demonstrations. Play the game to record those demonstrations
    and then set the game for training with a learning brain.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train an agent with behavioral cloning using the tennis environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train an agent with curiosity learning using the Pyramids scene.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set up and run a Unity environment for generalized training. Use the sampling
    to pull stochastic values from distributions for the environment. What effect
    do different distributions have on the agent's training performance?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert a PG method example such as PPO so that you can run a Unity environment.
    How does the performance compare with Rainbow DQN?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use these examples to familiarize yourself with Unity ML-Agents and more advanced
    concepts in RL. In the next section, we will summarize and complete this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we took a diversion and built our own DRL environments for
    training with our own code, or another framework, or using the ML-Agents framework
    from Unity. At first, we looked at the basics of installing the ML-Agents toolkit
    for the development of environments, training, and training with our own code.
    Then, we looked at how to build a basic Unity environment for training from a
    Gym interface like we have been doing throughout this whole book. After that,
    we learned how our RainbowDQN sample could be customized to train an agent. From
    there, we looked at how we can create a brand new environment from the basics.
    We finished this chapter by looking at managing rewards in environments and the
    set of tools ML-Agents uses to enhance environments with sparse rewards. There,
    we looked at several methods Unity has added to ML-Agents to assist with difficult
    environments and sparse rewards.
  prefs: []
  type: TYPE_NORMAL
- en: Moving on from this chapter, we will continue to explore other DRL frameworks
    that can be used to train agents. ML-Agents is one of many powerful frameworks
    that can be used to train agents, as we will soon see.
  prefs: []
  type: TYPE_NORMAL
