- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Training an NER Component with Your Own Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you will learn how to use your own data to train spaCy’s pre-trained
    models. We will do that by training a **named entity recognition** ( **NER** )
    pipeline, but you can apply the same knowledge to preprocess and train spaCy pipelines
    for any NLP task. In this chapter, we will focus more on how to collect and label
    your own data, since we saw how to train models with spaCy’s **config.cfg** file
    in [*Chapter 6*](B22441_06.xhtml#_idTextAnchor087) .
  prefs: []
  type: TYPE_NORMAL
- en: The learning journey of this chapter includes how to make the best use of **Prodigy**
    , the annotation tool from Explosion, and the team behind spaCy. We will also
    see how to annotate NER data using Jupyter Notebook. After that, we will update
    the spaCy pipeline’s NER component with this labeled data.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter takes you through a complete machine learning practice, including
    collecting data, annotating data, and training a model for information extraction.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you’ll be ready to train spaCy models on your own
    data. You’ll have the full skill set of collecting data, preprocessing data into
    the format that spaCy can recognize, and finally, training spaCy models with this
    data. We’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with data preparation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Annotating and preparing data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a NER pipeline component
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combining multiple NER components in the same pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code of this Chapter can be found at [https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition](https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition)
    . We will use the **nertk** library to annotate the NER data using a Jupyter notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with data preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: spaCy out-of-the-box models are very successful for general NLP purposes but
    sometimes we have to work on very specific domains that require custom training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Training models requires time and effort. Before even starting the training
    process, you should decide whether the training is necessary. To determine whether
    you really need custom training, a good starting point is to ask yourself the
    following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: Do spaCy models perform well enough on your data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does your domain include many labels that are absent in spaCy models?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is there a pre-trained model/application in Hugging Face Hub or elsewhere already?
    (We wouldn’t want to reinvent the wheel.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s discuss these two first questions in detail in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Do spaCy models perform well enough on your data?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In general, if the model performs well enough (generally, something above 0.75
    accuracy), then you can customize the model output by means of another spaCy component.
    For example, let’s say we work on the navigation domain and we have utterances
    such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s see what entities spaCy’s NER model outputs for these sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Here, **home** isn’t recognized as an entity at all, but we want it to be recognized
    as a location entity. Also, spaCy’s NER model labels **Oxford Street** as **FAC**
    , meaning a building/highway/airport/bridge type entity, which is not what we
    want.
  prefs: []
  type: TYPE_NORMAL
- en: We want this entity to be recognized as **GPE** , a location. Here, we can train
    NER further to recognize street names as **GPE** , as well as recognize some location
    words (such as work, home, and my mama’s house) as **GPE** .
  prefs: []
  type: TYPE_NORMAL
- en: Another example is the newspaper domain. In this domain, **person** , **place**
    , **date** , **time** , and **organization** are all target entities that should
    be extracted, but you need one more entity type – **vehicle** (car, bus, airplane,
    and so on). Hence, instead of training from scratch, you can add a new entity
    type by using spaCy’s **SpanRuler** (explained in [*Chapter 4*](B22441_04.xhtml#_idTextAnchor056)
    ). Always examine your data first and calculate the spaCy models’ success rate.
    If the success rate is satisfying, then use other spaCy components to customize.
  prefs: []
  type: TYPE_NORMAL
- en: Does your domain include many labels that are absent in spaCy models?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For instance, in the preceding newspaper example, only one entity label, **vehicle**
    , is missing from the spaCy’s NER model’s labels but other entity types are recognized.
    Since we can recognize the **vehicle** entity with **SpanRuler** , in this case,
    you don’t need custom training.
  prefs: []
  type: TYPE_NORMAL
- en: Consider the medical domain again. The entities are diseases, symptoms, drugs,
    dosages, chemical compound names, and so on. This is a specialized and long list
    of entities. For the medical domain, you require custom model training.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we need custom model training, we usually follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Collect our data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Annotate our data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Decide to update an existing model or train a model from scratch.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the data collection step, we decide how much data to collect: 1,000 sentences,
    5,000 sentences, or more. The amount of data depends on the complexity of our
    task and domain. Usually, we start with an acceptable amount of data, train the
    model for the first time, and see how it performs; then, we can add more data
    and retrain the model.'
  prefs: []
  type: TYPE_NORMAL
- en: After collecting your dataset, you need to annotate your data in such a way
    that the spaCy training code recognizes it. In the next section, we will see the
    training data format and how to annotate data with Explosion’s Prodigy tool.
  prefs: []
  type: TYPE_NORMAL
- en: 'The third step is to decide on training a blank model from scratch or making
    updates to an existing model. Here, the rule of thumb is as follows: if your entities/labels
    are present in the existing model but you don’t see a very good performance, then
    update the model with your own data, such as in the preceding **vehicle** example.
    If your entities are not present in the current spaCy model at all, then most
    probably you need custom training.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll start our journey of building a model with the first step: preparing
    our training data. Let’s move on to the next section and see how to prepare and
    annotate our training data.'
  prefs: []
  type: TYPE_NORMAL
- en: Annotating and preparing data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step of training a model is always preparing training data. You usually
    collect data from customer logs and then turn them into a dataset by dumping the
    data as a CSV file or a JSON file. spaCy model training code works with **DocBin**
    objects, so we will be annotating the data and converting it to **DocBin** objects
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: After collecting our data, we annotate our data. Annotation means labeling the
    intent, entities, POS tags, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is an example of annotated data, taken from spaCy’s *Detecting Fashion
    Brands in Online Comments* tutorial (available at [https://github.com/explosion/projects/tree/v3/tutorials/ner_fashion_brands](https://github.com/explosion/projects/tree/v3/tutorials/ner_fashion_brands)
    ):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We annotated the data with the goal of pointing the statistical algorithm to
    what we want the model to learn. In this example, we want the model to learn about
    the entities; hence, we feed examples with entities annotated.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ll also see spaCy’s annotation tool, **Prodigy** , along
    with **nertk** , a simple open source Python library to annotate NER data using
    Jupyter Notebook. Prodigy is tailored for small teams working on fast-paced projects.
    Although it’s not free or open source, buying Prodigy helps fund open source initiatives,
    including the development of spaCy itself.
  prefs: []
  type: TYPE_NORMAL
- en: Annotating data with Prodigy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Prodigy is a modern tool powered by active learning. We will be using the *Prodigy
    live demo* ( [https://demo.prodi.gy/](https://demo.prodi.gy/) ) to exhibit how
    an annotation tool works.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We navigate to the Prodigy Live Demo and view an example text by Prodigy to
    be annotated, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Prodigy interface (screenshot taken from their Prodigy Demo
    page)](img/B22441_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – Prodigy interface (screenshot taken from their Prodigy Demo page)
  prefs: []
  type: TYPE_NORMAL
- en: This screenshot, Figure 8 .1, shows an example text that we want to annotate.
    The buttons at the bottom of the screenshot showcase the means to either accept
    this training example, reject it, or ignore it. If the text is relevant and the
    annotation is good, then we accept it and it joins our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll label the entities. To do that, first, we select an entity type
    from the upper bar. This corpus includes two types of entities, **PERSON** and
    **ORG** . Then, we’ll just select the words we want to label as an entity with
    the cursor, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.2 – Annotating PERSON entities on the Prodigy Demo page](img/B22441_08_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 – Annotating PERSON entities on the Prodigy Demo page
  prefs: []
  type: TYPE_NORMAL
- en: After we’re finished with annotating the text, we click the **Accept** button.
    Once the session is finished, you can dump the annotated data as a JSON file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prodigy features do not only include NER but also text classification, computer
    vision, prompt engineering, large language models, and more.
  prefs: []
  type: TYPE_NORMAL
- en: Annotating data with nertk
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another annotation tool is **nertk** , which is an open source Python library
    for NER text annotation ( [https://github.com/johnsmithm/nertk](https://github.com/johnsmithm/nertk)
    ). The tool works inline on Jupyter Notebook allowing you to interact with your
    data and annotate it quickly and easily. The project is currently in alpha development,
    so it’s useful only for simple and quick NER annotation sessions.
  prefs: []
  type: TYPE_NORMAL
- en: 'To work with **nertk** , you need to install it using **pip install nertk**
    , run a Jupyter Notebook server, and open the URL for the server in a web browser.
    To process the data for **nertk** , you need to tokenize the text and pass the
    data you want to annotate. Let’s do that step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to tokenize the data because **nertk** works with lists of words.
    Our dataset for this chapter is in Portuguese so we import the **en** model and
    select only the **tokenizer** component since is the only thing we need:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will use the dataset of spacy’s *Detecting fashion brands in online comments*
    tutorial. The raw text was sourced from the **r/MaleFashionAdvice** and **r/FemaleFashionAdvice**
    subreddits. The data is in the **jsonl** format, so we’ll use the **srsly** library
    to read it. Let’s print the first item to see what the data looks like:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Since the data is already annotated with Prodigy, it has the **['text', 'meta',
    '_input_hash', '_task_hash', 'tokens', 'spans', '_session_id', '_view_id', 'answer']**
    keys. To learn how we would go if we didn’t have the entities, let’s use just
    the **text** key, which contains the text of Reddit’s comment.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The next step is to tokenize the data. We need to create a list where each
    item is also a list, but a list of the words of the comment. To do that, we’ll
    loop through the data, tokenize it with the model we loaded previously, and add
    this to the main list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we are ready to start the **nertk** tool. We need to pass to the **Entator**
    class the list with the entity label names and the list with the words of each
    comment. Then, we can call the **run()** method to start annotating:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will annotate all the **FASHION_BRAND** entities. *Figure 8* *.3* shows
    the **nertk** interface:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.3 – The nertk interface](img/B22441_08_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 – The nertk interface
  prefs: []
  type: TYPE_NORMAL
- en: To select the tokens that are part of **FASHION_BRAND** entities, you simply
    click on them. When you are ready with the example, you can click on the next
    button to annotate another one. The results of the annotated tokens are stored
    in the **annotator.targets** attribute.
  prefs: []
  type: TYPE_NORMAL
- en: Converting the annotated data to DocBin objects
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To use this annotated data to train the models, we need to convert it to the
    **DocBin** format. To do that, we will create a **Doc** object for each comment
    and add the entities we’ve annotated. Let’s create the code to do that:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing to do is to create the **DocBin** object and loop through each
    comment getting the indexes of the annotated entities:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With **nertk** , we annotated each token, but an entity can encompass more
    than one token. To handle that, we use a helper function that gets the **indexes_entity_tokens**
    list and creates tuples with the indexes of the start and end token of each entity
    (this code is still inside the main **for** loop):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, it’s time to create the **Span** objects for each entity. We create an
    **ents** list to store these spans, set this list as the **doc.ents** parameter,
    and add this **Doc** to the **DocBin** object (this code is also still inside
    the main **for** loop):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we save the **DocBin** file to disk:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the full code to convert the **nertk** annotated entities to **DocBin**
    :'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This is the process of creating the training data to fine-tune the **ner**
    spaCy component. To create training data for any other trainable component, the
    process is the same. You create the **Doc** object, setting the annotations with
    the appropriate format, and then save these Docs as a **DocBin** object. You can
    check all the data formats here: [https://spacy.io/api/data-formats#dict-input](https://spacy.io/api/data-formats#dict-input)
    .'
  prefs: []
  type: TYPE_NORMAL
- en: With the training data set, the next step is to train the **ner** component.
    Let’s do that in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Training an NER pipeline component
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our goal is to create a pipeline to identify the **FASHION_BRAND** entities.
    We will do that by training the **EntityRecognizer** pipeline component. It uses
    a transition-based algorithm that assumes the most decisive information about
    the entities that are close to the initial tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing we need to do to train the component is to create the **config**
    file. We’ll use the **spacy init config** CLI command to do that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This creates the **cpu_config.cfg** file with all default configurations and
    sets the **ner** component for training optimized for **efficiency** (faster inference,
    smaller model, and lower memory consumption). We will use the **preprocess.py**
    script from the spaCy tutorial to convert the **jsonl** data into **DocBin** objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we are ready to train the model using the **spacy train** CLI command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The **output** parameter specifies the directory to store the trained pipeline.
    In *Figure 8* *.4* , we can see the messages produced during training.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4 – Messages produced during the training of the ner component](img/B22441_08_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 – Messages produced during the training of the ner component
  prefs: []
  type: TYPE_NORMAL
- en: We can see that, during training, there is a training loss for a **tok2vec**
    component and a **ner** component. That’s because the **ner** component by default
    uses the **spacy.TransitionBasedParser.v2** model architecture. As the spaCy documentation
    says ( [https://spacy.io/api/architectures#TransitionBasedParser](https://spacy.io/api/architectures#TransitionBasedParser)
    ), the model consists of either two or three subnetworks. The **tok2vec** component
    transforms each token into a vector, running once per batch. **Lower** generates
    a vector for each **(token, feature)** pair per batch, combining features and
    applying non-linearity to form the state representation. **Upper** is an optional
    feed-forward network that derives scores from the state representation. If omitted,
    the lower model’s output is used directly as action scores. In summary, the **tok2vec**
    component is responsible for mapping the tokens into vector representations.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the accuracy of the NER component
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can use the **spacy evaluate** CLI command to evaluate the accuracy of the
    pipeline. The command expects a loadable spaCy pipeline and evaluation data in
    the **DocBin** format. We don’t have a test dataset so we will use the **fashion_brands_eval.spacy**
    dataset just to see how the command works. The results are then **overestimated**
    because the dataset was also used to decide when to stop the model training. This
    is called **data leakage** . However, here, we are training the model for learning
    purposes so we’re fine. Let’s run the command and check the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The command displays the output and saves the metrics to the **training_cpu/metrics.json**
    file. *Figure 8* *.5* shows the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5 – Accuracy of the ner model](img/B22441_08_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.5 – Accuracy of the ner model
  prefs: []
  type: TYPE_NORMAL
- en: The precision score is **76.88** and the recall is **55.88** . This is a model
    trained on the CPU; what if we train it with GPU and use transformers? spaCy makes
    it simple to try this; we just need to create a new **config.cfg** file and run
    the **spacy train** command again. Let’s do that in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Training an NER component optimized for accuracy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Training a component optimized for accuracy and that runs on a GPU requires
    that you install the **spacy-transformers** library. After that, we can create
    the new config file like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can train the model again, now pointing to this **gpu_config.cfg**
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'My machine has a GPU so I’m setting it using the **–gpu-id 1** parameter. When
    we train for accuracy, usually, the model behaves better but we also end up with
    a potentially larger and slower model. *Figure 8* *.6* shows the messages produced
    during training:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.6 – Messages produced during the training of the ner component for
    accuracy](img/B22441_08_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.6 – Messages produced during the training of the ner component for
    accuracy
  prefs: []
  type: TYPE_NORMAL
- en: 'This architecture uses a transformer Model component instead of the **tok2vec**
    component. We can see that the scores of this pipeline at each evaluation point
    are higher, always above **0.78** . Let’s run the evaluation again to compare
    the models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The command displays the output and saves the metrics to the **training_gpu/metrics.json**
    file. *Figure 8* *.7* shows the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.7 – Accuracy of the ner model trained for accuracy](img/B22441_08_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.7 – Accuracy of the ner model trained for accuracy
  prefs: []
  type: TYPE_NORMAL
- en: 'The precision score is **88.79** and the recall is **79.83** . This is a significant
    increase if we compare it with the **76.88** and **55.88** results of the model
    trained for efficiency. We can load the models for usage by passing the directories
    created during training to the **nlp.load()** method. Let’s get the entities of
    the sentence **Givenchy is looking at buying U.K. startup for $** **1 billion**
    :'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we load the libraries and the pipeline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can apply the pipeline to the text:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result is displayed in *Figure 8* *.8* ; the pipeline was able to find
    the **Givenchy** entity:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.8 – The entities extracted using the ner pipeline trained for accuracy](img/B22441_08_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.8 – The entities extracted using the ner pipeline trained for accuracy
  prefs: []
  type: TYPE_NORMAL
- en: 'The component can only identify **FASHION_BRAND** entities, which was the only
    entity we’ve trained it for. But the **en_core_web_sm** model made available by
    spaCy can identify more entities. Let’s check it out:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the libraries and the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Apply the pipeline to the text:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result is displayed in *Figure 8* *.9* ; the pipeline was able to find
    the **GPE** and **MONEY** entities:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.9 – The entities extracted using the en_core_web_sm pipeline](img/B22441_08_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.9 – The entities extracted using the en_core_web_sm pipeline
  prefs: []
  type: TYPE_NORMAL
- en: What if we want to extract both the entities we’ve trained for and the entities
    the **en_core_web_sm** NER component is trained for? We can do that by combining
    multiple NER components in the same pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Combining multiple NER components in the same pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When spaCy loads a pipeline, it iterates over the pipeline names and looks up
    each component name in the **[components]** block. The components can be built
    either using **factory** or **source** . **Factories** are named functions that
    take settings and return a pipeline component function, and the source is used
    to reference the name of the path of a trained pipeline to copy components from.
  prefs: []
  type: TYPE_NORMAL
- en: To use both the NER component we’ve trained and the NER component of **en_core_web_sm**
    , we will create a new **config.cfg** file and source the NER components in the
    pipeline. With this **config.cfg** file set, we will then use the **spacy assemble**
    command to assemble the config without additional training. To be able to reference
    the trained NER component using **spacy assemble** , we will create a package
    for the pipeline and install it with **pip install** .
  prefs: []
  type: TYPE_NORMAL
- en: Creating a package for the trained pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **spacy package** CLI command generates an installable Python package from
    an existing pipeline directory. spaCy creates a build artifact that you can install
    with **pip install** . This is useful if you want to deploy your pipeline in a
    production environment or share it with others.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will provide the path to the directory containing pipeline data, the path
    to the directory in which to create the package folder, and the package name.
    This is the full command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The command creates a folder named **en_ner_fashion_brands-0.0.0** . We can
    now install this package and load it in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install the package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the package, load the pipeline, and apply it to the text:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: It works just like loading the pipeline from its directory. Now, we can point
    to the NER component of this pipeline in a new **config** file to create a pipeline
    that uses this NER component and the **en_core_web_sm** NER component.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a pipeline with different NER components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The **spacy assemble** CLI command assembles a pipeline from a **config** file
    without additional training. Simple as that. This **config** file is simpler since
    it’s used only to build the pipeline and not to train it. Let’s create the **combined_ner.cfg**
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we define the **nlp** object. We will specify the pipeline language
    ISO code and the pipeline components’ names in order. We will specify how to load
    them in the **[components]** section, so we can name them as we want:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Since we want to copy the components from existing pipelines, we will set the
    **source** argument of each component. The argument values can be a loadable spaCy
    pipeline package or a path. After defining, we set the **component** argument,
    which is the name of the component in the **source** pipeline. Let’s first define
    how to load the **ner** component of the **en_core_web_sm** pipeline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, to load the **ner** component we’ve trained to identify the fashion brands,
    we also need to include a copy of the **tok2vec** listener of the **source** pipeline
    using the **replace_listeners** argument, like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, we have a **config** file to assemble our final pipeline. We’ll do that
    next.
  prefs: []
  type: TYPE_NORMAL
- en: Assembling the pipeline from a config file
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To assemble the pipeline, we will provide the **combined_ner.cfg** file we
    have created and the output directory to store the final pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'If everything worked correctly, we could now load this pipeline and it will
    identify all the entities of both **ner** components. Let’s see whether that is
    the case:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first load the pipeline we’ve assembled:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s process the text and show the entities with **displacy** :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can see the results in *Figure 8* *.10* :'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.10 – Entities identified with the assembled pipeline #TODO: take
    new screenshot](img/B22441_08_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.10 – Entities identified with the assembled pipeline #TODO: take new
    screenshot'
  prefs: []
  type: TYPE_NORMAL
- en: Neat, right? We could combine as many **ner** components as we need.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored how to train spaCy NER components with our own
    domain and data. First, we learned the key points of deciding whether we really
    need custom model training. Then, we went through an essential part of model training
    – data collection and labeling.
  prefs: []
  type: TYPE_NORMAL
- en: We learned about two annotation tools – Prodigy and **nertk** – and learned
    how to convert the data for training and how to train the component using spaCy’s
    CLI. Then, we used spaCy CLI commands to train the component and create a Python
    package for the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we learned how to combine different NER components into a single pipeline.
    In the next chapter, we will learn how to manage and share end-to-end workflows
    for different use cases and domains using spaCy and Weasel.
  prefs: []
  type: TYPE_NORMAL
