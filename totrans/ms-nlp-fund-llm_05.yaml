- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Empowering Text Classification: Leveraging Traditional Machine Learning Techniques'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ll delve into the fascinating world of text classification,
    a foundational task in **natural language processing** (**NLP**) and **machine
    learning** (**ML**) that deals with categorizing text documents into predefined
    classes. As the volume of digital text data continues to grow exponentially, the
    ability to accurately and efficiently classify text has become increasingly important
    for a wide range of applications, such as sentiment analysis, spam detection,
    and document organization. This chapter provides a comprehensive overview of the
    key concepts, methodologies, and techniques that are employed in text classification,
    catering to readers from diverse backgrounds and skill levels.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll begin by exploring the various types of text classification tasks and
    their unique characteristics, offering insights into the challenges and opportunities
    each type presents. Next, we’ll introduce the concept of **N-grams** and discuss
    how they can be utilized as features for text classification, capturing not only
    individual words but also the local context and word sequences within the text.
    We’ll then examine the widely used **term frequency-inverse document frequency**
    (**TF-IDF**) method, which assigns weights to words based on their frequency in
    a document and across the entire corpus, showcasing its effectiveness in distinguishing
    relevant words for classification tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Following that, we’ll delve into the powerful **Word2Vec** algorithm and its
    application in text classification. We’ll discuss how **Word2Vec** creates dense
    vector representations of words that capture semantic meaning and relationships,
    and how these embeddings can be used as features to improve classification performance.
    Furthermore, we’ll cover popular architectures such as **continuous bag-of-words**
    (**CBOW**) and Skip-Gram, providing a deeper understanding of their inner workings.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we’ll explore the concept of topic modeling, a technique for discovering
    hidden thematic structures within a collection of documents. We’ll examine popular
    algorithms such as **latent Dirichlet allocation** (**LDA**) and describe how
    topic modeling can be applied to text classification, enabling the discovery of
    semantic relationships between documents and improving classification performance.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this chapter, we aim to provide a thorough understanding of the underlying
    concepts and techniques that are employed in text classification, equipping you
    with the knowledge and skills needed to successfully tackle real-world text classification
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Types of text classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text classification based on N-grams
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text classification based on TF-IDF
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Word2Vec and its application in text classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Topic modeling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reviewing our use case – ML system design for NLP classification in a Jupyter
    notebook
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To effectively read and understand this chapter, it is essential to have a solid
    foundation in various technical areas. A strong grasp of fundamental concepts
    in NLP, ML, and linear algebra is crucial. Familiarity with text preprocessing
    techniques, such as tokenization, stop word removal, and stemming or lemmatization,
    is necessary to comprehend the data preparation stage.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, understanding basic ML algorithms, such as logistic regression
    and **support vector machines** (**SVMs**), is crucial for implementing text classification
    models. Finally, being comfortable with evaluation metrics such as accuracy, precision,
    recall, and F1 score, along with concepts such as overfitting, underfitting, and
    hyperparameter tuning, will enable a deeper appreciation of the challenges and
    best practices in text classification.
  prefs: []
  type: TYPE_NORMAL
- en: Types of text classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Text classification is an NLP task where ML algorithms assign predefined categories
    or labels to text based on its content. It involves training a model on a labeled
    dataset to enable it to accurately predict the category of unseen or new text
    inputs. Text classification methods can be categorized into three main types –
    **supervised learning**, **unsupervised learning**, and **semi-supervised learning**:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Supervised learning**: This type of text classification involves training
    a model on labeled data, where each data point is associated with a target label
    or category. The model then uses this labeled data to learn the patterns and relationships
    between the input text and the target labels. Examples of supervised learning
    algorithms for text classification include naive bayes, SVMs, and neural networks
    such as **convolutional neural networks** (**CNNs**) and **recurrent neural**
    **networks** (**RNNs**).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unsupervised learning**: This type of text classification involves clustering
    or grouping text documents into categories or topics without any prior knowledge
    of the categories or labels. Unsupervised learning is useful when there is no
    labeled data available or when the number of categories or topics is not known.
    Examples of unsupervised learning algorithms for text classification include K-means
    clustering, LDA, and **hierarchical Dirichlet** **process** (**HDP**).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Semi-supervised learning**: This type of text classification combines both
    supervised and unsupervised learning approaches. It involves using a small amount
    of labeled data to train a model and then using the model to classify the remaining
    unlabeled data. The model then uses the unlabeled data to improve its classification
    performance. Semi-supervised learning is useful when labeled data is scarce or
    expensive to obtain. Examples of semi-supervised learning algorithms for text
    classification include **self-training**, **co-training**, and **multi-view learning**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of these text classification types has its strengths and weaknesses and
    is suitable for different types of applications. Understanding these types can
    help in choosing the appropriate approach for a given problem. In the following
    subsections, we’ll explain each of these methods in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Supervised learning is a type of ML where an algorithm learns from labeled data
    to predict the label of new, unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the context of text classification, supervised learning involves training
    a model on a labeled dataset, where each document or text sample is labeled with
    the corresponding category or class. The model then uses this training data to
    learn patterns and relationships between the text features and their associated
    labels:'
  prefs: []
  type: TYPE_NORMAL
- en: In a supervised text classification task, the first step is to obtain a labeled
    dataset, where each text sample is annotated with its corresponding category or
    class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A labeled dataset is assumed to possess the highest level of reliability. Often,
    it is derived by having subject matter experts manually review the text and assign
    the appropriate class to each item. In other scenarios, there may be automated
    methods for deriving the labels. For instance, in cybersecurity, you may collect
    historical data and then assign labels, which may collect the outcome that followed
    each item – that is, whether the action was legitimate or not. Since such historical
    data exists in most domains, that too can serve as a reliable labeled set.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The next step is to preprocess the text data to prepare it for modeling. This
    may include steps such as tokenization, stemming or lemmatization, removing stop
    words, and other text preprocessing techniques.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After preprocessing, the text data is transformed into numerical features, often
    using techniques such as bag-of-words or TF-IDF encoding.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, a supervised learning algorithm such as logistic regression, SVM, or a
    neural network is trained on the labeled dataset using these numerical features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the model has been trained, it can be used to predict the category or class
    of new, unseen text data based on the learned patterns and relationships between
    the text features and their associated labels.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning algorithms are commonly used for text classification tasks.
    Let’s look at some common supervised learning algorithms that are used for text
    classification.
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Naive Bayes is a probabilistic algorithm that is commonly used for text classification.
    It is based on Bayes’ theorem, which states that the probability of a hypothesis
    (in this case, a document belonging to a particular class), given some observed
    evidence (in this case, the words in the document), is proportional to the probability
    of the evidence given the hypothesis times the prior probability of the hypothesis.
    Naive Bayes assumes that the features (words) are independent of each other given
    the class label, which is where the “naive” part of the name comes from.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Logistic regression is a statistical method that is used for binary classification
    problems (that is, problems where there are only two possible classes). It models
    the probability of the document belonging to a particular class using a logistic
    function, which maps any real-valued input to a value between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: SVM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: SVM is a powerful classification algorithm that is used in a variety of applications,
    including text classification. SVM works by finding the hyperplane that best separates
    the data into different classes. In text classification, the features are typically
    the words in the document, and the hyperplane is used to divide the space of all
    possible documents into different regions corresponding to different classes.
  prefs: []
  type: TYPE_NORMAL
- en: All of these algorithms can be trained using labeled data, where the class labels
    are known for each document in the training set. Once trained, the model can be
    used to predict the class label of new, unlabeled documents. The performance of
    the model is typically evaluated using metrics such as accuracy, precision, recall,
    and F1 score.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unsupervised learning is a type of ML where the data is not labeled and the
    algorithm is left to find patterns and structures on its own. In the context of
    text classification, unsupervised learning methods can be used when there is no
    labeled data available or when the goal is to discover hidden patterns in the
    text data.
  prefs: []
  type: TYPE_NORMAL
- en: One common unsupervised learning method for text classification is **clustering**.
    Clustering algorithms group similar documents together based on their content,
    without any prior knowledge of what each document is about. Clustering can be
    used to identify topics in a collection of documents or to group similar documents
    together for further analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Another popular unsupervised learning algorithm for text classification is **LDA**.
    LDA is a probabilistic generative model that assumes that each document in a corpus
    is a mixture of topics, and each topic is a probability distribution over words.
    LDA can be used to discover the underlying topics in a collection of documents,
    even when the topics are not explicitly labeled.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, word embeddings are a popular unsupervised learning technique used
    for text classification. Word embeddings are dense vector representations of words
    that capture their semantic meaning based on the context in which they appear.
    They can be used to identify similar words and to find relationships between words,
    which can be useful for tasks such as text similarity and recommendation systems.
    Common word embedding models include Word2Vec and GloVe.
  prefs: []
  type: TYPE_NORMAL
- en: Word2Vec is a popular algorithm that’s used to generate word embeddings, which
    are vector representations of words in a high-dimensional space. The algorithm
    was developed by a team of researchers at Google, led by Tomas Mikolov, in 2013\.
    The main idea behind Word2Vec is that words that appear in similar contexts tend
    to have similar meanings.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm takes in a large corpus of text as input and generates a vector
    representation for each word in the vocabulary. The vectors are typically high-dimensional
    (for example, 100 or 300 dimensions) and can be used to perform various NLP tasks,
    such as sentiment analysis, text classification, and machine translation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Two main architectures are used in Word2Vec: **CBOW** and **skip-gram**. In
    the CBOW architecture, the algorithm tries to predict the target word given a
    window of context words. In the skip-gram architecture, the algorithm tries to
    predict the context words given a target word. The training objective is to maximize
    the likelihood of the target word or context words given the input.'
  prefs: []
  type: TYPE_NORMAL
- en: Word2Vec has been widely adopted in the NLP community and has shown state-of-the-art
    performance on various benchmarks. It has also been used in many real-world applications,
    such as recommender systems, search engines, and chatbots.
  prefs: []
  type: TYPE_NORMAL
- en: Semi-supervised learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Semi-supervised learning is an ML paradigm that sits between supervised and
    unsupervised learning. It utilizes a combination of labeled and unlabeled data
    for training, which is especially useful when the underlying models require labeled
    data which is expensive or time-consuming. This approach allows the model to leverage
    the information in the unlabeled data to improve its performance on the classification
    task.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of text classification, semi-supervised learning can be beneficial
    when we have a limited number of labeled documents but a large corpus of unlabeled
    documents. The goal is to improve the performance of the classifier by leveraging
    the information contained in the unlabeled data.
  prefs: []
  type: TYPE_NORMAL
- en: There are several common semi-supervised learning algorithms, including label
    propagation and co-training. We’ll discuss each of these in more detail next.
  prefs: []
  type: TYPE_NORMAL
- en: Label propagation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Label propagation is a graph-based semi-supervised learning algorithm. It builds
    a graph using both labeled and unlabeled data points, with each data point represented
    as a node and edges representing the similarity between nodes. The algorithm works
    by propagating the labels from the labeled nodes to the unlabeled nodes based
    on their similarity.
  prefs: []
  type: TYPE_NORMAL
- en: The key idea is that similar data points should have similar labels. The algorithm
    begins by assigning initial label probabilities to the unlabeled nodes, typically
    based on their similarity to labeled nodes. Then, an iterative process propagates
    these probabilities throughout the graph until convergence. The final label probabilities
    are used to classify the unlabeled data points.
  prefs: []
  type: TYPE_NORMAL
- en: Co-training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Co-training is another semi-supervised learning technique that trains multiple
    classifiers on different views of the data. A view is a subset of features that
    are sufficient for the learning task and are conditionally independent given the
    class label. The basic idea is to use one classifier’s predictions to label some
    of the unlabeled data, and then use that newly labeled data to train the other
    classifier. This process is performed iteratively, with each classifier improving
    the other until a stopping criterion is met.
  prefs: []
  type: TYPE_NORMAL
- en: To apply semi-supervised learning in a specific domain, let’s consider a medical
    domain where we want to classify scientific articles into different categories
    such as **cardiology**, **neurology**, and **oncology**. Suppose we have a small
    set of labeled articles and a large set of unlabeled articles.
  prefs: []
  type: TYPE_NORMAL
- en: A possible approach could be to use label propagation by creating a graph of
    articles where the nodes represent the articles and the edges represent the similarity
    between the articles. The similarity could be based on various factors, such as
    the words used, the topics covered, or the citation networks between the articles.
    After propagating the labels, we can classify the unlabeled articles based on
    the final label probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, we could use co-training by splitting the features into two views,
    such as the abstract and the full text of the articles. We would train two classifiers,
    one for each view, and iteratively update the classifiers using the predictions
    made by the other classifier on the unlabeled data.
  prefs: []
  type: TYPE_NORMAL
- en: In both cases, the goal is to leverage the information in the unlabeled data
    to improve the performance of the classifier in the specific domain.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll elaborate on supervised text classification and topic
    modeling.
  prefs: []
  type: TYPE_NORMAL
- en: Sentence classification using one-hot encoding vector representation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One-hot encoded vector representation is a method of representing categorical
    data, such as words, as binary vectors. In the context of text classification,
    one-hot encoding can be used to represent text data as numerical input features
    for a classification model. Here’s a detailed explanation of text classification
    using one-hot encoding vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Text preprocessing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first step is to preprocess the text data, as explained in the previous
    chapter. The main goal of preprocessing is to transform raw text into a more structured
    and consistent format that can be easily understood and processed by ML algorithms.
    Here are several reasons why text preprocessing is essential for one-hot encoded
    vector classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Noise reduction**: Raw text data often contains noise, such as typos, spelling
    errors, special characters, and formatting inconsistencies. Preprocessing helps
    to clean the text, reducing noise that may negatively impact the performance of
    the classification model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dimensionality reduction**: One-hot encoded vector representation has a high
    dimensionality as each unique word in the dataset corresponds to a separate feature.
    Preprocessing techniques, such as stop word removal, stemming, or lemmatization,
    can help reduce the size of the vocabulary, leading to a lower-dimensional feature
    space. This can improve the efficiency of the classification algorithm and reduce
    the risk of overfitting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Consistent representation**: Converting all text to lowercase and applying
    stemming or lemmatization ensures that words with the same meaning or root form
    are consistently represented in the one-hot encoding vectors. This can help the
    classification model learn more meaningful patterns from the data as it will not
    treat different forms of the same word as separate features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Handling irrelevant information**: Preprocessing can help remove irrelevant
    information, such as URLs, email addresses, or numbers, that may not contribute
    to the classification task. Removing such information can improve the model’s
    ability to focus on the meaningful words and patterns in the text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Improving model performance**: Preprocessed text data can lead to better
    performance of the classification model as the model will learn from a cleaner
    and more structured dataset. This can result in improved accuracy and generalization
    to new, unseen text data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once we preprocess the text, we can start extracting the words in the text.
    We call this task vocabulary construction.
  prefs: []
  type: TYPE_NORMAL
- en: Vocabulary construction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Construct a vocabulary containing all unique words in the preprocessed text.
    Assign a unique index to each word in the vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Vocabulary construction is an essential step in preparing text data for one-hot
    encoded vector classification. The vocabulary is a set of all unique words (tokens)
    in the preprocessed text data. It serves as a basis for creating one-hot-encoded
    feature vectors for each document. Here’s a detailed explanation of the vocabulary
    construction process for one-hot encoded vector classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Create a set of unique words**: After preprocessing the text data, gather
    all the words from all documents and create a set of unique words. This set will
    represent the vocabulary. The order of the words in the vocabulary does not matter,
    but it’s crucial to keep track of the indices assigned to each word as they will
    be used to create one-hot encoded vectors later.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For example, consider that the following preprocessed dataset consists of two
    documents:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Document 1**: “apple banana orange”'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Document 2**: “banana grape apple”'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The vocabulary for this dataset would be {“apple”, “banana”, “orange”, “grape”}.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Assign indices to the words**: Once you have the set of unique words, assign
    a unique index to each word in the vocabulary. These indices will be used to create
    one-hot-encoded vectors for each document.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Using the preceding example, you might assign the following indices:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '“apple”: 0'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '“banana”: 1'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '“orange”: 2'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '“grape”: 3'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: One-hot encoding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the constructed vocabulary and assigned indices, you can now create one-hot
    encoded vectors for each document in the dataset. One simple approach to creating
    a one-hot encoded vector is to use **bag-of-words**. For each word in a document,
    find its corresponding index in the vocabulary and set the value at that index
    to 1 in the one-hot-encoded vector. If a word appears multiple times in the document,
    its corresponding value in the one-hot-encoded vector remains 1\. All other values
    in the vector will be 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, using the vocabulary and indices mentioned previously, the one-hot
    encoded vectors for the documents would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Document 1**: [1, 1, 1, 0] (apple, banana, and orange are present)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Document 2**: [1, 1, 0, 1] (apple, banana, and grape are present)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once we have the corresponding values for each document, we can create a feature
    matrix with one-hot-encoded vectors as rows, where each row represents a document
    and each column represents a word from the vocabulary. This matrix will be used
    as input for the text classification model. For example, in the previous example,
    the feature vectors for two documents are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Apple | Banana | Orange | Grape |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Document 1 | 1 | 1 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Document 2 | 1 | 1 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: Table 5.1 – Sample one-hot-encoded vector for two documents
  prefs: []
  type: TYPE_NORMAL
- en: Please note that with text preprocessing, it helps to have a smaller vocabulary
    and it gives us better model performance. Besides that, if needed, we can perform
    feature selection methods (as explained previously in this book) on the extracted
    feature vectors to improve our model performance.
  prefs: []
  type: TYPE_NORMAL
- en: While creating a one-hot encoded vector from words is useful, sometimes, we
    need to consider the existence of two words beside each other. For example, “very
    good” and “not good” can have different meanings. To achieve this goal, we can
    use N-grams.
  prefs: []
  type: TYPE_NORMAL
- en: N-grams
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: N-grams are a generalization of the bag-of-words model that takes into account
    the order of words by considering sequences of *n* consecutive words. An N-gram
    is a contiguous sequence of *n* items (typically words) from a given text. For
    example, in the sentence “The cat is on the mat,” the 2-grams (bigrams) would
    be “The cat,” “cat is,” “is on,” “on the,” and “the mat.”
  prefs: []
  type: TYPE_NORMAL
- en: Using N-grams can help capture local context and word relationships, which may
    improve the performance of the classifier. However, it also increases the dimensionality
    of the feature space, which can be computationally expensive.
  prefs: []
  type: TYPE_NORMAL
- en: Model training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Train an ML model, such as logistic regression, SVM, or neural networks, on
    the feature matrix to learn the relationship between the one-hot encoded text
    features and the target labels. The model will learn to predict the class label
    based on the presence or absence of specific words in the document. Once we’ve
    decided on the training process, we need to perform the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model evaluation**: Evaluate the performance of the model using appropriate
    evaluation metrics, such as accuracy, precision, recall, F1 score, or confusion
    matrix, and use techniques such as cross-validation to get a reliable estimate
    of the model’s performance on unseen data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model application**: Apply the trained model to new, unseen text data. Preprocess
    and one-hot encode the new text data using the same vocabulary and use the model
    to predict the class labels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One potential limitation of using one-hot encoded vectors for text classification
    is that they do not capture word order, context, or semantic relationships between
    words. This can lead to suboptimal performance, especially in more complex classification
    tasks. More advanced techniques, such as word embeddings (for example, Word2Vec
    or GloVe) or deep learning models (for example, CNNs or RNNs), can provide better
    representations for text data in these cases.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, text classification using one-hot-encoded vectors involves preprocessing
    text data, constructing a vocabulary, representing text data as one-hot encoded
    feature vectors, training an ML model on the feature vectors, and evaluating and
    applying the model to new text data. The one-hot encoded vector representation
    is a simple but sometimes limited approach to text classification, and more advanced
    techniques may be necessary for complex tasks.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we’ve learned about classifying documents using N-grams. However, this
    approach has a drawback. There are a considerable number of words that occur in
    the documents frequently and do not add value to our models. To improve the models,
    text classification using TF-IDF has been proposed.
  prefs: []
  type: TYPE_NORMAL
- en: Text classification using TF-IDF
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One-hot encoded vector is a good approach to perform classification. However,
    one of its weaknesses is that it does not consider the importance of different
    words based on different documents. To solve this issue, using **TF-IDF** can
    be helpful.
  prefs: []
  type: TYPE_NORMAL
- en: TF-IDF is a numerical statistic that is used to measure the importance of a
    word in a document within a document collection. It helps reflect the relevance
    of words in a document, considering not only their frequency within the document
    but also their rarity across the entire document collection. The TF-IDF value
    of a word increases proportionally to its frequency in a document but is offset
    by the frequency of the word in the entire document collection.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a detailed explanation of the mathematical equations involved in calculating
    TF-IDF:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Term frequency (TF)**: The TF of a word, *t*, in a document, *d*, represents
    the number of times the word occurs in the document, normalized by the total number
    of words in the document. The TF can be calculated using the following equation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>T</mi><mi>F</mi><mo>(</mo><mi>t</mi><mo>,</mo><mi>d</mi><mo>)</mo><mo>=</mo><mo>(</mo><mi>N</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mi>s</mi><mi>w</mi><mi>o</mi><mi>r</mi><mi>d</mi><mo>′</mo><mi>t</mi><mo>′</mo><mi>a</mi><mi>p</mi><mi>p</mi><mi>e</mi><mi>a</mi><mi>r</mi><mi>s</mi><mi>i</mi><mi>n</mi><mi>d</mi><mi>o</mi><mi>c</mi><mi>u</mi><mi>m</mi><mi>e</mi><mi>n</mi><mi>t</mi><mo>′</mo><mi>d</mi><mo>′</mo><mo>)</mo><mo>/</mo><mo>(</mo><mi>T</mi><mi>o</mi><mi>t</mi><mi>a</mi><mi>l</mi><mi>n</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>w</mi><mi>o</mi><mi>r</mi><mi>d</mi><mi>s</mi><mi>i</mi><mi>n</mi><mi>d</mi><mi>o</mi><mi>c</mi><mi>u</mi><mi>m</mi><mi>e</mi><mi>n</mi><mi>t</mi><mo>′</mo><mi>d</mi><mo>′</mo><mo>)</mo></mrow></mrow></mrow></math>](img/259.png)'
  prefs: []
  type: TYPE_IMG
- en: The TF measures the importance of a word within a specific document.
  prefs: []
  type: TYPE_NORMAL
- en: '**Inverse document frequency (IDF)**: The IDF of a word, *t*, reflects the
    rarity of the word across the entire document collection. IDF can be calculated
    using the following equation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IDF(t) = log ((Total number of documents in the collection) / (Number of documents
    containing word ′t′))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The logarithm is used to dampen the effect of the IDF component. If a word appears
    in many documents, its IDF value will be closer to 0, and if it appears in fewer
    documents, its IDF value will be higher.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**TF-IDF computation**: The TF-IDF value of a word, *t*, in a document, *d*,
    can be calculated by multiplying the TF of the word in the document with the IDF
    of the word across the document collection:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>T</mi><mi>F</mi><mo>−</mo><mi>I</mi><mi>D</mi><mi>F</mi><mo>(</mo><mi>t</mi><mo>,</mo><mi>d</mi><mo>)</mo><mo>=</mo><mi>T</mi><mi>F</mi><mo>(</mo><mi>t</mi><mo>,</mo><mi>d</mi><mo>)</mo><mi
    mathvariant="normal">*</mi><mi>I</mi><mi>D</mi><mi>F</mi><mo>(</mo><mi>t</mi><mo>)</mo></mrow></mrow></mrow></math>](img/260.png)'
  prefs: []
  type: TYPE_IMG
- en: The resulting TF-IDF value represents the importance of a word in a document,
    taking into account both its frequency within the document and its rarity across
    the entire document collection. High TF-IDF values indicate words that are more
    significant in a particular document, whereas low TF-IDF values indicate words
    that are either common across all documents or rare within the specific document.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s consider a simple example of classifying movie reviews into two categories:
    positive and negative. We have a small dataset with three movie reviews and their
    respective labels, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Document 1 (positive)**: “I loved the movie. The acting was great and the
    story was captivating.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Document 2 (negative)**: “The movie was boring. I did not like the story,
    and the acting was terrible.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Document 3 (positive)**: “An amazing movie with a wonderful story and brilliant
    acting.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, we will use TF-IDF to classify a new, unseen movie review:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Document 4 (unknown)**: “The story was interesting, and the acting was good.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are the steps that we need to perform to have the classifier predict the
    class of our document:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1 – preprocess the text data**: Tokenize, lowercase, remove stop words,
    and apply stemming or lemmatization to the words in all documents:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Document 1**: “love movi act great stori captiv”'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Document 2**: “movi bore not like stori act terribl”'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Document 3**: “amaz movi wonder stori brilliant act”'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Document 4**: “stori interest act good”'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Step 2 – create the vocabulary**: Combine all unique words from the preprocessed
    documents:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Vocabulary: {“love”, “movi”, “act”, “great”, “stori”, “captiv”, “bore”, “not”,
    “like”, “terribl”, “amaz”, “wonder”, “brilliant”, “interest”, “good”}'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Step 3 – calculate the TF and IDF values**: Compute the TF and IDF for each
    word in each document.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For example, for the word “stori” in Document 4, we have the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>T</mi><mi>F</mi><mo>(</mo><mi
    mathvariant="normal">"</mi><mi>s</mi><mi>t</mi><mi>o</mi><mi>r</mi><mi>i</mi><mi
    mathvariant="normal">"</mi><mo>,</mo><mi>D</mi><mi>o</mi><mi>c</mi><mi>u</mi><mi>m</mi><mi>e</mi><mi>n</mi><mi>t</mi><mn>4</mn><mo>)</mo><mo>=</mo><mn>1</mn><mo>/</mo><mn>4</mn><mo>=</mo><mn>0.25</mn></mrow></mrow></mrow></math>](img/261.png)'
  prefs: []
  type: TYPE_IMG
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>I</mi><mi>D</mi><mi>F</mi><mo>(</mo><mi
    mathvariant="normal">"</mi><mi>s</mi><mi>t</mi><mi>o</mi><mi>r</mi><mi>i</mi><mi
    mathvariant="normal">"</mi><mo>)</mo><mo>=</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo>(</mo><mn>4</mn><mo>/</mo><mn>3</mn><mo>)</mo><mo>≈</mo><mn>0.287</mn></mrow></mrow></mrow></math>](img/262.png)'
  prefs: []
  type: TYPE_IMG
- en: '4. **Step 4 – compute the TF-IDF values**: Calculate the TF-IDF values for
    each word in each document.'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>T</mi><mi>F</mi><mo>−</mo><mi>I</mi><mi>D</mi><mi>F</mi><mo>(</mo><mi
    mathvariant="normal">"</mi><mi>s</mi><mi>t</mi><mi>o</mi><mi>r</mi><mi>i</mi><mi
    mathvariant="normal">"</mi><mo>,</mo><mi>D</mi><mi>o</mi><mi>c</mi><mi>u</mi><mi>m</mi><mi>e</mi><mi>n</mi><mi>t</mi><mn>4</mn><mo>)</mo><mo>=</mo><mn>0.25</mn><mi
    mathvariant="normal">*</mi><mn>0.287</mn><mo>≈</mo><mn>0.0717</mn></mrow></mrow></mrow></math>](img/263.png)'
  prefs: []
  type: TYPE_IMG
- en: Repeat this process for all words in all documents and create a feature matrix
    with the TF-IDF values.
  prefs: []
  type: TYPE_NORMAL
- en: '5. **Step 5 – train a classifier**: Split the dataset into a training set (documents
    1 to 3) and a test set (document 4). Train a classifier, such as logistic regression
    or SVM, using the training set’s TF-IDF feature matrix and their corresponding
    labels (positive or negative).'
  prefs: []
  type: TYPE_NORMAL
- en: '6. **Step 6 – predict the class label**: Preprocess and compute the TF-IDF
    values for the new movie review (document 4) using the same vocabulary. Use the
    trained classifier to predict the class label for document 4 based on its TF-IDF
    feature vector.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if the classifier predicts a positive label for document 4, the
    classification result would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Document 4 (****Predicted)**: “Positive”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By following these steps, you can use the TF-IDF representation to classify
    text documents based on the importance of words in the documents relative to the
    entire document collection.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, the TF-IDF value is calculated using the mathematical equations
    for TF and IDF. It serves as a measure of the importance of a word in a document
    relative to the entire document collection, considering both the frequency of
    the word within the document and its rarity across all documents.
  prefs: []
  type: TYPE_NORMAL
- en: Text classification using Word2Vec
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the methods to perform text classification is to convert the words into
    embedding vectors so that you can use those vectors for classification. Word2Vec
    is a well-known method to perform this task.
  prefs: []
  type: TYPE_NORMAL
- en: Word2Vec
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Word2Vec is a group of neural network-based models that are used to create
    word embeddings, which are dense vector representations of words in a continuous
    vector space. These embeddings capture the semantic meaning and relationships
    between words based on the context in which they appear in the text. Word2Vec
    has two main architectures. As mentioned previously, the two main architectures
    that were designed to learn word embeddings are **CBOW** and **skip-gram**. Both
    architectures are designed to learn word embeddings by predicting words based
    on their surrounding context:'
  prefs: []
  type: TYPE_NORMAL
- en: '**CBOW**: The CBOW architecture aims to predict the target word given its surrounding
    context words. It takes the average of the context word embeddings as input and
    predicts the target word. CBOW is faster to train and works well with smaller
    datasets but may be less accurate for infrequent words.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the CBOW model, the objective is to maximize the average log probability
    of observing the target word given the context words:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><msub><mrow><mi>O</mi><mi>b</mi><mi>j</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi></mrow><mrow><mi>C</mi><mi>B</mi><mi>o</mi><mi>w</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mi>T</mi></mfrac><mrow><munder><mo>∑</mo><mrow><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi></mrow></munder><mrow><mi>l</mi><mi>o</mi><mi>g</mi></mrow></mrow><mo>(</mo><mi>P</mi><mo>(</mo><mi>t</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>t</mi><mo>|</mo><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi><mo>)</mo><mo>)</mo></mrow></mrow></mrow></math>](img/264.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, T is the total number of words in the text, and P(target | context) is
    the probability of observing the target word given the context words, which is
    calculated using the softmax function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mi>t</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>t</mi><mo>|</mo><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi></mrow></mfenced><mo>=</mo><mfrac><msup><mi>e</mi><mrow><msubsup><mi
    mathvariant="bold">v</mi><mrow><mi>t</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>t</mi></mrow><mi>T</mi></msubsup><msub><mrow><mo>∙</mo><mi
    mathvariant="bold">v</mi></mrow><mrow><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi></mrow></msub></mrow></msup><mrow><msub><mo>∑</mo><mi>i</mi></msub><msup><mi>e</mi><mrow><msubsup><mi
    mathvariant="bold">v</mi><mi>i</mi><mi>T</mi></msubsup><msub><mrow><mo>∙</mo><mi
    mathvariant="bold">v</mi></mrow><mrow><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi></mrow></msub></mrow></msup></mrow></mfrac></mrow></mrow></math>](img/265.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/266.png)
    is the output vector (word embedding) of the target word, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/267.png)
    is the average input vector (context word embedding) of the context words, and
    the sum in the denominator runs over all words in the vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: '**Skip-gram**: The skip-gram architecture aims to predict the surrounding context
    words given the target word. It takes the target word embedding as input and predicts
    the context words. Skip-gram works well with larger datasets and can capture the
    meaning of infrequent words more accurately, but it may be slower to train compared
    to CBOW.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the skip-gram model, the objective is to maximize the average log probability
    of observing the context words given the target word:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>k</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mo>-</mml:mo><mml:mi>G</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:munder><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi
    mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi
    mathvariant="normal">g</mml:mi><mml:mo>⁡</mml:mo><mml:mo>(</mml:mo><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math>](img/268.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, T is the total number of words in the text, and P(context | target) is
    the probability of observing the context words given the target word, which is
    calculated using the softmax function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi><mo>|</mo><mi>t</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>t</mi></mrow></mfenced><mo>=</mo><mfrac><msup><mi>e</mi><mrow><msubsup><mi
    mathvariant="bold">v</mi><mrow><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi></mrow><mi>T</mi></msubsup><msub><mrow><mo>∙</mo><mi
    mathvariant="bold">v</mi></mrow><mrow><mi>t</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>t</mi></mrow></msub></mrow></msup><mrow><msub><mo>∑</mo><mi>i</mi></msub><msup><mi>e</mi><mrow><msubsup><mi
    mathvariant="bold">v</mi><mi>i</mi><mi>T</mi></msubsup><msub><mrow><mo>∙</mo><mi
    mathvariant="bold">v</mi></mrow><mrow><mi>t</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>t</mi></mrow></msub></mrow></msup></mrow></mfrac></mrow></mrow></math>](img/269.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/270.png)is
    the output vector (context word embedding) of the context word, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/271.png)is
    the input vector (word embedding) of the target word, and the sum in the denominator
    runs over all words in the vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: The training process for both CBOW and skip-gram involves iterating through
    the text and updating the input and output weight matrices using **stochastic
    gradient descent** (**SGD**) and backpropagation to minimize the difference between
    the predicted words and the actual words. The learned input weight matrix contains
    the word embeddings for each word in the vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: Text classification using Word2Vec
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Text classification using Word2Vec involves creating word embeddings using
    the Word2Vec algorithm and then training an ML model to classify text based on
    these embeddings. The following steps outline the process in detail, including
    the mathematical aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Text preprocessing**: Clean and preprocess the text data by tokenizing, lowercasing,
    removing stop words, and stemming or lemmatizing the words.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Train the Word2Vec model**: Train a Word2Vec model (either CBOW or Skip-Gram)
    on the preprocessed text data to create word embeddings. The Word2Vec algorithm
    learns to predict a target word based on its context (CBOW) or predict the context
    words based on a target word (skip-gram). The training objective is to maximize
    the average log probability of observing the context words given the target word:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>O</mi><mi>b</mi><mi>j</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mo>=</mo><mfrac><mn>1</mn><mi>T</mi></mfrac><mrow><munder><mo>∑</mo><mrow><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi></mrow></munder><mrow><mi
    mathvariant="normal">l</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">g</mi><mo>(</mo><mi>P</mi><mfenced
    open="(" close=")"><mrow><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi><mo>|</mo><mi>t</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>t</mi></mrow></mfenced><mo>)</mo></mrow></mrow></mrow></mrow></math>](img/272.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *T* is the total number of words in the text, and *P(context | target)*
    is the probability of observing the context words given the target word, which
    is calculated using the softmax function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi><mo>|</mo><mi>t</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>t</mi></mrow></mfenced><mo>=</mo><mfrac><msup><mi>e</mi><mrow><msubsup><mi
    mathvariant="bold">v</mi><mrow><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi></mrow><mi>T</mi></msubsup><msub><mrow><mo>∙</mo><mi
    mathvariant="bold">v</mi></mrow><mrow><mi>t</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>t</mi></mrow></msub></mrow></msup><mrow><msub><mo>∑</mo><mi>i</mi></msub><msup><mi>e</mi><mrow><msubsup><mi
    mathvariant="bold">v</mi><mi>i</mi><mi>T</mi></msubsup><msub><mrow><mo>∙</mo><mi
    mathvariant="bold">v</mi></mrow><mrow><mi>t</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>t</mi></mrow></msub></mrow></msup></mrow></mfrac></mrow></mrow></math>](img/273.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/274.png)is
    the output vector (context word embedding) of the context word, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/275.png)
    is the input vector (word embedding) of the target word, and the sum in the denominator
    runs over all words in the vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: '3. **Create document embeddings**: For each document in the dataset, calculate
    the document embedding by averaging the word embeddings of the words in the document:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>D</mi><mi>o</mi><mi>c</mi><mi>u</mi><mi>m</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>E</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>d</mi><mi>d</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><mrow><munder><mo>∑</mo><mi>i</mi></munder><msub><mrow><mi
    mathvariant="normal">W</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">r</mi><mi
    mathvariant="normal">d</mi><mi mathvariant="normal">E</mi><mi mathvariant="normal">m</mi><mi
    mathvariant="normal">b</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">d</mi><mi
    mathvariant="normal">d</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi><mi
    mathvariant="normal">g</mi></mrow><mi mathvariant="normal">i</mi></msub></mrow></mrow></mrow></math>](img/276.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, N is the number of words in the document, and the sum runs over all words
    in the document. Please note that based on our experience, this approach for text
    classification using Word2Vec is only useful when the document’s length is short.
    If you have longer documents or there are opposite words in the document, this
    approach won’t perform well. An alternative solution is to use Word2Vec and CNN
    together to fetch the word embeddings and then feed those embeddings as input
    of the CNN.
  prefs: []
  type: TYPE_NORMAL
- en: '4. **Model training**: Use the document embeddings as features to train an
    ML model, such as logistic regression, SVM, or a neural network, for text classification.
    The model learns to predict the class label based on the document embeddings.'
  prefs: []
  type: TYPE_NORMAL
- en: '5. **Model evaluation**: Evaluate the performance of the model using appropriate
    evaluation metrics, such as accuracy, precision, recall, F1 score, or confusion
    matrix, and use techniques such as cross-validation to get a reliable estimate
    of the model’s performance on unseen data.'
  prefs: []
  type: TYPE_NORMAL
- en: '6. **Model application**: Apply the trained model to new, unseen text data.
    Preprocess and compute the document embeddings for the new text data using the
    same Word2Vec model and vocabulary, and use the model to predict the class labels.'
  prefs: []
  type: TYPE_NORMAL
- en: In summary, text classification using Word2Vec involves creating word embeddings
    with the Word2Vec algorithm, averaging these embeddings to create document embeddings,
    and training an ML model to classify text based on these document embeddings.
    The Word2Vec algorithm learns word embeddings by maximizing the average log probability
    of observing context words given a target word, capturing the semantic relationships
    between words in the process.
  prefs: []
  type: TYPE_NORMAL
- en: Model evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Evaluating the performance of text classification models is crucial to ensure
    that they meet the desired level of accuracy and generalizability. Several metrics
    and techniques are commonly used to evaluate text classification models, including
    accuracy, precision, recall, F1 score, and confusion matrix. Let’s discuss each
    of these in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Accuracy**: Accuracy is the most straightforward metric for classification
    tasks. It measures the number of correctly classified records out of all classified
    records. It is defined as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>A</mi><mi>c</mi><mi>c</mi><mi>u</mi><mi>r</mi><mi>a</mi><mi>c</mi><mi>y</mi><mo>=</mo><mfrac><mrow><mo>(</mo><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mi>P</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>s</mi><mo>+</mo><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mi>N</mi><mi>e</mi><mi>g</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>s</mi><mo>)</mo></mrow><mrow><mo>(</mo><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mi>P</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>s</mi><mo>+</mo><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mi>N</mi><mi>e</mi><mi>g</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>s</mi><mo>+</mo><mi>F</mi><mi>a</mi><mi>l</mi><mi>s</mi><mi>e</mi><mi>P</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>s</mi><mo>+</mo><mi>F</mi><mi>a</mi><mi>l</mi><mi>s</mi><mi>e</mi><mi>N</mi><mi>e</mi><mi>g</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>s</mi><mo>)</mo></mrow></mfrac></mrow></mrow></math>](img/277.png)'
  prefs: []
  type: TYPE_IMG
- en: While accuracy is easy to understand, it may not be the best metric for imbalanced
    datasets, where the majority class can dominate the metric’s value.
  prefs: []
  type: TYPE_NORMAL
- en: '**Precision**: Precision gauges the ratio of correctly identified positive
    instances to the total instances predicted as positive by the model. It is also
    referred to as **positive predictive value** (**PPV**). Precision proves valuable
    in scenarios where the expense associated with false positives is significant.
    Precision is defined as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[[OMML-EQ-21D]]'
  prefs: []
  type: TYPE_NORMAL
- en: '**Recall**: Recall, also recognized as sensitivity or the **true positive rate**
    (**TPR**), assesses the ratio of correctly identified positive instances among
    the total actual positive instances. Recall is useful when the cost of false negatives
    is high. Mathematically, it is defined as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[[OMML-EQ-22D]]'
  prefs: []
  type: TYPE_NORMAL
- en: '**F1 score**: The F1 score, derived as the harmonic mean of precision and recall,
    integrates both metrics into a unified value. It is an important metric in the
    context of imbalanced datasets as it considers both false positives and false
    negatives. Spanning from 0 to 1, with 1 representing the optimal outcome, the
    F1 score is mathematically expressed as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>F</mi><mn>1</mn><mi>S</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mo>=</mo><mn>2</mn><mfrac><mrow><mi>P</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>∙</mo><mi>R</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi></mrow><mrow><mi>P</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>+</mo><mi>R</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi></mrow></mfrac></mrow></mrow></math>](img/278.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When dealing with multi-class classification, we have F1 micro and F1 macro.
    F1 micro and F1 macro are two ways to compute the F1 score for multi-class or
    multi-label classification problems. They aggregate precision and recall differently,
    leading to different interpretations of the classifier’s performance. Let’s discuss
    each in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '**F1 macro**: F1 macro computes the F1 score for each class independently and
    then takes the average of those values. This approach treats each class as equally
    important and does not consider the class imbalance. Mathematically, F1 macro
    is defined as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mrow><mi>F</mi><mn>1</mn></mrow><mrow><mi>M</mi><mi>a</mi><mi>c</mi><mi>r</mi><mi>o</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mrow><munder><mo>∑</mo><mi>i</mi></munder><msub><mrow><mi>F</mi><mn>1</mn></mrow><mi>i</mi></msub></mrow></mrow></mrow></math>](img/279.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *n* is the number of classes, and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/280.png)
    is the F1 score for the i-th class.
  prefs: []
  type: TYPE_NORMAL
- en: F1 macro is particularly useful when you want to evaluate the performance of
    a classifier across all classes without giving more weight to the majority class.
    However, it may not be suitable when the class distribution is highly imbalanced
    as it can provide an overly optimistic estimate of the model’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: '**F1 micro**: F1 micro, on the other hand, aggregates the contributions of
    all classes to compute the F1 score. It does this by calculating the global precision
    and recall values across all classes and then computing the F1 score based on
    these global values. F1 micro takes class imbalance into account as it considers
    the number of instances in each class. Mathematically, F1 micro is defined as
    follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mrow><mi>F</mi><mn>1</mn></mrow><mrow><mi>M</mi><mi>i</mi><mi>c</mi><mi>r</mi><mi>o</mi></mrow></msub><mo>=</mo><mn>2</mn><mfrac><mrow><mi>G</mi><mi>l</mi><mi>o</mi><mi>b</mi><mi>a</mi><mi>l</mi><mi>P</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>∙</mo><mi>G</mi><mi>l</mi><mi>o</mi><mi>b</mi><mi>a</mi><mi>l</mi><mi>R</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi></mrow><mrow><mi>G</mi><mi>l</mi><mi>o</mi><mi>b</mi><mi>a</mi><mi>l</mi><mi>P</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>+</mo><mi>G</mi><mi>l</mi><mi>o</mi><mi>b</mi><mi>a</mi><mi>l</mi><mi>R</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi></mrow></mfrac></mrow></mrow></math>](img/281.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Here, global precision and global recall are calculated as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>G</mi><mi>l</mi><mi>o</mi><mi>b</mi><mi>a</mi><mi>l</mi><mi>P</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>=</mo><mfrac><mrow><mo>∑</mo><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mi>P</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>s</mi></mrow><mrow><mo>∑</mo><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mi>P</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>s</mi><mo>+</mo><mo>∑</mo><mi>F</mi><mi>a</mi><mi>l</mi><mi>s</mi><mi>e</mi><mi>P</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>s</mi></mrow></mfrac></mrow></mrow></math>](img/282.png)'
  prefs: []
  type: TYPE_IMG
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>G</mi><mi>l</mi><mi>o</mi><mi>b</mi><mi>a</mi><mi>l</mi><mi>R</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi><mo>=</mo><mfrac><mrow><mo>∑</mo><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mi>P</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>s</mi></mrow><mrow><mo>∑</mo><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mi>P</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>s</mi><mo>+</mo><mo>∑</mo><mi>F</mi><mi>a</mi><mi>l</mi><mi>s</mi><mi>e</mi><mi>N</mi><mi>e</mi><mi>g</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>s</mi></mrow></mfrac></mrow></mrow></math>](img/283.png)'
  prefs: []
  type: TYPE_IMG
- en: F1 micro is useful when you want to evaluate the overall performance of a classifier
    considering the class distribution, especially when dealing with imbalanced datasets.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, F1 macro and F1 micro are two ways to compute the F1 score for multi-class
    or multi-label classification problems. F1 macro treats each class as equally
    important, regardless of the class distribution, while F1 micro takes class imbalance
    into account by considering the number of instances in each class. The choice
    between F1 macro and F1 micro depends on the specific problem and whether class
    imbalance is an important factor to consider.
  prefs: []
  type: TYPE_NORMAL
- en: Confusion matrix
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A confusion matrix serves as a tabular representation, showcasing the count
    of true positive, true negative, false positive, and false negative predictions
    made by a classification model. This matrix offers a nuanced perspective on the
    model’s efficacy, enabling a thorough comprehension of both its strengths and
    weaknesses.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a binary classification problem, the confusion matrix is arranged as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Actual/Predicted** | **(****Predicted) Positive** | **(****Predicted) Negative**
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| (Actual) Positive | True Positive | False Negative |'
  prefs: []
  type: TYPE_TB
- en: '| (Actual) Negative | False Positive | True Negative |'
  prefs: []
  type: TYPE_TB
- en: Table 5.2 – Confusion matrix – general view
  prefs: []
  type: TYPE_NORMAL
- en: For multi-class classification problems, the confusion matrix is extended to
    include the true and predicted counts for each class. The diagonal elements represent
    the correctly classified instances, while the off-diagonal elements represent
    misclassifications.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, evaluating text classification models involves using various metrics
    and techniques, such as accuracy, precision, recall, F1 score, and the confusion
    matrix. Selecting the appropriate evaluation metrics depends on the specific problem,
    dataset characteristics, and the trade-offs between false positives and false
    negatives. Evaluating a model using multiple metrics can provide a more comprehensive
    understanding of its performance and help guide further improvements.
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting and underfitting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Overfitting and underfitting are two common issues that arise during the training
    of ML models, including text classification models. They both relate to how well
    a model generalizes to new, unseen data. This section will explain overfitting
    and underfitting, when they happen, and how to prevent them.
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Overfitting arises when a model excessively tailors itself to the intricacies
    of the training data. In this case, the model captures noise and random fluctuations
    rather than discerning the fundamental patterns. Consequently, although the model
    may exhibit high performance on the training data, its effectiveness diminishes
    when applied to unseen data, such as a validation or test set.
  prefs: []
  type: TYPE_NORMAL
- en: 'To avoid overfitting in text classification, consider the following strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Regularization**: Introduce regularization techniques, such as ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="script">l</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/284.png)
    or ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi mathvariant="script">l</mi><mn>2</mn></msub></mrow></math>](img/285.png)L2
    regularization, which add a penalty to the loss function, discouraging overly
    complex models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Early stopping**: In this approach, we monitor the performance of the model
    on the validation set, and stop the training process as soon as the performance
    on the validation set starts getting worse, even though the model performance
    on the training set is getting better. It helps us to prevent overfitting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature selection**: Reduce the number of features used for classification
    by selecting the most informative features or using dimensionality reduction techniques
    such as PCA or LSA.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ensemble methods**: Combine multiple models, such as bagging or boosting,
    to reduce overfitting by averaging their predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cross-validation**: Use k-fold cross-validation to get a more reliable estimate
    of model performance on unseen data and fine-tune model hyperparameters accordingly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we’ll cover underfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Underfitting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Underfitting happens when a model is too simple and fails to capture the underlying
    patterns in the data. Consequently, the model performance is low on both training
    and test data. The model is too simple to represent the complexity of the data
    and can’t generalize well.
  prefs: []
  type: TYPE_NORMAL
- en: 'To avoid underfitting in text classification, consider the following strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Increase model complexity**: Use a more complex model, such as a deeper neural
    network, to capture more intricate patterns in the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature engineering**: Create new, informative features that help the model
    better understand the underlying patterns in the text data, such as adding N-grams
    or using word embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hyperparameter tuning**: Optimize model hyperparameters, such as the learning
    rate, number of layers, or number of hidden units, to improve the model’s ability
    to learn from the data. We’ll explain hyperparameter tuning and the different
    methods to perform this task in the next section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Increase training data**: If possible, collect more labeled data for training,
    as more examples can help the model learn the underlying patterns better.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reduce regularization**: If the model is heavily regularized, consider reducing
    the regularization strength, allowing the model to become more complex and better
    fit the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In summary, overfitting and underfitting are two common issues in text classification
    that affect a model’s ability to generalize to new data. Avoiding these issues
    involves balancing model complexity, using appropriate features, tuning hyperparameters,
    employing regularization, and monitoring model performance on a validation set.
    By addressing overfitting and underfitting, you can improve the performance and
    generalizability of your text classification models.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An important step in building an effective classification model is hyperparameter
    tuning. Hyperparameters are the model parameters that are defined before training;
    they will not change during training. These parameters determine the model architecture
    and behavior. Some of the hyperparameters that can be used are the learning rate
    and the number of iterations. They can significantly impact the model’s performance
    and generalizability.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process of hyperparameter tuning in text classification involves the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Define the hyperparameters and their search space**: Identify the hyperparameters
    you want to optimize and specify the range of possible values for each of them.
    Common hyperparameters in text classification include the learning rate, number
    of layers, number of hidden units, dropout rate, regularization strength, and
    feature extraction parameters such as N-grams or vocabulary size.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Choose a search strategy**: Select a method to explore the hyperparameter
    search space, such as grid search, random search, or Bayesian optimization. Grid
    search systematically evaluates all combinations of hyperparameter values, while
    random search samples random combinations within the search space. Bayesian optimization
    uses a probabilistic model to guide the search, balancing exploration and exploitation
    based on the model’s predictions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Choose an evaluation metric and method**: Select a performance metric that
    best represents the goals of your text classification task, such as accuracy,
    precision, recall, F1 score, or area under the ROC curve. Also, choose an evaluation
    method, such as k-fold cross-validation, to get a reliable estimate of the model’s
    performance on unseen data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Perform the search**: For each combination of hyperparameter values, train
    a model on the training data, and evaluate its performance using the chosen metric
    and evaluation method. Keep track of the best-performing hyperparameter combination.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Select the best hyperparameters**: After the search is complete, select the
    hyperparameter combination that yields the best performance on the evaluation
    metric. Retrain the model using these hyperparameters on the entire training set.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Evaluate on the test set**: Assess the performance of the final model with
    the optimized hyperparameters on a held-out test set to get an unbiased estimate
    of its generalizability.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hyperparameter tuning affects the performance of the model by finding the optimal
    combination of parameters that results in the best model performance on the chosen
    evaluation metric. Tuning hyperparameters can help address issues such as overfitting
    and underfitting, balance model complexity, and improve the model’s ability to
    generalize to new data.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter tuning is a crucial process in text classification that involves
    searching for the optimal combination of model parameters to maximize performance
    on a chosen evaluation metric. By carefully tuning hyperparameters, you can improve
    the performance and generalizability of your text classification models.
  prefs: []
  type: TYPE_NORMAL
- en: Additional topics in applied text classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the real world, applying text classification involves various practical considerations
    and challenges that arise from the nature of real-world data and problem requirements.
    Some common issues include dealing with imbalanced datasets, handling noisy data,
    and choosing appropriate evaluation metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s discuss each of these in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with imbalanced datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Text classification tasks often encounter imbalanced datasets, wherein certain
    classes boast a notably higher number of instances compared to others. This imbalance
    can result in models that are skewed, excelling in predicting the majority class
    while faltering in accurately classifying the minority class. To handle imbalanced
    datasets, consider the following strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Resampling**: You can oversample the minority class, undersample the majority
    class, or use a combination of both to balance the class distribution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weighted loss function**: Assign higher weights to the minority class in
    the loss function, making the model more sensitive to misclassifications in the
    minority class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ensemble methods**: Use ensemble techniques such as bagging or boosting with
    a focus on the minority class. For example, you can use random under-sampling
    with bagging or cost-sensitive boosting algorithms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evaluation metrics**: Choose evaluation metrics that are less sensitive to
    class imbalance, such as precision, recall, F1 score, or area under the ROC curve,
    instead of accuracy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Handling noisy data**: Real-world text data is often noisy, containing misspellings,
    grammatical errors, or irrelevant information. Noisy data can negatively impact
    the performance of text classification models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To handle noisy data, consider the following strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Preprocessing**: Clean the text data by correcting misspellings, removing
    special characters, expanding contractions, and converting text into lowercase'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stopword removal**: Remove common words that do not carry much meaning, such
    as “the,” “is,” “and,” and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stemming or lemmatization**: Reduce words to their root form to minimize
    the impact of morphological variations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature selection**: Use techniques such as chi-square or mutual information
    to select the most informative features, reducing the impact of noisy or irrelevant
    features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether we’re working on imbalanced data or not, we always need to evaluate
    our model, and choosing the right metric to evaluate our model is important. Next,
    we’ll explain how to select the best metric to evaluate our model.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing appropriate evaluation metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Selecting the right evaluation metrics is crucial for measuring the performance
    of your text classification model and guiding model improvements.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following when choosing evaluation metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Problem requirements**: Choose metrics that align with the specific goals
    of your text classification task, such as minimizing false positives or false
    negatives'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Class imbalance**: For imbalanced datasets, use metrics that account for
    class imbalance, such as precision, recall, F1 score, or area under the ROC curve,
    instead of accuracy'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-class or multi-label problems**: For multi-class or multi-label classification
    tasks, use metrics such as micro- and macro-averaged F1 scores, which aggregate
    precision and recall differently based on the problem’s requirements'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In summary, practical considerations in text classification include dealing
    with imbalanced datasets, handling noisy data, and choosing appropriate evaluation
    metrics. Addressing these issues can help improve the performance and generalizability
    of your text classification models and ensure that they meet the specific requirements
    of your problem.
  prefs: []
  type: TYPE_NORMAL
- en: Topic modeling – a particular use case of unsupervised text classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Topic modeling is an unsupervised ML technique that’s used to discover abstract
    topics or themes within a large collection of documents. It assumes that each
    document can be represented as a mixture of topics, and each topic is represented
    as a distribution over words. The goal of topic modeling is to find the underlying
    topics and their word distributions, as well as the topic proportions for each
    document.
  prefs: []
  type: TYPE_NORMAL
- en: There are several topic modeling algorithms, but one of the most popular and
    widely used is LDA. We will discuss LDA in detail, including its mathematical
    formulation.
  prefs: []
  type: TYPE_NORMAL
- en: LDA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'LDA is a generative probabilistic model that assumes the following generative
    process for each document:'
  prefs: []
  type: TYPE_NORMAL
- en: Choose the number of words in the document.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose a topic distribution (*θ*) for the document from a Dirichlet distribution
    with parameter α.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each word in the document, do the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose a topic (*z*) from the topic distribution (*θ*).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose a word (*w*) from the word distribution of the chosen topic (*φ*), which
    is a distribution over words for that topic, drawn from a Dirichlet distribution
    with parameter *β*.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The generative process is a theoretical model used by LDA to reverse-engineer
    the original documents from presumed topics.
  prefs: []
  type: TYPE_NORMAL
- en: LDA aims to find the topic-word distributions (*φ*) and document-topic distributions
    (*θ*) that best explain the observed documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, LDA can be described using the following notation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*M*: Number of documents'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*N*: Number of words in a document'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*K*: Number of topics'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*α*: Dirichlet before document-topic distribution, it affects the sparsity
    of topics within documents'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*β*: Dirichlet before topic-word distribution, it affects the sparsity of words
    within topics'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*θ*: Document-topic distributions (M × K matrix)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*φ*: Topic-word distributions (K × V matrix, where V is the vocabulary size)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*z*: Topic assignments for each word in each document (M × N matrix)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*w*: Observed words in the documents (M × N matrix)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The joint probability of the topic assignments (*z*) and words (*w*) in the
    documents, given the topic-word distributions (*φ*) and document-topic distributions
    (*θ*), can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mi>z</mi><mo>,</mo><mi>w</mi><mo>|</mo><mi>θ</mi><mo>,</mo><mi>φ</mi></mrow></mfenced><mo>=</mo><mrow><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover><mrow><munderover><mo>∏</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>|</mo><mi>φ</mi><mo>,</mo><msub><mi>z</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></mfenced><mi>P</mi><mfenced
    open="(" close=")"><mrow><msub><mi>z</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>|</mo><msub><mi>θ</mi><mi>i</mi></msub></mrow></mfenced></mrow></mrow></mrow></mrow></mrow></math>](img/286.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The objective of LDA is to maximize the likelihood of the observed words given
    the Dirichlet priors α and β:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>β</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:mo
    stretchy="false">∫</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">∫</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>φ</mml:mi></mml:mrow></mml:mfenced><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mfenced><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>φ</mml:mi></mml:mrow><mml:mrow><mml:mi>β</mml:mi></mml:mrow></mml:mfenced><mml:mi>d</mml:mi><mml:mi>θ</mml:mi><mml:mi>d</mml:mi><mml:mi>φ</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math>](img/287.png)'
  prefs: []
  type: TYPE_IMG
- en: However, computing the likelihood directly is intractable due to the integration
    over the latent variables θ and φ. Therefore, LDA uses approximate inference algorithms,
    such as Gibbs sampling or variational inference, to estimate the posterior distributions
    *P*(θ | w, α, β) and *P*(φ | w, α, β).
  prefs: []
  type: TYPE_NORMAL
- en: Once the posterior distributions have been estimated, we can obtain the document-topic
    distributions (θ) and topic-word distributions (φ), which can be used to analyze
    the discovered topics and their word distributions, as well as the topic proportions
    for each document.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider a simple example of topic modeling.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we have a collection of three documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Document 1**: “I love playing football with my friends.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Document 2**: “The football match was intense and exciting.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Document 3**: “My new laptop has an amazing battery life and performance.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We want to discover two topics (K = 2) in this document collection. Here are
    the steps that we need to perform:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Preprocessing**: First, we need to preprocess the text data, which typically
    involves tokenization, stopword removal, and stemming/lemmatization (which was
    explained previously in this chapter). In this example, we will skip these steps
    for simplicity and assume our documents are already preprocessed.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Initialization**: Choose the initial values for the Dirichlet priors, α and
    β. For example, we can set α = [1, 1] and β = [0.1, 0.1, ..., 0.1] (assuming a
    V-dimensional vector with 0.1 for each word in the vocabulary).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Random topic assignments**: Randomly assign a topic (1 or 2) to each word
    in each document.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Iterative inference (for example, Gibbs sampling or variational inference)**:
    Iteratively update the topic assignments and the topic-word and document-topic
    distributions (φ and θ) until convergence or a fixed number of iterations. This
    process refines the assignments and distributions, ultimately revealing the underlying
    topic structure.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Interpretation**: After the algorithm converges or reaches the maximum number
    of iterations, we can interpret the discovered topics by looking at the most probable
    words for each topic and the most probable topics for each document.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For our example, LDA might discover the following topics:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Topic 1**: {“football”, “playing”, “friends”, “match”, “intense”, “exciting”}'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Topic 2**: {“laptop”, “battery”, “life”, “performance”}'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With these topics, the document-topic distribution (θ) might look like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/288.png)
    = [0.9, 0.1] (Document 1 is 90% about Topic 1 and 10% about Topic 2)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_IMG
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/289.png)
    = [0.8, 0.2] (Document 2 is 80% about Topic 1 and 20% about Topic 2)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_IMG
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math>](img/290.png)
    = [0.1, 0.9] (Document 3 is 10% about Topic 1 and 90% about Topic 2)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_IMG
- en: In this example, topic 1 seems to be related to football and sports, while topic
    2 seems to be related to technology and gadgets. The topic distributions for each
    document show that documents 1 and 2 are mostly about football, while document
    3 is about technology.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that this is a simplified example, and real-world data would require
    more sophisticated preprocessing and a larger number of iterations for convergence.
  prefs: []
  type: TYPE_NORMAL
- en: We are now ready to discuss the paradigm for putting together a complete project
    in a work or research setting.
  prefs: []
  type: TYPE_NORMAL
- en: Real-world ML system design for NLP text classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section is dedicated to the practical implementation of the various methods
    we discussed. It will revolve around Python code, which serves as a complete pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'To provide a comprehensive learning experience, we will discuss the entire
    journey of a typical ML project. *Figure 5**.1* depicts the different phases of
    the ML project:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 – The paradigm of a typical ML project](img/B18949_05_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 – The paradigm of a typical ML project
  prefs: []
  type: TYPE_NORMAL
- en: Let’s break the problem down in a similar fashion to a typical project in the
    industry.
  prefs: []
  type: TYPE_NORMAL
- en: The business objective
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An ML project, whether in a business or research setting, stems from an original
    objective, which is often qualitative rather than technical.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: “We need to know which of our patients is at a higher risk.”
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: “We would like to maximize the engagement of our ad.”
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: “We need the autonomous car to be alerted when a person is stepping in front
    of it.”
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next comes the technical objective.
  prefs: []
  type: TYPE_NORMAL
- en: The technical objective
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The original objective needs to be translated into a technical objective, like
    so:'
  prefs: []
  type: TYPE_NORMAL
- en: “We will process every patient’s medical record and build a risk estimator based
    on the history of realized risk.”
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: “We will collect data about all the ads from the last year and will build a
    regressor to estimate the level of engagement based on the ad’s features.”
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: “We will collect a set of images taken by the car’s front camera and present
    those to our online users who are visiting our site, telling them it’s for security
    reasons and that they need to click on the parts that show a human to prove they
    are not robots. However, in practice, we’ll collect their free labels for training,
    develop a computer vision classifier for humans, and won’t give them any credit.”
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While the original business or research objective is somewhat of an open-ended
    question, the technical objective reflects an actionable plan. Note, however,
    that any given technical objective represents just one among several potential
    solutions aligned with the original business or research aim. It is the responsibility
    of the technical authority, such as the CTO, ML manager, or senior developer,
    to understand the original objective and translate it into a technical objective.
    Moreover, it may be that the technical objective would be refined or even replaced
    down the line. The next step after forming a technical objective is to form a
    plan for it.
  prefs: []
  type: TYPE_NORMAL
- en: Tentative high-level system design
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To realize the technical objective, we need to derive a plan to decide which
    data would be used to feed into the ML system, and what the expected output of
    the ML system is. In the first steps of a project, there may be several candidate
    sources of potential data that are believed to be indicative of the desired output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following the set of three examples mentioned previously, here are some examples
    of data descriptions:'
  prefs: []
  type: TYPE_NORMAL
- en: The input data would be columns A, B, and C of the **patient_records** SQL table
    and the risk would be assessed as *1/N*, where *N* is the number of days that
    passed from a given moment until the patient showed up in the emergency room.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The input data would be the geometric and color descriptions of the ads, and
    the level of engagement would be the number of clicks per day that the ad received.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The input data is the images of the car’s front camera to be fed to a computer
    vision neural network classifier, and the output data would be whether the image
    captures a person or not.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choosing a metric
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When defining a potential solution approach, extra attention should be dedicated
    to identifying the best metric to focus on, also known as the objective function
    or error function. This is the metric by which the success of the solution will
    be evaluated. It is important to relate the metric to the original business or
    research objective.
  prefs: []
  type: TYPE_NORMAL
- en: 'As per the previous examples, we could have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Minimize the 70th percentile confidence interval.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Minimize the mean absolute error.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Maximize precision while constraining on a fixed recall. This fixed recall will
    ideally be dictated by business leaders or the legal team, in the form of “the
    system must capture at least 99.9% of the cases where a person steps in front
    of a car.”
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we have a tentative plan, we can explore the data and evaluate the
    feasibility of the design.
  prefs: []
  type: TYPE_NORMAL
- en: Exploration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Exploration is divided into two parts – exploring the data and exploring the
    feasibility of the design. Let’s take a closer look.
  prefs: []
  type: TYPE_NORMAL
- en: Data exploration
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Data is not always perfect for our objective. We discussed some of the data
    shortcomings in previous chapters. In particular, free text is often notorious
    for having many abnormal phenomena, such as encodings, special characters, typos,
    and so on. When exploring our data, we want to uncover all these phenomena and
    make sure that the data can be brought to a form that serves the objective.
  prefs: []
  type: TYPE_NORMAL
- en: Feasibility study
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Here, we want to prospectively identify proxies for whether the planned design
    is expected to succeed. While with some problems there are known proxies for expected
    success, in most problems in the business and especially research setting, it
    takes much experience and ingenuity to suggest preliminary proxies for success.
  prefs: []
  type: TYPE_NORMAL
- en: An example of a very simple case is a simple regression problem with a single
    input variable and a single output variable. Let’s say the independent variable
    is the number of active viewers that your streaming service currently has, and
    the dependent variable is the risk that the company’s servers have for maxing
    out their capacity. The tentative design plan would be to build a regressor that
    estimates the risk at any given moment. A strong proxy for the feasibility of
    developing a successful regressor could be calculating the linear correlation
    between the historical data points. Calculating linear correlation based on sample
    data is easy and quick and if its result is close to 1 (or -1 in cases different
    than our business problem), then it means that a linear regressor is guaranteed
    to succeed, thus, making it a great proxy. However, note that if the linear correlation
    is close to 0, it doesn’t necessarily mean that a regressor would fail, only that
    a linear regression would fail. In such a case, a different proxy should be deferred
    to.
  prefs: []
  type: TYPE_NORMAL
- en: In the *Reviewing our use case – ML system design for NLP classification in
    a Jupyter Notebook* section, we’ll review our code solution. We’ll also present
    a method to assess the feasibility of a text classifier. The method aims to mimic
    a relationship between the input text to the output class. But since we want to
    have that method suit a variable that is text and not numeric, we’ll go back to
    the origin and calculate a measure for the statistical dependency between the
    input text and the output class. Statistical dependency is the most basic measure
    for a relationship between variables and thus doesn’t require either of them to
    be numeric.
  prefs: []
  type: TYPE_NORMAL
- en: Assuming the **feasibility study** is successful, we can move on to implementing
    the ML solution.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing an ML solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This part is where the expertise of the ML developer comes into play. There
    are different steps for it and the developer chooses which ones are relevant based
    on the problem – whether it’s data cleaning, text segmentation, feature design,
    model comparison, or metric choice.
  prefs: []
  type: TYPE_NORMAL
- en: We will elaborate on this as we review the specific use case we’ve solved.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We evaluate the solution given the metric that was chosen. This part requires
    some experience as ML developers tend to get better at this over time. The main
    pitfall in this task is the ability to set up an objective assessment of the result.
    That objective assessment is done by applying the finished model to data it had
    never “seen” before. But often folks who are only starting to apply ML find themselves
    improving their design after seeing what the results of that held-out set are.
    This leads to a feedback loop where the design is practically fitted to the no-longer-held-out
    set. While this may indeed improve the model and the design, it takes away from
    the ability to provide an objective forecast of how the model would perform when
    implemented in the real world. In the real world, it would see data that is truly
    held out and that it wasn’t fitted to.
  prefs: []
  type: TYPE_NORMAL
- en: Done and delivered
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Typically, when the design is done, the implementation is complete, and the
    results have been found satisfactory, the work is presented for business implementation,
    or in the research setting, for publication. In the business setting, implementation
    can take on different forms.
  prefs: []
  type: TYPE_NORMAL
- en: One of the simplest forms is where the output is used to provide business insights.
    Its purpose is to be presented. For instance, when looking to evaluate how much
    a marketing campaign was contributing to the growth in sales, the ML team may
    calculate an estimation for that measure of contribution and present it to leadership.
  prefs: []
  type: TYPE_NORMAL
- en: Another form of implementation is within a dashboard in real time. For instance,
    the model calculates the predicted risk of patients coming to the emergency room,
    and it does so on a daily cadence. The results are aggregated and a graph is presented
    on the hospital dashboard to show the expected number of people who would come
    to the emergency room for every day of the next 30 days.
  prefs: []
  type: TYPE_NORMAL
- en: A more advanced and common form is when the output of the data is directed so
    that it can be fed into downstream tasks. The model would then be implemented
    in production to become a microservice within a larger production pipeline. An
    example of that is when a classifier evaluates every post on your company’s Facebook
    page. When it identifies offensive language, it outputs a detection that then
    passes down the pipeline to another system that removes that post and perhaps
    blocks that user.
  prefs: []
  type: TYPE_NORMAL
- en: Code design
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The code’s design should suit the purpose of the code once the work is complete.
    As per the different forms of implementation mentioned previously, some implementations
    dictate a specific code structure. For instance, when the completed code is handed
    off to production within a larger, already existing pipeline, it is the production
    engineer who would dictate the constraints to the ML team. These constraints may
    be around computation and timing resources, but they would also be around code
    design. Often, basic code files, such as `.py` files, are necessary.
  prefs: []
  type: TYPE_NORMAL
- en: As with cases where the code is used for presentations, such as in the example
    of presenting how contributive the marketing campaign was, Jupyter Notebooks may
    be the better choice.
  prefs: []
  type: TYPE_NORMAL
- en: Jupyter Notebooks can be very informative and instructional. For that reason,
    many ML developers start their projects with Jupyter Notebooks for the exploration
    phase.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will review our design in a Jupyter Notebook. This will allow us to
    encapsulate the entire process in a single coherent file that is meant to be presented
    to the reader.
  prefs: []
  type: TYPE_NORMAL
- en: Reviewing our use case – ML system design for NLP classification in a Jupyter
    Notebook
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will walk through a hands-on example. We will follow the
    steps we presented previously for articulating the problem, designing the solution,
    and evaluating the results. This section portrays the process that an ML developer
    goes through when working on a typical project in the industry. Refer to the notebook
    at [https://colab.research.google.com/drive/1ZG4xN665le7X_HPcs52XSFbcd1OVaI9R?usp=sharing](https://colab.research.google.com/drive/1ZG4xN665le7X_HPcs52XSFbcd1OVaI9R?usp=sharing)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: The business objective
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this scenario, we are working for a financial news agency. Our objective
    is to publish news about companies and products in real time.
  prefs: []
  type: TYPE_NORMAL
- en: The technical objective
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The CTO derives several technical objectives from the business objective. One
    objective is for the ML team: given a stream of financial tweets in real time,
    detect those tweets that discuss information about companies or products.'
  prefs: []
  type: TYPE_NORMAL
- en: The pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s review the different parts of the pipeline, as shown in *Figure 5**.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2 – The structure of a typical ML pipeline](img/B18949_05_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 – The structure of a typical ML pipeline
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The phases of the pipeline in *Figure 5**.2* are explored in the following subsections
  prefs: []
  type: TYPE_NORMAL
- en: Code settings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this part of the code, we set the key parameters. We choose to have them
    as a part of the code as this is instructional code made for presentation. In
    cases where the code is expected to go to production, it may be better to host
    the parameters in a separate `.yaml` file. That would also suit heavy iterations
    during the development phase as it will allow you to iterate over different code
    parameters without having to change the code, which is often desirable.
  prefs: []
  type: TYPE_NORMAL
- en: As for the choice of these values, it should be stressed that some of these
    values should be optimized to suit the optimization of the solution. We have chosen
    fixed measures here to simplify the process. For instance, the number of features
    to be used for classification is a fixed quantity here, but it should also be
    optimized to fit the training set.
  prefs: []
  type: TYPE_NORMAL
- en: Gathering the data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This part loads the dataset. In our case, the loading function is simple. In
    other business cases, this part could be quite large as it may include a collection
    of SQL queries that are called. In such a case, it may be ideal to write a dedicated
    function in a separate `.py` file and source it via the imports section.
  prefs: []
  type: TYPE_NORMAL
- en: Processing the data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here, we format the data in a way that suits our work. We also observe some
    of it for the first time. This allows us to get a feel of its nature and quality.
  prefs: []
  type: TYPE_NORMAL
- en: One key action we take here is to define the classes we care about.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we discussed in [*Chapter 4*](B18949_04.xhtml#_idTextAnchor113), preprocessing
    is a key part of the pipeline. For instance, we notice that many of the tweets
    have a URL, which we choose to remove.
  prefs: []
  type: TYPE_NORMAL
- en: Preliminary data exploration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At this point, we have observed the quality of the text and the distribution
    of the classes. This is where we explore any other characteristics of the data
    that may imply either its quality or its ability to indicate the desired class.
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next, we start processing the text. We seek to represent the text of each observation
    as a set of numerical features. The main reason for this is that traditional ML
    models are designed to accept numbers as input, not text. For instance, a common
    linear regression or logistic regression model is applied to numbers, not words,
    categories, or image pixels. Thus, we need to suggest a numeric representation
    for the text. This design constraint is lifted when working with **language models**
    such as **BERT** and **GPT**. We will see this in the coming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: We partition the text into N-grams, where *N* is a parameter of the code. *N*
    is fixed in this code but should be optimized to best fit the training set.
  prefs: []
  type: TYPE_NORMAL
- en: Once the text has been partitioned into N-grams, they are modeled as numeric
    values. When a binary (that is, **one-hot encoding**) method is chosen, the numerical
    feature that represents some N-gram gets a “1” when the observed text includes
    that N-gram, and “0” otherwise. See *Figure 5**.3* for an example. If a BOW approach
    is chosen, then the value of the feature is the number of times the N-gram appears
    in the observed text. Another common feature engineering method that isn’t implemented
    here is **TF-IDF**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s what we get by using unigrams only:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input sentence: “filing submitted.”'
  prefs: []
  type: TYPE_NORMAL
- en: '| **N-gram** | **“****report”** | **“****filing”** | **“****submitted”** |
    **“****product”** | **“****quarterly”** | **The rest of** **the unigrams** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Feature value | 0 | 1 | 1 | 0 | 0 | (0’s) |'
  prefs: []
  type: TYPE_TB
- en: Figure 5.3 – Transforming an input text sentence into a numerical representation
    by partitioning to unigrams via one-hot encoding
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows what we get by using both unigrams and bigrams:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **N-gram** | **“****report”** | **“****filing”** | **“****filing submitted”**
    | **“****report news”** | **“****submitted”** | **The rest of** **the N-grams**
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Feature value | 0 | 1 | 1 | 0 | 1 | (0’s) |'
  prefs: []
  type: TYPE_TB
- en: Figure 5.4 – Transforming an input text sentence into a numerical representation
    by partitioning to unigrams and bigrams via one-hot encoding
  prefs: []
  type: TYPE_NORMAL
- en: Note that at this point in the code, the dataset hasn’t been partitioned into
    train and test sets, and the held-out set has not been excluded yet. This is because
    the binary and BOW feature engineering methods don’t depend on data outside of
    the underlying observation. With TF-IDF, this is different. Every feature value
    is calculated using the entire dataset for the document frequency.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the new numerical features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that our text has been represented as a feature, we can explore it numerically.
    We can look at its frequencies and statistics and get a sense of how it’s distributed.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting into train/test sets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is the part where we must pause and carve out a held-out set, also known
    as a test set, and sometimes as the validation set. Since these terms are used
    differently in different sources, it is important to explain that what we refer
    to as a test set is a held-out set. A held-out set is a data subset that we dedicate
    to evaluating our solution’s performance. It is held out to simulate the results
    that we expect to get when the system is implemented in the real world and will
    encounter new data samples.
  prefs: []
  type: TYPE_NORMAL
- en: How do we know when to carve out the held-out set?
  prefs: []
  type: TYPE_NORMAL
- en: If we carve it out “too early,” such as right after loading the data, then we
    are guaranteed to keep it held out, but we may miss discrepancies in the data
    as it won’t take part in the preliminary exploration. If we carve it out “too
    late,” our design decisions might become biased because of it. For example, if
    we choose one ML model over another based on results that include the would-be
    held-out set, then our design becomes tailored to that set, preventing us from
    offering an objective evaluation of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we need to carry out the test set right before the first action that will
    feed into design decisions. In the next section, we’ll perform statistical analysis,
    which we can then feed into feature selection. Since that selection should be
    agnostic to the held-out set, we’ll exclude that set from this part onwards.
  prefs: []
  type: TYPE_NORMAL
- en: Preliminary statistical analysis and feasibility study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is the second part of the exploration phase we spoke about a few pages
    ago. The first part was data exploration, and we implemented that in the previous
    parts of the code. Now that we have the text represented as numerical features,
    we can perform the feasibility study.
  prefs: []
  type: TYPE_NORMAL
- en: We seek to measure the statistical dependence between the text inputs and the
    class values. Again, the motivation is to mimic the proxy that linear correlation
    provides with a regression problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'We know that for two random variables, *X* and *Y*, if they are statistically
    independent, then we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>P</mi><mo>(</mo><mi>X</mi><mo>=</mo><mi>x</mi><mo>,</mo><mi>Y</mi><mo>=</mo><mi>y</mi><mo>)</mo><mo>=</mo><mi>P</mi><mo>(</mo><mi>X</mi><mo>=</mo><mi>x</mi><mo>)</mo><mi>P</mi><mo>(</mo><mi>Y</mi><mo>=</mo><mi>y</mi><mo>)</mo><mo>,</mo><mi>f</mi><mi>o</mi><mi>r</mi><mi>e</mi><mi>v</mi><mi>e</mi><mi>r</mi><mi>y</mi><mi>x</mi><mi>a</mi><mi>n</mi><mi>d</mi><mi>y</mi></mrow></mrow></mrow></math>](img/291.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Alternatively, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mfrac><mrow><mi>P</mi><mo>(</mo><mi>X</mi><mo>=</mo><mi>x</mi><mo>,</mo><mi>Y</mi><mo>=</mo><mi>y</mi><mo>)</mo></mrow><mrow><mi>P</mi><mo>(</mo><mi>X</mi><mo>=</mo><mi>x</mi><mo>)</mo><mi>P</mi><mo>(</mo><mi>Y</mi><mo>=</mo><mi>y</mi><mo>)</mo></mrow></mfrac><mo>=</mo><mn>1</mn><mo>,</mo></mrow></mrow></math>](img/292.png)'
  prefs: []
  type: TYPE_IMG
- en: This happens for every *x, y* value that yields a non-zero probability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Conversely, we could use Bayes’s rules:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mfrac><mrow><mi>P</mi><mo>(</mo><mi>X</mi><mo>=</mo><mi>x</mi><mo>|</mo><mi>Y</mi><mo>=</mo><mi>y</mi><mo>)</mo><mi>P</mi><mo>(</mo><mi>Y</mi><mo>=</mo><mi>y</mi><mo>)</mo></mrow><mrow><mi>P</mi><mo>(</mo><mi>X</mi><mo>=</mo><mi>x</mi><mo>)</mo><mi>P</mi><mo>(</mo><mi>Y</mi><mo>=</mo><mi>y</mi><mo>)</mo></mrow></mfrac><mo>=</mo><mn>1</mn></mrow></mrow></math>](img/293.png)'
  prefs: []
  type: TYPE_IMG
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mfrac><mrow><mi>P</mi><mo>(</mo><mi>X</mi><mo>=</mo><mi>x</mi><mo>|</mo><mi>Y</mi><mo>=</mo><mi>y</mi><mo>)</mo></mrow><mrow><mi>P</mi><mo>(</mo><mi>X</mi><mo>=</mo><mi>x</mi><mo>)</mo></mrow></mfrac><mo>=</mo><mn>1</mn><mo>.</mo></mrow></mrow></math>](img/294.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, let’s think about any two random variables that aren’t necessarily statistically
    independent. We would like to evaluate whether there is a statistical relationship
    between the two.
  prefs: []
  type: TYPE_NORMAL
- en: Let one random variable be any of our numerical features, and the other random
    variable be the output class taking on values 0 or 1\. Let’s assume the feature
    engineering method is binary, so the feature also takes on values of 0 or 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at the last equation, the expression on the left-hand side presents
    a very powerful measure of the ability of the relationship between *X* and *Y*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mfrac><mrow><mi>P</mi><mo>(</mo><mi>f</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>r</mi><mi>e</mi><mo>=</mo><mi>x</mi><mo>|</mo><mi>c</mi><mi>l</mi><mi>a</mi><mi>s</mi><mi>s</mi><mo>=</mo><mi>y</mi><mo>)</mo></mrow><mrow><mi>P</mi><mo>(</mo><mi>f</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>r</mi><mi>e</mi><mo>=</mo><mi>x</mi><mo>)</mo></mrow></mfrac><mo>,</mo><mi>x</mi><mo>,</mo><mi>y</mi><mi>b</mi><mi>e</mi><mi>l</mi><mi>o</mi><mi>n</mi><mi>g</mi><mi>t</mi><mi>o</mi><mo>{</mo><mn>0,1</mn><mo>}</mo><mo>.</mo></mrow></mrow></mrow></math>](img/295.png)'
  prefs: []
  type: TYPE_IMG
- en: It is powerful because if the feature is completely nonindicative of the class
    value, then in statistical terms, we say the two are statistically independent,
    and thus this measure would be equal to 1.
  prefs: []
  type: TYPE_NORMAL
- en: Conversely, the bigger the difference between this measure and 1, the stronger
    the relationship is between this feature and this class. When performing a **feasibility
    study** of our design, we want to see that there are features in the data that
    have a statistical relationship with the output class.
  prefs: []
  type: TYPE_NORMAL
- en: For that reason, we calculate the value of this expression for every pair of
    every feature and every class.
  prefs: []
  type: TYPE_NORMAL
- en: We present the most indicative terms for class “0,” which is the class of tweets
    that don’t indicate a company or product information, and we also present the
    terms that are most indicative of class “1,” meaning, when a tweet is discussing
    information about a company or a product.
  prefs: []
  type: TYPE_NORMAL
- en: This proves to us that there are indeed text terms that are indicative of the
    class value. This is a definite and clear success of the feasibility study. We
    are good to go and we are expecting productive outcomes when implementing a classification
    model.
  prefs: []
  type: TYPE_NORMAL
- en: As a side note, keep in mind that as with most evaluations, what we’ve just
    mentioned is just one sufficient condition for the potential of the text to predict
    the class. If it had failed, it would not necessarily indicate that there is no
    feasibility. Just like when the linear correlation between *X* and *Y* is near
    0, this doesn’t mean that *X* can’t infer *Y*. It just means that *X* cannot infer
    *Y* via a linear model. The linearity is an assumption that’s made to make things
    simple if it prevails.
  prefs: []
  type: TYPE_NORMAL
- en: In the method that we’ve suggested, we make two key assumptions. First, we assume
    a very particular manner for feature design, being a certain *N* for the N-gram
    partition, and a certain quantitative method for the value – binary. The second
    is that we perform the most simple evaluation of statistical dependency, a univariate
    statistical dependency. But it could be that only a higher order, such as univariate,
    would have statistical dependence on the outcome class.
  prefs: []
  type: TYPE_NORMAL
- en: With a **feasibility study** of text classification, it’s ideal if the method
    is as simple as possible while covering as much of the “signal” it is hoping to
    uncover. The approach we designed in this example was derived after years of experience
    with different sets and various problem settings. We find that it hits the target
    very well.
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the **feasibility study**, we often kill two birds with one stone. As a
    **feasibility study** is successful, it not only helps us by confirming our plan,
    but it often hints toward the next steps that we should take. As we saw, some
    features are indicative of the class, and we learned which are the most significant.
    This allows us to reduce the feature space that the classification model will
    need to partition. We do that by keeping the most indicative features for each
    of the two classes. The number of features that we choose to keep would ideally
    be derived by computation constraints (for example, too many features would take
    too long to compute a model around), model capabilities (for example, too many
    features can’t be handled well by the model due to co-linearity), and optimization
    of the train results. In our code, we fixed this number to make things quick and
    simple.
  prefs: []
  type: TYPE_NORMAL
- en: It should be stressed that in many ML models, feature selection is an inherited
    part of the model design. For instance, with the **least absolute shrinkage and
    selection operator** (**LASSO**), the hyperparameter scaler of the ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="script">l</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/296.png)
    norm component has an impact on which features get a zero coefficient, and thus
    get “thrown out.” It is possible and sometimes recommended to skip this part of
    the feature selection process, leave all features in, and let the model perform
    feature selection. It is advised to do so when all the models that are being evaluated
    and compared possess that characteristic.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that at this point, we are only observing the train set. Now that we
    have decided which features to keep, we need to apply that selection to the test
    set as well.
  prefs: []
  type: TYPE_NORMAL
- en: With that, our data has been prepared for ML modeling.
  prefs: []
  type: TYPE_NORMAL
- en: Iterating over ML models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To choose which model suits this problem best, we must train several models
    and see which one of them does best.
  prefs: []
  type: TYPE_NORMAL
- en: We should stress that we could do many things to try and identify the best model
    choice for a given problem. In our case, we only chose to evaluate a handful of
    models. Moreover, to make things simple and quick, we chose to not optimize the
    hyperparameters of each model in a comprehensive cross-validation approach. We
    simply fit each model to the training set with the default settings that its function
    comes with. Once we’ve identified the model we’d like to use, we optimize its
    hyperparameters for the train set via cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: By doing this, we identify the best model for the problem.
  prefs: []
  type: TYPE_NORMAL
- en: Generating the chosen model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here, we optimize the hyperparameters of the chosen model and fit it to our
    train set.
  prefs: []
  type: TYPE_NORMAL
- en: Generating the train results – design choices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At this stage, we observe the results of the model for the first time. This
    result can be used to feed insight back into the design choice and the parameters
    chosen, such as the feature engineering method, the number of features left in
    the feature selection, and even the preprocessing scheme.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Note that when feeding back insights from the results of the train set to the
    design of the solution, you are risking overfitting the train set. You’ll know
    whether you are by the gap between the results on the train set and the results
    on the test set.
  prefs: []
  type: TYPE_NORMAL
- en: While a gap is expected between these results in favor of the train results,
    a large gap should be treated as an alarm that the design isn’t optimal. In such
    cases, the design should be redone with systematic code-based parameters to ensure
    fair choices are made. It is possible to even carve out another semi-held-out
    set from the train set, often referred to as the validation set.
  prefs: []
  type: TYPE_NORMAL
- en: Generating the test results – presenting performance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: That’s it!
  prefs: []
  type: TYPE_NORMAL
- en: Now that the design has been optimized and we are confident that it suits our
    objective, we can apply it to our held-out set and observe the test results. These
    results are our most objective forecast of how well the system would do in the
    real world.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned previously, we should avoid letting these results impact our design
    choices.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we embarked on a comprehensive exploration of text classification,
    an indispensable aspect of NLP and ML. We delved into various types of text classification
    tasks, each presenting unique challenges and opportunities. This foundational
    understanding sets the stage for effectively tackling a broad range of applications,
    from sentiment analysis to spam detection.
  prefs: []
  type: TYPE_NORMAL
- en: We walked through the role of N-grams in capturing local context and word sequences
    within text, thereby enhancing the feature set used for classification tasks.
    We also illuminated the power of the TF-IDF method, the role of Word2Vec in text
    classification, and popular architectures such as CBOW and skip-gram, giving you
    a deep understanding of their mechanics.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we introduced topic modeling and examined how popular algorithms such
    as LDA can be applied to text classification.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we introduced a professional paradigm for leading an NLP-ML project
    in a business or research setting. We discussed the objectives and the project
    design aspect, and then dove into the system design. We implemented a real-world
    example in code and experimented with this.
  prefs: []
  type: TYPE_NORMAL
- en: In essence, this chapter has aimed to equip you with a holistic understanding
    of text classification and topic modeling by touching on the key concepts, methodologies,
    and techniques in the field. The knowledge and skills imparted will enable you
    to effectively approach and solve real-world text classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will introduce advanced methods for text classification.
    We will review deep learning methods such as language models, discuss their theory
    and design, and present a hands-on system design in code.
  prefs: []
  type: TYPE_NORMAL
