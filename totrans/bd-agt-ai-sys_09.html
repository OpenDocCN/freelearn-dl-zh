<html><head></head><body>
  <div><h1 class="chapter-number" id="_idParaDest-192">
    <a id="_idTextAnchor203">
    </a>
    
     9
    
   </h1>
   <h1 id="_idParaDest-193">
    <a id="_idTextAnchor204">
    </a>
    
     Managing Safety and  Ethical Considerations
    
   </h1>
   <p>
    <a id="_idTextAnchor205">
    </a>
    
     In the previous chapter, we explored the pivotal role of trust in facilitating the successful adoption and acceptance of generative AI systems.
    
    
     We examined ways to foster trust, highlighting the role of transparency, explainability, addressing biases and uncertainties, and clear communication of AI outputs to improve user understanding and confidence.
    
    
     As generative AI technologies rapidly advance, fueled by immense interest and excitement across diverse domains from creative industries to healthcare, a sense of urgency has arisen to address the safety and ethical implications of these powerful systems.
    
    
     The discussion now turns to potential risks and challenges associated with generative AI, strategies for safe and responsible deployment, ethical guidelines, and considerations regarding privacy
    
    
     
      and security.
     
    
   </p>
   <p>
    
     The remarkable capabilities of generative AI systems have sparked both awe and concern, highlighting the need for a proactive approach to mitigating potential risks and ensuring responsible development and deployment.
    
    
     While these technologies hold immense potential for driving innovation and positive change, their misuse or unintended consequences could have far-reaching implications.
    
    
     This chapter is divided into the following
    
    
     
      main sections:
     
    
   </p>
   <ul>
    <li>
     
      Understanding potential risks
     
     
      
       and challenges
      
     
    </li>
    <li>
     
      Ensuring safe and
     
     
      
       responsible AI
      
     
    </li>
    <li>
     
      Exploring ethical guidelines
     
     
      
       and frameworks
      
     
    </li>
    <li>
     
      Addressing privacy and
     
     
      
       security concerns
      
     
    </li>
   </ul>
   <p>
    
     By the end of this chapter, you will understand the key risks and challenges of generative AI, including misinformation and bias concerns, know strategies for safe deployment, and have gained insight into crucial ethical considerations around privacy and data protection.
    
    
     You will also discover frameworks and guidelines for responsible AI development that balance innovation with
    
    
     
      societal well-being.
     
    
   </p>
   <h1 id="_idParaDest-194">
    <a id="_idTextAnchor206">
    </a>
    
     Understanding potential risks and challenges
    
   </h1>
   <p>
    
     The landscape of AI has evolved significantly with the emergence of
    
    <strong class="bold">
     
      large language models
     
    </strong>
    
     (
    
    <strong class="bold">
     
      LLMs
     
    </strong>
    
     ) that power
    
    <a id="_idIndexMarker673">
    </a>
    
     both generative AI and agentic systems.
    
    
     While generative AI focuses primarily on creating content based on prompts and patterns, agentic systems built on these same LLMs take this capability further by
    
    <a id="_idIndexMarker674">
    </a>
    
     incorporating decision-making, planning, and goal-oriented behavior.
    
    
     This combination of generative capabilities with agency creates a powerful but potentially
    
    
     
      risky synergy.
     
    
   </p>
   <p>
    
     Agentic systems leverage the generative capabilities of LLMs to not just produce content but also to actively analyze situations, formulate strategies, and take action toward specific objectives.
    
    
     This means that any inherent risks in generative AI systems – such as biases, hallucinations, or the generation of misleading information – become particularly critical when the system is empowered to act autonomously or semi-autonomously based on this
    
    
     
      generated content.
     
    
   </p>
   <p>
    
     Generative AI systems are powered by massive language models, which, while incredibly powerful, also exhibit a range of vulnerabilities and risks.
    
    
     These risks can be broadly classified into the following
    
    
     
      key areas.
     
    
   </p>
   <h2 id="_idParaDest-195">
    <a id="_idTextAnchor207">
    </a>
    
     Adversarial attacks
    
   </h2>
   <p>
    
     One of the significant risks associated with generative AI systems is their susceptibility to adversarial
    
    <a id="_idIndexMarker675">
    </a>
    
     attacks.
    
    
     Malicious individuals can exploit flaws in these systems by crafting carefully designed inputs or perturbations that corrupt the data in a way that leads to harmful outputs or extracts confidential information.
    
    
     These adversarial attacks can have serious consequences, such as data breaches, unauthorized access to sensitive information, or the generation of malicious or
    
    
     
      misleading content.
     
    
   </p>
   <p>
    
     When these vulnerabilities extend to agentic systems, the risks become even more pronounced as these systems not only generate responses but also execute actions based on their understanding.
    
    
     An adversarial attack on an agentic system could potentially manipulate its decision-making process, causing it to take harmful actions or make dangerous choices autonomously.
    
    
     For instance, an agentic system managing supply chain operations could be tricked into making catastrophic inventory decisions, or a trading agent could be manipulated into executing harmful
    
    
     
      financial transactions.
     
    
   </p>
   <p>
    
     In the travel industry, consider a scenario where an agentic AI system is used by a travel agency to not only provide personalized travel recommendations but also to automatically book flights, hotels, and activities.
    
    
     An adversarial attack on such a system could potentially lead to disastrous consequences.
    
    
     Beyond just recommending unsafe destinations, the system could actively make bookings in dangerous areas, confirm reservations with fraudulent providers, or execute financial transactions that compromise
    
    
     
      clients’ security.
     
    
   </p>
   <p>
    
     Additionally, adversarial attacks could be used to extract sensitive information, such as customer travel histories, credit card details, or personal preferences, from the system.
    
    
     This risk is amplified in agentic systems because they often have broader access to execute transactions and make decisions, potentially exposing more sensitive data and control points
    
    
     
      to attackers.
     
    
   </p>
   <p>
    
     Real-world
    
    <a id="_idIndexMarker676">
    </a>
    
     examples of adversarial attacks on AI systems have already been documented.
    
    
     In 2017, researchers demonstrated how minor perturbations to images could fool state-of-the-art computer vision models into misclassifying objects, such as a stop sign being recognized as a speed limit sign.
    
    
     Similarly, in the natural language processing domain, researchers have shown how carefully crafted input sequences can cause language models to generate harmful or inappropriate content.
    
    
     When these vulnerabilities are exploited in agentic systems, the impact could extend beyond content generation to actual real-world actions
    
    
     
      and decisions.
     
    
   </p>
   <p>
    
     Similarly, in a medical context, an adversarial attack on an agentic AI system used for diagnosis or treatment recommendations could potentially lead to life-threatening errors or data leaks.
    
    
     Imagine a scenario where an adversarial input causes the AI to not only misdiagnose a condition but also automatically schedule incorrect treatments, order wrong medications, or make dangerous adjustments to medical devices under
    
    
     
      its control.
     
    
   </p>
   <p>
    
     These examples highlight the severe consequences that adversarial attacks can have on both generative and agentic AI systems, underscoring the importance of robust security measures and ongoing research into defense mechanisms against such attacks.
    
    
     Techniques such as adversarial training, input sanitization, and anomaly detection can help mitigate the risks, but it is an ongoing challenge that requires vigilance and collaboration within the AI community.
    
    
     For agentic systems, additional safeguards such as action verification, decision auditing, and multi-step authentication processes become crucial to prevent malicious exploitation of their
    
    
     
      autonomous capabilities.
     
    
   </p>
   <h2 id="_idParaDest-196">
    <a id="_idTextAnchor208">
    </a>
    
     Bias and discrimination
    
   </h2>
   <p>
    
     We are aware that generative AI models are trained on vast datasets that may carry inherent biases
    
    <a id="_idIndexMarker677">
    </a>
    
     and historical prejudices.
    
    
     When these models form the foundation for agentic systems, the implications of bias become even more critical as these systems not only generate content but also make autonomous decisions that can directly impact
    
    
     
      people’s lives.
     
    
   </p>
   <p>
    
     The issue of bias in AI systems has been a long-standing concern, and both generative AI models and the agentic systems built upon them are susceptible to this challenge.
    
    
     These models learn from their training data, and if that data contains biases or reflects societal prejudices, the AI will inevitably absorb and perpetuate those biases not just in its outputs but in its decision-making processes and
    
    
     
      actions too.
     
    
   </p>
   <p>
    
     For instance, consider an agentic AI system used not just for screening job candidates but also for making autonomous hiring decisions, scheduling interviews, and managing employee assignments.
    
    
     If biased, such a system could systematically discriminate against certain demographic groups throughout the entire employment life cycle, from initial screening to promotion decisions.
    
    
     This automated perpetuation of bias could be particularly harmful as it operates at scale and may be harder to detect than
    
    
     
      human bias.
     
    
   </p>
   <p>
    
     In the travel industry, bias in agentic AI systems could manifest beyond mere recommendations to actual booking decisions and resource allocations.
    
    
     An autonomous travel management system might systematically direct certain demographic groups to specific neighborhoods or price ranges, effectively implementing digital redlining.
    
    
     It might also autonomously negotiate different rates or terms for different users based on biased assumptions, creating a form of algorithmic discrimination in pricing and
    
    
     
      service delivery.
     
    
   </p>
   <p>
    
     In 2018, researchers found that commercial facial recognition systems exhibited higher error rates for identifying women and people with darker skin tones.
    
    
     When such biased systems are integrated into agentic AI that controls access to buildings, financial services, or healthcare resources, these technical shortcomings transform into systemic barriers that actively restrict opportunities and services for
    
    
     
      certain groups.
     
    
   </p>
   <p>
    
     Addressing bias in agentic AI systems requires an expanded approach beyond what’s needed for traditional generative AI.
    
    
     While diverse training data and debiasing algorithms remain important, additional measures are needed to ensure fairness in autonomous decision-making.
    
    
     This includes implementing decision auditing systems, creating accountability frameworks for autonomous actions, and developing real-time bias detection mechanisms that can intervene before discriminatory actions
    
    
     
      are taken.
     
    
   </p>
   <p>
    
     Moreover, involving diverse stakeholders becomes even more crucial when developing agentic systems, as these stakeholders can help identify potential negative impacts across the full range of autonomous actions the system might take.
    
    
     Regular audits of not just the system’s outputs but also its decision-making patterns and action histories are essential for detecting and correcting
    
    
     
      systematic biases.
     
    
   </p>
   <p>
    
     By proactively
    
    <a id="_idIndexMarker678">
    </a>
    
     addressing biases in both generative and agentic AI systems, organizations can ensure these technologies serve as tools for promoting equity rather than reinforcing discrimination.
    
    
     This is particularly critical for agentic systems, as their ability to autonomously act on biased assumptions can multiply the harmful effects of discrimination and create self-reinforcing cycles
    
    
     
      of inequity.
     
    
   </p>
   <h2 id="_idParaDest-197">
    <a id="_idTextAnchor209">
    </a>
    
     Misinformation and hallucinations
    
   </h2>
   <p>
    
     Generative AI systems have a tendency to produce information that may be factually incorrect
    
    <a id="_idIndexMarker679">
    </a>
    
     or inconsistent with reality, a phenomenon
    
    <a id="_idIndexMarker680">
    </a>
    
     known as
    
    <strong class="bold">
     
      hallucination
     
    </strong>
    
     .
    
    
     When these systems are integrated into autonomous agents, the implications become even more serious, as hallucinated information can directly influence real-world decisions and actions taken by the agent.
    
    
     The hallucination problem in both generative and agentic AI systems stems from their underlying architecture.
    
    
     While incredibly powerful, these models lack a true understanding of the world and cannot reliably distinguish between factual information and fabricated content.
    
    
     In agentic systems, this limitation is particularly concerning because the agent may act upon hallucinated information without human verification, potentially causing cascading errors or
    
    
     
      harmful decisions.
     
    
   </p>
   <p>
    
     In the realm of autonomous decision-making, an agentic system that hallucinates could take actions based on non-existent information or false assumptions.
    
    
     For instance, an autonomous trading agent might execute large financial transactions based on hallucinated market trends, or a healthcare management agent might schedule treatments based on incorrectly generated medical histories.
    
    
     These scenarios are far more dangerous than simple content generation errors, as they involve direct
    
    
     
      real-world consequences.
     
    
   </p>
   <p>
    
     For example, consider an agentic AI system deployed in emergency response management.
    
    
     If the system hallucinates information about the severity or location of an emergency, it could autonomously dispatch resources to the wrong location or make inappropriate response decisions, potentially putting lives at risk.
    
    
     Unlike a generative system that merely produces incorrect text, an agentic system’s hallucinations can lead to immediate, real-world actions with
    
    
     
      serious consequences.
     
    
   </p>
   <p>
    
     In the travel industry, hallucinations in agentic AI systems could go beyond just providing incorrect information – they could result in actual bookings being made based on non-existent flights or hotels, autonomous rerouting of travelers based on hallucinated weather conditions, or emergency evacuations triggered by fabricated
    
    
     
      security threats.
     
    
   </p>
   <p>
    
     Real-world
    
    <a id="_idIndexMarker681">
    </a>
    
     examples of hallucinations in AI systems have been documented across various domains.
    
    
     In 2022, researchers found that large language models such as GPT-3 can produce
    
    <em class="italic">
     
      hallucinated
     
    </em>
    
     scientific claims that sound plausible but are entirely fabricated.
    
    
     For agentic systems built on these models, such hallucinations could lead to automated decisions in research resource allocation, experimental design, or data analysis that could compromise
    
    
     
      scientific integrity.
     
    
   </p>
   <p>
    
     Addressing hallucinations in agentic AI systems requires additional safeguards beyond those used for generative AI.
    
    
     While fact-checking and knowledge grounding remain important, agentic systems also need real-time verification mechanisms, action validation protocols, and fallback procedures for cases where information reliability is uncertain.
    
    
     Moreover, implementing
    
    <em class="italic">
     
      uncertainty-aware
     
    </em>
    
     decision-making processes that can appropriately handle cases where the agent is not confident about its information
    
    
     
      is crucial.
     
    
   </p>
   <p>
    
     When deploying agentic systems, organizations must implement robust monitoring systems that can detect and prevent actions based on hallucinated information before they occur.
    
    
     This might include multi-step verification processes for critical decisions, confidence thresholds for autonomous actions, and human oversight mechanisms for high-stakes situations.
    
    
     By proactively addressing hallucinations in agentic AI systems, organizations can better ensure that autonomous agents make decisions based on reliable information.
    
    
     This is particularly critical as these systems become more prevalent in domains where incorrect actions could have significant consequences for safety, security, or
    
    
     
      business operations.
     
    
   </p>
   <h2 id="_idParaDest-198">
    <a id="_idTextAnchor210">
    </a>
    
     Data privacy violations
    
   </h2>
   <p>
    
     Generative AI
    
    <a id="_idIndexMarker682">
    </a>
    
     models are trained on vast amounts of data, which may inadvertently include
    
    <strong class="bold">
     
      personally identifiable information
     
    </strong>
    
     (
    
    <strong class="bold">
     
      PII
     
    </strong>
    
     ) or sensitive
    
    <a id="_idIndexMarker683">
    </a>
    
     data.
    
    
     In agentic systems, this risk is compounded because these systems not only process and generate information but also actively access, manipulate, and make decisions about personal data as part of their
    
    
     
      autonomous operations.
     
    
   </p>
   <p>
    
     The sheer volume of data required to train and operate these systems increases the likelihood of privacy violations.
    
    
     For agentic systems, this risk extends beyond training data to include operational data that they actively collect and use, such as user interactions, transaction histories, and real-time behavioral data that helps them
    
    
     
      make decisions.
     
    
   </p>
   <p>
    
     For example, an agentic AI system in healthcare might not only have access to historical medical records for training but also actively manage patient scheduling, treatment plans, and medical device settings.
    
    
     If such a system mishandles private information, it could autonomously share sensitive medical details with unauthorized parties, schedule appointments that reveal confidential conditions, or make treatment decisions that inadvertently expose protected
    
    
     
      health information.
     
    
   </p>
   <p>
    
     In the travel industry, privacy violations could occur when agentic systems go beyond simple data exposure to actively making privacy-compromising decisions.
    
    
     An autonomous travel assistant might not just leak travel itineraries but could also make bookings that reveal sensitive personal information, automatically share location data with third parties, or create patterns of behavior that expose confidential business travel or personal relationships.
    
    
     The risks became evident in 2019 when OpenAI’s language model was found to have memorized and reproduced portions of its training data such as personal information like emails, home addresses, and phone numbers.
    
    
     For agentic systems, similar issues could lead to automated decisions being made based on memorized private information, potentially causing systematic privacy violations
    
    
     
      at scale.
     
    
   </p>
   <p>
    
     Addressing data privacy violations in agentic AI systems requires an enhanced approach beyond traditional generative AI safeguards.
    
    
     While robust data governance and sanitization remain crucial, agentic systems also need real-time privacy monitoring, decision auditing systems, and automatic privacy-preserving mechanisms that prevent unauthorized data access or sharing during autonomous operations.
    
    
     Additionally, techniques such as differential privacy must be adapted for dynamic decision-making scenarios.
    
    
     Organizations need to implement privacy-aware decision protocols that ensure autonomous actions don’t inadvertently reveal sensitive information through patterns of behavior or chains of decisions, even when individual actions
    
    
     
      appear privacy-compliant.
     
    
   </p>
   <p>
    
     To safeguard privacy in these systems, new frameworks must extend beyond traditional data protection measures.
    
    
     Teams deploying agentic AI need to scrutinize how autonomous decisions could compromise privacy across time – watching for subtle patterns that might reveal sensitive information through a series of seemingly innocent actions.
    
    
     This means rethinking privacy from the ground up: privacy isn’t just about protecting data anymore, but about understanding how chains of autonomous decisions could inadvertently reveal what should
    
    
     
      stay hidden.
     
    
   </p>
   <p>
    
     The most
    
    <a id="_idIndexMarker684">
    </a>
    
     successful deployments of agentic AI will likely be those that make privacy an integral part of their system’s “nervous system” rather than an afterthought.
    
    
     This means building systems that instinctively protect privacy at every decision point, much like how humans naturally modulate their behavior to protect sensitive information in different contexts.
    
    
     When privacy becomes part of the agent’s core decision-making process rather than just a compliance checkbox, we can better ensure these powerful systems enhance rather than endanger our privacy rights in an increasingly
    
    
     
      automated world.
     
    
   </p>
   <h2 id="_idParaDest-199">
    <a id="_idTextAnchor211">
    </a>
    
     Intellectual property risks
    
   </h2>
   <p>
    
     The integration of generative AI capabilities into agentic systems introduces complex intellectual
    
    <a id="_idIndexMarker685">
    </a>
    
     property challenges that go far beyond traditional content generation concerns.
    
    
     When autonomous agents are empowered to not only create content but also make decisions about how to use, modify, and deploy intellectual property, the stakes become
    
    
     
      significantly higher.
     
    
   </p>
   <p>
    
     The increasing
    
    <a id="_idIndexMarker686">
    </a>
    
     use of autonomous agents in content generation raises significant concerns about
    
    <strong class="bold">
     
      intellectual property
     
    </strong>
    
     (
    
    <strong class="bold">
     
      IP
     
    </strong>
    
     ) infringement, necessitating robust detection and mitigation strategies.
    
    
     AI-generated content tracking systems such as
    
    <em class="italic">
     
      Copyleaks
     
    </em>
    
     for plagiarism detection, Google’s
    
    <em class="italic">
     
      SynthID
     
    </em>
    
     for watermarking AI-generated images, and
    
    <em class="italic">
     
      Truepic
     
    </em>
    
     for verifying digital authenticity help identify
    
    <a id="_idIndexMarker687">
    </a>
    
     unauthorized use of copyrighted material.
    
    
     Dataset auditing tools such as Hugging Face’s
    
    <em class="italic">
     
      Dataset Card Standard
     
    </em>
    
     , LAION’s transparency efforts, and Adobe’s
    
    <strong class="bold">
     
      Content Authenticity Initiative
     
    </strong>
    
     (
    
    <strong class="bold">
     
      CAI
     
    </strong>
    
     ) ensure that datasets used by autonomous agents comply with licensing and provenance requirements.
    
    
     Automated copyright violation detection services, including Microsoft’s
    
    <em class="italic">
     
      Azure Content Moderator
     
    </em>
    
     ,
    
    <em class="italic">
     
      Amazon Rekognition
     
    </em>
    
     for identifying copyrighted images and logos, and Meta’s
    
    <em class="italic">
     
      Rights Manager
     
    </em>
    
     for monitoring IP violations across social platforms, further enhance compliance efforts.
    
    
     Additionally, legal and policy compliance frameworks, such as
    
    <em class="italic">
     
      WIPO PROOF
     
    </em>
    
     for timestamping IP ownership (now discontinued), IBM’s
    
    <em class="italic">
     
      AI Governance Toolkit
     
    </em>
    
     for assessing infringement risks, and OpenAI’s licensing agreements that impose API-level restrictions, provide structured safeguards against IP violations.
    
    
     By integrating these methodologies, organizations can ensure that
    
    <a id="_idIndexMarker688">
    </a>
    
     autonomous agents operate within ethical and legal boundaries, minimizing the risks associated with unauthorized content generation
    
    
     
      and distribution.
     
    
   </p>
   <p>
    
     The fundamental challenge stems from both the training and operational aspects of these systems.
    
    
     During training, agentic AI systems, like their generative counterparts, ingest vast amounts of potentially copyrighted material – from code and design files to creative works and proprietary business processes.
    
    
     But unlike purely generative systems, agents can actively implement this learned information in ways that could systematically violate intellectual property rights at scale and at
    
    
     
      machine speed.
     
    
   </p>
   <p>
    
     Consider an autonomous software development agent that doesn’t just suggest code snippets but actively writes and deploys applications.
    
    
     Such a system might inadvertently incorporate proprietary algorithms or protected code patterns across thousands of projects before any violation is detected.
    
    
     Similarly, in creative industries, an agentic system managing content production could autonomously remix and repurpose copyrighted materials in ways that create complex chains of derivative works, each with its own potential
    
    
     
      infringement issues.
     
    
   </p>
   <p>
    
     The real-world implications are already emerging.
    
    
     The 2022 lawsuit against Stability AI’s Stable Diffusion image generator highlighted concerns about training data usage (
    
    <a href="https://jipel.law.nyu.edu/andersen-v-stability-ai-the-landmark-case-unpacking-the-copyright-risks-of-ai-image-generators/">
     
      https://jipel.law.nyu.edu/andersen-v-stability-ai-the-landmark-case-unpacking-the-copyright-risks-of-ai-image-generators/
     
    </a>
    
     ), but agentic systems raise even thornier questions.
    
    
     What happens when an AI agent autonomously creates and executes marketing campaigns using style elements it learned from copyrighted works?
    
    
     Or when it modifies and redistributes protected content based on its understanding of
    
    
     
      fair use?
     
    
   </p>
   <p>
    
     Addressing these challenges requires a radical rethinking of intellectual property protection in an age of autonomous systems.
    
    
     Organizations must develop new frameworks that can anticipate and prevent potential IP violations before they occur, rather than just detecting them after the fact.
    
    
     This means implementing real-time monitoring systems that can track the provenance of agent-generated content and decision trees that can evaluate IP implications before autonomous actions
    
    
     
      are taken.
     
    
   </p>
   <p>
    
     Technical innovation will play a crucial role in this evolution.
    
    
     We’re seeing the emergence of new approaches such as blockchain-based content tracking, automated license verification systems, and AI agents specifically designed to audit other agents for potential IP violations.
    
    
     These tools, combined with traditional legal safeguards, form the foundation of a new approach to IP protection in the age of
    
    
     
      autonomous systems.
     
    
   </p>
   <p>
    
     As we navigate
    
    <a id="_idIndexMarker689">
    </a>
    
     this complex landscape, flexibility and adaptation will be key.
    
    
     The legal frameworks governing intellectual property were designed for a world of human creators and human decision-makers.
    
    
     As agentic AI systems become more prevalent, these frameworks will need to evolve – not just to protect existing rights but also to foster innovation in a world where machines are increasingly active participants in the
    
    
     
      creative process.
     
    
   </p>
   <h1 id="_idParaDest-200">
    <a id="_idTextAnchor212">
    </a>
    
     Ensuring safe and responsible AI
    
   </h1>
   <p>
    
     The deployment of LLM-based agentic systems introduces unique safety and responsibility
    
    <a id="_idIndexMarker690">
    </a>
    
     challenges that go beyond those of traditional generative AI.
    
    
     While generative AI primarily focuses on content creation, agentic systems can autonomously plan, decide, and act, making their safe deployment significantly more complex and critical.
    
    
     Core safety considerations for agentic systems include
    
    
     
      the following:
     
    
   </p>
   <ul>
    <li>
     <strong class="bold">
      
       Action boundaries
      
     </strong>
     
      : Defining strict action boundaries is critical to ensuring that
     
     <a id="_idIndexMarker691">
     </a>
     
      agentic systems operate within safe and ethical constraints.
     
     
      These boundaries can be enforced using policy-based governance frameworks such as OpenAI’s Function Calling API and Amazon Bedrock Guardrails, which allow agents to interact with external systems while adhering to predefined operational
     
     <a id="_idIndexMarker692">
     </a>
     
      limits.
     
     
      Additionally,
     
     <strong class="bold">
      
       role-based access control
      
     </strong>
     
      (
     
     <strong class="bold">
      
       RBAC
      
     </strong>
     
      ) and context-aware permissions can be implemented to restrict agents from taking unauthorized actions, particularly in high-risk domains such as finance
     
     
      
       and healthcare.
      
     
    </li>
    <li>
     <strong class="bold">
      
       Decision verification
      
     </strong>
     
      : Agentic systems must incorporate multi-step validation processes
     
     <a id="_idIndexMarker693">
     </a>
     
      for critical decisions, ensuring robustness and alignment with human oversight.
     
     
      This can be achieved using neural-symbolic reasoning, constraint satisfaction models, and logical verification techniques that validate each decision against predefined ethical and operational constraints before execution.
     
     
      Techniques such as tree search algorithms and Monte Carlo simulations can be applied to evaluate multiple possible outcomes and ensure optimal decision-making in
     
     
      
       real time.
      
     
    </li>
    <li>
     <strong class="bold">
      
       Rollback capabilities
      
     </strong>
     
      : The ability to undo or reverse autonomous actions is essential
     
     <a id="_idIndexMarker694">
     </a>
     
      for mitigating unintended consequences.
     
     
      This can be implemented through event sourcing and state management frameworks such as Apache Kafka and Temporal.io, which maintain an immutable log of agent actions, enabling controlled rollbacks.
     
     
      Version control for decision states, combined with checkpointing mechanisms, can allow systems to revert to a stable state when anomalies or failures
     
     
      
       are detected.
      
     
    </li>
    <li>
     <strong class="bold">
      
       Real-time monitoring
      
     </strong>
     
      : Continuous monitoring of agent behavior is crucial for detecting
     
     <a id="_idIndexMarker695">
     </a>
     
      deviations and preventing harmful actions.
     
     
      Anomaly detection models such as Facebook’s AI Anomaly Detection Pipeline and Amazon CloudWatch anomaly detection use machine learning-based pattern recognition to track behavioral shifts in real time.
     
     
      Additionally, drift detection algorithms can identify when an agent’s behavior
     
     <a id="_idIndexMarker696">
     </a>
     
      diverges from expected patterns, triggering alerts or initiating corrective actions.
     
     <strong class="bold">
      
       Explainable AI
      
     </strong>
     
      (
     
     <strong class="bold">
      
       XAI
      
     </strong>
     
      ) techniques further enhance monitoring by providing human-readable insights into why an agent made a
     
     
      
       particular decision.
      
     
    </li>
    <li>
     <strong class="bold">
      
       Reinforcement learning feedback loops
      
     </strong>
     
      : Incorporating
     
     <a id="_idIndexMarker697">
     </a>
     
      human-in-the-loop oversight
     
     <a id="_idIndexMarker698">
     </a>
     
      through
     
     <strong class="bold">
      
       reinforcement learning from human feedback
      
     </strong>
     
      (
     
     <strong class="bold">
      
       RLHF
      
     </strong>
     
      ) helps fine-tune agentic decision-making.
     
     
      By continuously integrating feedback from human reviewers, agents can improve their behavior over time while maintaining safety and ethical alignment.
     
     
      In high-stakes environments, hybrid AI-human
     
     <a id="_idIndexMarker699">
     </a>
     
      workflows can be used to escalate decisions that require
     
     
      
       human judgment.
      
     
    </li>
    <li>
     <strong class="bold">
      
       Performance metrics
      
     </strong>
     
      : Evaluating agentic systems requires more than just output
     
     <a id="_idIndexMarker700">
     </a>
     
      quality; it must also assess decision consistency, ethical alignment, risk assessment, and adaptability.
     
     
      AI auditing tools such as IBM’s AI Fairness 360 and Google’s Explainable AI provide comprehensive evaluation frameworks that measure not only accuracy but also transparency, robustness, and fairness.
     
     
      Additionally, causal inference models can help quantify the impact of agent decisions, ensuring alignment with ethical and
     
     
      
       regulatory standards.
      
     
    </li>
   </ul>
   <p>
    
     By integrating these technologies and methodologies, organizations can deploy agentic systems that are
    
    <em class="italic">
     
      safe
     
    </em>
    
     ,
    
    <em class="italic">
     
      transparent
     
    </em>
    
     , and
    
    <em class="italic">
     
      aligned with regulatory and ethical considerations
     
    </em>
    
     , reducing the risks associated with autonomous decision-making while maintaining
    
    
     
      operational efficiency.
     
    
   </p>
   <p>
    
     Let’s examine
    
    <a id="_idIndexMarker701">
    </a>
    
     how these safety measures manifest in practical deployments.
    
    
     Consider an agentic system managing a corporate travel program – beyond just generating recommendations, it actively books flights, adjusts schedules, and manages expenses.
    
    
     A system like this demands layered safety protocols that address both its generative and autonomous aspects, as
    
    
     
      highlighted here:
     
    
   </p>
   <ul>
    <li>
     
      Action boundaries might include financial limits on booking changes without approval, restrictions on booking destinations flagged as high risk, and rules about when schedule changes can be
     
     
      
       made autonomously
      
     
    </li>
    <li>
     
      Decision verification could involve multi-step checks before finalizing expensive bookings – perhaps requiring human approval for transactions above certain thresholds or automated cross-verification with company
     
     
      
       travel policies
      
     
    </li>
    <li>
     
      The system’s rollback capabilities would need to account for real-world constraints, such as airline cancellation policies or hotel booking deadlines, ensuring that autonomous actions don’t incur
     
     
      
       unnecessary penalties
      
     
    </li>
    <li>
     
      Real-time monitoring in this context would track patterns of bookings and expenses, flagging unusual activities such as multiple booking changes in short succession or deviations from typical corporate
     
     
      
       travel patterns
      
     
    </li>
    <li>
     
      Performance
     
     <a id="_idIndexMarker702">
     </a>
     
      metrics would go beyond simple measures such as successful bookings to evaluate decision quality – for instance, assessing whether the system consistently makes cost-effective choices while respecting traveler preferences and
     
     
      
       company policies
      
     
    </li>
   </ul>
   <p>
    
     This travel management example demonstrates how safety measures must be carefully tailored to both protect against potential risks and ensure efficient operation.
    
    
     The system needs to balance autonomy (such as automatically rebooking disrupted flights) with appropriate caution (such as requiring approval for significant itinerary changes), all while maintaining clear audit trails and explanation capabilities for its decisions.
    
    
     <em class="italic">
      
       Figure 9
      
     </em>
    
    <em class="italic">
     
      .1
     
    </em>
    
     shows the safety measures for this agentic travel
    
    
     
      management system:
     
    
   </p>
   <div><div><img alt="img" role="presentation" src="img/B31483_09_01.jpg"/>
     
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    
     Figure 9.1 – Safety measures for agentic travel management system
    
   </p>
   <p>
    
     Testing for agentic systems must be more comprehensive than traditional generative AI testing.
    
    
     While generative AI testing focuses on output quality, agentic system testing must evaluate
    
    <a id="_idIndexMarker703">
    </a>
    
     entire decision chains and action sequences.
    
    
     This includes simulating complex scenarios where the agent must make interconnected decisions, handle unexpected situations, and maintain safety constraints across
    
    
     
      multiple actions.
     
    
   </p>
   <p>
    
     Human oversight takes on new dimensions with agentic systems.
    
    
     Rather than simply reviewing generated content, humans must monitor decision patterns, intervene in complex situations, and help refine the system’s understanding of acceptable actions.
    
    
     This creates a need for new oversight tools and frameworks that can track and evaluate autonomous behavior in
    
    
     
      real time.
     
    
   </p>
   <p>
    
     The concept of
    
    <em class="italic">
     
      safe learning
     
    </em>
    
     becomes crucial for agentic systems.
    
    
     These systems must be able to learn from experience without compromising safety during operation.
    
    
     This might involve creating sandboxed environments where agents can safely explore new strategies or
    
    <a id="_idIndexMarker704">
    </a>
    
     implementing gradual automation where human oversight is reduced as the system proves its reliability.
    
    
     Critical implementation strategies include
    
    
     
      the following:
     
    
   </p>
   <ul>
    <li>
     <strong class="bold">
      
       Progressive autonomy
      
     </strong>
     
      : Starting with heavily restricted action capabilities and gradually expanding them based on
     
     
      
       demonstrated reliability
      
     
    </li>
    <li>
     <strong class="bold">
      
       Contextual safety bounds
      
     </strong>
     
      : Implementing different safety protocols based on the risk level of
     
     
      
       specific actions
      
     
    </li>
    <li>
     <strong class="bold">
      
       Continuous validation
      
     </strong>
     
      : Regular assessment of decision patterns to identify potential
     
     
      
       safety risks
      
     
    </li>
    <li>
     <strong class="bold">
      
       Emergency protocols
      
     </strong>
     
      : Clear procedures for rapid human intervention
     
     
      
       when needed
      
     
    </li>
   </ul>
   <p>
    
     Trust building with agentic systems requires more than just transparency – it needs demonstrable reliability in autonomous operation.
    
    
     Organizations must develop clear frameworks for communicating both the capabilities and limitations of their agentic systems, helping stakeholders understand when and how to rely on
    
    
     
      autonomous decisions.
     
    
   </p>
   <p>
    
     The ethical deployment of agentic systems also requires careful consideration of societal impact.
    
    
     These systems must be designed to respect not just individual privacy and rights but also broader social values and norms.
    
    
     Implementing explicit ethical constraints in the decision-making process involves encoding predefined ethical rules, fairness constraints, and compliance policies into the system’s logic using techniques such as constraint programming, rule-based ethics engines, and reinforcement learning with ethical reward models.
    
    
     For example, symbolic AI approaches can integrate formal ethics rules (e.g., Asimov’s laws of robotics and GDPR privacy requirements) directly into decision-making pipelines, ensuring that agents adhere to predefined ethical boundaries.
    
    
     Additionally, differential privacy mechanisms and bias mitigation algorithms (such as IBM’s AI Fairness 360) can enforce fairness and privacy compliance
    
    
     
      at runtime.
     
    
   </p>
   <p>
    
     To ensure
    
    <a id="_idIndexMarker705">
    </a>
    
     ethical adaptability, organizations can implement community feedback loops using
    
    <strong class="bold">
     
      human-in-the-loop
     
    </strong>
    
     (
    
    <strong class="bold">
     
      HITL
     
    </strong>
    
     ) systems, where flagged decisions are reviewed and incorporated into future model refinements.
    
    
     Additionally, governance frameworks should include periodic ethical audits, the establishment of red-teaming exercises to stress-test decision-making under edge cases, and mechanisms for incorporating stakeholder feedback into system improvements.
    
    
     As agentic systems become more prevalent, these comprehensive governance measures will be critical
    
    <a id="_idIndexMarker706">
    </a>
    
     in balancing automation with ethical responsibility, ensuring that AI-driven decisions align with societal expectations and
    
    
     
      regulatory requirements.
     
    
   </p>
   <p>
    
     By understanding and addressing these unique challenges of agentic systems, organizations can work toward deployments that not only leverage the power of autonomous operation but do so in a way that prioritizes safety, responsibility, and ethical considerations throughout the system’s
    
    
     
      life cycle.
     
    
   </p>
   <h1 id="_idParaDest-201">
    <a id="_idTextAnchor213">
    </a>
    
     Exploring ethical guidelines and frameworks
    
   </h1>
   <p>
    
     As generative AI systems become increasingly sophisticated and integrated into various aspects
    
    <a id="_idIndexMarker707">
    </a>
    
     of society, it is crucial to establish robust ethical guidelines and frameworks to ensure their responsible development and deployment.
    
    
     A sound ethical framework should encompass a range of principles and guidelines that prioritize human well-being, accountability, privacy protection, and
    
    
     
      inclusive governance.
     
    
   </p>
   <h2 id="_idParaDest-202">
    <a id="_idTextAnchor214">
    </a>
    
     Human-centric design
    
   </h2>
   <p>
    
     At the core of ethical AI development lies the principle of human-centric design.
    
    
     Generative
    
    <a id="_idIndexMarker708">
    </a>
    
     AI systems should be designed with a focus on enhancing human well-being and delivering positive experiences.
    
    
     This requires developing intuitive, accessible, and inclusive solutions that are aligned with human values, such as fairness, dignity, and respect for
    
    
     
      individual autonomy.
     
    
   </p>
   <p>
    
     For example, in the context of a travel agency, a human-centric generative AI system would prioritize personalized recommendations that cater to diverse preferences, cultural sensitivities, and accessibility needs, ensuring that all users can benefit from the technology in a meaningful and
    
    
     
      respectful manner.
     
    
   </p>
   <h2 id="_idParaDest-203">
    <a id="_idTextAnchor215">
    </a>
    
     Accountability and responsibility
    
   </h2>
   <p>
    
     Organizations developing and deploying generative AI systems must be held accountable
    
    <a id="_idIndexMarker709">
    </a>
    
     for the outputs and potential impacts of these technologies.
    
    
     This involves establishing clear lines of responsibility, comprehensive documentation of decision-making processes, and mechanisms for reviewing and addressing
    
    
     
      ethical implications.
     
    
   </p>
   <p>
    
     Implementing review boards or advisory committees comprising interdisciplinary experts, including ethicists, legal professionals, and representatives from potentially affected communities, can help organizations navigate complex ethical challenges and ensure
    
    
     
      responsible decision-making.
     
    
   </p>
   <h2 id="_idParaDest-204">
    <a id="_idTextAnchor216">
    </a>
    
     Privacy and data protection
    
   </h2>
   <p>
    
     User privacy and data protection should be embedded as foundational principles in the development
    
    <a id="_idIndexMarker710">
    </a>
    
     of generative AI systems.
    
    
     Organizations must adopt a
    
    <strong class="bold">
     
      privacy-by-design
     
    </strong>
    
     approach, practicing data minimization, anonymizing sensitive data, and ensuring that data handling practices comply with relevant privacy laws and regulations.
    
    
     A privacy-by-design
    
    <a id="_idIndexMarker711">
    </a>
    
     approach ensures that AI systems
    
    <a id="_idIndexMarker712">
    </a>
    
     embed privacy protections at every
    
    <a id="_idIndexMarker713">
    </a>
    
     stage, minimizing risks while complying with laws such as
    
    <strong class="bold">
     
      General Data Protection Regulation
     
    </strong>
    
     (
    
    <strong class="bold">
     
      GDPR
     
    </strong>
    
     ),
    
    <strong class="bold">
     
      California Consumer Privacy Act
     
    </strong>
    
     (
    
    <strong class="bold">
     
      CCPA
     
    </strong>
    
     ), and
    
    <strong class="bold">
     
      Health Insurance Portability and Accountability Act
     
    </strong>
    
     (
    
    <strong class="bold">
     
      HIPAA
     
    </strong>
    
     ).
    
    
     This includes data minimization (collecting only essential information), anonymization (using techniques such
    
    <a id="_idIndexMarker714">
    </a>
    
     as k-anonymity and pseudonymization), and
    
    <strong class="bold">
     
      privacy-preserving machine learning
     
    </strong>
    
     (
    
    <strong class="bold">
     
      PPML
     
    </strong>
    
     ) methods such
    
    <a id="_idIndexMarker715">
    </a>
    
     as federated learning, homomorphic encryption, and
    
    <strong class="bold">
     
      secure multi-party computation
     
    </strong>
    
     (
    
    <strong class="bold">
     
      SMPC
     
    </strong>
    
     ).
    
    
     For example, in a healthcare AI assistant, patient data can be encrypted and processed locally
    
    <a id="_idIndexMarker716">
    </a>
    
     using federated learning, while
    
    <strong class="bold">
     
      role-based access control
     
    </strong>
    
     (
    
    <strong class="bold">
     
      RBAC
     
    </strong>
    
     ) ensures that only authorized personnel can access sensitive data.
    
    
     Additionally, automated audit logs and explainability tools track decisions for accountability.
    
    
     These techniques help organizations deploy AI responsibly, ensuring privacy without
    
    
     
      sacrificing functionality.
     
    
   </p>
   <p>
    
     In the travel industry, this could involve implementing robust data governance frameworks, obtaining explicit consent from users for data collection and usage, and implementing
    
    <a id="_idIndexMarker717">
    </a>
    
     secure data storage and processing mechanisms to protect sensitive information such as travel histories, preferences, and
    
    
     
      payment details.
     
    
   </p>
   <h2 id="_idParaDest-205">
    <a id="_idTextAnchor217">
    </a>
    
     Involvement of diverse stakeholders
    
   </h2>
   <p>
    
     Ethical AI development requires the involvement of diverse stakeholders, including ethicists, technologists, policymakers, and representatives from potentially affected communities.
    
    
     This collaborative approach fosters inclusive dialogue, identifies potential blind spots
    
    <a id="_idIndexMarker718">
    </a>
    
     or unintended consequences, and promotes more equitable and socially responsible approaches to
    
    
     
      AI governance.
     
    
   </p>
   <p>
    
     For instance, in the development of a generative AI system for travel recommendations, engaging with stakeholders from diverse cultural backgrounds, disability rights advocates, and environmental organizations could help identify potential biases, accessibility barriers, or sustainability concerns, leading to more inclusive and
    
    
     
      responsible solutions.
     
    
   </p>
   <p>
    
     By adhering to these ethical guidelines and frameworks, organizations can foster trust, accountability, and responsible innovation in the development and deployment of generative AI technologies.
    
    
     This approach not only mitigates potential risks and unintended consequences but also unlocks the full potential of these powerful technologies to drive positive societal impact while upholding fundamental human rights
    
    
     
      and values.
     
    
   </p>
   <h1 id="_idParaDest-206">
    <a id="_idTextAnchor218">
    </a>
    
     Addressing privacy and security concerns
    
   </h1>
   <p>
    
     As generative AI systems become increasingly prevalent across various domains, addressing
    
    <a id="_idIndexMarker719">
    </a>
    
     privacy and security concerns is of utmost importance.
    
    
     Organizations must take proactive measures to safeguard sensitive data, protect against potential breaches, and ensure the resilience of their AI systems against
    
    
     
      malicious attacks.
     
    
   </p>
   <p>
    
     In the context of a travel agency employing a generative AI system for personalized recommendations and itinerary planning, implementing a comprehensive data governance framework is crucial.
    
    
     This framework should outline data handling practices, access controls, and compliance measures to protect private information within the organization, such as customer travel histories, preferences, and
    
    
     
      payment details.
     
    
   </p>
   <p>
    
     Access controls and role-based permissions can help ensure that only authorized personnel can access and modify sensitive data used for training or generating recommendations.
    
    
     Additionally, adhering to relevant data protection laws and industry-specific
    
    <a id="_idIndexMarker720">
    </a>
    
     regulations, such as the GDPR or the
    
    <strong class="bold">
     
      Payment Card Industry Data Security Standard
     
    </strong>
    
     (
    
    <strong class="bold">
     
      PCI DSS
     
    </strong>
    
     ), is essential to maintain compliance and avoid potential
    
    
     
      legal liabilities.
     
    
   </p>
   <p>
    
     Incorporating security considerations into the AI development life cycle is also vital.
    
    
     This includes conducting regular security risk assessments to identify potential vulnerabilities, implementing secure coding standards to mitigate coding errors or vulnerabilities, and performing regular testing and audits to detect and address any security
    
    <a id="_idIndexMarker721">
    </a>
    
     weaknesses in the AI system.
    
    
     For example, the travel agency could employ penetration testing techniques to simulate potential attack scenarios and assess the resilience of their generative AI system against adversarial attacks or data breaches.
    
    
     This proactive approach can help identify and address security gaps before they are exploited by
    
    
     
      malicious actors.
     
    
   </p>
   <p>
    
     Educating users about the potential risks associated with generative AI and providing training on safe usage practices can empower them to make informed decisions and recognize potential threats.
    
    
     In the travel agency scenario, this could involve educating customers about the importance of safeguarding their personal information, recognizing phishing attempts or suspicious communications, and reporting any concerns or
    
    
     
      incidents promptly.
     
    
   </p>
   <p>
    
     Organizations should also establish robust incident response plans to deal with potential security breaches or data leaks effectively.
    
    
     These plans should outline clear protocols for rapid response, containment, investigation, and mitigation strategies to limit the damage and protect affected individuals
    
    
     
      or entities.
     
    
   </p>
   <p>
    
     In the event of a data breach involving customer information, the travel agency should be prepared to swiftly notify affected individuals, regulatory authorities, and stakeholders, while implementing measures to secure the compromised systems and prevent further
    
    
     
      data loss.
     
    
   </p>
   <p>
    
     Additionally, techniques such as adversarial training and anomaly detection can help improve the resilience of generative AI systems against adversarial attacks specifically.
    
    
     Adversarial training involves exposing the AI model to carefully crafted adversarial examples during the training process, enhancing its ability to recognize and defend against such attacks.
    
    
     Anomaly detection algorithms can identify and flag suspicious or anomalous inputs or outputs, enabling timely intervention and mitigation efforts.
    
    
     By prioritizing
    
    <a id="_idIndexMarker722">
    </a>
    
     privacy and security considerations throughout the AI development and deployment life cycle, organizations can foster trust and confidence in their generative AI systems, while ensuring compliance with relevant regulations and safeguarding sensitive data and
    
    
     
      intellectual property.
     
    
   </p>
   <h1 id="_idParaDest-207">
    <a id="_idTextAnchor219">
    </a>
    
     Summary
    
   </h1>
   <p>
    
     In this chapter, we discovered that while advanced intelligent agentic systems hold immense potential to drive innovation, enhance creativity, and revolutionize various industries, their deployment and development must be approached with utmost care and responsibility.
    
    
     Armed with awareness of the potential risks and challenges associated with generative AI, organizations and stakeholders can proactively implement measures to ensure safety, uphold ethical principles, and address privacy and security concerns.
    
    
     By doing so, they can harness the transformative power of these technologies in a trustworthy and accountable manner, fostering confidence among users and stakeholders.
    
    
     Embracing a proactive and responsible approach to generative AI development involves implementing robust testing and monitoring frameworks, adhering to ethical guidelines and frameworks that prioritize human well-being, accountability, and inclusive governance, and establishing comprehensive data governance and security protocols to safeguard sensitive information and
    
    
     
      intellectual property.
     
    
   </p>
   <p>
    
     It is crucial to address the uncertainties and biases in AI systems.
    
    
     By employing techniques such as probabilistic modeling, uncertainty quantification, and debiasing algorithms, developers can improve the reliability and fairness of generative AI models, fostering trust and responsible adoption.
    
    
     Collaboration among stakeholders, including developers, researchers, policymakers, and ethicists, is essential for navigating the challenges and ethical implications of generative AI.
    
    
     An inclusive, multidisciplinary approach helps identify blind spots, mitigate unintended consequences, and align solutions with human values.
    
    
     Agentic systems heighten AI risks by autonomously acting on biased or compromised information, making robust safety measures, including action boundaries, decision verification, and real-time monitoring, critical.
    
    
     Effective deployment requires balancing autonomy with appropriate human oversight, especially for high-stake decisions.
    
    
     Privacy protection must extend beyond data safeguards to account for the potential exposure of sensitive information through autonomous decisions.
    
    
     Additionally, intellectual property frameworks must evolve to handle AI agents as active creators, with real-time monitoring and verification systems
    
    
     
      in place.
     
    
   </p>
   <p>
    
     In the next chapter, we will explore some of the common use cases and applications of LLM-based intelligent agents using various patterns and techniques that we’ve learned
    
    
     
      so far.
     
    
   </p>
   <h1 id="_idParaDest-208">
    <a id="_idTextAnchor220">
    </a>
    
     Questions
    
   </h1>
   <ol>
    <li>
     
      How do the risks of hallucination differ between generative AI and agentic systems?
     
     
      Why are hallucinations potentially more dangerous in
     
     
      
       agentic systems?
      
     
    </li>
    <li>
     
      What are the core safety considerations that need to be implemented when deploying LLM-based agentic systems, and how do they manifest in a practical example such as a travel
     
     
      
       management system?
      
     
    </li>
    <li>
     
      How does bias in agentic AI systems differ from bias in traditional generative AI systems, and what additional measures are needed to
     
     
      
       address it?
      
     
    </li>
    <li>
     
      What unique challenges do agentic systems pose for data privacy compared to traditional generative AI systems, and how should organizations address
     
     
      
       these challenges?
      
     
    </li>
    <li>
     
      How do intellectual property risks evolve when moving from generative AI to agentic systems, and what new approaches are needed to address
     
     
      
       these risks?
      
     
    </li>
   </ol>
   <h1 id="_idParaDest-209">
    <a id="_idTextAnchor221">
    </a>
    
     Answers
    
   </h1>
   <ol>
    <li value="1">
     
      In generative AI, hallucinations primarily result in incorrect content generation, but in agentic systems, hallucinated information can directly influence real-world decisions and actions.
     
     
      For example, while a generative AI might simply produce incorrect text, an agentic system might execute financial transactions based on hallucinated market trends or make medical decisions based on fabricated patient histories.
     
     
      This is more dangerous because it leads to immediate real-world consequences without
     
     
      
       human verification.
      
     
    </li>
    <li>
     
      Core safety considerations include action boundaries, decision verification, rollback capabilities, real-time monitoring, and performance metrics.
     
     
      In a travel management system, they manifest as financial limits on booking changes, multi-step checks for expensive bookings, mechanisms to handle cancellation policies, tracking of booking patterns for anomalies, and evaluation of decision quality against company policies and traveler preferences.
     
     
      These measures ensure both protection against risks and
     
     
      
       efficient operation.
      
     
    </li>
    <li>
     
      Bias in agentic systems goes beyond generating biased content to actively making biased decisions that affect people’s lives.
     
     
      For example, while a generative AI might produce biased text, an agentic system could systematically discriminate in hiring decisions or resource allocations.
     
     
      Additional measures needed include decision auditing systems, accountability frameworks for autonomous actions, real-time bias detection mechanisms, and regular audits of decision-making patterns and
     
     
      
       action histories.
      
     
    </li>
    <li>
     
      Agentic systems not only process and generate information but actively access, manipulate, and make decisions about personal data during operations.
     
     
      They need enhanced safeguards including real-time privacy monitoring, decision auditing systems, and privacy-aware decision protocols.
     
     
      Organizations must scrutinize how chains of autonomous decisions could reveal sensitive information over time, even when individual actions appear privacy-compliant, and make privacy an integral part of the system’s
     
     
      
       decision-making process.
      
     
    </li>
    <li>
     
      Agentic systems can actively implement learned information and make decisions about intellectual property use at machine speed and scale.
     
     
      For example, they might autonomously incorporate proprietary code across thousands of projects or create complex chains of derivative works.
     
     
      New approaches needed include real-time monitoring systems for content provenance, decision trees for evaluating IP implications before actions, blockchain-based content tracking, and automated license verification systems.
     
     
      Legal frameworks need to evolve to handle machines as active participants in the
     
     
      
       creative process.
      
     
    </li>
   </ol>
   <h1 id="_idParaDest-210">
    <a id="_idTextAnchor222">
    </a>
    
     Join our communities on Discord and Reddit
    
   </h1>
   <p>
    
     Have questions about the book or want to contribute to discussions on Generative AI and LLMs?
    
    
     Join our Discord server at
    
    <a href="https://packt.link/I1tSU">
     
      https://packt.link/I1tSU
     
    </a>
    
     and our Reddit channel at
    
    <a href="https://packt.link/ugMW0">
     
      https://packt.link/ugMW0
     
    </a>
    
     to connect, share, and collaborate with
    
    
     
      like-minded enthusiasts.
     
    
   </p>
   <div><div><img alt="img" role="presentation" src="img/B31483_Discord_QR_new.jpg"/>
     
    </div>
   </div>
   <p>
   </p>
   <div><div><img alt="img" role="presentation" src="img/qrcode_Reddit_Channel.jpg"/>
     
    </div>
   </div>
  </div>
 </body></html>