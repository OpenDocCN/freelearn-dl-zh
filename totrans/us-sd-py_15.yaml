- en: '15'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generating Image Descriptions Using BLIP-2 and LLaVA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine you have an image in hand and need to upscale it or generate new images
    based on it, but you don’t have the prompt or description associated with it.
    You may say, *“Fine, I can write up a new prompt for it.”* For one image, that
    is acceptable, what if there are thousands or even millions of images without
    descriptions? It is impossible to write them all up manually.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, we can use artificial intelligence (AI) to help us generate descriptions.
    There are many pretrained models that can achieve this goal, and the number is
    always increasing. In this chapter, I am going to introduce two AI solutions to
    generate the caption, description, or prompt for an image, all fully automated:'
  prefs: []
  type: TYPE_NORMAL
- en: 'BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders
    and Large Language Models [1]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LLaVA: Large Language and Vision Assistant [3]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BLIP-2 [1] is fast and requires relatively low hardware, while LLaVA [3] (with
    its `llava-v1.5-13b` model) is the newest and most powerful model at the time
    of writing.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Generally understand how BLIP-2 and LLaVA work
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Write up Python code to use BLIP-2 and LLaVA to generate descriptions from images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before diving into the BLIP-2 and LLaVA, let’s use Stable Diffusion to generate
    an image for testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, load up a `deliberate-v2` model without sending it to CUDA:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, in the following code, we first send the model to CUDA and generate an
    image, then we offload the model to CPU RAM, and clear the model out from CUDA:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will give us an image similar to that shown in the following
    figure, which will be used in the following sections:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.1: An image of an astronaut riding a horse, generated by SD v1.5](img/B21263_15_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.1: An image of an astronaut riding a horse, generated by SD v1.5'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s get started.
  prefs: []
  type: TYPE_NORMAL
- en: BLIP-2 – Bootstrapping Language-Image Pre-training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the *BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language
    Understanding and Generation* paper [4], Junnan Li et al. proposed a solution
    to bridge the gap between natural language and vision modalities. Notably, the
    BLIP model has demonstrated exceptional capabilities in generating high-quality
    image descriptions, surpassing existing benchmarks at the time of its publication.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The reason behind its excellent quality is that Junnan Li et al. used an innovative
    technique to build two models from their first pretrained model:'
  prefs: []
  type: TYPE_NORMAL
- en: Filter model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Captioner model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The filter model can filter out low-quality text-image pairs, thus improving
    the training data quality, while its caption generation model can generate surprisingly
    good, short descriptions for the image. With the help of these two models, the
    authors of the paper not only improved the training data quality but also enlarged
    its size automatically. Then, they used the boosted train data to train the BLIP
    model again and the result was impressively good. But this was the story of 2022.
  prefs: []
  type: TYPE_NORMAL
- en: In June 2023, the same team from Salesforce brought out the new BLIP-2.
  prefs: []
  type: TYPE_NORMAL
- en: How BLIP-2 works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'BLIP was good at the time, but the language part of its model was still relatively
    weak. **Large language models** (**LLMs**) such as OpenAI’s GPT and Meta’s LLaMA
    are powerful, but also extremely expensive to train. So the BLIP team framed the
    challenge by asking themselves: can we take off-the-shelf, pretrained frozen image
    encoders and frozen LLMs and use them for vision language pretraining while still
    preserving their learned representations?'
  prefs: []
  type: TYPE_NORMAL
- en: The answer is yes. BLIP-2 solved this by introducing a Query Transformer that
    helps generate the visual representations corresponding to a text caption, which
    then is fed to a frozen LLM to decode text descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: The Query Transformer, often referred to as the Q-Former [2], is a crucial component
    of the BLIP-2 model. It serves as a bridge connecting the frozen image encoder
    and the frozen LLM. The primary function of the Q-Former is to map a set of “query
    tokens” to query embeddings. These query embeddings help in extracting visual
    features from the image encoder that are most pertinent to the given text instruction.
  prefs: []
  type: TYPE_NORMAL
- en: During the training process of the BLIP-2 model, the weights of the image encoder
    and the LLM remain frozen. Meanwhile, the Q-Former undergoes training, allowing
    it to adapt and optimize its performance based on the specific task requirements.
    By employing a set of learnable query vectors, the Q-Former effectively distills
    valuable information from the image encoder, making it possible for the LLM to
    generate accurate and contextually appropriate responses grounded in visual content.
  prefs: []
  type: TYPE_NORMAL
- en: A similar concept is also employed in LLaVA, which we will discuss later. The
    core idea of BLIP is to reuse the effective vision and language components, and
    only train a middle model to bridge them together.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s start using BLIP-2.
  prefs: []
  type: TYPE_NORMAL
- en: Using BLIP-2 to generate descriptions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Using BLIP-2 is easy and clean with the help of the Hugging Face transformers
    [5] package. If you don’t have the package installed, simply run the following
    command to install or update it to the newest version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then load up the BLIP-2 model data with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Your first run will automatically download the model weights data from the
    Hugging Face model repository. It may take some time, so please be patient. Once
    the downloading is finished, run the following code to ask BLIP-2 about the image
    we provide:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The code returns the description `astronaut on horseback in space`, which is
    good and accurate. What if we ask how many planets are in the background? Let’s
    change the prompt to `how many planets in the background:`. And it returns `the
    universe is bigger than you think`. Not good enough this time.
  prefs: []
  type: TYPE_NORMAL
- en: So, BLIP-2 is good at generating short descriptions of an entire image quickly.
    However, to generate more detailed descriptions or even interact with images,
    we can leverage the power of LLaVA.
  prefs: []
  type: TYPE_NORMAL
- en: LLaVA – Large Language and Vision Assistant
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As suggested by its name **LLaVA** [3], this model is very close to LLaMA, not
    only in name but also in terms of their internals. LLaVA uses LLaMA as its language
    part. making it possible to swap out the language model if needed This is definitely
    a killer feature for many scenarios. One of the key features of Stable Diffusion
    is its openness for model swapping and fine-tuning. Similar to Stable Diffusion,
    LLaVA is designed to leverage open-sourced LLM models.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s take a look at how LLaVA works.
  prefs: []
  type: TYPE_NORMAL
- en: How LLaVA works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The LLaVA authors, Haotian Liu et al. [3], present a beautiful, accurate diagram
    showing how the model leverages pretrained CLIP and LLaMA models in its architecture,
    as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.2: Architecture of LLaVA](img/B21263_15_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.2: Architecture of LLaVA'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s read the diagram from the bottom up. During the inference, we provide
    an image denoted as X v, and a language instruction denoted as X q. The vision
    encoder is the CLIP vision encoder ViT-L/14 [6]. The same CLIP is used by Stable
    Diffusion v1.5 as its text encoder.
  prefs: []
  type: TYPE_NORMAL
- en: 'The CLIP model encodes the image to Z v, and the projection W is the model
    data provided by LLaVA. The projection model projects the encoded image embedding
    Z v to H v as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: H v = W ⋅ Z v
  prefs: []
  type: TYPE_NORMAL
- en: On the other side, the language instruction is encoded into CLIP’s 512-dimensional
    embedding chunks. Both the image and language embeddings share the same dimensionality.
  prefs: []
  type: TYPE_NORMAL
- en: In this manner, the language model f ϕ is aware of both the image and the language!
    This method bears some similarity to Stable Diffusion’s textual inversion technique,
    being lightweight yet powerful.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s write some code to instruct LLaVA to interact with an image.
  prefs: []
  type: TYPE_NORMAL
- en: Installing LLaVA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For an optimal experience, it is strongly advised to use LLaVA on a Linux machine.
    Using it on Windows may result in unexpected missing components. It is also suggested
    to establish a Python virtual environment for using LLaVA. Detailed steps and
    commands for setting up a Python virtual environment were provided in [*Chapter
    2*](B21263_02.xhtml#_idTextAnchor037).
  prefs: []
  type: TYPE_NORMAL
- en: 'Clone the LLaVA repository to your local folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Then install LLaVA by simply running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, download the model file from the Hugging Face model repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Please note that the model file is large and the download will take some time.
    At the time of writing this chapter, you can also download the 13B model by simply
    changing the `7` to `13` in the URL in the preceding code snippet to use the 13B
    LLaVA model.
  prefs: []
  type: TYPE_NORMAL
- en: That’s it for the setup. Now, let’s proceed to writing the Python code.
  prefs: []
  type: TYPE_NORMAL
- en: Using LLaVA to generate image descriptions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since we just installed LLaVA, we can reference the following related modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the `tokenizer`, `image_processor`, and `model` components. `tokenizer`
    will convert text to token IDs, `image_processor` will convert images to tensors,
    and `model` is the pipeline that we will use to generate the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is a breakdown of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '`model_path`: This path points to the folder storing the pretrained models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model_base`: This is set to `None`, meaning no specific parent architecture
    has been specified.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model_name`: The name of the pretrained model (`llava-v1.5`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`load_4bit`: If set to `True`, this enables 4-bit quantization during inferencing.
    This reduces memory usage and boosts speed, but might negatively impact the quality
    of results slightly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device`: This specifies CUDA as the device where computations should occur.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device_map`: This is used to map GPU devices to different parts of the model
    if you want to distribute the workload across multiple GPUs. Since only one device
    is mapped here, it implies a single GPU execution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let’s create the image descriptions:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a `conv` object to hold the conversation history:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert the image to a tensor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Append an image placeholder to the conversation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Get the prompt and convert it to tokens for inference:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Prepare the stopping criteria:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, get the output from LLaVA:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As we can see in the following output, LLaVA can generate amazing descriptions:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: I have attempted to condense the code as much as possible, but it remains lengthy.
    It requires careful copying or transcription into your code editor. I recommend
    copy-pasting the code provided in the repository that accompanies this book. You
    can execute the aforementioned code in a single cell to observe how effectively
    LLaVA generates descriptions from an image.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, our primary focus was on two AI solutions designed to generate
    image descriptions. The first is BLIP-2, an effective and efficient solution for
    generating concise captions for images. The second is the LLaVA solution, which
    is capable of generating more detailed and accurate descriptive information from
    an image.
  prefs: []
  type: TYPE_NORMAL
- en: With the assistance of LLaVA, we can even interact with an image to extract
    further information from it.
  prefs: []
  type: TYPE_NORMAL
- en: The integration of vision and language capabilities also lays the groundwork
    for the development of even more powerful multimodal models, the potential of
    which we can only begin to imagine.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, let’s get started using Stable Diffusion XL.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, *BLIP-2: Bootstrapping Language-Image
    Pre-training with Frozen Image Encoders and Large Language* *Models*: [https://arxiv.org/abs/2301.12597](https://arxiv.org/abs/2301.12597)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'BLIP-2 Hugging Face documentation: [https://huggingface.co/docs/transformers/main/model_doc/blip-2](https://huggingface.co/docs/transformers/main/model_doc/blip-2)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, *LLaVA: Large Language
    and Vision* *Assistant*: [https://llava-vl.github.io/](https://llava-vl.github.io/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi, *BLIP: Bootstrapping Language-Image
    Pre-training for Unified Vision-Language Understanding and* *Generation*: [https://arxiv.org/abs/2201.12086](https://arxiv.org/abs/2201.12086)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Hugging Face Transformers GitHub repository: [https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini
    Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger,
    Ilya Sutskever, *Learning transferable visual models from natural language* *supervision*:
    [https://arxiv.org/abs/2103.00020](https://arxiv.org/abs/2103.00020)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'LLaVA GitHub repository: [https://github.com/haotian-liu/LLaVA](https://github.com/haotian-liu/LLaVA)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
