- en: '15'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '15'
- en: Generating Image Descriptions Using BLIP-2 and LLaVA
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用BLIP-2和LLaVA生成图像描述
- en: Imagine you have an image in hand and need to upscale it or generate new images
    based on it, but you don’t have the prompt or description associated with it.
    You may say, *“Fine, I can write up a new prompt for it.”* For one image, that
    is acceptable, what if there are thousands or even millions of images without
    descriptions? It is impossible to write them all up manually.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你手中有一张图像，需要将其放大或基于它生成新的图像，但你没有与之相关的提示或描述。你可能会说，“好吧，我可以为它编写一个新的提示。”对于一张图像来说，这是可以接受的，但如果有成千上万甚至数百万张没有描述的图像呢？手动编写它们是不可能的。
- en: 'Fortunately, we can use artificial intelligence (AI) to help us generate descriptions.
    There are many pretrained models that can achieve this goal, and the number is
    always increasing. In this chapter, I am going to introduce two AI solutions to
    generate the caption, description, or prompt for an image, all fully automated:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们可以使用人工智能（AI）来帮助我们生成描述。有许多预训练模型可以实现这一目标，而且数量总是在增加。在本章中，我将介绍两种AI解决方案来生成图像的标题、描述或提示，全部自动化：
- en: 'BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders
    and Large Language Models [1]'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BLIP-2：使用冻结图像编码器和大型语言模型进行语言-图像预训练的引导 [1]
- en: 'LLaVA: Large Language and Vision Assistant [3]'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLaVA：大型语言和视觉助手 [3]
- en: BLIP-2 [1] is fast and requires relatively low hardware, while LLaVA [3] (with
    its `llava-v1.5-13b` model) is the newest and most powerful model at the time
    of writing.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: BLIP-2 [1] 运行速度快，对硬件要求相对较低，而LLaVA [3]（其`llava-v1.5-13b`模型）在撰写本文时是最新的、功能最强大的模型。
- en: 'By the end of this chapter, you will be able to do the following:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将能够做到以下事情：
- en: Generally understand how BLIP-2 and LLaVA work
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常了解BLIP-2和LLaVA的工作原理
- en: Write up Python code to use BLIP-2 and LLaVA to generate descriptions from images
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写Python代码以使用BLIP-2和LLaVA从图像生成描述
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: Before diving into the BLIP-2 and LLaVA, let’s use Stable Diffusion to generate
    an image for testing.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨BLIP-2和LLaVA之前，让我们使用Stable Diffusion生成一张用于测试的图像。
- en: 'First, load up a `deliberate-v2` model without sending it to CUDA:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，加载一个`deliberate-v2`模型，但不将其发送到CUDA：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, in the following code, we first send the model to CUDA and generate an
    image, then we offload the model to CPU RAM, and clear the model out from CUDA:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在以下代码中，我们首先将模型发送到CUDA并生成图像，然后我们将模型卸载到CPU RAM中，并从CUDA中清除模型：
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The preceding code will give us an image similar to that shown in the following
    figure, which will be used in the following sections:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将为我们生成与以下图中类似的图像，该图像将在以下章节中使用：
- en: '![Figure 15.1: An image of an astronaut riding a horse, generated by SD v1.5](img/B21263_15_01.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图15.1：由SD v1.5生成的宇航员骑马的图像](img/B21263_15_01.jpg)'
- en: 'Figure 15.1: An image of an astronaut riding a horse, generated by SD v1.5'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.1：由SD v1.5生成的宇航员骑马的图像
- en: Now, let’s get started.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始吧。
- en: BLIP-2 – Bootstrapping Language-Image Pre-training
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BLIP-2 – 语言-图像预训练的引导
- en: 'In the *BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language
    Understanding and Generation* paper [4], Junnan Li et al. proposed a solution
    to bridge the gap between natural language and vision modalities. Notably, the
    BLIP model has demonstrated exceptional capabilities in generating high-quality
    image descriptions, surpassing existing benchmarks at the time of its publication.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在*BLIP：为统一视觉-语言理解和生成进行语言-图像预训练的引导*论文[4]中，李俊南等人提出了一种弥合自然语言和视觉模态之间差距的解决方案。值得注意的是，BLIP模型在生成高质量图像描述方面表现出卓越的能力，在发布时超越了现有的基准。
- en: 'The reason behind its excellent quality is that Junnan Li et al. used an innovative
    technique to build two models from their first pretrained model:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 其高质量背后的原因是李俊南等人使用了一种创新的技术，从他们的第一个预训练模型中构建了两个模型：
- en: Filter model
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过滤模型
- en: Captioner model
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标题生成模型
- en: The filter model can filter out low-quality text-image pairs, thus improving
    the training data quality, while its caption generation model can generate surprisingly
    good, short descriptions for the image. With the help of these two models, the
    authors of the paper not only improved the training data quality but also enlarged
    its size automatically. Then, they used the boosted train data to train the BLIP
    model again and the result was impressively good. But this was the story of 2022.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤模型可以过滤掉低质量的文本-图像对，从而提高训练数据质量，同时其标题生成模型可以为图像生成令人惊讶的好、简短的描述。借助这两个模型，论文的作者不仅提高了训练数据质量，还自动扩大了其规模。然后，他们使用增强的训练数据重新训练
    BLIP 模型，结果令人印象深刻。但这是 2022 年的故事。
- en: In June 2023, the same team from Salesforce brought out the new BLIP-2.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 到 2023 年 6 月，Salesforce 的同一团队推出了新的 BLIP-2。
- en: How BLIP-2 works
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BLIP-2 的工作原理
- en: 'BLIP was good at the time, but the language part of its model was still relatively
    weak. **Large language models** (**LLMs**) such as OpenAI’s GPT and Meta’s LLaMA
    are powerful, but also extremely expensive to train. So the BLIP team framed the
    challenge by asking themselves: can we take off-the-shelf, pretrained frozen image
    encoders and frozen LLMs and use them for vision language pretraining while still
    preserving their learned representations?'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在当时，BLIP 表现良好，但其模型的语言部分仍然相对较弱。**大型语言模型**（**LLMs**）如 OpenAI 的 GPT 和 Meta 的 LLaMA
    强大，但训练成本极高。因此，BLIP 团队通过问自己提出了挑战：我们能否使用现成的、预训练的冻结图像编码器和冻结 LLM 进行视觉语言预训练，同时保留它们学习到的表示？
- en: The answer is yes. BLIP-2 solved this by introducing a Query Transformer that
    helps generate the visual representations corresponding to a text caption, which
    then is fed to a frozen LLM to decode text descriptions.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是肯定的。BLIP-2 通过引入查询转换器来解决这个问题，该转换器有助于生成与文本标题对应的视觉表示，然后将其输入到冻结的 LLM 中以解码文本描述。
- en: The Query Transformer, often referred to as the Q-Former [2], is a crucial component
    of the BLIP-2 model. It serves as a bridge connecting the frozen image encoder
    and the frozen LLM. The primary function of the Q-Former is to map a set of “query
    tokens” to query embeddings. These query embeddings help in extracting visual
    features from the image encoder that are most pertinent to the given text instruction.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 查询转换器，通常被称为 Q-Former [2]，是 BLIP-2 模型的一个关键组件。它充当连接冻结图像编码器和冻结语言大模型（LLM）的桥梁。Q-Former
    的主要功能是将一组“查询标记”映射到查询嵌入。这些查询嵌入有助于从图像编码器中提取与给定文本指令最相关的视觉特征。
- en: During the training process of the BLIP-2 model, the weights of the image encoder
    and the LLM remain frozen. Meanwhile, the Q-Former undergoes training, allowing
    it to adapt and optimize its performance based on the specific task requirements.
    By employing a set of learnable query vectors, the Q-Former effectively distills
    valuable information from the image encoder, making it possible for the LLM to
    generate accurate and contextually appropriate responses grounded in visual content.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在 BLIP-2 模型的训练过程中，图像编码器和 LLM 的权重保持冻结。同时，Q-Former 进行训练，使其能够根据特定任务要求进行适应和优化性能。通过使用一组可学习的查询向量，Q-Former
    有效地从图像编码器中提炼出有价值的信息，使得语言大模型（LLM）能够基于视觉内容生成准确且上下文适当的响应。
- en: A similar concept is also employed in LLaVA, which we will discuss later. The
    core idea of BLIP is to reuse the effective vision and language components, and
    only train a middle model to bridge them together.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 类似的概念也应用于 LLaVA，我们将在后面讨论。BLIP 的核心思想是重用有效的视觉和语言组件，并且只训练一个中间模型来将它们连接起来。
- en: Next, let’s start using BLIP-2.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们开始使用 BLIP-2。
- en: Using BLIP-2 to generate descriptions
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 BLIP-2 生成描述
- en: 'Using BLIP-2 is easy and clean with the help of the Hugging Face transformers
    [5] package. If you don’t have the package installed, simply run the following
    command to install or update it to the newest version:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Hugging Face transformers [5] 包的帮助下，使用 BLIP-2 既简单又干净。如果您还没有安装该包，只需运行以下命令来安装或更新到最新版本：
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then load up the BLIP-2 model data with the following code:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用以下代码加载 BLIP-2 模型数据：
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Your first run will automatically download the model weights data from the
    Hugging Face model repository. It may take some time, so please be patient. Once
    the downloading is finished, run the following code to ask BLIP-2 about the image
    we provide:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 您的第一次运行将自动从 Hugging Face 模型存储库下载模型权重数据。这可能需要一些时间，所以请耐心等待。下载完成后，运行以下代码以询问 BLIP-2
    我们提供的图像：
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The code returns the description `astronaut on horseback in space`, which is
    good and accurate. What if we ask how many planets are in the background? Let’s
    change the prompt to `how many planets in the background:`. And it returns `the
    universe is bigger than you think`. Not good enough this time.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 该代码返回描述“太空中的骑马宇航员”，这是好的且准确的。如果我们问背景中有多少颗行星呢？让我们将提示改为“背景中有多少颗行星：”。然后它返回“宇宙比你想象的要大”。这次不够好。
- en: So, BLIP-2 is good at generating short descriptions of an entire image quickly.
    However, to generate more detailed descriptions or even interact with images,
    we can leverage the power of LLaVA.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，BLIP-2擅长快速生成整个图像的简短描述。然而，要生成更详细的描述或甚至与图像交互，我们可以利用LLaVA的力量。
- en: LLaVA – Large Language and Vision Assistant
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLaVA – 大型语言和视觉助手
- en: As suggested by its name **LLaVA** [3], this model is very close to LLaMA, not
    only in name but also in terms of their internals. LLaVA uses LLaMA as its language
    part. making it possible to swap out the language model if needed This is definitely
    a killer feature for many scenarios. One of the key features of Stable Diffusion
    is its openness for model swapping and fine-tuning. Similar to Stable Diffusion,
    LLaVA is designed to leverage open-sourced LLM models.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名称**LLaVA**[3]所示，这个模型不仅在名称上，而且在内部结构上与LLaMA非常接近。LLaVA使用LLaMA作为其语言部分，这使得在需要时可以更换语言模型。这绝对是一个许多场景的杀手特性。Stable
    Diffusion的一个关键特性是其对模型更换和微调的开放性。与Stable Diffusion类似，LLaVA被设计为利用开源的LLM模型。
- en: Next, let’s take a look at how LLaVA works.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看LLaVA是如何工作的。
- en: How LLaVA works
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLaVA的工作原理
- en: 'The LLaVA authors, Haotian Liu et al. [3], present a beautiful, accurate diagram
    showing how the model leverages pretrained CLIP and LLaMA models in its architecture,
    as shown in the following figure:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: LLaVA的作者，刘浩天等人[3]，展示了一个精美、准确的图表，展示了该模型如何在架构中利用预训练的CLIP和LLaMA模型，如下所示：
- en: '![Figure 15.2: Architecture of LLaVA](img/B21263_15_02.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图15.2：LLaVA的架构](img/B21263_15_02.jpg)'
- en: 'Figure 15.2: Architecture of LLaVA'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.2：LLaVA的架构
- en: Let’s read the diagram from the bottom up. During the inference, we provide
    an image denoted as X v, and a language instruction denoted as X q. The vision
    encoder is the CLIP vision encoder ViT-L/14 [6]. The same CLIP is used by Stable
    Diffusion v1.5 as its text encoder.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从下往上阅读这个图表。在推理过程中，我们提供一个表示为Xv的图像和一个表示为Xq的语言指令。视觉编码器是CLIP视觉编码器ViT-L/14[6]。与Stable
    Diffusion v1.5一样，CLIP也用作其文本编码器。
- en: 'The CLIP model encodes the image to Z v, and the projection W is the model
    data provided by LLaVA. The projection model projects the encoded image embedding
    Z v to H v as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP模型将图像编码为Zv，投影W是LLaVA提供的模型数据。投影模型将编码后的图像嵌入Zv投影到 Hv 如下：
- en: H v = W ⋅ Z v
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Hv = W ⋅ Zv
- en: On the other side, the language instruction is encoded into CLIP’s 512-dimensional
    embedding chunks. Both the image and language embeddings share the same dimensionality.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一方面，语言指令被编码为CLIP的512维嵌入块。图像和语言嵌入具有相同的维度性。
- en: In this manner, the language model f ϕ is aware of both the image and the language!
    This method bears some similarity to Stable Diffusion’s textual inversion technique,
    being lightweight yet powerful.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式，语言模型fϕ既了解图像又了解语言！这种方法与Stable Diffusion的文本反转技术有一些相似之处，既轻量又强大。
- en: Next, let’s write some code to instruct LLaVA to interact with an image.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们编写一些代码来指导LLaVA与图像交互。
- en: Installing LLaVA
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装LLaVA
- en: For an optimal experience, it is strongly advised to use LLaVA on a Linux machine.
    Using it on Windows may result in unexpected missing components. It is also suggested
    to establish a Python virtual environment for using LLaVA. Detailed steps and
    commands for setting up a Python virtual environment were provided in [*Chapter
    2*](B21263_02.xhtml#_idTextAnchor037).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得最佳体验，强烈建议在Linux机器上使用LLaVA。在Windows上使用可能会导致意外缺失组件。还建议为使用LLaVA建立Python虚拟环境。在[*第2章*](B21263_02.xhtml#_idTextAnchor037)中提供了设置Python虚拟环境的详细步骤和命令。
- en: 'Clone the LLaVA repository to your local folder:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 将LLaVA仓库克隆到本地文件夹：
- en: '[PRE5]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Then install LLaVA by simply running the following command:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 然后通过运行以下命令安装LLaVA：
- en: '[PRE6]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, download the model file from the Hugging Face model repository:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，从Hugging Face模型仓库下载模型文件：
- en: '[PRE7]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Please note that the model file is large and the download will take some time.
    At the time of writing this chapter, you can also download the 13B model by simply
    changing the `7` to `13` in the URL in the preceding code snippet to use the 13B
    LLaVA model.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，模型文件很大，下载需要一些时间。在编写本章时，您也可以通过将前面代码片段中的 `7` 更改为 `13` 来下载 13B 模型，以使用 13B LLaVA
    模型。
- en: That’s it for the setup. Now, let’s proceed to writing the Python code.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 设置到此结束。现在，让我们继续编写 Python 代码。
- en: Using LLaVA to generate image descriptions
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 LLaVA 生成图像描述
- en: 'Since we just installed LLaVA, we can reference the following related modules:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们刚刚安装了 LLaVA，我们可以参考以下相关模块：
- en: '[PRE8]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Load the `tokenizer`, `image_processor`, and `model` components. `tokenizer`
    will convert text to token IDs, `image_processor` will convert images to tensors,
    and `model` is the pipeline that we will use to generate the output:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 加载 `tokenizer`、`image_processor` 和 `model` 组件。`tokenizer` 将文本转换为标记 ID，`image_processor`
    将图像转换为张量，而 `model` 是我们将用于生成输出的管道：
- en: '[PRE9]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The following is a breakdown of the preceding code:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是对前面代码的分解：
- en: '`model_path`: This path points to the folder storing the pretrained models.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_path`：此路径指向存储预训练模型的文件夹。'
- en: '`model_base`: This is set to `None`, meaning no specific parent architecture
    has been specified.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_base`：此参数设置为 `None`，表示未指定特定的父架构。'
- en: '`model_name`: The name of the pretrained model (`llava-v1.5`).'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_name`：预训练模型的名称（`llava-v1.5`）。'
- en: '`load_4bit`: If set to `True`, this enables 4-bit quantization during inferencing.
    This reduces memory usage and boosts speed, but might negatively impact the quality
    of results slightly.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`load_4bit`：如果设置为 `True`，则在推理过程中启用 4 位量化。这可以减少内存使用并提高速度，但可能会略微影响结果的品质。'
- en: '`device`: This specifies CUDA as the device where computations should occur.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device`：此参数指定 CUDA 作为计算发生的设备。'
- en: '`device_map`: This is used to map GPU devices to different parts of the model
    if you want to distribute the workload across multiple GPUs. Since only one device
    is mapped here, it implies a single GPU execution.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device_map`：此参数用于将 GPU 设备映射到模型的各个部分，如果您想将工作负载分配到多个 GPU 上。由于此处只映射了一个设备，这意味着将执行单
    GPU 运行。'
- en: 'Now, let’s create the image descriptions:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建图像描述：
- en: 'Create a `conv` object to hold the conversation history:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个 `conv` 对象以保存对话历史：
- en: '[PRE10]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Convert the image to a tensor:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像转换为张量：
- en: '[PRE13]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Append an image placeholder to the conversation:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像占位符添加到对话中：
- en: '[PRE19]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Get the prompt and convert it to tokens for inference:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取提示并将其转换为推理的标记：
- en: '[PRE25]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Prepare the stopping criteria:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备停止条件：
- en: '[PRE35]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Finally, get the output from LLaVA:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，从 LLaVA 获取输出：
- en: '[PRE40]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'As we can see in the following output, LLaVA can generate amazing descriptions:'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如以下输出所示，LLaVA 可以生成令人惊叹的描述：
- en: '[PRE57]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: I have attempted to condense the code as much as possible, but it remains lengthy.
    It requires careful copying or transcription into your code editor. I recommend
    copy-pasting the code provided in the repository that accompanies this book. You
    can execute the aforementioned code in a single cell to observe how effectively
    LLaVA generates descriptions from an image.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经尽可能地精简了代码，但它仍然很长。它需要仔细复制或转录到您的代码编辑器中。我建议复制粘贴此书附带的存储库中提供的代码。您可以在单个单元格中执行上述代码，以观察
    LLaVA 从图像中生成描述的有效性。
- en: Summary
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, our primary focus was on two AI solutions designed to generate
    image descriptions. The first is BLIP-2, an effective and efficient solution for
    generating concise captions for images. The second is the LLaVA solution, which
    is capable of generating more detailed and accurate descriptive information from
    an image.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们的主要重点是两种旨在生成图像描述的 AI 解决方案。第一个是 BLIP-2，这是一种生成图像简洁标题的有效且高效的解决方案。第二个是 LLaVA
    解决方案，它能够从图像中生成更详细和准确的描述信息。
- en: With the assistance of LLaVA, we can even interact with an image to extract
    further information from it.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LLaVA 的帮助下，我们甚至可以与图像互动以从中提取更多信息。
- en: The integration of vision and language capabilities also lays the groundwork
    for the development of even more powerful multimodal models, the potential of
    which we can only begin to imagine.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉和语言能力的集成也为开发更强大的多模态模型奠定了基础，其潜力我们只能开始想象。
- en: In the next chapter, let’s get started using Stable Diffusion XL.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将开始使用 Stable Diffusion XL。
- en: References
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, *BLIP-2: Bootstrapping Language-Image
    Pre-training with Frozen Image Encoders and Large Language* *Models*: [https://arxiv.org/abs/2301.12597](https://arxiv.org/abs/2301.12597)'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 李俊南，李东旭，西尔维奥·萨瓦雷塞，何思婷，*BLIP-2：使用冻结图像编码器和大型语言模型进行语言-图像预训练的引导*：[https://arxiv.org/abs/2301.12597](https://arxiv.org/abs/2301.12597)
- en: 'BLIP-2 Hugging Face documentation: [https://huggingface.co/docs/transformers/main/model_doc/blip-2](https://huggingface.co/docs/transformers/main/model_doc/blip-2)'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: BLIP-2 Hugging Face 文档：[https://huggingface.co/docs/transformers/main/model_doc/blip-2](https://huggingface.co/docs/transformers/main/model_doc/blip-2)
- en: 'Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, *LLaVA: Large Language
    and Vision* *Assistant*: [https://llava-vl.github.io/](https://llava-vl.github.io/)'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 刘浩天，李春远，吴庆阳，李庸宰，*LLaVA：大型语言和视觉* 助手：[https://llava-vl.github.io/](https://llava-vl.github.io/)
- en: 'Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi, *BLIP: Bootstrapping Language-Image
    Pre-training for Unified Vision-Language Understanding and* *Generation*: [https://arxiv.org/abs/2201.12086](https://arxiv.org/abs/2201.12086)'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 李俊南，李东旭，熊才明，何思婷，*BLIP：为统一视觉语言理解和生成进行语言-图像预训练的引导*：[https://arxiv.org/abs/2201.12086](https://arxiv.org/abs/2201.12086)
- en: 'Hugging Face Transformers GitHub repository: [https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hugging Face Transformers GitHub 仓库：[https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)
- en: 'Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini
    Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger,
    Ilya Sutskever, *Learning transferable visual models from natural language* *supervision*:
    [https://arxiv.org/abs/2103.00020](https://arxiv.org/abs/2103.00020)'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Alec Radford，金钟宇，克里斯·霍拉西，阿迪亚·拉梅什，加布里埃尔·高，桑迪尼·阿加瓦尔，吉里什·萨斯特里，阿曼达·阿斯凯尔，帕梅拉·米什金，杰克·克拉克，格雷琴·克鲁格，伊利亚·苏茨克维，*从自然语言监督中学习可迁移的视觉模型*：[https://arxiv.org/abs/2103.00020](https://arxiv.org/abs/2103.00020)
- en: 'LLaVA GitHub repository: [https://github.com/haotian-liu/LLaVA](https://github.com/haotian-liu/LLaVA)'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LLaVA GitHub 仓库：[https://github.com/haotian-liu/LLaVA](https://github.com/haotian-liu/LLaVA)
