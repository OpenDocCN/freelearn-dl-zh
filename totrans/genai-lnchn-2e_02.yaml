- en: '1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Rise of Generative AI: From Language Models to Agents'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The gap between experimental and production-ready agents is stark. According
    to LangChain’s State of Agents report, performance quality is the #1 concern among
    51% of companies using agents, yet only 39.8% have implemented proper evaluation
    systems. Our book bridges this gap on two fronts: first, by demonstrating how
    LangChain and LangSmith provide robust testing and observability solutions; second,
    by showing how LangGraph’s state management enables complex, reliable multi-agent
    systems. You’ll find production-tested code patterns that leverage each tool’s
    strengths for enterprise-scale implementation and extend basic RAG into robust
    knowledge systems.'
  prefs: []
  type: TYPE_NORMAL
- en: LangChain accelerates time-to-market with readily available building blocks,
    unified vendor APIs, and detailed tutorials. Furthermore, LangChain and LangSmith
    debugging and tracing functionalities simplify the analysis of complex agent behavior.
    Finally, LangGraph has excelled in executing its philosophy behind agentic AI
    – it allows a developer to give a **large language model** (**LLM**) partial control
    flow over the workflow (and to manage the level of how much control an LLM should
    have), while still making agentic workflows reliable and well-performant.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll explore how LLMs have evolved into the foundation for
    agentic AI systems and how frameworks like LangChain and LangGraph transform these
    models into production-ready applications. We’ll also examine the modern LLM landscape,
    understand the limitations of raw LLMs, and introduce the core concepts of agentic
    applications that form the basis for the hands-on development we’ll tackle throughout
    this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a nutshell, the following topics will be covered in this book:'
  prefs: []
  type: TYPE_NORMAL
- en: The modern LLM landscape
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From models to agentic applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing LangChain
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The modern LLM landscape
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Artificial intelligence** (**AI**) has long been a subject of fascination
    and research, but recent advancements in generative AI have propelled it into
    mainstream adoption. Unlike traditional AI systems that classify data or make
    predictions, generative AI can create new content—text, images, code, and more—by
    leveraging vast amounts of training data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The generative AI revolution was catalyzed by the 2017 introduction of the
    transformer architecture, which enabled models to process text with unprecedented
    understanding of context and relationships. As researchers scaled these models
    from millions to billions of parameters, they discovered something remarkable:
    larger models didn’t just perform incrementally better—they exhibited entirely
    new emergent capabilities like few-shot learning, complex reasoning, and creative
    generation that weren’t explicitly programmed. Eventually, the release of ChatGPT
    in 2022 marked a turning point, demonstrating these capabilities to the public
    and sparking widespread adoption.'
  prefs: []
  type: TYPE_NORMAL
- en: The landscape shifted again with the open-source revolution led by models like
    Llama and Mistral, democratizing access to powerful AI beyond the major tech companies.
    However, these advanced capabilities came with significant limitations—models
    couldn’t reliably use tools, reason through complex problems, or maintain context
    across interactions. This gap between raw model power and practical utility created
    the need for specialized frameworks like LangChain that transform these models
    from impressive text generators into functional, production-ready agents capable
    of solving real-world problems.
  prefs: []
  type: TYPE_NORMAL
- en: '**Key terminologies**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Tools**: External utilities or functions that AI models can use to interact
    with the world. Tools allow agents to perform actions like searching the web,
    calculating values, or accessing databases to overcome LLMs’ inherent limitations.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Memory**: Systems that allow AI applications to store and retrieve information
    across interactions. Memory enables contextual awareness in conversations and
    complex workflows by tracking previous inputs, outputs, and important information.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reinforcement learning from human feedback** (**RLHF**): A training technique
    where AI models learn from direct human feedback, optimizing their performance
    to align with human preferences. RLHF helps create models that are more helpful,
    safe, and aligned with human values.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Agents**: AI systems that can perceive their environment, make decisions,
    and take actions to accomplish goals. In LangChain, agents use LLMs to interpret
    tasks, choose appropriate tools, and execute multi-step processes with minimal
    human intervention.'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Yea****r** | **Development** | **Key Features** |'
  prefs: []
  type: TYPE_TB
- en: '| 1990s | IBM Alignment Models | Statistical machine translation |'
  prefs: []
  type: TYPE_TB
- en: '| 2000s | Web-scale datasets | Large-scale statistical models |'
  prefs: []
  type: TYPE_TB
- en: '| 2009 | Statistical models dominate | Large-scale text ingestion |'
  prefs: []
  type: TYPE_TB
- en: '| 2012 | Deep learning gains traction | Neural networks outperform statistical
    models |'
  prefs: []
  type: TYPE_TB
- en: '| 2016 | Neural Machine Translation (NMT) | Seq2seq deep LSTMs replace statistical
    methods |'
  prefs: []
  type: TYPE_TB
- en: '| 2017 | Transformer architecture | Self-attention revolutionizes NLP |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | BERT and GPT-1 | Transformer-based language understanding and generation
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | GPT-2 | Large-scale text generation, public awareness increases |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | GPT-3 | API-based access, state-of-the-art performance |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | ChatGPT | Mainstream adoption of LLMs |'
  prefs: []
  type: TYPE_TB
- en: '| 2023 | Large Multimodal Models (LMMs) | AI models process text, images, and
    audio |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '2024'
  prefs: []
  type: TYPE_NORMAL
- en: '| OpenAI o1 | Stronger reasoning capabilities |'
  prefs: []
  type: TYPE_TB
- en: '| 2025 | DeepSeek R1 | Open-weight, large-scale AI model |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1.1: A timeline of major developments in language models'
  prefs: []
  type: TYPE_NORMAL
- en: The field of LLMs is rapidly evolving, with multiple models competing in terms
    of performance, capabilities, and accessibility. Each provider brings distinct
    advantages, from OpenAI’s advanced general-purpose AI to Mistral’s open-weight,
    high-efficiency models. Understanding the differences between these models helps
    practitioners make informed decisions when integrating LLMs into their applications.
  prefs: []
  type: TYPE_NORMAL
- en: Model comparison
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following points outline key factors to consider when comparing different
    LLMs, focusing on their accessibility, size, capabilities, and specialization:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Open-source vs. closed-source models**: Open-source models like Mistral and
    LLaMA provide transparency and the ability to run locally, while closed-source
    models like GPT-4 and Claude are accessible through APIs. Open-source LLMs can
    be downloaded and modified, enabling developers and researchers to investigate
    and build upon their architectures, though specific usage terms may apply.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Size and capabilities**: Larger models generally offer better performance
    but require more computational resources. This makes smaller models great for
    use on devices with limited computing power or memory, and can be significantly
    cheaper to use. **Small language models** (**SLMs**) have a relatively small number
    of parameters, typically using millions to a few billion parameters, as opposed
    to LLMs, which can have hundreds of billions or even trillions of parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Specialized models**: Some LLMs are optimized for specific tasks, such as
    code generation (for example, Codex) or mathematical reasoning (e.g., Minerva).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The increase in the scale of language models has been a major driving force
    behind their impressive performance gains. However, recently there has been a
    shift in architecture and training methods that has led to better parameter efficiency
    in terms of performance.
  prefs: []
  type: TYPE_NORMAL
- en: '**Model scaling laws**'
  prefs: []
  type: TYPE_NORMAL
- en: Empirically derived scaling laws predict the performance of LLMs based on the
    given training budget, dataset size, and the number of parameters. If true, this
    means that highly powerful systems will be concentrated in the hands of Big Tech,
    however, we have seen a significant shift over recent months.
  prefs: []
  type: TYPE_NORMAL
- en: The **KM scaling law**, proposed by Kaplan et al., derived through empirical
    analysis and fitting of model performance with varied data sizes, model sizes,
    and training compute, presents power-law relationships, indicating a strong codependence
    between model performance and factors such as model size, dataset size, and training
    compute.
  prefs: []
  type: TYPE_NORMAL
- en: The **Chinchilla scaling law**, proposed by the Google DeepMind team, involved
    experiments with a wider range of model sizes and data sizes. It suggests an optimal
    allocation of compute budget to model size and data size, which can be determined
    by optimizing a specific loss function under a constraint.
  prefs: []
  type: TYPE_NORMAL
- en: However, future progress may depend more on model architecture, data cleansing,
    and model algorithmic innovation rather than sheer size. For example, models such
    as phi, first presented in *Textbooks Are All You Need* (2023, Gunasekar et al.),
    with about 1 billion parameters, showed that models can – despite a smaller scale
    – achieve high accuracy on evaluation benchmarks. The authors suggest that improving
    data quality can dramatically change the shape of scaling laws.
  prefs: []
  type: TYPE_NORMAL
- en: Further, there is a body of work on simplified model architectures, which have
    substantially fewer parameters and only modestly drop accuracy (for example, *One
    Wide Feedforward is All You Need*, Pessoa Pires et al., 2023). Additionally, techniques
    such as fine-tuning, quantization, distillation, and prompting techniques can
    enable smaller models to leverage the capabilities of large foundations without
    replicating their costs. To compensate for model limitations, tools like search
    engines and calculators have been incorporated into agents, and multi-step reasoning
    strategies, plugins, and extensions may be increasingly used to expand capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: The future could see the co-existence of massive, general models with smaller
    and more accessible models that provide faster and cheaper training, maintenance,
    and inference.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now discuss a comparative overview of various LLMs, highlighting their
    key characteristics and differentiating factors. We’ll delve into aspects such
    as open-source vs. closed-source models, model size and capabilities, and specialized
    models. By understanding these distinctions, you can select the most suitable
    LLM for your specific needs and applications.
  prefs: []
  type: TYPE_NORMAL
- en: LLM provider landscape
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can access LLMs from major providers like OpenAI, Google, and Anthropic,
    along with a growing number of others, through their websites or APIs. As the
    demand for LLMs grows, numerous providers have entered the space, each offering
    models with unique capabilities and trade-offs. Developers need to understand
    the various access options available for integrating these powerful models into
    their applications. The choice of provider will significantly impact development
    experience, performance characteristics, and operational costs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The table below provides a comparative overview of leading LLM providers and
    examples of the models they offer:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Provider** | **Notable models** | **Key features and strengths** |'
  prefs: []
  type: TYPE_TB
- en: '| **OpenAI** | GPT-4o, GPT-4.5; o1; o3-mini | Strong general performance, proprietary
    models, advanced reasoning; multimodal reasoning across text, audio, vision, and
    video in real time |'
  prefs: []
  type: TYPE_TB
- en: '| **Anthropic** | Claude 3.7 Sonnet; Claude 3.5 Haiku | Toggle between real-time
    responses and extended “thinking” phases; outperforms OpenAI’s o1 in coding benchmarks
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Google** | Gemini 2.5, 2.0 (flash and pro), Gemini 1.5 | Low latency and
    costs, large context window (up to 2M tokens), multimodal inputs and outputs,
    reasoning capabilities |'
  prefs: []
  type: TYPE_TB
- en: '| **Cohere** | Command R, Command R Plus | Retrieval-augmented generation,
    enterprise AI solutions |'
  prefs: []
  type: TYPE_TB
- en: '| **Mistral AI** | Mistral Large; Mistral 7B | Open weights, efficient inference,
    multilingual support |'
  prefs: []
  type: TYPE_TB
- en: '| **AWS** | Titan | Enterprise-scale AI models, optimized for the AWS cloud
    |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '**DeepSeek**'
  prefs: []
  type: TYPE_NORMAL
- en: '| R1 | Maths-first: solves Olympiad-level problems; cost-effective, optimized
    for multilingual and programming tasks |'
  prefs: []
  type: TYPE_TB
- en: '| **Together AI** | Infrastructure for running open models | Competitive pricing;
    growing marketplace of models |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1.2: Comparative overview of major LLM providers and their flagship models
    for LangChain implementation'
  prefs: []
  type: TYPE_NORMAL
- en: Other organizations develop LLMs but do not necessarily provide them through
    **application programming interfaces** (**APIs**) to developers. For example,
    Meta AI develops the very influential Llama model series, which has strong reasoning,
    code-generation capabilities, and is released under an open-source license.
  prefs: []
  type: TYPE_NORMAL
- en: There is a whole zoo of open-source models that you can access through Hugging
    Face or through other providers. You can even download these open-source models,
    fine-tune them, or fully train them. We’ll try this out practically starting in
    [*Chapter 2*](E_Chapter_2.xhtml#_idTextAnchor044).
  prefs: []
  type: TYPE_NORMAL
- en: Once you’ve selected an appropriate model, the next crucial step is understanding
    how to control its behavior to suit your specific application needs. While accessing
    a model gives you computational capability, it’s the choice of generation parameters
    that transforms raw model power into tailored output for different use cases within
    your applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we’ve covered the LLM provider landscape, let’s discuss another critical
    aspect of LLM implementation: licensing considerations. The licensing terms of
    different models significantly impact how you can use them in your applications.'
  prefs: []
  type: TYPE_NORMAL
- en: Licensing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs are available under different licensing models that impact how they can
    be used in practice. Open-source models like Mixtral and BERT can be freely used,
    modified, and integrated into applications. These models allow developers to run
    them locally, investigate their behavior, and build upon them for both research
    and commercial purposes.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, proprietary models like GPT-4 and Claude are accessible only through
    APIs, with their internal workings kept private. While this ensures consistent
    performance and regular updates, it means depending on external services and typically
    incurring usage costs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some models like Llama 2 take a middle ground, offering permissive licenses
    for both research and commercial use while maintaining certain usage conditions.
    For detailed information about specific model licenses and their implications,
    refer to the documentation of each model or consult the model openness framework:
    [https://isitopen.ai/](https://isitopen.ai/).'
  prefs: []
  type: TYPE_NORMAL
- en: The **model openness framework** (**MOF**) evaluates language models based on
    criteria such as access to model architecture details, training methodology and
    hyperparameters, data sourcing and processing information, documentation around
    development decisions, ability to evaluate model workings, biases, and limitations,
    code modularity, published model card, availability of servable model, option
    to run locally, source code availability, and redistribution rights.
  prefs: []
  type: TYPE_NORMAL
- en: In general, open-source licenses promote wide adoption, collaboration, and innovation
    around the models, benefiting both research and commercial development. Proprietary
    licenses typically give companies exclusive control but may limit academic research
    progress. Non-commercial licenses often restrict commercial use while enabling
    research.
  prefs: []
  type: TYPE_NORMAL
- en: By making knowledge and knowledge work more accessible and adaptable, generative
    AI models have the potential to level the playing field and create new opportunities
    for people from all walks of life.
  prefs: []
  type: TYPE_NORMAL
- en: The evolution of AI has brought us to a pivotal moment where AI systems can
    not only process information but also take autonomous action. The next section
    explores the transformation from basic language models to more complex, and finally,
    fully agentic applications.
  prefs: []
  type: TYPE_NORMAL
- en: The information provided about AI model licensing is for educational purposes
    only and does not constitute legal advice. Licensing terms vary significantly
    and evolve rapidly. Organizations should consult qualified legal counsel regarding
    specific licensing decisions for their AI implementations.
  prefs: []
  type: TYPE_NORMAL
- en: From models to agentic applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed so far, LLMs have been demonstrating remarkable fluency in natural
    language processing. However, as impressive as they are, they remain fundamentally
    *reactive* rather than *proactive*. They lack the ability to take independent
    actions, interact meaningfully with external systems, or autonomously achieve
    complex objectives.
  prefs: []
  type: TYPE_NORMAL
- en: To unlock the next phase of AI capabilities, we need to move beyond passive
    text generation and toward **agentic AI**—systems that can plan, reason, and take
    action to accomplish tasks with minimal human intervention. Before exploring the
    potential of agentic AI, it’s important to first understand the core limitations
    of LLMs that necessitate this evolution.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations of traditional LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Despite their advanced language capabilities, LLMs have inherent constraints
    that limit their effectiveness in real-world applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Lack of true understanding**: LLMs generate human-like text by predicting
    the next most likely word based on statistical patterns in training data. However,
    they do not understand meaning in the way humans do. This leads to hallucinations—confidently
    stating false information as fact—and generating plausible but incorrect, misleading,
    or nonsensical outputs. As Bender et al. (2021) describe, LLMs function as “stochastic
    parrots”—repeating patterns without genuine comprehension.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Struggles with complex reasoning and problem-solving**: While LLMs excel
    at retrieving and reformatting knowledge, they struggle with multi-step reasoning,
    logical puzzles, and mathematical problem-solving. They often fail to break down
    problems into sub-tasks or synthesize information across different contexts. Without
    explicit prompting techniques like chain-of-thought reasoning, their ability to
    deduce or infer remains unreliable.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Outdated knowledge and limited external access**: LLMs are trained on static
    datasets and do not have real-time access to current events, dynamic databases,
    or live information sources. This makes them unsuitable for tasks requiring up-to-date
    knowledge, such as financial analysis, breaking news summaries, or scientific
    research requiring the latest findings.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**No native tool use or action-taking abilities**: LLMs operate in isolation—they
    cannot interact with APIs, retrieve live data, execute code, or modify external
    systems. This lack of tool integration makes them less effective in scenarios
    that require real-world actions, such as conducting web searches, automating workflows,
    or controlling software systems.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Bias, ethical concerns, and reliability issues:** Because LLMs learn from
    large datasets that may contain biases, they can unintentionally reinforce ideological,
    social, or cultural biases. Importantly, even with open-source models, accessing
    and auditing the complete training data to identify and mitigate these biases
    remains challenging for most practitioners. Additionally, they can generate misleading
    or harmful information without understanding the ethical implications of their
    outputs.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Computational costs and efficiency challenges**: Deploying and running LLMs
    at scale requires **significant** computational resources, making them costly
    and energy-intensive. Larger models can also introduce latency, slowing response
    times in real-time applications.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To overcome these limitations, AI systems must evolve from passive text generators
    into active agents that can plan, reason, and interact with their environment.
    This is where agentic AI comes in—integrating LLMs with tool use, decision-making
    mechanisms, and autonomous execution capabilities to enhance their functionality.
  prefs: []
  type: TYPE_NORMAL
- en: While frameworks like LangChain provide comprehensive solutions to LLM limitations,
    understanding fundamental prompt engineering techniques remains valuable. Approaches
    like few-shot learning, chain-of-thought, and structured prompting can significantly
    enhance model performance for specific tasks. [*Chapter 3*](E_Chapter_3.xhtml#_idTextAnchor107)
    will cover these techniques in detail, showing how LangChain helps standardize
    and optimize prompting patterns while minimizing the need for custom prompt engineering
    in every application.
  prefs: []
  type: TYPE_NORMAL
- en: The next section explores how agentic AI extends the capabilities of traditional
    LLMs and unlocks new possibilities for automation, problem-solving, and intelligent
    decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding LLM applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'LLM applications represent the bridge between raw model capability and practical
    business value. While LLMs possess impressive language processing abilities, they
    require thoughtful integration to deliver real-world solutions. These applications
    broadly fall into two categories: complex integrated applications and autonomous
    agents.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Complex integrated applications** enhance human workflows by integrating
    LLMs into existing processes, including:'
  prefs: []
  type: TYPE_NORMAL
- en: Decision support systems that provide analysis and recommendations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Content generation pipelines with human review
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interactive tools that augment human capabilities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Workflow automation with human oversight
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Autonomous agents** operate with minimal human intervention, further augmenting
    workflows through LLM integration. Examples include:'
  prefs: []
  type: TYPE_NORMAL
- en: Task automation agents that execute defined workflows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Information gathering and analysis systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-agent systems for complex task coordination
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LangChain provides frameworks for both integrated applications and autonomous
    agents, offering flexible components that support various architectural choices.
    This book will explore both approaches, demonstrating how to build reliable, production-ready
    systems that match your specific requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Autonomous systems of agents are potentially very powerful, and it’s therefore
    worthwhile exploring them a bit more.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding AI agents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is sometimes joked that AI is just a fancy word for ML, or AI is ML in a
    suit, as illustrated in this image; however, there’s more to it, as we’ll see.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.1: ML in a suit. Generated by a model on replicate.com, Diffusers
    Stable Diffusion v2.1](img/B32363_01_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.1: ML in a suit. Generated by a model on replicate.com, Diffusers
    Stable Diffusion v2.1'
  prefs: []
  type: TYPE_NORMAL
- en: An AI agent represents the bridge between raw cognitive capability and practical
    action. While an LLM possesses vast knowledge and processing ability, it remains
    fundamentally reactive without agency. AI agents transform this passive capability
    into active utility through structured workflows that parse requirements, analyze
    options, and execute actions.
  prefs: []
  type: TYPE_NORMAL
- en: Agentic AI enables autonomous systems to make decisions and act independently,
    with minimal human intervention. Unlike deterministic systems that follow fixed
    rules, agentic AI relies on patterns and likelihoods to make informed choices.
    It functions through a network of autonomous software components called agents,
    which learn from user behavior and large datasets to improve over time.
  prefs: []
  type: TYPE_NORMAL
- en: '*Agency* in AI refers to a system’s ability to act independently to achieve
    goals. True agency means an AI system can perceive its environment, make decisions,
    act, and adapt over time by learning from interactions and feedback. The distinction
    between raw AI and agents parallels the difference between knowledge and expertise.
    Consider a brilliant researcher who understands complex theories but struggles
    with practical application. An agent system adds the crucial element of purposeful
    action, turning abstract capability into concrete results.'
  prefs: []
  type: TYPE_NORMAL
- en: In the context of LLMs, agentic AI involves developing systems that act autonomously,
    understand context, adapt to new information, and collaborate with humans to solve
    complex challenges. These AI agents leverage LLMs to process information, generate
    responses, and execute tasks based on defined objectives.
  prefs: []
  type: TYPE_NORMAL
- en: 'Particularly, AI agents extend the capabilities of LLMs by integrating memory,
    tool use, and decision-making frameworks. These agents can:'
  prefs: []
  type: TYPE_NORMAL
- en: Retain and recall information across interactions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilize external tools, APIs, and databases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Plan and execute multi-step workflows.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The value of agency lies in reducing the need for constant human oversight.
    Instead of manually prompting an LLM for every request, an agent can proactively
    execute tasks, react to new data, and integrate with real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: AI agents are systems designed to act on behalf of users, leveraging LLMs alongside
    external tools, memory, and decision-making frameworks. The hope behind AI agents
    is that they can automate complex workflows, reducing human effort while increasing
    efficiency and accuracy. By allowing systems to act autonomously, agents promise
    to unlock new levels of automation in AI-driven applications. But are the hopes
    justified?
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite their potential, AI agents face significant challenges:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reliability**: Ensuring agents make correct, context-aware decisions without
    supervision is difficult.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generalization**: Many agents work well in narrow domains but struggle with
    open-ended, multi-domain tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lack of trust**: Users must trust that agents will act responsibly, avoid
    unintended actions, and respect privacy constraints.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Coordination complexity**: Multi-agent systems often suffer from inefficiencies
    and miscommunication when executing tasks collaboratively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Production-ready agent systems must address not just theoretical challenges
    but practical implementation hurdles like:'
  prefs: []
  type: TYPE_NORMAL
- en: Rate limitations and API quotas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Token context overflow errors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hallucination management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cost optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LangChain and LangSmith provide robust solutions for these challenges, which
    we’ll explore in depth in [*Chapter 8*](E_Chapter_8.xhtml#_idTextAnchor390) and
    [*Chapter 9*](E_Chapter_9.xhtml#_idTextAnchor448). These chapters will cover how
    to build reliable, observable AI systems that can operate at an enterprise scale.
  prefs: []
  type: TYPE_NORMAL
- en: 'When developing agent-based systems, therefore, several key factors require
    careful consideration:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Value generation**: Agents must provide a clear utility that outweighs their
    costs in terms of setup, maintenance, and necessary human oversight. This often
    means starting with well-defined, high-value tasks where automation can demonstrably
    improve outcomes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Trust and safety**: As agents take on more responsibility, establishing and
    maintaining user trust becomes crucial. This encompasses both technical reliability
    and transparent operation that allows users to understand and predict agent behavior.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Standardization**: As the agent ecosystem grows, standardized interfaces
    and protocols become essential for interoperability. This parallels the development
    of web standards that enabled the growth of internet applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While early AI systems focused on pattern matching and predefined templates,
    modern AI agents demonstrate emergent capabilities such as reasoning, problem-solving,
    and long-term planning. Today’s AI agents integrate LLMs with interactive environments,
    enabling them to function autonomously in complex domains.
  prefs: []
  type: TYPE_NORMAL
- en: The development of agent-based AI is a natural progression from statistical
    models to deep learning and now to reasoning-based systems. Modern AI agents leverage
    multimodal capabilities, reinforcement learning, and memory-augmented architectures
    to adapt to diverse tasks. This evolution marks a shift from predictive models
    to truly autonomous systems capable of dynamic decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: Looking ahead, AI agents will continue to refine their ability to reason, plan,
    and act within structured and unstructured environments. The rise of open-weight
    models, combined with advances in agent-based AI, will likely drive the next wave
    of innovations in AI, expanding its applications across science, engineering,
    and everyday life.
  prefs: []
  type: TYPE_NORMAL
- en: With frameworks like LangChain, developers can build complex and agentic structured
    systems that overcome the limitations of raw LLMs. It offers built-in solutions
    for memory management, tool integration, and multi-step reasoning that align with
    the ecosystem model presented here. In the next section we will explore how LangChain
    facilitates the development of production-ready AI agents.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing LangChain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LangChain exists as both an open-source framework and a venture-backed company.
    The framework, introduced in 2022 by Harrison Chase, streamlines the development
    of LLM-powered applications with support for multiple programming languages including
    Python, JavaScript/TypeScript, Go, Rust, and Ruby.
  prefs: []
  type: TYPE_NORMAL
- en: The company behind the framework, LangChain, Inc., is based in San Francisco
    and has secured significant venture funding through multiple rounds, including
    a Series A in February 2024\. With 11-50 employees, the company maintains and
    expands the framework while offering enterprise solutions for LLM application
    development.
  prefs: []
  type: TYPE_NORMAL
- en: 'While the core framework remains open source, the company provides additional
    enterprise features and support for commercial users. Both share the same mission:
    accelerating LLM application development by providing robust tools and infrastructure.'
  prefs: []
  type: TYPE_NORMAL
- en: Modern LLMs are undeniably powerful, but their practical utility in production
    applications is constrained by several inherent limitations. Understanding these
    challenges is essential for appreciating why frameworks like LangChain have become
    indispensable tools for AI developers.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges with raw LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Despite their impressive capabilities, LLMs face fundamental constraints that
    create significant hurdles for developers building real-world applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Context window limitations**: LLMs process text as tokens (subword units),
    not complete words. For example, “LangChain” might be processed as two tokens:
    “Lang” and “Chain.” Every LLM has a fixed context window—the maximum number of
    tokens it can process at once—typically ranging from 2,000 to 128,000 tokens.
    This creates several practical challenges:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Document processing**: Long documents must be chunked effectively to fit
    within context limits'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Conversation history**: Maintaining information across extended conversations
    requires careful memory management'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Cost management**: Most providers charge based on token count, making efficient
    token use a business imperative'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: These constraints directly impact application architecture, making techniques
    like RAG (which we’ll explore in [*Chapter 4*](E_Chapter_4.xhtml#_idTextAnchor152))
    essential for production systems.
  prefs: []
  type: TYPE_NORMAL
- en: '**Limited tool orchestration**: While many modern LLMs offer native tool-calling
    capabilities, they lack the infrastructure to discover appropriate tools, execute
    complex workflows, and manage tool interactions across multiple turns. Without
    this orchestration layer, developers must build custom solutions for each integration.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Task coordination challenges**: Managing multi-step workflows with LLMs requires
    structured control mechanisms. Without them, complex processes involving sequential
    reasoning or decision-making become difficult to implement reliably.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Tools in this context refer to functional capabilities that extend an LLM’s
    reach: web browsers for searching the internet, calculators for precise mathematics,
    coding environments for executing programs, or APIs for accessing external services
    and databases. Without these tools, LLMs remain confined to operating within their
    training knowledge, unable to perform real-world actions or access current information.'
  prefs: []
  type: TYPE_NORMAL
- en: These fundamental limitations create three key challenges for developers working
    with raw LLM APIs, as demonstrated in the following table.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Challenge** | **Description** | **Impact** |'
  prefs: []
  type: TYPE_TB
- en: '| **Reliability** | Detecting hallucinations and validating outputs | Inconsistent
    results that may require human verification |'
  prefs: []
  type: TYPE_TB
- en: '| **Resource Management** | Handling context windows and rate limits | Implementation
    complexity and potential cost overruns |'
  prefs: []
  type: TYPE_TB
- en: '| **Integration Complexity** | Building connections to external tools and data
    sources | Extended development time and maintenance burden |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1.3: Three key developer challenges'
  prefs: []
  type: TYPE_NORMAL
- en: LangChain addresses these challenges by providing a structured framework with
    tested solutions, simplifying AI application development and enabling more sophisticated
    use cases.
  prefs: []
  type: TYPE_NORMAL
- en: How LangChain enables agent development
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'LangChain provides the foundational infrastructure for building sophisticated
    AI applications through its modular architecture and composable patterns. With
    the evolution to version 0.3, LangChain has refined its approach to creating intelligent
    systems:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Composable workflows**: The **LangChain Expression Language** (**LCEL**)
    allows developers to break down complex tasks into modular components that can
    be assembled and reconfigured. This composability enables systematic reasoning
    through the orchestration of multiple processing steps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integration ecosystem**: LangChain offers battle-tested abstract interfaces
    for all generative AI components (LLMs, embeddings, vector databases, document
    loaders, search engines). This lets you build applications that can easily switch
    between providers without rewriting core logic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unified model access**: The framework provides consistent interfaces to diverse
    language and embedding models, allowing seamless switching between providers while
    maintaining application logic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'While earlier versions of LangChain handled memory management directly, version
    0.3 takes a more specialized approach to application development:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Memory and state management**: For applications requiring persistent context
    across interactions, LangGraph now serves as the recommended solution. LangGraph
    maintains conversation history and application state with purpose-built persistence
    mechanisms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Agent architecture**: Though LangChain contains agent implementations, LangGraph
    has become the preferred framework for building sophisticated agents. It provides:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graph-based workflow definition for complex decision paths
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Persistent state management across multiple interactions
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming support for real-time feedback during processing
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Human-in-the-loop capabilities for validation and corrections
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Together, LangChain and its companion projects like LangGraph and LangSmith
    form a comprehensive ecosystem that transforms LLMs from simple text generators
    into systems capable of sophisticated real-world tasks, combining strong abstractions
    with practical implementation patterns optimized for production use.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the LangChain architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'LangChain’s philosophy centers on composability and modularity. Rather than
    treating LLMs as standalone services, LangChain views them as components that
    can be combined with other tools and services to create more capable systems.
    This approach is built on several principles:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Modular architecture**: Every component is designed to be reusable and interchangeable,
    allowing developers to integrate LLMs seamlessly into various applications. This
    modularity extends beyond LLMs to include numerous building blocks for developing
    complex generative AI applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Support for agentic workflows**: LangChain offers best-in-class APIs that
    allow you to develop sophisticated agents quickly. These agents can make decisions,
    use tools, and solve problems with minimal development overhead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Production readiness**: The framework provides built-in capabilities for
    tracing, evaluation, and deployment of generative AI applications, including robust
    building blocks for managing memory and persistence across interactions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Broad vendor ecosystem**: LangChain offers battle-tested abstract interfaces
    for all generative AI components (LLMs, embeddings, vector databases, document
    loaders, search engines, etc.). Vendors develop their own integrations that comply
    with these interfaces, allowing you to build applications on top of any third-party
    provider and easily switch between them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s worth noting that there’ve been major changes since LangChain version 0.1
    when the first edition of this book was written. While early versions attempted
    to handle everything, LangChain version 0.3 focuses on excelling at specific functions
    with companion projects handling specialized needs. LangChain manages model integration
    and workflows, while LangGraph handles stateful agents and LangSmith provides
    observability.
  prefs: []
  type: TYPE_NORMAL
- en: LangChain’s memory management, too, has gone through major changes. Memory mechanisms
    within the base LangChain library have been deprecated in favor of LangGraph for
    persistence, and while agents are present, LangGraph is the recommended approach
    for their creation in version 0.3\. However, models and tools continue to be fundamental
    to LangChain’s functionality. In [*Chapter 3*](E_Chapter_3.xhtml#_idTextAnchor107),
    we’ll explore LangChain and LangGraph’s memory mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: To translate model design principles into practical tools, LangChain has developed
    a comprehensive ecosystem of libraries, services, and applications. This ecosystem
    provides developers with everything they need to build, deploy, and maintain sophisticated
    AI applications. Let’s examine the components that make up this thriving environment
    and how they’ve gained adoption across the industry.
  prefs: []
  type: TYPE_NORMAL
- en: Ecosystem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LangChain has achieved impressive ecosystem metrics, demonstrating strong market
    adoption with over 20 million monthly downloads and powering more than 100,000
    applications. Its open-source community is thriving, evidenced by 100,000+ GitHub
    stars and contributions from over 4,000 developers. This scale of adoption positions
    LangChain as a leading framework in the AI application development space, particularly
    for building reasoning-focused LLM applications. The framework’s modular architecture
    (with components like LangGraph for agent workflows and LangSmith for monitoring)
    has clearly resonated with developers building production AI systems across various
    industries.
  prefs: []
  type: TYPE_NORMAL
- en: '**Core libraries**'
  prefs: []
  type: TYPE_NORMAL
- en: 'LangChain (Python): Reusable components for building LLM applications'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LangChain.js: JavaScript/TypeScript implementation of the framework'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LangGraph (Python): Tools for building LLM agents as orchestrated graphs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LangGraph.js: JavaScript implementation for agent workflows'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Platform services**'
  prefs: []
  type: TYPE_NORMAL
- en: 'LangSmith: Platform for debugging, testing, evaluating, and monitoring LLM
    applications'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LangGraph: Infrastructure for deploying and scaling LangGraph agents'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Applications and extensions**'
  prefs: []
  type: TYPE_NORMAL
- en: 'ChatLangChain: Documentation assistant for answering questions about the framework'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Open Canvas: Document and chat-based UX for writing code/markdown (TypeScript)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenGPTs: Open source implementation of OpenAI’s GPTs API'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Email assistant: AI tool for email management (Python)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Social media agent: Agent for content curation and scheduling (TypeScript)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The ecosystem provides a complete solution for building reasoning-focused AI
    applications: from core building blocks to deployment platforms to reference implementations.
    This architecture allows developers to use components independently or stack them
    for fuller and more complete solutions.'
  prefs: []
  type: TYPE_NORMAL
- en: From customer testimonials and company partnerships, LangChain is being adopted
    by enterprises like Rakuten, Elastic, Ally, and Adyen. Organizations report using
    LangChain and LangSmith to identify optimal approaches for LLM implementation,
    improve developer productivity, and accelerate development workflows.
  prefs: []
  type: TYPE_NORMAL
- en: 'LangChain also offers a full stack for AI application development:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Build**: with the composable framework'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Run**: deploy with LangGraph Platform'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Manage**: debug, test, and monitor with LangSmith'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Based on our experience building with LangChain, here are some of its benefits
    we’ve found especially helpful:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Accelerated development cycles**: LangChain dramatically speeds up time-to-market
    with ready-made building blocks and unified APIs, eliminating weeks of integration
    work.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Superior observability**: The combination of LangChain and LangSmith provides
    unparalleled visibility into complex agent behavior, making trade-offs between
    cost, latency, and quality more transparent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Controlled agency balance**: LangGraph’s approach to agentic AI is particularly
    powerful—allowing developers to give LLMs partial control flow over workflows
    while maintaining reliability and performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Production-ready patterns**: Our implementation experience has proven that
    LangChain’s architecture delivers enterprise-grade solutions that effectively
    reduce hallucinations and improve system reliability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Future-proof flexibility**: The framework’s vendor-agnostic design creates
    applications that can adapt as the LLM landscape evolves, preventing technological
    lock-in.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These advantages stem directly from LangChain’s architectural decisions, which
    prioritize modularity, observability, and deployment flexibility for real-world
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: Modular design and dependency management
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LangChain evolves rapidly, with approximately 10-40 pull requests merged daily.
    This fast-paced development, combined with the framework’s extensive integration
    ecosystem, presents unique challenges. Different integrations often require specific
    third-party Python packages, which can lead to dependency conflicts.
  prefs: []
  type: TYPE_NORMAL
- en: LangChain’s package architecture evolved as a direct response to scaling challenges.
    As the framework rapidly expanded to support hundreds of integrations, the original
    monolithic structure became unsustainable—forcing users to install unnecessary
    dependencies, creating maintenance bottlenecks, and hindering contribution accessibility.
    By dividing into specialized packages with lazy loading of dependencies, LangChain
    elegantly solved these issues while preserving a cohesive ecosystem. This architecture
    allows developers to import only what they need, reduces version conflicts, enables
    independent release cycles for stable versus experimental features, and dramatically
    simplifies the contribution path for community developers working on specific
    integrations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The LangChain codebase follows a well-organized structure that separates concerns
    while maintaining a cohesive ecosystem:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Core structure**'
  prefs: []
  type: TYPE_NORMAL
- en: '`docs/`: Documentation resources for developers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`libs/`: Contains all library packages in the monorepo'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Library organization**'
  prefs: []
  type: TYPE_NORMAL
- en: '`langchain-core/`: Foundational abstractions and interfaces that define the
    framework'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`langchain/`: The main implementation library with core components:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vectorstores/`: Integrations with vector databases (Pinecone, Chroma, etc.)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chains/`: Pre-built chain implementations for common workflows'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other component directories for retrievers, embeddings, etc.
  prefs: []
  type: TYPE_NORMAL
- en: '`langchain-experimental/`: Cutting-edge features still under development'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**langchain-community**: Houses third-party integrations maintained by the
    LangChain community. This includes most integrations for components like LLMs,
    vector stores, and retrievers. Dependencies are optional to maintain a lightweight
    package.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Partner packages**: Popular integrations are separated into dedicated packages
    (e.g., **langchain-openai**, **langchain-anthropic**) to enhance independent support.
    These packages reside outside the LangChain repository but within the GitHub “langchain-ai”
    organization (see [github.com/orgs/langchain-ai](https://github.com/langchain-ai)).
    A full list is available at [python.langchain.com/v0.3/docs/integrations/platforms/](https://python.langchain.com/docs/integrations/providers/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**External partner packages**: Some partners maintain their integration packages
    independently. For example, several packages from the Google organization ([github.com/orgs/googleapis/repositories?q=langchain](https://github.com/orgs/googleapis/repositories?q=langchain)),
    such as the `langchain-google-cloud-sql-mssql` package, are developed and maintained
    outside the LangChain ecosystem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 1.2: Integration ecosystem map](img/B32363_01_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.2: Integration ecosystem map'
  prefs: []
  type: TYPE_NORMAL
- en: 'For full details on the dozens of available modules and packages, refer to
    the comprehensive LangChain API reference: [https://api.python.langchain.com/](https://api.python.langchain.com/).
    There are also hundreds of code examples demonstrating real-world use cases: [https://python.langchain.com/v0.1/docs/use_cases/](https://python.langchain.com/v0.1/docs/use_cases/).'
  prefs: []
  type: TYPE_NORMAL
- en: LangGraph, LangSmith, and companion tools
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'LangChain’s core functionality is extended by the following companion projects:'
  prefs: []
  type: TYPE_NORMAL
- en: '**LangGraph**: An orchestration framework for building stateful, multi-actor
    applications with LLMs. While it integrates smoothly with LangChain, it can also
    be used independently. LangGraph facilitates complex applications with cyclic
    data flows and supports streaming and human-in-the-loop interactions. We’ll talk
    about LangGraph in more detail in [*Chapter 3*](E_Chapter_3.xhtml#_idTextAnchor107).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LangSmith**: A platform that complements LangChain by providing robust debugging,
    testing, and monitoring capabilities. Developers can inspect, monitor, and evaluate
    their applications, ensuring continuous optimization and confident deployment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These extensions, along with the core framework, provide a comprehensive ecosystem
    for developing, managing, and visualizing LLM applications, each with unique capabilities
    that enhance functionality and user experience.
  prefs: []
  type: TYPE_NORMAL
- en: LangChain also has an extensive array of tool integrations, which we’ll discuss
    in detail in [*Chapter 5*](E_Chapter_5.xhtml#_idTextAnchor231). New integrations
    are added regularly, expanding the framework’s capabilities across domains.
  prefs: []
  type: TYPE_NORMAL
- en: Third-party applications and visual tools
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Many third-party applications have been built on top of or around LangChain.
    For example, LangFlow and Flowise introduce visual interfaces for LLM development,
    with UIs that allow for the drag-and-drop assembly of LangChain components into
    executable workflows. This visual approach enables rapid prototyping and experimentation,
    lowering the barrier to entry for complex pipeline creation, as illustrated in
    the following screenshot of Flowise:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.3: Flowise UI with an agent that uses an LLM, a calculator, and
    a search tool (Source: https://github.com/FlowiseAI/Flowise)](img/B32363_01_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.3: Flowise UI with an agent that uses an LLM, a calculator, and a
    search tool (Source: https://github.com/FlowiseAI/Flowise)'
  prefs: []
  type: TYPE_NORMAL
- en: In the UI above, you can see an agent connected to a search interface (Serp
    API), an LLM, and a calculator. LangChain and similar tools can be deployed locally
    using libraries like Chainlit, or on various cloud platforms, including Google
    Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, LangChain simplifies the development of LLM applications through
    its modular design, extensive integrations, and supportive ecosystem. This makes
    it an invaluable tool for developers looking to build sophisticated AI systems
    without reinventing fundamental components.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter introduced the modern LLM landscape and positioned LangChain as
    a powerful framework for building production-ready AI applications. We explored
    the limitations of raw LLMs and then showed how these frameworks transform models
    into reliable, agentic systems capable of solving complex real-world problems.
    We also examined the LangChain ecosystem’s architecture, including its modular
    components, package structure, and companion projects that support the complete
    development lifecycle. By understanding the relationship between LLMs and the
    frameworks that extend them, you’re now equipped to build applications that go
    beyond simple text generation.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll set up our development environment and take our first
    steps with LangChain, translating the conceptual understanding from this chapter
    into working code. You’ll learn how to connect to various LLM providers, create
    your first chains, and begin implementing the patterns that form the foundation
    of enterprise-grade AI applications.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What are the three primary limitations of raw LLMs that impact production applications,
    and how does LangChain address each one?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare and contrast open-source and closed-source LLMs in terms of deployment
    options, cost considerations, and use cases. When might you choose each type?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between a LangChain chain and a LangGraph agent? When
    would you choose one over the other?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explain how LangChain’s modular architecture supports the rapid development
    of AI applications. Provide an example of how this modularity might benefit an
    enterprise use case.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the key components of the LangChain ecosystem, and how do they work
    together to support the development lifecycle from building to deployment to monitoring?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does agentic AI differ from traditional LLM applications? Describe a business
    scenario where an agent would provide significant advantages over a simple chain.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What factors should you consider when selecting an LLM provider for a production
    application? Name at least three considerations beyond just model performance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does LangChain help address common challenges like hallucinations, context
    limitations, and tool integration that affect all LLM applications?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explain how the LangChain package structure (`langchain-core`, `langchain`,
    `langchain-community`) affects dependency management and integration options in
    your applications.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What role does LangSmith play in the development lifecycle of production LangChain
    applications?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
