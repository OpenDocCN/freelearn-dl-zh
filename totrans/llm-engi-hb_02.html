<html><head></head><body>
  <div id="_idContainer041" class="Basic-Text-Frame">
    <h1 class="chapterNumber">2</h1>
    <h1 id="_idParaDest-43" class="chapterTitle">Tooling and Installation</h1>
    <p class="normal">This chapter presents all the essential tools that will be used throughout the book, especially in implementing and deploying the LLM Twin project. At this point in the book, we don’t plan to present in-depth LLM, RAG, MLOps, or LLMOps concepts. We will quickly walk you through our tech stack and prerequisites to avoid repeating ourselves throughout the book on how to set up a particular tool and why we chose it. Starting with <em class="italic">Chapter 3</em>, we will begin exploring our LLM Twin use case by implementing a data collection ETL that crawls data from the internet.</p>
    <p class="normal">In the first part of the chapter, we will present the tools within the Python ecosystem to manage multiple Python versions, create a virtual environment, and install the pinned dependencies required for our project to run. Alongside presenting these tools, we will also show how to install the <code class="inlineCode">LLM-Engineers-Handbook</code> repository on your local machine (in case you want to try out the code yourself): <a href="https://github.com/PacktPublishing/LLM-Engineers-Handbook"><span class="url">https://github.com/PacktPublishing/LLM-Engineers-Handbook</span></a>.</p>
    <p class="normal">Next, we will explore all the MLOps and LLMOps tools we will use, starting with more generic tools, such as a model registry, and moving on to more LLM-oriented tools, such as LLM evaluation and prompt monitoring tools. We will also understand how to manage a project with multiple ML pipelines using ZenML, an orchestrator bridging the gap between ML and MLOps. Also, we will quickly explore what databases we will use for NoSQL and vector storage. We will show you how to run all these components on your local machine using Docker. Lastly, we will quickly review AWS and show you how to create an AWS user and access keys and install and configure the AWS CLI to manipulate your cloud resources programmatically. We will also explore SageMaker and why we use it to train and deploy our open-source LLMs.</p>
    <p class="normal">If you are familiar with these tools, you can safely skip this chapter. We also explain how to install the project and set up all the necessary components in the repository’s <code class="inlineCode">README</code>. Thus, you also have the option to use that as more concise documentation if you plan to run the code while reading the book.</p>
    <p class="normal">To sum all that up, in this chapter, we will explore the following topics:</p>
    <ul>
      <li class="bulletList">Python ecosystem and project installation</li>
      <li class="bulletList">MLOps and LLMOps tooling</li>
      <li class="bulletList">Databases for storing unstructured and vector data</li>
      <li class="bulletList">Preparing for AWS</li>
    </ul>
    <p class="normal">By the end of this chapter, you will be aware of all the tools we will use across the book. Also, you will have learned how to install the <code class="inlineCode">LLM-Engineers-Handbook</code> repository, set up the rest of the tools, and use them if you run the code while reading the book.</p>
    <h1 id="_idParaDest-44" class="heading-1">Python ecosystem and project installation</h1>
    <p class="normal">Any Python project needs<a id="_idIndexMarker064"/> three fundamental tools: the Python interpreter, dependency management, and a task execution tool. The Python interpreter executes your Python project as expected. All the code within the book is tested with Python 3.11.8. You can download the Python interpreter from here: <a href="https://www.python.org/downloads/"><span class="url">https://www.python.org/downloads/</span></a>. We recommend installing the exact Python version (Python 3.11.8) to run the LLM Twin project using <code class="inlineCode">pyenv</code>, making the installation process straightforward.</p>
    <p class="normal">Instead of installing multiple global Python versions, we recommend managing them using <code class="inlineCode">pyenv</code>, a Python version management tool that lets you manage multiple Python versions between projects. You can install it using this link: <a href="https://github.com/pyenv/pyenv?tab=readme-ov-file#installation"><span class="url">https://github.com/pyenv/pyenv?tab=readme-ov-file#installation</span></a>.</p>
    <p class="normal">After you have installed <code class="inlineCode">pyenv</code>, you can install the latest version of Python 3.11, using <code class="inlineCode">pyenv</code>, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con">pyenv install 3.11.8
</code></pre>
    <p class="normal">Now list all installed Python versions to see that it was installed correctly:</p>
    <pre class="programlisting con"><code class="hljs-con">pyenv versions
</code></pre>
    <p class="normal">You should see something like this:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta"># </span>* system
<span class="hljs-con-meta"># </span>  3.11.8
</code></pre>
    <p class="normal">To make Python 3.11.8 the default version across your entire system (whenever you open a new terminal), use <a id="_idIndexMarker065"/>the following command:</p>
    <pre class="programlisting con"><code class="hljs-con">pyenv global 3.11.8
</code></pre>
    <p class="normal">However, we aim to use Python 3.11.8 locally only in our repository. To achieve that, first, we have to clone the repository and navigate to it:</p>
    <pre class="programlisting con"><code class="hljs-con">git clone https://github.com/PacktPublishing/LLM-Engineers-Handbook.git 
cd LLM-Engineers-Handbook
</code></pre>
    <p class="normal">Because we defined a <code class="inlineCode">.python-version</code> file within the repository, <code class="inlineCode">pyenv</code> will know to pick up the version from that file and use it locally whenever you are working within that folder. To double-check that, run the following command while you are in the repository:</p>
    <pre class="programlisting con"><code class="hljs-con">python --version
</code></pre>
    <p class="normal">It should output:</p>
    <pre class="programlisting con"><code class="hljs-con"># Python 3.11.8
</code></pre>
    <p class="normal">To create the <code class="inlineCode">.python-version</code> file, you must run <code class="inlineCode">pyenv local 3.11.8</code> once. Then, <code class="inlineCode">pyenv</code> will always know to use that Python version while working within a specific directory.</p>
    <p class="normal">Now that we have installed the correct Python version using <code class="inlineCode">pyenv</code>, let’s move on to Poetry, which we will <a id="_idIndexMarker066"/>use as our dependency and virtual environment manager.</p>
    <h2 id="_idParaDest-45" class="heading-2">Poetry: dependency and virtual environment management</h2>
    <p class="normal">Poetry is one<a id="_idIndexMarker067"/> of the most popular dependency and virtual environment managers within the Python ecosystem. But let’s start by clarifying what a dependency manager is. In Python, a dependency manager allows you to specify, install, update, and manage external libraries or packages (dependencies) that a project relies on. For example, this is a simple Poetry requirements file that uses Python 3.11 and the <code class="inlineCode">requests</code> and <code class="inlineCode">numpy</code> Python packages.</p>
    <pre class="programlisting code"><code class="hljs-code">[tool.poetry.dependencies]
python = "^3.11"
requests = "^2.25.1"
numpy = "^1.19.5"
[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"
</code></pre>
    <p class="normal">By using Poetry to pin your dependencies, you always ensure that you install the correct version of the dependencies that your projects work with. Poetry, by default, saves all its requirements in <code class="inlineCode">pyproject.toml</code> files, which are stored at the root of your repository, as you can see in the cloned LLM-Engineers-Handbook repository.</p>
    <p class="normal">Another massive advantage of using Poetry is that it creates a new Python virtual environment in which it installs the specified Python version and requirements. A virtual environment allows you to isolate your project’s dependencies from your global Python dependencies and other projects. By doing so, you ensure there are no version clashes between projects. For example, let’s assume that Project A needs <code class="inlineCode">numpy == 1.19.5</code>, and Project B needs <code class="inlineCode">numpy == 1.26.0</code>. If you keep both projects in the global Python environment, that will not work, as Project B will override Project A’s <code class="inlineCode">numpy</code> installation, which will corrupt Project A and stop it from working. Using Poetry, you can isolate each project in its own Python environment with its own Python dependencies, avoiding any dependency clashes.</p>
    <p class="normal">You can install Poetry from here: <a href="https://python-poetry.org/docs/"><span class="url">https://python-poetry.org/docs/</span></a>. We use Poetry 1.8.3 throughout the book. Once Poetry is installed, navigate to your cloned LLM-Engineers-Handbook repository and run the following command to install all the necessary Python dependencies:</p>
    <pre class="programlisting con"><code class="hljs-con">poetry install --without aws
</code></pre>
    <p class="normal">This command knows to pick up all the dependencies from your repository that are listed in the <code class="inlineCode">pyproject.toml</code> and <code class="inlineCode">poetry.lock</code> files. After the installation, you can activate your <a id="_idIndexMarker068"/>Poetry environment by running <code class="inlineCode">poetry shell</code> in your terminal or by prefixing all your CLI commands as follows: <code class="inlineCode">poetry run &lt;your command&gt;</code>.</p>
    <p class="normal">One final note on Poetry is that it locks down the exact versions of the dependency tree in the <code class="inlineCode">poetry.lock</code> file based on the definitions added to the <code class="inlineCode">project.toml</code> file. While the <code class="inlineCode">pyproject.toml</code> file may specify version ranges (e.g., <code class="inlineCode">requests = "^2.25.1"</code>), the <code class="inlineCode">poetry.lock</code> file records the exact version (e.g., <code class="inlineCode">requests = "2.25.1"</code>) that was installed. It also locks the versions of sub-dependencies (dependencies of your dependencies), which may not be explicitly listed in your <code class="inlineCode">pyproject.toml</code> file. By locking all the dependencies and sub-dependencies to specific versions, the <code class="inlineCode">poetry.lock</code> file ensures that all project installations use the same versions of each package. This consistency leads to predictable behavior, reducing the likelihood of encountering “works on my machine” issues.</p>
    <p class="normal">Other tools similar to Poetry are Venv and Conda for creating virtual environments. Still, they lack the dependency management option. Thus, you must do it through Python’s default <code class="inlineCode">requirements.txt</code> files, which are less powerful than Poetry’s <code class="inlineCode">lock</code> files. Another option is Pipenv, which feature-wise is more like Poetry but slower, and <code class="inlineCode">uv</code>, which is a replacement for Poetry built in Rust, making it blazing fast. <code class="inlineCode">uv</code> has lots of potential to replace Poetry, making it worthwhile to test out: <a href="https://github.com/astral-sh/uv"><span class="url">https://github.com/astral-sh/uv</span></a>.</p>
    <p class="normal">The final piece<a id="_idIndexMarker069"/> of the puzzle is to look at the task execution tool we used to manage all our CLI commands.</p>
    <h2 id="_idParaDest-46" class="heading-2">Poe the Poet: task execution tool</h2>
    <p class="normal">Poe the Poet is a <a id="_idIndexMarker070"/>plugin on top of Poetry that is used to manage and execute all the CLI commands required to interact with the project. It helps you define and run tasks within your Python project, simplifying automation and script execution. Other popular options are Makefile, Invoke, or shell scripts, but Poe the Poet eliminates the need to write separate shell scripts or Makefiles for managing project tasks, making it an elegant way to manage tasks using the same configuration file that Poetry already uses for dependencies.</p>
    <p class="normal">When working with Poe the Poet, instead of having all your commands documented in a README file or other document, you can add them directly to your <code class="inlineCode">pyproject.toml</code> file and execute them in the command line with an alias. For example, using Poe the Poet, we can define the following tasks in a <code class="inlineCode">pyproject.toml</code> file:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-section">[tool.poe.tasks]</span>
<span class="hljs-attr">test</span> = <span class="hljs-string">"pytest"</span>
<span class="hljs-attr">format</span> = <span class="hljs-string">"black ."</span>
<span class="hljs-attr">start</span> = <span class="hljs-string">"python main.py"</span>
</code></pre>
    <p class="normal">You can then run these tasks using the <code class="inlineCode">poe</code> command:</p>
    <pre class="programlisting con"><code class="hljs-con">poetry poe test
poetry poe format
poetry poe start
</code></pre>
    <p class="normal">You can install Poe the Poet as a Poetry plugin, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con">poetry self add 'poethepoet[poetry_plugin]'
</code></pre>
    <p class="normal">To conclude, using a tool as a façade over all your CLI commands is necessary to run your application. It significantly simplifies the application’s complexity and enhances collaboration as it acts as out-of-the-box documentation.</p>
    <p class="normal">Assuming you have <code class="inlineCode">pyenv</code> and Poetry installed, here are all the commands you need to run to<a id="_idIndexMarker071"/> clone the repository and install the dependencies and Poe the Poet as a Poetry plugin:</p>
    <pre class="programlisting con"><code class="hljs-con">git clone https://github.com/PacktPublishing/LLM-Engineers-Handbook.gitcd LLM-Engineers-Handbook
poetry install --without aws
poetry self add 'poethepoet[poetry_plugin]'
</code></pre>
    <p class="normal">To make the project fully operational, there are still a few steps to follow, such as filling out a <code class="inlineCode">.env</code> file with your credentials and getting tokens from OpenAI and Hugging Face. But this book isn’t an installation guide, so we’ve moved all these details into the repository’s README as they are useful only if you plan to run the repository: <a href="https://github.com/PacktPublishing/LLM-Engineers-Handbook"><span class="url">https://github.com/PacktPublishing/LLM-Engineers-Handbook</span></a>.</p>
    <p class="normal">Now that we have installed our Python project, let’s present the MLOps tools we will use in the book. If you are already familiar with these tools, you can safely skip the following <a id="_idIndexMarker072"/>tooling section and move on to the <em class="italic">Databases for storing unstructured and vector data</em> section.</p>
    <h1 id="_idParaDest-47" class="heading-1">MLOps and LLMOps tooling</h1>
    <p class="normal">This section will <a id="_idIndexMarker073"/>quickly present all the MLOps and LLMOps tools we will use throughout the book and their role in building ML systems using MLOps best practices. At this point in the book, we don’t aim to detail all the MLOps components we will use to implement the LLM Twin use case, such as model registries and orchestrators, but only provide a quick idea of what they are and how to use them. As we develop the LLM Twin project throughout the book, you will see hands-on examples of how we use all these tools. In <em class="italic">Chapter 11</em>, we will dive deeply into the theory of MLOps and LLMOps and connect all the dots. As the MLOps and LLMOps fields are highly practical, we will leave the theory of these aspects to the end, as it will be much easier to understand it after you go through the LLM Twin use case implementation.</p>
    <p class="normal">Also, this section is not dedicated to showing you how to set up each tool. It focuses primarily on what each tool is used for and highlights the core features used throughout this book.</p>
    <p class="normal">Still, using Docker, you can quickly run the whole infrastructure locally. If you want to run the steps within the book yourself, you can host the application locally with these three simple steps:</p>
    <ol>
      <li class="numberedList" value="1">Have Docker 27.1.1 (or higher) installed.</li>
      <li class="numberedList">Fill your <code class="inlineCode">.env</code> file with all the necessary credentials as explained in the repository README.</li>
      <li class="numberedList">Run <code class="inlineCode">poetry</code> <code class="inlineCode">poe</code> <code class="inlineCode">local-infrastructure-up</code> to locally spin up ZenML (<code class="inlineCode">http://127.0.0.1:8237/</code>) and the MongoDB and Qdrant databases.</li>
    </ol>
    <p class="normal">You can read more details on how to run everything locally in the LLM-Engineers-Handbook repository README: <a href="https://github.com/PacktPublishing/LLM-Engineers-Handbook"><span class="url">https://github.com/PacktPublishing/LLM-Engineers-Handbook</span></a>. Within the book, we will also show you how to deploy each component to the<a id="_idIndexMarker074"/> cloud.</p>
    <h2 id="_idParaDest-48" class="heading-2">Hugging Face: model registry</h2>
    <p class="normal">A model registry <a id="_idIndexMarker075"/>is a centralized repository <a id="_idIndexMarker076"/>that manages ML models throughout their lifecycle. It stores models along with their metadata, version history, and performance metrics, serving as a single source of truth. In MLOps, a model registry is crucial for tracking, sharing, and documenting model versions, facilitating team collaboration. Also, it is a fundamental element in the deployment process as it integrates <a id="_idIndexMarker077"/>with <strong class="keyWord">continuous integration</strong> <strong class="keyWord">and</strong> <strong class="keyWord">continuous deployment</strong> (<strong class="keyWord">CI/CD</strong>) pipelines.</p>
    <p class="normal">We used Hugging Face as our model registry, as we can leverage its ecosystem to easily share our fine-tuned LLM Twin models with anyone who reads the book. Also, by following the Hugging Face model registry interface, we can easily integrate the model with all the frameworks around the LLMs ecosystem, such as Unsloth for fine-tuning and SageMaker for inference.</p>
    <p class="normal">Our fine-tuned LLMs are available <a id="_idIndexMarker078"/>on Hugging Face at:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">TwinLlama 3.1 8B</strong> (after fine-tuning): <a href="https://huggingface.co/mlabonne/TwinLlama-3.1-8B"><span class="url">https://huggingface.co/mlabonne/TwinLlama-3.1-8B</span></a></li>
      <li class="bulletList"><strong class="keyWord">TwinLlama 3.1 8B DPO</strong> (after preference alignment): <a href="https://huggingface.co/mlabonne/TwinLlama-3.1-8B-DPO"><span class="url">https://huggingface.co/mlabonne/TwinLlama-3.1-8B-DPO</span></a></li>
    </ul>
    <figure class="mediaobject"><img src="../Images/B31105_02_01.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.1: Hugging Face model registry example</p>
    <p class="normal">For a quick demo, we have them available on Hugging Face Spaces:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">TwinLlama 3.1 8B: </strong><a href="https://huggingface.co/spaces/mlabonne/TwinLlama-3.1-8B"><span class="url">https://huggingface.co/spaces/mlabonne/TwinLlama-3.1-8B</span></a></li>
      <li class="bulletList"><strong class="keyWord">TwinLlama 3.1 8B DPO: </strong><a href="https://huggingface.co/spaces/mlabonne/TwinLlama-3.1-8B-DPO"><span class="url">https://huggingface.co/spaces/mlabonne/TwinLlama-3.1-8B-DPO</span></a></li>
    </ul>
    <p class="normal">Most ML tools provide model registry features. For example, ZenML, Comet, and SageMaker, which we will present in future sections, also offer their own model registries. They are good options, but we picked Hugging Face solely because of its ecosystem, which provides easy shareability and integration throughout the open-source environment. Thus, you will usually select the model registry<a id="_idIndexMarker079"/> that integrates the most with your<a id="_idIndexMarker080"/> project’s tooling and requirements.</p>
    <h2 id="_idParaDest-49" class="heading-2">ZenML: orchestrator, artifacts, and metadata</h2>
    <p class="normal">ZenML acts as <a id="_idIndexMarker081"/>the bridge between ML and MLOps. Thus, it <a id="_idIndexMarker082"/>offers multiple MLOps features that make your ML pipeline traceability, reproducibility, deployment, and maintainability easier. At its core, it is designed to create reproducible workflows in machine learning. It addresses the challenge of transitioning from exploratory research in Jupyter notebooks to a production-ready ML environment. It tackles production-based replication issues, such as versioning difficulties, reproducing experiments, organizing complex ML workflows, bridging the gap between training and deployment, and tracking metadata. Thus, ZenML’s main features are orchestrating ML pipelines, storing and versioning ML pipelines as outputs, and attaching metadata to artifacts for better observability.</p>
    <p class="normal">Instead of being another ML platform, ZenML introduced the concept of a <em class="italic">stack,</em> which allows you to run ZenML on multiple infrastructure options. A stack will enable you to connect ZenML to different cloud services, such as:</p>
    <ul>
      <li class="bulletList">An orchestrator and compute engine (for example, AWS SageMaker or Vertex AI)</li>
      <li class="bulletList">Remote storage (for instance, AWS S3 or Google Cloud Storage buckets)</li>
      <li class="bulletList">A container registry (for example, Docker Registry or AWS ECR)</li>
    </ul>
    <p class="normal">Thus, ZenML <a id="_idIndexMarker083"/>acts as a glue that brings all your infrastructure and tools together in one<a id="_idIndexMarker084"/> place through its <em class="italic">stack</em> feature, allowing you to quickly iterate through your development processes and easily monitor your entire ML system. The beauty of this is that ZenML doesn’t vendor-lock you into any cloud platform. It completely abstracts away the implementation of your Python code from the infrastructure it runs on. For example, in our LLM Twin use case, we used the AWS stack:</p>
    <ul>
      <li class="bulletList">SageMaker as our orchestrator and compute</li>
      <li class="bulletList">S3 as our remote storage used to store and track artifacts</li>
      <li class="bulletList">ECR as our container registry</li>
    </ul>
    <p class="normal">However, the Python code contains no S3 or ECR particularities, as ZenML takes care of them. Thus, we can easily switch to other providers, such as Google Cloud Storage or Azure. For more details on ZenML <em class="italic">stacks</em>, you can start here: <a href="https://docs.zenml.io/user-guide/production-guide/understand-stacks"><span class="url">https://docs.zenml.io/user-guide/production-guide/understand-stacks</span></a>.</p>
    <div class="note">
      <p class="normal">We will focus only on the ZenML features used throughout the book, such as orchestrating, artifacts, and metadata. For more details on ZenML, check out their starter guide: <a href="https://docs.zenml.io/user-guide/starter-guide"><span class="url">https://docs.zenml.io/user-guide/starter-guide</span></a>.</p>
    </div>
    <p class="normal">The local version of the ZenML server comes installed as a Python package. Thus, when running <code class="inlineCode">poetry install</code>, it installs a ZenML debugging server that you can use locally. In <em class="italic">Chapter 11</em>, we will show you how to use their cloud serverless<a id="_idIndexMarker085"/> option <a id="_idIndexMarker086"/>to deploy the ML pipelines to AWS.</p>
    <h3 id="_idParaDest-50" class="heading-3">Orchestrator</h3>
    <p class="normal">An orchestrator is a <a id="_idIndexMarker087"/>system that automates, schedules, and coordinates all your ML pipelines. It ensures that each pipeline—such as data ingestion, preprocessing, model training, and deployment—executes in the correct order and handles dependencies efficiently. By managing these processes, an orchestrator optimizes resource utilization, handles failures gracefully, and enhances scalability, making complex ML pipelines more reliable and easier to manage.</p>
    <p class="normal"><strong class="screenText">How does ZenML work as an orchestrator?</strong> It works with <strong class="screenText">pipelines</strong> and <strong class="screenText">steps</strong>. A pipeline is a high-level object that contains multiple steps. A function becomes a ZenML pipeline by being decorated with <code class="inlineCode">@pipeline</code>, and a step when decorated with <code class="inlineCode">@step</code>. This is a standard pattern when using orchestrators: you have a high-level function, often called a pipeline, that calls multiple units/steps/tasks.</p>
    <p class="normal">Let’s explore how we can implement a ZenML pipeline with one of the ML pipelines implemented for the LLM Twin project. In the code snippet below, we defined a ZenML pipeline that queries the database for a user based on its full name and crawls all the provided links under that user:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> zenml <span class="hljs-keyword">import</span> pipeline
<span class="hljs-keyword">from</span> steps.etl <span class="hljs-keyword">import</span> crawl_links, get_or_create_user
<span class="hljs-meta">@pipeline</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">digital_data_etl</span>(<span class="hljs-params">user_full_name: </span><span class="hljs-built_in">str</span><span class="hljs-params">, links: </span><span class="hljs-built_in">list</span><span class="hljs-params">[</span><span class="hljs-built_in">str</span><span class="hljs-params">]</span>) -&gt; <span class="hljs-literal">None</span>:
    user = get_or_create_user(user_full_name)
    crawl_links(user=user, links=links)
</code></pre>
    <p class="normal">You can run the pipeline with the following CLI command: <code class="inlineCode">poetry poe run-digital-data-etl</code>. To visualize the pipeline run, you can go to your ZenML dashboard (at <code class="inlineCode">http://127.0.0.1:8237/</code>) and, on the left panel, click on the <strong class="screenText">Pipelines</strong> tab and<a id="_idIndexMarker088"/> then on the <strong class="screenText">digital_data_etl</strong> pipeline, as illustrated in <em class="italic">Figure 2.2</em>:</p>
    <figure class="mediaobject"><img src="../Images/B31105_02_02.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.2: ZenML Pipelines dashboard</p>
    <p class="normal">After clicking on the <strong class="screenText">digital_data_etl</strong> pipeline, you can visualize all the previous and current pipeline runs, as seen in <em class="italic">Figure 2.3</em>. You can see which one succeeded, failed, or is still running. Also, you can see the stack used to run the pipeline, where the default stack is the one used to run your ML pipelines locally.</p>
    <figure class="mediaobject"><img src="../Images/B31105_02_03.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.3: ZenML digital_data_etl pipeline dashboard. Example of a specific pipeline</p>
    <p class="normal">Now, after clicking <a id="_idIndexMarker089"/>on the latest <strong class="screenText">digital_data_etl</strong> pipeline run (or any other run that succeeded or is still running), we can visualize the pipeline’s steps, outputs, and insights, as illustrated in <em class="italic">Figure 2.4</em>. This structure is often called a<strong class="keyWord"> directed acyclic graph</strong> (<strong class="keyWord">DAG</strong>). More on DAGs in <em class="italic">Chapter 11</em>.</p>
    <figure class="mediaobject"><img src="../Images/B31105_02_04.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.4: ZenML digital_data_etl pipeline run dashboard (example of a specific pipeline run)</p>
    <p class="normal">By clicking on a <a id="_idIndexMarker090"/>specific step, you can get more insights into its code and configuration. It even aggregates the logs output by that specific step to avoid switching between tools, as shown in <em class="italic">Figure 2.5</em>.</p>
    <figure class="mediaobject"><img src="../Images/B31105_02_05.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.5: Example of insights from a specific step of the digital_data_etl pipeline run</p>
    <p class="normal">Now that we understand how to define a ZenML pipeline and how to look it up in the dashboard, let’s quickly look at how to define a ZenML step. In the code snippet below, we <a id="_idIndexMarker091"/>defined the <code class="inlineCode">get_or_create_user()</code> step, which works just like a normal Python function but is decorated with <code class="inlineCode">@step</code>. We won’t go into the details of the logic, as we will cover the ETL logic in <em class="italic">Chapter 3</em>. For now, we will focus only on the ZenML functionality.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> loguru <span class="hljs-keyword">import</span> logger
<span class="hljs-keyword">from</span> typing_extensions <span class="hljs-keyword">import</span> Annotated
<span class="hljs-keyword">from</span> zenml <span class="hljs-keyword">import</span> get_step_context, step
<span class="hljs-keyword">from</span> llm_engineering.application <span class="hljs-keyword">import</span> utils
<span class="hljs-keyword">from</span> llm_engineering.domain.documents <span class="hljs-keyword">import</span> UserDocument
<span class="hljs-meta">@step</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">get_or_create_user</span>(<span class="hljs-params">user_full_name: </span><span class="hljs-built_in">str</span>) -&gt; Annotated[UserDocument, <span class="hljs-string">"user"</span>]:
    logger.info(<span class="hljs-string">f"Getting or creating user: </span><span class="hljs-subst">{user_full_name}</span><span class="hljs-string">"</span>)
    first_name, last_name = utils.split_user_full_name(user_full_name)
    user = UserDocument.get_or_create(first_name=first_name, last_name=last_name)
    <span class="hljs-keyword">return</span> user
</code></pre>
    <p class="normal">Within a ZenML step, you can define any Python logic your use case needs. In this simple example, we are just creating or retrieving a user, but we could replace that code with anything, starting from data collection to feature engineering and training. What is essential to notice is that to integrate ZenML with your code, you have to write modular code, where each function does just one thing. The modularity of your code makes it easy to decorate your functions with <code class="inlineCode">@step</code> and then glue multiple steps together within a main function decorated with <code class="inlineCode">@pipeline</code>. One design choice that will impact your application is deciding the granularity of each step, as each will run as a different unit on a different machine when deployed in the cloud.</p>
    <p class="normal">To decouple our<a id="_idIndexMarker092"/> code from ZenML, we encapsulated all the application and domain logic into the <code class="inlineCode">llm_engineering</code> Python module. We also defined the <code class="inlineCode">pipelines</code> and <code class="inlineCode">steps</code> folders, where we defined our ZenML logic. Within the <code class="inlineCode">steps</code> module, we only used what we needed from the <code class="inlineCode">llm_engineering</code> Python module (similar to how you use a Python package). In the <code class="inlineCode">pipelines</code> module, we only aggregated ZenML steps to glue them into the final pipeline. Using this design, we can easily swap ZenML with another orchestrator or use our application logic in other use cases, such as a REST API. We only have to replace the ZenML code without touching the <code class="inlineCode">llm_engineering</code> module where all our logic resides. </p>
    <p class="normal">This folder structure is reflected at the root of the LLM-Engineers-Handbook repository, as illustrated in <em class="italic">Figure 2.6</em>:</p>
    <figure class="mediaobject"><img src="../Images/B31105_02_06.png" alt=""/></figure>
    <figure class="mediaobject"><em class="italic">Figure 2.6</em>: LLM-Engineers-Handbook repository folder structure</figure>
    <p class="normal">One last thing to consider when writing ZenML steps is that if you return a value, it should be serializable. ZenML can serialize most objects that can be reduced to primitive data types, but there are a few exceptions. For example, we used UUID types as IDs throughout the code, which aren’t natively supported by ZenML. Thus, we had to extend ZenML’s materializer to support UUIDs. We raised this issue to ZenML. Hence, in future ZenML versions, UUIDs will be supported, but it was an excellent example of the serialization aspect of transforming function outputs in artifacts.</p>
    <h3 id="_idParaDest-51" class="heading-3">Artifacts and metadata</h3>
    <p class="normal">As mentioned in the <a id="_idIndexMarker093"/>previous section, ZenML transforms any step output into an artifact. First, let’s quickly understand what an artifact is. In MLOps, an <strong class="screenText">artifact</strong> is any file(s) produced during the machine learning lifecycle, such as datasets, trained models, checkpoints, or logs. Artifacts are crucial for reproducing experiments and deploying models. We can transform anything into an artifact. For example, the model registry is a particular use case for an artifact. Thus, artifacts have these unique properties: they are versioned, sharable, and have metadata attached to them to understand what’s inside quickly. For example, when wrapping your dataset with an artifact, you can add to its metadata the size of the dataset, the train-test split ratio, the size, types of labels, and anything else useful to understand what’s inside the dataset without actually downloading it.</p>
    <p class="normal">Let’s circle back to<a id="_idIndexMarker094"/> our <strong class="screenText">digital_data_etl </strong>pipeline example, where we had as a step output an artifact, the crawled links, which are an artifact, as seen in <em class="italic">Figure 2.7</em></p>
    <figure class="mediaobject"><img src="../Images/B31105_02_07.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.7: ZenML artifact example using the digital_data_etl pipeline as an example</p>
    <p class="normal">By clicking on the <code class="inlineCode">crawled_links</code> artifact and navigating to the <strong class="screenText">Metadata</strong> tab, we can quickly see all the domains we crawled for a particular author, the number of links we crawled for each domain, and how many were successful, as illustrated in <em class="italic">Figure 2.8</em>:</p>
    <figure class="mediaobject"><img src="../Images/B31105_02_08.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.8: ZenML metadata example using the digital_data_etl pipeline as an example</p>
    <p class="normal">A more interesting example of an artifact and its metadata is the generated dataset artifact. In <em class="italic">Figure 2.9</em>, we can visualize the metadata of the <code class="inlineCode">instruct_datasets</code> artifact, which was automatically generated and will be used to fine-tune the LLM Twin <a id="_idIndexMarker095"/>model. More details on the <code class="inlineCode">instruction datasets</code> are in <em class="chapterRef">Chapter 5</em>. For now, we want to highlight that within the dataset’s metadata, we have precomputed a lot of helpful information about it, such as how many data categories it contains, its storage size, and the number of samples per training and testing split. </p>
    <figure class="mediaobject"><img src="../Images/B31105_02_09.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.9: ZenML metadata example for the instruct_datasets artifact</p>
    <p class="normal">The metadata is manually added to the artifact, as shown in the code snippet below. Thus, you can precompute and attach to the artifact’s metadata anything you consider <a id="_idIndexMarker096"/>helpful for dataset discovery across your business and projects:</p>
    <pre class="programlisting code"><code class="hljs-code">… <span class="hljs-comment"># More imports</span>
<span class="hljs-keyword">from</span> zenml <span class="hljs-keyword">import</span> ArtifactConfig, get_step_context, step
<span class="hljs-meta">@step</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">generate_intruction_dataset</span>(
<span class="hljs-params">    prompts: Annotated[</span><span class="hljs-built_in">dict</span><span class="hljs-params">[DataCategory, </span><span class="hljs-built_in">list</span><span class="hljs-params">[GenerateDatasetSamplesPrompt]], </span><span class="hljs-string">"prompts"</span><span class="hljs-params">]</span>) -&gt; Annotated[
    InstructTrainTestSplit,
    ArtifactConfig(
        name=<span class="hljs-string">"instruct_datasets"</span>,
        tags=[<span class="hljs-string">"dataset"</span>, <span class="hljs-string">"instruct"</span>, <span class="hljs-string">"cleaned"</span>],
    ),
]:
    datasets = … <span class="hljs-comment"># Generate datasets</span>
    step_context = get_step_context()
    step_context.add_output_metadata(output_name=<span class="hljs-string">"instruct_datasets"</span>, metadata=_get_metadata_instruct_dataset(datasets))
    <span class="hljs-keyword">return</span> datasets
<span class="hljs-keyword">def</span> <span class="hljs-title">_get_metadata_instruct_dataset</span>(<span class="hljs-params">datasets: InstructTrainTestSplit</span>) -&gt; <span class="hljs-built_in">dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Any</span>]:
    instruct_dataset_categories = <span class="hljs-built_in">list</span>(datasets.train.keys())
    train_num_samples = {
        category: instruct_dataset.num_samples <span class="hljs-keyword">for</span> category, instruct_dataset <span class="hljs-keyword">in</span> datasets.train.items()
    }
    test_num_samples = {category: instruct_dataset.num_samples <span class="hljs-keyword">for</span> category, instruct_dataset <span class="hljs-keyword">in</span> datasets.test.items()}
    <span class="hljs-keyword">return</span> {
        <span class="hljs-string">"data_categories"</span>: instruct_dataset_categories,
        <span class="hljs-string">"test_split_size"</span>: datasets.test_split_size,
        <span class="hljs-string">"train_num_samples_per_category"</span>: train_num_samples,
        <span class="hljs-string">"test_num_samples_per_category"</span>: test_num_samples,
    }
</code></pre>
    <p class="normal">Also, you can easily download and access a specific version of the dataset using its <strong class="keyWord">Universally Unique Identifier</strong> (<strong class="keyWord">UUID</strong>), which you can find using the ZenML dashboard or CLI:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> zenml.client <span class="hljs-keyword">import</span> Client
artifact = Client().get_artifact_version(<span class="hljs-string">'8bba35c4-8ff9-4d8f-a039-08046efc9fdc'</span>)
loaded_artifact = artifact.load()
</code></pre>
    <p class="normal">The last step in<a id="_idIndexMarker097"/> exploring ZenML is understanding how to run and configure a ZenML pipeline.</p>
    <h3 id="_idParaDest-52" class="heading-3">How to run and configure a ZenML pipeline</h3>
    <p class="normal">All the ZenML pipelines <a id="_idIndexMarker098"/>can be called from the <code class="inlineCode">run.py</code> file, accessed at <code class="inlineCode">tools/run.py</code> in our GitHub repository. Within the <code class="inlineCode">run.py</code> file, we implemented a simple CLI that allows you to specify what pipeline to run. For example, to call the <code class="inlineCode">digital_data_etl</code> pipeline to crawl Maxime’s content, you have to run:</p>
    <pre class="programlisting con"><code class="hljs-con">python -m tools.run --run-etl --no-cache --etl-config-filename digital_data_etl_maxime_labonne.yaml
</code></pre>
    <p class="normal">Or, to crawl Paul’s content, you can run:</p>
    <pre class="programlisting con"><code class="hljs-con">python -m tools.run --run-etl --no-cache --etl-config-filename digital_data_etl_paul_iusztin.yaml
</code></pre>
    <p class="normal">As explained when introducing Poe the Poet, all our CLI commands used to interact with the project will be executed through Poe to simplify and standardize the project. Thus, we encapsulated these Python calls under the following <code class="inlineCode">poe</code> CLI commands:</p>
    <pre class="programlisting con"><code class="hljs-con">poetry poe run-digital-data-etl-maxime
poetry poe run-digital-data-etl-paul
</code></pre>
    <p class="normal">We only change the ETL config file name when scraping content for different people. ZenML allows us to inject specific configuration files at runtime as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">config_path = root_dir / <span class="hljs-string">"configs"</span> / etl_config_filename
assert config_path.exists(), f<span class="hljs-string">"Config file not found: { config_path }"</span>
run_args_etl = {
 <span class="hljs-string">"config_path"</span>: config_path,
 <span class="hljs-string">"run_name"</span>: f<span class="hljs-string">"digital_data_etl_run_{dt.now().strftime('</span><span class="hljs-variable">%Y</span><span class="hljs-string">_</span><span class="hljs-variable">%m</span><span class="hljs-string">_</span><span class="hljs-variable">%d</span><span class="hljs-string">_</span><span class="hljs-variable">%H</span><span class="hljs-string">_</span><span class="hljs-variable">%M</span><span class="hljs-string">_</span><span class="hljs-variable">%S</span><span class="hljs-string">')}"</span>
}
 digital_data_etl.with_options()(**run_args_etl)
</code></pre>
    <p class="normal">In the config file, we specify all the parameters that will input the pipeline as parameters. For <a id="_idIndexMarker099"/>example, the <code class="inlineCode">configs/digital_data_etl_maxime_labonne.yaml</code> configuration file looks as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">parameters:
  user_full_name: Maxime Labonne <span class="hljs-comment"># [First Name(s)] [Last Name]</span>
  links:
    <span class="hljs-comment"># Personal Blog</span>
    - https://mlabonne.github.io/blog/posts/2024-07-29_Finetune_Llama31.html
    - https://mlabonne.github.io/blog/posts/2024-07-15_The_Rise_of_Agentic_Data_Generation.html
    # Substack
    - https://maximelabonne.substack.com/p/uncensor-any-llm-with-abliteration-d30148b7d43e
    … <span class="hljs-comment"># More links</span>
</code></pre>
    <p class="normal">Where the <code class="inlineCode">digital_data_etl</code> function signature looks like this:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-meta">@pipeline</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">digital_data_etl</span>(<span class="hljs-params">user_full_name: </span><span class="hljs-built_in">str</span><span class="hljs-params">, links: </span><span class="hljs-built_in">list</span><span class="hljs-params">[</span><span class="hljs-built_in">str</span><span class="hljs-params">]</span>) -&gt; <span class="hljs-built_in">str</span>:
</code></pre>
    <p class="normal">This approach allows us to configure each pipeline at runtime without modifying the code. We can also clearly track the inputs for all our pipelines, ensuring reproducibility. As seen in <em class="italic">Figure 2.10</em>, we have one or more configs for each pipeline.</p>
    <figure class="mediaobject"><img src="../Images/B31105_02_10.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.10: ZenML pipeline configs</p>
    <p class="normal">Other popular<a id="_idIndexMarker100"/> orchestrators similar to ZenML that we’ve personally tested and consider powerful are Airflow, Prefect, Metaflow, and Dagster. Also, if you are a heavy user of Kubernetes, you can opt for Agro Workflows or Kubeflow, the latter of which works only on top of Kubernetes. We still consider ZenML the best trade-off between ease of use, features, and costs. Also, none of these tools offer the stack feature that is offered by ZenML, which allows it to avoid vendor-locking you in to any cloud ecosystem.</p>
    <p class="normal">In <em class="italic">Chapter 11</em>, we will explore in more depth how to leverage an orchestrator to implement MLOps <a id="_idIndexMarker101"/>best practices. But now that we understand ZenML, what it is helpful for, and how to use it, let’s move on to the experiment tracker.</p>
    <h2 id="_idParaDest-53" class="heading-2">Comet ML: experiment tracker</h2>
    <p class="normal">Training ML <a id="_idIndexMarker102"/>models is an entirely iterative and <a id="_idIndexMarker103"/>experimental process. Unlike traditional software development, it involves running multiple parallel experiments, comparing them based on predefined metrics, and deciding which one should advance to production. An experiment tracking tool allows you to log all the necessary information, such as metrics and visual representations of your model predictions, to compare all your experiments and quickly select the best model. Our LLM project is no exception.</p>
    <p class="normal">As illustrated in <em class="italic">Figure 2.11</em>, we used Comet to track metrics such as training and evaluation loss or the value of the gradient norm across all our experiments.</p>
    <figure class="mediaobject"><img src="../Images/B31105_02_11.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.11: Comet ML training metrics example</p>
    <p class="normal">Using an experiment tracker, you can go beyond training and evaluation metrics and log your training<a id="_idIndexMarker104"/> hyperparameters to track different configurations between experiments. </p>
    <p class="normal">It also logs out-of-the-box system metrics such as GPU, CPU, or memory utilization to give you a clear picture of what resources you need during training and where potential bottlenecks slow down your training, as seen in <em class="italic">Figure 2.12</em>.</p>
    <figure class="mediaobject"><img src="../Images/B31105_02_12.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.12: Comet ML system metrics example</p>
    <p class="normal">You don’t have<a id="_idIndexMarker105"/> to set up Comet locally. We will use their online version for free without any constraints throughout this book. Also, if you want to look more in-depth into the Comet ML experiment tracker, we made the training experiments tracked with Comet ML public while fine-tuning our LLM Twin models. You can access them here: <a href="https://www.comet.com/mlabonne/llm-twin-training/view/new/panels"><span class="url">https://www.comet.com/mlabonne/llm-twin-training/view/new/panels</span></a>.</p>
    <p class="normal">Other popular experiment trackers are W&amp;B, MLflow, and Neptune. We’ve worked with all of them and can state that they all have mostly the same features, but Comet ML differentiates itself through its ease of use and intuitive interface. Let’s <a id="_idIndexMarker106"/>move on to the final piece of the MLOps <a id="_idIndexMarker107"/>puzzle: Opik for prompt monitoring.</p>
    <h2 id="_idParaDest-54" class="heading-2">Opik: prompt monitoring</h2>
    <p class="normal">You cannot <a id="_idIndexMarker108"/>use <a id="_idIndexMarker109"/>standard tools and techniques when logging and monitoring prompts. The reason for this is complicated. We will dig into it in <em class="italic">Chapter 11</em>. However, to quickly give you some understanding, you cannot use standard logging tools as prompts are complex and unstructured chains. </p>
    <p class="normal">When interacting with an LLM application, you chain multiple input prompts and the generated output into a trace, where one prompt depends on previous prompts. </p>
    <p class="normal">Thus, instead of plain text logs, you need an intuitive way to group these traces into a specialized dashboard that makes debugging and monitoring traces of prompts easier.</p>
    <p class="normal">We used Opik, an open-source tool made by Comet, as our prompt monitoring tool because it follows Comet’s philosophy of simplicity and ease of use, which is currently relatively rare in the LLM landscape. Other options offering similar features are Langfuse (open source, <a href="https://langfuse.com"><span class="url">https://langfuse.com</span></a>), Galileo (not open source, <a href="https://rungalileo.io"><span class="url">rungalileo.io</span></a>), and LangSmith (not open source, <a href="https://www.langchain.com/langsmith"><span class="url">https://www.langchain.com/langsmith</span></a>), but we found their solutions more cumbersome to use and implement. Opik, along with its serverless option, also provides a free <a id="_idIndexMarker110"/>open-source <a id="_idIndexMarker111"/>version that you have complete control over. You can read more on Opik at <a href="https://github.com/comet-ml/opik"><span class="url">https://github.com/comet-ml/opik</span></a>.</p>
    <h1 id="_idParaDest-55" class="heading-1">Databases for storing unstructured and vector data</h1>
    <p class="normal">We also want to present the NoSQL and<a id="_idIndexMarker112"/> vector databases we will use within our examples. When working locally, they are already integrated through Docker. Thus, when running <code class="inlineCode">poetry poe local-infrastructure-up</code>, as instructed a few sections above, local images of Docker for both databases will be pulled and run on your machine. Also, when deploying the project, we will show you how to use their serverless option and integrate it with the rest of the LLM Twin project.</p>
    <h2 id="_idParaDest-56" class="heading-2">MongoDB: NoSQL database</h2>
    <p class="normal">MongoDB is one <a id="_idIndexMarker113"/>of today’s most<a id="_idIndexMarker114"/> popular, robust, fast, and feature-rich NoSQL databases. It integrates well with most cloud ecosystems, such as AWS, Google Cloud, Azure, and Databricks. Thus, using MongoDB as our NoSQL database was a no-brainer.</p>
    <p class="normal">When we wrote this book, MongoDB was used by big players such as Novo Nordisk, Delivery Hero, Okta, and Volvo. This widespread adoption suggests that MongoDB will remain a leading NoSQL database for a long time.</p>
    <p class="normal">We use MongoDB as a NoSQL database to store the raw data we collect from the internet before processing it and pushing it into the vector database. As we work with unstructured<a id="_idIndexMarker115"/> text data, the<a id="_idIndexMarker116"/> flexibility of the NoSQL database fits like a charm.</p>
    <h2 id="_idParaDest-57" class="heading-2">Qdrant: vector database</h2>
    <p class="normal">Qdrant (<a href="https://qdrant.tech/"><span class="url">https://qdrant.tech/</span></a>) is one of the most popular, robust, and feature-rich vector <a id="_idIndexMarker117"/>databases. We could have <a id="_idIndexMarker118"/>used almost any vector database for our small MVP, but we wanted to pick something light and likely to be used in the industry for many years to come. </p>
    <p class="normal">We will use Qdrant to store the data from MongoDB after it’s processed and transformed for GenAI usability.</p>
    <p class="normal">Qdrant is used by big players such as X (formerly Twitter), Disney, Microsoft, Discord, and Johnson &amp; Johnson. Thus, it is highly probable that Qdrant will remain in the vector database game for a long time.</p>
    <p class="normal">While writing the book, other popular options were Milvus, Redis, Weaviate, Pinecone, Chroma, and pgvector (a PostgreSQL plugin for vector indexes). We found that Qdrant offers the best trade-off between RPS, latency, and index time, making it a solid choice for many generative AI applications.</p>
    <p class="normal">Comparing all the vector databases in detail could be a chapter in itself. We don’t want to do that here. Still, if curious, you can check the <em class="italic">Vector DB Comparison</em> resource from Superlinked at <a href="https://superlinked.com/vector-db-comparison"><span class="url">https://superlinked.com/vector-db-comparison</span></a>, which compares all the top vector databases in terms of everything you can think about, from the license and release year to database features, embedding<a id="_idIndexMarker119"/> models, and frameworks<a id="_idIndexMarker120"/> supported.</p>
    <h1 id="_idParaDest-58" class="heading-1">Preparing for AWS</h1>
    <p class="normal">This last part of the<a id="_idIndexMarker121"/> chapter will focus on setting up an AWS account (if you don’t already have one), an AWS access key, and the CLI. Also, we will look into what SageMaker is and why we use it.</p>
    <p class="normal">We picked AWS as our cloud provider because it’s the most popular out there and the cloud in which we (the writers) have the most experience. The reality is that other big cloud providers, such as GCP or Azure, offer similar services. Thus, depending on your specific application, there is always a trade-off between development time (in which you have the most experience), features, and costs. But for our MVP, AWS, it’s the perfect option as it provides robust features for everything we need, such as S3 (object storage), ECR (container registry), and SageMaker (compute for training and inference).</p>
    <h2 id="_idParaDest-59" class="heading-2">Setting up an AWS account, an access key, and the CLI</h2>
    <p class="normal">As AWS could <a id="_idIndexMarker122"/>change its UI/UX, the best way<a id="_idIndexMarker123"/> to<a id="_idIndexMarker124"/> instruct you on how to create an AWS account is by redirecting you to their official tutorial: <a href="https://docs.aws.amazon.com/accounts/latest/reference/manage-acct-creating.html"><span class="url">https://docs.aws.amazon.com/accounts/latest/reference/manage-acct-creating.html</span></a>.</p>
    <p class="normal">After successfully creating an AWS account, you can access the AWS console at <a href="http://console.aws.amazon.com"><span class="url">http://console.aws.amazon.com</span></a>. Select <strong class="screenText">Sign in using root user email </strong>(found under the <strong class="screenText">Sign in </strong>button), then enter your account’s email address and password.</p>
    <p class="normal">Next, we must generate access keys to access AWS programmatically. The best option to do so is first to create an IAM user with administrative access as described in this AWS official tutorial: <a href="https://docs.aws.amazon.com/streams/latest/dev/setting-up.html"><span class="url">https://docs.aws.amazon.com/streams/latest/dev/setting-up.html</span></a></p>
    <p class="normal">For production accounts, it is best practice to grant permissions with a policy of least privilege, giving each user only the permissions they require to perform their role. However, to simplify the setup of our test account, we will use the <code class="inlineCode">AdministratorAccess</code> managed policy, which gives our user full access, as explained in the tutorial<a id="_idIndexMarker125"/> above<a id="_idIndexMarker126"/> and <a id="_idIndexMarker127"/>illustrated in <em class="italic">Figure 2.13</em>.</p>
    <figure class="mediaobject"><img src="../Images/B31105_02_13.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.13: IAM user permission policies example</p>
    <p class="normal">Next, you have to create an access key for the IAM user you just created using the following tutorial: <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html"><span class="url">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html</span></a>.</p>
    <p class="normal">The access keys will look as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">aws_access_key_id</span> = &lt;your_access_key_id&gt;
<span class="hljs-attr">aws_secret_access_key</span> = &lt;your_secret_access_key&gt;
</code></pre>
    <p class="normal">Just be careful to store them somewhere safe, as you won’t be able to access them after you create them. Also, be cautious with who you share them, as they could be used to access your AWS account and manipulate various AWS resources.</p>
    <p class="normal">The last step is to install the AWS CLI and configure it with your newly created access keys. You can install the AWS CLI using the following link: <a href="https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html"><span class="url">https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html</span></a>.</p>
    <p class="normal">After installing the AWS CLI, you can configure it by running <code class="inlineCode">aws configure</code>. Here is an example of our AWS configuration:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-section">[default]</span>
<span class="hljs-attr">aws_access_key_id</span> = *************
<span class="hljs-attr">aws_secret_access_key</span> = ************
<span class="hljs-attr">region</span> = eu-central-<span class="hljs-number">1</span>
<span class="hljs-attr">output</span> = json
</code></pre>
    <p class="normal">For more details <a id="_idIndexMarker128"/>on<a id="_idIndexMarker129"/> how to configure the<a id="_idIndexMarker130"/> AWS CLI, check out the following tutorial: <a href="https://docs.aws.amazon.com/cli/v1/userguide/cli-configure-files.html"><span class="url">https://docs.aws.amazon.com/cli/v1/userguide/cli-configure-files.html</span></a>.</p>
    <p class="normal">Also, to configure the project with your AWS credentials, you must fill in the following variables within your <code class="inlineCode">.env</code> file:</p>
    <pre class="programlisting con"><code class="hljs-con">AWS_REGION="eu-central-1" # Change it with your AWS region. By default, we use "eu-central-1".
AWS_ACCESS_KEY="&lt;your_aws_access_key&gt;"
AWS_SECRET_KEY="&lt;your_aws_secret_key&gt;"
</code></pre>
    <div class="note">
      <p class="normal"><strong class="keyWord">An important note about costs associated with hands-on tasks in this book</strong></p>
      <p class="normal">All the cloud services used across the book stick to their freemium option, except AWS. Thus, if you use a personal AWS account, you will be responsible for AWS costs as you follow along in this book. While some services may fall under AWS Free Tier usage, others will not. Thus, you are responsible for checking your billing console regularly.</p>
      <p class="normal">Most of the costs will come when testing SageMaker for training and inference. Based on our tests, the AWS costs can vary between $50 and $100 using the specifications provided in this book and repository.</p>
      <p class="normal">See the AWS<a id="_idIndexMarker131"/> documentation<a id="_idIndexMarker132"/> on setting up<a id="_idIndexMarker133"/> billing alarms to monitor your costs at <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/monitor_estimated_charges_with_cloudwatch.html"><span class="url">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/monitor_estimated_charges_with_cloudwatch.html</span></a>.</p>
    </div>
    <h2 id="_idParaDest-60" class="heading-2">SageMaker: training and inference compute</h2>
    <p class="normal">The last topic of this<a id="_idIndexMarker134"/> chapter is understanding SageMaker and<a id="_idIndexMarker135"/> why we decided to use it. SageMaker is an ML platform used to train and deploy ML models. An official definition is as follows: AWS SageMaker is a fully managed machine learning service by AWS that enables developers and data scientists to build, train, and deploy machine learning models at scale. It simplifies the process by handling the underlying infrastructure, allowing users to focus on developing high-quality models efficiently.</p>
    <p class="normal">We will use SageMaker to fine-tune and operationalize our training pipeline on clusters of GPUs and to deploy our custom LLM Twin model as a REST API that can be accessed in real time from anywhere in the world.</p>
    <h3 id="_idParaDest-61" class="heading-3">Why AWS SageMaker?</h3>
    <p class="normal">We must also<a id="_idIndexMarker136"/> discuss why we chose AWS SageMaker over simpler and more cost-effective options, such as AWS Bedrock. First, let’s explain Bedrock and its benefits.</p>
    <p class="normal">Amazon Bedrock is a serverless solution for deploying LLMs. Serverless means that there are no servers or infrastructure to manage. It provides pre-trained models, which you can access directly through API calls. When we wrote this book, they provided support only for Mistral, Flan, Llama 2, and Llama 3 (quite a limited list of options). You can send input data and receive predictions from the models without managing the underlying infrastructure or software. This approach significantly reduces the complexity and time required to integrate AI capabilities into applications, making it more accessible to developers with limited machine learning expertise. However, this ease of integration comes at the cost of limited customization options, as you’re restricted to the pre-trained models and APIs provided by Amazon Bedrock. In terms of pricing, Bedrock uses a simple pricing model based on the number of API calls. This straightforward pricing structure makes it more efficient to estimate and control costs.</p>
    <p class="normal">Meanwhile, SageMaker provides a comprehensive platform for building, training, and deploying machine learning models. It allows you to customize your ML processes entirely or even use the platform for research. That’s why SageMaker is mainly used by data scientists and machine learning experts who know how to program, understand machine learning concepts, and are comfortable working with cloud platforms such as AWS. SageMaker is a double-edged sword regarding costs, following a pay-as-you-go pricing model similar to most AWS services. This means you have to pay for the usage of computing resources, storage, and any other services required to build your applications.</p>
    <p class="normal">In contrast to Bedrock, even if the SageMaker endpoint is not used, you will still pay for the deployed<a id="_idIndexMarker137"/> resources on AWS, such as online EC2 instances. Thus, you have to design autoscaling systems that delete unused resources. To conclude, Bedrock offers an out-of-the-box solution that allows you to quickly deploy an API endpoint powered by one of the available foundation models. Meanwhile, SageMaker is a multi-functional platform enabling you to customize your ML logic fully.</p>
    <p class="normal">So why did we choose SageMaker over Bedrock? Bedrock would have been an excellent solution for quickly prototyping something, but this is a book on LLM engineering, and our goal is to dig into all the engineering aspects that Bedrock tries to mask away. Thus, we chose SageMaker because of its high level of customizability, allowing us to show you all the engineering required to deploy a model.</p>
    <p class="normal">In reality, even SageMaker isn’t fully customizable. If you want complete control over your deployment, use EKS, AWS’s Kubernetes self-managed service. In this case, you have direct access to the virtual machines, allowing you to fully customize how you build your ML pipelines, how they interact, and how you manage your resources. You could do the same thing with AWS ECS, AWS’s version of Kubernetes. Using EKS or ECS, you could also reduce the costs, as these services cost considerably less.</p>
    <p class="normal">To conclude, SageMaker strikes a balance between complete control and customization and a fully managed service that hides all the engineering complexity behind<a id="_idIndexMarker138"/> the scenes. This balance ensures that you have the control you need while also benefiting from the managed service’s convenience.</p>
    <h1 id="_idParaDest-62" class="heading-1">Summary</h1>
    <p class="normal">In this chapter, we reviewed the core tools used across the book. First, we understood how to install the correct version of Python that supports our repository. Then, we looked over how to create a virtual environment and install all the dependencies using Poetry. Finally, we understood how to use a task execution tool like Poe the Poet to aggregate all the commands required to run the application.</p>
    <p class="normal">The next step was to review all the tools used to ensure MLOps best practices, such as a model registry to share our models, an experiment tracker to manage our training experiments, an orchestrator to manage all our ML pipelines and artifacts, and metadata to manage all our files and datasets. We also understood what type of databases we need to implement the LLM Twin use case. Finally, we explored the process of setting up an AWS account, generating an access key, and configuring the AWS CLI for programmatic access to the AWS cloud. We also gained a deep understanding of AWS SageMaker and the reasons behind choosing it to build our LLM Twin application.</p>
    <p class="normal">In the next chapter, we will explore the implementation of the LLM Twin project by starting with the data collection ETL that scrapes posts, articles, and repositories from the internet and stores them in a data warehouse.</p>
    <h1 id="_idParaDest-63" class="heading-1">References</h1>
    <ul>
      <li class="bulletList">Acsany, P. (2024, February 19). <em class="italic">Dependency Management With Python Poetry</em>. <a href="https://realpython.com/dependency-management-python-poetry/"><span class="url">https://realpython.com/dependency-management-python-poetry/</span></a></li>
      <li class="bulletList">Comet.ml. (n.d.).<em class="italic"> comet-ml/opik: Open-source end-to-end LLM Development Platform</em>. GitHub. <a href="https://github.com/comet-ml/opik"><span class="url">https://github.com/comet-ml/opik</span></a></li>
      <li class="bulletList">Czakon, J. (2024, September 25). <em class="italic">ML Experiment Tracking: What It Is, Why It Matters, and How to Implement It</em>. neptune.ai. <a href="https://neptune.ai/blog/ml-experiment-tracking"><span class="url">https://neptune.ai/blog/ml-experiment-tracking</span></a></li>
      <li class="bulletList">Hopsworks. (n.d.). <em class="italic">ML Artifacts (ML Assets)? </em>Hopsworks. <a href="https://www.hopsworks.ai/dictionary/ml-artifacts"><span class="url">https://www.hopsworks.ai/dictionary/ml-artifacts</span></a></li>
      <li class="bulletList"><em class="italic">Introduction | Documentation | Poetry – Python dependency management and packaging made easy</em>. (n.d.). <a href="https://python-poetry.org/docs"><span class="url">https://python-poetry.org/docs</span></a></li>
      <li class="bulletList">Jones, L. (2024, March 21). <em class="italic">Managing Multiple Python Versions With pyenv</em>. <a href="https://realpython.com/intro-to-pyenv/"><span class="url">https://realpython.com/intro-to-pyenv/</span></a></li>
      <li class="bulletList">Kaewsanmua, K. (2024, January 3). <em class="italic">Best Machine Learning Workflow and Pipeline Orchestration Tools</em>. neptune.ai. <a href="https://neptune.ai/blog/best-workflow-and-pipeline-orchestration-tools"><span class="url">https://neptune.ai/blog/best-workflow-and-pipeline-orchestration-tools</span></a></li>
      <li class="bulletList">MongoDB. (n.d.). <em class="italic">What is NoSQL? </em>NoSQL databases explained. <a href="https://www.mongodb.com/resources/basics/databases/nosql-explained"><span class="url">https://www.mongodb.com/resources/basics/databases/nosql-explained</span></a></li>
      <li class="bulletList">Nat-N. (n.d.).<em class="italic"> nat-n/poethepoet: A task runner that works well with poetry.</em> GitHub. <a href="https://github.com/nat-n/poethepoet"><span class="url">https://github.com/nat-n/poethepoet</span></a></li>
      <li class="bulletList">Oladele, S. (2024, August 29). <em class="italic">ML Model Registry: The Ultimate Guide</em>. neptune.ai. <a href="https://neptune.ai/blog/ml-model-registry"><span class="url">https://neptune.ai/blog/ml-model-registry</span></a></li>
      <li class="bulletList">Schwaber-Cohen, R. (n.d.). <em class="italic">What is a Vector Database &amp; How Does it Work? Use Cases + Examples</em>. Pinecone. <a href="https://www.pinecone.io/learn/vector-database/"><span class="url">https://www.pinecone.io/learn/vector-database/</span></a></li>
      <li class="bulletList"><em class="italic">Starter guide | ZenML Documentation</em>. (n.d.). <a href="https://docs.zenml.io/user-guide/starter-guide"><span class="url">https://docs.zenml.io/user-guide/starter-guide</span></a></li>
      <li class="bulletList"><em class="italic">Vector DB Comparison</em>. (n.d.). <a href="https://superlinked.com/vector-db-comparison"><span class="url">https://superlinked.com/vector-db-comparison</span></a></li>
    </ul>
    <h1 id="_idParaDest-64" class="heading-1">Join our book’s Discord space</h1>
    <p class="normal">Join our community’s Discord space for discussions with the authors and other readers:</p>
    <p class="normal"><a href="https://packt.link/llmeng"><span class="url">https://packt.link/llmeng</span></a></p>
    <p class="normal"><span class="url"><img src="../Images/QR_Code79969828252392890.png" alt=""/></span></p>
  </div>
</body></html>