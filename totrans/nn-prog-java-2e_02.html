<html><head></head><body><div><div><div><div><div><h1 class="title"><a id="ch02" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Chapter 2. Getting Neural Networks to Learn</h1></div></div></div><p class="calibre11">Now that you have been introduced to neural networks, it is time to learn about their learning process. In this chapter, we're going to explore the concepts involved with neural network learning, along with their implementation in Java. We will make a review on the foundations and inspirations for the neural learning process that will guide us in implementation of learning algorithms in Java to be applied on our neural network code. In summary, these are the concepts addressed in this chapter:</p><div><ul class="itemizedlist"><li class="listitem">Learning ability</li><li class="listitem">How learning helps</li><li class="listitem">Learning paradigms</li><li class="listitem">Supervised</li><li class="listitem">Unsupervised</li><li class="listitem">The learning process</li><li class="listitem">Optimization foundations</li><li class="listitem">The cost function</li><li class="listitem">Error measurement</li><li class="listitem">Learning algorithms</li><li class="listitem">Delta rule</li><li class="listitem">Hebbian rule</li><li class="listitem">Adaline/perceptron</li><li class="listitem">Training, test, and validation</li><li class="listitem">Dataset splitting</li><li class="listitem">Overfitting and overtraining</li><li class="listitem">Generalization</li></ul></div><div><div><div><div><h1 class="title2"><a id="ch02lvl1sec18" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Learning ability in neural networks</h1></div></div></div><p class="calibre11">What is really <a id="id69" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>amazing in neural networks is their capacity to <a id="id70" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>learn from the environment, just like brain-gifted beings are able to do so. We, as humans, experience the learning process through observations and repetitions, until some task, or concept is completely mastered. From the physiological point of view, the learning process in the human brain is a reconfiguration of the neural connections between the nodes (neurons), which results in a new thinking structure.</p><p class="calibre11">While the connectionist nature of neural networks distributes the learning process all over the entire structure, this feature makes this structure flexible enough to learn a wide variety of knowledge. As opposed to ordinary digital computers that can execute only tasks they are programmed to do, neural systems are able to improve and perform new activities according to some satisfaction criteria. In other words, neural networks don't need to be programmed; they learn the program by themselves.</p><div><div><div><div><h2 class="title5"><a id="ch02lvl2sec17" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How learning helps solving problems</h2></div></div></div><p class="calibre11">Considering<a id="id71" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> that every task to solve may have a huge number of theoretically possible solutions, the learning process seeks to find an optimal solution that can produce a satisfying result. The use of structures such as <strong class="calibre12">artificial neural networks</strong> (<strong class="calibre12">ANN</strong>) is encouraged due to their ability to acquire knowledge of any type, strictly by receiving input stimuli, that is, data relevant to the task/problem. At first, the <a id="id72" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>ANN will produce a random result and an error, and based on this error, the ANN parameters are adjusted.</p><div><div><h3 class="title6"><a id="tip04" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Tip</h3><p class="calibre17">We can then think of the ANN parameters (weights) as the components of a solution. Let's imagine that each weight corresponds to a dimension and one single solution represents a single point in the solution hyperspace. For each single solution, there is an error measure informing how far that solution is from the satisfaction criteria. The learning algorithm then iteratively seeks a solution closer to the satisfaction criteria.</p></div></div></div></div></div></div>



  
<div><div><div><div><div><h1 class="title2"><a id="ch02lvl1sec19" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Learning paradigms</h1></div></div></div><p class="calibre11">There are basically <a id="id73" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>two types of learning for neural networks, namely supervised, and unsupervised. The learning in the human mind, for example, also works in this way. We are able to build knowledge from observations without any target (unsupervised) or we can have a teacher who shows us the right pattern to follow (supervised). The difference between these two paradigms relies mainly on the relevancy of a target pattern, and varies from problem to problem.</p><div><div><div><div><h2 class="title5"><a id="ch02lvl2sec18" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Supervised learning</h2></div></div></div><p class="calibre11">This learning type <a id="id74" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>deals with pairs of <em class="calibre16">xs</em> (independent values), and <em class="calibre16">ys</em> (dependent values) with the objective to map them in a function . Here<a id="id75" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> the Y data is the <em class="calibre16">supervisor</em>, the target desired outputs, and the X are the source independent data that jointly generate the Y data. It is analogous to a teacher who is teaching somebody a certain task to be performed:</p><div><img src="img/B05964_02_01.jpg" alt="Supervised learning" class="calibre46"/></div><p class="calibre11">One particular feature of this learning paradigm is that there is a direct error reference which is just the comparison between the target and the current actual result. The network parameters are fed into a cost function which quantifies the mismatch between desired and actual outputs.</p><div><div><h3 class="title6"><a id="tip05" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Tip</h3><p class="calibre17">A cost function is just a measurement to be minimized in an optimization problem. That means one seeks to find the parameters that drive the cost function to the lowest possible value.</p><p class="calibre17">The cost function will be covered in detail later in this chapter</p></div></div><p class="calibre11">The supervised learning is suitable for tasks having a defined pattern to be reproduced. Some examples include classification of images, speech recognition, function approximation, and forecasting. Note that the neural network should be provided a previous knowledge of both input independent values (X) and the output dependent values (Y). The presence of a dependent output value is a necessary condition for the learning to be supervised.</p></div><div><div><div><div><h2 class="title5"><a id="ch02lvl2sec19" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Unsupervised learning</h2></div></div></div><p class="calibre11">In <a id="id76" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>unsupervised learning, we deal only with data <a id="id77" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>without any labeling or classification. Instead, one tries to make an inference and extract knowledge by taking into account only the independent data X:</p><div><img src="img/B05964_02_02.jpg" alt="Unsupervised learning" class="calibre47"/></div><p class="calibre11">This is <a id="id78" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>analogous to self-learning, when someone<a id="id79" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> takes into account his/her own experience and a set of supporting criteria. In unsupervised learning, we don't have a defined desired pattern; instead, we use the provided data to infer a dependent output Y without any supervision.</p><div><div><h3 class="title6"><a id="tip06" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Tip</h3><p class="calibre17">In unsupervised learning, the closer the independent data is, more similar the generated output should be, and this should be considered in the cost function, as opposed to the supervised paradigm.</p></div></div><p class="calibre11">Examples of tasks that unsupervised learning can be applied to are clustering, data compression, statistical modeling, and language modeling. This learning paradigm will be covered in more detail in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch04.xhtml" title="Chapter 4. Self-Organizing Maps">Chapter 4</a>, <em class="calibre16">Self-Organizing Maps</em>.</p></div></div></div>



  
<div><div><div><div><div><h1 class="title2"><a id="ch02lvl1sec20" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>The learning process</h1></div></div></div><p class="calibre11">So far, we <a id="id80" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>have theoretically defined the learning process and how it is carried out. But in practice, we must dive a little bit deeper into the mathematical logic, in order to implement the learning algorithm itself. For simplicity, in this chapter, we are basically covering the supervised learning case; however, we will present here a rule for updating weights in unsupervised learning. A learning algorithm is a procedure that drives the learning process of neural networks, and it is strongly determined by the neural network architecture. From the mathematical point of view, one wishes to find the optimal weights W that can drive the cost function C(X, Y) to the lowest possible value. However, sometimes the learning process cannot find a good set of weights capable of meeting the acceptance criteria, but a stop condition must be set to prevent the neural network from learning forever and thereby causing the Java program to freeze.</p><p class="calibre11">In general, this<a id="id81" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> process is carried out in the fashion presented in the following flowchart:</p><div><img src="img/B05964_02_03.jpg" alt="The learning process" class="calibre48"/></div><div><div><div><div><h2 class="title5"><a id="ch02lvl2sec20" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>The cost function finding the way down to the optimum</h2></div></div></div><p class="calibre11">Now let's <a id="id82" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>find out in detail what role the cost function<a id="id83" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> plays. Let's think of cost function as a two-variable function whose shape is represented by a hypersurface. For simplicity, let's consider for now only two weights (two-dimensional space plus height representing cost function). Suppose our cost function has the following shape:</p><div><img src="img/B05964_02_04.jpg" alt="The cost function finding the way down to the optimum" class="calibre49"/></div><p class="calibre11">Visually, we <a id="id84" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>can see that there is an optimum, by which the <a id="id85" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>cost function roughly approaches zero. But how can we make this programmatically? The answer lies in the mathematical optimization, whereby the cost function is defined as an optimization problem:</p><div><img src="img/B05964_02_04_01.jpg" alt="The cost function finding the way down to the optimum" class="calibre50"/></div><p class="calibre11">By recalling the optimization Fermat's theorems, the optimal solution lies in a place where the surface slope should be zero at all dimensions, that is, the partial derivative should be zero, and it should be convex (for the minimum case). Considering that one starts with an arbitrary solution W, the search for the optimum should take into account the direction<a id="id86" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> to which the surface height is going down. This is the so-called gradient method.</p></div><div><div><div><div><h2 class="title5"><a id="ch02lvl2sec21" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Learning in progress - weight update</h2></div></div></div><p class="calibre11">According to the <a id="id87" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>cost function used, an update rule will dictate how the weights, the neural flexible parameters, should be changed, so the cost function will have a lower value at the new weights:</p><div><img src="img/B05964_02_04_02.jpg" alt="Learning in progress - weight update" class="calibre51"/></div><p class="calibre11">Here, k refers<a id="id88" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> to the kth iteration and <em class="calibre16">W(k)</em> refers to the neural weights at the kth iteration, and subsequently <em class="calibre16">k+1</em> refers to the next iteration.</p><p class="calibre11">The weight update operation can be performed in online or batch mode. Online here implies that the weights are updated after every single record from the dataset. Batch update means that first all the records from the dataset are presented to the neural network before it starts updating its weights. This will be explored in detail in the code at the end of this chapter.</p></div><div><div><div><div><h2 class="title5"><a id="ch02lvl2sec22" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Calculating the cost function</h2></div></div></div><p class="calibre11">When a <a id="id89" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>neural network learns, it receives data<a id="id90" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> from an environment and adapts its weights according to the objective. This data is referred to as the training dataset and has several samples. The idea behind the word training lies in the process of adapting the neural weights, as if they were <em class="calibre16">training</em> to give the desired response in the neural network. While the neural network is still learning, there is an error between the target outputs (Y) and the neural outputs (), in the supervised case:</p><div><img src="img/B05964_02_04_03.jpg" alt="Calculating the cost function" class="calibre52"/></div><div><div><h3 class="title6"><a id="tip07" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Tip</h3><p class="calibre17">Some literature about neural networks identifies the target variable with the letter T, and the neural output as Y, while in this book we are going to the denote it as Y and, to not confuse the reader, since it was presented initially as Y.</p></div></div><p class="calibre11">Well, given that the training dataset has multiple values, there will be N values of errors for each single record. So, how to get an overall error? One intuitive approach is to get an average of all errors, but this is misleading. The error vector can take on both positive and negative values, therefore an average of all error values is very likely to be closer to zero, regardless of how big the error measurements may be. Using the absolute value to generate an average seems to be a smarter approach, but this function has a discontinuity at the origin, what is awkward in calculating its derivative:</p><div><img src="img/B05964_02_05.jpg" alt="Calculating the cost function" class="calibre53"/></div><p class="calibre11">So, the <a id="id91" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>reasonable option we have is to use the <a id="id92" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>average of a quadratic sum of the error, also<a id="id93" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> known as <strong class="calibre12">mean squared error (MSE)</strong>:</p><div><img src="img/B05964_02_05_01.jpg" alt="Calculating the cost function" class="calibre54"/></div></div><div><div><div><div><h2 class="title5"><a id="ch02lvl2sec23" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>General error and overall error</h2></div></div></div><p class="calibre11">We need to <a id="id94" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>clarify one thing before going further. The neural <a id="id95" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>network being a multiple output structure, we have to deal with the multiple output case, when instead of an error vector, we will have an error matrix:</p><div><img src="img/B05964_02_05_02.jpg" alt="General error and overall error" class="calibre55"/></div><p class="calibre11">Well, in such <a id="id96" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>cases, there may be a huge number of errors to work <a id="id97" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>with, whether regarding one specific output, a specific record, or the whole dataset. To facilitate understanding, let's call the specific-to-record error the general error, by which all output errors are given one scalar for the general output error; and the error referring to the whole data as overall error.</p><p class="calibre11">The general error for single output network is a mere difference between target and output, but in the multiple output case, it needs be composed of each output error. As we saw, the squared error is a suitable approach to summarize error measures, therefore the general error can be calculated using the square of each output error:</p><div><img src="img/B05964_02_05_03.jpg" alt="General error and overall error" class="calibre56"/></div><p class="calibre11">As for the overall error, it actually considers the general error but for all records in the dataset. Since the dataset can be huge, it is better to calculate the overall error using the MSE of the quadratic general errors.</p></div><div><div><div><div><h2 class="title5"><a id="ch02lvl2sec24" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Can the neural network learn forever? When is it good to stop?</h2></div></div></div><p class="calibre11">As the learning<a id="id98" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> process is run, the neural network must give results closer and closer to the expectation, until finally it reaches the acceptation criteria or one limitation in learning iterations, that we'll call epochs. The learning process is then considered to be finished when one of these conditions is met:</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre12">Satisfaction criterion</strong>: minimum overall error or minimum weight distance, according to<a id="id99" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> the learning paradigm</li><li class="listitem"><strong class="calibre12">Maximum number of epochs</strong></li></ul></div></div></div></div>



  
<div><div><div><div><div><h1 class="title2"><a id="ch02lvl1sec21" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Examples of learning algorithms</h1></div></div></div><p class="calibre11">Let's now merge the theoretical content presented so far together into simple examples of learning <a id="id100" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>algorithms. In this chapter, we are going to explore a couple of learning algorithms in single layer neural networks; multiple layers will be covered in the next chapter.</p><p class="calibre11">In the Java code, we will create one new superclass <code class="literal">LearningAlgorithm</code> in a new package <code class="literal">edu.packt.neural.learn</code>. Another useful package called <code class="literal">edu.packt.neural.data</code> will be created to handle datasets that will be processed by the neural network, namely the classes <code class="literal">NeuralInputData</code>, and <code class="literal">NeuralOutputData</code>, both referenced by the <code class="literal">NeuralDataSet</code> class. We recommend the reader takes a glance at the code documentation to understand how these classes are organized, to save text space here.</p><p class="calibre11">The <code class="literal">LearningAlgorithm</code> class has the following attributes and methods:</p><div><pre class="programlisting">public abstract class LearningAlgorithm {
    protected NeuralNet neuralNet;
    public enum LearningMode {ONLINE,BATCH};
    protected enum LearningParadigm {SUPERVISED,UNSUPERVISED};
//…
    protected int MaxEpochs=100;
    protected int epoch=0;
    protected double MinOverallError=0.001;
    protected double LearningRate=0.1;
    protected NeuralDataSet trainingDataSet;
    protected NeuralDataSet testingDataSet;
    protected NeuralDataSet validatingDataSet;
    public boolean printTraining=false;
    public abstract void train() throws NeuralException;
    public abstract void forward() throws NeuralException;
    public abstract void forward(int i) throws NeuralException;
    public abstract Double calcNewWeight(int layer,int input,int neuron) throws NeuralException;
    public abstract Double calcNewWeight(int layer,int input,int neuron,double error) throws NeuralException;
//…
}</pre></div><p class="calibre11">The <code class="literal">neuralNet</code> object is a reference to the neural network that will be trained by this learning algorithm. The <code class="literal">enums</code> define the learning mode and learning paradigm. The learning executing parameters are defined (MaxEpochs, MinOverallError, LearningRate), and the datasets that will be taken into account during the learning process.</p><p class="calibre11">The <a id="id101" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>method <code class="literal">train( )</code> should be overridden by each learning algorithm implementation. All the training process will occur in this method. The methods <code class="literal">forward( )</code> and <code class="literal">forward(int k)</code> process the neural network with all input data and with the kth input data record, respectively. And finally, the method <code class="literal">calcNewWeight( )</code> will perform the weight update for the weight connecting an input to a neuron in a specific layer. A variation in the <code class="literal">calcNewWeight( )</code> method allows providing a specific error to be taken in the update operation.</p><div><div><div><div><h2 class="title5"><a id="ch02lvl2sec25" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>The delta rule</h2></div></div></div><p class="calibre11">This<a id="id102" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> algorithm updates the weights according to the cost function. Following the gradient approach, one wants to know which weights can drive the<a id="id103" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> cost function to a lower value. Note that we can find the direction by computing the partial derivative of the cost function to each of the weights. To help in understanding, let's consider one simple approach with only one neuron, one weight, and one bias, and therefore one input. The output will be as follows:</p><div><img src="img/B05964_02_05_04.jpg" alt="The delta rule" class="calibre57"/></div><p class="calibre11">Here, g is the activation function, X is the vector containing x values, and Y is the output vector generated by the neural network. The general error for the kth sample is quite simple:</p><div><img src="img/B05964_02_05_05.jpg" alt="The delta rule" class="calibre58"/></div><p class="calibre11">However, it is possible to define this error as square error, N-degree error, or MSE. But, for simplicity, let's consider the simple error difference for the general error. Now the overall error, that will be the cost function, should be computed as follows:</p><div><img src="img/B05964_02_05_06.jpg" alt="The delta rule" class="calibre59"/></div><p class="calibre11">The weight <a id="id104" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>and bias are updated according to the delta rule, that<a id="id105" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> considers the partial derivatives <img src="img/B05964_02_05_07.jpg" alt="The delta rule" class="calibre60"/> with respect to the weight and the bias, respectively. For the batch training mode, X and E are vectors:</p><div><img src="img/B05964_02_05_08.jpg" alt="The delta rule" class="calibre61"/></div><p class="calibre11">If the training mode is online, we don't need to perform dot product:</p><div><img src="img/B05964_02_05_09.jpg" alt="The delta rule" class="calibre62"/></div></div><div><div><div><div><h2 class="title5"><a id="ch02lvl2sec26" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>The learning rate</h2></div></div></div><p class="calibre11">Note in the<a id="id106" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> preceding equations the presence of the term α that<a id="id107" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> indicates the learning rate. It plays an important role in weight update, because it can drive faster or slower to the minimum cost value. Let's see a cost error surface in relation to two weights:</p><div><img src="img/B05964_02_06.jpg" alt="The learning rate" class="calibre63"/></div></div><div><div><div><div><h2 class="title5"><a id="ch02lvl2sec27" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Implementing the delta rule</h2></div></div></div><p class="calibre11">We <a id="id108" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>will implement the delta rule in a class<a id="id109" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> called <code class="literal">DeltaRule</code>, that will extend the <code class="literal">LearningAlgorithm</code> class:</p><div><pre class="programlisting">public class DeltaRule extends LearningAlgorithm {
    public ArrayList&lt;ArrayList&lt;Double&gt;&gt; error;
    public ArrayList&lt;Double&gt; generalError;
    public ArrayList&lt;Double&gt; overallError;
    public double overallGeneralError;
    public double degreeGeneralError=2.0;
    public double degreeOverallError=0.0;
    public enum ErrorMeasurement {SimpleError, SquareError,NDegreeError,MSE}
    
    public ErrorMeasurement generalErrorMeasurement=ErrorMeasurement.SquareError;
    public ErrorMeasurement overallErrorMeasurement=ErrorMeasurement.MSE;
    private int currentRecord=0;
    private ArrayList&lt;ArrayList&lt;ArrayList&lt;Double&gt;&gt;&gt; newWeights;
//…
}</pre></div><p class="calibre11">The <a id="id110" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>errors discussed in the error measurement <a id="id111" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>section (general and overall errors) are implemented in the <code class="literal">DeltaRule</code> class, because the delta rule learning algorithm considers these errors during the training. They are arrays because there will be a general error for each dataset record, and there will be an overall error for each output. An attribute <code class="literal">overallGeneralError</code> takes on the cost function result, or namely the overall error for all outputs and records. A matrix called error, stores the errors for each output record combination.</p><p class="calibre11">This class also allows multiple ways of calculating the overall and general errors. The attributes <code class="literal">generalErrorMeasurement</code> and <code class="literal">overallErrorMeasurement</code> can take on one of the input values for simple error, square error calculation, Nth degree error (cubic, quadruple, and so on), or the MSE. The default will be simple error for the general error and MSE for the overall.</p><p class="calibre11">Two important attributes are worth noting in this code: <code class="literal">currentRecord</code> refers to the index of the record being fed into the neural network during training, and the <code class="literal">newWeights</code> cubic matrix is a collection of all new values of weights that will be updated in the neural network. The <code class="literal">currentRecord</code> attribute is useful in the online training, and the <code class="literal">newWeights</code> matrix helps the neural network to keep all of its original weights until all new weights calculation is finished, preventing new weights to be updated during the forward processing stage, what could compromise the training quality significantly.</p></div><div><div><div><div><h2 class="title5"><a id="ch02lvl2sec28" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>The core of the delta rule learning - train and calcNewWeight methods</h2></div></div></div><p class="calibre11">To save <a id="id112" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>space, we will not<a id="id113" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> detail here the implementation of the forward methods. As described in the previous section, forward means that neural dataset records should be fed into the neural network and then the error values are calculated:</p><div><pre class="programlisting">@Override
public void train() throws NeuralException{
//…
  switch(learningMode){
    case BATCH: //this is the batch training mode
      epoch=0;
      forward(); //all data are presented to the neural network
      while(epoch&lt;MaxEpochs &amp;&amp; overallGeneralError&gt;MinOverallError){ //continue condition
        epoch++; //new epoch                       
        for(int j=0;j&lt;neuralNet.getNumberOfOutputs();j++){
          for(int i=0;i&lt;=neuralNet.getNumberOfInputs();i++){
            //here the new weights are calculated
            newWeights.get(0).get(j).set(i,calcNewWeight(0,i,j));
          }
        }
//only after all weights are calculated, they are applied
        applyNewWeights();
// the errors are updated with the new weights
        forward();
      }
      break;
    case ONLINE://this is the online training
      epoch=0;
      int k=0;
      currentRecord=0; //this attribute is used in weight update
      forward(k); //only the k-th record is presented
      while(epoch&lt;MaxEpochs &amp;&amp; overallGeneralError&gt;MinOverallError){
        for(int j=0;j&lt;neuralNet.getNumberOfOutputs();j++){
          for(int i=0;i&lt;=neuralNet.getNumberOfInputs();i++){
            newWeights.get(0).get(j).set(i,calcNewWeight(0,i,j));
          }
        }
//the new weights will be considered for the next record
        applyNewWeights();
        currentRecord=++k;
        if(k&gt;=trainingDataSet.numberOfRecords){
          k=0; //if it was the last record, again the first
          currentRecord=0;
          epoch++; //epoch completes after presenting all records
        }
        forward(k); //presenting the next record
      }
    break;
    }
  }</pre></div><p class="calibre11">We note <a id="id114" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>that in the <code class="literal">train( )</code> method, there<a id="id115" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> is a loop with a condition to continue training. This means that while the training will stop when this condition no longer holds true. The condition checks the <code class="literal">epoch</code> number and the overall error. When the <code class="literal">epoch</code> number reaches the maximum or the error reaches the minimum, the training is finished. However, there are some cases in which the overall error fails to meet the minimum requirement, and the neural network needs to stop training.</p><p class="calibre11">The new weight is calculated using the <code class="literal">calcNewWeight( )</code> method:</p><div><pre class="programlisting">@Override
public Double calcNewWeight(int layer,int input,int neuron)
            throws NeuralException{
//…
  Double deltaWeight=LearningRate;
  Neuron currNeuron=neuralNet.getOutputLayer().getNeuron(neuron);
  switch(learningMode){
    case BATCH: //Batch mode
      ArrayList&lt;Double&gt; derivativeResult=currNeuron.derivativeBatch(trainingDataSet.getArrayInputData());
      ArrayList&lt;Double&gt; _ithInput;
      if(input&lt;currNeuron.getNumberOfInputs()){ // weights
        _ithInput=trainingDataSet.getIthInputArrayList(input);
      }
      else{ // bias
        _ithInput=new ArrayList&lt;&gt;();
        for(int i=0;i&lt;trainingDataSet.numberOfRecords;i++){
          _ithInput.add(1.0);
        }
      }
      Double multDerivResultIthInput=0.0; // dot product
      for(int i=0;i&lt;trainingDataSet.numberOfRecords;i++){
        multDerivResultIthInput+=error.get(i).get(neuron)*derivativeResult.get(i)*_ithInput.get(i);
      }
      deltaWeight*=multDerivResultIthInput;
    break;
    case ONLINE:
      deltaWeight*=error.get(currentRecord).get(neuron);
      deltaWeight*=currNeuron.derivative(neuralNet.getInputs());
      if(input&lt;currNeuron.getNumberOfInputs()){
        deltaWeight*=neuralNet.getInput(input);
      }
      break;
  }
return currNeuron.getWeight(input)+deltaWeight;
//…
}</pre></div><p class="calibre11">Note that in the weight update, there a call to the derivative of the activation function of the given neuron. This is needed to meet the delta rule. In the activation function interface, we've <a id="id116" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>added this<a id="id117" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> method <code class="literal">derivative( )</code> to be overridden in each of the implementing classes.</p><div><div><h3 class="title6"><a id="note02" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre17">Note: For the batch mode the call to the <code class="literal">derivativeBatch( )</code>, that receives and returns an array of values, instead of a single scalar.</p></div></div><p class="calibre11">In the <code class="literal">train( )</code> method, we've seen that new weights are stored in the <code class="literal">newWeights</code> attribute, to not influence the current learning process, and are only applied after the training iteration has finished.</p></div><div><div><div><div><h2 class="title5"><a id="ch02lvl2sec29" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Another learning algorithm - Hebbian learning</h2></div></div></div><p class="calibre11">In the 1940s, the <a id="id118" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>neuropsychologist Donald Hebb postulated<a id="id119" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> that the connections between neurons that activate or fire simultaneously, or using his words, repeatedly or persistently, should be increased. This is one approach of unsupervised learning, since no target output is specified for Hebbian learning:</p><div><img src="img/B05964_02_07.jpg" alt="Another learning algorithm - Hebbian learning" class="calibre64"/></div><p class="calibre11">In summary, the weight update rule for Hebbian learning takes into account only the input and outputs of the neuron. Given a neuron j whose connection to neuron i (weight ij) is to be updated, the update is given by the following equation:</p><div><img src="img/B05964_02_07_01.jpg" alt="Another learning algorithm - Hebbian learning" class="calibre65"/></div><p class="calibre11">Here, α is a learning rate, oj is the output of the neuron j, and oi is the output of the neuron i, also the input i for the neuron j. For the batch training case, oi and oj will be vectors, and we'll need to perform a dot product.</p><p class="calibre11">Since we don't <a id="id120" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>include error measurement in Hebbian learning, a stop condition can be determined by the maximum number of epochs or the<a id="id121" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> increase in the overall average of neural outputs. Given N records, we compute the expectancy or average of all outputs produced by the neural network. When this average increases over a certain level, it is time to stop the training, to prevent the neural outputs from blowing up.</p><p class="calibre11">We'll develop a new class for Hebbian learning, also inheriting from <code class="literal">LearningAlgorithm</code>:</p><div><pre class="programlisting">public class Hebbian extends LearningAlgorithm {
//…
    private ArrayList&lt;ArrayList&lt;ArrayList&lt;Double&gt;&gt;&gt; newWeights;
    private ArrayList&lt;Double&gt; currentOutputMean;
    private ArrayList&lt;Double&gt; lastOutputMean;
}</pre></div><p class="calibre11">All parameters except for the absent error measures and the new measures of mean are identical to the <code class="literal">DeltaRule</code> class. The methods are quite similar, except for the <code class="literal">calcNewWeight( )</code>:</p><div><pre class="programlisting">@Override
public Double calcNewWeight(int layer,int input,int neuron)
         throws NeuralException{
//…
  Double deltaWeight=LearningRate;
  Neuron currNeuron=neuralNet.getOutputLayer().getNeuron(neuron);
  switch(learningMode){
    case BATCH:
//…
//the batch case is analogous to the implementation in Delta Rule
//but with the neuron's output instead of the error
//we're suppressing here to save space
      break;
    case ONLINE:
      deltaWeight*=currNeuron.getOutput();
      if(input&lt;currNeuron.getNumberOfInputs()){
        deltaWeight*=neuralNet.getInput(input);
      }
      break;
    }
    return currNeuron.getWeight(input)+deltaWeight;
  }</pre></div></div><div><div><div><div><h2 class="title5"><a id="ch02lvl2sec30" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Adaline</h2></div></div></div><p class="calibre11">Adaline is an <a id="id122" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>architecture standing for Adaptive Linear Neuron, developed by Bernard Widrow and Ted Hoff, based on the McCulloch, and Pitts neuron. It<a id="id123" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> has only one layer of neurons and can be trained similarly to the delta rule. The main difference lies in the fact that the update rule is given by the error between the weighted sum of inputs and biases and the target output, instead of updating based on the neuron output after the activation function. This may be desirable when one wants to perform continuous learning for classification problems, which tend to use discrete values instead of continuous.</p><p class="calibre11">The following figure illustrates how Adaline learns:</p><div><img src="img/B05964_02_08.jpg" alt="Adaline" class="calibre66"/></div><p class="calibre11">So the weights are updated by the following equation:</p><div><img src="img/B05964_02_08_01.jpg" alt="Adaline" class="calibre67"/></div><p class="calibre11">In order to<a id="id124" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> implement Adaline, we create a class called <strong class="calibre12">Adaline</strong> with <a id="id125" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the following overridden <code class="literal">weight calcNewWeight</code>. To save space, we're presenting only the online case:</p><div><pre class="programlisting">@Override
public Double calcNewWeight(int layer,int input,int neuron)
            throws NeuralException{
//…
  Double deltaWeight=LearningRate;
  Neuron currNeuron=neuralNet.getOutputLayer().getNeuron(neuron);
  switch(learningMode){
    case BATCH:
//…
    break;
    case ONLINE:
      deltaWeight*=error.get(currentRecord).get(neuron)
        *currNeuron.getOutputBeforeActivation();
      if(input&lt;currNeuron.getNumberOfInputs()){
        deltaWeight*=neuralNet.getInput(input);
      }
    break;
  }
  return currNeuron.getWeight(input)+deltaWeight;
}</pre></div><p class="calibre11">Note the method <code class="literal">getOutputBeforeActivation( )</code>; we mentioned in the last chapter that this property would be useful in the future.</p></div></div></div>



  
<div><div><div><div><div><h1 class="title2"><a id="ch02lvl1sec22" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Time to see the learning in practice!</h1></div></div></div><p class="calibre11">Let's work on<a id="id126" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> a very simple yet illustrative example. Suppose you want a single neuron neural network to learn how to fit a simple linear function such as the following:</p><div><table border="1" class="calibre20"><colgroup class="calibre21"><col class="calibre22"/><col class="calibre22"/></colgroup><tbody class="calibre27"><tr class="calibre28"><td class="calibre29">
<div><img src="img/B05964_02_09_01.jpg" alt="Time to see the learning in practice!" class="calibre68"/></div>
</td><td class="calibre29">
<div><img src="img/B05964_02_09.jpg" alt="Time to see the learning in practice!" class="calibre69"/></div>
</td></tr></tbody></table></div><p class="calibre11">This is quite easy even for those who have little math background, so guess what? It is a nice start for our simplest neural network to prove its learning ability!</p><div><div><div><div><h2 class="title5"><a id="ch02lvl2sec31" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Teaching the neural network – the training dataset</h2></div></div></div><p class="calibre11">We;re going to <a id="id127" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>structure the dataset for the neural network to learn using the following code, which you can find in the main method of the file <code class="literal">NeuralNetDeltaRuleTest</code>:</p><div><pre class="programlisting">Double[][] _neuralDataSet = {
  {1.2 , fncTest(1.2)}
 ,   {0.3 , fncTest(0.3)}
 ,   {-0.5 , fncTest(-0.5)}
 ,   {-2.3 , fncTest(-2.3)}
 ,   {1.7 , fncTest(1.7)}
 ,   {-0.1 , fncTest(-0.1)}
 ,   {-2.7 , fncTest(-2.7)}  };
int[] inputColumns = {0};
int[] outputColumns = {1};
NeuralDataSet neuralDataSet = newNeuralDataSet(_neuralDataSet,inputColumns,outputColumns);</pre></div><p class="calibre11">The <code class="literal">funcTest</code> function is defined as the function we mentioned:</p><div><pre class="programlisting">    public static double fncTest(double x){
        return 0.11*x;
    }</pre></div><p class="calibre11">Note that we're <a id="id128" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>using the class <code class="literal">NeuralDataSet</code> to structure all this data in such a way that they will be fed into the neural network the right way. Now let's link this dataset to the neural network. Remember that this network has a single neuron in the output. Let's use a nonlinear activation function such as hyperbolic tangent at the output with a coefficient <code class="literal">0.85</code>:</p><div><pre class="programlisting">int numberOfInputs=1;
int numberOfOutputs=1;
HyperTan htAcFnc = new HyperTan(0.85);
NeuralNet nn = new NeuralNet(numberOfInputs,numberOfOutputs,
                 htAcFnc);</pre></div><p class="calibre11">Let's now instantiate the <code class="literal">DeltaRule</code> object and link it to the neural network created. Then we'll set the learning parameters such as learning rate, minimum overall error, and maximum number of epochs:</p><div><pre class="programlisting">DeltaRule deltaRule=new DeltaRule(nn,neuralDataSet.LearningAlgorithm.LearningMode.ONLINE);
deltaRule.printTraining=true;
deltaRule.setLearningRate(0.3);
deltaRule.setMaxEpochs(1000);
deltaRule.setMinOverallError(0.00001);</pre></div><p class="calibre11">Now let's see the first neural output of the untrained neural network, after calling the method <code class="literal">forward( )</code> of the <code class="literal">deltaRule</code> object:</p><div><pre class="programlisting">deltaRule.forward();
neuralDataSet.printNeuralOutput();</pre></div><div><img src="img/B05964_02_10.jpg" alt="Teaching the neural network – the training dataset" class="calibre70"/></div><p class="calibre11">Plotting a chart, we find that the output generated by the neural network is a little bit different:</p><div><img src="img/B05964_02_11.jpg" alt="Teaching the neural network – the training dataset" class="calibre71"/></div><p class="calibre11">We will <a id="id129" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>start training the neural network in the online mode. We've set the <code class="literal">printTraining</code> attribute as true, so we will receive in the screen an update. The following piece of code will produce the subsequent screenshot:</p><div><pre class="programlisting">System.out.println("Beginning training");
  deltaRule.train();
System.out.println("End of training");
  if(deltaRule.getMinOverallError()&gt;=deltaRule.getOverallGeneralError()){
  System.out.println("Training succesful!");
}
else{
  System.out.println("Training was unsuccesful");
}</pre></div><div><img src="img/B05964_02_12.jpg" alt="Teaching the neural network – the training dataset" class="calibre72"/></div><p class="calibre11">The training begins and the overall error information is updated after every weight update. Note the error is decreasing:</p><div><img src="img/B05964_02_13.jpg" alt="Teaching the neural network – the training dataset" class="calibre73"/></div><p class="calibre11">After five <a id="id130" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>epochs, the error reaches the minimum; now let's see the neural outputs and the plot:</p><div><table border="1" class="calibre20"><colgroup class="calibre21"><col class="calibre22"/><col class="calibre22"/></colgroup><tbody class="calibre27"><tr class="calibre28"><td class="calibre29">
<div><img src="img/B05964_02_14.jpg" alt="Teaching the neural network – the training dataset" class="calibre74"/></div>
</td><td class="calibre29">
<div><img src="img/B05964_02_15.jpg" alt="Teaching the neural network – the training dataset" class="calibre75"/></div>
</td></tr></tbody></table></div><p class="calibre11">Quite amazing, isn't it? The target and the neural output are practically the same. Now let's take a look at the final <code class="literal">weight</code> and <code class="literal">bias</code>:</p><div><pre class="programlisting">weight = nn.getOutputLayer().getWeight(0, 0);
bias = nn.getOutputLayer().getWeight(1, 0);
System.out.println("Weight found:"+String.valueOf(weight));
System.out.println("Bias found:"+String.valueOf(bias));
//Weight found:0.2668421011698528
//Bias found:0.0011258204676042108</pre></div></div></div></div>



  
<div><div><div><div><div><h1 class="title2"><a id="ch02lvl1sec23" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Amazing, it learned! Or, did it really? A further step – testing</h1></div></div></div><p class="calibre11">Well, we might ask <a id="id131" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>now: so the neural network has already learned from the<a id="id132" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> data; how can we attest it has effectively learned? Just like in exams students are subjected to, we need to check the network response after training. But wait! Do you think it is likely a teacher would put in an exam the same questions he/she has presented in class? There is no sense in evaluating somebody's learning with examples that are already known, or a suspecting teacher would conclude the student might have memorized the content, instead of having learned it.</p><p class="calibre11">Okay, let's now explain this part. What we are talking about here is testing. The learning process we have <a id="id133" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>covered is called training. After training a neural network, we should test whether it has really learnt. For testing, we must present to the neural network another fraction of data from the same environment it has learnt from. This is necessary because, just like the student, the neural network could respond properly to only the data points it has been <a id="id134" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>exposed to; this is called overtraining. To check whether the neural network has not passed on overtraining, we must check its response to other data points.</p><p class="calibre11">The following figure<a id="id135" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> illustrates the overtraining problem. Imagine that our <a id="id136" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>network is designed to approximate some function f(x) whose definition is unknown. The neural network was fed with some data from that function and produced the result shown on the left in the following figure. But when expanding to a wider domain, for example, adding a testing dataset, we note the neural response does not follow the data (on the right in the figure):</p><div><img src="img/B05964_02_16.jpg" alt="Amazing, it learned! Or, did it really? A further step – testing" class="calibre76"/></div><p class="calibre11">In this case, we see that the neural network failed to learn the whole environment (the function <em class="calibre16">f(x)</em>). This happens because of a number of reasons:</p><div><ul class="itemizedlist"><li class="listitem">The neural network didn't receive enough information from the environment</li><li class="listitem">The data from the environment is nondeterministic</li><li class="listitem">The training and testing datasets are poorly defined</li><li class="listitem">The neural network has learnt so much on the training data, it has <em class="calibre16">forgotten</em> about the testing data</li></ul></div><p class="calibre11">Throughout<a id="id137" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> this book, we are going to cover the process to prevent this <a id="id138" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>and other issues that may arise during training.</p><div><div><div><div><h2 class="title5"><a id="ch02lvl2sec32" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Overfitting and overtraining</h2></div></div></div><p class="calibre11">In our previous <a id="id139" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>example, the neural network seemed to have learned amazingly <a id="id140" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>well. However, there is a risk of overfitting and <a id="id141" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>overtraining. The difference between these two concepts is very<a id="id142" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> subtle. An overfitting occurs when the neural network memorizes the problem's behavior, so that it can provide good values only on training points, therefore losing a generalization capacity. Overtraining, which can be a cause for overfitting, occurs when the training error becomes much smaller than the testing error, or actually, the testing error starts to increase as the neural network continues (over)training:</p><div><img src="img/B05964_02_17.jpg" alt="Overfitting and overtraining" class="calibre77"/></div><p class="calibre11">One of ways to prevent overtraining and overfitting is checking the testing error when the training goes on. When the testing error starts to increase, it is time to stop. This will be covered more in detail in the next chapters.</p><p class="calibre11">Now, let's see if there is the case in our example. Let's now add some more data and test it:</p><div><pre class="programlisting">Double[][] _testDataSet ={
  {-1.7 , fncTest(-1.7) }
, {-1.0 , fncTest(-1.0) }
, {0.0 , fncTest(0.0) }
, {0.8 , fncTest(0.8) }
, {2.0 , fncTest(2.0) }
};
NeuralDataSet testDataSet = new NeuralDataSet(_testDataSet, ....inputColumns, outputColumns);
deltaRule.setTestingDataSet(testDataSet);
deltaRule.test();
testDataSet.printNeuralOutput();</pre></div><div><table border="1" class="calibre20"><colgroup class="calibre21"><col class="calibre22"/><col class="calibre22"/></colgroup><tbody class="calibre27"><tr class="calibre28"><td class="calibre29">
<div><img src="img/B05964_02_18.jpg" alt="Overfitting and overtraining" class="calibre78"/></div>
</td><td class="calibre29">
<div><img src="img/B05964_02_19.jpg" alt="Overfitting and overtraining" class="calibre79"/></div>
</td></tr></tbody></table></div><p class="calibre11">As can<a id="id143" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> be <a id="id144" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>seen, the neural network presents<a id="id145" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> a generalization capacity in this case. In spite of the simplicity<a id="id146" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> of this example, we can still see the learning skill of the neural network.</p></div></div></div>



  
<div><div><div><div><div><h1 class="title2"><a id="ch02lvl1sec24" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Summary</h1></div></div></div><p class="calibre11">This chapter presented the reader with the whole learning process of which neural networks are capable. We presented the very basic foundations of learning, inspired by human learning itself. To illustrate this process in practice, we have implemented two learning algorithms in Java, and applied them in two examples. With this, the reader can have a basic but useful understanding on how neural networks learn and even how one can systematically describe the process of learning. This will be the foundation for the next chapters, which will present more complex examples.</p></div></div>



  </body></html>