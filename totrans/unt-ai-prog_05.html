<html><head></head><body>
		<div><h1 id="_idParaDest-71"><em class="italic"><a id="_idTextAnchor270"/>Chapter <a id="_idTextAnchor271"/>4</em>: Implementing Sensors</h1>
			<p>As we discussed in the previous chapter, a character AI system needs to be aware of its surrounding environment. For example, <strong class="bold">Non-Player Characters</strong> (<strong class="bold">NPCs</strong>) need to know where the obstacles are, the direction the player is looking, whether they are in the player's sight, and a lot more. The quality of the AI of our NPCs depends, for the most part, on the information they can get from the environment. Sensor mistakes are apparent to the player: we've all experienced playing a video game and laughing at an NPC that clearly should have seen us, or, on the other hand, been frustrated because an NPC spotted us from behind a wall.</p>
			<p>Video game characters usually get the input information required by their underlying AI decision-making algorithms from sensory information. For simplicity, in this chapter, we will consider <em class="italic">sensory information</em> as any kind of data coming from the game world. If there's not enough information, characters might show unusual behaviors, such as choosing the wrong places to take cover, idling, or looping in strange actions without knowing how to proceed. A quick search for AI glitches on YouTube opens the door to a vast collection of common funny behaviors of AI, even in AAA games.</p>
			<p>In this chapter, we will look at the following topics:</p>
			<ul>
				<li>Introducing sensory systems</li>
				<li>Discovering what a sensory system is and how to implement two senses—sight and touch—in Unity</li>
				<li>Building a demo where we can see our sensory system in action</li>
			</ul>
			<h1 id="_idParaDest-72"><a id="_idTextAnchor272"/><a id="_idTextAnchor273"/><a id="_idTextAnchor274"/><a id="_idTextAnchor275"/>Technical requirements</h1>
			<p>For this chapter, you just need Unity3D 2022. You can find the example project described in this chapter in the <code>Chapter 4</code> folder in the book repository: <a id="_idTextAnchor276"/><a id="_idTextAnchor277"/><a href="https://github.com/PacktPublishing/Unity-Artificial-Intelligence-Programming-Fifth-Edition/tree/main/Chapter04">https://github.com/PacktPublishing/Unity-Artificial-Intelligence-Programming-Fifth-Edition/tree/main/Chapter04</a>.</p>
			<h1 id="_idParaDest-73"><a id="_idTextAnchor278"/>Basic sensory systems</h1>
			<p>An AI sensory system <a id="_idIndexMarker219"/>emulates senses such as sight, hearing, and<a id="_idIndexMarker220"/> even smell to get information from other GameObjects. In such a system, the NPCs need to examine the environment and check for such senses periodically based on their particular interest.</p>
			<p>In a minimal sensory system, we have<a id="_idIndexMarker221"/> two principal <a id="_idIndexMarker222"/>elements: <strong class="bold">aspect</strong> (also called <strong class="bold">event emitters</strong>) and <strong class="bold">sense</strong> (also called <strong class="bold">event senses</strong>). Every<a id="_idIndexMarker223"/> sense<a id="_idIndexMarker224"/> can perceive only a <a id="_idIndexMarker225"/>specific<a id="_idIndexMarker226"/> aspect; for instance, an NPC with just the sense of hearing can only perceive the sound (one of the aspects) emitted by another GameObject, or a zombie NPC can use its sense of smell to prey on the player's brain. As in real life, we do not need a single sense for every NPC; they can have sight, smell, and touch all at once.</p>
			<p>In our demo, we'll implement a base interface, called <code>Sense</code>, that we'll use to implement custom senses. In this chapter, we'll implement sight and touch senses. Sight is what we use to see the world around them; if our AI character sees an enemy, we receive an event in our code, and we act accordingly by doing some action in response. Likewise, with touch, when an enemy gets too close, we want to be able to sense that. Finally, we'll implement a minimal <code>Aspect</code> class that our senses can pe<a id="_idTextAnchor279"/><a id="_idTextAnchor280"/><a id="_idTextAnchor281"/><a id="_idTextAnchor282"/>rceive.</p>
			<h1 id="_idParaDest-74"><a id="_idTextAnchor283"/>Setting up our scene</h1>
			<p>Let's get <a id="_idIndexMarker227"/>started by setting up our scene:</p>
			<ol>
				<li>First, we add a plane as a floor.</li>
				<li>Let's create a few walls to block the line of sight from our AI character to the enemy. We make these out of short—but wide—cubes that we group under an empty GameObject called <strong class="bold">Obstacles</strong>.</li>
				<li>Finally, we add a directional light to see what is going on in our scene.</li>
			</ol>
			<p>We represent the player with a tank, similar to what we used earlier, and we represent the NPCs with simple cubes. We also have a <strong class="bold">Target</strong> object to show us where the tank is moving in our scene. Our Scene hierarchy should look similar to the following screenshot:</p>
			<div><div><img src="img/B17984_04_1.jpg" alt="Figure 4.1 – The setup of the example's Hierarchy&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.1 – The setup of the example's Hierarchy</p>
			<p>Now, let's <a id="_idIndexMarker228"/>position the tank, AI character, and walls randomly in our scene. First, make sure to increase the size of the plane to something that looks good. Fortunately, in this demo, all the objects are locked on the plane, and there is no simulated gravity so that nothing can fall off the plane. Also, be sure to adjust the camera so that we can have a clear view of the following scene:</p>
			<div><div><img src="img/B17984_04_2.jpg" alt="Figure 4.2 – The space that our tank and player wander in&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.2 – The space that our tank and player wa<a id="_idTextAnchor284"/><a id="_idTextAnchor285"/>nder in</p>
			<p>Now that <a id="_idIndexMarker229"/>we have the basics set up, let's look at how to implement the tank, AI ch<a id="_idTextAnchor286"/>aracter, and aspects for our player ch<a id="_idTextAnchor287"/><a id="_idTextAnchor288"/>aracter.</p>
			<h1 id="_idParaDest-75"><a id="_idTextAnchor289"/>The player's tank and the aspect class</h1>
			<p>The <strong class="bold">Target</strong> object <a id="_idIndexMarker230"/>is a simple sphere object with the mesh render disabled. We have also created a point light and made it a child of our Target object. Make sure that the light is centered, or it will not be very helpful.</p>
			<p>Look at the following code in the <code>Target.cs</code> file:</p>
			<pre>using UnityEngine;
public class Target : MonoBehaviour { 
    [SerializeField]
    private float hOffset = 0.2f;
    void Update () {
        int button = 0;
        //Get the point of the hit position when the mouse 
        //is being clicked 
        if(Input.GetMouseButtonDown(button)) {
            Ray ray = Camera.main.ScreenPointToRay(
              Input.mousePosition);
            RaycastHit hitInfo;
            if (Physics.Raycast(ray.origin, ray.direction,
                out hitInfo)) {
                Vector3 targetPosition = hitInfo.point;
                transform.position = targetPosition + 
                  new Vector3(0.0f, hOffset, 0.0f);
            }
        }
    }
}</pre>
			<p>Attach this script to<a id="_idIndexMarker231"/> the Target object. The script detects the mouse-click event and then, using the raycasting technique, detects the mouse-click location on the plane in the 3D space, and updates the Target object's position in <a id="_idTextAnchor290"/><a id="_idTextAnchor291"/><a id="_idTextAnchor292"/><a id="_idTextAnchor293"/>our scene. We will have a look at the player's tank in the following section.</p>
			<h2 id="_idParaDest-76"><a id="_idTextAnchor294"/>The player's tank</h2>
			<p>The player's tank is <a id="_idIndexMarker232"/>the simple model we used in <a href="B17984_02_Epub.xhtml#_idTextAnchor100"><em class="italic">Chapter 2</em></a>, <em class="italic">Finite State Machines</em>, with a non-kinematic <code>Rigidbody</code> component. We need the <code>Rigidbody</code> component to generate trigger events whenever we do collision detection with AI characters and environment objects. Finally, we need to assign the <strong class="bold">Player</strong> tag to our tank.</p>
			<p>As we <a id="_idIndexMarker233"/>can easily see from its name, the <code>PlayerTank</code> script controls the player's tank. The following is the code for the <code>PlayerTank.cs</code> file:</p>
			<pre>using UnityEngine;
public class PlayerTank : MonoBehaviour { 
    public Transform targetTransform;
    [SerializeField]
    private float movementSpeed = 10.0f;
    [SerializeField]
    private float rotSpeed = 2.0f;
    [SerializeField]
    private float targerReactionRadius = 5.0f;
    void Update () {
        //Stop once you reached near the target position
        if (Vector3.Distance(transform.position,
          targetTransform.position) &lt; targetReactionRadius) 
          return;
        //Calculate direction vector from current position
        // to target position
        
        Vector3 tarPos = targetTransform.position;
        tarPos.y = transform.position.y;
        Vector3 dirRot = tarPos - transform.position;
        //Build a Quaternion for this new rotation vector
        //using LookRotation method
        Quaternion tarRot = 
          Quaternion.LookRotation(dirRot);
        //Move and rotate with interpolation
        transform.rotation= Quaternion.Slerp(
          transform.rotation, tarRot, 
          rotSpeed * Time.deltaTime);
        transform.Translate(new Vector3(0, 0, 
          movementSpeed * Time.deltaTime));
    }
}</pre>
			<p>This script <a id="_idIndexMarker234"/>retrieves the Target position on the map and updates the tank's destination point and direction accordingly. The result of the preceding code is shown in the following panel:</p>
			<div><div><img src="img/B17984_04_3.jpg" alt="Figure 4.3 – The properties of our Tank object&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.3 – The properties of our Tank object</p>
			<p>After we assign the preceding<a id="_idIndexMarker235"/> script to the tank, be sure to assign the Target object to the <code>targetTransf<a id="_idTextAnchor295"/><a id="_idTextAnchor296"/><a id="_idTextAnchor297"/><a id="_idTextAnchor298"/>orm</code> variable.</p>
			<h2 id="_idParaDest-77"><a id="_idTextAnchor299"/>Aspect</h2>
			<p>Next, let's take <a id="_idIndexMarker236"/>a look at the <code>Aspect</code> class. <code>Aspect</code> is an elementary class with just one public property, called <code>aspectName</code>. That's all the variables we need in this chapter.</p>
			<p>Whenever our AI character senses something, we'll check this against <code>aspectName</code> to see whether it's the aspect that the AI has been looking for:</p>
			<pre>using UnityEngine;
public class Aspect : MonoBehaviour {
    public enum Affiliation {
        Player,
        Enemy
    }
    public Affiliation affiliation;
}</pre>
			<p>Attach this <a id="_idIndexMarker237"/>aspect script <a id="_idIndexMarker238"/>to our player's tank and set the <code>aspectName</code> pro<a id="_idTextAnchor300"/><a id="_idTextAnchor301"/><a id="_idTextAnchor302"/><a id="_idTextAnchor303"/>perty as <code>Player</code>.</p>
			<h1 id="_idParaDest-78"><a id="_idTextAnchor304"/>AI characters</h1>
			<p>In this example, the AI characters <a id="_idIndexMarker239"/>roam around the scene in a random direction. They have two senses: sight and touch. The sight sense checks whether the enemy aspect is within a set visible range and distance. Touch detects whether the enemy aspect has collided with the <code>Box Collider</code> around the character. As we have seen previously, our player's tank has the <code>Player</code> aspect. Consequently, these senses are triggered when they detect the player's tank.</p>
			<p>For now, let's<a id="_idIndexMarker240"/> look at the script we use to move the NPCs around:</p>
			<pre>using UnityEngine;
using System.Collections;
public class Wander : MonoBehaviour { 
    private Vector3 tarPos;
    [SerializeField]
    private float movementSpeed = 5.0f;
    [SerializeField]
    private float rotSpeed = 2.0f;
    [SerializeField]
    private float minX = -45.0f;
    [SerializeField]
    private float maxX = 45.0f;
    [SerializeField]
    private float minZ = -45.0f;
    [SerializeField]
    private float maxZ = -45.0f;
    [SerializeField]
    private float targetReactionRadius = 5.0f;
    [SerializeField]
    private float targetVerticalOffset = 0.5f;
    void Start () {
        //Get Wander Position 
        GetNextPosition();
    }
    void Update () {
        // Check if we're near the destination position
        if (Vector3.Distance(tarPos, transform.position) &lt;=
          targetReactionRadius) GetNextPosition(); 
        // generate new random position
        // Set up quaternion for rotation toward
        // destination
        Quaternion tarRot = Quaternion.LookRotation(
          tarPos - transform.position);
        // Update rotation and translation
        transform.rotation = Quaternion.Slerp(
          transform.rotation,
          tarRot, rotSpeed * Time.deltaTime);
        transform.Translate(new Vector3(0, 0, movementSpeed
                            * Time.deltaTime));
    }
    void GetNextPosition() {
        tarPos = new Vector3(Random.Range(minX, maxX),
          targetVerticalOffset, Random.Range(minZ, maxZ));
    }
}</pre>
			<p>The <code>Wander</code> script<a id="_idIndexMarker241"/> generates <a id="_idIndexMarker242"/>a new random position in a specified range whenever an AI character reaches its current destination point. Then, the <code>Update</code> method <a id="_idIndexMarker243"/>rotates the NPCs and moves them toward their new destination. Attach this script to our<a id="_idTextAnchor305"/><a id="_idTextAnchor306"/> AI character so that it can move<a id="_idTextAnchor307"/><a id="_idTextAnchor308"/> around in the scene.</p>
			<h2 id="_idParaDest-79"><a id="_idTextAnchor309"/>Sense</h2>
			<p>The <code>Sense</code> class is<a id="_idIndexMarker244"/> the interface of our sensory <a id="_idIndexMarker245"/>system that the other custom senses can implement. It defines two virtual methods, <code>Initialize</code> and <code>UpdateSense</code>, executed from the <code>Start</code> and <code>Update</code> methods, respectively, and that we can override when implementing custom senses as shown in the following code block:</p>
			<pre>using UnityEngine;
public class Sense : MonoBehaviour { 
    public bool bDebug = true;
    public Aspect.Affiliation targetAffiliation = 
      Aspect.Affiliation.Enemy;
    public float detectionRate = 1.0f;
    protected float elapsedTime = 0.0f;
    protected virtual void Initialize() { }
    protected virtual void UpdateSense() { }
    void Start () {
        Initialize();
    }
    void Update () {
        UpdateSense();
    }
}</pre>
			<p>The <a id="_idIndexMarker246"/>basic properties of this script are the intervals between two consecutive sensing operations and the name of the aspect it should look for. This script is not attached to any objects; instead, we use it as a base for specific senses, <a id="_idTextAnchor310"/><a id="_idTextAnchor311"/><a id="_idTextAnchor312"/><a id="_idTextAnchor313"/>such as <code>Sight</code> and <code>Touch</code>.</p>
			<h2 id="_idParaDest-80"><a id="_idTextAnchor314"/>Sight</h2>
			<p>The <code>Sight</code> sense <a id="_idIndexMarker247"/>detects <a id="_idIndexMarker248"/>whether a specific aspect is within the perception field of the character. If it perceives anything, it takes the specified action as shown in the following code block:</p>
			<pre>using UnityEngine;
public class Sight: Sense { 
    public int FieldOfView = 45; 
    public int ViewDistance = 100;
    private Transform playerTrans;
    private Vector3 rayDirection;
    protected override void Initialize() {
        //Find player position
        playerTrans = GameObject.FindGameObjectWithTag(
          "Player").transform;
    }
    protected override void UpdateSense() {
        elapsedTime += Time.deltaTime;
        // Detect perspective sense if within the detection 
        // rate
        if (elapsedTime &gt;= detectionRate) {
            DetectAspect();
            elapsedTime = 0.0f;
        }
    }
    //Detect perspective field of view for the AI Character
    void DetectAspect() {
        //Direction from current position to player 
        //position
        rayDirection = (playerTrans.position – 
                        transform.position).normalized;
        //Check the angle between the AI character's 
        //forward vector and the direction vector between 
        //player and AI to detect if the Player is in the 
        //field of view.
        if ((Vector3.Angle(rayDirection,
             transform.forward)) &lt; FieldOfView) {
            RaycastHit hit;
            if (Physics.Raycast(transform.position,
                rayDirection, out hit, ViewDistance)) {
                Aspect aspect =
                  hit.collider.GetComponent&lt;Aspect&gt;();
                if (aspect != null) {
                    //Check the aspect
                    if (aspect.affiliation == 
                        targetAffiliation) {
                        print("Enemy Detected");
                    }
                }
            }
        }
    }</pre>
			<p>We need to<a id="_idIndexMarker249"/> implement the <code>Initialize</code> and <code>UpdateSense</code> methods <a id="_idIndexMarker250"/>of the parent <code>Sense</code> class, respectively. Then, in<a id="_idIndexMarker251"/> the <code>DetectAspect</code> method, we first check the angle <a id="_idIndexMarker252"/>between the player and the AI's current direction. Then, if it's in the field-of-view range, we shoot a ray in the direction of the player's tank. The length of the ray is the value in the visible distance property.</p>
			<p>The <code>Raycast</code> method<a id="_idIndexMarker253"/> returns when it first hits another object. Then, we check this against the aspect component and the aspect name. In this way, even if the player is in the visible range, the AI character will not see the player <a id="_idTextAnchor315"/><a id="_idTextAnchor316"/>if they hide behind a wall.</p>
			<p>The <code>OnDrawGizmos</code> method<a id="_idIndexMarker254"/> draws lines based on the perspective field (determined by the view angle and viewing distance) to see the AI character's line of sight in the editor window during playtesting. Attach this script to the AI character, and ensure to set the aspect name to <code>Enemy</code>.</p>
			<p>This method can be illustrated as follows:</p>
			<pre>    void OnDrawGizmos() {
        if (!Application.isEditor|| playerTrans == null)
          return;
        Debug.DrawLine(transform.position,
                       playerTrans.position, Color.red);
        Vector3 frontRayPoint = transform.position + 
          (transform.forward * ViewDistance);
        //Approximate perspective visualization
        Vector3 leftRayPoint = Quaternion.Euler(
          0,FieldOfView * 0.5f ,0) * frontRayPoint;
        Vector3 rightRayPoint = Quaternion.Euler(0, 
          - FieldOfView*0.5f, 0) * frontRayPoint;
        Debug.DrawLine(transform.position, frontRayPoint,
                       Color.green);
        Debug.DrawLine(transform.position, leftRayPoint,
                       Color.green);
        Debug.DrawLine(transform.position, rightRayPoint, 
            <a id="_idTextAnchor317"/><a id="_idTextAnchor318"/><a id="_idTextAnchor319"/>           Color.green);
    }
}</pre>
			<p><code>OnDrawGizmos</code> is an <a id="_idIndexMarker255"/>event function that we<a id="_idIndexMarker256"/> can use when we want to draw gizmos in the scene. <code>DrawLine</code>, <code>DrawIcon</code>, and <code>DrawSphere</code>.</p>
			<p>They are a handy way to quickly provide some visual feedback to our algorithms. You can learn more about the <a id="_idIndexMarker258"/>gizmo functions by following this link: <a href="https://docs.unity3d.com/ScriptReference/Gizmos.html">https://docs.unity3d.com/ScriptReference/Gizmos.html</a>.</p>
			<h2 id="_idParaDest-81"><a id="_idTextAnchor320"/>Touch</h2>
			<p>Another sense we're going to<a id="_idIndexMarker259"/> implement is <code>Touch</code>, which is <a id="_idIndexMarker260"/>triggered when the player entity is within a specific range of the AI entity as shown in the following code block. Our AI character has a box collider component, and its <code>Is Trigger</code> flag is on:</p>
			<pre>using UnityEngine;
public class Touch : Sense {
    void OnTriggerEnter(Collider other) {
        Aspect aspect = other.GetComponent&lt;Aspect&gt;();
        if (aspect != null) {
            //Check the aspect
            if (aspect.affiliation == targetAffiliation) {
                print("Enemy Touch Detected");
            }
        }
    }
}</pre>
			<p>We need<a id="_idIndexMarker261"/> to implement the <code>OnTriggerEnter</code> event fired whenever the collider component collides with another collider component. Since our tank entity also has collider and<a id="_idIndexMarker262"/> <code>Rigidbody</code> components, a collision event occurs as soon as the colliders of the AI character and the player's tank coincide.</p>
			<p>The following screenshot shows the box collider of our enemy AI that we are using to implement the <code>Touch</code> sense:</p>
			<div><div><img src="img/B17984_04_4.jpg" alt="Figure 4.4 – The collider component around our player&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.4 – The collider component around our player</p>
			<p>In the <a id="_idIndexMarker263"/>following <a id="_idIndexMarker264"/>screenshot, we can see how our AI character is set up:<a id="_idTextAnchor321"/><a id="_idTextAnchor322"/></p>
			<div><div><img src="img/B17984_04_5.jpg" alt="Figure 4.5 – Properties of our player&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.5 – Properties of our player</p>
			<p>Inside the <code>OnTriggerEnter</code> method, we<a id="_idIndexMarker265"/> access the aspect component of the other collider entity and check whether the name of the aspect is the same aspect that this AI character is<a id="_idIndexMarker266"/> looking for. For demonstration purposes, we print out in the console that the character detects the enemy aspect by the <code>Touch</code> sense. In a real game, we would not print the event but rather trigger other actions, such as turning to face an<a id="_idTextAnchor323"/><a id="_idTextAnchor324"/> ene<a id="_idTextAnchor325"/><a id="_idTextAnchor326"/>my and then chasing, attacking, and so on. Let's move on to testing our game.</p>
			<h1 id="_idParaDest-82"><a id="_idTextAnchor327"/>Testing the game</h1>
			<p>Now, play<a id="_idIndexMarker267"/> the game in Unity3D and move the player's tank near the wandering AI character by clicking on the ground. You should see the <strong class="bold">Enemy touch detected</strong> message in the console log window whenever our AI character gets close to our player's tank.</p>
			<div><div><img src="img/B17984_04_6.jpg" alt="Figure 4.6 – Our player and tank in action&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.6 – Our player and tank in action</p>
			<p>The previous screenshot shows an AI agent with touch and perspective senses looking for an enemy aspect. Move the player's tank in front of the AI character, and you'll get the <em class="italic">Enemy detected</em> message. If you go into the editor view while running the game, you should see the rendered debug drawings thanks to the <code>OnDrawGizmos</code> me<a id="_idTextAnchor328"/><a id="_idTextAnchor329"/>thod implemented in the <code>Sight sense</code> class.</p>
			<h1 id="_idParaDest-83"><a id="_idTextAnchor330"/>Summary</h1>
			<p>This chapter introduced the concept of using sensors in implementing game AI, and we implemented two senses, <code>Sight</code> and <code>Touch</code>, for our AI character. The sensory system is just the first element of the decision-making system of a whole AI system. For example, we can use the sensory system to control the execution of a behavior system or change the state of a Finite State Machine once we have detected an enemy within the AI's line of sight.</p>
			<p>We will cover how to apply behavior tree systems in <a href="B17984_09_Epub.xhtml#_idTextAnchor487"><em class="italic">Chapter 9</em></a>, <em class="italic">Behavior Trees</em>. In the meantime, in the next chapter, we'll look at how to implement flocking behaviors in Unity3D, as well as how to implement Craig Reynold's flocking algorithm.</p>
		</div>
	</body></html>