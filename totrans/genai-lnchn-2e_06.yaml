- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building Intelligent Agents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As generative AI adoption grows, we start using LLMs for more open and complex
    tasks that require knowledge about fresh events or interaction with the world.
    This is what is generally called agentic applications. We’ll define what an agent
    is later in this chapter, but you’ve likely seen the phrase circulating in the
    media: *2025 is the year of agentic AI*. For example, in a recently introduced
    RE-Bench benchmark that consists of complex open-ended tasks, AI agents outperform
    humans in some settings (for example, with a thinking budget of 30 minutes) or
    on some specific class of tasks (like writing Triton kernels).'
  prefs: []
  type: TYPE_NORMAL
- en: To understand how these agentic capabilities are built in practice, we’ll start
    by discussing tool calling with LLMs and how it is implemented on LangChain. We’ll
    look in detail at the ReACT pattern, and how LLMs can use tools to interact with
    the external environment and improve their performance on specific tasks. Then,
    we’ll touch on how tools are defined in LangChain, and which pre-built tools are
    available. We’ll also talk about developing your own custom tools, handling errors,
    and using advanced tool-calling capabilities. As a practical example, we’ll look
    at how to generate structured outputs with LLM using tools versus utilizing built-in
    capabilities offered by model providers.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we’ll talk about what agents are and look into more advanced patterns
    of building agents with LangGraph before we then develop our first ReACT agent
    with LangGraph—a research agent that follows a plan-and-solve design pattern and
    uses tools such as web search, *arXiv*, and *Wikipedia*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a nutshell, the following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: What is a tool?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining built-in LangChain tools and custom tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advanced tool-calling capabilities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Incorporating tools into workflows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are agents?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can find the code for this chapter in the `chapter5/` directory of the book’s
    GitHub repository. Please visit [https://github.com/benman1/generative_ai_with_langchain/tree/second_edition](https://github.com/benman1/generative_ai_with_langchain/tree/second_edition)
    for the latest updates.
  prefs: []
  type: TYPE_NORMAL
- en: See [*Chapter 2*](E_Chapter_2.xhtml#_idTextAnchor044) for setup instructions.
    If you have any questions or encounter issues while running the code, please create
    an issue on GitHub or join the discussion on Discord at [https://packt.link/lang](https://packt.link/lang).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin with tools. Rather than diving straight into defining what an agent
    is, it’s more helpful to first explore how enhancing LLMs with tools actually
    works in practice. By walking through this step by step, you’ll see how these
    integrations unlock new capabilities. So, what exactly are tools, and how do they
    extend what LLMs can do?
  prefs: []
  type: TYPE_NORMAL
- en: What is a tool?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LLMs are trained on vast general corpus data (like web data and books), which
    gives them broad knowledge but limits their effectiveness in tasks that require
    domain-specific or up-to-date knowledge. However, because LLMs are good at reasoning,
    they can interact with the external environment through tools—APIs or interfaces
    that allow the model to interact with the external world. These tools enable LLMs
    to perform specific tasks and receive feedback from the external world.
  prefs: []
  type: TYPE_NORMAL
- en: 'When using tools, LLMs perform three specific generation tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Choose a tool to use by generating special tokens and the name of the tool.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate a payload to be sent to the tool.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate a response to a user based on the initial question and a history of
    interactions with tools (for this specific run).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now it’s time to figure out how LLMs invoke tools and how we can make LLMs
    tool-aware. Consider a somewhat artificial but illustrative question: *What is
    the square root of the current US president’s age multiplied by 132*? This question
    presents two specific challenges:'
  prefs: []
  type: TYPE_NORMAL
- en: It references current information (as of March 2025) that likely falls outside
    the model’s training data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It requires a precise mathematical calculation that LLMs might not be able to
    answer correctly just by autoregressive token generation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rather than forcing an LLM to generate an answer solely based on its internal
    knowledge, we’ll give an LLM access to two tools: a search engine and a calculator.
    We expect the model to determine which tools it needs (if any) and how to use
    them.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For clarity, let’s start with a simpler question and mock our tools by creating
    dummy functions that always give the same response. Later in this chapter, we’ll
    implement fully functional tools and invoke them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s make sure that when the LLM has enough internal knowledge, it replies
    directly to the user:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let’s give the model output of a tool by incorporating it into a prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'As a last observation, if the search result is not successful, the LLM will
    try to refine the query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'With that, we have demonstrated how tool calling works. Please note that we’ve
    provided prompt examples for demonstration purposes only. Another foundational
    LLM might require some prompt engineering, and our prompts are just an illustration.
    And good news: using tools is easier than it seems from these examples!'
  prefs: []
  type: TYPE_NORMAL
- en: As you can note, we described everything in our prompt, including a tool description
    and a tool-calling format. These days, most LLMs provide a better API for tool
    calling since modern LLMs are post-trained on datasets that help them excel in
    such tasks. The LLMs’ creators know how these datasets were constructed. That’s
    why, typically, you don’t incorporate a tool description yourself in the prompt;
    you just provide both a prompt and a tool description as separate arguments, and
    they are combined into a single prompt on the provider’s side. Some smaller open-source
    LLMs expect tool descriptions to be part of the raw prompt, but they would expect
    a well-defined format.
  prefs: []
  type: TYPE_NORMAL
- en: LangChain makes it easy to develop pipelines where an LLM invokes different
    tools and provides access to many helpful built-in tools. Let’s look at how tool
    handling works with LangChain.
  prefs: []
  type: TYPE_NORMAL
- en: Tools in LangChain
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With most modern LLMs, to use tools, you can provide a list of tool descriptions
    as a separate argument. As always in LangChain, each particular integration implementation
    maps the interface to the provider’s API. For tools, this happens through LangChain’s
    `tools` argument to the `invoke` method (and some other useful methods such as
    `bind_tools` and others, as we will learn in this chapter).
  prefs: []
  type: TYPE_NORMAL
- en: When defining a tool, we need to specify its schema in OpenAPI format. We provide
    a *title* and a *description* of the tool and also specify its parameters (each
    parameter has a *type*, *title*, and *description*). We can inherit such a schema
    from various formats, which LangChain translates into OpenAPI format. As we go
    through the next few sections, we’ll illustrate how we can do this from functions,
    docstrings, Pydantic definitions, or by inheriting from a `BaseTool` class and
    providing descriptions directly. For an LLM, a tool is anything that has an OpenAPI
    specification—in other words, it can be called by some external mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: The LLM itself doesn’t bother about this mechanism, it only produces instructions
    for when and how to call a tool. For LangChain, a tool is also something that
    can be called (and we will see later that tools are inherited from `Runnables`)
    when we execute our program.
  prefs: []
  type: TYPE_NORMAL
- en: 'The wording that you use in the *title* and *description* fields is extremely
    important, and you can treat it as a part of the prompt engineering exercise.
    Better wording helps LLMs make better decisions on when and how to call a specific
    tool. Please note that for more complex tools, writing a schema like this can
    become tedious, and we’ll see a simpler way to define tools later in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'If we inspect the `result.content` field, it would be empty. That’s because
    the LLM has decided to call a tool, and the output message has a hint for that.
    What happens under the hood is that LangChain maps a specific output format of
    the model provider into a unified tool-calling format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Keep in mind that some model providers might return non-empty content even in
    the case of tool calling (for example, there might be reasoning traces on why
    the model decided to call a tool). You need to look at the model provider specification
    to understand how to treat such cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, an LLM returned an array of tool-calling dictionaries—each of
    them contains a unique identifier, the name of the tool to be called, and a dictionary
    with arguments to be provided to this tool. Let’s move to the next step and invoke
    the model again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '`ToolMessage` is a special message on LangChain that allows you to feed the
    output of a tool execution back to the model. The `content` field of such a message
    contains the tool’s output, and a special field `tool_call_id` maps it to the
    specific tool calling that was generated by the model. Now, we can send the whole
    sequence (consisting of the initial output, the step with tool calling, and the
    output) back to the model as a list of messages.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It might be odd to always pass a list of tools to the LLM (since, typically,
    such a list is fixed for a given workflow). For that reason, LangChain `Runnables`
    offer a `bind` method that memorizes arguments and adds them to every further
    invocation. Take a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'When we call `llm.bind(tools=[search_tool])`, LangChain creates a new object
    (assigned here to `llm_with_tools`) that automatically includes `[search_tool]`
    in every subsequent call to a copy of the initial `llm` one. Essentially, you
    no longer need to pass the tools argument with each `invoke` method. So, calling
    the preceding code is the same as doing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This is because bind has “memorized” your tools list for all future invocations.
    It’s mainly a convenience feature—ideal if you want a fixed set of tools for repeated
    calls rather than specifying them every time. Now let’s see how we can utilize
    tool calling even more, and improve LLM reasoning!
  prefs: []
  type: TYPE_NORMAL
- en: ReACT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As you have probably thought already, LLMs can call multiple tools before generating
    the final reply to the user (and the next tool to be called or a payload sent
    to this tool might depend on the outcome from the previous tool calls). This was
    proposed by a ReACT approach introduced in 2022 by researchers from Princeton
    University and Google Research: *Reasoning and ACT* ([https://arxiv.org/abs/2210.03629](https://arxiv.org/abs/2210.03629)).
    The idea is simple—we should give the LLM access to tools as a way to interact
    with an external environment, and let the LLM run in a loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reason**: Generate a text output with observations about the current situation
    and a plan to solve the task.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Act**: Take an action based on the reasoning above (interact with the environment
    by calling a tool, or respond to the user).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has been demonstrated that ReACT can help reduce hallucination rates compared
    to CoT prompting, which we discussed in [*Chapter 3*](E_Chapter_3.xhtml#_idTextAnchor107).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1: ReACT pattern](img/B32363_05_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.1: ReACT pattern'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s build a ReACT application ourselves. First, let’s create mocked search
    and calculator tools:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In the next section, we’ll see how we can build actual tools. For now, let’s
    define a schema for the calculator tool and make the LLM aware of both tools it
    can use. We’ll also use building blocks that we’re already familiar with—`ChatPromptTemplate`
    and `MessagesPlaceholder`—to prepend a predetermined system message when we call
    our graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have an LLM that can call tools, let’s create the nodes we need.
    We need one function that calls an LLM, another function that invokes tools and
    returns tool-calling results (by appending `ToolMessages` to the list of messages
    in the state), and a function that will determine whether the orchestrator should
    continue calling tools or whether it can return the result to the user:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s bring everything together in a LangGraph workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This demonstrates how the LLM made several calls to handle a complex question—first,
    to `Google Search` and then two calls to `Calculator`—and each time, it used the
    previously received information to adjust its actions. This is the ReACT pattern
    in action.
  prefs: []
  type: TYPE_NORMAL
- en: 'With that, we’ve learned how the ReACT pattern works in detail by building
    it ourselves. The good news is that LangGraph offers a pre-built implementation
    of a ReACT pattern, so you don’t need to implement it yourself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: In [*Chapter 6*](E_Chapter_6.xhtml#_idTextAnchor274), we’ll see some additional
    adjustments you can use with the `create_react_agent` function.
  prefs: []
  type: TYPE_NORMAL
- en: Defining tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have defined tools as OpenAPI schemas. But to run the workflow end
    to end, LangGraph should be able to call tools itself during the execution. Hence,
    in this section, let’s discuss how we define tools as Python functions or callables.
  prefs: []
  type: TYPE_NORMAL
- en: 'A LangChain tool has three essential components:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Name`: A unique identifier for the tool'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Description`: Text that helps the LLM understand when and how to use the tool'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Payload schema`: A structured definition of the inputs the tool accepts'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It allows an LLM to decide when and how to call a tool. Another important distinction
    of a LangChain tool is that it can be executed by an orchestrator, such as LangGraph.
    The base interface for a tool is `BaseTool`, which inherits from a `RunnableSerializable`
    itself. That means it can be invoked or batched as any `Runnable`, or serialized
    or deserialized as any `Serializable`.
  prefs: []
  type: TYPE_NORMAL
- en: Built-in LangChain tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LangChain has many tools already available across various categories. Since
    tools are often provided by third-party vendors, some tools require paid API keys,
    some of them are completely free, and some of them have a free tier. Some tools
    are grouped together in toolkits—collections of tools that are supposed to be
    used together when working on a specific task. Let’s see some examples of using
    tools.
  prefs: []
  type: TYPE_NORMAL
- en: Tools give an LLM access to search engines, such as Bing, DuckDuckGo, Google,
    and Tavily. Let’s take a look at `DuckDuckGoSearchRun` as this search engine doesn’t
    require additional registration and an API key.
  prefs: []
  type: TYPE_NORMAL
- en: Please see [*Chapter 2*](E_Chapter_2.xhtml#_idTextAnchor044) for setup instructions.
    If you have any questions or encounter issues while running the code, please create
    an issue on GitHub or join the discussion on Discord at [https://packt.link/lang](https://packt.link/lang).
  prefs: []
  type: TYPE_NORMAL
- en: 'As with any tool, this tool has a name, description, and schema for input arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The argument schema, `arg_schema`, is a Pydantic model and we’ll see why it’s
    useful later in this chapter. We can explore its fields either programmatically
    or by going to the documentation page—it expects only one input field, a query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can invoke this tool and get a string output back (results from the
    search engine):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also invoke the LLM with tools, and let’s make sure that the LLM invokes
    the search tool and does not answer directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Our tool is now a callable that LangGraph can call programmatically. Let’s
    put everything together and create our first agent. When we stream our graph,
    we get updates to the state. In our case, these are only messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 5.2: A pre-built ReACT workflow on LangGraph](img/B32363_05_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.2: A pre-built ReACT workflow on LangGraph'
  prefs: []
  type: TYPE_NORMAL
- en: That’s exactly what we saw earlier as well—an LLM is calling tools until it
    decides to stop and return the answer to the user. Let’s test it out!
  prefs: []
  type: TYPE_NORMAL
- en: 'When we stream LangGraph, we get new events that are updates to the graph’s
    state. We’re interested in the `message` field of the state. Let’s print out the
    new messages added:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Our agent is represented by a list of messages since this is the input and
    output that the LLM expects. We’ll see that pattern again when we dive deeper
    into agentic architectures and discuss it in the next chapter. For now, let’s
    briefly mention other types of tools that are already available on LangChain:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Tools that enhance the LLM’s knowledge besides using a search engine**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Academic research: arXiv and PubMed'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Knowledge bases: Wikipedia and Wikidata'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Financial data: Alpha Vantage, Polygon, and Yahoo Finance'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Weather: OpenWeatherMap'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Computation: Wolfram Alpha'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tools that enhance your productivity**: You can interact with Gmail, Slack,
    Office 365, Google Calendar, Jira, Github, etc. For example, `GmailToolkit` gives
    you access to `GmailCreateDraft`, `GmailSendMessage`, `GmailSearch`, `GmailGetMessage`,
    and `GmailGetThread` tools that allow you to search, retrieve, create, and send
    messages with your Gmail account. As you can see, not only can you give the LLM
    additional context about the user but, with some of these tools, LLMs can take
    actions that actually influence the outside environment, such as creating a pull
    request on GitHub or sending a message on Slack!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tools that give an LLM access to a code interpreter**: These tools give LLMs
    access to a code interpreter by remotely launching an isolated container and giving
    LLMs access to this container. These tools require an API key from a vendor providing
    the sandboxes. LLMs are especially good at coding, and it’s a widely used pattern
    to ask an LLM to solve some complex task by writing code that solves it instead
    of asking it to generate tokens that represent the solution of the task. Of course,
    you should execute code generated by LLMs with caution, and that’s why isolated
    sandboxes play a huge role. Some examples are:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Code execution: Python REPL and Bash'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cloud services: AWS Lambda'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'API tools: GraphQL and Requests'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'File operations: File System'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tools that give an LLM access to databases by writing and executing SQL code**:
    For example, `SQLDatabase` includes tools to get information about the database
    and its objects and execute SQL queries. You can also access Google Drive with
    `GoogleDriveLoader` or perform operations with usual file system tools from a
    `FileManagementToolkit`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Other tools**: These comprise tools that integrate third-party systems and
    allow the LLM to gather additional information or act. There are also tools that
    can integrate data retrieval from Google Maps, NASA, and other platforms and organizations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tools for using other AI systems or automation**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Image generation: DALL-E and Imagen'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Speech synthesis: Google Cloud TTS and Eleven Labs'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Model access: Hugging Face Hub'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Workflow automation: Zapier and IFTTT'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Any external system with an API can be wrapped as a tool if it enhances an
    LLM like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Provides relevant domain knowledge to the user or the workflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allows an LLM to take actions on the user’s behalf
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When integrating such tools with LangChain, consider these key aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Authentication**: Secure access to the external system'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Payload schema**: Define proper data structures for input/output'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Error handling**: Plan for failures and edge cases'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Safety considerations**: For example, when developing a SQL-to-text agent,
    restrict access to read-only operations to prevent unintended modifications'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Therefore, an important toolkit is the `RequestsToolkit`, which allows one
    to easily wrap any HTTP API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s take a free open-source currency API ([https://frankfurter.dev/](https://frankfurter.dev/)).
    It’s a random free API we took from the Internet for illustrative purposes only,
    just to show you how you can wrap any existing API as a tool. First, we need to
    put together an API spec based on the OpenAPI format. We truncated the spec but
    you can find the full version on our GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s build and run our ReACT agent; we’ll see that the LLM can query the
    third-party API and provide fresh answers on currency exchange rates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Observe that, this time, we use a `stream_mode="values"` option, and in this
    option, each time, we get a full current state from the graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are over 50 tools already available. You can find a full list on the
    documentation page: [https://python.langchain.com/docs/integrations/tools/](https://python.langchain.com/docs/integrations/tools/).'
  prefs: []
  type: TYPE_NORMAL
- en: Custom tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We looked at the variety of built-in tools offered by LangGraph. Now it’s time
    to discuss how you can create your own custom tools, besides the example we looked
    at when we wrapped the third-party API with the `RequestsToolkit` by providing
    an API spec. Let’s get down to it!
  prefs: []
  type: TYPE_NORMAL
- en: Wrapping a Python function as a tool
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Any `Python` function (or callable) can be wrapped as a tool. As we remember,
    a tool on LangChain should have a name, a description, and an argument schema.
    Let’s build our own calculator based on the Python `numexr` library—a fast numerical
    expression evaluator based on NumPy ([https://github.com/pydata/numexpr](https://github.com/pydata/numexpr)).
    We’re going to use a special `@tool` decorator that will wrap our function as
    a tool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s explore the calculator object we have! Notice that LangChain auto-inherited
    the name, the description, and args schema from the docstring and type hints.
    Please note that we used a few-shot technique (discussed in [*Chapter 3*](E_Chapter_3.xhtml#_idTextAnchor107))
    to teach LLMs how to prepare the payload for our tool by adding two examples in
    the docstring:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s try out our new tool to evaluate an expression with complex numbers,
    which extend real numbers with a special imaginary unit `i` that has a property
    `i**2=-1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'With just a few lines of code, we’ve successfully extended our LLM’s capabilities
    to work with complex numbers. Now we can put together the example we started with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We haven’t provided the full output here in the book (you can find it on our
    GitHub), but if you run this snippet, you should see that the LLM was able to
    query tools step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: It called the search engine with the query `"current US president"`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, it again called the search engine with the query `"donald trump age"`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As the last step, the LLM called the calculator tool with the expression `"sqrt(78*132)"`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, it returned the correct answer to the user.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At every step, the LLM reasoned based on the previously collected information
    and then acted with an appropriate tool—that’s the essence of the ReACT approach.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a tool from a Runnable
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Sometimes, LangChain might not be able to derive a passing description or args
    schema from a function, or we might be using a complex callable that is difficult
    to wrap with a decorator. For example, we can use another LangChain chain or LangGraph
    graph as a tool. We can create a tool from any `Runnable` by explicitly specifying
    all needed descriptions. Let’s create a calculator tool from a function in an
    alternative fashion, and we will tune the retry behavior (in our case, we’re going
    to retry three times and add an exponential backoff between consecutive attempts):'
  prefs: []
  type: TYPE_NORMAL
- en: Please note that we use the same function as above but we removed the `@tool`
    decorator.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Observe that we defined our function in a similar way to how we define LangGraph
    nodes—it takes a state (which now is a Pydantic model) and a config. Then, we
    wrapped this function as `RunnableLambda` and added retries. It might be useful
    if we want to keep our Python function as a function without wrapping it with
    a decorator, or if we want to wrap an external API (hence, description and arguments
    schema can’t be auto-inherited from the docstrings). We can use any Runnable (for
    example, a chain or a graph) to create a tool, and that allows us to build multi-agent
    systems since now one LLM-based workflow can invoke another LLM-based one. Let’s
    convert our Runnable to a tool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s test our new `calculator` function with the LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can note, LangChain didn’t inherit the `args` schema fully; that’s why
    it created artificial names for arguments like `__arg1`. Let’s change our tool
    to accept a Pydantic model instead, in a similar fashion to how we define LangGraph
    nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Now the full schema is a proper one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s test it together with an LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'We can call our calculator tool and pass it to the LangGraph configuration
    in runtime:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: With that, we have learned how we can easily convert any Runnable to a tool
    by providing additional details to LangChain to ensure an LLM can correctly handle
    this tool.
  prefs: []
  type: TYPE_NORMAL
- en: Subclass StructuredTool or BaseTool
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Another method to define a tool is by creating a custom tool by subclassing
    the `BaseTool` class. As with other approaches, you must specify the tool’s name,
    description, and argument schema. You’ll also need to implement one or two abstract
    methods: `_run` for synchronous execution and, if necessary, `_arun` for asynchronous
    behavior (if it differs from simply wrapping the sync version). This option is
    particularly useful when your tool needs to be stateful (for example, to maintain
    long-lived connection clients) or when its logic is too complex to be implemented
    as a single function or `Runnable`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want more flexibility than a `@tool` decorator gives you but don’t want
    to implement your own class, there’s an intermediate approach. You can also use
    the `StructuredTool.from_function` class method, which allows you to explicitly
    specify tools’ meta parameters such as description or `args_schema` with a few
    lines of code only:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: One last note about synchronous and asynchronous implementations is necessary
    at this point. If an underlying function besides your tool is a synchronous function,
    LangChain will wrap it for the tool’s asynchronous implementation by launching
    it in a separate thread. In most cases, it doesn’t matter, but if you care about
    the additional overhead of creating a separate thread, you have two options—either
    subclass from the `BaseClass` and override async implementation, or create a separate
    async implementation of your function and pass it to the `StructruredTool.from_function`
    as a `coroutine` argument. You can also provide only async implementation, but
    then you won’t be able to invoke your workflows in a synchronous manner.
  prefs: []
  type: TYPE_NORMAL
- en: To conclude, let’s take another look at three options that we have to create
    a LangChain tool, and when to use each of them.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Method to create a tool** | **When to use** |'
  prefs: []
  type: TYPE_TB
- en: '| @tool decorator | You have a function with clear docstrings and this function
    isn’t used anywhere in your code |'
  prefs: []
  type: TYPE_TB
- en: '| convert_runnable_to_tool | You have an existing Runnable, or you need more
    detailed controlled on how arguments or tool descriptions are passed to an LLM
    (you wrap an existing function by a RunnableLambda in that case) |'
  prefs: []
  type: TYPE_TB
- en: '| subclass from StructuredTool or BaseTool | You need full control over tool
    description and logic (for example, you want to handle sync and async requests
    differently) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5.1: Options to create a LangChain tool'
  prefs: []
  type: TYPE_NORMAL
- en: When an LLM generates payloads and calls tools, it might hallucinate or make
    other mistakes. Therefore, we need to carefully think about error handling.
  prefs: []
  type: TYPE_NORMAL
- en: Error handling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We already discussed error handling in [*Chapter 3*](E_Chapter_3.xhtml#_idTextAnchor107),
    but it becomes even more important when you enhance an LLM with tools; you need
    logging, working with exceptions, and so on even more. One additional consideration
    is to think about whether you would like your workflow to continue and try to
    auto-recover if one of your tools fails. LangChain has a special `ToolException`
    that allows the workflow to continue its execution by handling the exception.
  prefs: []
  type: TYPE_NORMAL
- en: '`BaseTool` has two special flags: `handle_tool_error` and `handle_validation_error`.
    Of course, since `StructuredTool` inherits from `BaseTool`, you can pass these
    flags to the `StructuredTool.from_function` class method. If this flag is set,
    LangChain would construct a string to return as a result of tools’ execution if
    either a `ToolException` or a Pydantic `ValidationException` (when validating
    input payload) happens.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand what happens, let’s take a look at the LangChain source code
    for the `_handle_tool_error` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, we can set this flag to a Boolean, string, or callable (that
    converts a `ToolException` to a string). Based on this, LangChain would try to
    handle `ToolException` and pass a string to the next stage instead. We can incorporate
    this feedback into our workflow and add an auto-recover loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at an example. We adjust our `calculator` function by removing a
    substitution `i->j` (a substitution from an imaginary unit in math to an imaginary
    unit in Python), and we also make `StructuredTool` auto-inherit descriptions and
    `arg_schema` from the docstring:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, now our execution of a calculator fails, but since the error
    description is not clear enough, the LLM decides to respond itself without using
    the tool. Depending on your use case, you might want to adjust the behavior; for
    example, provide more meaningful errors from the tool, force the workflow to try
    to adjust the payload for the tool, etc.
  prefs: []
  type: TYPE_NORMAL
- en: LangGraph also offers a built-in `ValidationNode` that takes the last messages
    (by inspecting the `messages` key in the graph’s state) and checks whether it
    has tool calls. If that’s the case, LangGraph validates the schema of the tool
    call, and if it doesn’t follow the expected schema, it raises a `ToolMessage`
    with the validation error (and a default command to fix it). You can add a conditional
    edge that cycles back to the LLM and then the LLM would regenerate the tool call,
    similar to the pattern we discussed in [*Chapter 3*](E_Chapter_3.xhtml#_idTextAnchor107).
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve learned what a tool is, how to create one, and how to use built-in
    LangChain tools, it’s time to take a look at additional instructions that you
    can pass to an LLM on how to use tools.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced tool-calling capabilities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many LLMs offer you some additional configuration options on tool calling. First,
    some models support parallel function calling—specifically, an LLM can call multiple
    tools at once. LangChain natively supports this since the `tool_calls` field of
    an `AIMessage` is a list. When you return `ToolMessage` objects as function call
    results, you should carefully match the `tool_call_id` field of a `ToolMessage`
    to the generated payload. This alignment is necessary so that LangChain and the
    underlying LLM can match them together when doing the next turn.
  prefs: []
  type: TYPE_NORMAL
- en: Another advanced capability is forcing an LLM to call a tool, or even to call
    a specific tool. Generally speaking, an LLM decides whether it should call a tool,
    and if it should, which tool to call from the list of provided tools. Typically,
    it’s handled by `tool_choice` and/or `tool_config` arguments passed to the `invoke`
    method, but implementation depends on the model’s provider. Anthropic, Google,
    OpenAI, and other major providers have slightly different APIs, and although LangChain
    tries to unify arguments, in such cases, you should double-check details by the
    model’s provider.
  prefs: []
  type: TYPE_NORMAL
- en: 'Typically, the following options are available:'
  prefs: []
  type: TYPE_NORMAL
- en: '`"auto"`: An LLM can respond or call one or many tools.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"any"`: An LLM is forced to respond by calling one or many tools.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"tool"` or `"any"` with a provided list of tools: An LLM is forced to respond
    by calling a tool from the restricted list.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"None"`: An LLM is forced to respond without calling a tool.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another important thing to keep in mind is that schemas might become pretty
    complex—i.e., they might have nullable fields or nested fields, include enums,
    or reference other schemas. Depending on the model’s provider, some definitions
    might not be supported (and you will see warning or compiling errors). Although
    LangChain aims to make switching across vendors seamless, for some complex workflows,
    this might not be the case, so pay attention to warnings in the error logs. Sometimes,
    compilations of a provided schema to a schema supported by the model’s provider
    are done on the best effort basis—for example, a field with a type of `Union[str,
    int]` is compiled to a `str` type if an underlying LLM doesn’t support `Union`
    types with tool calling. You’ll get a warning, but ignoring such a warning during
    a migration might change the behavior of your application unpredictably.
  prefs: []
  type: TYPE_NORMAL
- en: As a final note, it is worth mentioning that some providers (for example, OpenAI
    or Google) offer custom tools, such as a code interpreter or Google search, that
    can be invoked by the model itself, and the model will use the tool’s output to
    prepare a final generation. You can think of this as a ReACT agent on the provider’s
    side, where the model receives an enhanced response based on a tool it calls.
    This approach reduces latency and costs. In these cases, you typically supply
    the LangChain wrapper with a custom tool created using the provider’s SDK rather
    than one built with LangChain (i.e., a tool that doesn’t inherit from the `BaseTool`
    class), which means your code won’t be transferable across models.
  prefs: []
  type: TYPE_NORMAL
- en: Incorporating tools into workflows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we know how to create and use tools, let’s discuss how we can incorporate
    the tool-calling paradigm deeper into the workflows we’re developing.
  prefs: []
  type: TYPE_NORMAL
- en: Controlled generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [*Chapter 3*](E_Chapter_3.xhtml#_idTextAnchor107), we started to discuss
    a *controlled* generation, when you want an LLM to follow a specific schema. We
    can improve our parsing workflows not only by creating more sophisticated and
    reliable parsers but also by being more strict in forcing an LLM to adhere to
    a certain schema. Calling a tool requires controlled generation since the generated
    payload should follow a specific schema, but we can take a step back and substitute
    our expected schema with a forced tool calling that follows the expected schema.
    LangChain has a built-in mechanism to help with that—an LLM has the `with_structured_output`
    method that takes a schema as a Pydantic model, converts it to a tool, invokes
    the LLM with a given prompt by forcing it to call this tool, and parses the output
    by compiling to a corresponding Pydantic model instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Later in this chapter, we’ll discuss a plan-and-solve agent, so let’s start
    preparing a building block. Let’s ask our LLM to generate a plan for a given action,
    but instead of parsing the plan, let’s define it as a Pydantic model (a `Plan`
    is a list of `Steps`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Keep in mind that we use nested models (one field is referencing another),
    but LangChain will compile a unified schema for us. Let’s put together a simple
    workflow and run it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'If we inspect the output, we’ll see that we got a Pydantic model as a result.
    We don’t need to parse the output anymore; we got a list of specific steps out
    of the box (and later, we’ll see how we can use it further):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Controlled generation provided by the vendor
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Another way is vendor-dependent. Some foundational model providers offer additional
    API parameters that can instruct a model to generate a structured output (typically,
    a JSON or enum). You can force the model to use JSON generation the same way as
    above using `with_structured_output`, but provide another argument, `method="json_mode"`
    (and double-check that the underlying model provider supports controlled generation
    as JSON):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the JSON schema doesn’t contain descriptions of the fields, hence
    typically, your prompts should be more detailed and informative. But as an output,
    we get a full-qualified Python dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'You can instruct the LLM instance directly to follow controlled generation
    instructions. Note that specific arguments and functionality might vary from one
    model provider to another (for example, OpenAI models use a `response_format`
    argument). Let’s look at how to instruct Gemini to return JSON:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also ask Gemini to return an enum—in other words, only one value from
    a set of values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: LangChain abstracts the details of the model provider’s implementation with
    the `method="json_mode"` parameter or by allowing custom `kwargs` to be passed
    to the model. Some of the controlled generation capabilities are model-specific.
    Check your model’s documentation for supported schema types, constraints, and
    arguments.
  prefs: []
  type: TYPE_NORMAL
- en: ToolNode
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To simplify agent development, LangGraph has built-in capabilities such as `ToolNode`
    and `tool_conditions`. The `ToolNode` checks the last message in `messages` (you
    can redefine the key name). If this message contains tool calls, it invokes the
    corresponding tools and updates the state. On the other hand, `tool_conditions`
    is a conditional edge that checks whether `ToolNode` should be called (or finishes
    otherwise).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can build our ReACT engine in minutes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Tool-calling paradigm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tool calling is a very powerful design paradigm that requires a change in how
    you develop your applications. In many cases, instead of performing rounds of
    prompt engineering and many attempts to improve your prompts, think whether you
    could ask the model to call a tool instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s assume we’re working on an agent that deals with contract cancellations
    and it should follow certain business logic. First, we need to understand the
    contract starting date (and dealing with dates might be difficult!). If you try
    to come up with a prompt that can correctly handle cases like this, you’ll realize
    it might be quite difficult:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Instead, force a model to call a tool (and maybe even through a ReACT agent!).
    For example, we have two very native tools in Python—`date` and `timedelta`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Now it works like a charm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: We learned how to use tools, or function calls, to enhance LLMs’ performance
    on complex tasks. This is one of the fundamental architectural patterns behind
    agents—now it’s time to discuss what an agent is.
  prefs: []
  type: TYPE_NORMAL
- en: What are agents?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Agents are one of the hottest topics of generative AI these days. People talk
    about agents a lot, but there are many different definitions of what an agent
    is. LangChain itself defines an agent as “*a system that uses an LLM to decide
    the control flow of an application*.” While we feel it’s a great definition that
    is worth citing, it missed some aspects.
  prefs: []
  type: TYPE_NORMAL
- en: 'As Python developers, you might be familiar with duck typing to determine an
    object’s behavior by the so-called duck test: “*If it walks like a duck and it
    quacks like a duck, then it must be a duck*.” With that concept in mind, let’s
    describe some properties of an agent in the context of generative AI:'
  prefs: []
  type: TYPE_NORMAL
- en: Agents help a user solve complex non-deterministic tasks without being given
    an explicit algorithm on how to do it. Advanced agents can even act on behalf
    of a user.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To solve a task, agents typically perform multiple steps and iterations. They
    *reason* (generate new information based on available context), *act* (interact
    with the external environment), *observe* (incorporate feedback from the external
    environment), and *communicate* (interact and/or work collaboratively with other
    agents or humans).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agents utilize LLMs for reasoning (and solving tasks).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While agents have certain autonomy (and to a certain extent, they even figure
    out what is the best way to solve the task by thinking and learning from interacting
    with the environment), when running an agent, we’d still like to keep a certain
    degree of control of the execution flow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retaining control over an agent’s behavior—an agentic workflow—is a core concept
    behind LangGraph. While LangGraph provides developers with a rich set of building
    blocks (such as memory management, tool invocation, and cyclic graphs with recursion
    depth control), its primary design pattern focuses on managing the flow and level
    of autonomy that LLMs exercise in executing tasks. Let’s start with an example
    and develop our agent.
  prefs: []
  type: TYPE_NORMAL
- en: Plan-and-solve agent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What do we as humans typically do when we have a complex task ahead of us? We
    plan! In 2023, Lei Want et al. demonstrated that plan-and-solve prompting improves
    LLM reasoning. It has been also demonstrated by multiple studies that LLMs’ performance
    tends to deteriorate as the complexity (in particular, the length and the number
    of instructions) of the prompt increases.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, the first design pattern to keep in mind is *task decomposition*—to decompose
    complex tasks into a sequence of smaller ones, keep your prompts simple and focused
    on a single task, and don’t hesitate to add examples to your prompts. In our case,
    we are going to develop a research assistant.
  prefs: []
  type: TYPE_NORMAL
- en: Faced with a complex task, let’s first ask the LLM to come up with a detailed
    plan to solve this task, and then use the same LLM to execute on every step. Remember,
    at the end of the day, LLMs autoregressively generate output tokens based on input
    tokens. Such simple patterns as ReACT or plan-and-solve help us to better use
    their implicit reasoning capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to define our planner. There’s nothing new here; we’re using
    building blocks that we have already discussed—chat prompt templates and controlled
    generation with a Pydantic model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'For a step execution, let’s use a ReACT agent with built-in tools—DuckDuckGo
    search, retrievers from arXiv and Wikipedia, and our custom `calculator` tool
    we developed earlier in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let’s define our workflow state. We need to keep track of the initial
    task and initially generated plan, and let’s add `past_steps` and `final_response`
    to the state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, it’s time to define our nodes and edges:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'And put together the final graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 5.3: Plan-and-solve agentic workflow](img/B32363_05_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.3: Plan-and-solve agentic workflow'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can run the workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: You can see the full output on our GitHub, and we encourage you to play with
    it yourself. It might be especially interesting to investigate whether you like
    the result more compared to a single LLM prompt with a given task.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored how to enhance LLMs by integrating tools and design
    patterns for tool invocation, including the ReACT pattern. We started by building
    a ReACT agent from scratch and then demonstrated how to create a customized one
    with just one line of code using LangGraph.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we delved into advanced techniques for controlled generation—showing how
    to force an LLM to call any tool or a specific one, and instructing it to return
    responses in structured formats (such as JSON, enums, or Pydantic models). In
    that context, we covered LangChain’s `with_structured_output` method, which transforms
    your data structure into a tool schema, prompts the model to call the tool, parses
    the output, and compiles it into a corresponding Pydantic instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we built our first plan-and-solve agent with LangGraph, applying all
    the concepts we’ve learned so far: tool calling, ReACT, structured outputs, and
    more. In the next chapter, we’ll continue discussing how to develop agents and
    look into more advanced architectural patterns.'
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What are the key benefits of using tools with LLMs, and why are they important?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does LangChain’s ToolMessage class facilitate communication between the
    LLM and the external environment?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explain the ReACT pattern. What are its two main steps? How does it improve
    LLM performance?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How would you define a generative AI agent? How does this relate to or differ
    from LangChain’s definition?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explain some advantages and disadvantages of using the with_structured_output
    method compared to using a controlled generation directly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can you programmatically define a custom tool in LangChain?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explain the purpose of the Runnable.bind() and bind_tools() methods in LangChain.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does LangChain handle errors that occur during tool execution? What options
    are available for configuring this behavior?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Subscribe to our weekly newsletter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Subscribe to AI_Distilled, the go-to newsletter for AI professionals, researchers,
    and innovators, at [https://packt.link/Q5UyU](E_Chapter_5.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Newsletter_QRcode1.jpg)'
  prefs: []
  type: TYPE_IMG
