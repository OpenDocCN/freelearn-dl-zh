["```py\nfrom langchain_openai import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain_community.document_loaders import GitLoader\n# Load repository context\nrepo_loader = GitLoader( clone_url=\"https://github.com/example/repo.git\", branch=\"main\", file_filter=lambda file_path: file_path.endswith(\".py\") ) documents = repo_loader.load()\n# Create context-aware prompt\nsystem_template = \"\"\"You are an expert Python developer. Analyze the following repository files and implement the requested feature. Repository structure: {repo_context}\"\"\"\nhuman_template = \"\"\"Implement a function that: {feature_request}\"\"\"\nprompt = ChatPromptTemplate.from_messages([ (\"system\", system_template), (\"human\", human_template) ])\n# Create model with extended context window\nmodel = ChatOpenAI(model=\"gpt-4o\", temperature=0.2)\n```", "```py\nfrom langchain.prompts import PromptTemplate\nvalidation_template = \"\"\"Analyze the following Python code for:\n1\\. Potential security vulnerabilities\n2\\. Logic errors\n3\\. Performance issues\n4\\. Edge case handling\n\nCode to analyze:\n```", "```py\n\nThis validation approach creates a specialized LLM-based code review step in the workflow, focusing on critical security and quality aspects.\n\nMost successful implementations incorporate execution feedback, allowing models to iteratively improve their output based on compiler errors and runtime behavior. Research on Text-to-SQL systems by Boyan Li and colleagues (*The Dawn of Natural Language to SQL: Are We Fully Ready?*, 2024) demonstrates that incorporating feedback mechanisms significantly improves query generation accuracy, with systems that use execution results to refine their outputs and consistently outperform those without such capabilities.\n\nWhen deploying code-generating LLMs in production LangChain applications, several factors require attention:\n\n*   **Model selection tradeoffs**: While closed-source models such as GPT-4 and Claude demonstrate superior performance on code benchmarks, open-source alternatives such as Llama 3 (70.3% on HumanEval) offer advantages in cost, latency, and data privacy. The appropriate choice depends on specific requirements regarding accuracy, deployment constraints, and budget considerations.\n*   **Context window management**: Effective handling of limited context windows remains crucial. Recent techniques such as recursive chunking and hierarchical summarization (Li et al., 2024) can improve performance by up to 25% on large codebase tasks.\n*   **Framework integration** extends basic LLM capabilities by leveraging specialized tools such as LangChain for workflow management. Organizations implementing this pattern establish custom security policies tailored to their domain requirements and build feedback loops that enable continuous improvement of model outputs. This integration approach allows teams to benefit from advances in foundation models while maintaining control over deployment specifics.\n*   **Human-AI collaboration** establishes clear divisions of responsibility between developers and AI systems. This pattern maintains human oversight for all critical decisions while delegating routine tasks to AI assistants. An essential component is systematic documentation and knowledge capture, ensuring that AI-generated solutions remain comprehensible and maintainable by the entire development team. Companies successfully implementing this pattern report both productivity gains and improved knowledge transfer among team members.\n\n## Security and risk mitigation\n\nWhen building LLM-powered applications with LangChain, implementing robust security measures and risk mitigation strategies becomes essential. This section focuses on practical approaches to addressing security vulnerabilities, preventing hallucinations, and ensuring code quality through LangChain-specific implementations.\n\nSecurity vulnerabilities in LLM-generated code present significant risks, particularly when dealing with user inputs, database interactions, or API integrations. LangChain allows developers to create systematic validation processes to identify and mitigate these risks. The following validation chain can be integrated into any LangChain workflow that involves code generation, providing structured security analysis before deployment:\n\n```", "```py\n\n```", "```py\n\nThe Pydantic output parser ensures that results are properly structured and can be programmatically processed for automated gatekeeping. LLM-generated code should never be directly executed in production environments without validation. LangChain provides tools to create safe execution environments for testing generated code.\n\nTo ensure security when building LangChain applications that handle code, a layered approach is crucial, combining LLM-based validation with traditional security tools for robust defense. Structure security findings using Pydantic models and LangChain’s output parsers for consistent, actionable outputs. Always isolate the execution of LLM-generated code in sandboxed environments with strict resource limits, never running it directly in production. Explicitly manage dependencies by verifying imports against available packages to avoid hallucinations. Continuously improve code generation through feedback loops incorporating execution results and validation findings. Maintain comprehensive logging of all code generation steps, security findings, and modifications for auditing. Adhere to the principle of least privilege by generating code that follows security best practices such as minimal permissions and proper input validation. Finally, utilize version control to store generated code and implement human review for critical components.\n\n## Validation framework for LLM-generated code\n\nOrganizations should implement a structured validation process for LLM-generated code and analyses before moving to production. The following framework provides practical guidance for teams adopting LLMs in their data science workflows:\n\n*   **Functional validation** forms the foundation of any assessment process. Start by executing the generated code with representative test data and carefully verify that outputs align with expected results. Ensure all dependencies are properly imported and compatible with your production environment—LLMs occasionally reference outdated or incompatible libraries. Most importantly, confirm that the code actually addresses the original business requirements, as LLMs sometimes produce impressive-looking code that misses the core business objective.\n*   **Performance assessment** requires looking beyond mere functionality. Benchmark the execution time of LLM-generated code against existing solutions to identify potential inefficiencies. Testing with progressively larger datasets often reveals scaling limitations that weren’t apparent with sample data. Profile memory usage systematically, as LLMs may not optimize for resource constraints unless explicitly instructed. This performance data provides crucial information for deployment decisions and identifies opportunities for optimization.\n*   **Security screening** should never be an afterthought when working with generated code. Scan for unsafe functions, potential injection vulnerabilities, and insecure API calls—issues that LLMs may introduce despite their training in secure coding practices. Verify the proper handling of authentication credentials and sensitive data, especially when the model has been instructed to include API access. Check carefully for hardcoded secrets or unintentional data exposures that could create security vulnerabilities in production.\n*   **Robustness testing** extends validation beyond the happy path scenarios. Test with edge cases and unexpected inputs that reveal how the code handles extreme conditions. Verify that error handling mechanisms are comprehensive and provide meaningful feedback rather than cryptic failures. Evaluate the code’s resilience to malformed or missing data, as production environments rarely provide the pristine data conditions assumed in development.\n*   **Business logic verification** focuses on domain-specific requirements that LLMs may not fully understand. Confirm that industry-specific constraints and business rules are correctly implemented, especially regulatory requirements that vary by sector. Verify calculations and transformations against manual calculations for critical processes, as subtle mathematical differences can significantly impact business outcomes. Ensure all regulatory or policy requirements relevant to your industry are properly addressed—a crucial step when LLMs may lack domain-specific compliance knowledge.\n*   **Documentation and explainability** complete the validation process by ensuring sustainable use of the generated code. Either require the LLM to provide or separately generate inline comments that explain complex sections and algorithmic choices. Document any assumptions made by the model that might impact future maintenance or enhancement. Create validation reports that link code functionality directly to business requirements, providing traceability that supports both technical and business stakeholders.\n\nThis validation framework should be integrated into development workflows, with appropriate automation incorporated where possible to reduce manual effort. Organizations embarking on LLM adoption should start with well-defined use cases clearly aligned with business objectives, implement these validation processes systematically, invest in comprehensive staff training on both LLM capabilities and limitations, and establish clear governance frameworks that evolve with the technology.\n\n## LangChain integrations\n\nAs we’re aware, LangChain enables the creation of versatile and robust AI agents. For instance, a LangChain-integrated agent can safely execute code using dedicated interpreters, interact with SQL databases for dynamic data retrieval, and perform real-time financial analysis, all while upholding strict quality and security standards.\n\nIntegrations range from code execution and database querying to financial analysis and repository management. This wide-ranging toolkit facilitates building applications that are deeply integrated with real-world data and systems, ensuring that AI solutions are both powerful and practical. Here are some examples of integrations:\n\n*   **Code execution and isolation:** Tools such as the Python REPL, Azure Container Apps dynamic sessions, Riza Code Interpreter, and Bearly Code Interpreter provide various environments to safely execute code. They enable LLMs to delegate complex calculations or data processing tasks to dedicated code interpreters, thereby increasing accuracy and reliability while maintaining security.\n*   **Database and data handling:** Integrations for Cassandra, SQL, and Spark SQL toolkits allow agents to interface directly with different types of databases. Meanwhile, JSON Toolkit and pandas DataFrame integration facilitate efficient handling of structured data. These capabilities are essential for applications that require dynamic data retrieval, transformation, and analysis.\n*   **Financial data and analysis:** With FMP Data, Google Finance, and the FinancialDatasets Toolkit, developers can build AI agents capable of performing sophisticated financial analyses and market research. Dappier further extends this by connecting agents to curated, real-time data streams.\n*   **Repository and version control integration:** The GitHub and GitLab toolkits enable agents to interact with code repositories, streamlining tasks such as issue management, code reviews, and deployment processes—a crucial asset for developers working in modern DevOps environments.\n*   **User input and visualization:** Google Trends and PowerBI Toolkit highlight the ecosystem’s focus on bringing in external data (such as market trends) and then visualizing it effectively. The “human as a tool” integration is a reminder that, sometimes, human judgment remains indispensable, especially in ambiguous scenarios.\n\nHaving explored the theoretical framework and potential benefits of LLM-assisted software development, let’s now turn to practical implementation. In the following section, we’ll demonstrate how to generate functional software code with LLMs and execute it directly from within the LangChain framework. This hands-on approach will illustrate the concepts we’ve discussed and provide you with actionable examples you can adapt to your own projects.\n\n# Writing code with LLMs\n\nIn this section, we demonstrate code generation using various models integrated with LangChain. We’ve selected different models to showcase:\n\n*   LangChain’s diverse integrations with AI tools\n*   Models with different licensing and availability\n*   Options for local deployment, including smaller models\n\nThese examples illustrate LangChain’s flexibility in working with various code generation models, from cloud-based services to open-source alternatives. This approach allows you to understand the range of options available and choose the most suitable solution for your specific needs and constraints.\n\nPlease make sure you have installed all the dependencies needed for this book, as explained in [*Chapter 2*](E_Chapter_2.xhtml#_idTextAnchor044). Otherwise, you might run into issues.\n\nGiven the pace of the field and the development of the LangChain library, we are making an effort to keep the GitHub repository up to date. Please see [https://github.com/benman1/generative_ai_with_langchain](https://github.com/benman1/generative_ai_with_langchain).\n\nFor any questions or if you have any trouble running the code, please create an issue on GitHub or join the discussion on Discord: [https://packt.link/lang](https://packt.link/lang).\n\n## Google generative AI\n\nThe Google generative AI platform offers a range of models designed for instruction following, conversion, and code generation/assistance. These models also have different input/output limits and training data and are often updated. Let’s see if the Gemini Pro model can solve **FizzBuzz**, a common interview question for entry-level software developer positions.\n\nTo test the model’s code generation capabilities, we’ll use LangChain to interface with Gemini Pro and provide the FizzBuzz problem statement:\n\n```", "```py\n\nGemini Pro immediately returns a clean, correct Python solution that properly handles all the FizzBuzz requirements:\n\n```", "```pypython\n    answer = []\n\n for i in range(1, n+1):\n if i % 3 == 0 and i % 5 == 0:\n            answer.append(\"FizzBuzz\")\n elif i % 3 == 0:\n            answer.append(\"Fizz\")\n elif i % 5 == 0:\n            answer.append(\"Buzz\")\n else:\n            answer.append(str(i))\n\n return answer\n```", "```py\n\nThe model produced an efficient, well-structured solution that correctly implements the logic for the FizzBuzz problem without any errors or unnecessary complexity. Would you hire Gemini Pro for your team?\n\n## Hugging Face\n\nHugging Face hosts a lot of open-source models, many of which have been trained on code, some of which can be tried out in playgrounds, where you can ask them to either complete (for older models) or write code (instruction-tuned models). With LangChain, you can either download these models and run them locally, or you can access them through the Hugging Face API. Let’s try the local option first with a prime number calculation example:\n\n```", "```py\n\n```", "```py\n\nWhen executed, CodeGemma completes the function by implementing the Sieve of Eratosthenes algorithm, a classic method for finding prime numbers efficiently. The model correctly interprets the docstring, understanding that the function should return all prime numbers up to n rather than just checking whether a number is prime. The generated code demonstrates how specialized code models can produce working implementations from minimal specifications.\n\nPlease note that the downloading and loading of the models can take a few minutes.\n\nIf you’re getting an error saying you “`cannot access a gated repo`\" when trying to use a URL with LangChain, it means you’re attempting to access a private repository on Hugging Face that requires authentication with a personal access token to view or use the model; you need to create a Hugging Face access token and set it as an environment variable named `\"HF_TOKEN\"` to access the gated repository. You can get the token on the Hugging Face website at [https://huggingface.co/docs/api-inference/quicktour#get-your-api-token](https://huggingface.co/docs/api-inference/quicktour#get-your-api-token).\n\nWhen our code from the previous example executes successfully with CodeGemma, it generates a complete implementation for the prime number calculator function. The output looks like this:\n\n```", "```py\n\n```", "```py\n\nNotice how the model not only implemented the requested `calculate_primes()` function but also created a helper function, `is_prime()`, which uses a more efficient algorithm checking divisibility only up to the square root of the number. The model even added a complete `main()` function with user input handling, demonstrating its understanding of Python programming patterns.\n\nInstead of downloading and running models locally, which requires significant computational resources, we can also run models directly on Hugging Face’s infrastructure using their Inference API. This approach is simpler to set up and doesn’t require powerful hardware. Here’s how to implement the same example using Hugging Face’s hosted services:\n\n```", "```py\n\n```", "```py\n\nWhen executed, this code connects to Hugging Face’s servers to run the StarCoder model, a specialized code generation model trained on a vast corpus of source code. The expected output would be similar to our previous example—a complete implementation of the prime number calculator—but potentially with different algorithmic approaches since we’re using a different model. This hosted approach trades some flexibility and control for convenience and reduced local resource requirements, making it ideal for quick prototyping or when working on hardware with limited capabilities.\n\n## Anthropic\n\nAnthropic Claude series models have been particularly good in coding-related tasks. Let’s see how Claude does at a coding task:\n\n```", "```py\n\nWe get an algorithm and a very detailed explanation:\n\n```", "```pypython\ndef calculate_primes(n):\n    \"\"\"Create a list of consecutive integers from 2 up to N.\"\"\"\n    # Create a list of consecutive integers from 2 to N\n    numbers = list(range(2, n + 1))\n    # Initialize an empty list to store the prime numbers\n    primes = []\n    while numbers:\n        # Take the first number as prime\n        prime = numbers[0]\n        primes.append(prime)\n        # Remove all multiples of the prime number from the list\n        numbers = [num for num in numbers if num % prime != 0]\n    return primes\n```", "```py\n\n```", "```py\n\nLet’s test the function with the given example:\n\n```", "```pypython\nprint(calculate_primes(20))\n```", "```py\n[2, 3, 5, 7, 11, 13, 17, 19]\n```", "```py\n\nThe function correctly returns the list of prime numbers up to 20\\. This implementation follows the Sieve of Eratosthenes algorithm again.\n\n## Agentic approach\n\nWe can also create an LLM agent that can execute Python code to solve problems:\n\n```", "```py\n\nThe agent will:\n\n1.  Determine what it needs to write Python code.\n2.  Use `PythonREPLTool` to execute the code.\n3.  Return the results.\n\nWhen run, it will show its reasoning steps and code execution before giving the final answer. We should be seeing an output like this:\n\n```", "```py\n\n## Documentation RAG\n\nWhat is also quite interesting is the use of documents to help write code or to ask questions about documentation. Here’s an example of loading all documentation pages from LangChain’s website using `DocusaurusLoader`:\n\n```", "```py\n\n```", "```py\n\n`DocusaurusLoader` automatically scrapes and extracts content from LangChain’s documentation website. This loader is specifically designed to navigate Docusaurus-based sites and extract properly formatted content. Meanwhile, the `nest_asyncio.apply()` function is necessary for a Jupyter Notebook environment, which has limitations with asyncio’s event loop. This line allows us to run asynchronous code within the notebook’s cells, which is required for many web-scraping operations. After execution, the documents variable contains all the documentation pages, each represented as a `Document` object with properties like `page_content` and metadata. We can then set up embeddings with caching:\n\n```", "```py\n\nBefore we can feed our models into a vector store, we need to split them, as discussed in [*Chapter 4*](E_Chapter_4.xhtml#_idTextAnchor152):\n\n```", "```py\n\nNow we’ll create a vector store from the document splits:\n\n```", "```py\n\nWe’ll also need to initialize the LLM or chat model:\n\n```", "```py\n\nThen, we set up the RAG components:\n\n```", "```py\n\nFinally, we’ll build the RAG chain:\n\n```", "```py\n\nLet’s query the chain:\n\n```", "```py\n\nEach component builds on the previous one, creating a complete RAG system that can answer questions using the LangChain documentation.\n\n## Repository RAG\n\nOne powerful application of RAG systems is analyzing code repositories to enable natural language queries about codebases. This technique allows developers to quickly understand unfamiliar code or find relevant implementation examples. Let’s build a code-focused RAG system by indexing a GitHub repository.\n\nFirst, we’ll clone the repository and set up our environment:\n\n```", "```py\n\nAfter cloning the repository, we need to parse the Python files using LangChain’s specialized loaders that understand code structure. LanguageParser helps maintain code semantics during processing:\n\n```", "```py\n\nThis code performs three key operations: it clones our book’s GitHub repository, loads all Python files using language-aware parsing, and splits the code into smaller, semantically meaningful chunks. The language-specific splitter ensures we preserve function and class definitions when possible, making our retrieval more effective.\n\nNow we’ll create our RAG system by embedding these code chunks and setting up a retrieval chain:\n\n```", "```py\n\nHere, we’ve built our complete RAG pipeline: we store code embeddings in a Chroma vector database, configure a retriever to use maximal marginal relevance (which helps provide diverse results), and create a QA chain that combines retrieved code with our prompt template before sending it to the LLM.\n\nLet’s test our code-aware RAG system with a question about software development examples:\n\n```", "```py\n\n```", "```py\n\nThe response is somewhat limited, likely because our small chunk size (50 characters) may have fragmented code examples. While the system correctly identifies mentions of task planning and debugging, it doesn’t provide detailed code examples or context. In a production environment, you might want to increase the chunk size or implement hierarchical chunking to preserve more context. Additionally, using a code-specific embedding model could further improve the relevance of retrieved results.\n\nIn the next section, we’ll explore how generative AI agents can automate and enhance data science workflows. LangChain agents can write and execute code, analyze datasets, and even build and train ML models with minimal human guidance. We’ll demonstrate two powerful applications: training a neural network model and analyzing a structured dataset.\n\n# Applying LLM agents for data science\n\nThe integration of LLMs into data science workflows represents a significant, though nuanced, evolution in how analytical tasks are approached. While traditional data science methods remain essential for complex numerical analysis, LLMs offer complementary capabilities that primarily enhance accessibility and assist with specific aspects of the workflow.\n\nIndependent research reveals a more measured reality than some vendor claims suggest. According to multiple studies, LLMs demonstrate variable effectiveness across different data science tasks, with performance often declining as complexity increases. A study published in PLOS One found that “the executability of generated code decreased significantly as the complexity of the data analysis task increased,” highlighting the limitations of current models when handling sophisticated analytical challenges.\n\nLLMs exhibit a fundamental distinction in their data focus compared to traditional methods. While traditional statistical techniques excel at processing structured, tabular data through well-defined mathematical relationships, LLMs demonstrate superior capabilities with unstructured text. They can generate code for common data science tasks, particularly boilerplate operations involving data manipulation, visualization, and routine statistical analyses. Research on GitHub Copilot and similar tools indicates that these assistants can meaningfully accelerate development, though the productivity gains observed in independent studies (typically 7–22%) are more modest than some vendors claim. BlueOptima’s analysis of over 218,000 developers found productivity improvements closer to 4% rather than the 55% claimed in controlled experiments.\n\nText-to-SQL capabilities represent one of the most promising applications, potentially democratizing data access by allowing non-technical users to query databases in natural language. However, the performance often drops on the more realistic BIRD benchmark compared to Spider, and accuracy remains a key concern, with performance varying significantly based on the complexity of the query, the database schema, and the benchmark used.\n\nLLMs also excel at translating technical findings into accessible narratives for non-technical audiences, functioning as a communication bridge in data-driven organizations. While systems such as InsightLens demonstrate automated insight organization capabilities, the technology shows clear strengths and limitations when generating different types of content. The contrast is particularly stark with synthetic data: LLMs effectively create qualitative text samples but struggle with structured numerical datasets requiring complex statistical relationships. This performance boundary aligns with their core text processing capabilities and highlights where traditional statistical methods remain superior. A study published in JAMIA (*Evaluating Large Language Models for Health-Related Text Classification Tasks with Public Social Media Data*, 2024) found that “LLMs (specifically GPT-4, but not GPT-3.5) [were] effective for data augmentation in social media health text classification tasks but ineffective when used alone to annotate training data for supervised models.”\n\nThe evidence points toward a future where LLMs and traditional data analysis tools coexist and complement each other. The most effective implementations will likely be hybrid systems leveraging:\n\n*   LLMs for natural language interaction, code assistance, text processing, and initial exploration\n*   Traditional statistical and ML techniques for rigorous analysis of structured data and high-stakes prediction tasks\n\nThe transformation brought by LLMs enables both technical and non-technical stakeholders to interact with data effectively. Its primary value lies in reducing the cognitive load associated with repetitive coding tasks, allowing data scientists to maintain the flow and focus on higher-level analytical challenges. However, rigorous validation remains essential—independent studies consistently identify concerns regarding code quality, security, and maintainability. These considerations are especially critical in two key workflows that LangChain has revolutionized: training ML models and analyzing datasets.\n\nWhen training ML models, LLMs can now generate synthetic training data, assist in feature engineering, and automatically tune hyperparameters—dramatically reducing the expertise barrier for model development. Moreover, for data analysis, LLMs serve as intelligent interfaces that translate natural language questions into code, visualizations, and insights, allowing domain experts to extract value from data without deep programming knowledge. The following sections explore both of these areas with LangChain.\n\n## Training an ML model\n\nAs you know by now, LangChain agents can write and execute Python code for data science tasks, including building and training ML models. This capability is particularly valuable when you need to perform complex data analysis, create visualizations, or implement custom algorithms on the fly without switching contexts.\n\nIn this section, we’ll explore how to create and use Python-capable agents through two main steps: setting up the Python agent environment and configuring the agent with the right model and tools; and implementing a neural network from scratch, guiding the agent to create a complete working model.\n\n### Setting up a Python-capable agent\n\nLet’s start by creating a Python-capable agent using LangChain’s experimental tools:\n\n```", "```py\n\n```", "```py\n\nThis code creates a Python agent with the Claude 3 Opus model, which offers strong reasoning capabilities for complex programming tasks. `PythonREPLTool` provides the agent with a Python execution environment, allowing it to write and run code, see outputs, and iterate based on results. Setting `verbose=True` lets us observe the agent’s thought process, which is valuable for understanding its approach and debugging.\n\n**Security caution**\n\nPythonREPLTool executes arbitrary Python code with the same permissions as your application. While excellent for development and demonstrations, this presents significant security risks in production environments. For production deployments, consider:\n\n*   Using restricted execution environments such as RestrictedPython or Docker containers\n*   Implementing custom tools with explicit permission boundaries\n*   Running the agent in a separate isolated service with limited permissions\n*   Adding validation and sanitization steps before executing generated code\n\nThe `AgentExecutor`, on the other hand, is a LangChain component that orchestrates the execution loop for agents. It manages the agent’s decision-making process, handles interactions with tools, enforces iteration limits, and processes the agent’s final output. Think of it as the runtime environment where the agent operates.\n\n### Asking the agent to build a neural network\n\nNow that we’ve set up our Python agent, let’s test its capabilities with a practical ML task. We’ll challenge the agent to implement a simple neural network that learns a basic linear relationship. This example demonstrates how agents can handle end-to-end ML development tasks from data generation to model training and evaluation.\n\nThe following code instructs our agent to create a single-neuron neural network in PyTorch, train it on synthetic data representing the function `y=2x`, and make a prediction:\n\n```", "```py\n\nThis concise prompt instructs the agent to implement a full neural network pipeline: generating PyTorch code for a single-neuron model, creating synthetic training data that follows `y=2x`, training the model over 1,000 epochs with periodic progress reports, and, finally, making a prediction for a new input value of `x=5`.\n\n### Agent execution and results\n\nWhen we run this code, the agent begins reasoning through the problem and executing Python code. Here’s the abbreviated verbose output showing the agent’s thought process and execution:\n\n```", "```py\n\n```", "```py\n\nThe results demonstrate that our agent successfully built and trained a neural network. The prediction for x=5 is approximately 9.97, very close to the expected value of 10 (since 2×5=10). This accuracy confirms that the model effectively learned the underlying linear relationship from our synthetic data.\n\nIf your agent produces unsatisfactory results, consider increasing specificity in your prompt (e.g., specify learning rate or model architecture), requesting validation steps such as plotting the loss curve, lowering the LLM temperature for more deterministic results, or breaking complex tasks into sequential prompts.\n\nThis example showcases how LangChain agents can successfully implement ML workflows with minimal human intervention. The agent demonstrated strong capabilities in understanding the requested task, generating correct PyTorch code without reference examples, creating appropriate synthetic data, configuring and training the neural network, and evaluating results against expected outcomes.\n\nIn a real-world scenario, you could extend this approach to more complex ML tasks such as classification problems, time series forecasting, or even custom model architectures. Next, we’ll explore how agents can assist with data analysis and visualization tasks that build upon these fundamental ML capabilities.\n\n## Analyzing a dataset\n\nNext, we’ll demonstrate how LangChain agents can analyze structured datasets by examining the well-known `Iris` dataset. The `Iris` dataset, created by British statistician Ronald Fisher, contains measurements of sepal length, sepal width, petal length, and petal width for three species of iris flowers. It’s commonly used in machine learning for classification tasks.\n\n### Creating a pandas DataFrame agent\n\nData analysis is a perfect application for LLM agents. Let’s explore how to create an agent specialized in working with pandas DataFrames, enabling natural language interaction with tabular data.\n\nFirst, we’ll load the classic Iris dataset and save it as a CSV file for our agent to work with:\n\n```", "```py\n\nNow we’ll create a specialized agent for working with pandas DataFrames:\n\n```", "```py\n\n```", "```py\n\n**Security warning**\n\nWe’ve used `allow_dangerous_code=True`, which permits the agent to execute any Python code on your machine. This could potentially be harmful if the agent generates malicious code. Only use this option in development environments with trusted data sources, and never in production scenarios without proper sandboxing.\n\nThe example above works well with small datasets like Iris (150 rows), but real-world data analysis often involves much larger datasets that exceed LLM context windows. When implementing DataFrame agents in production environments, several strategies can help overcome these limitations.\n\nData summarization and preprocessing techniques form your first line of defense. Before sending data to your agent, consider extracting key statistical information such as shape, column names, data types, and summary statistics (mean, median, max, etc.). Including representative samples—perhaps the first and last few rows or a small random sample—provides context without overwhelming the LLM’s token limit. This preprocessing approach preserves critical information while dramatically reducing the input size.\n\nFor datasets that are too large for a single context window, chunking strategies offer an effective solution. You can process the data in manageable segments, run your agent on each chunk separately, and then aggregate the results. The aggregation logic would depend on the specific analysis task—for example, finding global maximums across chunk-level results for optimization queries or combining partial analyses for more complex tasks. This approach trades some global context for the ability to handle datasets of any size.\n\nQuery-specific preprocessing adapts your approach based on the nature of the question. Statistical queries can often be pre-aggregated before sending to the agent. For correlation questions, calculating and providing the correlation matrix upfront helps the LLM focus on interpretation rather than computation. For exploratory questions, providing dataset metadata and samples may be sufficient. This targeted preprocessing makes efficient use of context windows by including only relevant information for each specific query type.\n\n### Asking questions about the dataset\n\nNow that we’ve set up our data analysis agent, let’s explore its capabilities by asking progressively complex questions about our dataset. A well-designed agent should be able to handle different types of analytical tasks, from basic exploration to statistical analysis and visualization. The following examples demonstrate how our agent can work with the classic Iris dataset, which contains measurements of flower characteristics.\n\nWe’ll test our agent with three types of queries that represent common data analysis workflows: understanding the data structure, performing statistical calculations, and creating visualizations. These examples showcase the agent’s ability to reason through problems, execute appropriate code, and provide useful answers.\n\nFirst, let’s ask a fundamental exploratory question to understand what data we’re working with:\n\n```", "```py\n\nThe agent executes this request by examining the dataset structure:\n\n```", "```py\n\n```", "```py\n\nThis initial query demonstrates how the agent can perform basic data exploration by checking the structure and first few rows of the dataset. Notice how it correctly identifies that the data contains flower measurements, even without explicit species labels in the preview. Next, let’s challenge our agent with a more analytical question that requires computation:\n\n```", "```py\n\nThe agent tackles this by creating a new calculated column and finding its maximum value:\n\n```", "```py\n\n```", "```py\n\nThis example shows how our agent can perform more complex analysis by:\n\n*   Creating derived metrics (the difference between two columns)\n*   Finding the maximum value of this metric\n*   Identifying which row contains this value\n\nFinally, let’s see how our agent handles a request for data visualization:\n\n```"]