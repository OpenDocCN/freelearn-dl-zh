<html><head></head><body>
		<div><h1 id="_idParaDest-122" class="chapter-number"><a id="_idTextAnchor197"/>10</h1>
			<h1 id="_idParaDest-123"><a id="_idTextAnchor198"/>Overcoming 77-Token Limitations and Enabling Prompt Weighting</h1>
			<p>From <a href="B21263_05.xhtml#_idTextAnchor097"><em class="italic">Chapter 5</em></a>, we know that Stable Diffusion utilizes OpenAI’s CLIP model as its text encoder. The CLIP model’s tokenization implementation, as per the source code [6], has a context length of 77 tokens.</p>
			<p>This 77-token limit in the CLIP model extends to Hugging Face Diffusers, restricting the maximum input prompt to 77 tokens. Unfortunately, it’s not possible to assign keyword weights within these input prompts due to this constraint without some modifications.</p>
			<p>For instance, let’s say you give a prompt string that produces more than 77 tokens, like this:</p>
			<pre class="source-code">
from diffusers import StableDiffusionPipeline
import torch
pipe = StableDiffusionPipeline.from_pretrained(
    "stablediffusionapi/deliberate-v2",
    torch_dtype=torch.float16).to("cuda")
prompt = "a photo of a cat and a dog driving an aircraft "*20
image = pipe(prompt = prompt).images[0]
image</pre>
			<p>Diffusers will show up a warning message, like this:</p>
			<pre class="source-code">
The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens…</pre>
			<p>You can’t highlight the cat by providing the weight, such as in the following:</p>
			<pre class="source-code">
a photo (cat:1.5) and a dog driving an aircraft</pre>
			<p>By default, the <code>Diffusers</code> package does not include functionality for overcoming the 77-token limitation or assigning weights to individual tokens, as stated in its documentation. This is because Diffusers aims to serve as a versatile toolbox, providing essential features that can be utilized in various projects.</p>
			<p>Nonetheless, using the core functionalities offered by Diffusers, we can develop a custom prompt parser. This parser will help us circumvent the 77-token restriction and assign weights to each token. In this chapter, we will delve into the structure of text embeddings and outline a method to surpass the 77-token limit while also assigning weight values to each token.</p>
			<p>In this chapter, we will cover the following:</p>
			<ul>
				<li>Understanding the 77-token limitation</li>
				<li>Overcoming the 77-token limitation</li>
				<li>Enabling long prompts with weighting</li>
				<li>Overcoming the 77-token limitation using community pipelines</li>
			</ul>
			<p>If you want to start using a full feature pipeline supporting long prompt weighting, please turn to the <em class="italic">Overcoming the 77-token limitation using the community </em><em class="italic">pipelines</em> section.</p>
			<p>By the end of the chapter, you will be able to use weighted prompts without size limitations and know how to implement them using Python.</p>
			<h1 id="_idParaDest-124"><a id="_idTextAnchor199"/>Understanding the 77-token limitation</h1>
			<p>The Stable Diffusion (v1.5) text encoder uses the CLIP encoder from OpenAI [<em class="italic">2</em>]. The CLIP text encoder has a 77-token limit, and<a id="_idIndexMarker308"/> this limitation propagates to the downstream Stable Diffusion. We can reproduce the 77-token limitation with the following steps:</p>
			<ol>
				<li>We can take out the encoder from Stable Diffusion and verify it. Let’s say we have the prompt <code>a photo of a cat and dog driving an aircraft</code> and we multiply it by 20 to make the prompt’s token size larger than 77:<pre class="source-code">
prompt = "a photo of a cat and a dog driving an aircraft "*20</pre></li>
				<li>Reuse the pipeline we initialized at the beginning of the chapter and take out <code>tokenizer</code> and <code>text_encoder</code>:<pre class="source-code">
tokenizer = pipe.tokenizer</pre><pre class="source-code">
text_encoder = pipe.text_encoder</pre></li>
				<li>Use <code>tokenizer</code> to get the token IDs from the prompt:<pre class="source-code">
tokens = tokenizer(</pre><pre class="source-code">
    prompt,</pre><pre class="source-code">
    truncation = False,</pre><pre class="source-code">
    return_tensors = 'pt'</pre><pre class="source-code">
)["input_ids"]</pre><pre class="source-code">
print(len(tokens[0]))</pre></li>
				<li>Since we set <code>truncation = False</code>, <code>tokenizer</code> will convert any length string to token IDs. The preceding code will output a token list with a length of 181. <code>return_tensors = 'pt'</code> will tell the function to return the result in a <code>[1,181]</code> tensor object.<p class="list-inset">Try encoding the <a id="_idIndexMarker309"/>token IDs to <code>embeddings</code>:</p><pre class="source-code">
embeddings = pipe.text_encoder(tokens.to("cuda"))[0]</pre><p class="list-inset">We will see a <code>RuntimeError</code> message that says the following:</p><pre class="source-code">
RuntimeError: The size of tensor a (181) must match the size of tensor b (77) at non-singleton dimension 1</pre><p class="list-inset">From the preceding steps, we can see that CLIP’s text encoder only accepts 77 tokens at a time.</p></li>
				<li>Now, let’s take a look at the first and last tokens. If we take away <code>*20</code> and only tokenize the prompt <code>a photo cat and dog driving an aircraft</code>, when we print out the token IDs, we will see 10 token IDs instead of 8:<pre class="source-code">
tensor([49406,   320,  1125,  2368,   537,  1929,  4161,   550,  7706, 49407])</pre></li>
				<li>In the preceding token IDs, the first (<code>49406</code>) and last (<code>49407</code>) are automatically added. We can use <code>tokenizer._convert_id_to_token</code> to convert the token IDs to a string:<pre class="source-code">
print(tokenizer._convert_id_to_token(49406))</pre><pre class="source-code">
print(tokenizer._convert_id_to_token(49407))</pre><p class="list-inset">We can see<a id="_idIndexMarker310"/> that the two additional tokens are added to the prompt:</p><pre class="source-code">
&lt;|startoftext|&gt;</pre><pre class="source-code">
&lt;|endoftext|&gt;</pre></li>
			</ol>
			<p>Why do we need to check this? Because we need to remove the automatically added beginning and end tokens when concatenating tokens. Next, let’s proceed to the steps to overcome the 77-token limitatio<a id="_idTextAnchor200"/>n.</p>
			<h1 id="_idParaDest-125"><a id="_idTextAnchor201"/>Overcoming the 77-tokens limitation</h1>
			<p>Fortunately, the <a id="_idIndexMarker311"/>Stable Diffusion UNet doesn’t enforce this 77-token limitation. If we can get the embeddings in batches, concatenate those chunked embeddings into one tensor, and provide it to the UNet, we should be able to overcome the 77-token limitation. Here’s an overview of the process:</p>
			<ol>
				<li>Extract the text tokenizer and text encoder from the Stable Diffusion pipeline.</li>
				<li>Tokenize the input prompt, regardless of its size.</li>
				<li>Eliminate the added beginning and end tokens.</li>
				<li>Pop out the first 77 tokens and encode them into embeddings.</li>
				<li>Stack the embeddings into a tensor of size <code>[1, </code><code>x, 768]</code>.</li>
			</ol>
			<p>Now, let’s implement this idea using Python code:</p>
			<ol>
				<li>Take out the text tokenizer and text encoder:<pre class="source-code">
# step 1. take out the tokenizer and text encoder</pre><pre class="source-code">
tokenizer = pipe.tokenizer</pre><pre class="source-code">
text_encoder = pipe.text_encoder</pre><p class="list-inset">We can reuse the tokenizer and text encoder from the Stable Diffusion pipeline.</p></li>
				<li>Tokenize any<a id="_idIndexMarker312"/> size of input prompt:<pre class="source-code">
# step 2. encode whatever size prompt to tokens by setting </pre><pre class="source-code">
# truncation = False.</pre><pre class="source-code">
tokens = tokenizer(</pre><pre class="source-code">
    prompt,</pre><pre class="source-code">
    truncation = False</pre><pre class="source-code">
)["input_ids"]</pre><pre class="source-code">
print("token length:", len(tokens))</pre><pre class="source-code">
# step 2.2. encode whatever size neg_prompt, </pre><pre class="source-code">
# padding it to the size of prompt.</pre><pre class="source-code">
negative_ids = pipe.tokenizer(</pre><pre class="source-code">
    neg_prompt,</pre><pre class="source-code">
    truncation    = False,</pre><pre class="source-code">
    padding       = "max_length",</pre><pre class="source-code">
    max_length    = len(tokens)</pre><pre class="source-code">
).input_ids</pre><pre class="source-code">
print("neg_token length:", len(negative_ids))</pre><p class="list-inset">In the <a id="_idIndexMarker313"/>preceding code, we did the following:</p><ul><li>We set <code>truncation = False</code> to allow tokenization beyond the default 77-token limit. This ensures that the entire prompt is tokenized, regardless of its size.</li><li>The tokens are returned as a Python list instead of a torch tensor. Tokens in the Python list will make it easier for us to add additional elements. Note that the token list will be converted to a torch tensor before providing it to the text encoder.</li><li>There are two additional parameters, <code>padding = "max_length"</code> and <code>max_length  = len(tokens)</code>. We use these to make sure prompt tokens and negative prompt tokens are the same size.</li></ul></li>
				<li>Remove the beginning and end tokens.<p class="list-inset">The tokenizer will automatically add two additional tokens: the beginning token (<code>49406</code>) and the end token (<code>49407</code>).</p><p class="list-inset">In the subsequent step, we will segment the token sequence and feed the chunked tokens to the text encoder. Each chunk will have its own beginning and end tokens. But before that, we will need to exclude them initially from the long original token list:</p><pre class="source-code">
tokens = tokens[1:-1]</pre><pre class="source-code">
negative_ids = negative_ids[1:-1]</pre><p class="list-inset">And then add these beginning and end tokens back to the chunked tokens, each chunk with a size of size <code>75</code>. We will add the beginning and the end tokens back in <em class="italic">step 4</em>.</p></li>
				<li>Encode the 77-sized chunked tokens into embeddings:<pre class="source-code">
# step 4. Pop out the head 77 tokens, </pre><pre class="source-code">
# and encode the 77 tokens to embeddings.</pre><pre class="source-code">
embeds,neg_embeds = [],[]</pre><pre class="source-code">
chunk_size = 75</pre><pre class="source-code">
bos = pipe.tokenizer.bos_token_id</pre><pre class="source-code">
eos = pipe.tokenizer.eos_token_id</pre><pre class="source-code">
for i in range(0, len(tokens), chunk_size):</pre><pre class="source-code">
# Add the beginning and end token to the 75 chunked tokens to </pre><pre class="source-code">
# make a 77-token list</pre><pre class="source-code">
    sub_tokens = [bos] + tokens[i:i + chunk_size] + [eos]</pre><pre class="source-code">
# text_encoder support torch.Size([1,x]) input tensor</pre><pre class="source-code">
# that is why use [sub_tokens], </pre><pre class="source-code">
# instead of simply give sub_tokens.</pre><pre class="source-code">
    tensor_tokens = torch.tensor(</pre><pre class="source-code">
        [sub_tokens],</pre><pre class="source-code">
        dtype = torch.long,</pre><pre class="source-code">
        device = pipe.device</pre><pre class="source-code">
    )</pre><pre class="source-code">
    chunk_embeds = text_encoder(tensor_tokens)[0]</pre><pre class="source-code">
    embeds.append(chunk_embeds)</pre><pre class="source-code">
# Add the begin and end token to the 75 chunked neg tokens to </pre><pre class="source-code">
# make a 77 token list</pre><pre class="source-code">
    sub_neg_tokens = [bos] + negative_ids[i:i + chunk_size] + \</pre><pre class="source-code">
        [eos]</pre><pre class="source-code">
    tensor_neg_tokens = torch.tensor(</pre><pre class="source-code">
        [sub_neg_tokens],</pre><pre class="source-code">
        dtype = torch.long,</pre><pre class="source-code">
        device = pipe.device</pre><pre class="source-code">
    )</pre><pre class="source-code">
    neg_chunk_embeds= text_encoder(tensor_neg_tokens)[0]</pre><pre class="source-code">
    neg_embeds.append(neg_chunk_embeds)</pre><p class="list-inset">The <a id="_idIndexMarker314"/>preceding code loops through the token list, taking out 75 tokens at a time. Then, it adds the beginning and end tokens to the 75-token list to create a 77-token list. Why 77 tokens? Because the text encoder can encode 77 tokens to embeddings at one time.</p><p class="list-inset">Inside the <code>for</code> loop, the first part handles the prompt embeddings, and the second part handles the negative embeddings. Even though we give an empty negative prompt, to enable classification-free guidance diffusion, we still need a negative embedding list with the same size of positive prompt embeddings (inside of the denoising loop, the conditioned latent will subtract the unconditional latent, which is generated from the negative prompt).</p></li>
				<li>Stack the embeddings to a <code>[1,x,768]</code> size torch tensor.<p class="list-inset">Before this step, the <code>embeds</code> list holds data like this:</p><pre class="source-code">
[tensor1, tensor2...]</pre><p class="list-inset">The Stable Diffusion pipeline’s embedding parameters accept tensors in the size of <code>torch.Size([1,x,768])</code>.</p><p class="list-inset">We still need to convert the list to a three-dimension tensor using these two lines of code:</p><pre class="source-code">
# step 5. Stack the embeddings to a [1,x,768] size torch tensor.</pre><pre class="source-code">
prompt_embeds = torch.cat(embeds, dim = 1)</pre><pre class="source-code">
prompt_neg_embeds = torch.cat(neg_embeds, dim = 1)</pre><p class="list-inset">In the preceding code, we have the following:</p><ul><li><code>embeds</code> and <code>neg_embeds</code> are lists of PyTorch tensors. The <code>torch.cat()</code> function is used to concatenate these tensors along the dimension specified by <code>dim</code>. In this case, we have <code>dim=1</code>, which means the tensors are concatenated along their second dimension (since Python uses 0-based indexing).</li><li><code>prompt_embeds</code> is a tensor that contains all the embeddings from <code>embeds</code> concatenated together. Similarly, <code>prompt_neg_embeds</code> contains<a id="_idIndexMarker315"/> all the embeddings from <code>neg_embeds</code> concatenated together.</li></ul></li>
			</ol>
			<p>By now, we have a functioning text encoder that can convert whatever length of prompt to embeddings, which can be used by a Stable Diffusion pipeline. Next, let’s put all the code to<a id="_idTextAnchor202"/>gether.</p>
			<h2 id="_idParaDest-126"><a id="_idTextAnchor203"/>Putting all the code together into a function</h2>
			<p>Let’s go a step further to put all the previous code in a packed function:</p>
			<pre class="source-code">
def long_prompt_encoding(
    pipe:StableDiffusionPipeline,
    prompt,
    neg_prompt = ""
):
    bos = pipe.tokenizer.bos_token_id
    eos = pipe.tokenizer.eos_token_id
    chunk_size = 75
    # step 1. take out the tokenizer and text encoder
    tokenizer = pipe.tokenizer
    text_encoder = pipe.text_encoder
    # step 2.1. encode whatever size prompt to tokens by setting 
    # truncation = False.
    tokens = tokenizer(
        prompt.
        truncation = False,
        # return_tensors = 'pt'
    )["input_ids"]
    # step 2.2. encode whatever size neg_prompt, 
    # padding it to the size of prompt.
    negative_ids = pipe.tokenizer(
        neg_prompt,
        truncation = False,
        # return_tensors = "pt",
        Padding = "max_length",
        max_length = len(tokens)
    ).input_ids
    # Step 3. remove begin and end tokens
    tokens = tokens[1:-1]
    negative_ids = negative_ids[1:-1]
    # step 4. Pop out the head 77 tokens, 
    # and encode the 77 tokens to embeddings.
    embeds,neg_embeds = [],[]
    for i in range(0, len(tokens), chunk_size):
# Add the beginning and end tokens to the 75 chunked tokens to make a 
# 77-token list
        sub_tokens = [bos] + tokens[i:i + chunk_size] + [eos]
# text_encoder support torch.Size([1,x]) input tensor
# that is why use [sub_tokens], instead of simply give sub_tokens.
        tensor_tokens = torch.tensor(
            [sub_tokens],
            dtype = torch.long,
            device = pipe.device
        )
        chunk_embeds = text_encoder(tensor_tokens)[0]
        embeds.append(chunk_embeds)
# Add beginning and end token to the 75 chunked neg tokens to make a 
# 77-token list
        sub_neg_tokens = [bos] + negative_ids[i:i + chunk_size] + \
            [eos]
        tensor_neg_tokens = torch.tensor(
            [sub_neg_tokens],
            dtype = torch.long,
            device = pipe.device
        )
        neg_chunk_embeds = text_encoder(tensor_neg_tokens)[0]
        neg_embeds.append(neg_chunk_embeds)
# step 5. Stack the embeddings to a [1,x,768] size torch tensor.
    prompt_embeds = torch.cat(embeds, dim = 1)
    prompt_neg_embeds = torch.cat(neg_embeds, dim = 1)
    return prompt_embeds, prompt_neg_embeds</pre>
			<p>Let’s create a long prompt to test whether the preceding function works or not:</p>
			<pre class="source-code">
prompt = "photo, cute cat running on the grass" * 10 #&lt;- long prompt
prompt_embeds, prompt_neg_embeds = long_prompt_encoding(
    pipe, prompt, neg_prompt="low resolution, bad anatomy"
)
print(prompt_embeds.shape)
image = pipe(
    prompt_embeds = prompt_embeds,
    negative_prompt_embeds = prompt_neg_embeds,
    generator = torch.Generator("cuda").manual_seed(1)
).images[0]
image</pre>
			<p>The result is shown in <em class="italic">Figure 10</em><em class="italic">.1</em>:</p>
			<div><div><img src="img/B21263_10_01.jpg" alt="Figure 10.1: Cute cat running on the grass, using a long prompt"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.1: Cute cat running on the grass, using a long prompt</p>
			<p>If our new function works for long prompts, the generated image should reflect additional appended prompts. Let’s extend the prompt to the following:</p>
			<pre class="source-code">
prompt = "photo, cute cat running on the grass" * 10
prompt = prompt + ",pure white cat" * 10</pre>
			<p>The new prompt will generate an image as shown in <em class="italic">Figure 10</em><em class="italic">.2</em>:</p>
			<div><div><img src="img/B21263_10_02.jpg" alt="Figure 10.2: Cute cat running on the grass, with the additional prompt of pure white cat"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.2: Cute cat running on the grass, with the additional prompt of pure white cat</p>
			<p>As you can see, the new appended prompt works and there are more white elements added to the cat; however, it is still not pure white as requested in the prompt. We will solve this with prompt weighting, which we’ll cover in the upcoming sec<a id="_idTextAnchor204"/>tion.</p>
			<h1 id="_idParaDest-127"><a id="_idTextAnchor205"/>Enabling long prompts with weighting</h1>
			<p>We just<a id="_idIndexMarker316"/> built a whatever size of text encoder for a Stable <a id="_idIndexMarker317"/>Diffusion pipeline (v1.5-based). All of those steps are paving the way to build long prompts with a weighting text encoder.</p>
			<p>A weighted Stable Diffusion prompt refers to the practice of assigning different levels of importance to specific words or phrases within a text prompt used for generating images through the Stable Diffusion algorithm. By adjusting these weights, we can control the degree to which certain concepts influence the generated output, allowing for greater customization and refinement of the resulting images.</p>
			<p>The process typically involves scaling up or down the text embedding vectors associated with each concept in the prompt. For instance, if you want the Stable Diffusion model to emphasize a particular subject while deemphasizing another, you would increase the weight of the former and decrease the weight of the latter. Weighted prompts enable us to better direct the image generation toward desired outcomes.</p>
			<p>The core of adding weight to the prompt is simply vector multiplication:</p>
			<p>weighted _ embeddings = [embedding1, embedding2, ..., embedding768] × weight</p>
			<p>Before that, we still need to do some preparations to make a weighted prompt embedding, as follows:</p>
			<ol>
				<li><code>a (white) cat</code> into a list like this: <code>[['a ', 1.0], ['white', 1.1], ['cat', 1.0]]</code>. We’ll adopt the prevalent prompt format used in the Automatic1111 <strong class="bold">Stable Diffusion</strong> (<strong class="bold">SD</strong>) WebUI, as defined<a id="_idIndexMarker318"/> in the open source SD WebUI [4].</li>
				<li><strong class="bold">Token and weight extraction</strong>: Separate the token IDs and their corresponding weights into two distinct lists.</li>
				<li><strong class="bold">Prompt and negative prompt padding</strong>: Ensure that both the prompt and negative prompt tokens have the same maximum length. If the prompt is longer than the negative prompt, pad the negative prompt to match the prompt’s length. Otherwise, pad the prompt to align with the negative prompt’s length.</li>
			</ol>
			<p>Regarding<a id="_idIndexMarker319"/> attention and emphasis (weighting), we will<a id="_idIndexMarker320"/> implement the following weighting format [4]:</p>
			<pre class="source-code">
a (word) - increase attention to word by a factor of 1.1
a ((word)) - increase attention to word by a factor of 1.21 (= 1.1 * 1.1)
a [word] - decrease attention to word by a factor of 1.1
a (word:1.5) - increase attention to word by a factor of 1.5
a (word:0.25) - decrease attention to word by a factor of 4 (= 1 / 0.25)
a \(word\) - use literal () characters in prompt</pre>
			<p>Let’s go through each of those steps in more detail:</p>
			<ol>
				<li>Build the <code>parse_prompt_attention</code> function.<p class="list-inset">To make sure the prompt format is fully compatible with Automatic1111’s SD WebUI, we will extract and reuse the function from the open sourced <code>parse_prompt_attention</code> function [3]:</p><pre class="source-code">
def parse_prompt_attention(text):</pre><pre class="source-code">
    import re</pre><pre class="source-code">
    re_attention = re.compile(</pre><pre class="source-code">
        r"""</pre><pre class="source-code">
            \\\(|\\\)|\\\[|\\]|\\\\|\\|\(|\[|:([+-]?[.\d]+)\)|</pre><pre class="source-code">
            \)|]|[^\\()\[\]:]+|:</pre><pre class="source-code">
        """</pre><pre class="source-code">
        , re.X</pre><pre class="source-code">
    )</pre><pre class="source-code">
    re_break = re.compile(r"\s*\bBREAK\b\s*", re.S)</pre><pre class="source-code">
    res = []</pre><pre class="source-code">
    round_brackets = []</pre><pre class="source-code">
    square_brackets = []</pre><pre class="source-code">
    round_bracket_multiplier = 1.1</pre><pre class="source-code">
    square_bracket_multiplier = 1 / 1.1</pre><pre class="source-code">
    def multiply_range(start_position, multiplier):</pre><pre class="source-code">
        for p in range(start_position, len(res)):</pre><pre class="source-code">
            res[p][1] *= multiplier</pre><pre class="source-code">
    for m in re_attention.finditer(text):</pre><pre class="source-code">
        text = m.group(0)</pre><pre class="source-code">
        weight = m.group(1)</pre><pre class="source-code">
        if text.startswith('\\'):</pre><pre class="source-code">
            res.append([text[1:], 1.0])</pre><pre class="source-code">
        elif text == '(':</pre><pre class="source-code">
            round_brackets.append(len(res))</pre><pre class="source-code">
        elif text == '[':</pre><pre class="source-code">
            square_brackets.append(len(res))</pre><pre class="source-code">
        elif weight is not None and len(round_brackets) &gt; 0:</pre><pre class="source-code">
            multiply_range(round_brackets.pop(), float(weight))</pre><pre class="source-code">
        elif text == ')' and len(round_brackets) &gt; 0:</pre><pre class="source-code">
            multiply_range(round_brackets.pop(), \</pre><pre class="source-code">
                round_bracket_multiplier)</pre><pre class="source-code">
        elif text == ']' and len(square_brackets) &gt; 0:</pre><pre class="source-code">
            multiply_range(square_brackets.pop(), \</pre><pre class="source-code">
                square_bracket_multiplier)</pre><pre class="source-code">
        else:</pre><pre class="source-code">
            parts = re.split(re_break, text)</pre><pre class="source-code">
            for i, part in enumerate(parts):</pre><pre class="source-code">
                if i &gt; 0:</pre><pre class="source-code">
                    res.append(["BREAK", -1])</pre><pre class="source-code">
                res.append([part, 1.0])</pre><pre class="source-code">
    for pos in round_brackets:</pre><pre class="source-code">
        multiply_range(pos, round_bracket_multiplier)</pre><pre class="source-code">
    for pos in square_brackets:</pre><pre class="source-code">
        multiply_range(pos, square_bracket_multiplier)</pre><pre class="source-code">
    if len(res) == 0:</pre><pre class="source-code">
        res = [["", 1.0]]</pre><pre class="source-code">
    # merge runs of identical weights</pre><pre class="source-code">
    i = 0</pre><pre class="source-code">
    while i + 1 &lt; len(res):</pre><pre class="source-code">
        if res[i][1] == res[i + 1][1]:</pre><pre class="source-code">
            res[i][0] += res[i + 1][0]</pre><pre class="source-code">
            res.pop(i + 1)</pre><pre class="source-code">
        else:</pre><pre class="source-code">
            i += 1</pre><pre class="source-code">
    return res</pre><p class="list-inset">Call the <a id="_idIndexMarker321"/>previously created function <a id="_idIndexMarker322"/>using the following:</p><pre class="source-code">
parse_prompt_attention("a (white) cat")</pre><p class="list-inset">This will return the following:</p><pre class="source-code">
[['a ', 1.0], ['white', 1.1], [' cat', 1.0]]</pre></li>
				<li>Get prompts with weights.<p class="list-inset">With the help of the preceding function, we can get a list of prompt and weight pairs. The text encoder will only encode the tokens of the prompt (no, you don’t need to provide weights as input to the text encoder). We will need to further process the prompt-weight pair to two independent lists with the same size, one for token IDs and one for weights, like this:</p><pre class="source-code">
tokens: [1,2,3...]</pre><pre class="source-code">
weights: [1.0, 1.0, 1.0...]</pre><p class="list-inset">This work can be done by the following function:</p><pre class="source-code">
# step 2. get prompts with weights</pre><pre class="source-code">
# this function works for both prompt and negative prompt</pre><pre class="source-code">
def get_prompts_tokens_with_weights(</pre><pre class="source-code">
    pipe: StableDiffusionPipeline,</pre><pre class="source-code">
    prompt: str</pre><pre class="source-code">
):</pre><pre class="source-code">
    texts_and_weights = parse_prompt_attention(prompt)</pre><pre class="source-code">
    text_tokens,text_weights = [],[]</pre><pre class="source-code">
    for word, weight in texts_and_weights:</pre><pre class="source-code">
        # tokenize and discard the starting and the ending token</pre><pre class="source-code">
        token = pipe.tokenizer(</pre><pre class="source-code">
            word,</pre><pre class="source-code">
            # so that tokenize whatever length prompt</pre><pre class="source-code">
            truncation = False</pre><pre class="source-code">
        ).input_ids[1:-1]</pre><pre class="source-code">
        # the returned token is a 1d list: [320, 1125, 539, 320]</pre><pre class="source-code">
        # use merge the new tokens to the all tokens holder: </pre><pre class="source-code">
        # text_tokens</pre><pre class="source-code">
        text_tokens = [*text_tokens,*token]</pre><pre class="source-code">
        # each token chunk will come with one weight, like ['red </pre><pre class="source-code">
        # cat', 2.0]</pre><pre class="source-code">
        # need to expand the weight for each token.</pre><pre class="source-code">
        chunk_weights = [weight] * len(token)</pre><pre class="source-code">
        # append the weight back to the weight holder: text_</pre><pre class="source-code">
        # weights</pre><pre class="source-code">
        text_weights = [*text_weights, *chunk_weights]</pre><pre class="source-code">
    return text_tokens,text_weights</pre><p class="list-inset">The <a id="_idIndexMarker323"/>preceding function takes two <a id="_idIndexMarker324"/>parameters: the SD pipeline and the prompt string. The input string can be the positive prompt or the negative prompt.</p><p class="list-inset">Inside the function body, we first call the <code>parse_prompt_attention</code> function to have the prompts with weight associated in the smallest grain (The weight is applied in the individual token level). Then, we loop through the list, tokenize the text, and remove the tokenizer-added beginning and end token IDs with the indexing operation, <code>[1:-1]</code>.</p><p class="list-inset">Merge the new token IDs back to the list that holds all token IDs. In the meantime, expand the weights number and merge back to the list that holds all weights numbers.</p><p class="list-inset">Let’s reuse the prompt of <code>a (white) cat</code> and call the function:</p><pre class="source-code">
prompt = "a (white) cat"</pre><pre class="source-code">
tokens, weights = get_prompts_tokens_with_weights(pipe, prompt)</pre><pre class="source-code">
print(tokens,weights)</pre><p class="list-inset">The preceding code will return the following:</p><pre class="source-code">
[320, 1579, 2368] [1.0, 1.1, 1.0]</pre><p class="list-inset">Notice that the second token ID from <code>white</code> now has a weight of <code>1.1</code> instead of <code>1.0</code>.</p></li>
				<li>Pad the tokens.<p class="list-inset">In this step, we will further transform the token ID list and its weights into a chunked list.</p><p class="list-inset">Let’s say we have a list of token IDs containing more than 77 elements:</p><p class="list-inset"><code>[</code><code>1,2,3,...,100]</code></p><p class="list-inset">We need to transform it into a list that includes chunks, with 77 (maximum) tokens inside each chunk:</p><p class="list-inset"><code>[[</code><code>49406,1,2...75,49407],[49406,76,77,...,100,49407]]</code></p><p class="list-inset">This is so that, in the next step, we can loop through the outer layer of the list and encode the 77-token list one at a time.</p><p class="list-inset">Now, you<a id="_idIndexMarker325"/> may wonder why we need to<a id="_idIndexMarker326"/> provide a maximum of 77 tokens to the text encoder at a time. What if we simply loop through each element and encode one token at a time? Good question, but we can’t do it like this because encoding <code>white</code> and then encoding <code>cat</code> will produce different embeddings compared with encoding <code>white cat</code> together at one time.</p><p class="list-inset">We can use a quick test to find out the difference. First, let’s encode <code>white</code> only:</p><pre class="source-code">
# encode "white" only</pre><pre class="source-code">
white_token = 1579</pre><pre class="source-code">
white_token_tensor = torch.tensor(</pre><pre class="source-code">
    [[white_token]],</pre><pre class="source-code">
    dtype = torch.long,</pre><pre class="source-code">
    device = pipe.device</pre><pre class="source-code">
)</pre><pre class="source-code">
white_embed = pipe.text_encoder(white_token_tensor)[0]</pre><pre class="source-code">
print(white_embed[0][0])</pre><p class="list-inset">Then, encode <code>white </code><code>cat</code> together:</p><pre class="source-code">
# encode "white cat"</pre><pre class="source-code">
white_token, cat_token = 1579, 2369</pre><pre class="source-code">
white_cat_token_tensor = torch.tensor(</pre><pre class="source-code">
    [[white_token, cat_token]],</pre><pre class="source-code">
    dtype = torch.long,</pre><pre class="source-code">
    device = pipe.device</pre><pre class="source-code">
)</pre><pre class="source-code">
white_cat_embeds = pipe.text_encoder(white_cat_token_tensor)[0]</pre><pre class="source-code">
print(white_cat_embeds[0][0])</pre><p class="list-inset">Give the<a id="_idIndexMarker327"/> preceding code a try; you will find<a id="_idIndexMarker328"/> that the same <code>white</code> will lead to a different embedding. What is the root cause? The token and embedding is not a one-to-one mapping; the embedding is generated based on the self-attention mechanism [5]. A single <code>white</code> can represent the color or a family name, while <code>white</code> in <code>white cat</code> is clearly saying that it is a color that describes the cat.</p><p class="list-inset">Let’s get back to the padding work. The following code will check the length of the token list. If the token ID list length is larger than 75, then take the first 75 tokens and loop this operation the remaining tokens are fewer than 75, which will be handled by a separate logic:</p><pre class="source-code">
# step 3. padding tokens</pre><pre class="source-code">
def pad_tokens_and_weights(</pre><pre class="source-code">
    token_ids: list,</pre><pre class="source-code">
    weights: list</pre><pre class="source-code">
):</pre><pre class="source-code">
    bos,eos = 49406,49407</pre><pre class="source-code">
    # this will be a 2d list</pre><pre class="source-code">
    new_token_ids = []</pre><pre class="source-code">
    new_weights   = []</pre><pre class="source-code">
    while len(token_ids) &gt;= 75:</pre><pre class="source-code">
        # get the first 75 tokens</pre><pre class="source-code">
        head_75_tokens = [token_ids.pop(0) for _ in range(75)]</pre><pre class="source-code">
        head_75_weights = [weights.pop(0) for _ in range(75)]</pre><pre class="source-code">
        # extract token ids and weights</pre><pre class="source-code">
        temp_77_token_ids = [bos] + head_75_tokens + [eos]</pre><pre class="source-code">
        temp_77_weights   = [1.0] + head_75_weights + [1.0]</pre><pre class="source-code">
        # add 77 tokens and weights chunks to the holder list</pre><pre class="source-code">
        new_token_ids.append(temp_77_token_ids)</pre><pre class="source-code">
        new_weights.append(temp_77_weights)</pre><pre class="source-code">
    # padding the left</pre><pre class="source-code">
    if len(token_ids) &gt; 0:</pre><pre class="source-code">
        padding_len = 75 - len(token_ids)</pre><pre class="source-code">
        padding_len = 0</pre><pre class="source-code">
        temp_77_token_ids = [bos] + token_ids + [eos] * \</pre><pre class="source-code">
            padding_len + [eos]</pre><pre class="source-code">
        new_token_ids.append(temp_77_token_ids)</pre><pre class="source-code">
        temp_77_weights = [1.0] + weights   + [1.0] * \</pre><pre class="source-code">
            padding_len + [1.0]</pre><pre class="source-code">
        new_weights.append(temp_77_weights)</pre><pre class="source-code">
    # return</pre><pre class="source-code">
    return new_token_ids, new_weights</pre><p class="list-inset">Next, use <a id="_idIndexMarker329"/>the<a id="_idIndexMarker330"/> following function:</p><pre class="source-code">
t,w = pad_tokens_and_weights(tokens.copy(), weights.copy())</pre><pre class="source-code">
print(t)</pre><pre class="source-code">
print(w)</pre><p class="list-inset">The preceding function takes the following previously generated <code>tokens</code> and <code>weights</code> list:</p><pre class="source-code">
[320, 1579, 2368] [1.0, 1.1, 1.0]</pre><p class="list-inset">It transforms it into this:</p><pre class="source-code">
[[49406, 320, 1579, 2368, 49407]]</pre><pre class="source-code">
[[1.0, 1.0, 1.1, 1.0, 1.0]]</pre></li>
				<li>Get the weighted embeddings.<p class="list-inset">This is the final step, and we will get the Automatic1111-compatible embeddings without a token size limitation:</p><pre class="source-code">
def get_weighted_text_embeddings(</pre><pre class="source-code">
    pipe: StableDiffusionPipeline,</pre><pre class="source-code">
    prompt : str      = "",</pre><pre class="source-code">
    neg_prompt: str   = ""</pre><pre class="source-code">
):</pre><pre class="source-code">
    eos = pipe.tokenizer.eos_token_id</pre><pre class="source-code">
    prompt_tokens, prompt_weights = \ </pre><pre class="source-code">
        get_prompts_tokens_with_weights(</pre><pre class="source-code">
        pipe, prompt</pre><pre class="source-code">
    )</pre><pre class="source-code">
    neg_prompt_tokens, neg_prompt_weights = \</pre><pre class="source-code">
        get_prompts_tokens_with_weights(pipe, neg_prompt)</pre><pre class="source-code">
    # padding the shorter one</pre><pre class="source-code">
    prompt_token_len        = len(prompt_tokens)</pre><pre class="source-code">
    neg_prompt_token_len    = len(neg_prompt_tokens)</pre><pre class="source-code">
    if prompt_token_len &gt; neg_prompt_token_len:</pre><pre class="source-code">
        # padding the neg_prompt with eos token</pre><pre class="source-code">
        neg_prompt_tokens   = (</pre><pre class="source-code">
            neg_prompt_tokens  + \</pre><pre class="source-code">
            [eos] * abs(prompt_token_len - neg_prompt_token_len)</pre><pre class="source-code">
        )</pre><pre class="source-code">
        neg_prompt_weights  = (</pre><pre class="source-code">
            neg_prompt_weights +</pre><pre class="source-code">
            [1.0] * abs(prompt_token_len - neg_prompt_token_len)</pre><pre class="source-code">
        )</pre><pre class="source-code">
    else:</pre><pre class="source-code">
        # padding the prompt</pre><pre class="source-code">
        prompt_tokens       = (</pre><pre class="source-code">
            prompt_tokens \</pre><pre class="source-code">
            + [eos] * abs(prompt_token_len - \</pre><pre class="source-code">
            neg_prompt_token_len)</pre><pre class="source-code">
        )</pre><pre class="source-code">
        prompt_weights      = (</pre><pre class="source-code">
            prompt_weights \</pre><pre class="source-code">
            + [1.0] * abs(prompt_token_len - \</pre><pre class="source-code">
            neg_prompt_token_len)</pre><pre class="source-code">
        )</pre><pre class="source-code">
    embeds = []</pre><pre class="source-code">
    neg_embeds = []</pre><pre class="source-code">
    prompt_token_groups ,prompt_weight_groups = \</pre><pre class="source-code">
        pad_tokens_and_weights(</pre><pre class="source-code">
            prompt_tokens.copy(),</pre><pre class="source-code">
            prompt_weights.copy()</pre><pre class="source-code">
    )</pre><pre class="source-code">
    neg_prompt_token_groups, neg_prompt_weight_groups = \</pre><pre class="source-code">
        pad_tokens_and_weights(</pre><pre class="source-code">
            neg_prompt_tokens.copy(),</pre><pre class="source-code">
            neg_prompt_weights.copy()</pre><pre class="source-code">
        )</pre><pre class="source-code">
    # get prompt embeddings one by one is not working.</pre><pre class="source-code">
    for i in range(len(prompt_token_groups)):</pre><pre class="source-code">
        # get positive prompt embeddings with weights</pre><pre class="source-code">
        token_tensor = torch.tensor(</pre><pre class="source-code">
            [prompt_token_groups[i]],</pre><pre class="source-code">
            dtype = torch.long, device = pipe.device</pre><pre class="source-code">
        )</pre><pre class="source-code">
        weight_tensor = torch.tensor(</pre><pre class="source-code">
            prompt_weight_groups[i],</pre><pre class="source-code">
            dtype     = torch.float16,</pre><pre class="source-code">
            device    = pipe.device</pre><pre class="source-code">
        )</pre><pre class="source-code">
        token_embedding = \</pre><pre class="source-code">
            pipe.text_encoder(token_tensor)[0].squeeze(0)</pre><pre class="source-code">
        for j in range(len(weight_tensor)):</pre><pre class="source-code">
            token_embedding[j] = token_embedding[j] * </pre><pre class="source-code">
                weight_tensor[j]</pre><pre class="source-code">
        token_embedding = token_embedding.unsqueeze(0)</pre><pre class="source-code">
        embeds.append(token_embedding)</pre><pre class="source-code">
        # get negative prompt embeddings with weights</pre><pre class="source-code">
        neg_token_tensor = torch.tensor(</pre><pre class="source-code">
            [neg_prompt_token_groups[i]],</pre><pre class="source-code">
            dtype = torch.long, device = pipe.device</pre><pre class="source-code">
        )</pre><pre class="source-code">
        neg_weight_tensor = torch.tensor(</pre><pre class="source-code">
            neg_prompt_weight_groups[i],</pre><pre class="source-code">
            dtype     = torch.float16,</pre><pre class="source-code">
            device    = pipe.device</pre><pre class="source-code">
        )</pre><pre class="source-code">
        neg_token_embedding = \</pre><pre class="source-code">
            pipe.text_encoder(neg_token_tensor)[0].squeeze(0)</pre><pre class="source-code">
        for z in range(len(neg_weight_tensor)):</pre><pre class="source-code">
            neg_token_embedding[z] = (</pre><pre class="source-code">
                neg_token_embedding[z] * neg_weight_tensor[z]</pre><pre class="source-code">
            )</pre><pre class="source-code">
        neg_token_embedding = neg_token_embedding.unsqueeze(0)</pre><pre class="source-code">
        neg_embeds.append(neg_token_embedding)</pre><pre class="source-code">
    prompt_embeds       = torch.cat(embeds, dim = 1)</pre><pre class="source-code">
    neg_prompt_embeds   = torch.cat(neg_embeds, dim = 1)</pre><pre class="source-code">
    return prompt_embeds, neg_prompt_embeds</pre><p class="list-inset">The<a id="_idIndexMarker331"/> function looks a bit long but the logic is<a id="_idIndexMarker332"/> simple. Let me explain it section by section:</p><ul><li>In the <em class="italic">padding-the-shorter-one</em> section, the logic will fill the shorter prompt with the ending token (<code>eos</code>) so that both the prompt and negative prompt token lists share the same size (so that the generated latent can do a subtraction operation).</li><li>We call the <code>pad_tokens_and_weights</code> function to break all tokens and weights into chunks, each chunk with 77 elements.</li><li>We loop through the chunk list and encode the 77 tokens to embed in one step.</li><li>We use <code>token_embedding = pipe.text_encoder(token_tensor)[0].squeeze(0)</code> to remove empty dimensions, so that we can multiply each element with its weight. Note that, now, each token is represented by a 768-element vector.</li><li>Finally, we<a id="_idIndexMarker333"/> exit the loop and stack the <a id="_idIndexMarker334"/>tensor list to a higher dimension tensor using <code>prompt_embeds = tor<a id="_idTextAnchor206"/>ch.cat(embeds, dim = </code><code>1)</code>.</li></ul></li>
			</ol>
			<h1 id="_idParaDest-128"><a id="_idTextAnchor207"/>Verifying the work</h1>
			<p>After the not-so-many lines of code, we finally have all the logic ready, so let’s give the code a test.</p>
			<p>In the simple version of a <em class="italic">long prompt encoder</em>, we still get a cat with some patterns in the body instead of <code>pure white</code>, as we gave in the prompt. Now, let’s add weight to the <code>white</code> keyword to see whether anything happens:</p>
			<pre class="source-code">
prompt = "photo, cute cat running on the grass" * 10
prompt = prompt + ",pure (white:1.5) cat" * 10
neg_prompt = "low resolution, bad anatomy"
prompt_embeds, prompt_neg_embeds = get_weighted_text_embeddings(
    pipe, prompt = prompt, neg_prompt = neg_prompt
)
image = pipe(
    prompt_embeds = prompt_embeds,
    negative_prompt_embeds = prompt_neg_embeds,
    generator = torch.Generator("cuda").manual_seed(1)
).images[0]
image</pre>
			<p>Our new embedding function magically enabled us to generate a pure white cat, as we gave a <code>1.5</code> weight to the <code>white</code> keyword.</p>
			<div><div><img src="img/B21263_10_03.jpg" alt="Figure 10.3: Cute pure white cat running on the grass, with a 1.5 weight on the word white"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.3: Cute pure white cat running on the grass, with a 1.5 weight on the word white</p>
			<p>That is all! Now, we can reuse or extend this function to build any custom prompt parser as we want. But what if you don’t want to build your own function to implement; are there ways to start using unlimited weighted prompts? Yes, next we are going to introduce two pipelines contributed from the open source community and int<a id="_idTextAnchor208"/>egrated into Diffusers.</p>
			<h1 id="_idParaDest-129"><a id="_idTextAnchor209"/>Overcoming the 77-token limitation using community pipelines</h1>
			<p>Implementing a <a id="_idIndexMarker335"/>pipeline supporting <a id="_idIndexMarker336"/>long prompt weighting from scratch can be challenging. Often, we simply wish to utilize Diffusers to generate images using detailed and nuanced prompts. Fortunately, the open source community has provided implementations for SD v1.5 and SDXL. The SDXL implementation was originally initialized by Andrew Zhu, the author of this book, and massively improved by the community.</p>
			<p>I’ll now provide two examples of how to use the community pipeline for SD v1.5 and SDXL:</p>
			<ol>
				<li>This example uses the <code>lpw_stable_diffusion</code> pipeline for SD v1.5.<p class="list-inset">Use the following code to start a long prompt weighted pipeline:</p><pre class="source-code">
from diffusers import DiffusionPipeline</pre><pre class="source-code">
import torch</pre><pre class="source-code">
model_id_or_path = "stablediffusionapi/deliberate-v2"</pre><pre class="source-code">
pipe = DiffusionPipeline.from_pretrained(</pre><pre class="source-code">
    model_id_or_path,</pre><pre class="source-code">
    torch_dtype = torch.float16,</pre><pre class="source-code">
    custom_pipeline = "lpw_stable_diffusion"</pre><pre class="source-code">
).to("cuda:0")</pre><p class="list-inset">In the preceding code, <code>custom_pipeline = "lpw_stable_diffusion"</code> will actually download the <code>lpw_stable_diffusion</code> file from the Hugging Face server and will be invoked inside of the <code>DiffusionPipeline</code> pipeline.</p></li>
				<li>Let’s generate an image using the pipeline:<pre class="source-code">
prompt = "photo, cute cat running on the grass" * 10</pre><pre class="source-code">
prompt = prompt + ",pure (white:1.5) cat" * 10</pre><pre class="source-code">
neg_prompt = "low resolution, bad anatomy"</pre><pre class="source-code">
image = pipe(</pre><pre class="source-code">
    prompt = prompt,</pre><pre class="source-code">
    negative_prompt = neg_prompt,</pre><pre class="source-code">
    generator = torch.Generator("cuda").manual_seed(1)</pre><pre class="source-code">
).images[0]</pre><pre class="source-code">
image</pre><p class="list-inset">You will see an image the same as in <em class="italic">Figure 10</em><em class="italic">.3</em>.</p></li>
				<li>Now let’s<a id="_idIndexMarker337"/> see an example<a id="_idIndexMarker338"/> using the <code>lpw_stable_diffusion</code> pipeline for SDXL.<p class="list-inset">The usage is almost the same as the one we used in SD v1.5. The only differences are that we are loading an SDXL model and that we use another custom pipeline name: <code>lpw_stable_diffusion_xl</code>. See the following code:</p><pre class="source-code">
from diffusers import DiffusionPipeline</pre><pre class="source-code">
import torch</pre><pre class="source-code">
model_id_or_path = "stabilityai/stable-diffusion-xl-base-1.0"</pre><pre class="source-code">
pipe = DiffusionPipeline.from_pretrained(</pre><pre class="source-code">
    model_id_or_path,</pre><pre class="source-code">
    torch_dtype = torch.float16,</pre><pre class="source-code">
    custom_pipeline = "lpw_stable_diffusion_xl",</pre><pre class="source-code">
).to("cuda:0")</pre><p class="list-inset">The image <a id="_idIndexMarker339"/>generation <a id="_idIndexMarker340"/>code is exactly the same as the one we used for SD v1.5:</p><pre class="source-code">
prompt = "photo, cute cat running on the grass" * 10</pre><pre class="source-code">
prompt = prompt + ",pure (white:1.5) cat" * 10</pre><pre class="source-code">
neg_prompt = "low resolution, bad anatomy"</pre><pre class="source-code">
image = pipe(</pre><pre class="source-code">
    prompt = prompt,</pre><pre class="source-code">
    negative_prompt = neg_prompt,</pre><pre class="source-code">
    generator = torch.Generator("cuda").manual_seed(7)</pre><pre class="source-code">
).images[0]</pre><pre class="source-code">
image</pre><p class="list-inset">We will see an image generated as shown in <em class="italic">Figure 10</em><em class="italic">.4</em>:</p></li>
			</ol>
			<div><div><img src="img/B21263_10_04.jpg" alt="Figure 10.4: Cute pure white cat running on the grass, with a 1.5 weight on the word white, using lpw_stable_diffusion_xl"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.4: Cute pure white cat running on the grass, with a 1.5 weight on the word white, using lpw_stable_diffusion_xl</p>
			<p>From the<a id="_idIndexMarker341"/> image, we can clearly <a id="_idIndexMarker342"/>see what <code>pure (white:1.5) cat</code> is bringing into the image: proof that the pipeline can be used to generate image<a id="_idTextAnchor210"/>s using long weighted prompts.</p>
			<h1 id="_idParaDest-130"><a id="_idTextAnchor211"/>Summary</h1>
			<p>This chapter tried to solve one of the most discussed topics: overcoming the 77-token limitation and adding prompt weights for the Stable Diffusion pipeline using the <code>Diffusers</code> package. Automatic1111’s Stable Diffusion WebUI provides a versatile UI and is now (as I am writing this) the most prevailing prompt weighting and attention format. However, if we take a look at the code from Automatic1111, we will probably get lost soon; its code is long without clear documentation.</p>
			<p>This chapter started with understanding the root cause of the 77-token limitation and advanced to how the Stable Diffusion pipeline uses prompt embeddings. We implemented two functions to overcome the 77-token limitation.</p>
			<p>One simple function without weighting was implemented to show how to walk around the 77-token limitation. We also built another function with the full function of a long prompt usage without length limitations and also have prompt weighting implemented.</p>
			<p>By understanding and implementing these two functions, we can leverage the idea to not only use Diffuser to produce high-quality images the same as we can by using Automatic1111’s WebUI but we can also further extend it to add more powerful features. In terms of which feature to add, it is in your hands now. In the next chapter, we’ll start another exciting topic: using Stable Diffus<a id="_idTextAnchor212"/>ion to fix and upscale images.</p>
			<h1 id="_idParaDest-131"><a id="_idTextAnchor213"/>References</h1>
			<ol>
				<li>Hugging Face, weighted prompts: <a href="https://huggingface.co/docs/diffusers/main/en/using-diffusers/weighted_prompts">https://huggingface.co/docs/diffusers/main/en/using-diffusers/weighted_prompts</a></li>
				<li>OpenAI CLIP, Connecting text and images: <a href="https://openai.com/research/clip">https://openai.com/research/clip</a></li>
				<li>Automatic1111, Stable Diffusion WebUI prompt parser: <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/master/modules/prompt_parser.py#L345C19-L345C19&#13;">https://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/master/modules/prompt_parser.py#L345C19-L345C19</a></li>
				<li>Automatic1111, Attention/emphasis: <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features#attentionemphasis&#13;">https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features#attentionemphasis</a></li>
				<li>Ashish et al., <em class="italic">Attention Is All You </em><em class="italic">Need</em>: <a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a></li>
				<li>Source of the 77-token size limitation: <a href="https://github.com/openai/CLIP/blob/4d120f3ec35b30bd0f992f5d8af2d793aad98d2a/clip/clip.py#L206">https://github.com/openai/CLIP/blob/4d120f3ec35b30bd0f992f5d8af2d793aad98d2a/clip/clip.py#L206</a></li>
			</ol>
		</div>
	</body></html>