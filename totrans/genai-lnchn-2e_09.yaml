- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Evaluation and Testing
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估和测试
- en: As we’ve discussed so far in this book, LLM agents and systems have diverse
    applications across industries. However, taking these complex neural network systems
    from research to real-world deployment comes with significant challenges and necessitates
    robust evaluation strategies and testing methodologies.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本书中迄今为止所讨论的，LLM代理和系统在各个行业都有广泛的应用。然而，将这些复杂的神经网络系统从研究转移到实际部署面临着重大的挑战，并需要强大的评估策略和测试方法。
- en: Evaluating LLM agents and apps in LangChain comes with new methods and metrics
    that can help ensure optimized, reliable, and ethically sound outcomes. This chapter
    delves into the intricacies of evaluating LLM agents, covering system-level evaluation,
    evaluation-driven design, offline and online evaluation methods, and practical
    examples with Python code.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在LangChain中评估LLM代理和应用程序带来了新的方法和指标，这些可以帮助确保优化、可靠和道德上合理的成果。本章深入探讨了评估LLM代理的复杂性，包括系统级评估、评估驱动设计、离线和在线评估方法以及使用Python代码的实践示例。
- en: 'By the end of this chapter, you will have a comprehensive understanding of
    how to evaluate LLM agents and ensure their alignment with intended goals and
    governance requirements. In all, this chapter will cover:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您将全面了解如何评估LLM代理并确保其与预期目标和治理要求保持一致。总的来说，本章将涵盖：
- en: Why evaluations matter
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估的重要性
- en: 'What we evaluate: core agent capabilities'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们评估的内容：核心代理能力
- en: 'How we evaluate: methodologies and approaches'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何评估：方法和途径
- en: Evaluating LLM agents in practice
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实践中评估LLM代理
- en: Offline evaluation
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 离线评估
- en: You can find the code for this chapter in the `chapter8/` directory of the book’s
    GitHub repository. Given the rapid developments in the field and the updates to
    the LangChain library, we are committed to keeping the GitHub repository current.
    Please visit [https://github.com/benman1/generative_ai_with_langchain](https://github.com/benman1/generative_ai_with_langchain)
    for the latest updates.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在本书GitHub仓库的`chapter8/`目录中找到本章的代码。鉴于该领域的快速发展以及LangChain库的更新，我们致力于保持GitHub仓库的更新。请访问[https://github.com/benman1/generative_ai_with_langchain](https://github.com/benman1/generative_ai_with_langchain)以获取最新更新。
- en: See [*Chapter 2*](E_Chapter_2.xhtml#_idTextAnchor044) for setup instructions.
    If you have any questions or encounter issues while running the code, please create
    an issue on GitHub or join the discussion on Discord at [https://packt.link/lang](https://packt.link/lang).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅[*第2章*](E_Chapter_2.xhtml#_idTextAnchor044)以获取设置说明。如果您在运行代码时遇到任何问题或有任何疑问，请在GitHub上创建问题或在Discord上加入讨论，详情请见[https://packt.link/lang](https://packt.link/lang)。
- en: In the realm of developing LLM agents, evaluations play a pivotal role in ensuring
    these complex systems function reliably and effectively across real-world applications.
    Let’s start discussing why rigorous evaluation is indispensable!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发LLM代理的领域，评估在确保这些复杂的系统在实际应用中可靠和有效方面发挥着关键作用。让我们开始讨论为什么严格的评估是不可或缺的！
- en: Why evaluation matters
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估的重要性
- en: LLM agents represent a new class of AI systems that combine language models
    with reasoning, decision-making, and tool-using capabilities. Unlike traditional
    software with predictable behaviors, these agents operate with greater autonomy
    and complexity, making thorough evaluation essential before deployment.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: LLM代理代表了一类新的AI系统，这些系统结合了语言模型、推理、决策和工具使用能力。与传统具有可预测行为的软件不同，这些代理具有更高的自主性和复杂性，因此在部署前进行彻底的评估至关重要。
- en: 'Consider the real-world consequences: unlike traditional software with deterministic
    behavior, LLM agents make complex, context-dependent decisions. If unevaluated
    before being implemented, an AI agent in customer support might provide misleading
    information that damages brand reputation, while a healthcare assistant could
    influence critical treatment decisions—highlighting why thorough evaluation is
    essential.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑现实世界的后果：与传统具有确定性行为的软件不同，LLM代理会做出复杂、依赖上下文的决策。如果在实施前未经评估，客户支持中的AI代理可能会提供误导性信息，损害品牌声誉，而医疗助手可能会影响关键的治疗决策——这突显了为什么彻底的评估是必不可少的。
- en: 'Before diving into specific evaluation techniques, it’s important to distinguish
    between two fundamentally different types of evaluation:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究具体的评估技术之前，区分两种根本不同的评估类型非常重要：
- en: '**LLM model evaluation:**'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**LLM模型评估：**'
- en: Focuses on the raw capabilities of the base language model
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 专注于基础语言模型的原始能力
- en: Uses controlled prompts and standardized benchmarks
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用受控提示和标准化基准
- en: Evaluates inherent abilities like reasoning, knowledge recall, and language
    generation
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估内在能力，如推理、知识回忆和语言生成
- en: Typically conducted by model developers or researchers comparing different models
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常由模型开发者或研究人员进行，比较不同的模型
- en: '**LLM system/application evaluation:**'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**LLM系统/应用评估：**'
- en: Assesses the complete application that includes the LLM plus additional components
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估包括LLM以及附加组件在内的完整应用程序
- en: Examines real-world performance with actual user queries and scenarios
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过实际用户查询和场景检验现实世界的性能
- en: Evaluates how components work together (retrieval, tools, memory, etc.)
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估组件如何协同工作（检索、工具、记忆等）
- en: Measures end-to-end effectiveness at solving user problems
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 衡量解决用户问题的端到端有效性
- en: While both types of evaluation are important, this chapter focuses on system-level
    evaluation, as practitioners building LLM agents with LangChain are concerned
    with overall application performance rather than comparing base models. A weaker
    base model with excellent prompt engineering and system design might outperform
    a stronger model with poor integration in real-world applications.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这两种类型的评估都很重要，但本章重点介绍系统级评估，因为使用LangChain构建LLM代理的实践者更关注整体应用性能，而不是比较基础模型。一个基础模型较弱，但具有出色的提示工程和系统设计，可能在现实应用中优于一个集成较差但能力更强的模型。
- en: Safety and alignment
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安全与契合度
- en: 'Alignment in the context of LLMs has a dual meaning: as a process, referring
    to the post-training techniques used to ensure that models behave according to
    human expectations and values; and as an outcome, measuring the degree to which
    a model’s behavior conforms to intended human values and safety guidelines. Unlike
    task-related performance which focuses on accuracy and completeness, alignment
    addresses the fundamental calibration of the system to human behavioral standards.
    While fine-tuning improves a model’s performance on specific tasks, alignment
    specifically targets ethical behavior, safety, and reduction of harmful outputs.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLM的背景下，契合度有两个含义：作为一个过程，指的是用于确保模型行为符合人类期望和价值观的培训后技术；作为一个结果，衡量模型行为符合预期的人类价值观和安全指南的程度。与关注准确性和完整性的任务相关性能不同，契合度解决的是系统对人类行为标准的根本校准。虽然微调可以提高模型在特定任务上的性能，但契合度专门针对道德行为、安全性和有害输出的减少。
- en: 'This distinction is crucial because a model can be highly capable (well fine-tuned)
    but poorly aligned, creating sophisticated outputs that violate ethical norms
    or safety guidelines. Conversely, a model can be well-aligned but lack task-specific
    capabilities in certain domains. Alignment with human values is fundamental to
    responsible AI deployment. Evaluation must verify that agents align with human
    expectations across multiple dimensions: factual accuracy in sensitive domains,
    ethical boundary recognition, safety in responses, and value consistency.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这种区分至关重要，因为一个模型可能能力很强（经过良好微调），但与人类价值观的契合度差，可能会产生违反伦理规范或安全指南的复杂输出。相反，一个模型可能与人类价值观契合良好，但在某些领域的特定任务能力上可能不足。与人类价值观的契合度对于负责任的AI部署是基本的。评估必须验证代理在多个维度上与人类期望的一致性：敏感领域的客观准确性、伦理边界识别、响应的安全性以及价值一致性。
- en: Alignment evaluation methods must be tailored to domain-specific concerns. In
    financial services, alignment evaluation focuses on regulatory compliance with
    frameworks like GDPR and the EU AI Act, particularly regarding automated decision-making.
    Financial institutions must evaluate bias in fraud detection systems, implement
    appropriate human oversight mechanisms, and document these processes to satisfy
    regulatory requirements. In retail environments, alignment evaluation centers
    on ethical personalization practices, balancing recommendation relevance with
    customer privacy concerns and ensuring transparent data usage policies when generating
    personalized content.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 契合度评估方法必须针对特定领域的关注点进行定制。在金融服务领域，契合度评估侧重于符合GDPR和欧盟AI法案等框架的监管合规性，特别是关于自动化决策。金融机构必须评估欺诈检测系统中的偏差，实施适当的人类监督机制，并记录这些流程以满足监管要求。在零售环境中，契合度评估集中在道德个性化实践上，平衡推荐的相关性与客户隐私关注，并在生成个性化内容时确保透明的数据使用政策。
- en: Manufacturing contexts require alignment evaluation focused on safety parameters
    and operational boundaries. AI systems must recognize potentially dangerous operations,
    maintain appropriate human intervention protocols for quality control, and adhere
    to industry safety standards. Alignment evaluation includes testing whether predictive
    maintenance systems appropriately escalate critical safety issues to human technicians
    rather than autonomously deciding maintenance schedules for safety-critical equipment.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 制造环境需要关注安全参数和操作边界的对齐评估。人工智能系统必须识别潜在的危险操作，维护适当的人类干预协议以进行质量控制，并遵守行业标准。对齐评估包括测试预测性维护系统是否适当地将关键安全问题升级给人类技术人员，而不是自主决定关键设备的安全维护计划。
- en: In educational settings, alignment evaluation must consider developmental appropriateness
    across student age groups, fair assessment standards across diverse student populations,
    and appropriate transparency levels. Educational AI systems require evaluation
    of their ability to provide balanced perspectives on complex topics, avoid reinforcing
    stereotypes in learning examples, and appropriately defer to human educators on
    sensitive or nuanced issues. These domain-specific alignment evaluations are essential
    for ensuring AI systems not only perform well technically but also operate within
    appropriate ethical and safety boundaries for their application context.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在教育环境中，对齐评估必须考虑学生年龄组的发育适宜性，不同学生群体中的公平评估标准，以及适当的透明度水平。教育人工智能系统需要评估其提供复杂话题平衡视角的能力，避免在学习示例中强化刻板印象，并在敏感或微妙问题上适当尊重人类教育者的意见。这些特定领域的对齐评估对于确保人工智能系统不仅技术上表现良好，而且在其应用环境中符合适当的伦理和安全边界至关重要。
- en: Performance and efficiency
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 性能和效率
- en: 'Like early challenges in software testing that were resolved through standardized
    practices, agent evaluations face similar hurdles. These include:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于早期软件测试中的挑战，通过标准化实践得到解决，智能体评估也面临着类似的障碍。这些包括：
- en: '**Overfitting**: Where systems perform well only on test data but not in real-world
    situations'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过拟合**：系统仅在测试数据上表现良好，但在实际场景中表现不佳'
- en: '**Gaming benchmarks**: Optimizing for specific test scenarios rather than general
    performance'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**游戏基准**：针对特定测试场景进行优化，而不是通用性能'
- en: '**Insufficient diversity in evaluation datasets**: Failing to test performance
    across the breadth of real-world situations the system will encounter, including
    edge cases and unexpected inputs'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评估数据集中缺乏多样性**：未能测试系统将在实际场景中遇到的各种情况下的性能，包括边缘情况和意外输入'
- en: Drawing lessons from software testing and other domains, comprehensive evaluation
    frameworks need to measure not only the accuracy but also the scalability, resource
    utilization, and safety of LLM agents.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 从软件测试和其他领域汲取经验教训，全面的评估框架需要衡量不仅准确性，还包括可扩展性、资源利用率和LLM智能体的安全性。
- en: '*Performance evaluation* determines whether agents can reliably achieve their
    intended goals, including:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*性能评估*决定智能体是否能可靠地实现其预期目标，包括：'
- en: '**Accuracy** in task completion across varied scenarios'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在各种场景中完成任务**的准确性**
- en: '**Robustness** when handling novel inputs that differ from evaluation examples'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理与评估示例不同的新颖输入时的**鲁棒性**
- en: '**Resistance** to adversarial inputs or manipulation'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对对抗性输入或操纵的**抵抗'
- en: '**Resource efficiency** in computational and operational costs'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**资源效率**在计算和运营成本方面'
- en: Rigorous evaluations identify potential failure modes and risks in diverse real-world
    scenarios, as evidenced by modern benchmarks and contests. Ensuring an agent can
    operate safely and reliably across variations in real-world conditions is paramount.
    Evaluation strategies and methodologies continue to evolve, enhancing agent design
    effectiveness through iterative improvement.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 严格的评估可以识别出各种实际场景中可能出现的故障模式和风险，正如现代基准和竞赛所证明的那样。确保智能体能够在实际条件的变化中安全可靠地运行至关重要。评估策略和方法持续发展，通过迭代改进提高智能体设计的效果。
- en: Effective evaluations prevent the adoption of unnecessarily complex and costly
    solutions by balancing accuracy with resource efficiency. For example, the DSPy
    framework optimizes both cost and task performance, highlighting how evaluations
    can guide resource-effective solutions. LLM agents benefit from similar optimization
    strategies, ensuring their computational demands align with their benefits.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 有效的评估通过平衡准确性与资源效率，防止采用不必要的复杂和昂贵解决方案。例如，DSPy框架优化了成本和任务性能，突出了评估如何引导资源有效解决方案。LLM代理也受益于类似的优化策略，确保其计算需求与其收益相匹配。
- en: User and stakeholder value
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用户和利益相关者价值
- en: Evaluations help quantify the actual impact of LLM agents in practical settings.
    During the COVID-19 pandemic, the WHO’s implementation of screening chatbots demonstrated
    how AI could achieve meaningful practical outcomes, evaluated through metrics
    like user adherence and information quality. In financial services, JPMorgan Chase’s
    COIN (Contract Intelligence) platform for reviewing legal documents showcased
    value by reducing 360,000 hours of manual review work annually, with evaluations
    focusing on accuracy rates and cost savings compared to traditional methods. Similarly,
    Sephora’s Beauty Bot demonstrated retail value through increased conversion rates
    (6% higher than traditional channels) and higher average order values, proving
    stakeholder value across multiple dimensions.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 评估有助于量化LLM代理在实际环境中的实际影响。在COVID-19大流行期间，世界卫生组织实施筛查聊天机器人的举措展示了人工智能如何通过用户依从性和信息质量等指标实现有意义的实际成果。在金融服务领域，摩根大通（JPMorgan
    Chase）的COIN（合同智能）平台通过每年减少36万小时的手动审查工作展示了价值，评估重点在于与传统方法相比的准确率和成本节约。同样，丝芙兰（Sephora）的美容机器人通过提高转化率（比传统渠道高出6%）和更高的平均订单价值，证明了在多个维度上的利益相关者价值。
- en: User experience is a cornerstone of successful AI deployment. Systems like Alexa
    and Siri undergo rigorous evaluations for ease of use and engagement, which inform
    design improvements. Similarly, assessing user interaction with LLM agents helps
    refine interfaces and ensures the agents meet or exceed user expectations, thereby
    improving overall satisfaction and adoption rates.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 用户体验是成功部署人工智能的基石。像Alexa和Siri这样的系统会经历严格的易用性和参与度评估，这些评估有助于设计改进。同样，评估用户与LLM代理的交互有助于优化界面并确保代理满足或超越用户期望，从而提高整体满意度和采用率。
- en: A critical aspect of modern AI systems includes understanding how human interventions
    affect outcomes. In healthcare settings, evaluations show how human feedback enhances
    the performance of chatbots in therapeutic contexts. In manufacturing, a predictive
    maintenance LLM agent deployed at a major automotive manufacturer demonstrated
    value through reduced downtime (22% improvement), extended equipment lifespan,
    and positive feedback from maintenance technicians about the system’s interpretability
    and usefulness. For LLM agents, incorporating human oversight in evaluations reveals
    insights into decision-making processes and highlights both strengths and areas
    needing improvement.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现代人工智能系统的一个关键方面是理解人类干预如何影响结果。在医疗保健环境中，评估显示了人类反馈如何增强聊天机器人在治疗环境中的性能。在制造业中，一家主要汽车制造商部署的预测性维护LLM代理通过减少停机时间（提高了22%）、延长设备使用寿命以及维护技术人员对系统可解释性和有用性的积极反馈，展示了价值。对于LLM代理，评估中融入人类监督揭示了决策过程方面的见解，并突出了优势和需要改进的领域。
- en: Comprehensive agent evaluation requires addressing the distinct perspectives
    and priorities of multiple stakeholders across the agent lifecycle. The evaluation
    methods deployed should reflect this diversity, with metrics tailored to each
    group’s primary concerns.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的代理评估需要解决多个利益相关者在代理生命周期中的不同观点和优先事项。部署的评估方法应反映这种多样性，并针对每个群体的主要关注点定制指标。
- en: End users evaluate agents primarily through the lens of practical task completion
    and interaction quality. Their assessment revolves around the agent’s ability
    to understand and fulfill requests accurately (task success rate), respond with
    relevant information (answer relevancy), maintain conversation coherence, and
    operate with reasonable speed (response time). This group values satisfaction
    metrics most highly, with user satisfaction scores and communication efficiency
    being particularly important in conversational contexts. In application-specific
    domains like web navigation or software engineering, users may prioritize domain-specific
    success metrics—such as whether an e-commerce agent successfully completed a purchase
    or a coding agent resolved a software issue correctly.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 最终用户主要通过实际任务完成和交互质量来评估智能代理。他们的评估围绕代理理解并准确满足请求的能力（任务成功率）、提供相关信息（答案相关性）、保持对话连贯性以及以合理的速度操作（响应时间）。这一群体最重视满意度指标，在对话环境中，用户满意度评分和沟通效率尤为重要。在特定应用领域，如网络导航或软件工程中，用户可能会优先考虑特定领域的成功指标——例如，电子商务代理是否成功完成购买或编码代理是否正确解决软件问题。
- en: Technical stakeholders require a deeper evaluation of the agent’s internal processes
    rather than just outcomes. They focus on the quality of planning (plan feasibility,
    plan optimality), reasoning coherence, tool selection accuracy, and adherence
    to technical constraints. For SWE agents, metrics like code correctness and test
    case passing rate are critical. Technical teams also closely monitor computational
    efficiency metrics such as token consumption, latency, and resource utilization,
    as these directly impact operating costs and scalability. Their evaluation extends
    to the agent’s robustness—measuring how it handles edge cases, recovers from errors,
    and performs under varying loads.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 技术利益相关者需要更深入地评估代理的内部流程，而不仅仅是结果。他们关注规划的质量（计划可行性、计划最优性）、推理的连贯性、工具选择的准确性以及遵守技术约束。对于SWE代理，代码正确性和测试用例通过率等指标至关重要。技术团队还密切监控计算效率指标，如令牌消耗、延迟和资源利用率，因为这些直接影响到运营成本和可扩展性。他们的评估还扩展到代理的鲁棒性——衡量它如何处理边缘情况、从错误中恢复以及在不同负载下的表现。
- en: 'Business stakeholders evaluate agents through metrics connecting directly to
    organizational value. Beyond basic ROI calculations, they track domain-specific
    KPIs that demonstrate tangible impact: reduced call center volume for customer
    service agents, improved inventory accuracy for retail applications, or decreased
    downtime for manufacturing agents. Their evaluation framework includes the agent’s
    alignment with strategic goals, competitive differentiation, and scalability across
    the organization. In sectors like finance, metrics bridging technical performance
    to business outcomes—such as reduced fraud losses while maintaining customer convenience—are
    especially valuable.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 商业利益相关者通过直接关联到组织价值的指标来评估智能代理。除了基本的投资回报率计算之外，他们跟踪特定领域的KPIs，这些指标展示了可衡量的影响：客户服务代理减少呼叫中心工作量、零售应用提高库存准确性或制造代理减少停机时间。他们的评估框架包括代理与战略目标的契合度、竞争优势以及在整个组织中的可扩展性。在金融等行业，将技术性能与业务成果连接起来的指标——例如，在保持客户便利性的同时减少欺诈损失——特别有价值。
- en: Regulatory stakeholders, particularly in high-stakes domains like healthcare,
    finance, and legal services, evaluate agents through strict compliance and safety
    lenses. Their assessment encompasses the agent’s adherence to domain-specific
    regulations (like HIPAA in healthcare or financial regulations in banking), bias
    detection measures, robustness against adversarial inputs, and comprehensive documentation
    of decision processes. For these stakeholders, the thoroughness of safety testing
    and the agent’s consistent performance within defined guardrails outweigh pure
    efficiency or capability metrics. As autonomous agents gain wider deployment,
    this regulatory evaluation dimension becomes increasingly crucial to ensure ethical
    operation and minimize potential harm.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 监管利益相关者，尤其是在高风险领域如医疗保健、金融和法律服务中，通过严格的合规性和安全性视角来评估智能代理。他们的评估包括代理遵守特定领域法规的情况（如医疗保健中的HIPAA或银行中的金融法规）、偏见检测措施、对抗性输入的鲁棒性以及决策过程的全面文档记录。对于这些利益相关者来说，安全测试的彻底性和代理在定义的安全轨道内的一致表现比纯效率或能力指标更为重要。随着自主代理的更广泛部署，这一监管评估维度变得越来越关键，以确保道德操作并最小化潜在危害。
- en: For organizational decision-makers, evaluations should include cost-benefit
    analyses, especially important at the deployment stage. In healthcare, comparing
    the costs and benefits of AI interventions versus traditional methods ensures
    economic viability. Similarly, evaluating the financial sustainability of LLM
    agent deployments involves analyzing operational costs against achieved efficiencies,
    ensuring scalability without sacrificing effectiveness.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 对于组织决策者来说，评估应包括成本效益分析，特别是在部署阶段尤为重要。在医疗保健领域，比较AI干预与传统方法的成本和效益确保了经济可行性。同样，评估LLM代理部署的财务可持续性涉及分析运营成本与实现效率，确保可扩展性而不牺牲有效性。
- en: Building consensus for LLM evaluation
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建LLM评估的共识
- en: Evaluating LLM agents presents a significant challenge due to their open-ended
    nature and the subjective, context-dependent definition of *good* performance.
    Unlike traditional software with clear-cut metrics, LLMs can be convincingly wrong,
    and human judgment on their quality varies. This necessitates an evaluation strategy
    centered on building organizational consensus.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 由于LLM具有开放性本质和主观的、依赖上下文的“良好”性能定义，评估LLM代理带来重大挑战。与具有明确指标的传统软件不同，LLM可能会被说服是错误的，而且人们对它们质量的判断各不相同。这需要一种以建立组织共识为中心的评估策略。
- en: The foundation of effective evaluation lies in prioritizing user outcomes. Instead
    of starting with technical metrics, developers should identify what constitutes
    success from the user’s perspective, understanding the value the agent should
    deliver and the potential risks. This outcomes-based approach ensures evaluation
    priorities align with real-world impact.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 有效的评估基础在于优先考虑用户结果。开发者不应从技术指标开始，而应确定从用户角度构成成功的因素，理解代理应提供的价值以及潜在的风险。这种基于结果的方法确保评估优先级与实际影响相一致。
- en: Addressing the subjective nature of LLM evaluation requires establishing robust
    evaluation governance. This involves creating cross-functional working groups
    comprising technical experts, domain specialists, and user representatives to
    define and document formalized evaluation criteria. Clear ownership of different
    evaluation dimensions and decision-making frameworks for resolving disagreements
    is crucial. Maintaining version control for evaluation standards ensures transparency
    as understanding evolves.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 解决LLM评估的主观性质需要建立稳健的评估治理。这包括创建由技术专家、领域专家和用户代表组成的跨职能工作组，以定义和记录正式的评估标准。明确不同评估维度和解决分歧的决策框架至关重要。维护评估标准的版本控制确保随着理解的演变保持透明度。
- en: In organizational contexts, balancing diverse stakeholder perspectives is key.
    Evaluation frameworks must accommodate technical performance metrics, domain-specific
    accuracy, and user-centric helpfulness. Effective governance facilitates this
    balance through mechanisms like weighted scoring systems and regular cross-functional
    reviews, ensuring all viewpoints are considered.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在组织环境中，平衡不同利益相关者的观点至关重要。评估框架必须适应技术性能指标、特定领域的准确性和以用户为中心的有用性。有效的治理通过加权评分系统、定期跨职能审查等机制促进这种平衡，确保所有观点都被考虑。
- en: Ultimately, evaluation governance serves as a mechanism for organizational learning.
    Well-structured frameworks help identify specific failure modes, provide actionable
    insights for development, enable quantitative comparisons between system versions,
    and support continuous improvement through integrated feedback loops. Establishing
    a “model governance committee” with representatives from all stakeholder groups
    can help review results, resolve disputes, and guide deployment decisions. Documenting
    not just results but the discussions around them captures valuable insights into
    user needs and system limitations.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，评估治理作为组织学习的机制。良好的框架有助于识别特定的失败模式，为开发提供可操作的见解，使系统能够进行定量比较，并通过集成反馈循环支持持续改进。建立一个由所有利益相关者群体代表组成的“模型治理委员会”可以帮助审查结果，解决争议，并指导部署决策。不仅记录结果，还要记录围绕它们的讨论，可以捕捉到用户需求和系统局限性的宝贵见解。
- en: In conclusion, rigorous and well-governed evaluation is an integral part of
    the LLM agent development lifecycle. By implementing structured frameworks that
    consider technical performance, user value, and organizational alignment, teams
    can ensure these systems deliver benefits effectively while mitigating risks.
    The subsequent sections will delve into evaluation methodologies, including concrete
    examples relevant to developers working with tools like LangChain.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，严格和规范化的评估是LLM代理开发生命周期的重要组成部分。通过实施考虑技术性能、用户价值和组织一致性的结构化框架，团队可以确保这些系统有效地提供利益，同时减轻风险。接下来的章节将深入探讨评估方法，包括与使用LangChain等工具的开发者相关的具体示例。
- en: Building on the foundational principles of LLM agent evaluation and the importance
    of establishing robust governance, we now turn to the practical realities of assessment.
    Developing reliable agents requires a clear understanding of what aspects of their
    behavior need to be measured and how to apply effective techniques to quantify
    their performance. The upcoming sections provide a detailed guide on the *what*
    and *how* of evaluating LLM agents, breaking down the core capabilities you should
    focus on and the diverse methodologies you can employ to build a comprehensive
    evaluation framework for your applications.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 建立在LLM代理评估的基本原则和建立稳健治理的重要性之上，我们现在转向评估的实际情况。开发可靠的代理需要清楚地了解其行为哪些方面需要衡量，以及如何应用有效技术来量化其性能。接下来的章节将提供评估LLM代理的“什么”和“如何”的详细指南，分解你应该关注的核心理念，以及你可以采用的各种方法来为你的应用程序构建全面的评估框架。
- en: 'What we evaluate: core agent capabilities'
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们评估的内容：核心代理能力
- en: At the most fundamental level, an LLM agent’s value is tied directly to its
    ability to successfully accomplish the tasks it was designed for. If an agent
    cannot reliably complete its core function, its utility is severely limited, regardless
    of how sophisticated its underlying model or tools are. Therefore, this task performance
    evaluation forms the cornerstone of agent assessment. In the next subsection,
    we’ll explore the nuances of measuring task success, looking at considerations
    relevant to assessing how effectively your agent executes its primary functions
    in real-world scenarios.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在最基本层面上，LLM代理的价值直接与其成功完成其设计任务的能力相关联。如果一个代理无法可靠地完成其核心功能，无论其底层模型或工具多么复杂，其效用都将严重受限。因此，这项任务表现评估构成了代理评估的基础。在下一小节中，我们将探讨衡量任务成功的细微差别，考虑与评估你的代理在现实场景中如何有效地执行其主要功能相关的因素。
- en: Task performance evaluation
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 任务表现评估
- en: Task performance forms the foundation of agent evaluation, measuring how effectively
    an agent accomplishes its intended goals. Successful agents demonstrate high task
    completion rates while producing relevant, factually accurate responses that directly
    address user requirements. When evaluating task performance, organizations typically
    assess both the correctness of the final output and the efficiency of the process
    used to achieve it.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 任务表现构成了代理评估的基础，衡量代理如何有效地完成其既定目标。成功的代理展示了高任务完成率，同时产生相关、事实准确的响应，直接满足用户需求。在评估任务表现时，组织通常评估最终输出的正确性和实现该输出所使用过程的效率。
- en: TaskBench (Shen and colleagues., 2023) and AgentBench (Liu and colleagues, 2023)
    provide standardized multi-stage evaluations of LLM-powered agents. TaskBench
    divides tasks into decomposition, tool selection, and parameter prediction, then
    reports that models like GPT-4 exceed 80% success on single-tool invocations but
    drop to around 50% on end-to-end task automation. AgentBench’s eight interactive
    environments likewise show top proprietary models vastly outperform smaller open-source
    ones, underscoring cross-domain generalization challenges.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: TaskBench（Shen及其同事，2023）和AgentBench（Liu及其同事，2023）提供了由LLM驱动的代理的标准化多阶段评估。TaskBench将任务分为分解、工具选择和参数预测，然后报告说，像GPT-4这样的模型在单工具调用上的成功率超过80%，但在端到端任务自动化上的成功率降至约50%。AgentBench的八个交互式环境同样显示，顶级专有模型远远优于较小的开源模型，突显了跨领域泛化挑战。
- en: Financial services applications demonstrate task performance evaluation in practice,
    though we should view industry-reported metrics with appropriate skepticism. While
    many institutions claim high accuracy rates for document analysis systems, independent
    academic assessments have documented significantly lower performance in realistic
    conditions. A particularly important dimension in regulated industries is an agent’s
    ability to correctly identify instances where it lacks sufficient information—a
    critical safety feature that requires specific evaluation protocols beyond simple
    accuracy measurement.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 金融服务业应用展示了实际中的任务性能评估，尽管我们应该对行业报告的指标持适当的怀疑态度。虽然许多机构声称文档分析系统具有高准确率，但独立的学术评估在现实条件下记录了显著较低的性能。在受监管行业中，一个特别重要的维度是智能体正确识别其缺乏足够信息的情况的能力——这是一个需要特定评估协议的临界安全功能，而不仅仅是简单的准确度测量。
- en: Tool usage evaluation
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工具使用评估
- en: 'Tool usage capability—an agent’s ability to select, configure, and leverage
    external systems—has emerged as a crucial evaluation dimension that distinguishes
    advanced agents from simple question-answering systems. Effective tool usage evaluation
    encompasses multiple aspects: the agent’s ability to select the appropriate tool
    for a given subtask, provide correct parameters, interpret tool outputs correctly,
    and integrate these outputs into a coherent solution strategy.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 工具使用能力——智能体选择、配置和利用外部系统的能力——已成为区分高级智能体和简单问答系统的关键评估维度。有效的工具使用评估涵盖了多个方面：智能体为给定子任务选择适当工具的能力，提供正确的参数，正确解释工具输出，并将这些输出整合到连贯的解决方案策略中。
- en: 'The T-Eval framework, developed by Liu and colleagues (2023), decomposes tool
    usage into distinct measurable capabilities: planning the sequence of tool calls,
    reasoning about the next steps, retrieving the correct tool from available options,
    understanding tool documentation, correctly formatting API calls, and reviewing
    responses to determine if goals were met. This granular approach allows organizations
    to identify specific weaknesses in their agent’s tool-handling capabilities rather
    than simply observing overall failures.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 由刘及其同事（2023年）开发的T-Eval框架，将工具使用分解为可区分的可测量能力：规划工具调用的顺序，推理下一步行动，从可用选项中检索正确的工具，理解工具文档，正确格式化API调用，以及审查响应以确定是否达到目标。这种细粒度方法允许组织识别其智能体工具处理能力的具体弱点，而不仅仅是观察整体失败。
- en: Recent benchmarks like ToolBench and ToolSandbox demonstrate that even state-of-the-art
    agents struggle with tool usage in dynamic environments. In production systems,
    evaluation increasingly focuses on efficiency metrics alongside basic correctness—measuring
    whether agents avoid redundant tool calls, minimize unnecessary API usage, and
    select the most direct path to solve user problems. While industry implementations
    often claim significant efficiency improvements, peer-reviewed research suggests
    more modest gains, with optimized tool selection typically reducing computation
    costs by 15-20% in controlled studies while maintaining outcome quality.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 近期基准测试，如ToolBench和ToolSandbox，表明即使在动态环境中，最先进的智能体在使用工具方面也面临挑战。在生产系统中，评估越来越关注效率指标，同时兼顾基本正确性——衡量智能体是否避免了冗余的工具调用，最小化了不必要的API使用，并选择了最直接的方法来解决用户问题。尽管行业实施往往声称有显著的效率提升，但同行评审的研究表明，收益更为适度，优化后的工具选择通常在受控研究中将计算成本降低15-20%，同时保持结果质量。
- en: RAG evaluation
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RAG评估
- en: 'RAG system evaluation represents a specialized but crucial area of agent assessment,
    focusing on how effectively agents retrieve and incorporate external knowledge.
    Four key dimensions form the foundation of comprehensive RAG evaluation: retrieval
    quality, contextual relevance, faithful generation, and information synthesis.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: RAG系统评估代表了智能体评估的一个专业但至关重要的领域，专注于智能体检索和整合外部知识的有效性。四个关键维度构成了全面RAG评估的基础：检索质量、上下文相关性、忠实生成和信息综合。
- en: '*Retrieval quality* measures how well the system finds the most appropriate
    information from its knowledge base. Rather than using simple relevance scores,
    modern evaluation approaches assess retrieval through precision and recall at
    different ranks, considering both the absolute relevance of retrieved documents
    and their coverage of the information needed to answer user queries. Academic
    research has developed standardized test collections with expert annotations to
    enable systematic comparison across different retrieval methodologies.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '*检索质量*衡量系统从其知识库中找到最合适信息的能力。而不是使用简单的相关性评分，现代评估方法通过不同排名的精确度和召回率来评估检索，同时考虑检索文档的绝对相关性及其覆盖用户查询所需信息的范围。学术研究已经开发了带有专家注释的标准测试集合，以实现不同检索方法之间的系统比较。'
- en: '*Contextual relevance*, on the other hand, examines how precisely the retrieved
    information matches the specific information need expressed in the query. This
    involves evaluating whether the system can distinguish between superficially similar
    but contextually different information requests. Recent research has developed
    specialized evaluation methodologies for testing disambiguation capabilities in
    financial contexts, where similar terminology might apply to fundamentally different
    products or regulations. These approaches specifically measure how well retrieval
    systems can distinguish between queries that use similar language but have distinct
    informational needs.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，*上下文相关性*考察检索到的信息与查询中表达的具体信息需求匹配的精确程度。这涉及到评估系统是否能够区分表面上相似但上下文不同的信息请求。最近的研究已经开发了针对金融环境中歧义消除能力的专门评估方法，在这些环境中，相似的术语可能适用于根本不同的产品或法规。这些方法具体衡量检索系统区分使用相似语言但具有不同信息需求的查询的能力。
- en: '*Faithful generation*—the degree to which the agent’s responses accurately
    reflect the retrieved information without fabricating details—represents perhaps
    the most critical aspect of RAG evaluation. Recent studies have found that even
    well-optimized RAG systems still show non-trivial hallucination rates, between
    3-15% on complex domains, highlighting the ongoing challenge in this area. Researchers
    have developed various evaluation protocols for faithfulness, including source
    attribution tests and contradiction detection mechanisms that systematically compare
    generated content with the retrieved source material.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '*忠实生成*——代理的响应准确反映检索到的信息而不添加细节的程度——可能是RAG评估中最关键的部分。最近的研究发现，即使是优化良好的RAG系统仍然显示出非微不足道的幻觉率，在复杂领域为3-15%，突显了这一领域的持续挑战。研究人员已经开发了各种评估协议，包括来源归属测试和矛盾检测机制，这些机制系统地比较生成内容与检索到的源材料。'
- en: Finally, *information synthesis* quality evaluates the agent’s ability to integrate
    information from multiple sources into coherent, well-structured responses. Rather
    than simply concatenating or paraphrasing individual documents, advanced agents
    must reconcile potentially conflicting information, present balanced perspectives,
    and organize content logically. Evaluation here extends beyond automated metrics
    to include expert assessment of how effectively the agent has synthesized complex
    information into accessible, accurate summaries that maintain appropriate nuance.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，*信息综合*质量评估代理将来自多个来源的信息整合成连贯、结构良好的响应的能力。而不仅仅是简单地将单个文档连接或改写，高级代理必须调和可能冲突的信息，呈现平衡的视角，并逻辑地组织内容。这里的评估不仅超越了自动化指标，还包括专家对代理如何有效地将复杂信息综合成易于理解、准确且保持适当细微差别的总结的有效性评估。
- en: Planning and reasoning evaluation
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 规划和推理评估
- en: Planning and reasoning capabilities form the cognitive foundation that enables
    agents to solve complex, multi-step problems that cannot be addressed through
    single operations. Evaluating these capabilities requires moving beyond simple
    input-output testing to assess the quality of the agent’s thought process and
    problem-solving strategy.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 规划和推理能力构成了认知基础，使代理能够解决复杂的多步骤问题，这些问题不能通过单一操作来解决。评估这些能力需要超越简单的输入输出测试，以评估代理思维过程和问题解决策略的质量。
- en: 'Plan feasibility gauges whether every action in a proposed plan respects the
    domain’s preconditions and constraints. Using the PlanBench suite, Valmeekam and
    colleagues in their 2023 paper *PlanBench: An Extensible Benchmark for Evaluating
    Large Language Models on Planning and Reasoning about Change* showed that GPT-4
    correctly generates fully executable plans in only about 34% of classical IPC-style
    domains under zero-shot conditions—far below reliable thresholds and underscoring
    persistent failures to account for environment dynamics and logical preconditions.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '计划可行性衡量提议的计划中的每个动作是否尊重领域的先决条件和约束。使用PlanBench套件，Valmeekam及其同事在2023年的论文*PlanBench:
    用于评估大型语言模型在规划和关于变化推理上的可扩展基准*中表明，在零样本条件下，GPT-4仅在约34%的经典IPC风格领域中正确生成完全可执行的计划——远低于可靠的阈值，并强调了持续未能考虑环境动态和逻辑先决条件的失败。'
- en: Plan optimality extends evaluation beyond basic feasibility to consider efficiency.
    This dimension assesses whether agents can identify not just any working solution
    but the most efficient approach to accomplishing their goals. The Recipe2Plan
    benchmark specifically evaluates this by testing whether agents can effectively
    multitask under time constraints, mirroring real-world efficiency requirements.
    Current state-of-the-art models show significant room for improvement, with published
    research indicating optimal planning rates between 45% and 55% for even the most
    capable systems.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 计划最优性将评估扩展到基本可行性之外，以考虑效率。这个维度评估智能体是否不仅能识别任何可行的解决方案，而且能识别实现目标的最有效方法。Recipe2Plan基准通过测试智能体是否能在时间限制下有效多任务处理来具体评估这一点，这反映了现实世界的效率要求。当前最先进的模型显示出显著的改进空间，已发表的研究表明，即使是能力最强的系统，最优规划率也在45%到55%之间。
- en: Reasoning coherence evaluates the logical structure of the agent’s problem-solving
    approach—whether individual reasoning steps connect logically, whether conclusions
    follow from premises, and whether the agent maintains consistency throughout complex
    analyses. Unlike traditional software testing where only the final output matters,
    agent evaluation increasingly examines intermediate reasoning steps to identify
    failures in logical progression that might be masked by a correct final answer.
    Multiple academic studies have demonstrated the importance of this approach, with
    several research groups developing standardized methods for reasoning trace analysis.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑一致性评估了智能体问题解决方法的逻辑结构——是否每个推理步骤逻辑上相连，结论是否从前提中得出，以及智能体在复杂分析中是否保持一致性。与只关注最终输出的传统软件测试不同，智能体评估越来越多地检查中间推理步骤，以识别可能被正确最终答案掩盖的逻辑进展中的失败。多项学术研究表明，这种方法的重要性，并且有几个研究小组开发了用于推理跟踪分析的标准化方法。
- en: 'Recent studies (*CoLadder: Supporting Programmers with Hierarchical Code Generation
    in Multi-Level Abstraction*, 2023, and *Generating a Low-code Complete Workflow
    via Task Decomposition and RAG*, 2024) show that decomposing code-generation tasks
    into smaller, well-defined subtasks—often using hierarchical or as-needed planning—leads
    to substantial gains in code quality, developer productivity, and system reliability
    across both benchmarks and live engineering settings.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '近期研究（*CoLadder: 在多级抽象中通过分层代码生成支持程序员*，2023年，以及*通过任务分解和RAG生成低代码完整工作流*，2024年）表明，将代码生成任务分解为更小、定义良好的子任务——通常使用分层或按需规划——在基准测试和实际工程环境中都能显著提高代码质量、开发人员生产力和系统可靠性。'
- en: Building on the foundational principles of LLM agent evaluation and the importance
    of establishing robust governance, we now turn to the practical realities of assessment.
    Developing reliable agents requires a clear understanding of what aspects of their
    behavior need to be measured and how to apply effective techniques to quantify
    their performance.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLM智能体评估的基础原则和建立稳健治理的重要性基础上，我们现在转向评估的实际现实。开发可靠的智能体需要清楚地了解需要衡量其行为的哪些方面以及如何应用有效的技术来量化其性能。
- en: Identifying the core capabilities to evaluate is the first critical step. The
    next is determining how to effectively measure them, given the complexities and
    subjective aspects inherent in LLM agents compared to traditional software. Relying
    on a single metric or approach is insufficient. In the next subsection, we’ll
    explore the various methodologies and approaches available for evaluating agent
    performance in a robust, scalable, and insightful manner. We’ll cover the role
    of automated metrics for consistency, the necessity of human feedback for subjective
    assessment, the importance of system-level analysis for integrated agents, and
    how to combine these techniques into a practical evaluation framework that drives
    improvement.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 确定要评估的核心能力是第一步关键。接下来是确定如何有效地测量它们，考虑到与传统的软件相比，LLM代理固有的复杂性和主观方面。依赖于单一指标或方法是不够的。在下一小节中，我们将探讨评估代理性能的稳健、可扩展和有洞察力的各种方法和途径。我们将涵盖自动化指标在一致性方面的作用、主观评估中人类反馈的必要性、系统级分析对集成代理的重要性，以及如何将这些技术结合成一个实用的评估框架，以推动改进。
- en: 'How we evaluate: methodologies and approaches'
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们如何评估：方法和途径
- en: LLM agents, particularly those built with flexible frameworks like LangChain
    or LangGraph, are typically composed of different functional parts or *skills*.
    An agent’s overall performance isn’t a single monolithic metric; it’s the result
    of how well it executes these individual capabilities and how effectively they
    work together. In the following subsection, we’ll delve into these core capabilities
    that distinguish effective agents, outlining the specific dimensions we should
    assess to understand where our agent excels and where it might be failing.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: LLM代理，尤其是那些使用灵活框架（如LangChain或LangGraph）构建的代理，通常由不同的功能部分或*技能*组成。代理的整体性能不是一个单一的单一指标；它是它执行这些个别能力以及它们如何有效协作的结果。在下一小节中，我们将深入研究这些区分有效代理的核心能力，概述我们应该评估的具体维度，以了解我们的代理在哪些方面表现优异，在哪些方面可能存在不足。
- en: Automated evaluation approaches
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动评估方法
- en: Automated evaluation methods provide scalable, consistent assessment of agent
    capabilities, enabling systematic comparison across different versions or implementations.
    While no single metric can capture all aspects of agent performance, combining
    complementary approaches allows for comprehensive automated evaluation that complements
    human assessment.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 自动评估方法提供了对代理能力可扩展、一致的评估，使得在不同版本或实现之间进行系统比较成为可能。虽然没有任何单一指标可以捕捉到代理性能的所有方面，但结合互补的方法可以实现全面的自动化评估，这有助于补充人工评估。
- en: Reference-based evaluation compares each agent output against one or more gold-standard
    answers or trajectories. While BLEU/ROUGE and early embedding measures like BERTScore
    / **Universal Sentence Encoder** (**USE**) were vital first steps, today’s state-of-the-art
    relies on learned metrics (BLEURT, COMET, BARTScore), QA-based frameworks (QuestEval),
    and LLM-powered judges, all backed by large human‐rated datasets to ensure robust,
    semantically aware evaluation.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 基于参考的评估将每个代理输出与一个或多个黄金标准答案或轨迹进行比较。虽然BLEU/ROUGE和早期的嵌入度量（如BERTScore / **通用句子编码器**（**USE**））是重要的第一步，但今天的最佳实践依赖于学习指标（BLEURT、COMET、BARTScore）、基于问答的框架（QuestEval）和由LLM驱动的评委，所有这些都由大量人工评分数据集支持，以确保稳健、语义感知的评估。
- en: Rather than using direct string comparison, modern evaluation increasingly employs
    criterion-based assessment frameworks that evaluate outputs against specific requirements.
    For example, the T-Eval framework evaluates tool usage through a multi-stage process
    examining planning, reasoning, tool selection, parameter formation, and result
    interpretation. This structured approach allows precise identification of where
    in the process an agent might be failing, providing far more actionable insights
    than simple success/failure metrics.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 与直接字符串比较相比，现代评估越来越多地采用基于标准的评估框架，这些框架通过检查规划、推理、工具选择、参数形成和结果解释等多阶段过程来评估工具的使用情况。这种结构化方法可以精确地识别出在哪个环节代理可能会失败，提供比简单的成功/失败指标多得多的可操作见解。
- en: LLM-as-a-judge approaches represent a rapidly evolving evaluation methodology
    where powerful language models serve as automated evaluators, assessing outputs
    according to defined rubrics. Research by Zheng and colleagues (*Judging LLM-as-a-Judge
    with MT-Bench and Chatbot Arena*, 2023) demonstrates that with carefully designed
    prompting, models like GPT-4 can achieve substantial agreement with human evaluators
    on dimensions like factual accuracy, coherence, and relevance. This approach can
    help evaluate subjective qualities that traditional metrics struggle to capture,
    though researchers emphasize the importance of human verification to mitigate
    potential biases in the evaluator models themselves.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: LLM-as-a-judge approaches represent a rapidly evolving evaluation methodology
    where powerful language models serve as automated evaluators, assessing outputs
    according to defined rubrics. Research by Zheng and colleagues (*Judging LLM-as-a-Judge
    with MT-Bench and Chatbot Arena*, 2023) demonstrates that with carefully designed
    prompting, models like GPT-4 can achieve substantial agreement with human evaluators
    on dimensions like factual accuracy, coherence, and relevance. This approach can
    help evaluate subjective qualities that traditional metrics struggle to capture,
    though researchers emphasize the importance of human verification to mitigate
    potential biases in the evaluator models themselves.
- en: Human-in-the-loop evaluation
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人类在环评估
- en: Human evaluation remains essential for assessing subjective dimensions of agent
    performance that automated metrics cannot fully capture. Effective human-in-the-loop
    evaluation requires structured methodologies to ensure consistency and reduce
    bias while leveraging human judgment where it adds the most value.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 人类评估对于评估自动化指标无法完全捕捉的代理性能的主观维度仍然至关重要。有效的人类在环评估需要结构化的方法来确保一致性并减少偏见，同时利用人类判断在最有价值的地方。
- en: Expert review provides in-depth qualitative assessment from domain specialists
    who can identify subtle errors, evaluate reasoning quality, and assess alignment
    with domain-specific best practices. Rather than unstructured feedback, modern
    expert review employs standardized rubrics that decompose evaluation into specific
    dimensions, typically using Likert scales or comparative rankings. Research in
    healthcare and financial domains has developed standardized protocols for expert
    evaluation, particularly for assessing agent responses in complex regulatory contexts.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 专家评审提供了来自领域专家的深入定性评估，他们可以识别细微错误、评估推理质量并评估与特定领域最佳实践的符合程度。与现代专家评审相比，现代专家评审采用标准化的评分标准，将评估分解为特定的维度，通常使用李克特量表或比较排名。在医疗和金融领域的研究已经开发了专家评估的标准协议，特别是用于评估在复杂监管环境中的代理响应。
- en: User feedback captures the perspective of end users interacting with the agent
    in realistic contexts. Structured feedback collection through embedded rating
    mechanisms (for example, thumbs up/down, 1-5 star ratings) provides quantitative
    data on user satisfaction, while free-text comments offer qualitative insights
    into specific strengths or weaknesses. Academic studies of conversational agent
    effectiveness increasingly implement systematic feedback collection protocols
    where user ratings are analyzed to identify patterns in agent performance across
    different query types, user segments, or time periods.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 用户反馈捕捉了与代理在现实环境中互动的最终用户的观点。通过嵌入的评分机制（例如，点赞/踩，1-5 星评级）进行结构化反馈收集提供了关于用户满意度的定量数据，而自由文本评论则提供了对特定优势或弱点的定性见解。对话代理有效性的学术研究越来越多地实施系统性的反馈收集协议，其中用户评分被分析以识别不同查询类型、用户群体或时间跨度的代理性能模式。
- en: A/B testing methodologies allow controlled comparison of different agent versions
    or configurations by randomly routing users to different implementations and measuring
    performance differences. This experimental approach is particularly valuable for
    evaluating changes to agent prompting, tool integration, or retrieval mechanisms.
    When implementing A/B testing, researchers typically define primary metrics (like
    task completion rates or user satisfaction) alongside secondary metrics that help
    explain observed differences (such as response length, tool usage patterns, or
    conversation duration).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: A/B 测试方法通过随机将用户路由到不同的实现并测量性能差异，允许对不同的代理版本或配置进行受控比较。这种实验方法在评估代理提示、工具集成或检索机制的变化方面尤其有价值。在实施
    A/B 测试时，研究人员通常定义主要指标（如任务完成率或用户满意度）以及帮助解释观察到的差异的次要指标（例如响应长度、工具使用模式或对话持续时间）。
- en: Academic research on conversational agent optimization has demonstrated the
    effectiveness of controlled experiments in identifying specific improvements to
    agent configurations.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 学术研究在对话代理优化方面已经证明了控制实验在识别特定改进代理配置方面的有效性。
- en: System-level evaluation
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 系统级评估
- en: System-level evaluation is crucial for complex LLM agents, particularly RAG
    systems, because testing individual components isn’t enough. Research indicates
    that a significant portion of failures (over 60% in some studies) stem from integration
    issues between components that otherwise function correctly in isolation. For
    example, issues can arise from retrieved documents not being used properly, query
    reformulation altering original intent, or context windows truncating information
    during handoffs. System-level evaluation addresses this by examining how information
    flows between components and how the agent performs as a unified system.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 对于复杂的LLM代理，尤其是RAG系统，系统级评估至关重要，因为测试单个组件是不够的。研究表明，相当一部分失败（在某些研究中超过60%）源于在独立运行时功能正常的组件之间的集成问题。例如，问题可能源于检索到的文档未正确使用，查询重构改变了原始意图，或上下文窗口在交接过程中截断信息。系统级评估通过检查组件之间的信息流以及代理作为统一系统的表现来解决此问题。
- en: Core approaches to system-level evaluation include using diagnostic frameworks
    that trace information flow through the entire pipeline to identify breakdown
    points, like the RAG Diagnostic Tool. Tracing and observability tools (such as
    LangSmith, Langfuse, and DeepEval) provide visibility into the agent’s internal
    workings, allowing developers to visualize reasoning chains and pinpoint where
    errors occur. End-to-end testing methodologies use comprehensive scenarios to
    assess how the entire system handles ambiguity, challenge inputs, and maintain
    context over multiple turns, using frameworks like GAIA.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 系统级评估的核心方法包括使用诊断框架来追踪整个管道中的信息流，以识别故障点，例如RAG诊断工具。追踪和可观察性工具（如LangSmith、Langfuse和DeepEval）提供了对代理内部工作的可见性，使开发者能够可视化推理链并确定错误发生的位置。端到端测试方法使用全面的场景来评估整个系统如何处理歧义、挑战性输入以及在多个回合中保持上下文，使用GAIA等框架。
- en: Effective evaluation of LLM applications requires running multiple assessments.
    Rather than presenting abstract concepts, here are a few practical steps!
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 对LLM应用的有效评估需要运行多个评估。而不是呈现抽象概念，这里有一些实用的步骤！
- en: '**Define business metrics**: Start by identifying metrics that matter to your
    organization. Focus on functional aspects like accuracy and completeness, technical
    factors such as latency and token usage, and user experience elements including
    helpfulness and clarity. Each application should have specific criteria with clear
    measurement methods.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定义业务指标**：首先确定对您的组织重要的指标。关注功能性方面，如准确性、完整性，技术因素，如延迟和令牌使用，以及用户体验元素，包括有用性和清晰度。每个应用都应具有具体的标准，并采用明确的测量方法。'
- en: '**Create diverse test datasets**: Develop comprehensive test datasets covering
    common user queries, challenging edge cases, and potential compliance issues.
    Categorize examples systematically to ensure broad coverage. Continuously expand
    your dataset as you discover new usage patterns or failure modes.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**创建多样化的测试数据集**：开发涵盖常见用户查询、具有挑战性的边缘情况和潜在合规问题的全面测试数据集。系统性地分类示例以确保广泛的覆盖。随着发现新的使用模式或故障模式，持续扩展您的数据集。'
- en: '**Combine multiple evaluation methods**: Use a mix of evaluation approaches
    for thorough assessment. Automated checks for factual accuracy and correctness
    should be combined with domain-specific criteria. Consider both quantitative metrics
    and qualitative assessments from subject matter experts when evaluating responses.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结合多种评估方法**：使用多种评估方法进行彻底的评估。对事实准确性和正确性的自动检查应与特定领域的标准相结合。在评估响应时，考虑来自领域专家的定量指标和定性评估。'
- en: '**Deploy progressively**: Adopt a staged deployment approach. Begin with development
    testing against offline benchmarks, then proceed to limited production release
    with a small user subset. Only roll out fully after meeting performance thresholds.
    This cautious approach helps identify issues before they affect most users.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**逐步部署**：采用分阶段部署方法。从针对离线基准的开发测试开始，然后进行针对小用户子集的有限生产发布。只有在满足性能阈值后，才全面推出。这种谨慎的方法有助于在影响大多数用户之前识别问题。'
- en: '**Monitor production performance**: Implement ongoing monitoring in live environments.
    Track key performance indicators like response time, error rates, token usage,
    and user feedback. Set up alerts for anomalies that might indicate degraded performance
    or unexpected behavior.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监控生产性能**：在实时环境中实施持续监控。跟踪关键性能指标，如响应时间、错误率、令牌使用量和用户反馈。为可能表明性能下降或意外行为的异常设置警报。'
- en: '**Establish improvement cycles**: Create structured processes to translate
    evaluation insights into concrete improvements. When issues are identified, investigate
    root causes, implement specific solutions, and validate the effectiveness of changes
    through re-evaluation. Document patterns of problems and successful solutions
    for future reference.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**建立改进周期**：创建结构化的流程，将评估洞察转化为具体的改进。当发现问题时，调查根本原因，实施特定解决方案，并通过重新评估验证更改的有效性。记录问题和成功解决方案的模式，供未来参考。'
- en: '**Foster cross-functional collaboration**: Include diverse perspectives in
    your evaluation process. Technical teams, domain experts, business stakeholders,
    and compliance specialists all bring valuable insights. Regular review sessions
    with these cross-functional teams help ensure the comprehensive assessment of
    LLM applications.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**促进跨职能协作**：在评估过程中包含不同的观点。技术团队、领域专家、业务利益相关者和合规专家都带来了有价值的见解。与这些跨职能团队定期进行审查会议有助于确保对LLM应用的全面评估。'
- en: '**Maintain living documentation**: Keep centralized records of evaluation results,
    improvement actions, and outcomes. This documentation builds organizational knowledge
    and helps teams learn from past experiences, ultimately accelerating the development
    of more effective LLM applications.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**维护活文档**：保留评估结果、改进措施和成果的集中记录。这些文档构建了组织知识，并帮助团队从过去的经验中学习，最终加速更有效的LLM应用的开发。'
- en: It’s time now to put the theory to the test and get into the weeds of evaluating
    LLM agents. Let’s dive in!
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候将理论付诸实践，深入到评估LLM代理的细节中。让我们开始吧！
- en: Evaluating LLM agents in practice
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实践中评估LLM代理
- en: LangChain provides several predefined evaluators for different evaluation criteria.
    These evaluators can be used to assess outputs based on specific rubrics or criteria
    sets. Some common criteria include conciseness, relevance, correctness, coherence,
    helpfulness, and controversiality.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain为不同的评估标准提供了几个预定义的评估器。这些评估器可以用于根据特定的评分标准或标准集评估输出。一些常见的标准包括简洁性、相关性、正确性、连贯性、有用性和争议性。
- en: We can also compare results from an LLM or agent against reference results using
    different methods starting from pairwise string comparisons, string distances,
    and embedding distances. The evaluation results can be used to determine the preferred
    LLM or agent based on the comparison of outputs. Confidence intervals and p-values
    can also be calculated to assess the reliability of the evaluation results.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用不同的方法，从成对字符串比较、字符串距离和嵌入距离开始，将LLM或代理的结果与参考结果进行比较。评估结果可用于根据输出比较确定首选的LLM或代理。还可以计算置信区间和p值，以评估评估结果的可靠性。
- en: Let’s go through a few basics and apply useful evaluation strategies. We’ll
    start with LangChain.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一些基础知识，并应用有用的评估策略。我们将从LangChain开始。
- en: Evaluating the correctness of results
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估结果的正确性
- en: Let’s think of an example, where we want to verify that an LLM’s answer is correct
    (or how far it is off). For example, when asked about the Federal Reserve’s interest
    rate, you might compare the output against a reference answer using both an exact
    match and a string distance evaluator.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个例子，我们想要验证LLM的答案是否正确（或者它偏离多远）。例如，当被问及美联储的利率时，您可能会使用精确匹配和字符串距离评估器将输出与参考答案进行比较。
- en: '[PRE0]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now, obviously this won’t be very useful if the output comes in a different
    format or if we want to gauge how far off the answer is. In the repository, you
    can find an implementation of a custom comparison that would parse answers such
    as “It is 0.50%” and “A quarter percent.”
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，显然如果输出以不同的格式出现，或者我们想要衡量答案偏离多远，这将不会很有用。在存储库中，您可以找到一个自定义比较的实现，该实现可以解析“它是0.50%”和“四分之一百分比”之类的答案。
- en: A more generalizable approach is LLM‐as‐a‐judge for evaluating correctness.
    In this example, instead of using simple string extraction or an exact match,
    we call an evaluation LLM (for example, an upper mid-range model such as Mistral)
    that parses and scores the prompt, the prediction, and a reference answer and
    then returns a numerical score plus reasoning. This works in scenarios where the
    prediction might be phrased differently but still correct.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 一种更通用的方法是使用LLM作为裁判来评估正确性。在这个例子中，我们不是使用简单的字符串提取或精确匹配，而是调用一个评估LLM（例如，一个中高端模型如Mistral），它解析并评分提示、预测和参考答案，然后返回一个数值评分和推理。这在预测可能措辞不同但仍正确的情况下有效。
- en: '[PRE1]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output demonstrates how the LLM evaluator assesses the response quality
    with nuanced reasoning:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 输出展示了LLM评估者如何通过细腻的推理来评估响应质量：
- en: '[PRE3]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This evaluation highlights an important advantage of the LLM-as-a-judge approach:
    it can identify subtle issues that simple matching would miss. In this case, the
    evaluator correctly identified that the response lacked important context. With
    a score of 3 out of 5, the LLM judge provides a more nuanced assessment than binary
    correct/incorrect evaluations, giving developers actionable feedback to improve
    response quality in financial applications where accuracy and proper attribution
    are critical.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这次评估突出了LLM作为裁判方法的一个重要优势：它可以识别简单匹配可能遗漏的微妙问题。在这种情况下，评估者正确地指出，该响应缺少重要的上下文。以5分为满分，LLM裁判给出了比二元正确/错误评估更细腻的评估，为开发者提供了可操作的反馈，以改善金融应用中的响应质量，在这些应用中准确性和适当的归属至关重要。
- en: 'The next example shows how to use Mistral AI to evaluate a model’s prediction
    against a reference answer. Please make sure to set your `MISTRAL_API_KEY` environment
    variable and install the required package: `pip install langchain_mistralai`.
    This should already be installed if you followed the instructions in [*Chapter
    2*](E_Chapter_2.xhtml#_idTextAnchor044).'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个示例展示了如何使用Mistral AI评估模型预测与参考答案的匹配情况。请确保设置你的`MISTRAL_API_KEY`环境变量并安装所需的包：`pip
    install langchain_mistralai`。如果你遵循了[第2章](E_Chapter_2.xhtml#_idTextAnchor044)中的说明，这个包应该已经安装好了。
- en: This approach is more appropriate when you have ground truth responses and want
    to assess how well the model’s output matches the expected answer. It’s particularly
    useful for factual questions with clear, correct answers.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 当你有真实响应并想评估模型输出与预期答案的匹配程度时，这种方法更为合适。它特别适用于有明确、正确答案的事实性问题。
- en: '[PRE4]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The output shows how providing a reference answer significantly changes the
    evaluation results:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示了提供参考答案如何显著改变评估结果：
- en: '[PRE5]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Notice how the score increased dramatically from 3 (in the previous example)
    to 8 when we provided a reference answer. This demonstrates the importance of
    ground truth in evaluation. Without a reference, the evaluator focused on the
    lack of citation and timestamp. With a reference confirming the factual accuracy,
    the evaluator now focuses on assessing completeness and depth instead of verifiability.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当我们提供一个参考答案时，分数从先前的3分（在上一个示例中）急剧上升到8分。这证明了在评估中真实信息的重要性。没有参考，评估者关注的是缺乏引用和时间戳。有了确认事实准确性的参考，评估者现在专注于评估完整性和深度，而不是可验证性。
- en: Both of these approaches leverage Mistral’s LLM as an evaluator, which can provide
    more nuanced and context-aware assessments than simple string matching or statistical
    methods. The results from these evaluations should be consistent when using `temperature=0`,
    though outputs may differ from those shown in the book due to changes on the provider
    side.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法都利用了Mistral的LLM作为评估者，它可以提供比简单的字符串匹配或统计方法更细腻和上下文感知的评估。当使用`temperature=0`时，这些评估的结果应该是一致的，尽管由于提供方的变化，输出可能与书中所示的不同。
- en: Your output may differ from the book example due to model version differences
    and inherent variations in LLM responses (depending on the temperature).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 你的输出可能与书中示例不同，这可能是由于模型版本差异和LLM响应（根据温度）的固有变化。
- en: Evaluating tone and conciseness
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估语气和简洁性
- en: 'Beyond factual accuracy, many applications require responses that meet certain
    stylistic criteria. Healthcare applications, for example, must provide accurate
    information in a friendly, approachable manner without overwhelming patients with
    unnecessary details. The following example demonstrates how to evaluate both conciseness
    and tone using LangChain’s criteria evaluators, allowing developers to assess
    these subjective but critical aspects of response quality:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 除了事实准确性之外，许多应用程序还需要满足某些风格标准的响应。例如，医疗保健应用程序必须以友好、可接近的方式提供准确的信息，而不会让患者感到不必要的细节过多。以下示例演示了如何使用LangChain的评估器来评估简洁性和语气，使开发者能够评估响应质量的这些主观但关键方面：
- en: 'We start by importing the evaluator loader and a chat LLM for evaluation (for
    example GPT-4o):'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入评估加载器和用于评估的聊天LLM（例如GPT-4o）：
- en: '[PRE6]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Our example prompt and the answer we’ve obtained is:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的示例提示和获得的答案是：
- en: '[PRE7]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now let’s evaluate conciseness using a built-in `conciseness` criterion:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用内置的`conciseness`标准来评估简洁性：
- en: '[PRE8]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The result includes a score (0 or 1), a value (“Y” or “N”), and a reasoning
    chain of thought:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 结果包括一个分数（0或1）、一个值（“Y”或“N”）和一个推理思维链：
- en: '[PRE9]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'As for friendliness, let’s define a `custom` criterion:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 关于友好性，让我们定义一个`custom`标准：
- en: '[PRE10]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The evaluator should return whether the tone is friendly (`Y`/`N`) along with
    reasoning. In fact, this is what we get:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 评估者应返回语气是否友好（`Y`/`N`）以及推理。事实上，这正是我们得到的：
- en: '[PRE12]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This evaluation approach is particularly valuable for applications in healthcare,
    customer service, and educational domains where the manner of communication is
    as important as the factual content. The explicit reasoning provided by the evaluator
    helps development teams understand exactly which elements of the response contribute
    to its tone, making it easier to debug and improve response generation. While
    binary `Y`/`N` scores are useful for automated quality gates, the detailed reasoning
    offers more nuanced insights for continuous improvement. For production systems,
    consider combining multiple criteria evaluators to create a comprehensive quality
    score that reflects all aspects of your application’s communication requirements.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这种评估方法对于医疗保健、客户服务和教育领域中的应用尤其有价值，在这些领域中，沟通方式的重要性与事实内容相当。评估者提供的明确推理有助于开发团队了解哪些响应元素导致了其语气，这使得调试和改进响应生成更加容易。虽然二进制的`Y`/`N`评分对于自动质量门很有用，但详细的推理为持续改进提供了更细微的见解。对于生产系统，考虑结合多个标准评估器来创建一个全面的质量评分，该评分反映了您应用程序通信要求的各个方面。
- en: Evaluating the output format
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估输出格式
- en: 'When working with LLMs to generate structured data like JSON, XML, or CSV,
    format validation becomes critical. Financial applications, reporting tools, and
    API integrations often depend on correctly formatted data structures. A technically
    perfect response that fails to adhere to the expected format can break downstream
    systems. LangChain provides specialized evaluators for validating structured outputs,
    as demonstrated in the following example using JSON validation for a financial
    report:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 当与LLM一起生成结构化数据（如JSON、XML或CSV）时，格式验证变得至关重要。金融应用程序、报告工具和API集成通常依赖于正确格式的数据结构。一个技术上完美的响应，如果未能遵守预期的格式，可能会破坏下游系统。LangChain提供了用于验证结构化输出的专用评估器，以下示例展示了使用JSON验证财务报告：
- en: '[PRE13]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We’ll see a score indicating the JSON is valid:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到一个分数，表示JSON有效：
- en: '[PRE15]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'For the invalid JSON, we are getting a score indicating the JSON is invalid:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 对于无效的JSON，我们得到一个分数，表示JSON无效：
- en: '[PRE16]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This validation approach is particularly valuable in production systems where
    LLMs interface with other software components. The `JsonValidityEvaluator` not
    only identifies invalid outputs but also provides detailed error messages pinpointing
    the exact location of formatting errors. This facilitates rapid debugging and
    can be incorporated into automated testing pipelines to prevent format-related
    failures. Consider implementing similar validators for other formats your application
    may generate, such as XML, CSV, or domain-specific formats like FIX protocol for
    financial transactions.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这种验证方法在生产系统中特别有价值，其中LLM与其他软件组件交互。`JsonValidityEvaluator`不仅识别无效输出，还提供详细的错误消息，指明格式错误的准确位置。这有助于快速调试，并可以集成到自动化测试管道中，以防止格式相关的失败。考虑为您的应用程序可能生成的其他格式实现类似的验证器，例如XML、CSV或金融交易中的FIX协议等特定领域的格式。
- en: Evaluating agent trajectory
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估代理轨迹
- en: 'Complex agents require evaluation across three critical dimensions:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 复杂智能体需要在三个关键维度上进行评估：
- en: '**Final response evaluation**: Assess the ultimate output provided to the user
    (factual accuracy, helpfulness, quality, and safety)'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最终响应评估**：评估提供给用户的最终输出（事实准确性、有用性、质量和安全性）'
- en: '**Trajectory evaluation**: Examine the path the agent took to reach its conclusion'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**轨迹评估**：检查智能体达到结论所采取的路径'
- en: '**Single-step evaluation**: Analyze individual decision points in isolation'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单步评估**：单独分析决策点'
- en: While final response evaluation focuses on outcomes, trajectory evaluation examines
    the process itself. This approach is particularly valuable for complex agents
    that employ multiple tools, reasoning steps, or decision points to complete tasks.
    By evaluating the path taken, we can identify exactly where and how agents succeed
    or fail, even when the final answer is incorrect.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然最终响应评估侧重于结果，但轨迹评估则考察流程本身。这种方法对于使用多个工具、推理步骤或决策点来完成任务的复杂智能体尤其有价值。通过评估所采取的路径，我们可以精确地识别智能体成功或失败的确切位置和方式，即使最终答案是错误的。
- en: Trajectory evaluation compares the actual sequence of steps an agent took against
    an expected sequence, calculating a score based on how many expected steps were
    completed correctly. This gives partial credit to agents that follow some correct
    steps even if they don’t reach the right final answer.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 轨迹评估将智能体实际采取的步骤序列与预期序列进行比较，根据正确完成预期步骤的数量计算得分。即使智能体没有达到正确最终答案，也会对遵循一些正确步骤的智能体给予部分信用。
- en: 'Let’s implement a `custom trajectory` evaluator for a healthcare agent that
    responds to medication questions:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为响应药物问题的医疗智能体实现一个`自定义轨迹`评估器：
- en: '[PRE17]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Please remember to set your `LANGSMITH_API_KEY` environment variable! If you
    get a `Using legacy API key` error, you might need to generate a new API key from
    the LangSmith dashboard: [https://smith.langchain.com/settings](https://smith.langchain.com/settings).
    You always want to use the latest version of the LangSmith package.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住设置您的`LANGSMITH_API_KEY`环境变量！如果您遇到`使用旧版API密钥`错误，您可能需要从LangSmith仪表板生成新的API密钥：[https://smith.langchain.com/settings](https://smith.langchain.com/settings)。您始终希望使用LangSmith包的最新版本。
- en: 'To evaluate the agent’s trajectory, we need to capture the actual sequence
    of steps taken. With LangGraph, we can use streaming capabilities to record every
    node and tool invocation:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估智能体的轨迹，我们需要捕捉实际采取的步骤序列。使用LangGraph，我们可以利用流式处理能力来记录每个节点和工具调用：
- en: '[PRE19]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We can also analyze results on the dataset, which we can download from LangSmith:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以在数据集上分析结果，这些数据集我们可以从LangSmith下载：
- en: '[PRE21]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In this case, this is nonsensical, but this is to illustrate the idea.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，这是不合逻辑的，但这是为了说明这个想法。
- en: 'The following screenshot visually demonstrates what trajectory evaluation results
    look like in the LangSmith interface. It shows the perfect trajectory match score
    (**1.00**), which validates that the agent followed the expected path:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图直观地展示了在LangSmith界面中轨迹评估结果的外观。它显示了完美的轨迹匹配得分（**1.00**），这验证了智能体遵循了预期的路径：
- en: '![Figure 8.1: Trajectory evaluation in LangSmith](img/B32363_08_01.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![图8.1：LangSmith中的轨迹评估](img/B32363_08_01.png)'
- en: 'Figure 8.1: Trajectory evaluation in LangSmith'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1：LangSmith中的轨迹评估
- en: Please note that LangSmith displays the actual trajectory steps side by side
    with the reference trajectory and that it includes real execution metrics like
    latency and token usage.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，LangSmith将实际轨迹步骤与参考轨迹并排显示，并且它包括实际的执行指标，如延迟和令牌使用。
- en: 'Trajectory evaluation provides unique insights beyond simple pass/fail assessments:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 轨迹评估提供了超越简单通过/失败评估的独特见解：
- en: '**Identifying failure points**: Pinpoint exactly where agents deviate from
    expected paths'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**识别失败点**：精确地指出智能体偏离预期路径的位置'
- en: '**Process improvement**: Recognize when agents take unnecessary detours or
    inefficient routes'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流程改进**：识别智能体是否采取了不必要的绕路或不高效的路线'
- en: '**Tool usage patterns**: Understand how agents leverage available tools and
    when they make suboptimal choices'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工具使用模式**：了解智能体如何利用可用工具，以及它们何时做出次优选择'
- en: '**Reasoning quality**: Evaluate the agent’s decision-making process independent
    of final outcomes'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推理质量**：评估智能体的决策过程，独立于最终结果'
- en: For example, an agent might provide a correct medication dosage but reach it
    through an inappropriate trajectory (bypassing safety checks or using unreliable
    data sources). Trajectory evaluation reveals these process issues that outcome-focused
    evaluation would miss.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个代理可能提供了正确的药物剂量，但通过不适当的轨迹（绕过安全检查或使用不可靠的数据源）达到它。轨迹评估揭示了结果导向评估会错过的这些流程问题。
- en: Consider using trajectory evaluation in conjunction with other evaluation types
    for a holistic assessment of your agent’s performance. This approach is particularly
    valuable during development and debugging phases, where understanding the *why*
    behind agent behavior is as important as measuring final output quality.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑结合使用轨迹评估和其他评估类型，对代理的性能进行全面评估。这种方法在开发和调试阶段尤其有价值，在这些阶段，理解代理行为背后的“为什么”与衡量最终输出质量一样重要。
- en: By implementing continuous trajectory monitoring, you can track how agent behaviors
    evolve as you refine prompts, add tools, or modify the underlying model, ensuring
    improvements in one area don’t cause regressions in the agent’s overall decision-making
    process.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 通过实施连续轨迹监控，您可以跟踪随着您细化提示、添加工具或修改底层模型，代理行为如何演变，确保一个领域的改进不会导致代理整体决策过程的退化。
- en: Evaluating CoT reasoning
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估CoT推理
- en: Now suppose we want to evaluate the agent’s reasoning. For example, going back
    to our earlier example, the agent must not only answer “What is the current interest
    rate?” but also provide reasoning behind its answer. We can use the `COT_QA` evaluator
    for chain-of-thought evaluation.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设我们想评估代理的推理。例如，回到我们之前的例子，代理不仅必须回答“当前利率是多少？”还必须提供其答案背后的推理。我们可以使用`COT_QA`评估器进行思维链评估。
- en: '[PRE22]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The returned score and reasoning allow us to judge whether the agent’s thought
    process is sound and comprehensive:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的分数和推理使我们能够判断代理的思维过程是否合理和全面：
- en: '[PRE24]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Please note that in this evaluation, the agent provides detailed reasoning along
    with its answer. The evaluator (using chain-of-thought evaluation) compares the
    agent’s reasoning with an expected explanation.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在此评估中，代理在提供答案的同时提供详细的推理。评估者（使用思维链评估）将代理的推理与预期的解释进行比较。
- en: Offline evaluation
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 离线评估
- en: Offline evaluation involves assessing the agent’s performance under controlled
    conditions before deployment. This includes benchmarking to establish general
    performance baselines and more targeted testing based on generated test cases.
    Offline evaluations provide key metrics, error analyses, and pass/fail summaries
    from controlled test scenarios, establishing baseline performance.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 离线评估涉及在部署前在受控条件下评估代理的性能。这包括基准测试以建立一般性能基线，以及基于生成的测试用例的更针对性测试。离线评估提供关键指标、错误分析和受控测试场景的通过/失败总结，建立基线性能。
- en: While human assessments are sometimes seen as the gold standard, they are hard
    to scale and require careful design to avoid bias from subjective preferences
    or authoritative tones. Benchmarking involves comparing the performance of LLMs
    against standardized tests or tasks. This helps identify the strengths and weaknesses
    of the models and guides further development and improvement.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然人类评估有时被视为黄金标准，但它们难以扩展，并且需要精心设计以避免来自主观偏好或权威语调的偏见。基准测试涉及将LLMs的性能与标准化测试或任务进行比较。这有助于识别模型的优点和缺点，并指导进一步的开发和改进。
- en: In the next section, we’ll discuss creating an effective evaluation dataset
    within the context of RAG system evaluation.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论在RAG系统评估的背景下创建有效的评估数据集。
- en: Evaluating RAG systems
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估RAG系统
- en: 'The dimensions of RAG evaluation discussed earlier (retrieval quality, contextual
    relevance, faithful generation, and information synthesis) provided a foundation
    for understanding how to measure RAG system effectiveness. Understanding failure
    patterns of RAG systems helps create more effective evaluation strategies. Barnett
    and colleagues in their 2024 paper *Seven Failure Points When Engineering a Retrieval
    Augmented Generation System* identified several distinct ways RAG systems fail
    in production environments:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 之前讨论的RAG评估维度（检索质量、上下文相关性、忠实生成和信息综合）为理解如何衡量RAG系统有效性提供了基础。了解RAG系统的失败模式有助于创建更有效的评估策略。Barnett及其同事在2024年的论文《在构建检索增强生成系统时七个失败点》中确定了RAG系统在生产环境中失败的一些不同方式：
- en: First, **missing content** **failures** occur when the system fails to retrieve
    relevant information that exists in the knowledge base. This might happen because
    of chunking strategies that split related information, embedding models that miss
    semantic connections, or content gaps in the knowledge base itself.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，**内容缺失** **失败**发生在系统未能检索出知识库中存在的相关信息时。这可能是由于分割相关信息的块化策略、遗漏语义连接的嵌入模型或知识库本身的内容差距导致的。
- en: Second, **ranking failures** happen when relevant documents exist but aren’t
    ranked highly enough to be included in the context window. This commonly stems
    from suboptimal embedding models, vocabulary mismatches between queries and documents,
    or poor chunking granularity.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，**排名失败**发生在相关文档存在但排名不够高以至于不能包含在上下文窗口中时。这通常源于次优嵌入模型、查询与文档之间的词汇不匹配或较差的块划分粒度。
- en: '**Context window limitations** create another failure mode when key information
    is spread across documents that exceed the model’s context limit. This forces
    difficult tradeoffs between including more documents and maintaining sufficient
    detail from each one.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文窗口限制**在关键信息分布在超过模型上下文限制的多个文档中时，又产生了一种故障模式。这迫使在包含更多文档和保持每个文档足够细节之间做出困难的权衡。'
- en: Perhaps most critically, **information extraction failures** occur when relevant
    information is retrieved but the LLM fails to properly synthesize it. This might
    happen due to ineffective prompting, complex information formats, or conflicting
    information across documents.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能最重要的是，**信息提取失败**发生在相关信息被检索出来但LLM未能正确综合它的情况下。这可能是由于无效的提示、复杂的信息格式或文档之间的冲突信息导致的。
- en: 'To effectively evaluate and address these specific failure modes, we need a
    structured and comprehensive evaluation approach. The following example demonstrates
    how to build a carefully designed evaluation dataset in LangSmith that allows
    for testing each of these failure patterns in the context of financial advisory
    systems. By creating realistic questions with expected answers and relevant metadata,
    we can systematically identify which failure modes most frequently affect our
    particular implementation:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效地评估和解决这些特定的故障模式，我们需要一个结构化和全面的评估方法。以下示例演示了如何在LangSmith中构建一个精心设计的评估数据集，该数据集允许在金融咨询服务背景下测试这些故障模式中的每一个。通过创建具有预期答案和相关信息元数据的真实问题，我们可以系统地识别哪些故障模式最频繁地影响我们的特定实现：
- en: '[PRE25]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This dataset structure serves multiple evaluation purposes. First, it identifies
    specific documents that should be retrieved, allowing evaluation of retrieval
    accuracy. It then defines key points that should appear in the response, enabling
    assessment of information extraction. Finally, it connects each example to testing
    objectives, making it easier to diagnose specific system capabilities.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据集结构服务于多个评估目的。首先，它确定了应该检索的具体文档，从而允许评估检索准确性。然后，它定义了应该出现在响应中的关键点，从而能够评估信息提取。最后，它将每个示例与测试目标相连接，使得诊断特定系统能力变得更加容易。
- en: When implementing this dataset in practice, organizations typically load these
    examples into evaluation platforms like LangSmith, allowing automated testing
    of their RAG systems. The results reveal specific patterns in system performance—perhaps
    strong retrieval but weak synthesis, or excellent performance on simple factual
    questions but struggles with complex perspective inquiries.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用此数据集时，组织通常将这些示例加载到评估平台如LangSmith中，以允许对他们的RAG系统进行自动化测试。结果揭示了系统性能中的特定模式——可能是强大的检索能力但合成能力较弱，或者简单事实问题的出色表现但复杂视角查询的困难。
- en: However, implementing effective RAG evaluation goes beyond simply creating datasets;
    it requires using diagnostic tools to pinpoint exactly where failures occur within
    the system pipeline. Drawing on research, these diagnostics identify specific
    failure modes, such as poor document ranking (information exists but isn’t prioritized)
    or poor context utilization (the agent ignores relevant retrieved documents).
    By diagnosing these issues, organizations gain actionable insights—for instance,
    consistent ranking failures might suggest implementing hybrid search, while context
    utilization problems could lead to refined prompting or structured outputs.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，实施有效的RAG评估不仅仅是创建数据集；它需要使用诊断工具来精确地确定系统管道中失败发生的具体位置。借鉴研究，这些诊断识别出特定的故障模式，例如较差的文档排名（信息存在但未优先考虑）或较差的上下文利用（代理忽略了相关检索到的文档）。通过诊断这些问题，组织可以获得可操作的见解——例如，一致的排名失败可能表明需要实施混合搜索，而上下文利用问题可能导致改进的提示或结构化输出。
- en: 'The ultimate goal of RAG evaluation is to drive continuous improvement. Organizations
    achieving the most success follow an iterative cycle: running comprehensive diagnostics
    to find specific failure patterns, prioritizing fixes based on their frequency
    and impact, implementing targeted changes, and then re-evaluating to measure the
    improvement. By systematically diagnosing issues and using those insights to iterate,
    teams can build more accurate and reliable RAG systems with fewer common errors.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: RAG评估的最终目标是推动持续改进。取得最大成功的组织遵循一个迭代周期：运行全面的诊断以找到特定的故障模式，根据其频率和影响优先处理修复，实施有针对性的更改，然后重新评估以衡量改进。通过系统地诊断问题并利用这些见解进行迭代，团队可以构建更准确、更可靠的RAG系统，并减少常见错误。
- en: In the next section, we’ll see how we can use **LangSmith**, a companion project
    for LangChain, to benchmark and evaluate our system’s performance on a dataset.
    Let’s step through an example!
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将看到如何使用**LangSmith**，LangChain的配套项目，来对数据集上的系统性能进行基准测试和评估。让我们通过一个示例来逐步操作！
- en: Evaluating a benchmark in LangSmith
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在LangSmith中评估基准
- en: As we’ve mentioned, comprehensive benchmarking and evaluation, including testing,
    are critical for safety, robustness, and intended behavior. LangSmith, despite
    being a platform designed for testing, debugging, monitoring, and improving LLM
    applications, offers tools for evaluation and dataset management. LangSmith integrates
    seamlessly with LangChain Benchmarks, providing a cohesive framework for developing
    and assessing LLM applications.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们提到的，全面的基准测试和评估，包括测试，对于安全性、鲁棒性和预期行为至关重要。LangSmith，尽管是一个旨在测试、调试、监控和改进LLM应用的平台，但提供了评估和数据集管理的工具。LangSmith与LangChain
    Benchmarks无缝集成，为开发和评估LLM应用提供了一个统一的框架。
- en: 'We can run evaluations against benchmark datasets in LangSmith, as we’ll see
    now. First, please make sure you create an account on LangSmith here: [https://smith.langchain.com/](https://smith.langchain.com/).'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在LangSmith中运行针对基准数据集的评估，正如我们接下来将要看到的。首先，请确保您在这里创建LangSmith的账户：[https://smith.langchain.com/](https://smith.langchain.com/)。
- en: 'You can obtain an API key and set it as `LANGCHAIN_API_KEY` in your environment.
    We can also set environment variables for project ID and tracing:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在环境中获取一个API密钥，并将其设置为`LANGCHAIN_API_KEY`。我们还可以设置项目ID和跟踪的环境变量：
- en: '[PRE27]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This configuration establishes a connection to LangSmith and directs all traces
    to a specific project. When no project ID is explicitly defined, LangChain logs
    against the default project. The `LANGCHAIN_TRACING_V2` flag enables the most
    recent version of LangSmith’s tracing capabilities.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 此配置建立了与LangSmith的连接，并将所有跟踪信息导向特定的项目。当没有明确定义项目ID时，LangChain将对默认项目进行日志记录。`LANGCHAIN_TRACING_V2`标志启用了LangSmith跟踪功能的最新版本。
- en: 'After configuring the environment, we can begin logging interactions with our
    LLM applications. Each interaction creates a traceable record in LangSmith:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在配置好环境后，我们可以开始记录与我们的LLM应用的交互。每次交互都会在LangSmith中创建一个可追踪的记录：
- en: '[PRE28]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: When this code executes, it performs a simple interaction with the ChatOpenAI
    model and automatically logs the request, response, and performance metrics to
    LangSmith. These logs appear in the LangSmith project dashboard at [https://smith.langchain.com/projects](https://smith.langchain.com/projects),
    allowing for detailed inspection of each interaction.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 当此代码执行时，它与ChatOpenAI模型进行简单交互，并自动将请求、响应和性能指标记录到LangSmith。这些日志出现在LangSmith项目仪表板中，网址为[https://smith.langchain.com/projects](https://smith.langchain.com/projects)，允许对每个交互进行详细检查。
- en: 'We can create a dataset from existing agent runs with the `create_example_from_run()`
    function—or from anything else. Here’s how to create a dataset with a set of questions:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`create_example_from_run()`函数从现有的代理运行中创建一个数据集，或者从任何其他东西中创建。以下是如何使用一组问题创建数据集的方法：
- en: '[PRE29]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: This code creates a new evaluation dataset in LangSmith containing financial
    advisory questions. Each example includes an input query and an expected output
    answer, establishing a reference standard against which we can evaluate our LLM
    application responses.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码在LangSmith中创建一个新的评估数据集，包含财务咨询问题。每个示例都包括一个输入查询和一个预期的输出答案，建立了一个参考标准，我们可以据此评估我们的LLM应用响应。
- en: 'We can now define our RAG system with a function like this:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用类似这样的函数来定义我们的RAG系统：
- en: '[PRE30]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: In a complete implementation, you would prepare a vector store with relevant
    financial documents, create appropriate prompt templates, and configure the retrieval
    and response generation components. The concepts and techniques for building robust
    RAG systems are covered extensively in [*Chapter 4*](E_Chapter_4.xhtml#_idTextAnchor152),
    which provides step-by-step guidance on document processing, embedding creation,
    vector store setup, and chain construction.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在完整的实现中，你会准备一个包含相关财务文件的向量存储，创建适当的提示模板，并配置检索和响应生成组件。构建健壮RAG系统的概念和技术在[*第4章*](E_Chapter_4.xhtml#_idTextAnchor152)中进行了广泛讨论，该章节提供了关于文档处理、嵌入创建、向量存储设置和链构建的逐步指导。
- en: We can make changes to our chain and evaluate changes in the application. Does
    the change improve the result or not? Changes can be in any part of our application,
    be it a new model, a new prompt template, or a new chain or agent. We can run
    two versions of the application with the same input examples and save the results
    of the runs. Then we evaluate the results by comparing them side by side.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以对链进行更改，并在应用中评估更改。这个更改是否改善了结果？更改可以出现在我们应用的任何部分，无论是新模型、新提示模板、新链或新代理。我们可以用相同的输入示例运行两个版本的应用，并保存运行的结果。然后我们通过并排比较来评估结果。
- en: 'To run an evaluation on a dataset, we can either specify an LLM or—for parallelism—use
    a constructor function to initialize the model or LLM app for each input. Now,
    to evaluate the performance against our dataset, we need to define an evaluator
    as we saw in the previous section:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 要在数据集上运行评估，我们可以指定一个LLM，或者为了并行处理，使用构造函数为每个输入初始化模型或LLM应用。现在，为了评估与我们的数据集的性能，我们需要定义一个评估器，就像我们在上一节中看到的那样：
- en: '[PRE31]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This shows how to configure multi-dimensional evaluation for RAG systems, assessing
    factual accuracy, groundedness, and retrieval quality using LLM-based judges.
    The criteria are defined by a dictionary that includes a criterion as a key and
    a question to check for as the value.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 这展示了如何为RAG系统配置多维度评估，使用基于LLM的评委来评估事实准确性、扎根性和检索质量。标准由一个字典定义，其中标准作为键，检查问题的问句作为值。
- en: 'We’ll now pass a dataset together with the evaluation configuration with evaluators
    to `run_on_dataset()` to generate metrics and feedback:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将数据集以及评估配置和评估器一起传递给`run_on_dataset()`以生成指标和反馈：
- en: '[PRE33]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: In the same way, we could pass a dataset and evaluators to `run_on_dataset()`
    to generate metrics and feedback asynchronously.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以将数据集和评估器传递给`run_on_dataset()`以异步生成指标和反馈。
- en: This practical implementation provides a framework you can adapt for your specific
    domain. By creating a comprehensive evaluation dataset and assessing your RAG
    system across multiple dimensions (correctness, groundedness, and retrieval quality),
    you can identify specific areas for improvement and track progress as you refine
    your system.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这种实际实现提供了一个你可以根据特定领域进行调整的框架。通过创建一个全面的评估数据集，并在多个维度（正确性、扎根性和检索质量）上评估你的RAG系统，你可以确定具体的改进领域，并在你改进系统时跟踪进度。
- en: When implementing this approach, consider incorporating real user queries from
    your application logs (appropriately anonymized) to ensure your evaluation dataset
    reflects actual usage patterns. Additionally, periodically refreshing your dataset
    with new queries and updated information helps prevent overfitting and ensures
    your evaluation remains relevant as user needs evolve.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现这种方法时，考虑将来自你应用日志的真实用户查询（适当匿名化）纳入其中，以确保你的评估数据集反映了实际的用法模式。此外，定期用新的查询和更新信息刷新你的数据集有助于防止过拟合，并确保你的评估随着用户需求的变化而保持相关。
- en: Let’s use the datasets and evaluate libraries by HuggingFace to check a coding
    LLM approach to solving programming problems.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用HuggingFace的数据集和评估库来检查编码LLM解决编程问题的方法。
- en: Evaluating a benchmark with HF datasets and Evaluate
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用HF数据集和Evaluate评估基准
- en: 'As a reminder: the `pass@k` metric is a way to evaluate the performance of
    an LLM in solving programming exercises. It measures the proportion of exercises
    for which the LLM generated at least one correct solution within the top `k` candidates.
    A higher `pass@k` score indicates better performance, as it means the LLM was
    able to generate a correct solution more often within the top `k` candidates.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 提醒一下：`pass@k`指标是评估LLM在解决编程练习中性能的一种方式。它衡量了LLM在至少一个正确解决方案在排名前`k`的候选人中生成的情况下，这些练习的比例。更高的`pass@k`分数表示更好的性能，因为它意味着LLM能够在排名前`k`的候选人中更频繁地生成正确解决方案。
- en: 'Hugging Face’s `Evaluate` library makes it very easy to calculate `pass@k`
    and other metrics. Here’s an example:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face的`Evaluate`库使得计算`pass@k`和其他指标变得非常容易。以下是一个示例：
- en: '[PRE34]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We should get an output like this:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该得到如下输出：
- en: '[PRE35]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'For this code to run, you need to set the `HF_ALLOW_CODE_EVAL` environment
    variable to 1\. Please be cautious: running LLM code on your machine comes with
    a risk.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 要使此代码运行，您需要将`HF_ALLOW_CODE_EVAL`环境变量设置为1。请谨慎行事：在您的机器上运行LLM代码存在风险。
- en: This shows how to evaluate code generation models using HuggingFace’s `code_eval`
    metric, which measures a model’s ability to produce functioning code solutions.
    This is great. Let’s see another example.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 这展示了如何使用HuggingFace的`code_eval`指标评估代码生成模型，该指标衡量模型产生功能代码解决方案的能力。这很好。让我们看看另一个示例。
- en: Evaluating email extraction
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估电子邮件提取
- en: Let’s show how we can use it to evaluate an LLM’s ability to extract structured
    information from insurance claim texts.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们展示如何使用它来评估LLM从保险索赔文本中提取结构化信息的能力。
- en: We’ll first create a synthetic dataset using LangSmith. In this synthetic dataset,
    each example consists of a raw insurance claim text (input) and its corresponding
    expected structured output (output). We will use this dataset to run extraction
    chains and evaluate your model’s performance.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将使用LangSmith创建一个合成数据集。在这个合成数据集中，每个示例由一个原始保险索赔文本（输入）及其对应的预期结构化输出（输出）组成。我们将使用此数据集运行提取链并评估您的模型性能。
- en: We assume that you’ve already set up your LangSmith credentials.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设您已经设置了您的LangSmith凭证。
- en: '[PRE36]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We can upload this dataset to LangSmith:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将此数据集上传到LangSmith：
- en: '[PRE38]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Now let’s run our `InsuranceClaim` dataset on LangSmith. We’ll first define
    a schema for our claims:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在LangSmith上运行我们的`InsuranceClaim`数据集。我们首先为我们的索赔定义一个模式：
- en: '[PRE39]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Now we’ll define our extraction chain. We are keeping it very simple; we’ll
    just ask for a JSON object that follows the `InsuranceClaim` schema. The extraction
    chain is defined with ChatOpenAI LLM with function calling bound to our schema:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将定义我们的提取链。我们将其保持得非常简单；我们只需请求一个遵循`InsuranceClaim`模式的JSON对象。提取链是通过ChatOpenAI
    LLM定义的，函数调用绑定到我们的模式上：
- en: '[PRE41]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Finally, we can run the extraction chain on our sample insurance claim:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以在我们的样本保险索赔上运行提取链：
- en: '[PRE42]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: This showed how to evaluate structured information extraction from insurance
    claims text, using a Pydantic schema to standardize extraction and LangSmith to
    assess performance.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 这展示了如何使用Pydantic模式标准化提取和LangSmith评估性能，来评估从保险索赔文本中提取结构化信息。
- en: Summary
  id: totrans-267
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we outlined critical strategies for evaluating LLM applications,
    ensuring robust performance before production deployment. We provided an overview
    of the importance of evaluation, architectural challenges, evaluation strategies,
    and types of evaluation. We then demonstrated practical evaluation techniques
    through code examples, including correctness evaluation using exact matches and
    LLM-as-a-judge approaches. For instance, we showed how to implement the `ExactMatchStringEvaluator`
    for comparing answers about Federal Reserve interest rates, and how to use `ScoreStringEvalChain`
    for more nuanced evaluations. The examples also covered JSON format validation
    using `JsonValidityEvaluator` and assessment of agent trajectories in healthcare
    scenarios.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们概述了评估LLM应用的关键策略，确保在生产部署前性能稳健。我们提供了评估的重要性、架构挑战、评估策略和评估类型的概述。然后，我们通过代码示例展示了实际的评估技术，包括使用精确匹配和LLM作为裁判方法的正确性评估。例如，我们展示了如何实现`ExactMatchStringEvaluator`来比较关于联邦储备利率的答案，以及如何使用`ScoreStringEvalChain`进行更细致的评估。示例还涵盖了使用`JsonValidityEvaluator`进行JSON格式验证以及评估医疗场景中代理轨迹的方法。
- en: Tools like LangChain provide predefined evaluators for criteria such as conciseness
    and relevance, while platforms like LangSmith enable comprehensive testing and
    monitoring. The chapter presented code examples using LangSmith to create and
    evaluate datasets, demonstrating how to assess model performance across multiple
    criteria. The implementation of `pass@k` metrics using Hugging Face’s Evaluate
    library was shown for assessing code generation capabilities. We also walked through
    an example of evaluating insurance claim text extraction using structured schemas
    and LangChain’s evaluation capabilities.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于LangChain这样的工具为简洁性和相关性等标准提供预定义的评估器，而像LangSmith这样的平台则允许进行全面的测试和监控。本章展示了使用LangSmith创建和评估数据集的代码示例，展示了如何根据多个标准评估模型性能。展示了使用Hugging
    Face的Evaluate库实现`pass@k`指标的方法，用于评估代码生成能力。我们还通过使用结构化模式和LangChain的评估能力，展示了评估保险索赔文本提取的示例。
- en: Now that we’ve evaluated our AI workflows, in the next chapter we’ll look at
    how we can deploy and monitor them. Let’s discuss deployment and observability!
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经评估了我们的AI工作流程，在下一章中，我们将探讨如何部署和监控它们。让我们来讨论部署和可观察性！
- en: Questions
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Describe three key metrics used in evaluating AI agents.
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 描述在评估AI代理时使用的三个关键指标。
- en: What’s the difference between online and offline evaluation?
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在线评估和离线评估有何区别？
- en: What are system-level and application-level evaluations and how do they differ?
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 系统级和应用级评估是什么，它们之间有何区别？
- en: How can LangSmith be used to compare different versions of an LLM application?
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何使用LangSmith比较LLM应用的不同版本？
- en: How does chain-of-thought evaluation differ from traditional output evaluation?
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 思维链评估与传统输出评估有何不同？
- en: Why is trajectory evaluation important for understanding agent behavior?
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么轨迹评估对于理解代理行为很重要？
- en: What are the key considerations when evaluating LLM agents for production deployment?
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估LLM代理进行生产部署时，有哪些关键考虑因素？
- en: How can bias be mitigated when using language models as evaluators?
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在使用语言模型作为评估器时，如何减轻偏差？
- en: What role do standardized benchmarks play, and how can we create benchmark datasets
    for LLM agent evaluation?
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标准化基准在评估LLM代理中扮演什么角色，我们如何为LLM代理评估创建基准数据集？
- en: How do you balance automated evaluation metrics with human evaluation in production
    systems?
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何在生产系统中平衡自动化评估指标与人工评估？
