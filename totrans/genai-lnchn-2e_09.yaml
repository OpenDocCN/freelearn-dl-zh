- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Evaluation and Testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we’ve discussed so far in this book, LLM agents and systems have diverse
    applications across industries. However, taking these complex neural network systems
    from research to real-world deployment comes with significant challenges and necessitates
    robust evaluation strategies and testing methodologies.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating LLM agents and apps in LangChain comes with new methods and metrics
    that can help ensure optimized, reliable, and ethically sound outcomes. This chapter
    delves into the intricacies of evaluating LLM agents, covering system-level evaluation,
    evaluation-driven design, offline and online evaluation methods, and practical
    examples with Python code.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will have a comprehensive understanding of
    how to evaluate LLM agents and ensure their alignment with intended goals and
    governance requirements. In all, this chapter will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Why evaluations matter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'What we evaluate: core agent capabilities'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'How we evaluate: methodologies and approaches'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating LLM agents in practice
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Offline evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can find the code for this chapter in the `chapter8/` directory of the book’s
    GitHub repository. Given the rapid developments in the field and the updates to
    the LangChain library, we are committed to keeping the GitHub repository current.
    Please visit [https://github.com/benman1/generative_ai_with_langchain](https://github.com/benman1/generative_ai_with_langchain)
    for the latest updates.
  prefs: []
  type: TYPE_NORMAL
- en: See [*Chapter 2*](E_Chapter_2.xhtml#_idTextAnchor044) for setup instructions.
    If you have any questions or encounter issues while running the code, please create
    an issue on GitHub or join the discussion on Discord at [https://packt.link/lang](https://packt.link/lang).
  prefs: []
  type: TYPE_NORMAL
- en: In the realm of developing LLM agents, evaluations play a pivotal role in ensuring
    these complex systems function reliably and effectively across real-world applications.
    Let’s start discussing why rigorous evaluation is indispensable!
  prefs: []
  type: TYPE_NORMAL
- en: Why evaluation matters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LLM agents represent a new class of AI systems that combine language models
    with reasoning, decision-making, and tool-using capabilities. Unlike traditional
    software with predictable behaviors, these agents operate with greater autonomy
    and complexity, making thorough evaluation essential before deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the real-world consequences: unlike traditional software with deterministic
    behavior, LLM agents make complex, context-dependent decisions. If unevaluated
    before being implemented, an AI agent in customer support might provide misleading
    information that damages brand reputation, while a healthcare assistant could
    influence critical treatment decisions—highlighting why thorough evaluation is
    essential.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before diving into specific evaluation techniques, it’s important to distinguish
    between two fundamentally different types of evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**LLM model evaluation:**'
  prefs: []
  type: TYPE_NORMAL
- en: Focuses on the raw capabilities of the base language model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uses controlled prompts and standardized benchmarks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluates inherent abilities like reasoning, knowledge recall, and language
    generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Typically conducted by model developers or researchers comparing different models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LLM system/application evaluation:**'
  prefs: []
  type: TYPE_NORMAL
- en: Assesses the complete application that includes the LLM plus additional components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examines real-world performance with actual user queries and scenarios
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluates how components work together (retrieval, tools, memory, etc.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measures end-to-end effectiveness at solving user problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While both types of evaluation are important, this chapter focuses on system-level
    evaluation, as practitioners building LLM agents with LangChain are concerned
    with overall application performance rather than comparing base models. A weaker
    base model with excellent prompt engineering and system design might outperform
    a stronger model with poor integration in real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: Safety and alignment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Alignment in the context of LLMs has a dual meaning: as a process, referring
    to the post-training techniques used to ensure that models behave according to
    human expectations and values; and as an outcome, measuring the degree to which
    a model’s behavior conforms to intended human values and safety guidelines. Unlike
    task-related performance which focuses on accuracy and completeness, alignment
    addresses the fundamental calibration of the system to human behavioral standards.
    While fine-tuning improves a model’s performance on specific tasks, alignment
    specifically targets ethical behavior, safety, and reduction of harmful outputs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This distinction is crucial because a model can be highly capable (well fine-tuned)
    but poorly aligned, creating sophisticated outputs that violate ethical norms
    or safety guidelines. Conversely, a model can be well-aligned but lack task-specific
    capabilities in certain domains. Alignment with human values is fundamental to
    responsible AI deployment. Evaluation must verify that agents align with human
    expectations across multiple dimensions: factual accuracy in sensitive domains,
    ethical boundary recognition, safety in responses, and value consistency.'
  prefs: []
  type: TYPE_NORMAL
- en: Alignment evaluation methods must be tailored to domain-specific concerns. In
    financial services, alignment evaluation focuses on regulatory compliance with
    frameworks like GDPR and the EU AI Act, particularly regarding automated decision-making.
    Financial institutions must evaluate bias in fraud detection systems, implement
    appropriate human oversight mechanisms, and document these processes to satisfy
    regulatory requirements. In retail environments, alignment evaluation centers
    on ethical personalization practices, balancing recommendation relevance with
    customer privacy concerns and ensuring transparent data usage policies when generating
    personalized content.
  prefs: []
  type: TYPE_NORMAL
- en: Manufacturing contexts require alignment evaluation focused on safety parameters
    and operational boundaries. AI systems must recognize potentially dangerous operations,
    maintain appropriate human intervention protocols for quality control, and adhere
    to industry safety standards. Alignment evaluation includes testing whether predictive
    maintenance systems appropriately escalate critical safety issues to human technicians
    rather than autonomously deciding maintenance schedules for safety-critical equipment.
  prefs: []
  type: TYPE_NORMAL
- en: In educational settings, alignment evaluation must consider developmental appropriateness
    across student age groups, fair assessment standards across diverse student populations,
    and appropriate transparency levels. Educational AI systems require evaluation
    of their ability to provide balanced perspectives on complex topics, avoid reinforcing
    stereotypes in learning examples, and appropriately defer to human educators on
    sensitive or nuanced issues. These domain-specific alignment evaluations are essential
    for ensuring AI systems not only perform well technically but also operate within
    appropriate ethical and safety boundaries for their application context.
  prefs: []
  type: TYPE_NORMAL
- en: Performance and efficiency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Like early challenges in software testing that were resolved through standardized
    practices, agent evaluations face similar hurdles. These include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Overfitting**: Where systems perform well only on test data but not in real-world
    situations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gaming benchmarks**: Optimizing for specific test scenarios rather than general
    performance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Insufficient diversity in evaluation datasets**: Failing to test performance
    across the breadth of real-world situations the system will encounter, including
    edge cases and unexpected inputs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Drawing lessons from software testing and other domains, comprehensive evaluation
    frameworks need to measure not only the accuracy but also the scalability, resource
    utilization, and safety of LLM agents.
  prefs: []
  type: TYPE_NORMAL
- en: '*Performance evaluation* determines whether agents can reliably achieve their
    intended goals, including:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Accuracy** in task completion across varied scenarios'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Robustness** when handling novel inputs that differ from evaluation examples'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resistance** to adversarial inputs or manipulation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resource efficiency** in computational and operational costs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rigorous evaluations identify potential failure modes and risks in diverse real-world
    scenarios, as evidenced by modern benchmarks and contests. Ensuring an agent can
    operate safely and reliably across variations in real-world conditions is paramount.
    Evaluation strategies and methodologies continue to evolve, enhancing agent design
    effectiveness through iterative improvement.
  prefs: []
  type: TYPE_NORMAL
- en: Effective evaluations prevent the adoption of unnecessarily complex and costly
    solutions by balancing accuracy with resource efficiency. For example, the DSPy
    framework optimizes both cost and task performance, highlighting how evaluations
    can guide resource-effective solutions. LLM agents benefit from similar optimization
    strategies, ensuring their computational demands align with their benefits.
  prefs: []
  type: TYPE_NORMAL
- en: User and stakeholder value
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Evaluations help quantify the actual impact of LLM agents in practical settings.
    During the COVID-19 pandemic, the WHO’s implementation of screening chatbots demonstrated
    how AI could achieve meaningful practical outcomes, evaluated through metrics
    like user adherence and information quality. In financial services, JPMorgan Chase’s
    COIN (Contract Intelligence) platform for reviewing legal documents showcased
    value by reducing 360,000 hours of manual review work annually, with evaluations
    focusing on accuracy rates and cost savings compared to traditional methods. Similarly,
    Sephora’s Beauty Bot demonstrated retail value through increased conversion rates
    (6% higher than traditional channels) and higher average order values, proving
    stakeholder value across multiple dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: User experience is a cornerstone of successful AI deployment. Systems like Alexa
    and Siri undergo rigorous evaluations for ease of use and engagement, which inform
    design improvements. Similarly, assessing user interaction with LLM agents helps
    refine interfaces and ensures the agents meet or exceed user expectations, thereby
    improving overall satisfaction and adoption rates.
  prefs: []
  type: TYPE_NORMAL
- en: A critical aspect of modern AI systems includes understanding how human interventions
    affect outcomes. In healthcare settings, evaluations show how human feedback enhances
    the performance of chatbots in therapeutic contexts. In manufacturing, a predictive
    maintenance LLM agent deployed at a major automotive manufacturer demonstrated
    value through reduced downtime (22% improvement), extended equipment lifespan,
    and positive feedback from maintenance technicians about the system’s interpretability
    and usefulness. For LLM agents, incorporating human oversight in evaluations reveals
    insights into decision-making processes and highlights both strengths and areas
    needing improvement.
  prefs: []
  type: TYPE_NORMAL
- en: Comprehensive agent evaluation requires addressing the distinct perspectives
    and priorities of multiple stakeholders across the agent lifecycle. The evaluation
    methods deployed should reflect this diversity, with metrics tailored to each
    group’s primary concerns.
  prefs: []
  type: TYPE_NORMAL
- en: End users evaluate agents primarily through the lens of practical task completion
    and interaction quality. Their assessment revolves around the agent’s ability
    to understand and fulfill requests accurately (task success rate), respond with
    relevant information (answer relevancy), maintain conversation coherence, and
    operate with reasonable speed (response time). This group values satisfaction
    metrics most highly, with user satisfaction scores and communication efficiency
    being particularly important in conversational contexts. In application-specific
    domains like web navigation or software engineering, users may prioritize domain-specific
    success metrics—such as whether an e-commerce agent successfully completed a purchase
    or a coding agent resolved a software issue correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Technical stakeholders require a deeper evaluation of the agent’s internal processes
    rather than just outcomes. They focus on the quality of planning (plan feasibility,
    plan optimality), reasoning coherence, tool selection accuracy, and adherence
    to technical constraints. For SWE agents, metrics like code correctness and test
    case passing rate are critical. Technical teams also closely monitor computational
    efficiency metrics such as token consumption, latency, and resource utilization,
    as these directly impact operating costs and scalability. Their evaluation extends
    to the agent’s robustness—measuring how it handles edge cases, recovers from errors,
    and performs under varying loads.
  prefs: []
  type: TYPE_NORMAL
- en: 'Business stakeholders evaluate agents through metrics connecting directly to
    organizational value. Beyond basic ROI calculations, they track domain-specific
    KPIs that demonstrate tangible impact: reduced call center volume for customer
    service agents, improved inventory accuracy for retail applications, or decreased
    downtime for manufacturing agents. Their evaluation framework includes the agent’s
    alignment with strategic goals, competitive differentiation, and scalability across
    the organization. In sectors like finance, metrics bridging technical performance
    to business outcomes—such as reduced fraud losses while maintaining customer convenience—are
    especially valuable.'
  prefs: []
  type: TYPE_NORMAL
- en: Regulatory stakeholders, particularly in high-stakes domains like healthcare,
    finance, and legal services, evaluate agents through strict compliance and safety
    lenses. Their assessment encompasses the agent’s adherence to domain-specific
    regulations (like HIPAA in healthcare or financial regulations in banking), bias
    detection measures, robustness against adversarial inputs, and comprehensive documentation
    of decision processes. For these stakeholders, the thoroughness of safety testing
    and the agent’s consistent performance within defined guardrails outweigh pure
    efficiency or capability metrics. As autonomous agents gain wider deployment,
    this regulatory evaluation dimension becomes increasingly crucial to ensure ethical
    operation and minimize potential harm.
  prefs: []
  type: TYPE_NORMAL
- en: For organizational decision-makers, evaluations should include cost-benefit
    analyses, especially important at the deployment stage. In healthcare, comparing
    the costs and benefits of AI interventions versus traditional methods ensures
    economic viability. Similarly, evaluating the financial sustainability of LLM
    agent deployments involves analyzing operational costs against achieved efficiencies,
    ensuring scalability without sacrificing effectiveness.
  prefs: []
  type: TYPE_NORMAL
- en: Building consensus for LLM evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Evaluating LLM agents presents a significant challenge due to their open-ended
    nature and the subjective, context-dependent definition of *good* performance.
    Unlike traditional software with clear-cut metrics, LLMs can be convincingly wrong,
    and human judgment on their quality varies. This necessitates an evaluation strategy
    centered on building organizational consensus.
  prefs: []
  type: TYPE_NORMAL
- en: The foundation of effective evaluation lies in prioritizing user outcomes. Instead
    of starting with technical metrics, developers should identify what constitutes
    success from the user’s perspective, understanding the value the agent should
    deliver and the potential risks. This outcomes-based approach ensures evaluation
    priorities align with real-world impact.
  prefs: []
  type: TYPE_NORMAL
- en: Addressing the subjective nature of LLM evaluation requires establishing robust
    evaluation governance. This involves creating cross-functional working groups
    comprising technical experts, domain specialists, and user representatives to
    define and document formalized evaluation criteria. Clear ownership of different
    evaluation dimensions and decision-making frameworks for resolving disagreements
    is crucial. Maintaining version control for evaluation standards ensures transparency
    as understanding evolves.
  prefs: []
  type: TYPE_NORMAL
- en: In organizational contexts, balancing diverse stakeholder perspectives is key.
    Evaluation frameworks must accommodate technical performance metrics, domain-specific
    accuracy, and user-centric helpfulness. Effective governance facilitates this
    balance through mechanisms like weighted scoring systems and regular cross-functional
    reviews, ensuring all viewpoints are considered.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, evaluation governance serves as a mechanism for organizational learning.
    Well-structured frameworks help identify specific failure modes, provide actionable
    insights for development, enable quantitative comparisons between system versions,
    and support continuous improvement through integrated feedback loops. Establishing
    a “model governance committee” with representatives from all stakeholder groups
    can help review results, resolve disputes, and guide deployment decisions. Documenting
    not just results but the discussions around them captures valuable insights into
    user needs and system limitations.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, rigorous and well-governed evaluation is an integral part of
    the LLM agent development lifecycle. By implementing structured frameworks that
    consider technical performance, user value, and organizational alignment, teams
    can ensure these systems deliver benefits effectively while mitigating risks.
    The subsequent sections will delve into evaluation methodologies, including concrete
    examples relevant to developers working with tools like LangChain.
  prefs: []
  type: TYPE_NORMAL
- en: Building on the foundational principles of LLM agent evaluation and the importance
    of establishing robust governance, we now turn to the practical realities of assessment.
    Developing reliable agents requires a clear understanding of what aspects of their
    behavior need to be measured and how to apply effective techniques to quantify
    their performance. The upcoming sections provide a detailed guide on the *what*
    and *how* of evaluating LLM agents, breaking down the core capabilities you should
    focus on and the diverse methodologies you can employ to build a comprehensive
    evaluation framework for your applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'What we evaluate: core agent capabilities'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the most fundamental level, an LLM agent’s value is tied directly to its
    ability to successfully accomplish the tasks it was designed for. If an agent
    cannot reliably complete its core function, its utility is severely limited, regardless
    of how sophisticated its underlying model or tools are. Therefore, this task performance
    evaluation forms the cornerstone of agent assessment. In the next subsection,
    we’ll explore the nuances of measuring task success, looking at considerations
    relevant to assessing how effectively your agent executes its primary functions
    in real-world scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Task performance evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Task performance forms the foundation of agent evaluation, measuring how effectively
    an agent accomplishes its intended goals. Successful agents demonstrate high task
    completion rates while producing relevant, factually accurate responses that directly
    address user requirements. When evaluating task performance, organizations typically
    assess both the correctness of the final output and the efficiency of the process
    used to achieve it.
  prefs: []
  type: TYPE_NORMAL
- en: TaskBench (Shen and colleagues., 2023) and AgentBench (Liu and colleagues, 2023)
    provide standardized multi-stage evaluations of LLM-powered agents. TaskBench
    divides tasks into decomposition, tool selection, and parameter prediction, then
    reports that models like GPT-4 exceed 80% success on single-tool invocations but
    drop to around 50% on end-to-end task automation. AgentBench’s eight interactive
    environments likewise show top proprietary models vastly outperform smaller open-source
    ones, underscoring cross-domain generalization challenges.
  prefs: []
  type: TYPE_NORMAL
- en: Financial services applications demonstrate task performance evaluation in practice,
    though we should view industry-reported metrics with appropriate skepticism. While
    many institutions claim high accuracy rates for document analysis systems, independent
    academic assessments have documented significantly lower performance in realistic
    conditions. A particularly important dimension in regulated industries is an agent’s
    ability to correctly identify instances where it lacks sufficient information—a
    critical safety feature that requires specific evaluation protocols beyond simple
    accuracy measurement.
  prefs: []
  type: TYPE_NORMAL
- en: Tool usage evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Tool usage capability—an agent’s ability to select, configure, and leverage
    external systems—has emerged as a crucial evaluation dimension that distinguishes
    advanced agents from simple question-answering systems. Effective tool usage evaluation
    encompasses multiple aspects: the agent’s ability to select the appropriate tool
    for a given subtask, provide correct parameters, interpret tool outputs correctly,
    and integrate these outputs into a coherent solution strategy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The T-Eval framework, developed by Liu and colleagues (2023), decomposes tool
    usage into distinct measurable capabilities: planning the sequence of tool calls,
    reasoning about the next steps, retrieving the correct tool from available options,
    understanding tool documentation, correctly formatting API calls, and reviewing
    responses to determine if goals were met. This granular approach allows organizations
    to identify specific weaknesses in their agent’s tool-handling capabilities rather
    than simply observing overall failures.'
  prefs: []
  type: TYPE_NORMAL
- en: Recent benchmarks like ToolBench and ToolSandbox demonstrate that even state-of-the-art
    agents struggle with tool usage in dynamic environments. In production systems,
    evaluation increasingly focuses on efficiency metrics alongside basic correctness—measuring
    whether agents avoid redundant tool calls, minimize unnecessary API usage, and
    select the most direct path to solve user problems. While industry implementations
    often claim significant efficiency improvements, peer-reviewed research suggests
    more modest gains, with optimized tool selection typically reducing computation
    costs by 15-20% in controlled studies while maintaining outcome quality.
  prefs: []
  type: TYPE_NORMAL
- en: RAG evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'RAG system evaluation represents a specialized but crucial area of agent assessment,
    focusing on how effectively agents retrieve and incorporate external knowledge.
    Four key dimensions form the foundation of comprehensive RAG evaluation: retrieval
    quality, contextual relevance, faithful generation, and information synthesis.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Retrieval quality* measures how well the system finds the most appropriate
    information from its knowledge base. Rather than using simple relevance scores,
    modern evaluation approaches assess retrieval through precision and recall at
    different ranks, considering both the absolute relevance of retrieved documents
    and their coverage of the information needed to answer user queries. Academic
    research has developed standardized test collections with expert annotations to
    enable systematic comparison across different retrieval methodologies.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Contextual relevance*, on the other hand, examines how precisely the retrieved
    information matches the specific information need expressed in the query. This
    involves evaluating whether the system can distinguish between superficially similar
    but contextually different information requests. Recent research has developed
    specialized evaluation methodologies for testing disambiguation capabilities in
    financial contexts, where similar terminology might apply to fundamentally different
    products or regulations. These approaches specifically measure how well retrieval
    systems can distinguish between queries that use similar language but have distinct
    informational needs.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Faithful generation*—the degree to which the agent’s responses accurately
    reflect the retrieved information without fabricating details—represents perhaps
    the most critical aspect of RAG evaluation. Recent studies have found that even
    well-optimized RAG systems still show non-trivial hallucination rates, between
    3-15% on complex domains, highlighting the ongoing challenge in this area. Researchers
    have developed various evaluation protocols for faithfulness, including source
    attribution tests and contradiction detection mechanisms that systematically compare
    generated content with the retrieved source material.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, *information synthesis* quality evaluates the agent’s ability to integrate
    information from multiple sources into coherent, well-structured responses. Rather
    than simply concatenating or paraphrasing individual documents, advanced agents
    must reconcile potentially conflicting information, present balanced perspectives,
    and organize content logically. Evaluation here extends beyond automated metrics
    to include expert assessment of how effectively the agent has synthesized complex
    information into accessible, accurate summaries that maintain appropriate nuance.
  prefs: []
  type: TYPE_NORMAL
- en: Planning and reasoning evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Planning and reasoning capabilities form the cognitive foundation that enables
    agents to solve complex, multi-step problems that cannot be addressed through
    single operations. Evaluating these capabilities requires moving beyond simple
    input-output testing to assess the quality of the agent’s thought process and
    problem-solving strategy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Plan feasibility gauges whether every action in a proposed plan respects the
    domain’s preconditions and constraints. Using the PlanBench suite, Valmeekam and
    colleagues in their 2023 paper *PlanBench: An Extensible Benchmark for Evaluating
    Large Language Models on Planning and Reasoning about Change* showed that GPT-4
    correctly generates fully executable plans in only about 34% of classical IPC-style
    domains under zero-shot conditions—far below reliable thresholds and underscoring
    persistent failures to account for environment dynamics and logical preconditions.'
  prefs: []
  type: TYPE_NORMAL
- en: Plan optimality extends evaluation beyond basic feasibility to consider efficiency.
    This dimension assesses whether agents can identify not just any working solution
    but the most efficient approach to accomplishing their goals. The Recipe2Plan
    benchmark specifically evaluates this by testing whether agents can effectively
    multitask under time constraints, mirroring real-world efficiency requirements.
    Current state-of-the-art models show significant room for improvement, with published
    research indicating optimal planning rates between 45% and 55% for even the most
    capable systems.
  prefs: []
  type: TYPE_NORMAL
- en: Reasoning coherence evaluates the logical structure of the agent’s problem-solving
    approach—whether individual reasoning steps connect logically, whether conclusions
    follow from premises, and whether the agent maintains consistency throughout complex
    analyses. Unlike traditional software testing where only the final output matters,
    agent evaluation increasingly examines intermediate reasoning steps to identify
    failures in logical progression that might be masked by a correct final answer.
    Multiple academic studies have demonstrated the importance of this approach, with
    several research groups developing standardized methods for reasoning trace analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recent studies (*CoLadder: Supporting Programmers with Hierarchical Code Generation
    in Multi-Level Abstraction*, 2023, and *Generating a Low-code Complete Workflow
    via Task Decomposition and RAG*, 2024) show that decomposing code-generation tasks
    into smaller, well-defined subtasks—often using hierarchical or as-needed planning—leads
    to substantial gains in code quality, developer productivity, and system reliability
    across both benchmarks and live engineering settings.'
  prefs: []
  type: TYPE_NORMAL
- en: Building on the foundational principles of LLM agent evaluation and the importance
    of establishing robust governance, we now turn to the practical realities of assessment.
    Developing reliable agents requires a clear understanding of what aspects of their
    behavior need to be measured and how to apply effective techniques to quantify
    their performance.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying the core capabilities to evaluate is the first critical step. The
    next is determining how to effectively measure them, given the complexities and
    subjective aspects inherent in LLM agents compared to traditional software. Relying
    on a single metric or approach is insufficient. In the next subsection, we’ll
    explore the various methodologies and approaches available for evaluating agent
    performance in a robust, scalable, and insightful manner. We’ll cover the role
    of automated metrics for consistency, the necessity of human feedback for subjective
    assessment, the importance of system-level analysis for integrated agents, and
    how to combine these techniques into a practical evaluation framework that drives
    improvement.
  prefs: []
  type: TYPE_NORMAL
- en: 'How we evaluate: methodologies and approaches'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LLM agents, particularly those built with flexible frameworks like LangChain
    or LangGraph, are typically composed of different functional parts or *skills*.
    An agent’s overall performance isn’t a single monolithic metric; it’s the result
    of how well it executes these individual capabilities and how effectively they
    work together. In the following subsection, we’ll delve into these core capabilities
    that distinguish effective agents, outlining the specific dimensions we should
    assess to understand where our agent excels and where it might be failing.
  prefs: []
  type: TYPE_NORMAL
- en: Automated evaluation approaches
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Automated evaluation methods provide scalable, consistent assessment of agent
    capabilities, enabling systematic comparison across different versions or implementations.
    While no single metric can capture all aspects of agent performance, combining
    complementary approaches allows for comprehensive automated evaluation that complements
    human assessment.
  prefs: []
  type: TYPE_NORMAL
- en: Reference-based evaluation compares each agent output against one or more gold-standard
    answers or trajectories. While BLEU/ROUGE and early embedding measures like BERTScore
    / **Universal Sentence Encoder** (**USE**) were vital first steps, today’s state-of-the-art
    relies on learned metrics (BLEURT, COMET, BARTScore), QA-based frameworks (QuestEval),
    and LLM-powered judges, all backed by large human‐rated datasets to ensure robust,
    semantically aware evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Rather than using direct string comparison, modern evaluation increasingly employs
    criterion-based assessment frameworks that evaluate outputs against specific requirements.
    For example, the T-Eval framework evaluates tool usage through a multi-stage process
    examining planning, reasoning, tool selection, parameter formation, and result
    interpretation. This structured approach allows precise identification of where
    in the process an agent might be failing, providing far more actionable insights
    than simple success/failure metrics.
  prefs: []
  type: TYPE_NORMAL
- en: LLM-as-a-judge approaches represent a rapidly evolving evaluation methodology
    where powerful language models serve as automated evaluators, assessing outputs
    according to defined rubrics. Research by Zheng and colleagues (*Judging LLM-as-a-Judge
    with MT-Bench and Chatbot Arena*, 2023) demonstrates that with carefully designed
    prompting, models like GPT-4 can achieve substantial agreement with human evaluators
    on dimensions like factual accuracy, coherence, and relevance. This approach can
    help evaluate subjective qualities that traditional metrics struggle to capture,
    though researchers emphasize the importance of human verification to mitigate
    potential biases in the evaluator models themselves.
  prefs: []
  type: TYPE_NORMAL
- en: Human-in-the-loop evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Human evaluation remains essential for assessing subjective dimensions of agent
    performance that automated metrics cannot fully capture. Effective human-in-the-loop
    evaluation requires structured methodologies to ensure consistency and reduce
    bias while leveraging human judgment where it adds the most value.
  prefs: []
  type: TYPE_NORMAL
- en: Expert review provides in-depth qualitative assessment from domain specialists
    who can identify subtle errors, evaluate reasoning quality, and assess alignment
    with domain-specific best practices. Rather than unstructured feedback, modern
    expert review employs standardized rubrics that decompose evaluation into specific
    dimensions, typically using Likert scales or comparative rankings. Research in
    healthcare and financial domains has developed standardized protocols for expert
    evaluation, particularly for assessing agent responses in complex regulatory contexts.
  prefs: []
  type: TYPE_NORMAL
- en: User feedback captures the perspective of end users interacting with the agent
    in realistic contexts. Structured feedback collection through embedded rating
    mechanisms (for example, thumbs up/down, 1-5 star ratings) provides quantitative
    data on user satisfaction, while free-text comments offer qualitative insights
    into specific strengths or weaknesses. Academic studies of conversational agent
    effectiveness increasingly implement systematic feedback collection protocols
    where user ratings are analyzed to identify patterns in agent performance across
    different query types, user segments, or time periods.
  prefs: []
  type: TYPE_NORMAL
- en: A/B testing methodologies allow controlled comparison of different agent versions
    or configurations by randomly routing users to different implementations and measuring
    performance differences. This experimental approach is particularly valuable for
    evaluating changes to agent prompting, tool integration, or retrieval mechanisms.
    When implementing A/B testing, researchers typically define primary metrics (like
    task completion rates or user satisfaction) alongside secondary metrics that help
    explain observed differences (such as response length, tool usage patterns, or
    conversation duration).
  prefs: []
  type: TYPE_NORMAL
- en: Academic research on conversational agent optimization has demonstrated the
    effectiveness of controlled experiments in identifying specific improvements to
    agent configurations.
  prefs: []
  type: TYPE_NORMAL
- en: System-level evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: System-level evaluation is crucial for complex LLM agents, particularly RAG
    systems, because testing individual components isn’t enough. Research indicates
    that a significant portion of failures (over 60% in some studies) stem from integration
    issues between components that otherwise function correctly in isolation. For
    example, issues can arise from retrieved documents not being used properly, query
    reformulation altering original intent, or context windows truncating information
    during handoffs. System-level evaluation addresses this by examining how information
    flows between components and how the agent performs as a unified system.
  prefs: []
  type: TYPE_NORMAL
- en: Core approaches to system-level evaluation include using diagnostic frameworks
    that trace information flow through the entire pipeline to identify breakdown
    points, like the RAG Diagnostic Tool. Tracing and observability tools (such as
    LangSmith, Langfuse, and DeepEval) provide visibility into the agent’s internal
    workings, allowing developers to visualize reasoning chains and pinpoint where
    errors occur. End-to-end testing methodologies use comprehensive scenarios to
    assess how the entire system handles ambiguity, challenge inputs, and maintain
    context over multiple turns, using frameworks like GAIA.
  prefs: []
  type: TYPE_NORMAL
- en: Effective evaluation of LLM applications requires running multiple assessments.
    Rather than presenting abstract concepts, here are a few practical steps!
  prefs: []
  type: TYPE_NORMAL
- en: '**Define business metrics**: Start by identifying metrics that matter to your
    organization. Focus on functional aspects like accuracy and completeness, technical
    factors such as latency and token usage, and user experience elements including
    helpfulness and clarity. Each application should have specific criteria with clear
    measurement methods.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Create diverse test datasets**: Develop comprehensive test datasets covering
    common user queries, challenging edge cases, and potential compliance issues.
    Categorize examples systematically to ensure broad coverage. Continuously expand
    your dataset as you discover new usage patterns or failure modes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Combine multiple evaluation methods**: Use a mix of evaluation approaches
    for thorough assessment. Automated checks for factual accuracy and correctness
    should be combined with domain-specific criteria. Consider both quantitative metrics
    and qualitative assessments from subject matter experts when evaluating responses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deploy progressively**: Adopt a staged deployment approach. Begin with development
    testing against offline benchmarks, then proceed to limited production release
    with a small user subset. Only roll out fully after meeting performance thresholds.
    This cautious approach helps identify issues before they affect most users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitor production performance**: Implement ongoing monitoring in live environments.
    Track key performance indicators like response time, error rates, token usage,
    and user feedback. Set up alerts for anomalies that might indicate degraded performance
    or unexpected behavior.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Establish improvement cycles**: Create structured processes to translate
    evaluation insights into concrete improvements. When issues are identified, investigate
    root causes, implement specific solutions, and validate the effectiveness of changes
    through re-evaluation. Document patterns of problems and successful solutions
    for future reference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Foster cross-functional collaboration**: Include diverse perspectives in
    your evaluation process. Technical teams, domain experts, business stakeholders,
    and compliance specialists all bring valuable insights. Regular review sessions
    with these cross-functional teams help ensure the comprehensive assessment of
    LLM applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Maintain living documentation**: Keep centralized records of evaluation results,
    improvement actions, and outcomes. This documentation builds organizational knowledge
    and helps teams learn from past experiences, ultimately accelerating the development
    of more effective LLM applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s time now to put the theory to the test and get into the weeds of evaluating
    LLM agents. Let’s dive in!
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating LLM agents in practice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LangChain provides several predefined evaluators for different evaluation criteria.
    These evaluators can be used to assess outputs based on specific rubrics or criteria
    sets. Some common criteria include conciseness, relevance, correctness, coherence,
    helpfulness, and controversiality.
  prefs: []
  type: TYPE_NORMAL
- en: We can also compare results from an LLM or agent against reference results using
    different methods starting from pairwise string comparisons, string distances,
    and embedding distances. The evaluation results can be used to determine the preferred
    LLM or agent based on the comparison of outputs. Confidence intervals and p-values
    can also be calculated to assess the reliability of the evaluation results.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go through a few basics and apply useful evaluation strategies. We’ll
    start with LangChain.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the correctness of results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s think of an example, where we want to verify that an LLM’s answer is correct
    (or how far it is off). For example, when asked about the Federal Reserve’s interest
    rate, you might compare the output against a reference answer using both an exact
    match and a string distance evaluator.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Now, obviously this won’t be very useful if the output comes in a different
    format or if we want to gauge how far off the answer is. In the repository, you
    can find an implementation of a custom comparison that would parse answers such
    as “It is 0.50%” and “A quarter percent.”
  prefs: []
  type: TYPE_NORMAL
- en: A more generalizable approach is LLM‐as‐a‐judge for evaluating correctness.
    In this example, instead of using simple string extraction or an exact match,
    we call an evaluation LLM (for example, an upper mid-range model such as Mistral)
    that parses and scores the prompt, the prediction, and a reference answer and
    then returns a numerical score plus reasoning. This works in scenarios where the
    prediction might be phrased differently but still correct.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The output demonstrates how the LLM evaluator assesses the response quality
    with nuanced reasoning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This evaluation highlights an important advantage of the LLM-as-a-judge approach:
    it can identify subtle issues that simple matching would miss. In this case, the
    evaluator correctly identified that the response lacked important context. With
    a score of 3 out of 5, the LLM judge provides a more nuanced assessment than binary
    correct/incorrect evaluations, giving developers actionable feedback to improve
    response quality in financial applications where accuracy and proper attribution
    are critical.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The next example shows how to use Mistral AI to evaluate a model’s prediction
    against a reference answer. Please make sure to set your `MISTRAL_API_KEY` environment
    variable and install the required package: `pip install langchain_mistralai`.
    This should already be installed if you followed the instructions in [*Chapter
    2*](E_Chapter_2.xhtml#_idTextAnchor044).'
  prefs: []
  type: TYPE_NORMAL
- en: This approach is more appropriate when you have ground truth responses and want
    to assess how well the model’s output matches the expected answer. It’s particularly
    useful for factual questions with clear, correct answers.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows how providing a reference answer significantly changes the
    evaluation results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Notice how the score increased dramatically from 3 (in the previous example)
    to 8 when we provided a reference answer. This demonstrates the importance of
    ground truth in evaluation. Without a reference, the evaluator focused on the
    lack of citation and timestamp. With a reference confirming the factual accuracy,
    the evaluator now focuses on assessing completeness and depth instead of verifiability.
  prefs: []
  type: TYPE_NORMAL
- en: Both of these approaches leverage Mistral’s LLM as an evaluator, which can provide
    more nuanced and context-aware assessments than simple string matching or statistical
    methods. The results from these evaluations should be consistent when using `temperature=0`,
    though outputs may differ from those shown in the book due to changes on the provider
    side.
  prefs: []
  type: TYPE_NORMAL
- en: Your output may differ from the book example due to model version differences
    and inherent variations in LLM responses (depending on the temperature).
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating tone and conciseness
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Beyond factual accuracy, many applications require responses that meet certain
    stylistic criteria. Healthcare applications, for example, must provide accurate
    information in a friendly, approachable manner without overwhelming patients with
    unnecessary details. The following example demonstrates how to evaluate both conciseness
    and tone using LangChain’s criteria evaluators, allowing developers to assess
    these subjective but critical aspects of response quality:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by importing the evaluator loader and a chat LLM for evaluation (for
    example GPT-4o):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Our example prompt and the answer we’ve obtained is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s evaluate conciseness using a built-in `conciseness` criterion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The result includes a score (0 or 1), a value (“Y” or “N”), and a reasoning
    chain of thought:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'As for friendliness, let’s define a `custom` criterion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The evaluator should return whether the tone is friendly (`Y`/`N`) along with
    reasoning. In fact, this is what we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This evaluation approach is particularly valuable for applications in healthcare,
    customer service, and educational domains where the manner of communication is
    as important as the factual content. The explicit reasoning provided by the evaluator
    helps development teams understand exactly which elements of the response contribute
    to its tone, making it easier to debug and improve response generation. While
    binary `Y`/`N` scores are useful for automated quality gates, the detailed reasoning
    offers more nuanced insights for continuous improvement. For production systems,
    consider combining multiple criteria evaluators to create a comprehensive quality
    score that reflects all aspects of your application’s communication requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the output format
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When working with LLMs to generate structured data like JSON, XML, or CSV,
    format validation becomes critical. Financial applications, reporting tools, and
    API integrations often depend on correctly formatted data structures. A technically
    perfect response that fails to adhere to the expected format can break downstream
    systems. LangChain provides specialized evaluators for validating structured outputs,
    as demonstrated in the following example using JSON validation for a financial
    report:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll see a score indicating the JSON is valid:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'For the invalid JSON, we are getting a score indicating the JSON is invalid:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This validation approach is particularly valuable in production systems where
    LLMs interface with other software components. The `JsonValidityEvaluator` not
    only identifies invalid outputs but also provides detailed error messages pinpointing
    the exact location of formatting errors. This facilitates rapid debugging and
    can be incorporated into automated testing pipelines to prevent format-related
    failures. Consider implementing similar validators for other formats your application
    may generate, such as XML, CSV, or domain-specific formats like FIX protocol for
    financial transactions.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating agent trajectory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Complex agents require evaluation across three critical dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Final response evaluation**: Assess the ultimate output provided to the user
    (factual accuracy, helpfulness, quality, and safety)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Trajectory evaluation**: Examine the path the agent took to reach its conclusion'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Single-step evaluation**: Analyze individual decision points in isolation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While final response evaluation focuses on outcomes, trajectory evaluation examines
    the process itself. This approach is particularly valuable for complex agents
    that employ multiple tools, reasoning steps, or decision points to complete tasks.
    By evaluating the path taken, we can identify exactly where and how agents succeed
    or fail, even when the final answer is incorrect.
  prefs: []
  type: TYPE_NORMAL
- en: Trajectory evaluation compares the actual sequence of steps an agent took against
    an expected sequence, calculating a score based on how many expected steps were
    completed correctly. This gives partial credit to agents that follow some correct
    steps even if they don’t reach the right final answer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s implement a `custom trajectory` evaluator for a healthcare agent that
    responds to medication questions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Please remember to set your `LANGSMITH_API_KEY` environment variable! If you
    get a `Using legacy API key` error, you might need to generate a new API key from
    the LangSmith dashboard: [https://smith.langchain.com/settings](https://smith.langchain.com/settings).
    You always want to use the latest version of the LangSmith package.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To evaluate the agent’s trajectory, we need to capture the actual sequence
    of steps taken. With LangGraph, we can use streaming capabilities to record every
    node and tool invocation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also analyze results on the dataset, which we can download from LangSmith:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: In this case, this is nonsensical, but this is to illustrate the idea.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot visually demonstrates what trajectory evaluation results
    look like in the LangSmith interface. It shows the perfect trajectory match score
    (**1.00**), which validates that the agent followed the expected path:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1: Trajectory evaluation in LangSmith](img/B32363_08_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.1: Trajectory evaluation in LangSmith'
  prefs: []
  type: TYPE_NORMAL
- en: Please note that LangSmith displays the actual trajectory steps side by side
    with the reference trajectory and that it includes real execution metrics like
    latency and token usage.
  prefs: []
  type: TYPE_NORMAL
- en: 'Trajectory evaluation provides unique insights beyond simple pass/fail assessments:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Identifying failure points**: Pinpoint exactly where agents deviate from
    expected paths'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Process improvement**: Recognize when agents take unnecessary detours or
    inefficient routes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tool usage patterns**: Understand how agents leverage available tools and
    when they make suboptimal choices'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reasoning quality**: Evaluate the agent’s decision-making process independent
    of final outcomes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, an agent might provide a correct medication dosage but reach it
    through an inappropriate trajectory (bypassing safety checks or using unreliable
    data sources). Trajectory evaluation reveals these process issues that outcome-focused
    evaluation would miss.
  prefs: []
  type: TYPE_NORMAL
- en: Consider using trajectory evaluation in conjunction with other evaluation types
    for a holistic assessment of your agent’s performance. This approach is particularly
    valuable during development and debugging phases, where understanding the *why*
    behind agent behavior is as important as measuring final output quality.
  prefs: []
  type: TYPE_NORMAL
- en: By implementing continuous trajectory monitoring, you can track how agent behaviors
    evolve as you refine prompts, add tools, or modify the underlying model, ensuring
    improvements in one area don’t cause regressions in the agent’s overall decision-making
    process.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating CoT reasoning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now suppose we want to evaluate the agent’s reasoning. For example, going back
    to our earlier example, the agent must not only answer “What is the current interest
    rate?” but also provide reasoning behind its answer. We can use the `COT_QA` evaluator
    for chain-of-thought evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The returned score and reasoning allow us to judge whether the agent’s thought
    process is sound and comprehensive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Please note that in this evaluation, the agent provides detailed reasoning along
    with its answer. The evaluator (using chain-of-thought evaluation) compares the
    agent’s reasoning with an expected explanation.
  prefs: []
  type: TYPE_NORMAL
- en: Offline evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Offline evaluation involves assessing the agent’s performance under controlled
    conditions before deployment. This includes benchmarking to establish general
    performance baselines and more targeted testing based on generated test cases.
    Offline evaluations provide key metrics, error analyses, and pass/fail summaries
    from controlled test scenarios, establishing baseline performance.
  prefs: []
  type: TYPE_NORMAL
- en: While human assessments are sometimes seen as the gold standard, they are hard
    to scale and require careful design to avoid bias from subjective preferences
    or authoritative tones. Benchmarking involves comparing the performance of LLMs
    against standardized tests or tasks. This helps identify the strengths and weaknesses
    of the models and guides further development and improvement.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll discuss creating an effective evaluation dataset
    within the context of RAG system evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating RAG systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The dimensions of RAG evaluation discussed earlier (retrieval quality, contextual
    relevance, faithful generation, and information synthesis) provided a foundation
    for understanding how to measure RAG system effectiveness. Understanding failure
    patterns of RAG systems helps create more effective evaluation strategies. Barnett
    and colleagues in their 2024 paper *Seven Failure Points When Engineering a Retrieval
    Augmented Generation System* identified several distinct ways RAG systems fail
    in production environments:'
  prefs: []
  type: TYPE_NORMAL
- en: First, **missing content** **failures** occur when the system fails to retrieve
    relevant information that exists in the knowledge base. This might happen because
    of chunking strategies that split related information, embedding models that miss
    semantic connections, or content gaps in the knowledge base itself.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Second, **ranking failures** happen when relevant documents exist but aren’t
    ranked highly enough to be included in the context window. This commonly stems
    from suboptimal embedding models, vocabulary mismatches between queries and documents,
    or poor chunking granularity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Context window limitations** create another failure mode when key information
    is spread across documents that exceed the model’s context limit. This forces
    difficult tradeoffs between including more documents and maintaining sufficient
    detail from each one.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perhaps most critically, **information extraction failures** occur when relevant
    information is retrieved but the LLM fails to properly synthesize it. This might
    happen due to ineffective prompting, complex information formats, or conflicting
    information across documents.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To effectively evaluate and address these specific failure modes, we need a
    structured and comprehensive evaluation approach. The following example demonstrates
    how to build a carefully designed evaluation dataset in LangSmith that allows
    for testing each of these failure patterns in the context of financial advisory
    systems. By creating realistic questions with expected answers and relevant metadata,
    we can systematically identify which failure modes most frequently affect our
    particular implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: This dataset structure serves multiple evaluation purposes. First, it identifies
    specific documents that should be retrieved, allowing evaluation of retrieval
    accuracy. It then defines key points that should appear in the response, enabling
    assessment of information extraction. Finally, it connects each example to testing
    objectives, making it easier to diagnose specific system capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: When implementing this dataset in practice, organizations typically load these
    examples into evaluation platforms like LangSmith, allowing automated testing
    of their RAG systems. The results reveal specific patterns in system performance—perhaps
    strong retrieval but weak synthesis, or excellent performance on simple factual
    questions but struggles with complex perspective inquiries.
  prefs: []
  type: TYPE_NORMAL
- en: However, implementing effective RAG evaluation goes beyond simply creating datasets;
    it requires using diagnostic tools to pinpoint exactly where failures occur within
    the system pipeline. Drawing on research, these diagnostics identify specific
    failure modes, such as poor document ranking (information exists but isn’t prioritized)
    or poor context utilization (the agent ignores relevant retrieved documents).
    By diagnosing these issues, organizations gain actionable insights—for instance,
    consistent ranking failures might suggest implementing hybrid search, while context
    utilization problems could lead to refined prompting or structured outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The ultimate goal of RAG evaluation is to drive continuous improvement. Organizations
    achieving the most success follow an iterative cycle: running comprehensive diagnostics
    to find specific failure patterns, prioritizing fixes based on their frequency
    and impact, implementing targeted changes, and then re-evaluating to measure the
    improvement. By systematically diagnosing issues and using those insights to iterate,
    teams can build more accurate and reliable RAG systems with fewer common errors.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll see how we can use **LangSmith**, a companion project
    for LangChain, to benchmark and evaluate our system’s performance on a dataset.
    Let’s step through an example!
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating a benchmark in LangSmith
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we’ve mentioned, comprehensive benchmarking and evaluation, including testing,
    are critical for safety, robustness, and intended behavior. LangSmith, despite
    being a platform designed for testing, debugging, monitoring, and improving LLM
    applications, offers tools for evaluation and dataset management. LangSmith integrates
    seamlessly with LangChain Benchmarks, providing a cohesive framework for developing
    and assessing LLM applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can run evaluations against benchmark datasets in LangSmith, as we’ll see
    now. First, please make sure you create an account on LangSmith here: [https://smith.langchain.com/](https://smith.langchain.com/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can obtain an API key and set it as `LANGCHAIN_API_KEY` in your environment.
    We can also set environment variables for project ID and tracing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This configuration establishes a connection to LangSmith and directs all traces
    to a specific project. When no project ID is explicitly defined, LangChain logs
    against the default project. The `LANGCHAIN_TRACING_V2` flag enables the most
    recent version of LangSmith’s tracing capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'After configuring the environment, we can begin logging interactions with our
    LLM applications. Each interaction creates a traceable record in LangSmith:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: When this code executes, it performs a simple interaction with the ChatOpenAI
    model and automatically logs the request, response, and performance metrics to
    LangSmith. These logs appear in the LangSmith project dashboard at [https://smith.langchain.com/projects](https://smith.langchain.com/projects),
    allowing for detailed inspection of each interaction.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create a dataset from existing agent runs with the `create_example_from_run()`
    function—or from anything else. Here’s how to create a dataset with a set of questions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: This code creates a new evaluation dataset in LangSmith containing financial
    advisory questions. Each example includes an input query and an expected output
    answer, establishing a reference standard against which we can evaluate our LLM
    application responses.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now define our RAG system with a function like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: In a complete implementation, you would prepare a vector store with relevant
    financial documents, create appropriate prompt templates, and configure the retrieval
    and response generation components. The concepts and techniques for building robust
    RAG systems are covered extensively in [*Chapter 4*](E_Chapter_4.xhtml#_idTextAnchor152),
    which provides step-by-step guidance on document processing, embedding creation,
    vector store setup, and chain construction.
  prefs: []
  type: TYPE_NORMAL
- en: We can make changes to our chain and evaluate changes in the application. Does
    the change improve the result or not? Changes can be in any part of our application,
    be it a new model, a new prompt template, or a new chain or agent. We can run
    two versions of the application with the same input examples and save the results
    of the runs. Then we evaluate the results by comparing them side by side.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run an evaluation on a dataset, we can either specify an LLM or—for parallelism—use
    a constructor function to initialize the model or LLM app for each input. Now,
    to evaluate the performance against our dataset, we need to define an evaluator
    as we saw in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: This shows how to configure multi-dimensional evaluation for RAG systems, assessing
    factual accuracy, groundedness, and retrieval quality using LLM-based judges.
    The criteria are defined by a dictionary that includes a criterion as a key and
    a question to check for as the value.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll now pass a dataset together with the evaluation configuration with evaluators
    to `run_on_dataset()` to generate metrics and feedback:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: In the same way, we could pass a dataset and evaluators to `run_on_dataset()`
    to generate metrics and feedback asynchronously.
  prefs: []
  type: TYPE_NORMAL
- en: This practical implementation provides a framework you can adapt for your specific
    domain. By creating a comprehensive evaluation dataset and assessing your RAG
    system across multiple dimensions (correctness, groundedness, and retrieval quality),
    you can identify specific areas for improvement and track progress as you refine
    your system.
  prefs: []
  type: TYPE_NORMAL
- en: When implementing this approach, consider incorporating real user queries from
    your application logs (appropriately anonymized) to ensure your evaluation dataset
    reflects actual usage patterns. Additionally, periodically refreshing your dataset
    with new queries and updated information helps prevent overfitting and ensures
    your evaluation remains relevant as user needs evolve.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use the datasets and evaluate libraries by HuggingFace to check a coding
    LLM approach to solving programming problems.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating a benchmark with HF datasets and Evaluate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As a reminder: the `pass@k` metric is a way to evaluate the performance of
    an LLM in solving programming exercises. It measures the proportion of exercises
    for which the LLM generated at least one correct solution within the top `k` candidates.
    A higher `pass@k` score indicates better performance, as it means the LLM was
    able to generate a correct solution more often within the top `k` candidates.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hugging Face’s `Evaluate` library makes it very easy to calculate `pass@k`
    and other metrics. Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We should get an output like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'For this code to run, you need to set the `HF_ALLOW_CODE_EVAL` environment
    variable to 1\. Please be cautious: running LLM code on your machine comes with
    a risk.'
  prefs: []
  type: TYPE_NORMAL
- en: This shows how to evaluate code generation models using HuggingFace’s `code_eval`
    metric, which measures a model’s ability to produce functioning code solutions.
    This is great. Let’s see another example.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating email extraction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s show how we can use it to evaluate an LLM’s ability to extract structured
    information from insurance claim texts.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll first create a synthetic dataset using LangSmith. In this synthetic dataset,
    each example consists of a raw insurance claim text (input) and its corresponding
    expected structured output (output). We will use this dataset to run extraction
    chains and evaluate your model’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: We assume that you’ve already set up your LangSmith credentials.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We can upload this dataset to LangSmith:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s run our `InsuranceClaim` dataset on LangSmith. We’ll first define
    a schema for our claims:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we’ll define our extraction chain. We are keeping it very simple; we’ll
    just ask for a JSON object that follows the `InsuranceClaim` schema. The extraction
    chain is defined with ChatOpenAI LLM with function calling bound to our schema:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can run the extraction chain on our sample insurance claim:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: This showed how to evaluate structured information extraction from insurance
    claims text, using a Pydantic schema to standardize extraction and LangSmith to
    assess performance.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we outlined critical strategies for evaluating LLM applications,
    ensuring robust performance before production deployment. We provided an overview
    of the importance of evaluation, architectural challenges, evaluation strategies,
    and types of evaluation. We then demonstrated practical evaluation techniques
    through code examples, including correctness evaluation using exact matches and
    LLM-as-a-judge approaches. For instance, we showed how to implement the `ExactMatchStringEvaluator`
    for comparing answers about Federal Reserve interest rates, and how to use `ScoreStringEvalChain`
    for more nuanced evaluations. The examples also covered JSON format validation
    using `JsonValidityEvaluator` and assessment of agent trajectories in healthcare
    scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Tools like LangChain provide predefined evaluators for criteria such as conciseness
    and relevance, while platforms like LangSmith enable comprehensive testing and
    monitoring. The chapter presented code examples using LangSmith to create and
    evaluate datasets, demonstrating how to assess model performance across multiple
    criteria. The implementation of `pass@k` metrics using Hugging Face’s Evaluate
    library was shown for assessing code generation capabilities. We also walked through
    an example of evaluating insurance claim text extraction using structured schemas
    and LangChain’s evaluation capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve evaluated our AI workflows, in the next chapter we’ll look at
    how we can deploy and monitor them. Let’s discuss deployment and observability!
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Describe three key metrics used in evaluating AI agents.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What’s the difference between online and offline evaluation?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are system-level and application-level evaluations and how do they differ?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can LangSmith be used to compare different versions of an LLM application?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does chain-of-thought evaluation differ from traditional output evaluation?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is trajectory evaluation important for understanding agent behavior?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the key considerations when evaluating LLM agents for production deployment?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can bias be mitigated when using language models as evaluators?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What role do standardized benchmarks play, and how can we create benchmark datasets
    for LLM agent evaluation?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do you balance automated evaluation metrics with human evaluation in production
    systems?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
