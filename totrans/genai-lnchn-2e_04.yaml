- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building Workflows with LangGraph
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we’ve learned about LLMs, LangChain as a framework, and how to use LLMs
    with LangChain in a vanilla mode (just asking to generate a text output based
    on a prompt). In this chapter, we’ll start with a quick introduction to LangGraph
    as a framework and how to develop more complex workflows with LangChain and LangGraph
    by chaining together multiple steps. As an example, we’ll discuss parsing LLM
    outputs and look into error handling patterns with LangChain and LangGraph. Then,
    we’ll continue with more advanced ways to develop prompts and explore what building
    blocks LangChain offers for few-shot prompting and other techniques.
  prefs: []
  type: TYPE_NORMAL
- en: We’re also going to cover working with multimodal inputs, utilizing the long
    context, and adjusting your workloads to overcome limitations related to the context
    window size. Finally, we’ll look into the basic mechanisms of managing memory
    with LangChain. Understanding these fundamental and key techniques will help us
    read LangGraph code, understand tutorials and code samples, and develop our own
    complex workflows. We’ll, of course, discuss what LangGraph workflows are and
    will continue building on that skill in *Chapters 5* and *6*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a nutshell, we’ll cover the following main topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: LangGraph fundamentals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompt engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with short context windows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding memory mechanisms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As always, you can find all the code samples on our public GitHub repository
    as Jupyter notebooks: [https://github.com/benman1/generative_ai_with_langchain/tree/second_edition/chapter3](https://github.com/benman1/generative_ai_with_langchain/tree/second_edition/chapter3).'
  prefs: []
  type: TYPE_NORMAL
- en: LangGraph fundamentals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LangGraph is a framework developed by LangChain (as a company) that helps control
    and orchestrate workflows. Why do we need another orchestration framework? Let’s
    park this question until [*Chapter 5*](E_Chapter_5.xhtml#_idTextAnchor231), where
    we’ll touch on agents and agentic workflows, but for now, let us mention the flexibility
    of LangGraph as an orchestration framework and its robustness in handling complex
    scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike many other frameworks, LangGraph allows cycles (most other orchestration
    frameworks operate only with directly acyclic graphs), supports streaming out
    of the box, and has many pre-built loops and components dedicated to generative
    AI applications (for example, human moderation). LangGraph also has a very rich
    API that allows you to have very granular control of your execution flow if needed.
    This is not fully covered in our book, but just keep in mind that you can always
    use a more low-level API if you need to.
  prefs: []
  type: TYPE_NORMAL
- en: A **Directed Acyclic Graph (DAG)** is a special type of graph in graph theory
    and computer science. Its edges (connections between nodes) have a direction,
    which means that the connection from node A to node B is different from the connection
    from node B to node A. It has no cycles. In other words, there is no path that
    starts at a node and returns to the same node by following the directed edges.
  prefs: []
  type: TYPE_NORMAL
- en: DAGs are often used as a model of workflows in data engineering, where nodes
    are tasks and edges are dependencies between these tasks. For example, an edge
    from node A to node B means that we need output from node A to execute node B.
  prefs: []
  type: TYPE_NORMAL
- en: For now, let’s start with the basics. If you’re new to this framework, we would
    also highly recommend a free online course on LangGraph that is available at [https://academy.langchain.com/](https://academy.langchain.com/)
    to deepen your understanding.
  prefs: []
  type: TYPE_NORMAL
- en: State management
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: State management is crucial in real-world AI applications. For example, in a
    customer service chatbot, the state might track information such as customer ID,
    conversation history, and outstanding issues. LangGraph’s state management lets
    you maintain this context across a complex workflow of multiple AI components.
  prefs: []
  type: TYPE_NORMAL
- en: LangGraph allows you to develop and execute complex workflows called **graphs**.
    We will use the words *graph* and *workflow* interchangeably in this chapter.
    A graph consists of nodes and edges between them. Nodes are components of your
    workflow, and a workflow has a *state*. What is it? Firstly, a state makes your
    nodes aware of the current context by keeping track of the user input and previous
    computations. Secondly, a state allows you to persist your workflow execution
    at any point in time. Thirdly, a state makes your workflow truly interactive since
    a node can change the workflow’s behavior by updating the state. For simplicity,
    think about a state as a Python dictionary. Nodes are Python functions that operate
    on this dictionary. They take a dictionary as input and return another dictionary
    that contains keys and values to be updated in the state of the workflow.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s understand that with a simple example. First, we need to define a state’s
    schema:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: A `TypedDict` is a Python type constructor that allows to define dictionaries
    with a predefined set of keys and each key can have its own type (as opposed to
    a `Dict[str, str]` construction).
  prefs: []
  type: TYPE_NORMAL
- en: LangGraph state’s schema shouldn’t necessarily be defined as a `TypedDict`;
    you can use data classes or Pydantic models too.
  prefs: []
  type: TYPE_NORMAL
- en: 'After we have defined a schema for a state, we can define our first simple
    workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Here, we defined two Python functions that are components of our workflow. Then,
    we defined our workflow by providing a state’s schema, adding nodes and edges
    between them. `add_node` is a convenient way to add a component to your graph
    (by providing its name and a corresponding Python function), and you can reference
    this name later when you define edges with `add_edge`. `START` and `END` are reserved
    built-in nodes that define the beginning and end of the workflow accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at our workflow by using a built-in visualization mechanism:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 3.1: LangGraph built-in visualization of our first workflow](img/B32363_03_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.1: LangGraph built-in visualization of our first workflow'
  prefs: []
  type: TYPE_NORMAL
- en: Our function accesses the state by simply reading from the dictionary that LangGraph
    automatically provides as input. LangGraph isolates state updates. When a node
    receives the state, it gets an immutable copy, not a reference to the actual state
    object. The node must return a dictionary containing the specific keys and values
    it wants to update. LangGraph then handles merging these updates into the master
    state. This pattern prevents side effects and ensures that state changes are explicit
    and traceable.
  prefs: []
  type: TYPE_NORMAL
- en: 'The only way for a node to modify a state is to provide an output dictionary
    with key-value pairs to be updated, and LangGraph will handle it. A node should
    modify at least one key in the state. A `graph` instance itself is a `Runnable`
    (to be precise, it inherits from `Runnable`) and we can execute it. We should
    provide a dictionary with the initial state, and we’ll get the final state as
    an output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We used a very simple graph as an example. With your real workflows, you can
    define parallel steps (for example, you can easily connect one node with multiple
    nodes) and even cycles. LangGraph executes the workflow in so-called *supersteps*
    that can call multiple nodes at the same time (and then merge state updates from
    these nodes). You can control the depth of recursion and amount of overall supersteps
    in the graph, which helps you avoid cycles running forever, especially because
    the LLMs output is non-deterministic.
  prefs: []
  type: TYPE_NORMAL
- en: '**A superstep** on LangGraph represents a discrete iteration over one or a
    few nodes, and it’s inspired by Pregel, a system built by Google for processing
    large graphs at scale. It handles parallel execution of nodes and updates sent
    to the central graph’s state.'
  prefs: []
  type: TYPE_NORMAL
- en: In our example, we used direct edges from one node to another. It makes our
    graph no different from a sequential chain that we could have defined with LangChain.
    One of the key LangGraph features is the ability to create conditional edges that
    can direct the execution flow to one or another node depending on the current
    state. A conditional edge is a Python function that gets the current state as
    an input and returns a string with the node’s name to be executed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ve defined an edge `is_suitable_condition` that takes a state and returns
    either an `END` or `generate_application` string by analyzing the current state.
    We used a `Literal` type hint since it’s used by LangGraph to determine which
    destination nodes to connect the source node with when it’s creating conditional
    edges. If you don’t use a type hint, you can provide a list of destination nodes
    directly to the `add_conditional_edges` function; otherwise, LangGraph will connect
    the source node with all other nodes in the graph (since it doesn’t analyze the
    code of an edge function itself when creating a graph). The following figure shows
    the output generated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2: A workflow with conditional edges (represented as dotted lines)](img/B32363_03_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2: A workflow with conditional edges (represented as dotted lines)'
  prefs: []
  type: TYPE_NORMAL
- en: Conditional edges are visualized with dotted lines, and now we can see that,
    depending on the output of the `analyze_job_description` step, our graph can perform
    different actions.
  prefs: []
  type: TYPE_NORMAL
- en: Reducers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, our nodes have changed the state by updating the value for a corresponding
    key. From another point of view, at each superstep, LangGraph can produce a new
    value for a given key. In other words, for every key in the state, there’s a sequence
    of values, and from a functional programming perspective, a `reduce` function
    can be applied to this sequence. The default reducer on LangGraph always replaces
    the final value with the new value. Let’s imagine we want to track custom actions
    (produced by nodes) and compare three options.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the first option, a node should return a list as a value for the key `actions`.
    We provide short code samples just for illustration purposes, but you can find
    full ones on Github. If such a value already exists in the state, it will be replaced
    with the new one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Another option is to use the default `add` method with the `Annotated` type
    hint. By using this type hint, we tell the LangGraph compiler that the type of
    our variable in the state is a list of strings, and it should use the `add` method
    to concatenate two lists (if the value already exists in the state and a node
    produces a new one):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The last option is to write your own custom reducer. In this example, we write
    a custom reducer that accepts not only a list from the node (as a new value) but
    also a single string that would be converted to a list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'LangGraph has a few built-in reducers, and we’ll also demonstrate how you can
    implement your own. One of the important ones is `add_messages`, which allows
    us to merge messages. Many of your nodes would be LLM agents, and LLMs typically
    work with messages. Therefore, according to the conversational programming paradigm
    we’ll talk about in more detail in *Chapters 5* and *6*, you typically need to
    keep track of these messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Since this is such an important reducer, there’s a built-in state that you
    can inherit from:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Now, as we have discussed reducers, let’s talk about another important concept
    for any developer – how to write reusable and modular workflows by passing configurations
    to them.
  prefs: []
  type: TYPE_NORMAL
- en: Making graphs configurable
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LangGraph provides a powerful API that allows you to make your graph configurable.
    It allows you to separate parameters from user input – for example, to experiment
    between different LLM providers or pass custom callbacks. A node can also access
    the configuration by accepting it as a second argument. The configuration will
    be passed as an instance of `RunnableConfig.`
  prefs: []
  type: TYPE_NORMAL
- en: '`RunnableConfig` is a typed dictionary that gives you control over execution
    control settings. For example, you can control the maximum number of supersteps
    with the `recursion_limit` parameter. `RunnableConfig` also allows you to pass
    custom parameters as a separate dictionary under a `configurable` key.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s allow our node to use different LLMs during application generation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s now compile and execute our graph with a custom configuration (if you
    don’t provide any, LangGraph will use the default one):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we’ve established how to structure complex workflows with LangGraph,
    let’s look at a common challenge these workflows face: ensuring LLM outputs follow
    the exact structure needed by downstream components. Robust output parsing and
    graceful error handling are essential for reliable AI pipelines.'
  prefs: []
  type: TYPE_NORMAL
- en: Controlled output generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you develop complex workflows, one of the common tasks you need to solve
    is to force an LLM to generate an output that follows a certain structure. This
    is called a controlled generation. This way, it can be consumed programmatically
    by the next steps further down the workflow. For example, we can ask the LLM to
    generate JSON or XML for an API call, extract certain attributes from a text,
    or generate a CSV table. There are multiple ways to achieve this, and we’ll start
    exploring them in this chapter and continue in [*Chapter 5*](E_Chapter_5.xhtml#_idTextAnchor231).
    Since an LLM might not always follow the exact output structure, the next step
    might fail, and you’ll need to recover from the error. Hence, we’ll also begin
    discussing error handling in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Output parsing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Output parsing is essential when integrating LLMs into larger workflows, where
    subsequent steps require structured data rather than natural language responses.
    One way to do that is to add corresponding instructions to the prompt and parse
    the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see a simple task. We’d like to classify whether a certain job description
    is suitable for a junior Java programmer as a step of our pipeline and, based
    on the LLM’s decision, we’d like to either continue with an application or ignore
    this specific job description. We can start with a simple prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the output of the LLM is free text, which might be difficult
    to parse or interpret in subsequent pipeline steps. What if we add a specific
    instruction to a prompt?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Now, how can we parse this output? Of course, our next step can be to just look
    at the text and have a condition based on a string comparison. But that won’t
    work for more complex use cases – for example, if the next step expects the output
    to be a JSON object. To deal with that, LangChain offers plenty of OutputParsers
    that take the output generated by the LLM and try to parse it into a desired format
    (by checking a schema if needed) – a list, CSV, enum, pandas DatafFrame, Pydantic
    model, JSON, XML, and so on. Each parser implements a `BaseGenerationOutputParser`
    interface, which extends the `Runnable` interface with an additional `parse_result`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s build a parser that parses an output into an enum:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The `EnumOutputParser` converts text output into a corresponding `Enum` instance.
    Note that the parser handles any generation-like output (not only strings), and
    it actually also strips the output.
  prefs: []
  type: TYPE_NORMAL
- en: You can find a full list of parsers in the documentation at [https://python.langchain.com/docs/concepts/output_parsers/](https://python.langchain.com/docs/concepts/output_parsers/),
    and if you need your own parser, you can always build a new one!
  prefs: []
  type: TYPE_NORMAL
- en: 'As a final step, let’s combine everything into a chain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s make this chain part of our LangGraph workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We made two important changes. First, our newly built chain is now part of a
    Python function that represents the `analyze_job_description` node, and that’s
    how we implement the logic within the node. Second, our conditional edge function
    doesn’t return a string anymore, but we added a mapping of returned values to
    destination edges to the `add_conditional_edges` function, and that’s an example
    of how you could implement a branching of your workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take some time to discuss how to handle potential errors if our parsing
    fails!
  prefs: []
  type: TYPE_NORMAL
- en: Error handling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Effective error management is essential in any LangChain workflow, including
    when handling tool failures (which we’ll explore in [*Chapter 5*](E_Chapter_5.xhtml#_idTextAnchor231)
    when we get to tools). When developing LangChain applications, remember that failures
    can occur at any stage:'
  prefs: []
  type: TYPE_NORMAL
- en: API calls to foundation models may fail
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs might generate unexpected outputs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: External services could become unavailable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One of the possible approaches would be to use a basic Python mechanism for
    catching exceptions, logging them for further analysis, and continuing your workflow
    either by wrapping an exception as a text or by returning a default value. If
    your LangChain chain calls some custom Python function, think about appropriate
    exception handling. The same goes for your LangGraph nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Logging is essential, especially as you approach production deployment. Proper
    logging ensures that exceptions don’t go unnoticed, allowing you to monitor their
    occurrence. Modern observability tools provide alerting mechanisms that group
    similar errors and notify you about frequently occurring issues.
  prefs: []
  type: TYPE_NORMAL
- en: 'Converting exceptions to text enables your workflow to continue execution while
    providing downstream LLMs with valuable context about what went wrong and potential
    recovery paths. Here is a simple example of how you can log the exception but
    continue executing your workflow by sticking to the default behavior:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'To test our error handling, we need to simulate LLM failures. LangChain has
    a few `FakeChatModel` classes that help you to test your chain:'
  prefs: []
  type: TYPE_NORMAL
- en: '`GenericFakeChatModel` returns messages based on a provided iterator'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FakeChatModel` always returns a `"fake_response"` string'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FakeListChatModel` takes a list of messages and returns them one by one on
    each invocation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s create a fake LLM that fails every second time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'When we provide this to our graph (the full code sample is available in our
    GitHub repo), we can see that the workflow continues despite encountering an exception:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: When an error occurs, sometimes it helps to try again. LLMs have a non-deterministic
    nature, and the next attempt might be successful; also, if you’re using third-party
    APIs, various failures might happen on the provider’s side. Let’s discuss how
    to implement proper retries with LangGraph.
  prefs: []
  type: TYPE_NORMAL
- en: Retries
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There are three distinct retry approaches, each suited to different scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: Generic retry with Runnable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Node-specific retry policies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Semantic output repair
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s look at these in turn, starting with generic retries that are available
    for every `Runnable.`
  prefs: []
  type: TYPE_NORMAL
- en: 'You can retry any `Runnable`or LangGraph node using a built-in mechanism:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'With LangGraph, you can also describe specific retries for every node. For
    example, let’s retry our `analyze_job_description` node two times in case of a
    `ValueError`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The components you’re using, often known as building blocks, might have their
    own retry mechanism that tries to algorithmically fix the problem by giving an
    LLM additional input on what went wrong. For example, many chat models on LangChain
    have client-side retries on specific server-side errors.
  prefs: []
  type: TYPE_NORMAL
- en: ChatAnthropic has a `max_retries` parameter that you can define either per instance
    or per request. Another good example of a more advanced building block is trying
    to recover from a parsing error. Retrying a parsing step won’t help since typically
    parsing errors are related to the incomplete LLM output. What if we retry the
    generation step and hope for the best, or actually give LLM a hint about what
    went wrong? That’s exactly what a `RetryWithErrorOutputParser` is doing.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3: Adding a retry mechanism to a chain that has multiple steps](img/B32363_03_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3: Adding a retry mechanism to a chain that has multiple steps'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to use `RetryWithErrorOutputParser`, we need to first initialize it
    with an LLM (used to fix the output) and our parser. Then, if our parsing fails,
    we run it and provide our initial prompt (with all substituted parameters), generated
    response, and parsing error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We can read the source code on GitHub to better understand what’s going on,
    but in essence, that’s an example of a pseudo-code without too many details. We
    illustrate how we can pass the parsing error and the original output that led
    to this error back to an LLM and ask it to fix the problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: We introduced the `StrOutputParser` in [*Chapter 2*](E_Chapter_2.xhtml#_idTextAnchor044)
    to convert the output of the ChatModel from an AIMessage to a string so that we
    can easily pass it to the next step in the chain.
  prefs: []
  type: TYPE_NORMAL
- en: Another thing to keep in mind is that LangChain building blocks allow you to
    redefine parameters, including default prompts. You can always check them on Github;
    sometimes it’s a good idea to customize default prompts for your workflows.
  prefs: []
  type: TYPE_NORMAL
- en: "You can read about other available output-fixing parsers here: [https://python.langchain.com/docs/how_to/output_parser_retry/](https://python.l\uFEFF\
    angchain.com/docs/how_to/output_parser_retry/)."
  prefs: []
  type: TYPE_NORMAL
- en: Fallbacks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In software development, a **fallback** is an alternative program that allows
    you to recover if your base one fails. LangChain allows you to define fallbacks
    on a `Runnable` level. If execution fails, an alternative chain is triggered with
    the same input parameters. For example, if the LLM you’re using is not available
    for a short period of time, your chain will automatically switch to a different
    one that uses an alternative provider (and probably different prompts).
  prefs: []
  type: TYPE_NORMAL
- en: 'Our fake model fails every second time, so let’s add a fallback to it. It’s
    just a lambda that prints a statement. As we can see, every second time, the fallback
    is executed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Generating complex outcomes that can follow a certain template and can be parsed
    reliably is called structured generation (or controlled generation). This can
    help to build more complex workflows, where an output of one LLM-driven step can
    be consumed by another programmatic step. We’ll pick this up again in more detail
    in *Chapters 5* and *6*.
  prefs: []
  type: TYPE_NORMAL
- en: Prompts that you send to an LLM are one of the most important building blocks
    of your workflows. Hence, let’s discuss some basics of prompt engineering next
    and see how to organize your prompts with LangChain.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s continue by looking into prompt engineering and exploring various LangChain
    syntaxes related to it. But first, let’s discuss how prompt engineering is different
    from prompt design. These terms are sometimes used interchangeably, and it creates
    a certain level of confusion. As we discussed in [*Chapter 1*](E_Chapter_1.xhtml#_idTextAnchor001),
    one of the big discoveries about LLMs was that they have the capability of domain
    adaptation by *in-context learning*. It’s often enough to describe the task we’d
    like it to perform in a natural language, and even though the LLM wasn’t trained
    on this specific task, it performs extremely well. But as we can imagine, there
    are multiple ways of describing the same task, and LLMs are sensitive to this.
    Improving our prompt (or prompt template, to be specific) to increase performance
    on a specific task is called prompt engineering. However, developing more universal
    prompts that guide LLMs to generate generally better responses on a broad set
    of tasks is called prompt design.
  prefs: []
  type: TYPE_NORMAL
- en: There exists a large variety of different prompt engineering techniques. We
    won’t discuss many of them in detail in this section, but we’ll touch on just
    a few of them to illustrate key LangChain capabilities that would allow you to
    construct any prompts you want.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find a good overview of prompt taxonomy in the paper *The Prompt Report:
    A Systematic Survey of Prompt Engineering Techniques*, published by Sander Schulhoff
    and colleagues: [https://arxiv.org/abs/2406.06608](https://arxiv.org/abs/2406.06608).'
  prefs: []
  type: TYPE_NORMAL
- en: Prompt templates
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What we did in [*Chapter 2*](E_Chapter_2.xhtml#_idTextAnchor044) is called *zero-shot
    prompting*. We created a prompt template that contained a description of each
    task. When we run the workflow, we substitute certain values of this prompt template
    with runtime arguments. LangChain has some very useful abstractions to help with
    that.
  prefs: []
  type: TYPE_NORMAL
- en: 'In [*Chapter 2*](E_Chapter_2.xhtml#_idTextAnchor044), we introduced `PromptTemplate`,
    which is a `RunnableSerializable`. Remember that it substitutes a string template
    during invocation – for example, you can create a template based on f-string and
    add your chain, and LangChain would pass parameters from the input, substitute
    them in the template, and pass the string to the next step in the chain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'For chat models, an input can not only be a string but also a list of `messages`
    – for example, a system message followed by a history of the conversation. Therefore,
    we can also create a template that prepares a list of messages, and a template
    itself can be created based on a list of messages or message templates, as in
    this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also do the same more conveniently without using chat prompt templates
    but by submitting a tuple (just because it’s faster and more convenient sometimes)
    with a type of message and a templated string instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Another important concept is a *placeholder*. This substitutes a variable with
    a list of messages provided in real time. You can add a placeholder to your prompt
    by using a `placeholder` hint, or adding a `MessagesPlaceholder`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Now our input consists of four messages – a system message, two history messages
    that we provided, and one human message from a templated prompt. The best example
    of using a placeholder is to input a history of a chat, but we’ll see more advanced
    ones later in this book when we’ll talk about how an LLM interacts with an external
    world or how different LLMs coordinate together in a multi-agent setup.
  prefs: []
  type: TYPE_NORMAL
- en: Zero-shot vs. few-shot prompting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we have discussed, the first thing that we want to experiment with is improving
    the task description itself. A description of a task without examples of solutions
    is called **zero-shot** prompting, and there are multiple tricks that you can
    try.
  prefs: []
  type: TYPE_NORMAL
- en: What typically works well is assigning the LLM a certain role (for example,
    “*You are a useful enterprise assistant working for XXX Fortune-500 company*”)
    and giving some additional instruction (for example, whether the LLM should be
    creative, concise, or factual). Remember that LLMs have seen various data and
    they can do different tasks, from writing a fantasy book to answering complex
    reasoning questions. But your goal is to instruct them, and if you want them to
    stick to the facts, you’d better give very specific instructions as part of their
    role profile. For chat models, such role setting typically happens through a system
    message (but remember that, even for a chat model, everything is combined to a
    single input prompt formatted on the server side).
  prefs: []
  type: TYPE_NORMAL
- en: 'The Gemini prompting guide recommends that each prompt should have four parts:
    a persona, a task, a relevant context, and a desired format. Keep in mind that
    different model providers might have different recommendations on prompt writing
    or formatting, hence if you have complex prompts, always check the documentation
    of the model provider, evaluate the performance of your workflows before switching
    to a new model provider, and adjust prompts accordingly if needed. If you want
    to use multiple model providers in production, you might end up with multiple
    prompt templates and select them dynamically based on the model provider.'
  prefs: []
  type: TYPE_NORMAL
- en: Another big improvement can be to provide an LLM with a few examples of this
    specific task as input-output pairs as part of the prompt. This is called few-shot
    prompting. Typically, few-shot prompting is difficult to use in scenarios that
    require a long input (such as RAG, which we’ll talk about in the next chapter)
    but it’s still very useful for tasks with relatively short prompts, such as classification,
    extraction, etc.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, you can always hard-code examples in the prompt template itself,
    but this makes it difficult to manage them as your system grows. A better way
    might be to store examples in a separate file on disk or in a database and load
    them into your prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Chaining prompts together
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As your prompts become more advanced, they tend to grow in size and complexity.
    One common scenario is to partially format your prompts, and you can do this either
    by string or function substitution. The latter is relevant if some parts of your
    prompt depend on dynamically changing variables (for example, current date, user
    name, etc.). Below, you can find an example of a partial substitution in a prompt
    template:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Another way to make your prompts more manageable is to split them into pieces
    and chain them together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also build more complex substitutions by using the class `langchain_core.prompts.PipelinePromptTemplate`.
    Additionally, you can pass templates into a `ChatPromptTemplate` and they will
    automatically be composed together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Dynamic few-shot prompting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As the number of examples used in your few-shot prompts continues to grow, you
    might limit the number of examples to be passed into a specific prompt’s template
    substitution. We select examples for every input – by searching for examples similar
    to the user’s input (we’ll talk more about semantic similarity and embeddings
    in [*Chapter 4*](E_Chapter_4.xhtml#_idTextAnchor152)), limiting them by length,
    taking the freshest ones, etc.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4: An example of a workflow with a dynamic retrieval of examples
    to be passed to a few-shot prompt](img/B32363_03_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.4: An example of a workflow with a dynamic retrieval of examples to
    be passed to a few-shot prompt'
  prefs: []
  type: TYPE_NORMAL
- en: There are a few already built-in selectors under `langchain_core.example_selectors`.
    You can directly pass an instance of an example selector to the `FewShotPromptTemplate`
    instance during instantiation.
  prefs: []
  type: TYPE_NORMAL
- en: Chain of Thought
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Google Research team introduced the **Chain-of-Thought** (**CoT**) technique
    early in 2022\. They demonstrated that a relatively simple modification to a prompt
    that encouraged a model to generate intermediate step-by-step reasoning steps
    significantly increased the LLM’s performance on complex symbolic reasoning, common
    sense, and math tasks. Such an increase in performance has been replicated multiple
    times since then.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can read the original paper introducing CoT, *Chain-of-Thought Prompting
    Elicits Reasoning in Large Language Models*, published by Jason Wei and colleagues:
    [https://arxiv.org/abs/2201.11903](https://arxiv.org/abs/2201.11903).'
  prefs: []
  type: TYPE_NORMAL
- en: There are different modifications of CoT prompting, and because it has long
    outputs, typically, CoT prompts are zero-shot. You add instructions that encourage
    an LLM to think about the problem first instead of immediately generating tokens
    representing the answer. A very simple example of CoT is just to add to your prompt
    template something like “Let’s think step by step.”
  prefs: []
  type: TYPE_NORMAL
- en: 'There are various CoT prompts reported in different papers. You can also explore
    the CoT template available on LangSmith. For our learning purposes, let’s use
    a CoT prompt with few-shot examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We used a prompt from LangSmith Hub – a collection of private and public artifacts
    that you can use with LangChain. You can explore the prompt itself here: [https://smith.langchain.com/hub.](https://smith.langchain.com/hub.
    )'
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, you might want to wrap a CoT invocation with an extraction step
    to provide a concise answer to the user. For example, let us first run a `cot_chain`
    and then pass its output (please note that we pass a dictionary with an initial
    `question` and a `cot_output` to the next step) to an LLM that will use a prompt
    to create a final answer based on CoT reasoning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Although a CoT prompt seems to be relatively simple, it’s extremely powerful
    since, as we’ve mentioned, it has been demonstrated multiple times that it significantly
    increases performance in many cases. We will see its evolution and expansion when
    we discuss agents in *Chapters 5* and *6*.
  prefs: []
  type: TYPE_NORMAL
- en: These days, we can observe how the CoT pattern gets more and more application
    with so-called reasoning models such as o3-mini or gemini-flash-thinking. To a
    certain extent, these models do exactly the same (but often in a more advanced
    manner) – they think before they answer, and this is achieved not only by changing
    the prompt but also by preparing training data (sometimes synthetic) that follows
    a CoT format.
  prefs: []
  type: TYPE_NORMAL
- en: 'Please note that alternatively to using reasoning models, we can use CoT modification
    with additional instructions by asking an LLM to first generate output tokens
    that represent a reasoning process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Self-consistency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The idea behind self-consistency is simple: let’s increase an LLM’s temperature,
    sample the answer multiple times, and then take the most frequent answer from
    the distribution. This has been demonstrated to improve the performance of LLM-based
    workflows on certain tasks, and it works especially well on tasks such as classification
    or entity extraction, where the output’s dimensionality is low.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use a chain from a previous example and try a quadratic equation. Even
    with CoT prompting, the first attempt might give us a wrong answer, but if we
    sample from a distribution, we will be more likely to get the right one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we first created a list containing multiple outputs generated
    by an LLM for the same input and then created a `Counter` class that allowed us
    to easily find the most common element in this list, and we took it as a final
    answer.
  prefs: []
  type: TYPE_NORMAL
- en: '**Switching between model providers**'
  prefs: []
  type: TYPE_NORMAL
- en: Different providers might have slightly different guidance on how to construct
    the best working prompts. Always check the documentation on the provider’s side
    – for example, Anthropic emphasizes the importance of XML tags to structure your
    prompts. Reasoning models have different prompting guidelines (for example, typically,
    you should not use either CoT or few-shot prompting with such models).
  prefs: []
  type: TYPE_NORMAL
- en: Last but not least, if you’re changing the model provider, we highly recommend
    running an evaluation and estimating the quality of your end-to-end application.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned how to efficiently organize your prompt and use different
    prompt engineering approaches with LangChain, let’s talk about what can we do
    if prompts become too long and they don’t fit into the model’s context window.
  prefs: []
  type: TYPE_NORMAL
- en: Working with short context windows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A context window of 1 or 2 million tokens seems to be enough for almost any
    task we could imagine. With multimodal models, you can just ask the model questions
    about one, two, or many PDFs, images, or even videos. To process multiple documents
    (for summarization or question answering), you can use what’s known as the **stuff**
    approach. This approach is straightforward: use prompt templates to combine all
    inputs into a single prompt. Then, send this consolidated prompt to an LLM. This
    works well when the combined content fits within your model’s context window.
    In the coming chapter, we’ll discuss further ways of using external data to improve
    models’ responses.'
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that, typically, PDFs are treated as images by a multimodal LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 'Compared to the context window length of 4096 input tokens that we were working
    with only 2 years ago, the current context window of 1 or 2 million tokens is
    tremendous progress. But it is still relevant to discuss techniques of overcoming
    limitations of context window size for a few reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Not all models have long context windows, especially open-sourced ones or the
    ones served on edge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our knowledge bases and the complexity of tasks we’re handling with LLMs are
    also expanding since we might be facing limitations even with current context
    windows.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shorter inputs also help reduce costs and latency.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inputs like audio or video are used more and more, and there are additional
    limitations on the input length (total size of PDF files, length of the video
    or audio, etc.).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hence, let’s take a close look at what we can do to work with a context that
    is larger than a context window that an LLM can handle – summarization is a good
    example of such a task. Handling a long context is similar to a classical Map-Reduce
    (a technique that was actively developed in the 2000s to handle computations on
    large datasets in a distributed and parallel manner). In general, we have two
    phases:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Map**: We split the incoming context into smaller pieces and apply the same
    task to every one of them in a parallel manner. We can repeat this phase a few
    times if needed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reduce**: We combine outputs of previous tasks together.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 3.5: A Map-Reduce summarization pipeline](img/B32363_03_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.5: A Map-Reduce summarization pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: Summarizing long video
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s build a LangGraph workflow that implements the Map-Reduce approach presented
    above. First, let’s define the state of the graph that keeps track of the video
    in question, the intermediate summaries we produce during the phase step, and
    the final summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Our state schema now tracks all input arguments (so that they can be accessed
    by various nodes) and intermediate results so that we can pass them across nodes.
    However, the Map-Reduce pattern presents another challenge: we need to schedule
    many similar tasks that process different parts of the original video in parallel.
    LangGraph provides a special `Send` node that enables dynamic scheduling of execution
    on a node with a specific state. For this approach, we need an additional state
    schema called `_ChunkState` to represent a map step. It’s worth mentioning that
    ordering is guaranteed – results are collected (in other words, applied to the
    main state) in exactly the same order as nodes are scheduled.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s define two nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`summarize_video_chunk` for the Map phase'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`_generate_final_summary` for the Reduce phase'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The first node operates on a state different from the main state, but its output
    is added to the main state. We run this node multiple times and outputs are combined
    into a list within the main graph. To schedule these map tasks, we will create
    a conditional edge connecting the `START` and `_summarize_video_chunk` nodes with
    an edge based on a `_map_summaries` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s put everything together and run our graph. We can pass all arguments
    to the pipeline in a simple manner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Now, as we’re prepared to build our first workflows with LangGraph, there’s
    one last important topic to discuss. What if your history of conversations becomes
    too long and won’t fit into the context window or it would start distracting an
    LLM from the last input? Let’s discuss the various memory mechanisms LangChain
    offers.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding memory mechanisms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LangChain chains and any code you wrap them with are stateless. When you deploy
    LangChain applications to production, they should also be kept stateless to allow
    horizontal scaling (more about this in [*Chapter 9*](E_Chapter_9.xhtml#_idTextAnchor448)).
    In this section, we’ll discuss how to organize memory to keep track of interactions
    between your generative AI application and a specific user.
  prefs: []
  type: TYPE_NORMAL
- en: Trimming chat history
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Every chat application should preserve a dialogue history. In prototype applications,
    you can store it in a variable, though this won’t work for production applications,
    which we’ll address in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: The chat history is essentially a list of messages, but there are situations
    where trimming this history becomes necessary. While this was a very important
    design pattern when LLMs had a limited context window, these days, it’s not that
    relevant since most of the models (even small open-sourced models) now support
    8192 tokens or even more. Nevertheless, understanding trimming techniques remains
    valuable for specific use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are five ways to trim the chat history:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discard messages based on length** (like tokens or messages count): You keep
    only the most recent messages so their total length is shorter than a threshold.
    The special LangChain function `from langchain_core.messages import trim_messages`
    allows you to trim a sequence of messages. You can provide a function or an LLM
    instance as a `token_counter` argument to this function (and a corresponding LLM
    integration should support a `get_token_ids` method; otherwise, a default tokenizer
    might be used and results might differ from token counts for this specific LLM
    provider). This function also allows you to customize how to trim the messages
    – for example, whether to keep a system message and whether a human message should
    always come first since many model providers require that a chat always starts
    with a human message (or with a system message). In that case, you should trim
    the original sequence of `human, ai, human, ai` to a `human, ai` one and not `ai,
    human, ai` even if all three messages do fit within the context window threshold.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Summarize the previous conversation**: On each turn, you can summarize the
    previous conversation to a single message that you prepend to the next user’s
    input. LangChain offered some building blocks for a running memory implementation
    but, as of March 2025, the recommended way is to build your own summarization
    node with LangGraph.You can find a detailed guide in the LangChain documentation
    section: [https://langchain-ai.github.io/langgraph/how-tos/memory/add-summary-conversation-history/](https://langchain-ai.github.io/langgraph/how-tos/memory/add-summary-conversation-history/)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When implementing summarization or trimming, think about whether you should
    keep both histories in your database for further debugging, analytics, etc. You
    might want to keep the short-memory history of the latest summary and the message
    after that summary for the application itself, and you probably want to keep track
    of the whole history (all raw messages and all the summaries) for further analysis.
    If yes, design your application carefully. For example, you probably don’t need
    to load all the raw history and summary messages; it’s enough to dump new messages
    into the database keeping track of the raw history.
  prefs: []
  type: TYPE_NORMAL
- en: '**Combine both trimming and summarization**: Instead of simply discarding old
    messages that make the context window too long, you could summarize these messages
    and prepend the remaining history.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Summarize long messages into a short one**: You could also summarize long
    messages. This might be especially relevant for RAG use cases, which we’re going
    to discuss in the next chapter, when your input to the model might include a lot
    of additional context added on top of the actual user’s input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Implement your own trimming logic**: The recommended way is to implement
    your own tokenizer that can be passed to a `trim_messages` function since you
    can reuse a lot of logic that this function already cares for.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of course, the question remains on how you can persist the chat history. Let’s
    examine that next.
  prefs: []
  type: TYPE_NORMAL
- en: Saving history to a database
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned above, an application deployed to production can’t store chat history
    in a local memory. If you have your code running on more than one machine, there’s
    no guarantee that a request from the same user will hit the same server at the
    next turn. Of course, you can store history on the frontend and send it back and
    forth each time, but that also makes sessions not sharable, increases the request
    size, etc.
  prefs: []
  type: TYPE_NORMAL
- en: Various database providers might offer an implementation that inherits from
    the `langchain_core.chat_history.BaseChatMessageHistory`, which allows you to
    store and retrieve a chat history by `session_id`. If you’re saving a history
    to a local variable while prototyping, we recommend using `InMemoryChatMessageHistory`
    instead of a list to be able to later switch to integration with a database.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at an example. We create a fake chat model with a callback that
    prints out the amount of input messages each time it’s called. Then we initialize
    the dictionary that keeps histories, and we create a separate function that returns
    a history given the `session_id`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we create a trimmer that uses a `len` function and threshold `1` – i.e.,
    it always removes the entire history and keeps a system message only:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s run it and make sure that our history keeps all the interactions
    with the user but a trimmed history is passed to the LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: We used a `RunnableWithMessageHistory` that takes a chain and wraps it (like
    a decorator) with calls to history before executing the chain (to retrieve the
    history and pass it to the chain) and after finishing the chain (to add new messages
    to the history).
  prefs: []
  type: TYPE_NORMAL
- en: Database providers might have their integrations as part of the `langchain_commuity`
    package or outside of it – for example, in libraries such as `langchain_postgres`
    for a standalone PostgreSQL database or `langchain-google-cloud-sql-pg` for a
    managed one.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the full list of integrations to store chat history on the documentation
    page: [python.langchain.com/api_reference/community/chat_message_histories.html](https://python.langchain.com/api_reference/community/chat_message_histories.html).'
  prefs: []
  type: TYPE_NORMAL
- en: When designing a real application, you should be cautious about managing access
    to somebody’s sessions. For example, if you use a sequential `session_id`, users
    might easily access sessions that don’t belong to them. Practically, it might
    be enough to use a `uuid` (a uniquely generated long identifier) instead of a
    sequential `session_id`, or, depending on your security requirements, add other
    permissions validations during runtime.
  prefs: []
  type: TYPE_NORMAL
- en: LangGraph checkpoints
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A checkpoint is a snapshot of the current state of the graph. It keeps all
    the information to continue running the workflow from the moment when the snapshot
    has been taken – including the full state, metadata, nodes that were planned to
    be executed, and tasks that failed. This is a different mechanism from storing
    the chat history since you can store the workflow at any given point in time and
    later restore from the checkpoint to continue. It is important for multiple reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Checkpoints allow deep debugging and “time travel.”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Checkpoints allow you to experiment with different paths in your complex workflow
    without the need to rerun it each time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Checkpoints facilitate human-in-the-loop workflows by making it possible to
    implement human intervention at a given point and continue further.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Checkpoints help to implement production-ready systems since they add a required
    level of persistence and fault tolerance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s build a simple example with a single node that prints the amount of messages
    in the state and returns a fake `AIMessage`. We use a built-in `MessageGraph`
    that represents a state with only a list of messages, and we initiate a `MemorySaver`
    that will keep checkpoints in local memory and pass it to the graph during compilation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, each time we invoke the graph, we should provide either a specific checkpoint
    or a thread-id (a unique identifier of each run). We invoke our graph two times
    with different `thread-id` values, make sure they each start with an empty history,
    and then check that the first thread has a history when we invoke it for the second
    time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'We can inspect checkpoints for a given thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s also restore from the initial checkpoint for `thread-a`. We’ll see that
    we start with an empty history:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also start from an intermediate checkpoint, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'One obvious use case for checkpoints is implementing workflows that require
    additional input from the user. We’ll run into exactly the same problem as above
    – when deploying our production to multiple instances, we can’t guarantee that
    the next request from the user hits the same server as before. Our graph is stateful
    (during the execution), but the application that wraps it as a web service should
    remain stateless. Hence, we can’t store checkpoints in local memory, and we should
    write them to the database instead. LangGraph offers two integrations: `SqliteSaver`
    and `PostgresSaver`. You can always use them as a starting point and build your
    own integration if you’d like to use another database provider since all you need
    to implement is storing and retrieving dictionaries that represent a checkpoint.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, you’ve learned the basics and are fully equipped to develop your own workflows.
    We’ll continue to look at more complex examples and techniques in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we dived into building complex workflows with LangChain and
    LangGraph, going beyond simple text generation. We introduced LangGraph as an
    orchestration framework designed to handle agentic workflows and also created
    a basic workflow with nodes and edges, and conditional edges, that allow workflow
    to branch based on the current state. Next, we shifted to output parsing and error
    handling, where we saw how to use built-in LangChain output parsers and emphasized
    the importance of graceful error handling.
  prefs: []
  type: TYPE_NORMAL
- en: We then looked into prompt engineering and discussed how to use zero-shot and
    dynamic few-shot prompting with LangChain, how to construct advanced prompts such
    as CoT prompting, and how to use substitution mechanisms. Finally, we discussed
    how to work with long and short contexts, exploring techniques for managing large
    contexts by splitting the input into smaller pieces and combining the outputs
    in a Map-Reduce fashion, and worked on an example of processing a large video
    that doesn’t fit into a context.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we covered memory mechanisms in LangChain, emphasized the need for
    statelessness in production deployments, and discussed methods for managing chat
    history, including trimming based on length and summarizing conversations.
  prefs: []
  type: TYPE_NORMAL
- en: We will use what we learned here to develop a RAG system in [*Chapter 4*](E_Chapter_4.xhtml#_idTextAnchor152)
    and more complex agentic workflows in *Chapters 5* and *6*.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is LangGraph, and how does LangGraph workflow differ from LangChain’s vanilla
    chains?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a “state” in LangGraph, and what are its main functions?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explain the purpose of *add_node* and *add_edge* in LangGraph.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are “supersteps” in LangGraph, and how do they relate to parallel execution?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do conditional edges enhance LangGraph workflows compared to sequential
    chains?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the purpose of the Literal type hint when defining conditional edges?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are reducers in LangGraph, and how do they allow modification of the state?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is error handling crucial in LangChain workflows, and what are some strategies
    for achieving it?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can memory mechanisms be used to trim the history of a conversational bot?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the use case of LangGraph checkpoints?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Subscribe to our weekly newsletter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Subscribe to AI_Distilled, the go-to newsletter for AI professionals, researchers,
    and innovators, at [https://packt.link/Q5UyU](E_Chapter_3.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Newsletter_QRcode1.jpg)'
  prefs: []
  type: TYPE_IMG
