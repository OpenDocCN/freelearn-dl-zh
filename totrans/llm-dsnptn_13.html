<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div id="_idContainer028">
			<h1 id="_idParaDest-160" class="chapter-number"><a id="_idTextAnchor209"/>13</h1>
			<h1 id="_idParaDest-161"><a id="_idTextAnchor210"/>Quantization</h1>
			<p>In this chapter, we’ll dive into <strong class="bold">quantization</strong> methods <a id="_idIndexMarker634"/>that can optimize LLMs for deployment on resource-constrained devices, such as mobile phones, embedded systems, or edge <span class="No-Break">computing environments.</span></p>
			<p>Quantization is a technique that reduces the precision of numerical representations, thus shrinking the model’s size and improving its inference speed without heavily compromising <span class="No-Break">its performance.</span></p>
			<p>Quantization is particularly beneficial in the <span class="No-Break">following scenarios:</span></p>
			<ul>
				<li><strong class="bold">Resource-constrained deployment</strong>: When deploying models on devices with limited memory, storage, or computational power, such as mobile phones, IoT devices, or edge <span class="No-Break">computing platforms</span></li>
				<li><strong class="bold">Latency-sensitive applications</strong>: When real-time or near-real-time responses are required, quantization can significantly reduce <span class="No-Break">inference time</span></li>
				<li><strong class="bold">Large-scale deployment</strong>: When deploying models at scale, even modest reductions in model size and inference time can translate to substantial cost savings in infrastructure and <span class="No-Break">energy consumption</span></li>
				<li><strong class="bold">Bandwidth-limited scenarios</strong>: When models need to be downloaded to devices over limited bandwidth connections, smaller quantized models reduce transmission time and <span class="No-Break">data usage</span></li>
				<li><strong class="bold">Models with redundant precision</strong>: When many LLMs are trained with higher precision than necessary for good performance, they become excellent candidates <span class="No-Break">for quantization.</span></li>
			</ul>
			<p>However, quantization may not be suitable in some cases, such as <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Highly precision-sensitive tasks</strong>: For applications where even minor degradation in accuracy is unacceptable, such as certain medical diagnostics or critical <span class="No-Break">financial models</span></li>
				<li><strong class="bold">Models already optimized for low precision</strong>: If a model was specifically designed or trained to operate efficiently at lower precisions, further quantization may cause significant <span class="No-Break">performance drops</span></li>
				<li><strong class="bold">Small models</strong>: For already compact models, the overhead of quantization operations might outweigh the benefits in some <span class="No-Break">hardware configurations</span></li>
				<li><strong class="bold">Development and fine-tuning phases</strong>: During active development and experimentation, working with full-precision models is often preferable for maximum flexibility and to avoid masking <span class="No-Break">potential issues</span></li>
				<li><strong class="bold">Hardware incompatibility</strong>: Target hardware may lack efficient support for the specific quantized formats you’re planning to use (e.g., some devices may not have optimized INT8 or INT4 <span class="No-Break">computation capabilities)</span></li>
				<li><strong class="bold">Complex architectures with varying sensitivity</strong>: Some parts of an LLM architecture (such as attention mechanisms) may be more sensitive to quantization than others, requiring more sophisticated mixed-precision approaches rather than <span class="No-Break">naive quantization</span></li>
			</ul>
			<p>By understanding these considerations, you can make informed decisions about whether and how to apply quantization techniques to your LLM deployments, balancing performance requirements against <span class="No-Break">resource constraints.</span></p>
			<p>In this chapter, you will learn about different quantization strategies, and by the end of this chapter, you’ll be able to apply quantization methods to make your LLMs more efficient, while ensuring that any reduction in precision has minimal impact on the <span class="No-Break">model’s performance.</span></p>
			<p>In this chapter, we’ll be covering the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Understanding <span class="No-Break">the basics</span></li>
				<li><span class="No-Break">Mixed-precision quantization</span></li>
				<li><span class="No-Break">Hardware-specific considerations</span></li>
				<li>Comparing <span class="No-Break">quantization strategies</span></li>
				<li>Combining quantization with other <span class="No-Break">optimization techniques</span><a id="_idTextAnchor211"/></li>
			</ul>
			<h1 id="_idParaDest-162"><a id="_idTextAnchor212"/>Understanding the basics</h1>
			<p>Quantization refers to<a id="_idIndexMarker635"/> reducing the precision of the weights and activations of a model, typically from <strong class="bold">32-bit floating point</strong> (<strong class="bold">FP32</strong>) to lower precision formats such as <strong class="bold">16-bit</strong> (<strong class="bold">FP16</strong>) or even <strong class="bold">8-bit integers</strong> (<strong class="bold">INT8</strong>). The goal is to decrease memory usage, speed up computation, and make the model more deployable on hardware with limited computational capacity. While quantization can lead to performance degradation, carefully tuned quantization schemes usually result in only minor losses in accuracy, especially for LLMs with <span class="No-Break">robust architectures.</span></p>
			<p>There are two primary quantization methods: <strong class="bold">dynamic quantization</strong> and <span class="No-Break"><strong class="bold">static quantization</strong></span><span class="No-Break">.</span></p>
			<ul>
				<li><strong class="bold">Dynamic quantization</strong>: Calculates <a id="_idIndexMarker636"/>quantization <a id="_idIndexMarker637"/>parameters on the fly during inference based on the actual input values. This adapts better to varying data distributions but introduces some computational overhead compared to <span class="No-Break">static approaches.</span><p class="list-inset">In the following example, we use <strong class="source-inline">torch.quantization.quantize_dynamic</strong> to dynamically quantize the linear layers of a <span class="No-Break">pre-trained LLM:</span></p><pre class="source-code">
import torch
from torch.quantization import quantize_dynamic
# Assume 'model' is a pre-trained LLM (e.g., transformer-based model)
model = ...
# Apply dynamic quantization on linear layers for INT8 precision
quantized_model = quantize_dynamic(
    model, {torch.nn.Linear}, dtype=torch.qint8
)
# Check size reduction
print(f"Original model size: {torch.cuda.memory_allocated()} bytes")
print(f"Quantized model size: {torch.cuda.memory_allocated()} bytes")</pre><p class="list-inset">This <a id="_idIndexMarker638"/>immediately reduces <a id="_idIndexMarker639"/>memory requirements and increases <span class="No-Break">inference speed.</span></p></li>				<li><strong class="bold">Static quantization</strong>: Converts <a id="_idIndexMarker640"/>weights to lower precision<a id="_idIndexMarker641"/> using pre-computed scaling factors determined during a calibration phase with representative data. Once quantized, these parameters remain fixed during inference, providing consistent performance and <span class="No-Break">maximum speedup.</span><p class="list-inset">In the following example, we statically quantize a simple model using <strong class="source-inline">torch.quantization.prepare</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">torch.quantization.convert</strong></span><span class="No-Break">:</span></p><pre class="source-code">
import torch
import torch.nn as nn
import torch.quantization
# Define a simple model
class SimpleModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc = nn.Linear(784, 256)
        self.relu = nn.ReLU()
        self.out = nn.Linear(256, 10)
    def forward(self, x):
        x = self.relu(self.fc(x))
        return self.out(x)
# Create and prepare model for static quantization
model_fp32 = SimpleModel()
model_fp32.eval()
model_fp32.qconfig = torch.quantization.get_default_qconfig(
    'fbgemm')
prepared_model = torch.quantization.prepare(model_fp32)
# Calibration step: run representative data through the model
# (This example uses random data; replace with real samples)
for _ in range(100):
    sample_input = torch.randn(1, 784)
    prepared_model(sample_input)
# Convert to quantized version
quantized_model = torch.quantization.convert(prepared_model)
# Model is now statically quantized and ready for inference
print(quantized_model</pre><p class="list-inset">This statically<a id="_idIndexMarker642"/> quantized model uses fixed scale and zero-point <a id="_idIndexMarker643"/>parameters for each quantized tensor, allowing hardware accelerators to achieve higher <span class="No-Break">inference efficiency.</span></p><p class="list-inset">In contrast to dynamic quantization, static quantization requires a calibration phase with representative data before inference. During this phase, the model is run in evaluation mode to collect activation statistics, which are then used to compute quantization parameters. The weights and activations are then quantized ahead of time and remain fixed during inference, allowing for faster execution and more <span class="No-Break">predictable performance.</span></p></li>			</ul>
			<p>There<a id="_idIndexMarker644"/> are also two main quantization approaches based on when quantization <span class="No-Break">is applied:</span></p>
			<ul>
				<li><strong class="bold">Post-training quantization (PTQ)</strong>: Applies quantization after the model has been fully trained, with minimal or no additional training. Can be implemented as either static (with calibration) <span class="No-Break">or dynamic.</span></li>
				<li><strong class="bold">Quantization-aware training (QAT)</strong>: Simulates quantization effects during training<a id="_idIndexMarker645"/> by<a id="_idIndexMarker646"/> adding fake quantization operations in the forward pass while keeping gradients in full precision. Typically results in static quantization <span class="No-Break">for deployment.</span></li>
			</ul>
			<h2 id="_idParaDest-163"><a id="_idTextAnchor213"/>PTQ</h2>
			<p>PTQ is the<a id="_idIndexMarker647"/> most straightforward form of quantization<a id="_idIndexMarker648"/> and is applied after a model has been fully trained. It doesn’t require model retraining and works by converting the high-precision weights and activations into lower-precision formats, typically INT8. PTQ is ideal for models where retraining is expensive or impractical, and it works best for tasks that are not overly sensitive to <span class="No-Break">precision loss.</span></p>
			<p>Keep in mind that some PTQ methods often require a calibration step on a representative dataset to determine optimal quantization parameters such as scaling factors and zero points, capture activation distributions during inference, and minimize the error between original and quantized model outputs. This calibration process helps the quantization algorithm understand the numerical range and distribution of weights and activations <a id="_idIndexMarker649"/>across the network, allowing more accurate mapping from higher precision formats (such as FP32) to lower precision formats (such as INT8 or INT4), ultimately<a id="_idIndexMarker650"/> preserving model accuracy while reducing memory footprint and computational requirements <span class="No-Break">for deployment.</span></p>
			<p>This example demonstrates <span class="No-Break">static PTQ:</span></p>
			<pre class="source-code">
import torch
import torch.quantization as quant
# Load pre-trained model
model = ...
# Convert model to quantization-ready state
model.eval()
model.qconfig = torch.quantization.default_qconfig
# Prepare for static quantization
model_prepared = quant.prepare(model)
# Apply quantization
model_quantized = quant.convert(model_prepared)</pre>			<p>The model is first put in evaluation mode using <strong class="source-inline">.eval()</strong>, then prepared for quantization using the <strong class="source-inline">.prepare()</strong> method, and finally converted into a quantized model. This method provides an efficient means of deploying LLMs on low-power devices with <span class="No-Break">minimal overhead.</span></p>
			<h2 id="_idParaDest-164"><a id="_idTextAnchor214"/>QAT</h2>
			<p>QAT goes <a id="_idIndexMarker651"/>beyond simple PTQ by incorporating the effects of<a id="_idIndexMarker652"/> quantization into the training process itself. This allows the model to learn how to compensate for the quantization-induced noise, often resulting in better performance than PTQ, particularly for more <span class="No-Break">complex tasks.</span></p>
			<p>During QAT, both weights and activations are simulated at lower precision during training but are kept at higher precision for gradient calculations. This method is particularly useful when the application requires high performance with <span class="No-Break">aggressive quantization.</span></p>
			<p>In the following example, we configure the model for QAT using <strong class="source-inline">get_default_qat_qconfig()</strong>, which simulates the quantized behavior during the <span class="No-Break">training phase:</span></p>
			<pre class="source-code">
import torch.quantization as quant
# Set up QAT
model.train()
model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')
# Prepare for QAT
model_prepared = quant.prepare_qat(model)
# Training loop (for simplicity, only showing initialization)
for epoch in range(num_epochs):
    train_one_epoch(model_prepared, train_loader, optimizer)
    validate(model_prepared, val_loader)
# Convert to quantized version
model_quantized = quant.convert(model_prepared.eval())</pre>			<p>Once <a id="_idIndexMarker653"/>the<a id="_idIndexMarker654"/> model has been trained, it is converted to a quantized version suitable for deployment. QAT typically results in better model accuracy compared to PTQ, particularly for more complex or <span class="No-Break">cr<a id="_idTextAnchor215"/>itical applications.</span></p>
			<h1 id="_idParaDest-165"><a id="_idTextAnchor216"/>Mixed-precision quantization</h1>
			<p><strong class="bold">Mixed-precision quantization</strong> is a<a id="_idIndexMarker655"/> more<a id="_idIndexMarker656"/> flexible approach that leverages multiple levels of numerical precision within a single model. For instance, less critical layers of the model can use INT8, while more sensitive layers remain in FP16 or FP32. This allows greater control over the trade-off between performance and precision. Using mixed-precision quantization can significantly reduce model size and inference time while keeping critical aspects of the <span class="No-Break">LLM intact.</span></p>
			<p>The following code demonstrates an example of quantization to optimize memory usage and speed in LLM training <span class="No-Break">or inference:</span></p>
			<pre class="source-code">
from torch.cuda.amp import autocast
# Mixed precision in LLM training or inference
model = ...
# Use FP16 where possible, fall back to FP32 for sensitive computations
with autocast():
    output = model(input_data)</pre>			<p>In this example, we use the <strong class="source-inline">autocast()</strong> function from PyTorch’s <strong class="bold">Automatic Mixed Precision</strong> (<strong class="bold">AMP</strong>) library<a id="_idIndexMarker657"/> to enable FP16 computation in parts of the model where precision is less critical, while<a id="_idIndexMarker658"/> FP32 is<a id="_idIndexMarker659"/> retained for more sensitive layers. This method helps reduce memory usage and inference time without severe<a id="_idTextAnchor217"/>ly <span class="No-Break">affecting performance.</span></p>
			<h1 id="_idParaDest-166"><a id="_idTextAnchor218"/>Hardware-specific considerations</h1>
			<p>Different <a id="_idIndexMarker660"/>hardware platforms—such as GPUs, CPUs, or specialized accelerators such as TPUs—can have vastly different capabilities and performance characteristics when it comes to handling quantized models. For instance, some hardware may natively support INT8 operations, while others are optimized <span class="No-Break">for FP16.</span></p>
			<p>Understanding the target deployment hardware is crucial for selecting the right quantization technique. For example, NVIDIA GPUs are well-suited to FP16 computations due to their support for mixed-precision training and inference, while CPUs often perform better with INT8 quantization because of hardware-accelerated <span class="No-Break">integer operations.</span></p>
			<p>When deploying LLMs in production, it is important to experiment with quantization strategies tailored to your specific hardware and ensure that your model leverages the <a id="_idTextAnchor219"/>strengths of <span class="No-Break">the platform.</span></p>
			<h1 id="_idParaDest-167"><a id="_idTextAnchor220"/>Comparing quantization strategies</h1>
			<p>When<a id="_idIndexMarker661"/> comparing different quantization strategies, each approach offers distinct advantages and challenges, which can be measured through factors such as implementation complexity, accuracy preservation, performance impact, and <span class="No-Break">resource requirements.</span></p>
			<p>In terms of implementation complexity, PTQ is the simplest to execute, requiring minimal additional work beyond the training of the original model. Dynamic quantization is more complex, as it involves more runtime considerations due to the dynamic handling of activations. Mixed-precision quantization introduces more complexity since it requires a granular, layer-by-layer assessment of precision sensitivity and potentially custom kernel development for optimized execution. QAT ranks as the most complex, requiring the integration of fake quantization nodes into the training graph and extended training times to account for the noise introduced <span class="No-Break">by quantization.</span></p>
			<p>When it <a id="_idIndexMarker662"/>comes to accuracy preservation, QAT performs the best, maintaining accuracy within a small margin of floating-point performance, especially when targeting aggressive quantization (sub-8-bit). Mixed-precision quantization also ranks high in accuracy retention since it allows critical layers to remain in higher precision, balancing performance and accuracy well. PTQ generally maintains accuracy within acceptable limits, though more complex architectures may suffer higher losses in precision. Dynamic quantization typically retains better accuracy than PTQ in RNN-based models, but struggles in CNN architectures, particularly when activations are sensitive to input <span class="No-Break">distribution changes.</span></p>
			<p>In terms of resource requirements, PTQ demands the fewest resources, making it ideal for fast deployment scenarios with limited computational availability. Dynamic quantization ranks slightly higher in resource consumption because it handles activation quantization at runtime, though this is offset by the reduced burden on memory and storage. Mixed-precision quantization, while requiring more resources during implementation due to sensitivity analysis, can be efficient during inference, particularly on hardware that supports multiple precisions. QAT is the most resource-intensive, as it necessitates additional training time, higher memory usage during training, and more compute resources to adapt the model <span class="No-Break">to quantization.</span></p>
			<p>From a performance perspective, PTQ offers considerable improvements in memory savings and computational speedup, typically reducing storage by 75% and achieving 2–4x acceleration on compatible hardware. However, QAT, while similar in compression ratio, adds overhead during training but compensates by producing models that can handle more aggressive quantization without significant performance loss. Dynamic quantization provides similar memory savings as PTQ, but its compute acceleration is generally lower due to runtime overhead. Mixed-precision quantization can offer near-floating-point performance, with speedups dependent on how efficiently the hardware can execute models with varying <span class="No-Break">precision levels.</span></p>
			<p>The decision framework for choosing the optimal quantization strategy hinges on specific project requirements. PTQ is appropriate when fast deployment is a priority, the model architecture is relatively simple, and slight accuracy loss is acceptable. QAT is the best choice when accuracy is paramount, retraining resources are available, and aggressive quantization is needed. Dynamic quantization fits scenarios that require runtime flexibility and the handling of varying input distributions, especially in RNN-based architectures. Mixed-precision quantization is optimal for complex models with varying <a id="_idIndexMarker663"/>precision needs, where both high accuracy and performance are required, and where the hardware can efficiently manage multiple <span class="No-Break">precision formats.</span></p>
			<p>Each quantization strategy serves a different purpose based on the trade-off between accuracy, complexity, performance, and resources, allowing users to tailor their approach to the specific needs of their <span class="No-Break">deployment environment.</span></p>
			<p><em class="italic">Table 13.1</em> compares <span class="No-Break">each strategy.</span></p>
			<table id="table001-3" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Strategy</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Accuracy</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Complexity</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Performance</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Resources</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">PTQ</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Good for simple models; declines <span class="No-Break">with complexity</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Low; <span class="No-Break">minimal setup</span></p>
						</td>
						<td class="No-Table-Style">
							<p>75% storage reduction; <span class="No-Break">2–4x speedup</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Low; minimal <span class="No-Break">compute needed</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">QAT</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Highest; best <span class="No-Break">for sub-8-bit</span></p>
						</td>
						<td class="No-Table-Style">
							<p>High; requires <span class="No-Break">extended training</span></p>
						</td>
						<td class="No-Table-Style">
							<p>High compression with the <span class="No-Break">best accuracy</span></p>
						</td>
						<td class="No-Table-Style">
							<p>High; intensive <span class="No-Break">training needs</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Dynamic</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Good for RNNs; weak <span class="No-Break">for CNNs</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Medium; <span class="No-Break">runtime overhead</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Good memory savings; <span class="No-Break">slower compute</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Medium; runtime <span class="No-Break">processing</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Mixed-Precision</span></p>
						</td>
						<td class="No-Table-Style">
							<p>High; flexible <span class="No-Break">precision options</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Medium-high; <span class="No-Break">layer-specific tuning</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Hardware-dependent <span class="No-Break">speedup</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Medium-high <span class="No-Break">during setup</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 13.1 – Comparison of quantization strategies</p>
			<p>In practice, some scenarios may benefit from combining strategies. For example, you might initially apply PTQ to achieve quick deployment, then use QAT selectively on accuracy-sensitive layers. Another<a id="_idIndexMarker664"/> approach could involve using mixed-precision for specific layers while applying dynamic quantization for activations to balance runtime flexibility <span class="No-Break">and performance.</span><a id="_idTextAnchor221"/></p>
			<h1 id="_idParaDest-168"><a id="_idTextAnchor222"/>Combining quantization with other optimization techniques</h1>
			<p>Quantization<a id="_idIndexMarker665"/> can be combined with other optimization techniques, such as pruning and knowledge distillation, to create highly efficient models that are suitable for deployment on resource-constrained devices. By leveraging multiple methods, you can significantly reduce model size while maintaining or minimally impacting performance. This is especially useful when deploying LLMs on edge devices or mobile platforms where computational and memory resources <span class="No-Break">are limited<a id="_idTextAnchor223"/>.</span></p>
			<h2 id="_idParaDest-169"><a id="_idTextAnchor224"/>Pruning and quantization</h2>
			<p>One of the <a id="_idIndexMarker666"/>most effective combinations is <strong class="bold">pruning</strong> followed <a id="_idIndexMarker667"/>by quantization. First, pruning removes redundant weights from the model, reducing the number of parameters. Quantization then reduces the <a id="_idIndexMarker668"/>precision of the remaining weights, which further decreases the model size and improves inference speed. Here’s <span class="No-Break">an example:</span></p>
			<pre class="source-code">
import torch
import torch.nn.utils.prune as prune
import torch.quantization as quant
# Step 1: Prune the model
model = ...  # Pre-trained LLM model
for name, module in model.named_modules():
    if isinstance(module, torch.nn.Linear):
        prune.l1_unstructured(module, name='weight', amount=0.5)  
        # Prune 50% of the weights
        prune.remove(module, 'weight')
# Step 2: Apply dynamic quantization to the pruned model
quantized_model = quant.quantize_dynamic(
    model, {torch.nn.Linear}, dtype=torch.qint8  # Convert to INT8 precision
)
# Check size reduction
print("Original model size:", torch.cuda.memory_allocated())
print("Quantized model size:", torch.cuda.memory_allocated())</pre>			<p>In this<a id="_idIndexMarker669"/> example, pruning is applied to remove 50% of the weights in all linear layers, and dynamic quantization reduces the precision of the <a id="_idIndexMarker670"/>remaining weights to INT8 for further <span class="No-Break">size reduction.</span></p>
			<p>The result is a compact, highly optimized model that consumes fewer computational resources, making it suitable for deployment on devices with limited <span class="No-Break">hardware capabil<a id="_idTextAnchor225"/>ities.</span></p>
			<h2 id="_idParaDest-170"><a id="_idTextAnchor226"/>Knowledge distillation and quantization</h2>
			<p>Another <a id="_idIndexMarker671"/>powerful combination is knowledge <a id="_idIndexMarker672"/>distillation followed by quantization. In this scenario, a smaller student model is trained to replicate <a id="_idIndexMarker673"/>the behavior of a larger teacher model. Once the student model is trained, quantization is applied to further optimize the student model for deployment. This combination is particularly useful when you need to maintain high performance with minimal <span class="No-Break">computational overhead.</span></p>
			<p>Let’s look at an <a id="_idIndexMarker674"/>example step <span class="No-Break">by step:</span></p>
			<ol>
				<li>Define teacher and <span class="No-Break">student models:</span><pre class="source-code">
import torch
import torch.nn.functional as F
teacher_model = ...  # Larger, fully trained model
student_model = ...  # Smaller model to be trained through distillation</pre></li>				<li>Define<a id="_idIndexMarker675"/> a knowledge distillation <span class="No-Break">loss function:</span><pre class="source-code">
def distillation_loss(
    student_outputs, teacher_outputs, temperature=2.0
):
    teacher_probs = F.softmax(
        teacher_outputs / temperature, dim=1)
    student_probs = F.log_softmax(
        student_outputs / temperature, dim=1)
    return F.kl_div(student_probs, teacher_probs,
        reduction='batchmean')</pre></li>				<li>Add a training loop for <span class="No-Break">knowledge distillation:</span><pre class="source-code">
optimizer = torch.optim.Adam(student_model.parameters(),
    lr=1e-4)
for batch in train_loader:
    inputs, _ = batch
    optimizer.zero_grad()</pre></li>				<li>Forward pass through the teacher and <span class="No-Break">student models:</span><pre class="source-code">
    teacher_outputs = teacher_model(inputs)
    student_outputs = student_model(inputs)</pre><p class="list-inset">The forward pass through both the teacher and student models generates their respective <a id="_idIndexMarker676"/>output logits for the same input data. This parallel inference step is necessary to compute the distillation loss, which quantifies how closely the student replicates the teacher’s behavior. By comparing these outputs, the training process can guide the student to internalize the teacher’s knowledge without requiring the <span class="No-Break">original labels.</span></p></li>				<li>Compute the <span class="No-Break">distillation loss:</span><pre class="source-code">
    loss = distillation_loss(student_outputs, teacher_outputs)
    loss.backward()
    optimizer.step()</pre><p class="list-inset">Computing<a id="_idIndexMarker677"/> the distillation loss allows the student model to learn from the teacher by minimizing the discrepancy between their output distributions. This guides the student to approximate the behavior of the larger, more accurate teacher model while maintaining its own compact structure. By backpropagating this loss and updating the model parameters through optimization, the student progressively aligns its predictions with the teacher, leading to improved performance with reduced <span class="No-Break">model complexity.</span></p></li>				<li>Quantize the distilled <span class="No-Break">student model:</span><pre class="source-code">
quantized_student_model = quant.quantize_dynamic(
    student_model, {torch.nn.Linear}, dtype=torch.qint8
)</pre></li>				<li>Check size and <span class="No-Break">efficiency improvements:</span><pre class="source-code">
print("Quantized student model size:",
    torch.cuda.memory_allocated())</pre><p class="list-inset">Knowledge distillation is used to train a smaller student model that mimics the behavior of the larger teacher model, and quantization is applied to the student model, reducing the precision of its weights to further optimize it <span class="No-Break">for deployment.</span></p></li>			</ol>
			<p>This method<a id="_idIndexMarker678"/> helps maintain performance <a id="_idIndexMarker679"/>while drastically reducing the model’s size, making it well-suited for low-power or <span class="No-Break">real-time applications.</span></p>
			<p>By combining quantization with pruning and knowledge distillation, you can achieve highly optimized models that balance size, efficiency, and performance. These models are especially useful for deployment on edge devices or environments with stringent <span class="No-Break">resour<a id="_idTextAnchor227"/>ce constraints.</span></p>
			<h1 id="_idParaDest-171"><a id="_idTextAnchor228"/>Summary</h1>
			<p>In this chapter, we explored different quantization techniques for optimizing LLMs, including PTQ, QAT, and mixed-precision quantization. We also covered hardware-specific considerations and methods for evaluating quantized models. By combining quantization with other optimization methods, such as pruning or knowledge distillation, LLMs can be made both efficient and powerful for <span class="No-Break">real-world applications.</span></p>
			<p>In the next chapter, we will delve into the process of evaluating LLMs, focusing on metrics for text generation, language understanding, and dialogue systems. Understanding these evaluation methods is key to ensuring your optimized models perform as expected across <span class="No-Break">diverse tasks.</span></p>
		</div>
	</div></div>
<div id="book-content"><div id="sbo-rt-content"><div id="_idContainer029" class="Content" epub:type="part">&#13;
			<h1 id="_idParaDest-172" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor229"/>Part 3: Evaluation and Interpretation of Large Language Models</h1>&#13;
			<p>In this part, we focus on methods for evaluating and interpreting LLMs to ensure that they meet performance expectations and align with the intended use cases. You will learn how to use evaluation metrics tailored to various NLP tasks and apply cross-validation techniques to reliably assess your models. We explore interpretability methods that allow you to understand the inner workings of LLMs, as well as techniques for identifying and addressing biases in their outputs. Adversarial robustness is another key area covered, helping you defend models against attacks. Additionally, we introduce Reinforcement Learning from Human Feedback (RLHF) as a powerful method for aligning LLMs with user preferences. By mastering these evaluation and interpretation techniques, you will be able to fine-tune your models to achieve transparency, fairness, <span class="No-Break">and reliability.</span></p>&#13;
			<p>This part has the <span class="No-Break">following chapters:</span></p>&#13;
			<ul>&#13;
				<li><a href="B31249_14.xhtml#_idTextAnchor230"><em class="italic">Chapter 14</em></a>, <em class="italic">Evaluation Metrics</em></li>&#13;
				<li><a href="B31249_15.xhtml#_idTextAnchor247"><em class="italic">Chapter 15</em></a>, <em class="italic">Cross-Validation</em></li>&#13;
				<li><a href="B31249_16.xhtml#_idTextAnchor265"><em class="italic">Chapter 16</em></a>, <em class="italic">Interpretability</em></li>&#13;
				<li><a href="B31249_17.xhtml#_idTextAnchor276"><em class="italic">Chapter 17</em></a>, <em class="italic">Fairness and Bias Detection</em></li>&#13;
				<li><a href="B31249_18.xhtml#_idTextAnchor286"><em class="italic">Chapter 18</em></a>, <em class="italic">Adversarial Robustness</em></li>&#13;
				<li><a href="B31249_19.xhtml#_idTextAnchor295"><em class="italic">Chapter 19</em></a>, <em class="italic">Reinforcement Learning from Human Feedback</em></li>&#13;
			</ul>&#13;
		</div>&#13;
		<div>&#13;
			<div id="_idContainer030">&#13;
			</div>&#13;
		</div>&#13;
		<div>&#13;
			<div id="_idContainer031" class="Basic-Graphics-Frame">&#13;
			</div>&#13;
		</div>&#13;
	</div></div></body></html>