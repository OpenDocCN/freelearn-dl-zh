# 11

# 顶级专家独家行业洞察：来自世界级专家的观点和预测

随着本书的旅程展开，探索自然语言处理（**NLP**）和大型语言模型（**LLMs**）的广阔领域，我们来到了一个关键的转折点[*第11章*](B18949_11.xhtml#_idTextAnchor551)。本章不仅是对之前主题和讨论的总结，也是通往NLP和LLMs领域未开发潜力以及即将到来的挑战的桥梁。通过各章节，我们的努力是描绘NLP从基础概念到LLMs建筑奇迹的演变，剖析机器学习（**ML**）策略、数据预处理、模型训练以及改变行业和社会互动的实用应用的复杂性。

本章的动机源于对NLP和LLM技术发展速度的敏锐认识以及它们对我们数字社会结构的多方面影响的深刻认识。在探索这些高级模型及其引发的趋势的复杂性时，从创新、研究和道德思考的前沿寻求指导至关重要。与来自不同领域的专家——法律、研究和高管——的对话，为我们理解LLMs如何与专业实践的各个方面相交以及未来可能的发展轨迹提供了灯塔。

本文讨论的主题反映了本书更广泛的主题，但更深入地探讨了LLMs带来的具体挑战和机遇。从减轻数据集中的偏差到将开放研究与隐私相协调，从人工智能（**AI**）带来的组织结构调整到LLMs内部学习范式的演变，每一次讨论都是洞察的拼贴，描绘了当前状态和未来的全面图景。

在本章中，我们将涵盖以下内容：

+   我们专家概述

+   我们的问题和专家的回答

# 我们专家概述

让我们先逐一介绍每位专家。

# Nitzan Mekel-Bobrov, 博士

Nitzan Mekel-Bobrov是eBay的**首席人工智能官**（**CAIO**），负责公司范围内的人工智能和技术创新战略。作为一名训练有素的研发科学家，Nitzan在其职业生涯中致力于开发机器智能系统，这些系统直接集成到关键任务产品中。他领导了多个行业的企业AI组织，包括医疗保健、金融服务和电子商务，Nitzan是实时AI规模化交付转型影响的思想领袖，改变公司的商业模式和核心价值主张，使其客户受益。Nitzan在芝加哥大学获得了博士学位，目前作为eBay纽约市的总经理居住在纽约市。

# David Sontag, 博士

David Sontag是麻省理工学院电气工程和计算机科学系的教授，同时是医学工程与科学研究所和计算机科学及人工智能实验室的一部分。他的研究专注于推进机器学习和人工智能，并利用这些技术来改变医疗保健。此前，他在纽约大学担任计算机科学和数据科学助理教授，是**计算机智能、学习、视觉和机器人**（**CILVR**）实验室的一部分。他还是Layer Health的联合创始人和首席执行官。

# John D. Halamka，医学博士，理学硕士

John D. Halamka，医学博士，理学硕士，Mayo Clinic平台的总裁，在2023年领导了一项影响4500万人的变革性数字健康倡议。他在医疗信息策略和急诊医学方面拥有超过40年的经验，他的工作包括在Beth Israel Deaconess医疗中心（**BIDMC**）服务，从乔治·W·布什到巴拉克·奥巴马的政府顾问，以及作为哈佛医学院教授的教学。作为斯坦福大学、UCSF和UC伯克利大学的校友，Halamka还是Mayo Clinic医学院和科学学院的急诊医学教授。他著有15本书和数百篇文章，并于2020年当选为国家医学院院士。

# Xavier Amatriain，博士

Xavier Amatriain最近是LinkedIn的AI产品策略副总裁，在那里他领导了从平台和基础设施到产品功能的公司级生成式AI工作。他还是Curai Health的董事会成员，这是一家医疗/人工智能初创公司，他共同创立并担任CTO至2022年。在此之前，他领导了Quora的工程团队，并在Netflix担任研究/工程总监，在那里他创立并领导了构建著名的Netflix推荐算法的算法团队。Xavier在学术界和工业界都以研究者的身份开始了他的职业生涯。他拥有超过100篇研究论文（以及6,000次引用），以他在人工智能和机器学习领域，尤其是推荐系统方面的工作而闻名。

# Melanie Garson，博士

Dr. Melanie Garson，Tony Blair研究所的网络安全政策与技术地缘政治负责人，深入研究网络安全政策、地缘政治AI、计算和互联网、科技公司作为地缘政治行为者的崛起、数据治理，以及颠覆性技术、外交政策、国防和外交的交汇点。在伦敦大学学院，她是副教授，教授新兴技术对冲突、谈判和技术外交的影响。Melanie是BBC和CNN等国际论坛和媒体的常客演讲者，她的背景包括在Freshfields Bruckhaus Deringer担任认证调解人和律师。她拥有伦敦大学学院的博士学位和弗莱彻法律与外交学院的硕士学位。

# 我们的问题和专家的回答

我们有机会向这些经验丰富的每个人请教，了解他们的职业生涯如何与AI和LLMs交叉并利用。我们为每个人量身定制了问题，以便他们通过他们的洞察力和观点来教导我们。我们发现这些讨论很有价值，因为它们揭示了常见的话题，对于阅读这本书的任何人来说都将是宝贵的。让我们直接进入正题。

## Nitzan Mekel-Bobrov

Nitzan带来了CAIO的视角，因为他和Ebay正在遇到AI和LLMs所能提供的巨大潜力。他分享了许多CAIO必须解决和决定的问题。

让我们与Nitzan Mekel-Bobrov一起探讨问题和答案。

## Q1.1 – LLM的未来 – 混合学习范式：鉴于学习方案的演变，您认为在LLMs中结合不同学习范式的下一个突破是什么？

在思考在LLMs（大型语言模型）中结合不同学习范式可能出现的下一个突破时，我可以阐述以下想法：

+   **过渡到大型基础模型（LFMs）**：学习范式演变中的下一个明显步骤是向完全多模态模型或LFMs的过渡。这些模型同时整合和处理多种形式的数据（例如，文本、图像、音频），提供更全面的理解并生成更具情境丰富性的响应。这一过渡预计将先于当前模型底层架构的任何重大变化。

+   **可扩展性和模型尺寸优化**：部署LLMs的主要挑战之一是可扩展性。未来的发展可能会集中在创建在尺寸上显著较小的同时保持高性能的模型。这涉及到减少超参数的数量并优化模型以更有效地使用较少的计算资源。

+   **实时模型分类**：实时选择最适合每个特定提示的最佳模型预计将成为一个重要的改进领域。这涉及到优化给定的约束条件，如计算资源、响应时间或性能。它允许根据手头的任务动态选择最合适的模型，而不是仅仅依赖于可用的最大模型。

+   **通过多个LLMs减轻幻觉**：一个模型越通用，生成幻觉（不准确或虚构的信息）的风险就越高。缓解这一问题的有前景的方法是使用多个LLMs，同时使用几个LLMs来检查彼此的答案以验证响应。这不仅提高了准确性，还利用了各种模型之间的协同作用，每个模型都扮演着专业化的角色。

+   **模仿人类能力以实现广泛用途**：为了使大型语言模型（LLMs）具有广泛的应用价值，它们需要更接近地模仿人类智能。这不仅包括生成准确的信息，还包括以更具有情境性和细微差别的方式进行推理，而不仅仅是二元真/假输出。向能够理解和解释类似于人类思维过程的复杂、模糊逻辑的模型演变是未来突破的关键领域。

这些想法指向一个未来，其中人工智能模型不仅更高效和可扩展，而且显著更智能，能够进行细微的理解和推理。对多模态、可扩展性、实时优化和增强推理能力的强调突出了人工智能发展向更全面、类似人类智能和实用性的方向。

### Q1.2 – 在同时使用多个LLMs的背景下，我们如何优化这些“专家”模型之间的协同作用，以实现更精细和全面的输出？

使用多个LLMs的概念可以超越验证和减少幻觉的范畴。一个更广泛的想法，有时被称为K-LLMs，可以利用多个LLMs来回答问题或创建复杂的解决方案。正如之前讨论的那样，其中一个方案是每个模型检查其他模型的答案以验证响应。另一种可能的方法是分配给它们各自的角色，每个角色都有其特定的专业领域（例如，产品经理、设计师、前端工程师、后端工程师和QA工程师），并且它们在解决方案上迭代，形成一个专家团队。这也可以允许使用更小、更专业的LLMs，因此训练成本更低，处理速度更快，计算需求更小。

## Q2.1 – 随着首席人工智能官在企业管理层中的重要性日益增强，你预见在弥合人工智能潜力与实际商业应用之间的差距时有哪些独特的挑战，以及首席人工智能官的角色应该如何演变以应对这些挑战？

作为首席人工智能官，我的角色包括在我们组织内部各个领域导航人工智能的广泛影响。以下是我关注的几个最重要的领域：

+   **人工智能影响的广度**：人工智能在大型企业内部各个领域的广泛影响需要首席人工智能官（CAIO）对后台和前台需求都有深入的理解。这要求公司在整个范围内广泛参与，以识别和优先考虑人工智能变革性影响的机会。

+   **努力和优先级**：由于无法参与大型企业的每个方面，这个角色需要大量的努力来进行优先级排序。这包括在有限的数据下做出决策，确定最大的投资回报在哪里，借鉴其他公司的经验，并了解内部运营以评估人工智能可以产生重大影响的领域。

+   **快速产生影响的压力**：存在明显的压力迅速产生可衡量的结果，同时应对现有的技术、流程和人员限制。在不彻底改变现有流程的情况下将人工智能创新整合到当前生态系统中是一个巨大的挑战。

### Q2.2 – 作为关于CAIO角色的问题的延续，你能告诉我监管方面的内容以及CAIO的角色如何与之相遇吗？

在监管方面，我花费了大量时间与我们的法律团队、合规官员和信息安全人员进行讨论。人工智能监管的领域在很大程度上是未知的，这意味着在先例很少的情况下制定指南和护栏。理想情况下，我寻求明确的“可以”和“不可以”的指导，但通常，这是一个定义这些指南的协作努力。这次持续的对话侧重于管理风险、保护我们的客户和推进创新，同时最大限度地减少我们的风险敞口。

我们已经成立了一个负责人工智能的办公室，负责定义人工智能应用的适当商业环境。这项工作的大部分内容涉及在仅仅法律合规性之外的伦理考量，尤其是由于法规往往针对高风险领域。然而，典型的公司运营中有大约90%不属于这些高风险类别，使我们处于监管灰色地带。在这里，伦理判断变得至关重要。虽然我支持新兴的全球法规，但我认识到它们提供了一个框架而不是完整的解决方案。这些法规主要关注高风险领域，仍然需要在我们日常运营中进行细微的应用。

从本质上讲，作为CAIO的角色要求我采取一种灵活的方法，平衡技术专长、伦理前瞻性和战略规划。这是关于负责任地利用人工智能的潜力，并有效地在业务中应用人工智能的广泛适用性和人工智能伦理与法规不断变化的格局中导航。

## Q3 – 基础模型和主要科技公司向开源的策略如何影响数据所有权及其对企业的价值？

作为首席人工智能官，我发现自己经常在思考我们当前以人工智能驱动的商业范式下，专有数据所有权的重要性如何变化。一方面，基础模型正在民主化人工智能，显著降低了缺乏大量专有数据集的公司进入门槛。这些模型提供的性能似乎与在专用、专有数据上训练的一样强大。这一趋势可能表明，拥有独特数据集的价值可能正在下降，因为强大的AI能力正在被更广泛的实体所获取，而这些实体没有大量的数据资产。

然而，这个领域是微妙的。我们正在见证微调和其他预训练技术的兴起，这些技术将这些通用模型定制到特定需求，微妙地重新确立了独特数据的重要性。这种定制能力暗示着数据所有权可能会演变而不是减少其相关性，成为新的竞争优势或进入障碍。

此外，像Meta这样的主要公司向开源其AI解决方案的战略转变并非纯粹利他，而是旨在颠覆现状，挑战微软和谷歌等巨头的统治地位。这一向开源的转变正在重塑行业，迫使这些巨头在其模型周围增加更全面、面向企业的生态系统。最终的价值主张不再是模型本身，而是整个包——支持它们的生态系统，使其对企业应用具有吸引力。

在此背景下，监管者的角色以及不同国家在数据隐私和共享方面的立场差异开始发挥作用，可能将市场引向各种方向。这创造了一个复杂的环境，企业必须在这个环境中不仅应对技术进步，还要应对可能影响数据所有权战略价值的监管格局。

总结来说，虽然通过基础模型和开源倡议实现人工智能的民主化挑战了传统的数据所有权观念，但它同时也开辟了新的竞争差异化途径。企业必须保持敏捷，根据这些发展重新评估他们的数据策略，以有效地利用人工智能，同时在这个不断发展的环境中应对监管和战略的微妙之处。

## 大卫·桑塔格

大卫·桑塔格拥有丰富的学术研究记录，他将这些研究与行业参与和合作相结合。在本节中，他分享了他对LLMs（大型语言模型）中一些新兴发展的独特见解。

让我们与大卫·桑塔格一起探讨问题和答案。

## Q1 – 随着我们朝着创建更公平和无偏见的数据库迈进，您认为哪些策略在识别和减轻大型数据集中的隐含偏见方面最为有效？

在医疗保健领域，机器学习的应用已远远超出单纯的预测分析，它正在培养能够从根本上改变患者护理和结果的认识。这一领域的复杂性通过捕捉健康的社会决定因素——如生活环境、食品安全和交通可及性等变量——的挑战得到强调，这些变量对健康结果有重大影响。然而，当前的数据收集和模型训练领域往往忽视了这些关键但难以量化的患者生活方面，导致在个性化应用机器学习预测方面存在差距。

一个主要问题源于依赖于无法完全封装个体复杂性的数据集中的替代品或代理。这种依赖可能会掩盖每个患者固有的细微差别，从而削弱机器学习在医疗环境中产生有意义变化的可能性。数据模型训练所依据的内容与它们应用于的现实世界环境之间的差异进一步复杂化了这个问题。例如，在通用文本数据上训练的LLMs缺乏细腻应用所需的上下文丰富性，例如根据个人的社会环境定制医疗建议。

这种脱节不仅阻碍了模型在提供相关见解方面的效用，还引入了意外的偏见。当模型缺乏上下文或对其训练数据的局限性缺乏了解时，将泛化的预测错误地应用于个别案例时，这些偏见就会出现。解决这一挑战需要共同努力，丰富数据收集过程，以捕捉患者社会决定因素的更全面视角，并确保模型能够有效地解释和应用这些信息。

为了减轻大数据集中的隐含偏见并推进公平的机器学习模型，一个关注数据收集、分析和模型优化的多方面方法是必不可少的。关键策略包括将歧视指标分解为偏差、方差和噪声（“*我的分类器为什么具有歧视性？*”）以识别不公平的具体来源，强调丰富且足够大的训练样本在提高公平性和准确性方面的关键作用。

此外，通过增加更具代表性的样本和相关变量来扩充数据集可以解决不同群体之间预测性能的差异（“*机器学习中的潜在偏见及其对健康保险公司的应对机会*”）。实施这些策略需要严格、持续的评估模型输出和影响，确保它们不会延续现有的偏见或引入新的偏见。向算法警觉、敏感数据的道德使用以及将不同观点纳入模型开发过程中的行业协作努力也是至关重要的。通过将公平性作为模型准确性和效用基本方面的优先考虑，我们可以利用机器学习在各个领域提供更公正和公平的结果。

总结来说，在深入探讨之前提出的创建公平和无偏数据集的策略之前，承认机器学习在医疗保健中面临的根本挑战至关重要。这些挑战包括对病人社会决定因素的更深入理解，以及弥合数据模型训练所依据的内容与它们部署的上下文之间的差距。解决这些问题是充分利用机器学习以改善医疗保健结果并确保机器学习的创新对病人护理产生积极和公平贡献的先决条件。

## Q2 – 你如何看待这些策略随着NLP技术的进步而演变，你预见在LLMs（大型语言模型）内部结合不同学习范式方面的下一个突破是什么？

随着NLP技术的持续发展，增强其效用和公平性的策略也在进步，尤其是在麻省理工学院David Sontag团队领导的工作中。David分享了他们在实验室中引领的这三项研究进展：

1.  **透明度**：他们研究的基础是开发方法，以提供对NLP模型输出的每一块信息的全面归因。这涉及到回溯到训练数据，以识别影响模型预测的来源。这种做法不仅增强了NLP应用的信誉和可靠性，而且赋予了用户验证他们所呈现信息来源的能力。通过使输出到输入的清晰谱系透明化，用户可以理解模型决策背后的理由，从而增强对NLP系统的信任。

1.  **通用LLM在特定领域的应用**：该团队正在探索创新的方法，将通用LLM如GPT-4适应特定领域，而无需进行大量的重新训练或微调。这是通过一种允许这些模型协作的方法实现的，它们利用其通用能力，同时结合具有特定领域知识（如医学）的模型，以提供更准确和相关的输出。这种策略标志着向更适应性和高效使用现有NLP资源的转变，确保该领域的进步可以迅速应用于各种特定情境，而不会产生高昂的成本或时间延迟。（*个人评论*：这个用例是我们覆盖的两个用例之一的一个特殊情况，这两个用例都围绕同时利用多个LLM。第一个是K-LLMs方案，其中多个模型在一个旨在模仿专家委员会的设置中相互交互。每个模型都有自己的角色（例如，软件开发者与QA工程师合作，或项目经理与设计师合作），并且它们轮流改进结果输出。在这里，每个角色可以由同一个模型扮演；例如，每个角色可以由OpenAI的GPT代表，或者不同的模型可以承担不同的角色，其中每个模型承担的角色是基于模型的优势和劣势选择的。第二个是一个有几个不同模型的情况，每个模型都有自己的优势和劣势（例如，一个速度快但不会产生高质量见解；另一个速度慢但非常精确），并且“正确”的模型将通过一个针对给定约束条件优化的决策过程根据每个输入选择。例如，需要对给定的小句子集进行二进制*是/否*推理的提示可能被引导到一个简单的LLM，而需要应用法律判断的提示可能被引导到最新的GPT版本。）

1.  **高效微调大型语言模型**：他们研究的另一个焦点是如何以数据高效和计算高效的方式对大型语言模型（LLM）进行微调。这包括识别LLM架构中最具影响力的超参数进行调整，确定哪些应该保持固定，哪些应该调整以适应特定需求。这里的目的是在优化特定应用的同时，保持原始模型的结构完整性和强度，从而以最小的资源消耗扩展LLM在各个领域的实用性。

这些进步强调了改善自然语言处理（NLP）技术灵活性、透明度和适用性的更广泛承诺。通过关注这些关键领域，麻省理工学院（MIT）的David Sontag的研究旨在推动该领域的发展，确保NLP工具不仅更强大，而且对各个领域的用户来说更易于访问、理解和道德。这种方法与学术和实践的最高标准相一致，有望塑造下一代在医疗保健等领域的NLP应用。

## 第三季度——我们正在见证围绕人工智能的监管从训练数据和模型使用方面的持续演变。在这个受监管的环境中，对于未来LLMs的发展有何影响？

在围绕人工智能不断演变的监管环境中，对于未来大型语言模型（LLMs）的发展产生了重大影响。随着监管的持续进步，重点关注人工智能的安全性，包括国家安全威胁和人工智能的道德使用，LLMs开发和部署的框架正在重塑：

1.  **演变的监管**：人工智能的监管预计将加剧，强调在人工智能技术应用中的安全性和适当性。这种不断演变的监管环境需要采取积极主动的合规方法，LLMs的开发者必须确保他们的模型不仅有效，而且与新兴的法律和伦理标准保持一致。这些法规旨在减轻与人工智能相关的风险，引导行业走向负责任的创新。

1.  **数据和模型的质量**：行业和学术界都在积极提升用于训练模型的数据质量。追求质量是开发更准确、更可靠的LLMs的基础，因为模型从精心整理和具有代表性的数据中受益。研究表明，在数据使用方面具有效率的潜力，选择“正确”的数据可以大幅减少对大型数据集的需求，同时不损害模型的表现。这种效率不仅符合监管对透明度和问责制的需求，还为更可持续的模型开发流程开辟了途径。

1.  **元数据和模型监控**：将元数据纳入训练过程代表了向更大责任和可解释性转变的关键步骤。通过将详细元数据附加到用于模型训练的数据点上，开发者可以提供清晰的审计轨迹，阐明模型如何得出结论。这种能力对于监控模型性能和确保LLMs在伦理和法律边界内运行至关重要。这也反映了更广泛的行业趋势，即拥抱机器学习可解释性方法，使利益相关者能够审查和理解LLMs的决策过程。

这些发展，由大卫·桑塔格的洞察力所预测，强调了这样一个未来：LLMs不仅在技术上先进，而且在道德上扎根，符合监管要求。这一轨迹确保了随着LLMs在各个领域的日益嵌入，它们将以优先考虑安全、公平和透明的方式进行。这种做法不仅符合学术卓越的最高标准，而且使LLMs能够对社会产生积极和负责任的影响。

## 约翰·D·哈拉姆卡

约翰为这一章节带来了执行层面的内容。在这一部分，他阐述了一系列见解和行动，公司和组织可以实施，以实现AI的进步，并保持高度监控和负责任的方向。

让我们与约翰·D·哈拉姆卡一起探讨问题和答案。

### Q1.1 – 梅奥诊所如何制定政策，在NLP社区中协调开放、可重复研究的需求与严格的隐私保护，以及它是如何在国际法规的复杂环境中进行导航的？

在协调开放、可重复研究的需求与NLP社区中保护个人隐私的需求之间，由**梅奥诊所平台**开创的“数据背后的玻璃”模型提供了一个令人信服的解决方案。这一模型在处理敏感健康数据方面代表了范式转变，体现了一种以平台为中心的方法，确保数据质量、符合监管要求，并且在整个数据生命周期中维护患者信任。

在其核心，梅奥诊所平台连接作为一个分布式数据网络，体现了联邦架构。在这个网络中，合作伙伴贡献他们独特的数据集，同时保持对其数据的严格控制，保护在其组织IT边界内的隐私和机密性。这种联邦方法为数据共享和利用提供了一个协作且安全的环境。

这一模型成功的关键在于细致入微的数据去标识化过程。通过采用符合隐私法律和法规的行业认可的统计方法，数据被匿名化，确保在保留数据在研发价值的同时，保护个人隐私。如哈希、统一日期偏移和标记化等技术被用于模糊数据，便于在联邦学习中使用而不损害患者隐私。

此外，Connect 所基于的“设计即安全”哲学确保数据**知识产权**（**IP**）始终处于各自所有者的控制之下，仅按授权访问。这种方法不仅保护了隐私，还通过允许梅奥诊所平台客户在去标识化数据群体上开发、培训和验证算法，促进了创新。严格的控制措施，包括代码库审查、严格的访问管理和禁止数据导入导出，进一步强化了平台对隐私和安全的承诺。

“玻璃背后的数据”模型独特地定位在解决不断变化的监管环境。随着国际监管机构加强对 AI 和 ML 应用程序的审查，梅奥诊所平台的可适应框架旨在应对全球隐私法规的复杂拼图。无论是欧盟的**通用数据保护条例**（**GDPR**），巴西的**通用数据保护法**（**LGPD**），还是中国的安全和隐私规则，该模型都确保了合规性，同时促进了全球合作。

总结来说，“玻璃背后的数据”模型为自然语言处理（NLP）社区实现双重目标提供了一条可行的途径：一方面促进开放研究，另一方面保护隐私。通过去标识化、安全化和联邦化数据，梅奥诊所平台使数据的使用民主化，同时不损害患者隐私，为在透明度和隐私之间保持平衡的时代树立了负责任的数据处理的先例。这一模型展示了技术创新与对道德标准的深厚承诺相结合如何为医疗保健乃至更广泛的领域中的变革性进步铺平道路，确保患者信任始终处于数字健康倡议的前沿。

### Q1.2 – 在这个监管环境中，LLM 的未来发展趋势有哪些影响？

*让我们首先回顾一个强大的指导来源，该来源旨在促进围绕 LLM 和 AI 在医疗保健领域的使用进行政策制定：T***he Coalition of Health** **AI** *(***CHAI™***).*

在其网站上，CHAI 讨论了以下倡议：

“健康人工智能联盟（CHAI™）([https://coalitionforhealthai.org/](https://coalitionforhealthai.org/)) *致力于制定指导方针，通过采用可信、公平和透明的健康人工智能系统来推动高质量的医疗保健。我们提供了一个可信赖的 AI 实施指导草案和医疗保健 V1.0 的保证，供公众[https://coalitionforhealthai.org/insights](https://coalitionforhealthai.org/)) *审查和评论。*”

CHAI通过制定采用可信、公平和透明的健康AI系统的指南，为医疗保健行业做出贡献。他们关于可信AI实施和保证的草案蓝图强调了与**美国商务部国家标准与技术研究院**（**NIST**）的AI风险管理框架保持一致的重要性，并将这些概念扩展到医疗保健领域。主要贡献包括以下方面：

+   **框架一致性**：将指导与NIST定义的结构化，重点关注验证、可靠性和AI风险管理的*映射*、*衡量*、*管理*和*治理*功能

+   **可信度要素**：强调在AI的设计、开发和部署中承担专业责任和社会责任，以积极和可持续地影响社会

+   **医疗保健中的实用性**：倡导AI算法不仅有效可靠，而且对病人和医疗保健提供有益，需要临床验证和持续监控

+   **验证和可靠性**：强调在受监管的AI/ML技术中，包括**软件作为医疗设备**（**SaMD**），软件验证的重要性，并确保AI系统的准确性、可操作性和预期用途

+   **可重复性和可靠性**：解决AI/ML对硬件和软件变化的敏感性，强调在医疗保健环境中实现可靠性和可重复性的必要性

+   **监控和测试**：倡导对AI工具进行持续监控和测试，以确保可靠性，检测输入数据或工具输出的变化，并保持人机协作的质量

+   **可用性和益处**：将可用性定义为依赖于模型的环境、最终用户视角、简单性和工作流程集成，并衡量算法对预期结果的影响

+   **安全措施**：确保AI系统不会对人类生命、健康、财产或环境构成风险，重点是防止比现状更糟糕的结果

+   **责任和透明度**：强调可审计性、最小化伤害、报告负面影响，并使设计权衡和纠正机会清晰

+   **可解释性和可理解性**：平衡AI系统在操作上可理解以及在输出上有意义的需求，这对于建立用户对健康AI的信任至关重要

+   **公平性和偏见管理**：解决选定群体之间的不同表现或结果，并确保AI不会加剧偏见或不利公平结果的风险

+   **安全和弹性**：强调AI系统需要能够承受不利事件，维持功能，并确保机密性、完整性和可用性

+   **隐私增强**：遵守医疗保健中隐私的既定标准，如**健康保险可携带性和责任法案**（**HIPAA**），同时适应其他司法管辖区规则，如GDPR

CHAI的努力旨在确保医疗保健中的人工智能系统以符合伦理标准、提高患者护理和维护公众信任的方式开发和部署。

## Q2 – 人工智能驱动的组织结构 – 你预测人工智能将以哪些方式继续重塑公司的组织结构，以最大化人工智能的益处？

“人工智能确实重塑了公司。特别是，在梅奥诊所，我们问自己，我们应该将人工智能运营集中化还是分散到组织内部？我观察到许多案例中采用了不同的方法。在梅奥诊所，我们的做法是将所有人工智能工作去中心化，但将数据治理和政策制定集中化。这使创新没有遗憾。”

让我们回顾一下这种工作模式的一些关键好处。

### 去中心化人工智能工作模型的好处

+   **增强创新和敏捷性**：通过去中心化人工智能运营，像梅奥诊所这样的组织营造了一个环境，使各个部门能够根据其特定的需求和挑战创新和应用定制的人工智能解决方案。这种灵活性允许更快地适应和实施人工智能技术。

+   **赋权和所有权**：去中心化人工智能赋予个人团队和部门探索人工智能应用和解决方案的自主权。这种所有权感可以推动更加投入和有动力的团队，从而带来创新解决方案和运营改进。

+   **多样化的应用和解决方案**：去中心化的方法使人工智能在组织的不同方面得到更广泛的探索。不同的部门可以通过人工智能解决各种问题，从而产生一系列针对各种组织需求的人工智能驱动解决方案和应用。

+   **快速实验和学习**：随着人工智能的去中心化，团队可以快速测试、学习和迭代人工智能项目，而不会受到集中决策瓶颈的限制。这种快速实验可以导致更快地发现，并从成功和失败中更有效地学习。

### 集中化数据治理的好处

+   **数据安全和隐私**：集中化数据治理确保有持续的政策和协议来保护敏感信息并遵守隐私法规。这在医疗保健和其他数据隐私至关重要的领域至关重要。

+   **数据质量和完整性**：集中化的数据治理方法有助于在整个组织中保持高数据质量和完整性。通过统一的标准和政策，组织可以确保人工智能模型是在准确、干净和可靠的数据上训练的。

+   **高效资源管理**：集中数据治理允许更有效地管理数据资源，避免重复并确保数据资产在整个组织中得到最佳利用。这可能导致成本节约和更高效的数据存储和计算资源的使用。

+   **法规合规**：通过集中数据治理，组织可以更有效地确保符合不断变化的法规要求。统一的数据政策制定方法可以帮助在复杂的法律环境中导航，并降低不合规的风险。

通过采用一种将AI工作去中心化同时集中数据治理和政策制定的模型，像梅奥诊所这样的组织可以在确保数据安全、质量和法规合规的同时，刺激AI应用的创新和适应性。这种平衡的方法可以实现“无遗憾的创新”，允许以负责任和有效的方式探索和实施AI解决方案。

## Q3 – 伦理担忧和应对过度委托的策略——随着AI继续渗透到日常决策过程中，你推荐哪些策略来防止过度依赖AI系统，并保持健康水平的人类批判性思维和自主性？

**医疗保险与医疗补助服务中心（Centers for Medicare & Medicaid Services，**CMS**）关于拟议规则制定的公告在提供关于AI角色的指导方面非常有帮助。约翰解释说：“*该提案指出，所有AI都应增强，而不是替代，* *人类决策*。”

我们深入研究了在线提出的提案([https://www.govinfo.gov/content/pkg/FR-2022-08-04/pdf/2022-16217.pdf](https://www.govinfo.gov/content/pkg/FR-2022-08-04/pdf/2022-16217.pdf))。特别是，我们关注了第47880页的*临床算法在决策中的应用（§ 92.210）*部分，并得出以下结论：

+   **临床算法的非歧视性**：CMS强调，临床算法不应导致基于种族、肤色、国籍、性别、年龄或残疾的歧视。临床算法的使用不应被禁止，而应受到监控，以防止产生歧视性结果。

+   **增强而非替代**：CMS提议临床算法应增强而非替代人类临床判断。不考虑其潜在的歧视性影响而过度依赖算法可能违反现有法规。

+   **基于临床算法的决策责任**：虽然实体对其未开发的算法不承担责任，但如果这些决策导致歧视，它们可能因基于这些算法做出的决策而承担责任。

+   **算法偏差意识**：CMS强调临床算法中普遍存在的“种族校正”或“种族规范”做法，这些做法可能导致基于种族或民族的歧视性待遇。他们提倡使用没有已知偏差的更新工具。

+   **种族和民族意识变量的适当使用**：虽然种族和民族变量可能在某些情况下用于解决健康差异，但CMS警告不要以可能导致歧视的方式使用它们。

+   **对残疾和年龄的担忧**：算法也可能歧视残疾人和老年人，尤其是在公共卫生紧急事件期间危机标准和资源分配决策中。

+   **拟议规则§ 92.210**：这项新规定明确禁止通过使用临床算法进行歧视，旨在确保这些工具不会取代临床判断或导致歧视性结果。

+   **指导和专业技术支持**：CMS表示致力于提供专业技术支持以支持遵守民权义务，征求对规定范围、缓解措施和所需技术援助类型的意见。

总结来说，CMS的方法强调了利用AI改善医疗保健和确保这些工具不会削弱人类判断或延续歧视之间的关键平衡。他们提出的规则和征求意见反映了持续努力制定对AI在医疗决策中作用的响应性和负责任指南。

## Xavier Amatriain

让我们与Xavier Amatriain一起探讨问题和答案。

## Q1.1 – LLM的未来 – 混合学习范式：鉴于学习方案的演变，您如何看待在LLM中结合不同学习范式的下一个突破？

最重要的是要记住，我们在LLM研究领域还非常早期，这是一个快速发展的领域。虽然注意力驱动的Transformer已经带我们走得很远，但还有许多其他方法的空间。例如，在预训练方面，现在有许多关于后注意力方法（如**结构化状态空间模型**（**SSMs**或**S4**））的有趣研究。同样，**专家混合**（**MoEs**），虽然不是新概念，但最近证明它们在提供更小、更高效的模型方面具有惊人的力量，例如Mistral AI的Mixtral。而这只是预训练空间。对于对齐，我们已经看到**直接偏好**（**DP**）或**卡尼曼-特沃斯基**（**KT**）等方法迅速显示出大量前景。不用说，自我博弈作为改进和对齐的机制也被使用。

我在这里的主要信息是，我们应该保持警惕，并期待在接下来的几年里，会有大量的创新迅速出现在我们面前。我认为几年后，我们会回顾并认为GPT4架构是过时且完全不高效的。非常重要的一点是，其中一些改进将使LLM在准确性上更好，同时也在成本和规模上更加高效，因此我们应该期待GPT4类似的模型能在我们的手机上运行。

### Q1.2 – LLM的未来 – 集成方法中的专用LLM：考虑到K-LLMs方法，即使用具有互补优势的多个LLM的概念，应该有哪些具体标准来指导在集成中LLMs的选择和组合以处理复杂任务？

在LLMs的背景下，有许多方式和地方可以使用集成技术。选择和组合这些技术的标准取决于用途和这种组合发生的地方。以下是三个使用LLMs组合有用的地方：

在预训练阶段，**专家混合**（**MoEs**）是一种集成形式，其中不同的深度神经网络被组合起来以提高输出。选择和权衡不同专家的权重是在预训练期间学习的。重要的是，其中一些权重为零，这使得推理变得更加高效，因为并非所有专家都需要完成所有任务。

另一种组合不同LLMs的方法是在蒸馏阶段。在一些方法，如教师/学生蒸馏中，LLMs被用来生成数据，然后训练一个更小或更具体的模型。每个LLM的选择和权重是在学生模型的训练阶段学习的。

最后，我们可以通过将每个LLM实例视为一个代理来在应用层组合LLMs。这导致了多代理系统的概念，其中专门针对特定任务的LLM代理被组合起来执行更复杂的任务。

## Q2 – AI驱动的组织结构 – 你预测AI将以哪些方式继续重塑内部业务运营，公司应如何准备调整其组织结构以最大化AI的益处，尤其是在决策和运营效率方面？

生成式AI将彻底改变组织的各个方面。我坚定的预测是AI将成为组织中的另一个成员。例如，软件工程师将在日常工作中与一个AI（或几个AI）协作。这将使他们的效率提高不是10倍而是100倍。

当然，这样的革命性力量将改变我们组织团队、招聘人员或评估他们绩效的方式。我认为我们为即将到来的世界做好准备非常重要，在这个世界中，组织中的任何人都将需要具备与AI协作和工作的能力。

## 梅兰妮·加森

梅兰妮带来了在法律和监管领域工作的丰富经验。随着AI和LLMs继续推动政策和指南，这种专业知识的价值变得越来越清晰和重要。

让我们与梅兰妮·加森一起回顾问题和答案。

## Q1 – 由于这本书旨在面向ML和AI世界的技术实践者，他们了解各种法律和监管方面会有什么价值？

理解围绕人工智能的地缘政治格局，包括监管、法律和风险考虑因素，对于从开发者到**主题专家**（**SMEs**）的技术实践者来说至关重要。在人工智能领域，随着公司进行战略和政策讨论，让技术熟练的个人参与这些对话是必不可少的。决策者越来越认识到，在决策过程中拥有技术视角的价值，以确保决策全面且受技术可能性和限制的指导。

一个信息丰富的技术专业人士可以有效地传达他们的见解，弥合技术潜力和执行愿景之间的差距。这种能力不仅增强了决策过程，还确保了策略的稳健性、合规性，并意识到不断变化的监管格局。

此外，随着组织努力使自己的运营与监管要求保持一致并减轻潜在风险，他们很可能会建立专门的团队，负责开发和实施符合这些新战略方向的科技解决方案。熟悉塑造人工智能行业的法律和监管动态的技术专家将发现自己处于显著优势，准备为这些团队做出实质性贡献。他们的专业知识不仅使他们成为无价之才，而且使他们为在这些战略举措中的领导角色做好准备，推动合规性、创新和在全球监管市场中获得竞争优势。

## Q2 – 从法律专家的角度来看，我们如何对与人工智能技术迅速发展相关的各种风险进行分类？

从法律角度来看，人工智能技术的快速进步带来了一系列风险，这些风险可以分为几个不同的类别，每个类别都有其独特的挑战和影响。这些风险包括以下内容：

+   **技术风险**：这些风险源于人工智能算法内在的缺陷，例如招聘过程中的偏见或针对意外、有害结果的优化系统。一个臭名昭著的例子是谷歌的Gemini，被发现生成不准确的历史图像。Gemini创建了历史人物的各种图像，其中所选择的描绘人物的性别和种族与历史事实完全矛盾。另一个案例是微软的Tay聊天机器人，它从其在Twitter上的互动中适应了种族歧视的侮辱性语言，突显了人工智能系统如何由于不匹配或恶意输入而与预期功能大相径庭。

+   **伦理风险**：伦理考量至关重要，尤其是对于面部识别等技术，这些技术对个人隐私构成重大威胁。此外，关于为大型人工智能模型提供训练数据的人的剥削也出现了伦理困境，这些人通常在补偿或工作条件不充分的情况下工作。

+   **社会风险**：人工智能传播虚假信息或侵蚀社会信任的能力体现了其社会风险。虚假信息的传播和可信来源的破坏可能对公共讨论和社会凝聚力产生深远影响。

+   **经济风险**：人工智能的经济影响广泛，从侵犯知识产权到可能增加市场集中度和失业，这些风险突显了人工智能对竞争格局和劳动力市场变革性的影响。

+   **安全风险**：恶意行为者滥用人工智能代表了一个重大的安全关切。这包括利用人工智能制造化学神经毒剂或进行数据提取攻击，其中大型语言模型（LLMs）可能被利用来访问私人个人信息，从而损害数据隐私和安全。

+   **存在风险**：或许最深刻的危险是由超越人类智能的人工智能系统带来的存在威胁。这样的系统，如果未能与人类价值观和目标充分对齐，可能会以对人类造成灾难性后果的方式追求其目标。

认识到这些风险的广泛性和深度对于国家、开发者和整个社会来说至关重要，以确保人工智能技术的部署以最小化潜在危害的方式进行。这需要一种积极的治理方法、开发实践和社会参与方法，以负责任地应对人工智能进步的复杂格局。

## Q3 – 如何引导人工智能和大型语言模型（LLMs）的开发和部署，以减轻诸如偏见等伦理问题，并确保其在决策过程中的负责任使用，尤其是在高风险和受监管的行业？

为了减轻诸如偏见等伦理问题，并确保人工智能和大型语言模型（LLMs）在决策过程中的负责任使用，尤其是在高风险和受监管的行业，需要采取多方面的方法。这种方法应解决将人工智能系统整合到商业和社会关键领域的技术和社会技术挑战。以下策略可以指导人工智能系统的开发和部署：

+   **发展重点转移**：AI 系统的设计应旨在增强而非复制人类思维。这种关注点的转变有助于通过确保 AI 系统支持和增强人类决策而不是取代它来维持公众对 AI 的信任。信任对于 AI 在决策过程中的长期集成至关重要，而维护它需要清楚地展示 AI 对人类能力的补充作用。

+   **合规监管和偏见缓解**：遵守新兴法规，例如 2024 年通过的欧盟 AI 法案，以及旨在限制高风险用例中偏见的协议标准，是至关重要的。开发者还应意识到偏见在合规监管之外的更广泛影响，认识到以西方和英语为中心的 AI 系统带来的挑战。应努力使数据集和算法多样化，以反映全球人口结构并减少固有的偏见。

+   **压力测试和安全措施**：AI 系统，尤其是 LLMs，应接受严格的压力测试，以确保它们能够处理具有更多确定性结果的高风险用例。应开发安全性和缓解策略来应对潜在的 AI 失败，重点是预防灾难性的“脆弱”失败，这些失败可能产生广泛的影响。

+   **人类监督**：将人类纳入作为战略瓶颈可以作为一种有效的保障措施，以防止 AI 决策的意外后果。这种策略确保 AI 系统持续受到人类判断的监控和指导，特别是在 AI 决策具有重大影响的情况下。

+   **构建基础 AI 基础设施**：政府和组织应投资于创建支持 AI 伦理和负责任部署的基础 AI 基础设施。这包括促进私营部门、学术界和政府之间的合作，共同开发既创新又符合社会价值观的 AI 工具。

+   **技能和文化发展**：在劳动力中推广实验和 AI 技术的安全使用文化至关重要。这包括对公务员和行业专业人士进行 AI 伦理使用培训，包括理解其局限性和潜在偏见。

+   **长期战略规划**：建立长期机制来识别、试点和部署前沿 AI 应用至关重要。这种规划应考虑 AI 技术的伦理、社会和经济影响，旨在利用 AI 为公众谋福利，同时最大限度地减少对公民和社会的风险。

通过采用这些策略，AI 开发者和政策制定者可以解决偏见问题，并确保 AI 和大型语言模型（LLMs）被负责任和有效地使用，尤其是在它们影响最深远、可能产生变革性影响的领域。

## Q4 – 可以实施哪些策略来实现从传统角色到协作人机团队的转变，确保在职场中人工智能整合的同时发展人类专业知识？

要从传统角色过渡到协作人机团队，并确保在职场中人工智能整合的同时发展人类专业知识，需要采取多方面的方法。这种策略包括以下内容：

+   **创造新的技能发展途径**：解决自动化对入门级角色的替代风险，需要建立新的职业发展途径和专业技能发展途径。这涉及到利用**生成式人工智能**（**GenAI**）工具的潜力，如斯坦福和麻省理工学院的研究所示，以提高工人生产力，同时探索人工智能对工作职能的更广泛影响。设计教育和培训计划，为劳动力准备更高层次的分析和战略角色至关重要，确保中小企业与人工智能进步同步发展。

+   **培养对人工智能输出的批判性参与**：为了对抗对人工智能和自动化的过度依赖，需要一种文化转变，鼓励员工批判性地评估人工智能决策。实施提供改进的可解释性系统——“玻璃盒子”，可以阐明人工智能决策背后的推理——可以赋予员工理解、质疑和有效协作人工智能工具的能力。这确保了人类认知技能和人工智能能力的平衡整合，增强了决策过程和对人工智能应用的信任。

+   **加强工作场所整合评估机制**：将人工智能有效融入工作场所，不仅超越了与基准数据集相比的性能指标。它需要全面理解现实世界的工作流程、潜在的限制以及管理异常情况的战略。这意味着开发评估方法，以评估人工智能系统如何在特定操作环境中补充人类角色，认识到自动化可以处理任务，但并不一定能够完全取代人类工作的细微和复杂性质。

+   **促进人机协作团队**：商业的未来需要接受一种范式，即人类和机器协作以实现共同目标。这种方法强调双方互补的优势，利用人工智能提高效率和规模，同时利用人类专业知识进行创新、道德考虑和复杂问题解决。实现这种协同效应需要战略性的组织规划、持续的学习机会，以及营造一种技术增强而非取代人类贡献的环境。

通过解决这些关键问题，组织可以培养一个环境，其中AI赋能的工具被深思熟虑地整合到工作场所。这确保了人类专业知识不仅得到保留，而且得到增强，为未来铺平道路，在那里协作的人机团队以负责任的方式推动创新、生产力和可持续增长。

# 摘要

在我们探索NLP和LLMs动态世界的最后一章中，我们有幸与各个领域的专家进行交流。他们富有洞察力的讨论揭示了LLMs的复杂发展、法律考量、运营方法、监管影响以及新兴能力。通过他们的专业视角，我们深入探讨了紧迫问题，如创建公平的数据集、推进NLP技术、在研究中导航隐私保护、围绕AI重组组织以及预测学习范式的突破。

与这些杰出人物的对话突出了一个共同主题：技术创新与伦理、法律和组织考虑因素的交汇。当我们思考减轻数据集偏差的策略、展望混合学习范式的未来，以及评估基础模型对数据所有权的影响时，变得明显的是，NLP和LLMs的演变不仅仅是一个技术旅程，而是一个多学科的事业，它挑战我们深入思考这些进步的更广泛影响。

本章作为本书的压轴之作，将全书各章节中讨论的广泛主题串联起来，从NLP的基础及其与ML的集成到LLMs的复杂设计、应用以及它们预示的未来趋势。它概括了我们旅程的精髓——强调学术界与工业界之间的合作，在充分理解伦理和法律环境的基础上，对于发挥LLMs的潜力至关重要。

在我们不仅结束这一章，而且结束整本书之际，我们站在NLP和LLMs新时代的边缘。我们专家分享的见解并不标志着结束，而是未来探索和创新的灯塔。本书旨在为来自学术界或工业界的读者提供对NLP和LLMs演变的全面理解和前瞻性见解，鼓励他们通过自己的研究、发展和伦理考量为这个不断发展的叙事做出贡献。
