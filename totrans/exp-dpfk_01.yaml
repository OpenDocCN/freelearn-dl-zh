- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1'
- en: Surveying Deepfakes
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度伪造调查
- en: 'Understanding deepfakes begins with understanding where they came from and
    what they can do. In this chapter, we’ll begin to explore deepfakes and their
    operation. We will go through the basics of what makes a deepfake work, talking
    about the differences between a **generative auto-encoder** and a **generative
    adversarial network** (**GAN**). We will examine their usTo PD: es in media, education,
    and advertising. We’ll investigate their limitations and consider how to plan
    and design your deepfakes to avoid the common pitfalls. Finally, we’ll examine
    existing deepfake software and discuss what each kind can do.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 理解深度伪造始于了解它们的起源和功能。在本章中，我们将开始探索深度伪造及其运作。我们将讨论使深度伪造工作原理的基础知识，包括**生成自编码器**和**生成对抗网络**（GAN）之间的区别。我们将探讨它们在媒体、教育和广告中的应用。我们将研究它们的局限性，并考虑如何规划和设计你的深度伪造以避免常见的陷阱。最后，我们将检查现有的深度伪造软件，并讨论每种类型能做什么。
- en: 'We’ll cover this in the following sections:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在以下章节中介绍：
- en: Introducing deepfakes
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍深度伪造
- en: Exploring the uses of deepfakes
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索深度伪造的用途
- en: Discovering how deepfakes work
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发现深度伪造是如何工作的
- en: Assessing the limitations of generative AI
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估生成式AI的限制
- en: Looking at existing deepfake software
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看现有的深度伪造软件
- en: Introducing deepfakes
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍深度伪造
- en: The name **deepfake** comes from a portmanteau of “deep”, referring to **deep
    learning**, and “fake,” referring to the fact that the images generated are not
    genuine. The term first came into use on the popular website Reddit, where the
    original author released several deepfakes of adult actresses with other women’s
    faces artificially applied to them.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: “深度伪造”这个名字来源于“深度”和“伪造”的合成词，其中“深度”指的是**深度学习**，“伪造”指的是生成的图像并非真实。这个术语最初在流行的网站Reddit上被使用，原始作者在那里发布了几个将成人女演员的脸部用其他女性的脸人工替换的深度伪造视频。
- en: Note
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The ethics of deepfakes are controversial, and we will cover this in more depth
    in [*Chapter 2*](B17535_02.xhtml#_idTextAnchor035)*,* *Examining Deepfake Ethics*
    *and Dangers.*
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 深度伪造的伦理问题存在争议，我们将在[*第二章*](B17535_02.xhtml#_idTextAnchor035)*，*考察深度伪造的伦理和危险*中对此进行更深入的探讨。
- en: This unethical beginning is still what the technology is most known for, but
    it’s not all that it can be used for. Since that time, deepfakes have moved into
    movies, memes, and more. Tom Cruise signed up for Instagram only after “Deep Tom
    Cruise” beat him to it. Steve Buscemi has remarked to Stephen Colbert that he
    “never looked better” when his face was placed on top of Jennifer Lawrence’s and
    a younger version of Bill Nighy was deepfaked onto his own older self for a news
    clip from the “past” in the movie *Detective Pikachu*.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这种不道德的起点仍然是这项技术最知名的地方，但它并非只能用于此。从那时起，深度伪造已经进入了电影、表情包等领域。汤姆·克鲁斯在“深度汤姆·克鲁斯”先他一步注册Instagram之后才注册。史蒂夫·布西米对斯蒂芬·科尔特说，当他的脸被贴在詹妮弗·劳伦斯的脸上，而一个年轻的比尔·奈伊版本被深度伪造到他的老脸上，以电影《侦探皮卡丘》中的“过去”新闻片段时，他“从未看起来这么好”。
- en: In this book, we will be taking a fairly narrow view of what deepfaking is,
    so let’s define it now. A deepfake is the use of a **neural network** trained
    on two faces to replace one face with another. There are other technologies to
    swap faces that aren’t deepfakes, and there are generative AIs that do other things
    besides swapping faces but to include all of those in the term just muddies the
    water and confuses the issue.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们将对深度伪造有一个相当狭窄的看法，所以现在让我们给它下个定义。深度伪造是使用在两个面部上训练的**神经网络**来替换一个面部为另一个面部。还有其他一些技术可以交换面部，但它们不是深度伪造，还有生成式AI可以执行除面部交换之外的其他任务，但将这些都包含在术语中只会使问题复杂化并混淆视听。
- en: Exploring the uses of deepfakes
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索深度伪造的用途
- en: The original use of Deepfakes might be the one that required the least amount
    of imagination. Putting one person’s face on another’s person has many different
    uses in various fields. Please don’t consider the ideas here as the full extent
    of the capabilities of deepfakes – someone is bound to imagine something new!
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 深度伪造的原始用途可能是最不需要想象力的一个。将一个人的脸贴在另一个人身上在各个领域有许多不同的用途。请别把这里提到的想法视为深度伪造能力的全部——肯定有人会想象出新的用途！
- en: Entertainment
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 娱乐
- en: 'Entertainment is the first area that comes to mind for most people when they
    consider the usage of deepfakes. There are two main areas of entertainment in
    which I see deepfakes playing a significant role: *narrative* and *parody*.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 娱乐是大多数人想到深度伪造用途时首先想到的领域。我认为深度伪造在娱乐领域有两个主要的应用领域：*叙事*和*恶搞*。
- en: Narrative
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 叙事
- en: The utility of deepfakes in movies is obvious. Imagine an actor’s face being
    superimposed onto their stunt double or an actor who becomes unavailable being
    replaced by another performer without any changes to the faces in the final movie.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 深度伪造在电影中的效用是显而易见的。想象一下，一个演员的脸被叠加到他们的替身身上，或者一个无法出现的演员被另一个表演者取代，而电影中最终的面部没有任何变化。
- en: While deepfakes may not seem good enough, deepfakes are already being used in
    Hollywood and other media today – from *Detective Pikachu*, which used deepfakes
    to de-age Bill Nighy, to *For All Mankind*, which used it to put actors face to
    face with Ronald Reagan. Agencies and VFX shops are all examining how to use deepfakes
    in their work.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然深度伪造可能看起来还不够好，但深度伪造已经在今天的好莱坞和其他媒体中得到了应用——从使用深度伪造让比尔·奈伊返老还童的《侦探皮卡丘》，到使用它让演员与罗纳德·里根面对面交流的《为了全人类》，机构和企业特效工作室都在研究如何在他们的工作中使用深度伪造。
- en: These techniques are not unique to deepfakes. CGI (in this book, referring to
    3D graphics) face replacements have been used in many movies. However, using CGI
    face replacement is expensive and complicated, requiring filming to be done in
    particular ways with lots of extra data captured to be used by the artists to
    get the CGI face to look good in the final scene. This is an art more than a science
    and requires extensive skills and knowledge to accomplish. Deepfakes solve many
    of these problems making new forms of face replacements possible.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术并不特指深度伪造。CGI（在这本书中指3D图形）面部替换已经在许多电影中得到了应用。然而，使用CGI面部替换既昂贵又复杂，需要以特定的方式拍摄，并捕获大量额外数据供艺术家使用，以便在最终场景中使CGI面部看起来很好。这更像是一门艺术而不是科学，需要广泛的技术和知识来完成。深度伪造解决了许多这些问题，使得新的面部替换形式成为可能。
- en: Making a deepfake requires no special filming techniques (although some awareness
    will make the process smoother). Deepfakes also require very little attention
    or skill compared to CGI face replacements. This makes it ideal for lower-cost
    face replacements, but it can also be higher-quality since the AI accounts for
    details that even the most dedicated artist can’t recreate.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 制作深度伪造不需要特殊的拍摄技巧（尽管一些意识会使过程更加顺畅）。与CGI面部替换相比，深度伪造也几乎不需要注意或技能。这使得它非常适合低成本面部替换，但也可以是高质量的，因为AI考虑到了即使是最专注的艺术家也无法复制的细节。
- en: Parody
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 恶搞
- en: 'Parody is an extremely popular form of social criticism and forms the basis
    for entire To PD: movies, TV shows, and other forms of media. Parody is normally
    done by professional impersonators. In some cases, those impersonators look (or
    can be made to look) similar to the person they’re impersonating. Other times,
    there is a reliance on their performance to make the impersonation clear.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 恶搞是一种极其流行的社会批评形式，是整个To PD：电影、电视剧和其他媒体形式的基础。恶搞通常由专业模仿者完成。在某些情况下，那些模仿者看起来（或可以让人看起来）与被模仿的人相似。在其他时候，依赖于他们的表演来使模仿清晰。
- en: Deepfakes provide an opportunity to change the art of parody wherein the impersonator
    can be made to look like the individual being parodied via a deepfake instead
    of by chance of birth. By removing the attention from basic appearance, deepfakes
    allow the focus to be placed directly on the performance itself.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Deepfakes提供了一种改变恶搞艺术的机会，通过深度伪造，模仿者可以看起来像被恶搞的人，而不是仅仅依靠出生的偶然。通过将注意力从基本外观上移开，深度伪造允许直接将焦点放在表演本身上。
- en: Deepfakes also enable a whole new form of parody in which normal situations
    can become parodic simply due to the changed face. This particular form becomes
    humorous due to the distinct oddity of very different faces, instead of an expected
    swap.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 深度伪造还使一种全新的恶搞形式成为可能，在这种形式中，由于面部改变，正常情况可以简单地变得具有讽刺意味。这种特定形式由于非常不同面部的独特怪异而变得幽默，而不是预期的交换。
- en: '![Figure 1.1 – Steve Buscemi as Jennifer Lawrence by birbfakes](img/B17535_01_001.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图1.1 – 通过birbfakes将史蒂夫·布西米打造成詹妮弗·劳伦斯](img/B17535_01_001.jpg)'
- en: Figure 1.1 – Steve Buscemi as Jennifer Lawrence by birbfakes
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1 – 通过birbfakes将史蒂夫·布西米打造成詹妮弗·劳伦斯
- en: Note
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'This image is included with the kind permission of its original creator, birbfakes.
    You can view the original video here: [https://youtu.be/r1jng79a5xc](https://youtu.be/r1jng79a5xc).'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图片是在其原始创作者birbfakes的允许下包含的。您可以在这里查看原始视频：[https://youtu.be/r1jng79a5xc](https://youtu.be/r1jng79a5xc)。
- en: Video games
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 视频游戏
- en: Video games present an interesting opportunity when it comes to deepfakes. The
    idea here is that a computer-generated character could be deepfaked into a photorealistic
    avatar. This could be done for any character in the game, even the player’s character.
    For example, it would be possible to make a game in which, when the player’s character
    looked into a mirror, they would see their own face looking back at them. Another
    possibility would be to replace a non-player character with a deepfake of the
    original actor, allowing for a far more realistic appearance without making a
    complete 3D clone of the actor.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在视频游戏方面，深度伪造提供了一个有趣的机会。这里的想法是，一个计算机生成的角色可以被深度伪造成一个逼真的虚拟形象。这可以应用于游戏中的任何角色，甚至包括玩家的角色。例如，可以制作一个游戏，当玩家的角色照镜子时，他们会看到自己的脸映照回来。另一种可能性是用原始演员的深度伪造来替换非玩家角色，这样可以得到一个更加逼真的外观，而不必制作演员的完整3D复制品。
- en: Education
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 教育
- en: Education could also benefit from deepfakes. Imagine if your history class had
    a video of Abraham Lincoln himself reading the Gettysburg address. Or a corporate
    training video in which the entire video is hosted by the public mascot (who may
    not even be a real person) without having to resort to costumes or CGI. It could
    even be used to allow multiple videos or scenes filmed at significantly different
    times to appear to be more cohesive by appearing to show the actor at the same
    time.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 教育也可以从深度伪造中受益。想象一下，如果你的历史课有一段亚伯拉罕·林肯本人朗读葛底斯堡演说的视频。或者一个企业培训视频，整个视频都是由公众吉祥物（可能甚至不是真人）主持的，而不必依赖服装或CGI。甚至可以用来让在显著不同时间拍摄的多个视频或场景看起来更加连贯，仿佛演员在同一时间出现。
- en: Many people are very visual learners and seeing a person “come alive” can really
    bring the experience home. Bringing the pre-video past to life using deepfakes
    enables a whole new learning experience. One example of this is the Dalí Museum,
    which created a series of videos of Salvador Dalí talking to guests. This was
    done by training a deepfake model on an actor to put Dalí’s face on the videos.
    Once the model was trained and set up, they were able to convert many videos,
    saving a lot of time and effort compared to a CGI solution.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 许多人是非常视觉化的学习者，看到一个人“活过来”真的可以让人身临其境。使用深度伪造将过去的视频内容“复活”可以带来全新的学习体验。一个例子是达利博物馆，它创建了一系列萨尔瓦多·达利与客人交谈的视频。这是通过训练一个深度伪造模型，将达利的面孔应用到视频上实现的。一旦模型训练完成并设置好，他们就能转换许多视频，与CGI解决方案相比，节省了大量时间和精力。
- en: Advertisements
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 广告
- en: Advertising agencies are always looking for the newest way to grab attention
    and deepfakes could be a whole new way to catch viewers’ attention. Imagine if
    you walked past a clothing store, you stopped to look at an item of clothing in
    the window, and suddenly the screen beside the item showed a video of an actor
    wearing the item but with your face, allowing you to see how the item would look
    on you. Alternatively, a mascot figure could be brought to life in a commercial.
    Deepfakes offer a whole new tool for creative use, which can grab attention and
    provide whole new experiences in advertising.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 广告机构总是在寻找吸引注意力的最新方式，深度伪造可能是一个全新的吸引观众注意力的方式。想象一下，如果你路过一家服装店，你停下来看橱窗里的衣服，突然旁边屏幕上显示了一个演员穿着这件衣服的视频，但脸上是你的脸，让你看到这件衣服在你身上会是什么样子。或者，一个吉祥物角色可以在商业广告中被“复活”。深度伪造为创意使用提供了全新的工具，可以在广告中吸引注意力并提供全新的体验。
- en: Now that we’ve got some idea of a few potential uses for deepfakes, let’s take
    a quick look under the hood and see how they work.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对深度伪造的一些潜在用途有了大致的了解，让我们快速了解一下其内部工作原理。
- en: Discovering how deepfakes work
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 发现深度伪造的工作原理
- en: Deepfakes are a unique variation of a generative auto-encoder being used to
    generate the face swap. This requires a special structure, which we will explain
    in this section.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 深度伪造是一种独特的生成自编码器变体，用于生成面部交换。这需要一个特殊结构，我们将在本节中解释。
- en: Generative auto-encoders
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成自编码器
- en: The particular type of neural network that regular deepfakes use is called a
    **generative auto-encoder**. Unlike a **Generative Adversarial Network** (**GAN**),
    an auto-encoder does not use a discriminator or any “adversarial” techniques.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 正规深度伪造使用的特定类型的神经网络被称为**生成自编码器**。与**生成对抗网络**（GAN）不同，自编码器不使用判别器或任何“对抗”技术。
- en: All auto-encoders work by training a collection of neural network models to
    solve a problem. In the case of generative auto-encoders, the AI is used to generate
    a new image with new details that weren’t in the original image. However, with
    a normal auto-encoder, the problem is usually something such as **classification**
    (deciding what an image is), **object identification** (finding something inside
    an image), or **segmentation** (identifying different parts of an image). To do
    this, there are two types of models used in the autoencoder – the **encoder**
    and **decoder**. Let’s see how this works.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 所有自动编码器都是通过训练一系列神经网络模型来解决一个问题。在生成型自动编码器的情况下，AI用于生成一个包含原始图像中没有的新细节的新图像。然而，在普通自动编码器中，问题通常是**分类**（决定图像是什么）、**物体识别**（在图像中找到某物）或**分割**（识别图像的不同部分）。为此，在自动编码器中使用了两种类型的模型
    – **编码器**和**解码器**。让我们看看这是如何工作的。
- en: The deepfake training cycle
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深伪影训练周期
- en: 'The training cycle is a cyclical process in which the model is continuously
    trained on images until stopped. The process can be broken down into four steps:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 训练周期是一个循环过程，模型在图像上持续训练直到停止。这个过程可以分为四个步骤：
- en: '**Encode** faces into smaller intermediate representations.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码**面孔成更小的中间表示。'
- en: '**Decode** the intermediate representations back into faces.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解码**中间表示回面孔。'
- en: Calculate the **loss** of (meaning, the difference between) the original face
    and the output of the model.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算原始人脸与模型输出之间的**损失**（即，两者之间的差异）。
- en: Modify (**backpropagate**) the models toward the correct answer.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修改（**反向传播**）模型以趋向正确答案。
- en: '![Figure 1.2 – Diagram of the training cycle](img/B17535_01_002.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图1.2 – 训练周期图](img/B17535_01_002.jpg)'
- en: Figure 1.2 – Diagram of the training cycle
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2 – 训练周期图
- en: 'In more detail, the process unfolds as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 更详细地说，这个过程如下展开：
- en: The **encoder’s** job is to **encode** two different faces into an array, which
    we call the intermediate representation. The intermediate representation is much
    smaller than the original image size, with enough space to describe the lighting,
    pose, and expression of the faces. This process is similar to compression, where
    unnecessary data is thrown out to fit the data into a smaller space.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码器**的工作是将两个不同的面孔编码成一个数组，我们称之为中间表示。中间表示比原始图像尺寸小得多，有足够的空间来描述光照、姿势和表情。这个过程类似于压缩，其中不必要的数据被丢弃以适应更小的空间。'
- en: The **decoder** is actually a matched pair of models, which turn the intermediate
    representation back into faces. There is one decoder for each of the input faces,
    which is trained only on images of that one person’s face. This process tries
    to create a new face that matches the original face that was given to the encoder
    and encoded into the intermediate representation.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解码器**实际上是一对匹配的模型，它们将中间表示转换回面孔。每个输入面孔都有一个解码器，它只在该一个人的面孔图像上训练。这个过程试图创建一个与编码器接收到的原始面孔匹配的新面孔，该面孔被编码到中间表示中。'
- en: '![Figure 1.3 – Encoder and decoder](img/B17535_01_003.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图1.3 – 编码器和解码器](img/B17535_01_003.jpg)'
- en: Figure 1.3 – Encoder and decoder
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3 – 编码器和解码器
- en: '**Loss** is a score that is given to the auto-encoder based on how well it
    recreates the original faces. This is calculated by comparing the original image
    to the output from the encoder-decoder process. This comparison can be done in
    many ways, from a strict difference between them or something significantly more
    complicated that includes human perception as part of the calculation. No matter
    how it’s done, the result is the same: a number from 0 to 1, with 0 being the
    score for the model returning the exact same image and 1 being the exact opposite
    or the image. Most of the numbers will fall between 0 to 1\. However, a perfect
    reconstruction (or its opposite) is impossible.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**损失**是一个根据自动编码器如何重新创建原始人脸而给出的分数。这是通过比较原始图像与编码器-解码器过程的输出来计算的。这种比较可以以多种方式进行，从两者之间的严格差异到包含人类感知作为计算一部分的复杂得多的情况。无论怎样做，结果都是一样的：一个介于0到1之间的数字，其中0是模型返回完全相同图像的分数，1是完全相反或图像。大多数数字将介于0到1之间。然而，完美的重建（或其相反）是不可能的。'
- en: Note
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The loss is where an auto-encoder differs from a GAN. In a GAN, the comparison
    loss is either replaced or supplemented with an additional network (usually an
    auto-encoder itself), which then produces a loss score of its own. The theory
    behind this structure is that the loss model (called a discriminator) can learn
    to get better at detecting the output of the generating model (called a generator)
    while the generator can learn to get better at fooling the discriminator.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 损失是自动编码器与生成对抗网络（GAN）之间的区别所在。在GAN中，比较损失要么被替换，要么由一个额外的网络（通常是一个自动编码器本身）补充，然后该网络产生它自己的损失分数。这个结构的理论依据是，损失模型（称为判别器）可以学会更好地检测生成模型（称为生成器）的输出，而生成器可以学会更好地欺骗判别器。
- en: Finally, there is **b****ackpropagation**, a process in which the models are
    adjusted by following the path back through both the decoder and encoder that
    generated the face and nudging those paths toward the correct answer.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，是**反向传播**，这是一个通过沿着生成面孔和解码器以及编码器生成的路径回溯，并推动这些路径向正确答案靠近的过程。
- en: '![Figure 1.4 – Loss and backpropagation](img/B17535_01_004.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图1.4 – 损失和反向传播](img/B17535_01_004.jpg)'
- en: Figure 1.4 – Loss and backpropagation
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.4 – 损失和反向传播
- en: Once complete, the whole process starts back over at the encoder again. This
    continues to repeat until the neural network has finished training. The decision
    of when to end training can happen in several ways. It can happen when a certain
    number of repetitions have occurred (called **iterations**), when all the data
    has been gone through (called an **epoch**), or when the results meet a certain
    loss score.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成，整个过程就会从编码器重新开始。这会一直重复，直到神经网络完成训练。何时结束训练的决定可以以几种方式发生。它可以在发生了一定数量的重复（称为**迭代**）时发生，当所有数据都经过（称为**一个epoch**）时，或者当结果达到一定的损失分数时。
- en: Why not GANs?
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么不是GANs？
- en: GANs are one of the current darlings of generative networks. They are extremely
    popular and used extensively, being used particularly for super-resolution (intelligent
    upscaling), music generation, and even sometimes deepfakes. However, there are
    some reasons that they’re not used in all deepfake solutions.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: GAN是目前生成网络中的宠儿之一。它们非常受欢迎，被广泛使用，尤其是在超分辨率（智能放大）、音乐生成，甚至有时是深度伪造。然而，有一些原因使得它们没有被用于所有的深度伪造解决方案。
- en: GANs are popular due to their “imaginative” nature. They learn through the interaction
    of their generator and discriminator to fill in gaps in the data. Because they
    can fill in missing pieces, they are great at reconstruction tasks or at tasks
    where new data is required.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: GAN之所以受欢迎，是因为它们的“富有想象力”的特性。它们通过生成器和判别器的交互来学习，填补数据中的空白。因为它们可以填补缺失的部分，所以在重建任务或需要新数据的任务中非常出色。
- en: The ability of a GAN to create new data where it is missing is great for numerous
    tasks, but it has a critical flaw when used for deepfakes. In deepfakes, the goal
    is to replace one face with another face. An imaginative GAN would likely learn
    to fill the gaps in the data from one face with the data from the other. This
    leads to a problem that we call “identity bleed” where the two faces aren’t swapped
    properly; instead, they’re blended into a face that doesn’t look like either person,
    but a mix of the two.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: GAN在创建缺失数据方面的能力对于许多任务来说非常出色，但当用于深度伪造时，它有一个关键的缺陷。在深度伪造中，目标是替换一个面孔为另一个面孔。一个富有想象力的GAN可能会学会用另一个面孔的数据来填补一个面孔数据中的空白。这导致了一个我们称之为“身份混合”的问题，即两个面孔没有正确交换；相反，它们混合成了一个既不像任何一个人，又像是两个人混合的面孔。
- en: This flaw in a GAN-created deepfake can be corrected or prevented but requires
    much more careful data collection and processing. In general, it’s easier to get
    a full swap instead of a blending by using a generative auto-encoder instead of
    a GAN.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: GAN创建的深度伪造中的这个缺陷可以纠正或预防，但这需要更加仔细的数据收集和处理。一般来说，使用生成式自动编码器而不是GAN，更容易实现完全替换而不是混合。
- en: The auto-encoder structure
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自动编码器结构
- en: 'Another name for an auto-encoder is an “hourglass” model. The reason for this
    is that each layer of an encoder is smaller than the layer before it while each
    layer of a decoder is larger than the one before. Because of this, the auto-encoder
    figure starts out large at the beginning, shrinks toward the middle, and then
    widens back out again as it reaches the end:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器的另一个名字是“沙漏”模型。这是因为编码器的每一层都比前一层小，而解码器的每一层都比前一层大。正因为如此，自动编码器的图在开始时很大，向中间缩小，然后在达到末端时再次变宽：
- en: '![Figure 1.5 – Hourglass structure of an autoencoder](img/B17535_01_005.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图1.5 – 自动编码器的沙漏结构](img/B17535_01_005.jpg)'
- en: Figure 1.5 – Hourglass structure of an autoencoder
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.5 – 自动编码器的沙漏结构
- en: While these methods are flexible and have many potential uses, there are limitations.
    Let’s examine those limitations now.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些方法具有灵活性并且有众多潜在用途，但它们也存在限制。现在让我们来探讨这些限制。
- en: Assessing the limitations of generative AI
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估生成式AI的限制
- en: Generative AIs like those used in deepfakes are not a panacea and actually have
    some significant limitations. However, by knowing about these limitations, they
    can generally be worked around or sidestepped with careful design.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 深度伪造中使用的生成式AI并非万能，实际上存在一些显著的限制。然而，通过了解这些限制，它们通常可以通过精心设计来克服或规避。
- en: Resolution
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分辨率
- en: 'Deepfakes are limited in the resolution that they can swap. This is a hardware
    and time limitation: greater hardware and more time can provide higher resolution
    swaps. However, this is not a 1:1 linear growth. Doubling the resolution (from,
    say, 64x64 to 128x128) actually quadruples the amount of required **VRAM** – that
    is, the memory that a GPU has direct access to – and the time necessary to train
    is expanded a roughly equivalent amount. Because of this, resolution is often
    a balancing act, where you’ll want to make the deepfake the lowest resolution
    you can without sacrificing the results.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 深度伪造在可以交换的分辨率上有限制。这是一个硬件和时间限制：更强大的硬件和更多的时间可以提供更高分辨率的交换。然而，这并不是1:1的线性增长。将分辨率加倍（例如，从64x64到128x128）实际上将所需的**VRAM**（即GPU可以直接访问的内存）数量增加四倍，并且训练所需的时间也大致增加相同的量。因此，分辨率通常是一个权衡，你可能会想要将深度伪造的分辨率降到最低，同时不牺牲结果。
- en: Training required for each face pair
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 每对脸所需的训练
- en: To provide the best results, traditional deepfakes require that you train on
    every face pair that you wish to swap. This means that if you wanted to swap your
    own face with two of your friends, you’d have to train two separate models. This
    is because each model has one encoder and two decoders, which are trained only
    to swap the faces they were given.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供最佳结果，传统的深度伪造需要你在你希望交换的每一对脸上进行训练。这意味着，如果你想用自己的脸交换你两个朋友的脸，你就必须训练两个不同的模型。这是因为每个模型都有一个编码器和两个解码器，它们只被训练来交换它们所接收到的脸。
- en: There is a workaround to some multi-face swaps. In order to swap additional
    faces, you could write your own version with more than two decoders allowing you
    to swap additional faces. This is an imperfect solution, however, as each decoder
    takes up a significant amount of VRAM, requiring you to balance the number of
    faces carefully.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些多脸交换，存在一种解决方案。为了交换更多的脸，你可以编写一个具有两个以上解码器的版本，从而允许你交换额外的脸。然而，这是一个不完美的解决方案，因为每个解码器都占用大量的VRAM，这要求你仔细平衡脸的数量。
- en: It may be better to simply train multiple pairs. By splitting the task up on
    multiple computers, you could train multiple models simultaneously, allowing you
    to create many face pairs at once.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 可能简单地训练多个对会更好。通过在多台计算机上分配任务，你可以同时训练多个模型，从而一次创建多个脸对。
- en: 'Another option is to use a different type of AI face replacement. **First Order
    Model** (which is covered in the *Looking at existing deepfake software* section
    of this chapter) uses a different technique: instead of a paired approach, it
    uses AI to animate an image to match the actions of a replacement. This solution
    removes the need to retrain on each face pair, but comes at the cost of greatly
    reduced quality of the swap.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选项是使用不同类型的AI人脸替换。**一阶模型**（在本章的“查看现有深度伪造软件”部分有所介绍）使用不同的技术：它不是采用配对方法，而是使用AI来动画化图像以匹配替换者的动作。这种解决方案消除了在每个脸对上重新训练的需求，但代价是交换的质量大大降低。
- en: Training data
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练数据
- en: Generative AIs requires a significant amount of training data to accomplish
    their tasks. Sometimes, finding sufficient data or data of a high-enough quality
    is not possible. For example, how would someone create a deepfake of William Shakespeare
    when there are no videos or photographs of him? This is a tricky problem but can
    be worked around in several ways. While it is unfortunately impossible to create
    a proper deepfake of England’s greatest playwright, it would be possible to use
    an actor who looks like his portraits and then deepfake that actor as Shakespeare.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式AI需要大量的训练数据来完成其任务。有时，找到足够的数据或足够高质量的数据是不可能的。例如，当没有威廉·莎士比亚的视频或照片时，一个人如何创建他的深度伪造？这是一个棘手的问题，但可以通过几种方式解决。虽然不幸的是，无法创建英格兰最伟大的剧作家的正确深度伪造，但可以使用看起来像他的肖像的演员，然后深度伪造这位演员成为莎士比亚。
- en: Tip
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: We will cover more on how to deal with poor or insufficient data in [*Chapter
    3*](B17535_03.xhtml#_idTextAnchor054), *Mastering Data.*
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[*第3章*](B17535_03.xhtml#_idTextAnchor054)，“掌握数据”中更详细地介绍如何处理不良或不足的数据。
- en: Finding sufficient data (or clever workarounds) is the most difficult challenge
    that any data scientist faces. Occasionally, there simply is no way to get sufficient
    data. This is when you might need to re-examine the video to see whether there
    is another way to shoot it to avoid the lack of data, or you might try using other
    sources of similar data to patch the gaps. Sometimes, just knowing the limitations
    in advance can prevent a problem – other times, a workaround in the last minutes
    may be enough to save a project from failure.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找足够的数据（或巧妙的工作方案）是任何数据科学家面临的最困难挑战。有时，根本无法获得足够的数据。这时，你可能需要重新审视视频，看看是否有其他拍摄方式来避免数据不足，或者你可能尝试使用其他类似数据来源来填补空白。有时，提前了解限制可以防止问题发生——有时，在最后几分钟的解决方案可能足以挽救一个项目免于失败。
- en: While everyone should know the data limitations, knowing the limitations of
    the process is only for experts. If you are only looking to use deepfakes, you’ll
    probably use existing software. Let’s explore those next.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然每个人都应该了解数据限制，但了解流程的限制仅限于专家。如果你只是想使用深度伪造，你可能会使用现有的软件。让我们接下来探索这些软件。
- en: Looking at existing deepfake software
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查看现有的深度伪造软件
- en: There have been many programs that have risen to fill the niche of deepfaking;
    however, few of them are still under development or supported. The rapid development
    of GPU hardware and AI software has led to unique challenges in software development,
    and many deepfake programs are no longer usable. However, there are still several
    deepfake software programs and, in this section, we’ll go over the major options.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 已经有许多程序填补了深度伪造的空白；然而，其中很少仍在开发或受到支持。GPU硬件和AI软件的快速发展导致了软件开发中的独特挑战，许多深度伪造程序已经不再可用。然而，仍然有几种深度伪造软件程序，在本节中，我们将介绍主要选项。
- en: Important Note
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The authors have made every effort to be unbiased in this section, but are among
    the developers of Faceswap. Faceswap will be covered in more detail in [*Chapter
    4*](B17535_04.xhtml#_idTextAnchor071), *The Deepfake Workflow*, with a walkthrough
    of the workflow of a deepfake through the Faceswap software.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们已经尽最大努力在本节中保持无偏见，但他们也是Faceswap的开发者之一。Faceswap将在[*第4章*](B17535_04.xhtml#_idTextAnchor071)，“深度伪造工作流程”中更详细地介绍，其中包括通过Faceswap软件的深度伪造工作流程的演练。
- en: Faceswap
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Faceswap
- en: Faceswap is a **Free and Open Source** (**FOSS**) software program for creating
    deepfakes. It’s released under the GPL3 and can be used by anyone anywhere. It’s
    written in Python and runs AI on the TensorFlow backend. It supports NVIDIA, AMD,
    and Apple GPUs for accelerating the machine learning models, or can be run on
    a CPU at a reduced speed. There are installers for Windows and Linux that can
    help by installing all the needed libraries and tools inside of a self-contained
    environment.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Faceswap是一个**免费和开源**（**FOSS**）的软件程序，用于创建深度伪造。它根据GPL3发布，任何人都可以在任何地方使用。它用Python编写，在TensorFlow后端运行AI。它支持NVIDIA、AMD和Apple
    GPU以加速机器学习模型，或者可以在CPU上以较低的速度运行。它提供了Windows和Linux的安装程序，可以在一个自包含的环境中安装所有需要的库和工具。
- en: It’s available at [https://Faceswap.dev/](https://Faceswap.dev/).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 它可在[https://Faceswap.dev/](https://Faceswap.dev/)找到。
- en: DeepFaceLab
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DeepFaceLab
- en: Originally a fork of Faceswap, DeepFaceLab is now developed mostly by Ivan Perov.
    DeepFaceLab is another FOSS software program for deepfakes. It is known for more
    experimental models and features. There is no GUI, but there are Jupyter Notebooks
    that can be run in any of the Jupyter environments. There is also a DirectML version,
    which provides another option for people using Windows. There are fully contained
    builds that are packaged together into a single compressed file, which provides
    a fully working package for many operating systems.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: DeepFaceLab最初是Faceswap的一个分支，现在主要由Ivan Perov开发。DeepFaceLab是另一个用于深度伪造的开源软件程序，以其更多的实验模型和功能而闻名。它没有图形用户界面，但提供了可以在任何Jupyter环境中运行的Jupyter笔记本。还有一个DirectML版本，为使用Windows的用户提供了另一个选项。还有完全封装的构建版本，打包成一个压缩文件，为许多操作系统提供了一个完全工作的软件包。
- en: It’s available at [https://github.com/iperov/DeepFaceLab](https://github.com/iperov/DeepFaceLab).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 它可在[https://github.com/iperov/DeepFaceLab](https://github.com/iperov/DeepFaceLab)找到。
- en: First Order Model
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: First Order Model
- en: '**First Order Model** works in a fundamentally different way from Faceswap
    and DeepFaceLab. Instead of swapping a face onto a new video, First Order Model
    “puppets” the face, making it match the movements of a video but leaving the face
    the same. Furthermore, it doesn’t require training on each face pair, making it
    easy to use to make quick deepfakes where you can “animate” a person even with
    just a single photo of them.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**First Order Model**与Faceswap和DeepFaceLab的工作方式在本质上不同。它不是将面部交换到新视频中，而是“操纵”面部，使其与视频的动作相匹配，但保持面部不变。此外，它不需要对每一对面部进行训练，这使得制作快速深度伪造变得容易，即使只有一张照片，也可以“动画化”一个人。'
- en: 'It is important to note that while the First Order Model software is available
    freely, it is licensed only for non-commercial use: if you want to use it in a
    commercial context, you’ll need to contact the author for a license. It’s available
    at [https://github.com/AliaksandrSiarohin/first-order-model](https://github.com/AliaksandrSiarohin/first-order-model).'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，尽管First Order Model软件可以免费获得，但它仅限于非商业用途的许可：如果你想在商业环境中使用它，你需要联系作者获取许可。它可在[https://github.com/AliaksandrSiarohin/first-order-model](https://github.com/AliaksandrSiarohin/first-order-model)找到。
- en: Reface
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Reface
- en: '**Reface** is yet another method of creating deepfakes. Reface is closed source
    and proprietary, so we can’t analyze exactly how it works, but it uses a zero-shot
    learning method like First Order Model to swap faces without requiring training
    on a pair of swaps. Reface offers apps for Apple iOS and Android and does the
    swap in the cloud, making it easier to get a quick result, but means that you
    might not be able to swap the exact clip you want, and licensing may be an issue.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**Reface**是创建深度伪造的另一种方法。Reface是闭源和专有软件，因此我们无法确切分析其工作原理，但它使用了一种零样本学习方法，类似于First
    Order Model，可以在不要求在成对交换上进行训练的情况下交换面部。Reface为Apple iOS和Android提供应用程序，并在云端进行交换，这使得快速获得结果变得更容易，但可能无法交换你想要的精确剪辑，并且可能存在许可问题。'
- en: It’s available at [https://reface.ai/](https://reface.ai/).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 它可在[https://reface.ai/](https://reface.ai/)找到。
- en: Summary
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: The technology of deepfakes is not itself anything new or unique. These techniques
    existed in various forms long before they were applied to face-swapping, but deepfakes
    have caught public attention in a way that other AI techniques have never really
    been able to. There is something very visceral about seeing a face where it doesn’t
    belong, seeing an actor in a role you know that they didn’t play, or seeing your
    own face doing something you’ve never done.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 深度伪造技术本身并不是什么新或独特的东西。这些技术以各种形式存在，在它们被应用于面部交换之前就已经存在，但深度伪造以一种其他AI技术从未真正实现的方式引起了公众的关注。看到面部出现在它不应该出现的地方，看到你认识的演员扮演他们从未扮演过的角色，或者看到你自己的面部做着你从未做过的事情，这些都让人感到非常直观。
- en: While the techniques that make up deepfakes have all existed previously on their
    own, together, they provide completely new possibilities. There are numerous use
    cases that deepfakes can be applied to, from stunt-double replacement to advertising.
    The technology is here, and its use will only grow as more and more industries
    find ways to use it.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然构成深度伪造的技术各自之前都存在，但结合起来，它们提供了全新的可能性。深度伪造可以应用于众多用例，从替身演员的替换到广告。这项技术已经存在，随着越来越多的行业找到使用它的方法，其应用将只会增长。
- en: There are still limits to the capabilities of generative AI. Knowing what a
    deepfake cannot do is as important as knowing what it can do. Especially regarding
    data, knowing how to work around those limitations is key to a quality result.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式人工智能的能力仍然有限。了解深度伪造不能做什么，与了解它能做什么一样重要。特别是在数据方面，了解如何克服这些限制是获得高质量结果的关键。
- en: We’ve given an overview of deepfakes, covering what they are, what they can
    be used for, how they work, their limitations, and the existing software you can
    use to make them. In the next chapter, we’ll cover the potential dangers of deepfakes
    and talk about the ethical questions that the technology brings with it.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经概述了深度伪造技术，包括它们是什么，可以用于什么，如何工作，它们的局限性，以及你可以用来制作它们的现有软件。在下一章中，我们将探讨深度伪造的潜在危险，并讨论这项技术带来的伦理问题。
- en: EBSCOhost - printed on 11/27/2023 6:20 AM via . All use subject to [https://www.ebsco.com/terms-of-use](https://www.ebsco.com/terms-of-use)
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: EBSCOhost - 2023年11月27日 上午6:20打印。所有使用均受[https://www.ebsco.com/terms-of-use](https://www.ebsco.com/terms-of-use)条款约束。
