<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-209"><a id="_idTextAnchor226"/>12</h1>
<h1 id="_idParaDest-210"><a id="_idTextAnchor227"/>Ensuring Security and Privacy in Amazon Bedrock</h1>
<p>GenAI has been making remarkable developments, enabling machines to produce human-like content across various domains, including text, images, and even code. However, there have been concerns about the risks and challenges of using GenAI models and how the data is handled. In this chapter, we are <a id="_idIndexMarker987"/>going to look at security, privacy, and <strong class="bold">Guardrails for </strong><strong class="bold">Amazon Bedrock</strong>.</p>
<p>Ensuring data privacy and security is a top priority in today’s digital landscape, and Amazon Bedrock has implemented robust measures to address this concern. We are going to look at data localization, isolation, and encryption and learn ways to ensure that your data remains within your designated AWS region, never shared nor stored, and protected through robust encryption protocols. Then, we will understand how Amazon Bedrock is integrated with AWS IAM to provide granular control over access privileges, ensuring that only authorized personnel can interact with your resources.</p>
<p>Moreover, we are going to discuss ethical practices and guardrails, allowing you to implement safeguards aligned with safe and responsible AI policies, such as content filters, denied topics, word filters, and sensitive information filters.</p>
<p>Here are key topics that will be covered in this chapter:</p>
<ul>
<li>Security and privacy overview</li>
<li>Data encryption</li>
<li>AWS IAM</li>
<li>Securing the network</li>
<li>Network flow</li>
<li>Ethical practices</li>
<li>Guardrails for Amazon Bedrock<a id="_idTextAnchor228"/><a id="_idTextAnchor229"/><a id="_idTextAnchor230"/></li>
</ul>
<h1 id="_idParaDest-211"><a id="_idTextAnchor231"/>Technical requirements</h1>
<p>This chapter requires you to have access to an AWS account. If you don’t have it already, you can go to <a href="https://aws.amazon.com/getting-started/">https://aws.amazon.com/getting-started/</a> and create an AWS account.</p>
<p>Secondly, you will need to set up AWS Python SDK (Boto3), which you can do by going to <a href="https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html">https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html</a>.</p>
<p>You can carry out the Python setup in any way: install it on your local machine, or use AWS Cloud9, or utilize AWS Lambda, or leverage Amazon SageMaker.</p>
<p class="callout-heading">Note</p>
<p class="callout">There will be a charge associated with the invocation and customization of the FMs of Amazon Bedrock. Please refer to <a href="https://aws.amazon.com/bedrock/pricing/">https://aws.amazon.com/bedrock/pricing/</a> to learn more.</p>
<h1 id="_idParaDest-212"><a id="_idTextAnchor232"/>Security and privacy overview</h1>
<p>One of the core <a id="_idIndexMarker988"/>security principles of Amazon Bedrock is that you, as a user of Amazon Bedrock, are always in control of your data. Your data is never shared with other users or customers and is never used to improve or train the FMs. Let us look at the intricate layers of protection that Amazon Bedrock provides:</p>
<ul>
<li><strong class="bold">Data localization</strong>: The inference data or training data for model customization remains within the AWS region that you are using. This means that all API requests and data processing occur solely within the specified region, eliminating the risk of data migration or exposure beyond your designated boundaries. This regional isolation guarantees that your data never leaves the designated geographical boundaries, so you get an additional layer of protection and compliance with regional data regulations.</li>
<li><strong class="bold">Data isolation</strong>: Your inference or training data for model customization is stored in Amazon S3, but is never retained in the service-managed account, eliminating the possibility of accidental leaks, unauthorized access, or misuse by third parties, including model vendors or AWS itself. The only information stored pertains to operational metrics, such as usage data for billing purposes and metadata necessary for console functionality.</li>
<li><strong class="bold">Encryption</strong>: When it comes to encrypting data, Amazon Bedrock employs robust encryption protocols to safeguard the information. All the communications to, from, and within the service are encrypted in transit, with a minimum requirement of TLS 1.2 and a recommendation for TLS 1.3. Additionally, Amazon Bedrock encourages the encryption of your customization training data stored in an Amazon S3 bucket and customized models using your own KMS keys, ensuring that only authorized parties with the correct credentials can access and utilize these resources.</li>
<li><strong class="bold">IAM</strong>: Amazon Bedrock’s integration with AWS IAM empowers you with granular control over access privileges. You can selectively allow or deny access to specific models, specific API calls, model customization jobs, or Amazon Bedrock itself. This fine-grained access control ensures that only authorized personnel can interact with your resources, minimizing the risk of unauthorized access or accidental modifications.</li>
<li><strong class="bold">Comprehensive monitoring and logging</strong>: As we have seen in <a href="B22045_11.xhtml#_idTextAnchor207"><em class="italic">Chapter 11</em></a>, transparency and auditability are essential components of data privacy and protection. Amazon <a id="_idIndexMarker989"/>Bedrock offers comprehensive monitoring and logging capabilities, so you can track usage metrics, build customized dashboards using Amazon CloudWatch, and monitor API activity through AWS CloudTrail. These features provide invaluable insights into your data’s usage, aiding in troubleshooting and ensuring compliance with regulatory requirements.</li>
<li><strong class="bold">Compliance standards</strong>: Amazon Bedrock is committed to meeting the industry<a id="_idIndexMarker990"/> standards <a id="_idIndexMarker991"/>with <a id="_idIndexMarker992"/>data<a id="_idIndexMarker993"/> privacy and protection. It has multiple accreditations, such as <strong class="bold">GDPR</strong> (<strong class="bold">General Data Protection Regulation</strong>), <strong class="bold">HIPAA</strong> (<strong class="bold">Health Insurance Portability and Accountability Act</strong>), <strong class="bold">SOC</strong> (<strong class="bold">System and Organization Control</strong>) <strong class="bold">1</strong>, <strong class="bold">2</strong> and <strong class="bold">3</strong>, <strong class="bold">ISO</strong> (<strong class="bold">International Organization for Standardization</strong>), <strong class="bold">STAR</strong> (<strong class="bold">Security Trust Assurance and Risk</strong>), and <strong class="bold">PCI-DSS</strong> (<strong class="bold">Payment Card Industry Data Security Standard</strong>). The comprehensive <a id="_idIndexMarker994"/>compliance posture<a id="_idIndexMarker995"/> enables you to leverage Amazon Bedrock with confidence, knowing that your data is protected and handled in <a id="_idIndexMarker996"/>accordance with industry best practices and regulatory mandates.</li>
</ul>
<p>Next, let us look at protecting the data through data encryption.</p>
<h1 id="_idParaDest-213"><a id="_idTextAnchor233"/>Data encryption</h1>
<p>If you have used AWS<a id="_idIndexMarker997"/> services before, you might be familiar with the <strong class="bold">AWS Shared Responsibility Model</strong>, where<a id="_idIndexMarker998"/> AWS manages and is responsible for securing underlying cloud infrastructure, and you are responsible for protecting the data and applications hosted on this infrastructure. If you would like to read through the AWS Shared Responsibility Model, you can go to <a href="https://aws.amazon.com/compliance/shared-responsibility-model/">https://aws.amazon.com/compliance/shared-responsibility-model/</a>.</p>
<p>When it comes to protecting the data, you can perform encryption on them using AWS KMS or you can also perform client-side encryption before writing to AWS resources. Let us look at the encryption of different resources that you can perform to safeguard your data:</p>
<ul>
<li><strong class="bold">Knowledge bases</strong>: KMS can be used to encrypt data that is in transition for knowledge bases. During the creation or update of a data source, you can provide the KMS key ARN to encrypt the ingested data, ensuring the confidentiality of your knowledge base content. <em class="italic">Figure 12</em><em class="italic">.1</em> demonstrates <strong class="bold">Advanced settings</strong> that can be used for working with KMS.</li>
</ul>
<div><div><img alt="Figure 12.1 – KMS key for transient data storage" src="img/B22045_12_01.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.1 – KMS key for transient data storage</p>
<p class="list-inset">As shown in the preceding figure, when you create or update the data source of a knowledge base, you can specify whether to use the default AWS-managed KMS key or <strong class="bold">Customize encryption settings</strong> where you choose your own customer-managed KMS key. Encryption for knowledge bases can happen at various stages. During the data ingestion phase, Bedrock employs the KMS encryption key<a id="_idIndexMarker999"/> to secure the transient data storage. This temporary storage facilitates the secure ingestion of your data sources, ensuring that your information remains protected while in transition.</p>
<p class="list-inset">If you set up OpenSearch as a vector index for knowledge bases, the information passed to this service is also encrypted using a KMS key, providing an additional layer of security. Furthermore, Bedrock extends its encryption capabilities to the following resources associated with your knowledge bases:</p>
<ul>
<li><strong class="bold">Data in S3 bucket</strong>: By encrypting these sources with a KMS key, you can be assured that your valuable data remains confidential and inaccessible to unauthorized parties.<p class="list-inset">You can allow Amazon Bedrock to decrypt the data from the S3 bucket by attaching the following IAM policy to the Amazon Bedrock service role:</p><pre class="source-code">
{</pre><pre class="source-code">
"Version": "2012-10-17",</pre><pre class="source-code">
"Statement": [{</pre><pre class="source-code">
"Effect": "Allow",</pre><pre class="source-code">
"Action": ["KMS:Decrypt"],</pre><pre class="source-code">
"Resource": ["arn:aws:kms:region:account-id:key/key-id"],</pre><pre class="source-code">
"Condition": {"StringEquals": {"kms:ViaService": ["s3.region.amazonaws.com"]}}}]</pre><pre class="source-code">
}</pre><p class="list-inset">Please note that you would need to update the resource in the policy with the ARN of the KMS key.</p></li>
<li><strong class="bold">Third-party vector stores</strong>: If you leverage external vector stores, Bedrock allows you to encrypt them using a KMS key, maintaining the security and integrity of your knowledge bases across multiple platforms.<p class="list-inset">For more details and <a id="_idIndexMarker1000"/>the IAM permissions needed for knowledge base resources, you can check out <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/encryption-kb.html">https://docs.aws.amazon.com/bedrock/latest/userguide/encryption-kb.html</a>.</p></li>
<li><strong class="bold">Model customization</strong>: When it comes to creating model customization resources, it is essential to understand how the platform handles the data during the process. Firstly, Amazon Bedrock does not use the training data to improve the base FMs and is also never accessible by any of the model providers. When creating the customization job, Amazon Bedrock creates a copy of the FM, and your data is used to fine-tune that copied model. Importantly, your training data is not used to train the base FMs themselves, nor it is shared or seen by model providers.<p class="list-inset">Additionally, Amazon Bedrock takes measures to protect the confidentiality of your data. Once the fine-tuning process is completed, your training or validation data is not stored by the service. However, it’s worth noting that fine-tuned models may inadvertently reproduce portions of the training data during output generation. To mitigate this risk, it’s recommended to filter out any sensitive or confidential information from your training data before initiating the customization process.</p><p class="list-inset">Regarding the encryption options for customization jobs, by default, custom models are encrypted using AWS managed KMS key. Alternatively, you can use your own customer-managed KMS key, which provides more control over your data encryption. To use a customer-managed key, you would need to create the key, attach a resource-based policy granting appropriate permissions, and specify the key <a id="_idIndexMarker1001"/>when creating the customization job:</p><pre class="source-code">
{</pre><pre class="source-code">
    "Version": "2012-10-17",</pre><pre class="source-code">
    "Id": "KMS Key Policy",</pre><pre class="source-code">
    "Statement": [{</pre><pre class="source-code">
            "Sid": "Permissions for custom model builders",</pre><pre class="source-code">
            "Effect": "Allow",</pre><pre class="source-code">
            "Principal": {"AWS": "arn:aws:iam::account-id:user/role"},</pre><pre class="source-code">
            "Action": [</pre><pre class="source-code">
                "kms:Decrypt",</pre><pre class="source-code">
                "kms:GenerateDataKey",</pre><pre class="source-code">
                "kms:DescribeKey",</pre><pre class="source-code">
                "kms:CreateGrant"</pre><pre class="source-code">
            ],</pre><pre class="source-code">
            "Resource": "*"},</pre><pre class="source-code">
        {</pre><pre class="source-code">
            "Sid": "Permissions for custom model users",</pre><pre class="source-code">
            "Effect": "Allow",</pre><pre class="source-code">
            "Principal": {"AWS": "arn:aws:iam::account-id:user/role"},</pre><pre class="source-code">
            "Action": "kms:Decrypt",</pre><pre class="source-code">
            "Resource": "*"</pre><pre class="source-code">
        }</pre><pre class="source-code">
}</pre><p class="list-inset">This policy grants specific permissions related to KMS key management for two different roles: <code>account-id</code> and <code>user/role</code> placeholders with<a id="_idIndexMarker1004"/> your actual AWS account ID and the appropriate IAM user or role names.</p></li>
<li><strong class="bold">Bedrock agent encryption</strong>: Regarding Agents for Amazon Bedrock, by default, an AWS-managed key is used by Amazon Bedrock. However, you can encrypt the agent resources with your own customer-managed key.<p class="list-inset">First, you would need to attach an identity-based policy, such as the following, to the IAM user or the role, so Amazon Bedrock can perform encryption/decryption on the Bedrock agent resources:</p><pre class="source-code">
{</pre><pre class="source-code">
    "Version": "2012-10-17",</pre><pre class="source-code">
    "Statement": [{</pre><pre class="source-code">
            "Sid": "Allow Bedrock to encrypt/decrypt the bedrock agent resources ,</pre><pre class="source-code">
            "Effect": "Allow",    "Action":["kms:GenerateDataKey","kms:Decrypt"],</pre><pre class="source-code">
            "Resource": "arn:aws:kms:${region}:${account-id}:key/${key-id}",</pre><pre class="source-code">
            "Condition": {"StringEquals": {</pre><pre class="source-code">
"kms:EncryptionContext:aws:bedrock:arn": "arn:aws:bedrock:${region}:${account-id}:agent/${agent-id}"</pre><pre class="source-code">
                }}}]</pre><pre class="source-code">
}</pre><p class="list-inset">In addition, make sure the KMS key has permissions as mentioned in the following link: https://docs.aws.amazon.com/bedrock/latest/userguide/encryption-agents.html.</p></li>
<li><strong class="bold">Guardrails encryption</strong>: By default, Amazon Bedrock uses an AWS-managed encryption key to protect your guardrails. However, you have the option to use your own customer-managed KMS key for enhanced control and customization. To create a customer-managed KMS key for your guardrail, you’ll need to have the necessary permissions in your AWS account. For more details on what permissions you would need to set after creating the customer-managed key, you can check out the following link: https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails-permissions.html.</li>
</ul>
<p>Now that we have<a id="_idIndexMarker1005"/> looked at the encryption options for Amazon Bedrock, let us look at how we can grant users and AWS resources required permissions to Amazon Bedrock.</p>
<h1 id="_idParaDest-214"><a id="_idTextAnchor234"/>AWS IAM</h1>
<p>Using IAM, you can <a id="_idIndexMarker1006"/>provide secure access to only designated users or resources to Amazon Bedrock and its capabilities. IAM allows you to create user accounts and assign permissions to those accounts, determining what actions they can perform on specific resources. Here are some of the key points on how IAM works with Amazon Bedrock:</p>
<ul>
<li><strong class="bold">Identities</strong>: IAM supports<a id="_idIndexMarker1007"/> various types of identities, including IAM users, groups, and roles. Users represent individual people or applications, groups are collections of users, and roles are assumed by trusted entities to gain temporary access.</li>
<li><strong class="bold">Authentication</strong>: To use Amazon Bedrock securely, you must first prove your identity through authentication. This can be achieved by logging in as an AWS root user, an IAM user, or by assuming an IAM role. Additionally, you can authenticate using external <a id="_idIndexMarker1008"/>identities, such as <strong class="bold">SAML </strong>(<strong class="bold">Security Assertion Markup Language</strong>) authentication <strong class="bold">identity providers</strong> (<strong class="bold">IdPs</strong>). These external identities are passed to IAM, which then grants you access to Amazon Bedrock. Your administrator will have set up special roles to enable this access.<p class="list-inset">Alternatively, you can use social media accounts, such as Google or Facebook, to authenticate and gain access to Amazon Bedrock. Again, your administrator will have configured the necessary roles and permissions to allow this form of authentication.</p></li>
<li><code>bedrock</code>. For example, the actions can be <code>bedrock:InvokeModel</code> or <code>bedrock:InvokeModelWithResponseStream</code>.</li><li><strong class="bold">Policy resources</strong>: Policies can specify Bedrock resources using ARNs to grant or deny access.</li><li><strong class="bold">Policy condition keys</strong>: Condition keys add an extra layer of control, allowing you to specify conditions under which a policy is applicable, such as resource tags.</li></ul></li>
<li><strong class="bold">Cross-account access</strong>:<ul><li>Roles can be used to grant access to Bedrock resources across different AWS accounts</li><li><strong class="bold">Forward access sessions</strong> (<strong class="bold">FAS</strong>) enable <a id="_idIndexMarker1010"/>Bedrock to perform actions in other services on your behalf while maintaining your permissions</li></ul></li>
<li><strong class="bold">Service roles</strong>: Bedrock uses service roles, which are IAM roles that Bedrock assumes to perform actions on your behalf.</li>
<li><strong class="bold">Temporary credentials</strong>: Bedrock supports the use of temporary credentials, which are short-lived access keys that provide temporary access to AWS resources, ensuring enhanced security compared to long-term access keys. These credentials<a id="_idIndexMarker1011"/> are automatically generated when you sign in to the AWS Management Console using methods such as <strong class="bold">single sign-on</strong> (<strong class="bold">SSO</strong>) or<a id="_idIndexMarker1012"/> role switching. Alternatively, you can manually create temporary credentials using the AWS CLI or API, allowing you to access AWS resources without exposing your long-term access keys.</li>
<li><strong class="bold">Attribute-based access control (ABAC)</strong>: It’s a flexible approach to granting access<a id="_idIndexMarker1013"/> permissions based on attributes or tags associated with users, roles, and resources. Instead of relying solely on predefined roles or groups, ABAC allows you to define access rules that consider dynamic attributes. For example, you could grant read access to a specific AWS S3 bucket only to users with a particular department tag, ensuring that data access is restricted based on the user’s organizational context. ABAC simplifies access management in rapidly evolving environments by eliminating the need for complex policy configurations.</li>
</ul>
<p>By understanding and properly configuring IAM for Amazon Bedrock, you can ensure that only authorized individuals and applications have access to your resources, minimizing the risk <a id="_idIndexMarker1014"/>of data breaches and unauthorized access.</p>
<p>Let us look at some of the patterns of IAM policies that can be used with Amazon Bedrock.</p>
<h2 id="_idParaDest-215"><a id="_idTextAnchor235"/>Deny access</h2>
<p>With IAM, you can allow <a id="_idIndexMarker1015"/>or deny access to<a id="_idIndexMarker1016"/> perform actions to the model. For example, a user or a role could be denied invocation to a particular model, but they can list the models:</p>
<pre class="source-code">
{
    "Version": "2012-10-17",
    "Statement":
    {
        "Sid": "DenyInference",
        "Effect": "Deny",
        "Action": "bedrock:InvokeModel",
        "Resource": "arn:aws:bedrock:::foundation-model/&lt;model-id&gt;"
    }
 }</pre>
<p>The preceding IAM policy shows the <code>Deny</code> action, where the invocation to a specific model has been denied. For instance, the infrastructure team may be granted the ability to provision computational capacity for a particular model while being restricted from performing inferences on that same model. Conversely, the data science team could be solely permitted to perform inferences on a pre-approved set of models.</p>
<h2 id="_idParaDest-216"><a id="_idTextAnchor236"/>Principle of least privilege</h2>
<p>The <code>dev</code>) team is working on a project where access is only needed for image-generation models and listing any FMs. You <a id="_idIndexMarker1019"/>can then <a id="_idIndexMarker1020"/>apply the following policy:</p>
<pre class="source-code">
{
     "Version": "2012-10-17",
     "Statement": [
          {
               "Sid": "Bedrock Invoke model",
               "Effect": "Allow",
               "Action": "bedrock:InvokeModel",
               "Resource": "arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-image-generator-v1",
               "Condition": {
                    "StringLike": {
                         "aws:ResourceTag/Env": "Dev"
                    }
               }
          },
          {
               "Sid": "List FMs",
               "Effect": "Allow",
               "Action": "bedrock:ListFoundationModels",
               "Resource": "*",
               "Condition": {
                    "StringLike": {
                         "aws:ResourceTag/Env": "Dev"
                    }
               }
          }
     ]
}</pre>
<p>In this policy, the <code>bedrock:InvokeModel</code> action is allowed on the <code>arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-image-generator-v1</code> resource with the condition that the <code>Env</code> resource tag is set to <code>Dev</code>. Secondly, the <code>bedrock:ListFoundationModels</code> action is allowed on all resources (<code>*</code>) with the condition that the <code>Env</code> resource tag is set to <code>Dev</code>.</p>
<p>The principle of least privilege minimizes the potential attack surface and reduces the risk of unintended access or data breaches. Let us look at the best practices and implementation<a id="_idIndexMarker1021"/> steps to help you <a id="_idIndexMarker1022"/>audit access and enforce the principle of least privilege:</p>
<ul>
<li><strong class="bold">Review and analyze </strong><strong class="bold">access patterns</strong>:<ul><li>Regularly review AWS CloudTrail logs to understand the actions performed by users and resources within your environment</li><li>Use tools such as IAM Access Analyzer to generate policies based on actual usage patterns, ensuring that permissions align with real-world requirements</li><li>Leverage IAM access advisor to identify unused permissions and remove them from policies, reducing the attack surface</li></ul></li>
<li><strong class="bold">Implement granular </strong><strong class="bold">permissions policies</strong>:<ul><li>Create fine-grained permissions policies that grant only the necessary actions and resources required for specific job roles or functions</li><li>Consider using AWS-managed policies as a starting point for common job functions, and then customize them as needed</li><li>Regularly review and trim overly permissive policies to adhere to the principle of least privilege</li></ul></li>
<li><strong class="bold">Limit access to </strong><strong class="bold">production environments</strong>:<ul><li>Ensure that users have limited access to production environments, granting access only when there is a valid use case</li><li>Revoke production access promptly after the user completes the specific tasks requiring that level of access</li></ul></li>
<li><strong class="bold">Leverage </strong><strong class="bold">permissions boundaries</strong>:<ul><li>Implement permissions boundaries, which are managed policies that set the maximum permissions an identity-based policy can grant to an IAM entity</li><li>Use permissions boundaries to enforce organizational-wide access controls and prevent unintended privilege escalation</li></ul></li>
<li><strong class="bold">Utilize resource tags for </strong><strong class="bold">access control</strong>:<ul><li>Implement an ABAC model using resource tags, which allows you to grant access based on resource attributes such as purpose, owner, or environment</li><li>Combine <a id="_idIndexMarker1023"/>resource<a id="_idIndexMarker1024"/> tags with permissions policies to achieve fine-grained resource access without overly complex custom policies</li></ul></li>
<li><strong class="bold">Implement service control policies in </strong><strong class="bold">AWS Organizations</strong>:<ul><li>Use service control policies to centrally control the maximum available permissions for member accounts within your AWS organization</li><li>Restrict root user permissions in member accounts using service control policies</li><li>Consider using AWS Control Tower for prescriptive managed controls and defining your own custom controls</li></ul></li>
<li><strong class="bold">Establish user life </strong><strong class="bold">cycle policies</strong>:<ul><li>Define and implement user life cycle policies that outline tasks to be performed when users are onboarded, change roles, or no longer require access to AWS</li><li>Conduct periodic permission reviews during each step of the user life cycle to prevent permissions creep</li></ul></li>
<li><strong class="bold">Schedule regular </strong><strong class="bold">permission audits</strong>:<ul><li>Establish a regular schedule to review user permissions and remove any unneeded or excessive permissions</li><li>Leverage tools such as AWS Config and IAM Access Analyzer to assist in auditing user permissions and identifying potential issues</li></ul></li>
<li><strong class="bold">Develop a job </strong><strong class="bold">role matrix</strong>:<ul><li>Create a job role matrix that visualizes the various roles and access levels required within your AWS footprint</li><li>Use groups to separate permissions based on user responsibilities within your organization, rather than applying permissions directly to individual users or roles</li></ul></li>
</ul>
<p>By following <a id="_idIndexMarker1025"/>these best practices and <a id="_idIndexMarker1026"/>implementing the necessary steps, you can effectively audit access and ensure that the principle of least privilege is enforced.</p>
<h2 id="_idParaDest-217"><a id="_idTextAnchor237"/>Model customization</h2>
<p>When working with <a id="_idIndexMarker1027"/>model customization, Amazon Bedrock needs to assume an AWS IAM role on your behalf to initiate the fine-tuning job. This requires you to establish a trust relationship between Amazon Bedrock and the IAM role you want to use.</p>
<p>To set up this trust relationship, you need to add a trust policy to the IAM role you wish to use for model customization. The trust policy grants Amazon Bedrock permission to assume the role and perform the necessary actions on your behalf.</p>
<p>Here’s an example of the trust policy you need to add to your IAM role:</p>
<pre class="source-code">
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
                "Service": "bedrock.amazonaws.com"
            },
            "Action": "sts:AssumeRole"
        }
    ]
 }</pre>
<p>This trust policy specifies that the AWS service bedrock.amazonaws.com (which represents Amazon Bedrock) is allowed to assume the role by calling the <code>sts:AssumeRole</code> action.</p>
<p>To add this trust<a id="_idIndexMarker1028"/> policy to your IAM role, you can follow these steps:</p>
<ol>
<li>Open the AWS Management Console and navigate to the IAM service.</li>
<li>In the left navigation pane, click <strong class="bold">Roles</strong>.</li>
<li>Find the role you want to use for model customization or create a new role if needed.</li>
<li>Click on the role name to open the role details.</li>
<li>In the <strong class="bold">Trust relationships</strong> tab, click the <strong class="bold">Edit trust </strong><strong class="bold">policy</strong> button.</li>
<li>Replace the existing policy document with the provided trust policy.</li>
<li>Click the <strong class="bold">Update policy</strong> button to save the changes.</li>
</ol>
<p>By adding this trust policy, you establish a secure trust relationship between Amazon Bedrock and your IAM role, allowing Amazon Bedrock to assume the role and perform the necessary actions for model customization on your behalf.</p>
<p>Next, the necessary permissions required for the customization process involve accessing the training and validation data that is stored in the S3 bucket, and the output path where Amazon Bedrock should deliver the results of the fine-tuning job. To learn more about the permissions needed for model customization, you can go through the following link: https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-iam-role.html.</p>
<p>Now that we<a id="_idIndexMarker1029"/> have looked at patterns of IAM policies needed for Amazon Bedrock, let us look at the aspect of network security.</p>
<h1 id="_idParaDest-218"><a id="_idTextAnchor238"/>Securing the network</h1>
<p>In the previous sections, we<a id="_idIndexMarker1030"/> looked at the data encryption techniques. Another measure to safeguard the data from a network perspective is to use <strong class="bold">Amazon VPC</strong> for model <a id="_idIndexMarker1031"/>customization and creating a secure, isolated environment for your workloads. By doing so, you gain granular control over network traffic, enabling you to monitor and regulate all incoming and outgoing data flows using VPC Flow Logs. The following figure shows the VPC settings that you can specify while creating the fine-tuning or continued pre-training job.</p>
<div><div><img alt="Figure 12.2 – VPC settings" src="img/B22045_12_02.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.2 – VPC settings</p>
<p>Furthermore, Amazon Bedrock integrates with <a id="_idIndexMarker1032"/>AWS <strong class="bold">PrivateLink</strong>, so you can establish a private connection between your VPC and the Amazon Bedrock service. This connection is facilitated through the creation of a VPC interface endpoint, essentially a private entry point for traffic destined for Amazon Bedrock. In addition, Amazon Bedrock does not use public IP addresses or internet gateways, ensuring that your data never traverses the public internet, thus minimizing potential exposure to cyber threats.</p>
<p>To enhance security<a id="_idIndexMarker1033"/> even further, you can implement endpoint policies, which are permissions that can be attached to your VPC interface endpoint. These policies enable you to precisely define the principals (AWS accounts, IAM users, and IAM roles) authorized to perform specific actions on designated resources. By crafting custom endpoint policies, you can exercise fine-grained control over the access granted to Amazon Bedrock from within your VPC, effectively hardening your security posture.</p>
<p>Here is an example VPC endpoint policy that grants access to anyone (<code>"Principal": "*"</code>) to perform <code>InvokeModel</code> and <code>InvokeModelWithResponseStream</code> of Bedrock:</p>
<pre class="source-code">
{
   "Version": "2012-10-17",
   "Statement": [
      {
         "Principal": "*",
         "Effect": "Allow",
         "Action": [
            "bedrock:InvokeModel",
            "bedrock:InvokeModelWithResponseStream"
         ],
         "Resource":"*"
      }
   ]
}</pre>
<p>Now, let us look at <a id="_idIndexMarker1034"/>what network flow looks like behind the scenes.</p>
<h1 id="_idParaDest-219"><a id="_idTextAnchor239"/>Network flow</h1>
<p>We have looked at <a id="_idIndexMarker1035"/>how the encryption with Amazon works and how you can secure the network via AWS PrivateLink when you are in Amazon VPC. Now, let us look at the network and data flow work behind the scenes for both invocation and model customization jobs.</p>
<h2 id="_idParaDest-220"><a id="_idTextAnchor240"/>On-demand architecture</h2>
<p>With on-demand <a id="_idIndexMarker1036"/>mode, you share the compute environment of the model with other users and are billed based on the usage, without any long-term commitments. <em class="italic">Figure 12</em><em class="italic">.3</em> shows the overview of the on-demand network architecture employed by Amazon Bedrock.</p>
<div><div><img alt="Figure 12.3 – On-demand compute environment architecture" src="img/B22045_12_03.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.3 – On-demand compute environment architecture</p>
<p>Let us understand<a id="_idIndexMarker1037"/> the figure in detail:</p>
<ul>
<li>In the middle, we have <strong class="bold">Amazon Bedrock service account</strong>, which acts as the entry point for all incoming requests. This account is managed and controlled by Amazon, ensuring secure and reliable access to the service. The Amazon Bedrock service is responsible for handling these incoming requests and routing them to the appropriate runtime inference environment. This environment is designed to process and execute the requested operations on the deployed models.</li>
<li>On the right, we have <strong class="bold">Model deployment account</strong>, which is owned and operated by Amazon. Interestingly, there is one such account for each model provider and AWS region combination. This segregation ensures that no model vendor can access or tamper with the data or models of other vendors, enhancing security and privacy. Within the <strong class="bold">Model deployment account</strong>, we find the on-demand compute resources, which are dynamically provisioned and scaled based on the incoming workload. Additionally, this account hosts the base model in S3 bucket, which stores these FMs.</li>
</ul>
<p>The flow of an inference request is as follows:</p>
<ol>
<li>A user initiates a request, which is received by the API endpoint of the Amazon Bedrock service account.</li>
<li>The request is authenticated and authorized using the AWS IAM service.</li>
<li>Once validated, the request is forwarded to the runtime inference environment.</li>
<li>The runtime inference component interacts with the relevant compute cluster within the model deployment account, fetching the required model and executing the requested operation.</li>
<li>The result is then securely returned to the user via the Amazon Bedrock service account.</li>
</ol>
<p>Throughout this process, several measures are in place to ensure data security and privacy:</p>
<ul>
<li>All internal traffic is encrypted using TLS 1.2 or higher encryption standards</li>
<li>No customer data is stored or persisted within the Amazon Bedrock service account</li>
<li>Detailed logs and audit trails are maintained using AWS CloudTrail and Amazon CloudWatch services</li>
<li>Model providers, including Amazon’s own models like Titan, cannot access or influence the customer’s data or models within the model deployment account</li>
</ul>
<p>Now, let us look <a id="_idIndexMarker1038"/>at provisioned throughput capacity architecture.</p>
<h2 id="_idParaDest-221"><a id="_idTextAnchor241"/>Provisioned throughput architecture</h2>
<p>Provisioned throughput<a id="_idIndexMarker1039"/> allows you to purchase the model units for base or customized models, designed for large-scale inference workloads requiring guaranteed throughput.</p>
<p><strong class="bold">Model units</strong> are a key<a id="_idIndexMarker1040"/> concept in Amazon Bedrock’s provisioned throughput feature, designed to provide consistent and scalable performance for LLM inference. One can think of a model unit as a representation of fraction of the underlying GPU hardware resources allocated to run a specific model. It’s not a standardized measure across all models, but rather model-specific.</p>
<p>These model units are purchased allocations of computational resources for a specific base model in Amazon Bedrock. Each model unit provides a guaranteed level of throughput, measured in tokens processed per minute. This applies to both input and output tokens. By using model units, you can ensure consistent performance for high-volume AI workloads, with the flexibility to scale resources based on your needs.</p>
<p><em class="italic">Figure 12</em><em class="italic">.4</em> shows the overview of the provisioned throughput architecture employed by Amazon Bedrock.</p>
<div><div><img alt="Figure 12.4 – Provisioned throughput compute architecture" src="img/B22045_12_04.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.4 – Provisioned throughput compute architecture</p>
<p>Similar to the<a id="_idIndexMarker1041"/> on-demand architecture, we have <strong class="bold">Amazon Bedrock service account</strong> acting as the entry point for incoming requests. This account is managed by Amazon, ensuring secure access to the service. Here is a breakdown of the preceding figure:</p>
<ul>
<li>Within the <strong class="bold">Model deployment account</strong>, we find two distinct components: the <strong class="bold">Base model S3 bucket</strong> and the <strong class="bold">Customized model S3 bucket</strong>. The base model bucket stores the base models provided by Amazon and other vendors like AI21, Cohere, etc., while the customized model S3 bucket stores the custom models or copies of the baseline models tailored to individual requirements.</li>
<li>The <strong class="bold">Runtime inference</strong> component is responsible for processing the incoming requests and determining the appropriate compute environment to handle the requested operation. This decision is based on whether the request is for a baseline model or a customized model.</li>
</ul>
<p>The inference can be performed on provisioned throughput mode by called <code>InvokeModel</code> API or <code>InvokeModelWithResponseStream</code> API and specifying the <code>modelId</code> as provisioned throughput model ARN. The flow of an inference request for a provisioned capacity compute environment is as follows:</p>
<ol>
<li>A customer initiates a request, which is received by the API endpoint of the Amazon Bedrock service account.</li>
<li>The request is authenticated and authorized using AWS IAM services.</li>
<li>Once validated, the request is forwarded to the runtime inference component.</li>
<li>The runtime inference component analyzes the request and determines whether it is for a baseline model or a customized model.</li>
<li>If the request is for a customized model, the runtime inference component directs the request to the dedicated provisioned throughput compute environment associated with that customer or model.</li>
<li>The provisioned throughput compute environment executes the requested operation on the specified customized model and returns the result.</li>
<li>The computed result is then securely returned to the customer via the Amazon Bedrock service account.</li>
</ol>
<p>The provisioned<a id="_idIndexMarker1042"/> capacity architecture shares several key features with the on-demand architecture, including the following:</p>
<ul>
<li>Encrypted internal traffic using TLS 1.2 or higher encryption standards</li>
<li>No customer data storage or persistence within the Amazon Bedrock service account</li>
<li>Detailed logging and auditing through AWS CloudTrail and Amazon CloudWatch services</li>
<li>Strict isolation and access controls, ensuring that model providers cannot access or influence customer data or models</li>
</ul>
<p>From a developer’s perspective, the process of invoking a baseline model or a customized model is<a id="_idIndexMarker1043"/> seamless if you are using on-demand or provisioned throughput mode.</p>
<p>Now, let us look at the architecture overview of model customization.</p>
<h2 id="_idParaDest-222"><a id="_idTextAnchor242"/>Model customization architecture</h2>
<p>Model customization <a id="_idIndexMarker1044"/>is where we are either fine-tuning or continued pre-training the base model tailored to the domain specific use-case. <em class="italic">Figure 12</em><em class="italic">.5</em> provides an overview of the model customization architecture employed by Amazon Bedrock.</p>
<div><div><img alt="Figure 12.5 – Model customization architecture" src="img/B22045_12_05.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.5 – Model customization architecture</p>
<p>Let us look at the steps:</p>
<ol>
<li>The process begins with the user initiating a request through the API endpoint of the Amazon Bedrock service account. This account is managed by Amazon and serves as a secure entry point for all incoming requests.</li>
<li>The Amazon Bedrock service account routes the request to the training orchestration component, which orchestrates the customization process within the relevant model deployment account owned and operated by Amazon.</li>
<li>The training orchestration component initiates an Amazon SageMaker training job, which is responsible for the actual model customization process. Amazon SageMaker is an AWS ML service, where you can build, train, deploy, and monitor ML models. If you are interested in learning about SageMaker, here is an interesting book by Julien Simon: <em class="italic">Learn Amazon SageMaker: A guide to building, training, and deploying machine learning models for developers and data scientists</em>, available at https://www.amazon.com/Learn-Amazon-SageMaker-developers-scientists/dp/180020891X.<p class="list-inset">The training job in the model deployment account leverages the following resources:</p><ul><li>The base<a id="_idIndexMarker1045"/> model S3 bucket, which stores the baseline models provided by Amazon and other vendors from Meta, Cohere, and AI21.</li><li>The user’s training data, which is securely retrieved from an S3 bucket within the user’s account, optionally via a VPC connection for enhanced security.</li></ul></li>
<li>During the training process, the user’s training data is used to customize the selected base model, creating a tailored version specific to your requirements.</li>
<li>Upon completion of the training job, the customized model is encrypted and stored in the customized model S3 bucket within the model deployment account.</li>
<li>It’s important to note that at no point do model vendors have access to or visibility into the user’s training data or the resulting customized model.</li>
<li>Additionally, the training job generates output metrics and logs, which are securely delivered to an S3 bucket specified by the user during the initial request.</li>
</ol>
<p>The model customization architecture incorporates several security and privacy measures:</p>
<ul>
<li>Strict access controls and isolation ensure that model vendors cannot access your data or customized models</li>
<li>Your training data is securely retrieved from your account, either directly from an S3 bucket or via a VPC connection</li>
<li>Encryption is employed to protect the customized model during storage and transfer</li>
<li>Detailed logging and auditing are provided through CloudTrail, IAM, and CloudWatch services</li>
</ul>
<p>By leveraging this architecture, Amazon Bedrock allows you to tailor the base models to your specific needs while maintaining strict data privacy and security standards. The separation of responsibilities and the secure data handling mechanisms ensure that sensitive information remains protected throughout the customization process. For a more detailed <a id="_idIndexMarker1046"/>exploration of how to adapt models to your unique requirements, please refer to <a href="B22045_04.xhtml#_idTextAnchor073"><em class="italic">Chapter 4</em></a>.</p>
<p>Now, that we have looked at the security and network flow components, let us look at ethical practices, where we cover challenges and risks with GenAI.</p>
<h1 id="_idParaDest-223"><a id="_idTextAnchor243"/>Ethical practices</h1>
<p>There have been <a id="_idIndexMarker1047"/>rapid advancements in GenAI, but at the same time, they raise new challenges and risks. Some of these are as follows:</p>
<ul>
<li>Would my data be used with the provider?</li>
<li>Could it hurt the legal rights of the company?</li>
<li>Will the model hallucinate and provide non-sensical, biased, or factually incorrect responses in production?</li>
<li>Would the GenAI models inadvertently use or reproduce intellectual property, such as copyrighted text or images, during training or generation?</li>
</ul>
<p>Let us cover these <a id="_idIndexMarker1048"/>challenges and best practices that can be adopted.</p>
<h2 id="_idParaDest-224"><a id="_idTextAnchor244"/>Veracity</h2>
<p><strong class="bold">Veracity</strong>, or the<a id="_idIndexMarker1049"/> truthfulness and accuracy of information <a id="_idIndexMarker1050"/>generated by AI models, is a crucial aspect of ethical practices in the field of GenAI. When models produce outputs that are verifiably false or hallucinated, it can lead to the spread of misinformation and undermine trust in the technology. One common example of hallucinations is when an AI model is asked to provide information about a specific topic, such as academic papers by a particular author. Instead of searching for and retrieving actual citations, the model will tend to generate fictitious paper titles, topics, and co-author names that seem plausible but do not correspond to real published works.</p>
<p>To mitigate the risk of hallucinations and improve veracity, several best practices can be employed:</p>
<ul>
<li><strong class="bold">Prompt engineering</strong>: Providing <a id="_idIndexMarker1051"/>clear and specific instructions to the AI model can help guide it toward generating more accurate and truthful outputs. Well-crafted prompts that clearly define the desired output and provide relevant context can reduce the likelihood of hallucinations.</li>
<li><strong class="bold">Providing more context</strong>: Techniques such as RAG, continued pre-training, fine-tuning, and the use of agents can help ground the model’s outputs in factual information by providing additional context and knowledge from external sources.</li>
<li><strong class="bold">Inference parameter tuning</strong>: Adjusting parameters such as temperature, Top P, and Top K, which control the randomness of the model’s outputs, can help strike a balance between creativity and factual accuracy, reducing the likelihood of hallucinations while still allowing for novel and useful generations. If you would like to learn <a id="_idIndexMarker1052"/>more about these parameters, please go through <a href="B22045_02.xhtml#_idTextAnchor034"><em class="italic">Chapter 2</em></a>.</li>
</ul>
<p>By implementing these best practices and continuously validating the veracity of AI-generated content, we <a id="_idIndexMarker1053"/>can promote trust in GenAI models and ensure their responsible and ethical deployment in various domains.</p>
<h2 id="_idParaDest-225"><a id="_idTextAnchor245"/>Intellectual property</h2>
<p>One major challenge <a id="_idIndexMarker1054"/>with early LLMs is their tendency to directly repeat or copy portions of the text data they were trained on. This raises privacy concerns, as the training data may contain sensitive or personal information. It also raises copyright issues, as the models could be reproducing copyrighted content without permission. The root cause of this problem lies in how these LLMs are trained. They are exposed to massive amounts of data from various sources, including books, websites, and databases, many of which are copyrighted works. However, this data is ingested without keeping track of sources or obtaining proper licenses. As a result, when generating text, the models may regurgitate verbatim sentences or passages from their training data, inadvertently exposing copyrighted content or private information. Addressing this requires more robust techniques for filtering training data, tracking sources, and properly licensing materials used to train these powerful language models. Here are some of the key considerations:</p>
<ul>
<li><strong class="bold">Transparency</strong>: AI providers should be transparent about the training data used for their models, disclosing potential sources of copyrighted material, and acknowledging the limitations of their systems in handling intellectual property.</li>
<li><strong class="bold">Fair use</strong>: While some use of copyrighted material for training AI models may fall under fair use exceptions, it is essential to carefully assess the extent and nature of such use to avoid potential infringement.</li>
<li><strong class="bold">Licensing and permissions</strong>: Whenever possible, AI developers should obtain appropriate licenses or permissions to use copyrighted material in their training data, ensuring proper attribution and compensation to the rights holders.</li>
<li><strong class="bold">Content filtering</strong>: Implementing robust content filtering mechanisms can help mitigate the risk of reproducing copyrighted material in the outputs of AI systems. This can involve techniques such as watermarking, fingerprinting, and other content recognition technologies.</li>
<li><strong class="bold">Human oversight</strong>: Incorporating human oversight and review processes can help identify and address potential instances of copyright infringement or unauthorized use <a id="_idIndexMarker1055"/>of intellectual property in the outputs of AI systems.</li>
</ul>
<h2 id="_idParaDest-226"><a id="_idTextAnchor246"/>Safety and toxicity</h2>
<p>Toxicity and safety are<a id="_idIndexMarker1056"/> also important considerations when developing and deploying AI systems, particularly those involving language generation models. Toxic outputs can perpetuate harm, spread misinformation, and undermine the trust and integrity of these systems. Therefore, implementing robust measures to mitigate toxicity and uphold safety is necessary. Here are some key points to address these concerns:</p>
<ul>
<li><strong class="bold">Content filtering</strong>: Implementing strict filters to exclude advice or information related to individual medical, legal, political, or financial matters, as well as instructions for creating weapons or engaging in illegal activities, is essential. Such content could potentially cause direct harm or enable dangerous behavior if provided without proper oversight.</li>
<li><strong class="bold">Guardrails</strong>: Implementing guardrails, such as content filtering, bias detection, and toxic language identification, can help prevent the generation of harmful or inappropriate content. Continuously monitoring and updating these guardrails is necessary to adapt to evolving language patterns and potential misuse. In the next section, we will be covering the guardrails provided by Amazon Bedrock.</li>
<li><strong class="bold">Data curation</strong>: Carefully curating and vetting the training data used for language models to minimize the propagation of biases, toxic language, or factual inaccuracies. Performing regular audits and updates to the training data can help improve model performance and safety over time.</li>
<li><strong class="bold">Watermarking and traceability</strong>: Embedding watermarks or traceable identifiers within generated content can aid in attribution and accountability, discouraging misuse and enabling rapid response to incidents involving toxic or harmful content. For more details on watermark detection by Amazon Bedrock, refer to <a href="B22045_09.xhtml#_idTextAnchor171"><em class="italic">Chapter 9</em></a>.</li>
<li><strong class="bold">Responsible AI policies</strong>: Organizations should implement and establish robust responsible AI policies, guidelines, and best practices. These policies should prioritize ethical considerations, transparency, accountability, and the well-being of users and society.</li>
</ul>
<p>We would recommend that you read through the blog post titled <em class="italic">Responsible AI in the generative era</em> by Michael Kearns: <a href="https://www.amazon.science/blog/responsible-ai-in-the-generative-era">https://www.amazon.science/blog/responsible-ai-in-the-generative-era</a>. This blog post talks about the challenges around issues such as fairness, toxicity, hallucinations, intellectual property violations, and so on, and the active work being done by the tech community to address these challenges, from carefully curating training data, developing guardrail <a id="_idIndexMarker1057"/>models to filter outputs, watermarking approaches, and leveraging more focused use cases.</p>
<p>Let us now look at the guardrails provided by Amazon Bedrock to address some of the concerns we discussed.</p>
<h1 id="_idParaDest-227"><a id="_idTextAnchor247"/>Guardrails for Amazon Bedrock</h1>
<p>With Guardrails <a id="_idIndexMarker1058"/>for Amazon Bedrock, organizations can implement safeguards that align with safe and responsible AI policies. These safeguards provide an additional layer of control, complementing the existing protections built in the FMs. You can implement these guardrails to all the FMs available within Amazon Bedrock, along with any fine-tuned models and Agents for Amazon Bedrock that you create. Let us look at some of the examples when the implementation of guardrails will be required by various industries:</p>
<ul>
<li><strong class="bold">Healthcare industry</strong>:<ul><li>Guardrails can be used in medical chatbots or virtual assistants to prevent the exchange of information related to self-diagnosis, prescription drugs, or medical advice without proper authorization</li><li>In applications that analyze medical records or imaging data, guardrails can be employed to redact sensitive patient information and ensure compliance with privacy regulations such as HIPAA</li></ul></li>
<li><strong class="bold">Legal industry</strong>:<ul><li>Guardrails can be used in legal document analysis tools to prevent the disclosure of privileged communication between attorneys and clients</li><li>In legal research or contract review applications, guardrails can be implemented to prevent the generation of misleading or legally non-compliant content</li></ul></li>
<li><strong class="bold">Financial industry</strong>:<ul><li>Guardrails can be used in financial applications to prevent the generation of content related to stock recommendations, investment advice, or insider trading</li><li>In applications that process financial transactions or customer data, guardrails can<a id="_idIndexMarker1059"/> be used to redact sensitive information like account numbers or credit card details</li></ul></li>
<li><strong class="bold">Media and </strong><strong class="bold">entertainment industry</strong>:<ul><li>Guardrails can be used in content generation applications to prevent the creation of copyrighted material, hate speech, or explicit content that may violate content guidelines or community standards</li><li>In applications related to broadcast media, guardrails can be implemented to ensure adherence to strict regulatory guidelines, protect against copyright infringement, and maintain appropriate content standards</li></ul></li>
<li><strong class="bold">Retail and </strong><strong class="bold">e-commerce industry</strong>:<ul><li>Guardrails can be implemented in e-commerce platforms to protect customer privacy by redacting personal information and preventing the unauthorized access or display of sensitive data such as payment details or shipping addresses</li><li>In product recommendation or customer service applications, guardrails can be used to prevent the promotion of harmful or illegal products, ensuring compliance <a id="_idIndexMarker1060"/>wit<a id="_idTextAnchor248"/>h local regulations and industry standards</li></ul></li>
</ul>
<p>Now, let us understand how Guardrails for Amazon Bedrock works.</p>
<h2 id="_idParaDest-228"><a id="_idTextAnchor249"/>How does Guardrails for Amazon Bedrock work?</h2>
<p>Guardrails examines <a id="_idIndexMarker1061"/>both the user input prompts and the corresponding model responses via four policy filters, as shown in <em class="italic">Figure 12</em><em class="italic">.6</em>.</p>
<div><div><img alt="Figure 12.6 – How Guardrails for Amazon Bedrock works" src="img/B22045_12_06.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.6 – How Guardrails for Amazon Bedrock works</p>
<p>Guardrails for Amazon Bedrock is available under the <strong class="bold">Safeguards</strong> section of the Bedrock console. When you create a guardrail, you will be asked to configure four policy filters and a blocked message. The four policy filters in Guardrails for Amazon Bedrock are listed as follows:</p>
<ul>
<li>Content filters</li>
<li>Denied topics</li>
<li>Word filters</li>
<li>Sensitive information filters</li>
</ul>
<p>Based on the policy filter that you select, both input prompts and model responses are rigorously vetted against every configured policy.</p>
<p>If any policy violation is detected, either in the input prompt or response, the Guardrails component<a id="_idIndexMarker1062"/> intervenes by overriding the offending content. For more details, please check the <em class="italic">Blocked </em><em class="italic">messaging</em> sub-section.</p>
<p>Let us dive deeper into each of the policy filters.</p>
<h2 id="_idParaDest-229"><a id="_idTextAnchor250"/>Content filters</h2>
<p>With <strong class="bold">content filters</strong>, you can <a id="_idIndexMarker1063"/>configure<a id="_idIndexMarker1064"/> thresholds to detect and block harmful content across various categories. You can adjust the filter strength for prompts and for responses enabling granular control over the strictness of content filtering, as shown in <em class="italic">Figure 12</em><em class="italic">.7</em>.</p>
<div><div><img alt="Figure 12.7 – Configuring content filters" src="img/B22045_12_07.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.7 – Configuring content filters</p>
<p>Categories covered <a id="_idIndexMarker1065"/>include hate speech, insults, sexual<a id="_idIndexMarker1066"/> content, violence, misconduct, and prompt attacks, allowing you to address a wide range of potential risks. The higher the filter strength, the higher the likelihood of filtering out potentially harmful content within a given category, providing a flexible way to balance risk mitigation and content accessibility.</p>
<p>When implementing content filters, it’s essential to consider your use case, target audience, and ethical guidelines. A higher filter strength may be appropriate for applications serving vulnerable populations or dealing with sensitive topics, while a more relaxed approach could be suitable for certain creative or educational contexts. In addition, regularly reviewing and updating your filter configurations in response to evolving societal norms and organizational policies is recommended. By leveraging content filters, you can proactively mitigate the risks associated with harmful content generation, encouraging<a id="_idIndexMarker1067"/> a more trustworthy and <a id="_idIndexMarker1068"/>responsible AI ecosystem.</p>
<h2 id="_idParaDest-230"><a id="_idTextAnchor251"/>Denied topics</h2>
<p><strong class="bold">Denied topics</strong> allow you<a id="_idIndexMarker1069"/> to proactively<a id="_idIndexMarker1070"/> prevent your application from engaging with or generating content related to specific subjects that may be deemed undesirable or inappropriate within your use case.</p>
<p>You can define up to 30 denied topics by providing a concise name and a clear natural language description for each topic you wish to restrict. These serve as the foundation for detecting and blocking user inputs or model responses that fall within the defined topic boundaries, ensuring a consistent and reliable filtering mechanism.</p>
<p>When specifying a denied topic, it’s advisable to provide a comprehensive definition that captures the essence of the subject matter you aim to exclude, surrounding all relevant inquiries, guidance, or recommendations.</p>
<p>For example, in the healthcare domain, maintaining patient privacy and adhering to strict ethical guidelines is important. A hospital or medical institution could leverage denied topics to prevent the models from engaging in discussions or providing information related to specific topics that could potentially violate patient confidentiality or medical ethics.</p>
<p>One denied topic could be <em class="italic">Disclosing Patient Information</em>, with a definition along the lines of <em class="italic">Sharing or revealing personal details, medical records, or any identifying information about patients without </em><em class="italic">proper authorization</em>.</p>
<p>Another relevant denied topic might be <em class="italic">Unauthorized Medical Advice</em>, defined as <em class="italic">Providing diagnostic assessments, treatment recommendations, or any form of medical guidance without being a licensed healthcare professional or having access to a patient’s complete </em><em class="italic">medical history</em>.</p>
<p><em class="italic">Figure 12</em><em class="italic">.8</em> shows how to add denied topics.</p>
<div><div><img alt="Figure 12.8 – Adding a denied topic" src="img/B22045_12_08.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.8 – Adding a denied topic</p>
<h2 id="_idParaDest-231"><a id="_idTextAnchor252"/>Word filters</h2>
<p>By <a id="_idIndexMarker1071"/>configuring <strong class="bold">word filters</strong>, you<a id="_idIndexMarker1072"/> can effectively block undesirable words, phrases, and profanity from appearing in user inputs or model responses, promoting a positive and inclusive user experience. Here is how:</p>
<ul>
<li>The <strong class="bold">profanity filter</strong> offers a <a id="_idIndexMarker1073"/>convenient way to block a predefined list of commonly recognized profane words. This list is based on a global definition of profanity and is subject to regular updates, ensuring that the filter remains aligned with evolving societal norms.</li>
<li>Additionally, you can customize the word filtering experience by specifying up to 10,000 custom words and phrases that should be blocked. This flexibility allows you to tailor the filters to your specific needs, such as excluding offensive terms, competitor names, or any other language that may be deemed inappropriate for your use case.</li>
<li>Custom words and phrases can be added through various convenient methods, including a manual entry in the console, uploading a local file (e.g., <code>.txt</code> or <code>.csv</code>), or populating the list from an Amazon S3 object, providing flexibility in managing your word filter list.</li>
</ul>
<p><em class="italic">Figure 12</em><em class="italic">.9</em> shows how you can configure the word filter with different options.</p>
<div><div><img alt="Figure 12.9 – Adding word filters" src="img/B22045_12_09.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.9 – Adding word filters</p>
<p>By using word filters, you can create a more controlled and inclusive environment for your users. For instance, an educational platform could block offensive language to promote a positive<a id="_idIndexMarker1074"/> learning atmosphere, while <a id="_idIndexMarker1075"/>a corporate application might filter out competitor names to maintain brand integrity and avoid potential conflicts.</p>
<h2 id="_idParaDest-232"><a id="_idTextAnchor253"/>Sensitive information filters</h2>
<p><strong class="bold">Sensitive information filters</strong> allow you<a id="_idIndexMarker1076"/> to proactively <a id="_idIndexMarker1077"/>identify and take appropriate actions on various types of PII and custom-defined sensitive data patterns.</p>
<p>Guardrails offer a comprehensive list of predefined PII types, covering a wide range of sensitive information such as names, addresses, email addresses, phone numbers, credit card details, and social security numbers. These PII types are constantly updated to ensure compliance with evolving regulations and privacy norms. For a complete list, you can visit <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/Guardrails-sensitive-filters.html">https://docs.aws.amazon.com/bedrock/latest/userguide/Guardrails-sensitive-filters.html</a>.</p>
<p>When sensitive information is detected, you can configure guardrails to either block or mask the content:</p>
<ul>
<li><strong class="bold">Block mode</strong>: This prevents the sensitive information from being processed at all. If applied to the input, it stops the prompt containing sensitive data from reaching the model. If applied to the output, it prevents the model’s response containing sensitive information from reaching the user.</li>
<li><code>123-45-6789</code> might be replaced with <code>[SSN]</code>. This ensures privacy while preserving the overall context of the content.</li>
</ul>
<p>Additionally, you can define<a id="_idIndexMarker1078"/> custom <strong class="bold">regular expression</strong> (<strong class="bold">regex</strong>) patterns to filter specific types of <a id="_idIndexMarker1079"/>sensitive information<a id="_idIndexMarker1080"/> relevant to your organization or use case. This flexibility allows you to protect proprietary data, such as serial numbers, booking IDs, or any other critical information that requires safeguarding. For example:</p>
<ul>
<li><code>^ABC-\d{5}-[A-Z]{2}$</code> (matches patterns such as <code>ABC-12345-XY</code>)</li>
<li><code>^BK-\d{6}-[A-Z]{3}$</code> (matches patterns such as <code>BK-123456-NYC</code>)</li>
<li><code>^EMP-\d{4}-[A-Z]{2}$</code> (matches patterns such as <code>EMP-1234-AB</code>)</li>
</ul>
<p>These custom patterns can be used alongside the predefined PII types to create a comprehensive sensitive information protection strategy tailored to your specific needs.</p>
<div><div><img alt="Figure 12.10 – Adding sensitive information filters" src="img/B22045_12_10.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.10 – Adding sensitive information filters</p>
<p><em class="italic">Figure 12</em><em class="italic">.10</em> shows the configuration options you can specify for sensitive information filters.</p>
<p>By leveraging sensitive information filters, you can ensure that your applications maintain the highest standards of privacy and data protection. For instance, a healthcare provider <a id="_idIndexMarker1081"/>could configure<a id="_idIndexMarker1082"/> guardrails to mask patient information in summaries, while a financial institution could block queries related to credit card details or account numbers, mitigating the risk of data breaches and maintaining regulatory compliance.</p>
<h2 id="_idParaDest-233"><a id="_idTextAnchor254"/>Blocked messaging</h2>
<p>Once we have defined<a id="_idIndexMarker1083"/> the policy filters, you can then next define the blocked messaging, when the guardrail blocks any input prompt or response from the model. In such cases, a pre-approved response is provided, tailored to the specific use case or organizational guidelines. <em class="italic">Figure 12</em><em class="italic">.11</em> shows what messaging you would like to display for both input prompts and model responses.</p>
<div><div><img alt="Figure 12.11 – Blocked messaging" src="img/B22045_12_11.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.11 – Blocked messaging</p>
<p>This approach ensures that the end user receives a safe, appropriate, and compliant response, even<a id="_idIndexMarker1084"/> in scenarios where the initial input or model output raises concerns.</p>
<h2 id="_idParaDest-234"><a id="_idTextAnchor255"/>Testing and deploying guardrails</h2>
<p>By testing the <a id="_idIndexMarker1085"/>guardrail, you can iteratively refine and test the models, ensuring they align with your intended use case and adhere to ethical standards. Here’s how it works:</p>
<p>First, you create a guardrail, which initializes a working draft (<code>DRAFT</code>) version. Think of this as a sandbox environment where you can experiment without impacting live systems. This working draft is something you can continuously edit and tweak until you’re satisfied with its performance.</p>
<div><div><img alt="Figure 12.12 – Test Guardrails for Amazon Bedrock" src="img/B22045_12_12.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.12 – Test Guardrails for Amazon Bedrock</p>
<p><em class="italic">Figure 12</em><em class="italic">.12</em> shows the test results of the working draft. Here, I have taken a simple example of soccer, where I have provided <code>soccer</code> as a denied topic. When I provide a prompt pertaining to soccer, the model blocks the response. You can also see from the figure the prompt and model response trace.</p>
<p>Once you’ve perfected the working draft, you can create a version – a snapshot of the guardrail’s configurations at that point in time. Additionally, these versions function as immutable <a id="_idIndexMarker1086"/>checkpoints in the system. This immutability serves a critical purpose: it prevents potential immediate negative impacts on the runtime environment that could occur if a draft version were accidentally deployed to production. For instance, if an engineer inadvertently made a modification to a draft version directly through the console interface, these immutable checkpoints would ensure that such changes don’t affect the live production environment. This safeguard helps maintain system stability and prevents unintended consequences from impromptu modifications.</p>
<p>It’s important to note that any changes made to the working draft will automatically update the live application if the draft is being used in the application. You must explicitly incorporate the desired version into your application to reflect the latest guardrail configurations.</p>
<p>Now that we have <a id="_idIndexMarker1087"/>created, tested and deployed guardrails, let us look at how we can use it.</p>
<h2 id="_idParaDest-235"><a id="_idTextAnchor256"/>Using guardrails</h2>
<p>Guardrails can be<a id="_idIndexMarker1088"/> used in various ways:</p>
<ul>
<li><code>InvokeModel</code>, <code>InvokeModelWithResponseStream</code>, and <code>Converse</code> APIs.</li>
<li><strong class="bold">Knowledge base</strong>: You can include guardrails when querying your knowledge base in the Amazon Bedrock console or via APIs.</li>
<li><strong class="bold">Guardrail to your agent</strong>: Associate a guardrail with your agent when creating or updating an agent in the Amazon Bedrock console or API.</li>
</ul>
<p><em class="italic">Figure 12</em><em class="italic">.13</em> shows the response with and without using Guardrails within Amazon Bedrock Playground.</p>
<div><div><img alt="Figure 12.13 – Using Guardrails in Amazon Bedrock Playground" src="img/B22045_12_13.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.13 – Using Guardrails in Amazon Bedrock Playground</p>
<p>With Amazon Bedrock APIs, you can specify <code>guardrailIdentifier</code> and <code>guardrailIdentifier</code> as<a id="_idIndexMarker1089"/> shown in the code:</p>
<pre class="source-code">
%pip install boto3 botocore
#import the main packages and libraries
import os
import boto3
import json
import botocore
bedrock_runtime = boto3.client('bedrock-runtime') # Provide the desired region
prompt = "Can you tell me about the treatment for chronic condition?"
body = json.dumps({"inputText": prompt,
                   "textGenerationConfig":{
                       "maxTokenCount":4096,
                       "stopSequences":[],
                       "temperature":0,
                       "topP":1
                   },
                  })
modelId = 'amazon.titan-tg1-large' # change this to use a different version from the model provider
accept = 'application/json'
contentType = 'application/json'
response1 = bedrock_runtime.invoke_model(body=body,
                                        modelId=modelId,
                                        accept=accept,
                                        contentType
                                          =contentType,
                                        trace="ENABLED",
                                        guardrailIdentifier
                                          = 'vtfzfvd8ccoz',
                                        guardrailVersion=
                                          "1"
                                        )
response_g = json.loads(response1.get('body').read())
#print(response_body.get('results')[0].get('outputText'))
#output_body = json.loads(response1["body"].read().decode())
action = response_g["amazon-bedrock-guardrailAction"]
if action == "INTERVENED":
    print("Guardrail Intervention: {}".format(json.dumps(response_g["amazon-bedrock-trace"]["guardrail"], indent=2)))
print("Guardrail action: {}".format(response_g["amazon-bedrock-guardrailAction"]))
print("Output text: {}".format(response_g["results"][0]["outputText"]))</pre>
<p>When running<a id="_idIndexMarker1090"/> the preceding code, you can see that the response would be similar to what is shown in <em class="italic">Figure 12</em><em class="italic">.14</em>.</p>
<div><div><img alt="Figure 12.14 – Response with Guardrails" src="img/B22045_12_14.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.14 – Response with Guardrails</p>
<p>You can see that the prompt provided in the code relates to medical treatment, for which we have configured Denied Topics filter in <code>Guardrail</code>. The code snippet includes <code>guardrailIdentifier</code> and <code>guardrailVersion</code> to enable Bedrock Guardrails, which can intervene and modify the model’s output based on the filter that is configured.</p>
<p>Furthermore, when using a guardrail with model inference, you can selectively evaluate user input by tagging specific content within the input text. This feature allows you to apply guardrails to certain parts of the input while leaving other parts unprocessed.</p>
<p>For example, imagine you’re creating a conversational AI assistant for a banking application. While you want your assistant to provide helpful information to users, you also need to ensure it doesn’t reveal sensitive account details or encourage risky financial behavior. By <a id="_idIndexMarker1091"/>selectively evaluating user input, you can apply guardrails to specific sections of the conversation, like user queries, while leaving system prompts and conversation history untouched.</p>
<p>Here’s how you could implement this:</p>
<ul>
<li>Use input tags to mark user queries, for example, <code>&lt;amazon-bedrock-guardrails-guardContent_abc&gt;How much money is in my </code><code>savings account?&lt;/amazon-bedrock-guardrails-guardContent_abc&gt;</code></li>
<li>Configure a dynamic tag suffix (for example, <code>abc</code>) in the <code>amazon-bedrock-guardrailConfig</code> to prevent prompt injection attacks.</li>
<li>Any content outside the tags such as system prompts and conversation history won’t be processed by guardrails.</li>
<li>For user queries within the tags, guardrails will ensure the AI assistant’s response doesn’t reveal sensitive financial information or promote irresponsible money management.</li>
</ul>
<p>This approach not only enhances security and control but also optimizes performance and reduces costs, as guardrails only evaluate the tagged user input instead of the entire prompt.</p>
<p>When using guardrails with streaming responses, you can either configure it to <strong class="bold">synchronous mode</strong> or <strong class="bold">asynchronous mode</strong>. The <a id="_idIndexMarker1092"/>synchronous mode introduces some latency as<a id="_idIndexMarker1093"/> the guardrail buffers and applies policies to response chunks before sending them to the user, ensuring better accuracy. Alternatively, the asynchronous mode sends<a id="_idIndexMarker1094"/> response chunks immediately while applying policies in the background, sacrificing accuracy for lower latency.</p>
<p>You can enable asynchronous mode by including <code>"streamProcessingMode": "ASYNCHRONOUS"</code> in <code>amazon-bedrock-guardrailConfig</code>. Here is how:</p>
<pre class="source-code">
{
   "amazon-bedrock-guardrailConfig": {
   "streamProcessingMode": "ASYNCHRONOUS"
   }
}</pre>
<p>For conversational applications built with <code>Converse</code> API, you can use guardrails to block inappropriate content entered by the user or generated by the model. When calling the <code>Converse</code> or <code>ConverseStream</code> operations, include the guardrail configuration in the <code>guardrailConfig</code> parameter. Here is how:</p>
<pre class="source-code">
{
        "guardrailIdentifier": "Guardrail ID",
        "guardrailVersion": "Guardrail version",
        "trace": "enabled"
}</pre>
<p>Here is an example code on how you can guard the conversation by using guardrails on <code>Converse</code> API : <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails-use-converse-api.html#converse-api-guardrail-example">https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails-use-converse-api.html#converse-api-guardrail-example</a>.</p>
<p>Guardrails for Amazon Bedrock is a feature that allows organizations to implement safeguards and policy filters to ensure the safe and responsible use of AI models. It provides four policy filters – content filters, denied topics, word filters, and sensitive information filters – to block or redact unwanted content, topics, words, and sensitive information from both user inputs and model outputs. Guardrails helps maintain compliance with<a id="_idIndexMarker1095"/> regulations, protect user privacy, and prevent the generation of harmful or unethical content.</p>
<h1 id="_idParaDest-236"><a id="_idTextAnchor257"/>Summary</h1>
<p>This chapter began by emphasizing the importance of data privacy and protection for organizations in today’s digital landscape. It highlighted Amazon Bedrock’s robust security measures, which ensure that users maintain complete control over their data, along with key aspects such as data localization, isolation, encryption, and access management through IAM.</p>
<p>The chapter then dove deeper into responsible AI practices, addressing challenges such as veracity, intellectual property rights, safety, and toxicity. It provided guidance on implementing content filtering, guardrails, data curation, watermarking, traceability, and establishing robust responsible AI policies. Additionally, the chapter introduced Guardrails for Amazon Bedrock, which offers four policy filters: content filters, denied topics, word filters, and sensitive information filters. These filters enable organizations to implement safeguards aligning with their safe and responsible AI policies, promoting ethical AI deployment across various industries.</p>
<p>Congratulations! You have reached the end of this book. By now, you have gained deep hands-on knowledge of Amazon Bedrock. From understanding the foundational concepts to practical implementations and real-world use cases, we have covered a wide range of topics that will provide you with the knowledge and skills to build scalable and innovative GenAI applications.</p>
<p>Throughout the chapters, we have explored the power of prompt engineering, continuous pre-training, fine-tuning models, and RAG, along with the development of intelligent agents with Amazon Bedrock.</p>
<p>We explored various architectural patterns, such as text generation, summarization, question answering, entity extraction, code generation, and image creation. Additionally, we addressed crucial aspects of monitoring, security, and privacy, ensuring that you can confidently navigate the intricate world of GenAI with Amazon Bedrock while adhering to ethical standards and best practices. You now have a comprehensive understanding of Amazon Bedrock, its capabilities, and the techniques required to harness its full potential.</p>
</div>
</body></html>