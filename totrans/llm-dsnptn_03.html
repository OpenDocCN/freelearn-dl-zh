<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div id="_idContainer015">
			<h1 id="_idParaDest-41" class="chapter-number"><a id="_idTextAnchor049"/>3</h1>
			<h1 id="_idParaDest-42"><a id="_idTextAnchor050"/>Data Augmentation</h1>
			<p>Data augmentation plays a pivotal role in enhancing the performance and generalization capabilities of LLMs. By artificially expanding the training dataset, we can expose our models to a wider range of linguistic variations and contexts, improving their ability to handle diverse inputs and generate more coherent and contextually <span class="No-Break">appropriate outputs.</span></p>
			<p>In the context of LLMs, data augmentation takes on unique challenges and opportunities. Unlike <strong class="bold">image data</strong>, where<a id="_idIndexMarker101"/> simple transformations such as rotation or flipping can create valid new samples, <strong class="bold">text data</strong> requires <a id="_idIndexMarker102"/>more nuanced approaches to maintain semantic integrity and linguistic coherence. The main goals of data augmentation for LLMs include increasing dataset size and diversity, addressing data imbalance and bias, improving model robustness to variations in input, and enhancing generalization to <span class="No-Break">unseen data.</span></p>
			<p>In <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.1</em>, I illustrate the key aspects of <span class="No-Break">data augmentation.</span></p>
			<div>
				<div id="_idContainer014" class="IMG---Figure">
					<img src="image/B31249_03_001.jpg" alt="Figure 3.1 – Key elements of data augmentation" width="810" height="695"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.1 – Key elements of data augmentation</p>
			<p>There are three main components, namely <strong class="bold">Techniques</strong>, <strong class="bold">Considerations</strong>, and <strong class="bold">Evaluation</strong>. Each has specific sub-components, which we’ll cover in detail in <span class="No-Break">this chapter.</span></p>
			<p>By the end of this chapter, you’ll have learned<a id="_idIndexMarker103"/> about the <strong class="bold">data augmentation</strong> pattern in depth, from increasing the diversity of your training dataset to maintaining <span class="No-Break">its integrity:</span></p>
			<ul>
				<li>Text data <span class="No-Break">augmentation techniques</span></li>
				<li>Leveraging existing LLMs for <span class="No-Break">data generation</span></li>
				<li>Multilingual data <span class="No-Break">augmentation strategies</span></li>
				<li>Semantic preservation in <span class="No-Break">text augmentation</span></li>
				<li>Balancing augmentation and <span class="No-Break">data quality</span></li>
				<li>Evaluating the impact of <span class="No-Break">data augmentation</span></li>
			</ul>
			<h1 id="_idParaDest-43"><a id="_idTextAnchor051"/>Text data augmentation techniques</h1>
			<p>Text data augmentation <a id="_idIndexMarker104"/>encompasses a wide range of techniques, from simple word-level manipulations to more complex <span class="No-Break">semantic transformations.</span></p>
			<h2 id="_idParaDest-44"><a id="_idTextAnchor052"/>Synonym replacement</h2>
			<p>This technique involves<a id="_idIndexMarker105"/> replacing words in<a id="_idIndexMarker106"/> the original text with their synonyms. We <a id="_idIndexMarker107"/>can use <strong class="bold">WordNet</strong>, a lexical database for the English language, to <span class="No-Break">find synonyms:</span></p>
			<pre class="source-code">
def synonym_replacement(text, n=1):
    words = text.split()
    new_words = words.copy()
    random_word_list = list(
        set([word for word in words if word.isalnum()])
    )
    random.shuffle(random_word_list)
    num_replaced = 0
    for random_word in random_word_list:
        synonyms = get_synonyms(random_word)
        if len(synonyms) &gt;= 1:
            synonym = random.choice(list(synonyms))
            new_words = [
                synonym if word == random_word else word
                for word in new_words
            ]
            num_replaced += 1
        if num_replaced &gt;= n:
            break
    return ' '.join(new_words)</pre>			<p>The <strong class="source-inline">synonym_replacement</strong> function<a id="_idIndexMarker108"/> takes a text input and replaces <a id="_idIndexMarker109"/>a specified number (the default is 1) of words with their synonyms. The default value of 1 is chosen to minimize text <a id="_idIndexMarker110"/>alteration, preserving meaning and readability while allowing easy experimentation. You can increase this number if you want <span class="No-Break">more replacements.</span></p>
			<p>The function splits the text into words, creates a list of unique alphanumeric words, shuffles this list, and then iterates through it. For each word, it attempts to find synonyms using an<a id="_idIndexMarker111"/> undefined <strong class="source-inline">get_synonyms</strong> function. If synonyms are found, it randomly selects one and replaces all occurrences of the original word in the text. The function keeps track of how many words have been replaced and stops when it reaches the specified number. Finally, it rejoins the modified words into a single string and <span class="No-Break">returns it.</span></p>
			<h2 id="_idParaDest-45"><a id="_idTextAnchor053"/>Back-translation</h2>
			<p>This method involves <a id="_idIndexMarker112"/>translating the text to another language and then back to the original language. It’s particularly effective for introducing natural variations in sentence structure and <span class="No-Break">word choice:</span></p>
			<pre class="source-code">
def back_translation(text, target_lang='fr'):
    translator = Translator()
    translated = translator.translate(text, dest=target_lang)
    back_translated = translator.translate(translated.text, dest='en')
    return back_translated.text</pre>			<h2 id="_idParaDest-46"><a id="_idTextAnchor054"/>Text generation with T5</h2>
			<p>The <strong class="bold">Text-To-Text Transfer Transformer</strong> (<strong class="bold">T5</strong>) model, developed <a id="_idIndexMarker113"/>by Google <a id="_idIndexMarker114"/>Research, is a versatile <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>) model based<a id="_idIndexMarker115"/> on the transformer architecture. Its key innovation is framing all NLP tasks as text-to-text problems, allowing it to handle multiple tasks without task-specific architectures. Pre-trained on a large web text corpus using a “span corruption” objective, T5 is available in various sizes and has demonstrated powerful performance across a wide range of <span class="No-Break">NLP tasks.</span></p>
			<p>T5 handles a wide range of text-based tasks by framing them all as text-to-text problems. This means that irrespective of the task, whether it’s summarization, translation, question answering, or classification, both the input and output are treated as text. This unified approach allows T5 to perform various tasks without needing task-specific modifications, making it highly adaptable for different <span class="No-Break">use cases.</span></p>
			<p>When it comes to data augmentation, T5 plays a key role by generating variations of existing text data, which is essential for expanding and diversifying datasets. Data augmentation is especially valuable when training machine learning models, as it helps them generalize better by exposing them to a wider variety of examples, reducing overfitting and improving robustness. Here’s how T5 aids in <span class="No-Break">data augmentation:</span></p>
			<ul>
				<li><strong class="bold">Paraphrasing</strong>: T5 can rephrase sentences while maintaining their original meaning. For example, if the input is “The movie was boring,” T5 could generate a paraphrased version such as “The film was dull.” This variety in expression provides additional examples for a model to learn from, helping it generalize better to different ways of phrasing the <span class="No-Break">same idea.</span></li>
				<li><strong class="bold">Synonym replacement</strong>: T5 can replace words with their synonyms, creating slight variations in meaning while retaining the overall sentiment or context. For instance, from “The movie was long and tedious,” T5 might generate “The film was lengthy and boring.” This simple modification increases the diversity of the dataset, offering more training examples for models that rely on understanding slight variations <span class="No-Break">in language.</span></li>
				<li><strong class="bold">Sentiment-based transformation</strong>: T5 can also transform the sentiment of a sentence. For example, given a negative sentence such as “The movie was very disappointing,” T5 can generate a neutral or positive version, such as “The movie had a slow start but improved later.” This capability allows for the creation of multiple examples across different sentiment categories, which is particularly useful in tasks such as sentiment analysis, where a model needs to distinguish between positive, neutral, and <span class="No-Break">negative sentiments.</span></li>
				<li><strong class="bold">Text expansion</strong>: T5 can take a short sentence and expand it by adding more context, details, or descriptions. For instance, from the sentence “The event was great,” T5 could generate a more detailed version such as “The event was great, with excellent speakers and engaging discussions.” By adding more context, T5 provides additional variations of the sentence that help in training models to handle more <span class="No-Break">complex inputs.</span></li>
			</ul>
			<p>We can use a pre-trained <a id="_idIndexMarker116"/>T5 model to generate variations of input text. This method is particularly powerful as it can produce more diverse and contextually rich augmentations. Let’s <span class="No-Break">see this:</span></p>
			<pre class="source-code">
def t5_augmentation(text, model, tokenizer, num_return_sequences=1):
    input_ids = tokenizer.encode(
        f"paraphrase: {text}",
        return_tensors="pt",
        max_length=512,
        truncation=True
    )
    outputs = model.generate(
        input_ids=input_ids,
        max_length=150,
        num_return_sequences=num_return_sequences,
        num_beams=5,
        no_repeat_ngram_size=2,
        top_k=50,
        top_p=0.95,
    )
    return [
        tokenizer.decode(
            output, skip_special_tokens=True
        ) for output in outputs
    ]</pre>			<p>This function takes a text input, a pre-trained T5 model, its tokenizer, and the number of paraphrases to generate (the default is 1). The default of 1 return sequence is chosen for simplicity, but you can request multiple paraphrases by increasing <span class="No-Break">this value.</span></p>
			<p>The function encodes<a id="_idIndexMarker117"/> the input text with a <strong class="source-inline">"paraphrase:"</strong> prefix, limiting it to <strong class="source-inline">512</strong> tokens. It then uses the model to generate paraphrases with a maximum length of <strong class="source-inline">150</strong> tokens. The generation process uses beam search with 5 beams, prevents repetition of 2-grams, and applies <strong class="bold">top-k</strong> (<strong class="source-inline">50</strong>) and <strong class="bold">top-p</strong> (<strong class="source-inline">0.95</strong>) <strong class="bold">sampling</strong> for diversity. The numerical parameters (<strong class="source-inline">512</strong>, <strong class="source-inline">150</strong>, <strong class="source-inline">5</strong>, <strong class="source-inline">2</strong>, <strong class="source-inline">50</strong>, <strong class="source-inline">0.95</strong>) can also be adjusted based on specific use cases to control the length, diversity, and quality of the <span class="No-Break">generated paraphrases.</span></p>
			<p>The function decodes and returns the generated paraphrases, skipping any special tokens added during <span class="No-Break">the process.</span></p>
			<p>Using temperature control as an additional parameter in language generation systems allows fine-tuning the balance between creativity and coherence. Temperature is a scalar value, typically ranging from 0 to 1, that influences the probability distribution over the next token during generation. Low values (close to 0) concentrate the distribution, making the model more deterministic and coherent but potentially repetitive or <a id="_idIndexMarker118"/>conservative. High values (close to 1) flatten the distribution, increasing randomness and diversity at the cost <span class="No-Break">of coherence.</span></p>
			<h1 id="_idParaDest-47"><a id="_idTextAnchor055"/>Leveraging existing LLMs for data generation</h1>
			<p>One of the most powerful <a id="_idIndexMarker119"/>approaches to data augmentation for LLMs is to use existing models to generate new training examples. This technique, often referred to as <strong class="bold">self-supervised learning</strong> or <strong class="bold">model-based data augmentation</strong>, allows<a id="_idIndexMarker120"/> us to create vast amounts<a id="_idIndexMarker121"/> of diverse, high-quality <span class="No-Break">training data.</span></p>
			<p>We’ll explore <a id="_idIndexMarker122"/>how to <a id="_idIndexMarker123"/>use <strong class="bold">GPT-4o</strong> and the <strong class="bold">OpenAI API</strong> for <span class="No-Break">data generation:</span></p>
			<pre class="source-code">
def gpt4o_data_generation(prompt, num_samples=5):
    response = openai.ChatCompletion.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=150,
        n=num_samples,
        temperature=0.7,
    )
    return [choice.message.content.strip() 
        for choice in response.choices
    ]</pre>			<p>This function sends a single user message containing the provided prompt for a chat completion request. It limits the response to a maximum of <strong class="source-inline">150</strong> tokens, which balances between getting a substantive response and controlling the output length. The <strong class="source-inline">n</strong> parameter, set to <strong class="source-inline">num_samples</strong>, determines the number of alternative completions to generate. A temperature of <strong class="source-inline">0.7</strong> is used, which provides a balance between creativity and coherence in the generated text: high values increase randomness, while low values would make the output more deterministic. The function then extracts and returns the content of each generated completion, stripping any leading or trailing whitespace. These parameters (<strong class="source-inline">150</strong> tokens, <strong class="source-inline">0.7</strong> temperature) can be adjusted based on specific needs for output length <span class="No-Break">and creativity.</span></p>
			<p>When using this approach, we need to consider <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Prompt engineering</strong>: Crafting effective prompts is needed for generating relevant and <span class="No-Break">diverse samples.</span></li>
				<li><strong class="bold">Quality control</strong>: Implement filtering mechanisms to ensure the generated data meets your <span class="No-Break">quality standards.</span></li>
				<li><strong class="bold">Diversity</strong>: Use temperature and top-p sampling to control the randomness and diversity of <span class="No-Break">generated samples.</span></li>
			</ul>
			<p>We’ve explored data<a id="_idIndexMarker124"/> augmentation techniques using GPT-4o and examined essential considerations. Now, let’s turn our attention to strategies for multilingual <span class="No-Break">data augmentation.</span></p>
			<h1 id="_idParaDest-48"><a id="_idTextAnchor056"/>Multilingual data augmentation strategies</h1>
			<p>For LLMs designed to<a id="_idIndexMarker125"/> handle multiple languages, multilingual data augmentation is essential. We can adapt our previous techniques to work <span class="No-Break">across languages.</span></p>
			<h2 id="_idParaDest-49"><a id="_idTextAnchor057"/>Cross-lingual back-translation</h2>
			<p>Translate the<a id="_idIndexMarker126"/> text <a id="_idIndexMarker127"/>into multiple languages before translating it back to the <span class="No-Break">original language:</span></p>
			<pre class="source-code">
def cross_lingual_back_translation(text, 
    target_langs=['fr', 'de', 'es']
):
    translator = Translator()
    augmented_texts = []
    for lang in target_langs:
        translated = translator.translate(text, dest=lang)
        back_translated = translator.translate(
            translated.text, dest='en'
        )
        augmented_texts.append(back_translated.text)
    return augmented_texts</pre>			<p>The <strong class="source-inline">cross_lingual_back_translation</strong> function takes a text input and generates augmented versions of it by first translating it into multiple target languages (defaulting to French, German, and Spanish) and then back to English. The function uses the <strong class="source-inline">Translator</strong> object to perform these translations, storing each back-translated version in a list, which <a id="_idIndexMarker128"/>is returned as <span class="No-Break">the output.</span></p>
			<h2 id="_idParaDest-50"><a id="_idTextAnchor058"/>Multilingual T5 augmentation</h2>
			<p>You can use a<a id="_idIndexMarker129"/> multilingual <a id="_idIndexMarker130"/>T5 model to generate paraphrases in <span class="No-Break">different languages:</span></p>
			<pre class="source-code">
def multilingual_t5_augmentation(
    text, model, tokenizer, target_langs=['fr', 'de', 'es']
):
    augmented_texts = []
    for lang in target_langs:
        input_ids = tokenizer.encode(
            f"translate English to {lang}: {text}",
            return_tensors="pt", max_length=512,
            truncation=True
        )
        outputs = model.generate(input_ids=input_ids, max_length=150)
        translated = tokenizer.decode(outputs[0],
            skip_special_tokens=True)
        augmented_texts.append(translated)
    return augmented_texts</pre>			<p>The <strong class="source-inline">multilingual_t5_augmentation</strong> function uses a T5 model to augment a given text by translating it into multiple target languages (defaulting to French, German, and Spanish). For each target language, it encodes the text with a prompt for translation, generates <a id="_idIndexMarker131"/>the translated<a id="_idIndexMarker132"/><a id="_idIndexMarker133"/><a id="_idIndexMarker134"/> output using the model, and decodes the result. The translated texts are collected in a list and returned as the augmented versions of the <span class="No-Break">original text.</span></p>
			<h1 id="_idParaDest-51"><a id="_idTextAnchor059"/>Semantic preservation in text augmentation</h1>
			<p>Maintaining semantic <a id="_idIndexMarker135"/>integrity is crucial when augmenting data for LLMs. We must ensure that our techniques don’t alter the original meaning of <span class="No-Break">the text.</span></p>
			<h2 id="_idParaDest-52"><a id="_idTextAnchor060"/>Use of sentence embeddings</h2>
			<p>By comparing<a id="_idIndexMarker136"/> the <strong class="bold">sentence embeddings</strong> of the<a id="_idIndexMarker137"/> original and augmented texts, you can ensure <span class="No-Break"><strong class="bold">semantic similarity</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
def semantic_similarity(original, augmented, model):
    original_embedding = model.encode(original)
    augmented_embedding = model.encode(augmented)
    similarity = cosine_similarity(
        [original_embedding], [augmented_embedding]
    )[0][0]
    return similarity
def filter_by_semantic_similarity(
    original, augmented_list, model, threshold=0.8
):
    return [
        aug for aug in augmented_list
        if semantic_similarity(original, aug, model) &gt;= threshold
    ]</pre>			<p>We define two functions for measuring and filtering text based on <span class="No-Break">semantic similarity:</span></p>
			<ul>
				<li><strong class="source-inline">semantic_similarity(original, augmented, model)</strong> calculates the semantic similarity between two texts using the cosine similarity of their embeddings. It uses a provided model (probably a sentence embedding model) to encode the original and augmented texts into vector representations. The cosine similarity between these vectors is then computed, resulting in a value between -1 and 1, where 1 indicates <span class="No-Break">perfect similarity.</span></li>
				<li><strong class="source-inline">filter_by_semantic_similarity(original, augmented_list, model, threshold=0.8)</strong> filters a list of augmented texts based on their semantic similarity to the original. The <strong class="source-inline">semantic_similarity</strong> function compares each augmented text with the original. The default threshold is set to <strong class="source-inline">0.8</strong>: by default, it will keep only the augmented texts that have a similarity of <strong class="source-inline">0.8</strong> or higher to the original. This threshold is commonly used in NLP tasks, as it typically indicates a strong semantic similarity while allowing some variation. It can be adjusted based on how strict or lenient you want the filtering to be: a higher threshold will result in more similar (but possibly fewer) augmentations; a lower threshold will allow more diverse (but <a id="_idIndexMarker138"/>potentially less <a id="_idIndexMarker139"/><span class="No-Break">relevant) augmentations.</span></li>
			</ul>
			<h2 id="_idParaDest-53"><a id="_idTextAnchor061"/>Contextual word embeddings for synonym replacement</h2>
			<p>You can<a id="_idIndexMarker140"/> use <strong class="bold">contextual word embeddings</strong> to find more appropriate synonyms based on the context. Contextual word embeddings refer to the use of word representations generated by language models that capture the meaning of a word within its specific sentence or passage, rather than treating the word as having a fixed meaning. Unlike traditional static embeddings where a word has the same vector regardless of context, contextual embeddings assign different vectors to the same word depending on its surrounding words. This allows for more accurate synonym replacement as the chosen synonym aligns not only with the dictionary meaning but also with how the word is used in a particular context. For example, the word “bank” in “river bank” versus “savings bank” would be represented differently, leading to contextually appropriate synonym suggestions such as “shore” or “financial institution,” respectively. The following code snippet shows how <span class="No-Break">it works:</span></p>
			<pre class="source-code">
def contextual_synonym_replacement(text, model, tokenizer, n=1):
    words = text.split()
    new_words = words.copy()
    for i in range(n):
        word_index = random.randint(0, len(words) - 1)
        original_word = words[word_index]
        inputs = tokenizer(text, return_tensors="pt")
        with torch.no_grad():
            outputs = model(inputs)
        word_embedding = outputs.last_hidden_state[0, word_index]
        similar_words = find_similar_words(
            word_embedding, model, tokenizer
        )
        if similar_words:
            new_words[word_index] = random.choice(similar_words)
    return ' '.join(new_words)</pre>			<p>This function performs <a id="_idIndexMarker141"/>context-aware word replacement using a <span class="No-Break">language model:</span></p>
			<ol>
				<li>It takes a text input, a pre-trained language model, its tokenizer, and the number of words to replace (the default <span class="No-Break">is 1).</span></li>
				<li>The text is split into words, and a copy is made <span class="No-Break">for modification.</span></li>
				<li>The function iterates <strong class="source-inline">n</strong> times (the default is 1). It does the following <span class="No-Break">each time:</span><ol><li class="upper-roman">Randomly selects a <span class="No-Break">word index</span></li><li class="upper-roman">Tokenizes the <span class="No-Break">entire text</span></li><li class="upper-roman">Runs it through the model to get <span class="No-Break">contextualized embeddings</span></li><li class="upper-roman">Extracts the embedding of the <span class="No-Break">chosen word</span></li><li class="upper-roman">Finds similar words based on this embedding (using an undefined <span class="No-Break"><strong class="source-inline">find_similar_words</strong></span><span class="No-Break"> function)</span></li><li class="upper-roman">If similar words are found, it randomly chooses one to replace <span class="No-Break">the original</span></li></ol></li>
				<li>Finally, it joins the modified words back into a string and <span class="No-Break">returns it.</span></li>
			</ol>
			<p>The default <strong class="source-inline">n</strong>=1 is chosen to make minimal changes while still introducing variation. This preserves most of the original meaning and structure. You can increase <strong class="source-inline">n</strong> to get more replacements, but higher values might alter the text’s meaning <span class="No-Break">more significantly.</span></p>
			<p>This method is more context-aware than simple synonym replacement as it considers the word’s usage in<a id="_idIndexMarker142"/> the full text when finding replacements. The exact behavior will depend on the model and tokenizer used, as well as the implementation of the <span class="No-Break"><strong class="source-inline">find_similar_words</strong></span><span class="No-Break"> function.</span></p>
			<h1 id="_idParaDest-54"><a id="_idTextAnchor062"/>Balancing augmentation and data quality</h1>
			<p>While data augmentation can significantly improve LLM performance, we need to strike a balance between quantity <span class="No-Break">and quality.</span></p>
			<p>You should limit the proportion of augmented data in your training set. A common practice is to start with a 1:1 ratio of original to augmented data and adjust based on <span class="No-Break">model performance.</span></p>
			<h2 id="_idParaDest-55"><a id="_idTextAnchor063"/>Quality filtering</h2>
			<p>You can implement quality checks to filter out low-quality <span class="No-Break">augmented samples:</span></p>
			<pre class="source-code">
def quality_filter(
    augmented_texts, original_text,
    similarity_threshold=0.8, perplexity_threshold=100
):
    filtered_texts = []
    for aug_text in augmented_texts:
        if (
            semantic_similarity(
                original_text, aug_text, similarity_model
            ) &gt;= similarity_threshold and
            calculate_perplexity(
                aug_text, perplexity_model
            ) &lt;= perplexity_threshold
        ):
            filtered_texts.append(aug_text)
    return filtered_texts</pre>			<h2 id="_idParaDest-56"><a id="_idTextAnchor064"/>Human-in-the-loop validation</h2>
			<p>For critical applications, incorporate human validation into your <span class="No-Break">augmentation pipeline.</span></p>
			<p><strong class="bold">Human-in-the-loop</strong> (<strong class="bold">HITL</strong>) validation is a control mechanism used in AI pipelines where humans are deliberately inserted into automated workflows to ensure correctness, especially in tasks involving subjective judgment, sensitive content, or critical decision-making. This is particularly important in applications where data quality directly affects safety, fairness, or compliance—for example, healthcare diagnostics, legal document analysis, or autonomous systems. In the context of data augmentation, where the goal is to expand the training dataset by generating variations of existing samples, HITL is used to validate whether the generated samples are coherent, accurate, and aligned with the intended label <span class="No-Break">or task:</span></p>
			<pre class="source-code">
def human_validation(augmented_texts):
    validated_texts = []
    for text in augmented_texts:
        if input(
            f"Is this text valid? (y/n)\n{text}\n"
        ).lower() == 'y':
            validated_texts.append(text)
    return validated_texts</pre>			<p>This function is designed to manually validate a list of augmented text samples by soliciting binary feedback—yes or no—from a human operator. Its presence within an augmentation pipeline acknowledges that not all automatically generated data can be trusted at face value. The decision to retain or discard a given sample is made interactively, reinforcing human oversight in tasks where semantic integrity <span class="No-Break">is non-negotiable.</span></p>
			<p>Each iteration of the function’s loop represents a decision point. The human validator is shown the generated text and asked to assess whether it meets the expected criteria. These criteria are typically based on task-specific requirements such as grammaticality, semantic equivalence to the original data, tone appropriateness, or domain alignment. For example, in a medical text classification task, a paraphrased sentence must preserve all critical clinical entities. A slight shift in terminology introduced by an augmentation technique could mislead the model if not caught during validation. This is where human evaluation <span class="No-Break">becomes indispensable.</span></p>
			<p>The logic behind converting the input to lowercase is to handle inconsistent user input. Whether the user types <strong class="source-inline">Y</strong>, <strong class="source-inline">y</strong>, or any other casing, the comparison becomes case-agnostic. Only if the input is equivalent to <strong class="source-inline">y</strong> does the function accept the sample. This binary check is deliberately strict to prevent ambiguous approvals. The rejected samples are silently discarded and not logged or returned, implying that any further inspection or correction of rejected samples would need to be <span class="No-Break">implemented separately.</span></p>
			<p>The function concludes by returning a list of only those samples that have been explicitly validated. This output can then be used to expand the training dataset with higher confidence in the integrity of the new data points. Importantly, this approach does not replace automated quality checks but supplements them in high-stakes applications. HITL validation is particularly useful when deploying models in environments where false positives or negatives carry high costs, such as legal recommendation systems, fraud detection, or autonomous navigation. The manual validation process helps to mitigate risks that stem from over-reliance on generative augmentation methods that lack explicit <span class="No-Break">semantic guarantees.</span></p>
			<p>In a larger system, this kind of function would usually be embedded in a broader workflow where automated filters screen out obviously low-quality or irrelevant augmentations first. The human validator would only evaluate the borderline or high-impact cases. For operational efficiency, the interaction would typically be handled via a web interface or integrated annotation tool rather than a command-line prompt. However, the function demonstrates the principle in its simplest form: human judgment is used as the final arbiter of quality before incorporating augmented data into <span class="No-Break">model training.</span></p>
			<h1 id="_idParaDest-57"><a id="_idTextAnchor065"/>Evaluating the impact of data augmentation</h1>
			<p>To assess the effectiveness of our data augmentation techniques, we need to assess their impact on <span class="No-Break">LLM performance.</span></p>
			<h2 id="_idParaDest-58"><a id="_idTextAnchor066"/>Perplexity</h2>
			<p>You can measure a model’s perplexity (see <a href="B31249_02.xhtml#_idTextAnchor035"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>) on a held-out test set before and after data augmentation to assess whether it has improved the model’s ability to predict <span class="No-Break">unseen text:</span></p>
			<pre class="source-code">
def evaluate_perplexity(model, tokenizer, test_data):
    model.eval()
    total_loss = 0
    total_tokens = 0
    with torch.no_grad():
        for text in test_data:
            inputs = tokenizer(
                text, return_tensors="pt"
            ).to(model.device)
            outputs = model(inputs, labels=inputs["input_ids"])
            total_loss += (
                outputs.loss.item() * inputs["input_ids"].size(1)
            )
            total_tokens += inputs["input_ids"].size(1)
    perplexity = math.exp(total_loss / total_tokens)
    return perplexity</pre>			<p>This function, <strong class="source-inline">evaluate_perplexity</strong>, calculates the perplexity of a language model on a given test dataset. Here’s <span class="No-Break">a breakdown:</span></p>
			<ol>
				<li>It takes a pre-trained language model, its tokenizer, and a test dataset <span class="No-Break">as input.</span></li>
				<li>The model is set to evaluation mode to disable dropout and other <span class="No-Break">training-specific behavior.</span></li>
				<li>It initializes variables to track the total loss and total number of <span class="No-Break">tokens processed.</span></li>
				<li>For each text in the test data, the following is <span class="No-Break">carried out:</span><ol><li class="upper-roman">The text is tokenized and converted <span class="No-Break">to tensors.</span></li><li class="upper-roman">The model processes the input, calculating <span class="No-Break">the loss.</span></li><li class="upper-roman">The loss is accumulated, weighted by the number of tokens in <span class="No-Break">the input.</span></li></ol></li>
				<li>After processing all texts, it calculates the perplexity using the following formula: <strong class="source-inline">exp(total_loss / </strong><span class="No-Break"><strong class="source-inline">total_tokens)</strong></span><span class="No-Break">.</span></li>
			</ol>
			<p>This implementation uses the model in a zero-shot manner, treating each input as both the context and the target to predict. The use of <strong class="source-inline">torch.no_grad()</strong> ensures that no gradients are computed, making the evaluation <span class="No-Break">more efficient.</span></p>
			<p>This function assumes the model and data are compatible (i.e., the model can handle the maximum sequence length of the data). In practice, you might need to add checks or truncation to handle very <span class="No-Break">long sequences.</span></p>
			<h2 id="_idParaDest-59"><a id="_idTextAnchor067"/>Task-specific metrics</h2>
			<p>You can evaluate the model on downstream tasks relevant to your use case, such as text classification or <span class="No-Break">question answering:</span></p>
			<pre class="source-code">
def evaluate_classification(
    model, tokenizer, test_data, test_labels
):
    model.eval()
    predictions = []
    with torch.no_grad():
        for text in test_data:
            inputs = tokenizer(
                text, return_tensors="pt"
            ).to(model.device)
            outputs = model(inputs)
            predictions.append(torch.argmax(outputs.logits).item())
    accuracy = accuracy_score(test_labels, predictions)
    f1 = f1_score(test_labels, predictions, average='weighted')
    return accuracy, f1</pre>			<p>This function assesses the performance of a classification model on a <span class="No-Break">test dataset:</span></p>
			<ol>
				<li>It takes a pre-trained classification model, its tokenizer, test data (text), and corresponding test labels <span class="No-Break">as inputs.</span></li>
				<li>The model is set to evaluation mode to disable dropout and other <span class="No-Break">training-specific behavior.</span></li>
				<li>It processes each text in the test data, tokenizing it and using the model to <span class="No-Break">make predictions.</span></li>
				<li>After processing all texts, it calculates two <span class="No-Break">evaluation metrics:</span><ul><li><strong class="bold">Accuracy</strong>: The proportion of correct predictions out of all <span class="No-Break">predictions made.</span></li><li><strong class="bold">F1 score</strong>: A balanced measure of the model’s precision and recall. The F1 score is the harmonic mean of <strong class="bold">precision</strong> (the ratio of true positive predictions to all positive predictions) and <strong class="bold">recall</strong> (the ratio of true positive predictions to all actual <span class="No-Break">positive instances).</span></li></ul><p class="list-inset">The F1 score formula is <em class="italic">F1 = 2 * (precision * recall) / (precision + </em><span class="No-Break"><em class="italic">recall)</em></span><span class="No-Break">.</span></p><p class="list-inset">The F1 score ranges from 0 to 1, where 1 indicates perfect precision and recall. It’s particularly useful for imbalanced datasets where accuracy alone might be misleading. The weighted average calculates F1 for each class and averages them, weighted by the number of instances in <span class="No-Break">each class.</span></p></li>
				<li>The function returns both the accuracy and F1 score, providing a more comprehensive evaluation of the model’s performance across potentially <span class="No-Break">imbalanced classes.</span></li>
			</ol>
			<p>This implementation also uses <strong class="source-inline">torch.no_grad()</strong> for efficiency and assumes that the necessary scikit-learn metrics are imported. In practice, you might want to add error handling for unexpected model outputs or mismatched <span class="No-Break">prediction/label counts.</span></p>
			<h2 id="_idParaDest-60"><a id="_idTextAnchor068"/>Diversity metrics</h2>
			<p>It’s important to assess the diversity of your <span class="No-Break">augmented dataset:</span></p>
			<pre class="source-code">
def calculate_diversity_metrics(texts):
    all_words = [word for text in texts for word in text.split()]
    vocab_size = len(set(all_words))
    all_trigrams = [text[i:i+3] for text in texts 
        for i in range(len(text)-2)]
    unique_trigrams = len(set(all_trigrams))
    return {
        "vocabulary_size": vocab_size,
        "unique_trigrams": unique_trigrams
    }</pre>			<p>This function takes a collection of texts as input and computes <strong class="bold">diversity metrics</strong>. Once this has been done, this function returns a dictionary with these <span class="No-Break">two metrics:</span></p>
			<ul>
				<li><strong class="bold">Vocabulary size</strong> (ranging from 1 to the total number of words): This gives an idea of lexical diversity. A high number suggests diverse word usage across the texts. This metric splits each text into words, combines all words from all texts, and then counts the number of unique words using a <strong class="bold">set</strong>. In this context, a set refers to a data structure that automatically removes <span class="No-Break">duplicate elements.</span></li>
				<li><strong class="bold">Unique trigrams</strong> (ranging from 1 to the total number of trigrams): These indicate character-level diversity. A high number suggests varied character sequences, potentially indicating diverse sentence structures or word choices. This metric creates trigrams (sequences of three characters) from each text and counts the number of unique trigrams using a set that only contains <span class="No-Break">unique elements.</span></li>
			</ul>
			<p>These metrics are useful for comparing the diversity of original texts versus augmented texts or for assessing the variety in a dataset. However, the results should be interpreted in context, as high diversity might indicate incoherence or noise in <span class="No-Break">the data.</span></p>
			<p>By systematically applying these techniques, we can quantify the impact of our data augmentation strategies on LLM performance and make informed decisions about which techniques to use and how<a id="_idTextAnchor069"/> to fine<a id="_idTextAnchor070"/>-tune our <span class="No-Break">augmentation pipeline.</span></p>
			<h1 id="_idParaDest-61"><a id="_idTextAnchor071"/>Summary</h1>
			<p>In this chapter, we explored advanced data augmentation techniques for LLMs, covering text manipulation methods, leveraging existing models for data generation, multilingual strategies, semantic preservation, quality control, and several metrics. We also discussed the importance of balancing augmentation with data quality and provided practical Python implementations for <span class="No-Break">various techniques.</span></p>
			<p>In the next chapter, we’ll focus on handling large datasets for <span class="No-Break">LLM training.</span></p>
		</div>
	</div></div></body></html>