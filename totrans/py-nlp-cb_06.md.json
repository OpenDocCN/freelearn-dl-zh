["```py\n    import pandas as pd\n    import nltk\n    import re\n    from nltk.corpus import stopwords\n    from gensim.utils import simple_preprocess\n    from gensim.models.ldamodel import LdaModel\n    import gensim.corpora as corpora\n    from pprint import pprint\n    from gensim.corpora import MmCorpus\n    ```", "```py\n    stop_words = stopwords.words('english')\n    stop_words.append(\"said\")\n    bbc_df = pd.read_csv(\"../data/bbc-text.csv\")\n    print(bbc_df)\n    ```", "```py\n    def clean_text(input_string):\n        input_string = re.sub(r'[^\\w\\s]', ' ', input_string)\n        input_string = re.sub(r'\\d', '', input_string)\n        input_list = simple_preprocess(input_string)\n        input_list = [word for word in input_list if word not in \n            stop_words]\n        return input_list\n    ```", "```py\n    bbc_df['text'] = bbc_df['text'].apply(lambda x: clean_text(x))\n    print(bbc_df)\n    ```", "```py\ntexts = bbc_df['text'].values\nid_dict = corpora.Dictionary(texts)\ncorpus = [id_dict.doc2bow(text) for text in texts]\n```", "```py\n    num_topics = 5\n    lda_model = LdaModel(corpus=corpus,\n                         id2word=id_dict,\n                         num_topics=num_topics,\n                         chunksize=100,\n                         passes=20)\n    pprint(lda_model.print_topics())\n    ```", "```py\n    def save_model(lda, lda_path, id_dict, dict_path, \n        corpus, corpus_path):\n        lda.save(lda_path)\n        id_dict.save(dict_path)\n        MmCorpus.serialize(corpus_path, corpus)\n    ```", "```py\n    model_path = \"../models/bbc_gensim/lda.model\"\n    dict_path = \"../models/bbc_gensim/id2word.dict\"\n    corpus_path = \"../models/bbc_gensim/corpus.mm\"\n    save_model(lda_model, model_path, id_dict, dict_path, \n        corpus, corpus_path)\n    ```", "```py\n    lda_model = LdaModel.load(model_path)\n    id_dict = corpora.Dictionary.load(dict_path)\n    ```", "```py\n    new_example = \"\"\"Manchester United players slumped to the turf\n    at full-time in Germany on Tuesday in acknowledgement of what their\n    latest pedestrian first-half display had cost them. The 3-2 loss at\n    RB Leipzig means United will not be one of the 16 teams in the draw\n    for the knockout stages of the Champions League. And this is not the\n    only price for failure. The damage will be felt in the accounts, in\n    the dealings they have with current and potentially future players\n    and in the faith the fans have placed in manager Ole Gunnar Solskjaer.\n    With Paul Pogba's agent angling for a move for his client and ex-United\n    defender Phil Neville speaking of a \"witchhunt\" against his former team-mate\n    Solskjaer, BBC Sport looks at the ramifications and reaction to a big loss for United.\"\"\"\n    ```", "```py\n    input_list = clean_text(new_example)\n    bow = id_dict.doc2bow(input_list)\n    topics = lda_model[bow]\n    print(topics)\n    ```", "```py\n    [(1, 0.7338447), (2, 0.15261793), (4, 0.1073401)]\n    ```", "```py\n    import pandas as pd\n    import nltk\n    import re\n    from nltk.corpus import stopwords\n    from sentence_transformers import SentenceTransformer, util\n    ```", "```py\n    %run -i \"../util/lang_utils.ipynb\"\n    ```", "```py\n    bbc_df = pd.read_csv(\"../data/bbc-text.csv\")\n    print(bbc_df)\n    ```", "```py\n    model = SentenceTransformer('all-MiniLM-L6-v2')\n    embeddings = model.encode(bbc_df[\"text\"], convert_to_tensor=True)\n    ```", "```py\n    clusters = util.community_detection(\n        embeddings, threshold=0.7, min_community_size=10)\n    print(clusters)\n    ```", "```py\n    [[117, 168, 192, 493, 516, 530, 638, 827, 883, 1082, 1154, 1208, 1257, 1359, 1553, 1594, 1650, 1898, 1938, 2059, 2152], [76, 178, 290, 337, 497, 518, 755, 923, 1057, 1105, 1151, 1172, 1242, 1560, 1810, 1813, 1882, 1942, 1981], [150, 281, 376, 503, 758, 900, 1156, 1405, 1633, 1636, 1645, 1940, 1946, 1971], [389, 399, 565, 791, 1014, 1018, 1259, 1288, 1440, 1588, 1824, 1917, 2024], [373, 901, 1004, 1037, 1041, 1323, 1499, 1534, 1580, 1621, 1751, 2178], [42, 959, 1063, 1244, 1292, 1304, 1597, 1915, 2081, 2104, 2128], [186, 193, 767, 787, 1171, 1284, 1625, 1651, 1797, 2148], [134, 388, 682, 1069, 1476, 1680, 2106, 2129, 2186, 2198]]\n    ```", "```py\n    def print_words_by_cluster(clusters, input_df):\n        for i, cluster in enumerate(clusters):\n            print(f\"\\nCluster {i+1}, {len(cluster)} elements \")\n            sentences = input_df.iloc[cluster][\"text\"]\n            all_text = \" \".join(sentences)\n            freq_words = get_most_frequent_words(all_text)\n            print(freq_words)\n    ```", "```py\n    Cluster 1, 21 elements\n    ['mr', 'labour', 'brown', 'said', 'blair', 'election', 'minister', 'prime', 'chancellor', 'would', 'party', 'new', 'campaign', 'told', 'government', ...]\n    Cluster 2, 19 elements\n    ['yukos', 'us', 'said', 'russian', 'oil', 'gazprom', 'court', 'rosneft', 'russia', 'yugansk', 'company', 'bankruptcy', 'auction', 'firm', 'unit', ...]\n    Cluster 3, 14 elements\n    ['kenteris', 'greek', 'thanou', 'iaaf', 'said', 'athens', 'tests', 'drugs', 'olympics', 'charges', 'also', 'decision', 'test', 'athletics', 'missing', ...]\n    Cluster 4, 13 elements\n    ['mr', 'tax', 'howard', 'labour', 'would', 'said', 'tory', 'election', 'government', 'taxes', 'blair', 'spending', 'tories', 'party', 'cuts',...]\n    Cluster 5, 12 elements\n    ['best', 'film', 'aviator', 'director', 'actor', 'foxx', 'swank', 'actress', 'baby', 'million', 'dollar', 'said', 'win', 'eastwood', 'jamie',...]\n    Cluster 6, 11 elements\n    ['said', 'prices', 'market', 'house', 'uk', 'figures', 'mortgage', 'housing', 'year', 'lending', 'november', 'price', 'december', 'rise', 'rose', ...]\n    Cluster 7, 10 elements\n    ['lse', 'deutsche', 'boerse', 'bid', 'euronext', 'said', 'exchange', 'london', 'offer', 'stock', 'would', 'also', 'shareholders', 'german', 'market',...]\n    Cluster 8, 10 elements\n    ['dollar', 'us', 'euro', 'said', 'currency', 'deficit', 'analysts', 'trading', 'yen', 'record', 'exports', 'economic', 'trade', 'markets', 'european',...]\n    ```", "```py\n    import re\n    import string\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n    from sklearn.cluster import KMeans\n    from nltk.probability import FreqDist\n    from nltk.corpus import stopwords\n    from sentence_transformers import SentenceTransformer\n    ```", "```py\n    %run -i \"../util/lang_utils.ipynb\"\n    ```", "```py\n    bbc_df = pd.read_csv(\"../data/bbc-text.csv\")\n    print(bbc_df)\n    ```", "```py\n    bbc_train, bbc_test = train_test_split(bbc_df, test_size=0.1)\n    print(len(bbc_train))\n    print(len(bbc_test))\n    ```", "```py\n    2002\n    223\n    ```", "```py\n    documents = bbc_train['text'].values\n    model = SentenceTransformer('all-MiniLM-L6-v2')\n    encoded_data = model.encode(documents)\n    km = KMeans(n_clusters=5, n_init='auto', init='k-means++')\n    km.fit(encoded_data)\n    ```", "```py\n    print_most_common_words_by_cluster(documents, km, 5)\n    ```", "```py\n    0\n    ['said', 'people', 'new', 'also', 'mr', 'technology', 'would', 'one', 'mobile', ...]\n    1\n    ['said', 'game', 'england', 'first', 'win', 'world', 'last', 'would', 'one', 'two', 'time',...]\n    2\n    ['said', 'film', 'best', 'music', 'also', 'year', 'us', 'one', 'new', 'awards', 'show',...]\n    3\n    ['said', 'mr', 'would', 'labour', 'government', 'people', 'blair', 'party', 'election', 'also', 'minister', ...]\n    4\n    ['said', 'us', 'year', 'mr', 'would', 'also', 'market', 'company', 'new', 'growth', 'firm', 'economy', ...]\n    ```", "```py\n    bbc_test[\"prediction\"] = bbc_test[\"text\"].apply(\n        lambda x: km.predict(model.encode([x]))[0])\n    print(bbc_test)\n    ```", "```py\n    topic_mapping = {0:\"tech\", 1:\"sport\",\n        2:\"entertainment\", 3:\"politics\", 4:\"business\"}\n    bbc_test[\"pred_category\"] = bbc_test[\"prediction\"].apply(\n        lambda x: topic_mapping[x])\n    print(classification_report(bbc_test[\"category\"],\n        bbc_test[\"pred_category\"]))\n    ```", "```py\n                   precision    recall  f1-score   support\n         business       0.98      0.96      0.97        55\n    entertainment       0.95      1.00      0.97        38\n         politics       0.97      0.93      0.95        42\n            sport       0.98      0.96      0.97        47\n             tech       0.93      0.98      0.95        41\n         accuracy                           0.96       223\n        macro avg       0.96      0.97      0.96       223\n     weighted avg       0.96      0.96      0.96       223\n    ```", "```py\n    new_example = \"\"\"Manchester United players slumped to the turf\n    at full-time in Germany on Tuesday in acknowledgement of what their\n    latest pedestrian first-half display had cost them. The 3-2 loss at\n    RB Leipzig means United will not be one of the 16 teams in the draw\n    for the knockout stages of the Champions League. And this is not the\n    only price for failure. The damage will be felt in the accounts, in\n    the dealings they have with current and potentially future players\n    and in the faith the fans have placed in manager Ole Gunnar Solskjaer.\n    With Paul Pogba's agent angling for a move for his client and ex-United\n    defender Phil Neville speaking of a \"witchhunt\" against his former team-mate\n    Solskjaer, BBC Sport looks at the ramifications and reaction to a big loss for United.\"\"\"\n    ```", "```py\n    predictions = km.predict(model.encode([new_example]))\n    print(predictions[0])\n    ```", "```py\n    1\n    ```", "```py\n    import pandas as pd\n    import numpy as np\n    from bertopic import BERTopic\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import classification_report\n    ```", "```py\n    %run -i \"../util/lang_utils.ipynb\"\n    ```", "```py\n    stop_words = stopwords.words('english')\n    stop_words.append(\"said\")\n    stop_words.append(\"mr\")\n    bbc_df = pd.read_csv(\"../data/bbc-text.csv\")\n    ```", "```py\n    bbc_df[\"text\"] = bbc_df[\"text\"].apply(\n        lambda x: word_tokenize(x))\n    bbc_df[\"text\"] = bbc_df[\"text\"].apply(\n        lambda x: [w for w in x if w not in stop_words])\n    bbc_df[\"text\"] = bbc_df[\"text\"].apply(lambda x: \" \".join(x))\n    ```", "```py\n    bbc_train, bbc_test = train_test_split(bbc_df, test_size=0.1)\n    print(len(bbc_train))\n    print(len(bbc_test))\n    ```", "```py\n    2002\n    223\n    ```", "```py\n    docs = bbc_train[\"text\"].values\n    ```", "```py\n    topic_model = BERTopic(nr_topics=6)\n    topics, probs = topic_model.fit_transform(docs)\n    ```", "```py\n    print(topic_model.get_topic_info())\n    ```", "```py\n       Topic  Count                                 Name  \\\n    0     -1    222             -1_also_company_china_us\n    1      0    463             0_england_game_win_first\n    2      1    393      1_would_labour_government_blair\n    3      2    321             2_film_best_music_awards\n    4      3    309  3_people_mobile_technology_software\n    5      4    294             4_us_year_growth_economy\n                                          Representation  \\\n    0  [also, company, china, us, would, year, new, p...\n    1  [england, game, win, first, club, world, playe...\n    2  [would, labour, government, blair, election, p...\n    3  [film, best, music, awards, show, year, band, ...\n    4  [people, mobile, technology, software, digital...\n    5  [us, year, growth, economy, economic, company,...\n                                     Representative_Docs\n    0  [us retail sales surge december us retail sale...\n    1  [ireland win eclipses refereeing errors intern...\n    2  [lib dems unveil election slogan liberal democ...\n    3  [scissor sisters triumph brits us band scissor...\n    4  [mobiles media players yet mobiles yet ready a...\n    5  [consumer spending lifts us growth us economic...\n    ```", "```py\n    topic_model.generate_topic_labels(\n        nr_words=5, topic_prefix=True, separator='_')\n    ```", "```py\n    ['-1_also_company_china_us_would',\n     '0_england_game_win_first_club',\n     '1_would_labour_government_blair_election',\n     '2_film_best_music_awards_show',\n     '3_people_mobile_technology_software_digital',\n     '4_us_year_growth_economy_economic']\n    ```", "```py\n    def get_prediction(input_text, model):\n        pred = model.transform(input_text)\n        pred = pred[0][0]\n        return pred\n    ```", "```py\n    bbc_test[\"prediction\"] = bbc_test[\"text\"].apply(\n        lambda x: get_prediction(x, topic_model))\n    topic_mapping = {0:\"sport\", 1:\"politics\",\n        2:\"entertainment\", 3:\"tech\", 4:\"business\", -1:\"discard\"}\n    ```", "```py\n    bbc_test[\"pred_category\"] = bbc_test[\"prediction\"].apply(\n        lambda x: topic_mapping[x])\n    test_data = bbc_test.loc[bbc_test['prediction'] != -1]\n    print(classification_report(test_data[\"category\"],\n        test_data[\"pred_category\"]))\n    ```", "```py\n                   precision    recall  f1-score   support\n         business       0.95      0.86      0.90        21\n    entertainment       0.97      1.00      0.98        30\n         politics       0.94      1.00      0.97        46\n            sport       1.00      1.00      1.00        62\n             tech       0.96      0.88      0.92        25\n         accuracy                           0.97       184\n        macro avg       0.96      0.95      0.95       184\n     weighted avg       0.97      0.97      0.97       184\n    ```", "```py\n    new_input = bbc_test[\"text\"].iloc[0]\n    print(new_input)\n    howard dismisses tory tax fears michael howard dismissed fears conservatives plans £4bn tax cuts modest . defended package saying plan tories first budget hoped able go . tories monday highlighted £35bn wasteful spending would stop allow tax cuts reduced borrowing spending key services . ...\n    ```", "```py\n    print(topic_model.transform(new_input))\n    ```", "```py\n    ([1], array([1.]))\n    ```", "```py\n    topics, similarity = topic_model.find_topics(\"sports\", top_n=5)\n    sim_topics = list(zip(topics, similarity))\n    print(sim_topics)\n    ```", "```py\n    [(0, 0.29033981040460977), (3, 0.049293092462828376), (-1, -0.0047265937178774895), (2, -0.02074380026102955), (4, -0.03699168959416969)]\n    ```", "```py\n    topics, similarity = topic_model.find_topics(\n        \"business and economics\",\n        top_n=5)\n    sim_topics = list(zip(topics, similarity))\n    print(sim_topics)\n    ```", "```py\n    [(4, 0.29003573983158404), (-1, 0.26259758927249205), (3, 0.15627005753581313), (1, 0.05491237184012845), (0, 0.010567363445904386)]\n    ```", "```py\n    input_text = \"\"\"YouTube removed a snippet of code that publicly disclosed whether a channel receives ad payouts,\n    obscuring which creators benefit most from the platform.\"\"\"\n    topics, similarity = topic_model.find_topics(\n        input_text, top_n=5)\n    sim_topics = list(zip(topics, similarity))\n    print(sim_topics)\n    ```", "```py\n    [(3, 0.2540850599909866), (-1, 0.172097560474608), (2, 0.1367798346494483), (4, 0.10243553209139492), (1, 0.06954579004136925)]\n    ```", "```py\n    import pandas as pd\n    from nltk.corpus import stopwords\n    from contextualized_topic_models.utils.preprocessing import( \n        WhiteSpacePreprocessingStopwords)\n    from contextualized_topic_models.models.ctm import ZeroShotTM\n    from contextualized_topic_models.utils.data_preparation import( \n        TopicModelDataPreparation)\n    ```", "```py\n    import warnings\n    warnings.filterwarnings('ignore')\n    warnings.filterwarnings(\"ignore\", category = DeprecationWarning)\n    import os\n    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n    ```", "```py\n    stop_words = stopwords.words('english')\n    stop_words.append(\"said\")\n    bbc_df = pd.read_csv(\"../data/bbc-text.csv\")\n    ```", "```py\n    documents = bbc_df[\"text\"]\n    preprocessor = WhiteSpacePreprocessingStopwords(\n        documents, stopwords_list=stop_words)\n    preprocessed_documents,unpreprocessed_documents,vocab,indices =\\\n        preprocessor.preprocess()\n    ```", "```py\n    tp = TopicModelDataPreparation(\n        \"distiluse-base-multilingual-cased\")\n    training_dataset = tp.fit(\n        text_for_contextual=unpreprocessed_documents,\n        text_for_bow=preprocessed_documents)\n    ```", "```py\n    ctm = ZeroShotTM(bow_size=len(tp.vocab),\n        contextual_size=512, n_components=5,\n        num_epochs=100)\n    ctm.fit(training_dataset)\n    ```", "```py\n    ctm.get_topics()\n    ```", "```py\n    spanish_news_piece = \"\"\"IBM anuncia el comienzo de la \"era de la utilidad cuántica\" y anticipa un superordenador en 2033.\n    La compañía asegura haber alcanzado un sistema de computación que no se puede simular con procedimientos clásicos.\"\"\"\n    testing_dataset = tp.transform([spanish_news_piece])\n    ```", "```py\n    ctm.get_doc_topic_distribution(testing_dataset)\n    ```", "```py\n    array([[0.5902461,0.09361929,0.14041995,0.07586181,0.0998529 ]],\n          dtype=float32)\n    ```"]