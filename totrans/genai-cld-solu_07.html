<html><head></head><body>
		<div id="_idContainer092">
			<h1 id="_idParaDest-143" class="chapter-number"><a id="_idTextAnchor143"/>7</h1>
			<h1 id="_idParaDest-144"><a id="_idTextAnchor144"/>Deploying ChatGPT in the Cloud: Architecture Design and Scaling Strategies</h1>
			<p>In the previous chapters, you learned more about how to fine-tune LLMs and add external data. You also gained a deep understanding of how prompts and responses work under the covers. Then, you learned how to develop applications with GenAI while using popular programming frameworks for the various LLMs. As we continue building on our learning of GenAI/ChatGPT for cloud solutions, we will realize that limits are placed on how these cloud services process tokens for prompts and completions. As large-scale deployments need to be “enterprise-ready,” we must take advantage of the cloud to provide the necessary services and support to enable an enterprise solution, with less effort than creating a service from the ground up, on our own. Services, such as security (this topic will be covered in more detail in the next chapter) and identity, are pre-baked into a cloud service, and thus in the cloud solution we are trying to build. However, limits are imposed by a cloud provider and we must understand these limits and design around them for a successful <span class="No-Break">cloud solution.</span></p>
			<p>In this chapter, we’ll focus on understanding that GenAI can be scaled to support many thousands of users, with a large number of concurrent connections, and submitting prompts. This is not only limited to users of GenAI and can also include applications and other LLMs, to name a few. The entire solution, from architecture design, deployment, scaling, performance tuning, monitoring, and logging all combine to make a robust, scalable cloud solution <span class="No-Break">for ChatGPT.</span></p>
			<p>In this chapter, we will cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li><span class="No-Break">Understanding limits</span></li>
				<li>Cloud scaling, design patterns, and <span class="No-Break">error handling</span></li>
				<li>Monitoring, logging, and HTTP <span class="No-Break">response codes</span></li>
				<li>Costs, training <span class="No-Break">and support</span></li>
			</ul>
			<div>
				<div id="_idContainer088" class="IMG---Figure">
					<img src="image/B21443_07_1.jpg" alt="Figure 7.1 – Too many requests and too many tokens"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.1 – Too many requests and too many tokens</p>
			<h1 id="_idParaDest-145"><a id="_idTextAnchor145"/>Understanding limits</h1>
			<p>Any large-scale cloud deployment <a id="_idIndexMarker650"/>needs to be “enterprise-ready,” ensuring both the end user experience is acceptable and the business objectives and requirements are met. “Acceptable” is a loose term that can vary per user and workload. To understand how to scale to meet any user or business requirements, as the appetite for a service increases, we must first understand the basic limits, such as token limits. We covered these limits for most of the common generative AI GPT models in <a href="B21443_05.xhtml#_idTextAnchor098"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, however, we will quickly revisit <span class="No-Break">them here.</span></p>
			<p>As organizations scale up using an enterprise-ready service, such as Azure OpenAI, there are rate limits on how fast tokens are processed in the prompt+completion request. There is a limit to how many text prompts can be sent due to these token limits for each model that can be consumed in a single prompt+completion. It is important to note that the overall size of tokens for rate limiting includes <em class="italic">both</em> the prompt (text sent to the AOAI model) size <em class="italic">plus</em> the return completion (response back from the model) size, and depending on the model, the token limits on the model will vary. That is, the number of maximum token numbers used per a single prompt, will vary depending on the GenAI <span class="No-Break">model used.</span></p>
			<p>You can see your rate limits<a id="_idIndexMarker651"/> on the <a id="_idIndexMarker652"/>Azure OpenAI overview page or OpenAI account page. You can also view important information about your rate limits, such as the remaining requests, tokens, and other metadata in the headers of the HTTP response. Please see the reference link at the end of this chapter for details on what these header <span class="No-Break">fields contain.</span></p>
			<p>Here are a few token limits<a id="_idIndexMarker653"/> for various <span class="No-Break">GPT models:</span></p>
			<table id="table001-5" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Model</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Token Limit</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">GPT-3.5-turbo 0301</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">4,096</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">GPT-3.5-turbo-16k</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">16,385</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">GPT-3.5-turbo-0613</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">4,096</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">GPT-3.5-turbo-16k-0613</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">16,384</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">GPT-4</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">8,192</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">GPT-4-0613</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">32,768</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">GPT-4-32K</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">32,768</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">GPT-4-32-0613</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">32,768</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">GPT-4-Turbo</span></p>
						</td>
						<td class="No-Table-Style">
							<p>128,000 (context) and <span class="No-Break">4,096 (output)</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.2 – Token limits for some GenAI models</p>
			<p>While we already discussed prompt optimization techniques earlier in this book, in this chapter, we will look at some of the other ways to scale an enterprise-ready cloud GenAI service for<a id="_idIndexMarker654"/> applications and services that can easily exceed the token limits for a specific model and <span class="No-Break">scale effectively.</span></p>
			<h1 id="_idParaDest-146"><a id="_idTextAnchor146"/>Cloud scaling and design patterns</h1>
			<p>Since you learned about some of the limits imposed by Azure OpenAI and OpenAI in the previous section, we will now look at how to overcome <span class="No-Break">these limits.</span></p>
			<p>Overcoming these limits through a well-designed architecture or design pattern is critical for businesses to ensure they are meeting any internal <strong class="bold">service-level agreements</strong> (<strong class="bold">SLAs</strong>) and are<a id="_idIndexMarker655"/> providing a robust service without a lot of latency, or delay, in the user or <span class="No-Break">application experience.</span></p>
			<h2 id="_idParaDest-147"><a id="_idTextAnchor147"/>What is scaling?</h2>
			<p>As we described earlier, limits are <a id="_idIndexMarker656"/>imposed on any cloud architecture, just as there are hardware limits on your laptop (amount of RAM or disk space), on-premises data centers, and so on. Resources are finite, so we have come to expect these limits, even in cloud services. However, there are a few techniques we can use to overcome limitations so that we can meet our business requirements or user behavior <span class="No-Break">and appetite.</span></p>
			<h2 id="_idParaDest-148"><a id="_idTextAnchor148"/>Understanding TPM, RPM, and PTUs</h2>
			<p>As <a id="_idIndexMarker657"/>we <a id="_idIndexMarker658"/>scale, we will need to understand some additional terminology, such as <strong class="bold">tokens per minute</strong> (<strong class="bold">TPM</strong>), <strong class="bold">request per minute</strong> (<strong class="bold">RPM</strong>), and <strong class="bold">provisioned throughput units</strong> (<strong class="bold">PTUs</strong>), as well as other <a id="_idIndexMarker659"/>additional services, such as <strong class="bold">Azure API Management</strong> (<strong class="bold">APIM</strong>), which <a id="_idIndexMarker660"/>support a cloud environment <span class="No-Break">in Azure.</span></p>
			<h3>TPMs</h3>
			<p>With a<a id="_idIndexMarker661"/> cloud provider such as Microsoft Azure, Azure OpenAI’s quota management service built into Azure AI Studio enables you to assign quota limits for your deployments, up to whatever amount is the specified limit – that is, your “quota.” You can assign a quota to an Azure subscription on a per-region, per-model basis in units of TPM. The billing component of TPM is also known as pay-as-you-go, where pricing will be based on the pay-as-you-go consumption model, with a price per unit specific for each type of model deployed. Please refer to <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.2</em> for a list of some models and what their token <span class="No-Break">limit is.</span></p>
			<p>When you create an Azure OpenAI service within a subscription, you will receive the default TPM quota size. You can then adjust the TPM to that deployment or any additional deployment you create, at which point the overall <strong class="bold">available</strong> quota for that model will be reduced by that<a id="_idIndexMarker662"/> amount. TPMs/pay-as-you-go are also the default mechanism for billing within the Azure OpenAI (AOAI) service. We will cover some of the costs a bit later, but for more details on AOAI quota management, take a look at the link provided at the end of <span class="No-Break">this chapter.</span></p>
			<p>If you are using OpenAI directly, scaling works very similarly – in OpenAI models, you can scale by adjusting the TPM bar to the “max” under the <span class="No-Break">advanced options.</span></p>
			<p>Now, let’s look at an example and deep dive <span class="No-Break">into TPMs.</span></p>
			<p>In the Microsoft Azure cloud, for example, there is an overall limit (quota) of 240,000 TPMs for GPT-35-Turbo in the Azure East US region. This means you can have a <em class="italic">single deployment of 240K TPM</em> per Azure OpenAI account, <em class="italic">two deployments of 120K TPM each</em>, or any number of deployments in one or multiple deployments, so long as the TPMs add up to 240K (or less) total in the East <span class="No-Break">US region.</span></p>
			<p>So, <em class="italic">one way to scale up is by adding ADDITIONAL (Azure) OpenAI accounts</em>. With additional AOAI accounts, you can stack or add limits together. So, in this example, rather than having a single 240K GPT-35-Turbo limit, we can add an additional 240K times <em class="italic">X</em>, where <em class="italic">X</em> is 30 <span class="No-Break">or less.</span></p>
			<p>The maximum number of<a id="_idIndexMarker663"/> Azure OpenAI accounts (or resources) <em class="italic">per region per Azure subscription</em> is <em class="italic">30</em> (at the time of writing) and is also dependent on regional capacity <em class="italic">availability</em>. We expect this number to be increased over time as additional GPU-based capacity continues to be <span class="No-Break">made available.</span></p>
			<h3>RPM</h3>
			<p>Beyond the<a id="_idIndexMarker664"/> TPM limit, an RPM rate limit is also enforced, where the amount of RPM available to a model is set proportionally to the TPM assignment using a ratio of 6 RPM per <span class="No-Break">1,000 TPM.</span></p>
			<p>RPM is not a direct billing component, but it is a component of rate limits. It is important to note that while the billing for AOAI is token-based (TPM), the actual two triggers in which rate limits occur are <span class="No-Break">as follows:</span></p>
			<p>On a per-second basis, not at the per-minute <span class="No-Break">billing level.</span></p>
			<p>The rate limit will occur at either <strong class="bold">tokens per second</strong> (<strong class="bold">TPS</strong>) or <a id="_idIndexMarker665"/>RPM evaluated over a small period (1-10 seconds). That is, if you exceed the total TPS for a specific model, then a rate limit applies. If you exceed the RPM over a short period, then a rate limit will also apply, returning limit error <span class="No-Break">codes (429).</span></p>
			<p>The throttled rate limits can easily be managed using the scaling special sauce, as well as following some of the best practices described later in <span class="No-Break">this chapter.</span></p>
			<p>You can <a id="_idIndexMarker666"/>read more about quota management and the details on how TPM/RPM rate limits apply in the <em class="italic">Manage Azure OpenAI Service</em> link at the end of <span class="No-Break">this chapter.</span></p>
			<h3>PTUs</h3>
			<p>The Microsoft <a id="_idIndexMarker667"/>Azure cloud recently introduced the ability to use reserved capacity, or PTUs, for AOAI earlier this summer. Beyond the default TPMs described above, this new Azure OpenAI service feature, PTUs, defines the model processing capacity, using reserved resources, for processing prompts and <span class="No-Break">generating completions.</span></p>
			<p><em class="italic">PTUs are another way an enterprise can scale up to meet business requirements as they can provide reserved capacity for your most demanding and complex </em><span class="No-Break"><em class="italic">prompt/completion scenarios.</em></span></p>
			<p>Different types of PTUs are available, where size of these PTUs is available in smaller increments or larger increments of PTU units. For example, the first version of PTUs, which we will call Classic PTUs, and newer PTU offerings, such as “managed” PTUs, size offering differs to accommodate various size workloads in a more <span class="No-Break">predictable fashion.</span></p>
			<p>PTUs are purchased as a monthly commitment with an auto-renewal option, which will <em class="italic">reserve</em> AOAI capacity within an Azure subscription, using a specific model, in a specific Azure region. Let’s say you have 300 PTUs provisioned for GPT 3.5 Turbo. The PTUs are only provisioned for GPT 3.5 Turbo deployments, within a specific Azure subscription, not for GPT 4. You can have separate PTUs for GPT 4, with the minimum PTUs described in the following table , for classic PTUs. There are also managed PTUs, which can vary in <span class="No-Break">min. size.</span></p>
			<p>Keep in mind that while having reserved capacity does <em class="italic">provide consistent latency, predictable performance and throughput</em>, this throughput amount is highly dependent on your scenario – that is, throughput will be affected by a few items, including <em class="italic">the number and ratio of prompts and generation tokens, the number of simultaneous requests, and the type and version of the model used</em>. The following table describes the <em class="italic">approximate</em> TPMs expected concerning PTUs per model. Throughput can vary, so an approximate <a id="_idIndexMarker668"/>range has <span class="No-Break">been provided:</span></p>
			<div>
				<div id="_idContainer089" class="IMG---Figure">
					<img src="image/B21443_07_2.jpg" alt="Figure 7.3 – Approximate throughput range of Classic PTUs"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.3 – Approximate throughput range of Classic PTUs</p>
			<p>As you can scale by creating multiple (Azure) OpenAI accounts, you can <em class="italic">also scale by increasing the number of PTUs</em>. For scaling purposes, you can multiply the minimum number of PTUs required in terms of whatever your application or <span class="No-Break">service requires.</span></p>
			<p>The following table describes this scaling of <span class="No-Break">classic PTUs:</span></p>
			<table id="table002-1" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Model</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Minimum Classic PTUs Required to </strong><span class="No-Break"><strong class="bold">Create Deployment</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Classic PTUs for Incrementally Scaling </strong><span class="No-Break"><strong class="bold">the Deployment</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Example Deployment </strong><span class="No-Break"><strong class="bold">Sizes (PTUs)</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">GPT-3.5-Turbo (4K)</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">300</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">100</span></p>
						</td>
						<td class="No-Table-Style">
							<p>300, <span class="No-Break">400, 500…</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">GPT-3.5-Turbo (16K)</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">600</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">200</span></p>
						</td>
						<td class="No-Table-Style">
							<p>600, <span class="No-Break">800, 1,000...</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">GPT-4 (8K)</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">900</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">300</span></p>
						</td>
						<td class="No-Table-Style">
							<p>900, <span class="No-Break">1,200, 1,500…</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">GPT-4 (32K)</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">1,800</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">600</span></p>
						</td>
						<td class="No-Table-Style">
							<p>1,800, <span class="No-Break">2,400, 3,000…</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.4 – PTU minimums and incremental scaling (classic PTU)</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The PTU size and type are continuously evolving. The two tables above are just to give a sense about the approximate scale of the PTUs with respect to TPMs and how it differs based on model and version. For more updated information, you can visit the <a id="_idTextAnchor149"/>Provisioned Throughput Units (PTU) getting <span class="No-Break">started guide</span><span class="No-Break">.</span></p>
			<p>Now we have <a id="_idIndexMarker669"/>understood the essential components for scaling purposes like TPM, RPM and PTU. Now let’s delve into the scaling strategies and how to circumvent these limits with our special scaling sauce for a large-scale and global <span class="No-Break">enterprise-ready application.</span></p>
			<h2 id="_idParaDest-149"><a id="_idTextAnchor150"/>Scaling Design patterns</h2>
			<p>One<a id="_idIndexMarker670"/> area we haven’t covered yet is how these multiple TPMs or PTU-based Azure OpenAI accounts can work in unison. That is, once you have set up multiple AOAI accounts, how would you send prompts to each? Or, if you are sending too many prompts at once, how can you manage the <span class="No-Break">error/response codes?</span></p>
			<p>The answer is by using the Azure APIM service. APIs form the basis of an APIM service instance. Each API consists of a group of operations that app developers can use. Each API has a link to the backend service that provides the API, and its operations correspond to backend operations. Operations in APIM have many configuration options, with control over URL mapping, query and path parameters, request and response content, and operation response caching. We won’t cover these additional features, such as URL mapping and response caching, in this book, but you can read more about APIM in the reference link at the end of <span class="No-Break">this chapter.</span></p>
			<p><em class="italic">Using APIM is yet another way to help organizations scale up to meet business and </em><span class="No-Break"><em class="italic">user requirements.</em></span></p>
			<p>For example, you can also create a “spillover” scenario, where you may be sending prompts to PTUs that have been enabled for deploying an AOAI account. Then, if you exceed PTU limits, you can spill over to a TPM-enabled AOAI account that is used in the <span class="No-Break">pay-as-you-go model.</span></p>
			<p>The <a id="_idIndexMarker671"/>following figure shows the basic setup, but this architecture can scale and also include many other Azure cloud resources. However, for simplicity and focus, only the relevant services are <span class="No-Break">depicted here:</span></p>
			<div>
				<div id="_idContainer090" class="IMG---Figure">
					<img src="image/Figure_7.5_-_AOAI_and_APIM_in_a_single_Azure_region.jpg" alt="Figure 7.5 – AOAI and APIM in a single Azure region"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.5 – AOAI and APIM in a single Azure region</p>
			<p>As described in the single region scenario, you can use APIM to queue and send prompts to any AOAI endpoint, so long as those endpoints can be reached. In a multi-region example, as shown in the following figure, we have two AOAI accounts in one region (one PTU and another TPM), and then a third Azure OpenAI account in another <span class="No-Break">Azure region.</span></p>
			<p>Thus, a single APIM service can easily scale and support many AOAI accounts, even across multiple regions, as <span class="No-Break">described here:</span></p>
			<div>
				<div id="_idContainer091" class="IMG---Figure">
					<img src="image/Figure_7.6_-_Multi-region_AOAI_deployment_using_a_single_APIM_service.jpg" alt="Figure 7.6 – Multi-region AOAI deployment using a single APIM service"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.6 – Multi-region AOAI deployment using a single APIM service</p>
			<p>As you<a id="_idIndexMarker672"/> can see, a single APIM service can serve multiple AOAI accounts, both in the same Azure region <em class="italic">and</em> also in <span class="No-Break">multiple regions.</span></p>
			<p>As we continue our “scaling” journey, it is a good time to mention that APIM has three production-level tiers: Basic, Standard, and Premium. With the Premium tier, you can use a single APIM instance in as many Azure regions as you need, so long as APIM can access the AOAI endpoint in the other region(s). When you make an APIM service, the instance has only one unit in a single Azure region (the main region). What does this provide? If you have a multi-regional Azure OpenAI deployment, does this mean you are required to also have a multi-region (Premium) SKU of APIM? No, not necessarily. As shown in the preceding multi-region architecture, a single APIM service instance can support multi-region, multi-AOAI accounts. Having a single APIM service makes sense when an application using the service is in the same region and you <a id="_idIndexMarker673"/>do not need <strong class="bold">disaster </strong><span class="No-Break"><strong class="bold">recovery</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">DR</strong></span><span class="No-Break">).</span></p>
			<p>However, as this chapter is about scaling at an enterprise level, we recommend multiple APIM service accounts to cover the DR scenario using the APIM <span class="No-Break">Premium SKU.</span></p>
			<p>The Premium SKU allows you to have one region be the primary and any number of regions as secondaries. In this case, you can use a secondary, or multiple secondaries, in different scenarios – for example, if you are planning for any DR scenarios, which is always recommended for any enterprise architecture. Note that your enterprise applications should also be designed for data resiliency using DR strategies. Another example is if you are monitoring the APIM services. If you are seeing extremely heavy usage and can scale out your application(s) across regions, then you may want to deploy APIM service instances across <span class="No-Break">multiple regions.</span></p>
			<p>For more <a id="_idIndexMarker674"/>information on how to deploy an APIM service instance to multiple Azure regions, please see How to deploy an Azure API Management service instance to multiple Azure <span class="No-Break">regions: </span><a href="https://learn.microsoft.com/en-us/azure/api-management/api-management-howto-deploy-multi-reg﻿ion"><span class="No-Break">https://</span></a><a href="https://learn.microsoft.com/en-us/azure/api-management/api-management-howto-deploy-multi-region"/><span class="No-Break">https://learn.microsoft.com/en-us/azure/api-management/api-management-howto-deploy-multi-region</span></p>
			<h2 id="_idParaDest-150"><a id="_idTextAnchor151"/>Retries with exponential backoff – the scaling special sauce</h2>
			<p>So, how do <a id="_idIndexMarker675"/>we control (or queue) messages when using multiple Azure OpenAI instances (accounts)? How do we manage return error codes highly efficiently to optimize the <span class="No-Break">AOAI experience?</span></p>
			<p>As a best practice, Microsoft, and any other cloud vendor, will recommend the use of “retry logic” or a “retry pattern” whenever using a cloud service. This retry pattern, when used in cloud applications, helps the applications deal with temporary (transient) failures while then attempting to re-establish a connection, or reconnect, to a service to perform requests on that service, thus automatically repeating a failed operation without additional user intervention. As cloud services are cloud-based and applications or users are remote to the cloud-based service, this retry pattern is paramount. This retry logic can improve the stability of the application and provide a better end <span class="No-Break">user experience.</span></p>
			<p>Using a cloud-based service, such as ChatGPT on Azure OpenAI, especially at scale via an application, is <span class="No-Break">no exception.</span></p>
			<p>While you can add some retry logic directly to your application, you are quite limited as you scale across the enterprise. Are you now using the retry logic again and again with every application? What if the application was written by a third party? In that scenario, you can’t (usually) edit <span class="No-Break">code directly.</span></p>
			<p>Instead, to achieve stability and high scalability, using the APIM service described previously will provide the necessary retry pattern/logic. For example, if your application sends prompt and if the server is too busy or some other error occurs, APIM will be able to resend the same prompt again, without any additional end user interaction. This will all <span class="No-Break">happen seamlessly.</span></p>
			<p>APIM allows us to do this easily using the scaling special sauce – the concept of <em class="italic">retries with exponential backoff</em>, which allows for extremely high, concurrent <span class="No-Break">user loads.</span></p>
			<p>Retries with exponential backoff is a method that tries an operation again, with a wait time that grows exponentially, until it reaches a maximum number of retries (the exponential backoff). This technique accepts the fact that cloud resources may sometimes be unreachable for more than a few seconds for any reason, known as a transient error, or if an error is returned due to too many tokens per second being processed in a <span class="No-Break">large-scale deployment.</span></p>
			<p>This can be accomplished via APIM’s retry policy. Here’s <span class="No-Break">an example:</span></p>
			<pre class="source-code">
&lt;retry condition="@(context.Response.StatusCode == 429 || context.Response.StatusCode &gt;= 500)" interval="1" delta="1" max-interval="30" count="3"&gt;</pre>			<p>In this example, the<a id="_idIndexMarker676"/> error is specific to an HTTP response status code equal to 429, which is the return code for “server busy.” This states that <em class="italic">too many concurrent requests</em> were sent to a particular model, measured at <em class="italic">a per-second rate</em>. This can occur as an enterprise organization is scaling to a large number <span class="No-Break">of users.</span></p>
			<p>Here are the detailed values and explanation of the APIM <span class="No-Break">policy statement:</span></p>
			<pre class="source-code">
&lt;retry
    condition="Boolean expression or literal"
    count="number of retry attempts"
    interval="retry interval in seconds"
    max-interval="maximum retry interval in seconds"
    delta="retry interval delta in seconds"
    first-fast-retry="boolean expression or literal"&gt;
        &lt;!-- One or more child policies. No restrictions. --&gt;
&lt;/retry&gt;</pre>			<p>The format and what each value means is fairly evident, however for a deeper dive, you can learn more about the parameters by reading the link to the documentation provided at the end of <span class="No-Break">this chapter.</span></p>
			<p>The main and extremely important point to understand is that when the APIM’s interval, max interval, and delta parameters are specified, as they are in the preceding example, then an <em class="italic">exponential interval retry</em> algorithm is automatically applied by APIM. This is what we call the <em class="italic">scaling special sauce</em> – that is, the exponential interval retry special sauce needed to scale using any combination of multiple AOAI accounts to meet the most demanding <span class="No-Break">business/user requirements.</span></p>
			<p>For those interested<a id="_idIndexMarker677"/> in the mathematical logic behind this, here is the calculation that’s used by APIM for the exponential interval <span class="No-Break">retry formula:</span></p>
			<pre class="source-code">
interval + 2^(count - 1) * random(delta * 0.8, delta * 1.2), up to the maximum interval (max-interval)</pre>			<p>Without the scaling special sauce (APIM using retries with exponential backoff), once the initial rate limit is hit, say due to too many concurrent users sending too many prompts, then a 429 error return code (server busy) response code is <span class="No-Break">sent back.</span></p>
			<p>Furthermore, as additional subsequent prompts/completions are sent, the issue can be compounded quickly as more 429 errors are returned, and the error rates increase further and further. It is with the retries with exponential backoff that you are then able to scale many thousands of concurrent users with very low error responses, providing scalability to the <span class="No-Break">AOAI service.</span></p>
			<p>In addition to using retries with exponential backoff, APIM also supports content-based routing. This is where the message routing endpoint is determined by the content of the message at runtime. You can leverage this to send AOAI prompts to multiple AOAI accounts, including both PTUs and TPMs, to meet further scaling requirements. For example, if your model API request states a specific version, say gpt-35-turbo-16k, you can route this request to your GPT 3.5 Turbo (16K) PTUs deployment. This is true whether you’re in the <a id="_idIndexMarker678"/>same region or a <span class="No-Break">multi-region deployment.</span></p>
			<p>We could write an entire book on all the wonderful scaling features APIM provides, but for additional details on APIM, please check out the APIM link at the end of this chapter. Alternatively, you can refer to the great book <em class="italic">Enterprise API Management</em>, by Luis Weir, published by <span class="No-Break">Packt Publishing</span></p>
			<h2 id="_idParaDest-151"><a id="_idTextAnchor152"/>Rate Limiting Policy in Azure API Management</h2>
			<p>Rate limiting<a id="_idIndexMarker679"/> in Azure API Management is a policy that restricts <a id="_idIndexMarker680"/>the number of requests a user can make to an API within a certain timeframe, ensuring cost control, fair usage and protecting the API from overuse and abuse. Just as we have rate limits at the OpenAI API level discussed above with TPM and RPM, we can also set rate limiting policies in Azure API management too. This has several benefits as mentioned <span class="No-Break">below –</span></p>
			<ul>
				<li><strong class="bold">Prevents Overuse</strong>: Ensures no single user can monopolize API resources by making too <span class="No-Break">many requests.</span></li>
				<li><strong class="bold">Manages Resources</strong>: Helps in evenly distributing server resources to maintain <span class="No-Break">service reliability.</span></li>
				<li><strong class="bold">Controls Costs</strong>: Avoids unexpected spikes in usage that could lead to higher <span class="No-Break">operational costs.</span></li>
				<li><strong class="bold">Enhances Security</strong>: Acts as a defense layer against attacks, such as Denial of Service (DoS), by limiting <span class="No-Break">request rates.</span></li>
				<li><strong class="bold">Ensures Quality of Service</strong>: Guarantees fair resource distribution among all users to maintain expected <span class="No-Break">service levels.</span></li>
				<li><strong class="bold">Promotes Operational Stability</strong>: Contributes to the API’s stability and predictability by allowing for effective <span class="No-Break">resource planning.</span></li>
			</ul>
			<p>Now that we have a good grasp on fundamental components of scaling and strategies with our special scaling sauce on Azure API Management, let’s turn our attention to Monitoring and Logging capabilities that can help build telemetry on our Gen AI application that can help you measure critical metrics to determine the performance and availability of <span class="No-Break">your application.</span></p>
			<h1 id="_idParaDest-152"><a id="_idTextAnchor153"/>Monitoring, logging, and HTTP return codes</h1>
			<p>As we have learned in the previous sections, both limits and how we manage these limits using various scaling techniques can help us provide a robust, enterprise-class, highly scalable cloud GenAI service to many thousands of users/demanding <span class="No-Break">enterprise applications.</span></p>
			<p>But as with any good enterprise-class service, it’s important to configure and deploy the basic telemetry data provided by monitoring and logging to ensure optimal performance and timely notifications in case <span class="No-Break">of issues.</span></p>
			<h2 id="_idParaDest-153"><a id="_idTextAnchor154"/>Monitoring and logging</h2>
			<p>One of the most critical operational<a id="_idIndexMarker681"/> categories that is required for any robust enterprise service or solution that’s designed to be enterprise-ready are monitoring/instrumentation/observability and logging of <span class="No-Break">the solution.</span></p>
			<p>These components are required for any enterprise-level service, and you may already be familiar with the concepts or have a lot of experience in these areas, so we will not cover this extensively, only how monitoring and logging pertain to running a GenAI/ChatGPT-based cloud service, as well as some <span class="No-Break">best practices.</span></p>
			<p>Any enterprise monitoring solution can be used for health-checking applications and services, as well as setting up alerts to be notified if certain thresholds are reached or exceeded, such as protection against automated and high volume misuse or other anomalies related to unusual usage patterns. Two very well and broadly used services, Azure Monitoring and DataDog, both have operational modules for use with OpenAI/Azure OpenAI. These enterprise tools know which metrics are important to collect, display, and alert on for the success and optimal health of your cloud <span class="No-Break">GenAI service.</span></p>
			<p>Monitoring transactional<a id="_idIndexMarker682"/> events, such as <strong class="bold">TokenTransaction</strong>, <strong class="bold">Latency</strong>, or <strong class="bold">TotalError</strong> to <a id="_idIndexMarker683"/>name <a id="_idIndexMarker684"/>a few, can provide valuable insight into how your Cloud ChatGPT service is operating, or alert you if settings or conditions are not within your ideal parameters. The alerting and notification of these available metrics are highly configurable. You can find the complete list of metrics <span class="No-Break">here: </span><a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/monitoring#azure-openai-metrics"><span class="No-Break">https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/monitoring#azure-openai-metrics</span></a><span class="No-Break">.</span></p>
			<p>For more information about<a id="_idIndexMarker685"/> OpenAI monitoring by Datadog, check <span class="No-Break">out </span><a href="https://www.datadoghq.com/solutions/openai/"><span class="No-Break">https://www.datadoghq.com/solutions/openai/</span></a><span class="No-Break">.</span></p>
			<p>On a related note, application <a id="_idIndexMarker686"/>logging is critical to the success of reviewing events either in real time or after they have occurred. All metrics described previously can be collected and stored, reported in real-time for historical analysis, and output to visualization tools such as Fabric (Power BI) using Log Analytics Workspace in Azure, <span class="No-Break">for example.</span></p>
			<p>Every cloud GenAI application will have different logging requirements defined by the business/organization. As such, Microsoft has created a monitoring and logging AOAI best practices guide, a link to which you can find at the end of <span class="No-Break">this chapter.</span></p>
			<h2 id="_idParaDest-154"><a id="_idTextAnchor155"/>HTTP return codes</h2>
			<p>HTTP return codes, sometimes <a id="_idIndexMarker687"/>generically called “error codes” and briefly mentioned in the previous section, provide a way to validate. This is a standard web pattern many web developers will <span class="No-Break">easily recognize.</span></p>
			<p>Remember that when your application is sending prompts, it does so via HTTP <span class="No-Break">API calls.</span></p>
			<p>As described in the <em class="italic">Retries with exponential backoff – the scaling special sauce</em> section, you can use retries with exponential backoff for any 429 errors based on the APIM retry <span class="No-Break">policy document.</span></p>
			<p>However, as a best practice, you should always configure error checking regarding the size of the prompt against the model this prompt is intended for first. For example, for GPT-4 (8k), this model supports a maximum request token limit of 8,192 tokens for each prompt+completion. If your prompt has a 10K token size, then this will cause the entire prompt to fail due to the token size being too large. You can continue retrying but the result will be the same – any subsequent retries would fail as well as the token limit size has already been exceeded. As a best practice, ensure the size of the prompt does not exceed the maximum request token limit immediately, before sending the prompt across the wire to the AOAI service. Again, here are the token size limits for <span class="No-Break">each model:</span></p>
			<table id="table003-1" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">HTTP </strong><span class="No-Break"><strong class="bold">Response Code</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Cause</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Remediation</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Notes</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">200</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Processed the prompt. Completion <span class="No-Break">without error.</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">N/A</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Successful completion</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>429 (v0613 <span class="No-Break">AOAI Models)</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Server busy (rate limit reached <span class="No-Break">for requests).</span></p>
						</td>
						<td class="No-Table-Style">
							<p>APIM – retries with <span class="No-Break">exponential backoff</span></p>
						</td>
						<td class="No-Table-Style">
							<p>When the APIM’s interval, max interval, and delta are specified, an exponential interval retry algorithm is <span class="No-Break">automatically applied</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>424 (v0301 <span class="No-Break">AOAI Models)</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Server busy (rate limit reached <span class="No-Break">for requests)</span></p>
						</td>
						<td class="No-Table-Style">
							<p>APIM – retries with <span class="No-Break">exponential backoff</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Same <span class="No-Break">as above</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">408</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Request timeout</span></p>
						</td>
						<td class="No-Table-Style">
							<p>APIM retry <span class="No-Break">with interval</span></p>
						</td>
						<td class="No-Table-Style">
							<p>There are many reasons why a timeout could occur, such as a network <span class="No-Break">connection/transient error</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">50x</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Internal server error due to transient backend <span class="No-Break">service error.</span></p>
						</td>
						<td class="No-Table-Style">
							<p>APIM retry with <span class="No-Break">an interval</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Retry <span class="No-Break">policy: </span><a href="https://learn.microsoft.com/en-us/azure/api-management/retry-policy"><span class="No-Break">https://learn.microsoft.com/en-us/azure/api-management/retry-policy</span></a></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">400</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Another issue with the prompt itself, such as the prompt size being too large for the <span class="No-Break">model type</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Use APIM logic or application logic to return a custom error immediately, without sending it to the model for <span class="No-Break">further processing</span></p>
						</td>
						<td class="No-Table-Style">
							<p>After immediately evaluating the prompt, a response is sent back, so no further processing <span class="No-Break">is needed</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.7 – HTTP return codes</p>
			<p>The <a id="_idIndexMarker688"/>preceding table lists the most common HTTP return codes so that you can programmatically manage and handle each return code accordingly, based on the response. Together with monitoring and logging, your application and services can better handle most scaling aspects of generative AI <span class="No-Break">service behaviors.</span></p>
			<p>Next, we will learn about some additional considerations you should account for in your generative AI <span class="No-Break">scaling journey.</span></p>
			<h1 id="_idParaDest-155"><a id="_idTextAnchor156"/>Costs, training and support</h1>
			<p>To round off this chapter on deploying ChatGPT in the cloud with architecture design and scaling strategies, three additional areas are associated with a scaled enterprise service: costs, training <span class="No-Break">and support.</span></p>
			<h2 id="_idParaDest-156"><a id="_idTextAnchor157"/>Costs</h2>
			<p>Throughout this chapter, we discussed many <a id="_idIndexMarker689"/>services for a robust, enterprise-ready cloud ChatGPT service. While we wanted to focus on technical aspects of architecture design and scaling strategies, the topic of costs will (and should) be discussed, a critical factor from an ROI perspective that executives invariably weigh. Recognizing its significance, this section is dedicated to understanding the various elements that influence costs, alongside discussing strategies for cost optimization across different architectural layers – namely, the Model, Data, Application, and <span class="No-Break">Infrastructure Layers.</span></p>
			<p>There are variations in costs and these costs also change over time for any service. That is the nature of any business, not only a technology-based solution such as ChatGPT. We won’t list exact pricing here as it will have already changed once this book has been published, if not sooner! Instead, we wanted to mention some of the categories to consider when pricing the solution. This varies by vendor, how large or small your enterprise solution is, and a dozen <span class="No-Break">other factors.</span></p>
			<p>You must understand that there is not only the pricing of the GenAI/LLM models themselves to consider, each with its versions and types, but also how quickly you want those processed and also cost varies depending on the cost model – Pay-As-You-Go or PTU, as we described when we covered the TPMs and PTUs topic earlier <span class="No-Break">this chapter.</span></p>
			<p>Of course, there is the <a id="_idIndexMarker690"/>cost of any ancillary services to support your enterprise-ready GenAI deployment, and the costs of training and support, as described earlier in this section, as well as the cost of staff who design, deploy, manage, and operate the robust enterprise <span class="No-Break">cloud solution.</span></p>
			<p>Below, we list cost considerations and some optimization best practices to help lower cost or <span class="No-Break">reduce resources:</span></p>
			<h3>Model and Data Layer</h3>
			<ul>
				<li><strong class="bold">Model selection</strong>: Choose a <a id="_idIndexMarker691"/>pre-trained model that closely aligns with your task requirements. This can reduce the need for extensive fine-tuning and data collection, saving time and resources. Use popular benchmarks discussed in <a href="B21443_03.xhtml#_idTextAnchor052"><span class="No-Break"><em class="italic">Chapter 3</em></span></a> to shortlist your models for a particular task. Consider small language models and open source models for low-impact, internal (non-client) facing applications and batch tasks to reduce costs where quality and performance is not of the <span class="No-Break">highest importance.</span></li>
				<li><strong class="bold">Data efficiency</strong>: Utilize data augmentation techniques to create more training data from your existing dataset. This can help you achieve better results with less data, reducing storage and processing costs. Textbook quality data can help you achieve more high performing models with less tokens. For example, Phi-2 a 2.7B parameter model was created using textbook quality synthetic datasets. It outperforms models 25x its size on <span class="No-Break">complex benchmarks.</span></li>
				<li><strong class="bold">Early stopping</strong>: Implement early stopping during training to prevent overfitting and reduce training time. This helps you find a good model without wasting resources on <span class="No-Break">unnecessary iterations.</span></li>
				<li><strong class="bold">Model optimization</strong>: Prune or quantize your model to reduce its size and computational requirements. This can lead to faster training and inference, lowering cloud costs. Model quantization leads to reduced memory, faster computation, energy <a id="_idIndexMarker692"/>efficiency, network efficiency and hence leading to <span class="No-Break">reduced costs.</span></li>
			</ul>
			<h3>Application Layer</h3>
			<ul>
				<li><strong class="bold">API Parameters</strong>: These are configurable settings or values used to customize the behavior of an API, allowing users to control aspects such as data processing, request format, and response content. Setting appropriate parameters ensures efficient utilization of resources and optimal interaction with <span class="No-Break">the API.</span><ul><li><strong class="bold">Token Size</strong>: Always <a id="_idIndexMarker693"/>set the max_tokens parameter to control the token size per <span class="No-Break">API call.</span></li><li><strong class="bold">Batch requests</strong>: Instead of individual requests consider sending batch requests to reduce the <span class="No-Break">overall costs.</span></li></ul></li>
				<li><strong class="bold">Caching</strong>: For applications where the same inputs might result in the same outputs frequently, implement caching mechanisms to save on compute costs by serving cached results instead of <span class="No-Break">regenerating them.</span></li>
				<li><strong class="bold">Prompt Guide</strong>: Offer users guidance on crafting effective prompts with a sample prompt guide/collection. This approach ensures users can achieve their desired outcomes.with <span class="No-Break">minimal iterations.</span></li>
				<li><strong class="bold">Context Window</strong>: Despite the rise of context window lengths up to a million in LLMs, it’s crucial not to default to utilizing the full extent in every instance. Especially in RAG applications, strategically optimizing to use only a minimal number of tokens is key for <span class="No-Break">cost efficiency.</span></li>
			</ul>
			<h3>Infrastructure Layer</h3>
			<ul>
				<li><strong class="bold">Cloud infrastructure</strong>: Leverage <a id="_idIndexMarker694"/>cloud platforms that offer flexible pricing options and pay-as-you-go models. This allows you to scale your resources up or down based on your needs, avoiding unnecessary costs. Consider using managed services like Autoscaling and terminate compute instances <span class="No-Break">when idle.</span></li>
				<li><strong class="bold">Spot VMs or Preemptimble VMs</strong>: If not using PaaS services, then look for Spot or Low Priority VMs for model training or fine-tuning to benefit from <span class="No-Break">lower pricing.</span></li>
				<li><strong class="bold">Reserved Instances</strong>: If you have predictable, steady-state workloads, purchasing reserved instances can offer significant savings over on-demand pricing in exchange for committing to a one-year or three-year term. This is beneficial for customer facing workloads where predictable performance is important. E.g <span class="No-Break">Azure PTUs</span></li>
				<li><strong class="bold">Rate Limiting</strong>: Rate limiting in Azure API Management is a policy to control the number of processed requests by a client within a specified time period, ensuring fair usage and preventing abuse of the API. This can help control <span class="No-Break">costs too.</span></li>
				<li><strong class="bold">Monitoring and logging</strong>: Continuously monitor your model’s performance and resource usage. This helps you identify areas for optimization and potential cost savings. You can build this telemetry using Azure API Management and Azure <span class="No-Break">Cost </span><span class="No-Break"><a id="_idIndexMarker695"/></span><span class="No-Break">Monitor.</span></li>
			</ul>
			<p class="callout-heading">Note:</p>
			<p class="callout">We advise implementing a telemetry solution early to monitor your application’s token usage for prompts and completions. This allows for informed decisions between PTU and Pay-As-You-Go as your workload grows. Gradually scaling your solution to production through a ramp-up approach is recommended for <span class="No-Break">cost-effective management.</span></p>
			<ul>
				<li><strong class="bold">Data Transfer Costs/Egress Costs</strong>: In a multi-cloud and/or a multi-region setup, monitoring egress usage and fees is crucial to managing total solution costs effectively. The <span class="No-Break">traditional observability</span></li>
				<li><strong class="bold">Data Storage</strong>: Store your training datasets or files generated from AI applications in lower cost object storage like Azure Blob, S3 or Google Cloud Storage when <a id="_idIndexMarker696"/>possible. Utilize compression techniques to reduce <span class="No-Break">storage costs.</span></li>
			</ul>
			<h2 id="_idParaDest-157"><a id="_idTextAnchor158"/>Training</h2>
			<p>You have already started your journey of <a id="_idIndexMarker697"/>training for ChatGPT and OpenAI, especially if you have read this book thus far. There are many forms of learning and training that we already know about, but the key point here is that it is important to be knowledgeable or have staff/colleagues trained in not only the ChatGPT services themselves but other related services as well. We mentioned a few of these other services in the previous chapter, such as the APIM service, enterprise monitoring, instrumentation, logging, application and web development and management, and data science and analytics to name <span class="No-Break">a few.</span></p>
			<p>Another aspect of training may include database management training, especially a NoSQL type of enterprise service such as Azure CosmosDB. Why? Typically, a large organization would want to save their prompt and completion history, for example, so that they can retrieve it later or search without having to resend the same prompts again. This does make for a highly efficient and optimized ChatGPT cloud service, with all the benefits a NoSQL database, such as CosmosDB, can provide – such as being highly performant, having lower costs, and being a globally scalable service. <strong class="bold">Based on our experience, we have found that CosmosDB can be beneficial for Caching and Session Management of Conversational generative </strong><span class="No-Break"><strong class="bold">AI applications.</strong></span></p>
			<p>Of course, no one person can run an enterprise solution, so you are not expected to know the intricate details and job tasks for every service – this is what an enterprise cloud team does… and does it as a team! However, identifying training requirements for the enterprise services you will run and identifying any gaps early in the service planning life cycle is highly recommended and a <span class="No-Break">best practice.</span></p>
			<h2 id="_idParaDest-158"><a id="_idTextAnchor159"/>Support</h2>
			<p>Just like training is a critical part of <a id="_idIndexMarker698"/>designing and scaling a ChatGPT for cloud solutions, so is supporting this <span class="No-Break">enterprise solution/service.</span></p>
			<p>Many aspects of support need to be considered: internal technical support for the end users who may be using your enterprise-ready service and the internal support provided by various workload owners, including both primary and ancillary services, as <span class="No-Break">described earlier.</span></p>
			<p>However, this is not only internal support, but also any external, third-party, and vendor cloud support you will need to consider. Both OpenAI and Azure provide many tiers of support, whether it is free-to-low-cost self-service forums, where communities support each other, or paid support by trained personnel who can quickly resolve an enterprise issue, and they have personnel trained in all aspects (components) of the service. These paid support services can have many tiers of support, depending on how quickly you want the solution to be resolved <a id="_idIndexMarker699"/>based on your <span class="No-Break">internal SLAs.</span></p>
			<p>When designing and scaling a ChatGPT for cloud solutions, ensure “support” is on your checklist of items for a successful, robust deployment. This category cannot be overlooked <span class="No-Break">or skipped.</span></p>
			<h1 id="_idParaDest-159"><a id="_idTextAnchor160"/>Summary</h1>
			<p>In this chapter on deploying GenAI in the cloud, we learned how to design and scale a robust, enterprise ready GenAI cloud solution. We covered what limits exist within each of the models and how to overcome these limits either by adding additional (Azure) OpenAI accounts and/or using an Azure <span class="No-Break">APIM service.</span></p>
			<p>APIM, with its very important exponential interval retry setting, is yet another way to help organizations scale up to meet business and <span class="No-Break">user requirements.</span></p>
			<p>Reserved capacity, known as PTUs in Microsoft Azure, is another way an enterprise can scale up to meet business requirements. We described how additional PTUs can be added and scaled by increasing the number <span class="No-Break">of PTUs.</span></p>
			<p>During our cloud scaling journey, we learned how to scale across multiple geographies, or multi-regions, to support broader scale globally, while also supporting our enterprise <span class="No-Break">DR scenarios.</span></p>
			<p>We now understand how to handle various response and error codes when making API calls against our generative AI models, and we also know about best practices such as always configuring error checking the size of the prompt against the model this prompt is intended for first for a more <span class="No-Break">optimized experience.</span></p>
			<p>Then, you learned about the scaling special sauce, an insightful technique that ensures both a large-scale and seamless experience by using a retry pattern known as retries with exponential backoff. With this technique, scaling at extremely large user and prompt counts can <span class="No-Break">be achieved.</span></p>
			<p>As we wrapped up, we described how monitoring/instrumentation/observability plays a critical part in the overall solution by providing alerting notifications and deeper insights into the operational side of the service. Logging further supports the operational requirements for the enterprise, such as using logs for real-time analytics or historical data, so that it can be presented <span class="No-Break">in reports.</span></p>
			<p>Finally, we covered categories that will require further investigation as you design a scalable and robust enterprise ChatGPT cloud solution – training, support, <span class="No-Break">and costs.</span></p>
			<p>In the next chapter, we will learn about another important aspect for enterprises that want to scale and deploy ChatGPT in the cloud: security. We will look at some of the critical security considerations or concerns for deploying ChatGPT for cloud solutions, as well as how to best address them for a continued robust, enterprise-ready <span class="No-Break">cloud solution.</span></p>
			<h1 id="_idParaDest-160"><a id="_idTextAnchor161"/>References</h1>
			<ul>
				<li>Manage Azure OpenAI Service <span class="No-Break">Quota: </span><a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/quota?tabs=rest"><span class="No-Break">https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/quota?tabs=rest</span></a></li>
				<li>OpenAI rate limits in <span class="No-Break">headers: </span><a href="https://platform.openai.com/docs/guides/rate-limits/rate-limits-in-headers"><span class="No-Break">https://platform.openai.com/docs/guides/rate-limits/rate-limits-in-headers</span></a></li>
				<li>What is Azure API <span class="No-Break">Management?: </span><a href="https://learn.microsoft.com/en-us/azure/api-management/api-management-key-concepts"><span class="No-Break">https://learn.microsoft.com/en-us/azure/api-management/api-management-key-concepts</span></a></li>
				<li>Azure API Management retry <span class="No-Break">policy: </span><a href="https://learn.microsoft.com/en-us/azure/api-management/retry-policy"><span class="No-Break">https://learn.microsoft.com/en-us/azure/api-management/retry-policy</span></a></li>
				<li>How to deploy an Azure API Management service instance to multiple Azure <span class="No-Break">regions: </span><a href="https://learn.microsoft.com/en-us/azure/api-management/api-management-howto-deploy-multi-region"><span class="No-Break">https://learn.microsoft.com/en-us/azure/api-management/api-management-howto-deploy-multi-region</span></a></li>
				<li>Token size limits for each model in Azure <span class="No-Break">OpenAI: </span><a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models"><span class="No-Break">https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models</span></a></li>
				<li>Provisioned Throughput Units (PTU) getting started <span class="No-Break">guide: </span><a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/provisioned-get-started"><span class="No-Break">https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/provisioned-get-started</span></a></li>
				<li>Azure OpenAI monitoring and logging best practices <span class="No-Break">guide: </span><a href="https://techcommunity.microsoft.com/t5/fasttrack-for-azure/optimizing-azure-openai-a-guide-to-limits-quotas-and-best/ba-p/4076268"><span class="No-Break">https://techcommunity.microsoft.com/t5/fasttrack-for-azure/optimizing-azure-openai-a-guide-to-limits-quotas-and-best/ba-p/4076268</span></a></li>
				<li>Azure OpenAI <span class="No-Break">pricing: </span><a href="https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/"><span class="No-Break">https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/</span></a></li>
			</ul>
		</div>
	

		<div id="_idContainer093" class="Content">
			<h1 id="_idParaDest-161" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor162"/>Part 4: Building Safe and Secure AI – Security and  Ethical Considerations</h1>
		</div>
		<div id="_idContainer094">
			<p>This part will cover everything you need to know about creating AI applications that are not only safe and secure but also built with a responsible AI-first mindset. We’ll look into the security risks associated with generative AI, including the dangers of deepfakes, and discuss strategies to counter these issues, such as Red Teaming. We’ll introduce the principles of Responsible AI, highlight the emerging start-up ecosystem in this field, and examine the current global regulations surrounding AI. Additionally, we’ll explore how organizations can best prepare for these <span class="No-Break">regulatory environments.</span></p>
			<p>This part contains the <span class="No-Break">following chapters:</span></p>
			<ul>
				<li><a href="B21443_08.xhtml#_idTextAnchor163"><em class="italic">Chapter 8</em></a>, <em class="italic">Security and Privacy Considerations for Generative AI: Building Safe and Secure LLMs</em></li>
				<li><a href="B21443_09.xhtml#_idTextAnchor184"><em class="italic">Chapter 9</em></a>, <em class="italic">Responsible Development of AI Solutions: Building with Integrity and Care</em></li>
			</ul>
		</div>
		<div>
			<div id="_idContainer095">
			</div>
		</div>
		<div>
			<div id="_idContainer096" class="Basic-Graphics-Frame">
			</div>
		</div>
	</body></html>